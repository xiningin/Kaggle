{"cell_type":{"96af73e4":"code","c0a9a7f5":"code","02a08c4c":"code","48f6ade6":"code","dc997355":"code","12a331be":"code","b500dd2a":"code","bacc2bb4":"code","85806d95":"code","d9e52302":"code","24e12a0f":"code","f16bd275":"code","57f6c2ae":"code","19dbdb19":"code","d9c9616b":"code","7e233e3c":"code","3f350d45":"code","cee080e6":"code","0651520b":"code","3e38a648":"code","cf861d01":"code","7c732993":"code","92230676":"code","d59b6df5":"code","1eb5f1bb":"code","31ab15a5":"code","0a1349c2":"code","de39e04d":"code","0f3c803f":"code","4422199b":"code","024590f5":"code","75a95056":"code","4c27f076":"code","99b708d3":"code","b9563ebe":"code","e235cafe":"code","41c803da":"code","c80be996":"code","58207371":"code","5c975ac6":"code","f22d5203":"code","0d98f396":"code","70bc934d":"code","b3057c85":"code","90ba4869":"code","d333dfbf":"code","2fe45130":"code","ef8b8ff3":"code","c83efd72":"code","2a3d1180":"code","5c0438ba":"code","7ca2f4ce":"code","d8218c20":"code","7bfb4d29":"code","71709982":"code","e609febf":"code","debd0b7e":"code","7f09c187":"code","f3763976":"markdown","9b78eabe":"markdown","addfa965":"markdown","c5f872ad":"markdown","2ef753ad":"markdown","8002db98":"markdown","145e97b2":"markdown","af0f68de":"markdown","26abd9a1":"markdown","8a476c55":"markdown","07df4c8c":"markdown","6e85d522":"markdown","964e7613":"markdown","14e7da97":"markdown","4e034dac":"markdown","caea41cb":"markdown","29a318e6":"markdown","e3775eb0":"markdown","ebf2bc74":"markdown","c2c8b9d5":"markdown","a7659a81":"markdown","75dbceb0":"markdown","31e96c33":"markdown","81175fcc":"markdown","0fa5059e":"markdown","4f1232a4":"markdown","05db9b2f":"markdown","ebe34584":"markdown","ca17b426":"markdown","3f036f56":"markdown","97caace5":"markdown","6a004000":"markdown","94c46f37":"markdown","0e1285b1":"markdown","478f3c8d":"markdown","28a8f5b2":"markdown","11622482":"markdown","d05f54db":"markdown","0dba9078":"markdown","d4251709":"markdown","c9973c0d":"markdown","960749ad":"markdown","961944bb":"markdown","7dcfe74f":"markdown","ae8ecbaf":"markdown","8770849e":"markdown","53ca07e3":"markdown","201c8d07":"markdown","efab81f9":"markdown","2e488504":"markdown","2c6790d3":"markdown","21b6b846":"markdown","eabd5177":"markdown","bd118663":"markdown"},"source":{"96af73e4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom scipy.stats import norm, skew\nimport pylab\n\nimport warnings\n\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n%matplotlib inline","c0a9a7f5":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","02a08c4c":"train.head(3)","48f6ade6":"train.shape","dc997355":"# Check for duplicates\nUniqueIds = len(set(train.Id))\nidsTotal = train.shape[0]\nidsDupli = idsTotal - UniqueIds\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")","12a331be":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#drop the  'Id' colum since we don't need it for prediction\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n","b500dd2a":"train.info()","bacc2bb4":"numeric_var = train.select_dtypes(exclude=['object']).drop(['MSSubClass','SalePrice'], axis=1).copy() #MSSubClass is nominal\nnumeric_var.columns","85806d95":"disc_num_var = ['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\n\ncont_num_var = []\nfor i in numeric_var.columns:\n    if i not in disc_num_var:\n        cont_num_var.append(i)\nprint(\"Continuous features are:\\n\",cont_num_var,\"\\n\")\nprint(\"Discrete features are:\\n\",disc_num_var)","d9e52302":"cat_train = train.select_dtypes(include=['object']).copy()\ncat_train['MSSubClass'] = train['MSSubClass']   #MSSubClass is nominal\ncat_train.columns","24e12a0f":"fig = plt.figure(figsize=(14,15))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(8,3,index+1)\n    sns.histplot(numeric_var.loc[:,col].dropna(), kde=False)\nfig.tight_layout(pad=1.0)","f16bd275":"fig = plt.figure(figsize=(14,15))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.boxplot(y=col, data=numeric_var.dropna())\nfig.tight_layout(pad=1.0)","57f6c2ae":"fig = plt.figure(figsize=(14,15))\nfor index,col in enumerate(disc_num_var):\n    plt.subplot(5,3,index+1)\n    sns.countplot(x=col, data=numeric_var.dropna())\nfig.tight_layout(pad=1.0)","19dbdb19":"fig = plt.figure(figsize=(20,20))\nfor index in range(len(cat_train.columns)):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=cat_train.iloc[:,index], data=cat_train.dropna())\n    plt.xticks(rotation=90)\nfig.tight_layout(pad=1.0)","d9c9616b":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(corrmat, mask = corrmat <0.5, vmax=.8, square=True, cmap='Reds');","7e233e3c":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values, cmap='Reds')\nplt.show()","3f350d45":"#descriptive statistics summary\ntrain['SalePrice'].describe()","cee080e6":"#histogram\nsns.displot(train['SalePrice'], kde=True);","0651520b":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","3e38a648":"#applying log transformation on SalePrice to make it normal distribution\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\nsns.displot(train['SalePrice'], kde=True);","cf861d01":"stats.probplot(train.SalePrice,plot=pylab)","7c732993":"out_col = ['LotFrontage','LotArea','BsmtFinSF1','TotalBsmtSF','GrLivArea']\nfig = plt.figure(figsize=(20,5))\nfor index,col in enumerate(out_col):\n    plt.subplot(1,5,index+1)\n    sns.boxplot(y=col, data=train)\nfig.tight_layout(pad=1.5)","92230676":"#train = train.drop(train[train['LotFrontage'] > 200].index)\n#train = train.drop(train[train['LotArea'] > 100000].index)\n#train = train.drop(train[train['BsmtFinSF1'] > 4000].index)\ntrain = train.drop(train[train['TotalBsmtSF'] > 5000].index)\ntrain = train.drop(train[train['GrLivArea'] > 4000].index)","d59b6df5":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","1eb5f1bb":"all_data.drop(['GarageYrBlt','TotRmsAbvGrd','GarageArea'], axis=1, inplace=True)","31ab15a5":"#missing data\ntotal = all_data.isnull().sum().sort_values(ascending=False)\npercent = (100 * all_data.isnull().sum()\/all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data = missing_data.head(35)\nmissing_data.style.background_gradient(cmap='Reds')\n","0a1349c2":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","de39e04d":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nall_data['GarageCars'] = all_data['GarageCars'].fillna(0)","0f3c803f":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","4422199b":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","024590f5":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","75a95056":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","4c27f076":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","99b708d3":"# Drop\nall_data = all_data.drop(['Utilities'], axis=1)","b9563ebe":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","e235cafe":"# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","41c803da":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","c80be996":"all_data = all_data.drop(['BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF'], axis=1)","58207371":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n#Adding all bathrooms\nall_data['TotalBath'] = all_data['BsmtFullBath'] + all_data['FullBath'] + all_data['HalfBath']","5c975ac6":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#OverallCond: Rates the overall condition of the house\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","f22d5203":"numeric_features = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"Skew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head()\n","0d98f396":"skewness = skewness[abs(skewness) > 0.5]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))","70bc934d":"# Fixing Skewed features using boxcox transformation. \nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlmbd = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lmbd)","b3057c85":"from sklearn.preprocessing import LabelEncoder\ncols = ('LotShape', 'LandSlope', 'Street', 'Alley', 'OverallQual', 'OverallCond', 'ExterQual',\n        'ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1', 'BsmtFinType2','HeatingQC',\n        'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish','GarageQual', 'GarageCond',\n        'PavedDrive', 'PoolQC', 'Fence') \n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","90ba4869":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","d333dfbf":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","2fe45130":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nimport xgboost as xgb\nimport lightgbm as lgb","ef8b8ff3":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","c83efd72":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","2a3d1180":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","5c0438ba":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","7ca2f4ce":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","d8218c20":"score1 = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score1.mean(), score1.std()))\n\nscore2 = rmsle_cv(ENet)\nprint(\"\\nElastic Net score: {:.4f} ({:.4f})\\n\".format(score2.mean(), score2.std()))\n\nscore3 = rmsle_cv(KRR)\nprint(\"\\nKernelRidge score: {:.4f} ({:.4f})\\n\".format(score3.mean(), score3.std()))\n\nscore4 = rmsle_cv(GBoost)\nprint(\"\\nGradient Boosting score: {:.4f} ({:.4f})\\n\".format(score4.mean(), score4.std()))\n\n","7bfb4d29":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","71709982":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","e609febf":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","debd0b7e":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","7f09c187":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = stacked_pred\nsub.to_csv('submission.csv',index=False)","f3763976":"## 3.4 Skewed Features","9b78eabe":"Most of the features have only 1 value. \n\n* BsmtFinSF2\n* LowQualFinSF\n* EnclosedPorch\n* 3SsnPorch\n* ScreenPorch\n* PoolArea\n* MiscVal\n\nLots of 0s in these features. May be we can drop it.","addfa965":"Now it looks more normally distributed","c5f872ad":"* Elastic Net Regression :\nagain made robust to outliers","2ef753ad":"* GarageType: Garage location\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n\n","8002db98":"* Gradient Boosting Regression :\nWith huber loss that makes it robust to outliers","145e97b2":"The following features has extreme outliers\n* LotFrontage\n* LotArea\n* BsmtFinSF1\n* TotalBsmtSF\n* 1stFlrSF\n* GrLivArea","af0f68de":"* KitchenQual: Kitchen quality\n* Electrical: Electrical system\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* SaleType: Type of sale","26abd9a1":"## Credits:\nI followed these notebooks:\n\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Stacked-Regressions-to-predict-House-Prices\n* https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning\/notebook#1.-Understanding-Data\n* https:\/\/www.kaggle.com\/janiobachmann\/house-prices-useful-regression-techniques\/notebook","8a476c55":"## Importing Libraries","07df4c8c":"## Target Variable - SalePrice\n\nThe property's sale price in dollars. This is the target variable that you're trying to predict.","6e85d522":"* MasVnrType\n* MasVnrArea","964e7613":"## 3.5 Label Encoding Ordinal variables \n\nOrdinal data is a categorical data type where the variables have natural, ordered categories and the distances between the categories is not known. These data exist on an ordinal scale. The ordinal scale is distinguished from the nominal scale by having a ranking. \n\nOrdinal variables may contain ordered set information. So we need to Label Encode these variables.\n\nLabel encoding convets each value in a column feature to a number. It uses number sequencing that introduce relation\/comparison between values.\n","14e7da97":"## 3.2 Combining Features","4e034dac":"## Continuous variable visualization","caea41cb":"## 3.3 Changing Data Type\n* Since 'MSSubClass' and 'OverallCond' is actually a category but given as an integer column, we change its data type to string.\n* 'YrSold' and 'MoSold' are given as integer, we can change it into string","29a318e6":"# 3. Feature engineering\nFeature engineering refers to a process of selecting and transforming variables\/features in your dataset when creating a predictive model using machine learning.","e3775eb0":"### 2.3.1 Missing value Imputation","ebf2bc74":"## 2.3 Missing data","c2c8b9d5":"## 4.3 Base Models","a7659a81":"**Concatenate train and test dataset for convenience of preprocessing**","75dbceb0":"## Categorical feature visualization\n\nWe can easily plot categorical features using countplot.","31e96c33":"### 3.4.1 Box Cox Transformation of skewed features\n","81175fcc":"### Lets separate numerical and categorical columns for applying different visualization techniques on them","0fa5059e":"**Splitting the new train and test sets.**","4f1232a4":"## 4.2 Define a cross validation strategy\n","05db9b2f":"## 3.1 Drop unwanted features\n\nI have noticed that sum of ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF') is equal to '**TotalBsmtSF**'. So we can drop 'BsmtFinSF1', 'BsmtFinSF2',and 'BsmtUnfSF'\n\n","ebe34584":"* Utilities: Type of utilities available\n\nFor this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","ca17b426":"no more missing values","3f036f56":"Lets plot them using boxplots","97caace5":"The X-axis of the above plot has Quantiles values and Y-axis has the SalePrice values. From the plot, we can analyze that the data points of the SalePrice feature are falling on a straight line. This implies that it follows a normal distribution.","6a004000":"**log1p**, equal to log(1+x). When we have positively skewed data, we apply log transformation to bring them to same scale. \nHowever, log only works when all elements are greater than zero(log0 produces error as log0 is undefined). If your data is positively skewed and also contains 0 then we can not apply log transformation directly and hence we add 1 to every element and then apply log i.e. log(1+x) which in numpy can be done by log1p (meaning log 1 plus x) . \n\nTo check whether the log1p transformation worked i.e. our variable actually became normal after applying log1p, we run any statistical tests (Anderson Darling Test, KS Test etc) or simply check the probability plot. \nChecking the distribution of log1p transformation using any statistical tests (Anderson Darling Test, KS Test etc) or Q-Q plot\n\nA Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a roughly straight line.\n","94c46f37":"## 2.1 Outliers\n\n Outliers can be identified using boxplots\n \n In the visualization part , we saw that the following features have extreme outliers:\n\n* LotFrontage\n* LotArea\n* BsmtFinSF1\n* TotalBsmtSF\n* 1stFlrSF\n* GrLivArea","0e1285b1":"## Discrete numeric variable visualization","478f3c8d":"## 4.1 Importing Libraries","28a8f5b2":"# 4. Modelling","11622482":"* BsmtFinSF1\n* BsmtFinSF2\n* BsmtUnfSF\n* TotalBsmtSF\n* BsmtFullBath\n* BsmtHalfBath\n\nNA-> No Basement, for all these numeric basement values.","d05f54db":"The distribution is positive skewed","0dba9078":"**Summary:**\n\nHighly Correlated variables:\n\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* 1stFlrSF and TotalBsmtSF\n* GarageArea and GarageCars\n* OverallQual and SalePrice\n* GrLivArea and SalePrice\n","d4251709":"## 2.2 Removing redundant features\n\nWe found from the correlation matrix that the following explanatory features are highly correlated:\n\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* 1stFlrSF and TotalBsmtSF\n* GarageArea and GarageCars\n\nWe can remove some of these highly correlated features to avoid the problem of [multicollinearity](https:\/\/www.analyticsvidhya.com\/blog\/2021\/02\/multicollinearity-problem-detection-and-solution\/).\n\nWe can delete 'GarageYrblt', 'TotRmsAbvGrd', 'GarageArea'\n\n(GarageCars and GarageArea specifies the size of garage. (we keep GarageCars since it has more correlation to 'SalePrice')\n\nI am not deleting '1stFlrSF' or 'TotalBsmtSF', because we can do a little preprocessing steps on them.\n","c9973c0d":"* Functional: Home functionality (Assume typical unless deductions are warranted)","960749ad":"# 2. Data Preprocessing","961944bb":"## Data Correlation","7dcfe74f":"* MSZoning: Identifies the general zoning classification of the sale,\n\nfilling the missing values with the most frequent value","ae8ecbaf":"## Data Overview","8770849e":"### 4.3.1 Base models scores\nLet's see how these base models perform on the data by evaluating the cross-validation rmsle error","53ca07e3":"## 4.4 Stacking models","201c8d07":"### Summary\n\n1. OverallQual'and 'GrLivArea' are strongly correlated with 'SalePrice'.\n2. 'GarageCars' and 'GarageArea' also has strong correlation with 'SalePrice'. But both specify the same thing. (**GarageCars**: Size of garage in car capacity, **GarageArea**: Size of garage in square feet). So we can delete one. (we keep GarageCars since it has more correlation to 'SalePrice')\n","efab81f9":"* LASSO Regression :\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline\n\n","2e488504":"'SalePrice' is positive skewed. So have to do a simple data transformation, precisely log transformation to make it normal distribution.","2c6790d3":"## Load dataset","21b6b846":"* PoolQC : Pool quality (NA -> No Pool)\nIt makes sense that more than 99% PoolQC value is missing. Majority of houses doesn't have a pool.\n\n* MiscFeature: Miscellaneous feature not covered in other categories (NA -> None)\n* Alley: Type of alley access to property (NA -> No alley access)\n* Fence: Fence quality (NA -> No Fence)\n* FireplaceQu: Fireplace quality (Na -> No Fireplace)\n","eabd5177":"# 1. Data Exploration","bd118663":"* Kernel Ridge Regression :\n"}}