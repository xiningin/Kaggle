{"cell_type":{"80131c2c":"code","25193f8a":"code","587e7172":"code","fb5d7f09":"code","1577045f":"code","da91ca36":"code","7074db2a":"code","96d5466a":"code","a5193692":"code","e33dd346":"code","9cf77032":"code","86b3a4d1":"code","45939ce5":"code","ab9c3440":"code","b861b1a3":"code","25101be8":"code","0c90cd4f":"code","705e60db":"code","b59a595f":"code","df20ab68":"code","0a8697b4":"code","c8253385":"code","477ea7d9":"code","e3620def":"code","31766433":"code","c2ba1368":"code","c449d756":"code","8c2d731f":"code","27289b17":"code","92792ca6":"code","016a05b2":"code","a4858425":"code","cc46f401":"markdown","ad876591":"markdown","4e05cf20":"markdown","21bb0ac9":"markdown","a1fa762c":"markdown","5bd6f99a":"markdown","98630552":"markdown","8c4b2d72":"markdown","421d6d76":"markdown","402bcd49":"markdown","3c029701":"markdown","2afb7862":"markdown","25ccfaa3":"markdown","e89ba4cf":"markdown","dc139d33":"markdown","7d12b835":"markdown","373c45c4":"markdown","c2e9e6d6":"markdown","f7410fa6":"markdown","b48e78d3":"markdown","d0854098":"markdown"},"source":{"80131c2c":"#Importing Libraries\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","25193f8a":"import torch \nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\n\nimport torch.nn.functional as F","587e7172":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntrain.shape, test.shape","fb5d7f09":"#Labels(Targets) and Inputs\ntrain_labels = train['label'].values\ntrain_images = (train.iloc[:,1:].values).astype('float32')\ntest_images = (test.iloc[:,:].values).astype('float32')","1577045f":"#Training and Validation Split\ntrain_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels,\n                                                                     stratify=train_labels, random_state=302,\n                                                                     test_size=0.2)","da91ca36":"#train\ntrain_images_tensor = torch.tensor(train_images)\/255.0\ntrain_labels_tensor = torch.tensor(train_labels)\ntrain_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n#val\nval_images_tensor = torch.tensor(val_images)\/255.0\nval_labels_tensor = torch.tensor(val_labels)\nval_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n#test\ntest_images_tensor = torch.tensor(test_images)\/255.0","7074db2a":"train_loader = DataLoader(train_tensor, batch_size = 100, shuffle=True)\nval_loader = DataLoader(val_tensor, batch_size = 100, shuffle=True)\ntest_loader = DataLoader(test_images_tensor, batch_size = 100, shuffle=False)","96d5466a":"#Training and Test data size\nlen(train_images_tensor), len(val_images_tensor), len(test_images_tensor)","a5193692":"#Shape of an image\nimage = train_images[3,:]\nprint(image.shape)","e33dd346":"#Plotting an image\nimage = image.reshape([28,28])\nplt.imshow(image, cmap = \"gray\")\nprint(\"Image Label: \", train_labels[3])","9cf77032":"data_iter = iter(train_loader)\nimages, labels = next(data_iter)\nprint(\"Images shape: \", images.shape)\nprint(\"Lables shape: \", labels.shape)","86b3a4d1":"#weights and biases\nW = torch.randn(784, 10)\/np.sqrt(784)\nb = torch.zeros(10, requires_grad=True)\nW.requires_grad_()","45939ce5":"#input shape after flattening the images \nx = images.view(-1, 28*28)\nx.shape ","ab9c3440":"#Linear Transformation\ny = torch.matmul(x, W) + b\ny[0,:]","b861b1a3":"#Loss Function\ndef make_plot(x, f, name):\n    plt.figure()\n    plt.figure(figsize=(12,6))\n    plt.title(name, fontsize=20, fontweight='bold')\n    plt.xlabel('z', fontsize=15)\n    plt.ylabel('Activation function value', fontsize=15)\n    sns.set_style(\"whitegrid\")\n    plt.plot(x, f, label=\"f (z)\")\n    plt.legend(loc=4, prop={'size': 15}, frameon=True,shadow=True, facecolor=\"white\", edgecolor=\"black\")\n    plt.savefig(name + \".png\")\n    plt.show()\n\nz = np.arange(0, 10, 0.01)\nf = np.exp(z)\/sum(np.exp(z))\nmake_plot(z, f, \"Softmax\")","25101be8":"#Probability of classes function from scratch\nprob_eqn = torch.exp(y)\/torch.sum(torch.exp(y), dim=1, keepdim=True)\nprob_eqn[0]","0c90cd4f":"#Probability of classes using softmax\nprobs = F.softmax(y, dim=1)\nprobs[0]","705e60db":"#Viewing the shape of labels\nlabels.shape","b59a595f":"#Loss Function: Cross Entropy Loss\nloss = F.cross_entropy(y, labels)\nloss","df20ab68":"#Update Rule\noptimizer = torch.optim.SGD([W,b], lr=0.1)","0a8697b4":"loss.backward()","c8253385":"b.grad","477ea7d9":"optimizer.step()","e3620def":"b","31766433":"print(\"b.grad before zero_grad(): {}\".format(b.grad))\noptimizer.zero_grad()\nprint(\"b.grad after zero_grad(): {}\".format(b.grad))","c2ba1368":"fig, ax = plt.subplots(1,10, figsize =(20,2))\n\nfor d in range(10):\n    ax[d].imshow(W[:, d].detach().view(28,28), cmap='gray')","c449d756":"for images, labels in tqdm(train_loader):\n    optimizer.zero_grad() #Resetting the gradients to zero\n    \n    #Forward Propagation\n    x = images.view(-1, 28*28) #Flattening the images\n    y = torch.matmul(x, W) + b #Linear Transformation\n    loss = F.cross_entropy(y, labels)\n\n    #Back propagation\n    loss.backward()\n    optimizer.step()","8c2d731f":"correct = 0\nwith torch.no_grad():\n    for images, labels in tqdm(val_loader):\n        x = images.view(-1, 28*28)\n        y = torch.matmul(x, W)+b\n\n        prediction = torch.argmax(y, dim=1)\n        correct += torch.sum(prediction==labels).float()","27289b17":"print('Validation Accuracy : {}'.format(100*(correct\/len(val_images_tensor))) )","92792ca6":"fig, ax = plt.subplots(1,10, figsize =(20,2))\n\nfor d in range(10):\n    ax[d].imshow(W[:, d].detach().view(28,28), cmap='gray')","016a05b2":"test_preds = torch.LongTensor()\nfor images in tqdm(test_loader):\n    x = images.view(-1, 28*28)\n    y = torch.matmul(x, W)+b\n\n    prediction = torch.argmax(y, dim=1)\n    test_preds = torch.cat((test_preds, prediction), dim=0)","a4858425":"sub = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nsub['Label'] = test_preds.numpy().squeeze()\nsub.to_csv(\"results.csv\", index=False)\nsub.head()","cc46f401":"The 'require_grad' is true so that both weights and biases are learned. This tells PyTorch's autograd to track gradients for these two variable and the variables depending on W and b.","ad876591":"Initializing the parameters: Weights(W) and Biases(b),","4e05cf20":"Preprocessing the data to in torch data loaders with batches.","21bb0ac9":"Following are the softmax implementaions that squash output of forward propagation 'y' into range of (0,1) by applying softmax activation function. This can be done by either hard-coding the softmax function or by using sotmax from the torch.nn.Functional module.\n\n\\begin{align}\np(y_i) = \\text{softmax}(y_i) = \\frac{\\text{exp}(y_i)}{\\sum_j\\text{exp}(y_j)}\n\\end{align}","a1fa762c":"We've now successfully trained on a minibatch! However, one minibatch probably isn't enough. At this point, we've trained the model on 100 examples out of the 60000 in the training set. We're going to need to repeat this process, for more of the data.\n\nOne more thing to keep in mind though: gradients calculated by backward() don't override the old values; instead, they accumulate. Therefore, we'll need to clear the gradient buffers before you compute gradients for the next minibatch.","5bd6f99a":"## 5. Training\n\nTraining on complete dataset. ","98630552":"# Thank you!\n\nAuthor: [Pratik Kumar](https:\/\/pr2tik1.github.io)<br>\nReferences: [fastai](https:\/\/www.fast.ai), [Coursera](https:\/\/coursera.org\/share\/ea5cc945df9fab563eb9bc9de3089eb9)","8c4b2d72":"To compute the gradients for W and b we need to call the backward() finction on the cross entropy loss function(loss).","421d6d76":"## 6. Validation \n\nValidating our model,","402bcd49":"Loss Function\/Cost Function- Cross Entropy which is suitable for multi-class classification tasks.\n\n\\begin{align}\nH_{y'}(y)=-\\sum_i y'_i \\text{log}(y_i)\n\\end{align}\n","3c029701":"## 1. Data\n\nOur dataset is not in the form of images, but is in the form of csv. We will be separating the labels values and pixel intensity values from the columns of dataframe.","2afb7862":"Our learning rate is 0.1 so b is now updated by -0.1*b.grad,","25ccfaa3":"## 7. Plotting the learned weights\n\nThe following plots are what our logistic regression model has learned. These parameters are what our model 'sees' and classifies them into the classes of digits.","e89ba4cf":"## Predictions\n\nFinally predicting and submitting the results.","dc139d33":"Now the variables that required gradients have now accumulated gradients. For example,","7d12b835":"## 4. Backprop\n\nWe have forward propagation and loss, now we need to update weights that is measured by penalising the models's output from forward propagation using loss function. The updating of weights while going backwards in the model is what we call training process. This updating of weights needs an updating rule or 'optimization function' like gradient functions. We will be using SGD- Stochastic Gradient Descent. \n\n\n\\begin{align}\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta \\mathcal{L}\n\\end{align}\n\nwhere $\\theta$ is a parameter, $\\alpha$ is our learning rate (step size), and $\\nabla_\\theta \\mathcal{L}$ is the gradient of our loss with respect to $\\theta$.","373c45c4":"To apply gradient to update W and b we use the updation rule, i.e., our optimizer function. ","c2e9e6d6":"## 3. Forward pass \nForward Propagation is where we apply randomly initialized weights and biases. These parameters are the 'knobs' to tune our model for less loss and better prediction.\n\n\\begin{align}\nY = X W + b \n\\end{align}\n","f7410fa6":"# Logistic Regression using PyTorch\n---\n\nLogisitc Regression is a linear classifier, which is very simple to understand. This forms one of the reason why neural networks are used and how are they better when compared to linear classifier like Logisitc Regression. The Logisitc Regression model is \"linear\" because of its decision boundary is linear. This is good for simple datasets to classify them into multi or binary classes.\n\nIt's training or learning process can be broken down to forward and backward passes. We will be using the widely used MNIST data for multi-class image classification and PyTorch to implement model from scratch.    ","b48e78d3":"## 2. Model\n\nWe will be first performing one single forward pass and backpropagation, to understand what's going under the hood. This is rather very simple model. This does not involve anything 'deep'. On comparing it with neural networks we can consider neural networks as using logistic regression k-times with non-linear activation functions.  \n\n","d0854098":"Let's have a look on Weights for this training(1 minibatch). It looks messy! But our model will learn and we will check again at last. "}}