{"cell_type":{"6a92d76c":"code","592d7182":"code","fbdf817a":"code","79379f44":"code","1b247f36":"code","72ec521d":"code","2b854770":"code","29df421b":"code","5496c04a":"code","9ba8f843":"code","39354bd4":"code","3a253fa4":"code","4f0f42a7":"code","12c27cce":"code","37d5c2f7":"code","7f50daec":"code","cbf0cda0":"code","8f2cfc79":"code","e6e59c58":"code","f2d74eac":"code","c36ab84d":"code","153ca296":"code","3f8559e8":"code","679c21df":"code","1e65e746":"code","5e465f45":"code","18a412c4":"code","1adb7da3":"code","77043294":"code","89a3d66c":"code","3b0fec10":"code","0cfb5a36":"code","297c6564":"code","41a6b805":"code","8be2855c":"code","a190ee99":"code","540aa740":"code","ff00e5cb":"code","af2a3303":"markdown","91bb6266":"markdown","5586bc53":"markdown","8e686a3a":"markdown","d7449167":"markdown","7956b0a9":"markdown","b7206670":"markdown","a2239bdd":"markdown","fca067cc":"markdown","0f8c8190":"markdown"},"source":{"6a92d76c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","592d7182":"import nltk\nimport random\nfrom sklearn.metrics import confusion_matrix","fbdf817a":"# President 1\n\nwith open(\"..\/input\/apj01.txt\", \"r\", encoding = 'latin-1') as f:\n    apj01_docs = f.read()\n    apj01_docs = apj01_docs.split('\\n')\n\nwith open(\"..\/input\/apj02.txt\", \"r\", encoding = 'latin-1') as f:\n    apj02_docs = f.read()\n    apj02_docs = apj02_docs.split('\\n')\n    \nwith open(\"..\/input\/apj03.txt\", \"r\", encoding = 'latin-1') as f:\n    apj03_docs = f.read()\n    apj03_docs = apj03_docs.split('\\n')\n\nwith open(\"..\/input\/apj04.txt\", \"r\", encoding = 'latin-1') as f:\n    apj04_docs = f.read()\n    apj04_docs = apj04_docs.split('\\n')\n    \nwith open(\"..\/input\/apj05.txt\", \"r\", encoding = 'latin-1') as f:\n    apj05_docs = f.read()\n    apj05_docs = apj05_docs.split('\\n')\n    \nwith open(\"..\/input\/apj6.txt\", \"r\", encoding = 'latin-1') as f:\n    apj06_docs = f.read()\n    apj06_docs = apj06_docs.split('\\n')\n    \nwith open(\"..\/input\/apj07.txt\", \"r\", encoding = 'latin-1') as f:\n    apj07_docs = f.read()\n    apj07_docs = apj07_docs.split('\\n')\n    \nwith open(\"..\/input\/apj08.txt\", \"r\", encoding = 'latin-1') as f:\n    apj08_docs = f.read()\n    apj08_docs = apj08_docs.split('\\n')\n    \nwith open(\"..\/input\/apj09.txt\", \"r\", encoding = 'latin-1') as f:\n    apj09_docs = f.read()\n    apj09_docs = apj09_docs.split('\\n')\n    \nwith open(\"..\/input\/apj10.txt\", \"r\", encoding = 'latin-1') as f:\n    apj10_docs = f.read()\n    apj10_docs = apj10_docs.split('\\n')\n    \nwith open(\"..\/input\/apj11.txt\", \"r\", encoding = 'latin-1') as f:\n    apj11_docs = f.read()\n    apj11_docs = apj11_docs.split('\\n')\n    \nwith open(\"..\/input\/apj12.txt\", \"r\", encoding = 'latin-1') as f:\n    apj12_docs = f.read()\n    apj12_docs = apj12_docs.split('\\n')","79379f44":"combined_apj = apj01_docs+apj02_docs+apj03_docs+apj04_docs+apj05_docs+apj06_docs+apj07_docs+apj08_docs+apj09_docs+apj10_docs+apj11_docs+apj12_docs","1b247f36":"combined_apj","72ec521d":"# President 2\n\nwith open(\"..\/input\/niranjan01.txt\", \"r\", encoding = 'latin-1') as f:\n    niranjan01_docs = f.read()\n    niranjan01_docs = niranjan01_docs.split('\\n')\n        \nwith open(\"..\/input\/niranjan02.txt\", \"r\", encoding = 'latin-1') as f:\n    niranjan02_docs = f.read()\n    niranjan02_docs = niranjan02_docs.split('\\n')\n    \nwith open(\"..\/input\/niranjan03.txt\", \"r\", encoding = 'latin-1') as f:\n    niranjan03_docs = f.read()\n    niranjan03_docs = niranjan03_docs.split('\\n')\n    \nwith open(\"..\/input\/niranjan04.txt\", \"r\", encoding = 'latin-1') as f:\n    niranjan04_docs = f.read()\n    niranjan04_docs = niranjan04_docs.split('\\n')\n    \nwith open(\"..\/input\/niranjan05.txt\", \"r\", encoding = 'latin-1') as f:\n    niranjan05_docs = f.read()\n    niranjan05_docs = niranjan05_docs.split('\\n')","2b854770":"combined_niranjan = niranjan01_docs+niranjan02_docs+niranjan03_docs+niranjan04_docs+niranjan05_docs","29df421b":"combined_niranjan","5496c04a":"# President 3\n\nwith open(\"..\/input\/obama01.txt\", \"r\", encoding = 'latin-1') as f:\n    obama01_docs = f.read()\n    obama01_docs = obama01_docs.split('\\n')\n    \nwith open(\"..\/input\/obama02.txt\", \"r\", encoding = 'latin-1') as f:\n    obama02_docs = f.read()\n    obama02_docs = obama02_docs.split('\\n')\n    \nwith open(\"..\/input\/obama03.txt\", \"r\", encoding = 'latin-1') as f:\n    obama03_docs = f.read()\n    obama03_docs = obama03_docs.split('\\n')\n    \nwith open(\"..\/input\/obama04.txt\", \"r\", encoding = 'latin-1') as f:\n    obama04_docs = f.read()\n    obama04_docs = obama04_docs.split('\\n')\n    \nwith open(\"..\/input\/obama05.txt\", \"r\", encoding = 'latin-1') as f:\n    obama05_docs = f.read()\n    obama05_docs = obama05_docs.split('\\n')\n    \nwith open(\"..\/input\/obama06.txt\", \"r\", encoding = 'latin-1') as f:\n    obama06_docs = f.read()\n    obama06_docs = obama06_docs.split('\\n')\n    \nwith open(\"..\/input\/obama07.txt\", \"r\", encoding = 'latin-1') as f:\n    obama07_docs = f.read()\n    obama07_docs = obama07_docs.split('\\n')\n    \nwith open(\"..\/input\/obama08.txt\", \"r\", encoding = 'latin-1') as f:\n    obama08_docs = f.read()\n    obama08_docs = obama08_docs.split('\\n')\n    \nwith open(\"..\/input\/obama09.txt\", \"r\", encoding = 'latin-1') as f:\n    obama09_docs = f.read()\n    obama09_docs = obama09_docs.split('\\n')\n    \nwith open(\"..\/input\/obama10.txt\", \"r\", encoding = 'latin-1') as f:\n    obama10_docs = f.read()\n    obama10_docs = obama10_docs.split('\\n')\n    \nwith open(\"..\/input\/obama11.txt\", \"r\", encoding = 'latin-1') as f:\n    obama11_docs = f.read()\n    obama11_docs = obama11_docs.split('\\n')\n    \nwith open(\"..\/input\/obama12.txt\", \"r\", encoding = 'latin-1') as f:\n    obama12_docs = f.read()\n    obama12_docs = obama12_docs.split('\\n')","9ba8f843":"combined_obama = obama01_docs+obama02_docs+obama03_docs+obama04_docs+obama05_docs+obama06_docs+obama07_docs+obama08_docs+obama09_docs+obama10_docs+obama11_docs+obama12_docs\n","39354bd4":"combined_obama","3a253fa4":"# President 4\n\nwith open(\"..\/input\/pra01.txt\", \"r\", encoding = 'latin-1') as f:\n    pra01_docs = f.read()\n    pra01_docs = pra01_docs.split('\\n')\n    \nwith open(\"..\/input\/pra02.txt\", \"r\", encoding = 'latin-1') as f:\n    pra02_docs = f.read()\n    pra02_docs = pra02_docs.split('\\n')\n    \nwith open(\"..\/input\/pra03.txt\", \"r\", encoding = 'latin-1') as f:\n    pra03_docs = f.read()\n    pra03_docs = pra03_docs.split('\\n')\n    \nwith open(\"..\/input\/pra04.txt\", \"r\", encoding = 'latin-1') as f:\n    pra04_docs = f.read()\n    pra04_docs = pra04_docs.split('\\n')\n    \nwith open(\"..\/input\/pra05.txt\", \"r\", encoding = 'latin-1') as f:\n    pra05_docs = f.read()\n    pra05_docs = pra05_docs.split('\\n')\n    \nwith open(\"..\/input\/pra06.txt\", \"r\", encoding = 'latin-1') as f:\n    pra06_docs = f.read()\n    pra06_docs = pra06_docs.split('\\n')\n    \nwith open(\"..\/input\/pra07.txt\", \"r\", encoding = 'latin-1') as f:\n    pra07_docs = f.read()\n    pra07_docs = pra07_docs.split('\\n')\n    \nwith open(\"..\/input\/pra08.txt\", \"r\", encoding = 'latin-1') as f:\n    pra08_docs = f.read()\n    pra08_docs = pra08_docs.split('\\n')\n    \nwith open(\"..\/input\/pra09.txt\", \"r\", encoding = 'latin-1') as f:\n    pra09_docs = f.read()\n    pra09_docs = pra09_docs.split('\\n')\n    \nwith open(\"..\/input\/pra10.txt\", \"r\", encoding = 'latin-1') as f:\n    pra10_docs = f.read()\n    pra10_docs = pra10_docs.split('\\n')\n    \nwith open(\"..\/input\/pra11.txt\", \"r\", encoding = 'latin-1') as f:\n    pra11_docs = f.read()\n    pra11_docs = pra11_docs.split('\\n')\n    \nwith open(\"..\/input\/pra12.txt\", \"r\", encoding = 'latin-1') as f:\n    pra12_docs = f.read()\n    pra12_docs = pra12_docs.split('\\n')","4f0f42a7":"combined_pra = pra01_docs+pra02_docs+pra03_docs+pra04_docs+pra05_docs+pra06_docs+pra07_docs+pra08_docs+pra09_docs+pra10_docs+pra11_docs+pra12_docs","12c27cce":"combined_pra","37d5c2f7":"# President 5\n\nwith open(\"..\/input\/romney01.txt\", \"r\", encoding = 'latin-1') as f:\n    romney01_docs = f.read()\n    romney01_docs = romney01_docs.split('\\n')\n    \nwith open(\"..\/input\/romney02.txt\", \"r\", encoding = 'latin-1') as f:\n    romney02_docs = f.read()\n    romney02_docs = romney02_docs.split('\\n')\n    \nwith open(\"..\/input\/romney03.txt\", \"r\", encoding = 'latin-1') as f:\n    romney03_docs = f.read()\n    romney03_docs = romney03_docs.split('\\n')\n    \nwith open(\"..\/input\/romney04.txt\", \"r\", encoding = 'latin-1') as f:\n    romney04_docs = f.read()\n    romney04_docs = romney04_docs.split('\\n')\n    \nwith open(\"..\/input\/romney05.txt\", \"r\", encoding = 'latin-1') as f:\n    romney05_docs = f.read()\n    romney05_docs = romney05_docs.split('\\n')\n    \nwith open(\"..\/input\/romney06.txt\", \"r\", encoding = 'latin-1') as f:\n    romney06_docs = f.read()\n    romney06_docs = romney06_docs.split('\\n')\n    \nwith open(\"..\/input\/romney07.txt\", \"r\", encoding = 'latin-1') as f:\n    romney07_docs = f.read()\n    romney07_docs = romney07_docs.split('\\n')\n    \nwith open(\"..\/input\/romney08.txt\", \"r\", encoding = 'latin-1') as f:\n    romney08_docs = f.read()\n    romney08_docs = romney08_docs.split('\\n')\n    \nwith open(\"..\/input\/romney09.txt\", \"r\", encoding = 'latin-1') as f:\n    romney09_docs = f.read()\n    romney09_docs = romney09_docs.split('\\n')","7f50daec":"combined_romney = romney01_docs+romney02_docs+romney03_docs+romney04_docs+romney05_docs+romney06_docs+romney07_docs+romney08_docs+romney09_docs","cbf0cda0":"combined_romney","8f2cfc79":"print(len(combined_apj))\nprint(len(combined_niranjan))\nprint(len(combined_obama))\nprint(len(combined_pra))\nprint(len(combined_romney))","e6e59c58":"final_combined_doc = combined_apj + combined_niranjan + combined_obama + combined_pra + combined_romney\nlabels = ['APJ']*len(combined_apj) + ['NIR']*len(combined_niranjan) + ['OBA']*len(combined_obama) + ['PRA']*len(combined_pra) + ['ROM']*len(combined_romney)\n\nfinal_combined_df = pd.DataFrame({\"Review\": final_combined_doc, \"Sentiment\": labels})\n#combined_df = combined_df.sample(frac=1)\nfinal_combined_df = final_combined_df.sample(frac=1)\n\n\nfinal_combined_df.head(50)","f2d74eac":"final_combined_df.isna().sum(axis = 0)","c36ab84d":"final_combined_df2 = final_combined_df.copy()","153ca296":"final_combined_df2.reset_index(drop = True, inplace = True)","3f8559e8":"final_combined_df2.head(20)","679c21df":"final_combined = pd.DataFrame(final_combined_df2, columns=['Review', 'Sentiment']).to_csv('Final_Combined.csv')","1e65e746":"df = final_combined_df2.sort_values(['Review'])\ndf.reset_index(drop = True, inplace = True)\ndf.head()","5e465f45":"df = df[df.Review != '']\ndf.head()","18a412c4":"df = df[df.Review != ' ']\ndf.head()","1adb7da3":"df.reset_index(drop = True, inplace = True)\ndf","77043294":"combined_df = df.copy()","89a3d66c":"from sklearn.model_selection import train_test_split\n\ny = combined_df['Sentiment'].tolist()\nX = combined_df.loc[:,'Review'].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1224)","3b0fec10":"from sklearn.feature_extraction.text import TfidfVectorizer\ntokenizer = TfidfVectorizer(ngram_range = (1,3), stop_words=None,min_df=10) # min_df and stop_words act as dim reduction\nX_train_tf = tokenizer.fit_transform(X_train).toarray()\nX_test_tf = tokenizer.transform(X_test).toarray()\n\nprint(X_train_tf.shape)\nprint(X_test_tf.shape)","0cfb5a36":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=1200, n_iter=10, random_state=42)\nX_train_tf = svd.fit_transform(X_train_tf)\nprint(svd.explained_variance_ratio_.sum())\nX_test_tf = svd.transform(X_test_tf)","297c6564":"X_test_tf.shape","41a6b805":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nNBclassifier = GaussianNB()\nNBclassifier.fit(X_train_tf, y_train)\n\n## Predictions\ntrain_nb_preds = NBclassifier.predict(X_train_tf)\ntest_nb_preds = NBclassifier.predict(X_test_tf)\n\nprint(\"Train Accuracy\",NBclassifier.score(X_train_tf,y_train))\nprint(confusion_matrix(y_train,train_nb_preds))\nprint(classification_report(y_train,train_nb_preds))\n\nprint(\"Test Accuracy\",NBclassifier.score(X_test_tf,y_test))\nprint(confusion_matrix(y_test,test_nb_preds))\nprint(classification_report(y_test,test_nb_preds))","8be2855c":"from sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.fit(X_train_tf,y_train)\n\n## Predictions\ntrain_logit_preds = logit.predict(X_train_tf)\ntest_logit_preds = logit.predict(X_test_tf)\n\nprint(\"Train Accuracy\",logit.score(X_train_tf,y_train))\nprint(confusion_matrix(y_train,train_logit_preds))\nprint(classification_report(y_train,train_logit_preds))\n\nprint(\"Test Accuracy\",logit.score(X_test_tf,y_test))\nprint(confusion_matrix(y_test,test_logit_preds))\nprint(classification_report(y_test,test_logit_preds))","a190ee99":"\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train_tf, y_train)","540aa740":"rfc_train_pred = classifier.predict(X_train_tf)\nrfc_test_pred = classifier.predict(X_test_tf)","ff00e5cb":"from sklearn.metrics import classification_report, recall_score, precision_score, f1_score, make_scorer, accuracy_score\nprint(\"Train\")\nprint(accuracy_score(y_train,rfc_train_pred))\nprint(confusion_matrix(y_train,rfc_train_pred))\nprint(classification_report(y_train,rfc_train_pred))\n\nprint(\"Test\")\nprint(accuracy_score(y_test,rfc_test_pred))\nprint(confusion_matrix(y_test,rfc_test_pred))\nprint(classification_report(y_test,rfc_test_pred))","af2a3303":"## Random Forest","91bb6266":"## Naive Bayes","5586bc53":"## Combining documents","8e686a3a":"## TF-IDF Vectorization","d7449167":"## Bringing down the dimentionality using SVD (optional)","7956b0a9":"## Reading the text file","b7206670":"## Logistic Regression","a2239bdd":"### Continuation: Grid Search, SVM, KNN","fca067cc":"## Train-Test Split","0f8c8190":"# President speech - text analysis"}}