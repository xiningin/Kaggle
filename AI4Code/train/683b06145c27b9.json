{"cell_type":{"3c373967":"code","81cbfcb2":"code","0c8a99fb":"code","65fb32ce":"code","12f85d06":"code","4cd0f454":"code","af46f11b":"code","0a6f5038":"code","48f5e113":"code","2a2c8605":"code","7edc4d1e":"code","21ef571f":"code","42da2b75":"code","d160de5f":"markdown","712bfde8":"markdown"},"source":{"3c373967":"import os\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","81cbfcb2":"df = pd.read_csv(\"..\/input\/isekai-light-novel-titles-and-descriptions\/light-novel-titles.csv\")\ndf[\"title_word_count\"] = df.titles.apply(lambda x: len(str(x).split()))\ndf.head()","0c8a99fb":"def preprocess_text(text):\n    text = text.lower()\n    text = text.replace(\"(LN)\", \"\")\n    text = re.sub(r'[^\\w\\s]','',text)\n    return str(text)","65fb32ce":"text_df = df.titles[df[\"title_word_count\"] > 4] # get rid of short titles\ntext = text_df.apply(preprocess_text).to_numpy() ","12f85d06":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nvocab_size = 10000\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(text)","4cd0f454":"total_words = len(tokenizer.word_index) + 1","af46f11b":"input_sequences = []\nfor title in text:\n    token_list = tokenizer.texts_to_sequences([title])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n\nys = tf.keras.utils.to_categorical(labels, num_classes=total_words)","0a6f5038":"model = keras.Sequential()\nmodel.add(keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(150)))\nmodel.add(keras.layers.Dense(total_words, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\nhistory = model.fit(xs, ys, epochs=55, verbose=1)","48f5e113":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use('dark_background') \nimport matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()\n\nplot_graphs(history, \"accuracy\")","2a2c8605":"def generate_text(seed_text, next_words=10):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        #predicted = model.predict_classes(token_list, verbose=0)\n        predicted = np.argmax(model.predict(token_list), axis=1)\n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    return(seed_text)","7edc4d1e":"generate_text(\"the incompetent hero\", 30)","21ef571f":"generate_text(\"i\", 25)","42da2b75":"generate_text(\"living a slow life in\", 30)","d160de5f":"Code modified from [here](https:\/\/colab.research.google.com\/github\/lmoroney\/dlaicourse\/blob\/master\/TensorFlow%20In%20Practice\/Course%203%20-%20NLP\/Course%203%20-%20Week%204%20-%20Lesson%202%20-%20Notebook.ipynb) and [here](https:\/\/github.com\/somvirs57\/text_generation_tensorflow\/blob\/master\/poem_text_generator_py.py)","712bfde8":"For data analysis see [analysis-on-data](https:\/\/www.kaggle.com\/andy8744\/analysis-on-data)"}}