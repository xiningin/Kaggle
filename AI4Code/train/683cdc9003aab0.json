{"cell_type":{"f9538cd2":"code","67577dc3":"code","81c0be4c":"code","08e42b73":"code","e922fb78":"code","7e9e990d":"code","c46e8a76":"code","e88e6bf8":"code","a4424b8b":"code","7856793d":"code","83670832":"code","57c39e95":"code","5bca4d96":"code","f11e51db":"code","27fac241":"code","6840cc7f":"code","77a96001":"code","02c6daba":"code","22f6e26a":"code","7e155d99":"code","96afce0f":"code","6c764586":"code","1845b8a4":"code","6a7a2135":"code","34d64a12":"code","8c4e5a05":"code","809ae5aa":"code","4388e503":"code","597b5131":"code","3403dc64":"code","4233abc8":"code","0d2f89e1":"code","efb80972":"code","b0f025f3":"code","cf3fb9d7":"code","c52fde39":"markdown","d0f5cfe7":"markdown","ba57786b":"markdown","c56a224d":"markdown","1c8d3863":"markdown","d9db02e5":"markdown","f9b91920":"markdown","8ba6934a":"markdown","1f02171f":"markdown","6fc75479":"markdown","e0b42fe3":"markdown","6c570389":"markdown","48536ba7":"markdown","854b7310":"markdown","bdf01a28":"markdown","0c806249":"markdown","7174445e":"markdown","48afc29b":"markdown"},"source":{"f9538cd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67577dc3":"train=pd.read_csv(\"\/kaggle\/input\/detecting-anomalies-in-wafer-manufacturing\/Train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/detecting-anomalies-in-wafer-manufacturing\/Test.csv\")\ntrain.head(4)","81c0be4c":"x=train.drop(\"Class\",axis=1,inplace=False)\ny=train[\"Class\"]","08e42b73":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX=scaler.fit_transform(x)\nX","e922fb78":"from sklearn.decomposition import PCA\npca=PCA(svd_solver=\"arpack\",random_state=42,tol=0.5)\npca.fit(X)\n# pca.components_","7e9e990d":"# pca.explained_variance_ this gives eigen values","c46e8a76":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=0.3)","e88e6bf8":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n# m=LogisticRegression()\nsteps = [(\"scal\",MinMaxScaler()),('pca', PCA(n_components=20)), ('m', LogisticRegression(max_iter=500))]\nmodel=Pipeline(steps=steps)\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","a4424b8b":"y_pred=model.predict(x_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n","7856793d":"y","83670832":"y.value_counts()","57c39e95":"from sklearn.utils import resample \nmaj=train[train[\"Class\"]==0]\nmin=train[train[\"Class\"]==1]\nmod_min=resample(min,n_samples=1620,replace=True,random_state=42)\nmod_min.shape\n","5bca4d96":"Train=pd.concat([maj,mod_min])\nTrain.head()","f11e51db":"xs=Train.drop(\"Class\",inplace=False,axis=1)\nys=Train[\"Class\"]\nfrom sklearn.model_selection import train_test_split\nxs_train,xs_test,ys_train,ys_test=train_test_split(xs,ys,random_state=42,test_size=0.3)\n","27fac241":"steps = [(\"scal\",MinMaxScaler()),('pca', PCA(n_components=300)), ('m', LogisticRegression(max_iter=500))]\nmodel=Pipeline(steps=steps)\nmodel.fit(xs_train,ys_train)\nmodel.score(xs_test,ys_test)","6840cc7f":"ys_pred=model.predict(xs_test)\nfrom sklearn.metrics import classification_report,roc_auc_score\nprint(classification_report(ys_test,ys_pred))","77a96001":"print(roc_auc_score(ys_test,ys_pred))","02c6daba":"stepz = [(\"scal\",MinMaxScaler())]\npipe=Pipeline(steps=stepz)\nTest=pipe.fit_transform(test)","22f6e26a":"from sklearn.model_selection import cross_val_score\ncv_scores=cross_val_score(model,xs,ys,cv=10,scoring=\"roc_auc\")\nnp.mean(cv_scores)","7e155d99":"result=pd.DataFrame(model.predict(Test))\nsubmission=result.to_excel(\"sub1.xlsx\",index=False)","96afce0f":"steps=[(\"norm\",MinMaxScaler()),(\"pca\",PCA(n_components=300)),(\"r\",RandomForestClassifier(n_estimators=100,random_state=42,max_depth=10,max_features=\"log2\"))]\nmodelR=Pipeline(steps=steps)\nmodelR.fit(xs_train,ys_train)\nmodelR.score(xs_test,ys_test)","6c764586":"ysr_pred=modelR.predict(xs_test)\nfrom sklearn.metrics import classification_report,roc_auc_score\nprint(roc_auc_score(ys_test,ysr_pred))\nprint(classification_report(ys_test,ysr_pred))","1845b8a4":"from sklearn.model_selection import cross_val_score\ncv_scores=cross_val_score(modelR,xs,ys,cv=10,scoring=\"roc_auc\")\nnp.mean(cv_scores)","6a7a2135":"res=pd.DataFrame(modelR.predict(Test))\nsubmission=res.to_excel(\"sub3.xlsx\",index=False)","34d64a12":"steps = [(\"scal\",StandardScaler())]\nmodel=Pipeline(steps=steps)\nXs=model.fit_transform(xs)","8c4e5a05":"stepz = [(\"scal\",StandardScaler())]\npipe=Pipeline(steps=stepz)\nTests=pipe.fit_transform(test)","809ae5aa":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(Xs,ys,random_state=42,test_size=0.3)","4388e503":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Dropout,Activation\nfrom tensorflow.keras.models import Sequential,load_model\nimport matplotlib\nmatplotlib.use('agg')","597b5131":"model=Sequential()\nmodel.add(Dense(500,input_shape=(1558,)))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(750))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(200))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(400))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(10))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(1))\nmodel.add(Activation(\"sigmoid\"))","3403dc64":"model.compile(loss='binary_crossentropy', metrics=['AUC'], optimizer='adam')","4233abc8":"history = model.fit(X_train, y_train,\n          batch_size=150, epochs=70,\n          verbose=2,\n          validation_data=(X_test, y_test)\n                   )","0d2f89e1":"loss_and_metrics = model.evaluate(X_test, y_test, verbose=2)\n\nprint(\"Test Loss\", loss_and_metrics[0])\nprint(\"Test Accuracy\", loss_and_metrics[1])","efb80972":"ydl_prob=model.predict_classes(X_test)\nprint(roc_auc_score(y_test,ydl_prob))","b0f025f3":"model.predict_classes(Test)","cf3fb9d7":"dlr=pd.DataFrame(model.predict_classes(Test))\ndlr.to_excel(\"dlr_final.xlsx\",index=False)\n","c52fde39":"# checking classification report to check balanced\/imbalanced data","d0f5cfe7":"# splitting the data","ba57786b":"# step 3: applying primary component analysis","c56a224d":"standardizing test data","1c8d3863":"#  Using Logistic reg","d9db02e5":"checking counts of classes","f9b91920":"getting roc_auc_score","8ba6934a":"predicted classes of test dataset","1f02171f":"# balncing data using oversampling tecnique","6fc75479":"# modelling part","e0b42fe3":"training the model and saving metrics in history","6c570389":"# using random forest","48536ba7":"# step 2: Standardizing the data","854b7310":"standarizing balanced train data","bdf01a28":"# using neural networks","0c806249":"modelling","7174445e":"compiling the sequential model using metric as AUC","48afc29b":"writing into excel file"}}