{"cell_type":{"585507aa":"code","45cc5da8":"code","addd7cd7":"code","6d91174c":"code","03079811":"code","6ffd036f":"code","e21acf0c":"code","aad82118":"code","55f615b4":"code","8dd4cc54":"code","8fa589cb":"code","85bb2778":"code","d91420c8":"code","e246ad2f":"code","1dd70a77":"code","af78aede":"code","fc82a695":"code","31a2399b":"code","c5d0beb7":"code","034c3db0":"code","fcad62e6":"code","5bfeb929":"code","912ea925":"code","737990a7":"code","f7290003":"code","8e19e2e4":"code","12e3871e":"code","73205e43":"code","0647ba37":"code","5c71d91f":"code","17f5d298":"code","40f3666f":"code","6a9338a2":"code","04289236":"markdown","321d91e9":"markdown","4bf24066":"markdown","25349db1":"markdown","9f159f34":"markdown","4c2b4324":"markdown","9b8b7256":"markdown","beb52166":"markdown","3e03bd04":"markdown","80614a33":"markdown","235fc944":"markdown","31e79b82":"markdown","0d242111":"markdown","0f04bfb3":"markdown","025f06bb":"markdown","b2b5c972":"markdown"},"source":{"585507aa":"!pip install dataprep","45cc5da8":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\nfrom dataprep.eda import create_report\nfrom dataprep.eda import plot_missing\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","addd7cd7":"data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\r\ndf = data.copy()\r\npd.set_option('display.max_row',df.shape[0])\r\npd.set_option('display.max_column',df.shape[1]) \r\ndf.head()","6d91174c":"(df.isna().sum()\/df.shape[0]*100).sort_values(ascending=False)","03079811":"plot_missing(df)","6ffd036f":"print('There is' , df.shape[0] , 'rows')\r\nprint('There is' , df.shape[1] , 'columns')","e21acf0c":"df.duplicated().sum()","aad82118":"df.loc[df.duplicated(keep=False),:]","55f615b4":"df.drop_duplicates(keep='first',inplace=True)\ndf.shape","8dd4cc54":"df['output'].value_counts(normalize=True) #Classes d\u00e9s\u00e9quilibr\u00e9es","8fa589cb":"for col in df.select_dtypes(include=['float64','int64']):\r\n    plt.figure()\r\n    sns.displot(df[col],kind='kde',height=3)\r\n    plt.show()","85bb2778":"X = df.drop('output',axis=1)\r\ny = df['output']","d91420c8":"riskyDF = df[y == 1]\r\nsafeDF = df[y == 0]","e246ad2f":"plt.figure(figsize=(4,4))\r\nsns.pairplot(data,height=1.5)\r\nplt.show()","1dd70a77":"corr = df.corr(method='pearson').abs()\r\n\r\nfig = plt.figure(figsize=(8,6))\r\nsns.heatmap(corr, annot=True, cmap='tab10', vmin=-1, vmax=+1)\r\nplt.title('Pearson Correlation')\r\nplt.show()\r\n\r\nprint (df.corr()['output'].abs().sort_values())","af78aede":"for col in df.select_dtypes(include=['float64','int64']):\r\n    plt.figure(figsize=(4,4))\r\n    sns.distplot(riskyDF[col],label='High Risk')\r\n    sns.distplot(safeDF[col],label='Low Risk')\r\n    plt.legend()\r\n    plt.show()","fc82a695":"for col in X.select_dtypes(include=['float64','int64']):\r\n    plt.figure(figsize=(4,4))\r\n    sns.lmplot(x='oldpeak', y=col, hue='output', data=df)","31a2399b":"create_report(df)","c5d0beb7":"def encoding(df):\r\n    code = {\r\n            # All columns are made of quantitative values (floats actually), so there is no need to encode the features\r\n           }\r\n    for col in df.select_dtypes('object'):\r\n        df.loc[:,col]=df[col].map(code)\r\n        \r\n    return df\r\n\r\ndef imputation(df):\r\n    \r\n    df = df.dropna(axis=0) # There are no NaN anyways\r\n    \r\n    return df\r\n\r\ndef feature_engineering(df):\r\n    useless_columns = [] # Let's consider we want to use all the features\r\n    df = df.drop(useless_columns,axis=1)\r\n    return df","034c3db0":"def preprocessing(df):\r\n    df = encoding(df)\r\n    df = feature_engineering(df)\r\n    df = imputation(df)\r\n    \r\n    X = df.drop('output',axis=1)\r\n    y = df['output']    \r\n      \r\n    return df,X,y","fcad62e6":"df = data.copy()\r\ntrainset, testset = train_test_split(df, test_size=0.2, random_state=0)\r\nprint(trainset['output'].value_counts())\r\nprint(testset['output'].value_counts())","5bfeb929":"_, X_train, y_train = preprocessing(trainset)\r\n_, X_test, y_test = preprocessing(testset)","912ea925":"preprocessor = make_pipeline(MinMaxScaler())\n\nPCAPipeline = make_pipeline(StandardScaler(), PCA(n_components=2,random_state=0))\n\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=0,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression(solver='sag'))","737990a7":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X))\nPCA_df = pd.concat([PCA_df, y], axis=1)\nPCA_df.head()","f7290003":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df['output'],palette=sns.color_palette(\"tab10\", 2))\nplt.show()","8e19e2e4":"dict_of_models = {'RandomForest': RandomPipeline,\r\n'AdaBoost': AdaPipeline,\r\n'SVM': SVMPipeline,\r\n'KNN': KNNPipeline,\r\n'LR': LRPipeline}","12e3871e":"def evaluation(model):\r\n    model.fit(X_train, y_train)\r\n    # calculating the probabilities\r\n    y_pred_proba = model.predict_proba(X_test)\r\n\r\n    # finding the predicted valued\r\n    y_pred = np.argmax(y_pred_proba,axis=1)\r\n    print('Accuracy = ', accuracy_score(y_test, y_pred))\r\n    print('-')\r\n    print(confusion_matrix(y_test,y_pred))\r\n    print('-')\r\n    print(classification_report(y_test,y_pred))\r\n    print('-')\r\n    \r\n    N, train_score, val_score = learning_curve(model, X_train, y_train, \r\n                                               cv=4, scoring='f1', \r\n                                               train_sizes=np.linspace(0.1,1,10))\r\n    plt.figure(figsize=(12,8))\r\n    plt.plot(N, train_score.mean(axis=1), label='train score')\r\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\r\n    plt.legend()","73205e43":"for name, model in dict_of_models.items():\r\n    print('---------------------------------')\r\n    print(name)\r\n    evaluation(model)","0647ba37":"AdaPipeline.fit(X_train, y_train)\r\ny_proba = AdaPipeline.predict_proba(X_test)\r\ny_pred = np.argmax(y_proba,axis=1)\r\n\r\nprint(\"Adaboost : \", accuracy_score(y_test, y_pred))","5c71d91f":"y_pred_prob = AdaPipeline.predict_proba(X_test)[:,1]\r\n\r\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\r\n\r\nplt.plot(fpr,tpr,label='AdaBoost ROC Curve')\r\nplt.xlabel(\"False Positive Rate\")\r\nplt.ylabel(\"True Positive Rate\")\r\nplt.title(\"AdaBoost ROC Curve\")\r\nplt.show()","17f5d298":"KNNPipeline.fit(X_train, y_train)\ny_proba = KNNPipeline.predict_proba(X_test)\ny_pred = np.argmax(y_proba,axis=1)\n\nprint(\"KNN : \", accuracy_score(y_test, y_pred))","40f3666f":"err = []\n  \nfor i in range(1, 40):\n    \n    model = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = i))\n    model.fit(X_train, y_train)\n    pred_i = model.predict(X_test)\n    err.append(np.mean(pred_i != y_test))\n  \nplt.figure(figsize =(10, 8))\nplt.plot(range(1, 40), err, color ='blue',\n                linestyle ='dashed', marker ='o',\n         markerfacecolor ='blue', markersize = 8)\n  \nplt.title('Mean Err = f(K)')\nplt.xlabel('K')\nplt.ylabel('Mean Err')","6a9338a2":"KNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = 7))\nKNNPipeline.fit(X_train, y_train)\ny_proba = KNNPipeline.predict_proba(X_test)\ny_pred = np.argmax(y_proba,axis=1)\n\nprint(\"KNN : \", accuracy_score(y_test, y_pred))","04289236":"<h1><center>\ud83d\udc93Heart Attack Data Analysis\ud83d\udd0e<\/center><\/h1>\n<h3><center>\ud83e\ude7a(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/intermountainhealthcare.org\/-\/media\/images\/modules\/blog\/posts\/2020\/02\/how-to-know-if-youre-having-a-heart-attack.jpg?la=en&h=595&w=896&mw=896&hash=86751E3A3A3AFDCD26067D5A66E861E4ED92AC72\" alt =\"Titanic\" style='width: 400px;'><\/center>\n\n<h3>What is a heart attack?<\/h3>\n<p>\nA heart attack, also called a myocardial infarction, happens when a part of the heart muscle doesn\u2019t get enough blood.\n    \nThe more time that passes without treatment to restore blood flow, the greater the damage to the heart muscle.\n    \nCoronary artery disease (CAD) is the main cause of heart attack. A less common cause is a severe spasm, or sudden contraction, of a coronary artery that can stop blood flow to the heart muscle.\n<\/p>\n\n<h3>What are the symptoms of heart attack?<\/h3>\n<p>\nThe major symptoms of a heart attack are :\n    \n* Chest pain or discomfort. Most heart attacks involve discomfort in the center or left side of the chest that lasts for more than a few minutes or that goes away and comes back. The discomfort can feel like uncomfortable pressure, squeezing, fullness, or pain.\n    \n* Feeling weak, light-headed, or faint. You may also break out into a cold sweat.\n    \n* Pain or discomfort in the jaw, neck, or back.\n    \n* Pain or discomfort in one or both arms or shoulders.\n    \n* Shortness of breath. This often comes along with chest discomfort, but shortness of breath also can happen before chest discomfort.\n<\/p>","321d91e9":"## Detailed Analysis","4bf24066":"## PCA Analysis","25349db1":"# Using KNN","9f159f34":"### Comments\r\n\r\nIt looks like we have some very useful features here, with a correlation > 0.4.\r\nThe following features seems promising for predicting wether a patient will have a heart attack or not :\r\n- **oldpeak**\r\n- **exng**\r\n- **cp**\r\n- **thalachh**\r\n\r\nWe can also notice that **sip** and **oldpeak** looks correlated, let's find out !","4c2b4324":"## Visualising Target and Features","9b8b7256":"# If you like please upvote !\n## Also check my other notebooks :\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83d\udc01Mice Trisomy (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83c\udf97\ufe0fBreast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### \ud83c\udf26\ud83c\udf21 Weather Forecasting \ud83d\udcc8 (98% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/weather-forecasting-98-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Heart Attack \ud83e\ude7a\ud83d\udc93 (90% Acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Mobile price (95.5% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83e\udde0 Stroke (74% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-stroke-74-acc","beb52166":"## Dataset Analysis","3e03bd04":"### Comments\r\n#### All 5 models look promising, but **AdaBoost** has a slightly better accuracy **(90%)****","80614a33":"## KNN Optimization","235fc944":"## Classification problem","31e79b82":"# Exploratory Data Analysis\r\n\r\n## Aim :\r\n- Understand the data (\"A small step forward is better than a big one backwards\")\r\n- Begin to develop a modelling strategy\r\n\r\n## Features\r\n\r\n- Age : Age of the patient\r\n\r\n- Sex : Sex of the patient\r\n\r\n- exang: exercise induced angina (1 = yes; 0 = no)\r\n\r\n- ca: number of major vessels (0-3)\r\n\r\n- cp : Chest Pain type chest pain type :\r\n  - Value 1: typical angina\r\n  - Value 2: atypical angina\r\n  - Value 3: non-anginal pain\r\n  - Value 4: asymptomatic\r\n  \r\n- trtbps : resting blood pressure (in mm Hg)\r\n\r\n- chol : cholestoral in mg\/dl fetched via BMI sensor\r\n\r\n- fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\r\n\r\n- rest_ecg : resting electrocardiographic results :\r\n  - Value 0: normal\r\n  - Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\r\n  - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\r\n\r\n- thalach : maximum heart rate achieved\r\n\r\n- target : 0= less chance of heart attack 1= more chance of heart attack\r\n\r\n\r\n## Base Checklist\r\n#### Shape Analysis :\r\n- **target feature** : output\r\n- **rows and columns** : 303 , 14\r\n- **features types** : qualitatives : 0 , quantitatives : 14\r\n- **NaN analysis** :\r\n    - NaN (0 % of NaN)\r\n\r\n#### Columns Analysis :\r\n- **Target Analysis** :\r\n    - Balanced (Yes\/No) : Yes\r\n    - Percentages : 55% \/ 45%\r\n- **Categorical values**\r\n    - There is 8 categorical features (0\/1) (not inluding the target)","0d242111":"### Comments\r\nWe can now analyze categorical features as quantitative features (rem : no qualitative features to be encoded here)","0f04bfb3":"# Modelling","025f06bb":"# A bit of data engineering ...","b2b5c972":"# Using AdaBoost"}}