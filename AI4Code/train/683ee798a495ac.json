{"cell_type":{"1b20e646":"code","700d29fd":"code","2d0474d6":"code","9cd46ee0":"code","e2a50ac4":"code","9565cc9d":"code","fe592160":"code","c4bbf911":"code","20a582f2":"code","56977756":"code","6ac6113a":"code","66656084":"code","144d859f":"code","dc2408f8":"code","ddf84431":"code","f0acb06d":"code","d7551522":"code","c4fdb154":"code","115fa92e":"code","1f709dae":"code","81185889":"code","d77f9f82":"code","2fbfa802":"code","f6439814":"code","bcef9faf":"code","22e3364d":"code","b9914436":"code","9171e0f3":"code","7a44d493":"code","e81faf54":"code","7762021d":"code","0cd725d4":"code","d7deba9c":"code","4c5de4a7":"code","b8c4f9d5":"code","d4ca6fe0":"code","7e7a9c94":"code","f8266186":"code","29edaa4c":"code","4ec663ba":"markdown","432977f4":"markdown","077be587":"markdown","46db76a8":"markdown","3ed0ee55":"markdown","ced84e3b":"markdown","0ff00292":"markdown","7e12739a":"markdown","924ee2c7":"markdown","ab3de9f3":"markdown","824c6b07":"markdown","dbd49fea":"markdown","9d24a60d":"markdown","e22fc68d":"markdown"},"source":{"1b20e646":"!head -2 ..\/input\/gendered-pronoun-resolution\/sample_submission_stage_1.csv","700d29fd":"!head -2 ..\/input\/gendered-pronoun-resolution\/test_stage_1.tsv","2d0474d6":"import pandas as pd\n\npd.set_option('max_colwidth', 4000)","9cd46ee0":"def read_df(path):\n    \n    df = pd.read_csv(path, index_col='ID', sep='\\t', encoding='utf-8')\n\n    # add some columns\n\n    # the ending offset of the pronoun and the candidates referring entities\n    df['Pronoun-offset-end'] = df['Pronoun-offset'] + df['Pronoun'].str.len()\n    df['A-offset-end'] = df['A-offset'] + df['A'].str.len()\n    df['B-offset-end'] = df['B-offset'] + df['B'].str.len()\n\n    # text length\n\n    df['Text-length'] = df['Text'].str.len()\n    \n    return df","e2a50ac4":"df = read_df('..\/input\/gendered-pronoun-resolution\/test_stage_1.tsv')\n\ndf.sample()","9565cc9d":"len(df)","fe592160":"from spacy import displacy\n\ndef display_entry(entry):\n\n    data = entry.to_dict()\n    \n    colors = {\n        'Pronoun': '#aa9cfc',\n        'A': '#fc9ce7' if not 'A-coref' in data or not data['A-coref'] else '#FFE14D',\n        'B': '#fc9ce7' if not 'B-coref' in data or not data['B-coref'] else '#FFE14D'\n    }\n\n    options = {\n        'colors': colors\n    }\n    \n    render_data = {\n        'text': data['Text'],\n        'ents': sorted([\n            {\n                'start': data['Pronoun-offset'],\n                'end': data['Pronoun-offset-end'],\n                'label': 'Pronoun'\n            },\n            {\n                'start': data['A-offset'],\n                'end': data['A-offset-end'],\n                'label': 'A'\n            },\n            {\n                'start': data['B-offset'],\n                'end': data['B-offset-end'],\n                'label': 'B'\n            }\n        ], key=lambda x: x['start'])\n    }\n    \n    displacy.render(render_data, style='ent', manual=True, jupyter=True, options=options)","c4bbf911":"sample = df.sample(random_state=100)\n\ndisplay_entry(sample.iloc[0])","20a582f2":"result = pd.DataFrame({'ID': df.index, 'A': 1, 'B': 1, 'NEITHER': 1})\n\nresult.to_csv('dummy_all_equal.csv', index=False)","56977756":"result['A'] = 1\nresult['B'] = 0\nresult['NEITHER'] = 0\n\nresult.to_csv('dummy_A.csv', index=False)","6ac6113a":"result['A'] = 0\nresult['B'] = 1\nresult['NEITHER'] = 0\n\nresult.to_csv('dummy_B.csv', index=False)","66656084":"result['A'] = 0\nresult['B'] = 0\nresult['NEITHER'] = 1\n\nresult.to_csv('dummy_NEITHER.csv', index=False)","144d859f":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.log_loss.html\nfrom sklearn.metrics import log_loss\n\ny_true = [\"spam\", \"ham\", \"ham\", \"spam\"]\n# The labels in y_pred are assumed to be ordered alphabetically, as done by preprocessing.LabelBinarizer\n# [\"ham\", \"spam\"]\ny_pred = [\n    [.1, .9], \n    [.9, .1], \n    [.8, .2], \n    [.35, .65]\n]\n\nlog_loss(y_true, y_pred)","dc2408f8":"# all prediction is correct\ny_pred = [\n    [0., 1.],\n    [1., 0.],\n    [1., 0.],\n    [0., 1.]\n]\n\nlog_loss(y_true, y_pred)","ddf84431":"# all prediction is wrong\ny_pred = [\n    [1., 0.],\n    [0., 1.],\n    [0., 1.],\n    [1., 0.]\n]\n\nlog_loss(y_true, y_pred)","f0acb06d":"samples = df.sample(n=10, random_state=100)\n\nfor _, s in samples.iterrows():\n    \n    display_entry(s)","d7551522":"import spacy\n\nnlp = spacy.load('en_core_web_lg')\n\ndisplacy.render(nlp(samples.iloc[-2]['Text']), style='dep', jupyter=True, options={'distance': 150})","c4fdb154":"!ls ..\/input\/googlegapcoreference\/","115fa92e":"# following: https:\/\/www.kaggle.com\/keyit92\/end2end-coref-resolution-by-attention-rnn\/data\n\ntrain_df = read_df(\"..\/input\/googlegapcoreference\/gap-test.tsv\")\ntest_df = read_df(\"..\/input\/googlegapcoreference\/gap-development.tsv\")\ndev_df = read_df(\"..\/input\/googlegapcoreference\/gap-validation.tsv\")","1f709dae":"print(f\"Train: {train_df.shape}\\nTest: {test_df.shape}\\nDevelopment: {dev_df.shape}\")","81185889":"sample = train_df.sample(random_state=555)\n\ndisplay_entry(sample.iloc[0])","d77f9f82":"# just testing if there is any entry with more than one answer\n\ntrain_df[train_df[['A-coref', 'B-coref']].sum(axis=1) > 1]","2fbfa802":"# adding a column with the answer\ndef get_answer(row):\n    \n    if row['A-coref']:\n        return 'A'\n    \n    if row['B-coref']:\n        return 'B'\n    \n    return 'NEITHER'\n    \ntrain_df['answer'] = train_df.apply(get_answer, axis=1)","f6439814":"train_df['answer'].value_counts()","bcef9faf":"train_df['Text-length'].hist()","22e3364d":"train_df.groupby(pd.qcut(train_df['Text-length'], q=[0, .25, .5, .75, 1.]))['answer'].value_counts().unstack()","b9914436":"train_df_A = train_df[train_df['answer'] == 'A']\ntrain_df_B = train_df[train_df['answer'] == 'B']\ntrain_df_NEITHER = train_df[train_df['answer'] == 'NEITHER']\n\nX_A_A = train_df_A.rename(columns={\n    'A': 'RE',\n    'A-offset': 'RE-offset',\n    'A-offset-end': 'RE-offset-end'\n})[['Text', 'Pronoun', 'RE', 'RE-offset', 'RE-offset-end', 'URL', 'Text-length']]\n\nX_A_A['y'] = 1\nX_A_A['referred-expression'] = 'A'\n\nX_A_B = train_df_A.rename(columns={\n    'B': 'RE',\n    'B-offset': 'RE-offset',\n    'B-offset-end': 'RE-offset-end'\n})[['Text', 'Pronoun', 'RE', 'RE-offset', 'RE-offset-end', 'URL', 'Text-length', 'answer']]\n\nX_A_B['y'] = 0\nX_A_B['referred-expression'] = 'A'\n\nX_B_B = train_df_B.rename(columns={\n    'B': 'RE',\n    'B-offset': 'RE-offset',\n    'B-offset-end': 'RE-offset-end'\n})[['Text', 'Pronoun', 'RE', 'RE-offset', 'RE-offset-end', 'URL', 'Text-length', 'answer']]\n\nX_B_B['y'] = 1\nX_B_B['referred-expression'] = 'B'\n\nX_B_A = train_df_B.rename(columns={\n    'A': 'RE',\n    'A-offset': 'RE-offset',\n    'A-offset-end': 'RE-offset-end'\n})[['Text', 'Pronoun', 'RE', 'RE-offset', 'RE-offset-end', 'URL', 'Text-length', 'answer']]\n\nX_B_A['y'] = 0\nX_B_A['referred-expression'] = 'B'\n\nX_NEITHER_A = train_df_NEITHER.rename(columns={\n    'A': 'RE',\n    'A-offset': 'RE-offset',\n    'A-offset-end': 'RE-offset-end'\n})[['Text', 'Pronoun', 'RE', 'RE-offset', 'RE-offset-end', 'URL', 'Text-length', 'answer']]\n\nX_NEITHER_A['y'] = 0\nX_NEITHER_A['referred-expression'] = 'A'\n\nX_NEITHER_B = train_df_NEITHER.rename(columns={\n    'B': 'RE',\n    'B-offset': 'RE-offset',\n    'B-offset-end': 'RE-offset-end'\n})[['Text', 'Pronoun', 'RE', 'RE-offset', 'RE-offset-end', 'URL', 'Text-length', 'answer']]\n\nX_NEITHER_B['y'] = 0\nX_NEITHER_B['referred-expression'] = 'B'\n\n\nX_df = pd.concat((X_A_A, X_A_B, X_B_A, X_B_B, X_NEITHER_A, X_NEITHER_B))","9171e0f3":"X_df.shape","7a44d493":"X_df.sample(random_state=1)","e81faf54":"import re\n\nPARENTHESIS_RE = re.compile(r'(.*?)\\((.*?)\\)')\nCAMELCASE_RE = re.compile(r'([a-z])([A-Z])')\n\ndef preprocess_so(so):\n\n    parenthesis_preprocessed = PARENTHESIS_RE.sub('\\g<2> \\g<1>', so)\n    underline_removed = parenthesis_preprocessed.replace('_', ' ')\n    camelcase_preprocessed = CAMELCASE_RE.sub('\\g<1> \\g<2>', underline_removed)\n\n    return camelcase_preprocessed.strip().replace('\"', '')","7762021d":"from textacy import similarity\n\ndef add_features(df, re_col, inplace=False):\n    \n    if inplace:\n        df_ = df\n    else:\n        df_ = df.copy()\n    \n    df_['URL_last_part'] = df_['URL'].str.rsplit('\/', n=1, expand=True)[1].apply(preprocess_so)\n    \n    df_['URL_distance_jaro_winkler'] = df_.apply(lambda row: similarity.jaro_winkler(row['URL_last_part'], row[re_col]), axis=1)\n    df_['URL_distance_levenshtein'] = df_.apply(lambda row: similarity.levenshtein(row['URL_last_part'], row[re_col]), axis=1)\n    df_['URL_distance_token_sort_ratio'] = df_.apply(lambda row: similarity.token_sort_ratio(row['URL_last_part'], row[re_col]), axis=1)\n    \n    return df_\n    \n\nadd_features(X_df, 'RE', inplace=True)\n\nX_df.sample(5, random_state=800)[['URL_last_part', 'URL']]","0cd725d4":"X_df.hist(column='URL_distance_jaro_winkler', by='y', figsize=(20, 5), bins=20, sharey=True)","d7deba9c":"X_df.hist(column='URL_distance_levenshtein', by='y', figsize=(20, 5), bins=20, sharey=True)","4c5de4a7":"X_df.hist(column='URL_distance_token_sort_ratio', by='y', figsize=(20, 5), bins=20, sharey=True)","b8c4f9d5":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX = X_df[['URL_distance_token_sort_ratio', 'URL_distance_levenshtein', 'URL_distance_jaro_winkler', 'Text-length', 'referred-expression']]\ny = X_df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23)\n\nX_train_features = X_train.drop(columns='referred-expression')\nX_train_referred_expression = X_train['referred-expression']\n\nX_test_features = X_test.drop(columns='referred-expression')\nX_test_referred_expression = X_test['referred-expression']\n\nlr = LinearRegression(normalize=True).fit(X_train_features, y_train)\n\ny_pred_ = lr.predict(X_test_features)","d4ca6fe0":"import numpy as np\n\ndef transform_to_submit(y_pred_, referred_expression):\n    \n    y_pred_comp = 1 - y_pred_\n    all_zero = np.zeros_like(y_pred_).reshape((-1, 1))\n\n    y_pred = np.hstack((\n                np.where(referred_expression == 'A', y_pred_, y_pred_comp).reshape((-1, 1)),\n                np.where(referred_expression == 'B', y_pred_comp, y_pred_).reshape((-1, 1)),\n                all_zero\n    ))\n    \n    return y_pred\n\ny_true = np.hstack((\n            np.where(((X_test_referred_expression == 'A') & (y_test)), 1, 0).reshape((-1, 1)),\n            np.where(((X_test_referred_expression == 'B') & (y_test)), 1, 0).reshape((-1, 1)),\n            np.zeros_like(y_test).reshape((-1, 1))\n))\n\n\n# TODO: refact\n# one of the ideas is to run the model over all the referred expressions and then calculate the final answer\n#y_pred_A = lr.predict(df_A).reshape(-1, 1)\n#y_pred_B = lr.predict(df_B).reshape(-1, 1)\n#all_zero = np.zeros_like(y_pred_A)\n\n#X_test_A = add_features(X_test, 'A', inplace=False)[['URL_distance_token_sort_ratio', 'URL_distance_levenshtein', 'URL_distance_jaro_winkler', 'Text-length']]\n#X_test_B = add_features(X_test, 'B', inplace=False)[['URL_distance_token_sort_ratio', 'URL_distance_levenshtein', 'URL_distance_jaro_winkler', 'Text-length']]\n\n#y_pred = np.hstack((y_pred_A,\n#                    y_pred_B,\n#                    all_zero\n#                   ))\n#y_true[(np.abs(y_true[:, 0] - y_true[:, 1]) < 0.1) & (y_true[:, 0] < .3), 2] = .5\n    \nlog_loss(y_true, transform_to_submit(y_pred_, X_test_referred_expression))","7e7a9c94":"y_true","f8266186":"X_features = X.drop(columns='referred-expression')\nX_referred_expression = X['referred-expression']\n\nlr.fit(X_features, y)","29edaa4c":"df_A = add_features(df, 'A', inplace=False)[['URL_distance_token_sort_ratio', 'URL_distance_levenshtein', 'URL_distance_jaro_winkler', 'Text-length']]\ndf_B = add_features(df, 'B', inplace=False)[['URL_distance_token_sort_ratio', 'URL_distance_levenshtein', 'URL_distance_jaro_winkler', 'Text-length']]\n\ny_pred_A = lr.predict(df_A).reshape(-1, 1)\ny_pred_B = lr.predict(df_B).reshape(-1, 1)\nall_zero = np.zeros_like(y_pred_A)\n\ny_pred = np.hstack((y_pred_A,\n                    y_pred_B,\n                    all_zero\n                   ))\n\nresult = pd.DataFrame(y_pred, index=df.index, columns=['A', 'B', 'NEITHER'])\n\nresult.loc[((result['A'] - result['B']).abs() < 0.1) & (result['A'] < .3), 'NEITHER'] = .3\n\nresult.to_csv('lr_over_URL_similarity.csv')","4ec663ba":"# Adding features\n\n* Does the referencing entity expression have more than one word?\n* Do both the referencing entity and the pronoun are related to similar concept in other parts of the text?","432977f4":"# Adding features","077be587":"## Analyzing the training dataset","46db76a8":"# Let's run it over the test set","3ed0ee55":"# Visualizing","ced84e3b":"## Distance between referred expression and URL last part","0ff00292":"# What about the evaluation?\n\nSubmissions are evaluated using the multi-class logarithmic loss. Each pronoun has been labeled with whether it refers to A, B, or NEITHER. For each pronoun, you must submit a set of predicted probabilities (one for each class). The formula is then,\n\n$$logloss=\u2212\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M}y_{ij}log(p_{ij})$$,\nwhere $N$ is the number of samples in the test set, $M$ is 3,  log is the natural logarithm, $y_{ij}$ is 1 if observation $i$ belongs to class $j$ and 0 otherwise, and $p_{ij}$ is the predicted probability that observation $i$ belongs to class $j$.\n\nThe submitted probabilities are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with $max(min(p,1\u221210^{\u221215}),10^{\u221215})$.","7e12739a":"# Google original datasets\n\nsource: https:\/\/github.com\/google-research-datasets\/gap-coreference","924ee2c7":"# Visualizing some samples","ab3de9f3":"# Generating a baseline model\n\nIt predicts equal probability to all classes","824c6b07":"# Raw files","dbd49fea":"# Let's make a simple linear regression model over the distances","9d24a60d":"# Reading with pandas","e22fc68d":"## Formatting the dataset\n\nSo, I'll create a dataset with the format (X, y), where X contains features of (Text, referred expression, pronoun, \\*features) and y means if the pronoun is referring the referred expression.\n\nThe idea is to train a model to answer Yes or No, indicating if the pronoun is referring the referred expression."}}