{"cell_type":{"7fe1bf0b":"code","d383b3dd":"code","83d706e2":"code","09a7a5b5":"code","ec162f62":"code","61c41440":"code","81aa099a":"code","c1fb8a36":"code","f76e0a93":"code","a0d49c0d":"code","5b7e4e9c":"code","7448b216":"code","fbe29777":"code","99abc3b3":"code","ae77eb79":"code","1c755ba7":"code","38f87624":"markdown","a7243379":"markdown","8d184a1f":"markdown"},"source":{"7fe1bf0b":"import gc\nimport torch\nimport numpy as np\nimport pandas as pd\n\nfrom torch import nn\nfrom transformers.file_utils import ModelOutput\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer","d383b3dd":"class PredictionDataset(Dataset):\n    def __init__(self, text_excerpts):\n        self.text_excerpts = text_excerpts\n    \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx]}\n        return sample","83d706e2":"def create_prediction_dataloader(data, batch_size, num_workers=4):\n    text_excerpts = data['excerpt'].tolist()\n    dataset = PredictionDataset(text_excerpts=text_excerpts)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=False)\n    return dataloader","09a7a5b5":"def clear_cuda():\n    gc.collect()\n    torch.cuda.empty_cache()","ec162f62":"def predict(dataloader, model, tokenizer, padding, max_length, device):\n    clear_cuda()\n    model.eval()\n    model.to(device)\n    predictions = []\n    for batch_num, batch in enumerate(dataloader):\n        # Forward Propagation\n        inputs = tokenizer(batch['text_excerpt'], padding=padding, truncation=True, max_length=max_length,return_tensors=\"pt\")\n        inputs = {key:value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        batch_predictions = outputs.logits.detach().cpu().numpy()\n        predictions.append(batch_predictions)\n    predictions = np.vstack(predictions)\n    return predictions","61c41440":"class RegressorOutput(ModelOutput):\n    loss = None\n    logits = None\n    hidden_states = None\n    attentions = None","81aa099a":"class RobertaPoolerRegressor(nn.Module):\n    def __init__(self, model_path, apply_sqrt_to_loss):\n        super(RobertaPoolerRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        self.loss_fn = nn.MSELoss()\n        self.apply_sqrt_to_loss = apply_sqrt_to_loss\n    \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        roberta_outputs = self.roberta(input_ids=input_ids, \n                                       attention_mask=attention_mask)\n        pooler_output = roberta_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        if self.apply_sqrt_to_loss:\n            loss = torch.sqrt(self.loss_fn(labels, logits)) if labels is not None else None\n        else:\n            loss = self.loss_fn(labels, logits) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","c1fb8a36":"class AttentionHead(nn.Module):\n    def __init__(self, hidden_dim):\n        super(AttentionHead, self).__init__()\n        self.W = nn.Linear(hidden_dim, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, x):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","f76e0a93":"class RobertaPoolerRegressor(nn.Module):\n    def __init__(self, model_path, apply_sqrt_to_loss):\n        super(RobertaPoolerRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        self.loss_fn = nn.MSELoss()\n        self.apply_sqrt_to_loss = apply_sqrt_to_loss\n    \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        roberta_outputs = self.roberta(input_ids=input_ids, \n                                       attention_mask=attention_mask)\n        pooler_output = roberta_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        if self.apply_sqrt_to_loss:\n            loss = torch.sqrt(self.loss_fn(labels, logits)) if labels is not None else None\n        else:\n            loss = self.loss_fn(labels, logits) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","a0d49c0d":"class RobertaLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path):\n        super(RobertaLastHiddenStateRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.roberta.config.hidden_size)\n        self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        self.loss_fn = nn.MSELoss()\n    \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        roberta_outputs = self.roberta(input_ids=input_ids,\n                                       attention_mask=attention_mask)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        loss = torch.sqrt(self.loss_fn(labels, logits)) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","5b7e4e9c":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_dataloader = create_prediction_dataloader(data=test_data, batch_size=8)","7448b216":"EXPERIMENT_NAME = 'experiment_7'","fbe29777":"experiment_1_fold_predictions = []\nfor fold in range(5):\n    print(f'Inferring fold: {fold}')\n    pretrained_model_path = '..\/input\/maunish-clrp-model\/clrp_roberta_base'\n    tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n    model = RobertaLastHiddenStateRegressor(model_path=pretrained_model_path)\n    finetuned_model_path = f'..\/input\/commonlit-making-my-transformer-good-enough\/experiment_1\/fold_{fold}\/model.pth'\n    model.load_state_dict(torch.load(finetuned_model_path, map_location=torch.device('cpu')))\n    model.to(device)\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n    padding = 'max_length'\n    max_length = 256\n    experiment_1_fold_predictions.append(predict(test_dataloader, model, tokenizer, padding, max_length, device))\nexperiment_1_fold_predictions = np.hstack(experiment_1_fold_predictions)","99abc3b3":"experiment_7_fold_predictions = []\nfor fold in range(5):\n    print(f'Inferring fold: {fold}')\n    pretrained_model_path = '..\/input\/commonlit-data-download\/roberta-base'\n    tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n    model = RobertaPoolerRegressor(model_path=pretrained_model_path, apply_sqrt_to_loss=False)\n    finetuned_model_path = f'..\/input\/commonlit-making-my-transformer-good-enough\/experiment_7\/fold_{fold}\/model.pth'\n    model.load_state_dict(torch.load(finetuned_model_path, map_location=torch.device('cpu')))\n    model.to(device)\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n    padding = True\n    max_length = None\n    experiment_7_fold_predictions.append(predict(test_dataloader, model, tokenizer, padding, max_length, device))\nexperiment_7_fold_predictions = np.hstack(experiment_7_fold_predictions)","ae77eb79":"fold_predictions = np.hstack((experiment_1_fold_predictions, experiment_7_fold_predictions))\nmean_predictions = np.mean(fold_predictions, axis=1)\ntest_data['target'] = mean_predictions\ntest_data[['id','target']].to_csv('submission.csv', index=False)","1c755ba7":"test_data[['id','target']]","38f87624":"# Context:\nIn the previous notebook we investigated which components improve the performance of my transformer using cross validation score. In this notebook we verify if the LB score increases.\n\nTrainer notebook: https:\/\/www.kaggle.com\/vigneshbaskaran\/commonlit-making-my-transformer-good-enough  \nStory of how I improved my transformer: https:\/\/www.kaggle.com\/vigneshbaskaran\/commonlit-halftime-recap-of-my-transformer-journey  ","a7243379":"# Prediction Dataset and Dataloader","8d184a1f":"# Model"}}