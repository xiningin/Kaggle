{"cell_type":{"aa53c600":"code","5806dc81":"code","62da89b7":"code","e8121125":"code","d10c5b9a":"code","7af7180e":"markdown"},"source":{"aa53c600":"from optiver_features import generate_test_df\nfrom fastai.tabular.all import *","5806dc81":"test_df = generate_test_df()\ntrain_df = pd.read_csv('..\/input\/optiver-train-features\/train_with_features.csv')","62da89b7":"def pred_tabular_nn(train_df, test_df):\n    train_df = train_df.drop(['time_id', 'row_id'], axis=1).fillna(0)\n    train_df.stock_id = train_df.stock_id.astype('category')\n    cont_nn,cat_nn = cont_cat_split(train_df,  dep_var='target')\n    dls = TabularPandas(train_df, [Categorify, Normalize], cat_nn, cont_nn, y_names='target').dataloaders(2048)\n    test_dl = dls.test_dl(test_df.fillna(0))\n    learn = tabular_learner(dls, y_range=(0,.1), layers=[500,200,100], n_out=1, path = '..\/input\/optiver-models\/')\n    res = torch.zeros(len(test_df))\n    for idx in range(5):\n        learn.load(f'nn_fold{idx}')\n        preds, _ = learn.get_preds(dl=test_dl)\n        res += preds.squeeze() \/ 5\n    return res.numpy()","e8121125":"def pred_lgb(test_df):\n    test_df = test_df.drop(['row_id', 'time_id'], axis=1)\n    res = np.zeros(len(test_df))\n    for idx in range(5):\n        filename = f'..\/input\/optiver-models\/models\/lgb_fold{idx}.pickle'\n        model = pickle.load(open(filename, 'rb'))\n        preds = model.predict(test_df)\n        res += preds \/ 5\n    return res","d10c5b9a":"nn_preds = pred_tabular_nn(train_df, test_df)\nlgb_preds = pred_lgb(test_df)\n\ntest_df['target']=(nn_preds+lgb_preds)\/2\ntest_df[['row_id', 'target']].to_csv('submission.csv', index =False)\npd.read_csv('submission.csv').head()","7af7180e":"This notebook doesn't do anything novel in terms of features, nor architecture. Instead I want to show how you can structure your code and data to run experiments in a fast and concise manner. \n\nThe preprocessing code and LGB models were taken from [here](https:\/\/www.kaggle.com\/tatudoug\/stock-embedding-ffnn-features-of-the-best-lgbm) and based on [this](https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline)\n\nThis is how it works:\n- The training set with features is cached and loaded from https:\/\/www.kaggle.com\/slawekbiel\/optiver-train-features\n- The code to generate those features is saved in an Utility Script: https:\/\/www.kaggle.com\/slawekbiel\/optiver-features and used to process the test data.\n- fast.ai library handles defining the NN model and preparing the data for it (normalization, embeddings, batching etc)\n- Both fastai nad LGB models are trained locally, serialized and then pushed to the dataset: https:\/\/www.kaggle.com\/slawekbiel\/optiver-models"}}