{"cell_type":{"81f02e20":"code","a120b682":"code","926cf5b4":"code","906e7bb7":"code","9a1bc167":"code","548f2b9e":"code","ae4636ac":"code","fd61b4e9":"code","1307e1cf":"code","6b5cc3d8":"code","e1382ecc":"code","6306ce4e":"code","f9ecb874":"code","07c7c5ff":"code","92dbf1b1":"code","f8c40d5b":"code","d2118daa":"code","e8da5983":"code","acefb3de":"code","80e06553":"code","ac987f12":"code","9795f2cb":"code","062ed0cf":"code","2d29c2d9":"code","0021cf3d":"code","1cb00acb":"code","cdaafa46":"code","c072c94c":"code","6f46e1ff":"code","a6f07a4e":"code","83b6d16b":"code","ac4a5214":"code","b997f8f7":"code","bc5a0964":"code","152285dd":"code","106e252d":"code","da7f22be":"code","80063b33":"code","25b1fd89":"code","beb97a28":"code","35c717c3":"code","f228ee46":"code","59546028":"code","585132b0":"code","7af2a087":"code","c882a810":"code","de44258e":"code","0e50d1b8":"code","594afabb":"code","4c54a6c7":"code","dd17d015":"code","c2a2a844":"code","a8ef9e5e":"code","2adffe55":"code","929a894b":"code","5550f047":"code","61983bba":"code","2f28b9f7":"code","39d0bda6":"markdown","1da6f327":"markdown","8f72bf43":"markdown","7dc8c1b6":"markdown","69f07b88":"markdown","07d65bbe":"markdown","61b96aa4":"markdown","f41192bf":"markdown","d40c58c8":"markdown","b08047dd":"markdown","55858aea":"markdown","94239958":"markdown","66954cc9":"markdown","63b6525d":"markdown","4c1e00e3":"markdown","415320af":"markdown","bb1bb79d":"markdown","1305b8fb":"markdown","19f4b077":"markdown","98ef2b0d":"markdown","34ec29bf":"markdown","1454419e":"markdown","e730cd26":"markdown","18e04186":"markdown"},"source":{"81f02e20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a120b682":"# Adding the train Data\ntrain_data = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')","926cf5b4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings \nwarnings.filterwarnings(\"ignore\")\n","906e7bb7":"train_data.shape","9a1bc167":"train_data.describe()","548f2b9e":"train_data.info()","ae4636ac":"train_data.columns","fd61b4e9":"data_numeric_without = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']","1307e1cf":"for i in data_numeric_without:\n    print(i.upper(),\"::\")\n    print(train_data.loc[train_data[i] == 1, \"comment_text\"].sample().values, \"\\n\")","6b5cc3d8":"train_data[\"non_toxic\"] = np.where((train_data[\"toxic\"] == 0) & (train_data[\"severe_toxic\"] == 0) & (train_data[\"obscene\"] == 0) & (train_data[\"threat\"] == 0) & (train_data[\"insult\"] ==0) & (train_data[\"identity_hate\"] == 0),1,0)\n                                   \n                                                             \n                                   ","e1382ecc":"data_numeric = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate', \"non_toxic\"]\n\n","6306ce4e":"for i in data_numeric:\n    print(f\"Percentage of 1 and 0 in {i.upper()}\")\n    print(round(train_data[i].value_counts(1)*100,2))\n  ","f9ecb874":"fig, axes = plt.subplots(ncols =3 , nrows =3 , figsize = (15,10), constrained_layout = True)\nfor i, ax in zip(data_numeric,axes.flat):\n    chart = sns.countplot(train_data[i], ax=ax)\n    chart.set_xticklabels(chart.get_xticklabels(),rotation = 90)\nplt.show()","07c7c5ff":"data_toxic = train_data[(train_data[\"toxic\"] == 1) | (train_data[\"severe_toxic\"] == 1)| (train_data[\"obscene\"] == 1)| (train_data[\"threat\"] == 1) | (train_data[\"insult\"] == 1) | (train_data[\"identity_hate\"] == 1)]\n\n","92dbf1b1":"percentage_category = dict()\nfor i in data_numeric:\n    print(i.upper())\n    print(train_data[i].mean()*100)\n    percentage_category.update({i:train_data[i].mean()*100})\nplt.figure(figsize=(10,7))   \nplt.bar(range(len(percentage_category)),list(percentage_category.values()),align = \"center\", color=['red', 'blue', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(percentage_category)), list(percentage_category.keys()),rotation = 90)\nplt.show()\nprint(\"---\"*30)\nprint(f\"Highest No. of Comments are non toxic, consisting of {round(max(list(percentage_category.values())),2)}%\")","f8c40d5b":"train_data.columns","d2118daa":"data_labels = train_data[['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate', 'non_toxic']]","e8da5983":"plt.figure(figsize = (12,9))\nsns.heatmap(data_labels.corr(), annot = True, cmap=\"BrBG\")\nplt.show()\nprint(\"---\"*30)\nprint(\"Toxic with insult and obscene -- shows fair co-relation \\nObscene with Insult -- exhibits good co-relation\")","acefb3de":"print(train_data[\"comment_text\"].apply(lambda x : len(x)))\nplt.figure(figsize = (12,8))\nsns.distplot(train_data[\"comment_text\"].apply(lambda x : len(x)), color = \"green\")\nplt.show()","80e06553":"color1 = ['red', 'blue', 'purple', 'green',\"cyan\",'maroon', \"pink\"]\nfig, axes = plt.subplots(ncols =2 , nrows =4 , figsize = (15,20), constrained_layout = True)\nfor i, ax, j in zip(data_numeric,axes.flat,color1):\n    chart = sns.distplot(train_data.loc[train_data[i] == 1, \"comment_text\"].apply(lambda x : len(x)),color = j, ax=ax)\n    chart.set_title(i)\nplt.show()","ac987f12":"print(f\"Mean of Length of Comments : {round(train_data['comment_text'].apply(lambda x : len(x)).mean(),2)}\")\nprint(f\"Median of Length of Comments : {round(train_data['comment_text'].apply(lambda x : len(x)).median(),2)}\")\nprint(f\"Max of Length of Comments : {train_data['comment_text'].apply(lambda x : len(x)).max()}\")\nprint(f\"Min of Length of Comments : {train_data['comment_text'].apply(lambda x : len(x)).min()}\")","9795f2cb":"mean_len_comments = dict()\nfor i in data_numeric:\n    mean_len_comments.update({i:round(train_data.loc[train_data[i] == 1, \"comment_text\"].apply(lambda x : len(x)).mean(),2)})\n\nmedian_len_comments = dict()\nfor i in data_numeric:\n    median_len_comments.update({i:round(train_data.loc[train_data[i] == 1, \"comment_text\"].apply(lambda x : len(x)).median(),2)})\n    \nmax_len_comments = dict()\nfor i in data_numeric:\n    max_len_comments.update({i:train_data.loc[train_data[i] == 1, \"comment_text\"].apply(lambda x : len(x)).max()})\n    \nmin_len_comments = dict()\nfor i in data_numeric:\n    min_len_comments.update({i:train_data.loc[train_data[i] == 1, \"comment_text\"].apply(lambda x : len(x)).min()})\n    ","062ed0cf":"print(mean_len_comments)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Mean Length Category Wise\")\nplt.bar(range(len(mean_len_comments)),list(mean_len_comments.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(mean_len_comments)), list(mean_len_comments.keys()),rotation = 90)\nplt.show()","2d29c2d9":"print(median_len_comments)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Median Length Category Wise\")\nplt.bar(range(len(median_len_comments)),list(median_len_comments.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(median_len_comments)), list(median_len_comments.keys()),rotation = 90)\nplt.show()","0021cf3d":"print(max_len_comments)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Max Length Category Wise\")\nplt.bar(range(len(max_len_comments)),list(max_len_comments.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(max_len_comments)), list(max_len_comments.keys()),rotation = 90)\nplt.show()","1cb00acb":"print(min_len_comments)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Min Length Category Wise\")\nplt.bar(range(len(min_len_comments)),list(min_len_comments.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(min_len_comments)), list(min_len_comments.keys()),rotation = 90)\nplt.show()","cdaafa46":"print(train_data[\"comment_text\"].str.split().apply(lambda x : len(x)))\nplt.figure(figsize = (12,8))\nsns.distplot(train_data[\"comment_text\"].str.split().apply(lambda x : len(x)), color = \"green\")\nplt.show()\n","c072c94c":"color1 = ['red', 'blue', 'purple', 'green',\"cyan\",'maroon', \"pink\"]\nfig, axes = plt.subplots(ncols =2 , nrows =4 , figsize = (15,20), constrained_layout = True)\nfor i, ax, j in zip(data_numeric,axes.flat,color1):\n    chart = sns.distplot(train_data.loc[train_data[i] == 1, \"comment_text\"].str.split().apply(lambda x : len(x)),color = j, ax=ax)\n    chart.set_title(i)\nplt.show()\n    ","6f46e1ff":"print(f\"Mean of Length of words in Comments : {round(train_data['comment_text'].str.split().apply(lambda x : len(x)).mean(),2)}\")\nprint(f\"Median of Length of words in Comments : {round(train_data['comment_text'].str.split().apply(lambda x : len(x)).median(),2)}\")\nprint(f\"Max of Length of words in Comments : {train_data['comment_text'].str.split().apply(lambda x : len(x)).max()}\")\nprint(f\"Min of Length of words in Comments : {train_data['comment_text'].str.split().apply(lambda x : len(x)).min()}\")","a6f07a4e":"mean_len_comments_words = dict()\nfor i in data_numeric:\n    mean_len_comments_words.update({i:round(train_data.loc[train_data[i] == 1, \"comment_text\"].str.split().apply(lambda x : len(x)).mean(),2)})\n\nmedian_len_comments_words = dict()\nfor i in data_numeric:\n    median_len_comments_words.update({i:round(train_data.loc[train_data[i] == 1, \"comment_text\"].str.split().apply(lambda x : len(x)).median(),2)})\n    \nmax_len_comments_words = dict()\nfor i in data_numeric:\n    max_len_comments_words.update({i:train_data.loc[train_data[i] == 1, \"comment_text\"].str.split().apply(lambda x : len(x)).max()})\n    \nmin_len_comments_words = dict()\nfor i in data_numeric:\n    min_len_comments_words.update({i:train_data.loc[train_data[i] == 1, \"comment_text\"].str.split().apply(lambda x : len(x)).min()})\n    ","83b6d16b":"print(mean_len_comments_words)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Mean Length of word - Category Wise\")\nplt.bar(range(len(mean_len_comments_words)),list(mean_len_comments_words.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(mean_len_comments_words)), list(mean_len_comments_words.keys()),rotation = 90)\nplt.show()","ac4a5214":"print(median_len_comments_words)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Median Length of word - Category Wise\")\nplt.bar(range(len(median_len_comments_words)),list(median_len_comments_words.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(median_len_comments_words)), list(median_len_comments_words.keys()),rotation = 90)\nplt.show()","b997f8f7":"print(max_len_comments_words)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Max Length of words -- Category Wise\")\nplt.bar(range(len(max_len_comments_words)),list(max_len_comments_words.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(max_len_comments_words)), list(max_len_comments_words.keys()),rotation = 90)\nplt.show()","bc5a0964":"print(min_len_comments_words)\nprint(\"---\"*30)\nplt.figure(figsize=(10,7))\nplt.title(\"Min Length Category Wise\")\nplt.bar(range(len(min_len_comments_words)),list(min_len_comments_words.values()),align = \"center\", color=['red', 'orange', 'purple', 'green',\"yellow\",'lavender', \"pink\"])\nplt.xticks(range(len(min_len_comments_words)), list(min_len_comments_words.keys()),rotation = 90)\nplt.show()","152285dd":"data_toxic.head()","106e252d":"from wordcloud import WordCloud, STOPWORDS\nimport string","da7f22be":"plt.figure(figsize=(15,8))\nwc = WordCloud(background_color = \"black\", max_words=5000, stopwords=STOPWORDS, max_font_size= 40)\nwc.generate(\" \".join(train_data[\"comment_text\"]))\nplt.title(\"Overall word cloud\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\nplt.axis('off')\nplt.show()","80063b33":"plt.figure(figsize=(15,8))\nwc = WordCloud(background_color = \"black\", max_words=5000, stopwords=STOPWORDS, max_font_size= 40)\nwc.generate(\" \".join(train_data[train_data[\"non_toxic\"] == 1][\"comment_text\"]))\nplt.title(\"Non Toxic word cloud\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\nplt.axis('off')\nplt.show()","25b1fd89":"for i in data_numeric_without:\n    plt.figure(figsize=(15,8))\n    wc = WordCloud(background_color = \"black\", max_words=5000, stopwords=STOPWORDS, max_font_size= 40)\n    wc.generate(\" \".join(train_data[train_data[i] == 1][\"comment_text\"]))\n    plt.title(\"{} word cloud\".format(i), fontsize=20)\n    plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n    plt.axis('off')\n    plt.show()","beb97a28":"print(train_data[\"comment_text\"].apply(lambda x : len([i for i in str(x) if i in string.punctuation])))\nplt.figure(figsize = (12,8))\nsns.distplot(train_data[\"comment_text\"].apply(lambda x : len([i for i in str(x) if i in string.punctuation])), color = \"maroon\")\nplt.show()","35c717c3":"color1 = ['red', 'blue', 'purple', 'green',\"cyan\",'maroon', \"pink\"]\nfig, axes = plt.subplots(ncols =2 , nrows =4 , figsize = (15,20), constrained_layout = True)\nfor i, ax, j in zip(data_numeric,axes.flat,color1):\n    chart = sns.distplot(train_data.loc[train_data[i] == 1, \"comment_text\"].apply(lambda x : len([m for m in str(x) if m in string.punctuation])),color = j, ax=ax)\n    chart.set_title(i)\nplt.show()","f228ee46":"print(train_data[\"comment_text\"].apply(lambda x : len([i for i in str(x) if i in STOPWORDS])))\nplt.figure(figsize = (12,8))\nsns.distplot(train_data[\"comment_text\"].apply(lambda x : len([i for i in str(x) if i in STOPWORDS])), color = \"maroon\")\nplt.show()","59546028":"color1 = ['red', 'blue', 'purple', 'green',\"cyan\",'maroon', \"pink\"]\nfig, axes = plt.subplots(ncols =2 , nrows =4 , figsize = (15,20), constrained_layout = True)\nfor i, ax, j in zip(data_numeric,axes.flat,color1):\n    chart = sns.distplot(train_data.loc[train_data[i] == 1, \"comment_text\"].apply(lambda x : len([m for m in str(x) if m in STOPWORDS])),color = j, ax=ax)\n    chart.set_title(i)\nplt.show()\n  ","585132b0":"from nltk.corpus import stopwords\nfrom plotly import subplots\nfrom collections import defaultdict\nfrom collections import Counter\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport re","7af2a087":"#Gram analysis on Training set- Bigram and Trigram\nstopword=set(stopwords.words('english'))\ndef gram_analysis(data,gram):\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n\n#Create frequency grams for analysis\n    \ndef create_dict(data,grams):\n    freq_dict=defaultdict(int)\n    for sentence in data:\n        for tokens in gram_analysis(sentence,grams):\n            freq_dict[tokens]+=1\n    return freq_dict\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ndef create_new_df(freq_dict,):\n    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n    freq_df.columns=['n_gram_words','n_gram_frequency']\n    trace=horizontal_bar_chart(freq_df[:20],'orange')\n    return trace\n    \ndef plot_grams(trace_zero,trace_one,trace_2,trace_3,trace_4,trace_5,trace_6):\n    fig = subplots.make_subplots(rows=7, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Toxic\", \n                                          \"Frequent words of severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\", \"non_toxic\" ])\n    fig.append_trace(trace_zero,1,1)\n    fig.append_trace(trace_ones,2,1 )\n    fig.append_trace(trace_2,3,1)\n    fig.append_trace(trace_3,4,1)\n    fig.append_trace(trace_4,5,1)\n    fig.append_trace(trace_5,6,1)\n    fig.append_trace(trace_6,7,1)\n    fig['layout'].update(height=5000, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')\n\ntrain_df_zero=train_data.loc[train_data[\"toxic\"] == 1, \"comment_text\"]\ntrain_df_ones=train_data.loc[train_data[\"severe_toxic\"] == 1, \"comment_text\"]\ntrain_df_2=train_data.loc[train_data[\"obscene\"] == 1, \"comment_text\"]\ntrain_df_3=train_data.loc[train_data[\"threat\"] == 1, \"comment_text\"]\ntrain_df_4=train_data.loc[train_data[\"insult\"] == 1, \"comment_text\"]\ntrain_df_5=train_data.loc[train_data[\"identity_hate\"] == 1, \"comment_text\"]\ntrain_df_6=train_data.loc[train_data[\"non_toxic\"] == 1, \"comment_text\"]\n\n","c882a810":"print(\"Bi-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],2)\ntrace_zero=create_new_df(freq_train_df_zero)\n\nfreq_train_df_ones=create_dict(train_df_ones[:200],2)\ntrace_ones = create_new_df(freq_train_df_ones)\n\nfreq_train_df_2=create_dict(train_df_2[:200],2)\ntrace_2 = create_new_df(freq_train_df_2)\n\nfreq_train_df_3=create_dict(train_df_3[:200],2)\ntrace_3 = create_new_df(freq_train_df_3)\n\nfreq_train_df_4=create_dict(train_df_4[:200],2)\ntrace_4 = create_new_df(freq_train_df_4)\n\nfreq_train_df_5=create_dict(train_df_5[:200],2)\ntrace_5 = create_new_df(freq_train_df_5)\n\nfreq_train_df_6=create_dict(train_df_6[:200],2)\ntrace_6 = create_new_df(freq_train_df_6)\n\nplot_grams(trace_zero,trace_ones,trace_2,trace_3,trace_4,trace_5,trace_6)\n","de44258e":"## Long word Challenge \ncategory_text = train_data[train_data['non_toxic']!=1]['comment_text'].values\ncategory_text = \" \".join(category_text)\nlong_len_words = [word for word in category_text.split() if len(word)>20]\nprint(\"num of long length words in toxic(all forms) text : {}\".format(len(long_len_words)))\nlong_len_words[:10]\n\n## transforming long word to short word \n\ntrain_data[train_data[\"comment_text\"].apply(lambda x: True if re.search(\"haha\",x) else False)]","0e50d1b8":"def long_word_replacement(a, zi = 'haha'):\n    b = [zi if re.search(zi,i) else i for i in a.split()]\n    d = ' '.join(b)\n    return(d)\ntrain_data[\"comment_text\"] = train_data['comment_text'].apply(lambda x: long_word_replacement(x))","594afabb":"train_data[\"comment_text\"] = train_data['comment_text'].apply(lambda x: long_word_replacement(x, zi = \"lol\"))","4c54a6c7":"train_data[\"comment_text\"] = train_data['comment_text'].apply(lambda x: long_word_replacement(x, zi = \"cunt\"))","dd17d015":"train_data[train_data[\"comment_text\"].apply(lambda x: True if re.search(\"haha\",x) else False)]","c2a2a844":"#Gram analysis on Training set- Bigram and Trigram\nstopword=set(stopwords.words('english'))\ndef gram_analysis(data,gram):\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n\n#Create frequency grams for analysis\n    \ndef create_dict(data,grams):\n    freq_dict=defaultdict(int)\n    for sentence in data:\n        for tokens in gram_analysis(sentence,grams):\n            freq_dict[tokens]+=1\n    return freq_dict\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ndef create_new_df(freq_dict,):\n    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n    freq_df.columns=['n_gram_words','n_gram_frequency']\n    trace=horizontal_bar_chart(freq_df[:20],'orange')\n    return trace\n    \ndef plot_grams(trace_zero,trace_one,trace_2,trace_3,trace_4,trace_5,trace_6):\n    fig = subplots.make_subplots(rows=7, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Toxic\", \n                                          \"Frequent words of severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\", \"non_toxic\" ])\n    fig.append_trace(trace_zero,1,1)\n    fig.append_trace(trace_ones,2,1 )\n    fig.append_trace(trace_2,3,1)\n    fig.append_trace(trace_3,4,1)\n    fig.append_trace(trace_4,5,1)\n    fig.append_trace(trace_5,6,1)\n    fig.append_trace(trace_6,7,1)\n    fig['layout'].update(height=5000, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')\n\ntrain_df_zero=train_data.loc[train_data[\"toxic\"] == 1, \"comment_text\"]\ntrain_df_ones=train_data.loc[train_data[\"severe_toxic\"] == 1, \"comment_text\"]\ntrain_df_2=train_data.loc[train_data[\"obscene\"] == 1, \"comment_text\"]\ntrain_df_3=train_data.loc[train_data[\"threat\"] == 1, \"comment_text\"]\ntrain_df_4=train_data.loc[train_data[\"insult\"] == 1, \"comment_text\"]\ntrain_df_5=train_data.loc[train_data[\"identity_hate\"] == 1, \"comment_text\"]\ntrain_df_6=train_data.loc[train_data[\"non_toxic\"] == 1, \"comment_text\"]\n\n","a8ef9e5e":"print(\"Bi-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],2)\ntrace_zero=create_new_df(freq_train_df_zero)\n\nfreq_train_df_ones=create_dict(train_df_ones[:200],2)\ntrace_ones = create_new_df(freq_train_df_ones)\n\nfreq_train_df_2=create_dict(train_df_2[:200],2)\ntrace_2 = create_new_df(freq_train_df_2)\n\nfreq_train_df_3=create_dict(train_df_3[:200],2)\ntrace_3 = create_new_df(freq_train_df_3)\n\nfreq_train_df_4=create_dict(train_df_4[:200],2)\ntrace_4 = create_new_df(freq_train_df_4)\n\nfreq_train_df_5=create_dict(train_df_5[:200],2)\ntrace_5 = create_new_df(freq_train_df_5)\n\nfreq_train_df_6=create_dict(train_df_6[:200],2)\ntrace_6 = create_new_df(freq_train_df_6)\n\nplot_grams(trace_zero,trace_ones,trace_2,trace_3,trace_4,trace_5,trace_6)\n","2adffe55":"print(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],3)\ntrace_zero=create_new_df(freq_train_df_zero)\n\nfreq_train_df_ones=create_dict(train_df_ones[:200],3)\ntrace_ones = create_new_df(freq_train_df_ones)\n\nfreq_train_df_2=create_dict(train_df_2[:200],3)\ntrace_2 = create_new_df(freq_train_df_2)\n\nfreq_train_df_3=create_dict(train_df_3[:200],3)\ntrace_3 = create_new_df(freq_train_df_3)\n\nfreq_train_df_4=create_dict(train_df_4[:200],3)\ntrace_4 = create_new_df(freq_train_df_4)\n\nfreq_train_df_5=create_dict(train_df_5[:200],3)\ntrace_5 = create_new_df(freq_train_df_5)\n\nfreq_train_df_6=create_dict(train_df_6[:200],3)\ntrace_6 = create_new_df(freq_train_df_6)\n\nplot_grams(trace_zero,trace_ones,trace_2,trace_3,trace_4,trace_5,trace_6)","929a894b":"from bs4 import BeautifulSoup\nimport re\nimport itertools","5550f047":"class TextCleaningUtils:\n    '''\n        This class contains implementations of various text cleaning operations (Static Methods)\n    '''\n\n    cleaning_regex_map = {\n        'web_links': r'(?i)(?:(?:http(?:s)?:)|(?:www\\.))\\S+',\n#         \n        'special_chars': r'[^a-zA-Z\\s\\.,!?;:]+',  ## removing nums\n        'redundant_spaces': r'\\s\\s+',\n        'redundant_newlines': r'[\\r|\\n|\\r\\n]+',\n        'twitter_handles': r'[#@]\\S+',\n        'punctuations': r'[\\.,!?;:]+'\n    }\n    \n    def expand_abbreviations(text):\n        text = re.sub(r\"he's\", \"he is\", text)\n        text = re.sub(r\"there's\", \"there is\", text)\n        text = re.sub(r\"We're\", \"We are\", text)\n        text = re.sub(r\"That's\", \"That is\", text)\n        text = re.sub(r\"won't\", \"will not\", text)\n        text = re.sub(r\"they're\", \"they are\", text)\n        text = re.sub(r\"Can't\", \"Cannot\", text)\n        text = re.sub(r\"wasn't\", \"was not\", text)\n        text = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", text)\n        text= re.sub(r\"aren't\", \"are not\", text)\n        text = re.sub(r\"isn't\", \"is not\", text)\n        text = re.sub(r\"What's\", \"What is\", text)\n        text = re.sub(r\"haven't\", \"have not\", text)\n        text = re.sub(r\"hasn't\", \"has not\", text)\n        text = re.sub(r\"There's\", \"There is\", text)\n        text = re.sub(r\"He's\", \"He is\", text)\n        text = re.sub(r\"It's\", \"It is\", text)\n        text = re.sub(r\"You're\", \"You are\", text)\n        text = re.sub(r\"I'M\", \"I am\", text)\n        text = re.sub(r\"shouldn't\", \"should not\", text)\n        text = re.sub(r\"wouldn't\", \"would not\", text)\n        text = re.sub(r\"couldn't\", \"could not\", text)\n        text = re.sub(r\"i'm\", \"I am\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", text)\n        text = re.sub(r\"I'm\", \"I am\", text)\n        text = re.sub(r\"Isn't\", \"is not\", text)\n        text = re.sub(r\"Here's\", \"Here is\", text)\n        text = re.sub(r\"you've\", \"you have\", text)\n        text = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", text)\n        text = re.sub(r\"we're\", \"we are\", text)\n        text = re.sub(r\"what's\", \"what is\", text)\n        text = re.sub(r\"couldn't\", \"could not\", text)\n        text = re.sub(r\"we've\", \"we have\", text)\n        text = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", text)\n        text = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", text)\n        text = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", text)\n        text = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", text)\n        text = re.sub(r\"who's\", \"who is\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", text)\n        text = re.sub(r\"y'all\", \"you all\", text)\n        text = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", text)\n        text = re.sub(r\"would've\", \"would have\", text)\n        text = re.sub(r\"it'll\", \"it will\", text)\n        text = re.sub(r\"we'll\", \"we will\", text)\n        text = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", text)\n        text = re.sub(r\"We've\", \"We have\", text)\n        text = re.sub(r\"he'll\", \"he will\", text)\n        text = re.sub(r\"Y'all\", \"You all\", text)\n        text = re.sub(r\"Weren't\", \"Were not\", text)\n        text = re.sub(r\"Didn't\", \"Did not\", text)\n        text = re.sub(r\"they'll\", \"they will\", text)\n        text = re.sub(r\"DON'T\", \"DO NOT\", text)\n        text = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", text)\n        text = re.sub(r\"they've\", \"they have\", text)\n        text = re.sub(r\"they'd\", \"they would\", text)\n        text = re.sub(r\"i'd\", \"I would\", text)\n        text = re.sub(r\"should've\", \"should have\", text)\n        text = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", text)\n        text = re.sub(r\"where's\", \"where is\", text)\n        text = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", text)\n        text = re.sub(r\"i'll\", \"I will\", text)\n        text = re.sub(r\"weren't\", \"were not\", text)\n        text = re.sub(r\"They're\", \"They are\", text)\n        text = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", text)\n        text = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", text)\n        text = re.sub(r\"let's\", \"let us\", text)\n        text = re.sub(r\"it's\", \"it is\", text)\n        text = re.sub(r\"can't\", \"cannot\", text)\n        text = re.sub(r\"don't\", \"do not\", text)\n        text = re.sub(r\"you're\", \"you are\", text)\n        text = re.sub(r\"i've\", \"I have\", text)\n        text = re.sub(r\"that's\", \"that is\", text)\n        text = re.sub(r\"i'll\", \"I will\", text)\n        text = re.sub(r\"doesn't\", \"does not\",text)\n        text = re.sub(r\"i'd\", \"I would\", text)\n        text = re.sub(r\"didn't\", \"did not\", text)\n        text = re.sub(r\"ain't\", \"am not\", text)\n        text = re.sub(r\"you'll\", \"you will\", text)\n        text = re.sub(r\"I've\", \"I have\", text)\n        text = re.sub(r\"Don't\", \"do not\", text)\n        text = re.sub(r\"I'll\", \"I will\", text)\n        text = re.sub(r\"I'd\", \"I would\", text)\n        text = re.sub(r\"Let's\", \"Let us\", text)\n        text = re.sub(r\"you'd\", \"You would\", text)\n        text = re.sub(r\"It's\", \"It is\", text)\n        text = re.sub(r\"Ain't\", \"am not\", text)\n        text = re.sub(r\"Haven't\", \"Have not\", text)\n        text = re.sub(r\"Hadn't\", \"Had not\", text)\n        text = re.sub(r\"Could've\", \"Could have\", text)\n        text = re.sub(r\"youve\", \"you have\", text)  \n        text = re.sub(r\"don\u00e5\u00abt\", \"do not\", text)  \n\n        return text\n    \n    def remove_emojis(text):\n        emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        text=emoji_clean.sub(r'',text)\n        url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n        text=url_clean.sub(r'',text)\n        return text\n    \n\n\n    @staticmethod\n    def clean_text_from_regex(text, text_clean_regex):\n        '''\n            Follow a particular cleaning expression, provided\n            as an input by an user to clean the text.\n        '''\n\n        text = text_clean_regex.sub(' ', text).strip()\n        return text\n\n    @staticmethod\n    def strip_html(text):\n        soup = BeautifulSoup(text, \"html.parser\")\n        return soup.get_text()\n\n    @staticmethod\n    def remove_special_chars(text):\n        '''\n            Replace any special character provided as default,\n            which is present in the text with space\n        '''\n\n        special_chars_regex = re.compile(TextCleaningUtils.cleaning_regex_map['special_chars'])\n        text = TextCleaningUtils.clean_text_from_regex(text, special_chars_regex)\n        return text\n\n    @staticmethod\n    def remove_redundant_spaces(text):\n        '''\n            Remove any redundant space provided as default,\n            that is present in the text.\n        '''\n\n        redundant_spaces_regex = re.compile(\n            TextCleaningUtils.cleaning_regex_map['redundant_spaces'])\n        text = TextCleaningUtils.clean_text_from_regex(text, redundant_spaces_regex)\n        return text\n\n    @staticmethod\n    def remove_web_links(text):\n        '''\n            Removes any web link that follows a particular default expression,\n            present in the text.\n        '''\n\n        web_links_regex = re.compile(TextCleaningUtils.cleaning_regex_map['web_links'])\n        text = TextCleaningUtils.clean_text_from_regex(text, web_links_regex)\n        return text\n\n    @staticmethod\n    def remove_twitter_handles(text):\n        '''\n            Removes any twitter handle present in the text.\n        '''\n\n        twitter_handles_regex = re.compile(TextCleaningUtils.cleaning_regex_map['twitter_handles'])\n        text = TextCleaningUtils.clean_text_from_regex(text, twitter_handles_regex)\n        return text\n\n    @staticmethod\n    def remove_redundant_newlines(text):\n        '''\n            Removes any redundant new line present in the text.\n        '''\n\n        redundant_newlines_regex = re.compile(\n            TextCleaningUtils.cleaning_regex_map['redundant_newlines'])\n        text = TextCleaningUtils.clean_text_from_regex(text, redundant_newlines_regex)\n        return text\n\n    @staticmethod\n    def remove_punctuations(text):\n        '''\n            Removes any punctuation that follows the default expression, in the text.\n        '''\n\n        remove_punctuations_regex = re.compile(TextCleaningUtils.cleaning_regex_map['punctuations'])\n        text = TextCleaningUtils.clean_text_from_regex(text, remove_punctuations_regex)\n        return text\n\n    @staticmethod\n    def remove_exaggerated_words(text):\n        '''\n            Removes any exaggerated word present in the text.\n        '''\n\n        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n\n    @staticmethod\n    def replace_multiple_chars(text):\n        '''\n            Replaces multiple characters present in the text.\n        '''\n\n        char_list = ['.', '?', '!', '#', '$', '\/', '@', '*', '(', ')', '+']\n        final_text = ''\n        for i in char_list:\n            if i in text:\n                pattern = \"\\\\\" + i + '{2,}'\n                repl_str = i.replace(\"\\\\\", \"\")\n                text = re.sub(pattern, repl_str, text)\n                final_text = ' '.join(text.split())\n        return final_text\n\n    @staticmethod\n    def replace_sign(text):\n        '''\n            Replaces any sign with words like & with 'and', in the text.\n        '''\n        sign_list = {'&': ' and ', '\/': ' or ', '\\xa0': ' '}\n        final_text = ''\n        for i in sign_list:\n            if i in text:\n                text = re.sub(i, sign_list[i], text)\n                final_text = ' '.join(text.split())\n        return final_text\n\n    @staticmethod\n    def remove_accented_char(text):\n        text = unicodedata.normalize('NFD', text) \\\n            .encode('ascii', 'ignore') \\\n            .decode(\"utf-8\")\n        return str(text)\n\n    @staticmethod\n    def replace_characters(text, replace_map):\n        '''\n            Replaces any character custom provided by an user.\n        '''\n\n        for char, replace_val in replace_map.items():\n            text = text.replace(char, replace_val)\n        return text\n    \ndef clean_data(df,col_to_clean):\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.expand_abbreviations)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_emojis)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_special_chars)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_spaces)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_punctuations)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_exaggerated_words)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_newlines)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_twitter_handles)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_web_links)\n    df[col_to_clean] = df[col_to_clean].astype(str)\n    df[col_to_clean] = df[col_to_clean].str.lower()\n    \n    return df","61983bba":"train_data = clean_data(train_data, 'comment_text')","2f28b9f7":"for i in data_numeric:\n    plt.figure(figsize=(15,8))\n    wc = WordCloud(background_color = \"black\", max_words=5000, stopwords=STOPWORDS, max_font_size= 40)\n    wc.generate(\" \".join(train_data[train_data[i] == 1][\"comment_text\"]))\n    plt.title(\"{} word cloud\".format(i), fontsize=20)\n    plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n    plt.axis('off')\n    plt.show()","39d0bda6":"##  Gram Statistics ","1da6f327":"### Length of words in a comment","8f72bf43":"### Basic Statistics Analysis of length in comments  category wise","7dc8c1b6":"### Length of words in a comment category wise","69f07b88":"### Basic Statical Analysis of length of words in comments ","07d65bbe":"Doing Gram Analytics again after changing the long words ","61b96aa4":"### Cleaning to remove the following nature of elements from text\n\n* Remove HTML codes\n\n* Remove URLs\n\n* Remove Emojis\n\n* Remove Stopwords\n\n* Remove Punctuations\n\n* Expanding Abbreviations","f41192bf":"### Basic Statistics Analysis of length  in comments ","d40c58c8":"# Length of Words","b08047dd":"### Basic Statistical Analysis of length of words in comments","55858aea":"### WordCloud -- Unprocessed Data ","94239958":"# Introduction \n\n**Over the years, social media and social networking use have been increasing exponentially due to an upsurge in the use of the internet.**\n\n**Highly productive and useful information is available in surplus  and could contribute significantly to the quality of human life. But, on the other hand, information availabe, could also be destructive and enormously dangerous which can lead to depression, stress issues, anxiety problems, cyber bullying etc.**\n\n**To ensure that we can find such jarring words and maintain the decency of the social, here is my approach to tacle this problem.**\n\n![](http:\/\/)","66954cc9":"# Text Cleaning ","63b6525d":"### StopWords","4c1e00e3":"### Length of  Comments Category wise ","415320af":"#### lets look into count of stop words category wise","bb1bb79d":"#### Lets look into count of punctuation category wise","1305b8fb":"# Punctuation and Stop Word Analysis ","19f4b077":"#### Lets look into count of stop words ","98ef2b0d":"#### Punctuation ","34ec29bf":"### Length of  Comments","1454419e":"Lets look into count of punctuations ","e730cd26":"### Word Cloud -- Clean Data ","18e04186":"# Comment Analysis "}}