{"cell_type":{"ef809fb3":"code","4778712c":"code","a8d50d75":"code","e17e7066":"code","73e88ddf":"code","bc733ddc":"code","d6b95aff":"code","32bab58d":"code","ce7cadd0":"code","5b7c0e85":"code","02546f9a":"code","ae108074":"code","441d9513":"code","f057cca5":"code","23edde1b":"code","575676d6":"code","39b31239":"code","58bcede9":"code","b368d55b":"code","ee33b60d":"code","a20fda0c":"code","18f2f447":"code","3dcc457e":"code","6b7804b7":"code","f7f80007":"code","b8259ae5":"code","592763e7":"code","e7bdff45":"code","f005525b":"code","09747d99":"code","dc25fac9":"code","4c38c634":"code","06213559":"code","c12a999b":"code","b7ad36a0":"code","e107365b":"code","5ad9cafb":"code","4a927972":"code","ae7fe65f":"code","35593e70":"code","be5df5b4":"code","098c0b21":"code","e5bfa14b":"code","a3931065":"code","d945c718":"code","bbeae60c":"code","9f5a999d":"code","44e6ea8c":"code","ccdbaf0d":"code","d580ddfa":"code","d6ba7a82":"code","6d2fa25f":"code","9b134781":"code","628dca9b":"code","b2dcc917":"code","9a64bcc8":"code","b720dffe":"code","71c8d300":"code","331a0944":"code","e4c09b55":"code","b1bf21e3":"markdown","2417d11c":"markdown","05c62eb7":"markdown","67b8a391":"markdown","6c094d2d":"markdown","bcfde413":"markdown","48a23b6e":"markdown","763ac745":"markdown","53f02945":"markdown","50a2ef1d":"markdown","6da23c7f":"markdown","53f696bc":"markdown","b6e279a3":"markdown","9e27d81e":"markdown","93498c51":"markdown","81917295":"markdown","bdff7fea":"markdown","a0b3da4b":"markdown","d7e3d357":"markdown","026cebae":"markdown","d822f5ba":"markdown","2043a4f5":"markdown","acef34d5":"markdown","1877ab29":"markdown","ec14bc8b":"markdown","b64a28d4":"markdown","1ccee252":"markdown","b69639ca":"markdown","9aefaa04":"markdown","5a39cbd4":"markdown"},"source":{"ef809fb3":"# # Set the random seed for reproducibility\n# seed_value = 98765\n# from numpy.random import seed\n# seed(seed_value)\n\n\n# Libraries import\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, date, timedelta\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Bidirectional \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-whitegrid')\n              \n# Set precision to two decimals\npd.set_option(\"display.precision\", 2)\n\n# Define date format for charts like Apr 16 or Mar 8\nmy_date_fmt = mdates.DateFormatter('%b %e')","4778712c":"# Download files from github\ncases_url = \"https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv\"\ndf_cases = pd.read_csv(cases_url, error_bad_lines=False)\n\ndeaths_url = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv'\ndf_deaths = pd.read_csv(deaths_url, error_bad_lines=False)\n\nrecovered_url = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_recovered_global.csv'\ndf_recovered = pd.read_csv(recovered_url, error_bad_lines=False)","a8d50d75":"# Drop Province\/State, Lat and Long\ndf_cases.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\ndf_deaths.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\ndf_recovered.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\n\n# Rename Country\/Region as Country\ndf_cases.rename(columns={'Country\/Region' : 'Country'}, inplace=True)\ndf_deaths.rename(columns={'Country\/Region' : 'Country'}, inplace=True)\ndf_recovered.rename(columns={'Country\/Region' : 'Country'}, inplace=True)\n\n# Some countries (Australia, Canada...) report data by province so we need to aggregate it\ndf_cases = df_cases.groupby(by='Country').sum()\ndf_deaths = df_deaths.groupby(by='Country').sum()\ndf_recovered = df_recovered.groupby(by='Country').sum()\n\n# Transpose dataframes and make the date column index of datetime type\ndf_cases = df_cases.T\ndf_cases.index = pd.to_datetime(df_cases.index)\ndf_deaths = df_deaths.T\ndf_deaths.index = pd.to_datetime(df_deaths.index)\ndf_recovered = df_recovered.T\ndf_recovered.index = pd.to_datetime(df_recovered.index)","e17e7066":"# Get last date in the set\nlast_date = df_cases.tail(1).index[0]\nprint('Last date in the dataset: ' + str(datetime.date(last_date)))","73e88ddf":"# List of countries for this work\ncountry_list = ['Belgium', 'France', 'Germany', 'Italy', 'Netherlands', 'Portugal', 'Spain', 'Sweden', 'Switzerland',  \n                'Argentina', 'Brazil', 'Canada', 'India', 'Iran',  'Mexico', 'Russia', 'United Kingdom', 'US']\nclist1 = ['Belgium', 'France', 'Germany', 'Italy', 'Netherlands', 'Portugal', 'Spain', 'Sweden', 'Switzerland']\nclist2 = ['Argentina', 'Brazil', 'Canada', 'India', 'Iran', 'Mexico', 'Russia', 'United Kingdom', 'US']","bc733ddc":"# Extract selection of countries\ndf_cases = df_cases[country_list]\ndf_recovered = df_recovered[country_list]\ndf_deaths = df_deaths[country_list]","d6b95aff":"# Active cases = Confirmed cases - Recoverres - Deaths\ndf_active = pd.DataFrame(columns=df_cases.columns, index=df_cases.index)\nfor x in country_list:\n    df_active[x] = df_cases[x] - df_recovered[x] - df_deaths[x] ","32bab58d":"# Mortality(%) = Deaths \/ Cases\ndf_mortality = pd.DataFrame(columns=df_cases.columns, index=df_cases.index)\nfor x in country_list:\n    df_mortality[x] = 100 * df_deaths[x] \/ df_cases[x] ","ce7cadd0":"# Compute daily variation of confirmed and active cases, and deaths\ndf_cases_diff = pd.DataFrame(columns=df_cases.columns, index=df_cases.index)\ndf_active_diff = pd.DataFrame(columns=df_active.columns, index=df_active.index)\ndf_deaths_diff = pd.DataFrame(columns=df_deaths.columns, index=df_deaths.index)\n\nfor x in country_list:\n    df_cases_diff[x] = df_cases[x].diff()\n    df_active_diff[x] = df_active[x].diff()\n    df_deaths_diff[x] = df_deaths[x].diff()\n    \ndf_cases_diff.fillna(value=0, inplace=True)\ndf_active_diff.fillna(value=0, inplace=True)\ndf_deaths_diff.fillna(value=0, inplace=True)","5b7c0e85":"# Confirmed cases and deaths are always growing, hence their derivatives must be positive or zero\ndf_cases_diff[df_cases_diff < 0] = 0\ndf_deaths_diff[df_deaths_diff < 0] = 0","02546f9a":"# First batch of 9 countries: EVOLUTION of CASES (1 of 2)\nfig1, ax1 = plt.subplots(3,3, figsize=(36,15))\nfig1.subplots_adjust(top=0.93)\ni = 0\nj = 0\nfor x in clist1:\n  ax1[i,j].set_title(x, fontsize='x-large')\n  ax1[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax1[i,j].xaxis.set_major_locator(plt.MultipleLocator(28)) \n  ax1[i,j].plot(df_cases.index, df_cases[x], color='navy', linewidth=1.5, label='Confirmed cases')\n  ax1[i,j].plot(df_active.index, df_active[x], color='skyblue', linewidth=1.5, label='Active cases')\n  ax1[i,j].plot(df_recovered.index, df_recovered[x], color='lime', linewidth=1.5, label='Recovered cases')\n  ax1[i,j].plot(df_deaths.index, df_deaths[x], color='coral', linewidth=2, label='Deaths')\n  if j<2:\n    j = j + 1\n  else:\n    j = 0\n    i = i + 1\n\nax1[0,0].legend(loc='upper left', fontsize='large')\nfig1.suptitle('Evolution of covid-19 cases by country (Europe)', fontsize='xx-large')  \nfig1.autofmt_xdate(rotation=45, ha='right')\nplt.show()","ae108074":"# Second batch of 9 countries: EVOLUTION of CASES (2 of 2)\nfig2, ax2 = plt.subplots(3,3, figsize=(36,15))\nfig2.subplots_adjust(top=0.93)\ni = 0\nj = 0\nfor x in clist2:\n  ax2[i,j].set_title(x, fontsize='x-large')\n  ax2[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax2[i,j].xaxis.set_major_locator(plt.MultipleLocator(28)) \n  ax2[i,j].plot(df_cases.index, df_cases[x], color='navy', linewidth=1.5, label='Confirmed cases')\n  ax2[i,j].plot(df_active.index, df_active[x], color='skyblue', linewidth=1.5, label='Active cases')\n  ax2[i,j].plot(df_recovered.index, df_recovered[x], color='lime', linewidth=1.5, label='Recovered cases')\n  ax2[i,j].plot(df_deaths.index, df_deaths[x], color='coral', linewidth=1.5, label='Deaths')  \n  if j<2:\n    j = j + 1\n  else:\n    j = 0\n    i = i + 1\n\nax2[0,0].legend(loc='upper left', fontsize='large')\nfig2.suptitle('Evolution of covid-19 cases by country (World excluding Europe)', fontsize='xx-large')  \nfig2.autofmt_xdate(rotation=45, ha='right')\nplt.show()","441d9513":"# Mortality(%) of covid-19 in Europe\nprint('Mortality(%) in Europe')\ndf_mortality[clist1].tail(1)","f057cca5":"# Mortality(%) of covid-19 in the world (excl. Europe)\nprint('Mortality(%) in the world (excl.Europe)')\ndf_mortality[clist2].tail(1)","23edde1b":"# First batch of 9 countries: DAILY VARIATION of CONFIRMED CASES (1 of 2)\n\nfig1, ax1 = plt.subplots(3,3, figsize=(36,15))\nfig1.subplots_adjust(top=0.93)\ni = 0\nj = 0\n\nfor x in clist1:\n  ax1[i,j].set_title(x, fontsize='x-large')\n  ax1[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax1[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax1[i,j].bar(df_cases_diff.index,  df_cases_diff[x], color='grey', alpha=0.2, label='Cases growth rate')\n  ax1[i,j].plot(df_cases_diff.index,  df_cases_diff[x].rolling(window=7).mean(), color='navy', linewidth=1.5, label='7-day MA')\n  if j<2:\n    j = j + 1\n  else:\n    j = 0\n    i = i + 1\n\nax1[0,0].legend(loc='upper left', fontsize='large')\nfig1.suptitle('Daily variation of covid-19 confirmed cases by country (Europe)', fontsize='xx-large')  \nfig1.autofmt_xdate(rotation=45, ha='right')\nplt.show()","575676d6":"# Second batch of 9 countries: DAILY VARIATION of CONFIRMED CASES (2 of 2)\n\nfig2, ax2 = plt.subplots(3,3, figsize=(36,15))\nfig2.subplots_adjust(top=0.93)\ni = 0\nj = 0\n\nfor x in clist2:\n  ax2[i,j].set_title(x, fontsize='x-large')\n  ax2[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax2[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax2[i,j].bar(df_cases_diff.index,  df_cases_diff[x], color='grey', alpha=0.2, label='Cases growth rate')\n  ax2[i,j].plot(df_cases_diff.index,  df_cases_diff[x].rolling(window=7).mean(), color='navy', linewidth=1.5, label='7-day MA')\n  if j<2:\n    j = j + 1\n  else:\n    j = 0\n    i = i + 1\n\nax2[0,0].legend(loc='upper left', fontsize='large')\nfig2.suptitle('Daily variation of covid-19 confirmed cases by country (World excluding Europe)', fontsize='xx-large')  \nfig2.autofmt_xdate(rotation=45, ha='right')\nplt.show()","39b31239":"# First batch of 9 countries: DAILY VARIATION of DEATHS (1 of 2)\n\nfig1, ax1 = plt.subplots(3,3, figsize=(36,15))\nfig1.subplots_adjust(top=0.93)\ni = 0\nj = 0\n\nfor x in clist1:\n  ax1[i,j].set_title(x, fontsize='x-large')\n  ax1[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax1[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax1[i,j].bar(df_deaths_diff.index,  df_deaths_diff[x], color='grey', alpha=0.2, label='Deaths growth rate')\n  ax1[i,j].plot(df_deaths_diff.index,  df_deaths_diff[x].rolling(window=7).mean(), color='coral', linewidth=2, label='7-day MA')\n  if j<2:\n    j = j + 1\n  else:\n    j = 0\n    i = i + 1\n\nax1[0,0].legend(loc='upper left', fontsize='large')\nfig1.suptitle('Daily variation of covid-19 deaths by country (Europe)', fontsize='xx-large')  \nfig1.autofmt_xdate(rotation=45, ha='right')\nplt.show()","58bcede9":"# Second batch of 9 countries : DAILY VARIATION of DEATHS (2 of 2)\n\nfig2, ax2 = plt.subplots(3,3, figsize=(36,15))\nfig2.subplots_adjust(top=0.93)\ni = 0\nj = 0\n\nfor x in clist2:\n  ax2[i,j].set_title(x, fontsize='x-large')\n  ax2[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax2[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))  \n  ax2[i,j].bar(df_deaths_diff.index,  df_deaths_diff[x], color='grey', alpha=0.2, label='Deaths growth rate')\n  ax2[i,j].plot(df_deaths.index,  df_deaths_diff[x].rolling(window=7).mean(), color='coral', linewidth=2, label='7-day MA')\n  if j<2:\n    j = j + 1\n  else:\n    j = 0\n    i = i + 1\n\nax2[0,0].legend(loc='upper left', fontsize='large')\nfig2.suptitle('Daily variation of covid-19 deaths by country (World excluding Europe)', fontsize='xx-large')  \nfig2.autofmt_xdate(rotation=45, ha='right')\nplt.show()","b368d55b":"# Countries populations\n# Source: https:\/\/www.worldometers.info\/world-population\/population-by-country\/\n\npop = {}\n\n\npop['Belgium'] = 11589623\npop['France'] = 65273511\npop['Germany'] = 83783942\npop['Italy'] = 60461826\npop['Netherlands'] = 17134872\npop['Portugal'] = 10196709\npop['Spain'] = 46754778\npop['Sweden'] = 10099265\npop['Switzerland'] = 8654622\npop['Argentina'] = 45195774\npop['Brazil'] = 212559417\npop['Canada'] = 37600000\npop['India'] = 1380004385\npop['Iran'] = 83992949\npop['Mexico'] = 128932753\npop['Russia'] = 145934462\npop['United Kingdom'] = 67886011\npop['US'] = 331002651\n\npop","ee33b60d":"# Calculate nbr of cases per million people\ndf_cases_per_million = pd.DataFrame(columns=df_cases.columns, index=df_cases.index)\nfor x in df_cases_per_million.columns:\n  df_cases_per_million[x] = 1000000 * df_cases[x] \/\/ pop[x]\n\nprint('Nbr of covid-19 cases per million people')\nprint(df_cases_per_million.tail(1))","a20fda0c":"# Calculate nbr of deaths per million people\ndf_deaths_per_million = pd.DataFrame(columns=df_deaths.columns, index=df_cases.index)\nfor x in df_deaths_per_million.columns:\n  df_deaths_per_million[x] = 1000000 * df_deaths[x] \/\/ pop[x]\n\nprint('Nbr of covid-19 deaths per million people')\nprint(df_deaths_per_million.tail(1))","18f2f447":"fig, ax = plt.subplots(1,2, figsize=(28,6))\n\n# Axis 0: cases per million\nfor x in df_cases.columns:\n  ax[0].bar(x, df_cases_per_million[x].tail(1))\n\n# ax[0].set_ylabel('Number of cases per million')\nax[0].set_xticklabels(country_list, rotation=45, horizontalalignment='right')\nax[0].set_title('Covid-19 confirmed cases per million people as of ' + str(last_date), fontsize='x-large')\n\n# Chart 2: deaths per million \nfor x in df_cases.columns:\n  ax[1].bar(x, df_deaths_per_million[x].tail(1))\n\n# ax[1].set_ylabel('Number of deaths per million')\nax[1].set_xticklabels(country_list, rotation=45, horizontalalignment='right', fontsize='large')\nax[1].set_title('Covid-19 deaths per million people as of ' +  str(last_date), fontsize='x-large')\n\nplt.show()","3dcc457e":"#########################################################################\n# Prediction model parameters (Confirmed cases and deaths)\n#########################################################################\n\n# Number of features Xi (Countries)\nNBR_FEATURES = len(country_list)\n\n# Number of predictions (days)\nNBR_PREDICTIONS = 90      \n\n# Size ot TRAIN and TEST samples\nNBR_SAMPLES = len(df_cases)\nNBR_TRAIN_SAMPLES = NBR_SAMPLES - NBR_PREDICTIONS\nNBR_TEST_SAMPLES = NBR_SAMPLES - NBR_TRAIN_SAMPLES\n\n# Number of input steps [x(t-1), x(t-2), x(t-3)...] to predict an output y(t)\nTIME_STEPS = 60\n\n# Number of overlapping training sequences of TIME_STEPS\nBATCH_SIZE = 8\n\n# Number of training cycles\nEPOCHS = 100\n\nprint('Prediction model parameters for confirmed cases and deaths')\nprint('..........................................................')\nprint('NBR_SAMPLES: ', NBR_SAMPLES)\nprint('NBR_TRAIN_SAMPLES: ', NBR_TRAIN_SAMPLES)\nprint('NBR_TEST_SAMPLES: ', NBR_TEST_SAMPLES)\nprint('NBR_PREDICTIONS: ', NBR_PREDICTIONS)\nprint()\nprint('NBR_FEATURES: ', NBR_FEATURES)\nprint('TIME_STEPS:', TIME_STEPS)\nprint('BATCH_SIZE: ', BATCH_SIZE)\nprint('EPOCHS: ', EPOCHS)\nprint('..........................................................')","6b7804b7":"# Process of CONFIRMED CASES \n\n# Split dataset into test and train subsets \ndf_train_1 = df_cases_per_million.iloc[0:NBR_TRAIN_SAMPLES, 0:NBR_FEATURES] \ndf_test_1 = df_cases.iloc[NBR_TRAIN_SAMPLES:, 0:NBR_FEATURES]\n\n# Normalize test and train data (range: 0 - 1)\nsc1 = MinMaxScaler(feature_range = (0, 1))\nsc1.fit(df_train_1)\nsc_df_train_1 = sc1.transform(df_train_1)\n# sc_df_test = sc.transform(df_test)\n\n# Prepare training sequences\nX_train_1 = []\ny_train_1 = []\nfor i in range(TIME_STEPS, NBR_TRAIN_SAMPLES):\n    X_train_1.append(sc_df_train_1[i-TIME_STEPS:i, 0:NBR_FEATURES])\n    y_train_1.append(sc_df_train_1[i, 0:NBR_FEATURES])\n   \nX_train_1, y_train_1 = np.array(X_train_1), np.array(y_train_1)\nX_train_1 = np.reshape(X_train_1, (X_train_1.shape[0], X_train_1.shape[1], NBR_FEATURES))","f7f80007":"# Build the RNN, dropout helps prevent overfitting\n\n# Initialize structure\nRNN1 = Sequential()\n\n# Build layers: 2 LSTM layers with dropout\nRNN1.add(LSTM(units = 512, return_sequences = True, input_shape = (X_train_1.shape[1], NBR_FEATURES)))\nRNN1.add(Dropout(0.25))\nRNN1.add(Bidirectional(LSTM(units = 256), merge_mode='ave'))\nRNN1.add(Dropout(0.25))\nRNN1.add(Dense(units = NBR_FEATURES, activation='selu'))\n\nRNN1.summary()","b8259ae5":"%%time\n# Compile the RNN \nRNN1.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Train the RNN\nhistory_RNN1 = RNN1.fit(X_train_1, y_train_1, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=0)","592763e7":"# Convert the training history to a dataframe\nhistory_RNN1_df = pd.DataFrame(history_RNN1.history)\n\n# use Pandas native plot method\nhistory_RNN1_df['loss'].plot(figsize=(8,4), title='MSE for \"Confirmed Cases\" neural network training (evaluation model)', color='brown');","e7bdff45":"# Use now the full dataframe to predict \/ evaluate the model\ndf_full_1 = df_cases_per_million.copy()\n\n# Scale full dataset (use same scaler fitted with train data earlier)\ndf_full_1 = sc1.transform(df_full_1)\n\nX_test_1 = []\nfor i in range(NBR_TRAIN_SAMPLES, NBR_SAMPLES):\n    X_test_1.append(df_full_1[i-TIME_STEPS:i, 0:NBR_FEATURES])\n\nX_test_1 = np.array(X_test_1)\nX_test_1 = np.reshape(X_test_1, (X_test_1.shape[0], X_test_1.shape[1], NBR_FEATURES))\n\n# Make predictions\npredicted_values_1 = RNN1.predict(X_test_1)\npredicted_values_1 = sc1.inverse_transform(predicted_values_1)\n\n# Reverse per million scaling\ni = 0\nfor x in country_list:\n  df_test_1[x + '_Predicted'] = predicted_values_1[:,i] * pop[x] \/ 1000000\n  df_train_1[x] = df_train_1[x] * pop[x] \/ 1000000\n  i = i + 1","f005525b":"fig, ax = plt.subplots(6,3, figsize=(36,30))\nfig.subplots_adjust(top=0.95)\ni = 0\nj = 0\n\nfor x in country_list:\n  ax[i,j].set_title(x, fontsize='x-large')\n  ax[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax[i,j].plot(df_train_1.index, df_train_1[x], color='blue', linewidth=1.5, label='Train')\n  ax[i,j].plot(df_test_1.index, df_test_1[x], color='grey', linewidth=1.5, alpha=0.5, label='Test')\n  ax[i,j].plot(df_test_1.index, df_test_1[x + '_Predicted'], color='indigo', linestyle=':', linewidth=2, label='Prediction')\n  ax[i,j].legend(loc='upper left', fontsize='large')\n  if j<2:\n    j = j + 1\n  else:\n    i = i + 1\n    j = 0\n\nfig.suptitle(str(NBR_PREDICTIONS) + '-day prediction of covid-19 cases vs. training and validation data', fontsize='xx-large')  \nfig.autofmt_xdate(rotation=45, ha='right')\nplt.show()","09747d99":"# Process of DEATHS \n# Split dataset into test and train subsets \ndf_train_2 = df_deaths_per_million.iloc[0:NBR_TRAIN_SAMPLES, 0:NBR_FEATURES] \ndf_test_2 = df_deaths.iloc[NBR_TRAIN_SAMPLES:, 0:NBR_FEATURES]\n\n# Normalize test and train data (range: 0 - 1)\nsc2 = MinMaxScaler(feature_range = (0, 1))\nsc2.fit(df_train_2)\nsc_df_train_2 = sc2.transform(df_train_2)\n\n# Prepare training sequences\nX_train_2 = []\ny_train_2 = []\nfor i in range(TIME_STEPS, NBR_TRAIN_SAMPLES):\n    X_train_2.append(sc_df_train_2[i-TIME_STEPS:i, 0:NBR_FEATURES])\n    y_train_2.append(sc_df_train_2[i, 0:NBR_FEATURES])\n   \nX_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)\nX_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], NBR_FEATURES))","dc25fac9":"# Build the RNN, dropout helps prevent overfitting\n\n# Initialize structure\nRNN2 = Sequential()\n\n# Build layers: 2 LSTM layers with dropout\nRNN2.add(LSTM(units = 512, return_sequences = True, input_shape = (X_train_2.shape[1], NBR_FEATURES)))\nRNN2.add(Dropout(0.25))\nRNN2.add(Bidirectional(LSTM(units = 256), merge_mode='ave'))\nRNN2.add(Dropout(0.25))\nRNN2.add(Dense(units = NBR_FEATURES, activation='selu'))\n\nRNN2.summary()","4c38c634":"%%time\n# Compile the RNN\nRNN2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Train the RNN\nhistory_RNN2 = RNN2.fit(X_train_2, y_train_2, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=0)","06213559":"# Convert the training history to a dataframe\nhistory_RNN2_df = pd.DataFrame(history_RNN2.history)\n\n# use Pandas native plot method\nhistory_RNN2_df['loss'].plot(figsize=(8,4), title='MSE for \"Deaths\" neural network training (evaluation model)', color='brown');","c12a999b":"# Use now the full dataframe to predict \/ evaluate the model\ndf_full_2 = df_deaths_per_million.copy()\n\n# Scale full dataset (use same scaler fitted with train data earlier)\ndf_full_2 = sc2.transform(df_full_2)\n\nX_test_2 = []\nfor i in range(NBR_TRAIN_SAMPLES, NBR_SAMPLES):\n    X_test_2.append(df_full_2[i-TIME_STEPS:i, 0:NBR_FEATURES])\n\nX_test_2 = np.array(X_test_2)\nX_test_2 = np.reshape(X_test_2, (X_test_2.shape[0], X_test_2.shape[1], NBR_FEATURES))\n\n# Make predictions\npredicted_values_2 = RNN2.predict(X_test_2)\npredicted_values_2 = sc2.inverse_transform(predicted_values_2)\n\n# Reverse per million scaling\ni = 0\nfor x in country_list:\n  df_test_2[x + '_Predicted'] = predicted_values_2[:,i] * pop[x] \/ 1000000\n  df_train_2[x] = df_train_2[x] * pop[x] \/ 1000000\n  i = i + 1","b7ad36a0":"fig, ax = plt.subplots(6,3, figsize=(36,30))\nfig.subplots_adjust(top=0.95)\ni = 0\nj = 0\n\nfor x in country_list:\n  ax[i,j].set_title(x, fontsize='x-large')\n  ax[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax[i,j].plot(df_train_2.index, df_train_2[x], color='coral', linewidth=1.5, label='Train')\n  ax[i,j].plot(df_test_2.index, df_test_2[x], color='grey', linewidth=1.5, alpha=0.5, label='Test')\n  ax[i,j].plot(df_test_2.index, df_test_2[x + '_Predicted'], color='black', linestyle=':', linewidth=2, label='Prediction')\n  ax[i,j].legend(loc='upper left', fontsize='large')\n  if j<2:\n    j = j + 1\n  else:\n    i = i + 1\n    j = 0\n\nfig.suptitle(str(NBR_PREDICTIONS) + '-day prediction of covid-19 deaths vs. training and validation data', fontsize='xx-large')  \nfig.autofmt_xdate(rotation=45, ha='right')\nplt.show()","e107365b":"#########################################################################\n# Future prediction model parameters for confirmed cases and deaths\n#########################################################################\n\n# Number of features Xi (Countries)\nNBR_FEATURES = len(country_list)\n\n# Number of predictions (days)\nNBR_PREDICTIONS = 90\n\n# Size ot TRAIN and TEST samples\nNBR_SAMPLES = len(df_cases)\nNBR_TRAIN_SAMPLES = NBR_SAMPLES\n\n# Number of input steps [x(t-1), x(t-2), x(t-3)...] to predict an output y(t)\nTIME_STEPS = 60\n\n# Number of overlapping training sequences of TIME_STEPS\nBATCH_SIZE = 8\n\n# Number of training cycles\nEPOCHS = 250\n\nprint('Future prediction model parameters for confirmed cases and deaths')\nprint('.................................................................')\nprint('NBR_SAMPLES: ', NBR_SAMPLES)\nprint('NBR_TRAIN_SAMPLES: ', NBR_TRAIN_SAMPLES)\nprint('NBR_PREDICTIONS: ', NBR_PREDICTIONS)\nprint()\nprint('TIME_STEPS:', TIME_STEPS)\nprint('NBR_FEATURES: ', NBR_FEATURES)\nprint('BATCH_SIZE: ', BATCH_SIZE)\nprint('EPOCHS: ', EPOCHS)\nprint('.................................................................')","5ad9cafb":"# Use full dataset as train data - CONFIRMED CASES\ndf_train_1 = df_cases_per_million.copy()\n\n# Create empty dataframe with NBR_PREDICTIONS samples\nstart_date = df_train_1.index[-1] + timedelta(days=1)\nind = pd.date_range(start_date, periods=NBR_PREDICTIONS, freq='D')\ndf_pred_1 = pd.DataFrame(index=ind, columns=df_train_1.columns)\ndf_pred_1.fillna(value=0, inplace=True)\n\n# Normalize train data (range: 0 - 1)\nsc1 = MinMaxScaler(feature_range = (0, 1))\nsc1.fit(df_train_1)\nsc_df_train_1 = sc1.transform(df_train_1)\n\n# Prepare training sequences\nX_train_1 = []\ny_train_1 = []\nfor i in range(TIME_STEPS, NBR_TRAIN_SAMPLES):\n    X_train_1.append(sc_df_train_1[i-TIME_STEPS:i, 0:NBR_FEATURES])\n    y_train_1.append(sc_df_train_1[i, 0:NBR_FEATURES])\n\nX_train_1, y_train_1 = np.array(X_train_1), np.array(y_train_1)\nX_train_1 = np.reshape(X_train_1, (X_train_1.shape[0], X_train_1.shape[1], NBR_FEATURES))","4a927972":"%%time\n# Will reuse RNN1 already defined and validated earlier\nRNN1.summary()\n\n# Retrain the RNN with all available data\nhistory_RNN1 = RNN1.fit(X_train_1, y_train_1, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=0)","ae7fe65f":"# Convert the training history to a dataframe\nhistory_RNN1_df = pd.DataFrame(history_RNN1.history)\n\n# use Pandas native plot method\nhistory_RNN1_df['loss'].plot(figsize=(8,4), title='MSE for \"Confirmed Cases\" neural network training (future predictive model)', color='brown');","35593e70":"# Make predictions \nLSTM_predictions_scaled_1 = list()\nbatch = sc_df_train_1[-TIME_STEPS:]\ncurrent_batch = batch.reshape((1, TIME_STEPS, NBR_FEATURES))\n\nfor i in range(len(df_pred_1)):   \n    LSTM_pred_1 = RNN1.predict(current_batch)[0]\n    LSTM_predictions_scaled_1.append(LSTM_pred_1) \n    current_batch = np.append(current_batch[:,1:,:],[[LSTM_pred_1]],axis=1)\n    \n# Reverse downscaling\nLSTM_predictions_1 = sc1.inverse_transform(LSTM_predictions_scaled_1)\ndf_pred_1 = pd.DataFrame(data=LSTM_predictions_1, index=df_pred_1.index, columns=df_pred_1.columns)\n\n# Reverse per million scaling\nfor x in country_list:\n    df_pred_1[x] = df_pred_1[x] * pop[x] \/ 1000000    ","be5df5b4":"fig, ax = plt.subplots(6,3, figsize=(36,30))\nfig.subplots_adjust(top=0.95)\ni = 0\nj = 0\n\nfor x in country_list:\n  ax[i,j].set_title(x, fontsize='x-large')\n  ax[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax[i,j].plot(df_cases.index, df_cases[x], color='blue', linewidth=1.5, label='Actual data')\n  ax[i,j].plot(df_pred_1.index, df_pred_1[x], color='indigo', linewidth=2, linestyle=':', label='Prediction')\n  ax[i,j].legend(loc='upper left', fontsize='large')\n  if j<2:\n    j = j + 1\n  else:\n    i = i + 1\n    j = 0\n\nfig.suptitle(str(NBR_PREDICTIONS) + '-day future prediction of covid-19 cases by country', fontsize='xx-large')  \nfig.autofmt_xdate(rotation=45, ha='right')\nplt.show()","098c0b21":"# Use full dataset as train data - DEATHS\ndf_train_2 = df_deaths_per_million.copy()\n\n# Create empty dataframe with NBR_PREDICTIONS samples\nstart_date = df_train_2.index[-1] + timedelta(days=1)\nind = pd.date_range(start_date, periods=NBR_PREDICTIONS, freq='D')\ndf_pred_2 = pd.DataFrame(index=ind, columns=df_train_2.columns)\ndf_pred_2.fillna(value=0, inplace=True)\n\n# Normalize train data (range: 0 - 1)\nsc2 = MinMaxScaler(feature_range = (0, 1))\nsc2.fit(df_train_2)\nsc_df_train_2 = sc2.transform(df_train_2)\n\n# Prepare training sequences\nX_train_2 = []\ny_train_2 = []\nfor i in range(TIME_STEPS, NBR_TRAIN_SAMPLES):\n    X_train_2.append(sc_df_train_2[i-TIME_STEPS:i, 0:NBR_FEATURES])\n    y_train_2.append(sc_df_train_2[i, 0:NBR_FEATURES])\n\nX_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)\nX_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], NBR_FEATURES))","e5bfa14b":"%%time\n# Will reuse RNN2 already defined and validated earlier\nRNN2.summary()\n\n# Retrain the RNN with all available data\nhistory_RNN2 = RNN2.fit(X_train_2, y_train_2, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=0)","a3931065":"# Convert the training history to a dataframe\nhistory_RNN2_df = pd.DataFrame(history_RNN2.history)\n\n# use Pandas native plot method\nhistory_RNN2_df['loss'].plot(figsize=(8,4), title='MSE for \"Deaths\" neural network training (future predictive model)', color='brown');","d945c718":"# Make predictions \nLSTM_predictions_scaled_2 = list()\nbatch = sc_df_train_2[-TIME_STEPS:]\ncurrent_batch = batch.reshape((1, TIME_STEPS, NBR_FEATURES))\n\nfor i in range(len(df_pred_2)):   \n    LSTM_pred_2 = RNN2.predict(current_batch)[0]\n    LSTM_predictions_scaled_2.append(LSTM_pred_2) \n    current_batch = np.append(current_batch[:,1:,:],[[LSTM_pred_2]],axis=1)\n    \n# Reverse downscaling\nLSTM_predictions_2 = sc2.inverse_transform(LSTM_predictions_scaled_2)\ndf_pred_2 = pd.DataFrame(data=LSTM_predictions_2, index=df_pred_2.index, columns=df_pred_2.columns)\n\n# Reverse per million scaling\nfor x in country_list:\n    df_pred_2[x] = df_pred_2[x] * pop[x] \/ 1000000","bbeae60c":"fig, ax = plt.subplots(6,3, figsize=(36,30))\nfig.subplots_adjust(top=0.95)\ni = 0\nj = 0\n\nfor x in country_list:\n  ax[i,j].set_title(x, fontsize='x-large')\n  ax[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax[i,j].plot(df_deaths.index, df_deaths[x], color='coral', linewidth=1.5, label='Actual data')\n  ax[i,j].plot(df_pred_2.index, df_pred_2[x], color='black', linewidth=2, linestyle=':', label='Prediction')\n  ax[i,j].legend(loc='upper left', fontsize='large')\n  if j<2:\n    j = j + 1\n  else:\n    i = i + 1\n    j = 0\n\nfig.suptitle(str(NBR_PREDICTIONS) + '-day future prediction of covid-19 deaths by country', fontsize='xx-large')  \nfig.autofmt_xdate(rotation=45, ha='right')\nplt.show()","9f5a999d":"#########################################################################\n# Prediction model parameters (Confirmed cases growth rate)\n#########################################################################\n\n# Number of features Xi (Countries)\nNBR_FEATURES = len(country_list)\n\n# Number of predictions (days)\nNBR_PREDICTIONS = 60\n\n# Size ot TRAIN and TEST samples\nNBR_SAMPLES = len(df_cases_diff)\nNBR_TRAIN_SAMPLES = NBR_SAMPLES - NBR_PREDICTIONS\nNBR_TEST_SAMPLES = NBR_SAMPLES - NBR_TRAIN_SAMPLES\n\n# Number of input steps [x(t-1), x(t-2), x(t-3)...] to predict an output y(t)\nTIME_STEPS = 30\n\n# Number of overlapping training sequences of TIME_STEPS\nBATCH_SIZE = 8\n\n# Number of training cycles\nEPOCHS = 200\n\nprint('Prediction model parameters for confirmed cases growth rates')\nprint('............................................................')\nprint('NBR_SAMPLES: ', NBR_SAMPLES)\nprint('NBR_TRAIN_SAMPLES: ', NBR_TRAIN_SAMPLES)\nprint('NBR_TEST_SAMPLES: ', NBR_TEST_SAMPLES)\nprint('NBR_PREDICTIONS: ', NBR_PREDICTIONS)\nprint()\nprint('NBR_FEATURES: ', NBR_FEATURES)\nprint('TIME_STEPS:', TIME_STEPS)\nprint('BATCH_SIZE: ', BATCH_SIZE)\nprint('EPOCHS: ', EPOCHS)\nprint('............................................................')","44e6ea8c":"# Process of CONFIRMED CASES GROWTH RATE data\n\n# Split dataset into test and train subsets \ndf_train_3 = df_cases_diff.iloc[0:NBR_TRAIN_SAMPLES, 0:NBR_FEATURES] \ndf_test_3 = df_cases_diff.iloc[NBR_TRAIN_SAMPLES:, 0:NBR_FEATURES]\n\n# Normalize test and train data (range: 0 - 1)\nsc3 = MinMaxScaler(feature_range = (0, 1))\nsc3.fit(df_train_3)\nsc_df_train_3 = sc3.transform(df_train_3)\n\n# Prepare training sequences\nX_train_3 = []\ny_train_3 = []\nfor i in range(TIME_STEPS, NBR_TRAIN_SAMPLES):\n    X_train_3.append(sc_df_train_3[i-TIME_STEPS:i, 0:NBR_FEATURES])\n    y_train_3.append(sc_df_train_3[i, 0:NBR_FEATURES])\n   \nX_train_3, y_train_3 = np.array(X_train_3), np.array(y_train_3)\nX_train_3 = np.reshape(X_train_3, (X_train_3.shape[0], X_train_3.shape[1], NBR_FEATURES))","ccdbaf0d":"# Build the RNN, dropout helps prevent overfitting\n\n# Initialize structure\nRNN3 = Sequential()\n\n# Build layers: 2 LSTM layers with dropout\nRNN3.add(LSTM(units = 512, return_sequences = True, input_shape = (X_train_3.shape[1], NBR_FEATURES)))\nRNN3.add(Dropout(0.3))\nRNN3.add(LSTM(units = 512))\nRNN3.add(Dropout(0.3))\nRNN3.add(Dense(units = NBR_FEATURES, activation='relu'))\n\nRNN3.summary()","d580ddfa":"%%time\n# Compile the RNN\nRNN3.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Retrain the RNN with all available data\nhistory_RNN3 = RNN3.fit(X_train_3, y_train_3, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=0)","d6ba7a82":"# convert the training history to a dataframe\nhistory_RNN3_df = pd.DataFrame(history_RNN3.history)\n\n# use Pandas native plot method\nhistory_RNN3_df['loss'].plot(figsize=(8,4), title='MSE for \"Cases Growth Rate\" neural network training (evaluation model)', color='brown');","6d2fa25f":"# Use now the full dataframe to predict \/ evaluate the model\ndf_full_3 = df_cases_diff.copy()\n\n# Scale full dataset (use same scaler fitted with train data earlier)\ndf_full_3 = sc3.transform(df_full_3)\n\nX_test_3 = []\nfor i in range(NBR_TRAIN_SAMPLES, NBR_SAMPLES):\n    X_test_3.append(df_full_3[i-TIME_STEPS:i, 0:NBR_FEATURES])\n\nX_test_3 = np.array(X_test_3)\nX_test_3 = np.reshape(X_test_3, (X_test_3.shape[0], X_test_3.shape[1], NBR_FEATURES))\n\n# Make predictions\npredicted_values_3 = RNN3.predict(X_test_3)\npredicted_values_3 = sc3.inverse_transform(predicted_values_3)\n\ni = 0\nfor x in country_list:\n  df_test_3[x + '_Predicted'] = predicted_values_3[:,i]\n  i = i + 1","9b134781":"# Plot future predictions of the cases growth rate\nfig, ax = plt.subplots(6,3, figsize=(36,30))\nfig.subplots_adjust(top=0.95)\ni = 0\nj = 0\n\nfor x in country_list:\n  ax[i,j].set_title(x, fontsize='x-large')\n  ax[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax[i,j].plot(df_train_3.index, df_train_3[x], color='indigo', linewidth=1.5, label='Train')\n  ax[i,j].plot(df_test_3.index, df_test_3[x], color='grey', linewidth=1.5, alpha=0.5, label='Test')\n  ax[i,j].plot(df_test_3.index, df_test_3[x + '_Predicted'], color='red', linestyle=':', linewidth=1.5, label='Prediction')\n  ax[i,j].legend(loc='upper left', fontsize='medium')\n  if j<2: \n    j = j + 1\n  else:\n    i = i + 1\n    j = 0\n\n\nfig.suptitle(str(NBR_PREDICTIONS) + '-day prediction of the covid-19 cases growth rate vs. training and validation data', fontsize='xx-large')  \nfig.autofmt_xdate(rotation=45, ha='right')\nplt.show()","628dca9b":"#########################################################################\n# Future prediction model parameters for confirmed cases growth rate\n#########################################################################\n\n# Number of features Xi (Countries)\nNBR_FEATURES = len(country_list)\n\n# Number of predictions (days)\nNBR_PREDICTIONS = 60\n\n# Size ot TRAIN and TEST samples\nNBR_SAMPLES = len(df_cases_diff)\nNBR_TRAIN_SAMPLES = NBR_SAMPLES\n\n# Number of input steps [x(t-1), x(t-2), x(t-3)...] to predict an output y(t)\nTIME_STEPS = 30\n\n# Number of overlapping training sequences of TIME_STEPS\nBATCH_SIZE = 8\n\n# Number of training cycles\nEPOCHS = 200\n\nprint('Future prediction model parameters for confirmed cases growth rate')\nprint('..................................................................')\nprint('NBR_SAMPLES: ', NBR_SAMPLES)\nprint('NBR_TRAIN_SAMPLES: ', NBR_TRAIN_SAMPLES)\nprint('NBR_PREDICTIONS: ', NBR_PREDICTIONS)\nprint()\nprint('TIME_STEPS:', TIME_STEPS)\nprint('NBR_FEATURES: ', NBR_FEATURES)\nprint('BATCH_SIZE: ', BATCH_SIZE)\nprint('EPOCHS: ', EPOCHS)\nprint('..................................................................')","b2dcc917":"# Use full dataset as train data - CONFIRMED CASES GROWTH RATE\ndf_train_3 = df_cases_diff.copy()\n\n# Create empty dataframe with NBR_PREDICTIONS samples\nstart_date = df_train_3.index[-1] + timedelta(days=1)\nind = pd.date_range(start_date, periods=NBR_PREDICTIONS, freq='D')\ndf_pred_3 = pd.DataFrame(index=ind, columns=df_train_3.columns)\ndf_pred_3.fillna(value=0, inplace=True)\n\n# Normalize train data (range: 0 - 1)\nsc3 = MinMaxScaler(feature_range = (0, 1))\nsc3.fit(df_train_3)\nsc_df_train_3 = sc3.transform(df_train_3)\n\n# Prepare training sequences\nX_train_3 = []\ny_train_3 = []\nfor i in range(TIME_STEPS, NBR_TRAIN_SAMPLES):\n    X_train_3.append(sc_df_train_3[i-TIME_STEPS:i, 0:NBR_FEATURES])\n    y_train_3.append(sc_df_train_3[i, 0:NBR_FEATURES])\n\nX_train_3, y_train_3 = np.array(X_train_3), np.array(y_train_3)\nX_train_3 = np.reshape(X_train_3, (X_train_3.shape[0], X_train_3.shape[1], NBR_FEATURES))","9a64bcc8":"%%time\n# Will reuse RNN3 already defined and validated earlier\nRNN3.summary()\n\n# Retrain the RNN with all available data\nhistory_RNN3 = RNN3.fit(X_train_3, y_train_3, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=0)","b720dffe":"# Convert the training history to a dataframe\nhistory_RNN3_df = pd.DataFrame(history_RNN3.history)\n\n# use Pandas native plot method\nhistory_RNN3_df['loss'].plot(figsize=(8,4), title='MSE for \"Cases Growth Rate\" neural network training (future predictive model)', color='brown');","71c8d300":"# Make predictions \nLSTM_predictions_scaled_3 = list()\nbatch = sc_df_train_3[-TIME_STEPS:]\ncurrent_batch = batch.reshape((1, TIME_STEPS, NBR_FEATURES))\n\nfor i in range(len(df_pred_3)):   \n    LSTM_pred_3 = RNN3.predict(current_batch)[0]\n    LSTM_predictions_scaled_3.append(LSTM_pred_3) \n    current_batch = np.append(current_batch[:,1:,:],[[LSTM_pred_3]],axis=1)\n    \n# Reverse downscaling\nLSTM_predictions_3 = sc3.inverse_transform(LSTM_predictions_scaled_3)\ndf_pred_3 = pd.DataFrame(data=LSTM_predictions_3, index=df_pred_3.index, columns=df_pred_3.columns)","331a0944":"fig, ax = plt.subplots(6,3, figsize=(36,30))\nfig.subplots_adjust(top=0.95)\ni = 0\nj = 0\n\nfor x in country_list:\n  ax[i,j].set_title(x, fontsize='x-large')\n  ax[i,j].xaxis.set_major_formatter(my_date_fmt)\n  ax[i,j].xaxis.set_major_locator(plt.MultipleLocator(28))\n  ax[i,j].plot(df_train_3.index, df_train_3[x], color='indigo', linewidth=1.5, label='Actual data')\n  ax[i,j].plot(df_pred_3.index, df_pred_3[x], color='red', linewidth=1.5, linestyle=':', label='Prediction')\n  ax[i,j].legend(loc='upper left', fontsize='medium')\n  if j<2:\n    j = j + 1\n  else:\n    i = i + 1\n    j = 0\n\nfig.suptitle(str(NBR_PREDICTIONS) + '-day future prediction of the confirmed cases growth rate by country', fontsize='xx-large')  \nfig.autofmt_xdate(rotation=45, ha='right')\nplt.show()","e4c09b55":"#np.random.get_state()","b1bf21e3":"### Plot cases and deaths per million people","2417d11c":"### Exogenous data: country populations","05c62eb7":"### Retrain confirmed cases RNN and make future predictions","67b8a391":"Sometimes the predictions are all wrong, with the values predicted for the cumulative variables (cases and deaths) decreasing which is just not possible. I do not know why it happens, but I have learnt that another run (or two) should bring me the desired results.","6c094d2d":"## Descriptive statistics\n---\n\nLet's have a look at where each country is in its specific pandemic expansion. \n\nThe following variables will be displayed for each of the 18 countries:\n- Confirmed cases, active cases and deaths\n- Mortality(%)\n- Growth rates: confirmed cases and deaths","bcfde413":"## Data load and pre-processing\n---","48a23b6e":"### Retrain deaths RNN and make future predictions","763ac745":"### Mortality\n\nMortality is calculated as the number of deaths divided by the number of confirmed cases, and expressed as %. As both the number of deaths and confirmed cases vary with time, we can only plot an \"instant mortality\" chart. However, the real mortality will only be known once the pandemic has been erradicated, so the total number of cases and deaths are known. \n\nAfter some thought about this, from version 124 I have removed the mortality charts and left only the \"current or latest mortality\" values, corresponding to the up-to-date cummulative numbers of cases and deaths.","53f02945":"### Build, train and evaluate RNN for confirmed cases","50a2ef1d":"### Confirmed cases growth rate\n\nThe daily change of the number of confirmed cases is calculated by substracting the value of cases on day t-1 from the value on day t. \n\nIt represents the rate of growth, that is, how quickly (or slowly) the number of detected cases is changing.\n\nSince the growth rate is the first derivative of an strictly growing variable (confirmed cases) it must be always positive or zero.","6da23c7f":"## Predictive model of confirmed cases growth rate\n---\n\nFinally, I will implement and train a LSTM-based predicive model of the cases growth rate. \n\nI initially wanted to implement a future prediction model of active cases growth rate (in fact, early versions of this notebook did so) but time and experience has taught me that was a bad variable to work with. Countries like Spain, UK or Sweden do not report covid-19 recovered figures, which is a component of active cases, so any further work on the latter will carry over any errors of the former.\n\nSo I decided to stick to the basics and do a predictive model of the number of cases growth rate instead. This is actually the very first variable reported and spoken about. When someone says in the TV news \"there were 2500 new cases registered today\" this is the value of the cases growth rate today. \n\nUnlike the confirmed cases and deaths, where I normalized per million people before training the RNNs, I have worked here with the total number of cases growth rate, and the results are acceptable.","53f696bc":"### Deaths growth rate\n\nThe daily variation of deaths (or deaths growth rate) tells us how quickly the number of deaths due to covid-19 is increasing (or decreasing).\n\nThe same as in the confirmed cases growth rate, the deaths growth rate can only be positive or zero, but not negative.","b6e279a3":"## Future predictions of confirmed cases and deaths\n---\n\nThe above predictons look quite impressive. However, it is neccessary to explain here that the predicted values are single-step predictions. \n\nThe LSTM neural network works in such way that it predicts y(t) from X(t-1), X(t-2), .... X(t-TIMESTEPS). So I have trained it to do so with batches formed with data up to NBR_TRAIN_SAMPLES. But then the validation is done by using such batches with data after NBR_TRAIN_SAMPLES and up to NBR_PREDICTIONS - 1. \n\nSo it is not bad, but it is not enough. What I really want is to make multi-step predictions. More precisely, I would like to predict 30 days ahead starting the day after today. I can do that by training the LSTM network with all data available (up to the date of execution) and then make recurring predictions retrofitting each new single-step prediction as an input for the next prediction.\n\nLet's see this at work.","9e27d81e":"## Covid-19 stats and predictions with LSTM neural networks\n---\nLast updated: 18\/12\/2021\n\n### Introduction\n\nThis noteboook makes a comparative analysis and future predictions of covid-19 cases and deaths for 18 countries: \n- 9 European Union countries: Spain, Belgium, France, Germany, Italy, Netherlands, Portugal, Sweden, Switzerland\n- 9 world countries: Argentina, Brazil, Canada, India, Iran, Mexico, Russia, United Kingdom, United States (US)\n\nThe primary (measured) variables I will work with, as obtained from the repository files, are:\n\n    - number of confirmed cases\n    - number of recovered cases\n    - number of deaths\n    \nThen I will calculate and investigate these additional variables:\n\n    - active cases\n    - mortality(%)\n    - growth rates of cases and deaths\n    \nFinally, I will implement multivariate - multistep LSTM predictive models for some of these variables. I will be using the data series for all countries as inputs (multivariate) which means each output will be a function of itself and the other country inputs. The idea is that being the selected countries in different stages of the pandemic evolution, the RNN will enhance its predictive power on those curves lagging behind by using its knowledge of those ahead.     \n\nI hope you will find it interesting. Please upvote me if you do!!!\n\n\n### Version release notes\n\nNote that a new version is saved every time the notebook is run, mostly with the purpose of updating with the latest data. I typically do this every two or three days since I started this notebook sometime in April 2020. I will not be commenting those version updates. \n\n-----------------------------------------------\n#### Version 205 (update on 1\/04\/2021)\n\nAdded bidirectional layer wrapper to deep layer in neural networks 1 and 2 (cummulative confirmed cases and deaths). Significant improvement in quality of predictions and reproducibility. \n\n-----------------------------------------------\n#### Version 181 (update on 10\/11\/2020)\n\nReplaced China series with Argentina. \n\n-----------------------------------------------\n#### Version 151 (update on 13\/10\/2020)\n \nAdded plots of neural networks training loss, to better understand the training process and results, adjusting number of epochs in consequence. \n\nAlso experimented with different activation functions (elu, selu, relu, swish) for the the neural networks output (dense).\n\n-----------------------------------------------\n#### Version 125 (update on 13\/09\/2020)\n\nCode has been added with the countries populations so to calculate number of confirmed cases and deaths per million people, before introducing the data to train the LSTM neural networks. Results seem to be at least more reliable (more predictions are good). Minor, disperse changes to update dependent code blocks.\n\nActive cases charts (per country) removed too due to the poor quality of the series for some countries.\n\n-----------------------------------------------\n#### Version 124 (update on 11\/09\/2020)\n\nMortality charts suppressed, left only the latest (current) values of mortality.\n\n-----------------------------------------------\n#### Version 68 (update on 14\/08\/2020)\n\nCode modified to read the data files directly from the COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (before they were being read from a pre-processed version in Kaggle): \n\n    https:\/\/github.com\/CSSEGISandData\/COVID-19\n\nThese files are updated daily and hence you can get an up-to-date, fresh execution any time you run the notebook. Also these files contain confirmed cases, recovered and dates cumulative numbers for 188 countries in the world, so whilst I use a subset of 18 countries, you can easily fork the notebook and taylor it to your needs.","93498c51":"## Data series normalization by countries population\n---\n\nThe purpose of this part is to re-calculate the variables object of study by dividing the confirmed cases and deaths time series of each country by its population, so to have numbers per million, which reduces the range of values of the set. \n\nThis will allow for a more convenient comparison of the countries infection levels, and also seems to give better results when fed into the predictive models.","81917295":"### Remove outliers","bdff7fea":"### Compute growth rates","a0b3da4b":"### Compute mortality","d7e3d357":"### Read files and tidy up","026cebae":"### Build, train and evaluate RNN for confirmed cases growth rate","d822f5ba":"\n\n(End of noteboook)","2043a4f5":"### Country selection","acef34d5":"### Evolution of covid-19 cases","1877ab29":"### Build, train and evaluate RNN for deaths","ec14bc8b":"### Compute active cases","b64a28d4":"## Future predictions of confirmed cases growth rate","1ccee252":"### Library imports and general settings","b69639ca":"### Calculate confirmed cases and deaths per million","9aefaa04":"## Predictive models of confirmed cases and deaths\n---\n\nI will now build, train and evaluate two LSTM-based predictive models for the main cumulative variables. I have used a \"Mean Square Error (MSE)\" metric to set the neural networks optimization objective and monitor its learning ability over the epochs.\n\nThis has been an iterative exercise of model parameters investigation,  hyperparameter tuning and activation function selection. Or maybe even more. What I am trying to say is do not expect all the choices below to make sense right away, as some may be the result of a lengthy trial - error effort.\n\nBased on various tests, it also appears the LSTM networks work better when the different timeseries have a similar range of values. Because of this, I will feed the variables per million to the predictive models to train them and make predictions. In order to plot the resulting charts with meaningful (real) values, I will then perform the inverse normalization by multiplying predicted data by each country population.","5a39cbd4":"### Retrain cases growth rate RNN and make future predictions"}}