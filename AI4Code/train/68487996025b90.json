{"cell_type":{"2d9c58d3":"code","da94131a":"code","0c79fd74":"code","cc553bda":"code","b7caa468":"code","34297b89":"code","c86182ae":"code","ef74a378":"code","0189efb9":"code","947ed8b7":"code","df7d729f":"code","125a5850":"code","4a822600":"code","2f2ba1e8":"code","e0655e40":"code","f64e8514":"code","3ddb3064":"code","07e9f0f6":"code","1d5e7876":"code","7e2c2e6e":"code","a77aa718":"code","559e67d3":"code","8d0f5a7c":"code","fe90f77e":"code","f3e04a83":"code","b138fc37":"code","28dd14af":"code","4f57ab60":"code","fa4a8ba4":"code","158fa720":"code","184bf0a8":"code","cfd0d8ad":"code","9d84b0ba":"code","f1083d0b":"code","ab5a0e8a":"code","e16d8ddc":"code","93012b9b":"code","af37d2a0":"code","f565f747":"code","d04d3e46":"code","15e52172":"code","45483c52":"code","d001d7e5":"code","dd8cf724":"code","c562e16a":"code","00f8718c":"code","45313590":"code","a0c2582a":"code","aee67a55":"code","1b651d10":"code","91d809c1":"code","03830495":"code","252f10cd":"code","feac33d5":"code","d7bd3bc1":"code","4711fd47":"code","9300381e":"code","49746449":"code","c23f11cd":"code","cc686b3a":"code","05387363":"code","38dbe8ce":"code","1602f616":"code","370d79f7":"code","c14f7109":"markdown","1a209f6b":"markdown","1bb95c8e":"markdown","db45ac77":"markdown","e15919b4":"markdown","acc288ed":"markdown","7bece67d":"markdown","0894b144":"markdown","0801f9ac":"markdown","c69d8799":"markdown","ecfa00ee":"markdown","daa24feb":"markdown","21deebdc":"markdown","d174f31a":"markdown","9d09c7e0":"markdown","b9d4eb7f":"markdown","7ea8563a":"markdown","945cd325":"markdown","b9c8e5d3":"markdown","53fc9bc6":"markdown"},"source":{"2d9c58d3":"#####################################################################################################\n## This notebook contains 5 popular techniques for XML using the Titanic dataset from kaggle\n\n## Make sure to install the following before running the notebook:\n### pip install LIME\n### pip install shap\n### pip install pdpbox\n### pip install deeplift\n\n#####################################################################################################\n## Pooja Vijaykumar","da94131a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport lime\nimport lime.lime_tabular\n\nimport shap\nfrom xgboost import XGBClassifier\n\nfrom pdpbox import pdp\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout \nfrom keras.models import model_from_json\n\nimport deeplift\nfrom deeplift.layers import NonlinearMxtsMode\nimport deeplift.conversion.kerasapi_conversion as kc","0c79fd74":"train = pd.read_csv('train.csv')","cc553bda":"test = pd.read_csv('test.csv')","b7caa468":"train.head()","34297b89":"train.isnull().sum()","c86182ae":"med1 = train.Age.median()\nmed2 = test.Age.median()","ef74a378":"train['Age'] = train['Age'].map(lambda x: med1 if np.isnan(x) else x)\ntest['Age'] = test['Age'].map(lambda x: med2 if np.isnan(x) else x)","0189efb9":"mode = train.Embarked.value_counts()[0]","947ed8b7":"train['Embarked'] = train['Embarked'].map(lambda x: mode if pd.isnull(x) else x)","df7d729f":"train = train.dropna()\ntest = test.dropna()","125a5850":"train.head()","4a822600":"train = train.join(pd.get_dummies(train['Sex'], prefix='Sex'))\ntrain = train.join(pd.get_dummies(train['Pclass'], prefix='Pclass'))\ntrain = train.join(pd.get_dummies(train['Embarked'], prefix='Embarked'))\ntrain = train.join(pd.get_dummies(train['SibSp'], prefix='SibSp'))\ntrain = train.join(pd.get_dummies(train['Parch'], prefix='Parch'))","2f2ba1e8":"rem = ['PassengerId','Name','Ticket','Cabin','Pclass','Sex','Embarked','SibSp','Parch']\n\nfor i in train.columns:\n    if i in rem:\n        train = train.drop(i, axis=1)","e0655e40":"train.head()","f64e8514":"test.head()","3ddb3064":"test = test.join(pd.get_dummies(test['Sex'], prefix='Sex'))\ntest = test.join(pd.get_dummies(test['Pclass'], prefix='Pclass'))\ntest = test.join(pd.get_dummies(test['Embarked'], prefix='Embarked'))\ntest = test.join(pd.get_dummies(test['SibSp'], prefix='SibSp'))\ntest = test.join(pd.get_dummies(test['Parch'], prefix='Parch'))","07e9f0f6":"rem = ['PassengerId','Name','Ticket','Cabin','Pclass','Sex','Embarked','SibSp','Parch']\n\nfor i in test.columns:\n    if i in rem:\n        test = test.drop(i, axis=1)","1d5e7876":"xtrain = train.drop('Survived',axis=1)\nytrain = train['Survived']\nxtest = test","7e2c2e6e":"######################################################","a77aa718":"rfc = RandomForestClassifier(n_estimators=500, n_jobs=-1).fit(xtrain, ytrain)","559e67d3":"## Group all model features and obtain their importance\n## In this case, the Gini importance is used\n## Gini importance = total decrease in node impurity averaged over all the trees in the ensemble \n\nfeats = {}\n\nfor x,y in zip(xtrain.columns, rfc.feature_importances_):\n    feats[x] = y","8d0f5a7c":"## sort features based on Gini importance in descending order\n\nimp = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0:'Gini-importance'})\nimp_sort = imp.sort_values(by = 'Gini-importance', ascending=False)\nimp_sort","fe90f77e":"## plot features and their Gini importance\n\nx = imp_sort.index\ny = []\nfor i in range(0,len(imp_sort)):\n    y.append(imp_sort['Gini-importance'][i])\n\nplt.figure(figsize=(15,7))\nbar_plot = plt.bar(x,y)\nplt.xticks(rotation=90)\nplt.title('Feature Importance using Gini Index')\nplt.xlabel('Features')\nplt.ylabel('Gini-importance')\nplt.show()","f3e04a83":"#########################################################################","b138fc37":"## Extract all the features from your data and place them in different lists based on their type \n## (float in one list, int64 in another)\n\nfl = []\ninf = []\n\nfor i in xtrain.columns:\n    if xtrain[i].dtype=='float64':\n        fl.append(i)\n    elif xtrain[i].dtype=='uint8':\n        inf.append(i)\n        \ntrain_fl = xtrain[fl]\ntrain_int = xtrain[inf]","28dd14af":"## concatenate the lists\nfeats = list(train_fl) + list(train_int)","4f57ab60":"predict_fn_rf = lambda x: rfc.predict_proba(x).astype(float)","fa4a8ba4":"explainer = lime.lime_tabular.LimeTabularExplainer(xtrain[feats].astype(int).values, \n                                                   mode='classification',\n                                                   training_labels=train['Survived'],\n                                                   feature_names=feats)","158fa720":"## explanation for index 0 whose output is class 1\nexp = explainer.explain_instance(xtest.iloc[0], predict_fn_rf, num_features=19)\nexp.show_in_notebook(show_all=False)","184bf0a8":"## explanation for index 5 whose output is class 0\nexp = explainer.explain_instance(xtest.iloc[5], predict_fn_rf, num_features=19)\nexp.show_in_notebook(show_all=False)","cfd0d8ad":"###############################################################################","9d84b0ba":"xtrain.columns","f1083d0b":"## CAUTION:\n## rename columns with special chars ('<,>') because XGB doesn't take special chars in feature names\n## do this to avoid basic syntax errors","ab5a0e8a":"xgb = XGBClassifier().fit(xtrain, ytrain)","e16d8ddc":"## load JS visalization for SHAP plots\nshap.initjs()","93012b9b":"## create SHAP Explainer\n## The TreeExplainer from the SHAP library is optimized to trace through the XGBoost tree to \n## find the Shapley value estimates of the features\n\nexplainer = shap.TreeExplainer(xgb)","af37d2a0":"## obtain the Shapley values\nshap_values = explainer.shap_values(xtest)","f565f747":"## SHAP plot for xtest[0] where output is '1'\nshap.force_plot(explainer.expected_value, \n                shap_values[0], \n                features=xtest.iloc[[0]], \n                feature_names=xtest.columns, \n                link='logit')","d04d3e46":"## SHAP plot for xtest[5] where output is '0'\nshap.force_plot(explainer.expected_value, \n                shap_values[5], \n                features=xtest.iloc[[5]], \n                feature_names=xtest.columns, \n                link='logit')","15e52172":"# global view\nshap.summary_plot(shap_values, feature_names=xtest.columns, features=xtest)","45483c52":"#######################################################################","d001d7e5":"## PDP for single feature - enter desired feature\n## pdp_plot plots the feature impact\n\nff = 'Age'\npdp_feat = pdp.pdp_isolate(model=rfc, dataset=test, model_features=xtest.columns,feature=ff)\nfig, axes = pdp.pdp_plot(pdp_feat,ff)","dd8cf724":"## Interaction between two features\n## PDP for feature combination -- enter two features\n## pdp_interact_plot plots the feature interaction \n## changing x_quantile and plot_pdp between True and False gives you two types of plots but with the same depiction\n\nf1 = 'Age'\nf2 = 'Sex_female'\n\ninter = pdp.pdp_interact(model = rfc, dataset=test, model_features=xtest.columns, features=[f1,f2])\nfig, axes = pdp.pdp_interact_plot(pdp_interact_out=inter,feature_names=[f1,f2] ,plot_type='contour',\n                                 x_quantile=False, plot_pdp=False)","c562e16a":"## give last two (x_quantile and plot_pdp) as True for different type of contour plot\nf1 = 'Age'\nf2 = 'Sex_female'\ninter1 = pdp.pdp_interact(model=rfc, dataset=test, model_features=xtest.columns, features=[f1, f2])\nfig, axes = pdp.pdp_interact_plot(\n    pdp_interact_out=inter1, feature_names=[f1, f2], plot_type='contour', x_quantile=True, plot_pdp=True)","00f8718c":"#########################################################################","45313590":"model = Sequential()\n\nmodel.add(Dense(200, activation='relu', input_dim=19))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\n\nmodel.add(Dense(1,activation='softmax'))","a0c2582a":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","aee67a55":"model.fit(xtrain, ytrain, epochs=20)","1b651d10":"## save model to json file\nmodel_json = model.to_json()\nwith open('model.json','w') as json_file:\n    json_file.write(model_json)","91d809c1":"## save model weights to HDF5\nmodel.save_weights('model.h5')","03830495":"keras_json = 'model.json'\nkeras_weights = 'model.h5'","252f10cd":"km = model_from_json(open(keras_json).read())","feac33d5":"km.load_weights(keras_weights)","d7bd3bc1":"## convert Keras Sequential model to DeepLIFT model\ndeepmodel = kc.convert_model_from_saved_files(\n                    json_file = keras_json, \n                    h5_file = keras_weights, \n                    nonlinear_mxts_mode=deeplift.layers.NonlinearMxtsMode.GuidedBackprop)","4711fd47":"deepmodel.get_layers()","9300381e":"## Specify the index of the layer to compute the importance scores of.\n## In the example below, we find scores for the input layer, which is idx 0 in deeplift_model.get_layers()\nfind_scores_layer_idx = 0","49746449":"## Compile the function that computes the contribution scores\n## target_layer_idx= -1 (regression), -2 (sigmoid\/softmax outputs)\ndeeplift_contribs_func = deepmodel.get_target_contribs_func(\n                            find_scores_layer_idx=find_scores_layer_idx,\n                            target_layer_idx=-2)","c23f11cd":"## choose the input row whose Contribution Score is to be observed\nidx = 0","cc686b3a":"## obtain the scores\nscores = np.array(deeplift_contribs_func(task_idx=0,\n                                         input_data_list=[xtest.iloc[[0]]],\n                                         batch_size=10,\n                                         progress_update=1000))","05387363":"scores","38dbe8ce":"## flatten the list\nflat_scores = [item for sublist in scores for item in sublist]","1602f616":"## Group all model features with their importance\nfeats = {}\n\nfor x,y in zip(xtest.columns, flat_scores):\n    feats[x] = y","370d79f7":"imp = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0:'Contribution Score'})\nimp\n\n## can sort it as well:\n#imp_sort = imp.sort_values(by = 'Contribution Score', ascending=False)\n#imp_sort","c14f7109":"#### For more information on DeepLIFT: https:\/\/github.com\/kundajelab\/deeplift","1a209f6b":"### 3) SHAP - SHapley Additive exPlanations\n#### XGBoost Classifier --> SHAP is optimized for this","1bb95c8e":"### 1) FEATURE IMPORTANCE\n#### Random Forest Classifier - to classify whether Survived or not ","db45ac77":"#### High probability of survival when Sex_female is 1 and Age between < 40\n#### for Age > ~45, survival is nearly independent of Sex_female, whereas for lower Ages, there is a strong dependence on Gender","e15919b4":"### end of the notebook.","acc288ed":"### 5) DeepLIFT - Deep Learning Important Features\n#### Keras NN for classification","7bece67d":"#### High values of Sex_female have a positive contribution to the prediction, while it\u2019s the opposite for Age, where high values have a negative contribution to the predictions","0894b144":"#### Sex_female<=0 indicates if this feature\u2019s value satisfies this criteria, it supports class 0","0801f9ac":"#### Blue shaded area indicates the level of confidence\n#### We can observe that having  Age above 30-40 decreases your chances of survival\n#### The model thinks you\u2019re more likely to survive if your Age is <30","c69d8799":"#### end of preprocessing\n#### split the target from the features of training data","ecfa00ee":"#### preprocessing both train and test data","daa24feb":"#### Base value: average of all predictions\n#### Each arrow is a Shapley value that pushes to increase (red) or decrease (blue) the prediction\n#### Person 1 has high predicted survival probability of 1\n#### Factors like Age and Sex_female increases the person\u2019s predicted survival probability","21deebdc":"#### Choose a specific GINI cut-off that gives top-N features and retrain the model with new set of features","d174f31a":"### 2) LIME - Local Interpretable Model-Agnostic Explanations","9d09c7e0":"#### Each point is a Shapley value for that instance \n#### Colour represents value of that feature\n#### Features are ordered according to their importance \n#### X-axis = Shapley values\n#### Y-axis = features","b9d4eb7f":"#### Negative\/Positive contribution score means that the input contributed to moving the output below\/above its reference value\n#### Reference value: by how much the output changes based on changes in input. Here it is 0 (which is the usual scenario unless specified otherwise)\n#### positive scores push the probability higher and negative scores push it lower (Similar to Shapley values)\n#### Scores closer to zero are considered \u201cunimportant\u201d by DeepLIFT, i.e., those features that DeepLIFT thinks wouldn\u2019t affect the output much","7ea8563a":"#### Sex_female is assigned an importance of 0.26 and the colour indicates if its contributing to 0 (blue) or 1 (orange)\n#### The range of Sex_female indicates if this feature\u2019s value satisfies this criteria, it supports class 1","945cd325":"#### Person 2 has low predicted survival probability of 0.20\n#### Factors like Fare, while it tends to increase the survival probability, is offset by decreasing effects like Sex_female, Age, etc. ","b9c8e5d3":"#### DeepLIFT decomposes the prediction on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input","53fc9bc6":"### 4) Partial Dependence Plots "}}