{"cell_type":{"fd6cae54":"code","b4508905":"code","74c4e544":"code","3c79cffc":"code","44ee51e4":"code","037c5a93":"code","6989bcee":"code","fe52abfb":"code","3d1dfa47":"code","e57ba4ee":"code","219c3873":"code","0b327c98":"code","ec50ae52":"code","29228581":"code","c2fc88e5":"code","a0accb03":"code","842510f9":"code","9b331c88":"code","09b315df":"code","31642d0b":"code","e21c7d1d":"code","cc0cac6a":"code","42c8ebbb":"code","f2dcc1cb":"code","b93f0b67":"code","e05b8b81":"code","f65134d0":"code","355f34ae":"code","6c0e794f":"code","c6c60c85":"code","57ad6454":"code","4fa59428":"code","fbb29435":"code","b47c98d6":"code","7fcf9a16":"code","65442313":"code","b62f2175":"code","8e2e1128":"code","41d6551a":"code","f1b8183e":"code","8111d8f7":"code","12d8b09d":"code","0b703c3c":"code","e4e957c7":"code","f2c5ddb2":"code","6a349176":"code","d8b641fa":"code","f76e931d":"markdown"},"source":{"fd6cae54":"import pandas as pd","b4508905":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","74c4e544":"train_df=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")","3c79cffc":"test_df=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","44ee51e4":"sub=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")","037c5a93":"train_df.head()","6989bcee":"train_df.shape","fe52abfb":"test_df.shape","3d1dfa47":"shop_df=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")","e57ba4ee":"shop_df.head()","219c3873":"items_df=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")","0b327c98":"items_df.head()","ec50ae52":"item_cat=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")","29228581":"item_cat.head()","c2fc88e5":"train_df.describe()","a0accb03":"train_df.describe().T","842510f9":"# getting the information about the data\n\ntrain_df.info()","9b331c88":"train_df.isnull().sum()","09b315df":"#check for null values in each data frame\nprint(\"No. of Null values in the train set :\", train_df.isnull().sum().sum())\nprint(\"No. of Null values in the test set :\", test_df.isnull().sum().sum())\nprint(\"No. of Null values in the item set :\", items_df.isnull().sum().sum())\nprint(\"No. of Null values in the shops set :\", shop_df.isnull().sum().sum())\nprint(\"No. of Null values in the item_categories set :\", item_cat.isnull().sum().sum())","31642d0b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e21c7d1d":"plt.rcParams['figure.figsize'] = (19, 9)\nsns.barplot(item_cat['item_category_id'], item_cat['item_category_id'], palette = 'colorblind')\nplt.title('Count for Different Items Categories', fontsize = 30)\nplt.xlabel('Item Categories', fontsize = 15)\nplt.ylabel('Items in each Categories', fontsize = 15)\nplt.show()","cc0cac6a":"plt.rcParams['figure.figsize'] = (19, 9)\nsns.countplot(train_df['date_block_num'])\nplt.title('Date blocks according to months', fontsize = 30)\nplt.xlabel('Different blocks of months', fontsize = 15)\nplt.ylabel('No. of Purchases', fontsize = 15)\nplt.show()","42c8ebbb":"plt.rcParams['figure.figsize'] = (13, 7)\nsns.distplot(train_df['item_price'], color = 'red')\nplt.title('Distribution of the price of Items', fontsize = 30)\nplt.xlabel('Range of price of items', fontsize = 15)\nplt.ylabel('Distrbution of prices over items', fontsize = 15)\nplt.show()","f2dcc1cb":"train_df.nunique()","b93f0b67":"train_df['item_id'].value_counts()","e05b8b81":"\n# checking the no. of unique item present in the stores\n\nx = train_df['item_id'].nunique()\nprint(\"The No. of Unique Items Present in the stores available: \", x)","f65134d0":"# checking the no. of unique item present in the stores\n\nx = item_cat['item_category_id'].nunique()\nprint(\"The No. of Unique categories for Items Present in the stores available: \", x)","355f34ae":"# checking the no. of unique shops given in the dataset\n\nx = train_df['shop_id'].nunique()\n","6c0e794f":"print(\"No. of Unique Shops are :\", x)","c6c60c85":"# making a word cloud for item categories name\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nplt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'lightblue',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(item_cat['item_category_name']))\n\n\nplt.title('Wordcloud for Item Category Names', fontsize = 30)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","57ad6454":"# making a word cloud for item name\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nplt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'pink',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(items_df['item_name']))\n\n\nplt.title('Wordcloud for Item Names', fontsize = 30)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","4fa59428":"# making a word cloud for shop name\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nplt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'gray',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(shop_df['shop_name']))\n\n\nplt.title('Wordcloud for Shop Names', fontsize = 30)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","fbb29435":"# making a new column day\ntrain_df['date'] = pd.to_datetime(train_df['date'], errors='coerce')","b47c98d6":"# making a new column month\ntrain_df['month'] = pd.to_datetime(train_df['date'], errors='coerce')\n\n# making a new column year\ntrain_df['year'] = pd.to_datetime(train_df['date'], errors='coerce')\n\n# making a new column week\ntrain_df['week'] =pd.to_datetime(train_df['date'], errors='coerce')\n\n# checking the new columns\ntrain_df.columns","7fcf9a16":"train_df.head()","65442313":"# checking which days are most busisiest for the shops\n\nplt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(train_df['date'])\nplt.title('The most busiest days for the shops', fontsize = 30)\nplt.xlabel('Days', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\n\nplt.show()","b62f2175":"# checking which months are most busisiest for the shops\n\nplt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(train_df['month'], palette = 'dark')\nplt.title('The most busiest months for the shops', fontsize = 30)\nplt.xlabel('Months', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\n\nplt.show()","8e2e1128":"# checking the columns of the train data\n\ntrain_df.columns","41d6551a":"train_df.dtypes","f1b8183e":"# converting the data into monthly sales data\n\n# making a dataset with only monthly sales data\ndata = train_df.groupby([train_df['date'].apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).sum().reset_index()\n\n# specifying the important attributes which we want to add to the data\ndata = data[['date','item_id','shop_id','item_cnt_day']]\n\n# at last we can select the specific attributes from the dataset which are important \ndata = data.pivot_table(index=['item_id','shop_id'], columns = 'date', values = 'item_cnt_day', fill_value = 0).reset_index()\n\n# looking at the newly prepared datset\ndata.shape","8111d8f7":"# let's merge the monthly sales data prepared to the test data set\n\ntest_df=pd.merge(test_df, data, on = ['item_id', 'shop_id'], how = 'left')\n\n# filling the empty values found in the dataset\ntest_df.fillna(0, inplace = True)\n\n# checking the dataset\ntest_df.head()","12d8b09d":"# now let's create the actual training data\n\nx_train = test_df.drop(['2015-10', 'item_id', 'shop_id'], axis = 1)\ny_train = test_df['2015-10']\n\n# deleting the first column so that it can predict the future sales data\nx_test = test_df.drop(['2013-01', 'item_id', 'shop_id'], axis = 1)\n\n# checking the shapes of the datasets\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_test :\", y_train.shape)","0b703c3c":"# let's check the x_train dataset\n\nx_train.head()","e4e957c7":"# let's check the x_test data\n\nx_test.head()","f2c5ddb2":"# splitting the data into train and valid dataset\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state = 0)\n\n# checking the shapes\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_valid :\", x_valid.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_valid :\", y_valid.shape)","6a349176":"# MODELING\n\nfrom lightgbm import LGBMRegressor\n\nmodel_lgb = LGBMRegressor( n_estimators=200,\n                           learning_rate=0.03,\n                           num_leaves=32,\n                           colsample_bytree=0.9497036,\n                           subsample=0.8715623,\n                           max_depth=8,\n                           reg_alpha=0.04,\n                           reg_lambda=0.073,\n                           min_split_gain=0.0222415,\n                           min_child_weight=40)\nmodel_lgb.fit(x_train, y_train)\n\ny_pred_lgb = model_lgb.predict(x_test)","d8b641fa":"# Get the test set predictions and clip values to the specified range\ny_pred_lgb = model_lgb.predict(x_test).clip(0., 20.)\n\n# Create the submission file and submit\npreds = pd.DataFrame(y_pred_lgb, columns=['item_cnt_month'])\npreds.to_csv('submission.csv',index_label='ID')","f76e931d":"## **Problem Statement**\nThe task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n"}}