{"cell_type":{"e6b32387":"code","b3c6db64":"code","16360796":"code","c29a7511":"code","6c158731":"code","8f71e04f":"code","6541f368":"code","dfd81703":"code","029879ca":"code","cde52371":"code","8aaec934":"code","42ddea31":"code","891ed2ea":"code","787aab93":"code","b04fe252":"code","aec1e778":"code","535e8b2e":"code","6ddad054":"code","d4f7dd03":"code","97146a5f":"code","22e2b221":"code","5e6d4621":"code","c2561359":"markdown","d80a1e22":"markdown","437061c6":"markdown","990a961b":"markdown"},"source":{"e6b32387":"# let us start with importing the dependencies for the analysis \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nimport re\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom bs4 import BeautifulSoup as bs\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","b3c6db64":"# We will now extract the reviews for a particular product from \"Amazon.in\",\n# say for \"Iphone 12\" (ideally selecting the product with more reviews)\n\n# creating an empty list\nipr = []\n\n'''defining a for loop to extract reviews from the amazon page for \"Iphone12\"\n    for 20 pages. Say one page has 10 reviews, we will be able to extract 200 reviews'''\nfor i in range(1,21):\n    ip = []\n    url = \"https:\/\/www.amazon.in\/Apple-iPhone-13-256GB-Product\/product-reviews\/B09G9HDN4Q\/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=\"+str(i)\n    response = requests.get(url) # getting the url and helping extract reviews from it\n    soup = bs(response.content, \"html.parser\") #help extract the url from html\n    reviews = soup.find_all(\"span\", attrs = {\"class\", \"a-size-base review-text review-text-content\"}) # helps extract the particular data from the mentioned tags in the html code\n    for i in range(len(reviews)):\n        ip.append(reviews[i].text)\n    # this helps extract the reviews to go page by page and extract them by appending them\n    ipr = ipr + ip ","16360796":"# checking the extracted text\nipr","c29a7511":"txt = pd.DataFrame(ipr)\ntxt[\"text\"] = txt\ntxt","6c158731":"import re # importing Regular Experession \"re\"\nnltk.download('stopwords') # downloading the stopwords\nstemmer = nltk.SnowballStemmer(\"english\") # defining the stemmer for the text to be done\nfrom nltk.corpus import stopwords\nimport string\nstopword = set(stopwords.words(\"english\")) # selecting the english stopwords from the list of various languages present\n\n# defining the cleaning the text function for the extracted text\ndef clean(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = [word for word in text.split(' ') if word not in stopword]\n    text=\" \".join(text)\n    text = [stemmer.stem(word) for word in text.split(' ')]\n    text=\" \".join(text)\n    return text\n\n# Post running the defined function we invoke the same for the text extracted\ntxt[\"text\"] = txt[\"text\"].apply(clean)\n","8f71e04f":"# Joining the cleansed text data into a single paragraph\ntext = \" \".join(i for i in txt.text)\ntext","6541f368":"# using the Vader_lexicon library to run the sentiment analysis on the text extracted and cleansed\n\nnltk.download('vader_lexicon')\nsentiments = SentimentIntensityAnalyzer()\ntxt[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in txt[\"text\"]] # defining the positive polarity for the text\ntxt[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in txt[\"text\"]] # defining the nagetive polarity for the text\ntxt[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in txt[\"text\"]] # defining the neutral polarity for the text\ntxt = txt[[\"text\", \"Positive\", \"Negative\", \"Neutral\"]]\nprint(txt.head())","dfd81703":"x = sum(txt[\"Positive\"])\ny = sum(txt[\"Negative\"])\nz = sum(txt[\"Neutral\"])\n\ndef sentiment_score(a, b, c):\n    if (a>b) and (a>c):\n        print(\"Positive \ud83d\ude0a \")\n    elif (b>a) and (b>c):\n        print(\"Negative \ud83d\ude20 \")\n    else:\n        print(\"Neutral \ud83d\ude42 \")\nsentiment_score(x, y, z)","029879ca":"# printing the sentiment scores for the text\nprint(\"Positive: \", x)\nprint(\"Negative: \", y)\nprint(\"Neutral: \", z)","cde52371":"ip_rev_string = \" \".join(txt[\"text\"])","8aaec934":"ip_rev_string","42ddea31":"# Creating a wordcloud of the text extracted\nwordcloud_ip = WordCloud(background_color='White',\n                      width=1800,\n                      height=1400\n                     ).generate(ip_rev_string)\n\nplt.imshow(wordcloud_ip)","891ed2ea":"# defining the cleaning the text function for the extracted text\ndef clean(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub(\"[^A-Za-z\" \"]+\",\" \", text)\n    text = re.sub(\"[0-9\" \"]+\",\" \", text)\n    text = [word for word in text.split(' ') if word not in stopword]\n    text=\" \".join(text)\n    text = [stemmer.stem(word) for word in text.split(' ')]\n    text=\" \".join(text)\n    return text\n\n# Post running the defined function we invoke the same for the text extracted\ntxt[\"text\"] = txt[\"text\"].apply(clean)","787aab93":"ip_rev_string = \" \".join(ipr)\n\nimport nltk\n# from nltk.corpus import stopwords\n\n\n# Removing unwanted symbols incase if exists\nip_rev_string = re.sub(\"[^A-Za-z\" \"]+\",\" \", ip_rev_string).lower()\nip_rev_string = re.sub(\"[0-9\" \"]+\",\" \", ip_rev_string)\n\n# words that contained in iphone XR reviews\nip_reviews_words = ip_rev_string.split(\" \")\nip_reviews_words","b04fe252":"#TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ip_reviews_words, use_idf=True,ngram_range=(1, 3))\nX = vectorizer.fit_transform(ip_reviews_words)\n\nwith open(\"..\/input\/sentiment-analysis\/stop.txt\",\"r\") as sw:\n    stop_words = sw.read()\n    \nstop_words = stop_words.split(\"\\n\")\n\nstop_words.extend([\"mobile\",\"time\",\"android\",\"phone\",\"device\",\"screen\",\"battery\",\"product\",\"good\",\"day\",\"price\"])\n\nip_reviews_words = [w for w in ip_reviews_words if not w in stop_words]\n\n# Joinining all the reviews into single paragraph \nip_rev_string = \" \".join(ip_reviews_words)\nip_rev_string","aec1e778":"# Corpus level word cloud\n\nwordcloud_ip = WordCloud(\n                      background_color='White',\n                      width=1800,\n                      height=1400\n                     ).generate(ip_rev_string)\n\nplt.imshow(wordcloud_ip)","535e8b2e":"# positive words # Choose the path for +ve words stored in system\nwith open(\"..\/input\/sentiment-analysis\/positive-words.txt\",\"r\") as pos:\n  poswords = pos.read().split(\"\\n\")\n\n# Positive word cloud\n# Choosing the only words which are present in positive words\nip_pos_in_pos = \" \".join ([w for w in ip_reviews_words if w in poswords])\n\nwordcloud_pos_in_pos = WordCloud(\n                      background_color='White',\n                      width=1800,\n                      height=1400\n                     ).generate(ip_pos_in_pos)\nplt.figure(2)\nplt.imshow(wordcloud_pos_in_pos)","6ddad054":"with open(\"..\/input\/sentiment-analysis\/negative-words.txt\", \"r\") as neg:\n  negwords = neg.read().split(\"\\n\")\n\n# negative word cloud\n# Choosing the only words which are present in negwords\nip_neg_in_neg = \" \".join ([w for w in ip_reviews_words if w in negwords])\n\nwordcloud_neg_in_neg = WordCloud(\n                      background_color='black',\n                      width=1800,\n                      height=1400\n                     ).generate(ip_neg_in_neg)\nplt.figure(3)\nplt.imshow(wordcloud_neg_in_neg)","d4f7dd03":"# we first cleanse the data again in a new manner\nnltk.download('punkt')\n\nWNL = nltk.WordNetLemmatizer()\n\n# Lowercase and tokenize\ntext = ip_rev_string.lower()\n\n# Remove single quote early since it causes problems with the tokenizer.\ntext = text.replace(\"'\", \"\")\n\ntokens = nltk.word_tokenize(text)\ntext1 = nltk.Text(tokens)\n\n# Remove extra chars and remove stop words.\ntext_content = [''.join(re.split(\"[ .,;:!?\u2018\u2019``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text1]\n\n# Create a set of stopwords\nstopwords_wc = set(STOPWORDS)\ncustomised_words = ['price', 'great'] # If you want to remove any particular word form text which does not contribute much in meaning\n\nnew_stopwords = stopwords_wc.union(customised_words)\n\n# Remove stop words\ntext_content = [word for word in text_content if word not in new_stopwords]\n","97146a5f":"# Take only non-empty entries\ntext_content = [s for s in text_content if len(s) != 0]\n\n# Best to get the lemmas of each word to reduce the number of similar words\ntext_content = [WNL.lemmatize(t) for t in text_content]\n\nnltk_tokens = nltk.word_tokenize(text)  \nbigrams_list = list(nltk.bigrams(text_content))\nprint(bigrams_list)\n\ndictionary2 = [' '.join(tup) for tup in bigrams_list]\nprint (dictionary2)","22e2b221":"# Using count vectoriser to view the frequency of bigrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(ngram_range=(2, 2))\nbag_of_words = vectorizer.fit_transform(dictionary2)\nvectorizer.vocabulary_\n\nsum_words = bag_of_words.sum(axis=0)\nwords_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\nwords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\nprint(words_freq[:100])","5e6d4621":"# Generating wordcloud\nwords_dict = dict(words_freq)\nWC_height = 1000\nWC_width = 1500\nWC_max_words = 200\nwordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width, stopwords=new_stopwords)\nwordCloud.generate_from_frequencies(words_dict)\n\nplt.figure(4)\nplt.title('Most frequently occurring bigrams connected by same colour and font size')\nplt.imshow(wordCloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","c2561359":"**The text as is from the extraction is filled with unwanted texts, stop words, punctuation marks, etc. to clean these off\nwe need to define a function to help do the job for us**","d80a1e22":"**In our daily lives we view many products and review them if we like or dislike the same on vaious platforms like the Amazon, \nFlipkart and many more. These reviews extraction helps the platform to check the sentiments attached to the product and the \nratings help the brands to pitch in the product for various offers they can offer depending on the popularity**\n\n**In this notebook\/ code we will try to extract the reviews from Amazon and try to checke the sentiment analysis for the same \nand frequency distribution and other NLP techniques for the analysis.**","437061c6":"**We notice that the words after processing are a bit senseless as they are not stemmed properly. We can also use the extracted text in an alternate way. To help understand let's tryout**","990a961b":"**We can also try and see bigrams and the wordclouds using them, additionally check the bigram frequency as well**"}}