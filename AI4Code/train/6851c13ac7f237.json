{"cell_type":{"8f1eeb18":"code","c662042a":"code","aa87683f":"code","2e5a5a21":"code","2a887caa":"code","4985e55f":"code","33ef9da6":"code","e0fe479b":"code","7fdbe645":"code","02892c7b":"code","e6cd2acd":"code","4d8b4626":"code","eaf53a82":"code","e7332205":"code","b10ec940":"code","49b0dd4e":"code","174f2f36":"code","74cf5ebf":"code","14d46473":"code","968241f2":"code","9f581a82":"code","b8a19433":"code","c874c5f9":"code","ce834059":"code","daa32aa4":"code","31fda028":"code","343e988a":"code","a50fbc2d":"code","fc747351":"code","5af5cd5b":"code","4c634612":"code","bf726703":"code","2498c20f":"code","5cd57b3d":"code","ca75a8fb":"code","7359d83d":"code","fe02640e":"code","8d439d8a":"code","4ea71ce8":"code","39a80957":"code","f09e0213":"code","02d99c4f":"code","f76dbb25":"code","9c8f62dd":"code","41e618fc":"code","73686911":"code","c6b0bce1":"code","d0e1ec19":"code","96da8cee":"code","155c137c":"code","97b268f9":"code","8bbe634d":"code","3ef8bdb7":"code","4e654e36":"code","9916d793":"code","015bda59":"code","5bb31488":"code","79ed4d97":"code","db57be81":"code","f93d89f5":"code","87e98291":"code","32367b2a":"code","4498a419":"code","525eaebf":"code","1d179693":"code","a30d53bc":"code","5bc1a1d4":"code","ecc4f56c":"code","248885c3":"code","5de9d728":"code","685e1ded":"code","f0a4257e":"code","d0f483ae":"code","e4590506":"code","779e97d1":"code","91e630a6":"code","b345d5a1":"code","0d8a01fc":"code","81d5d9b6":"code","fd6a9850":"code","d40cca09":"code","53a3717e":"code","25422eea":"code","3dd1ecbd":"code","7889b171":"code","243c1459":"code","58ff7b3d":"code","b8097f3a":"code","d16f1e5d":"code","7da30277":"code","e4fe43c6":"code","783028f5":"code","649202b7":"markdown","6fe38a84":"markdown","b6357902":"markdown","2fc38f2d":"markdown","533809d3":"markdown","452d8937":"markdown","e60312a7":"markdown","68283f62":"markdown","df3162d2":"markdown","27531a35":"markdown","f863210c":"markdown","b6edb046":"markdown","e39f6efe":"markdown","14c985ce":"markdown","7eb51343":"markdown","a6418043":"markdown","0786d9a2":"markdown","12e49e94":"markdown","b6cb8867":"markdown","9c34272a":"markdown","f85b868c":"markdown","d8322872":"markdown","9574fe85":"markdown","eb43cfd6":"markdown","131deabd":"markdown","0f304ef7":"markdown","adda23e9":"markdown","ff44060a":"markdown","2a59f600":"markdown","9d9bb73a":"markdown","0881e2af":"markdown","3ab399db":"markdown","91f2603e":"markdown","f7b9e338":"markdown","404cd3fc":"markdown","cb0cca65":"markdown","5450368a":"markdown","21b8534d":"markdown","7cb2f34c":"markdown","862ff124":"markdown","01e9b5e6":"markdown","7fffb641":"markdown","0fd3da12":"markdown","e5021afd":"markdown","479a2931":"markdown","3189ea10":"markdown","7ccccc0e":"markdown","4e3dbc29":"markdown","718d1a40":"markdown","df3c4981":"markdown"},"source":{"8f1eeb18":"!pip install scorecardpy","c662042a":"# Machine learning preliminaries\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport math\nimport tensorflow as tf\nimport sklearn as sk\nfrom sklearn.datasets import load_digits\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.naive_bayes import BernoulliNB, CategoricalNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.svm import SVC, NuSVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.estimator import DNNClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import make_scorer\nfrom xgboost import XGBClassifier\nimport scorecardpy as sc\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\n# To plot pretty figures\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)","aa87683f":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(df.head())","2e5a5a21":"sns.heatmap(df.corr(), annot = True, fmt = \".2f\")","2a887caa":"df.describe()","4985e55f":"df.Sex.value_counts()","33ef9da6":"df.Embarked.value_counts()","e0fe479b":"df.info()","7fdbe645":"df.Embarked.fillna('S', inplace=True)","02892c7b":"df['Title'] = df.Name.apply(lambda x: re.search('\\w+\\.', x).group()).str.replace('.', '')\nprint(df.Title.unique())\npd.value_counts(df.Title)","e6cd2acd":"Title_Dict = {}\nTitle_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 0))\nTitle_Dict.update(dict.fromkeys(['Sir', 'Countess', 'Don', 'Lady'], 1))\nTitle_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 2))\nTitle_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 3))\nTitle_Dict.update(dict.fromkeys(['Mr'], 4))\nTitle_Dict.update(dict.fromkeys(['Master','Jonkheer'], 5))\n\ndf['Title'] = df['Title'].map(Title_Dict)\nsns.barplot(x = \"Title\", y = \"Survived\", data = df)","4d8b4626":"def sex_label(sex):\n    if (sex == 'female'):\n        return 0\n    elif (sex == 'male'):\n        return 1\n\ndf['Sex'] = df['Sex'].apply(sex_label)","eaf53a82":"sns.barplot(x = \"SibSp\", y = \"Survived\", data = df)","e7332205":"sns.barplot(x = \"Parch\", y = \"Survived\", data = df)","b10ec940":"df['Family'] = df['SibSp'] + df['Parch']","49b0dd4e":"sns.barplot(x = \"Family\", y = \"Survived\", data = df)","174f2f36":"Ticket_Count = dict(df['Ticket'].value_counts())\ndf['TicketGroup'] = df['Ticket'].apply(lambda x: Ticket_Count[x])\nprint(pd.value_counts(df.TicketGroup))\nsns.barplot(x = \"TicketGroup\", y = \"Survived\", data = df)","74cf5ebf":"df['Cabin'] = df['Cabin'].fillna('Unknown')\ndf['Cabin'] = df['Cabin'].str.get(0)\nsns.barplot(x = \"Cabin\", y = \"Survived\", data = df)","14d46473":"def embark_label(em):\n    if (em == 'S'):\n        return 0\n    elif (em == 'C'):\n        return 1\n    elif (em == 'Q'):\n        return 2\n\ndf['Embarked'] = df['Embarked'].apply(embark_label)","968241f2":"df['Surname'] = df['Name'].apply(lambda x: x.split(',')[0].strip())\nSurname_Count = dict(df['Surname'].value_counts())\ndf['FamilyMembers'] = df['Surname'].apply(lambda x: Surname_Count[x])\n\nFemale_Child_Group = df.loc[(df['FamilyMembers'] >= 2) & ((df['Age'] <= 12) | (df['Sex'] == 0))]\nMale_NotChild_Group = df.loc[(df['FamilyMembers'] >= 2) & (df['Age'] > 12) & (df['Sex'] == 1)]","9f581a82":"Female_Child = pd.DataFrame(round(Female_Child_Group.groupby('Surname')['Survived'].mean(), 4).value_counts())\nFemale_Child.columns = ['GroupCount']\nprint(Female_Child)\nsns.barplot(x = Female_Child.index, y = Female_Child[\"GroupCount\"]).set_xlabel('AverageSurvived')","b8a19433":"Male_NotChild = pd.DataFrame(round(Male_NotChild_Group.groupby('Surname')['Survived'].mean(), 4).value_counts())\nMale_NotChild.columns = ['GroupCount']\nprint(Male_NotChild)\nsns.barplot(x = Male_NotChild.index, y = Male_NotChild[\"GroupCount\"]).set_xlabel('AverageSurvived')","c874c5f9":"# Find out the families with special passengers.\n\nFemale_Child_Group = Female_Child_Group.groupby('Surname')['Survived'].mean()\nDead_List = set(Female_Child_Group[Female_Child_Group.apply(lambda x: x == 0)].index)\nprint(Dead_List)\nMale_NotChild_List = Male_NotChild_Group.groupby('Surname')['Survived'].mean()\nSurvived_List = set(Male_NotChild_List[Male_NotChild_List.apply(lambda x: x == 1)].index)\nprint(Survived_List)","ce834059":"df.loc[(df['Surname'].apply(lambda x: x in Dead_List)) & ((df['Age'] <= 12) | (df['Sex'] == 0)), 'Sex'] = 1\ndf.loc[(df['Age'].apply(lambda x: x in Dead_List)) & ((df['Age'] <= 12) | (df['Sex'] == 0)), 'Sex'] = 30\ndf.loc[(df['Surname'].apply(lambda x: x in Survived_List)) & ((df['Age'] > 12) & (df['Sex'] == 1)), 'Sex'] = 0\ndf.loc[(df['Age'].apply(lambda x: x in Survived_List)) & ((df['Age'] > 12) & (df['Sex'] == 1)), 'Sex'] = 5","daa32aa4":"subdf = df.drop([\"Age\", \"PassengerId\", \"Name\", \"Ticket\", \"Surname\",\n                 \"Parch\", \"SibSp\", \"Fare\", \"Family\", \"TicketGroup\", \"FamilyMembers\"], axis = 'columns')","31fda028":"subdf = subdf.astype('string')\nsubdf = pd.concat([subdf, df.Fare, df.Family, df.TicketGroup, df.Age], axis = 1)","343e988a":"bins = sc.woebin(subdf, y = 'Survived', \n                 min_perc_fine_bin = 0.05, # How many bins to cut initially into\n                 min_perc_coarse_bin = 0.05,  # Minimum percentage per final bin\n                 stop_limit = 0.1, # Minimum information value \n                 max_num_bin = 8, # Maximum number of bins\n                 method = 'tree')","a50fbc2d":"sc.woebin_plot(bins)","fc747351":"def title_cabin(cabin):\n    if (cabin == 'T') | (cabin == 'U') | (cabin == 'A') | (cabin == 'G'):\n        return 0\n    elif (cabin == 'C') | (cabin == 'F'):\n        return 1\n    elif (cabin == 'B') | (cabin == 'E') | (cabin == 'D'):\n        return 2\n\nsubdf['Cabin'] = subdf['Cabin'].apply(title_cabin)","5af5cd5b":"def family_label(num):\n    if (num == 0):\n        return 0\n    elif (num == 1):\n        return 1\n    elif (num == 2) | (num == 3):\n        return 2\n    elif (num >= 4):\n        return 3\n\nsubdf['Family'] = subdf['Family'].apply(family_label)","4c634612":"def age_label(age):\n    if (age < 10):\n        return 0\n    elif (age >= 10) & (age < 30):\n        return 1\n    elif (age >= 30):\n        return 2\n    \nsubdf['Age'] = subdf['Age'].apply(age_label)","bf726703":"def fare_label(fare):\n    if (fare < 10):\n        return 0\n    elif (fare >= 10) & (fare < 50):\n        return 1\n    elif (fare >= 50) & (fare < 75):\n        return 2\n    elif (fare >= 75):\n        return 3\n\nsubdf['Fare'] = subdf['Fare'].apply(fare_label)","2498c20f":"def ticket_label(num):\n    if (num < 2):\n        return 0\n    elif (num == 2):\n        return 1\n    elif (num == 3) | (num == 4):\n        return 2\n    elif (num >= 5):\n        return 3\n\nsubdf['TicketGroup'] = subdf['TicketGroup'].apply(ticket_label)","5cd57b3d":"def title_label(title):\n    if (title == '0') | (title == '4'):\n        return 0\n    elif (title == '3') | (title == '5'):\n        return 1\n    elif (title == '1') | (title == '2'):\n        return 2\n\nsubdf['Title'] = subdf['Title'].apply(title_label)","ca75a8fb":"tem_age = subdf.Age\nsubdf = subdf.drop(\"Age\", axis = 1)\ndata = subdf.astype(int)\ndata = pd.concat([data, tem_age], axis = 1)","7359d83d":"data.hist(bins = 25, figsize = (10, 10))\n#save_fig(\"\/kaggle\/working\/attribute_histogram_plots\")\nplt.show()","fe02640e":"X = data.drop('Survived', axis = 1)\n(X.Age).head(10)\nnan_age_index = np.where(pd.isna(X.Age))[0]","8d439d8a":"X.index = np.arange(len(X))\nsubX = X.drop(nan_age_index.tolist(), axis = \"rows\")\nsubX.index = np.arange(len(subX))\nsubX_X = subX.drop('Age', axis = 'columns')\nsubX_y = subX.Age\nsubXtrain, subXtest, subytrain, subytest = train_test_split(subX_X, subX_y, test_size = 0.3, random_state = 0)","4ea71ce8":"X = data.drop('Survived', axis = 1)\nnan_age_index = np.where(pd.isna(X.Age))[0]\nX.index = np.arange(len(X))\nsubX = X.drop(nan_age_index.tolist(), axis = \"rows\")\nsubX.index = np.arange(len(subX))\nsubX_X = subX.drop('Age', axis = 'columns')\nsubX_y = subX.Age\nsubXtrain, subXtest, subytrain, subytest = train_test_split(subX_X, subX_y, test_size = 0.3, random_state = 0)","39a80957":"kf = KFold(n_splits = 10, shuffle = False)\n\nalpha = np.linspace(0.1, 10, 20)\nCat_accuracy_list = np.zeros(20)\nfor i in range(len(Cat_accuracy_list)):\n    cv_scores = cross_val_score(CategoricalNB(alpha = alpha[i]), subXtrain, subytrain, \n                                cv = kf, scoring = make_scorer(accuracy_score))\n    Cat_accuracy_list[i] = cv_scores.mean()\nalpha_best = np.argmax(Cat_accuracy_list)\n\nCat_Categorical = CategoricalNB(alpha = alpha_best)\nCat_Categorical_fit = Cat_Categorical.fit(subXtrain, subytrain)\nage_pred_Cat = Cat_Categorical.predict(subXtest)\nprint(f\"The accuracy of Categorical NB Classifier for age groups is: {accuracy_score(subytest, age_pred_Cat)}\")","f09e0213":"DT_age = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, \n                                min_samples_split = 10, min_samples_leaf = 15, random_state = 0)\n\nparam_grid = dict({'max_depth': [2, 3, 4, 5, 6],\n                   'min_samples_split': [5, 10, 15, 20],\n                   'min_samples_leaf': [5, 10, 15, 20]\n                  })\n\nGridXGB = GridSearchCV(DT_age, param_grid, cv = 10, scoring = 'accuracy', refit = False)\nGridXGB.fit(subXtrain, subytrain)\nprint(GridXGB.best_params_)\n\nDT_age = DecisionTreeClassifier(criterion = 'entropy',\n                                max_depth = GridXGB.best_params_.get('max_depth'), \n                                min_samples_split = GridXGB.best_params_.get('min_samples_split'),\n                                min_samples_leaf = GridXGB.best_params_.get('min_samples_leaf'),\n                                random_state = 0)\nDT_fit = DT_age.fit(subXtrain, subytrain)\nage_pred_DT = DT_fit.predict(subXtest)\nprint(f\"The accuracy of Decision Tree for age groups is: {accuracy_score(subytest, age_pred_DT)}\")","02d99c4f":"random_forest = RandomForestClassifier(criterion = 'entropy', max_depth = 5, min_samples_split = 40, \n                                       min_samples_leaf = 15, max_features = 8, random_state = 0)\n\nparam_grid = dict({'max_depth': [3, 4, 5],\n                   'min_samples_split': [20, 30, 40],\n                   'min_samples_leaf': [10, 15, 20],\n                  })\n\nGridXGB = GridSearchCV(random_forest, param_grid, cv = 10, scoring = 'accuracy', refit = False)\nGridXGB.fit(subXtrain, subytrain)\nprint(GridXGB.best_params_)\n\nrandom_forest = RandomForestClassifier(criterion = 'entropy',\n                                max_depth = GridXGB.best_params_.get('max_depth'), \n                                min_samples_split = GridXGB.best_params_.get('min_samples_split'),\n                                min_samples_leaf = GridXGB.best_params_.get('min_samples_leaf'),\n                                max_features = 8,\n                                random_state = 0)\nRF_fit = random_forest.fit(subXtrain, subytrain)\nage_pred_RF = RF_fit.predict(subXtest)\nprint(f\"The accuracy of Random Forest for age groups is: {accuracy_score(subytest, age_pred_RF)}\")","f76dbb25":"n_estimators = np.linspace(1, 101, 10)\nn_estimators = n_estimators.astype(int)\nlearning_rate = np.linspace(0.05, 1, 10)\nAB_accuracy_list = np.zeros(len(n_estimators) * len(learning_rate))\nidx = 0\n\nfor i in n_estimators:\n    for j in learning_rate:\n        cv_scores = cross_val_score(AdaBoostClassifier(n_estimators = i, learning_rate = j, algorithm = 'SAMME.R', random_state = 0), \n                                    subXtrain, subytrain, cv = kf, scoring = make_scorer(accuracy_score))\n        AB_accuracy_list[idx] = cv_scores.mean()\n        idx = idx + 1\nn_estimators_best = n_estimators[math.floor(np.argmax(AB_accuracy_list) \/ 10)]\nlearning_rate_best = learning_rate[np.argmax(AB_accuracy_list) % 10 - 1]\n\nadaboost = AdaBoostClassifier(n_estimators = n_estimators_best, learning_rate = learning_rate_best, algorithm = 'SAMME.R', random_state = 0)\nAB_fit = adaboost.fit(subXtrain, subytrain)\nage_pred_AB = AB_fit.predict(subXtest)\nprint(f\"The accuracy of Adaboost for age groups is: {accuracy_score(subytest, age_pred_AB)}\")","9c8f62dd":"kf = KFold(n_splits = 10, shuffle = False)\n\nn_neighbors = np.linspace(1, 49, 25)\nn_neighbors = n_neighbors.astype(int)\nKNN_accuracy_list = np.zeros(25)\nfor i in range(len(KNN_accuracy_list)):\n    cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors = n_neighbors[i]), subXtrain, subytrain, \n                                cv = kf, scoring = make_scorer(accuracy_score))\n    KNN_accuracy_list[i] = cv_scores.mean()\nn_best = n_neighbors[np.argmax(KNN_accuracy_list)]\n\nKNN = KNeighborsClassifier(n_neighbors = n_best)\nKNN_fit = KNN.fit(subXtrain, subytrain)\nage_pred_KNN = KNN_fit.predict(subXtest)    \nprint(f\"The accuracy of KNN for age groups is: {accuracy_score(subytest, age_pred_KNN)}\")","41e618fc":"perceptron = Perceptron(penalty = 'l1', alpha = 0.00001, random_state = 0) \nperceptron_fit = perceptron.fit(subXtrain, subytrain)\nage_pred_Perceptron = perceptron_fit.predict(subXtest)\nprint(f\"The accuracy of Perceptron for age groups is: {accuracy_score(subytest, age_pred_Perceptron)}\")","73686911":"GBDT = GradientBoostingClassifier(learning_rate = 0.01, n_estimators = 100,\n                                  min_samples_split = 5, min_samples_leaf = 10,\n                                  max_depth = 5, random_state = 0)\n\nparam_grid = dict({'n_estimators': [100, 200, 300],\n                   'min_samples_split': [5, 10, 15],\n                   'min_samples_leaf': [5, 10, 15],\n                   'max_depth': [3, 4, 5]\n                  })\n\nGridXGB = GridSearchCV(GBDT, param_grid, cv = 5, scoring = 'accuracy', refit = False)\nGridXGB.fit(subXtrain, subytrain)\nprint(GridXGB.best_params_)\n\nGBDT = GradientBoostingClassifier(learning_rate = 0.01, \n                                  n_estimators = GridXGB.best_params_.get('n_estimators'),\n                                  min_samples_split = GridXGB.best_params_.get('min_samples_split'), \n                                  min_samples_leaf = GridXGB.best_params_.get('min_samples_leaf'),\n                                  max_depth = GridXGB.best_params_.get('max_depth'), \n                                  random_state = 0)\nGBDT_fit = GBDT.fit(subXtrain, subytrain)\nage_pred_GBDT = GBDT_fit.predict(subXtest)\nprint(f\"The accuracy of GBDT for age groups is: {accuracy_score(subytest, age_pred_GBDT)}\")","c6b0bce1":"nan_age_fit = DT_age.fit(subX.drop('Age', axis = 'columns'), subX.Age)\nnan_age_predictor = X.iloc[nan_age_index, :].drop('Age', axis = 'columns')\nnan_age_pred = nan_age_fit.predict(nan_age_predictor)\nfor i in range(len(nan_age_index)):\n    X.Age[nan_age_index[i]] = nan_age_pred[i]","d0e1ec19":"y = data.Survived\nX = X.astype('category')\ny = y.astype('category')\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.3, random_state = 0)","96da8cee":"NB_Bernoulli = BernoulliNB(alpha = 1, binarize = 1)\n\nparam_grid = dict({'alpha': [0.1, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5],\n                   'binarize': [0.1, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5]\n                  })\n\nGridXGB = GridSearchCV(NB_Bernoulli, param_grid, cv = 10, scoring = 'accuracy', refit = False)\nGridXGB.fit(Xtrain, ytrain)\nprint(GridXGB.best_params_)\n\nNB_Bernoulli = BernoulliNB(alpha = GridXGB.best_params_.get('alpha'), \n                           binarize = GridXGB.best_params_.get('binarize'))\nNB_Bernoulli_fit = NB_Bernoulli.fit(Xtrain, ytrain)\nsurvival_pred_Ber = NB_Bernoulli.predict(Xtest)\nprint(f\"The accuracy of Bernoulli NB Classifier is: {accuracy_score(ytest, survival_pred_Ber)}\")","155c137c":"DT_survival = DecisionTreeClassifier(criterion = 'entropy', splitter = 'random', max_depth = 5, min_samples_split = 20, \n                                     min_samples_leaf = 10, max_features = 9, random_state = 0, class_weight = 'balanced')\n\nparam_grid = dict({'max_depth': [2, 3, 4, 5, 6, 7],\n                   'min_samples_split': [5, 10, 15, 20],\n                   'min_samples_leaf': [5, 10, 15, 20]\n                  })\n\nGridXGB = GridSearchCV(DT_survival, param_grid, cv = 10, scoring = 'accuracy', refit = False)\nGridXGB.fit(Xtrain, ytrain)\nprint(GridXGB.best_params_)\n\nDT_survival = DecisionTreeClassifier(criterion = 'entropy',\n                                splitter = 'random',\n                                max_depth = GridXGB.best_params_.get('max_depth'), \n                                min_samples_split = GridXGB.best_params_.get('min_samples_split'),\n                                min_samples_leaf = GridXGB.best_params_.get('min_samples_leaf'),\n                                max_features = 9,\n                                random_state = 0,\n                                class_weight = 'balanced')\nDT_fit = DT_survival.fit(Xtrain, ytrain)\nsurvival_pred_DT = DT_fit.predict(Xtest)\nprint(f\"The accuracy of Decision Tree is: {accuracy_score(ytest, survival_pred_DT)}\")\n\n\nBag = BaggingClassifier(DT_survival, n_estimators = 10, random_state = 0)\nBag_fit = Bag.fit(Xtrain, ytrain)\nsurvival_pred_Bag = Bag_fit.predict(Xtest)\nprint(f\"The accuracy of Bagging Decision Tree is: {accuracy_score(ytest, survival_pred_Bag)}\")","97b268f9":"n_estimators = np.linspace(1, 101, 10)\nn_estimators = n_estimators.astype(int)\nlearning_rate = np.linspace(0.05, 1, 10)\nAB_accuracy_list = np.zeros(len(n_estimators) * len(learning_rate))\nidx = 0\n\nfor i in n_estimators:\n    for j in learning_rate:\n        cv_scores = cross_val_score(AdaBoostClassifier(n_estimators = i, learning_rate = j, algorithm = 'SAMME.R', random_state = 0), \n                                    Xtrain, ytrain, cv = kf, scoring = make_scorer(accuracy_score))\n        AB_accuracy_list[idx] = cv_scores.mean()\n        idx = idx + 1\nn_estimators_best = n_estimators[math.floor(np.argmax(AB_accuracy_list) \/ 10)]\nlearning_rate_best = learning_rate[np.argmax(AB_accuracy_list) % 10 - 1]\n\nadaboost = AdaBoostClassifier(n_estimators = n_estimators_best, learning_rate = learning_rate_best, algorithm = 'SAMME.R', random_state = 0)\nAB_fit = adaboost.fit(Xtrain, ytrain)\nsurvival_pred_AB = AB_fit.predict(Xtest)\nprint(f\"The accuracy of Adaboost is: {accuracy_score(ytest, survival_pred_AB)}\")","8bbe634d":"random_forest = RandomForestClassifier(criterion = 'entropy', max_depth = 5,\n                                min_samples_split = 20, min_samples_leaf = 15, \n                                max_features = 9, random_state = 0,\n                                n_estimators = 5, class_weight = 'balanced')\n\nparam_grid = dict({'max_depth': [3, 4, 5, 6],\n                   'min_samples_split': [5, 10, 15, 20, 30],\n                   'min_samples_leaf': [5, 10, 15, 20],\n                   'n_estimators': [5, 10, 15, 20]\n                  })\n\nGridXGB = GridSearchCV(random_forest, param_grid, cv = 10, scoring = 'accuracy', refit = False)\nGridXGB.fit(Xtrain, ytrain)\nprint(GridXGB.best_params_)\n\nrandom_forest = RandomForestClassifier(criterion = 'entropy',\n                                max_depth = GridXGB.best_params_.get('max_depth'), \n                                min_samples_split = GridXGB.best_params_.get('min_samples_split'),\n                                min_samples_leaf = GridXGB.best_params_.get('min_samples_leaf'),\n                                max_features = 9,\n                                random_state = 0,\n                                n_estimators = GridXGB.best_params_.get('n_estimators'),\n                                class_weight = 'balanced')\n\nRF_fit = random_forest.fit(Xtrain, ytrain)\nsurvival_pred_RF = RF_fit.predict(Xtest)\nprint(f\"The accuracy of Random Forest is: {accuracy_score(ytest, survival_pred_RF)}\")","3ef8bdb7":"kf = KFold(n_splits = 10, shuffle = False)\n\nn_neighbors = np.linspace(1, 20, 10)\nn_neighbors = n_neighbors.astype(int)\nKNN_accuracy_list = np.zeros(10)\nfor i in range(len(KNN_accuracy_list)):\n    cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors = n_neighbors[i]), Xtrain, ytrain, \n                                cv = kf, scoring = make_scorer(accuracy_score))\n    KNN_accuracy_list[i] = cv_scores.mean()\nn_best = n_neighbors[np.argmax(KNN_accuracy_list)]\n\nKNN = KNeighborsClassifier(n_neighbors = n_best)\nKNN_fit = KNN.fit(Xtrain, ytrain)\nsurvival_pred_KNN = KNN_fit.predict(Xtest)    \nprint(f\"The accuracy of KNN is: {accuracy_score(ytest, survival_pred_KNN)}\")\nprint(f\"The best number of neighbors for the model is: {n_best}\")","4e654e36":"perceptron = Perceptron(penalty = 'l1', alpha = 0.01, random_state = 0, class_weight = 'balanced')\nperceptron_fit = perceptron.fit(Xtrain, ytrain)\nsurvival_pred_Perceptron = perceptron_fit.predict(Xtest)\nprint(f\"The accuracy of Perceptron is: {accuracy_score(ytest, survival_pred_Perceptron)}\")","9916d793":"GBDT = GradientBoostingClassifier(learning_rate = 0.01, n_estimators = 100, \n                                  min_samples_split = 5, min_samples_leaf = 10,\n                                  max_depth = 5, random_state = 0)\n\nparam_grid = dict({'n_estimators': [50, 100, 150, 200],\n                   'min_samples_split': [5, 10, 15, 20],\n                   'min_samples_leaf': [5, 10, 15, 20],\n                   'max_depth': [3, 4, 5, 6]\n                  })\n\nGridXGB = GridSearchCV(GBDT, param_grid, cv = 5, scoring = 'accuracy', refit = False)\nGridXGB.fit(Xtrain, ytrain)\nprint(GridXGB.best_params_)\n\nGBDT = GradientBoostingClassifier(learning_rate = 0.01, \n                                  n_estimators = GridXGB.best_params_.get('n_estimators'),\n                                  min_samples_split = GridXGB.best_params_.get('min_samples_split'), \n                                  min_samples_leaf = GridXGB.best_params_.get('min_samples_leaf'),\n                                  max_depth = GridXGB.best_params_.get('max_depth'), \n                                  random_state = 0)\n\nGBDT_fit = GBDT.fit(Xtrain, ytrain)\nsurvival_pred_GBDT = GBDT_fit.predict(Xtest)\nprint(f\"The accuracy of GBDT is: {accuracy_score(ytest, survival_pred_GBDT)}\")","015bda59":"SVC_model = SVC(coef0 = 10, random_state = 0, class_weight = 'balanced')\n\ncoef_list = np.linspace(1, 20, 50)\nSVC_accuracy_list = np.zeros(20)\nfor i in range(len(SVC_accuracy_list)):\n    cv_scores = cross_val_score(SVC(coef0 = coef_list[i], random_state = 0, class_weight = 'balanced'), \n                                Xtrain, ytrain, cv = kf, scoring = make_scorer(accuracy_score))\n    SVC_accuracy_list[i] = cv_scores.mean()\ncoef_best = coef_list[np.argmax(SVC_accuracy_list)]\n\nSVC_model = SVC(coef0 = coef_best, random_state = 0, class_weight = 'balanced')\nSVC_fit = SVC_model.fit(Xtrain, ytrain)\nsurvival_pred_SVC = SVC_fit.predict(Xtest)\n\nprint(f\"The best coef value is: {coef_best}\")\nprint(f\"The accuracy of Support Vector Machine is: {accuracy_score(ytest, survival_pred_SVC)}\")","5bb31488":"Clist = np.linspace(0.1, 20, 30)\nlog_accuracy_list = np.zeros(20)\nfor i in range(len(log_accuracy_list)):\n    cv_scores = cross_val_score(LogisticRegression(C = Clist[i], random_state = 0), Xtrain, ytrain, \n                                cv = kf, scoring = make_scorer(accuracy_score))\n    log_accuracy_list[i] = cv_scores.mean()\n\nlogreg = LogisticRegression(C = Clist[np.argmax(log_accuracy_list)], random_state = 0, max_iter = 1000, class_weight = 'balanced')\nlogreg_fit = logreg.fit(Xtrain, ytrain)\nsurvival_pred_log = logreg_fit.predict(Xtest)\n\nprint(f\"The accuracy of Logistic Regression is: {accuracy_score(ytest, survival_pred_log)}\")","79ed4d97":"np.random.seed(0)\ntf.random.set_seed(0)\n\nXtrain_DNN = (Xtrain.values).astype(int)\nytrain_DNN = (ytrain.values).astype(int)\n\ndef input_fn(Xtrain, ytrain):\n    dataset = tf.data.Dataset.from_tensor_slices(({'Pclass': Xtrain[:,0], 'Sex': Xtrain[:,1],\n                                             'Cabin': Xtrain[:,2], 'Embarked': Xtrain[:,3],\n                                             'Title': Xtrain[:,4], 'Fare': Xtrain[:,5],\n                                             'Family': Xtrain[:,6], 'TicketGroup': Xtrain[:,7],\n                                             'Age': Xtrain[:,8]}, ytrain))\n    dataset = dataset.batch(10)\n    return dataset\n\nfeature_columns = [\n    tf.feature_column.numeric_column(key = 'Pclass', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'Sex', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'Cabin', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'Embarked', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'Title', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'Fare', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'Family', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'TicketGroup', dtype = tf.int64),\n    tf.feature_column.numeric_column(key = 'Age', dtype = tf.int64)\n]\n\nDNN_accuracy_list = []\n\nfor i in range(10):\n    DNN = DNNClassifier(n_classes = 2, feature_columns = feature_columns, hidden_units = [50, 50, 50, 50])\n    DNN_fit = DNN.train(input_fn = lambda: input_fn(Xtrain_DNN, ytrain_DNN), max_steps = 1000)\n    survival_pred_DNN = DNN_fit.predict(input_fn = lambda: input_fn(Xtest.values, ytest.values))\n    survival_pred_DNN = np.array([p['class_ids'][0] for p in survival_pred_DNN])\n    DNN_accuracy_list.append(accuracy_score(ytest, survival_pred_DNN))\n    \nprint(f\"The accuracy of DNN is: {np.mean(DNN_accuracy_list)}\")\nprint(f\"The list of all the accuracy is: {DNN_accuracy_list}\")","db57be81":"XGB = XGBClassifier(max_depth = 50,        \n                    learning_rate = 0.1,\n                    n_estimators = 2,\n                    objective = 'binary:logistic',\n                    booster = 'gbtree',             \n                    subsample = 1,                  \n                    random_state = 0,               \n                    )\n\nparam_grid = dict({'n_estimators': [50, 100, 150, 200], 'max_depth': [5, 10, 15, 20],\n                   'learning_rate': [0.0001, 0.001, 0.01, 0.1], 'subsample': [0.6, 0.7]})\n\nGridXGB = GridSearchCV(XGB,        \n                       param_grid,          \n                       cv = 3,              \n                       scoring = 'roc_auc', \n                       n_jobs = -1,         \n                       refit = False,       \n                       verbose = 1\n                      )\n\nGridXGB.fit(Xtrain, ytrain)\nprint(GridXGB.best_params_)\nXGB = XGBClassifier(max_depth = GridXGB.best_params_.get('max_depth'),\n                    learning_rate = GridXGB.best_params_.get('learning_rate'),\n                    n_estimators = GridXGB.best_params_.get('n_estimators'), \n                    objective = 'binary:logistic',\n                    booster ='gbtree',\n                    subsample = GridXGB.best_params_.get('subsample'),\n                    random_state = 0\n                    )\n\nXGB_fit = XGB.fit(Xtrain.astype(int), ytrain.astype(int))\nXGB_pred_survival = XGB_fit.predict(Xtest.astype(int))\nprint(f\"The accuracy of XGBoost is: {accuracy_score(ytest.astype(int), XGB_pred_survival)}\")","f93d89f5":"summary_accuracy = [accuracy_score(ytest, survival_pred_Ber), accuracy_score(ytest, survival_pred_Bag), accuracy_score(ytest, survival_pred_RF),\n                   accuracy_score(ytest, survival_pred_AB), accuracy_score(ytest, survival_pred_KNN), accuracy_score(ytest, survival_pred_Perceptron),\n                   accuracy_score(ytest, survival_pred_GBDT), accuracy_score(ytest, survival_pred_SVC), accuracy_score(ytest, survival_pred_log),\n                   np.mean(DNN_accuracy_list), accuracy_score(ytest.astype(int), XGB_pred_survival)]\nsummary = pd.DataFrame({\"SummaryAccuracy\": summary_accuracy,\n                       \"Algorithm\": [\"Naive Bayes\", \"Decision Tree\", \"Random Forest\", \"Adaboost\", \n                                    \"KNN\", \"Perceptron\", \"GBDT\", \"SVM\", \"Logistic Regression\", \"DNN\", \"XGBoost\"]})\n\ng = sns.barplot(\"SummaryAccuracy\", \"Algorithm\", data = summary, palette = \"Set3\", orient = \"h\")\ng.set_xlabel(\"Accuracy\")","87e98291":"models = pd.DataFrame({\n    'Model': ['Support Vector Machine', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Bernoulli Naive Bayes Classifier', 'Perceptron', \n              'DNN', 'GBDT', 'Decision Tree', 'Adaboost', 'XGBoost'],\n    'Score': [accuracy_score(ytest, survival_pred_SVC), accuracy_score(ytest, survival_pred_KNN), accuracy_score(ytest, survival_pred_log), \n              accuracy_score(ytest, survival_pred_RF), accuracy_score(ytest, survival_pred_Ber), accuracy_score(ytest, survival_pred_Perceptron), \n              np.mean(DNN_accuracy_list), accuracy_score(ytest, survival_pred_GBDT), accuracy_score(ytest, survival_pred_Bag), accuracy_score(ytest, survival_pred_AB),\n             accuracy_score(ytest.astype(int), XGB_pred_survival)]})\nmodels.sort_values(by = 'Score', ascending = False)","32367b2a":"class Ensemble(object):\n    \n    def __init__(self, estimators):\n        self.estimator_names = []\n        self.estimators = []\n        for i in estimators:\n            self.estimator_names.append(i[0])\n            self.estimators.append(i[1])\n    \n    def fit(self, train_x, train_y):\n        x = []      \n        for i in self.estimators:\n            i.fit(train_x, train_y)\n            x.append(i.predict(train_x)) \n        x = np.array(x).T\n        train_y = np.array(train_y)\n        return x, train_y\n    \n    def predict(self, x):\n        xlist = []\n        for i in self.estimators:\n            i.fit(Xtrain.astype(int), ytrain.astype(int))\n            xlist.append(i.predict(x))        \n        xlist = np.array(xlist).T\n        return xlist","4498a419":"bag = Ensemble([('GBDT', GBDT), ('Logistic Regression', logreg), ('Random Forest', random_forest),\n                ('SVM', SVC_model), ('Decision Tree', Bag), ('Adaboost', adaboost), ('Perceptron', perceptron),\n                ('Naive Bayes Classifier', NB_Bernoulli), ('KNN', KNN), ('XGBoost', XGB)])\n\nxlabel_train, ylabel_train = bag.fit(Xtrain.astype(int), ytrain.astype(int))\nxlabel_test = bag.predict(Xtest.astype(int))\n\nGBDT_ens = GBDT\nGBDT_ensemble_fit = GBDT_ens.fit(xlabel_train, ylabel_train)\nGBDT_ensemble_pred = GBDT_ensemble_fit.predict(xlabel_test)\nensemble_accuracy = accuracy_score(ytest, GBDT_ensemble_pred)\nprint(f\"The accuracy of the ensembled model with GBDT is: {ensemble_accuracy}\")","525eaebf":"confusion_matrix_rf = confusion_matrix(y_true = ytest, \n                    y_pred = GBDT_ensemble_pred)\nconfusion_matrix_rf = confusion_matrix_rf.astype('float') \/ confusion_matrix_rf.sum(axis = 1)[:, np.newaxis]\ndf_cm = pd.DataFrame(\n        confusion_matrix_rf, index = ['Death', 'Survival'], columns = ['Death', 'Survival'], \n)\n\nfigsize = (8,6)\nfontsize = 14\nfig = plt.figure(figsize = figsize)\nheatmap = sns.heatmap(df_cm, annot = True, fmt = '.2f')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation = 0, \n                             ha = 'right', fontsize = fontsize)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation = 45,\n                             ha = 'right', fontsize = fontsize)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","1d179693":"df1 = pd.read_csv('..\/input\/titanic\/test.csv')\ndf1.describe()","a30d53bc":"df1.Sex.value_counts()","5bc1a1d4":"df1.Embarked.value_counts()","ecc4f56c":"df1.info()","248885c3":"df1['Cabin'] = df1['Cabin'].fillna('Unknown')\ndf1['Cabin'] = df1['Cabin'].str.get(0)","5de9d728":"def title_cabin(cabin):\n    if (cabin == 'T') | (cabin == 'U') | (cabin == 'A') | (cabin == 'G'):\n        return 0\n    elif (cabin == 'C') | (cabin == 'F'):\n        return 1\n    elif (cabin == 'B') | (cabin == 'E') | (cabin == 'D'):\n        return 2\n\ndf1['Cabin'] = df1['Cabin'].apply(title_cabin)","685e1ded":"df1['Title'] = df1.Name.apply(lambda x: re.search('\\w+\\.', x).group()).str.replace('.', '')\nprint(df1.Title.unique())\npd.value_counts(df1.Title)","f0a4257e":"Title_Dict = {}\nTitle_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 0))\nTitle_Dict.update(dict.fromkeys(['Sir', 'Countess', 'Dona', 'Lady'], 1))\nTitle_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 2))\nTitle_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 3))\nTitle_Dict.update(dict.fromkeys(['Mr'], 4))\nTitle_Dict.update(dict.fromkeys(['Master','Jonkheer'], 5))\n\ndf1['Title'] = df1['Title'].map(Title_Dict)","d0f483ae":"def sex_label(sex):\n    if (sex == 'female'):\n        return 0\n    elif (sex == 'male'):\n        return 1\n\ndf1['Sex'] = df1['Sex'].apply(sex_label)","e4590506":"df1['Family'] = df1['SibSp'] + df1['Parch']\ndef family_label(num):\n    if (num == 0):\n        return 0\n    elif (num == 1):\n        return 1\n    elif (num == 2) | (num == 3):\n        return 2\n    elif (num >= 4):\n        return 3\n\ndf1['Family'] = df1['Family'].apply(family_label)","779e97d1":"def age_label(age):\n    if (age < 10):\n        return 0\n    elif (age >= 10) & (age < 30):\n        return 1\n    elif (age >= 30):\n        return 2\n    \ndf1['Age'] = df1['Age'].apply(age_label)","91e630a6":"def fare_label(fare):\n    if (fare < 10):\n        return 0\n    elif (fare >= 10) & (fare < 50):\n        return 1\n    elif (fare >= 50) & (fare < 75):\n        return 2\n    elif (fare >= 75):\n        return 3\n\ndf1['Fare'] = df1['Fare'].apply(fare_label)","b345d5a1":"Ticket_Count = dict(df1['Ticket'].value_counts())\ndf1['TicketGroup'] = df1['Ticket'].apply(lambda x: Ticket_Count[x])\n\ndef ticket_label(num):\n    if (num < 2):\n        return 0\n    elif (num == 2):\n        return 1\n    elif (num == 3) | (num == 4):\n        return 2\n    elif (num >= 5):\n        return 3\n\ndf1['TicketGroup'] = df1['TicketGroup'].apply(ticket_label)","0d8a01fc":"def title_label(title):\n    if (title == 0) | (title == 4):\n        return 0\n    elif (title == 3) | (title == 5):\n        return 1\n    elif (title == 1) | (title == 2):\n        return 2\n\ndf1['Title'] = df1['Title'].apply(title_label)","81d5d9b6":"def embark_label(em):\n    if (em == 'S'):\n        return 0\n    elif (em == 'C'):\n        return 1\n    elif (em == 'Q'):\n        return 2\n\ndf1['Embarked'] = df1['Embarked'].apply(embark_label)","fd6a9850":"df1['Surname'] = df1['Name'].apply(lambda x: x.split(',')[0].strip())\n\ndf1.loc[(df1['Surname'].apply(lambda x: x in Dead_List)) & ((df1['Age'] <= 12) | (df1['Sex'] == 0)), 'Sex'] = 1\ndf1.loc[(df1['Surname'].apply(lambda x: x in Dead_List)) & ((df1['Age'] <= 12) | (df1['Sex'] == 0)), 'Age'] = 30\ndf1.loc[(df1['Surname'].apply(lambda x: x in Survived_List)) & ((df1['Age'] > 12) & (df1['Sex'] == 1)), 'Sex'] = 0\ndf1.loc[(df1['Surname'].apply(lambda x: x in Survived_List)) & ((df1['Age'] > 12) & (df1['Sex'] == 1)), 'Age'] = 5","d40cca09":"df1 = df1.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Surname'], axis = 1)","53a3717e":"df1.Fare.fillna(0, inplace=True)\ndf1.Fare = (df1.Fare).astype(int)\n(pd.DataFrame(df1.Age)).iloc[np.where(pd.isna(df1.Age) == False)[0], :] = ((pd.DataFrame(df1.Age)).iloc[np.where(pd.isna(df1.Age) == False)[0], :]).astype(int)","25422eea":"order = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Title', 'Fare', 'Family', 'TicketGroup', 'Age']\ndf1 = df1[order]","3dd1ecbd":"X_1 = df1\nnan_age_index = np.where(pd.isna(X_1.Age))[0]\nX_1.index = np.arange(len(X_1))\nsubX = X_1.drop(nan_age_index.tolist(), axis = \"rows\")\nsubX.index = np.arange(len(subX))\nsubX_X = subX.drop('Age', axis = 'columns')\nsubX_y = subX.Age\nsubXtrain, subXtest, subytrain, subytest = train_test_split(subX_X, subX_y, test_size = 0.3, random_state = 0)","7889b171":"DT_age = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, \n                                min_samples_split = 10, min_samples_leaf = 15, random_state = 0)\n\nparam_grid = dict({'max_depth': [2, 3, 4, 5, 6],\n                   'min_samples_split': [5, 10, 15, 20],\n                   'min_samples_leaf': [5, 10, 15, 20]\n                  })\n\nGridXGB = GridSearchCV(DT_age, param_grid, cv = 10, scoring = 'accuracy', refit = False)\nGridXGB.fit(subXtrain, subytrain)\nprint(GridXGB.best_params_)\n\nDT_age = DecisionTreeClassifier(criterion = 'entropy',\n                                max_depth = GridXGB.best_params_.get('max_depth'), \n                                min_samples_split = GridXGB.best_params_.get('min_samples_split'),\n                                min_samples_leaf = GridXGB.best_params_.get('min_samples_leaf'),\n                                random_state = 0)\nDT_fit = DT_age.fit(subXtrain, subytrain)\nage_pred_DT = DT_fit.predict(subXtest)\nprint(f\"The accuracy of Decision Tree for age groups is: {accuracy_score(subytest, age_pred_DT)}\")","243c1459":"nan_age_fit = DT_age.fit(subX.drop('Age', axis = 'columns'), subX.Age)\nnan_age_predictor = X_1.iloc[nan_age_index, :].drop('Age', axis = 'columns')\nnan_age_pred = nan_age_fit.predict(nan_age_predictor)\nfor i in range(len(nan_age_index)):\n    X_1.Age[nan_age_index[i]] = nan_age_pred[i]\nX_1 = X_1.astype(int)","58ff7b3d":"X_1 = X_1.astype('category')","b8097f3a":"class Ensemble(object):\n    \n    def __init__(self, estimators):\n        self.estimator_names = []\n        self.estimators = []\n        for i in estimators:\n            self.estimator_names.append(i[0])\n            self.estimators.append(i[1])\n    \n    def predict(self, x):\n        xlist = []\n        for i in self.estimators:\n            xlist.append(i.predict(x))        \n        xlist = np.array(xlist).T\n        return xlist","d16f1e5d":"GBDT_fit = GBDT.fit(Xtrain, ytrain)\n\nbag = Ensemble([('GBDT', GBDT_fit), ('Logistic Regression', logreg_fit), ('Random Forest', RF_fit),\n                ('SVM', SVC_fit), ('Decision Tree', Bag_fit), ('Adaboost', AB_fit), ('Perceptron', perceptron_fit),\n                ('Naive Bayes Classifier', NB_Bernoulli_fit), ('KNN', KNN_fit), ('XGBoost', XGB_fit)])\n\nxlabel_test_kaggle = bag.predict(X_1.astype(int))\n\nGBDTens = GBDT\nGBDT_ensemble_fit = GBDTens.fit(xlabel_train, ylabel_train)\nGBDT_ensemble_pred_kaggle = GBDT_ensemble_fit.predict(xlabel_test_kaggle)","7da30277":"from pandas import read_csv\n\nkaggle = read_csv(\"gender_submission.csv\")\nsur_label = pd.DataFrame(GBDT_ensemble_pred_kaggle)\nkaggle['Survived'] = sur_label.astype(int)\nkaggle.to_csv('gender_submission.csv', index = False)","e4fe43c6":"DT_fit_survival = DT_survival.fit(xlabel_train, ylabel_train)\nDT_pred_kaggle = DT_fit_survival.predict(xlabel_test_kaggle)","783028f5":"from pandas import read_csv\n\nkaggle = read_csv(\"gender_submission.csv\")\nsur_label = pd.DataFrame(DT_pred_kaggle)\nkaggle['Survived'] = sur_label.astype(int)\nkaggle.to_csv('gender_submission.csv', index = False)","649202b7":"As feature engineering here will be based on the relationship between each feature and the label (not included in the test set), it should be done first on the training set and applied on the test set. Feature \u201cPassengerId\u201d should be dropped as it has no specific meaning, only showing the order of the passengers in the data set. \u201cName\u201d is not useful if it is not processed and directly used for the prediction, but it contains passengers\u2019 titles, which are not as messy as the names and can be categorized more easily. Extract the titles and decide if it is necessary to group them.","6fe38a84":"It is shown that only \u201cFare\u201d, \u201cCabin\u201d, and \u201cAge\u201d have missing values. For \u201cFare\u201d, as the passenger with NaN fare had a lower-class ticket, he could be attributed to the first \u201cFare\u201d group, where fare is relatively low. For \u201cCabin\u201d and \u201cAge\u201d, follow the same steps again, and also clean and group other features as what has been done in the training set, and then construct models and make prediction.","b6357902":"Go back to the feature \u201cName\u201d. Feature \"Parch\" and \"SibSp\" have been combined to make a new feature \"Family\", but it only includes the very close relations of the passengers. Actually, some passengers may travel with their big family, such as grandparents\/grandsons, mother\/father-in-law, siblings' spouses, nephew\/niece, etc. It is difficult to exactly find the passengers in this case, but a relatively rough but simple way can be used to take such ca se into account: group passengers according to their surnames. A new feature \u201cSurname\u201d will be built. Another reason to build this feature: it is possible that people in a big family love each other so much and some may sacrifice their life to save others, and there might be some families who could not save themselves in a proper way and finally died together. It is known that females and children were more likely to survive, and men were more likely to die, so if the case mentioned above happened, it is possible that some passengers were not under a normal pattern and might mislead the final model. Therefore, such passengers can be extracted, and their genders and ages can be \"modified\" to make the model more accurate. \n\nTo find out special passengers mentioned above, firstly, children\/females with the same surname would be gathered together as a family (here children are those younger than 12 years old), and the survival rate of them in their families would be calculated. Tables and plots of group count are shown below.","2fc38f2d":"**KNN:** Do the same 10-fold cross validation among different number of neighbors and put the best one into the model. The accuracy is 84.33%, and the best number of neighbors is 11.","533809d3":"**Random Forest:** Do grid search to tune the maximum depth of the tree, minimum number of samples to be splitted, the minimum number of samples in a leaf node, and the number of trees in the forest. The given values for Grid search are: 3 to 6 for maximum depth, [5, 10, 15, 20, 30] for minimum number of samples to be splitted, and [5, 10, 15, 20] for minimum number of samples in a leaf node. The range for number of trees is [5, 10, 15, 20]. The criterion for splitting is the value of entropy, and the maximum number of features is 9, which means to take all the features into account. Since there are more \u201cDeath\u201d than \u201cSurvival\u201d, it is necessary to set the parameter \u201cclass_weight\u201d as \u201cbalanced\u201d. Do the same thing for other functions if there exists this parameter. The accuracy of prediction is 85.82%, and the best parameters are 3 for maximum depth, 5 for minimum samples in a leaf node, 20 for minimum number of samples to be splitted, and 10 for number of trees.","452d8937":"Next, feature \u201cEmbarked\u201d should be transformed. As it does not have any NaN value, and there are only three values in it, it is reasonable to convert them into 0, 1, and 2, making it easier to predict labels.","e60312a7":"In this project, the task is to figure out whether passengers on Titanic survived or not based on their features. To resolve this problem, the machine learning tasks include data structure analysis, data cleaning, feature analysis, feature engineering, model construction and evaluation, prediction, result evaluation, etc. This report is composed by two parts based on the above steps.","68283f62":"As shown in the table, there are many different titles, and most of them only correspond to fewer than 10 passengers, so it is necessary to group them by their meanings\/properties to make them less discrete and put the new categories into a new feature \u201cTitle\u201d. It is reasonable to put passengers with titles \u201cCapt\u201d, \u201cCol\u201d, \u201cMajor\u201d, \u201cDr\u201d, and \u201cRev\u201d into group 0 \u201cofficer\u201d, meaning the officers on Titanic; put passengers with titles \"Sir\", \"Countess\", \"Dona\", and \"Lady\" into group 1 \"royalty\", meaning they are related to royal class; put \u201cMme\u201d, \u201cMs\u201d, and \u201cMrs\u201d into group 2 \u201cMrs\u201d, and put \u201cMiss\u201d and \u201cMlle\u201d into group 3 \u201cMiss\u201d; put \u201cMr\u201d into group 4 \u201cMr\u201d; put the rest passengers into group 5 \u201cother\u201d, as it is difficult to categorize them. Then, draw a bar plot to show the relationship between the titles and the survival rate.","df3162d2":"After learning about the data set, load the training set, and draw a plot to show the correlations between each of the two variables. Only numerical variables are shown in the plot. There exists negative relationship between \u201cFare\u201d and \u201cPclass\u201d, as the fare is higher, the class should be higher (1 is the highest and 3 is the lowest); there exists negative relationship between \u201cAge\u201d and \u201cPclass\u201d, which might be due to the wealth cumulation and elder ones\u2019 demand for a more comfortable environment; there are positive relationship between \u201cSurvived\u201d and \u201cFare\u201d and negative relationship between \u201cSurvived\u201d and \u201cPclass\u201d, as higher fare and a higher ticket class indicate a higher socio-economic status of a passenger, who would be more likely to escape the shipwreck first.","27531a35":"We firstly import packages we need. We can also add to this list during coding and running.","f863210c":"**Decision Tree:** It is trained in the same way as the one in age imputation. The best parameters are 5 for maximum depth, 5 for minimum samples for splitting, and 10 for minimum samples in a leaf node. The number of features is set as 9, i.e. using all the features as predictors. Then put it into the Bagging model, where the number of trees is 10. The accuracy finally reaches 86.19%.","b6edb046":"Next, use different models to figure out passengers with unknown ages are in which age group (the groups are shown in the WOE plot above, including (-inf, 10), [10, 30), and [30, inf)). Divide the observations with known ages into training set (70%) and test set (30%). Models include Categorical Na\u00efve Bayes Classifier, Decision Tree, Random Forest, AdaBoost, KNN, Perceptron, and GBDT. All of them are evaluated by accuracy rate. Set the random seed as 0 for all the models.","e39f6efe":"This project provides two data sets, the training set and the test set. They include 891 and 418 observations respectively, and both of them have 11 features except the label (i.e. whether to survive or not). Briefly introduce the features:","14c985ce":"**Bernoulli Na\u00efve Bayes Classifier:** A wider range of possible values for alpha is given: [0.1, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5]. The same range will be given to parameter \u201cbinarize\u201d. Do Grid Search and find the best parameters for the model. Finally, the accuracy rate of prediction is 79.85%, and the best parameters are 0.1 for alpha, and 0.1 for binarize.","7eb51343":"Now there still exist some NaN values in the feature \u201cAge\u201d. Use the observations with known ages to train a predictive model and figure out the NaN values. Before imputing them, use the function *woebin* in package *scorecardpy* to calculate WOE and IV of each feature as well as bin them in a proper way. The results are shown in the plots below.","a6418043":"Next, use the training set to do data cleaning, feature analysis, and feature engineering. To do data cleaning, it is necessary to learn if there exist any NaN values in the training set. The result is shown below. There are 177 NaN in feature \u201cAge\u201d, 687 NaN in \u201cCabin\u201d, and 2 NaN in \u201cEmbarked\u201d.","0786d9a2":"In the bar plot, there exists difference in survival rate of passengers in different groups. This special distribution might be because passengers with the same ticket number sat together, and a moderate number of them means they could help each other to escape, while those with seven others having the same ticket number may get into chaos, and those with distinct ticket number could only save themselves. This feature will also be studied again in further feature engineering to find out a good way to be reorganized.","12e49e94":"**SVM:** Use 10-fold cross validation to find the best value of parameter coef. It will try 50 values between 1 and 20. Finally, the accuracy of SVM is 85.82%, and the best coef is 1.","b6cb8867":"It is also important to see if there exist any outliers in the data set. The table below shows the description of the data. The table only shows numeric variables. According to the table, each numerical variable has a normal range. Among the string variables, including \u201cName\u201d, \u201cCabin\u201d, \u201cTicket\u201d, \u201cSex\u201d, and \u201cEmbark\u201d, the first three ones are too messy and complicated, so it is hard to recognize any abnormal value. For \u201cSex\u201d and \u201cEmbark\u201d, since their values are in a given range, frequency tables can be used to find out any outlier. The two tables are also shown below. They show that all the values are normal, so for the training set, it is unnecessary to deal with outliers. ","9c34272a":"Draw a bar plot and a table to compare the accuracy of different models.","f85b868c":"**Decision Tree:** Do grid search to tune the maximum depth of the tree, minimum number of samples to be splitted, and the minimum number of samples in a leaf node. The given values for Grid search are: 2 to 6 for maximum depth, [5, 10, 15, 20] for minimum number of samples to be splitted, and [5, 10, 15, 20] for minimum number of samples in a leaf node. The criterion for splitting is the value of entropy. Its accuracy of prediction is 65.58%, and the best value for the three parameters are 4, 5, 10, respectively.","d8322872":"The two tables and plots show that there do exist some \u201cspecial\u201d passengers. Next modify their ages and genders. For \u201cspecial\u201d male passengers, their ages will be changed to 5, and their genders will be changed to 0 (i.e. female); for \u201cspecial\u201d female\/children passengers, their ages are changed to 30, and their genders are changed to 1 (i.e. male).","9574fe85":"**Random Forest:** Do grid search to tune the maximum depth of the tree, minimum number of samples to be splitted, and the minimum number of samples in a leaf node. The given values for Grid search are: 3 to 5 for maximum depth, [20, 30, 40] for minimum number of samples to be splitted, and [10, 15, 20] for minimum number of samples in a leaf node. The criterion for splitting is the value of entropy, and the maximum number of features is 8, which means to take all the features into account. The accuracy of prediction is 64%, and the best value for the three parameters are 3, 40, 10, respectively.","eb43cfd6":"**Logistic Regression:** tune the regularization parameter C in the function by cross validation. There are 30 possible values, based on which cross validation is done. The accuracy at the end is 84.70%.","131deabd":"**PassengerID:** passengers\u2019 ID. It has no specific meaning, and it is only for convenience in analysis.\\\n**Pclass:** Ticket class. It has three possible values, i.e. \u201c1\u201d, \u201c2\u201d, and \u201c3\u201d, meaning the upper class, the middle class, and the lower class respectively. This feature is a proxy for socio-economic status of passengers.\\\n**Name:** Passengers\u2019 names. It includes each passenger\u2019s name and title, such as Mr., Miss, Doctor, etc.\\\n**Sex:** Male or female.\\\n**Age:** Passengers\u2019 age in years. It is fractional if less than 1, and if it is estimated, it is in the form of \u201cxx.5\u201d.\\\n**SibSp:** The number of siblings and spouses on Titanic. Siblings include brother, sister, stepbrother, and stepsister. Spouses include husband and wife, but mistresses and fianc\u00e9s are not considered.\\\n**Parch:** The number of parents and children on Titanic. Parents mean mother and father, and children mean daughter, son, stepdaughter, and stepson. Grandparents and grandsons\/granddaughters are not included.\\\n**Ticket:** Ticket number.\\\n**Fare:** The fare of each passenger.\\\n**Cabin:** Cabin number.\\\n**Embark:** Port of embarkation. There are three possible ports, Cherbourg (\u201cC\u201d), Queenstown (\u201cQ\u201d), and Southampton (\u201cS\u201d).","0f304ef7":"Next process feature \u201cTicket\u201d. At first glance, this feature is messy too, but after careful observations, it can be found that some passengers had the same ticket number. From this perspective, the number of passengers who had the same ticket number can be the information in feature \u201cTicket\u201d. Think about taking it as a new feature \u201cTicketGroup\u201d. Form it and make a bar plot between it and the survival rate.","adda23e9":"It is shown that passengers in group 0 (i.e. officer) and 4 (i.e. Mr) were much more likely to die. It is reasonable, as it is known that in this shipwreck, women and children were asked to leave first, and officers and men were the last ones to escape, leaving opportunities for women and children to survive. The groups with higher survival rate may be grouped together in further feature engineering, so are those with lower survival rate, so that the organized feature could be a better indicator for survival.","ff44060a":"**AdaBoost:** Give the same range of parameters as that in age imputation for 10-fold cross validation and find the best ones for modelling. Finally, the accuracy of this classifier is 84.33%.","2a59f600":"**GBDT:** The learning rate is set as 0.01, and do Grid Search to find out the other parameters. The given number of trees include 100, 200, and 300, the given values for minimum samples for splitting include 5, 10, and 15, which are also the values for minimum samples in a leaf node. A value set of 3, 4, and 5 is given to maximum depth of each tree. The accuracy of GBDT is 63.26%, and the best parameters are: 3 for maximum depth of trees, 15 for minimum samples in a leaf node, 5 for minimum samples to be splitted, and 300 for the number of trees.","9d9bb73a":"Perceptron: The alpha is set as 0.01, and other parameters are not changed. The final result is 84.70%.","0881e2af":"**AdaBoost:** Do 10-fold cross validation to find the best number of trees and learning rate for the model. The range for searching the number of trees is 1 to 101 with uniform step size 10, and the range for learning rate is 0.05 to 1. The algorithm for the model is \u201cSAMME.R\u201d. The accuracy of prediction is 64%.","3ab399db":"Then construct a frame for ensemble learning and put all the models above into it. Finally, it gives an accuracy equal to 86.19%.","91f2603e":"**GBDT:** The learning rate is set as 0.01, and do Grid Search to find out the other parameters. The given number of trees include 50, 100, 150, and 200, the values for minimum samples for splitting include 5, 10, 15, and 20, which are also the values for minimum samples in a leaf node. A value set of 3, 4, 5, and 6 is given for maximum depth of each tree. The accuracy of GBDT is 85.82%. The best maximum depth of tree is 3, and the best minimum number of samples in a leaf node is 10, the minimum number of samples to be splitted should be 5, and the number of trees should be 200. ","f7b9e338":"## 1. Data Structure Analysis, Data Cleaning, Feature Analysis, and Feature Engineering","404cd3fc":"As mentioned before, feature \u201cParch\u201d and \u201cSibSp\u201d are replaced by \u201cFamily\u201d, and information in \u201cName\u201d is extracted to form a new feature \u201cTitle\u201d and modify the age & gender of some special observations. In the modification, feature \u201cSurname\u201d and \u201cFamilyMembers\u201d (indicating the number of passengers with the same surname) are created for analysis, and after modification they are no longer helpful, so they should be dropped. As feature \u201cTicketGroup\u201d contains the information of \u201cTicket\u201d, the latter one is also dropped. Now initial data cleaning and preprocessing are finished. Further feature analysis and engineering are based on WOE (weight of evidence) and IV (information value).","cb0cca65":"**DNN:** build a deep neural network by taking all the features into consideration. There are 4 hidden layers, each of which has the same number of nodes (50). The accuracy is 57.28%.","5450368a":"**KNN:** Do 10-fold cross validation among different number of neighbors (1 to 49 with uniform step wise 2) and put it into the model. The accuracy is 56%.","21b8534d":"**Categorical Na\u00efve Bayes Classifier:** Here tune the alpha in Categorical Na\u00efve Bayes Classifier. Give 10 alpha values with the uniform step size in a range from 0 to 20 and do 10-fold cross validation to figure out the best model. After training and testing it on the test set, get its accuracy of prediction 63.26%.","7cb2f34c":"**Perceptron:** Use L1 penalty and default alpha value to build the model and make prediction. The accuracy is 64%.","862ff124":"It is shown that there is no outlier in the test set. Next deal with the missing values in the test set.","01e9b5e6":"## 2. Model construction and evaluation, prediction, and result evaluation","7fffb641":"For feature \u201cCabin\u201d, it is reasonable to group them based on the first letter of the cabin number, as it may indicate the categories of cabins. Thus, all the values except NaN are replaced with their first letters. Most values in this feature are unknown. If the unknown ones are imputed with \u201cU\u201d and a bar plot is made, a relationship can be found.","0fd3da12":"Since the number of unknown embarkation ports is small, it is reasonable to use a simple way to do imputation. Here, use the mode to fill in them. As there are 644 \u201cS\u201d, 168 \u201cC\u201d, and 77 \u201cQ\u201d in this feature, impute the NaN values with \u201cS\u201d. Imputation for the rest two features will be done after feature analysis and engineering, as imputation for \u201cCabin\u201d is related to feature analysis, and imputation for \u201cAge\u201d will be done via prediction where engineered features are used as predictors.","e5021afd":"After comparison, the best model for age imputation is Decision Tree, so use it to fill in the unknown age values, and then begin to train models for label prediction. Divide the observations into training set (70%) and test set (30%). Here we are dividing the training data from Kaggle, so actually the training data in the training data is used for model fitting, but not the whole training set from Kaggle. Here models include Bernoulli Na\u00efve Bayes Classifier, Decision Tree, Random Forest, AdaBoost, KNN, Perceptron, GBDT, Support Vector Machine, Logistic Regression, DNN, and XGBoost. We will finally do ensemble learning and put all the models together to give a prediction. All of them are evaluated by accuracy rate. Set the random seed as 0 for all the models. Briefly discuss each of them. ","479a2931":"Draw the confusion matrix based on the ensembled model. From the plot below, this model does good in death recognition, but performs a little worser on survival observations. This should be attributed to the imbalanced distribution of labels.","3189ea10":"For feature \u201cSibSp\u201d and \u201cParch\u201d, draw bar plots to show the relationship between them and survival rate. From the first plot, passengers with moderate number of siblings or spouses on Titanic had higher survival rates. Also, passengers with moderate number of children or parents on Titanic had higher survival rates. Therefore, it can be helpful to add up \u201cSibSp\u201d and \u201cParch\u201d and create a new feature \u201cFamily\u201d to replace them, which will indicate survival better. This can also reduce the dimension. The new feature is meaningful: siblings, parents, spouses, and children are all immediate members in the family. Also draw a bar plot of the new feature. It is shown that passengers with different number of immediate family members do have more distinct survival rates. In the further feature engineering, \u201cFamily\u201d will be organized again to be not that discrete.","7ccccc0e":"The next step is to preprocess test data. Also, find out if there exist any outlier.","4e3dbc29":"The \u201cName\u201d feature will be used again later, which will be related to \u201cAge\u201d. Then the feature \u201cSex\u201d needs to be converted into 0 for female and 1 for male, as it is difficult to analyze strings directly.","718d1a40":"It is shown that passengers whose cabins are unknown are much more likely to die. This might be because many of these passengers died in the shipwreck, so their information about cabins could not be gathered. As it looks hard to group this feature manually, feature engineering would be done later.","df3c4981":"**XGBoost:** The parameter \u201cobjective\u201d and \u201cbooster\u201d of the model is set as \u201cbinary:logistic\u201d and \u201cgbtree\u201d respectively. The number of trees, maximum depth of trees, the learning rate, and the subsample size are learned by Grid Search. The finally accuracy is 86.19%."}}