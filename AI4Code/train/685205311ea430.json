{"cell_type":{"ae58870c":"code","b26c1ff3":"code","b0bd329b":"code","f0f4ddbd":"code","c866196d":"code","66452197":"code","8b6de376":"code","27a0537c":"code","97d3a89c":"code","2035c57a":"code","c6d74560":"code","01a0ab47":"code","d8056035":"code","472b4fda":"code","137c2dd5":"code","6d02d7c9":"code","2b8801b1":"code","008c1811":"code","7a4a872d":"code","cf3351f4":"code","2e37e66e":"code","98fceb7e":"code","e98094bf":"code","71554a57":"code","78908aee":"code","1068489c":"code","9dbb0c64":"code","824158d8":"markdown","f48b7646":"markdown","9cedd168":"markdown","d9a5b138":"markdown","dc9d3465":"markdown"},"source":{"ae58870c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b26c1ff3":"import pandas as pd\nimport numpy as np","b0bd329b":"df=pd.read_csv(\"\/kaggle\/input\/mushroom-csv-file\/mushrooms.csv\")","f0f4ddbd":"df.head()","c866196d":"df.shape","66452197":"from sklearn.preprocessing import LabelEncoder","8b6de376":"le=LabelEncoder()","27a0537c":"ds=df.apply(le.fit_transform)","97d3a89c":"LabelEncoder?\ndf.apply?","2035c57a":"ds.head()","c6d74560":"from sklearn.model_selection import train_test_split","01a0ab47":"data=ds.values\nprint(type(data))","d8056035":"data_y=data[:,0]\ndata_x=data[:,1:]","472b4fda":"x_train,x_test,y_train,y_test=train_test_split(data_x,data_y,test_size=0.2)","137c2dd5":"print(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","6d02d7c9":"# check the values or types of mushrooms avialable \nnp.unique(y_train)","2b8801b1":"#computing prior probability for each example in dataset\ndef prior_prob(y_train,label):\n    total_examples=y_train.shape[0]\n    class_examples=np.sum(y_train==label)\n    return (class_examples\/float(total_examples))\n\n#computing conditional probability\ndef cond_prob(x_train,y_train,feature_col,feature_val,label):\n    x_filtered=x_train[y_train==label]\n    numerator=np.sum(x_filtered[:,feature_col]==feature_val)\n    denominator=np.sum(y_train==label)\n    \n    return numerator\/float(denominator)","008c1811":"#now compute posterior probability for each class \ndef predict(x_train,y_train,xtest):\n    #xtest is a single testing point with n no. of features\n    classes = np.unique(y_train)\n    \n    #compute posterior for each class\n    \n    n_features=x_train.shape[1]\n    post_probs=[]#contain list oof prob for all classes and given a single testing point\n    \n    for label in classes:\n        #post_porb=likelihood*prior\n        \n        likelihood=1.0 #initiation as we multliply others with one \n        \n        for f in range(n_features):\n            cond=cond_prob(x_train,y_train,f,xtest[f],label)\n            likelihood *= cond\n            \n        prior = prior_prob(y_train,label)\n        post = likelihood*prior\n        post_probs.append(post)\n    \n    pred=np.argmax(post_probs)\n    return pred\n    ","7a4a872d":"output=predict(x_train,y_train,x_test[3])\nprint(output)\nprint(y_test[1])","cf3351f4":"def score(x_train,y_train,x_test,y_test):\n    pred=[]\n    for i in range(x_test.shape[0]):\n        pred_label=predict(x_train,y_train,x_test[i])\n        pred.append(pred_label)\n    pred=np.array(pred)\n    accuracy=np.sum(pred==y_test)\/float(y_test.shape[0])\n    return accuracy","2e37e66e":"print(score(x_train,y_train,x_test,y_test))","98fceb7e":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import make_classification","e98094bf":"gnb=GaussianNB()","71554a57":"X,Y=make_classification(n_samples=200,n_features=2,n_informative=2,n_redundant=0,random_state=2)","78908aee":"plt.scatter(X[:,0],X[:,1],c=Y)\nplt.show()","1068489c":"print(X[0])","9dbb0c64":"print(X.shape)","824158d8":"## building of classifier","f48b7646":"## Model Encoding using sklearn","9cedd168":"## preparing datasets","d9a5b138":"## breaking data into test and train","dc9d3465":"# Gaussian Naive bayes - for continous valued features"}}