{"cell_type":{"97d69d28":"code","1b975391":"code","e3779128":"code","2350983c":"code","d1b0c631":"code","4150b584":"code","e4338f9c":"code","5094a474":"code","3e408e84":"code","58e63639":"code","65b2da13":"code","9aa1a668":"code","e7fc52e2":"code","97e8faf6":"code","d075df5b":"code","aca4f7b5":"code","2f6b80a4":"code","fca40685":"code","3306f5a1":"code","7557b6c1":"code","47b7d588":"code","24ca9d2e":"code","5d8fb832":"code","bd0375df":"code","75435832":"code","3ddeac0a":"code","e1e51fca":"code","40cebdfb":"code","6ef2d463":"markdown","63e12fe9":"markdown","480ac5e0":"markdown","c6fe5f99":"markdown","47eb8e97":"markdown","16b42d17":"markdown"},"source":{"97d69d28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b975391":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","e3779128":"df.head()","2350983c":"df.columns","d1b0c631":"y = df['diagnosis']\nx = df.drop(['id','Unnamed: 32'], axis=1)","4150b584":"y.head()","e4338f9c":"x.head()","5094a474":"sns.countplot(y, label=\"Count\")\nB,M = y.value_counts()\nprint(\"Number of Benign : \", B)\nprint(\"Number of Malignant : \", M)","3e408e84":"x.describe()","58e63639":"x.columns","65b2da13":"sns.jointplot(x='area_mean',y='perimeter_mean',data=x, kind='reg')","9aa1a668":"plt.subplots(figsize=(18,18))\nsns.heatmap(x.corr(), annot=True, fmt='.1f')","e7fc52e2":"x2 = x.drop('diagnosis', axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx1 = pd.DataFrame(scaler.fit_transform(x2))\nx1.columns = x2.columns","97e8faf6":"x1.head()","d075df5b":"y1 = y\nplt.figure(figsize=(10,10))\ndata = pd.concat([y, x1.iloc[:,0:10]], axis=1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='values')\nsns.swarmplot(x='features', y='values', hue='diagnosis',  data=data)\nplt.xticks(rotation=90)","aca4f7b5":"plt.figure(figsize=(10,10))\ndata = pd.concat([y, x1.iloc[:,10:20]], axis=1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='values')\nsns.swarmplot(x='features', y='values', hue='diagnosis',  data=data)\nplt.xticks(rotation=90)","2f6b80a4":"plt.figure(figsize=(10,10))\ndata = pd.concat([y, x1.iloc[:,20:31]], axis=1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='values')\nsns.swarmplot(x='features', y='values', hue='diagnosis',  data=data)\nplt.xticks(rotation=90)","fca40685":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x2, y, test_size=0.2)","3306f5a1":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nrfe = RFE(estimator=clf, n_features_to_select=5, step=1)\nrfe = rfe.fit(X_train,y_train)","7557b6c1":"print('Features which are selected: ', X_train.columns[rfe.support_])","47b7d588":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nacs = accuracy_score(y_test,rfe.predict(X_test))\nprint('Accuracy is: ', acs)\ncm = confusion_matrix(y_test,rfe.predict(X_test))\nsns.heatmap(cm, annot=True)","24ca9d2e":"from sklearn.feature_selection import RFECV\nrfecv = RFECV(estimator=clf, step=1, cv=5, scoring='accuracy')\nrfecv = rfecv.fit(X_train,y_train)","5d8fb832":"print('Optimal number of features: ', rfecv.n_features_)\nprint('Features which are selected: ', X_train.columns[rfecv.support_])","bd0375df":"acs = accuracy_score(y_test,rfecv.predict(X_test))\nprint('Accuracy is: ', acs)\ncm = confusion_matrix(y_test,rfecv.predict(X_test))\nsns.heatmap(cm, annot=True)","75435832":"X_train1 = X_train[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'radius_se', 'area_se', 'symmetry_se',\n       'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n       'smoothness_worst', 'compactness_worst', 'concavity_worst',\n       'concave points_worst', 'symmetry_worst']]\nX_test1 = X_test[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'radius_se', 'area_se', 'symmetry_se',\n       'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n       'smoothness_worst', 'compactness_worst', 'concavity_worst',\n       'concave points_worst', 'symmetry_worst']]\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn = knn.fit(X_train1,y_train)","3ddeac0a":"acs = accuracy_score(y_test,knn.predict(X_test1))\nprint('Accuracy is: ', acs)\ncm = confusion_matrix(y_test,knn.predict(X_test1))\nsns.heatmap(cm, annot=True)","e1e51fca":"from sklearn.svm import SVC\nsvc = SVC()\nsvc = svc.fit(X_train1,y_train)","40cebdfb":"acs = accuracy_score(y_test,svc.predict(X_test1))\nprint('Accuracy is: ', acs)\ncm = confusion_matrix(y_test,svc.predict(X_test1))\nsns.heatmap(cm, annot=True)","6ef2d463":"There are a number of methods to extract features. We'll use one of the most popular technique Recursive Feature Elimination or RFE. After that, we'll run different classification models to obtain the best possible accuracy.","63e12fe9":"We're not getting a higher accuracy with other models.","480ac5e0":"We can see from the above swarm plots that not all features are very good at classifying 'Malignant' and 'Benign' and thus we'll have to extract the features from the complete set of features available. For ex, 'area_worst' is a good feature and is almost separating all the 'Benign' and 'Malignant' cancers.","c6fe5f99":"In the different columns present in our dataset, we can straight away see that the columns 'id' and 'unnamed:32' are of no use. Moreover, 'diagnosis' is the target class so we can remove that also.","47eb8e97":"On using RFECV and RandomForest Classifier, we are getting an accuracy of 100%. Now, let's try with a different model.","16b42d17":"There are a lot of features which have high correlation with others and this means we'll have to drop some in order to avoid multi-collinearity. For example, 'perimeter_mean', 'area_mean', and 'radius_mean' are all highly correlated and we'll have to drop 2\/3 in order to ensure there is no collinearity."}}