{"cell_type":{"7776fef3":"code","6a2ab5ef":"code","c4144886":"code","03c477c9":"code","2b54ed79":"code","02a9f5fd":"markdown","acb3b2ba":"markdown","3eadab8d":"markdown","44fd3065":"markdown","945cfcab":"markdown","0a3016ab":"markdown"},"source":{"7776fef3":"import os, inspect, warnings, argparse\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom sklearn.utils import shuffle\n\nPACK_PATH = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))","6a2ab5ef":"class Dataset(object):\n\n    def __init__(self, normalize=True):\n\n        print(\"\\nInitializing Dataset...\")\n\n        self.normalize = normalize\n\n        self.x_tr, self.y_tr = None, None\n        self.x_te, self.y_te = None, None\n\n        ftr = open(\"..\/input\/digit-recognizer\/train.csv\", \"r\")\n        while(True):\n            content = ftr.readline()\n            if not content: break\n            if(\"pixel\" in content): continue\n            tmp_label = content.replace(\"\\n\", \"\").split(\",\")[0]\n            tmp_data = np.reshape(np.asarray(content.replace(\"\\n\", \"\").split(\",\")[1:]), (1, 28, 28))\n            if(self.y_tr is None):\n                self.y_tr = tmp_label\n                self.x_tr = tmp_data\n            else:\n                self.y_tr = np.append(self.y_tr, tmp_label)\n                self.x_tr = np.append(self.x_tr, tmp_data, axis=0)\n        ftr.close()\n\n        fte = open(\"..\/input\/digit-recognizer\/test.csv\", \"r\")\n        while(True):\n            content = fte.readline()\n            if not content: break\n            if(\"pixel\" in content): continue\n            tmp_label = 0\n            tmp_data = np.reshape(np.asarray(content.replace(\"\\n\", \"\").split(\",\")), (1, 28, 28))\n            if(self.y_te is None):\n                self.y_te = tmp_label\n                self.x_te = tmp_data\n            else:\n                self.y_te = np.append(self.y_te, tmp_label)\n                self.x_te = np.append(self.x_te, tmp_data, axis=0)\n        fte.close()\n\n        self.x_tr = np.ndarray.astype(self.x_tr, np.float32)\n        self.x_te = np.ndarray.astype(self.x_te, np.float32)\n\n        self.num_tr, self.num_te = self.x_tr.shape[0], self.x_te.shape[0]\n        self.idx_tr, self.idx_te = 0, 0\n\n        print(\"Number of data\\nTraining: %d, Test: %d\\n\" %(self.num_tr, self.num_te))\n\n        x_sample, y_sample = self.x_te[0], self.y_te[0]\n        self.height = x_sample.shape[0]\n        self.width = x_sample.shape[1]\n        try: self.channel = x_sample.shape[2]\n        except: self.channel = 1\n\n        self.min_val, self.max_val = x_sample.min(), x_sample.max()\n        self.num_class = 10\n\n        print(\"Information of data\")\n        print(\"Shape  Height: %d, Width: %d, Channel: %d\" %(self.height, self.width, self.channel))\n        print(\"Value  Min: %.3f, Max: %.3f\" %(self.min_val, self.max_val))\n        print(\"Class  %d\" %(self.num_class))\n        print(\"Normalization: %r\" %(self.normalize))\n        if(self.normalize): print(\"(from %.3f-%.3f to %.3f-%.3f)\" %(self.min_val, self.max_val, 0, 1))\n\n    def reset_idx(self): self.idx_tr, self.idx_te = 0, 0\n\n    def label2vector(self, labels):\n\n        labels_v = None\n        for idx_l, label in enumerate(labels):\n            tmp_v = np.expand_dims(np.eye(self.num_class)[int(label)], axis=0)\n            if(labels_v is None): labels_v = tmp_v\n            else: labels_v = np.append(labels_v, tmp_v, axis=0)\n\n        return labels_v\n\n    def next_train(self, batch_size=1, fix=False):\n\n        start, end = self.idx_tr, self.idx_tr+batch_size\n        x_tr, y_tr = self.x_tr[start:end], self.y_tr[start:end]\n        x_tr = np.expand_dims(x_tr, axis=3)\n\n        terminator = False\n        if(end >= self.num_tr):\n            terminator = True\n            self.idx_tr = 0\n            self.x_tr, self.y_tr = shuffle(self.x_tr, self.y_tr)\n        else: self.idx_tr = end\n\n        if(fix): self.idx_tr = start\n\n        if(x_tr.shape[0] != batch_size):\n            x_tr, y_tr = self.x_tr[-1-batch_size:-1], self.y_tr[-1-batch_size:-1]\n            x_tr = np.expand_dims(x_tr, axis=3)\n\n        if(self.normalize):\n            min_x, max_x = x_tr.min(), x_tr.max()\n            x_tr = (x_tr - min_x) \/ (max_x - min_x)\n\n        return x_tr, self.label2vector(labels=y_tr), terminator\n\n    def next_test(self, batch_size=1):\n\n        start, end = self.idx_te, self.idx_te+batch_size\n        x_te, y_te = self.x_te[start:end], self.y_te[start:end]\n        x_te = np.expand_dims(x_te, axis=3)\n\n        terminator = False\n        if(end >= self.num_te):\n            terminator = True\n            self.idx_te = 0\n        else: self.idx_te = end\n\n        if(self.normalize):\n            min_x, max_x = x_te.min(), x_te.max()\n            x_te = (x_te - min_x) \/ (max_x - min_x)\n\n        return x_te, self.label2vector(labels=y_te), terminator","c4144886":"class SENet(object):\n\n    def __init__(self, height, width, channel, num_class, leaning_rate=1e-3):\n\n        print(\"\\nInitializing Neural Network...\")\n        self.height, self.width, self.channel = height, width, channel\n        self.num_class, self.k_size = num_class, 3\n        self.leaning_rate = leaning_rate\n\n        self.x = tf.placeholder(tf.float32, [None, self.height, self.width, self.channel])\n        self.y = tf.placeholder(tf.float32, [None, self.num_class])\n        self.batch_size = tf.placeholder(tf.int32, shape=[])\n\n        self.weights, self.biasis = [], []\n        self.w_names, self.b_names = [], []\n\n        self.y_hat = self.build_model(input=self.x)\n\n        self.smce = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.y_hat)\n        self.loss = tf.reduce_mean(self.smce)\n\n        self.optimizer = tf.train.AdamOptimizer( \\\n            self.leaning_rate, beta1=0.9, beta2=0.999).minimize(self.loss)\n\n        self.score = tf.nn.softmax(self.y_hat)\n        self.pred = tf.argmax(self.score, 1)\n        self.correct_pred = tf.equal(self.pred, tf.argmax(self.y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n\n        tf.summary.scalar('softmax_cross_entropy', self.loss)\n        self.summaries = tf.summary.merge_all()\n\n    def build_model(self, input):\n\n        print(\"SE-1\")\n        conv1_1 = self.conv2d(input=input, stride=1, padding='SAME', \\\n            filter_size=[self.k_size, self.k_size, 1, 16], activation=\"relu\", name=\"conv1_1\")\n\n        glo_avg_pool1 = tf.reduce_sum(conv1_1, axis=(1, 2))\n        squeeze1 = self.fully_connected(input=glo_avg_pool1, num_inputs=16, \\\n            num_outputs=8, activation=\"relu\", name=\"squeeze1\")\n        exitation1 = tf.reshape(self.fully_connected(input=squeeze1, num_inputs=8, \\\n            num_outputs=16, activation=\"sigmoid\", name=\"exitation1\"), [-1, 1, 1, 16])\n        exitated1 = tf.multiply(conv1_1, exitation1)\n\n        conv1_2 = self.conv2d(input=exitated1, stride=1, padding='SAME', \\\n            filter_size=[self.k_size, self.k_size, 16, 16], activation=\"relu\", name=\"conv1_2\")\n        max_pool1 = self.maxpool(input=conv1_2, ksize=2, strides=2, padding='SAME', name=\"max_pool1\")\n\n        print(\"SE-2\")\n        conv2_1 = self.conv2d(input=max_pool1, stride=1, padding='SAME', \\\n            filter_size=[self.k_size, self.k_size, 16, 32], activation=\"relu\", name=\"conv2_1\")\n\n        glo_avg_pool2 = tf.reduce_sum(conv2_1, axis=(1, 2))\n        squeeze2 = self.fully_connected(input=glo_avg_pool2, num_inputs=32, \\\n            num_outputs=16, activation=\"relu\", name=\"squeeze2\")\n        exitation2 = tf.reshape(self.fully_connected(input=squeeze2, num_inputs=16, \\\n            num_outputs=32, activation=\"sigmoid\", name=\"exitation2\"), [-1, 1, 1, 32])\n        exitated2 = tf.multiply(conv2_1, exitation2)\n\n        conv2_2 = self.conv2d(input=exitated2, stride=1, padding='SAME', \\\n            filter_size=[self.k_size, self.k_size, 32, 32], activation=\"relu\", name=\"conv2_2\")\n        max_pool2 = self.maxpool(input=conv2_2, ksize=2, strides=2, padding='SAME', name=\"max_pool2\")\n\n        print(\"SE-3\")\n        conv3_1 = self.conv2d(input=max_pool2, stride=1, padding='SAME', \\\n            filter_size=[self.k_size, self.k_size, 32, 64], activation=\"relu\", name=\"conv3_1\")\n\n        glo_avg_pool3 = tf.reduce_sum(conv3_1, axis=(1, 2))\n        squeeze3 = self.fully_connected(input=glo_avg_pool3, num_inputs=64, \\\n            num_outputs=32, activation=\"relu\", name=\"squeeze3\")\n        exitation3 = tf.reshape(self.fully_connected(input=squeeze3, num_inputs=32, \\\n            num_outputs=64, activation=\"sigmoid\", name=\"exitation3\"), [-1, 1, 1, 64])\n        exitated3 = tf.multiply(conv3_1, exitation3)\n\n        conv3_2 = self.conv2d(input=exitated3, stride=1, padding='SAME', \\\n            filter_size=[self.k_size, self.k_size, 64, 64], activation=\"relu\", name=\"conv3_2\")\n\n        print(\"FullCon\")\n        [n, h, w, c] = conv3_2.shape\n        fullcon_in = tf.reshape(conv3_2, shape=[self.batch_size, h*w*c], name=\"fullcon_in\")\n        fullcon1 = self.fully_connected(input=fullcon_in, num_inputs=int(h*w*c), \\\n            num_outputs=512, activation=\"relu\", name=\"fullcon1\")\n        fullcon2 = self.fully_connected(input=fullcon1, num_inputs=512, \\\n            num_outputs=self.num_class, activation=None, name=\"fullcon2\")\n\n        return fullcon2\n\n    def initializer(self):\n        return tf.initializers.variance_scaling(distribution=\"untruncated_normal\", dtype=tf.dtypes.float32)\n\n    def maxpool(self, input, ksize, strides, padding, name=\"\"):\n\n        out_maxp = tf.nn.max_pool(value=input, \\\n            ksize=ksize, strides=strides, padding=padding, name=name)\n        print(\"Max-Pool\", input.shape, \"->\", out_maxp.shape)\n\n        return out_maxp\n\n    def activation_fn(self, input, activation=\"relu\", name=\"\"):\n\n        if(\"sigmoid\" == activation):\n            out = tf.nn.sigmoid(input, name='%s_sigmoid' %(name))\n        elif(\"tanh\" == activation):\n            out = tf.nn.tanh(input, name='%s_tanh' %(name))\n        elif(\"relu\" == activation):\n            out = tf.nn.relu(input, name='%s_relu' %(name))\n        elif(\"lrelu\" == activation):\n            out = tf.nn.leaky_relu(input, name='%s_lrelu' %(name))\n        elif(\"elu\" == activation):\n            out = tf.nn.elu(input, name='%s_elu' %(name))\n        else: out = input\n\n        return out\n\n    def variable_maker(self, var_bank, name_bank, shape, name=\"\"):\n\n        try:\n            var_idx = name_bank.index(name)\n        except:\n            with tf.variable_scope(\"vars\", reuse=tf.AUTO_REUSE):\n                variable = tf.get_variable(name=name, \\\n                    shape=shape, initializer=self.initializer())\n\n            var_bank.append(variable)\n            name_bank.append(name)\n        else:\n            variable = var_bank[var_idx]\n\n        return var_bank, name_bank, variable\n\n    def conv2d(self, input, stride, padding, \\\n        filter_size=[3, 3, 16, 32], dilations=[1, 1, 1, 1], activation=\"relu\", name=\"\"):\n\n        self.weights, self.w_names, weight = self.variable_maker(var_bank=self.weights, name_bank=self.w_names, \\\n            shape=filter_size, name='%s_w' %(name))\n        self.biasis, self.b_names, bias = self.variable_maker(var_bank=self.biasis, name_bank=self.b_names, \\\n            shape=[filter_size[-1]], name='%s_b' %(name))\n\n        out_conv = tf.nn.conv2d(\n            input=input,\n            filter=weight,\n            strides=[1, stride, stride, 1],\n            padding=padding,\n            use_cudnn_on_gpu=True,\n            data_format='NHWC',\n            dilations=dilations,\n            name='%s_conv' %(name),\n        )\n        out_bias = tf.math.add(out_conv, bias, name='%s_add' %(name))\n\n        print(\"Conv\", input.shape, \"->\", out_bias.shape)\n        return self.activation_fn(input=out_bias, activation=activation, name=name)\n\n    def fully_connected(self, input, num_inputs, num_outputs, activation=\"relu\", name=\"\"):\n\n        self.weights, self.w_names, weight = self.variable_maker(var_bank=self.weights, name_bank=self.w_names, \\\n            shape=[num_inputs, num_outputs], name='%s_w' %(name))\n        self.biasis, self.b_names, bias = self.variable_maker(var_bank=self.biasis, name_bank=self.b_names, \\\n            shape=[num_outputs], name='%s_b' %(name))\n\n        out_mul = tf.matmul(input, weight, name='%s_mul' %(name))\n        out_bias = tf.math.add(out_mul, bias, name='%s_add' %(name))\n\n        print(\"Full-Con\", input.shape, \"->\", out_bias.shape)\n        return self.activation_fn(input=out_bias, activation=activation, name=name)","03c477c9":"def training(sess, saver, neuralnet, dataset, epochs, batch_size, normalize=True):\n\n    print(\"\\nTraining to %d epochs (%d of minibatch size)\" %(epochs, batch_size))\n\n    summary_writer = tf.summary.FileWriter(PACK_PATH+'\/Checkpoint', sess.graph)\n\n    iteration = 0\n\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n\n    test_sq = 20\n    test_size = test_sq**2\n    for epoch in range(epochs):\n\n        while(True):\n            x_tr, y_tr, terminator = dataset.next_train(batch_size) # y_tr does not used in this prj.\n\n            _, summaries = sess.run([neuralnet.optimizer, neuralnet.summaries], \\\n                feed_dict={neuralnet.x:x_tr, neuralnet.y:y_tr, neuralnet.batch_size:x_tr.shape[0]}, \\\n                options=run_options, run_metadata=run_metadata)\n            loss, accuracy = sess.run([neuralnet.loss, neuralnet.accuracy], \\\n                feed_dict={neuralnet.x:x_tr, neuralnet.y:y_tr, neuralnet.batch_size:x_tr.shape[0]})\n            summary_writer.add_summary(summaries, iteration)\n\n            iteration += 1\n            if(terminator): break\n\n        print(\"Epoch [%d \/ %d] (%d iteration)  Loss:%.3f, Acc:%.3f\" \\\n            %(epoch, epochs, iteration, loss, accuracy))\n        saver.save(sess, PACK_PATH+\"\/Checkpoint\/model_checker\")\n        summary_writer.add_run_metadata(run_metadata, 'epoch-%d' % epoch)\n\ndef test(sess, saver, neuralnet, dataset, batch_size):\n\n    if(os.path.exists(PACK_PATH+\"\/Checkpoint\/model_checker.index\")):\n        print(\"\\nRestoring parameters\")\n        saver.restore(sess, PACK_PATH+\"\/Checkpoint\/model_checker\")\n\n    print(\"\\nTest...\")\n\n    fsubmit = open(\"submission.csv\", \"w\")\n    fsubmit.write(\"ImageId,Label\\n\")\n    print(\"ImageId,Label\")\n    cntid = 1\n\n    confusion_matrix = np.zeros((dataset.num_class, dataset.num_class), np.int32)\n    while(True):\n        x_te, y_te, terminator = dataset.next_test(1) # y_te does not used in this prj.\n        class_score = sess.run(neuralnet.score, \\\n            feed_dict={neuralnet.x:x_te, neuralnet.batch_size:x_te.shape[0]})\n\n        label, logit = np.argmax(y_te[0]), np.argmax(class_score)\n        confusion_matrix[label, logit] += 1\n        print(\"%d,%d\" %(cntid,logit))\n        fsubmit.write(\"%d,%d\\n\" %(cntid,logit))\n        cntid += 1\n\n        if(terminator): break\n    fsubmit.close()","2b54ed79":"def main():\n\n    dataset = Dataset(normalize=FLAGS.datnorm)\n    neuralnet = SENet(height=dataset.height, width=dataset.width, channel=dataset.channel, \\\n        num_class=dataset.num_class, leaning_rate=FLAGS.lr)\n    \n    sess_config = tf.ConfigProto()\n    sess_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=sess_config)\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n\n    training(sess=sess, neuralnet=neuralnet, saver=saver, dataset=dataset, epochs=FLAGS.epoch, batch_size=FLAGS.batch, normalize=True)\n    test(sess=sess, neuralnet=neuralnet, saver=saver, dataset=dataset, batch_size=FLAGS.batch)\n    \nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--datnorm', type=bool, default=True, help='Data normalization')\n    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate for training')\n    parser.add_argument('--epoch', type=int, default=100, help='Training epoch')\n    parser.add_argument('--batch', type=int, default=32, help='Mini batch size')\n\n    FLAGS, unparsed = parser.parse_known_args()\n\n    main()","02a9f5fd":"# 1. Calling python libraries.","acb3b2ba":"# 3. Definition of the SENet.","3eadab8d":"# Title: Digit Recognizer based on SENet.  \nThe original source code is provided at the following link.  \nhttps:\/\/github.com\/YeongHyeon\/Kaggle-MNIST  \n\nThe performance of this deep neural network is 0.99871. This is the 122th of 2270 teams (top 6%) as of November 29, 2019.  \n![score](https:\/\/raw.githubusercontent.com\/YeongHyeon\/Kaggle-MNIST\/master\/figures\/performance.png)","44fd3065":"# 2. Definition of the Dataset class (with MNIST dataset).","945cfcab":"# 4. The function for training and test.","0a3016ab":"# 5. The main function for run the whole source code"}}