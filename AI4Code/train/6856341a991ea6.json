{"cell_type":{"438317ef":"code","d34adb45":"code","8c84b434":"code","e194f28a":"code","063fa17c":"code","c1f516d4":"code","a80a1366":"code","ce2b243a":"code","3c01e214":"code","87e5af80":"code","8a5f3cea":"code","a9685d59":"code","88fc0d4e":"code","f5ca0a0e":"code","c3c4cab2":"code","920af878":"code","f630dab0":"code","1826be23":"code","508b31a8":"markdown","9002a038":"markdown","21c02144":"markdown","29a902f2":"markdown","a1a92bab":"markdown","68f6297b":"markdown","f04dd932":"markdown","936a8562":"markdown","041443f8":"markdown","4b656e73":"markdown","248ca067":"markdown","9de5ef24":"markdown","be58e5ff":"markdown","f1eadd60":"markdown","dc368bd3":"markdown"},"source":{"438317ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d34adb45":"import pandas as pd\nimport numpy as np","8c84b434":"df = pd.read_csv(\"\/kaggle\/input\/xtu18-deep-lierning-hw0\/train.csv\")\ntestdf = pd.read_csv(\"\/kaggle\/input\/xtu18-deep-lierning-hw0\/testdata.csv\",header=None)","e194f28a":"df.head(18)","063fa17c":"data = df[df['observation']=='PM2.5']\ndata = data.iloc[:,3:].to_numpy(dtype='float').flatten()\ndata","c1f516d4":"x = []\ny = []\nfor i in range(len(data)-9):\n    x.append(list(data[i:i+9]))\ny = data[9:]\nx = np.array(x)","a80a1366":"#","ce2b243a":"w = np.matmul(np.matmul(np.linalg.pinv(np.matmul(x.transpose(),x)),x.transpose()),y)\nw","3c01e214":"def bgd(x,y,epochs=10000):\n    \"\"\"\n    bgd\n    \"\"\"\n    w = np.zeros(len(x[0]))\n    l_rate = 0.0001    # learning-rate\n    \n    for i in range(epochs):\n        error = np.dot(x,w)-y\n        loss = np.dot(error.T,error)\/len(x)\/2\n        grad = np.dot(x.T,error)\/len(x)\n        w = w - l_rate * grad\n        if i%100==0:\n            print ('epoch: %d ======>> Loss: %f ' % (i,loss))\n    \n    return w","87e5af80":"def sgd(x,y,epochs=10000):\n    \"\"\"\n    sgd\n    \"\"\"\n    w = np.zeros(len(x[0]))\n    l_rate = 0.0001    # learning-rate\n    G = np.zeros(len(x[0]))\n    \n    for i in range(epochs):\n        error = np.dot(x,w)-y\n        loss = np.dot(error.T,error)\/len(x)\/2\n        # random\n        j = np.random.randint(0, len(x[0]))\n        w = w - l_rate * (x[j] * (np.dot(x[j], w) - y[j]))\n        if i%100==0:\n            print ('epoch: %d ======>> Loss: %f ' % (i,loss))\n    \n    return w","8a5f3cea":"def mini(x,y,epochs=100000):\n    \"\"\"\n    mini\n    \"\"\"\n    w = np.zeros(len(x[0]))\n    l_rate = 0.00001    # learning-rate\n    G = np.zeros(len(x[0]))\n    \n    for i in range(epochs):\n        error = np.dot(x,w)-y\n        loss = np.dot(error.T,error)\/len(x)\/2\n        # \u4e00\u4e2a\u4e2abatch\u7b97\n        j = i%len(x)\n        w = w - l_rate * (x[j] * (np.dot(x[j], w) - y[j]))\n        if i%1000==0:\n            print ('epoch: %d ======>> Loss: %f ' % (i,loss))\n    \n    return w","a9685d59":"def adagrad(x,y,epochs=10000):\n    \"\"\"\n    adagrad\n    \"\"\"\n    w = np.zeros(len(x[0]))\n    l_rate = 10    # learning-rate\n    G = np.zeros(len(x[0]))\n    \n    for i in range(epochs):\n        error = np.dot(x,w)-y\n        loss = np.dot(error.T,error)\/len(x)\/2\n        grad = np.dot(x.T,error)\/len(x)\n        G += grad**2\n        ada = np.sqrt(G+1e-8)\n        w = w - l_rate * grad\/ada\n        if i%100==0:\n            print ('epoch: %d ======>> Loss: %f ' % (i,loss))\n    \n    return w","88fc0d4e":"def RMSprop(x,y,epochs=10000):\n    \"\"\"\n    RMSprop\n    \"\"\"\n    w = np.zeros(len(x[0]))\n    l_rate = 0.001   # learning-rate\n    beta = 0.9    # weight\n    G = np.zeros(len(x[0]))\n    \n    for i in range(epochs):\n        error = np.dot(x,w)-y\n        loss = np.dot(error.T,error)\/len(x)\/2\n        grad = np.dot(x.T,error)\/len(x)\n        G = beta*G + (1-beta)*grad**2\n        ada = np.sqrt(G+1e-8)\n        w = w - l_rate * grad\/ada\n        if i%100==0:\n            print ('epoch: %d ======>> Loss: %f ' % (i,loss))\n    \n    return w","f5ca0a0e":"def Adam(x,y,epochs=10000):\n    \"\"\"\n    Adam\n    \"\"\"\n    w = np.zeros(len(x[0]))\n    l_rate = 0.001   # learning-rate\n    beta1 = 0.9    \n    beta2 = 0.99\n    G = np.zeros(len(x[0]))\n    Mom = np.zeros(len(x[0]))\n    \n    for i in range(epochs):\n        error = np.dot(x,w)-y\n        loss = np.dot(error.T,error)\/len(x)\/2\n        grad = np.dot(x.T,error)\/len(x)\n        \n        Mom = beta1*Mom + (1-beta1)*grad\n        Mom_ = Mom\/(1-beta1**(i+1))\n        \n        G = beta2*G + (1-beta2)*grad**2\n        G_ = G \/ (1-beta2**(i+1))\n        ada = np.sqrt(G_+1e-8)\n        \n        w = w - l_rate * Mom_\/ada\n        if i%100==0:\n            print ('epoch: %d ======>> Loss: %f ' % (i,loss))\n    \n    return w","c3c4cab2":"w = adagrad(x,y)\nw","920af878":"testdata = testdf[testdf[1]=='PM2.5']\ntestx = testdata.iloc[:,2:].to_numpy(dtype='float')\ntestx","f630dab0":"test_y=np.dot(testx,w)\ntest_y[test_y<0]=0\ntest_y","1826be23":"#sample = pd.read_csv('\/kaggle\/input\/xtu18-deep-lierning-hw0\/sampleSubmission.csv')\n#sample['value']=test_y\n#sample.to_csv('sampleSubmission.csv',index=0)\n#pd.read_csv('sampleSubmission.csv')","508b31a8":"# Transform","9002a038":"$$y=\\sum_{i=1}^{9} w_i*x_i+b$$","21c02144":"# Adam","29a902f2":"# Stochastic gradient descent","a1a92bab":"# Training","68f6297b":"# Mini-batch-gd","f04dd932":"# least squares method","936a8562":"# normalization","041443f8":"# Adjustment\n\n+ Batch\n+ learning-rate\n+ ada\n+ grad","4b656e73":"# Save","248ca067":"# RMSprop","9de5ef24":"# Adagrad","be58e5ff":"# Testing","f1eadd60":"# BGD","dc368bd3":"# Feature choose"}}