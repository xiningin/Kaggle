{"cell_type":{"3ce37b7e":"code","c67e046b":"code","f7562e8f":"code","ecad28f6":"code","d3ac999b":"code","731386d3":"code","f2a8c0b1":"code","07ace855":"code","3a2fe10a":"code","a275ed67":"code","2355715b":"code","ae4f93f3":"code","00338715":"code","6bcd1a10":"code","ebab0075":"markdown","aa0edf87":"markdown","93bf15ea":"markdown","d2c9f3c2":"markdown","400b4109":"markdown","3da16624":"markdown","cd91872b":"markdown","0e7f4bf5":"markdown","e50f16aa":"markdown","05b561bb":"markdown","50cc5711":"markdown","d0d60c4c":"markdown","d67b5d6f":"markdown","7d1c5030":"markdown","a75184f5":"markdown","4e600fce":"markdown","25d610b7":"markdown","b305e9cf":"markdown","281fcc11":"markdown"},"source":{"3ce37b7e":"import pandas as pd\nimport numpy as np","c67e046b":"input_path = '..\/input\/california-housing-prices\/housing.csv'\ndf = pd.read_csv(input_path)\ndf.sample(7)","f7562e8f":"df.info()","ecad28f6":"from sklearn.impute import SimpleImputer\n\ndef fill_na(df, strategy='median'):\n\n    num_values = list(df.columns[:-1])\n\n    imputer = SimpleImputer(strategy=strategy)\n    imputer.fit(df[num_values])\n    df[num_values] = imputer.transform(df[num_values])\n    return df\n\n\ndf = fill_na(df)\ndf.info()","d3ac999b":"from sklearn.preprocessing import StandardScaler\n\ndef scale(df):\n    num_values = list(df.columns[:-1])\n\n    scaler = StandardScaler()\n    scaler.fit(df[num_values])\n    df[num_values] = scaler.transform(df[num_values])\n    return df\n\n\ndf = scale(df)\ndf.sample(5)","731386d3":"from sklearn.preprocessing import OneHotEncoder\n\ndef encode(df):\n    cat_values = ['ocean_proximity']\n    \n    encoder = OneHotEncoder()\n    encoder.fit(df[cat_values])\n    columns = [cat_values[0] + '_' + cat_name for cat_name in encoder.categories_][0]\n    encoded = pd.DataFrame(encoder.transform(df[cat_values]).toarray(), columns=columns)\n    return pd.concat([df, encoded.astype(int)], axis=1).drop('ocean_proximity', axis=1)\n\n\ndf = encode(df)\ndf","f2a8c0b1":"from sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([\n     ('fillna', SimpleImputer(strategy='median')),\n     ('scaler', StandardScaler()),\n])\n\nnum_values = list(df.columns[:9])\nnum_pipeline.fit(df[num_values])\nnum_df_transformed = num_pipeline.transform(df[num_values])\nnum_df_transformed","07ace855":"from sklearn.compose import ColumnTransformer\n\ncat_values = ['ocean_proximity']\n\nfull_pipeline = ColumnTransformer([\n    ('numeric', num_pipeline, num_values), \n    ('categorical', OneHotEncoder(), cat_values)\n])\n\ndf = pd.read_csv(input_path)\nX = full_pipeline.fit_transform(df)\nX","3a2fe10a":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CreateNewFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True):\n        self.rooms_ix = 3\n        self.bedrooms_ix = 4\n        self.population_ix = 5\n        self.households_ix = 6\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    \n    def fit(self, X, y=None):\n        return self # n\u00e3o fazemos nada\n    \n    def transform(self, X):\n        rooms_per_household = X[:, self.rooms_ix] \/ X[:, self.households_ix]\n        population_per_household = X[:, self.population_ix] \/ X[:, self.households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, self.bedrooms_ix] \/ X[:, self.rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        return np.c_[X, rooms_per_household, population_per_household]\n\nnew_features = CreateNewFeatures(add_bedrooms_per_room=True)\nhousing_extra_features = new_features.transform(df.values)\nhousing_extra_features","a275ed67":"num_pipeline = Pipeline([\n     ('fillna', SimpleImputer(strategy='median')),\n     ('feature_creator', CreateNewFeatures(add_bedrooms_per_room=True)),\n     ('scaler', StandardScaler()),\n])\n\n\ndf_transformed = num_pipeline.fit_transform(df[num_values])\ndf_transformed","2355715b":"preprocessing_pipeline = ColumnTransformer([\n    ('numeric', num_pipeline, num_values),\n    ('categorical', OneHotEncoder(), cat_values)\n])\n\ndf = pd.read_csv(input_path)\nX = preprocessing_pipeline.fit_transform(df)\nX","ae4f93f3":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\n# Dividindo o DataFrame em matrizes X (de features) e y (o target)\n# Al\u00e9m disso, definimos quais colunas s\u00e3o num\u00e9ricas e quais s\u00e3o categ\u00f3ricas (para usar no ColumnTransformer)\ntarget = 'median_house_value'\nX = df.loc[:, df.columns != target]\ny = df[target]\nnum_values = np.delete(X.columns, np.where(X.columns == 'ocean_proximity'))\ncat_values = ['ocean_proximity']\n\n# Agora definimos nossas Pipelines\nnum_pipeline = Pipeline([\n     ('fillna', SimpleImputer(strategy='median')),\n     ('feature_creator', CreateNewFeatures(add_bedrooms_per_room=True)),\n     ('scaler', StandardScaler()),\n])\npreprocessing_pipeline = ColumnTransformer([\n    ('numeric', num_pipeline, num_values),\n    ('categorical', OneHotEncoder(categories=[df['ocean_proximity'].unique()]), cat_values)\n])\nfinal_pipe = Pipeline([\n    ('preprocessing', preprocessing_pipeline),\n    ('ridge', Ridge())\n])\n\n# Veja que a \u00faltima Pipeline tem dois passos: o pr\u00e9-processamento e o estimador (o Ridge, que \u00e9 um modelo linear)\n\n# Agora tem uma parte que pode ser um pouco complexa, mas basta enteder o que est\u00e1 dentro de o que\n# Na hora de passar os nomes dos par\u00e2metros para o GridSearch, ele utiliza os nomes que passamos nas tuplas \n# o utiliza dois underlines para se referir a um par\u00e2metro daquele objeto (seria an\u00e1logo ao ponto que usamos normalmente)\nparams = {\n    'preprocessing__numeric__feature_creator__add_bedrooms_per_room' : [False, True],\n    'preprocessing__numeric__fillna__strategy': ['median', 'mean'],\n    'ridge__alpha' : [0.1, 1, 10],\n}\n\n# De resto, \u00e9 tudo igual, passamos o estimador (esse Pipeline \u00e9 um estimador pois o \u00faltimo passo \u00e9 o Ridge, que \u00e9 um estimador)\n# e passamos os par\u00e2metros, al\u00e9m de outros par\u00e2metros que normalmente usamos em um GridSearch\ngs = GridSearchCV(final_pipe, params, cv=5, n_jobs=-1)\ngs.fit(X, y)\ngs.best_params_ # E temos os melhores par\u00e2metros automaticamente =)","00338715":"class MyTransformer():\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return np.ones_like(X)\n        \ndf = pd.read_csv(input_path)\n\nmy_pipeline = Pipeline([\n    ('imputer', SimpleImputer()),\n    ('test', MyTransformer()), \n])\n\nmy_pipeline.fit_transform(df[num_values])\n# Mesmo n\u00e3o herdando de nada, o sklearn reconhece nossa classe como um Transformer =)","6bcd1a10":"final_pipe.get_params()","ebab0075":"# Pr\u00e9-processamento sem pipelines\n\nAntes de ver como usar pipelines, vamos demonstrar como far\u00edamos o pr\u00e9-processamento dos dados sem elas.\nOs passos do pr\u00e9-processamento ser\u00e3o:\n- Preencher os valores nulos\n- Normalizar e padronizar os valores num\u00e9ricos\n- Codificar os valores categ\u00f3ricos utilizando One-Hot Encoding","aa0edf87":"Vemos que existem alguns valores nulos na coluna `total_bedrooms`. ","93bf15ea":"Ent\u00e3o, basicamente criei tr\u00eas fun\u00e7\u00f5es, uma para cada tarefa e vamos modificando o `DataFrame` original at\u00e9 ele se tornar a matriz `X` que vamos passar para nosso modelo de Machine Learning (usando `.fit`).","d2c9f3c2":"Note que o objeto `Pipeline` tamb\u00e9m \u00e9 um `Transformer` (pois tem os m\u00e9todos `fit` e `transform`), ent\u00e3o podemos consider\u00e1-lo como se fosse uma caixa preta e passar para nosso `ColumnTransformer`.\n\nE \u00e9 isso, temos exatamente a mesma coisa que t\u00ednhamos feito antes, mas de uma forma muuito mais pr\u00e1tica (claro, entender como `Pipelines` funcionam talvez n\u00e3o seja exatamente f\u00e1cil, mas com certeza recompensa muito).\n\nEsse tutorial poderia parar por aqui. Mas vamos ver algumas outras coisinhas que podemos fazer utilizando `Pipelines`.","400b4109":"# Ap\u00eandice A","3da16624":"# Ap\u00eandice B","cd91872b":"# Pipelines\n\nAgora vejamos como fazer o mesmo processo, mas utilizando Pipelines. \n\nPara criar um `Pipeline`, importamos esse objeto e o instanciamos passando uma lista de tuplas. \n- O primeiro valor da tupla \u00e9 o nome daquele passo na nossa linha de montagem;\n- O segundo valor \u00e9 um `Transformer` do `sklearn`. Ou seja, deve ser um objeto que possua os m\u00e9todos `transform` e `fit` (pelo menos).\n\nVamos falar mais em detalhes de `Transformers` mais para frente.\n\n**_Nota_**: o mais correto \u00e9 dizer que todos os passos devem ser `Transformers`, exceto o \u00faltimo, que pode ser um `Estimator` (ou seja, possuir o m\u00e9todo `predict`).\n\nAp\u00f3s criar esse `Pipeline`, podemos chamar os seus m\u00e9todos `fit` e `transform`. Basicamente, o que ele faz \u00e9 chamar em sequ\u00eancia cada `Transformer` passando para o pr\u00f3ximo a sa\u00edda retornada pelo anterior. ","0e7f4bf5":"E pronto, temos um `Transformer` feito por n\u00f3s mesmos. O limite do que se pode fazer \u00e9 basicamente definido pela nossa imagina\u00e7\u00e3o =)\n\nAgora podemos utilizar esses `Transformers` na nossa `Pipeline` de antes para deix\u00e1-la mais sofisticada ainda.","e50f16aa":"# Criando seus pr\u00f3prios Transformers\n\nO sklearn nos fornece v\u00e1rios transformadores nativos, mas podemos querer criar outros. Um exemplo muito pr\u00e1tico \u00e9 fazer um `Transformer` que gere novas features do seu processo de Feature Engineering. Outro exemplo seria um `Transformer` que mat\u00e9m apenas determinadas features do Data Set (Feature Selection).\n\nVamos mostrar uma mistura desses dois exemplos. Vou criar um `Transformer` que cria duas novas features e tem um par\u00e2metro booleano se devemos criar ou n\u00e3o uma terceira nova feature (assim poderemos testar mais para frente se incluir ou n\u00e3o incluir essa feature \u00e9 melhor ou n\u00e3o). ","05b561bb":"Compare com o `DataFrame` que geramos pelo pr\u00e9-processamento anteriormente e voc\u00ea ver\u00e1 que o resultado \u00e9 o mesmo. A diferen\u00e7a \u00e9 que aqui temos diretamente um `ndarray` do `numpy` ao inv\u00e9s de um `DataFrame` do `pandas` (na pr\u00e1tica, n\u00e3o muda muito, pois o m\u00e9todo `fit` de um estimador transforma internamente `DataFrames` em `ndarrays`).","50cc5711":"# Importantando bibliotecas e dados\n\nPrimeiro, como sempre, vamos importar as bibliotecas e dados que vamos utilizar.\n\nEsse tutorial foi inspirado no segundo cap\u00edtulo do livro _Hands-On Machine Learning_, o notebook do cap\u00edtulo pode ser encontrado no [GitHub](https:\/\/github.com\/ageron\/handson-ml2). Ent\u00e3o vamos importar o data set utilizado por ele.","d0d60c4c":"Para criar um `Transformer`, basicamente basta criar uma classe que implemente os m\u00e9todos `fit` e `transform`, como j\u00e1 falei (sim, n\u00e3o precisa herdar de nenhuma classe do `sklearn` com o nome `Transformer` ou coisa do tipo. Mostrei isso no Ap\u00eandice A do notebook).\n\nEntretanto, existem duas classes que costumamos herdar, pois elas nos ajudam:\n- `TransformerMixin` cria para n\u00f3s um m\u00e9todo `fit_transform` automaticamente usando nossos m\u00e9todo `fit` e `transform`; e\n- `BaseEstimator` cria para n\u00f3s os m\u00e9todos `set_params` e `get_params` que s\u00e3o utilizados pelo `GridSearchCV` internamente.","d67b5d6f":"# Transforma\u00e7\u00f5es diferentes para colunas diferentes\n\nBem, agora precisamos fazer o One-Hot Encoding na coluna `ocean_proximity` e juntar com essa matriz que o `Pipeline` gerou, certo?\n\nInfelizmente, apenas com `Pipelines` isso n\u00e3o \u00e9 poss\u00edvel, pois o `Pipeline` n\u00e3o considera as diferentes colunas de uma matriz. Ele apenas aplica as transforma\u00e7\u00f5es (\u00e9 por isso que na hora de dar fit e transform, eu chamo `df[num_values]` ao inv\u00e9s de `df` inteiro).\n\nEntretanto, o objeto `ColumnTransformer` nos permite fazer exatamente o que precisamos: transformar um conjunto de colunas de um jeito e um outro conjunto de colunas de outro e juntar essas colunas em uma \u00fanica matriz.\n\nEles funcionam muito similar a `Pipelines`, a diferen\u00e7a \u00e9 que a tupla recebe um valor a mais: as colunas em que aquela transforma\u00e7\u00e3o ser\u00e1 aplicada.","7d1c5030":"E _voil\u00e0_, temos novamente uma matriz prontinha para qualquer modelo de Machine Learning utilizar.","a75184f5":"Contextualizando, temos dados de casas no estado da California e seus pre\u00e7os, que \u00e9 o que queremos predizer.","4e600fce":"Na minha opini\u00e3o, a parte mais dif\u00edcil de entender \u00e9 esses nomes enormes de par\u00e2metros e entender as abstra\u00e7\u00f5es de um `Pipeline` (pensar que o `Pipeline` herda o tipo do objeto no \u00faltimo passo - um `Estimator` ou `Transformer`). Por isso, vou deixar uma imagem para ficar mais claro.\n\n<img src=\"https:\/\/github.com\/Giatroo\/BeeData_Pipelines-in-Sklearn\/blob\/main\/Arvore-de-Pipelines.jpg?raw=true\" alt=\"drawing\" width=\"600\"\/>\n\nAl\u00e9m disso, para saber quais s\u00e3o os par\u00e2metros de um `Pipeline`, podemos utilizar o m\u00e9todo `get_params`, que nos retorna um dicion\u00e1rio com cada atributo e seu valor padr\u00e3o. Olhe o Ap\u00eandice B para ver o retorno desse m\u00e9todo para o nosso `Pipeline` final.","25d610b7":"# Conclus\u00e3o e Aprofundamento\n\nPor hoje \u00e9 tudo =)\n\nEspero que voc\u00ea tenha gostado e aproveitado bastante. \n\nConclu\u00edmos que os `Pipelines` s\u00e3o uma ferramenta incr\u00edvel que podemos utilizar para agilizar muito nossa vida como cientistas de dados. Al\u00e9m disso, aprendemos um pouco sobre como o `sklearn` opera (os diferentes tipos de objetos dele) e como criar nossos pr\u00f3prios `Transformers`, al\u00e9m de aplicar diferentes transforma\u00e7\u00f5es a diferentes colunas utilizando `ColumnTransformers`.\n\nPara se aprofundar e revisar, recomendo dar uma buscada no livro de onde eu tirei esses exemplos, que j\u00e1 citei, al\u00e9m de dar uma olhada nos seguintes recursos:\n\n[Livro - Hands On Machine Learning](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)\n\n[Guia do usu\u00e1rio - Pipeline and composite estimators](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#combining-estimators)\n\n[Documenta\u00e7\u00e3o Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)\n\n[Documenta\u00e7\u00e3o make_pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)\n\n[Documenta\u00e7\u00e3o ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer)\n\n[Documenta\u00e7\u00e3o TransformedTargetRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor)\n\n[Tutorial do towards data science](https:\/\/towardsdatascience.com\/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156)\n\n\n\n**~Lucas Paiolla, 23\/04\/2021**","b305e9cf":"# Pipelines e Hyperparameter Tuning\n\nPor fim, uma das funcionalidade que eu mais acho incr\u00edvel dos `Pipelines` \u00e9 a capacidade us\u00e1-los junto do `GridSearchCV` (ou `RandomizedSearchCV`) e podermos passar par\u00e2metros dos transformadores em si para serem testados.\n\nVamos dar uma olhada:","281fcc11":"# Pipelines do SkLearn\n\nHoje vamos aprender uma ferramenta poderos\u00edssima do `sklearn`: _pipelines_ (que em tradu\u00e7\u00e3o literal \u00e9 _oleoduto_, mas eu prefiro algo como _linha de montagem_). \n\nAs pipelines nos permitem colocar em sequ\u00eancia todos o passos do nosso projeto de Machine Learning e tamb\u00e9m automatizar o pr\u00e9-processamento, treinamento e afina\u00e7\u00e3o (_tuning_) de hiperpar\u00e2metros. Al\u00e9m disso, elas nos permitem fazer um `GridSearch` n\u00e3o s\u00f3 nos hyperpar\u00e2metros de um determinado modelo, mas tamb\u00e9m nos par\u00e2metros que usamos no pr\u00e9-processamento.\n\n- _Ser\u00e1 que eu preencho os valores nulos com a m\u00e9dia ou mediana?_\n- _Ser\u00e1 que eu uso ou n\u00e3o essa determinada feature?_\n\nEssas entre outras perguntas s\u00e3o muito comuns quando estamos lidando com um projeto. As pipelines permitem que a gente ache exatamente qual \u00e9 o melhor score que nossos modelos podem ter lidando com essas perguntas atrav\u00e9s de par\u00e2metros que passamos para um `GridSearch`."}}