{"cell_type":{"8824389e":"code","7b329ea7":"code","436452b0":"code","c942a170":"code","5118b6ec":"code","eea2b0ca":"code","4e7ad34b":"code","7c5415f5":"code","e78852b4":"code","594fdaa8":"code","c0debd0d":"code","fc3ccafa":"code","f8df50c6":"code","83fcf380":"code","458c929e":"code","89af235d":"code","44e6beff":"code","82bceb4e":"code","65515ced":"code","3eeaa686":"code","8cdaaa1e":"code","828716ff":"code","89598f78":"code","c0f95acc":"code","341d92b9":"code","afa217df":"code","b8f9975e":"code","d4c2ff8d":"markdown","85a4b639":"markdown","1041dd43":"markdown","27eefd1c":"markdown","ebdd665e":"markdown","526aad31":"markdown","2f7bed8c":"markdown","1084a4ec":"markdown","8b427d0a":"markdown","677650ad":"markdown"},"source":{"8824389e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.float_format', lambda x: '%.4f' % x)\nimport seaborn as sns\nsns.set_context(\"paper\", font_scale=1.3)\nsns.set_style('white')\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom time import time\nimport matplotlib.ticker as tkr\nfrom scipy import stats\nfrom statsmodels.tsa.stattools import adfuller\nfrom sklearn import preprocessing\nfrom statsmodels.tsa.stattools import pacf\n%matplotlib inline\nimport math\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom keras.callbacks import EarlyStopping\n# Any results you write to the current directory are saved as output.","7b329ea7":"df = pd.read_csv(\"..\/input\/energy-consumption-generation-prices-and-weather\/energy_dataset.csv\")","436452b0":"df.head()","c942a170":"df.info()","5118b6ec":"mape = np.mean(np.abs((df['total load actual'] - df['total load forecast']) \/ df['total load actual'])) * 100\nprint('MAPE of the forecasted data present in DataFrame:', mape)","eea2b0ca":"temp = df.copy() # make temporary copy of dataframe\ndataset = temp['total load actual'].dropna().values # numpy.ndarray of the actual load\ndataset = dataset.astype('float32') \ndataset = np.reshape(dataset, (-1, 1)) # reshape to one feature; required for the models\n\nscaler = MinMaxScaler(feature_range=(0, 1)) # Min Max scaler\ndataset = scaler.fit_transform(dataset) # fit and transform the dataset\n\n# Train and Test splits\ntrain_size = int(len(dataset) * 0.80) \ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n\ndef create_dataset(dataset, look_back=1):\n    X, Y = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        X.append(a)\n        Y.append(dataset[i + look_back, 0])\n    return np.array(X), np.array(Y)\n    \nlook_back = 25 # timesteps to lookback for predictions\nX_train, trainY = create_dataset(train, look_back)\nX_test, testY = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\nprint(\"Shapes: \\nTraining set: {}, Testing set: {}\".format(X_train.shape, X_test.shape))\nprint(\"Sample from training set: \\n{}\".format(X_train[0]))","4e7ad34b":"from statsmodels.tsa.ar_model import AR\n\nmodel = AR(train)\nmodel_fit = model.fit()","7c5415f5":"test_predict = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n# invert predictions\ntest_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\nY_test = scaler.inverse_transform(test)\nprint('Test Mean Absolute Error:', mean_absolute_error(Y_test, test_predict))\nprint('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test, test_predict)))","e78852b4":"mape = np.mean(np.abs((Y_test - test_predict) \/ Y_test)) * 100\nprint(\"Testing MAPE: {}\".format(mape))","594fdaa8":"idx = 200\naa=[x for x in range(idx)]\nplt.figure(figsize=(8,4))\nplt.plot(aa, Y_test[:idx], marker='.', label=\"actual\")\nplt.plot(aa, test_predict[:idx], 'r', label=\"prediction\")\n# plt.tick_params(left=False, labelleft=True) #remove ticks\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('TOTAL Load', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show();","c0debd0d":"test_predict = np.mean(X_test, axis=2)\nprint('Test Mean Absolute Error:', mean_absolute_error(testY, test_predict))\nprint('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(testY, test_predict)))","fc3ccafa":"mape = np.mean(np.abs((Y_test - test_predict) \/ Y_test)) * 100\nprint(\"Testing MAPE: {}\".format(mape))","f8df50c6":"idx = 200\naa=[x for x in range(idx)]\nplt.figure(figsize=(8,4))\nplt.plot(aa, testY[:idx], marker='.', label=\"actual\")\nplt.plot(aa, test_predict[:idx], 'r', label=\"prediction\")\n# plt.tick_params(left=False, labelleft=True) #remove ticks\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('TOTAL Load', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show();","83fcf380":"from statsmodels.tsa.arima_model import ARMA\n\nmodel = ARMA(train, order=(2, 1))\nmodel_fit = model.fit(disp=False)","458c929e":"test_predict = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n# invert predictions\ntest_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\nY_test = scaler.inverse_transform(test)\nprint('Test Mean Absolute Error:', mean_absolute_error(Y_test, test_predict))\nprint('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test, test_predict)))","89af235d":"mape = np.mean(np.abs((Y_test - test_predict) \/ Y_test)) * 100\nprint(\"Testing MAPE: {}\".format(mape))","44e6beff":"idx = 200\naa=[x for x in range(idx)]\nplt.figure(figsize=(8,4))\nplt.plot(aa, Y_test[:idx], marker='.', label=\"actual\")\nplt.plot(aa, test_predict[:idx], 'r', label=\"prediction\")\n# plt.tick_params(left=False, labelleft=True) #remove ticks\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('TOTAL Load', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show();","82bceb4e":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(train, order=(1, 1, 1))\nmodel_fit = model.fit(disp=False)","65515ced":"test_predict = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n# invert predictions\ntest_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\nY_test = scaler.inverse_transform(test)\nprint('Test Mean Absolute Error:', mean_absolute_error(Y_test, test_predict))\nprint('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test, test_predict)))","3eeaa686":"mape = np.mean(np.abs((Y_test - test_predict) \/ Y_test)) * 100\nprint(\"Testing MAPE: {}\".format(mape))","8cdaaa1e":"idx = 200\naa=[x for x in range(idx)]\nplt.figure(figsize=(8,4))\nplt.plot(aa, Y_test[:idx], marker='.', label=\"actual\")\nplt.plot(aa, test_predict[:idx], 'r', label=\"prediction\")\n# plt.tick_params(left=False, labelleft=True) #remove ticks\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('TOTAL Load', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show();","828716ff":"# making data again to remove inconsistencies\ntemp = df\ndataset = temp['total load actual'].dropna().values #numpy.ndarray\ndataset = dataset.astype('float32')\ndataset = np.reshape(dataset, (-1, 1))\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\ntrain_size = int(len(dataset) * 0.80)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n\ndef create_dataset(dataset, look_back=1):\n    X, Y = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        X.append(a)\n        Y.append(dataset[i + look_back, 0])\n    return np.array(X), np.array(Y)\n    \nlook_back = 25\nX_train, Y_train = create_dataset(train, look_back)\nX_test, Y_test = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))","89598f78":"model = Sequential()\nmodel.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nhistory = model.fit(X_train, Y_train, epochs=200, batch_size=70, validation_data=(X_test, Y_test),verbose=1, shuffle=False)\n\nmodel.summary()","c0f95acc":"train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n# invert predictions\ntrain_predict = scaler.inverse_transform(train_predict)\nY_train = scaler.inverse_transform([Y_train])\ntest_predict = scaler.inverse_transform(test_predict)\nY_test = scaler.inverse_transform([Y_test])\nprint('Train Mean Absolute Error:', mean_absolute_error(Y_train[0], train_predict[:,0]))\nprint('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0])))\nprint('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))\nprint('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))","341d92b9":"mape_train = np.mean(np.abs((Y_train[0] - train_predict[:,0]) \/ Y_train[0])) * 100\nmape_test = np.mean(np.abs((Y_test[0] - test_predict[:,0]) \/ Y_test[0])) * 100\n\nprint(\"Train MAPE: {}, Test MAPE: {}\".format(mape_train, mape_test))","afa217df":"plt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","b8f9975e":"idx = 200\naa=[x for x in range(idx)]\nplt.figure(figsize=(8,4))\nplt.plot(aa, Y_test[0][:idx], marker='.', label=\"actual\")\nplt.plot(aa, test_predict[:,0][:idx], 'r', label=\"prediction\")\n# plt.tick_params(left=False, labelleft=True) #remove ticks\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('TOTAL Load', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show();","d4c2ff8d":"## Analysis and Future Work\n\nThe LSTM model preformed close to the forecasted data present in the dataset. The MAPE can be improved further by forming this problem as a multivariate time series and tweaking the LSTM model. There are also many parameters that can improve the overall performance. The rest is upto your imagination!!","85a4b639":"# Preprocessing\n\nHere we extract the single feature we will predict, i.e. `total load actual`. Then we scale the feature using a MinMaxScaler. To prepare the data for the models, use `create_dataset` function which takes the data and creates chunks of it based on the `look_back`. \n\nThe preprocessing works as follows:\nExample data: `[1,2,3,4,5]`\nAfter preprocessing (x -> y): \n>`[1,2]` -> `[3]` <br>\n>`[2,3]` -> `[4]` <br>\n>`[3,4]` -> `[5]` <br>\n\nwhen `look_back` is set to 2. This preprocessing is only required for LSTM. Rest of the models take input as a series with single feature.","1041dd43":"## AutoRegressive\n\nAn autoregressive (AR) model predicts future behavior based on past behavior. The process is basically a linear regression of the data in the current series against one or more past values in the same series.","27eefd1c":"## ARIMA\n\nAuto Regressive Integrated Moving Average is a class of models that explains a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.","ebdd665e":"## LSTM\n\nLong short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data","526aad31":"# Models and their MAPE\n\nHere we test various models and visualize their predictions. Models used are:\n* AutoRegressive\n* Moving Average\n* ARMA\n* ARIMA\n* LSTM","2f7bed8c":"# Loading Data and Taking a peek\n\n**About the data:** This dataset contains 4 years of electrical consumption, generation, pricing, and weather data for Spain. Consumption and generation data was retrieved from ENTSOE a public portal for Transmission Service Operator (TSO) data. Settlement prices were obtained from the Spanish TSO Red Electric Espa\u00f1a. Weather data was purchased as part of a personal project from the Open Weather API for the 5 largest cities in Spain and made public here. \n\nThe dataset is unique because it contains hourly data for electrical consumption and the respective forecasts by the TSO for consumption and pricing. **We focus on predicting electrical comsumption better than the already present forecast in the data. The metrics we are using for comparision is Mean Absolute Percentage Error or MAPE.**\n\nThe data is multivariate time series as it contains multiple features. To keep this notebook accessible to beginners, we use only a single feature thereby handling univariate time series. Although, to predict more accurately, most of the features presented in the data should be used and the problem should be handled as multivariate.","1084a4ec":"## ARMA\n\nAn ARMA model, or Autoregressive Moving Average model, is used to describe weakly stationary stochastic time series in terms of two polynomials. The first of these polynomials is for autoregression, the second for the moving average.","8b427d0a":"## Moving Average\n\nThe moving average (MA) is a simple technical analysis tool that smooths out price data by creating a constantly updated average price. The average is taken over a specific period of time, like 10 days, 20 minutes, 30 weeks or any time period the trader chooses. We are taking moving average of 25 hours hence we can use the data we prepared for LSTM.","677650ad":"# Hourly Energy Demand Time Series Forecast\n\nIn this notebook we explore the various methods of forecasting in times series. Points covered in this notebook:\n* Preprocessing the data\n* Applying models and comparing their performance\n\nThis notebook is not exhaustive in presenting the methods for forecasting."}}