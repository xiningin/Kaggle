{"cell_type":{"a731c45b":"code","61f3af0c":"code","01331b05":"code","1d3f967a":"code","406df05d":"code","151e1178":"code","030c90fd":"code","d1027e8b":"code","03850387":"code","a0faf8fd":"code","18c1aa88":"markdown","08d03dab":"markdown","64e46b84":"markdown","165b016a":"markdown","a27389d2":"markdown","6f2250de":"markdown","1773231c":"markdown","07f55e66":"markdown"},"source":{"a731c45b":"!pip install ..\/input\/sacremoses > \/dev\/null\n\nimport sys\nsys.path.insert(0, \"..\/input\/transformers\/\")","61f3af0c":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n# import tensorflow_hub as hub\nimport tensorflow as tf\n# import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","01331b05":"PATH = '..\/input\/google-quest-challenge\/'\n\n# BERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'\n# tokenizer = tokenization.FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)\n\nBERT_PATH = '..\/input\/bert-base-uncased-huggingface-transformer\/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","1d3f967a":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","406df05d":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model","151e1178":"outputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","030c90fd":"#model_path = f'..\/input\/berttuned3epochs10folds\/'","d1027e8b":"!ls ..\/input\/","03850387":"gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)\n\nvalid_preds = []\ntest_preds = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n    train_outputs = outputs[train_idx]\n\n    valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n    valid_outputs = outputs[valid_idx]\n\n    K.clear_session()\n    model = create_model()\n    model.load_weights(f'..\/input\/bert-tuned-4-epochs-10-folds-2e5-lr\/bert-{fold}.h5')\n    #optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    #model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    #model.fit(train_inputs, train_outputs, epochs=5, batch_size=6)\n    #model.save_weights(f'bert-{fold}.h5') ### save weights, next time import and predict!\n    valid_preds.append(model.predict(valid_inputs))\n    test_preds.append(model.predict(test_inputs))\n\n    rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n    print('validation score = ', rho_val)","a0faf8fd":"df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n\ndf_sub.to_csv('submission.csv', index=False)","18c1aa88":"#### 6. Process and submit test predictions\n\nAverage fold predictions, then save as `submission.csv`","08d03dab":"# Google Quest Q&A Labeling (Inference Kernel)\n\nAuthor: W. H. Khoong\n\n## Overview\n\n- **Training kernel** can be found at: https:\/\/www.kaggle.com\/khoongweihao\/google-quest-bert-base-tf2-0-training\n- **Trained model weights** for each fold can be found at: https:\/\/www.kaggle.com\/khoongweihao\/berttuned5epochs and https:\/\/www.kaggle.com\/khoongweihao\/bertpretrained9epochs\n- If you find this kernel helpful, kindly **upvote** for the support, including the training kernel and the kernel these have been built-upon from! Much GPU and freee time has been spend on this work\n\n## Updates (will be periodically updated in each commit)\n\n- **Version 1**: using trained model weights from 5 folds and 5 epochs (LB 0.376)\n- **Version 2**: using trained model from 5 folds and 9 epochs \n\n## Acknowledgements\n\n- Built upon from https:\/\/www.kaggle.com\/akensert\/bert-base-tf2-0-now-huggingface-transformer","64e46b84":"#### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-\/loss-function. ","165b016a":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","a27389d2":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n\n*update 4:* credits to [Minh](https:\/\/www.kaggle.com\/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input, it will require significantly more memory when finetuning BERT.","6f2250de":"### Bert-base TensorFlow 2.0\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https:\/\/www.kaggle.com\/corochann\/google-quest-first-data-introduction), [getting started](https:\/\/www.kaggle.com\/phoenix9032\/get-started-with-your-questions-eda-model-nn) & [another getting started](https:\/\/www.kaggle.com\/hamditarek\/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using ~~TensorFow Hub~~ Huggingface transformer. <br><br>\n","1773231c":"#### 3. Create model\n\n`compute_spearmanr()` is used to compute the competition metric for the validation set\n<br><br>\n`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n","07f55e66":"#### 4. Obtain inputs and targets, as well as the indices of the train\/validation splits"}}