{"cell_type":{"e84d4cda":"code","d5b0a32c":"code","d34ba413":"code","d0193377":"code","a81f5644":"code","bbbda688":"code","aeefe38f":"code","56a81f60":"code","aedf55c7":"code","4cbb7eb2":"code","699cc29e":"code","edb36378":"code","53a34c43":"code","b2488d60":"code","ac034d57":"code","eec227fa":"code","3f3dbaf2":"code","ed29dad1":"code","7b6829cd":"code","12ad84db":"code","bb33468d":"code","9d780bdc":"code","7cd5bd1f":"code","114cd529":"code","9b705045":"code","19d0bfa0":"code","8ec6e18d":"code","0a69fad2":"code","1ebd7908":"code","7d5bee1e":"code","7bc0acee":"code","26e294cd":"code","d01dabe5":"code","738dbc87":"code","b4b88db1":"code","6ee2ff31":"code","173b75a1":"code","02e296ba":"code","2e7194cd":"code","2d6e3e3c":"code","1a71bce4":"code","266a65c0":"code","bfc4b575":"code","f3277f22":"code","c6914758":"code","d5c35160":"code","4820fbe2":"code","46ea114a":"code","4bb995ee":"code","ae049123":"code","9b05466e":"code","eb29eb8c":"code","b7a09339":"code","3ed76167":"code","f9ff8bb9":"code","9e10b56c":"code","4ad4efd7":"code","e7df3b37":"code","df58c110":"code","05c3c48a":"code","0f474956":"code","f9b627f9":"code","c01580e4":"code","aa9a7804":"code","1fa3518b":"code","3a0edd84":"code","d65a461e":"code","3f5d9a80":"code","df065030":"code","ed5ef152":"code","36b5582b":"code","31903253":"code","e4df5e6c":"code","120e1965":"code","eb324215":"code","311c7b08":"code","d118ecbe":"code","3700c308":"code","7bef511c":"code","73dd7620":"code","0730f4cf":"code","79176809":"code","d831a354":"code","e1d8b225":"code","99df4549":"code","a095eaad":"code","7a5b1d4f":"code","e4d502d7":"code","fc787573":"code","bb233c60":"code","2b066582":"code","e33830dd":"code","fc529df7":"code","3384a986":"code","57921734":"code","f17e75bb":"code","c0223123":"code","9547ee04":"code","0f11a567":"code","b2f0c4c2":"code","215bbe29":"markdown","2d54b530":"markdown","abc248d2":"markdown","2a16b521":"markdown","4202cdb8":"markdown","abc532f4":"markdown","7f7d69fc":"markdown","5870aeca":"markdown","a3e8689b":"markdown","dd42a610":"markdown","5528498f":"markdown","e94a7147":"markdown","0bcf3ae7":"markdown","1fb01c7b":"markdown","44114f24":"markdown","c51b8345":"markdown","88e2e236":"markdown","274cf44a":"markdown","e2fc4e58":"markdown","d44c3358":"markdown","fb99995b":"markdown","e531c910":"markdown","e4095aa6":"markdown","c17b6e0f":"markdown","60987361":"markdown","2b12ca7f":"markdown","c6b65068":"markdown","9c832113":"markdown","eb0710d5":"markdown","1ea04c52":"markdown","cb073030":"markdown","a183db0b":"markdown","8531eeaa":"markdown","32215160":"markdown","fd747685":"markdown","bcca9d7c":"markdown","0474fff2":"markdown","c5fac86f":"markdown","e7f14142":"markdown","876b5866":"markdown","e13bed23":"markdown","cc3e6ad6":"markdown","2d7ceb94":"markdown","e281e1a4":"markdown","6f8e5c9f":"markdown","94bcb83d":"markdown","c051c467":"markdown","3a05af71":"markdown","e16103ca":"markdown","e554fb8b":"markdown","4ab24fab":"markdown","53a5c8ce":"markdown","32b800dc":"markdown","42aa9863":"markdown","df899541":"markdown","e12bd26b":"markdown","df7244db":"markdown","2bb7f09d":"markdown","f717e648":"markdown","3ae0943f":"markdown","7a0d12c9":"markdown","620aa78b":"markdown","6afddd99":"markdown","aa493910":"markdown","7610f219":"markdown","20394fbe":"markdown","f5b093ba":"markdown","e4f78c7c":"markdown","ec0d964d":"markdown","81590f8e":"markdown","06805975":"markdown","7d21e006":"markdown","6898ed8c":"markdown","9a2edcb0":"markdown","f23d806f":"markdown","611a77b3":"markdown","2d926664":"markdown","987a5c93":"markdown","29eb6971":"markdown","e66a1731":"markdown","403bd490":"markdown","d6e86e20":"markdown","29b0b02d":"markdown","a44ba6ac":"markdown","6a8b645f":"markdown","87f7c54b":"markdown","4b2c31dc":"markdown","c43a1ef8":"markdown","bf7f0902":"markdown","aa9b65c7":"markdown","fd0a05dc":"markdown","c287b3a5":"markdown","95158139":"markdown","2037090a":"markdown","070d399a":"markdown","2005d1c6":"markdown","6bd95e2e":"markdown","a4c9e3b0":"markdown","5ed85b53":"markdown","72aac128":"markdown","e02e84e8":"markdown","14b03164":"markdown","c2a7c015":"markdown","c30d7e4f":"markdown","965c1f97":"markdown","8c4393d4":"markdown","236d2c27":"markdown","456a031e":"markdown","3d58e383":"markdown","a418e8b6":"markdown","10d2e26e":"markdown","2f2de9e3":"markdown","52018d81":"markdown","938d6215":"markdown","ea8c0c9f":"markdown","1934249c":"markdown","61a34fc7":"markdown","0068ffca":"markdown","d8c3c72b":"markdown"},"source":{"e84d4cda":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display\nfrom IPython.html import widgets\nfrom ipywidgets import interact, interactive, fixed, interact_manual","d5b0a32c":"df = pd.read_csv('..\/input\/auto_clean.csv')\ndf.head()","d34ba413":"from sklearn.linear_model import LinearRegression","d0193377":"lm = LinearRegression()\nlm","a81f5644":"X = df[['highway-mpg']]\nY = df['price']","bbbda688":"lm.fit(X,Y)","aeefe38f":"Yhat = lm.predict(X)\nYhat[0:5]   ","56a81f60":"lm.intercept_","aedf55c7":"lm.coef_","4cbb7eb2":"Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]","699cc29e":"lm.fit(Z, df['price'])","edb36378":"lm.intercept_","53a34c43":"lm.coef_","b2488d60":"import seaborn as sns\n%matplotlib inline ","ac034d57":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\nsns.regplot(x=\"highway-mpg\", y=\"price\", data=df)\nplt.ylim(0,)","eec227fa":"plt.figure(figsize=(width, height))\nsns.regplot(x=\"peak-rpm\", y=\"price\", data=df)\nplt.ylim(0,)","3f3dbaf2":"#The variable \"peak-rpm\" has a stronger correlation with \"price\", \n# it is approximate -0.704692  compared to   \"highway-mpg\" which is approximate -0.101616.\n\ndf[[\"peak-rpm\",\"highway-mpg\",\"price\"]].corr()","ed29dad1":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\nsns.residplot(df['highway-mpg'], df['price'])\nplt.show()","7b6829cd":"Y_hat = lm.predict(Z)","12ad84db":"plt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(Yhat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of Cars')\n\nplt.show()\nplt.close()","bb33468d":"def PlotPolly(model, independent_variable, dependent_variabble, Name):\n    x_new = np.linspace(15, 55, 100)\n    y_new = model(x_new)\n\n    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n    ax = plt.gca()\n    ax.set_facecolor((0.898, 0.898, 0.898))\n    fig = plt.gcf()\n    plt.xlabel(Name)\n    plt.ylabel('Price of Cars')\n\n    plt.show()\n    plt.close()","9d780bdc":"x = df['highway-mpg']\ny = df['price']","7cd5bd1f":"# Here we use a polynomial of the 3rd order (cubic) \nf = np.polyfit(x, y, 3)\np = np.poly1d(f)\nprint(p)","114cd529":"PlotPolly(p, x, y, 'highway-mpg')","9b705045":"np.polyfit(x, y, 3)","19d0bfa0":"from sklearn.preprocessing import PolynomialFeatures","8ec6e18d":"pr = PolynomialFeatures(degree=2)\npr","0a69fad2":"Z_pr = pr.fit_transform(Z)","1ebd7908":"Z.shape","7d5bee1e":"Z_pr.shape","7bc0acee":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler","26e294cd":"Input=[('scale', StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), \n       ('model', LinearRegression())]","d01dabe5":"pipe = Pipeline(Input)\npipe","738dbc87":"pipe.fit(Z,y)","b4b88db1":"ypipe=pipe.predict(Z)\nypipe[0:4]","6ee2ff31":"# highway_mpg_fit\nlm.fit(X, Y)\n\n# Find the R^2\nprint('The R-square is: ', lm.score(X, Y))","173b75a1":"Yhat = lm.predict(X)\nprint('The output of the first four predicted value is: ', Yhat[0:4])","02e296ba":"from sklearn.metrics import mean_squared_error","2e7194cd":"mse = mean_squared_error(df['price'], Yhat)\nprint('The mean square error of price and predicted value is: ', mse)","2d6e3e3c":"# fit the model \nlm.fit(Z, df['price'])\n\n# Find the R^2\nprint('The R-square is: ', lm.score(Z, df['price']))","1a71bce4":"Y_predict_multifit = lm.predict(Z)","266a65c0":"print('The mean square error of price and predicted value using multifit is: ', \\\n      mean_squared_error(df['price'], Y_predict_multifit))","bfc4b575":"from sklearn.metrics import r2_score","f3277f22":"r_squared = r2_score(y, p(x))\nprint('The R-square value is: ', r_squared)","c6914758":"mean_squared_error(df['price'], p(x))","d5c35160":"new_input = np.arange(1, 100, 1).reshape(-1, 1)","4820fbe2":"lm.fit(X, Y)\nlm","46ea114a":"yhat=lm.predict(new_input)\nyhat[0:5]","4bb995ee":"plt.plot(new_input, yhat)\nplt.show()","ae049123":"df._get_numeric_data().head()","9b05466e":"# First lets only use numeric data\ndf = df._get_numeric_data()","eb29eb8c":"def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(Title)\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()","b7a09339":"def PollyPlot(xtrain, xtest, y_train, y_test, lr, poly_transform):\n    width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n\n    #training data \n    #testing data \n    # lr:  linear regression object \n    #poly_transform:  polynomial transformation object \n    \n    xmax=max([xtrain.values.max(), xtest.values.max()])\n    xmin=min([xtrain.values.min(), xtest.values.min()])\n    x=np.arange(xmin, xmax, 0.1)\n    \n    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n    plt.plot(xtest, y_test, 'go', label='Test Data')\n    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), \n             label='Predicted Function')\n    plt.ylim([-10000, 60000])\n    plt.ylabel('Price')\n    plt.legend()","3ed76167":"y_data = df['price']","f9ff8bb9":"x_data=df.drop('price', axis=1)","9e10b56c":"from sklearn.model_selection import train_test_split\n\n\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, \n                                                    random_state=1)\n\n\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\", x_train.shape[0])\n","4ad4efd7":"lre = LinearRegression()","e7df3b37":"lre.fit(x_train[['horsepower']], y_train)","df58c110":"lre.score(x_test[['horsepower']], y_test)","05c3c48a":"lre.score(x_train[['horsepower']], y_train)","0f474956":"from sklearn.model_selection import cross_val_score","f9b627f9":"Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)","c01580e4":"Rcross","aa9a7804":"print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())","1fa3518b":"-1 * cross_val_score(lre,x_data[['horsepower']], y_data, cv=4, scoring='neg_mean_squared_error')","3a0edd84":"from sklearn.model_selection import cross_val_predict","d65a461e":"yhat = cross_val_predict(lre, x_data[['horsepower']], y_data,cv=4)\nyhat[0:5]","3f5d9a80":"lr = LinearRegression()\nlr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)","df065030":"yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]","ed5ef152":"yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]","36b5582b":"Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\nDistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)","31903253":"Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\nDistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)","e4df5e6c":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)","120e1965":"pr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(x_train[['horsepower']])\nx_test_pr = pr.fit_transform(x_test[['horsepower']])\npr","eb324215":"poly = LinearRegression()\npoly.fit(x_train_pr, y_train)","311c7b08":"yhat = poly.predict(x_test_pr)\nyhat[0:5]","d118ecbe":"print(\"Predicted values:\", yhat[0:4])\nprint(\"True values:\", y_test[0:4].values)","3700c308":"PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)","7bef511c":"poly.score(x_train_pr, y_train)","73dd7620":"poly.score(x_test_pr, y_test)","0730f4cf":"Rsqu_test = []\n\norder = [1, 2, 3, 4]\nfor n in order:\n    pr = PolynomialFeatures(degree=n)\n    \n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n    \n    lr.fit(x_train_pr, y_train)\n    Rsqu_test.append(lr.score(x_test_pr, y_test))\n\nplt.plot(order, Rsqu_test)\nplt.xlabel('order')\nplt.ylabel('R^2')\nplt.title('R^2 Using Test Data')\nplt.text(3, 0.75, 'Maximum R^2 ')    ","79176809":"def f(order, test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr = PolynomialFeatures(degree=order)\n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n    poly = LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)","d831a354":"interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))","e1d8b225":"pr = PolynomialFeatures(degree=2)\nx_train_pr = pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', \n                                       'highway-mpg','normalized-losses','symboling']])\nx_test_pr = pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', \n                                     'highway-mpg','normalized-losses','symboling']])","99df4549":"from sklearn.linear_model import Ridge","a095eaad":"RigeModel = Ridge(alpha=0.1)","7a5b1d4f":"RigeModel.fit(x_train_pr, y_train)","e4d502d7":"yhat = RigeModel.predict(x_test_pr)","fc787573":"print('predicted:', yhat[0:4])\nprint('test set :', y_test[0:4].values)","bb233c60":"Rsqu_test = []\nRsqu_train = []\ndummy1 = []\nALFA = 10 * np.array(range(0,1000))\nfor alfa in ALFA:\n    RigeModel = Ridge(alpha=alfa) \n    RigeModel.fit(x_train_pr, y_train)\n    Rsqu_test.append(RigeModel.score(x_test_pr, y_test))\n    Rsqu_train.append(RigeModel.score(x_train_pr, y_train))","2b066582":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\nplt.plot(ALFA,Rsqu_test, label='validation data  ')\nplt.plot(ALFA,Rsqu_train, 'r', label='training Data ')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.legend()","e33830dd":"from sklearn.model_selection import GridSearchCV","fc529df7":"parameters1 = [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\nparameters1","3384a986":"RR = Ridge()\nRR","57921734":"Grid1 = GridSearchCV(RR, parameters1, cv=4)","f17e75bb":"Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)","c0223123":"BestRR = Grid1.best_estimator_\nBestRR","9547ee04":"BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)","0f11a567":"parameters2 = [{'alpha': [0.001,0.1,1, 10, 100, 1000,10000,100000,100000],'normalize':[True,False]} ]\nGrid2 = GridSearchCV(Ridge(), parameters2, cv=4)\nGrid2.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']],y_data)\nGrid2.best_estimator_","b2f0c4c2":"Grid2.best_estimator_.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)","215bbe29":"We create a <b>PolynomialFeatures<\/b> object of degree 2: ","2d54b530":"Let's Calculate the R^2 on the test data:","abc248d2":"We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'. ","2a16b521":"<p>Comparing Figure 1 and Figure 2; it is evident the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent where the ranges are from 5000 to 15 000. This is where the distribution shape is exceptionally different. Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.<\/p>","4202cdb8":"<h3>Multiple Linear Regression<\/h3>\n\n<p>What if we want to predict car price using more than one variable?<\/p>\n\n<p>If we want to use more variables in our model to predict car price, we can use <b>Multiple Linear Regression<\/b>.\nMultiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between one continuous response (dependent) variable and <b>two or more<\/b> predictor (independent) variables.\nMost of the real-world regression models involve multiple predictors. We will illustrate the structure by using four predictor variables, but these results can generalize to any integer:<\/p>\n\n$$\nY: Response \\ Variable\\\\\nX_1 :Predictor\\ Variable \\ 1\\\\\nX_2: Predictor\\ Variable \\ 2\\\\\nX_3: Predictor\\ Variable \\ 3\\\\\nX_4: Predictor\\ Variable \\ 4\\\\\n$$\n\n$$\na: intercept\\\\\nb_1 :coefficients \\ of\\ Variable \\ 1\\\\\nb_2: coefficients \\ of\\ Variable \\ 2\\\\\nb_3: coefficients \\ of\\ Variable \\ 3\\\\\nb_4: coefficients \\ of\\ Variable \\ 4\\\\\n$$\n\nThe equation is given by\n\n$$\nYhat = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4\n$$\n\n<p>From the previous section  we know that other good predictors of price could be:<\/p>\n<ul>\n    <li>Horsepower<\/li>\n    <li>Curb-weight<\/li>\n    <li>Engine-size<\/li>\n    <li>Highway-mpg<\/li>\n<\/ul>\nLet's develop a model using these variables as the predictor variables.","abc532f4":" Let's import  <b>Ridge<\/b>  from the module <b>linear models<\/b>.","7f7d69fc":"<p style=\"text-align:center\"><i>Figure 2: Plot of predicted value using the test data compared to the test data.<\/i><\/p> ","5870aeca":" We can calculate the average and standard deviation of our estimate:","a3e8689b":"## Conclusions\n\nI hope you enjoyed my kernel. I would like to hear your thoughts on my kernels if you liked it!","dd42a610":"We can see the output of our model using the method  \"predict.\" then assign the values to \"yhat\".","5528498f":"<h4>How could Highway-mpg help us predict car price?<\/h4>\n\nFor this example, we want to look at how highway-mpg can help us predict car price.\nUsing simple linear regression, we will create a linear function with \"highway-mpg\" as the predictor variable and the \"price\" as the response variable.","e94a7147":"<h3>Decision Making: Determining a Good Model Fit<\/h3>\n\n<p>Now that we have visualized the different models, and generated the R-squared and MSE values for the fits, how do we determine a good model fit?\n<ul>\n    <li><i>What is a good R-squared value?<\/i><\/li>\n<\/ul>\n<\/p>\n\n<p>When comparing models, <b>the model with the higher R-squared value is a better fit<\/b> for the data.\n<ul>\n    <li><i>What is a good MSE?<\/i><\/li>\n<\/ul>\n<\/p>\n\n<p>When comparing models, <b>the model with the smallest MSE value is a better fit<\/b> for the data.<\/p>\n\n\n<h4>Let's take a look at the values for the different models.<\/h4>\n<p>Simple Linear Regression: Using Highway-mpg as a Predictor Variable of Price.\n<ul>\n    <li>R-squared: 0.49659118843391759<\/li>\n    <li>MSE: 3.16 x10^7<\/li>\n<\/ul>\n<\/p>\n    \n<p>Multiple Linear Regression: Using Horsepower, Curb-weight, Engine-size, and Highway-mpg as Predictor Variables of Price.\n<ul>\n    <li>R-squared: 0.80896354913783497<\/li>\n    <li>MSE: 1.2 x10^7<\/li>\n<\/ul>\n<\/p>\n    \n<p>Polynomial Fit: Using Highway-mpg as a Predictor Variable of Price.\n<ul>\n    <li>R-squared: 0.6741946663906514<\/li>\n    <li>MSE: 2.05 x 10^7<\/li>\n<\/ul>\n<\/p>","0bcf3ae7":"<h3>Simple Linear Model (SLR) vs Polynomial Fit<\/h3>\n\n<ul>\n    <li><b>MSE<\/b>: We can see that Polynomial Fit brought down the MSE, since this MSE is smaller than the one from the SLR.<\/li> \n    <li><b>R-squared<\/b>: The R-squared for the Polyfit is larger than the R-squared for the SLR, so the Polynomial Fit also brought up the R-squared quite a bit.<\/li>\n<\/ul>\n<p>Since the Polynomial Fit resulted in a lower MSE and a higher R-squared, we can conclude that this was a better fit model than the simple linear regression for predicting Price with Highway-mpg as a predictor variable.<\/p>","1fb01c7b":"Produce a prediction","44114f24":"<h2>5) Prediction and Decision Making<\/h2>\n<h3>Prediction<\/h3>\n\n<p>In the previous section, we trained the model using the method <b>fit<\/b>. Now we will use the method <b>predict<\/b> to produce a prediction.\n\nCreate a new input ","c51b8345":"Like regular regression, you can fit the model using the method <b>fit<\/b>.","88e2e236":"<h2>Part 2: Overfitting, Underfitting and Model Selection<\/h2>\n\n<p>It turns out that the test data sometimes referred to as the out of sample data is a much better measure of how well your model performs in the real world.  One reason for this is overfitting; let's go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.<\/p>\n\nLet's create Multiple linear regression objects and train the model using <b>'horsepower'<\/b>, <b>'curb-weight'<\/b>, <b>'engine-size'<\/b> and <b>'highway-mpg'<\/b> as features.","274cf44a":" Let's plot the function ","e2fc4e58":"lets get the variables","d44c3358":"<i>What is this plot telling us?<\/i>\n\n<p>We can see from this residual plot that the residuals are not randomly spread around the x-axis, which leads us to believe that maybe a non-linear model is more appropriate for this data.<\/p>","fb99995b":"<p>We can already see from plotting that this polynomial model performs better than the linear model. This is because the generated polynomial function  \"hits\" more of the data points.<\/p>\n\n<p>The analytical expression for Multivariate Polynomial function gets complicated. For example, the expression for a second-order (degree=2)polynomial with two variables is given by:<\/p>\n\n$$\nYhat = a + b_1 X_1 +b_2 X_2 +b_3 X_1 X_2+b_4 X_1^2+b_5 X_2^2\n$$\n\nWe can perform a polynomial transform on multiple features. First, we import the module:","e531c910":"<p>Comparing the regression plot of \"peak-rpm\" and \"highway-mpg\" we see that the points for \"highway-mpg\" are much closer to the generated line and on the average decrease. The points for \"peak-rpm\" have more spread around the predicted line, and it is much harder to determine if the points are decreasing or increasing as the \"highway-mpg\" increases.<\/p>","e4095aa6":"We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87.  The lower the R^2, the worse the model, a Negative R^2 is a sign of overfitting.","c17b6e0f":"Let's fit the polynomial using the function <b>polyfit<\/b>, then use the function <b>poly1d<\/b> to display the polynomial function.","60987361":"Fit the linear model using highway-mpg.","2b12ca7f":"We can also add more parameters to GridSearch as this will try all combinations of different parameters and return the best result for us","c6b65068":"So far the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values. ","9c832113":"<h3>Simple Linear Regression model (SLR) vs Multiple Linear Regression model (MLR)<\/h3>\n\n<p>Usually, the more variables you have, the better your model is at predicting, but this is not always true. Sometimes you may not have enough data, you may run into numerical problems, or many of the variables may not be useful and or even act as noise. As a result, you should always check the MSE and R^2.<\/p>\n\n<p>So to be able to compare the results of the MLR vs SLR models, we look at a combination of both the R-squared and MSE to make the best conclusion about the fit of the model.\n<ul>\n    <li><b>MSE<\/b>The MSE of SLR is  3.16x10^7  while MLR has an MSE of 1.2 x10^7.  The MSE of MLR is much smaller.<\/li>\n    <li><b>R-squared<\/b>: In this case, we can also see that there is a big difference between the R-squared of the SLR and the R-squared of the MLR. The R-squared for the SLR (~0.497) is very small compared to the R-squared for the MLR (~0.809).<\/li>\n<\/ul>\n<\/p>\n\nThis R-squared in combination with the MSE show that MLR seems like the better model fit in this case, compared to SLR.","eb0710d5":"<p style=\"text-align:center\"><i>Figure 1: Plot of predicted values using the training data compared to the training data.<\/i><\/p> ","1ea04c52":"We create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.","cb073030":"We see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points. \n\n R^2 of the training data:","a183db0b":"<h3>Multiple Linear Regression (MLR) vs Polynomial Fit<\/h3>\n\n<ul>\n    <li><b>MSE<\/b>: The MSE for the MLR is smaller than the MSE for the Polynomial Fit.<\/li>\n    <li><b>R-squared<\/b>: The R-squared for the MLR is also much larger than for the Polynomial Fit.<\/li>\n<\/ul>","8531eeaa":"We can say that ~ 67.419 % of the variation of price is explained by this polynomial fit","32215160":"<h4>What is the value of the intercept (a)?<\/h4>","fd747685":"<h3>Model 2: Multiple Linear Regression<\/h3>\n\nLet's calculate the R^2","bcca9d7c":" Fit the model ","0474fff2":" Sometimes you do not have sufficient testing data; as a result, you may want to perform Cross-validation. Let's  go over several methods that you can use for  Cross-validation. ","c5fac86f":"<h2>3) Polynomial Regression and Pipelines<\/h2>\n\n<p><b>Polynomial regression<\/b> is a particular case of the general linear regression model or multiple linear regression models.<\/p> \n<p>We get non-linear relationships by squaring or setting higher-order terms of the predictor variables.<\/p>\n\n<p>There are different orders of polynomial regression:<\/p>\n\n<center><b>Quadratic - 2nd order<\/b><\/center>\n$$Yhat = a + b_1 X^2 +b_2 X^2\\\\$$  \n  \n<center><b>Cubic - 3rd order<\/b><\/center>\n$$Yhat = a + b_1 X^2 +b_2 X^2 +b_3 X^3\\\\$$\n\n<center><b>Higher order<\/b><\/center>\n$$Y = a + b_1 X^2 +b_2 X^2 +b_3 X^3 ....\\\\$$\n\n<p>We saw earlier that a linear model did not provide the best fit while using highway-mpg as the predictor variable. Let's see if we can try fitting a polynomial model to the data instead.<\/p>\n\n<p>We will use the following function to plot the data:<\/p>","e7f14142":"<h2>Part 3: Ridge regression<\/h2> ","876b5866":"Let's create a Ridge regression object, setting the regularization parameter to 0.1 ","e13bed23":"Create a ridge grid search object ","cc3e6ad6":"<h3>MSE<\/h3>\n\nWe can also calculate the MSE:  ","2d7ceb94":"<h2>Pipeline<\/h2>\n\n<p>Data Pipelines simplify the steps of processing the data. We use the module <b>Pipeline<\/b> to create a pipeline. We also use <b>StandardScaler<\/b> as a step in our pipeline.<\/p>","e281e1a4":"<h3>What is the final estimated linear model we get?<\/h3>\n\nAs we saw above, we should get a final linear model with the structure:\n\n$$\nYhat = a + b  X\n$$\n\nPlugging in the actual values we get:\n\n<b>price<\/b> = 38423.31 - 821.73 x  <b>highway-mpg<\/b>","6f8e5c9f":"<h4>What is the value of the Slope (b)?<\/h4>","94bcb83d":"We will perform a degree 5 polynomial transformation on the feature <b>'horse power'<\/b>. ","c051c467":"<h2>4) Measures for In-Sample Evaluation<\/h2>\n\n<p>When evaluating our models, not only do we want to visualize the results, but we also want a quantitative measure to determine how accurate the model is.<\/p>\n\n<p>Two very important measures that are often used in Statistics to determine the accuracy of a model are:<\/p>\n<ul>\n    <li><b>R^2 \/ R-squared<\/b><\/li>\n    <li><b>Mean Squared Error (MSE)<\/b><\/li>\n<\/ul>\n    \n<b>R-squared<\/b>\n\n<p>R squared, also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.<\/p>\n    \n<p>The value of the R-squared is the percentage of variation of the response variable (y) that is explained by a linear model.<\/p>\n\n\n<b>Mean Squared Error (MSE)<\/b>\n\n<p>The Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value (y) and the estimated value (\u0177).<\/p>","3a05af71":"Fit the linear model using the four above-mentioned variables.","e16103ca":"We can say that ~ 49.659% of the variation of the price is explained by this simple linear model \"horsepower_fit\".\n\nLet's calculate the MSE\n\nWe can predict the output i.e., \"yhat\" using the predict method, where X is the input variable:","e554fb8b":"<h2>2)  Model Evaluation using Visualization<\/h2>","4ab24fab":"Now that we've developed some models, how do we evaluate our models and how do we choose the best one? One way to do this is by using visualization.","53a5c8ce":" we compare the predicted results with the actual results ","32b800dc":"We create a dictionary of parameter values:","42aa9863":"The red line in figure 6 represents the  R^2 of the test data, as Alpha increases the R^2 decreases; therefore as Alfa increases the model performs worse on the test data.  The blue line represents the R^2 on the validation data, as the value for Alfa increases the R^2 decreases.   ","df899541":"drop price data in x data","e12bd26b":"<p>In this section, we will develop several models that will predict the price of the car using the variables or features. This is just an estimate but should give us an objective idea of how much the car should cost.<\/p>","df7244db":"<b>Given the regression plots above is \"peak-rpm\" or \"highway-mpg\" more strongly correlated with \"price\"?.<\/b>","2bb7f09d":"Some questions we want to ask in this kernel\n<ul>\n    <li>do I know if the dealer is offering fair value for my trade-in?<\/li>\n    <li>do I know if I put a fair value on my car?<\/li>\n<\/ul>\n<p>Data Analytics, we often use <b>Model Development<\/b> to help us predict future observations from the data we have.<\/p>\n\n<p>A Model will help us understand the exact relationship between different variables and how these variables are used to predict the result.<\/p>","f717e648":"Create a ridge regions object:","3ae0943f":"<h4>Lets load the modules for linear regression<\/h4>","7a0d12c9":"<h2>Conclusions on Model Development:<\/h2>\n\n<p>Comparing these three models, we conclude that <b>the MLR model is the best model<\/b> to be able to predict price from our dataset. This result makes sense, since we have 27 variables in total, and we know that more than one of those variables are potential predictors of the final car price.<\/p>","620aa78b":"We see the R^2 gradually increases until an order three polynomial is used. Then the  R^2 dramatically decreases at four. Let's create a template function which allows us to experiment on different polynomial features.","6afddd99":"<h3>Model 3: Polynomial Fit<\/h3>\n\nLet's calculate the R^2\n\nlet\u2019s import the function <b>r2_score<\/b> from the module <b>metrics<\/b> as we are using a different function","aa493910":"The following interface allows you to experiment with different polynomial orders and different amounts of data. ","7610f219":"<h3>Regression Plot<\/h3>\n\n<p>When it comes to simple linear regression, an excellent way to visualize the fit of our model is by using <b>regression plots<\/b>.<\/p>\n\n<p>This plot will show a combination of a scattered data points (a <b>scatter plot<\/b>), as well as the fitted <b>linear regression<\/b> line going through the data. This will give us a reasonable estimate of the relationship between the two variables, the strength of the correlation, as well as the direction (positive or negative correlation).<\/p>\n\n Let's visualize Horsepower as potential predictor variable of price:","20394fbe":"Prediction using test data: ","f5b093ba":"we input the list as an argument to the pipeline constructor ","e4f78c7c":"What are the values of the coefficients (b1, b2, b3, b4)?","ec0d964d":" We can output a prediction ","81590f8e":"<b>Price<\/b> = -15678.742628061467 + 52.65851272 x <b>horsepower<\/b> + 4.69878948 x <b>curb-weight<\/b> + 81.95906216 x <b>engine-size<\/b> + 33.58258185 x <b>highway-mpg<\/b>","06805975":"The original data is of 201 samples and 4 features ","7d21e006":"<h3>Multiple Linear Regression<\/h3>\n\n<p>How do we visualize a model for Multiple Linear Regression? This gets a bit more complicated because you can't visualize it with regression or residual plot.<\/p>\n\n<p>One way to look at the fit of the model is by looking at the <b>distribution plot<\/b>: We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.<\/p>\n\nFirst lets make a prediction ","6898ed8c":"<h4>Setup<\/h4>","9a2edcb0":"What is the value of the intercept(a)?","f23d806f":"<p style=\"text-align:center\"><i>Figure 4: A polynomial regression model, red dots represent training data, green dots represent test data, and the blue line represents the model prediction. <\/i><\/p> \n","611a77b3":"<h3>Linear Regression<\/h3>\n\n\n<p>One example of a Data  Model that we will be using is<\/p>\n\n<b>Simple Linear Regression<\/b>.\n\n<br>\n<p>Simple Linear Regression is a method to help us understand the relationship between two variables:<\/p>\n<ul>\n    <li>The predictor\/independent variable (X)<\/li>\n    <li>The response\/dependent variable (that we want to predict)(Y)<\/li>\n<\/ul>\n\n<p>The result of Linear Regression is a <b>linear function<\/b> that predicts the response (dependent) variable as a function of the predictor (independent) variable.<\/p>\n\n\n\n$$\n Y: Response \\ Variable\\\\\n X: Predictor \\ Variables\n$$\n\n\n <b>Linear function:<\/b>\n$$\nYhat = a + b  X\n$$\n\n<ul>\n    <li>a refers to the <b>intercept<\/b> of the regression line0, in other words: the value of Y when X is 0<\/li>\n    <li>b refers to the <b>slope<\/b> of the regression line, in other words: the value with which Y changes when X increases by 1 unit<\/li>\n<\/ul>","2d926664":" In this section, we will review Ridge Regression we will see how the parameter Alfa changes the model. Just a note here our test data will be used as validation data.\n\n Let's perform a degree two polynomial transformation on our data. ","987a5c93":"<h2>Part 4: Grid Search<\/h2>\n\nThe term Alfa is a hyperparameter, sklearn has the class  <b>GridSearchCV<\/b> to make the process of finding the best hyperparameter simpler.\n\nLet's import <b>GridSearchCV<\/b> from  the module <b>model_selection<\/b>.","29eb6971":"The default scoring is R^2; each element in the array has the average  R^2 value in the fold:","e66a1731":"Let's see how the R^2 changes on the test data for different order polynomials and plot the results:","403bd490":"We can plot out the value of R^2 for different Alphas ","d6e86e20":"You can also use the function 'cross_val_predict' to predict the output. The function splits up the data into the specified number of folds, using one fold to get a prediction while the rest of the folds are used as test data. First import the function:","29b0b02d":"As we saw above, we should get a final linear function with the structure:\n\n$$\nYhat = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4\n$$\n\nWhat is the linear function we get in this example?","a44ba6ac":"We can normalize the data,  perform a transform and fit the model simultaneously. ","6a8b645f":"<p>We can see from this plot that price is negatively correlated to highway-mpg, since the regression slope is negative.\nOne thing to keep in mind when looking at a regression plot is to pay attention to how scattered the data points are around the regression line. This will give you a good indication of the variance of the data, and whether a linear model would be the best fit or not. If the data is too far off from the line, this linear model might not be the best model for this data. Let's compare this plot to the regression plot of \"peak-rpm\".<\/p>","87f7c54b":"We input the object, the feature in this case <b>'horsepower'<\/b> , the target data <b>y_data<\/b>. The parameter 'cv' determines the number of folds; in this case 4. We can produce an output:","4b2c31dc":"<h4>Overfitting<\/h4>\n<p>Overfitting occurs when the model fits the noise, not the underlying process. Therefore when testing your model using the test-set, your model does not perform as well as it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.<\/p>\n\nLet's use 55 percent of the data for testing and the rest for training:","c43a1ef8":"The <b>test_size<\/b> parameter sets the proportion of data that is split into the testing set. In the above, the testing set is set to 10% of the total dataset. \n\n We create a Linear Regression object:","bf7f0902":"after the transformation, there 201 samples and 15 features","aa9b65c7":"we compare the predicted results with the actual results ","fd0a05dc":"we can plot the data ","c287b3a5":" R^2 of the test data:","95158139":" What is the final estimated linear model that we get?","2037090a":"<h4>Create the linear regression object<\/h4>","070d399a":"Let's take the first five predicted values and compare it to the actual targets. ","2005d1c6":"The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:","6bd95e2e":"<h2>Part 1: Training and Testing<\/h2>\n\n<p>An important step in testing your model is to split your data into training and testing data. We will place the target data <b>price<\/b> in a separate dataframe <b>y<\/b>:<\/p>","a4c9e3b0":"Now let's create a linear regression model \"poly\" and train it.","5ed85b53":"Let's compare the first five predicted samples to our test set ","72aac128":"<p>We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement.<\/p>","e02e84e8":"<h2>Cross-validation Score<\/h2>\n\nLets import <b>model_selection<\/b> from the module <b>cross_val_score<\/b>.","14b03164":"lets import the function <b>mean_squared_error<\/b> from the module <b>metrics<\/b>","c2a7c015":"We select the value of Alfa that minimizes the test error, for example, we can use a for loop. ","c30d7e4f":"<h2>1. Linear Regression and Multiple Linear Regression<\/h2>","965c1f97":"Now we randomly split our data into training and testing data  using the function <b>train_test_split<\/b>. ","8c4393d4":"we can see the R^2 is much smaller using the test data.","236d2c27":"<h1>Model Development<\/h1>","456a031e":"We input the object, the feature in this case ' horsepower', the target data (y_data). The parameter 'cv'  determines the number of folds; in this case 4. ","3d58e383":"Prediction using training data:","a418e8b6":" Similarly,  we can normalize the data, perform a transform and produce a prediction  simultaneously","10d2e26e":"We can say that ~ 80.896 % of the variation of price is explained by this multiple linear regression \"multi_fit\".\n\nLet's calculate the MSE\n\n we produce a prediction ","2f2de9e3":"<h4>Model 1: Simple Linear Regression<\/h4>\n\nLet's calculate the R^2","52018d81":"We apply the function to get the value of r^2","938d6215":"Let's perform some model evaluation using our training and testing data separately and also examine the distribution of the predicted values of the training data.","ea8c0c9f":" We now test our model on the test data ","1934249c":"### Functions for Plotting","61a34fc7":"<h1> Model Evaluation and Refinement<\/h1>\n\nWe have built models and made predictions of vehicle prices. Now we will determine how accurate these predictions are. ","0068ffca":"<h3>Residual Plot<\/h3>\n\n<p>A good way to visualize the variance of the data is to use a residual plot.<\/p>\n\n<p>What is a <b>residual<\/b>?<\/p>\n\n<p>The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.<\/p>\n\n<p>So what is a <b>residual plot<\/b>?<\/p>\n\n<p>A residual plot is a graph that shows the residuals on the vertical y-axis and the independent variable on the horizontal x-axis.<\/p>\n\n<p>What do we pay attention to when looking at a residual plot?<\/p>\n\n<p>We look at the spread of the residuals:<\/p>\n\n<p>- If the points in a residual plot are <b>randomly spread out around the x-axis<\/b>, then a <b>linear model is appropriate<\/b> for the data. Why is that? Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.<\/p>","d8c3c72b":"we fit the model using the feature horsepower "}}