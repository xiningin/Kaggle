{"cell_type":{"954c1108":"code","66e20f1c":"code","a937c245":"code","37ddd006":"code","900b0432":"code","ed692e0d":"code","f2a319bf":"code","e86190f8":"code","5583af92":"code","03148e52":"code","130dc3ac":"code","83d11332":"code","35cbb16e":"code","e747769e":"code","9151460e":"code","e35fbd0f":"code","13adc01b":"code","cd4fce3a":"code","21df9c14":"code","7a2f9475":"code","9f974984":"code","5b4cbeef":"code","377e3a59":"code","7073de29":"code","ec9bf8c3":"code","535a8050":"code","2bab4cf0":"code","f1f65229":"code","2dd0fbd0":"code","7eca70e9":"code","3529e6f2":"code","73b5e5ef":"code","9377be47":"code","c2f497d8":"code","5c6e4051":"code","420d0181":"code","83444029":"code","829e9b90":"code","6a436db7":"code","1e24b8e1":"code","f242fa3f":"code","cc3596a2":"code","67f6d61c":"code","af09671d":"code","6c2f71ce":"code","611adf8a":"code","cef4af78":"code","e072be33":"code","53511eac":"code","53e4651d":"code","471272cd":"code","a4d4ecb8":"code","c6b07847":"code","4f7dc8ae":"code","e097032c":"code","c81af6a1":"code","688f9502":"code","706608f9":"code","38b0862f":"code","7f47210d":"code","43636b32":"code","e375bca4":"code","4c151493":"code","c76b1187":"code","b9832545":"code","8028892b":"code","9de8e770":"code","3c3750cf":"code","c9473313":"code","d93afcf3":"code","87462f4e":"code","3625a2b0":"code","59896029":"code","9bbf1fbd":"code","dedf4068":"code","fa24aeea":"code","3f13c264":"code","542d8fe8":"code","3690797a":"code","8b04097d":"code","4ec4bfb5":"code","efbf0d84":"code","71a6d46b":"code","3bc37b65":"code","814d9de6":"code","3c0f60a4":"code","056dd5d2":"code","cb3b3d16":"code","d1ae270d":"code","de46dfa2":"code","ee8adbfc":"code","ff205264":"code","9601ea26":"code","fc0791d6":"code","2dafab21":"code","fe5c8105":"code","8c7e4b88":"code","c36d00eb":"code","a0b1ec7f":"code","ea420eac":"code","cd1045cb":"code","465ff884":"code","000b1ff9":"code","8b5bc602":"code","3c6296d6":"code","672e64ef":"code","99a09fcd":"code","5127b097":"code","29323187":"code","7799a760":"code","7bc7a6f8":"code","57f4c953":"code","cf4d5984":"code","8513c2e2":"code","21fd2f0e":"code","3cb3b789":"code","580aae49":"code","fd3f4f5c":"code","b2deb1d7":"code","ab8ed1e4":"code","a90ac56f":"code","3640aca8":"markdown","004936cd":"markdown","a9b18561":"markdown","4fe07fe3":"markdown","1d531659":"markdown","3c982ce0":"markdown","e5123527":"markdown","5dcfd293":"markdown","5c8426ae":"markdown","8e08c74a":"markdown","a49f60c6":"markdown","f8cca858":"markdown","6ce69d7a":"markdown","dda2ffd5":"markdown","a2eb0f5f":"markdown","74a6a046":"markdown","7c98efdc":"markdown","6e5b44f7":"markdown","611fe031":"markdown","d5ee236e":"markdown","ea2c45a6":"markdown","4694f19f":"markdown","988183b7":"markdown","9a3e5a20":"markdown","07791e3f":"markdown","37090209":"markdown","77449979":"markdown","d823a6dd":"markdown","09f5668d":"markdown","a835c4be":"markdown","af8aaafa":"markdown","9dd0d456":"markdown","1b55b411":"markdown","7292f74b":"markdown","099d2f63":"markdown","3b45d32b":"markdown","9008c26d":"markdown","37224d90":"markdown","aea7cf0d":"markdown","dfddd229":"markdown","f20659cc":"markdown","4c4550e7":"markdown","ae13e72d":"markdown","578cab87":"markdown","745ad1ca":"markdown","c7702ce2":"markdown","b94e7afa":"markdown","8b53647b":"markdown","cc175f63":"markdown","57c26507":"markdown","eac84ed3":"markdown","a86cb256":"markdown","1b2e0ebe":"markdown","68deb37f":"markdown","8dcd016d":"markdown","a6fbd05f":"markdown"},"source":{"954c1108":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66e20f1c":"# importing the libaries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import norm # for scientific computing\nfrom scipy import stats, integrate","a937c245":"# Loading the dataset\nmelbourne_data = pd.read_csv(r\"..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv\")","37ddd006":"melbourne_data.head()","900b0432":"melbourne_data.shape","ed692e0d":"melbourne_data.info()","f2a319bf":"# Verifying columns with object data type\nprint(melbourne_data.select_dtypes([\"object\"]).columns)","e86190f8":"# changing all object data types to category - This step is necessary to be able\n# to plot categorical data for our analysis\n\nobjdtype_cols = melbourne_data.select_dtypes([\"object\"]).columns\nmelbourne_data[objdtype_cols] = melbourne_data[objdtype_cols].astype(\"category\")","5583af92":"melbourne_data.info()","03148e52":"# looking at data information above, we can notice that \"Date\" is also converted\n# to category.\n# in this step we will cast date to datetime\nmelbourne_data[\"Date\"] = pd.to_datetime(melbourne_data[\"Date\"])","130dc3ac":"melbourne_data.info()","83d11332":"# describe command will give us all statistical information about our \n# numerical variables\nmelbourne_data.describe().T","35cbb16e":"melbourne_data[\"Postcode\"] = melbourne_data[\"Postcode\"].astype(\"category\")\nmelbourne_data.describe().T","e747769e":"# in this step we will first confirm our above statement by observing \"Rooms\"\n# and \"Bedroom2\"\n\nmelbourne_data[\"b 2 r\"] = melbourne_data[\"Bedroom2\"] - melbourne_data[\"Rooms\"]\nmelbourne_data[[\"b 2 r\", \"Bedroom2\", \"Rooms\"]].head()","9151460e":"# we can see that the difference is very minimal here that will be wise to \n# remove\n# one of the 2 columns\n\nmelbourne_data = melbourne_data.drop([\"b 2 r\", \"Bedroom2\"],1)","e35fbd0f":"# visualizing missing values\nfig, ax = plt.subplots(figsize=(15,7))\nsns.heatmap(melbourne_data.isnull(), yticklabels=False,cmap=\"viridis\")","13adc01b":"# Percentage of missing values\nmelbourne_data.isnull().sum()\/len(melbourne_data)*100","cd4fce3a":"melbourne_data = melbourne_data.drop([\"Landsize\",\"BuildingArea\",\"YearBuilt\"],\n                                    axis=1)","21df9c14":"# Also since our target variable is price, it makes sense to drop rows for \n# price columns where price values are missing\nmelbourne_data.dropna(subset=[\"Price\"], inplace=True)","7a2f9475":"#from sklearn.preprocessing import Imputer\n#X=melbourne_data[['Bathroom','Car']]\n#imp=Imputer(missing_values='NaN',strategy='median',axis=0)\n#imp.fit(X)\n#X=pd.DataFrame(data=imp.transform(X),columns=X.columns)\n#melbourne_data[['Bathroom','Car']]=X","9f974984":"# filling up the missing data with \"Fillna\"\nmelbourne_data[\"Car\"] = melbourne_data[\"Car\"].fillna(melbourne_data[\"Car\"].\n                                                    mode()[0])\nmelbourne_data[\"Bathroom\"] = melbourne_data[\"Bathroom\"].fillna(melbourne_data\n                                                               [\"Bathroom\"].\n                                                              mode()[0])","5b4cbeef":"melbourne_data.shape","377e3a59":"# Percentage of missing values\nmelbourne_data.isnull().sum()\/len(melbourne_data)*100","7073de29":"melbourne_data.describe().T","ec9bf8c3":"## to findout outliers lets divide data into different price ranges to identify number of occurences of data in different price ranges\nmelbourne_data['PriceRange'] = np.where(melbourne_data['Price'] <= 100000, '0-100,000',  \n                                       np.where ((melbourne_data['Price'] > 100000) & (melbourne_data['Price'] <= 1000000), '100,001 - 1M',\n                                                np.where((melbourne_data['Price'] > 1000000) & (melbourne_data['Price'] <= 3000000), '1M - 3M',\n                                                        np.where((melbourne_data['Price']>3000000) & (melbourne_data['Price']<=5000000), '3M - 5M',\n                                                                np.where((melbourne_data['Price']>5000000) & (melbourne_data['Price']<=6000000), '5M - 6M',\n                                                                        np.where((melbourne_data['Price']>6000000) & (melbourne_data['Price']<=7000000), '6M - 7M',\n                                                                                np.where((melbourne_data['Price']>7000000) & (melbourne_data['Price']<=8000000), '7M-8M', \n                                                                                         np.where((melbourne_data['Price']>8000000) & (melbourne_data['Price']<=9000000), '8M-9M',\n                                                                                                  np.where((melbourne_data['Price']>9000000) & (melbourne_data['Price']<=10000000), '9M-10M', \n                                                                                                         np.where((melbourne_data['Price']>10000000) & (melbourne_data['Price']<=11000000), '10M-11M', \n                                                                                                                 np.where((melbourne_data['Price']>11000000) & (melbourne_data['Price']<=12000000), '11M-12M', '')\n                                                                                                                 ))))))))))\n                                                                                                  ","535a8050":"melbourne_data.groupby([\"PriceRange\"]).agg({\"PriceRange\": [\"count\"]})","2bab4cf0":"melbourne_data.info()","f1f65229":"melbourne_data.describe().T","2dd0fbd0":"# Lets drop those outliers\n\nmelbourne_data.drop(melbourne_data[(melbourne_data[\"PriceRange\"] == \"0-100,000\") | \n                                  (melbourne_data[\"PriceRange\"] == \"7M-8M\") | \n                                  (melbourne_data[\"PriceRange\"] == \"8M-9M\") | \n                                  (melbourne_data[\"PriceRange\"] == \"11M-12M\")].\n                   index, inplace=True)","7eca70e9":"melbourne_data.describe().T","3529e6f2":"melbourne_data.groupby([\"Rooms\"])[\"Rooms\"].count()","73b5e5ef":"# drop the outliers in rooms\n\nmelbourne_data.drop(melbourne_data[(melbourne_data[\"Rooms\"] == 12) |\n                                  (melbourne_data[\"Rooms\"] == 16)].index,\n                   inplace=True)","9377be47":"melbourne_data.describe().T","c2f497d8":"# sns.distplot(melbourne_data, kde=False, bins=20).set(xlabel=\"Price\");\nnumerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n# melbourne_data.select_dtypes(include = numerics)\nmelbourne_data.select_dtypes(include = numerics).hist(bins=15, figsize=(15, 6),\n                                                     layout=(2,4))","5c6e4051":"melbourne_data['Distance'] = round(melbourne_data['Distance'])","420d0181":"melbourne_data.shape","83444029":"## extract year from date\nmelbourne_data[\"Year\"] = melbourne_data[\"Date\"].apply(lambda x:x.year)\nmelbourne_data.head(5)","829e9b90":"# data subset by type\n# house price\nmelbourne_data_h = melbourne_data[melbourne_data[\"Type\"]==\"h\"]\n# condo price\nmelbourne_data_u = melbourne_data[melbourne_data[\"Type\"]==\"u\"]\n# townhouse price\nmelbourne_data_t = melbourne_data[melbourne_data[\"Type\"]==\"t\"]\n#house, condo and townhouse price groupby \"year\" and \"mean\"\nmelbourne_data_h_y = melbourne_data_h.groupby(\"Year\").mean()\nmelbourne_data_u_y = melbourne_data_u.groupby(\"Year\").mean()\nmelbourne_data_t_y = melbourne_data_t.groupby(\"Year\").mean()\nmelbourne_data_h_y.head()","6a436db7":"# sns.implot(x=\"Year\", y=\"Price\", hue=\"Type\", data=melbourne_data,\n# x_estimator=np.mean);\nmelbourne_data_h_y[\"Price\"].plot(kind=\"line\", color=\"r\", label=\"House\")\nmelbourne_data_u_y[\"Price\"].plot(kind=\"line\", color=\"g\", label=\"Condo\")\nmelbourne_data_t_y[\"Price\"].plot(kind=\"line\", color=\"b\", label=\"TownHouse\")\nyear_xticks=[2016,2017,2018]\nplt.ylabel(\"Price\")\nplt.xticks( year_xticks)\nplt.title(\"Melboune price trend VS Year per type\")\nplt.legend()","1e24b8e1":"melbourne_data.shape","f242fa3f":"melbourne_data.columns","cc3596a2":"melbourne_data_South_M = melbourne_data[melbourne_data[\"Regionname\"]==\"Southern Metropolitan\"]\nmelbourne_data_South_M_average = melbourne_data_South_M.groupby([\"Year\"])[\"Price\"].mean()\n# Series.to_frame()","67f6d61c":"# create X and y\nX = melbourne_data_South_M[[\"Year\"]]\ny = melbourne_data_South_M[[\"Price\"]]\n\n# instantiate and fit\nlm2 = LinearRegression()\nlm2.fit(X,y)\n\n# print the coefficients\nprint(lm2.intercept_)\nprint(lm2.coef_)","af09671d":"# stats modles\n\n# you have to create a dataframe since the statsmodels formula interface expects \n# it\nX_new = pd.DataFrame({\"Year\": [2019,2020,2021]})\n\n# predict for a new observation\nlm2.predict(X_new)","6c2f71ce":"melbourne_data_SM = melbourne_data[melbourne_data[\"Regionname\"]==\"Southern Metropolitan\"]\nmelbourne_data_SM_u = melbourne_data_SM[melbourne_data_SM[\"Type\"]==\"u\"]\nmelbourne_data_SM_u.shape","611adf8a":"# stats modles\n\n# create a fitted model\nlm1 = smf.ols(formula=\"Price ~ Year\", data=melbourne_data_SM_u).fit()\n\n# print the coefficients\nlm1.params","cef4af78":"# you have to create a dataframe since the statsmodels formular interface\n# expects it\nX_new = pd.DataFrame({\"Year\":[2016,2017,2018,2019,2020,2021]})\n\n# predict for a new observation\nlm1.predict(X_new)","e072be33":"lm1.rsquared","53511eac":"melbourne_data_E = melbourne_data[melbourne_data[\"Regionname\"]==\"Eastern Metropolitan\"]\nmelbourne_data_E_u = melbourne_data_E[melbourne_data_E[\"Type\"]==\"u\"]\nlme = smf.ols(formula=\"Price ~ Year\", data=melbourne_data_E_u).fit()\n\n# print the coefficients\nlme.params","53e4651d":"melbourne_data_E_u.shape","471272cd":"X_new = pd.DataFrame({\"Year\":[2016,2017,2018,2019,2020,2021]})\n\n# predict for a new observations\nlme.predict(X_new)","a4d4ecb8":"#get month information from date \n#df['year_month']=df.datetime_column.apply(lambda x: str(x)[:7])\n#per = df.Date.dt.to_period(\"M\")\n# How many calls, sms, and data entries are in each month?\n#data.groupby(['month', 'item'])\n#df['birthdate'].groupby([df.birthdate.dt.year, df.birthdate.dt.month]).agg('count')\nmelbourne_data['Month']=pd.DatetimeIndex(melbourne_data['Date']).month\n\n#lois[_y_m]=lois['Price'].groupby(['Month']).mean()\n#Prepare data for pie chart to check sales based on month in order to see\n# which month sell most.\nmelbourne_data_2016=melbourne_data[melbourne_data['Year']==2016]\nmelbourne_data_2017=melbourne_data[melbourne_data['Year']==2017]\nmelbourne_data_2018=melbourne_data[melbourne_data['Year']==2018]\nmelbourne_data_2016_count=melbourne_data_2016.groupby(['Month']).count()\nmelbourne_data_2017_count=melbourne_data_2017.groupby(['Month']).count()\nmelbourne_data_2018_count=melbourne_data_2018.groupby(['Month']).count()\nComparison={2016:melbourne_data_2016.shape,2017:melbourne_data_2017.shape,2018:melbourne_data_2018.shape}\nComparison","c6b07847":"label_2016=['January','March','April','May','June','July','August','September',\n            'October','November','December']\nplt.pie(melbourne_data_2016_count['Price'],labels=label_2016,autopct='%.1f %%')\nplt.title(' Sales rate for Year 2016')\nplt.show()","4f7dc8ae":"label_2017=['January','February','March','April','May','June','July','August',\n            'September','October','November','December']\nplt.pie(melbourne_data_2017_count['Price'],labels=label_2017,autopct='%.1f %%')\nplt.title('Sales rate for Year 2017')","e097032c":"label_2018=['January','February','March','June','October']\nplt.pie(melbourne_data_2018_count['Price'],labels=label_2018,autopct='%.1f %%')\nplt.title('Sales rate for Year 2018')","c81af6a1":"# Abbreviate Regionname categories for presentation\nmelbourne_data[\"Regionabb\"] = melbourne_data[\"Regionname\"].map({'Northern Metropolitan':'N Metro',\n                                            'Western Metropolitan':'W Metro', \n                                            'Southern Metropolitan':'S Metro', \n                                            'Eastern Metropolitan':'E Metro', \n                                            'South-Eastern Metropolitan':'SE Metro', \n                                            'Northern Victoria':'N Vic',\n                                            'Eastern Victoria':'E Vic',\n                                            'Western Victoria':'W Vic'})","688f9502":"#sns.lmplot(x=\"Year\", y=\"Price\", hue=\"Type\", data=melbourne_data, col=\"Regionabb\",x_estimator=np.mean, col_wrap=2)\n#plt.ylim(200000,2000000)\n#plt.xlim(2015,2020)","706608f9":"# South region price change VS year per type\nsns.lmplot(x=\"Year\", y=\"Price\", hue=\"Type\", data=melbourne_data[melbourne_data\n                                                                [\"Regionabb\"]==\n                                                                \"S Metro\"],\n           x_estimator=np.mean);","38b0862f":"# East region Price change vs year one type for Condo\nmelbourne_data_S = melbourne_data[melbourne_data[\"Regionabb\"]==\"S Metro\"]\nsns.lmplot(x=\"Year\", y=\"Price\", data=melbourne_data_S[melbourne_data_S[\"Type\"]==\n                                                     \"u\"], x_estimator=\n          np.mean);","7f47210d":"Pct_change = melbourne_data.groupby([\"Year\",\"Regionabb\", \"Type\"],\n                                    as_index=False)[\"Price\"].mean()\nPct_change = Pct_change.sort_values([\"Regionabb\",\"Type\",\n                                     \"Year\"]).set_index(np.arange(len\n                                                                  (Pct_change.\n                                                                  index)))\nPct_change.info()","43636b32":"melbourne_data_count_region_y=melbourne_data.groupby(['Year','Regionabb',\n                                                      'Type'],\n                                                     as_index=\n                                                     False)['Price'].count()\nmelbourne_data_count_region_y = melbourne_data_count_region_y.sort_values(\n    ['Regionabb','Type','Year']).set_index(np.arange(len\n                                                     (melbourne_data_count_region_y.\n                                                      index)))\nmelbourne_data_count_region_y.rename(columns={'Price':'Count'}, inplace=True)","e375bca4":"# define function to get year growth rate against price per region and type\ndef PCTM(gg):\n    df = pd.DataFrame(gg[\"Price\"].pct_change())\n    df[\"Year\"] = gg[\"Year\"]\n    df[\"region\"] =gg[\"Regionabb\"]\n    df[\"Type\"] = gg[\"Type\"]\n    df = df[df[\"Year\"]!=2016]\n    return df","4c151493":"# df2[df2[\"id\"].isin([\"SP.POP.TOTL\", \"NY.GNP.PCAP.CD\"])]\nmelboune_growthrate_y_t = PCTM(Pct_change)\nmelboune_growthrate_y_t1 = melboune_growthrate_y_t[melboune_growthrate_y_t[\"region\"].\n                                                    isin([\"N Metro\",\n                                                          \"S Metro\",\n                                                          \"E Metro\",\n                                                         \"SE Metro\",\n                                                         \"W Metro\",\n                                                         \"S Metro\"])]\nmelboune_growthrate_y_t1.rename(columns={\"Price\":\"Price Growth Rate\"}, \n                                inplace = True)\nmelboune_growthrate_y_t1[melboune_growthrate_y_t1[\"Price Growth Rate\"]>0.05]","c76b1187":"# Sales count for each region\nSales_count=melbourne_data.groupby(['Regionabb'])['Price'].count()\nSales_count.head(10)","b9832545":"Sales_count=melbourne_data.groupby(['Regionabb','Type'])['Price'].count()\nSales_count.nlargest(20)","8028892b":"# define function to get year growth rate against count per region and type\ndef PCTMC(gg):\n    df = pd.DataFrame(gg[\"Count\"].pct_change())\n    df[\"Year\"] = gg[\"Year\"]\n    df[\"region\"] =gg[\"Regionabb\"]\n    df[\"Type\"] = gg[\"Type\"]\n    df = df[df[\"Year\"]!=2016]\n    return df","9de8e770":"#df2[df2['id'].isin(['SP.POP.TOTL','NY.GNP.PCAP.CD'])]\nmelboune_growthrate_y_c=PCTMC(melbourne_data_count_region_y)\nmelboune_growthrate_y_c1=melboune_growthrate_y_c[melboune_growthrate_y_c['region'].isin(['N Metro','S Metro','E Metro','SE Metro','W Metro','S Metro'])]\n\nmelboune_growthrate_y_c1.rename(columns={'Count':'Count Growth Rate'}, inplace=True)\nmelboune_growthrate_y_c1[melboune_growthrate_y_c1['Count Growth Rate']>0.2]","3c3750cf":"melboune_count1=melbourne_data_count_region_y[melbourne_data_count_region_y['Regionabb'].isin(['S Metro','E Metro',\n                                                                                               'SE Metro','W Metro',\n                                                                                               'S Metro','N Metro'])]\nmelboune_count1[melboune_count1['Count']>1000]","c9473313":"sns.boxplot(x=\"Method\", y=\"Price\", data=melbourne_data)\nplt.show()\n# sold method did not affect price","d93afcf3":"#sns.lmplot(x=\"Year\",y=\"Price\",hue=\"Rooms\",data=melbourne_data,\n#          x_estimator=np.mean);","87462f4e":"# insights: Increase distance reduce price\nsns.lmplot(x=\"Distance\", y=\"Price\", data=melbourne_data, x_estimator=np.mean);","3625a2b0":"sns.lmplot(x=\"Car\", y=\"Price\", data=melbourne_data, x_estimator=np.mean);","59896029":"Ideal_House = melbourne_data.groupby([\"Regionabb\",\"Type\",\"Rooms\",\"Bathroom\"])[\n    \"Price\"\n].count()\n\nIdeal_House.loc[[\"S Metro\"],\"h\"].nlargest(10)","9bbf1fbd":"Ideal_House.nlargest(10)","dedf4068":"Ideal_House.loc[[\"E Metro\"],\"u\"].nlargest(10)","fa24aeea":"corrmat = melbourne_data.corr()","3f13c264":"fig, ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corrmat, annot=True, annot_kws={\"size\":12})","542d8fe8":"#define function to refine those correlation more than 0.3 with abs value\ndef getCorrelatedFeature(corrdata,threshold):\n    feature=[]\n    value=[]\n    \n    for i, index in enumerate(corrdata.index):\n        if abs(corrdata[index])>threshold:\n            feature.append(index)\n            value.append(corrdata[index])\n    df=pd.DataFrame(data=value,index=feature,columns=['Corr Value'])\n    return df","3690797a":"threshold=0.3\ncorr_value=getCorrelatedFeature(corrmat['Price'],threshold)\ncorr_value","8b04097d":"melbourne_data.isnull().sum()","4ec4bfb5":"melbourne_data['Type_Code'] = melbourne_data['Type'].map({'h':3,\n                                            't':2, \n                                            'u':1, \n                                            'dev site':0, \n                                            'o res':0, \n                                            'br':0})","efbf0d84":"# Group Regionname categories \nmelbourne_data1 = pd.get_dummies(melbourne_data['Regionabb'],drop_first=False)\nmelbourne_data=pd.concat([melbourne_data,melbourne_data1],axis=1)\nmelbourne_data.columns.values","71a6d46b":"#fig,ax=plt.subplots(figsize=(12,10))\n#df=melbourne_data[['Price','Rooms','Distance', 'Bathroom',  'Year', 'Type_Code','RegionCode']]\n#sns.heatmap(df,annot=True)\n#dff=melbourne_data[['Price','Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code',]].groupby('RegionCode')\n#dff.head()","3bc37b65":"melbourne_data_NN=melbourne_data[['Rooms','Distance', 'Bathroom', 'Car', \n                                  'Year', 'Propertycount','Type_Code',\n                                  'N Metro','W Metro','S Metro','E Metro',\n                                  'SE Metro','N Vic','E Vic',\n                                  'W Vic','Price']].dropna()\nmelbourne_data_NN[['Rooms','Distance', 'Bathroom','Car', 'Year', \n                   'Propertycount','Type_Code','N Metro','W Metro',\n                   'S Metro','E Metro','SE Metro','N Vic','E Vic','W Vic',\n                   'Price']].isnull().sum()","814d9de6":"melbourne_data_NN.shape","3c0f60a4":"#Finding coefficient\n\nX=melbourne_data_NN[['Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code',\n                     'N Metro','W Metro','S Metro','E Metro','SE Metro','N Vic','E Vic','W Vic']]\ny=melbourne_data_NN['Price']","056dd5d2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .20, random_state=5)","cb3b3d16":"# Fit\n# Import model\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression object\nregressor = LinearRegression()\n\n# Fit model to training data\nregressor.fit(X_train,y_train)","d1ae270d":"# Predict\n# Predicting test set results\ny_pred = regressor.predict(X_test)","de46dfa2":"regressor.score(X_test,y_test)","ee8adbfc":"plt.scatter(y_test, y_pred)","ff205264":"# Histogram of the distribution of residuals\nsns.distplot((y_test - y_pred))","9601ea26":"cdf = pd.DataFrame(data = regressor.coef_, index = X.columns,\n                   columns = ['Coefficients'])\ncdf","fc0791d6":"X.head()","2dafab21":"from sklearn.ensemble import RandomForestClassifier\n# model = RandomForestClassifier(n_estimators=20)\n# model.fit(X_train, y_train)\n\nclf = RandomForestClassifier(n_jobs=2, random_state=0)\nclf.fit(X,y)","fe5c8105":"clf.predict(X)","8c7e4b88":"clf.score(X_test, y_test)","c36d00eb":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfolds=StratifiedKFold(n_splits=3)","a0b1ec7f":"def get_score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    return model.score(X_test, y_test)","ea420eac":"print(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test))","cd1045cb":"print(get_score(LinearRegression(), X_train, X_test, y_train, y_test))","465ff884":"# Define Function to get correlation, refine results and storage values\n\ncorrelated_data=melbourne_data_NN[corr_value.index]\ncorrelated_data.head(10)","000b1ff9":"corr_value.index","8b5bc602":"sns.pairplot(correlated_data)\nplt.tight_layout()","3c6296d6":"sns.heatmap(correlated_data.corr(),annot=True,annot_kws={'size':12})","672e64ef":"X1=correlated_data.drop(labels=['Price'],axis=1)\ny1=correlated_data['Price']\nX1.head()","99a09fcd":"X1_train,X1_test,y1_train,y1_test=train_test_split(X1,y1,test_size=0.2,random_state=0)","5127b097":"X1_train.shape,X1_test.shape","29323187":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error","7799a760":"model = LinearRegression()\nmodel.fit(X1_train,y1_train)","7bc7a6f8":"y1_predict=model.predict(X1_test)","57f4c953":"y1_predict,y1_test","cf4d5984":"df=pd.DataFrame(data=[y1_predict,y1_test])\ndf.T.head(5)","8513c2e2":"from sklearn.metrics import r2_score","21fd2f0e":"score=r2_score(y1_test,y1_predict)\nmae=mean_absolute_error(y1_test,y1_predict)\nmse=mean_squared_error(y1_test,y1_predict)\nprint(\"r2_score\", score)\nprint(\"mae\", mae)\nprint(\"mse\", mse)","3cb3b789":"#store feature performance\ntotal_features=[]\ntotal_features_name=[]\nselected_correlation_value=[]\nr3_score=[]\nmae_value=[]\nmse_value=[]","580aae49":"def performance_metrics(features, th, y_true,y_pred):\n    score=r2_score(y_true,y_pred)\n    mae=mean_absolute_error(y_true,y_pred)\n    mse=mean_squared_error(y_true,y_pred)\n    \n    total_features.append(len(features)-1)\n    total_features_name.append(str(features))\n    selected_correlation_value.append(th)\n    r3_score.append(score)\n    mae_value.append(mae)\n    mse_value.append(mse)\n    \n    metrics_dataframe=pd.DataFrame(data=[total_features_name, total_features,\n                                         selected_correlation_value,r3_score,mae_value,mse_value],\n                                   index=['Features name','Total features','corre value','r2 score','mae','mse'])\n    return metrics_dataframe.T","fd3f4f5c":"def get_y_predict(corrdata):\n    X=corrdata.drop(labels=['Price'],axis=1)\n    y=corrdata['Price']\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n    model=LinearRegression()\n    model.fit(X_train,y_train)\n    y_predict=model.predict(X_test)\n    return y_predict","b2deb1d7":"th5=0.4\ncorr_value=getCorrelatedFeature(corrmat['Price'],th5)\ncorrelated_data=melbourne_data_NN[corr_value.index]\ny_predict=get_y_predict(correlated_data)\nperformance_metrics(correlated_data.columns,th5,y_test,y_predict)","ab8ed1e4":"#Ploting learning curves\nfrom sklearn.model_selection import learning_curve, ShuffleSplit","a90ac56f":"def plot_learning_curve(estimator,title,X,y,ylim=None,cv=None,n_jobs=None,train_sizes=np.linspace(0.1,1.0,10)):\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \n    train_sizes,train_scores,test_scores=learning_curve(estimator,X,y,cv=cv,n_jobs=n_jobs,train_sizes=train_sizes)\n    \n    train_scores_mean=np.mean(train_scores,axis=1)\n    train_scores_std=np.std(train_scores,axis=1)\n    test_scores_mean=np.mean(test_scores,axis=1)\n    test_scores_std=np.std(test_scores,axis=1)\n    \n    plt.grid()\n    \n    plt.fill_between(train_sizes,train_scores_mean - train_scores_std,train_scores_mean+train_scores_std,alpha=0.1,color=\"r\")\n    plt.fill_between(train_sizes,test_scores_mean - test_scores_std,test_scores_mean+test_scores_std,alpha=0.1,color=\"g\")\n    plt.plot(train_sizes,train_scores_mean,'o-',color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes,test_scores_mean,'o-',color=\"g\",label=\"Cross-validation score\")\n    \n    plt.legend(loc=\"best\")\n    return plt\n\nX=correlated_data.drop(labels=['Price'],axis=1)\ny=correlated_data['Price']\n\ntitle=\"learning curves (linear regression)\" + str(X.columns.values)\ncv=ShuffleSplit(n_splits=100,test_size=0.2,random_state=0)\n\nestimator=LinearRegression()\nplot_learning_curve(estimator,title,X1,y1,ylim=(0.7,1.01),cv=cv,n_jobs=-1)\n\nplt.show()","3640aca8":"### Suplots of other numeric features vs Price\n#### Method VS Price","004936cd":"#### Car Park vs Price","a9b18561":"House price was going down dramatically by 100,000 units Condo price climb up slowly while Townhouse price kept steady. From this graph, it is anticipated that house price will keptgoing down but less slope Townhouse price will kepte unchanged Condo price will increase. To developer, it is time to built more condos in 2019.House budget need to be cut For home buyers, it is time to buy house in 2019.","4fe07fe3":"In general, it looks like that winters in 2016 an 2017 have the least sales count. that means house sales will favors more from May to November. Year 2018 seems a lot of missing data and date shape only one third compard to the others thus it is hard to make conclusion.","1d531659":"### Seasonal Performance","3c982ce0":"### Count Growth rate over 0.2 table per region and type","e5123527":"### Region Price Change VS Year per type\nIn General, East, north, south and west Metro are popular area based on sales count.\n\nNext look at price against region per type","5dcfd293":"It looks like training examples between 6 000 and 15 000 will give less change to the model.\n\n### CONCLUSION\nBased on all 4 sentions, some finding and conclusion can be found as follow:\n\n1.From 2.1.1 section, house price in general will go down and Condo\/units price will go up. that means investing in Condo will be better.\n\n2.Regarding house, S Metro has over 5 % in price growth in 2017 and 4718 sales count in three years, ranking second place among all.In terms of Units\/Condo, S Metro has 2782 in sales count ranking No.4 and 8.7 % in price growth in 2017 from Section 2.4.\n\n3.Units and townhouses in E Metro and SE MEtro has great potential tough now they dont have such attraction as S Metro. They have count growth over 100% and price growth rate over 8% from Section 2.4.\n\n4.From 2.6 ideal house type, we can see uints\/condo in Southern Metro was ranked no.3. Though house in North and West Metro has the most count but their price is going down as showed in section 2.4 sns.lmplot. In terms of counts, Southern condo\/units has great market potential as its count rank No.3 and price kept increasing by years presented in section 2.4. Southen house also has great market and rank no.4 after its unit in same region. But the price kept dropping compared to units in same region.\n\n5.From section 3 and 4, though the score is low due to low coefficient values, Southern Metro show No.2 biggest coefficient compared to other regions. E vic coefficient is high but is low in count thus can be neglected. That means Southern Metro has the great market in price.\n\n6.Linear model has some limitations as if more rooms does equal to higher price and more sales count. But its coefficient can help to understand which factor has great impact. If price was only output dependent variables, the conclusion will be biased and not looking at the whole pictures. But if slaes count and price be presented as output variables, a clear picture will be clear. On the other hand, data is data, the perfect r2 sometimes doesn't mean the great insights of a business. The combination of business unstanding and data can present more real insights of business. The dataset miss great amount of value in 2018 and it results in that count cannot be used as model output. Only price can be output variable. So Cout in past three years and count in 2016 to 2017 will provide more leads for market.\n\n7.Continue improvement: Due to limited timeframe, some work need to done to improve linear relationship and reduce r2 value:\n\na.replace null value with median to see if r2 drops (Yes, it drops by 0.01 and reduce effect of increasing unit against price regarding bathroom, car spots; reduce training score and vailidation score by 0.02 and validation score is always under training score by 0.01 throughout 1400 samples). so the code is there in 1.3 but not in effect and more investigation using different strategy will be applied.\n\nb.different regressor or K-Fold model will be applied to reduced r2;\n\nc.more feature engineering against sales count will be explored, such as count growth in region against year.\n\n8.On the basis of conclusion 1-4, Units with 2 room and one bathroom in Southern Metro will be recommended to investor or home developer as it has 955 count in 3 years, ranking No.3 in counts and price kept increaing steadily 6 %. It is safe area for conservative investment agaist the unstable market.","5c8426ae":"### Predicting Price for E metro unit","8e08c74a":"### Performance evaluation\nUsing define function to evaluate different input threshold for correlation value\n\nExplore relationships between input sample size and machine learning scores\n\nOnly correlation more than 0.4 was selected below. so Rooms and Bathroom fall into this category","a49f60c6":"### Data Presentation and Relationship\n\nThe first fator we look at is the price Versus Year and season. Then predict \"Price \" using Linear fuction for 2019 and 2020","f8cca858":"Before using fillna.mode Every unit increase in those features will: Rooms is linked to an increase in Price by 170624.6 Distance is linked to an decrease in Price by 44797 Bathroom is linked to an increase in Price by 185170 Car space is linked to an increase in Price by 38890 Year is linked to an increase in Price by 44305 Type code is associated with increase in Price by 271838\n\nAfter using fillna Every unit increase in those features will: Rooms is linked to an increase in Price by 217253.4 Distance is linked to an decrease in Price by 42651.8 Bathroom is linked to an increase in Price by 141730.5 Car space is linked to an increase in Price by 37952 Year is linked to an increase in Price by 29458.7 Type code is associated with increase in Price by 235465\n\nType code, Rooms and bathroom are very important in house price. With limited landsize, the more rooms and bathroom, the higher the price is.\n\nS Metro, SE Metro seems to be linked to an increase in price\n\n### Random forest model for coefficient to present impact for \"Rooms, Distance, Bathroom, car, year, Propertycount, type_code, region\"\u00b6","6ce69d7a":"### Actual count over 1000 for each year per type and region","dda2ffd5":"### Distance vs Price","a2eb0f5f":"### Price Growth rate over 0.05 table per region and type","74a6a046":"27239 record will be used for presenting","7c98efdc":"From the visual above, it can be concluded that we have few missing data in Price, Bathroom, Car and Landsize, Longitude and Latitude columns. There are so many missing values in Building Area and Year Built features. In next step let's explore the count of missing values","6e5b44f7":"Predict price for S Metro unit","611fe031":"### Finding Outliers\n\nOutliers can significantly impact data analysis and can also impact normalization of data. It is very important in during data prepration to identify them remove them. In next few steps we will work in our data to get rid of outliers (if any)","d5ee236e":"Rooms and Bathroom has highest correlation with house price compared to other factors.\n\n### Linear model for coefficient to present impact for 'Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code',region\n\nPrepare the dataset and label for training models, include removing all null value, get_dummies of region, change type into numeric\n\n","ea2c45a6":"For Condo in S Metro region, price will be aroud 795272 dollars with lower R2","4694f19f":"From the information above, we can notice that few feature varaibles still have large percentage of missing values. At this point we are ignoring it, but at later state if we will take those as our feature variables for our model, we will explore ways to fill in those information or to remove those from our data.","988183b7":"Learning curve : Plot training scores against validation score","9a3e5a20":"Insights: In S Metro, house with 3 rooms and 1 or 2 bathroom and house with 4 rooms and 2 bathrooms have the most sales among all.","07791e3f":"for the next few steps we will be doing data preparation for Numerical features variables","37090209":"### Model Comparison","77449979":"### modelling","d823a6dd":"After carefully evaluating data, it can be noticed that variables \"Rooms\" and \"Bedroom2\" are pretty much similar and one of the columns should be removed to avoid duplication of data","09f5668d":"### Region Versus Price\u00b6","a835c4be":"There are multiple ways that we can use to explore missisng data. Here we will be using a visual way first to get some hint. In later step we will do some calculations to get exact number of missing data from each variables. Based on data, our experience and business need we will either fill in missing values or we will drop rows or columns having null values","af8aaafa":"For E Metro, price grow 10% from 2016 to 2017 and 7.7% from 2018 to 2019 can be expected. though count is less compared to southern region","9dd0d456":"### Number of Rooms #impact on Price VS Year","1b55b411":"From above information count growth percentage and acutal sales count by year, South metro and N Metro seems to be the area where people tend to pay more and buy more, but as price kept going up, those live in south try to move to E and SE Metro\n\nTo Conclude this section:\n\n1.Regarding house, S Metro has over 5 % in price growth in 2017 and 4718 sales count in three years, ranking second place among all.In terms of Units\/Condo, S Metro has 2782 in sales count ranking No.4 and 8.7 % in price growth in 2017.\n\n2.Units and townhouses in E Metro and SE MEtro has great potential tough now they dont have such attraction as S Metro. They have count growth over 100% and price growth rate over 8%.","7292f74b":"### Working with missing Data","099d2f63":"Observing above information abount numerical data, it can be noticed that Postcode is also being treated as numerical data. Since we know that Postcode is a catergorical data, we will be casting it to category","3b45d32b":"Melboune house price prediction,growth area identification and modeling (linear, random tree forest and logistic) Layout\nData structure. It inclues data cleaning against data type, outliers, null value.\n1.1 Exploring data, such select_dtypes().columns, describe(), info() and shape()\n\n1.2 Changing data type, date type and catogory type\n\n1.3 Dealing with null value, mainly replacing null in price with mode using fillna()\n\n1.4 Finding outliers\n\nData presentation and prediction 7 section. (27239 records)\n2.1 Year vs price per type\n\n2.2 Price prediction for 2019\n\n2.2.1 Predict price for unit in S Metro\n\n2.2.1 Predict price for unit in E Metro\n\n2.3 Season vs price\n\n2.4 Region vs price change and growth rate vs year\n\n2.4.1 Region price change vs year per type\n\n2.4.2 Feature engineering to get count growth and price growth against year per region and type\n\n2.4.2.1 Price growth rate over 0.05 table per region and type\n\n2.4.2.2 Top 20 region per type with largest count\n\n2.4.2.3 Count growth rate over 0.2 table per region and type\n\n2.4.2.4 Actual count over 1000 for each year pet type and region\n\n2.5 Other features method, distance, rooms, car vs price\n\n2.5.1 Method vs Price\n\n2.5.2 Rooms # impact on Price VS Year\n\n2.5.3 Distance vs Price\n\n2.5.4 Car spot vs Price\n\n2.6 Ideal house type for top 10 region per type max count of sales\n\n2.6.1 Top 10 house type in S Metro with different rooms and bathrooms by count\n\n2.6.2 Top 10 house type in all regions per type with different rooms and bathrooms by count\n\n2.6.3 Top 10 units in E Metro with different rooms and bathrooms by count\n\n2.7 Heat map for relationships\n\nData linear modeling to see which variable contribute most (20394 records)\n3.1 Prepare the dataset and label for training models, include removing all null value, get_dummies of region, change type into numeric\n\n3.2 Getting coefficient\n\n3.3 Random forest model\n\n3.4 Model comparison get score function\n\nPerformance evaluation sample size vs machine learning\n4.1 Define fuction to get correlation, refine results and storage value\n\n4.2 Learning curve : Plot training scores against validation score\n\nConclusion\u00b6","9008c26d":"Price trend against year per house","37224d90":"only extract data with high relations to price","aea7cf0d":"The best Condo type in East region Metropolitan is 2 rooms with one bed room","dfddd229":"Used get_dummies to change category data into numeric date. Region will be expressed in numerics","f20659cc":"From the statstical summary above we can see that max price in our data is nearly $11.2 million. That looks like a clear outlier. But before removing it, lets first ensure that we have very few values in that range.","4c4550e7":"#### Top 10 units in E Metro with different rooms and bathrooms by count\u00b6","ae13e72d":"in general, Townhouse in E Metro, Condo in East Metro, House in S Metro, Condo in S Metro and Condo in N Metro are growing each year.","578cab87":"### Feature engineering to get count growth and price growth against year per region and type","745ad1ca":"Top 20 region per type with largest sales count","c7702ce2":"Changing Data type","b94e7afa":"### Ideal House type\nIn this section count will be used to find out leads about the best sales type in all region\n\nTop 10 house type in S Metro with different rooms and bathrooms by count","8b53647b":"### Heat map for presenting realtionship","cc175f63":"Top 10 house type in all regions per type with different rooms and bathrooms by count","57c26507":"By exploring above table, it can be concluded that:\n\n1 data item in the range 0-100,000\n\n2 data item in range 7M - 8M\n\n1 data item in range 8M - 9M\n\n1 data item in range 11M - 12M For the purpose of this study, let us drop rows that match above mentioned conditions","eac84ed3":"Due to a lot of missing data in 2018, S Metro units has 8.7% in 2017 and 2.7% in 2018. If 2017 was chozen to look at growth in price, units and townhouse in E Metro,SE Metro and S Metro, townhouse in W Metro and house in S Metro present positive growth over 5%. looking at 2018, it seems people shift to buy more from SE or E Metro from S Metro. But this shift was not that significant yet in count as it showed in later table. Trend is there.","a86cb256":"From this rough approximation, 2019 average price will be 1557639, 1630019 for 2020 for all types in melboune","1b2e0ebe":"Looking at the data information above, we can see that non-numerical data is being considered as object. The list include the following columns: \"Suburb\",\"Address\",\"Type\",\"Method\",\"SellerG\",\"Date\",\"CouncilArea\",\"Regionname\". In the next few steps we will be changing object data types to categorical and Date data types.","68deb37f":"Exploring Data and Data Cleaning","8dcd016d":"Predicting house price for all types in South Metro, units in South Metro and units in East Melbourne for 2019 and 2020","a6fbd05f":"Insights:North metro house was the most favorable types among those and W Metro follow behind. In general, house is more favorable than other types. South Metro condo with 2 bed rooms and 1 bathroom was listed as top 3 in sales count."}}