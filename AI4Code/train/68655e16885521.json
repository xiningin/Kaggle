{"cell_type":{"c2d75418":"code","5bd8ce34":"code","fb9931d0":"code","6c3d17e4":"code","b86df521":"code","687052a5":"code","fab80297":"code","db0f3a20":"code","ece6ba86":"code","65a0ced1":"code","dfe3435b":"code","d3875011":"code","cb92fdfa":"code","9ed34582":"code","551b4a01":"code","ed21e7b5":"code","408a5a61":"code","fc346115":"code","f516f006":"code","6b44a71a":"code","3f0fce5e":"code","7972427d":"code","d25d2d58":"code","77c6cf39":"code","dadd5519":"code","14ae16b6":"code","2fc79232":"markdown","49931b73":"markdown","96b79a3b":"markdown","c0735670":"markdown"},"source":{"c2d75418":"import sys\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport gc\nprint(sys.executable)","5bd8ce34":"import torch\ntorch.cuda.is_available()","fb9931d0":"from pathlib import Path\nfrom tokenizers import ByteLevelBPETokenizer\nfrom tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\nfrom torch.utils.data import Dataset\nfrom transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM\nfrom transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments, pipeline\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport os\nos.getcwd()","6c3d17e4":"data_path = '..\/input\/goethe\/'\noutput_path = '.\/'\n# !mkdir ..\/output\/GoetheBERT","b86df521":"special_tokens_dict = {'pad_token': '<PAD>',\n                       'bos_token': '<BOS>', \n                       'eos_token': '<EOS>', \n                       'unk_token': '<UNK>',\n                       'mask_token':'<MASK>'\n                      }\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"dbmdz\/german-gpt2\")# **special_tokens_dict\n\nmodel = AutoModelWithLMHead.from_pretrained(\"dbmdz\/german-gpt2\")\ntokenizer.add_special_tokens(special_tokens_dict) \n \nmodel.resize_token_embeddings(len(tokenizer))\n\nmodel.num_parameters()\n# => 125 million parameters","687052a5":"m = 0\nwith open(data_path + 'full_processed.txt') as f:\n    for l in f:\n        m = max(m, len(tokenizer.encode(l)))\nm","fab80297":"test = tokenizer.encode(\"Ich hei\u00dfe Werther.\")\ntest","db0f3a20":"tokenizer.decode(test)","ece6ba86":"data_collator_mlm = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\ndata_collator_clm = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","65a0ced1":"train_0_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=os.path.join(data_path, 'train_0.txt'),\n    block_size=128,\n)\ntrain_1_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=os.path.join(data_path, 'train_1.txt'),\n    block_size=128,\n)\neval_0_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=os.path.join(data_path, 'eval_0.txt'),\n    block_size=128,\n)\neval_1_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=os.path.join(data_path, 'eval_1.txt'),\n    block_size=128,\n)","dfe3435b":"from copy import deepcopy as dc\nmodel_ = dc(model)","d3875011":"batch_size = 30\n\ntraining_args_0 = TrainingArguments(\n    output_dir                  = output_path,\n    overwrite_output_dir        = True,\n    load_best_model_at_end      = True,\n    metric_for_best_model       = 'eval_loss',\n    greater_is_better           = False,\n    num_train_epochs            = 15,\n    per_device_train_batch_size = batch_size, \n    per_device_eval_batch_size  = batch_size,\n    save_strategy               = 'epoch',\n    logging_strategy            = 'epoch',\n    evaluation_strategy         = 'epoch')\n\ntraining_args_1 = TrainingArguments(\n    output_dir                  = output_path,\n    overwrite_output_dir        = True,\n    load_best_model_at_end      = True,\n    metric_for_best_model       = 'eval_loss',\n    greater_is_better           = False,\n    num_train_epochs            = 5,\n    per_device_train_batch_size = batch_size, \n    per_device_eval_batch_size  = batch_size,\n    save_strategy               = 'epoch',\n    logging_strategy            = 'epoch',\n    evaluation_strategy         = 'epoch')\n\ntrainer_mlm = Trainer(\n    model         = model.cuda(),\n    args          = training_args_0,\n    data_collator = data_collator_mlm,\n    train_dataset = train_0_dataset,\n    eval_dataset  = eval_0_dataset)","cb92fdfa":"torch.cuda.empty_cache()","9ed34582":"%%time\n# history_mlm = trainer_mlm.train()\n# history_mlm","551b4a01":"trainer_clm = Trainer(\n    model         = model.cuda(),\n    args          = training_args_1,\n    data_collator = data_collator_clm,\n    train_dataset = train_1_dataset,\n    eval_dataset  = eval_1_dataset)\n\n%%time\n# history_clm = trainer_clm.train()\n# history_clm","ed21e7b5":"# torch.save(model.state_dict(), os.path.join(output_path, 'gpt2-german-trained.pth'))","408a5a61":"from pathlib import Path\ncheckpoint = torch.load(Path(os.path.join(data_path, 'gpt2-german-trained.pth')))\nmodel.load_state_dict(checkpoint)","fc346115":"from transformers import pipeline\n\npipe = pipeline('text-generation', model=model.cpu(),\n                 tokenizer=tokenizer)\n\ntext = [t[\"generated_text\"] for t in pipe(\"Der Sinn des Lebens ist es\", max_length=30, num_return_sequences=5)]\n\nprint('\\n'.join(text))","f516f006":"pipe(\"Wenn er mir auch nur verworren dient,\", max_length=30)[0][\"generated_text\"]","6b44a71a":"pipe(\"Der Dichter\", max_length=30)[0][\"generated_text\"]","3f0fce5e":"pipe(\"Ich bin eine Katze\", max_length=30)[0][\"generated_text\"]","7972427d":"pipe(\"Ach so\", max_length=30)[0][\"generated_text\"]","d25d2d58":"pipe(\"Diese Studenten waren\", max_length=30)[0][\"generated_text\"]","77c6cf39":"pipe(\"Ich bin eine \", max_length=30)[0][\"generated_text\"]","dadd5519":"pipe(\"Meine Liebe\", max_length=30)[0][\"generated_text\"]","14ae16b6":"text = [t[\"generated_text\"] for t in pipe(\"Heute morgen\", max_length=30, num_return_sequences=5)]\nprint('\\n'.join(text))","2fc79232":"We will try 2 models : a masked language model, which is bidirectional and mostly used in BERT-derivatives, and a causal language model, unidirectional and more adapted to GPT-derivatives.","49931b73":"On a Z by HP Z8 tower with NVIDIA GPU RTX6000 it runs in 4 minutes per epoch. I am not performing the training here because it simply kills Kaggle machines \ud83d\ude01 I have provided the trained model in the dataset, you just have to load it and play with it.","96b79a3b":"We're looking for the maximum input size, in order to adapt our model and avoid losing time in unnecessary computations.","c0735670":"In the following we will set the maximum block size to 125."}}