{"cell_type":{"775aee78":"code","3502a00c":"code","9750c912":"code","6311cd65":"code","8ffa9dd9":"code","4287d1cf":"code","c479b9e0":"code","04f53d6e":"code","e2aefa4f":"code","f4941d09":"code","b9111d01":"code","8e673f6e":"code","11bf4642":"code","1ef802da":"code","8cbf5b74":"code","c21db90a":"code","804f894f":"code","cd948a9a":"code","ce17c18f":"code","be4e5632":"code","b83a6cf1":"code","8a1a8fbc":"code","47dc40fe":"code","edf1e281":"code","ce6bc0b1":"code","dc781582":"code","24b1129f":"code","92134e53":"code","d04e702f":"code","6643953f":"code","ad5a19ab":"code","8a7c70e0":"code","b3882809":"code","7d093f35":"code","40e10129":"code","6c8d5add":"code","d156f189":"code","f6825809":"code","33b464b8":"code","85341e5e":"code","ba28cb9c":"code","24864764":"markdown","68df33da":"markdown","856890cb":"markdown","5c2539de":"markdown","05af2686":"markdown","729b13bf":"markdown","b8b655d3":"markdown","6052796b":"markdown","79190b71":"markdown","49410e8f":"markdown","574bfb04":"markdown","ef1fe7af":"markdown","0fd8d113":"markdown","5a9a5d8b":"markdown","38450f51":"markdown","63e2868d":"markdown","4e628386":"markdown"},"source":{"775aee78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3502a00c":"import os\nfrom pathlib import Path\n\nfrom scipy import stats\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.ensemble import HistGradientBoostingClassifier","9750c912":"def data_load():\n    data_path = Path(\"\/kaggle\/input\/titanic\/\")\n    df_train = pd.read_csv(data_path \/ \"train.csv\", index_col = \"PassengerId\")\n    df_test = pd.read_csv(data_path \/ \"test.csv\", index_col = \"PassengerId\")\n    # dummy variable for train and test set\n    df = pd.concat([df_train.assign(ind = \"train\"), df_test.assign(ind = \"test\")])\n    \n    clean(df)\n    encode(df)\n    impute(df)\n    \n    return df","6311cd65":"def clean(df):\n    df.drop(['Name', 'Ticket'], axis = 1, inplace = True)\n    \n    return df","8ffa9dd9":"# nominal category\nfeatures_nom = ['Sex', 'Cabin', 'Embarked', 'ind']\n# ordinal category\nordered_levels = {'Pclass': [\"Lower\", \"Middle\", \"Upper\"]}\n# create none in ordinal category\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\ndef encode(df):\n    pclass_dict = {1: \"Upper\", 2: \"Middle\", 3: \"Lower\"}\n    df['Pclass'].replace(pclass_dict, inplace = True)\n\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace = True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered = True))\n    return df","4287d1cf":"def fill_fare(df):\n    for pclass in df['Pclass'].unique():\n        filter_pclass = df['Pclass'] == pclass\n        fare_pclass = df.loc[filter_pclass, 'Fare'].median()\n        df.loc[filter_pclass, 'Fare'].fillna(fare_pclass, inplace = True)\n        \ndef impute(df):\n    # numeric\n    df['Age'].fillna(df['Age'].median(), inplace = True)\n    fill_fare(df)\n    \n    # category\n    df['Cabin'].fillna(\"None\", inplace = True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace = True)\n    \n    return df","c479b9e0":"df = data_load()\ndf.head()","04f53d6e":"def score(X, y, model = RandomForestClassifier()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    \n    score = cross_val_score(\n        model, X, y, cv = 5, scoring = \"accuracy\",\n    )\n    \n    return score.mean()","e2aefa4f":"df_train = df[df['ind'] == \"train\"]\nX = df_train.copy()\ny = X.pop(\"Survived\")\n\nbaseline_score = score(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} accuracy\")","f4941d09":"baseline_score = score(X, y, model = GradientBoostingClassifier())\nprint(f\"Baseline score: {baseline_score:.5f} accuracy\")","b9111d01":"# create bins\ndf['Age_bin'] = pd.cut(df['Age'], np.linspace(0, 100, 11))\n# replace pattern\ndf['Age_bin'] = df['Age_bin'].astype(str).str.replace(\"\\.0|[()[\\]]\", \"\", regex = True)\ndf['Age_bin'] = df['Age_bin'].astype(str).str.replace(\", \", \" - \", regex = True)\n# create categorical bin\nage_bin_order = df['Age_bin'].sort_values().unique()\ndf['Age_bin'] = df['Age_bin'].astype(CategoricalDtype(age_bin_order, ordered = True))\n\ndf_train = df[df['ind'] == \"train\"]","8e673f6e":"sns.countplot(data = df_train, x = \"Age_bin\")","11bf4642":"sns.relplot(data = df_train, x = \"Age\", y = \"Fare\", hue = \"Survived\")","1ef802da":"df_train['Pclass'].unique().sort_values()","8cbf5b74":"for pclass in df_train['Pclass'].unique().sort_values():\n    ax = sns.displot(data = df_train[df_train['Pclass'] == pclass], x = \"Fare\", kind = \"kde\")\n    ax.set(title = f'{pclass}')","c21db90a":"lower = df_train[df_train['Pclass'] == \"Lower\"]['Fare'].to_numpy()\nmiddle = df_train[df_train['Pclass'] == \"Middle\"]['Fare'].to_numpy()\nupper = df_train[df_train['Pclass'] == \"Upper\"]['Fare'].to_numpy()\nstats.kstest(lower, middle)","804f894f":"sns.countplot(data = df_train, x = \"Embarked\", hue = \"Pclass\")","cd948a9a":"sns.countplot(data = df_train, x = \"Pclass\", hue = \"Survived\")","ce17c18f":"sns.countplot(data = df_train, x = \"Parch\", hue = \"Survived\")","be4e5632":"df_train = df[df['ind'] == \"train\"]\nmale = df_train[df_train['Sex'] == \"male\"]\nfemale = df_train[df_train['Sex'] == \"female\"]\n\nmale_survived = male['Survived'][male['Survived'] == 1].count()\nfemale_survived = female['Survived'][female['Survived'] == 1].count()\n\nmale_survived_perc = round((male_survived \/ (male_survived + female_survived)) * 100 , 2)\nprint(f'male survived: {male_survived_perc}%')\nprint(f'female survived: {100 - male_survived_perc}%')","b83a6cf1":"df.describe()","8a1a8fbc":"df","47dc40fe":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X","edf1e281":"df['Cabin'].str[0].unique()","ce6bc0b1":"def new_features(df):\n    df['Cabin_ini'] = df['Cabin'].str[0].astype(\"category\")\n    df['Pclass_sex'] = df['Pclass'].str.cat(df['Sex'],sep=\"_\").astype(\"category\")\n    return df","dc781582":"df","24b1129f":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","92134e53":"df_train = df[df['ind'] == \"train\"]\nX = df_train.copy()\ny = X.pop(\"Survived\")\n\nmi_scores = make_mi_scores(X, y)\nplot_mi_scores(mi_scores)","d04e702f":"cluster_features = [\n    \"Pclass\",\n    \"Sex\",\n    \"Age\",\n    \"Fare\",\n    \"Cabin_ini\",\n]\n\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    for colname in X_scaled.select_dtypes([\"category\"]):\n        X_scaled[colname] = X_scaled[colname].cat.codes\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    X_new.index += 1\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","6643953f":"# df_train = df[df['ind'] == \"train\"]\n# X = df_train.copy()\n# y = X.pop(\"Survived\")\n\n# X = X.join(cluster_labels(X, cluster_features, n_clusters = 10))\n# X['Cluster'].unique()","ad5a19ab":"X","8a7c70e0":"def apply_pca(X, standardize=True):\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    X_pca.index += 1\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\npca_features = [\n    \"Pclass\",\n    \"Fare\",\n    \"Cabin\",\n    \"Sex\"\n]","b3882809":"\n\ndf_train = df[df['ind'] == \"train\"]\nX = df_train.copy()\ny = X.pop(\"Survived\")\nX = X.loc[:, pca_features]\nfor colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n\nX = (X - X.mean(axis=0)) \/ X.std(axis=0)\n\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings","7d093f35":"plot_variance(pca)","40e10129":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","6c8d5add":"# df_train = df[df['ind'] == \"train\"]\n# X = df_train.copy()\n# y = X.pop(\"Survived\")\n# for colname in X.select_dtypes([\"category\"]):\n#         X[colname] = X[colname].cat.codes\n# encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n# X = X.join(encoder.fit_transform(X, y, cols=[\"Pclass_sex\"]))","d156f189":"X","f6825809":"df = data_load()\ndf_train = df_train = df[df['ind'] == \"train\"].drop('ind', axis = 1)\ndf_train","33b464b8":"def create_features(df, df_test = None):\n    X = df.copy()\n    y = X.pop(\"Survived\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"Survived\")\n        X = pd.concat([X, X_test])\n\n    # Mutual Information\n#     X = drop_uninformative(X, mi_scores)\n\n    # Transformations\n    X = new_features(X)\n#     X = X.join(mathematical_transforms(X))\n#     X = X.join(interactions(X))\n#     X = X.join(counts(X))\n#     # X = X.join(break_down(X))\n#     X = X.join(group_transforms(X))\n\n    # Lesson 4 - Clustering\n    X = X.join(cluster_labels(X, cluster_features, n_clusters = 10))\n    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Lesson 5 - PCA\n#     X = X.join(pca_inspired(X))\n    X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n\n    X = label_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"Pclass_sex\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\n\ndf = data_load()\ndf_train = df_train = df[df['ind'] == \"train\"].drop('ind', axis = 1)\nX_train = create_features(df_train)\ny_train = df_train[\"Survived\"]\n\nscore(X_train, y_train)","85341e5e":"df = data_load()\ndf_train = df_train = df[df['ind'] == \"train\"].drop('ind', axis = 1)\nX_train = create_features(df_train)\ny_train = df_train[\"Survived\"]\n\nX_train","ba28cb9c":"score(X_train, y_train)","24864764":"### Mutual Information","68df33da":"### Create Features","856890cb":"### Target Encoding","5c2539de":"### Data cleaning","05af2686":"### Handling missing values\n\nFill with mean ou median for age. For fare, it could be the mean or median of the Pclass he\/she belongs. \nEmbarked needs to be mode. Lastly Cabin will be \"None\".","729b13bf":"A classe middle e lower parecem possuir o mesmo Fare","b8b655d3":"### Encode statistical data type","6052796b":"### PCA\n","79190b71":"'N' is for None","49410e8f":"## Data Preprocessing","574bfb04":"## Feature Engineering","ef1fe7af":"### Clustering","0fd8d113":"### Create final feature set","5a9a5d8b":"## Establishing baseline score","38450f51":"Quanto mais velho, mais rico? Provavelmente n\u00e3o\n\nTer mais dinheiro aumenta a taxa de sobrevivencia? Parece que sim","63e2868d":"## Exploratory Data Analysis","4e628386":"## Imports and configuration"}}