{"cell_type":{"8b0cbda4":"code","5667e7cc":"code","7caf19cd":"code","55cba1c0":"code","102e135f":"code","70fc8042":"code","0b16b06d":"code","1606d3a8":"code","b65fb6c0":"code","9e9b69f4":"code","c40ca7d4":"code","c3ca8bd0":"code","012302cb":"code","9bc0714d":"code","2568c16a":"code","16b8510f":"code","5469a51f":"code","a64d35f4":"code","c794bd6b":"code","72dd34df":"code","ee7f4244":"code","2f825186":"code","65460b9f":"code","40e0c751":"code","6bcb0289":"code","39c8c0d5":"code","3c3db468":"code","2d83aa19":"code","aaf1d473":"code","e809f1af":"code","264eb1c8":"code","95e3234d":"code","d1d31c0e":"code","1f1fbf84":"code","4ed3de13":"code","69af5f5a":"code","7fb889f9":"code","1c5d6c94":"markdown","e9b3c7d0":"markdown","4948cc74":"markdown","08f1398e":"markdown","e57ae9bc":"markdown","51a0c824":"markdown","25054eaf":"markdown","bd9c7056":"markdown","2fdd5dd6":"markdown","89e1df18":"markdown","3df1856f":"markdown","d604f8c2":"markdown","613f8ede":"markdown","1bc5e952":"markdown","ef90fc23":"markdown","2b4dbd0a":"markdown","87f09b8c":"markdown","c4523a24":"markdown","212994ef":"markdown","539e99e7":"markdown","31366a70":"markdown","39c7d11f":"markdown","c587a205":"markdown"},"source":{"8b0cbda4":"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.manifold import TSNE\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.stem import WordNetLemmatizer \n\nfrom imblearn.over_sampling import SMOTE","5667e7cc":"def get_sentence_word_count(text_list):\n    sent_count = 0\n    word_count = 0\n    vocab = {}\n    for text in text_list:\n        sentences=sent_tokenize(str(text).lower())\n        sent_count = sent_count + len(sentences)\n        for sentence in sentences:\n            words=word_tokenize(sentence)\n            for word in words:\n                if(word in vocab.keys()):\n                    vocab[word] = vocab[word] +1\n                else:\n                    vocab[word] =1 \n    word_count = len(vocab.keys())\n    return sent_count,word_count\n    ","7caf19cd":"clinical_text_df = pd.read_csv(\"\/kaggle\/input\/medicaltranscriptions\/mtsamples.csv\")\n\nprint(clinical_text_df.columns)\nclinical_text_df.head(5)","55cba1c0":"\nclinical_text_df = clinical_text_df[clinical_text_df['transcription'].notna()]\nsent_count,word_count= get_sentence_word_count(clinical_text_df['transcription'].tolist())\nprint(\"Number of sentences in transcriptions column: \"+ str(sent_count))\nprint(\"Number of unique words in transcriptions column: \"+str(word_count))\n\n\n\ndata_categories  = clinical_text_df.groupby(clinical_text_df['medical_specialty'])\ni = 1\nprint('===========Original Categories =======================')\nfor catName,dataCategory in data_categories:\n    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n    i = i+1\nprint('==================================')\n\n\n\n","102e135f":"filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\nfinal_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\ni=1\nprint('============Reduced Categories ======================')\nfor catName,dataCategory in final_data_categories:\n    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n    i = i+1\n\nprint('============ Reduced Categories ======================')\n","70fc8042":"plt.figure(figsize=(10,10))\nsns.countplot(y='medical_specialty', data = filtered_data_categories )\nplt.show()","0b16b06d":"data = filtered_data_categories[['transcription', 'medical_specialty']]\ndata = data.drop(data[data['transcription'].isna()].index)\ndata.shape\n","1606d3a8":"print('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\nprint('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\nprint('Sample Transcription 3:'+data.iloc[1000]['transcription'])\n","b65fb6c0":"def clean_text(text ): \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text1 = ''.join([w for w in text if not w.isdigit()]) \n    REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\n    #BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n    \n    text2 = text1.lower()\n    text2 = REPLACE_BY_SPACE_RE.sub('', text2) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    #text2 = BAD_SYMBOLS_RE.sub('', text2)\n    return text2\n\ndef lemmatize_text(text):\n    wordlist=[]\n    lemmatizer = WordNetLemmatizer() \n    sentences=sent_tokenize(text)\n    \n    intial_sentences= sentences[0:1]\n    final_sentences = sentences[len(sentences)-2: len(sentences)-1]\n    \n    for sentence in intial_sentences:\n        words=word_tokenize(sentence)\n        for word in words:\n            wordlist.append(lemmatizer.lemmatize(word))\n    for sentence in final_sentences:\n        words=word_tokenize(sentence)\n        for word in words:\n            wordlist.append(lemmatizer.lemmatize(word))       \n    return ' '.join(wordlist) \n","9e9b69f4":"\ndata['transcription'] = data['transcription'].apply(lemmatize_text)\ndata['transcription'] = data['transcription'].apply(clean_text)","c40ca7d4":"print('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\nprint('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\nprint('Sample Transcription 3:'+data.iloc[1000]['transcription'])","c3ca8bd0":"vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)\ntfIdfMat  = vectorizer.fit_transform(data['transcription'].tolist() )\nfeature_names = sorted(vectorizer.get_feature_names())\nprint(feature_names)","012302cb":"import gc\ngc.collect()\ntfIdfMatrix = tfIdfMat.todense()\nlabels = data['medical_specialty'].tolist()\ntsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\nplt.figure(figsize=(16,10))\npalette = sns.hls_palette(21, l=.6, s=.9)\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue=labels,\n    palette= palette,\n    legend=\"full\",\n    alpha=0.3\n)\nplt.show()\n\n","9bc0714d":"gc.collect()\npca = PCA(n_components=0.95)\ntfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\nlabels = data['medical_specialty'].tolist()\ncategory_list = data.medical_specialty.unique()\nX_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)   \n","2568c16a":"print('Train_Set_Size:'+str(X_train.shape))\nprint('Test_Set_Size:'+str(X_test.shape))","16b8510f":"clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)","5469a51f":"labels = category_list\ncm = confusion_matrix(y_test, y_test_pred, labels)","a64d35f4":"\nfig = plt.figure(figsize=(20,20))\nax= fig.add_subplot(1,1,1)\nsns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \nplt.show()","c794bd6b":"print(classification_report(y_test,y_test_pred,labels=category_list))","72dd34df":"filtered_data_categories['medical_specialty'] =filtered_data_categories['medical_specialty'].apply(lambda x:str.strip(x))\nmask = filtered_data_categories['medical_specialty'] == 'Surgery'\nfiltered_data_categories = filtered_data_categories[~mask]\nfinal_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\nmask = filtered_data_categories['medical_specialty'] == 'SOAP \/ Chart \/ Progress Notes'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Office Notes'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Consult - History and Phy.'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Emergency Room Reports'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Discharge Summary'\nfiltered_data_categories = filtered_data_categories[~mask]\n\n'''\nmask = filtered_data_categories['medical_specialty'] == 'Pediatrics - Neonatal'\nfiltered_data_categories = filtered_data_categories[~mask]\n'''\nmask = filtered_data_categories['medical_specialty'] == 'Pain Management'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'General Medicine'\nfiltered_data_categories = filtered_data_categories[~mask]\n\n\nmask = filtered_data_categories['medical_specialty'] == 'Neurosurgery'\nfiltered_data_categories.loc[mask, 'medical_specialty'] = 'Neurology'\nmask = filtered_data_categories['medical_specialty'] == 'Nephrology'\nfiltered_data_categories.loc[mask, 'medical_specialty'] = 'Urology'\n\n\ni=1\nprint('============Reduced Categories======================')\nfor catName,dataCategory in final_data_categories:\n    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n    i = i+1\n\nprint('============Reduced Categories======================')\n\n\ndata = filtered_data_categories[['transcription', 'medical_specialty']]\ndata = data.drop(data[data['transcription'].isna()].index)\ndata.shape","ee7f4244":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.5\/en_ner_bionlp13cg_md-0.2.5.tar.gz\n","2f825186":"import spacy\nimport en_ner_bionlp13cg_md\nnlp = en_ner_bionlp13cg_md.load()","65460b9f":"def process_Text( text):\n    wordlist=[]\n    doc = nlp(text)\n    for ent in doc.ents:\n        wordlist.append(ent.text)\n    return ' '.join(wordlist)     ","40e0c751":"data['transcription'] = data['transcription'].apply(process_Text)\ndata['transcription'] = data['transcription'].apply(lemmatize_text)\ndata['transcription'] = data['transcription'].apply(clean_text)\n","6bcb0289":"\nprint('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\nprint('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\nprint('Sample Transcription 3:'+data.iloc[1000]['transcription'])","39c8c0d5":"\nvectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75,min_df=5, use_idf=True, smooth_idf=True,sublinear_tf=True, max_features=1000)\ntfIdfMat  = vectorizer.fit_transform(data['transcription'].tolist() )\nfeature_names = sorted(vectorizer.get_feature_names())\nprint(feature_names)","3c3db468":"import gc\ngc.collect()\ntfIdfMatrix = tfIdfMat.todense()\nlabels = data['medical_specialty'].tolist()\ntsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\nplt.figure(figsize=(20,10))\npalette = sns.hls_palette(12, l=.3, s=.9)\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue=labels,\n    palette= palette,\n    legend=\"full\",\n    alpha=0.3\n)\nplt.show()\n\n","2d83aa19":"pca = PCA(n_components=0.95)\ntfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\nlabels = data['medical_specialty'].tolist()\ncategory_list = data.medical_specialty.unique()","aaf1d473":"X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)   \nprint('Train_Set_Size:'+str(X_train.shape))\nprint('Test_Set_Size:'+str(X_test.shape))","e809f1af":"#clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\nclf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)\n","264eb1c8":"labels = category_list\ncm = confusion_matrix(y_test, y_test_pred, labels)\n\nfig = plt.figure(figsize=(20,20))\nax= fig.add_subplot(1,1,1)\nsns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \nplt.show()\nprint(classification_report(y_test,y_test_pred,labels=category_list))\n","95e3234d":"smote_over_sample = SMOTE(sampling_strategy='minority')\nlabels = data['medical_specialty'].tolist()\nX, y = smote_over_sample.fit_resample(tfIdfMat_reduced, labels)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)   \nprint('Train_Set_Size:'+str(X_train.shape))\nprint('Test_Set_Size:'+str(X_test.shape))","d1d31c0e":"clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)","1f1fbf84":"labels = category_list\ncm = confusion_matrix(y_test, y_test_pred, labels)\n\nfig = plt.figure(figsize=(20,20))\nax= fig.add_subplot(1,1,1)\nsns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \nplt.show()\nprint(classification_report(y_test,y_test_pred,labels=category_list))","4ed3de13":"mask = filtered_data_categories['medical_specialty'] == 'Radiology'\nradiologyData = filtered_data_categories[mask]\nprint(radiologyData['transcription'].tolist()[1])","69af5f5a":"mask = clinical_text_df['medical_specialty'] ==  ' Pediatrics - Neonatal'\npediaData = clinical_text_df[mask]\nprint(pediaData ['transcription'].tolist()[1])","7fb889f9":"\nmask = clinical_text_df['medical_specialty'] ==  ' Hematology - Oncology'\noncoData = clinical_text_df[mask]\nprint(oncoData ['transcription'].tolist()[1])","1c5d6c94":"Here we look at Medical Transcriptions dataset from Kaggle\nhttps:\/\/www.kaggle.com\/tboyle10\/medicaltranscriptions\n\nThis data was scraped from mtsamples.com\n\nInspiration\nCan you correctly classify the medical specialties based on the transcription text?\n","e9b3c7d0":"My learnings from this dataset are:\nThis dataset is very noisy.\n\nLot of text in transcriptions overlaps across categories\n\nWe can apply domain knowledge to reduce the categories\n\nIt is imbalanced dataset and using SMOTE can improve the results\n\nHand coded features may improve results on this dataset but may not apply to generic transcription datasets.","4948cc74":"Let us import all the necessary libraries","08f1398e":"Let us create train and test sets.Let us use logistic regression for developing a classification model and then visualize the results","e57ae9bc":"Lets visualize the tf-idf features using t-sne plot. For more on t-sne check here: https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.[1] It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\nThe t-sne plot shows that lot of categories are overlapping with each other.","51a0c824":"Let us extract tf-idf features then perform dimensionality reduction on the features using t-sne and plot the t-sne features","25054eaf":"Let us pre-process data using scispacy to detect medical entities in transcriptions","bd9c7056":"Since some catgeories have less than 50 samples i remove them","2fdd5dd6":"Lets plot the categories","89e1df18":"Lets define soome methods for cleaning the data","3df1856f":"We are interested only in the 'transcription' and 'medical_specialty' columns in the dataset","d604f8c2":"Let us visualize the data","613f8ede":"A method to get unique words(vocabulary) and sentence count in a list of text ","1bc5e952":"Let us use Logisitic Regression to learn on training data and predict on test data\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html","ef90fc23":"Still some categories are not getting classofoed properly.Let us look at samples from these classes","2b4dbd0a":"Let us use sciscpacy models to detect medical entities in our text\nscispaCy is a Python package containing spaCy models for processing biomedical, scientific or clinical text.\nFor more on scispacy check here:https:\/\/allenai.github.io\/scispacy\/","87f09b8c":"The results are quite poor. Let us apply some domain knowledge and see if we can improve the results\nThe surgey category is kind of superset as there can be surgeries belonging to specializations like cardiology,neurolrogy etc. Similarly other categories like Emergency Room Reports, Discharge Summary, Notes also overlap with specialities. Hence i remove them.","c4523a24":"Lets us peform feature extraction using TfidfVectorizer to generate tf-idf features.\nFor more on tf-idf check here: https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf\nIn information retrieval, tf\u2013idf or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.","212994ef":"Lets clean the data","539e99e7":"Let us do PCA to reduce dimensionality of features.\nhttps:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis\nPCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on","31366a70":"There is marked improvement in results. Since some classes are in minority we can use SMOTE(Synthetic Minority Over-sampling Technique\n) to generate more sample form minority class to solve the data imbalance problem. For more on SMOTE check here:https:\/\/arxiv.org\/pdf\/1106.1813.pdf. Let us generate new dataset using SMOTE and then perform classification on them\n","39c7d11f":"Lets do some exploratory analysis of data","c587a205":"Let us visualize the confusion matrix and the classification results"}}