{"cell_type":{"961c79ed":"code","5e5c0800":"code","066acbf4":"code","a38e602a":"code","10be4c06":"code","2463afc5":"code","e7a00d40":"code","5f4ba8dc":"code","fef0ac4f":"code","4b9d0af6":"code","dbc9d715":"code","46815221":"code","19ae9fc5":"code","a72ad128":"code","941b11c6":"code","af64e621":"code","51f7ba83":"code","4bfade7a":"code","9031df09":"code","5a0eb46e":"code","161af627":"code","e5c83c2a":"markdown","8a231059":"markdown","13aa08c4":"markdown","b3082868":"markdown","eadeecf9":"markdown","6ef6f9cb":"markdown","1dd517f4":"markdown","6d95a851":"markdown","fafd8029":"markdown","4615de60":"markdown","57e5b689":"markdown","769496df":"markdown","7a447225":"markdown"},"source":{"961c79ed":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e5c0800":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntrain.head()","066acbf4":"train.language.value_counts()","a38e602a":"def convert_answers(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }","10be4c06":"train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\ntrain.head()","2463afc5":"# train.to_csv('train.csv',index=False)","e7a00d40":"# ! pip install datasets transformers\n!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq","5f4ba8dc":"from datasets import load_dataset, Dataset\nfrom pprint import pprint","fef0ac4f":"# model_checkpoint = \"xlm-roberta-base\"\n# model_checkpoint = \"deepset\/xlm-roberta-large-squad2\"\nmodel_checkpoint = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-base-squad2\"\n# model_checkpoint = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\"\n\nfrom transformers import XLMTokenizer,AutoTokenizer\n# tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","4b9d0af6":"max_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.","dbc9d715":"# In some padding required on left side\npad_on_right = tokenizer.padding_side == \"right\"","46815221":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n#     print(len(offset_mapping))\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start\/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","19ae9fc5":"from sklearn.model_selection import StratifiedKFold\nfolds = 5\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n# df = train[:64]\ndf = train\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=df.language.values)):\n        df.loc[v_, 'kfold'] = f","a72ad128":"# dataset = Dataset.from_pandas(train[:-64])\n# eval_dataset = Dataset.from_pandas(train[-64:])\n\n#just to verify pipeline\n# dataset = Dataset.from_pandas(train[:32])\n# eval_dataset = Dataset.from_pandas(train[32:48])","941b11c6":"# tokenized_train_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)\n# tokenized_eval_datasets = eval_dataset.map(prepare_train_features, batched=True, remove_columns=eval_dataset.column_names)","af64e621":"import gc\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoConfig, AutoModel\nfrom transformers import default_data_collator\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam,AdamW\nfrom tqdm import tqdm\n\ndata_collator = default_data_collator\ngc.collect()\n%env WANDB_DISABLED=True\nconfig = {\n    'model': model_checkpoint,\n    'batch_size': 4,\n    \"epochs\": 3,#1,\n    'lr':0.001,\n    'weight_decay':0,\n    'grad_acc': 8\n}","51f7ba83":"# def chaiiModel(model_checkpoint):\n    \n#      return AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\nclass chaiiModel(nn.Module):\n    \n    def __init__(self):\n        super(chaiiModel,self).__init__()\n        \n        self.model_config = AutoConfig.from_pretrained(config['model'])\n#         print(self.model_config)\n        self.model_config.return_dict=True\n#         self.model = AutoModelForQuestionAnswering.from_pretrained(config['model'],config=self.model_config)\n        self.model = AutoModel.from_pretrained(config['model'], config=self.model_config)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(self.model_config.hidden_size,2)\n        \n    def forward(self,input_ids,attention_mask):\n        \n        output = self.model(input_ids,attention_mask)\n#         print(output[0].shape)\n#         print(output[1].shape)\n        x = self.dropout(output[0])\n        x = self.fc(x)\n#         print(x.shape)\n        start_logits,end_logits = x.split(1,dim=-1)\n#         print(start_logits.shape)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n#         print(start_logits.shape)\n                \n        return start_logits, end_logits","4bfade7a":"def safe_div(x,y):\n    if y == 0:\n        return 1\n    return x \/ y\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return safe_div(float(len(c)) , (len(a) + len(b) - len(c)))\n\ndef get_jaccard_score(y_true,y_pred):\n    assert len(y_true)==len(y_pred)\n    score=0.0\n    for i in range(len(y_true)):\n        score += jaccard(y_true[i], y_pred[i])\n        \n    return score\n\ndef chaii_loss(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n#     print('chaii_loss')\n#     print(start_logits.shape)\n#     print(start_positions.shape)\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = (start_loss + end_loss)\/2\n    return total_loss","9031df09":"def getData(df, fold):\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    dataset = Dataset.from_pandas(train)\n    eval_dataset = Dataset.from_pandas(valid)\n    tokenized_train_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)\n    tokenized_eval_datasets = eval_dataset.map(prepare_train_features, batched=True, remove_columns=eval_dataset.column_names)\n    \n    train_loader = DataLoader(tokenized_train_datasets,batch_size=config['batch_size'])\n    valid_loader = DataLoader(tokenized_eval_datasets,batch_size=config['batch_size'])\n    dataloaders = {'train':train_loader,'valid':valid_loader}\n    \n    return dataloaders","5a0eb46e":"def train_and_eval(model, dataloaders, criterion, optimizer, filename):\n    val_loss = 10000\n    for i in range(config['epochs']):\n        for j in ['train','valid']:\n            if j=='train':\n                print(f'training start for epoch {i}')\n                model.zero_grad()\n                model.train()\n                optimizer.zero_grad()\n            else:\n                print(f'eval start for epoch {i}')\n                model.eval()\n                \n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            for idx, data in enumerate(tqdm(dataloaders[j])):\n                input_ids, attention_mask = torch.stack(data['input_ids']).cuda(), torch.stack(data['attention_mask']).cuda()\n                start, end = torch.tensor(data['start_positions']).cuda(), torch.tensor(data['end_positions']).cuda()\n#                 input_ids, attention_mask = data['input_ids'], data['attention_mask']\n#                 start, end = data['start_positions'], data['end_positions']\n                start_logits, end_logits = model(\n                    input_ids=input_ids,\n                    attention_mask = attention_mask\n                )\n#                 print(start_logits.shape)\n                # The input is expected to contain raw, unnormalized scores for each class.    \n#                 start_logits = torch.softmax(start_logits, dim=1)#.cpu().detach().numpy()\n#                 end_logits = torch.softmax(end_logits, dim=1)#.cpu().detach().numpy()\n                start_logits = start_logits.permute(1,0)\n                end_logits = end_logits.permute(1,0)\n#                 start_logits = torch.softmax(start_logits, dim=1)\n#                 end_logits = torch.softmax(start_logits, dim=1)\n                loss = criterion(start_logits, end_logits, start, end)\n#                 print(loss.item())\n                epoch_loss+=loss.item()\n                if j =='train':\n#                     print('grad calculated')\n                    loss = loss\/config['grad_acc']\n                    loss.backward()\n                    if idx%config['grad_acc']==0:\n#                         print('weights updated')\n                        optimizer.step()\n                        optimizer.zero_grad()\n            if j =='train': \n                optimizer.step()\n                optimizer.zero_grad()\n            avg_epoch_loss = epoch_loss\/len(dataloaders[j])\n            print(f'{j} loss:{avg_epoch_loss} for epoch {i}')\n            if j == 'valid':\n                if val_loss>avg_epoch_loss:\n                    print(f'saving model initial val loss:{val_loss} which is now improve to {avg_epoch_loss}')\n                    val_loss = avg_epoch_loss\n                    torch.save(model.state_dict(), filename)\n                else:\n                    print('No improvement')\n    return 'Train val complete'\n            \n#     pass","161af627":"# args = getTrainArgs(model_checkpoint)\nfor fold in range(folds):\n        print(f'Preparing data for fold number {fold}')\n        dataloaders = getData(df, fold)\n        print(f'model {fold} loading ...')\n        model = chaiiModel()\n        model.cuda()\n        criterion = chaii_loss\n        optimizer = AdamW(model.parameters(),lr=config['lr'], weight_decay=config['weight_decay'] )\n        train_and_eval(model, dataloaders, criterion, optimizer, f\"chaii-trained-model-{fold}\")\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n        print(f\"Training fold {fold}\")\n        print(\"----------------------\")","e5c83c2a":"# putting all together","8a231059":"# Start","13aa08c4":"# Notebook I used for reference\n\nhttps:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n\nhttps:\/\/www.kaggle.com\/thedrcat\/chaii-eda-baseline\/comments\n\nhttps:\/\/www.kaggle.com\/shahules\/chaii-xlm-base-custom-qa-train-infer\/data?scriptVersionId=72059052\n\nhttps:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit","b3082868":"# Preprocessing","eadeecf9":"# Reading data","6ef6f9cb":"# Model","1dd517f4":"# Doc_stride is used to handle large text: tokens>512","6d95a851":"# Train and eval","fafd8029":"# Loss and Jac score","4615de60":"# Finetune start","57e5b689":"# generate Data","769496df":"# Creating Fold ","7a447225":"# Loading custom data in hugging face datasets lib"}}