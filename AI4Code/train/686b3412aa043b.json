{"cell_type":{"807ac702":"code","fa8cd463":"code","4b7cbfbe":"code","09ad88df":"code","37f68ade":"code","8179a1f3":"code","8472d0c6":"code","63904f76":"code","f06eac44":"code","0123ca25":"code","ec047af1":"code","758c9431":"code","03a93373":"code","236175f0":"code","7cb22147":"code","f7b09be6":"code","734ff328":"code","49808e2c":"code","8f4259b7":"code","b3ab258a":"code","00f01362":"code","c653d0bd":"code","1333ab71":"code","b9f9b09c":"code","fac16339":"code","95d92d82":"code","b431d810":"code","8a96a4a4":"code","a7f50512":"code","beae405b":"code","3e60cb96":"code","430a84fd":"code","d8e6d62f":"code","927173dc":"code","b8dde163":"code","0955ba5e":"code","88f4505d":"code","6d531cc5":"code","c51b6228":"code","c3cd92ca":"code","d79d1e0c":"code","23e7a4b2":"code","ecb57de5":"code","919b8e91":"code","11e60342":"code","b41aafd6":"code","741c5aa4":"code","683c4885":"code","6fefccc8":"code","c5733972":"code","322ea234":"code","048447b4":"code","0e88ca7d":"code","275eb4ac":"code","883d9375":"code","5e7bd2f8":"code","cba982b4":"code","e25fc466":"markdown","2c41c5a3":"markdown","9c999f1c":"markdown","f22f7230":"markdown","705cfd7d":"markdown","08555bc9":"markdown","26970dab":"markdown","ac52f0fa":"markdown","52ff2609":"markdown","87ac3125":"markdown","4bafda84":"markdown"},"source":{"807ac702":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa8cd463":"ls '\/kaggle\/input\/bhsignet\/BHSig260\/Hindi\/'","4b7cbfbe":"#%tensorflow_version 2.x\nimport tensorflow\ntensorflow.__version__","09ad88df":"import sys\nimport numpy as np\nimport pickle\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2\nimport time\nimport itertools\nimport random\n\nfrom sklearn.utils import shuffle\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout,GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Lambda, Flatten, Dense\nfrom tensorflow.keras.initializers import glorot_uniform\n\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","37f68ade":"F  = 160 * 30\nG = 160 * 24\nF,G","8179a1f3":"# from google.colab import drive\n# drive.mount('\/content\/drive\/')","8472d0c6":"# import osa\n# os.chdir('\/content\/drive\/My Drive\/Deep Learning\/GL\/Siamese\/')","63904f76":"# from zipfile import ZipFile\n# with ZipFile('BHSig260.zip', 'r') as z:\n#   z.extractall()","f06eac44":"path = \"\/kaggle\/input\/bhsignet\/BHSig260\/Hindi\/\"","0123ca25":"# Get the list of all directories and sort them\ndir_list = next(os.walk(path))[1]\ndir_list.sort()","ec047af1":"dir_list","758c9431":"# For each person segregate the genuine signatures from the forged signatures\n# Genuine signatures are stored in the list \"orig_groups\"\n# Forged signatures are stored in the list \"forged_groups\"\norig_groups, forg_groups = [], []\nfor directory in dir_list:\n    images = os.listdir(path+directory)\n    images.sort()\n    images = [path+directory+'\/'+x for x in images]\n    forg_groups.append(images[:30]) # First 30 signatures in each folder are forrged\n    orig_groups.append(images[30:]) # Next 24 signatures are genuine","03a93373":"# Quick check to confirm we have data of all the 160 individuals\nlen(orig_groups), len(forg_groups)","236175f0":"orig_groups[1]","7cb22147":"forg_groups[1]","f7b09be6":"orig_lengths = [len(x) for x in orig_groups]\nforg_lengths = [len(x) for x in forg_groups]","734ff328":"# Quick check to confirm that there are 24 Genuine signatures for each individual\nprint(orig_lengths)","49808e2c":"# Quick check to confirm that there are 30 Forged signatures for each individual\nprint(forg_lengths)","8f4259b7":"orig_train, orig_val, orig_test = orig_groups[:120], orig_groups[120:140], orig_groups[140:]\nforg_train, forg_val, forg_test = forg_groups[:120], forg_groups[120:140], forg_groups[140:]","b3ab258a":"# Delete unnecessary variables\ndel orig_groups, forg_groups","00f01362":"# All the images will be converted to the same size before processing\nimg_h, img_w = 155, 220","c653d0bd":"def visualize_sample_signature():\n    '''Function to randomly select a signature from train set and\n    print two genuine copies and one forged copy'''\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (10, 10))\n    k = np.random.randint(len(orig_train))\n    orig_img_names = random.sample(orig_train[k], 2)\n    forg_img_name = random.sample(forg_train[k], 1)\n    orig_img1 = cv2.imread(orig_img_names[0], 0)\n    orig_img2 = cv2.imread(orig_img_names[1], 0)\n    forg_img = cv2.imread(forg_img_name[0], 0)\n    orig_img1 = cv2.resize(orig_img1, (img_w, img_h))\n    orig_img2 = cv2.resize(orig_img2, (img_w, img_h))\n    forg_img = cv2.resize(forg_img, (img_w, img_h))\n\n    ax1.imshow(orig_img1, cmap = 'gray')\n    ax2.imshow(orig_img2, cmap = 'gray')\n    ax3.imshow(forg_img, cmap = 'gray')\n\n    ax1.set_title('Genuine Copy')\n    ax1.axis('off')\n    ax2.set_title('Genuine Copy')\n    ax2.axis('off')\n    ax3.set_title('Forged Copy')\n    ax3.axis('off')","1333ab71":"visualize_sample_signature()","b9f9b09c":"120 ","fac16339":"276 * 120","95d92d82":"276 * 120 + 24 * 12 *120","b431d810":"24* 11 * 120","8a96a4a4":"24 * 12","a7f50512":"(120 * 288) + (33120)","beae405b":"def generate_batch(orig_groups, forg_groups, batch_size = 32):\n    '''Function to generate a batch of data with batch_size number of data points\n    Half of the data points will be Genuine-Genuine pairs and half will be Genuine-Forged pairs'''\n    while True:\n        orig_pairs = []\n        forg_pairs = []\n        gen_gen_labels = []\n        gen_for_labels = []\n        all_pairs = []\n        all_labels = []\n        \n        # Here we create pairs of Genuine-Genuine image names and Genuine-Forged image names\n        # For every person we have 24 genuine signatures, hence we have \n        # 24 choose 2 = 276 , 12 * 23 Genuine-Genuine image pairs for one person.\n        # To make Genuine-Forged pairs, we pair every Genuine signature of a person\n        # with 12 randomly sampled Forged signatures of the same person.\n        # Thus we make 24 * 12 = 276 Genuine-Forged image pairs for one person.\n        # In all we have 120 person's data in the training data.\n        # 24*23\/2=276 \n        # Total no. of Genuine-Genuine pairs = 120 * 276(24*23\/2) = 33120\n        # Total number of Genuine-Forged pairs = 120 * 288 (24*12) = 34560\n        # Total no. of data points = 33120 + 34560 = 67680\n        for orig, forg in zip(orig_groups, forg_groups):\n            orig_pairs.extend(list(itertools.combinations(orig, 2)))\n            for i in range(len(forg)):\n                forg_pairs.extend(list(itertools.product(orig[i:i+1], random.sample(forg, 12))))\n        \n        # Label for Genuine-Genuine pairs is 1\n        # Label for Genuine-Forged pairs is 0\n        #print(len(orig_pairs),len(forg_pairs))\n        gen_gen_labels = [1]*len(orig_pairs)\n        gen_for_labels = [0]*len(forg_pairs)\n        \n        # Concatenate all the pairs together along with their labels and shuffle them\n        all_pairs = orig_pairs + forg_pairs\n        all_labels = gen_gen_labels + gen_for_labels\n        del orig_pairs, forg_pairs, gen_gen_labels, gen_for_labels\n        all_pairs, all_labels = shuffle(all_pairs, all_labels)\n        \n        # Note the lists above contain only the image names and\n        # actual images are loaded and yielded below in batches\n        # Below we prepare a batch of data points and yield the batch\n        # In each batch we load \"batch_size\" number of image pairs\n        # These images are then removed from the original set so that\n        # they are not added again in the next batch.\n            \n        k = 0\n        pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n        #print(pairs)\n        targets=np.zeros((batch_size,))\n        #print(targets)\n        for ix, pair in enumerate(all_pairs):\n            img1 = cv2.imread(pair[0], 0)\n            img2 = cv2.imread(pair[1], 0)\n            img1 = cv2.resize(img1, (img_w, img_h))\n            img2 = cv2.resize(img2, (img_w, img_h))\n            img1 = np.array(img1, dtype = np.float64)\n            img2 = np.array(img2, dtype = np.float64)\n            img1 \/= 255\n            img2 \/= 255\n            img1 = img1[..., np.newaxis]\n            img2 = img2[..., np.newaxis]\n            pairs[0][k, :, :, :] = img1\n            pairs[1][k, :, :, :] = img2\n            targets[k] = all_labels[ix]\n            k += 1\n            if k == batch_size:\n                yield pairs, targets\n                k = 0\n                pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n                targets=np.zeros((batch_size,))\n        \n    return pairs, targets","3e60cb96":"def euclidean_distance(vects):\n    '''Compute Euclidean Distance between two vectors'''\n    x, y = vects\n    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))","430a84fd":"def eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)","d8e6d62f":"def contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    Source: http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\n    \n    Explanation:\n    When ytrue is 1, that means the sample are duplicates of each other, \n    so the Euclidean distance (ypred) between their outputs must be minimized.\n    So the loss is taken as the square of that Euclidean distance itself - K.square(y_pred).\n    When ytrue is 0, i.e. the samples are not duplicates, then the Euclidean distance \n    between them must be maximized, at least to the margin. So the loss to be minimized\n    is the difference of the margin and the Euclidean distance - (margin - y_pred).\n    If the Euclidean distance (ypred) is already greater than the margin, \n    then nothing is to be learned, so the loss is made to be zero in \n    that case by saying K.maximum(margin - y_pred, 0).\n    '''\n    margin = 1\n    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))","927173dc":"def create_base_network_signet(input_shape):\n    '''Base Siamese Network'''\n    \n    seq = Sequential()\n    seq.add(Conv2D(96, kernel_size=(11, 11), activation='relu', name='conv1_1', strides=4, input_shape= input_shape))\n    seq.add(BatchNormalization())\n    seq.add(MaxPooling2D((2,2), strides=(1,1)))  \n    seq.add(Dropout(0.3))\n    #seq.add(ZeroPadding2D((2, 2)))\n    \n    seq.add(Conv2D(256, kernel_size=(5, 5), activation='relu', name='conv2_1'))\n    seq.add(BatchNormalization())\n    seq.add(MaxPooling2D((2,2), strides=(1,1)))\n    seq.add(Dropout(0.3))\n    #seq.add(ZeroPadding2D((1, 1)))\n    \n    seq.add(Conv2D(512, kernel_size=(3, 3), activation='relu', name='conv3_1'))\n    seq.add(MaxPooling2D((2,2), strides=(1,1)))\n    seq.add(Dropout(0.3))\n    #seq.add(ZeroPadding2D((1, 1)))\n    \n    #seq.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv3_2'))    \n    #seq.add(MaxPooling2D((2,2), strides=(1,1)))\n    #seq.add(Dropout(0.3))\n    #seq.add(Flatten(name='flatten'))\n    seq.add(GlobalAveragePooling2D())\n    seq.add(Dense(128, activation='relu'))\n    return seq","b8dde163":"input_shape=(img_h, img_w, 1)\ninput_shape","0955ba5e":"# network definition\nbase_network = create_base_network_signet(input_shape)\n\ninput_a = Input(shape=(input_shape))\ninput_b = Input(shape=(input_shape))\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\n# Compute the Euclidean distance between the two vectors in the latent space\ndistance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model(inputs=[input_a, input_b], outputs=distance)","88f4505d":"batch_sz = 128\nnum_train_samples = 276*120 + 24 * 12 *120\nnum_val_samples = num_test_samples = 276*20 + 288*20\nnum_train_samples, num_val_samples, num_test_samples","6d531cc5":"# compile model using RMSProp Optimizer and Contrastive loss function defined above\nrms = RMSprop(lr=1e-4)\nmodel.compile(loss=contrastive_loss, optimizer=rms,metrics=[accuracy])","c51b6228":"# Using Keras Callbacks, save the model after every epoch\n# Reduce the learning rate by a factor of 0.1 if the validation loss does not improve for 5 epochs\n# Stop the training using early stopping if the validation loss does not improve for 12 epochs\ncallbacks = [\n    EarlyStopping(patience=3, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.000001, verbose=1),\n    ModelCheckpoint('.\/signet.h5', verbose=1, save_weights_only=True)\n]","c3cd92ca":"results = model.fit(generate_batch(orig_train, forg_train, batch_sz),\n                              #steps_per_epoch = num_train_samples\/\/batch_sz,\n                              epochs = 5,\n                              validation_data = generate_batch(orig_val, forg_val, batch_sz),\n                              #validation_steps = num_val_samples\/\/batch_sz,\n                              callbacks = callbacks)","d79d1e0c":"df_new = pd.DataFrame([1,2])\ndf_new.head()\ndf_new.to_csv(\"test.csv\",index=False)","23e7a4b2":"def plot_training(H, plotPath):\n\t# construct a plot that plots and saves the training history\n\tplt.style.use(\"ggplot\")\n\tplt.figure()\n\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n\tplt.title(\"Training Loss and Accuracy\")\n\tplt.xlabel(\"Epoch #\")\n\tplt.ylabel(\"Loss\/Accuracy\")\n\tplt.legend(loc=\"lower left\")\n\t#plt.savefig(plotPath)","ecb57de5":"plot_training(results, \"None\")","919b8e91":"def compute_accuracy_roc(predictions, labels):\n    '''Compute ROC accuracy with a range of thresholds on distances.\n    '''\n    dmax = np.max(predictions)\n    dmin = np.min(predictions)\n    nsame = np.sum(labels == 1)\n    ndiff = np.sum(labels == 0)\n   \n    step = 0.01\n    max_acc = 0\n    best_thresh = -1\n   \n    for d in np.arange(dmin, dmax+step, step):\n        idx1 = predictions.ravel() <= d\n        idx2 = predictions.ravel() > d\n       \n        tpr = float(np.sum(labels[idx1] == 1)) \/ nsame       \n        tnr = float(np.sum(labels[idx2] == 0)) \/ ndiff\n        acc = 0.5 * (tpr + tnr)       \n#       print ('ROC', acc, tpr, tnr)\n       \n        if (acc > max_acc):\n            max_acc, best_thresh = acc, d\n           \n    return max_acc, best_thresh","11e60342":"model.load_weights('signet.h5')","b41aafd6":"import time\ns = time.time()","741c5aa4":"orig_test[0:2]","683c4885":"test_gen = generate_batch(orig_test[0:2], forg_test[0:2], 1)\npred, tr_y = [], []\nfor i in range(num_test_samples):\n    (img1, img2), label = next(test_gen)\n    tr_y.append(label)\n    pred.append(model.predict([img1, img2])[0][0])\ne =time.time()\nprint((e-s)\/60,\" minutes\")","6fefccc8":"tr_acc, threshold = compute_accuracy_roc(np.array(pred), np.array(tr_y))\ntr_acc, threshold","c5733972":"threshold=0.5","322ea234":"def predict_score():\n    '''Predict distance score and classify test images as Genuine or Forged'''\n    test_point, test_label = next(test_gen)\n    img1, img2 = test_point[0], test_point[1]\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 10))\n    ax1.imshow(np.squeeze(img1), cmap='gray')\n    ax2.imshow(np.squeeze(img2), cmap='gray')\n    ax1.set_title('Genuine')\n    if test_label == 1:\n        ax2.set_title('Genuine')\n    else:\n        ax2.set_title('Forged')\n    ax1.axis('off')\n    ax2.axis('off')\n    plt.show()\n    result = model.predict([img1, img2])\n    diff = result[0][0]\n    print(\"Difference Score = \", diff)\n    if diff > threshold:\n        print(\"Its a Forged Signature\")\n    else:\n        print(\"Its a Genuine Signature\")","048447b4":"predict_score()","0e88ca7d":"predict_score()","275eb4ac":"predict_score()","883d9375":"predict_score()","5e7bd2f8":"ls '\/kaggle\/input\/test-sing-surv'","cba982b4":"pairs=[np.zeros((1, img_h, img_w, 1)) for i in range(2)]\n#targets=np.zeros((batch_size,))\nimg1 = cv2.imread(\"\/kaggle\/input\/test-sing-surv\/survesh_genuine.tif\", 0)\nimg2 = cv2.imread(\"\/kaggle\/input\/test-sing-surv\/survesh_genuine_2.tif\", 0)\nimg1 = cv2.resize(img1, (img_w, img_h))\nimg2 = cv2.resize(img2, (img_w, img_h))\nimg1 = np.array(img1, dtype = np.float64)\nimg2 = np.array(img2, dtype = np.float64)\nimg1 \/= 255\nimg2 \/= 255\nimg1 = img1[..., np.newaxis]\nimg2 = img2[..., np.newaxis]\n\n\npred, tr_y = [], []\ntr_y = 1\nmodel.predict([np.expand_dims(img1, axis=0), np.expand_dims(img2, axis=0)])[0][0]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 10))\nax1.imshow(np.squeeze(img1), cmap='gray')\nax2.imshow(np.squeeze(img2), cmap='gray')\nax1.set_title('Genuine')\nif tr_y == 1:\n    ax2.set_title('Genuine')\nelse:\n    ax2.set_title('Forged')\nax1.axis('off')\nax2.axis('off')\nplt.show()\nresult = model.predict([np.expand_dims(img1, axis=0), np.expand_dims(img2, axis=0)])\ndiff = result[0][0]\nprint(\"Difference Score = \", diff)\nif diff > threshold:\n    print(\"Its a Forged Signature\")\nelse:\n    print(\"Its a Genuine Signature\")","e25fc466":"### Note: The first image is always Genuine. Score prediction and classification is done for the second image","2c41c5a3":"### Dataset Links:\n\nhttps:\/\/drive.google.com\/open?id=0B29vNACcjvzVc1RfVkg5dUh2b1E","9c999f1c":"### About the Dataset:\n\n\nThe BHSig260 signature dataset contains the signatures of 260 persons, among them 100 were signed in Bengali and 160 are signed in Hindi. \n\nFor each of the signers, 24 genuine and 30 forged signatures are available. This results in 100 \u00d7 24 = 2, 400 genuine and 100 \u00d7 30 = 3, 000 forged signatures in Bengali, and 160 \u00d7 24 = 3, 840 genuine and 160\u00d730 = 4, 800 forged signatures in Hindi.\n\nIn this task we are considering only Hindi singatures for easeness. \n\n\n**Paper Link:**  https:\/\/arxiv.org\/pdf\/1707.02131.pdf","f22f7230":"![keras_siamese_networks_process.png](attachment:keras_siamese_networks_process.png)","705cfd7d":"### Problem Statement\nSignature is one of the most popular and commonly accepted biometric hallmarks that has been used since the ancient times for verifying different entities related to human beings, viz. documents, forms, bank checks, individuals, etc. Therefore, signature verification is a critical task and many efforts have been made to remove the uncertainty involved in the manual authentication procedure, which makes signature verification an important research line in the field of machine learning and pattern recognition. \n\n\nIn this notebook, we model a writer independent **signature verification** task with a **convolutional Siamese network**.\n\n","08555bc9":"### Accuracy ROC","26970dab":"#### Load the weights from the epoch which gave the best validation accuracy","ac52f0fa":"### Lets us test my signature","52ff2609":"#### Accuracy = 83.37% and Threshold = 0.75\nThus if the differnce score is less than 0.32, we predict the test image as Genuine and if the difference score is greater than 0.32, we predict it to be as forged\n\n#### Below we see some sample results","87ac3125":"### Considering only Hindi signatures from the dataset.","4bafda84":"#### Train-Validation-Test Split\n* Signatures of 120 people are used for training\n* Signatures of 20 people are used for validation\n* Signatures of 20 people are used for testing"}}