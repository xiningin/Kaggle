{"cell_type":{"3b068091":"code","5f1f9352":"code","34cc5a35":"code","2e049bb7":"code","f611e708":"code","1b57ac51":"code","f56e2f66":"code","df51115f":"code","c6e30d62":"code","ea2f5009":"code","9cce38f2":"code","86d1af05":"markdown","8dce10f2":"markdown","817eb63b":"markdown","1d0b7cac":"markdown","56cc1195":"markdown","0a03ed2b":"markdown"},"source":{"3b068091":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\n\n#graphs and plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#model building + optimization\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nimport time\nimport sys\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f1f9352":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv', index_col = 'id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv', index_col = 'id')\nsample = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv', index_col = 'id')\ntest.head()","34cc5a35":"X = train.drop(['loss'], axis = 1)\ny = train['loss']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42)\nX_train.head()","2e049bb7":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=42, n_estimators = 10000, output_process=False):\n    #prep data\n    train_data = lgb.Dataset(X, label = y, free_raw_data=False)\n    \n    #parameters\n    def lgb_eval(learning_rate, num_leaves, feature_fraction, bagging_fraction, max_depth):\n        params = {'application':'regression','metric':'rmse', 'boosting':'gbdt', 'num_iterations': 5000, 'early_stopping_rounds': 500 }\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n\n        \n        \n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified = True, verbose_eval=200, metrics=['rmse'])\n        return max(cv_result['rmse-mean'])\n    \n    lgbBO = BayesianOptimization(lgb_eval,{'learning_rate': (0.01, 0.5),\n                                            'num_leaves': (10, 200),\n                                            'feature_fraction': (0.1, 1.0),\n                                            'bagging_fraction': (0.1, 1.0),\n                                            'max_depth': (1, 30),\n                                           }, random_state=42)\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.   \n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    return lgbBO.max['params']\n\nopt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=42,n_estimators=10000)","f611e708":"# We have optimal parameters so now we need to do a cv test with the optimal parameterts\nopt_params","1b57ac51":"op = {'task':'train','application':'regression','metric':'rmse', 'boosting':'gbdt', 'num_iterations': 5000, 'early_stopping_rounds': 500,\n    'bagging_fraction': 0.3253848734026177,\n 'feature_fraction': 0.30860836505782663,\n 'learning_rate': 0.08689261851358254,\n 'max_depth': 2,\n 'num_leaves': 179\n     }\n","f56e2f66":"trn_data = lgb.Dataset(X_train, label = y_train)\nval_data = lgb.Dataset(X_val, label = y_val, reference = trn_data)\nregressor = lgb.train(op, trn_data, verbose_eval = 50, valid_sets = val_data)","df51115f":"y_pred = regressor.predict(test)","c6e30d62":"sub = pd.DataFrame(index=test.index)\nsub['loss'] = y_pred\nsub.head()","ea2f5009":"sub.describe()","9cce38f2":"sub.to_csv('submission.csv')","86d1af05":"## Parameter Optimization\nThis method has been inspired by : <a href=\"https:\/\/medium.com\/analytics-vidhya\/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9\">Hyperparameters Optimization for LightGBM, CatBoost and XGBoost Regressors using Bayesian Optimization.<\/a>","8dce10f2":"### Optimal parameters","817eb63b":"## Data Importing","1d0b7cac":"## Modeling\nUsing the optimal parameters detailed above we train our LGB model before using them to predict from the test set and submitting","56cc1195":"# LGBM Baseline\n\nBaseline score with no optimization was 7.95860\nWe will now use Bayesian optimization to try to increase that in a short amount of time.\n### Data preprocessing\nSince we know from EDA (as seen here <a href=\"https:\/\/www.kaggle.com\/subinium\/tps-aug-simple-eda\">Some awesome EDA not by me<\/a>), that there are no missing data points, and we are assuming there are no categorical variables (though there are quite a few non-float ones), the data preprocessing is quite simple. ","0a03ed2b":"# LGBM + Bayesian Optimization\nAfter running this once and it not going to plan, I am running it again after going over the code more closely. I still don't think this method is that good so any comments and criticisms for improvement are welcome.\n"}}