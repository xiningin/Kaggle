{"cell_type":{"a7fc1df4":"code","f14b4966":"code","31fd2edf":"code","512d84d9":"code","88fe8806":"code","c0ba885a":"code","6a3e140d":"code","df593266":"code","b8dee280":"code","3b409088":"code","46dc149b":"code","9c4baa30":"code","27bdac1e":"code","2f8eac67":"code","a500c909":"code","46d2fd8f":"code","86ea91f8":"code","7d19929b":"code","228bc867":"code","f387b4c0":"code","58dba498":"code","35d71c73":"markdown","9f617e57":"markdown","7bd695b8":"markdown","05b79aed":"markdown","1779ea67":"markdown","a690ad4f":"markdown"},"source":{"a7fc1df4":"from __future__ import print_function, division\nfrom builtins import range, input\n\nfrom keras.layers import Input, Lambda, Dense, Flatten\nfrom keras.layers import AveragePooling2D, MaxPooling2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.models import Model, Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom skimage.transform import resize\nimport tensorflow as tf\n\nimport keras.backend as K\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nimport itertools\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy.optimize import fmin_l_bfgs_b\n\ntf.compat.v1.disable_eager_execution()","f14b4966":"def VGG16_AvgPool(shape):\n  # we want to account for features across the entire image\n  # so get rid of the maxpool which throws away information\n  vgg = VGG16(input_shape=shape, weights='imagenet', include_top=False)\n  i = vgg.input\n  x = i\n  for layer in vgg.layers:\n    if layer.__class__ == MaxPooling2D:\n      # replace it with average pooling\n      x = AveragePooling2D()(x)\n    else:\n      x = layer(x)\n\n  return Model(i, x)\n\ndef VGG16_AvgPool_CutOff(shape, num_convs):\n  # there are 13 convolutions in total\n  # we can pick any of them as the \"output\"\n  # of our content model\n\n  if num_convs < 1 or num_convs > 13:\n    print(\"num_convs must be in the range [1, 13]\")\n    return None\n\n  model = VGG16_AvgPool(shape)\n  n = 0\n  output = None\n  for layer in model.layers:\n    if layer.__class__ == Conv2D:\n      n += 1\n    if n >= num_convs:\n      output = layer.output\n      break\n\n  return Model(model.input, output)","31fd2edf":"def unpreprocess(img):\n  img[..., 0] += 103.939\n  img[..., 1] += 116.779\n  img[..., 2] += 126.68\n  img = img[..., ::-1]\n  return img","512d84d9":"def scale_img(x):\n  x = x - x.min()\n  x = x \/ x.max()\n  return x","88fe8806":"def load_img_and_preprocess(path, shape=None):\n  img = image.load_img(path, target_size=shape)\n\n  # convert image to array and preprocess for vgg\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n  x = preprocess_input(x)\n\n  return x","c0ba885a":"def gram_matrix(img):\n  # input is (H, W, C) (C = # feature maps)\n  # we first need to convert it to (C, H*W)\n  X = K.batch_flatten(K.permute_dimensions(img, (2, 0, 1)))\n  \n  # now, calculate the gram matrix\n  # gram = XX^T \/ N\n  # the constant is not important since we'll be weighting these\n  G = K.dot(X, K.transpose(X)) \/ img.get_shape().num_elements()\n  return G","6a3e140d":"def style_loss(y, t):\n  return K.mean(K.square(gram_matrix(y) - gram_matrix(t)))\n\n\ndef minimize(fn, epochs, batch_shape):\n  t0 = datetime.now()\n  losses = []\n  x = np.random.randn(np.prod(batch_shape))\n  for i in range(epochs):\n    x, l, _ = fmin_l_bfgs_b(\n      func=fn,\n      x0=x,\n      maxfun=20\n    )\n    x = np.clip(x, -127, 127)\n    print(\"iter=%s, loss=%s\" % (i, l))\n    losses.append(l)\n\n  print(\"duration:\", datetime.now() - t0)\n  plt.plot(losses)\n  plt.show()\n\n  newimg = x.reshape(*batch_shape)\n  final_img = unpreprocess(newimg)\n  return final_img[0]","df593266":"style_path = '..\/input\/tamil-nst\/TamilStyleImages'\ncontent_path = '..\/input\/tamil-nst\/TamilContentImages'","b8dee280":"# useful for getting number of files\nstyle_files = glob(style_path + '\/*.jp*g')\ncontent_files = glob(content_path + '\/*.jp*g')","3b409088":"c_img = np.random.choice(content_files)\ns_img = np.random.choice(style_files)","46dc149b":"content_img = load_img_and_preprocess(\n  c_img,\n  #(225, 300),\n)","9c4baa30":"\nh, w = content_img.shape[1:3]\nstyle_img = load_img_and_preprocess(\n    s_img,\n  (h, w)\n)\n\n\n# we'll use this throughout the rest of the script\nbatch_shape = content_img.shape\nshape = content_img.shape[1:]","27bdac1e":"# we want to make only 1 VGG here\n# as you'll see later, the final model needs\n# to have a common input\nvgg = VGG16_AvgPool(shape)","2f8eac67":"# create the content model\n# we only want 1 output\ncontent_model = Model(vgg.input, vgg.layers[13].get_output_at(0))\ncontent_target = K.variable(content_model.predict(content_img))","a500c909":"# create the style model\n# we want multiple outputs\nsymbolic_conv_outputs = [\n  layer.get_output_at(1) for layer in vgg.layers \\\n  if layer.name.endswith('conv1')\n]\n\n# make a big model that outputs multiple layers' outputs\nstyle_model = Model(vgg.input, symbolic_conv_outputs)\n\n# calculate the targets that are output at each layer\nstyle_layers_outputs = [K.variable(y) for y in style_model.predict(style_img)]\n\n# we will assume the weight of the content loss is 1\n# and only weight the style losses\nstyle_weights = [0.2,0.4,0.3,0.5,0.2]","46d2fd8f":"# create the total loss which is the sum of content + style loss\nloss = K.mean(K.square(content_model.output - content_target))\n\nfor w, symbolic, actual in zip(style_weights, symbolic_conv_outputs, style_layers_outputs):\n  # gram_matrix() expects a (H, W, C) as input\n  loss += w * style_loss(symbolic[0], actual[0])\n\n#tf.compat.v1.disable_eager_execution()\n# once again, create the gradients and loss + grads function\n# note: it doesn't matter which model's input you use\n# they are both pointing to the same keras Input layer in memory\ngrads = K.gradients(loss, vgg.input)\n\nget_loss_and_grads = K.function(\n  inputs=[vgg.input],\n  outputs=[loss] + grads\n)","86ea91f8":"def get_loss_and_grads_wrapper(x_vec):\n  l, g = get_loss_and_grads([x_vec.reshape(*batch_shape)])\n  return l.astype(np.float64), g.flatten().astype(np.float64)","7d19929b":"final_img = minimize(get_loss_and_grads_wrapper, 10, batch_shape)","228bc867":"plt.imshow(image.load_img(c_img))","f387b4c0":"plt.imshow(image.load_img(s_img))","58dba498":"plt.imshow(scale_img(final_img))\nplt.show()","35d71c73":"**load the content image**","9f617e57":"# plot the style image","7bd695b8":"# plot the new image","05b79aed":"resize the style image\nsince we don't care too much about warping it","1779ea67":"# Introduction","a690ad4f":"# plot the content image"}}