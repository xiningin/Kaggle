{"cell_type":{"d3075c3f":"code","01b048bb":"code","0c90d755":"code","5488c7fa":"code","46ee06b4":"code","4841d4ae":"code","952f5af2":"code","53fb074b":"code","44f9cb07":"code","13c52bf4":"code","39f79ffc":"code","66c5fe13":"code","2dd0f8f3":"code","94f4b540":"code","f909b21e":"code","e3f444ab":"code","dd752f29":"code","cfce4c99":"code","8fa2f1bd":"code","81aae552":"code","70e0ef2f":"code","b5089af8":"code","b3975470":"code","4dd8ca73":"code","701ed200":"code","a98b67a3":"code","2d29aeae":"code","741a4b96":"code","77d8c553":"code","d70ded8a":"code","b20b7916":"code","9637c61b":"code","be89203a":"markdown","8a06b817":"markdown","5b3b65b7":"markdown","ffff4837":"markdown","b74c27ce":"markdown","456ae148":"markdown","5a622fb0":"markdown","d5a0a676":"markdown","62feb1ae":"markdown","d6d04d21":"markdown","4726048e":"markdown","b7ac2177":"markdown","39e9e81c":"markdown","ee8df781":"markdown","3e72f095":"markdown","0d179801":"markdown","f253bbe6":"markdown","f55d699a":"markdown","2b1bcf70":"markdown","392c0def":"markdown","5098c085":"markdown","523c4747":"markdown"},"source":{"d3075c3f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor,XGBRFRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nsns.set_style(\"darkgrid\")","01b048bb":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv',index_col='id')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv',index_col='id')\ndf_sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv',index_col='id')","0c90d755":"df_train.head()","5488c7fa":"df_test.head()","46ee06b4":"# df_sub.head()","4841d4ae":"df_train.info()","952f5af2":"df_test.info()","53fb074b":"df_train.describe()","44f9cb07":"df_test.describe()","13c52bf4":"# get the number of missing data points per column\ndf_train.isnull().sum()","39f79ffc":"def plot_feature_target_scatter(df, features):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(5, 3,figsize=(14, 24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5, 3, i)\n        plt.scatter(df[feature], df['target'], marker='+', color='purple')\n        plt.xlabel(feature, fontsize=9)\n    plt.show()","66c5fe13":"features = ['cont1', 'cont2','cont3','cont4', 'cont5', 'cont6', 'cont7',\n            'cont8', 'cont9','cont10','cont11', 'cont12', 'cont13', 'cont14']\n\nplot_feature_target_scatter(df_train[::15], features)","2dd0f8f3":"def plot_feature_distribution(df1, df2, features):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(5, 3,figsize=(14, 24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5, 3,i)\n        sns.distplot(df1[feature],color=\"orange\", kde=True,bins=120, label='train')\n        sns.distplot(df2[feature],color=\"purple\", kde=True,bins=120, label='test')\n        plt.xlabel(feature, fontsize=9); plt.legend()\n    plt.show()","94f4b540":"plot_feature_distribution(df_train[::15],df_test[::10], features)","f909b21e":"plt.figure(figsize=(16, 16))\nheatmap = sns.heatmap(np.round(df_train[features].corr(), 3), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features correlation', fontdict={'fontsize':10}, pad=10)\nplt.title(\"Spearman correlation - test data\")\nplt.show()","e3f444ab":"features_target = features + ['target']\nplt.figure(figsize=(16, 16))\nheatmap = sns.heatmap(np.round(df_train[features_target].corr(), 3), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features correlation', fontdict={'fontsize':10}, pad=10)\nplt.title(\"Spearman correlation - train data\")\nplt.show()","dd752f29":"target = df_train.pop('target')\nX_train, X_test, y_train, y_test = train_test_split(df_train, target, train_size=0.60)","cfce4c99":"def plot_results(name, y, yhat, num_to_plot=10000, lims=(0,12), figsize=(15,8)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, yhat, squared=False)\n    sns.scatterplot(y[:num_to_plot], yhat[:num_to_plot])\n    plt.plot(lims, lims)\n    plt.ylim(lims)\n    plt.xlim(lims)\n    plt.title(f'{name}: {score:0.5f}', fontsize=18)\n    plt.show()","8fa2f1bd":"linear_model = LinearRegression(fit_intercept=False)\nlinear_model.fit(X_train, y_train)\ny_linear = linear_model.predict(X_test)\nscore_linear = mean_squared_error(y_test, y_linear, squared=False)\nprint(f'{score_linear:0.5f}')","81aae552":"plot_results('Linear',y_test,y_linear)","70e0ef2f":"lasso_model = Lasso(fit_intercept=False)\nlasso_model.fit(X_train, y_train)\ny_lasso = lasso_model.predict(X_test)\nscore_lasso = mean_squared_error(y_test, y_lasso, squared=False)\nprint(f'{score_lasso:0.5f}')","b5089af8":"plot_results('Lasso',y_test,y_lasso)","b3975470":"dtree_model = DecisionTreeRegressor(random_state=0)\ndtree_model.fit(X_train, y_train)\ny_dtree = dtree_model.predict(X_test)\nscore_dtree = mean_squared_error(y_test, y_dtree, squared=False)\nprint(f'{score_dtree:0.5f}')","4dd8ca73":"plot_results('Decision Tree',y_test,y_dtree)","701ed200":"rf_model = RandomForestRegressor(n_estimators=50, n_jobs=-1)\nrf_model.fit(X_train, y_train)\ny_rf = rf_model.predict(X_test)\nscore_dtree = mean_squared_error(y_test, y_rf, squared=False)\nprint(f'{score_dtree:0.5f}')","a98b67a3":"plot_results('Random Forest',y_test,y_rf)","2d29aeae":"gb_model = GradientBoostingRegressor(n_estimators=100,max_depth=5)\ngb_model.fit(X_train, y_train)\ny_gb = gb_model.predict(X_test)\nscore_gb = mean_squared_error(y_test, y_gb, squared=False)\nprint(f'{score_gb:0.5f}')","741a4b96":"plot_results('Gradient Boosting',y_test,y_gb)","77d8c553":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = XGBRegressor(n_jobs = -1)\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           cv = 5,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n    return gsearch.best_params_\n","d70ded8a":"# hyperParameterTuning(X_train, y_train)","b20b7916":"xgb_model = XGBRegressor(\n        objective = 'reg:squarederror',\n        colsample_bytree = 0.5,\n        learning_rate = 0.05,\n        max_depth = 6,\n        min_child_weight = 1,\n        n_estimators = 1000,\n        subsample = 0.7)\n\nxgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)], verbose=False)\n\ny_pred_xgb = xgb_model.predict(X_test)\n\nmae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n\nprint(\"MAE: \", mae_xgb)","9637c61b":"# dtree_model = DecisionTreeRegressor()\n# dtree_model.fit(df_train, target)\n# df_sub['target'] = dtree_model.predict(df_test)\n# df_sub.to_csv('dtree_submission.csv')\n\n# model = RandomForestRegressor(n_estimators=50, n_jobs=-1)\n# model.fit(df_train, target)\n# df_sub['target'] = model.predict(df_test)\n# df_sub.to_csv('submission.csv')\n\n# model=GradientBoostingRegressor(n_estimators=100,max_depth=5)\n# model.fit(df_train, target)\n# df_sub['target'] = model.predict(df_test)\n# df_sub.to_csv('submission.csv')\n\n\ndf_sub['target'] = xgb_model.predict(df_test)\ndf_sub.to_csv(\"submission.csv\")","be89203a":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Lasso Regression Model<\/h2>","8a06b817":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Linear Regression<\/h2>\n\nLinear regression is a linear approach to modelling the relationship between a dependent variable and one or more independent variables.\n\nIn Multiple linear regression more than one predictor variables are used to predict the response variable.","5b3b65b7":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Reading the data<\/h2>","ffff4837":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Reference Notebook<\/h2>\n\n- [Tabular Playground Series January EDA](https:\/\/www.kaggle.com\/gpreda\/tabular-playground-series-january-eda)","b74c27ce":"<h3>Basic terminology used with Decision trees:<\/h3>\n\n- **Root Node:** It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n- **Splitting:** It is a process of dividing a node into two or more sub-nodes.\n- **Decision Node:** When a sub-node splits into further sub-nodes, then it is called decision node.\n- **Leaf\/ Terminal Node:** Nodes do not split is called Leaf or Terminal node.\n- **Pruning:** When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n- **Branch \/ Sub-Tree:** A sub section of entire tree is called branch or sub-tree.\n- **Parent and Child Node:** A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.","456ae148":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Random Forest Regressor Model<\/h2>","5a622fb0":"### Features correlation","d5a0a676":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">What is a Decision Tree ?<\/h2>\n\nDecision tree is a type of supervised learning algorithm that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter \/ differentiator in input variables.","62feb1ae":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Gradient Boosting Regressor Model<\/h2>","d6d04d21":"### Features distribution","4726048e":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Decision Tree Regressor Model<\/h2>","b7ac2177":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">XGBoost (eXtreme Gradient Boosting)<\/h2>\n\nXGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. It\u2019s feature to implement parallel computing makes it at least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.","39e9e81c":"### Scatter plot of features vs. target\n\nScatter plot of each feature in train vs. target values.","ee8df781":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Bivariate Analysis<\/h2>\n\n**Bivariate analysis** is the simultaneous analysis of two variables (attributes). It explores the concept of relationship\nbetween two variables, whether there exists an association and the strength of this association, or whether there are\ndifferences between two variables and the significance of these differences.","3e72f095":"<h1 style=\"background-color:#45ffa3;text-align:left;color:#aa45ff\">Contents<\/h1>\n\n- Basic Data Analysis and Visualization\n- Linear Algorithms\n- Tree Based Algorithms\n    - Decision Tree\n    - Random Forest\n    - Gradient Boosting (GBM)\n    - XGboost","0d179801":"<h1 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Tabular Playground Series - Jan 2021<\/h1>","f253bbe6":"<h3>Advantages and Disadvantages<\/h3>\n\n**Advantages**\n\n- Easy to Understand\n- Useful in Data exploration\n- Less data cleaning required\n- Data type is not a constraint\n- Non Parametric Method\n\n**Disadvantages**\n\n- Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning","f55d699a":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Importing required libraries<\/h2>","2b1bcf70":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Types of Decision Trees<\/h2>\n\nTypes of decision tree is based on the type of target variable we have. It can be of two types:\n\n1. **Categorical Variable Decision Tree:** Decision Tree which has categorical target variable then it called as categorical variable decision tree.\n2. **Continuous Variable Decision Tree:** Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.\n","392c0def":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Submission<\/h2>","5098c085":"### Best Fit","523c4747":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Introduction to Tree Based Algorithms<\/h2>\n\nTree based algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based algorithms empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).\nMethods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. "}}