{"cell_type":{"8b5b05e6":"code","be96b491":"code","71622426":"code","4c9ef58d":"code","ad9a99fb":"code","0e5e0da5":"code","218f9ec2":"code","997954d9":"code","ed88f7ba":"code","9b5e9945":"code","0c13efa8":"code","82dc76f5":"code","864e4bb1":"code","ecdf5bf4":"code","55d2ac2f":"code","1f550a34":"code","780dc64d":"code","5c6b02f7":"code","e2610e02":"code","21133248":"code","53828752":"code","0927d020":"code","9de131a5":"code","8739dfaf":"code","d8f02c5e":"code","20c12cee":"code","e1e3ac1a":"code","6129c185":"code","6acbff50":"code","f14fe124":"code","4aa76dbf":"code","38aa7374":"code","032e8d97":"code","74678b72":"code","42d8376d":"code","d1b7401e":"markdown","b252a11f":"markdown","e598d00c":"markdown","2d57e2a2":"markdown","b1a0256b":"markdown","8727d412":"markdown","0e82a4d0":"markdown","a3681786":"markdown","0f92577b":"markdown","326fd802":"markdown","c7d26853":"markdown","2126e07f":"markdown","dd3564ca":"markdown","9fbacd00":"markdown","104f0914":"markdown","eeeccf00":"markdown","f575d874":"markdown","8fc795ab":"markdown","78a49ef4":"markdown","e60a52dd":"markdown","ea4734f9":"markdown","7185ddde":"markdown"},"source":{"8b5b05e6":"import numpy as np\nimport pandas as pd \nimport os\nfrom typing import Callable, List, Tuple\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","be96b491":"SEED = 42","71622426":"train_df = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/test.csv')\nsample_submission = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/sample_submission.csv\")","4c9ef58d":"train_df.isnull().sum()","ad9a99fb":"test_df.isnull().sum()","0e5e0da5":"train_df.nunique()","218f9ec2":"cat_features = ['key', 'audio_mode', 'time_signature']\nnot_useful_features = [\"id\", \"song_popularity\", \"k_fold\"]\nuseful_features = [col for col in train_df.columns if col not in not_useful_features]\ntest_df = test_df[useful_features]","997954d9":"n_splits = 5\nX = train_df.drop('song_popularity', axis=1)\ny = train_df['song_popularity']\n\nskf = StratifiedKFold(n_splits = n_splits, random_state=SEED, shuffle=True)","ed88f7ba":"for fold, (train_ind, val_ind) in enumerate(skf.split(X, y)):\n    train_df.at[val_ind, \"k_fold\"] = fold","9b5e9945":"def predict_pipeline_whole(imputer: Callable, train_df: pd.DataFrame, test_df: pd.DataFrame, \n                           not_useful_features: List[str], useful_features: List[str], \n                           n_splits: int, add_indicator: bool = False) -> Tuple[List[list], List[float], Callable]:\n    final_predictions = []\n    val_scores = []\n    train_df_export, test_df_export = None, None\n    \n    train_df_imputed = imputer.fit_transform(train_df[useful_features])\n    test_df_imputed = imputer.transform(test_df)\n    \n    if add_indicator:\n        missing_values_columns = train_df.columns[train_df.isnull().sum() != 0]\n        missing_values_columns = [col + \"_missing\" for col in missing_values_columns]\n        useful_features_missing = useful_features + missing_values_columns\n        \n        train_df_export = pd.concat([pd.DataFrame(train_df_imputed, columns=useful_features_missing), \n                              train_df[not_useful_features]], axis=1)\n        test_df_export = pd.DataFrame(test_df_imputed, columns=useful_features_missing)\n        \n        train_df = pd.concat([pd.DataFrame(train_df_imputed, columns=useful_features_missing), \n                          train_df[not_useful_features]], axis=1).drop(missing_values_columns, axis=1)\n        test_df = pd.DataFrame(test_df_imputed, columns=useful_features_missing).drop(missing_values_columns, axis=1)\n\n    for fold in range(n_splits):\n        x_train = train_df[train_df['k_fold'] != fold].reset_index(drop=True)\n        x_val = train_df[train_df['k_fold'] == fold].reset_index(drop=True)\n        x_test = test_df.copy()\n\n        y_train = x_train['song_popularity']\n        y_val = x_val['song_popularity']\n\n        x_train = x_train[useful_features]\n        x_val = x_val[useful_features]\n#         x_train['key'] = x_train['key'].round()\n#         x_test['key'] = x_test['key'].round()\n#         for col in cat_features:\n#             x_train[col] = x_train[col].astype(\"category\")\n#             x_test[col] = x_test[col].astype(\"category\")\n             \n        model = XGBClassifier(random_state=fold, n_jobs=4, use_label_encoder=False, eval_metric='auc')\n#         model = XGBClassifier(random_state=fold, tree_method=\"gpu_hist\", enable_categorical=True, \n#                               use_label_encoder=False, eval_metric='auc')\n        model.fit(x_train, y_train)\n        val_pred = model.predict(x_val)\n        test_pred = model.predict(x_test)\n\n        final_predictions.append(test_pred)\n        roc_auc_value = roc_auc_score(y_val, val_pred)\n        val_scores.append(roc_auc_value)\n        print(fold, roc_auc_score(y_val, val_pred))\n    \n    return final_predictions, val_scores, imputer, train_df_export, test_df_export","0c13efa8":"def predict_pipeline_k_1(imputer: Callable, train_df: pd.DataFrame, test_df: pd.DataFrame, \n                           not_useful_features: List[str], useful_features: List[str], \n                           n_splits: int) -> Tuple[List[list], List[float], Callable]:\n    final_predictions = []\n    val_scores = []\n\n    for fold in range(n_splits):\n        x_train = train_df[train_df['k_fold'] != fold].reset_index(drop=True)\n        x_val = train_df[train_df['k_fold'] == fold].reset_index(drop=True)\n        x_test = test_df.copy()\n\n        y_train = x_train['song_popularity']\n        y_val = x_val['song_popularity']\n\n        x_train = x_train[useful_features]\n        x_val = x_val[useful_features]\n        \n        x_train_imputed = imputer.fit_transform(x_train)\n        x_val_imputed = imputer.transform(x_val)\n        x_test_imputed = imputer.transform(x_test)\n\n        model = XGBClassifier(random_state=fold, n_jobs=4, use_label_encoder=False, eval_metric='auc')\n        model.fit(x_train, y_train)\n        val_pred = model.predict(x_val)\n        test_pred = model.predict(x_test)\n\n        final_predictions.append(test_pred)\n        roc_auc_value = roc_auc_score(y_val, val_pred)\n        val_scores.append(roc_auc_value)\n        print(fold, roc_auc_score(y_val, val_pred))\n    \n    return final_predictions, val_scores, imputer","82dc76f5":"simple_imputer = SimpleImputer(strategy=\"mean\", add_indicator=False)","864e4bb1":"final_predictions_simple_mean_whole, val_scores, imp, _, _ = predict_pipeline_whole(simple_imputer, train_df, test_df, \n                                                                        not_useful_features, useful_features, \n                                                                        n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_whole' function with {imp} and {imp.strategy} strategy is \"\n      f\"{np.mean(val_scores).round(3)} +- {np.std(val_scores).round(3)}\") ","ecdf5bf4":"final_predictions_simple_mean_k_1, val_scores, imp = predict_pipeline_k_1(simple_imputer, train_df, test_df, \n                                                                        not_useful_features, useful_features, \n                                                                        n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_k_1' function with {imp} and {imp.strategy} strategy is \"\n      f\"{np.mean(val_scores).round(3)} +- {np.std(val_scores).round(3)}\") ","55d2ac2f":"simple_imputer = SimpleImputer(strategy=\"median\", add_indicator=False)","1f550a34":"final_predictions_simple_med_whole, val_scores, imp, _, _ = predict_pipeline_whole(simple_imputer, train_df, test_df, \n                                                                        not_useful_features, useful_features, \n                                                                        n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_whole' function with {imp} and {imp.strategy} strategy is \"\n      f\"{np.mean(val_scores).round(3)} +- {np.std(val_scores).round(3)}\") ","780dc64d":"final_predictions_simple_med_k_1, val_scores, imp = predict_pipeline_k_1(simple_imputer, train_df, test_df, \n                                                                        not_useful_features, useful_features, \n                                                                        n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_k_1' function with {imp} and {imp.strategy} strategy is \"\n      f\"{np.mean(val_scores).round(3)} +- {np.std(val_scores).round(3)}\") ","5c6b02f7":"knn_imputer = KNNImputer(n_neighbors=1)","e2610e02":"final_predictions_knn_whole, val_scores, imp, _, _ = predict_pipeline_whole(knn_imputer, train_df, test_df, \n                                                                            not_useful_features, useful_features, \n                                                                            n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_whole' function with {imp} and 1 n_neighbors is {np.mean(val_scores).round(3)}\"\n      f\" +- {np.std(val_scores).round(3)}\") ","21133248":"final_predictions_knn_k_1, val_scores, imp = predict_pipeline_k_1(knn_imputer, train_df, test_df, \n                                                                  not_useful_features, useful_features, n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_k_1' function with {imp} and 1 n_neighbors is {np.mean(val_scores).round(3)}\"\n      f\" +- {np.std(val_scores).round(3)}\") ","53828752":"iterative_imputer = IterativeImputer(max_iter=20, initial_strategy='median')","0927d020":"final_predictions_it_med_whole, val_scores, imp, _, _ = predict_pipeline_whole(iterative_imputer, train_df, test_df, \n                                                                   not_useful_features, useful_features, n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_whole' function with {imp} is {np.mean(val_scores).round(3)} \"\n      f\"+- {np.std(val_scores).round(3)}\")","9de131a5":"final_predictions_it_med_k_1, val_scores, imp = predict_pipeline_k_1(iterative_imputer, train_df, test_df, \n                                                                   not_useful_features, useful_features, n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_k_1' function with {imp} is {np.mean(val_scores).round(3)} \"\n      f\"+- {np.std(val_scores).round(3)}\")","8739dfaf":"iterative_imputer = IterativeImputer(max_iter=20, initial_strategy='mean', add_indicator=True)","d8f02c5e":"final_predictions_it_mean_whole, val_scores, imp, train_df_imputed, test_df_imputed = predict_pipeline_whole(\n    iterative_imputer, train_df, test_df, not_useful_features, useful_features, n_splits, add_indicator=True)\nprint(f\"ROC AUC score for {imp} and {imp.initial_strategy} strategy is {np.mean(val_scores).round(3)} \"\n      f\"+- {np.std(val_scores).round(3)}\") ","20c12cee":"train_df_imputed.to_csv(\"train_df_imputed_5folds.csv\", index=False)\ntest_df_imputed.to_csv(\"test_df_imputed_5folds.csv\", index=False)","e1e3ac1a":"iterative_imputer = IterativeImputer(max_iter=20, initial_strategy='mean)","6129c185":"final_predictions_it_mean_k_1, val_scores, imp = predict_pipeline_k_1(iterative_imputer, train_df, test_df, \n                                                                   not_useful_features, useful_features, n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_k_1' function with {imp} and {imp.initial_strategy} strategy is \"\n      f\"{np.mean(val_scores).round(3)} +- {np.std(val_scores).round(3)}\")","6acbff50":"# !rm -r kuma_utils\n!git clone https:\/\/github.com\/analokmaus\/kuma_utils.git","f14fe124":"import sys\nsys.path.append(\"kuma_utils\/\")\nfrom kuma_utils.preprocessing.imputer import LGBMImputer","4aa76dbf":"lgbm_imtr = LGBMImputer(n_iter=100)","38aa7374":"final_predictions_lgbm_whole, val_scores, imp, _, _ = predict_pipeline_whole(lgbm_imtr, train_df, test_df, not_useful_features, \n                                                                 useful_features, n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_whole' function with LGBMImputer is {np.mean(val_scores).round(3)} \"\n      f\"+- {np.std(val_scores).round(3)}\") ","032e8d97":"final_predictions_lgbm_k_1, val_scores, imp = predict_pipeline_k_1(lgbm_imtr, train_df, test_df, not_useful_features, \n                                                               useful_features, n_splits)\nprint(f\"ROC AUC score for 'predict_pipeline_k_1' function with LGBMImputer is {np.mean(val_scores).round(3)} \"\n      f\"+- {np.std(val_scores).round(3)}\") ","74678b72":"preds = np.mean(np.column_stack(final_predictions_it_mean_whole),axis=1)","42d8376d":"sample_submission[\"song_popularity\"] = preds\nsample_submission.to_csv(\"submission.csv\", index=False)","d1b7401e":"Saving datasets with imputed missing values for future modelling","b252a11f":"# Submitting first results","e598d00c":"# `LGBMImputer`","2d57e2a2":"Several columns (`song_duration_ms`, `acousticness`, `danceability`, `energy`, `instrumentalness`, `key`, `liveness`, `loudness`) are lacking roughly 10% of their data.","b1a0256b":"There are 2 predict functions.  \nFirst one (`predict_pipeline_whole`) fits imputer on the whole training dataset (`train_df`).  \nSecond one fits imputer on a `k-1` folds of training dataset (`x_train`)","8727d412":"Seems like, the Iterative imputer with 'mean' strategy is a way to go for this dataset. So, I'm going to use it for future modelling.  \n\nThere is one open question (for me), by the way. How should we measure the performance of imputation algorithms?   \nIs it correct to fit imputation algorithm on whole train dataset and only after that split it to train and validation subsets or should we do this inside of cross validation loop and train imputation algorithm on train subset and use it to transform train and val subsets?   ","0e82a4d0":"### Selecting needed and unnecessary columns","a3681786":"### Median strategy","0f92577b":"# TODO","326fd802":"# Sklearn's `KNNImputer`","c7d26853":"# Looking at data","2126e07f":"# Imputation results","dd3564ca":"One of the columns with missing data (`key`) is a categorical column. So, maybe it will be good to make it categorical again after imputing the missing values.","9fbacd00":"# Sklearn's `SimpleImputer`","104f0914":"### Mean strategy","eeeccf00":"# Building predict functions","f575d874":"### Mean strategy","8fc795ab":"# Sklearn's `IterativeImputer`","78a49ef4":"### Median strategy","e60a52dd":"* Check imputation on whole dataset (train + test) and discuss train-test contamination (data leakage)","ea4734f9":"# Cross Validation","7185ddde":"# Reading the data"}}