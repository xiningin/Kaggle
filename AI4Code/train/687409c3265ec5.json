{"cell_type":{"02d69006":"code","840d8194":"code","d8d8fc3b":"code","71d94ca4":"code","abf5d9a2":"code","91a7d63d":"code","96b8e898":"code","f51bb850":"code","277812a3":"code","5843b354":"code","a6dad913":"code","7a764bdd":"code","a7420158":"code","c982d747":"code","4c7a21c4":"code","158c1a6a":"code","e3a6c957":"code","862aef64":"markdown","18c571da":"markdown","b8da34e3":"markdown","c3e28275":"markdown"},"source":{"02d69006":"import pandas as pd\nimport numpy as np\nimport datatable as dt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom category_encoders import *\nimport gc","840d8194":"from warnings import filterwarnings\nfilterwarnings('ignore')","d8d8fc3b":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","71d94ca4":"%%time\ntrain = dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas().drop('id', axis=1)\ntrain = reduce_memory_usage(train)\ntest = dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas().drop('id', axis=1)\ntest = reduce_memory_usage(test)\nss = dt.fread('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()\nss = reduce_memory_usage(ss)","abf5d9a2":"bool_cols_train = []\nfor i, col in enumerate(train.columns):\n    if train[col].dtypes == bool:\n        bool_cols_train.append(i)","91a7d63d":"bool_cols_test = []\nfor i, col in enumerate(test.columns):\n    if train[col].dtypes == bool:\n        bool_cols_test.append(i)","96b8e898":"train.iloc[:, bool_cols_train] = train.iloc[:, bool_cols_train].astype(int)\ntest.iloc[:, bool_cols_test] = test.iloc[:, bool_cols_test].astype(int)","f51bb850":"print(\"Train set shape\", train.shape, \"\\n\", \"Test set shape\", test.shape)","277812a3":"train.head()","5843b354":"feature_cols = test.columns.tolist()\n\ncnt_features =[]\ncat_features =[]\n\nfor col in feature_cols:\n    if train[col].dtype in [\"float16\", \"float32\", \"float64\"]:\n        cnt_features.append(col)\n    else:\n        cat_features.append(col)\nprint(cat_features)","a6dad913":"X = train.drop('target', axis=1).copy()\ny = train['target'].copy()\nX_test = test.copy()\n\ndel train\ngc.collect\ndel test\ngc.collect","7a764bdd":"for cols in cat_features:\n    enc = TargetEncoder(cols=[cols])\n    X = enc.fit_transform(X, y)\n    X_test = enc.transform(X_test)","a7420158":"display(X.head())\ndisplay(X_test.head())","c982d747":"X['std'] = X.std(axis=1)\nX['min'] = X.min(axis=1)\nX['max'] = X.max(axis=1)\n\nX_test['std'] = X_test.std(axis=1)\nX_test['min'] = X_test.min(axis=1)\nX_test['max'] = X_test.max(axis=1)","4c7a21c4":"params = {\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","158c1a6a":"%%time\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=786)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    params['learning_rate']=0.007279718158350149\n    model1 = XGBClassifier(**params)\n    \n    model1.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False)\n    \n    params['learning_rate']=0.01\n    model2 = XGBClassifier(**params)\n    \n    model2.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model1)\n    \n    params['learning_rate']=0.05\n    model3 = XGBClassifier(**params)\n    \n    model3.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model2)\n    \n    pred_valid = model3.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model3.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","e3a6c957":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('.\/xgb.csv', index=False)\nss.head()","862aef64":"# Submission File","18c571da":"# Feature Engineering\n* Here I have target encoded the categorical features. Further I will update it with KFold target encoding.","b8da34e3":"# Memory Reduction\n\n* This memory reduction part taken from https:\/\/www.kaggle.com\/azzamradman\/tps-10-single-xgboost\/notebook\n  amazing notebook. Please upvote it if you like this part.","c3e28275":"# Model Training"}}