{"cell_type":{"4b8745f1":"code","a9f8f2a6":"code","db61de00":"code","5f7d96d5":"code","0d1b312d":"code","73ae1381":"code","58daee9a":"code","7dad27ea":"code","0025ef60":"code","24b72307":"code","aa8c5500":"code","b3f98598":"code","b311ee0e":"code","4d2b7601":"code","d57b0fae":"code","253028f2":"code","093207f5":"code","4e8e7f43":"code","bf48f231":"code","833444de":"code","a943fd3d":"code","b7fb5f71":"code","4e952951":"code","f5aee716":"code","ce40146a":"code","fc90e75e":"code","0a6a015d":"code","b5b9dfc9":"code","0d8e67a9":"code","9c242e2c":"code","cb1385c5":"markdown","3d3469ce":"markdown","48f89d9e":"markdown","3a25feea":"markdown","1b1220e0":"markdown","dbc39500":"markdown"},"source":{"4b8745f1":"import numpy as np \nimport pandas as pd\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score,accuracy_score\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","a9f8f2a6":"df=pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',encoding='Windows-1252',usecols=['v1','v2'])\ndf.columns=['label','message']\ndf.head()","db61de00":"df.info()","5f7d96d5":"df['label'].value_counts().plot.pie(autopct='%1.1f%%')","0d1b312d":"comments=''\nfor i in df['message']:\n    cmt=str(i)\n    tokens=cmt.split()\n    \n    comments += \" \".join(tokens)+\" \"","73ae1381":"stopwords =set(STOPWORDS)\n\nwordcloud = WordCloud(stopwords = stopwords, \n                min_font_size = 8).generate(comments)\nplt.figure(figsize = (12, 8), facecolor = None) \nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","58daee9a":"dfspam=df[df['label']=='spam']\ndfham=df[df['label']=='ham']","7dad27ea":"commentsspam=''\nfor i in dfspam['message']:\n    cmt=str(i)\n    tokens=cmt.split()\n    \n    commentsspam += \" \".join(tokens)+\" \"\n\ncommentsham=''\nfor i in dfham['message']:\n    cmt=str(i)\n    tokens=cmt.split()\n    \n    commentsham += \" \".join(tokens)+\" \"","0025ef60":"stopwords =set(STOPWORDS)\n\nwordcloud = WordCloud(stopwords = stopwords, \n                min_font_size = 8).generate(commentsspam)\nplt.figure(figsize = (12, 8), facecolor = None) \nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Spam')\nplt.axis(\"off\")\nplt.show()","24b72307":"stopwords =set(STOPWORDS)\n\nwordcloud = WordCloud(stopwords = stopwords, \n                min_font_size = 8).generate(commentsham)\nplt.figure(figsize = (12, 8), facecolor = None) \nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Ham')\nplt.axis(\"off\")\nplt.show()","aa8c5500":"X=df.message.values\ny=df.label.values","b3f98598":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nprint('Train size: ',X_train.shape,' Test size: ',X_test.shape)","b311ee0e":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nvectorizer=CountVectorizer(min_df = 0.01, max_df=0.9,ngram_range=(1,7),stop_words='english',analyzer='char')\nX_train_vec=vectorizer.fit_transform(X_train)\nX_test_vec=vectorizer.transform(X_test)\nX_train_vec.shape","4d2b7601":"print(vectorizer.get_feature_names()[:150])","d57b0fae":"clf=LogisticRegression(max_iter=1000)\nclf.fit(X_train_vec,y_train)\ny_pred=clf.predict(X_test_vec)\nacc=accuracy_score(y_test, y_pred)\nacc_hist = {'LogReg Char Full':acc}\nacc","253028f2":"confmatrix=confusion_matrix(y_test, y_pred)\nsns.heatmap(confmatrix,annot=True)","093207f5":"from sklearn.svm import SVC\nclf=SVC(kernel='rbf')\nclf.fit(X_train_vec, y_train)\ny_pred=clf.predict(X_test_vec)\nacc=accuracy_score(y_test,y_pred)\nacc_hist['SVM RBF Char Full']=acc\nacc","4e8e7f43":"clf=SVC(kernel='linear')\nclf.fit(X_train_vec, y_train)\ny_pred=clf.predict(X_test_vec)\nacc=accuracy_score(y_test,y_pred)\nacc_hist['SVM Linear Char Full']=acc\nacc","bf48f231":"\nselkb = SelectKBest(chi2, k=2048)\nX_train_sel=selkb.fit_transform(X_train_vec, y_train)\nX_test_sel=selkb.transform(X_test_vec)\nX_train_sel.shape","833444de":"clf=LogisticRegression(max_iter=1000)\nclf.fit(X_train_sel,y_train)\ny_pred=clf.predict(X_test_sel)\nacc=accuracy_score(y_test, y_pred)\nacc_hist['LogReg Char Sel']=acc\nacc","a943fd3d":"clf=SVC(kernel='linear')\nclf.fit(X_train_sel, y_train)\ny_pred=clf.predict(X_test_sel)\nacc=accuracy_score(y_test,y_pred)\nacc_hist['SVM Linear Char Sel']=acc\nacc","b7fb5f71":"confmatrix=confusion_matrix(y_test, y_pred)\nsns.heatmap(confmatrix,annot=True)","4e952951":"clf=SVC(kernel='rbf')\nclf.fit(X_train_sel, y_train)\ny_pred=clf.predict(X_test_sel)\nacc=accuracy_score(y_test, y_pred)\nacc_hist['SVM RBF Char Sel']=acc\nacc","f5aee716":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nvectorizer=CountVectorizer(min_df = 0.01, max_df=0.9,ngram_range=(1,7),stop_words='english')\nX_train_vec=vectorizer.fit_transform(X_train)\nX_test_vec=vectorizer.transform(X_test)\nX_train_vec.shape","ce40146a":"print(vectorizer.get_feature_names())","fc90e75e":"clf=LogisticRegression(max_iter=1000)\nclf.fit(X_train_vec,y_train)\ny_pred=clf.predict(X_test_vec)\nacc=accuracy_score(y_test, y_pred)\nacc_hist['LogReg Word']=acc\nacc","0a6a015d":"clf=SVC(kernel='rbf')\nclf.fit(X_train_sel, y_train)\ny_pred=clf.predict(X_test_sel)\nacc=accuracy_score(y_test, y_pred)\nacc_hist['SVM RBF Word']=acc\nacc","b5b9dfc9":"clf=SVC(kernel='linear')\nclf.fit(X_train_sel, y_train)\ny_pred=clf.predict(X_test_sel)\nacc=accuracy_score(y_test, y_pred)\nacc_hist['SVM Linear Word']=acc\nacc","0d8e67a9":"confmatrix=confusion_matrix(y_test, y_pred)\nsns.heatmap(confmatrix,annot=True)","9c242e2c":"pd.Series(acc_hist)","cb1385c5":"# Result","3d3469ce":"# Char CountVectorizer:\n## Full Features:","48f89d9e":"' af', ' aft', ' afte', ' after', ' after ' are features!\n\nSo we can remove some features!","3a25feea":"## Feature Selection:","1b1220e0":"# Word CountVectorizer:","dbc39500":"results are close to each other.\n"}}