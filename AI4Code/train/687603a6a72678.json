{"cell_type":{"a9f9d91c":"code","3d5ca879":"code","bc786262":"code","f4103d1c":"code","f8060065":"code","2be3e5d8":"code","16ad614b":"code","256a24b6":"code","01fade24":"code","5430fb88":"code","b57f7bce":"code","2a3b78bc":"code","e1194a24":"code","6f575ad7":"code","0ea3db81":"code","aeba33a0":"code","635736b2":"markdown","f46b4be6":"markdown"},"source":{"a9f9d91c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","3d5ca879":"import tempfile\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","bc786262":"train_data = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")","f4103d1c":"cols = list(train_data.columns)\ncols.remove(\"id\")\ncols.remove(\"target\")\nclasses = [f\"Class_{i}\" for i in range(1, 10)]\nn_classes = len(classes)","f8060065":"def custom_loss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred, 1e-15, 1-1e-15)\n    return tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)","2be3e5d8":"all_data = pd.concat((train_data, test_data))[cols]\ncategories = {col: sorted(all_data[col].unique()) for col in all_data}","16ad614b":"frequencies = train_data.target.value_counts().sort_index()\/len(train_data)\nbiases_calc = np.log(frequencies).values\nprint(frequencies)\n# print(biases_calc)\ndef softmax(x):\n    return np.exp(x) \/ np.sum(np.exp(x))\nprint(np.exp(biases_calc) \/ np.sum(np.exp(biases_calc)))\n\ndef bias_init(bias_shape,dtype):\n    return tf.Variable(biases_calc, dtype=dtype)","256a24b6":"X = train_data[cols]\ny = train_data[\"target\"]\ny_ohe = pd.get_dummies(y)","01fade24":"def assemble_model(hidden_layers = (32, ), third_skip=False, conv_layer=False, embed_opts={\"output_dim\": 2}, conv_opts={\"filters\": 12, \"kernel_size\": 1}):\n    inputs = layers.Input(len(X.columns), dtype=\"int32\")\n    feat_layer = layers.Embedding(X.max().max()+1, **embed_opts)(inputs)\n    if conv_layer:\n        feat_layer = layers.Conv1D(activation=\"relu\", **conv_opts)(feat_layer)\n        feat_layer = layers.Dropout(0.3)(feat_layer)\n    feat_layer = layers.Flatten()(feat_layer)\n    cur_layer = feat_layer\n    for layer_size in hidden_layers:\n        cur_layer = layers.Dropout(.2)(cur_layer)\n        lay = tfa.layers.WeightNormalization(layers.Dense(layer_size, activation=\"selu\",\n                                 kernel_initializer='lecun_normal',\n                                ))\n        cur_layer = lay(cur_layer)\n\n    first_hidden_layer = cur_layer\n\n    cur_layer = layers.Concatenate()([feat_layer, cur_layer])\n    cur_layer = layers.Dropout(.3)(cur_layer)\n    cur_layer = tfa.layers.WeightNormalization(layers.Dense(hidden_layers[-1], activation=\"relu\"))(cur_layer)\n    second_skip_layer = cur_layer\n    \n    cur_layer = layers.Concatenate()([feat_layer, cur_layer, first_hidden_layer])\n    cur_layer = layers.Dropout(.4)(cur_layer)\n    cur_layer = tfa.layers.WeightNormalization(layers.Dense(hidden_layers[-1], activation=\"elu\", kernel_initializer='lecun_normal',))(cur_layer)\n\n    if third_skip:\n        cur_layer = layers.Concatenate()([feat_layer, cur_layer, second_skip_layer, first_hidden_layer])\n        cur_layer = layers.Dropout(.3)(cur_layer)\n        cur_layer = tfa.layers.WeightNormalization(layers.Dense(hidden_layers[-1], activation=\"elu\"))(cur_layer)\n\n    \n    out = layers.Dense(n_classes, activation=\"softmax\", bias_initializer=bias_init\n                      )(cur_layer)\n\n    model = tf.keras.Model(inputs,\n                           out)\n\n    model.compile(\n            optimizer=\"adam\",\n            loss=tf.keras.losses.CategoricalCrossentropy(),\n            metrics=[tf.keras.metrics.CategoricalCrossentropy(), custom_loss],\n        )\n    return model","5430fb88":"def pack_test_data(proba):\n    predicted = pd.DataFrame(proba, columns=classes)\n    predicted[\"id\"] = test_data.id\n    predicted = predicted[[\"id\"]+classes]\n    return predicted","b57f7bce":"earlystop_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.005, patience=10, verbose=0,\n    mode='auto', baseline=None, restore_best_weights=True\n)\nreduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.7, patience=2, verbose=0\n)","2a3b78bc":"def train_predict_cv(get_model, nfolds=10):\n    kf = StratifiedKFold(n_splits=nfolds, shuffle=True)\n\n    probas = []\n    losses = []\n    for train_idx, test_idx in kf.split(X, y):\n\n        X_train, y_train = X.loc[train_idx], y_ohe.loc[train_idx]\n        X_test, y_test = X.loc[test_idx], y_ohe.loc[test_idx]\n        model = get_model()\n        model.fit(X_train, y_train, epochs=100, \n                  batch_size=256,\n                  validation_data=(X_test, y_test),\n                callbacks=[earlystop_callback, reduce_lr_on_plateau], verbose=0)\n        eval_dict = model.evaluate(X_test, y_test, return_dict=True, verbose=0)\n        print(eval_dict)\n        losses.append(eval_dict[\"custom_loss\"])\n        proba = model.predict(test_data[cols])\n        probas.append(proba)\n    print(f\"Mean loss: {np.mean(losses)}, std: {np.std(losses)}\")\n    return sum(probas)\/nfolds","e1194a24":"%%time\nproba = train_predict_cv(lambda: assemble_model(hidden_layers=(32,), conv_layer=True, \n                                                     embed_opts={\"output_dim\": 14, \"embeddings_regularizer\": 'l2'},\n                                                     conv_opts={\"filters\": 12, \"kernel_size\": 1}), nfolds=20)\npack_test_data(proba).to_csv(\"tf_model_conv.csv\", index=False)","6f575ad7":"def clip_and_save(proba, clip_percent=5, renormalize=False, fname=\"tf_model\"):\n    proba_clipped = np.clip(proba, clip_percent\/100, 1-clip_percent\/100)\n    if renormalize:\n        sums = proba.sum(axis=1)\n        proba = proba\/sums[:, np.newaxis]\n        sums = proba.sum(axis=1)\n        fname += \"_normed\"\n        \n    predicted = pack_test_data(proba_clipped)\n    predicted.to_csv(fname+f\"_clipped_{clip_percent}.csv\", index=False)","0ea3db81":"clip_and_save(proba, clip_percent=1.25, renormalize=True)","aeba33a0":"clip_and_save(proba, clip_percent=0.0, renormalize=False)","635736b2":"Inspired by Oscar Villarreal Escamilla's notebook https:\/\/www.kaggle.com\/oxzplvifi\/tabular-residual-network I am trying embedding with residual network, here in Python instead of R","f46b4be6":"I tried clipping the output probabilities to avoid extreme values affecting the score, but it did not seem to help much though. Here I try a few more cutoffs for the clipping"}}