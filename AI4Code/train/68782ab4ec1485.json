{"cell_type":{"82eff193":"code","aea6bdb3":"code","d7b74f1b":"code","f4826948":"code","04448a4c":"code","755e6372":"code","9df42ca7":"code","1b128e4c":"code","2980531b":"code","0827f064":"code","08e262a0":"code","0b3841bd":"code","b34bff74":"code","d81ede68":"code","2745e570":"code","5d496715":"code","75761d4d":"code","38fcbd62":"code","9be25f8c":"code","7d575e17":"code","5558f2f2":"code","0c72b62f":"code","31b07b68":"code","df66b76e":"code","ffba81e2":"code","bc48cd6f":"code","9782a18c":"markdown","c4c43ee9":"markdown","b12bcc9b":"markdown"},"source":{"82eff193":"!pip install --no-deps ..\/input\/segmentations-models-pytorch-7fa1020\/pretrainedmodels-0.7.4-py3-none-any.whl\nimport sys\nsys.path.append('..\/input\/smp-latest\/segmentation_models.pytorch-master\/')\nsys.path.append('..\/input\/efficientnet-pytorch\/efficientnet_pytorch-0.4.0\/')","aea6bdb3":"# CLASSIFICATION_THRES = 0.00","d7b74f1b":"# import sys\n# sys.path.insert(0, \"..\/input\/timm-models\/pytorch-image-models\/pytorch-image-models\")\n# from fastai.vision import * \n# from fastai import *\n# import timm\n# df=pd.read_csv('..\/input\/steel-classifier-csv\/train_classes.csv')\n# df.Label = (df.Label!='0').astype('int')\n# bs=64 ## Batch size\n# tfms = get_transforms() ## Transformation\n# stats =([0.334,0.334,0.334],[0.199,0.199,0.199])\n# data = (\n#         ImageList.from_df(df=df ,path='',cols='Image_Id', folder='..\/input\/severstal-steel-defect-detection\/train_images') \n#         .split_by_rand_pct(0.1)\n#         .label_from_df(cols='Label')\n#         .transform(tfms)\n#         .databunch(bs=bs,num_workers=4)\n#         .normalize(stats)\n#        )\n# learn_res34 = cnn_learner(data ,timm.models.resnet34,pretrained=False, metrics =[accuracy, FBeta(beta= 1)], callback_fns=ShowGraph)\n# learn_wrn50 = cnn_learner(data ,timm.models.wide_resnet50_2,pretrained=False, metrics =[accuracy, FBeta(beta= 1)], callback_fns=ShowGraph)\n# !mkdir models\/ && cp ..\/input\/ubambamodels\/rn34-stage-5.pth models\/\n# !cp ..\/input\/ubambamodels\/wrn50-stage-4.pth models\/\n# learn_res34.load('rn34-stage-5')\n# learn_wrn50.load('wrn50-stage-4')\n# !rm -r models\/\n# sample_submission_path = '..\/input\/severstal-steel-defect-detection\/sample_submission.csv'\n# test_df= pd.read_csv(sample_submission_path)\n# def changename(x):\n#     x=x[:-2]\n#     return x\n# test_df.ImageId_ClassId=test_df.ImageId_ClassId.apply(changename)\n# test_df=test_df.drop_duplicates(['ImageId_ClassId'])\n# learn_res34.data.add_test(\n#     ImageList.from_df(df=test_df ,path='',cols='ImageId_ClassId', folder='..\/input\/severstal-steel-defect-detection\/test_images')\n# )\n# learn_wrn50.data.add_test(\n#     ImageList.from_df(df=test_df ,path='',cols='ImageId_ClassId', folder='..\/input\/severstal-steel-defect-detection\/test_images')\n# )\n# x1,y1=learn_res34.get_preds(DatasetType.Test)\n# x2,y2=learn_wrn50.get_preds(DatasetType.Test)\n# arr1 = np.array(x1[:,1]>CLASSIFICATION_THRES).astype(int)\n# arr2 = np.array(x2[:,1]>CLASSIFICATION_THRES).astype(int)\n# arr3 = (arr1+arr2)\/2\n# test_df.EncodedPixels = np.ceil(arr3).astype(int)","f4826948":"from fastai.vision import * \nfrom fastai import *\ndf=pd.read_csv('..\/input\/steel-classifier-csv\/train_classes.csv')\ndf.Label = (df.Label!='0').astype('int')\nsample_submission_path = '..\/input\/severstal-steel-defect-detection\/sample_submission.csv'\ntest_df= pd.read_csv(sample_submission_path)\ndef changename(x):\n    x=x[:-2]\n    return x\ntest_df.ImageId_ClassId=test_df.ImageId_ClassId.apply(changename)\ntest_df=test_df.drop_duplicates(['ImageId_ClassId'])","04448a4c":"test_df.EncodedPixels.value_counts()","755e6372":"import pdb\nimport os\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose, HorizontalFlip)\nfrom albumentations.pytorch import ToTensor\nimport torch.utils.data as data","9df42ca7":"from torch.hub import load_state_dict_from_url\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\ndevice = torch.device(\"cuda\")","1b128e4c":"#https:\/\/www.kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","2980531b":"class TestDataset(Dataset):\n    '''Dataset for test prediction'''\n    def __init__(self, root, df, mean, std):\n        self.root = root\n        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n        self.fnames = df['ImageId'].unique().tolist()\n        self.num_samples = len(self.fnames)\n        self.transform = Compose(\n            [\n                Normalize(mean=mean, std=std, p=1),\n                ToTensor()\n            ]\n        )\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        path = os.path.join(self.root, fname)\n        image = cv2.imread(path)\n        images = self.transform(image=image)[\"image\"]\n        return fname, images\n\n    def __len__(self):\n        return self.num_samples","0827f064":"def post_process(probability, threshold, min_size):\n    '''Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored'''\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num","08e262a0":"sample_submission_path = '..\/input\/severstal-steel-defect-detection\/sample_submission.csv'\ntest_data_folder = \"..\/input\/severstal-steel-defect-detection\/test_images\"","0b3841bd":"test_df_= pd.read_csv(sample_submission_path)","b34bff74":"# initialize test dataloader\nfold_csv = pd.read_csv(sample_submission_path)\nnum_workers = 4\nbatch_size = 4\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ndf = pd.read_csv(sample_submission_path)\ntestset = DataLoader(\n    TestDataset(test_data_folder, test_df_, mean, std),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True\n)","d81ede68":"import re\nfrom collections import namedtuple\n\n\nGlobalParams = namedtuple('GlobalParams', ['batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'num_classes',\n                                           'width_coefficient', 'depth_coefficient', 'depth_divisor', 'min_depth',\n                                           'drop_connect_rate'])\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n\nBlockArgs = namedtuple('BlockArgs', ['kernel_size', 'num_repeat', 'input_filters', 'output_filters', 'expand_ratio',\n                                     'id_skip', 'strides', 'se_ratio'])\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\nIMAGENET_WEIGHTS = {\n    'efficientnet-b0': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet-b0-08094119.pth',\n    'efficientnet-b1': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet-b1-dbc7070a.pth',\n    'efficientnet-b2': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet-b2-27687264.pth',\n    'efficientnet-b3': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet-b3-c8376fa2.pth',\n    'efficientnet-b4': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet-b4-e116e8b3.pth',\n    'efficientnet-b5': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet-b5-586e6cc6.pth'\n}\n\n\ndef round_filters(filters, global_params):\n    \"\"\"Round number of filters\n    \"\"\"\n    multiplier = global_params.width_coefficient\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    if not multiplier:\n        return filters\n\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor \/ 2) \/\/ divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_filters < 0.9 * filters:\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    \"\"\"Round number of repeats\n    \"\"\"\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef get_efficientnet_params(model_name, override_params=None):\n    \"\"\"Get efficientnet params based on model name\n    \"\"\"\n    params_dict = {\n        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n        # Note: the resolution here is just for reference, its values won't be used.\n        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n    }\n    if model_name not in params_dict.keys():\n        raise KeyError('There is no model named {}.'.format(model_name))\n\n    width_coefficient, depth_coefficient, _, dropout_rate = params_dict[model_name]\n\n    blocks_args = [\n        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n        'r1_k3_s11_e6_i192_o320_se0.25',\n    ]\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=0.2,\n        num_classes=1000,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None)\n\n    if override_params:\n        global_params = global_params._replace(**override_params)\n\n    decoder = BlockDecoder()\n    return decoder.decode(blocks_args), global_params\n\n\nclass BlockDecoder(object):\n    \"\"\"Block Decoder for readability\n    \"\"\"\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n        assert isinstance(block_string, str)\n        ops = block_string.split('_')\n        options = {}\n        for op in ops:\n            splits = re.split(r'(\\d.*)', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        if 's' not in options or len(options['s']) != 2:\n            raise ValueError('Strides options should be a pair of integers.')\n\n        return BlockArgs(\n            kernel_size=int(options['k']),\n            num_repeat=int(options['r']),\n            input_filters=int(options['i']),\n            output_filters=int(options['o']),\n            expand_ratio=int(options['e']),\n            id_skip=('noskip' not in block_string),\n            se_ratio=float(options['se']) if 'se' in options else None,\n            strides=[int(options['s'][0]), int(options['s'][1])]\n        )\n\n    @staticmethod\n    def _encode_block_string(block):\n        \"\"\"Encodes a block to a string.\"\"\"\n        args = [\n            'r%d' % block.num_repeat,\n            'k%d' % block.kernel_size,\n            's%d%d' % (block.strides[0], block.strides[1]),\n            'e%s' % block.expand_ratio,\n            'i%d' % block.input_filters,\n            'o%d' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append('se%s' % block.se_ratio)\n        if block.id_skip is False:\n            args.append('noskip')\n        return '_'.join(args)\n\n    def decode(self, string_list):\n        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n        Args:\n          string_list: a list of strings, each string is a notation of block.\n        Returns:\n          A list of namedtuples to represent blocks arguments.\n        \"\"\"\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(self._decode_block_string(block_string))\n        return blocks_args\n\n    def encode(self, blocks_args):\n        \"\"\"Encodes a list of Blocks to a list of strings.\n        Args:\n          blocks_args: A list of namedtuples to represent blocks arguments.\n        Returns:\n          a list of strings, each string is a notation of block.\n        \"\"\"\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(self._encode_block_string(block))\n        return block_strings\n\n#########################################\n\nclass EfficientNet(nn.Module):\n\n    def __init__(self, block_args_list, global_params):\n        super().__init__()\n\n        self.block_args_list = block_args_list\n        self.global_params = global_params\n\n        # Batch norm parameters\n        batch_norm_momentum = 1 - self.global_params.batch_norm_momentum\n        batch_norm_epsilon = self.global_params.batch_norm_epsilon\n\n        # Stem\n        in_channels = 3\n        out_channels = round_filters(32, self.global_params)\n        self._conv_stem = Conv2dSamePadding(in_channels,\n                                            out_channels,\n                                            kernel_size=3,\n                                            stride=2,\n                                            bias=False,\n                                            name='stem_conv')\n        self._bn0 = BatchNorm2d(num_features=out_channels,\n                                momentum=batch_norm_momentum,\n                                eps=batch_norm_epsilon,\n                                name='stem_batch_norm')\n\n        self._swish = Swish(name='swish')\n\n        # Build _blocks\n        idx = 0\n        self._blocks = nn.ModuleList([])\n        for block_args in self.block_args_list:\n\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self.global_params),\n                output_filters=round_filters(block_args.output_filters, self.global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self.global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(block_args, self.global_params, idx=idx))\n            idx += 1\n\n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(input_filters=block_args.output_filters, strides=1)\n\n            # The rest of the _blocks\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self.global_params, idx=idx))\n                idx += 1\n\n        # Head\n        in_channels = block_args.output_filters  # output of final block\n        out_channels = round_filters(1280, self.global_params)\n        self._conv_head = Conv2dSamePadding(in_channels,\n                                            out_channels,\n                                            kernel_size=1,\n                                            bias=False,\n                                            name='head_conv')\n        self._bn1 = BatchNorm2d(num_features=out_channels,\n                                momentum=batch_norm_momentum,\n                                eps=batch_norm_epsilon,\n                                name='head_batch_norm')\n\n        # Final linear layer\n        self.dropout_rate = self.global_params.dropout_rate\n        self._fc = nn.Linear(out_channels, self.global_params.num_classes)\n\n    def forward(self, x):\n        # Stem\n        x = self._conv_stem(x)\n        x = self._bn0(x)\n        x = self._swish(x)\n\n        # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self.global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= idx \/ len(self._blocks)\n            x = block(x, drop_connect_rate)\n\n        # Head\n        x = self._conv_head(x)\n        x = self._bn1(x)\n        x = self._swish(x)\n\n        # Pooling and Dropout\n        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n        if self.dropout_rate > 0:\n            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n\n        # Fully-connected layer\n        x = self._fc(x)\n        return x\n\n    @classmethod\n    def from_name(cls, model_name, *, n_classes=1000, pretrained=False):\n        return _get_model_by_name(model_name, classes=n_classes, pretrained=pretrained)\n\n    @classmethod\n    def encoder(cls, model_name, *, pretrained=False):\n        model = cls.from_name(model_name, pretrained=pretrained)\n\n        class Encoder(nn.Module):\n            def __init__(self):\n                super().__init__()\n\n                self.name = model_name\n\n                self.global_params = model.global_params\n\n                self.stem_conv = model._conv_stem\n                self.stem_batch_norm = model._bn0\n                self.stem_swish = Swish(name='stem_swish')\n                self.blocks = model._blocks\n                self.head_conv = model._conv_head\n                self.head_batch_norm = model._bn1\n                self.head_swish = Swish(name='head_swish')\n\n            def forward(self, x):\n                # Stem\n                x = self.stem_conv(x)\n                x = self.stem_batch_norm(x)\n                x = self.stem_swish(x)\n\n                # Blocks\n                for idx, block in enumerate(self.blocks):\n                    drop_connect_rate = self.global_params.drop_connect_rate\n                    if drop_connect_rate:\n                        drop_connect_rate *= idx \/ len(self.blocks)\n                    x = block(x, drop_connect_rate)\n\n                # Head\n                x = self.head_conv(x)\n                x = self.head_batch_norm(x)\n                x = self.head_swish(x)\n                return x\n\n        return Encoder()\n\n    @classmethod\n    def custom_head(cls, model_name, *, n_classes=1000, pretrained=False):\n        if n_classes == 1000:\n            return cls.from_name(model_name, n_classes=n_classes, pretrained=pretrained)\n        else:\n            class CustomHead(nn.Module):\n                def __init__(self, out_channels):\n                    super().__init__()\n                    self.encoder = cls.encoder(model_name, pretrained=pretrained)\n                    self.custom_head = custom_head(self.n_channels * 2, out_channels)\n\n                @property\n                def n_channels(self):\n                    n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n                                       'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n                                       'efficientnet-b6': 2304, 'efficientnet-b7': 2560}\n                    return n_channels_dict[self.encoder.name]\n\n                def forward(self, x):\n                    x = self.encoder(x)\n                    mp = nn.AdaptiveMaxPool2d(output_size=(1, 1))(x)\n                    ap = nn.AdaptiveAvgPool2d(output_size=(1, 1))(x)\n                    x = torch.cat([mp, ap], dim=1)\n                    x = x.view(x.size(0), -1)\n                    x = self.custom_head(x)\n\n                    return x\n\n            return CustomHead(n_classes)\n\n\ndef _get_model_by_name(model_name, classes=1000, pretrained=False):\n    block_args_list, global_params = get_efficientnet_params(model_name, override_params={'num_classes': classes})\n    model = EfficientNet(block_args_list, global_params)\n    try:\n        if pretrained:\n            pretrained_state_dict = load_state_dict_from_url(IMAGENET_WEIGHTS[model_name])\n\n            if classes != 1000:\n                random_state_dict = model.state_dict()\n                pretrained_state_dict['_fc.weight'] = random_state_dict['_fc.weight']\n                pretrained_state_dict['_fc.bias'] = random_state_dict['_fc.bias']\n\n            model.load_state_dict(pretrained_state_dict)\n\n    except KeyError as e:\n        print(f\"NOTE: Currently model {e} doesn't have pretrained weights, therefore a model with randomly initialized\"\n              \" weights is returned.\")\n\n    return model\n\n################################\nclass Swish(nn.Module):\n    def __init__(self, name=None):\n        super().__init__()\n        self.name = name\n\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass Conv2dSamePadding(nn.Conv2d):\n    \"\"\"2D Convolutions with same padding\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True, name=None):\n        super().__init__(in_channels, out_channels, kernel_size, stride, padding=0, dilation=dilation, groups=groups,\n                         bias=bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n        self.name = name\n\n    def forward(self, x):\n        input_h, input_w = x.size()[2:]\n        kernel_h, kernel_w = self.weight.size()[2:]\n        stride_h, stride_w = self.stride\n        output_h, output_w = math.ceil(input_h \/ stride_h), math.ceil(input_w \/ stride_w)\n        pad_h = max((output_h - 1) * self.stride[0] + (kernel_h - 1) * self.dilation[0] + 1 - input_h, 0)\n        pad_w = max((output_w - 1) * self.stride[1] + (kernel_w - 1) * self.dilation[1] + 1 - input_w, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w \/\/ 2, pad_w - pad_w \/\/ 2, pad_h \/\/ 2, pad_h - pad_h \/\/ 2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass BatchNorm2d(nn.BatchNorm2d):\n    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, name=None):\n        super().__init__(num_features, eps=eps, momentum=momentum, affine=affine,\n                         track_running_stats=track_running_stats)\n        self.name = name\n\n\ndef drop_connect(inputs, drop_connect_rate, training):\n    if not training:\n        return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1.0 - drop_connect_rate\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs \/ keep_prob * binary_tensor\n    return output\n\n\nclass MBConvBlock(nn.Module):\n    \"\"\"Mobile Inverted Residual Bottleneck Block\n    \"\"\"\n\n    def __init__(self, block_args, global_params, idx):\n        super().__init__()\n\n        block_name = 'blocks_' + str(idx) + '_'\n\n        self.block_args = block_args\n        self.batch_norm_momentum = 1 - global_params.batch_norm_momentum\n        self.batch_norm_epsilon = global_params.batch_norm_epsilon\n        self.has_se = (self.block_args.se_ratio is not None) and (0 < self.block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip\n\n        self.swish = Swish(block_name + '_swish')\n\n        # Expansion phase\n        in_channels = self.block_args.input_filters\n        out_channels = self.block_args.input_filters * self.block_args.expand_ratio\n        if self.block_args.expand_ratio != 1:\n            self._expand_conv = Conv2dSamePadding(in_channels=in_channels,\n                                                  out_channels=out_channels,\n                                                  kernel_size=1,\n                                                  bias=False,\n                                                  name=block_name + 'expansion_conv')\n            self._bn0 = BatchNorm2d(num_features=out_channels,\n                                    momentum=self.batch_norm_momentum,\n                                    eps=self.batch_norm_epsilon,\n                                    name=block_name + 'expansion_batch_norm')\n\n        # Depth-wise convolution phase\n        kernel_size = self.block_args.kernel_size\n        strides = self.block_args.strides\n        self._depthwise_conv = Conv2dSamePadding(in_channels=out_channels,\n                                                 out_channels=out_channels,\n                                                 groups=out_channels,\n                                                 kernel_size=kernel_size,\n                                                 stride=strides,\n                                                 bias=False,\n                                                 name=block_name + 'depthwise_conv')\n        self._bn1 = BatchNorm2d(num_features=out_channels,\n                                momentum=self.batch_norm_momentum,\n                                eps=self.batch_norm_epsilon,\n                                name=block_name + 'depthwise_batch_norm')\n\n        # Squeeze and Excitation layer\n        if self.has_se:\n            num_squeezed_channels = max(1, int(self.block_args.input_filters * self.block_args.se_ratio))\n            self._se_reduce = Conv2dSamePadding(in_channels=out_channels,\n                                                out_channels=num_squeezed_channels,\n                                                kernel_size=1,\n                                                name=block_name + 'se_reduce')\n            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels,\n                                                out_channels=out_channels,\n                                                kernel_size=1,\n                                                name=block_name + 'se_expand')\n\n        # Output phase\n        final_output_channels = self.block_args.output_filters\n        self._project_conv = Conv2dSamePadding(in_channels=out_channels,\n                                               out_channels=final_output_channels,\n                                               kernel_size=1,\n                                               bias=False,\n                                               name=block_name + 'output_conv')\n        self._bn2 = BatchNorm2d(num_features=final_output_channels,\n                                momentum=self.batch_norm_momentum,\n                                eps=self.batch_norm_epsilon,\n                                name=block_name + 'output_batch_norm')\n\n    def forward(self, x, drop_connect_rate=None):\n        identity = x\n        # Expansion and depth-wise convolution\n        if self.block_args.expand_ratio != 1:\n            x = self._expand_conv(x)\n            x = self._bn0(x)\n            x = self.swish(x)\n\n        x = self._depthwise_conv(x)\n        x = self._bn1(x)\n        x = self.swish(x)\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(self.swish(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self.block_args.input_filters, self.block_args.output_filters\n        if self.id_skip and self.block_args.strides == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, drop_connect_rate=drop_connect_rate, training=self.training)\n            x = x + identity\n        return x\n\n\ndef double_conv(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef up_conv(in_channels, out_channels):\n    return nn.ConvTranspose2d(\n        in_channels, out_channels, kernel_size=2, stride=2\n    )\n\n\ndef custom_head(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Dropout(),\n        nn.Linear(in_channels, 512),\n        nn.ReLU(inplace=True),\n        nn.Dropout(),\n        nn.Linear(512, out_channels)\n    )\n\n################################################\n\nfrom collections import OrderedDict\n\n\n__all__ = ['EfficientUnet', 'get_efficientunet_b0', 'get_efficientunet_b1', 'get_efficientunet_b2',\n           'get_efficientunet_b3', 'get_efficientunet_b4', 'get_efficientunet_b5', 'get_efficientunet_b6',\n           'get_efficientunet_b7']\n\n\ndef get_blocks_to_be_concat(model, x):\n    shapes = set()\n    blocks = OrderedDict()\n    hooks = []\n    count = 0\n\n    def register_hook(module):\n\n        def hook(module, input, output):\n            try:\n                nonlocal count\n                if module.name == f'blocks_{count}_output_batch_norm':\n                    count += 1\n                    shape = output.size()[-2:]\n                    if shape not in shapes:\n                        shapes.add(shape)\n                        blocks[module.name] = output\n\n                elif module.name == 'head_swish':\n                    # when module.name == 'head_swish', it means the program has already got all necessary blocks for\n                    # concatenation. In my dynamic unet implementation, I first upscale the output of the backbone,\n                    # (in this case it's the output of 'head_swish') concatenate it with a block which has the same\n                    # Height & Width (image size). Therefore, after upscaling, the output of 'head_swish' has bigger\n                    # image size. The last block has the same image size as 'head_swish' before upscaling. So we don't\n                    # really need the last block for concatenation. That's why I wrote `blocks.popitem()`.\n                    blocks.popitem()\n                    blocks[module.name] = output\n\n            except AttributeError:\n                pass\n\n        if (\n                not isinstance(module, nn.Sequential)\n                and not isinstance(module, nn.ModuleList)\n                and not (module == model)\n        ):\n            hooks.append(module.register_forward_hook(hook))\n\n    # register hook\n    model.apply(register_hook)\n\n    # make a forward pass to trigger the hooks\n    model(x)\n\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    return blocks\n\n\nclass Attention_block(nn.Module):\n    def __init__(self,F_g,F_l,F_int):\n        super(Attention_block,self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n            )\n        \n        self.W_xa = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        \n        self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self,g,xa):\n        g1 = self.W_g(g)\n        x1 = self.W_xa(xa)\n        psi = self.relu(g1+x1)\n        psi = self.psi(psi)\n\n        return xa*psi\n\n\n\n\nclass EfficientUnet(nn.Module):\n    def __init__(self, encoder, out_channels=2, concat_input=True):\n        super().__init__()\n\n        self.encoder = encoder\n        self.concat_input = concat_input\n\n        self.up_conv1 = up_conv(self.n_channels, 512)\n        self.Att1 = Attention_block(F_g=512,F_l=592-512,F_int=56)\n        self.double_conv1 = double_conv(self.size[0], 512)\n        self.up_conv2 = up_conv(512, 256)\n        self.Att2 = Attention_block(F_g=256,F_l=296-256,F_int=28)\n        self.double_conv2 = double_conv(self.size[1], 256)\n        self.up_conv3 = up_conv(256, 128)\n        self.Att3 = Attention_block(F_g=128,F_l=152-128,F_int=16)\n        self.double_conv3 = double_conv(self.size[2], 128)\n        self.up_conv4 = up_conv(128, 64)\n        self.Att4 = Attention_block(F_g=64,F_l=80-64,F_int=12)\n        self.double_conv4 = double_conv(self.size[3], 64)\n\n        if self.concat_input:\n            self.up_conv_input = up_conv(64, 32)\n            self.double_conv_input = double_conv(self.size[4], 32)\n\n        self.final_conv = nn.Conv2d(self.size[5], out_channels, kernel_size=1)\n\n    @property\n    def n_channels(self):\n        n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n                           'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n                           'efficientnet-b6': 2304, 'efficientnet-b7': 2560}\n        return n_channels_dict[self.encoder.name]\n\n    @property\n    def size(self):\n        size_dict = {'efficientnet-b0': [592, 296, 152, 80, 35, 32], 'efficientnet-b1': [592, 296, 152, 80, 35, 32],\n                     'efficientnet-b2': [600, 304, 152, 80, 35, 32], 'efficientnet-b3': [608, 304, 160, 88, 35, 32],\n                     'efficientnet-b4': [624, 312, 160, 88, 35, 32], 'efficientnet-b5': [640, 320, 168, 88, 35, 32],\n                     'efficientnet-b6': [656, 328, 168, 96, 35, 32], 'efficientnet-b7': [672, 336, 176, 96, 35, 32]}\n        return size_dict[self.encoder.name]\n\n    def forward(self, x):\n        input_ = x\n\n        blocks = get_blocks_to_be_concat(self.encoder, x)\n        \n      \n        _, x = blocks.popitem()\n\n        x1 = self.up_conv1(x)        \n        _, y = blocks.popitem()   \n        x = self.Att1(g=x1,xa=y)      \n        x = torch.cat([x, x1], dim=1)\n        x = self.double_conv1(x)\n\n        x1 = self.up_conv2(x)\n        y = blocks.popitem()[1]\n        x = self.Att2(g=x1,xa=y)\n        x = torch.cat([x, x1], dim=1)\n        x = self.double_conv2(x)\n\n        x1 = self.up_conv3(x)\n        y = blocks.popitem()[1]\n        x = self.Att3(g=x1,xa=y)\n        x = torch.cat([x, x1], dim=1)\n        x = self.double_conv3(x)\n\n        x1 = self.up_conv4(x)\n        y = blocks.popitem()[1]\n        x = self.Att4(g=x1,xa=y) \n        x = torch.cat([x, x1], dim=1)\n        x = self.double_conv4(x)\n\n        if self.concat_input:\n            x = self.up_conv_input(x)\n            x = torch.cat([x, input_], dim=1)\n            x = self.double_conv_input(x)\n\n        x = self.final_conv(x)\n\n        return x\n\n\ndef get_efficientunet_b0(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b0', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n\n\ndef get_efficientunet_b1(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b1', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n\n\ndef get_efficientunet_b2(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b2', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n\n\ndef get_efficientunet_b3(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b3', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n\n\ndef get_efficientunet_b4(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b4', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n\n\ndef get_efficientunet_b5(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b5', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n\n\ndef get_efficientunet_b6(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b6', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n\n\ndef get_efficientunet_b7(out_channels=2, concat_input=True, pretrained=True):\n    encoder = EfficientNet.encoder('efficientnet-b7', pretrained=pretrained)\n    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n    return model\n","2745e570":"unet_b0 = get_efficientunet_b0(out_channels=4, concat_input=True, pretrained=False)\nunet_b1 = get_efficientunet_b1(out_channels=4, concat_input=True, pretrained=False)\nimport segmentation_models_pytorch as smp\nFPN_res34 = smp.FPN('resnet34', classes=4, activation=None,encoder_weights=None)\nPSP_seresx50 = smp.PSPNet('se_resnext50_32x4d', classes=4, activation=None,encoder_weights=None)\nFPN_seresx50 = smp.FPN('se_resnext50_32x4d', classes=4, activation=None,encoder_weights=None)\nFPN_b0 = smp.FPN('efficientnet-b0', classes=4, activation=None,encoder_weights=None)\nFPN_b1 = smp.FPN('efficientnet-b1', classes=4, activation=None,encoder_weights=None)\nFPN_b2 = smp.FPN('efficientnet-b2', classes=4, activation=None,encoder_weights=None)\nFPN_b3 = smp.FPN('efficientnet-b3', classes=4, activation=None,encoder_weights=None)\nFPN_b4 = smp.FPN('efficientnet-b4', classes=4, activation=None,encoder_weights=None)","5d496715":"# Initialize mode and load trained weights\ndef weight_loader(model,path):\n    ckpt_path = path\n    device = torch.device(\"cuda\")\n    model.to(device)\n    model.eval()\n    state = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n    model.load_state_dict(state[\"state_dict\"])\n    return model\n\ndef make_eval(models):\n    for i,m in enumerate(models):\n        models[i]=m.eval()\n    return models  \n\nunet_b0 = weight_loader(unet_b0,'..\/input\/b0fulldata\/b0att_complete.pth')\nFPN_seresx50 = weight_loader(FPN_seresx50,'..\/input\/b0fulldata\/fpnserex50complete.pth')\nFPN_b1 = weight_loader(FPN_b1,'..\/input\/fpn-b1\/fpn_b1-stage-4.pth')\nFPN_b0 = weight_loader(FPN_b0,'..\/input\/b0fulldata\/fpn_b0-complete-stage-2.pth')\nunet_b1 = weight_loader(unet_b1,'..\/input\/classres50\/effb1unetbceloss_lrreduce_full_2.pth')\n# PSP_seresx50 = weight_loader(PSP_seresx50,'..\/input\/pspresnxt50\/psp_se_resnext50_32x4d_alldata_reducelr.pth')\n# FPN_res34 = weight_loader(FPN_res34,'..\/input\/pspfpn_res34\/fpn_res34-stage-2lrreduce.pth')\nunet_se_resnext50_32x4d = torch.jit.load('\/kaggle\/input\/severstalmodels\/unet_se_resnext50_32x4d.pth').cuda().eval()\n# unet_mobilenet2 = torch.jit.load('\/kaggle\/input\/severstalmodels\/unet_mobilenet2.pth').cuda().eval()\nunet_resnet34 = torch.jit.load('\/kaggle\/input\/severstalmodels\/unet_resnet34.pth').cuda().eval()\nFPN_b2 = weight_loader(FPN_b2,'..\/input\/ubambamodels\/fpn_b2-complete.pth')\nFPN_b4 = weight_loader(FPN_b4,'..\/input\/fpnb41024\/fpn-b4-crop-stage-5-2-all.pth')\nFPN_b3 = weight_loader(FPN_b3,'..\/input\/fpnb3stage4\/fpn-b3_crop-stage-4-all.pth')","75761d4d":"class Model_sig:\n    def __init__(self, models):\n        self.models = models\n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        x = torch.flip(x,dims = [-1])\n        with torch.no_grad():\n            for m in self.models:\n                flipped_mask = m(x)\n                mask = torch.flip(flipped_mask, dims = [-1])\n                res.append(mask)\n        res = (torch.stack(res))\n\n        return torch.mean(torch.sigmoid(res), dim=0)\n\nclass Model:\n    def __init__(self, models):\n        self.models = models\n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        x = torch.flip(x,dims = [-1])\n        with torch.no_grad():\n            for m in self.models:\n                flipped_mask = m(x)\n                mask = torch.flip(flipped_mask,dims = [-1])\n                res.append(mask)\n        res = torch.stack(res)\n        return torch.sigmoid(torch.mean(res, dim=0))\n\n# model = Model([unet_b0, FPN_seresx50, FPN_b0,FPN_b1,FPN_b2,unet_b1,PSP_seresx50,FPN_res34, unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34])\nmodel = Model([FPN_b3,unet_b0,unet_b1, FPN_seresx50, FPN_b0, FPN_b1, FPN_b2, unet_se_resnext50_32x4d, unet_resnet34,FPN_b4])\n# model_sig = Model_sig([FPN_b2, unet_se_resnext50_32x4d, unet_resnet34])","38fcbd62":"# start prediction\nthresholds_max=[0.7,0.7,0.7,0.7]\nthresholds_min=[0.2,0.2,0.3,0.3]\nmin_area=[350, 500, 750, 1000]\nres = []\npredictions = []\nfnames_all = []\n\n## IF USING CLASSIFIER\nfor i, batch in enumerate(tqdm(testset)):\n    fnames, images = batch    \n    batch_preds = model(images.to(device)).detach().cpu().numpy()\n#     batch_preds += 3 * model_sig(images.to(device)).detach().cpu().numpy()\n#     batch_preds = batch_preds\/7\n\n    # Batch post processing\n    for p, file in zip(batch_preds, fnames):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            p_channel_ = p_channel\n            imageid_classid = file+'_'+str(i+1)\n            p_channel = (p_channel>thresholds_max[i]).astype(np.uint8)\n            if p_channel.sum() < min_area[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n            else:\n                p_channel = (p_channel_>thresholds_min[i]).astype(np.uint8)\n            \n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })\n        \ndf = pd.DataFrame(res)","9be25f8c":"# save predictions to submission.csv\n# df = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])\ndf.to_csv(\"submission.csv\", index=False)","7d575e17":"def change(x):\n    x=x[-1:]\n    return x\ndf_=df.copy()\ndf_.ImageId_ClassId=df_.ImageId_ClassId.apply(change)\ndf_[df_.EncodedPixels!=''].ImageId_ClassId.value_counts()","5558f2f2":"df.head()","0c72b62f":"# train_path = '..\/input\/severstal-steel-defect-detection\/train.csv'\n# train_df = pd.read_csv(train_path)","31b07b68":"# for i in range(20):\n#     NUMBER=train_df.dropna().index[i]\n#     train_data_folder = '..\/input\/severstal-steel-defect-detection\/train_images'\n#     num_workers = 4\n#     batch_size = 1\n#     mean = (0.485, 0.456, 0.406)\n#     std = (0.229, 0.224, 0.225)\n#     trainset = DataLoader(\n#         TestDataset(train_data_folder, train_df[NUMBER:NUMBER+1], mean, std),\n#         batch_size=batch_size,\n#         shuffle=False,\n#         num_workers=num_workers,\n#         pin_memory=True\n#     )\n#     thresholds=[0.2,0.5,0.5,0.5]\n#     min_area=[100, 600, 1000, 2000]\n#     res = []\n#     predictions = []\n#     fnames_all = []\n    \n#     ## IF USING CLASSIFIER\n#     for i, batch in enumerate(tqdm(trainset)):\n#         fnames, images = batch    \n#         batch_preds = torch.sigmoid(model(images.to(device))).detach().cpu().numpy()\n    \n#         # Batch post processing\n#         for p, file in zip(batch_preds, fnames):\n#             file = os.path.basename(file)\n#             # Image postprocessing\n#             for i in range(4):\n#                 p_channel = p[i]\n#                 imageid_classid = file+'_'+str(i+1)\n#                 p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n#                 if p_channel.sum() < min_area[i]:\n#                     p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n#                 a = p_channel\n#                 if(i==int(train_df.ImageId_ClassId.values[NUMBER][-1:])-1):\n#                     print(int(train_df.ImageId_ClassId.values[NUMBER][-1:]))\n#                     break\n#     kernel = np.ones((10,10),np.uint8)\n# #     a_er=cv2.erode(a,kernel,5)\n#     kernel = np.ones((5,5),np.uint8)\n#     a_er=cv2.dilate(a,kernel,1)\n#     plt.imshow(a,cmap='gray')\n#     plt.pause(0.00001)\n#     plt.imshow(a_er,cmap='gray')\n#     plt.pause(0.00001)\n#     plt.imshow(rle2mask(train_df.EncodedPixels.values[NUMBER]),cmap='gray')\n#     plt.pause(0.00001)","df66b76e":"# a.sum()","ffba81e2":"# a.shape","bc48cd6f":"# def rle2mask(mask_rle, shape=(1600,256)):\n#     '''\n#     mask_rle: run-length as string formated (start length)\n#     shape: (width,height) of array to return \n#     Returns numpy array, 1 - mask, 0 - background\n\n#     '''\n#     s = mask_rle.split()\n#     starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n#     starts -= 1\n#     ends = starts + lengths\n#     img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n#     for lo, hi in zip(starts, ends):\n#         img[lo:hi] = 1\n#     return img.reshape(shape).T","9782a18c":"## Classifier","c4c43ee9":"# Visualisation","b12bcc9b":"## Infernce"}}