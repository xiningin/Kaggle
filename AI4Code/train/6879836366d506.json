{"cell_type":{"32932fb5":"code","009abc70":"code","cbefadfa":"code","638b8025":"code","9ffc9052":"code","092e8838":"code","ebd7fd99":"code","b3f10fc9":"code","b7aafe38":"code","b4e6f7d5":"code","94f6e54c":"code","83c6881e":"code","876c63bd":"code","cd0030c5":"code","1b0b05a0":"code","ef11f700":"code","2e95ffa1":"code","10029c6a":"code","c245cf07":"code","4c93e820":"code","73de74e7":"code","909954f7":"code","700a508a":"code","b7600e2a":"code","687deef6":"code","eaddcdc9":"code","b3a5fef4":"code","e9f84902":"code","abcc08ab":"code","b9185053":"code","b53d4fe7":"code","d8f34d4f":"code","760cf2e3":"code","116c66f3":"code","9ca668f9":"code","716b10d5":"code","24e49ec1":"code","9f747857":"code","699acd5a":"code","8a7664b6":"code","d99ba53f":"code","896a4cec":"code","5505d0e5":"code","91972e57":"code","4be95722":"code","c8066cdb":"code","5a5a74de":"code","086ae2cd":"code","431aa552":"code","1b4016d2":"code","52105384":"code","926266fd":"code","14c17f3e":"code","dc30dafa":"code","fc319ae7":"code","2874f6d0":"markdown","7abbeb9d":"markdown","93b9a884":"markdown"},"source":{"32932fb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","009abc70":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type by https:\/\/www.kaggle.com\/nz0722\/aligned-timestamp-lgbm-by-meter-type\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","cbefadfa":"from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\ndates_range = pd.date_range(start='2015-12-31', end='2019-01-01')\nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())","638b8025":"basePath = '\/kaggle\/input\/ashrae-energy-prediction\/'\nbuilding_metadata = pd.read_csv(basePath+'building_metadata.csv')\nweather_train = pd.read_csv(basePath+'weather_train.csv')\nraw_train = pd.read_csv(basePath+'train.csv')\nprint(building_metadata.head(10))\nprint(weather_train.head(10))\nprint(raw_train.head(10))","9ffc9052":"print('The shape of plain train data is:',np.shape(raw_train))","092e8838":"weather_distribution_cols = ['dew_temperature','air_temperature','cloud_coverage','precip_depth_1_hr','wind_direction','wind_speed','sea_level_pressure']\nfor i,i_col in enumerate(weather_distribution_cols):\n    plt.figure(i)\n    ax = sns.distplot(weather_train[i_col].dropna())\n    ax.set_title(i_col + ' histogram distribution')","ebd7fd99":"weather_train['hour'] = pd.to_datetime(weather_train['timestamp']).dt.hour\nuniqueSites = np.unique(weather_train['site_id'])\nfor i,i_site in enumerate(uniqueSites):\n    plt.figure(i)\n    curSiteData = weather_train.loc[weather_train['site_id']==i_site,['hour','air_temperature']].groupby('hour').mean().reset_index()\n    ax = sns.lineplot(x=\"hour\", y=\"air_temperature\", data=curSiteData)\n    ax.set_title(i_site)  ","b3f10fc9":"del curSiteData\ngc.collect()","b7aafe38":"weather_train = weather_train.drop('hour',axis=1)","b4e6f7d5":"offsetBase = weather_train[['site_id','timestamp','air_temperature']]\noffsetBase['timestamp'] = pd.to_datetime(offsetBase['timestamp'])\noffsetBase['temp_rank'] = offsetBase.groupby(['site_id', offsetBase.timestamp.dt.date])['air_temperature'].rank('average')","94f6e54c":"offsetBase_2d = offsetBase.groupby(['site_id',offsetBase.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\nsite_offset = pd.Series(offsetBase_2d.values.argmax(axis=1))-14\nsite_offset.index.name = 'site_id'","83c6881e":"weather_train['timestamp'] = pd.to_datetime(weather_train['timestamp'])\ndef alignTimestamp(df):\n    df['offset'] = df.site_id.map(site_offset)\n    df['timestamp_updated'] = df['timestamp'] - pd.to_timedelta(df['offset'],unit='H')\n    df['timestamp'] = df['timestamp_updated']\n    df = df.drop('timestamp_updated',axis=1)\n    return df\n\nweather_train = alignTimestamp(weather_train)","876c63bd":"weather_na_cols = [\"wind_direction\",\"wind_speed\"]\nfor i,i_col in enumerate(weather_na_cols):\n    weather_train[i_col].fillna(weather_train[i_col].mean(),inplace=True)","cd0030c5":"raw_train['timestamp'] = pd.to_datetime(raw_train['timestamp'])\nraw_train['dayofweek'] = raw_train['timestamp'].dt.dayofweek\nraw_train['weekday'] = raw_train['timestamp'].dt.weekday\nraw_train['hour'] = raw_train['timestamp'].dt.hour\nraw_train['day'] = raw_train['timestamp'].dt.day\nraw_train['month'] = raw_train['timestamp'].dt.month","1b0b05a0":"groupedDOWTrainData = raw_train.groupby(['building_id','dayofweek','meter']).mean().reset_index()\ngroupedDOWTrainData_buildingInfo = pd.merge(groupedDOWTrainData, building_metadata, on='building_id', how='left')\nprimary_use = np.unique(groupedDOWTrainData_buildingInfo.primary_use)\nfor i,i_use in enumerate(primary_use):\n    plt.figure(i)\n    ax = sns.barplot(x=\"dayofweek\", y=\"meter_reading\", data=groupedDOWTrainData_buildingInfo.loc[groupedDOWTrainData_buildingInfo['primary_use']==i_use].groupby('dayofweek').mean().reset_index()[['dayofweek','meter_reading']])\n    ax.set_title(str(i_use) + ' average meter reading by day of week')  \n\n\ndel groupedDOWTrainData    ","ef11f700":"groupedHourTrainData = raw_train.groupby(['building_id','hour','meter']).mean().reset_index()\ngroupedHourTrainData_buildingInfo = pd.merge(groupedHourTrainData, building_metadata, on='building_id', how='left')\nfor i,i_use in enumerate(primary_use):\n    plt.figure(i)\n    ax = sns.barplot(x=\"hour\", y=\"meter_reading\", data=groupedHourTrainData_buildingInfo.loc[groupedHourTrainData_buildingInfo['primary_use']==i_use].groupby('hour').mean().reset_index()[['hour','meter_reading']])\n    ax.set_title(str(i_use) + ' average meter reading by hour')  \n    \ndel groupedHourTrainData","2e95ffa1":"del raw_train\ngc.collect()","10029c6a":"raw_train = pd.read_csv(basePath+'train.csv')","c245cf07":"raw_train_building_info = pd.merge(raw_train, building_metadata,left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\ngroupedYearBuiltTrainData = raw_train_building_info.groupby(['year_built']).mean().reset_index()\nax = sns.lineplot(x=\"year_built\", y=\"meter_reading\", data=groupedDOWTrainData_buildingInfo[['year_built','meter_reading']])\nax.set_title('Average Meter reading by year')  \ndel groupedYearBuiltTrainData  ","4c93e820":"groupedFloorCountTrainData = raw_train_building_info.groupby(['floor_count']).mean().reset_index()\nax = sns.lineplot(x=\"floor_count\", y=\"meter_reading\", data=groupedFloorCountTrainData[['floor_count','meter_reading']])\nax.set_title('Average Meter reading by floor count')  ","73de74e7":"del raw_train_building_info","909954f7":"ax = sns.distplot(raw_train['meter_reading'])\nax.set_title('Meter reading distribution on training data')\nraw_train['meter_reading'] = np.log1p(raw_train['meter_reading'])","700a508a":"# Remove outliers\n# the below portion is from https:\/\/www.kaggle.com\/aitude\/ashrae-kfold-lightgbm-without-leak-1-08\nraw_train = raw_train [ raw_train['building_id'] != 1099 ]\nraw_train = raw_train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","b7600e2a":"raw_train['timestamp'] = pd.to_datetime(raw_train['timestamp'])\nraw_train['is_holiday'] = (raw_train['timestamp'].dt.date.isin(us_holidays)).astype(np.int8)","687deef6":"building_traindata = pd.merge(raw_train,building_metadata,left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\nraw_train_data = pd.merge(building_traindata,weather_train,left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\nna_columns = raw_train_data.isna().sum()\nprint(na_columns\/raw_train_data.shape[0])","eaddcdc9":"raw_train_data['dayofweek'] = raw_train_data['timestamp'].dt.dayofweek.astype(np.uint8)\nraw_train_data['weekday'] = raw_train_data['timestamp'].dt.weekday.astype(np.uint8)\nraw_train_data['hour'] = raw_train_data['timestamp'].dt.hour.astype(np.uint8)\nraw_train_data['day'] = raw_train_data['timestamp'].dt.day.astype(np.uint8)\nraw_train_data['month'] = raw_train_data['timestamp'].dt.month.astype(np.uint8)","b3a5fef4":"ax = sns.barplot(x=\"meter\", y=\"meter_reading\", data=raw_train_data.groupby('meter').mean().reset_index()[['meter','meter_reading']])\nax.set_title('By meter average reading')      ","e9f84902":"ax = sns.barplot(x=\"meter\", y=\"counts\", data=raw_train_data.groupby('meter').size().reset_index(name='counts'))\nax.set_title('By meter count')      ","abcc08ab":"beaufort_vals = [(0,0,0.4),(1,0.5,1.5),(2,1.6,3.3),(3,3.4,5.5),(4,5.6,7.9),(5,8.0,10.7),(6,10.8,13.8),(7,13.9,17.1),\n                 (8,17.2,20.7),(9,20.8,24.4),(10,24.5,28.4),(11,28.5,32.6),(12,32.7,400)]","b9185053":"def degToCompass(num):\n    val=int((num\/22.5)+.5)\n    arr=[i for i in range(0,16)]\n    return arr[(val % 16)]","b53d4fe7":"raw_train_data['wind_direction'] = raw_train_data['wind_direction'].apply(degToCompass)","d8f34d4f":"for i_scale in beaufort_vals:\n    raw_train_data.loc[(raw_train_data['wind_speed']>=i_scale[1]) & (raw_train_data['wind_speed']<=i_scale[2]),'beaufort_scale'] = i_scale[0]","760cf2e3":"categoricalCols = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekday\", \"meter\",  \"wind_direction\",\"is_holiday\"]\nnumericCols = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\", 'precip_depth_1_hr', 'floor_count','beaufort_scale']","116c66f3":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(raw_train_data['primary_use'])\nraw_train_data['primary_use'] = le.transform(raw_train_data['primary_use'])      ","9ca668f9":"#featuresToUse = categoricalCols+numericCols\nfeaturesToUse = numericCols+categoricalCols\ntrain_data = raw_train_data.loc[:,featuresToUse]\ntrain_data['square_feet'] = np.log(train_data['square_feet'])\ntrain_data['year_built'] = train_data['year_built']-1900\ntrain_data = reduce_mem_usage(train_data)\ntarget_data = raw_train_data['meter_reading']","716b10d5":"del weather_train,raw_train_data\ngc.collect()","24e49ec1":"from sklearn.model_selection import StratifiedKFold,GroupKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\n\nn_folds=2\ncv = KFold(n_splits=n_folds, random_state=666, shuffle=False)\n\ndef computeRMSE(clf,index):\n    y_predict = clf.predict(train_data.iloc[index])\n    y_predict[y_predict<0]=0\n    return np.sqrt(mean_squared_error( target_data.iloc[index], y_predict ))","9f747857":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n            'subsample': 0.25,\n            'subsample_freq': 1,\n            'learning_rate': 0.4,\n            'num_leaves': 20,\n            'feature_fraction': 0.9,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'max_bin':64,\n            'early_stopping_rounds':200\n            }","699acd5a":"scores_lgb = []\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = train_data.columns     \nmodels = []\n\nfor (train, val), i in zip(cv.split(train_data, train_data['building_id']), range(n_folds)):\n    lgb_train = lgbm.Dataset(train_data.iloc[train,:], target_data.iloc[train],categorical_feature=categoricalCols)\n    lgb_val = lgbm.Dataset(train_data.iloc[val,:], target_data.iloc[val],categorical_feature=categoricalCols)\n    lgb_best = lgbm.train(\n            params,\n            lgb_train,\n            num_boost_round=2000,\n            valid_sets=(lgb_train, lgb_val),\n            verbose_eval = 100\n            )    \n    #feature_importances['fold_{}'.format(i + 1)] = lgb_best.feature_importances_\n    feature_importances['fold_{}'.format(i + 1)] = lgb_best.feature_importance()\n    rmse_train = computeRMSE(lgb_best,train)\n    rmse_val = computeRMSE(lgb_best,val)\n    scores_lgb.append((rmse_train, rmse_val))\n    models.append(lgb_best)","8a7664b6":"print(scores_lgb)","d99ba53f":"del train_data, target_data","896a4cec":"del lgb_train,lgb_val,train, val","5505d0e5":"feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(cv.n_splits)]].mean(axis=1)\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False), x='average', y='feature');\nplt.title('TOP feature importance over {} folds average'.format(cv.n_splits));","91972e57":"raw_test = pd.read_csv(basePath+'test.csv')\nraw_test['timestamp'] = pd.to_datetime(raw_test['timestamp'])\nraw_test['is_holiday'] = (raw_test['timestamp'].dt.date.isin(us_holidays)).astype(np.int8)\nbuilding_testdata = pd.merge(raw_test,building_metadata,left_on = [\"building_id\"], right_on = [\"building_id\"],how='left')","4be95722":"del building_metadata,raw_test\ngc.collect()\nbuilding_testdata = reduce_mem_usage(building_testdata)","c8066cdb":"weather_test = pd.read_csv(basePath+'weather_test.csv')\nweather_test['timestamp'] = pd.to_datetime(weather_test['timestamp'])\nweather_test = alignTimestamp(weather_test)\nfor i,i_col in enumerate(weather_na_cols):\n    weather_test[i_col].fillna(weather_test[i_col].mean(),inplace=True)\nraw_test_data = pd.merge(building_testdata,weather_test,left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"],how = 'left')\ndel weather_test,building_testdata\ngc.collect()","5a5a74de":"na_columns = raw_test_data.isna().sum()\nprint(na_columns\/raw_test_data.shape[0])\nfor i,i_col in enumerate(weather_na_cols):\n    raw_test_data[i_col].fillna(raw_test_data[i_col].mean(),inplace=True)","086ae2cd":"raw_test_data['wind_direction'] = raw_test_data['wind_direction'].apply(degToCompass)\nfor i_scale in beaufort_vals:\n    raw_test_data.loc[(raw_test_data['wind_speed']>=i_scale[1]) & (raw_test_data['wind_speed']<=i_scale[2]),'beaufort_scale'] = i_scale[0]","431aa552":"raw_test_data['primary_use'] = le.transform(raw_test_data['primary_use'])  \nraw_test_data = reduce_mem_usage(raw_test_data)","1b4016d2":"raw_test_data['dayofweek'] = raw_test_data['timestamp'].dt.dayofweek.astype(np.uint8)\nraw_test_data['weekday'] = raw_test_data['timestamp'].dt.weekday.astype(np.uint8)\nraw_test_data['hour'] = raw_test_data['timestamp'].dt.hour.astype(np.uint8)\nraw_test_data['day'] = raw_test_data['timestamp'].dt.day.astype(np.uint8)\nraw_test_data['month'] = raw_test_data['timestamp'].dt.month.astype(np.uint8)\nraw_test_data = raw_test_data.drop(['timestamp'],axis=1)","52105384":"raw_test_data['square_feet'] = np.log(raw_test_data['square_feet'])\nraw_test_data['year_built'] = raw_test_data['year_built']-1900\nraw_test_data = raw_test_data[featuresToUse]\nraw_test_data,NAList = reduce_mem_usage(raw_test_data) \ngc.collect()","926266fd":"from tqdm import tqdm\ni = 0\nstepSize = 50000\npredictions = []\nfor j in tqdm(range(int(np.ceil(raw_test_data.shape[0]\/stepSize)))):\n    predictions.append(np.expm1(sum([model.predict(raw_test_data.iloc[i:i+stepSize]) for model in models])\/n_folds))\n    i+=stepSize\npredictions = np.concatenate(predictions)","14c17f3e":"submission = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv')\nsubmission['meter_reading'] = predictions\nsubmission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\nsubmission.to_csv('submission.csv', index=False)\nsubmission","dc30dafa":"submission.describe()","fc319ae7":"sns.distplot(np.log1p(predictions))","2874f6d0":"Once again there are clear trends in energy usage - it is mostly low between hours 0-6 and starts peaking up slowly and remains high till around hour 21.","7abbeb9d":"Key thing to be noted is while for most of the building sites the average meter reading drops for Saturday and Sunday, it largely remains flat or inverts this trend for entertainment (it even dips here on weekdays), healthcare, parking...so day of week is likely to play a key role (which also makes sense intuitively) in the meter reading","93b9a884":"As noted in many of the other kernels, the timestamps seem to be not aligned in weather_train as in some occassions we notice peak temperature late in the night. If we assume 2 pm to be the peak temperature and update the timestamps it might be good!"}}