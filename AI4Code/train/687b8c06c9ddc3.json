{"cell_type":{"f49c1732":"code","626a248e":"code","6c4e7237":"code","9484f159":"code","a616c676":"code","f3a168f0":"code","0e5ab49d":"code","53efe9e0":"code","36569a77":"code","3dac2763":"code","4cedd287":"code","545bc8b7":"code","6a7e2963":"code","a4a2048b":"code","1718c7d3":"code","8a894c07":"code","f23f04a4":"code","36f14d46":"code","fb37cfd3":"code","0de1119b":"markdown","e58467a9":"markdown","d2c6c521":"markdown","02a6524e":"markdown","c102f7cb":"markdown","87bc31fe":"markdown","1b1e6bcf":"markdown","87c0b68a":"markdown","542de70c":"markdown","13cc840b":"markdown"},"source":{"f49c1732":"import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import ArtistAnimation\n\nfrom kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.rps.utils import get_score\nfrom kaggle_environments.envs.rps.agents import *\n\nplt.style.use('ggplot')\nplt.rcParams[\"animation.html\"] = \"html5\"","626a248e":"def animate_progress(agents_logs, agents_names, save=False):\n    \n    agent1_log, agent2_log = agents_logs\n    with open(agent1_log, 'rb') as f:\n        agent1_log = pickle.load(f)\n    with open(agent2_log, 'rb') as f:\n        agent2_log = pickle.load(f)\n    \n    agent1_name, agent2_name = agents_names\n    \n    agent1_hist = agent1_log['dist']\n    agent2_hist = agent2_log['dist']\n    \n    agent1_scores = agent1_log['score']\n    agent2_scores = agent2_log['score']\n    \n    images = []\n    steps = len(agent1_hist)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12,6), sharey=True)\n    for k in range(steps):\n\n        ax[0].set_ylabel('$P$')\n        ax[0].set_ylim([0, 1.1])\n        img1 = ax[0].bar(['R', 'P', 'S'], agent1_hist[k], color='tab:blue', alpha=0.8)\n        img2 = ax[1].bar(['R', 'P', 'S'], agent2_hist[k], color='tab:gray', alpha=0.8)\n        img3 = ax[0].text(0.4,1.2, f'{agent1_name} score: {agent1_scores[k]:.0f}', fontsize=14)\n        img4 = ax[1].text(0.4,1.2, f'{agent2_name} score: {agent2_scores[k]:.0f}', fontsize=14)\n        img5 = ax[0].text(2.5,1.2, f'iteration: {k:.0f}', fontsize=12)\n     \n        images.append([*img1, *img2, img3, img4, img5])\n\n    animation = ArtistAnimation(fig, images)\n    plt.close()\n    \n    if save:\n        animation.save('animation.gif', writer='imagemagick', fps=10)\n        \n    return animation","6c4e7237":"%%writefile statistical_agent.py\n\nimport pickle\nimport numpy as np\nfrom kaggle_environments.envs.rps.utils import get_score\n\n\nlog = {'dist': [],\n       'score': [0]}\nlog_file = 'statistical_agent.pkl'\n\nwith open(log_file, 'wb') as f:\n  pickle.dump(log, f)\n\ndef update_log(probs, score, log_file):\n    with open(log_file, 'rb') as f:\n        log = pickle.load(f)\n    log['dist'].append(list(probs))\n    log['score'].append(log['score'][-1] + score)\n    with open(log_file, 'wb') as f:\n        pickle.dump(log, f)\n\naction_histogram = {}\nagent_last_action = []\n\ndef statistical(observation, configuration):\n    \n    global action_histogram, agent_last_action\n    global log_file\n    \n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    \n    if agent_last_action:\n        score = get_score(agent_last_action[-1],\n                         observation.lastOpponentAction)\n    else:\n        score = 0 \n    \n    probs = [0, 0, 0]\n    for k, v in action_histogram.items():\n        probs[k] = v\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n        \n    probs = np.exp(probs) \/ np.sum(np.exp(probs))    \n    probs = np.roll(probs, 1)\n    \n    hand = (mode_action + 1) % configuration.signs\n    agent_last_action.append(hand)\n    \n    update_log(probs, score, log_file)\n    \n    return hand","9484f159":"%%writefile rock_agent.py\n\nimport pickle\nfrom kaggle_environments.envs.rps.utils import get_score\n\n\nagent_last_action = []\nlog_file = 'rock_agent.pkl'\n\nlog = {'dist': [],\n       'score': [0]}\n\nwith open(log_file, 'wb') as f:\n  pickle.dump(log, f)\n\ndef update_log(probs, score, log_file):\n    with open(log_file, 'rb') as f:\n        log = pickle.load(f)\n    log['dist'].append(list(probs))\n    log['score'].append(log['score'][-1] + score)\n    with open(log_file, 'wb') as f:\n        pickle.dump(log, f)\n\ndef rock(observation):\n    \n    global agent_last_action\n    global log_file\n    \n    if agent_last_action:\n        score = get_score(agent_last_action[-1],\n                         observation.lastOpponentAction)\n    else:\n        score = 0 \n    hand = 0\n    probs = [1, 0, 0]\n    agent_last_action.append(hand)\n    \n    update_log(probs, score, log_file)\n    \n    return hand","a616c676":"env = make('rps', configuration={\"episodeSteps\": 50}, debug=True)\nres = env.run(['statistical_agent.py', 'rock_agent.py'])","f3a168f0":"agents_logs = ['statistical_agent.pkl', 'rock_agent.pkl']\nagents_names = ['Statistical', 'Rock']","0e5ab49d":"animation = animate_progress(agents_logs, agents_names)\nanimation","53efe9e0":"%%writefile dirichlet_agent.py\n\nimport pickle\nimport numpy as np\nfrom kaggle_environments.envs.rps.utils import get_score\n\n\nagent_last_action = []\nlog_file = 'dirichlet_agent.pkl'\n\nlog = {'dist': [],\n       'score': [0]}\n\nwith open(log_file, 'wb') as f:\n  pickle.dump(log, f)\n\ndef update_log(probs, score, log_file):\n    with open(log_file, 'rb') as f:\n        log = pickle.load(f)\n    log['dist'].append(list(probs))\n    log['score'].append(log['score'][-1] + score)\n    with open(log_file, 'wb') as f:\n        pickle.dump(log, f)\n\nalphas = np.array([5, 5, 5])\n\ndef dirichlet_agent(observation, configuration):\n    \n    global alphas\n    global agent_last_action\n    global log_file\n    \n    if observation.step == 0:\n        hand = np.random.randint(3)\n        return hand\n    \n    opponent_last_action = observation.lastOpponentAction\n    N = np.array([1 if i == opponent_last_action else 0 for i in range(3)])\n    alphas += N\n    probs = (alphas - 1) \/ np.sum(alphas - 1)\n    opponent_next_acion = np.random.choice([0, 1, 2], p=probs)\n    hand = (opponent_next_acion + 1) % 3\n    \n    if agent_last_action:\n        score = get_score(agent_last_action[-1],\n                         observation.lastOpponentAction)\n    else:\n        score = 0\n    agent_last_action.append(hand)\n    probs = np.roll(probs, 1)\n    update_log(probs, score, log_file)\n    \n    return int(hand)","36569a77":"env = make('rps', configuration={\"episodeSteps\": 1000}, debug=True)\nres = env.run(['statistical_agent.py', 'dirichlet_agent.py'])","3dac2763":"agents_logs = ['statistical_agent.pkl', 'dirichlet_agent.pkl']\nagents_names = ['Statistical', 'Dirichlet']","4cedd287":"animation = animate_progress(agents_logs, agents_names)\nanimation","545bc8b7":"%%writefile cnn_agent.py\n\nimport os\nimport pickle\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\n\nfrom kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.rps.utils import get_score\n\n\nclass RPS(nn.Module):\n    \"\"\"\n    Class that predict logits of action probabilities given game history.\n        Inputs: game history [bs, 2, 10].\n        Outputs: logits of action probabilities [bs, 3].\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(2, 4, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(4, 8, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(8, 16, 2, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2)\n        )\n        self.head = nn.Sequential(\n            nn.Linear(16, 6),\n            nn.ReLU(),\n            nn.Linear(6, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = torch.flatten(x, 1)\n        x = self.head(x)\n        return x\n    \n    \ndef soft_cross_entropy(target, prediciton):\n    log_probs = nn.functional.log_softmax(prediciton, dim=1)\n    sce = -(target * log_probs).sum() \/ target.shape[0]\n    return sce\n\n\ndef train_step(model, data, optimizer):\n    model.train()\n    torch.set_grad_enabled(True)\n\n    X = data['X'].view(-1, 2, 10)\n    y = data['y'].view(-1, 3)\n    prd = model(X)\n    loss = soft_cross_entropy(y, prd)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n\nbs = 6 # batch size  \n\nopponent_actions = []\nagent_actions = []\nactions = []\nbatch_x = []\nbatch_y = []\n\nlog = {'dist': [],\n       'score': [0]}\nlog_file = 'cnn_agent.pkl'\n\nwith open(log_file, 'wb') as f:\n  pickle.dump(log, f)\n\ndef update_log(probs, score, log_file):\n    with open(log_file, 'rb') as f:\n        log = pickle.load(f)\n    log['dist'].append(list(probs))\n    log['score'].append(log['score'][-1] + score)\n    with open(log_file, 'wb') as f:\n        pickle.dump(log, f)\n\nmodel = RPS()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef agent(observation, configuration):\n    \n    global actions, agent_actions, opponent_actions\n    global model, optimizer\n    global batch_x, batch_y\n    global bs\n    global log_file\n    \n    # first step\n    if observation.step == 0:\n        hand = np.random.randint(3)\n        actions.append(hand)\n        return hand\n    \n    # first warm up rounds\n    if 0 < observation.step < 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        hand = np.random.randint(3)\n        actions.append(hand)\n        \n        probs = [1\/3, 1\/3, 1\/3]\n        score = get_score(agent_actions[-1], opponent_actions[-1])\n        update_log(probs, score, log_file)\n        \n        return hand\n    \n    # start to train CNN\n    elif observation.step >= 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        \n        score = get_score(agent_actions[-1], opponent_actions[-1])\n        \n        wining_action = (opponent_actions[-1] + 1) % 3 \n        fair_action = opponent_actions[-1]\n        lose_action = (opponent_actions[-1] - 1) % 3 \n\n        # soft labels for target    \n        y = [0, 0, 0]\n        y[wining_action] = 0.7\n        y[fair_action] = 0.2\n        y[lose_action] = 0.1 \n        \n        # add data for history\n        batch_x.append([opponent_actions[-2:-12:-1],\n                        agent_actions[-2:-12:-1]])\n        batch_y.append(y)\n        \n        # data for single CNN update \n        data = {'X': torch.Tensor([opponent_actions[-2:-12:-1],\n                                   agent_actions[-2:-12:-1]]),\n                'y': torch.Tensor(y)} \n        \n        # evaluate single training step\n        train_step(model, data, optimizer)\n        \n        # evaluate mini-batch training steps\n        if observation.step % 10 == 0:\n            k = 1 if observation.step < 100 else 3\n            for _ in range(k):\n                idxs = np.random.choice(list(range(len(batch_y))), bs)\n                data = {'X': torch.Tensor(np.array(batch_x)[idxs]),\n                        'y': torch.Tensor(np.array(batch_y)[idxs])}\n                train_step(model, data, optimizer)\n        \n        # data for current action prediction\n        X_prd = torch.Tensor([opponent_actions[-1:-11:-1],\n                              agent_actions[-1:-11:-1]]).view(1, 2, -1)\n        \n        # predict logits\n        probs = model(X_prd).view(3)\n        # calculate probabilities\n        probs = nn.functional.softmax(probs, dim=0).detach().cpu().numpy()\n        \n        update_log(probs, score, log_file)\n        \n        # choose action\n        hand = np.random.choice([0, 1, 2], p=probs)\n        actions.append(hand)\n        \n        return int(hand)","6a7e2963":"env = make('rps', configuration={\"episodeSteps\": 1000}, debug=True)\nres = env.run(['statistical_agent.py', 'cnn_agent.py'])","a4a2048b":"agents_logs = ['statistical_agent.pkl', 'cnn_agent.pkl']\nagents_names = ['Statistical', 'CNN']","1718c7d3":"animation = animate_progress(agents_logs, agents_names)\nanimation","8a894c07":"%%writefile cnn_agent_1.py\n\nimport os\nimport pickle\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\n\nfrom kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.rps.utils import get_score\n\n\nclass RPS(nn.Module):\n    \"\"\"\n    Class that predict logits of action probabilities given game history.\n        Inputs: game history [bs, 2, 10].\n        Outputs: logits of action probabilities [bs, 3].\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(2, 4, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(4, 8, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(8, 16, 2, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2)\n        )\n        self.head = nn.Sequential(\n            nn.Linear(16, 6),\n            nn.ReLU(),\n            nn.Linear(6, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = torch.flatten(x, 1)\n        x = self.head(x)\n        return x\n    \n    \ndef soft_cross_entropy(target, prediciton):\n    log_probs = nn.functional.log_softmax(prediciton, dim=1)\n    sce = -(target * log_probs).sum() \/ target.shape[0]\n    return sce\n\n\ndef train_step(model, data, optimizer):\n    model.train()\n    torch.set_grad_enabled(True)\n\n    X = data['X'].view(-1, 2, 10)\n    y = data['y'].view(-1, 3)\n    prd = model(X)\n    loss = soft_cross_entropy(y, prd)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n\nbs = 6 # batch size  \n\nopponent_actions = []\nagent_actions = []\nactions = []\nbatch_x = []\nbatch_y = []\n\nlog = {'dist': [],\n       'score': [0]}\nlog_file = 'cnn_agent_1.pkl'\n\nwith open(log_file, 'wb') as f:\n  pickle.dump(log, f)\n\ndef update_log(probs, score, log_file):\n    with open(log_file, 'rb') as f:\n        log = pickle.load(f)\n    log['dist'].append(list(probs))\n    log['score'].append(log['score'][-1] + score)\n    with open(log_file, 'wb') as f:\n        pickle.dump(log, f)\n\nmodel = RPS()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef agent(observation, configuration):\n    \n    global actions, agent_actions, opponent_actions\n    global model, optimizer\n    global batch_x, batch_y\n    global bs\n    global log_file\n    \n    # first step\n    if observation.step == 0:\n        hand = np.random.randint(3)\n        actions.append(hand)\n        return hand\n    \n    # first warm up rounds\n    if 0 < observation.step < 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        hand = np.random.randint(3)\n        actions.append(hand)\n        \n        probs = [1\/3, 1\/3, 1\/3]\n        score = get_score(agent_actions[-1], opponent_actions[-1])\n        update_log(probs, score, log_file)\n        \n        return hand\n    \n    # start to train CNN\n    elif observation.step >= 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        \n        score = get_score(agent_actions[-1], opponent_actions[-1])\n        \n        wining_action = (opponent_actions[-1] + 1) % 3 \n        fair_action = opponent_actions[-1]\n        lose_action = (opponent_actions[-1] - 1) % 3 \n\n        # soft labels for target    \n        y = [0, 0, 0]\n        y[wining_action] = 0.7\n        y[fair_action] = 0.2\n        y[lose_action] = 0.1 \n        \n        # add data for history\n        batch_x.append([opponent_actions[-2:-12:-1],\n                        agent_actions[-2:-12:-1]])\n        batch_y.append(y)\n        \n        # data for single CNN update \n        data = {'X': torch.Tensor([opponent_actions[-2:-12:-1],\n                                   agent_actions[-2:-12:-1]]),\n                'y': torch.Tensor(y)} \n        \n        # evaluate single training step\n        train_step(model, data, optimizer)\n        \n        # evaluate mini-batch training steps\n        if observation.step % 10 == 0:\n            k = 1 if observation.step < 100 else 3\n            for _ in range(k):\n                idxs = np.random.choice(list(range(len(batch_y))), bs)\n                data = {'X': torch.Tensor(np.array(batch_x)[idxs]),\n                        'y': torch.Tensor(np.array(batch_y)[idxs])}\n                train_step(model, data, optimizer)\n        \n        # data for current action prediction\n        X_prd = torch.Tensor([opponent_actions[-1:-11:-1],\n                              agent_actions[-1:-11:-1]]).view(1, 2, -1)\n        \n        # predict logits\n        probs = model(X_prd).view(3)\n        # calculate probabilities\n        probs = nn.functional.softmax(probs, dim=0).detach().cpu().numpy()\n        \n        update_log(probs, score, log_file)\n        \n        # choose action\n        hand = np.random.choice([0, 1, 2], p=probs)\n        actions.append(hand)\n        \n        return int(hand)","f23f04a4":"env = make('rps', configuration={\"episodeSteps\": 1000}, debug=True)\nres = env.run(['cnn_agent.py', 'cnn_agent_1.py'])","36f14d46":"agents_logs = ['cnn_agent.pkl', 'cnn_agent_1.pkl']\nagents_names = ['CNN', 'CNN_1']","fb37cfd3":"animation = animate_progress(agents_logs, agents_names)\nanimation","0de1119b":"So it works as expected, `Statistical` agent quickly adapts to static `Rock` agent and learns a winning distribution. <br>\nNow let's explore something more interesting, let's put up `Statistical` agent with some adaptive agent, I use `Dirichlet` agent here:","e58467a9":"Now let's put `Statistical` agent with some relatively storng agent. For stronger agent I use `CNN` agent [described here](http:\/\/www.kaggle.com\/glebkum\/cnn-bot-for-rock-paper-scissors). ","d2c6c521":"For those agents about 10 iterations are enough to converge, let's evaluate a short episode:","02a6524e":"Let's start simple and look at the learning process of `Statistical` agent vs `Rock` agent:","c102f7cb":"Finally lets see fight with own shadow `CNN` vs `CNN`:","87bc31fe":"Here `Statistical` agent behaviour is similar, but `CNN` agent finds a more flexible way to adapt and beats it more frequently than `Dirichlet` agent.","1b1e6bcf":"# Learning process animation \n\nIn this notebook I look at agents that return Rock, Paper or Scissors from multinomial distribution with learnable parameters. <br>\nOnly agents with single strategy (single distribution) are considered. <br>\nE.g. distributions for `Rock` and `Random` agents: <br>\n![image.png](attachment:image.png)\n\nIf one visualizes those distributions during one episode between `Rock` and `Random`, we'll see that they remain the same as those agents have a fixed strategy (have no capacity to coodapt). <br>\nBut for more sophisticated agents we can see the process of distiributions coodaptation. <br>\nSuch a visualisation can be insightfull or at least fun, so let's give it a try. <br>\n\n## Content:\n1. `Statistical` vs `Rock` (learnable vs static).\n2. `Statistical` vs `Dirichlet` (learnable vs learnable).\n3. `Statistical` vs `CNN` (learnable vs stronger learnable).\n4. `CNN` vs `CNN` (fight with shadow).","87c0b68a":"So here we can see some patterns:\n1. `Statistical` agent quickly converges to some local stationary state with probability about `1` for one of the actions. \n2. `Dirichlet` agent slowly adapts his distirbution to win aginst `Statistical` choosen state.\n3. After some iterations `Statistical` has some quick transients and changes his preferable action, then the process repeats.","542de70c":"So, the agents with the same adaptation speed behave almost randomly and never stick to some unimodal strategy. <br>\nIt is interesting to notice that if one does not sample form bot distribution but use `argmax` instead, the agent becomes way more deterministic and can be predicted easily (note that position of maximum probability changes not so frequently). <br>\n<br>\nSo it is seems that a strong agent should be close to the random (should have almost flat distribution) to avoid being predicted and rapidly adapt to opponent's distribution by adding a bit more probability to the current winning action. <br>\nNow it is just there is one small thing left to do - is to write such a bot :)","13cc840b":"Here things become more interesting, so let's evaluate the full episode:"}}