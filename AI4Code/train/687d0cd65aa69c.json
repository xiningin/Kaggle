{"cell_type":{"3be0d253":"code","6bc8f6c8":"code","f41f30a7":"code","882014a8":"code","9cd945e4":"code","7efedcc0":"code","35d278f5":"code","f8714c57":"code","aab0140d":"code","ec19d91e":"code","c69d904c":"code","a26f985f":"code","59cfe472":"code","02a1f69c":"code","ea29da7f":"code","34c7f420":"code","b558562a":"code","fce36f42":"code","f660ba8a":"code","b2266c36":"code","58194376":"code","9a6339cc":"code","afe9c052":"code","0ad85576":"code","44b7d83b":"code","618df860":"code","f85991a3":"code","3b1c3ed1":"code","b71002ce":"code","c7c39d80":"code","f680476f":"code","3699c224":"code","e780ce99":"code","068087af":"code","9cd2f6cb":"code","fd1039a6":"code","6997cb1e":"code","4ca80f20":"code","fa5e13ca":"code","dd628a29":"code","f2806b6d":"code","a3869fba":"code","2ce89a32":"code","9d71dbef":"code","c54fb2c9":"code","4c42f25c":"code","86e5d811":"code","3d58649c":"markdown","4c001960":"markdown","e530508d":"markdown","4db1961d":"markdown","66a19fce":"markdown","aca7d563":"markdown","4512968a":"markdown","5a758594":"markdown","447a44eb":"markdown","c04ce05b":"markdown","7f5ea7a6":"markdown","a1b7d5a2":"markdown","b14e9c24":"markdown","f704fcd5":"markdown","fa0697a4":"markdown","d9b26bb4":"markdown","26f1e72c":"markdown"},"source":{"3be0d253":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6bc8f6c8":"!pip install imblearn","f41f30a7":"df = pd.read_csv(\"..\/input\/fetal-health-classification\/fetal_health.csv\")\ndf.head()","882014a8":"df.info()","9cd945e4":"df.isnull().sum()","7efedcc0":"df[\"fetal_health\"].unique()","35d278f5":"sns.countplot(df['fetal_health'])","f8714c57":"df2 = df.copy(deep=True)\npie1  =pd.DataFrame(df2['fetal_health'].replace(1.0,'Normal').replace(2.0,'Suspect').replace(3.0,'Pathological').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of fetal health',y = 'fetal_health', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(6,8))","aab0140d":"from imblearn.over_sampling import SMOTE\n\noutput_df = pd.DataFrame(df, columns = df.columns)\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(output_df, df['fetal_health'])","ec19d91e":"y.value_counts()","c69d904c":"X.shape","a26f985f":"X = df.drop([\"fetal_health\"],axis=1)\ny = df[\"fetal_health\"]","59cfe472":"from sklearn.preprocessing import StandardScaler\ns = StandardScaler()\n\nX = s.fit_transform(X)","02a1f69c":"X = pd.DataFrame(X)\nX.head()","ea29da7f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","34c7f420":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","b558562a":"from sklearn.model_selection import GridSearchCV\n\nparams = { 'n_neighbors' : [5,7,9,11,13,15],\n               'weights' : ['uniform','distance'],\n               'metric' : ['minkowski','euclidean','manhattan']}\n\ntic = time.time()\n\nknn_gs = GridSearchCV(KNeighborsClassifier(), params, verbose = 1, cv=3, n_jobs = -1)\nknn_gs_res = knn_gs.fit(X_train, y_train)\nbest_params = knn_gs_res.best_params_\n\nprint(\"Best score: \",knn_gs_res.best_score_)\nprint(\"Best parameters: \",best_params)\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")","fce36f42":"knn = KNeighborsClassifier(**best_params)\nknn.fit(X_train, y_train)\n\ny_hat = knn.predict(X_train)\ny_pred = knn.predict(X_test)","f660ba8a":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint('Training set accuracy: ', metrics.accuracy_score(y_train, y_hat))\nprint('Test set accuracy: ',metrics.accuracy_score(y_test, y_pred))","b2266c36":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression","58194376":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred = logreg.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint('Test set accuracy: ',metrics.accuracy_score(y_test, y_pred))","9a6339cc":"params ={'C':[0.001,0.01,0.1,1,10,100,1000],\n      \"penalty\":[\"l1\",\"l2\"]} \n\ntic = time.time()\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,params,cv=10)\nlogreg_cv.fit(X_train,y_train)\n\nbest_params = logreg_cv.best_params_\n\nprint(\"Tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"Accuracy :\",logreg_cv.best_score_)\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n","afe9c052":"logreg = LogisticRegression(**best_params)\nlogreg.fit(X_train,y_train)\ny_pred_test = logreg.predict(X_test)","0ad85576":"print(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test, y_pred_test))\nprint('Test set accuracy: ',metrics.accuracy_score(y_test, y_pred_test))","44b7d83b":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred_test = tree_clf.predict(X_test)\n\nprint(\"\\nConfusion Matrix is,\\n\",confusion_matrix(y_test,y_pred_test))\nprint(\"\\nClassification report:\\n\", classification_report(y_test,y_pred_test))\nprint(\"Accuracy on test set with tree: {:.2f}\".format(metrics.accuracy_score(y_test, y_pred_test)))","618df860":"from sklearn.model_selection import GridSearchCV\n\nparams_dt = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\ntic = time.time()\n\ntree_clf= DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(tree_clf, params_dt, scoring=\"accuracy\",n_jobs=-1, cv=5)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nbest_score=tree_cv.best_score_\n\nprint(f\"Best score: {best_score}\")\nprint(f\"Best paramters: {best_params}\")\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")","f85991a3":"tree_clf = DecisionTreeClassifier(**best_params)\ntree_clf.fit(X_train, y_train)\ny_pred_test = tree_clf.predict(X_test)\n\nprint(\"\\nConfusion Matrix is,\\n\",confusion_matrix(y_test,y_pred_test))\nprint(\"\\nClassification report:\\n\", classification_report(y_test,y_pred_test))\nprint(\"Accuracy on test set with tree: {:.2f}\".format(metrics.accuracy_score(y_test, y_pred_test)))","3b1c3ed1":"from sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import SVC","b71002ce":"SVM_OVO = OneVsOneClassifier(SVC(kernel='poly', decision_function_shape='ovo'))\nSVM_OVO.fit(X_train,y_train)","c7c39d80":"y_predict = SVM_OVO.predict(X_test)\nprint(classification_report(y_test, y_predict))\nprint('Test set accuracy: ',metrics.accuracy_score(y_test, y_predict))","f680476f":"#checking parameters OVO for GridSearch parameters\nprint(SVM_OVO.get_params().keys())","3699c224":"SVM_OVO = OneVsOneClassifier(SVC(decision_function_shape='ovo'))\n\nparam_grid = {'estimator__C': [0.1,1,10,50 ,100], \n              'estimator__gamma': [0.1,0.01],\n              'estimator__kernel': ['rbf','poly','sigmoid']}\ntic = time.time()\n\nmodel_svm = GridSearchCV(SVM_OVO, param_grid,cv=4,scoring='accuracy')\nmodel_svm.fit(X_train, y_train)\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")","e780ce99":"best_params = model_svm.best_params_\nbest_score = model_svm.best_score_\n\nprint(f\"Best score: {best_score}\")\nprint(f\"Best paramters: {best_params}\")","068087af":"best_C = best_params.get('estimator__C')\nbest_gamma = best_params.get('estimator__gamma')\nbest_kernel = best_params.get('estimator__kernel')","9cd2f6cb":"SVM_OVO = SVC(kernel=best_kernel, C=best_C, gamma=best_gamma, decision_function_shape='ovo')\nSVM_OVO.fit(X_train,y_train)","fd1039a6":"y_predict_OVO = SVM_OVO.predict(X_test)\n\nprint(classification_report(y_test, y_predict_OVO))\nprint(metrics.accuracy_score(y_test,y_predict_OVO))","6997cb1e":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize","4ca80f20":"y = label_binarize(y, classes=[1, 2, 3])\nn_classes = y.shape[1]","fa5e13ca":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=0)\n\nSVM_OVA = OneVsRestClassifier(SVC(kernel='poly', decision_function_shape='ovr'))\nSVM_OVA.fit(X_train,y_train)","dd628a29":"y_predict = SVM_OVA.predict(X_test)\nprint(classification_report(y_test, y_predict))\nprint('Test set accuracy: ',metrics.accuracy_score(y_test, y_predict))","f2806b6d":"SVM_OVA = OneVsRestClassifier(SVC(decision_function_shape='ovr'))\nparam_grid = {'estimator__C': [10, 100], \n              'estimator__gamma': [0.1,0.01],\n              'estimator__kernel': ['linear','poly','sigmoid','rbf']}\n\ntic = time.time()\n\nmodel_svm = GridSearchCV(SVM_OVA, param_grid,cv=4,scoring='accuracy')\nmodel_svm.fit(X_train, y_train)\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")","a3869fba":"best_params = model_svm.best_params_\nbest_score = model_svm.best_score_\n\nprint(f\"Best score: {best_score}\")\nprint(f\"Best paramters: {best_params}\")","2ce89a32":"best_C = best_params.get('estimator__C')\nbest_gamma = best_params.get('estimator__gamma')\nbest_kernel = best_params.get('estimator__kernel')","9d71dbef":"SVM_OVA = OneVsRestClassifier(estimator=SVC(C=best_C, gamma=best_gamma, kernel=best_kernel, decision_function_shape='ovr'))\nSVM_OVA.fit(X_train,y_train)","c54fb2c9":"y_predict_OVA = SVM_OVA.predict(X_test)\n\nprint(classification_report(y_test, y_predict_OVA))\nprint(\"accuracy: \",metrics.accuracy_score(y_test,y_predict_OVA))","4c42f25c":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_predict_OVA[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_predict_OVA.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","86e5d811":"import matplotlib.pyplot as plt\n\nplt.figure()\nplt.figure(figsize=(12,8))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","3d58649c":"# Logistic Regression","4c001960":"Checking missing variable in dataset","e530508d":"## Data\n\nThis dataset contains 2126 records of features extracted from Cardiotocogram exams, which were then classified by three expert obstetritians into 3 classes:\n\n* Normal\n* Suspect\n* Pathological","4db1961d":"# KNN","66a19fce":"## Comments","aca7d563":"This operation is used for balanced the classes SMOTE ( Synthetic Minority Over-sampling )","4512968a":"Minimizing the range differences of the variables with StandartScaler","5a758594":"We create labels and features with dropping target columns which is fetal health","447a44eb":"ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output.","c04ce05b":"Parameter tuning, Finding optimum parameters","7f5ea7a6":"# SVM with OVO(One Versus One)","a1b7d5a2":"# Decision tree","b14e9c24":"# SVM-OVA(One Versus All-Rest)","f704fcd5":"We have 3 different class so our classification is multiclass classification.If you have more than 2 classes there is no such thing as linear separability.","fa0697a4":"There is no missing data in dataset","d9b26bb4":"##### Test Performance - Test and Training Time - Best Bias-Variance-Tradeoff\n* When compare test accuracy all classifiers you can see that Decision Trees has best performance with accuracy: 0.92 and according to the confusion matrix,precision,f1-score and recall scores. SVM-OVO also has 0.92 accuracy but the confusion matrix scores are not better than decision tree.\n* On the other hand Decision Tree has longest training time compare to others also I can say that the effect of using many parameters in the grid search has increased this training time.\n* If you compare testing times there are not very different time between all clasifiers but logistic regression has a little bit slower than others for testing time.\n* I used Grid Search Cross Validation to find best parameters and tuned the parameters with it. Also used SMOTE (Synthetic Minority Over-sampling) to balanced the classes and prevent high bias in most frequent class. Lastly plot the roc curve in SVM-OVA to see bias-variance-tradeoff.","26f1e72c":"Plotting Roc curve"}}