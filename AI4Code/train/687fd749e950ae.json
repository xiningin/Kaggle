{"cell_type":{"b6e9d2b7":"code","7c467e77":"code","ccd06f52":"code","c723866e":"code","9bf32284":"code","2c0397ca":"code","096ce530":"code","a4b21ac3":"code","4e36f746":"code","d77f255d":"code","49513fcb":"code","3ad5308a":"code","f78b2763":"code","14f2861b":"code","841bd98b":"code","6a43dac7":"code","264d87db":"code","b51f8a3c":"code","0f91cdf5":"code","d5141778":"code","910510a9":"code","79287c63":"markdown","9d96c829":"markdown","d89f9443":"markdown","ad76c72a":"markdown","6e32b3e6":"markdown"},"source":{"b6e9d2b7":"!pip install pytorch-tabnet\n!pip install rgf_python","7c467e77":"from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler, QuantileTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, BayesianRidge, LinearRegression, ElasticNet\nfrom scipy.optimize import minimize\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom rgf.sklearn import RGFRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn import model_selection\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nimport catboost\nimport lightgbm as lgbm\nimport xgboost as xgb\nimport optuna\nimport tqdm\nimport pickle\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=UserWarning)","ccd06f52":"input_dir = Path('..\/input\/tabular-playground-series-aug-2021\/')\ntrain_df = pd.read_csv(input_dir \/ 'train.csv')\ntest_df = pd.read_csv(input_dir \/ 'test.csv')\nsample_submission = pd.read_csv(input_dir \/ 'sample_submission.csv')","c723866e":"X = train_df.drop(['id', 'loss'], axis=1).values\ny = train_df['loss'].values\nX_test = test_df.drop(['id'], axis=1).values","9bf32284":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","2c0397ca":"def objectivexgb(trial):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 6, 12),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.05, 1.0, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, 100),\n        'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n        'learning_rate': trial.suggest_discrete_uniform('learning_rate', 0.01, 0.1, 0.01),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n        'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n        'min_child_weight': trial.suggest_int('min_child_weight', 5, 20),\n    }\n\n    reg = xgb.XGBRegressor(tree_method='gpu_hist', **params)\n    reg.fit(X_train, y_train,eval_set=[(X_valid, y_valid)], eval_metric='rmse',verbose=False)\n    \n    y_preds = reg.predict(X_valid)\n    loss = np.sqrt(mean_squared_error(y_valid, y_preds))\n    \n    return loss","096ce530":"study = optuna.create_study(direction='minimize', study_name='XGBoostOptuna')\nstudy.optimize(objectivexgb, n_trials=50)\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","a4b21ac3":"xgb_params = study.best_trial.params\nxgb_params['objective'] = 'reg:squarederror'","4e36f746":"def objectivecatb(trial):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 12000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'loss_function':'RMSE',\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'learning_rate' : trial.suggest_uniform('learning_rate',0.01,0.3),\n              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n              'subsample': trial.suggest_uniform('subsample',0,1),\n              'random_strength': trial.suggest_uniform('random_strength',1,50),\n              'depth': trial.suggest_int('depth',3,14),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    model = CatBoostRegressor(**params)  \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n        \n    y_preds = model.predict(X_test)\n    loss = np.sqrt(mean_squared_error(y_test, y_preds))\n    \n    return loss\n","d77f255d":"study2 = optuna.create_study(direction='minimize', study_name='CatBoostOptuna')\nstudy2.optimize(objectivecatb, n_trials=50)\n\nprint('Number of finished trials:', len(study2.trials))\nprint('Best trial: score {}, params {}'.format(study2.best_trial.value, study2.best_trial.params))","49513fcb":"catb_params = study2.best_trial.params\ncatb_params['loss_function'] = 'RMSE'\ncatb_params['eval_metric'] = 'RMSE'\ncatb_params['leaf_estimation_method'] = 'Newton'\ncatb_params['random_state'] = 42","3ad5308a":"def objectivelgbm(trial):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42)\n    params = {\n        \"metric\": \"RMSE\",\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.01,0.5),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n        'num_threads': trial.suggest_int('num_threads', 1, 10),\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n    }\n    model = lgbm.LGBMRegressor(**params,device = 'gpu',random_state=42)\n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose = False)\n        \n    y_preds = model.predict(X_test)\n    loss = np.sqrt(mean_squared_error(y_test, y_preds))\n    \n    return loss","f78b2763":"study3 = optuna.create_study(direction='minimize', study_name='LGBMOptuna')\nstudy3.optimize(objectivelgbm, n_trials=50)\n\nprint('Number of finished trials:', len(study3.trials))\nprint('Best trial: score {}, params {}'.format(study3.best_trial.value, study3.best_trial.params))","14f2861b":"lgb_params=study3.best_trial.params","841bd98b":"xgb_params","6a43dac7":"catb_params","264d87db":"lgb_params","b51f8a3c":"#Previous results are gathered after many hours of tuning.\n\n#You can also use parameters below.\n\nxgb_params={'max_depth': 11,\n 'subsample': 0.6500000000000001,\n 'n_estimators': 1700,\n 'eta': 0.02,\n 'learning_rate': 0.01,\n 'reg_alpha': 7,\n 'reg_lambda': 32,\n 'min_child_weight': 19,\n 'objective': 'reg:squarederror'}\n\ncatb_params={'iterations': 8195,\n 'od_wait': 2000,\n 'learning_rate': 0.01039421755643651,\n 'reg_lambda': 95.14582565179668,\n 'subsample': 0.6044381624463067,\n 'random_strength': 15.077418882976177,\n 'depth': 12,\n 'min_data_in_leaf': 5,\n 'leaf_estimation_iterations': 4,\n 'loss_function': 'RMSE',\n 'eval_metric': 'RMSE',\n 'leaf_estimation_method': 'Newton',\n 'random_state': 42}\n\n\nlgb_params={'lambda_l1': 0.19673487505279366,\n 'lambda_l2': 6.205681774095499e-05,\n 'num_leaves': 20,\n 'learning_rate': 0.1229039615047327,\n 'feature_fraction': 0.8566649457461354,\n 'bagging_fraction': 0.9999164419693399,\n 'bagging_freq': 10,\n 'min_child_samples': 92,\n 'num_threads': 5}","0f91cdf5":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\nfinal_test_preds = []\n\nX = pd.DataFrame(X)\ny = pd.DataFrame(y)\n\nxgb_base_model = xgb.XGBRegressor(**xgb_params, gpu_id=0, tree_method = 'gpu_hist')\nctb_base_model = catboost.CatBoostRegressor(**catb_params, task_type='GPU')\nlgb_base_model = lgbm.LGBMRegressor(**lgb_params, device = 'gpu', gpu_platform_id = 0, gpu_device_id = 0)\n\nmeta_estimator1 = LinearRegression()\nmeta_estimator2 = BayesianRidge()\nmeta_estimator3 = ElasticNet()\nfinal_estimator = Ridge()\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n    tbn_base_model = TabNetRegressor(verbose=0)\n    print('*'*15, f'Fold {fold+1}', '*'*15, '\\n')\n    print('Stage 1 Training\/Predictions', '\\n')\n    X_train, X_valid = X.iloc[train_idx].to_numpy(), X.iloc[test_idx].to_numpy()\n    y_train, y_valid = y.iloc[train_idx].to_numpy(), y.iloc[test_idx].to_numpy()\n    \n    tbn_base_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], patience=3, )\n    print(f'Stage 1 Model 1: TabNet Regressor | Fold {fold+1} Loss: {mean_squared_error(y_valid, tbn_base_model.predict(X_valid), squared=False)}')\n    \n    y_train, y_valid = y_train.squeeze(), y_valid.squeeze()\n    \n    lgb_base_model.fit(X_train, y_train)\n    print(f'Stage 1 Model 2: LightGBM Regressor | Fold {fold+1} Loss: {mean_squared_error(y_valid, lgb_base_model.predict(X_valid), squared=False)}')\n    \n    ctb_base_model.fit(X_train, y_train, verbose=False)\n    print(f'Stage 1 Model 3: CatBoost Regressor | Fold {fold+1} Loss: {mean_squared_error(y_valid, ctb_base_model.predict(X_valid), squared=False)}')\n    \n    xgb_base_model.fit(X_train, y_train, verbose=False)\n    print(f'Stage 1 Model 4: XGBoost Regressor | Fold {fold+1} Loss: {mean_squared_error(y_valid, xgb_base_model.predict(X_valid), squared=False)}')\n    \n    print('\\n', '*'*15, 'Stage 2 Training\/Predictions', '*'*15, '\\n')\n    \n    blend_train = np.c_[lgb_base_model.predict(X_valid), ctb_base_model.predict(X_valid), xgb_base_model.predict(X_valid), tbn_base_model.predict(X_valid)]\n    blend_test = np.c_[lgb_base_model.predict(X_test), ctb_base_model.predict(X_test), xgb_base_model.predict(X_test), tbn_base_model.predict(X_test)]\n    meta_estimator1.fit(blend_train, y_valid)\n    meta_valid1 = meta_estimator1.predict(blend_train)\n    meta_test1 = meta_estimator1.predict(blend_test)\n    \n    print(f'Meta Estimator 1: Linear Regression | Score: {mean_squared_error(y_valid, meta_valid1, squared=False)}')\n    \n    meta_estimator2.fit(blend_train, y_valid)\n    meta_valid2 = meta_estimator2.predict(blend_train)\n    meta_test2 = meta_estimator2.predict(blend_test)\n\n    print(f'Meta Estimator 2: Bayesian Ridge Regressor | Score: {mean_squared_error(y_valid, meta_valid2, squared=False)}')\n    \n    meta_estimator3.fit(blend_train, y_valid)\n    meta_valid3 = meta_estimator3.predict(blend_train)\n    meta_test3 = meta_estimator3.predict(blend_test)\n    \n    print(f'Meta Estimator 3: ElasticNet Regressor | Score: {mean_squared_error(y_valid, meta_valid3, squared=False)}')\n    \n    print('\\n', '*'*15, 'Stage 3 Training\/Predictions', '*'*15, '\\n')\n    \n    blend_train = np.c_[meta_valid1, meta_valid2, meta_valid3]\n    blend_test = np.c_[meta_test1, meta_test2, meta_test3]\n    final_estimator.fit(blend_train, y_valid)\n    print(f'Final Meta Estimator: Ridge Regressor | Score: {mean_squared_error(y_valid, final_estimator.predict(blend_train), squared=False)}')\n    final_test_preds.append(final_estimator.predict(blend_test))\n    print('\\n')","d5141778":"sample_submission['loss'] = sum(final_test_preds)\/5\nsample_submission.to_csv('submission.csv', index=False)","910510a9":"# It has public score of 7.87552","79287c63":"# Final results","9d96c829":"# XGBoost Hyperparameter Tuning with Optuna","d89f9443":"# Tuned parameter results","ad76c72a":"# LightGBM Hyperparameter Tuning with Optuna","6e32b3e6":"* Thank you for this valuable contribution for Stacking Ensemble Method! Please upvote notebook below!\n\n[Two Stage Stacking Ensemble](https:\/\/www.kaggle.com\/ryanbarretto\/two-stage-stacking-ensemble)"}}