{"cell_type":{"d295a75a":"code","f4f893ec":"code","84af1ede":"code","c3218142":"code","b8219062":"code","ce3fa4c0":"code","b1cac553":"code","c2c788a5":"code","5c852866":"code","68226054":"code","91033a69":"code","8c100514":"code","dbe30b2f":"code","ba97fd46":"code","049ef7f9":"code","f62710bd":"code","92ac76f7":"code","71bd62dd":"code","8ad7ff28":"code","4807fd6d":"code","94d6668d":"code","a4c6cbeb":"code","960f503f":"code","07ce1ef2":"code","32d6931b":"code","644aac59":"code","f9db3476":"code","e824bf7a":"code","bba6fd07":"code","b66246db":"code","f1d69691":"code","e7a412ad":"code","04be4e8d":"code","65e552c4":"code","68aace5a":"code","17a54b8d":"code","1094ff40":"code","900172b7":"code","433131f5":"code","de8bb4c1":"code","89dccb02":"code","a2d46b7b":"code","89a8bb17":"code","e50c7c9c":"code","e8ff2829":"code","6c220646":"code","3eda0bdf":"code","effc1283":"code","f49af046":"markdown","81b87cb6":"markdown","872d6fbe":"markdown","828be7de":"markdown","18ab4043":"markdown","74fc8c5c":"markdown","fcccf93d":"markdown","032906ea":"markdown","09cc44c3":"markdown","a78f2c9a":"markdown","5e0b778f":"markdown","5040a210":"markdown","183cd80b":"markdown","77fca344":"markdown","d8b3abc6":"markdown","315287c5":"markdown","da04c71e":"markdown","1f6c05eb":"markdown","bf267b56":"markdown","3d3ef09f":"markdown","52203288":"markdown","dd270f39":"markdown","a8dbae22":"markdown","4b356027":"markdown","436e52e7":"markdown","fe04c7b8":"markdown"},"source":{"d295a75a":"# DataFrame\nimport pandas as pd \n\n# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# nltk\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\n\n\n#tensorflow\nimport tensorflow.compat.v2 as tf\nimport tensorflow_datasets as tfds\n\n# Utility\nimport pandas as pd\nimport numpy as np \nimport warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport string\nimport pickle","f4f893ec":"# Construct a tf.data.Dataset\ndata = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='latin', names = ['polarity','id','date','query','user','text'])","84af1ede":"data = data.sample(frac=1)\ndata = data[:200000]","c3218142":"print(\"Dataset shape:\", data.shape)","b8219062":"data.head(10)","ce3fa4c0":"data['polarity'].unique()","b1cac553":"# Replacing the value 4 -->1 for ease of understanding.\ndata['polarity'] = data['polarity'].replace(4,1)\ndata.head()","c2c788a5":"data.describe()","5c852866":"# check the number of positive vs. negative tagged sentences\npositives = data['polarity'][data.polarity == 1 ]\nnegatives = data['polarity'][data.polarity == 0 ]\n\nprint('Total length of the data is:         {}'.format(data.shape[0]))\nprint('No. of positve tagged sentences is:  {}'.format(len(positives)))\nprint('No. of negative tagged sentences is: {}'.format(len(negatives)))","68226054":"# get a word count per of text\ndef word_count(words):\n    return len(words.split())","91033a69":"# plot word count distribution for both positive and negative \n\ndata['word count'] = data['text'].apply(word_count)\np = data['word count'][data.polarity == 1]\nn = data['word count'][data.polarity == 0]\nplt.figure(figsize=(12,6))\nplt.xlim(0,45)\nplt.xlabel('Word count')\nplt.ylabel('Frequency')\ng = plt.hist([p, n], color=['g','r'], alpha=0.5, label=['positive','negative'])\nplt.legend(loc='upper right')","8c100514":"# get common words in training dataset\nfrom collections import Counter\nall_words = []\nfor line in list(data['text']):\n    words = line.split()\n    for word in words:\n      if(len(word)>2):\n        all_words.append(word.lower())\n    \n    \nCounter(all_words).most_common(20)","dbe30b2f":"%matplotlib inline\nsns.countplot(data['polarity'])","ba97fd46":"# Removing the unnecessary columns.\ndata.drop(['date','query','user','word count'], axis=1, inplace=True)","049ef7f9":"data.drop('id', axis=1, inplace=True)","f62710bd":"data.head(10)","92ac76f7":"#Checking if any null values present\n(data.isnull().sum() \/ len(data))*100","71bd62dd":"#convrting pandas object to a string type\ndata['text'] = data['text'].astype('str')","8ad7ff28":"nltk.download('stopwords')\nstopword = set(stopwords.words('english'))\nprint(stopword)","4807fd6d":"nltk.download('punkt')\nnltk.download('wordnet')","94d6668d":"urlPattern = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\nuserPattern = '@[^\\s]+'\ndef process_tweets(tweet):\n  # Lower Casing\n    tweet = tweet.lower()\n    tweet=tweet[1:]\n    # Removing all URls \n    tweet = re.sub(urlPattern,'',tweet)\n    # Removing all @username.\n    tweet = re.sub(userPattern,'', tweet) \n    #Remove punctuations\n    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n    #tokenizing words\n    tokens = word_tokenize(tweet)\n    #Removing Stop Words\n    final_tokens = [w for w in tokens if w not in stopword]\n    #reducing a word to its word stem \n    wordLemm = WordNetLemmatizer()\n    finalwords=[]\n    for w in final_tokens:\n      if len(w)>1:\n        word = wordLemm.lemmatize(w)\n        finalwords.append(word)\n    return ' '.join(finalwords)","a4c6cbeb":"data['processed_tweets'] = data['text'].apply(lambda x: process_tweets(x))\nprint('Text Preprocessing complete.')","960f503f":"data.head(10)","07ce1ef2":"plt.figure(figsize = (15,15)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.polarity == 0].processed_tweets))\nplt.imshow(wc , interpolation = 'bilinear')","32d6931b":"plt.figure(figsize = (15,15)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.polarity == 1].processed_tweets))\nplt.imshow(wc , interpolation = 'bilinear')","644aac59":"X = data['processed_tweets'].values\ny = data['polarity'].values\n","f9db3476":"print(X.shape)\nprint(y.shape)","e824bf7a":"#Convert a collection of raw documents to a matrix of TF-IDF features.\nvector = TfidfVectorizer(sublinear_tf=True)\nX = vector.fit_transform(X)\nprint(f'Vector fitted.')\nprint('No. of feature_words: ', len(vector.get_feature_names()))","bba6fd07":"print(X.shape)\nprint(y.shape)","b66246db":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=101)","f1d69691":"print(\"X_train\", X_train.shape)\nprint(\"y_train\", y_train.shape)\nprint()\nprint(\"X_test\", X_test.shape)\nprint(\"y_test\", y_test.shape)","e7a412ad":"def model_Evaluate(model):\n    #accuracy of model on training data\n    acc_train=model.score(X_train, y_train)\n    #accuracy of model on test data\n    acc_test=model.score(X_test, y_test)\n    \n    print('Accuracy of model on training data : {}'.format(acc_train*100))\n    print('Accuracy of model on testing data : {} \\n'.format(acc_test*100))\n\n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","04be4e8d":"lg = LogisticRegression()\nhistory=lg.fit(X_train, y_train)\nmodel_Evaluate(lg)","65e552c4":"svm = LinearSVC()\nsvm.fit(X_train, y_train)\nmodel_Evaluate(svm)","68aace5a":"rf = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', max_depth=50)\nrf.fit(X_train, y_train)\nmodel_Evaluate(rf)","17a54b8d":"nb = BernoulliNB()\nnb.fit(X_train, y_train)\nmodel_Evaluate(nb)","1094ff40":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nimport re","900172b7":"import keras\nkeras.__version__","433131f5":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import regularizers\n\nmax_words = 5000\nmax_len = 200\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data.processed_tweets)\nsequences = tokenizer.texts_to_sequences(data.processed_tweets)\ntweets = pad_sequences(sequences, maxlen=max_len)\nprint(tweets)","de8bb4c1":"X_train, X_test, y_train, y_test = train_test_split(tweets, data.polarity.values, test_size=0.2, random_state=101)","89dccb02":"from keras.models import Sequential\nfrom keras import layers\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nmodel2 = Sequential()\nmodel2.add(layers.Embedding(max_words, 128))\nmodel2.add(layers.LSTM(64,dropout=0.5))\nmodel2.add(layers.Dense(16, activation='relu'))\nmodel2.add(layers.Dense(8, activation='relu'))\nmodel2.add(layers.Dense(1,activation='sigmoid'))\nmodel2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\ncheckpoint2 = ModelCheckpoint(\"rnn_model.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model2.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint2])\n","a2d46b7b":"sequence = tokenizer.texts_to_sequences(['this data science article is the worst ever'])\ntest = pad_sequences(sequence, maxlen=max_len)\npred = model2.predict(test)\nif pred > 0.5:\n  print('Positive')\nelse:\n  print('Negative')\n# print(pred)","89a8bb17":"model = keras.models.load_model('rnn_model.hdf5')\nsequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\ntest = pad_sequences(sequence, maxlen=max_len)\npred = model.predict(test)\nif pred > 0.5:\n  print('Positive')\nelse:\n  print('Negative')","e50c7c9c":"sequence = tokenizer.texts_to_sequences(['I had a bad day at work.'])\ntest = pad_sequences(sequence, maxlen=max_len)\npred = model.predict(test)\nif pred > 0.5:\n  print('Positive')\nelse:\n  print('Negative')","e8ff2829":"import pickle\n\nfile = open('vectoriser.pickle','wb')\npickle.dump(vector, file)\nfile.close()\n\nfile = open('logisticRegression.pickle','wb')\npickle.dump(lg, file)\nfile.close()\n\nfile = open('SVM.pickle','wb')\npickle.dump(svm, file)\nfile.close()\n\nfile = open('RandomForest.pickle','wb')\npickle.dump(rf, file)\nfile.close()\n\nfile = open('NaivesBayes.pickle','wb')\npickle.dump(nb, file)\nfile.close()","6c220646":"def load_models():\n    # Load the vectoriser.\n    file = open('vectoriser.pickle', 'rb')\n    vectoriser = pickle.load(file)\n    file.close()\n    # Load the LR Model.\n    file = open('logisticRegression.pickle', 'rb')\n    lg = pickle.load(file)\n    file.close()\n    return vectoriser, lg","3eda0bdf":"def predict(vectoriser, model, text):\n    # Predict the sentiment\n    processes_text=[process_tweets(sen) for sen in text]\n    textdata = vectoriser.transform(processes_text)\n    sentiment = model.predict(textdata)\n    \n    # Make a list of text with sentiment.\n    data = []\n    for text, pred in zip(text, sentiment):\n        data.append((text,pred))\n    # Convert the list into a Pandas DataFrame.\n    df = pd.DataFrame(data, columns = ['text','sentiment'])\n    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n    return df","effc1283":"if __name__==\"__main__\":\n    # Loading the models.\n    vectoriser, lg = load_models()\n    \n    # Text to classify should be in a list.\n    text = [\"I love machine learning\",\n            \"Work is too hectic.\",\n            \"Mr.Sharama, I feel so good\"]\n    \n    df = predict(vectoriser, lg, text)\n    print(df.head())\n","f49af046":"### Word-Cloud for Positive tweets.","81b87cb6":"### Naive Bayes <a id=\"11\"><\/a>","872d6fbe":"# Model Saving, Loading and Prediction <a id=\"13\"><\/a>","828be7de":"# Sentiment Analysis Using Various ML Classifiers ans well as using Recurrent Neural Network","18ab4043":"### Random Forest <a id=\"10\"><\/a>","74fc8c5c":"### Linear SVM <a id=\"9\"><\/a>","fcccf93d":"Dataset details\ntarget: the polarity of the tweet (0 = negative, 4 = positive)\n\n* date : the date of the tweet (Sat May 16 23:58:44 PDT 2009)\n* polarity : the polarity of the tweet (0 = negative 4 = positive)\n* user : the user that tweeted (TerraScene)\n* text : the text of the tweet (i'm 10x cooler than all of you)","032906ea":"> The Preprocessing steps taken are:\n\n* Lower Casing: Each text is converted to lowercase.\n* Removing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"\".\n\n* Removing Usernames: Replace @Usernames with word \"\". (eg: \"@XYZ\" to \"\")\n* Removing Short Words: Words with length less than 2 are removed.\n* Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n* Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: \u201cwolves\u201d to \u201cwolf\u201d)","09cc44c3":"### Logistic Regression <a id=\"8\"><\/a>","a78f2c9a":"### Model evaluating function","5e0b778f":"# Data Processing <a id=\"4\"><\/a>","5040a210":"# Index\n\n* <a href=\"#1\" style=\"color: blue;\">Importing Libraries<\/a>\n* <a href=\"#2\" style=\"color: blue;\">Loading Dataset<\/a>\n* <a href=\"#3\" style=\"color: blue;\">Data Visualization<\/a>\n* <a href=\"#4\" style=\"color: blue;\">Data Preprocessing<\/a>\n* <a href=\"#5\" style=\"color: blue;\">Analyzing the Data<\/a>\n* <a href=\"#6\" style=\"color: blue;\">Vectorization and Splitting the data<\/a>\n* <a href=\"#7\" style=\"color: blue;\">Model Building<\/a>\n* <a href=\"#8\" style=\"color: blue;\">Logistic Regression<\/a>\n* <a href=\"#9\" style=\"color: blue;\">Linear SVM<\/a>\n* <a href=\"#10\" style=\"color: blue;\">Random Forest<\/a>\n* <a href=\"#11\" style=\"color: blue;\">Naive Bayes<\/a>\n* <a href=\"#12\" style=\"color: blue;\">RNN<\/a>\n* <a href=\"#13\" style=\"color: blue;\">Model Saving, Loading and Prediction<\/a>","183cd80b":"### In this notebook you will see text precessing on twitter data set and after that I have performed different Machine Learning Algorithms on the data such as **Logistic Regression, RandomForestClassifier, SVC, Naive Bayes** to classifiy positive and negative tweets. After that I have also built a RNN network which is the best fit for such textual sentiment analysis, since it's a Sequential Dataset which is requirement for RNN network.\n\n#### Let's Dive into it.","77fca344":"#### What is RNN?\nRecurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data\n\n#### Embedding Layer\nEmbedding layer is one of the available layers in Keras. This is mainly used in Natural Language Processing related applications such as language modeling, but it can also be used with other tasks that involve neural networks. While dealing with NLP problems, we can use pre-trained word embeddings such as GloVe. Alternatively we can also train our own embeddings using Keras embedding layer.\n\n#### LSTM layer\nLong Short Term Memory networks, usually called \u201cLSTMs\u201d , were introduced by Hochreiter and Schmiduber. These have widely been used for speech recognition, language modeling, sentiment analysis and text prediction. Before going deep into LSTM, we should first understand the need of LSTM which can be explained by the drawback of practical use of Recurrent Neural Network (RNN). So, lets start with RNN.","d8b3abc6":"# Importing libraries <a id=\"1\"><\/a>","315287c5":"## Split train and test","da04c71e":"### Predict using saved model","1f6c05eb":"# Analyzing the data <a id=\"5\"><\/a>\n\n> #### Now we're going to analyse the preprocessed data to get an understanding of it. We'll plot Word Clouds for Positive and Negative tweets from our dataset and see which words occur the most.","bf267b56":"### Word-Cloud for Negative tweets.","3d3ef09f":"# Loading Dataset <a id=\"2\"><\/a>","52203288":"The Preprocessed Data is divided into 2 sets of data:\n\n* Training Data: The dataset upon which the model would be trained on. Contains 80% data.\n* Test Data: The dataset upon which the model would be tested against. Contains 20% data.\n\n\n","dd270f39":"## Convert text to word frequency vectors\n### TF-IDF\n This is an acronym than stands for **Term Frequency \u2013 Inverse Document** Frequency which are the components of the resulting scores assigned to each word.\n\n* Term Frequency: This summarizes how often a given word appears within a document.\n* Inverse Document Frequency: This downscales words that appear a lot across documents.","a8dbae22":"# Model Building <a id=\"7\"><\/a>","4b356027":"# RNN <a id=\"12\"><\/a>","436e52e7":"## Vectorization and Splitting the data <a id=\"6\"><\/a>\nStoring input variable-processes_tweets to X and output variable-polarity to y","fe04c7b8":"# Data Visualization <a id=\"3\"><\/a>"}}