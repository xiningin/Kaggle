{"cell_type":{"f36e4579":"code","0fd3fb72":"code","3451ec9e":"code","2a7673ab":"code","8791ed09":"code","a189b9a9":"code","c405e5d6":"code","7ed9b980":"code","69e4aa7a":"code","7ce23c5f":"code","04d3ac16":"code","c70ad710":"code","3fa25ece":"code","d41caf1a":"code","b739e910":"code","73d5b4c7":"code","43df9a09":"code","3bb28846":"markdown","4058120e":"markdown","8f2de7bd":"markdown","beaeec52":"markdown","516532df":"markdown","88aa7022":"markdown","c0f332c4":"markdown","ad355a48":"markdown","2cb47cc4":"markdown"},"source":{"f36e4579":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0fd3fb72":"df = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")","3451ec9e":"numeric_features = df.dtypes[df.dtypes != object].index\nNnum = (df.dtypes != object).sum()\nprint(\"Nnum : \", Nnum)\nprint(numeric_features)\ncategorical_features = df.dtypes[df.dtypes == object].index\nprint(\"Ncat : \", len(categorical_features))\nprint(categorical_features)","2a7673ab":"# Removing outliers \nX_clean = df.copy()\nprint(\"Original X_clean length: \", len(X_clean))\nX_clean = X_clean.drop(X_clean[X_clean['LotFrontage'] > 200].index)\nX_clean = X_clean.drop(X_clean[X_clean['LotArea'] > 100000].index)\nX_clean = X_clean.drop(X_clean[X_clean['BsmtFinSF1'] > 4000].index)\nX_clean = X_clean.drop(X_clean[X_clean['TotalBsmtSF'] > 5000].index)\nX_clean = X_clean.drop(X_clean[X_clean['GrLivArea'] > 4000].index)\nprint(\"Length of X_clean after dropping outliers: \", len(X_clean))","8791ed09":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_clean.drop(columns=\"SalePrice\"), X_clean[\"SalePrice\"],\n                                                    test_size=0.1)","a189b9a9":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler","c405e5d6":"class dropCols(BaseEstimator, TransformerMixin):\n    def __init__(self, thresh_diversity=1):\n        self.columns = None\n        self.too_many_missing = None\n        self.low_diversity = None\n        self.thresh_diversity = thresh_diversity\n        \n    def fit(self, X, y=None):\n        cols_null = X.isnull().mean()\n        self.too_many_missing = list(cols_null[cols_null>0.1].index)\n    \n        def vc(x):\n            return x.value_counts().iloc[0] \/ x.value_counts().sum()\n        div_score = X.apply(vc, axis=0)\n        self.low_diversity = list(div_score[div_score > self.thresh_diversity].index)\n        return self\n    \n    def transform(self, X, y=None):\n        # obtained with previous random forest run (not used finally)\n        not_relevant = ['MoSold','YrSold', 'LotShape','LandSlope','LandContour', 'Id']\n        drop_cols = self.too_many_missing + not_relevant + self.low_diversity\n        ret = X.drop(columns=drop_cols)\n        self.columns = list(ret.columns)      \n        return ret\n    \n    def get_feature_names(self):\n        return self.columns","7ed9b980":"class housingOrdinalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        #Mapping ordinal variables\n        rating_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual',\n                      'GarageCond', 'FireplaceQu']\n        rating_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\n        fintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\n        expose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n        fence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}\n\n        X_all = X.copy()\n        #Labeling ordinal variables\n        X_all[rating_col] = X_all[rating_col].applymap(lambda x: rating_map.get(x))\n\n        fin_col = ['BsmtFinType1','BsmtFinType2']\n        X_all[fin_col] = X_all[fin_col].applymap(lambda x: fintype_map.get(x)) \n\n        X_all['BsmtExposure'] = X_all['BsmtExposure'].map(expose_map)\n        X_all['Fence'] = X_all['Fence'].map(fence_map)\n        return X_all","69e4aa7a":"# check data encoding so far\nxenc = Pipeline([('ord', housingOrdinalEncoder()),\n                 ('drop', dropCols(thresh_diversity=0.7))]).fit_transform(X_train)\nprint(\"Numeric features :\", (xenc.dtypes != object).sum(),\"\\n\", xenc.dtypes[xenc.dtypes != object].index)\nprint(\"Categorical features :\", (xenc.dtypes == object).sum(),\"\\n\", xenc.dtypes[xenc.dtypes == object].index)","7ce23c5f":"# encode the remaining categorical features with onehot\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    (\"oe\", OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor_rf = Pipeline([\n        ('ord', housingOrdinalEncoder()),\n        ('drop', dropCols()),\n        (\"ct\", ColumnTransformer(transformers=[\n                    ('num', numeric_transformer, make_column_selector(dtype_exclude=object)),\n                    ('cat', categorical_transformer, make_column_selector(dtype_include=object))]))        \n    ])\n\n\nprint(\"final training shape :\", preprocessor_rf.fit_transform(X_train).shape)","04d3ac16":"from sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error \n\nfrom sklearn.ensemble import RandomForestRegressor\nimport scipy\n\nmodel = Pipeline([\n        (\"pp\", preprocessor_rf),\n        (\"rf\", RandomForestRegressor())\n])","c70ad710":"# hyperparams\nparams = {\n    \"rf__bootstrap\" : [True],\n    \"rf__max_features\": [0.1, 0.3, 0.5, 0.7, 1],\n    \"rf__max_samples\": [None],\n    #\"rf__min_samples_split\": [2,10,20,30],\n    #\"rf__min_samples_leaf\": [1,2,4,8],\n    'pp__drop__thresh_diversity': [0.8,  0.9, 0.95, 1],\n    \"rf__n_estimators\": [300]}\n\nsch = GridSearchCV(\n    estimator=model, param_grid=params, cv=7, \n    verbose=0, scoring=\"neg_mean_absolute_error\", n_jobs=-1, \n    #n_iter = 100,\n    return_train_score=True)\n\nsch.fit(X_train, y_train);\ntuned_model = sch.best_estimator_\ny_train_pred = tuned_model.predict(X_train)","3fa25ece":"from sklearn import set_config\nset_config(display='diagram')\ntuned_model","d41caf1a":"print(\"MAE train \", mean_absolute_error(y_train_pred, y_train))\nprint(\"MAE best test \", -sch.best_score_)\nprint(sch.best_params_)","b739e910":"df_results = pd.concat([\n    pd.DataFrame(sch.cv_results_['params']),\n    pd.Series(-sch.cv_results_['mean_train_score']).rename(\"mean_train_score\"),\n    pd.Series(-sch.cv_results_['mean_test_score']).rename(\"mean_test_score\")\n],axis=1)\ndf_results[\"overfitting\"] = df_results[\"mean_test_score\"] \/ df_results[\"mean_train_score\"]\ndf_results[\"rf__max_samples\"].fillna(1, inplace=True)\ndf_results[\"rf__max_features\"].fillna(1, inplace=True)\n#display(df_results.sort_values(by=\"overfitting\"))\n\ndfr = df_results[df_results[\"rf__max_samples\"]==1].pivot(index=\"rf__max_features\",columns=\"pp__drop__thresh_diversity\")\n\nf, (ax,ax1) = plt.subplots(1,2,figsize=(10,5), sharey=True)\ndfr[(\"mean_train_score\",)].plot(ax=ax)\ndfr[(\"mean_test_score\",)].plot(ax=ax1)\nax.set_title(\"train\")\nax1.set_title(\"test\")\nax.set_ylabel(\"MAE\")\n#ax.set_ylim(bottom=15000, top=20000)\nplt.show()","73d5b4c7":"from sklearn.model_selection import cross_val_score\n\nwith np.printoptions(precision=1):\n    cvs= -cross_val_score(tuned_model, X_train, y_train, scoring=\"neg_mean_absolute_error\", cv=10)\n    print(\"Cross validation MAE: {},\\nMean cv MAE: {:.1f}\".format(cvs, np.mean(cvs)))\n\n    print(\"MAE:\\n train: {:.1f}\\n test: {:.1f}\".format(\n        mean_absolute_error(tuned_model.predict(X_train), y_train),\n        mean_absolute_error(tuned_model.predict(X_test), y_test)\n    ))","43df9a09":"pd.Series(tuned_model.predict(df_test), index=df_test[\"Id\"].astype(int)).rename(\"SalePrice\").to_csv(\"submission.csv\", sep=\",\")","3bb28846":"* [Data exploration](#dataexploration)  \n* [Drop outliers](#dropoutliers)  \n* [Data encoding](#dataencoding) \n    * [Drop columns](#dropcolumns) \n    * [Ordinal features](#ordinalfeatures) \n* [Model](#model)\n* [Scores](#scores)\n* [Submit prediction](#submitprediction)","4058120e":"### Drop outliers <a class=\"anchor\" id=\"dropoutliers\"><\/a>\ncopied from : https:\/\/www.kaggle.com\/iamcharan\/systematic-approach-for-a-top-500-submission","8f2de7bd":"### Model <a class=\"anchor\" id=\"model\"><\/a>","beaeec52":"### Data exploration <a class=\"anchor\" id=\"dataexploration\"><\/a>","516532df":"### Submit prediction <a class=\"anchor\" id=\"submitprediction\"><\/a>\n","88aa7022":"### Scores <a class=\"anchor\" id=\"Scores\"><\/a>","c0f332c4":"### Drop columns <a class=\"anchor\" id=\"dropcolumns\"><\/a>","ad355a48":"#### Ordinal features <a class=\"anchor\" id=\"ordinalfeatures\"><\/a>\ncopied from : https:\/\/www.kaggle.com\/iamcharan\/systematic-approach-for-a-top-500-submission","2cb47cc4":"### Data encoding <a class=\"anchor\" id=\"dataencoding\"><\/a>\n"}}