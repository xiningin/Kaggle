{"cell_type":{"aec2998b":"code","83635d59":"code","6b1e80bf":"code","71eba390":"code","ec741482":"code","4d2be3e2":"code","cdcf6b95":"code","12a8bcbe":"code","7986b27e":"code","4a382657":"code","a7acc0d4":"code","9f71982d":"code","4c17728b":"code","d32d63af":"code","fd1de5ac":"code","49ead38e":"code","60a504e1":"code","bfb52d7e":"code","ec1a2c34":"code","123d1ee8":"code","591926eb":"code","10d6f1dc":"code","a59caab6":"code","05020bee":"code","645fdfd0":"code","e3d62e0d":"code","a8957166":"code","0bd7b9fd":"code","8f346b62":"code","49396b2f":"code","2400ea2e":"code","859586e7":"code","4f18d51c":"code","537e3f6e":"code","09f8dcec":"code","846e0bcd":"code","0f4afe9a":"code","c74fc359":"code","15085aaa":"code","680ebb80":"code","4913213f":"code","e8180678":"code","6181e7a1":"code","498d9c67":"code","722b34fc":"code","1a927ac7":"code","d4746f39":"code","5b10d02d":"code","b7a2a4ea":"code","fdd47c78":"code","990a44cd":"code","cdeccfc3":"code","51d7c095":"code","5fc9ca6c":"code","373fe0b4":"code","ae69adb8":"code","5af67f85":"code","dbe03b27":"code","442bf985":"code","8c5ca310":"code","2c0c571f":"code","fef94133":"code","dfc4336d":"code","85dcbfe0":"code","a721bb4e":"code","5e17bd85":"code","cb654168":"code","a8942f79":"code","dbf4a1f5":"code","4f314827":"code","c7daf2b1":"code","ae8ece8a":"code","a33fc377":"code","b8c7ed2b":"code","f085c3b4":"code","a7c3dc21":"code","1d5fef72":"code","411629e7":"code","ef7cbfe9":"code","74273ac8":"code","233a1170":"code","72f2af17":"code","919bb3d1":"code","38f5781e":"code","4fccc78b":"code","d18a16e4":"code","0743afb2":"code","3f4fd9ea":"code","7425e539":"code","10901638":"code","59615a7c":"code","1087f87e":"code","0ba76093":"code","8d4e3dec":"code","78cdbaf5":"code","eb599f3e":"code","d49815d8":"markdown","a9b0543f":"markdown","22c85b95":"markdown","722bf50d":"markdown","9440a397":"markdown","0a741926":"markdown","f9500176":"markdown","60f217eb":"markdown","391289ac":"markdown","f91f7902":"markdown","96a20769":"markdown","9ac21fe6":"markdown","b3da2b29":"markdown","aec9e6c2":"markdown","a48af47b":"markdown","cf0b1b1b":"markdown","e88ce759":"markdown","253b136a":"markdown","842d8cb8":"markdown","caa002be":"markdown","f2ba2143":"markdown","6047fc8e":"markdown","578d591a":"markdown","b3b39ce4":"markdown","b814553c":"markdown","95f26648":"markdown","e63f62e3":"markdown","b010f4e0":"markdown","bcf8d413":"markdown","921cc11a":"markdown","47414840":"markdown","efae8105":"markdown","b656dd88":"markdown","b0873b50":"markdown","2ae7929b":"markdown","53e29861":"markdown","916b459d":"markdown","fe8a8eb3":"markdown","f7c2f5ee":"markdown","bfa32bef":"markdown","25807cb5":"markdown"},"source":{"aec2998b":"# Importing all pakages that necessary.\n\n# loading pandas and numpy for data cleaning and exploratory anaysis.\n\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\n\n# Defining the dataset's new name in this project.\nsalary_file_path = '..\/input\/2017-18_NBA_salary.csv'\nsalary_data = pd.read_csv(salary_file_path)","83635d59":"# make copy to avoid changing original data when imputing.\n\ncopy_data = salary_data.copy()","6b1e80bf":"# Firstly, we should have a look whether the data is completed or not.\n# Because the missing value will have an adverse impact on the building of regression model.\n\nnull_values_col = copy_data.isnull().sum()\nnull_values_col = null_values_col[null_values_col != 0].sort_values(ascending = False).reset_index()\nnull_values_col.columns = [\"variable\", \"number of missing\"]\nnull_values_col.head()","71eba390":"# using median value of each column to fill the N\/A values, because it will not be influened by outliers.\n\ndef fillWithMedian(data):\n    return data.fillna(data.median(), inplace=True)\n\nfillWithMedian(copy_data)","ec741482":"copy_data.isnull().any()","4d2be3e2":"# read the data\n\ncopy_data.head(10)","cdcf6b95":"copy_data.columns","12a8bcbe":"copy_data.describe()","7986b27e":"# Matplotlib package for visualisation.\n\nimport matplotlib.pyplot as plt\n\ncopy_data.Salary.hist(bins=20, alpha=0.5)\nplt.title(\"NBA Players' Salaries in 2017-18 Season Histogram\")\nplt.xlabel(\"Salary($)\")\nplt.ylabel(\"Frequency\")","4a382657":"# Extracting two columns: Salary and NBA_Country.\n# sample variance - Why does Bessel's correction use N-1?\n# https:\/\/en.wikipedia.org\/wiki\/Bessel%27s_correction#Proof_of_correctness_-_Alternate_3\n\n# covariance\n# https:\/\/blog.csdn.net\/guomutian911\/article\/details\/43317019\n\ninf_data = pd.read_csv(salary_file_path,usecols=[2,1])\ninf_data.head(10)","a7acc0d4":"usa_data = inf_data[inf_data['NBA_Country'] == 'USA']\nnon_usa_data = inf_data[inf_data['NBA_Country'] != 'USA']\nu_data = usa_data[['Salary']] \nn_data = non_usa_data[['Salary']]","9f71982d":"u_mean = u_data.mean()\nn_mean = n_data.mean()\nu_stdev = u_data.std()\nn_stdev = n_data.std()\nu_count = u_data.count()\nn_count = n_data.count()\ndegree_of_freedom = u_count + n_count - 2","4c17728b":"standard_error = (u_stdev**2\/u_count + n_stdev**2\/n_count)**0.5\nt_statistics = (u_mean - n_mean)\/standard_error\nprint('the t-statistics is: {}'.format(t_statistics))\nprint('the degree of freedom is: {}'.format(degree_of_freedom))","d32d63af":"# Bayes Theorem\n\nusa_list = u_data['Salary'].values.tolist()\nnon_usa_list = n_data['Salary'].values.tolist()\n\nusa_count = u_data['Salary'].count()\nnon_usa_count = n_data['Salary'].count()\n\nusa_10m_count = 0\nnon_usa_10m_count = 0\n\nfor i in usa_list:\n    if i > 10000000:\n        usa_10m_count += 1\n\nfor i in non_usa_list:\n    if i > 10000000:\n        non_usa_10m_count += 1\n\n# P(a|b) = (P(b|a))*P(a)\/P(b)\n\nprobability = (usa_10m_count\/(usa_10m_count+non_usa_10m_count)) * ((usa_10m_count + non_usa_10m_count)\/\n               (usa_count + non_usa_count)) \/ (usa_count\/(usa_count + non_usa_count))\nprint(probability)","fd1de5ac":"# Bernoulli distribution\n\nu_country = usa_data[['NBA_Country']] \nn_country = non_usa_data[['NBA_Country']]\nbernoulli_count = [int(u_country.count()), int(n_country.count())]\nu_probability = bernoulli_count[0]\/(bernoulli_count[0] + bernoulli_count[1])\nn_probability = bernoulli_count[1]\/(bernoulli_count[0] + bernoulli_count[1])\n\n# Define the dataset\nprobability = [u_probability, n_probability]\nbars = ('0', '1')\ny_pos = np.arange(len(bars))\n \n# Create bars\nplt.bar(y_pos, probability)\n \n# Create names on the x-axis\nplt.xticks(y_pos, bars)\n \n# Show graphic\nplt.title(\"The Bernoulli Distribution about probability of USA Players and Oversea Players \\n\")\nplt.xlabel(\"0-USA, 1-Oversea\")\nplt.ylabel(\"Probability\")\nplt.show()","49ead38e":"# probability mass function, S-total = 1\n# When variables are continous, it becomes Probability Denstiy Function.\n\nimport seaborn as sns\n\nplt.figure(figsize=(10,6))\nsns.kdeplot(copy_data.Age, shade=True)\nplt.xlim((15,45))\nplt.title(\"PMF of Players' Age\")\nplt.ylabel(\"Density\")\nplt.xlabel('Age')\nplt.grid(True)\nplt.show()","60a504e1":"# Selecting Features\n\nprint(copy_data.columns)","bfb52d7e":"# Creating Features\n# buiding binary categories in order to make classifications prediction\n# normal-0, star-1\n\nconditions = [\n    (copy_data['Salary'] < 10000000)]\nchoices = [0]\ncopy_data['Binary'] = np.select(conditions, choices, default=1)\ncopy_data.head(10)\n\n# copy_data.drop(['USA\/NOT'], axis=1, inplace=True)\n\n# Then we build nomial categories\n# 0 - edge players\n# 1 - normal players\n# 2 - all stars\n# 3 - superstars\n\nconditions = [\n        (copy_data['Salary'] < 5000000),\n        (copy_data['Salary'] <= 10000000),\n        (copy_data['Salary'] <= 20000000)]\nchoices = [0, 1, 2]\ncopy_data['Nominal'] = np.select(conditions, choices, default=3)\ncopy_data.head(10)","ec1a2c34":"color_wheel = {0: \"#0392cf\", \n               1: \"#7bc043\"}\ncolors = copy_data['Binary'].map(lambda x: color_wheel.get(x))\nprint(copy_data.Binary.value_counts())\np=copy_data.Binary.value_counts().plot(kind=\"bar\")","123d1ee8":"colors = copy_data['Nominal'].map(lambda x: color_wheel.get(x))\nprint(copy_data.Nominal.value_counts())\np=copy_data.Nominal.value_counts().plot(kind=\"pie\")","591926eb":"df = DataFrame(copy_data,columns=['Salary', 'NBA_Country', 'NBA_DraftNumber', 'Age', 'Tm', 'G',\n       'MP', 'PER', 'TS', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS\/48', 'OBPM',\n       'DBPM', 'BPM', 'VORP'])\n\n'''\npandas.DataFrame.corr\nmethod : {\u2018pearson\u2019, \u2018kendall\u2019, \u2018spearman\u2019}\npearson : standard correlation coefficient\nkendall : Kendall Tau correlation coefficient\nspearman : Spearman rank correlation\n\nmin_periods : int, optional\nMinimum number of observations required per pair of columns to have a valid result. Currently only available for pearson and spearman correlation\n'''\n\ncorrmat = df.corr(method='pearson', min_periods=1)\nr_square = corrmat ** 2\n\n## Top 8 correlated variables\nk = 9 #number of variables for heatmap\ncols = r_square.nlargest(k, 'Salary')['Salary'].index\ncm = df[cols].corr()\ncm_square = cm ** 2\nf, ax = plt.subplots(figsize=(10, 10))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm_square, cbar=False, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","10d6f1dc":"cm_square.columns","a59caab6":"# Using scatter plots to detect the correlation value\n\nvariables = ['Salary', 'WS', 'VORP', 'OWS', 'MP', 'DWS', 'NBA_DraftNumber', 'Age', 'BPM']\n\nsns.set()\nsns.pairplot(df[variables], size = 2.5)\nplt.show()","05020bee":"# https:\/\/etav.github.io\/python\/vif_factor_python.html\n# https:\/\/onlinecourses.science.psu.edu\/stat501\/node\/347\/\n\nx = df[['WS', 'VORP', 'OWS', 'MP', 'DWS', 'NBA_DraftNumber', 'Age', 'BPM']]\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\nvif[\"features\"] = x.columns\n\nvif.round(1)","645fdfd0":"x = df[['NBA_DraftNumber', 'Age', 'WS', 'BPM']]\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\nvif[\"features\"] = x.columns\n\nvif.round(1)","e3d62e0d":"from sklearn.metrics import mean_squared_error\n\n# RMSE for testing data\n\ndef rmse_model(model, x_test, y_test):\n    predictions = model.predict(x_test)\n    rmse = np.sqrt(mean_squared_error(predictions, y_test))\n    return(rmse)","a8957166":"from sklearn.model_selection import train_test_split\n\nx = df[['NBA_DraftNumber', 'Age', 'WS', 'BPM']]\ny = df[['Salary']]\n\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)","0bd7b9fd":"# Cross Validation\n\n# Spliting dataset into three parts, for training, validation, and testing respectively.\n\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.25, random_state=1)","8f346b62":"print(\"the number of data for training:\")\nprint(y_train.count())\nprint(\"the number of data for validation:\")\nprint(y_val.count())\nprint(\"the number of data for testing:\")\nprint(y_test.count())","49396b2f":"from sklearn.linear_model import LinearRegression\n\nlinear_regression = LinearRegression()\nlinear_regression.fit(x_train, y_train)\n\nprint(rmse_model(linear_regression, x_test, y_test))\nprint(linear_regression.coef_)\nprint(linear_regression.intercept_)","2400ea2e":"# Bias-Variance Trade-off\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ntrain_rmses = []\nval_rmses = []\ndegrees = range(1,8)\n\nfor i in degrees:\n    \n    poly = PolynomialFeatures(degree=i, include_bias=False)\n    x_train_poly = poly.fit_transform(x_train)\n\n    poly_reg = LinearRegression()\n    poly_reg.fit(x_train_poly, y_train)\n    \n    # training RMSE\n    y_train_pred = poly_reg.predict(x_train_poly)\n    train_poly_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    \n    train_rmses.append(train_poly_rmse)\n    \n    # validation RMSE\n    x_val_poly = poly.fit_transform(x_val)\n    y_val_pred = poly_reg.predict(x_val_poly)\n    \n    val_poly_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n    val_rmses.append(val_poly_rmse)\n\n    print('degree = %s, training RMSE = %.2f, validation RMSE = %.2f' % (i, train_poly_rmse, val_poly_rmse))\n        \nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(degrees, train_rmses,label= 'training set')\nax.plot(degrees, val_rmses,label= 'validation set')\nax.set_yscale('log')\nax.set_xlabel('Degree')\nax.set_ylabel('RMSE')\nax.set_title('Bias\/Variance Trade-off')  \nplt.legend()\nplt.show()","859586e7":"# RMSE for testing data\n\nsecond_poly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = second_poly.fit_transform(x_train)\n\nsecond_reg = LinearRegression()\nsecond_reg.fit(x_train_poly, y_train)\n\nx_test_second_poly = second_poly.fit_transform(x_test)\ny_test_pred = second_reg.predict(x_test_second_poly)\n\nprint(rmse_model(second_reg, x_test_second_poly, y_test))\nprint(second_reg.coef_)\nprint(second_reg.intercept_)","4f18d51c":"# At first, we calculate the RMSE before regularization.\n\npoly = PolynomialFeatures(degree=4, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\n\npoly_reg = LinearRegression()\npoly_reg.fit(x_train_poly, y_train)\n\nx_test_poly = poly.fit_transform(x_test)\ny_test_pred = poly_reg.predict(x_test_poly)\n\nprint(rmse_model(poly_reg, x_test_poly, y_test))","537e3f6e":"# Ridge\n\n# https:\/\/blog.csdn.net\/hzw19920329\/article\/details\/77200475\n# https:\/\/www.kaggle.com\/sflender\/comparing-lin-regression-ridge-lasso\n# https:\/\/www.kaggle.com\/junyingzhang2018\/ridge-regression-score-0-119\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\n\nrmse=[]\nalpha=[1, 2, 5, 10, 20, 30, 40, 50, 75, 100]\n\nfor a in alpha:\n    ridge = make_pipeline(PolynomialFeatures(4), Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    predict=ridge.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)","09f8dcec":"# Adjust alpha based on previous result\n\nalpha=np.arange(20, 60, 2)\nrmse=[]\n\nfor a in alpha:\n    #ridge=Ridge(alpha=a, copy_X=True, fit_intercept=True)\n    #ridge.fit(x_train, y_train)\n    ridge = make_pipeline(PolynomialFeatures(4), Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    predict=ridge.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)","846e0bcd":"# Adjust alpha based on previous result\n\nalpha=np.arange(20, 30, 0.2)\nrmse=[]\n\nfor a in alpha:\n    #ridge=Ridge(alpha=a, copy_X=True, fit_intercept=True)\n    #ridge.fit(x_train, y_train)\n    ridge = make_pipeline(PolynomialFeatures(4), Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    predict=ridge.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)","0f4afe9a":"# Use alpha=40.4 to predict the test data\n\nridge = make_pipeline(PolynomialFeatures(4), Ridge(alpha=24.6))\nridge_model = ridge.fit(x_train, y_train)\n\npredictions = ridge_model.predict(x_test)\nprint(\"Ridge RMSE is: \" + str(rmse_model(ridge_model, x_test, y_test)))","c74fc359":"# Lasso\n\n# https:\/\/www.kaggle.com\/sflender\/comparing-lin-regression-ridge-lasso\n\nfrom sklearn.linear_model import Lasso\n\nrmse=[]\nalpha=[0.0001, 0.001, 0.01, 0.1, 1]\n\nfor a in alpha:\n    lasso=make_pipeline(PolynomialFeatures(4), Lasso(alpha=a))\n    lasso.fit(x_train, y_train)\n    predict=lasso.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)","15085aaa":"lasso = make_pipeline(PolynomialFeatures(4), Lasso(alpha=0.0001))\nlasso_model = lasso.fit(x_train, y_train)\npredictions = lasso_model.predict(x_test)\nprint(\"RMSE in Testing : \" + str(rmse_model(lasso_model, x_test, y_test)))","680ebb80":"# ElasticNet\n\n# https:\/\/www.kaggle.com\/jack89roberts\/top-7-using-elasticnet-with-interactions\n\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\n\nrmse=[]\nalpha=[0.000001, 0.00001, 0.0001, 0.001,0.01,0.1]\n\nfor a in alpha:\n    elasticnet=make_pipeline(PolynomialFeatures(4), ElasticNet(alpha=a))\n    elasticnet.fit(x_train, y_train)\n    predict=elasticnet.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\n                             \nprint(rmse)\nplt.scatter(alpha, rmse)","4913213f":"elasticnet=make_pipeline(PolynomialFeatures(4), ElasticNet(alpha=0.000001))\nelasticnet_model = elasticnet.fit(x_train, y_train)\npredictions = elasticnet_model.predict(x_test)\nprint(\"RMSE in Testing : \" + str(rmse_model(elasticnet_model, x_test, y_test)))","e8180678":"# Comparison\n\nprint(\"For testing dataset\\n\")\n\nprint(\"Linear RMSE is: \" + str(rmse_model(linear_regression, x_test, y_test)))\nprint(\"2nd Polynomial RMSE is: \" + str(rmse_model(second_reg, x_test_second_poly, y_test)))\n\nprint(\"\\nFor 4th order polynomial (RMSE = 128255850.32699986 before regualarization)\")\nprint(\"Ridge RMSE is: \" + str(rmse_model(ridge_model, x_test, y_test)))\nprint(\"Lasso RMSE is: \" + str(rmse_model(lasso_model, x_test, y_test)))\nprint(\"ElasticNet RMSE is: \" + str(rmse_model(elasticnet_model, x_test, y_test)))","6181e7a1":"data = np.array([['','Parameter','RMSE'],\n                ['1st-order Poly',1,4508432.2],\n                ['2nd-order Poly',2,4136581.4],\n                ['4nd-order Poly',4,128255850.3],\n                ['4nd-order Lasso','<0.0001',10261503.6],\n                ['4nd-order Ridge',24.6,32093519.5],\n                ['4nd-order ElasticNet','<0.0001',10261504.7]])\n                \nregression_comparison = pd.DataFrame(data=data[1:,1:],\n                                      index=data[1:,0],\n                                    columns=data[0,1:])\nregression_comparison","498d9c67":"#http:\/\/www.science.smith.edu\/~jcrouser\/SDS293\/labs\/lab10-py.html\n\nmy_ridge = Ridge(alpha = 24.6, normalize = True)\nmy_ridge.fit(x_train, y_train) \n#pd.Series(my_ridge.coef_,index = ['NBA_DraftNumber', 'Age', 'WS', 'BPM'])\nmy_ridge.coef_","722b34fc":"my_lasso = Lasso(alpha = 0.0001, normalize = True)\nmy_lasso.fit(x_train, y_train) \nmy_lasso.coef_","1a927ac7":"my_elasticnet = ElasticNet(alpha = 0.0001, normalize = True)\nmy_elasticnet.fit(x_train, y_train) \nmy_elasticnet.coef_","d4746f39":"#https:\/\/www.zhihu.com\/question\/38121173\n\ndata = np.array([['','NBA_DraftNumber','Age', 'WS', 'BPM'],\n                ['Ridge',-5063.8823843 , 23745.62461498, 69180.1194312 , 14071.36231572],\n                ['Lasso',-71799.85643407,  478944.20547638, 1505350.67035793, -28969.97816668],\n                ['ElasticNet',-71487.33915305,  473914.28999172, 1478049.57849672, -22200.94106611]])\n                \nregularization_comparison = pd.DataFrame(data=data[1:,1:],\n                                      index=data[1:,0],\n                                    columns=data[0,1:])\nregularization_comparison","5b10d02d":"# https:\/\/www.kaggle.com\/pablovargas\/naive-bayes-svm-spam-filtering\n# for binary target variables\n\nfrom sklearn import metrics\n\ndef confusion_matrix(model, x_test, y_test):\n    model_confusion_test = metrics.confusion_matrix(y_test, model.predict(x_test))\n    matrix = pd.DataFrame(data = model_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n                 index = ['Actual 0', 'Actual 1'])\n    return matrix","b7a2a4ea":"print(copy_data.columns)","fdd47c78":"df = DataFrame(copy_data,columns=['Binary', 'Nominal', 'Age','NBA_DraftNumber','MP', 'PER', 'TS', '3PAr', \n        'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%','STL%', 'BLK%', \n        'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS\/48', 'OBPM', \n        'DBPM', 'BPM', 'VORP'])","990a44cd":"x = df[['NBA_DraftNumber', 'Age', 'WS', 'BPM']]\ny = df[['Binary']]\n\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.25, random_state=1)","cdeccfc3":"# Model Tuning\n\n# 5-fold cross validation\n\nfrom sklearn.model_selection import KFold, cross_val_score\n\ndef rmse_cv(model):\n    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(x_model.values)\n    predictions = model.predict(x_test)\n    rmse= np.sqrt(-cross_val_score(model, x_model.values, y_model, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","51d7c095":"# How to find K?\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold\n\ntrain_scores = []\nvalidation_scores = []\n\nx_model_values = x_model.values\ny_model_values = y_model.values\n\n# 5-fold cross validation\n\nkfold = KFold(5, shuffle=True, random_state=42)\n\nfor i in range(1,20):\n    knn = KNeighborsClassifier(i)\n    \n    tr_scores = []\n    va_scores = []\n    \n    for a, b in kfold.split(x_model_values):\n\n        x_train_fold, y_train_fold = x_model_values[a], y_model_values[a]\n        x_val_fold, y_val_fold = x_model_values[b], y_model_values[b]\n        \n        knn.fit(x_train_fold, y_train_fold.ravel())\n        \n        va_scores.append(knn.score(x_val_fold, y_val_fold))\n        tr_scores.append(knn.score(x_train_fold, y_train_fold))\n        \n    validation_scores.append(np.mean(va_scores))\n    train_scores.append(np.mean(tr_scores))","5fc9ca6c":"plt.title('k-NN Varying number of neighbours')\nplt.plot(range(1,20),validation_scores,label=\"Validation\")\nplt.plot(range(1,20),train_scores,label=\"Train\")\nplt.legend()\nplt.xticks(range(1,20))\nplt.show()","373fe0b4":"# Learning Curve\n\n# How KNN algorithm performs in both small-size data and big-size data \n\n# choose an acceptable color\n# https:\/\/www.spycolor.com\/ff8040\n\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(KNeighborsClassifier(5), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        n_jobs=-1, \n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve \\n k-fold=5, number of neighbours=5\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","ae69adb8":"# curse of dimensionality\n\n# one or two features are simple, but it cannot recognize and divide our categories. more features means\n# more evidence in different dimensions, but it could cause overfitting.\n\nX = df[[ 'Age', 'NBA_DraftNumber','MP', 'PER', '3PAr',\n        'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%','STL%', 'BLK%', \n        'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS\/48', 'OBPM', \n        'DBPM', 'BPM', 'VORP']]\nY = df[['Binary']]\n\nX_model, X_test, Y_model, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\nX_train, X_val, Y_train, Y_val = train_test_split(X_model, Y_model, test_size=0.25, random_state=1)","5af67f85":"# [:, :2]extract columns\n\n# convert[[1],[2],[3],...] to [1,2,3,4,0]\n# x_train_values_list = np.array(x_train_values).tolist() \n\n'''\ny_train_value = [j for i in y_train_values for j in i] - delete sublists to just one list\n\ndimensionality = []\nfor i in range(10):\n\na = [item[:, :2] for item in list(x_train_values)]\nprint(a)\n'''\n\nd_train = []\nd_val = []\n\nX_train_values = X_train.values\nY_train_values = Y_train.values\nX_val_values = X_val.values\nY_val_values = Y_val.values\n\nfor i in range(1,23):\n    \n    X_train_value = X_train_values[:,:i].tolist() #convert dataframe\n    X_val_value = X_val_values[:,:i].tolist()\n    \n    knn = KNeighborsClassifier(5)\n    Knn = knn.fit(X_train_value, Y_train_values.ravel())\n\n    d_train.append(Knn.score(X_train_value, Y_train_values))\n    d_val.append(Knn.score(X_val_value, Y_val_values))\n\nplt.title('k-NN Curse of Dimensionality')\nplt.plot(range(1,23),d_val,label=\"Validation\")\nplt.plot(range(1,23),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,23))\nplt.show()","dbe03b27":"# The best result is captured at k = 5 hence it is used for the final model.\n\n#Setup a knn classifier with k neighbors\n\nkfold = KFold(5, shuffle=True, random_state=42)\nknn = KNeighborsClassifier(5)\n\nfor m,n in kfold.split(x_model_values):\n        \n        x_train_fold, y_train_fold = x_model_values[m], y_model_values[m]\n        \n        Knn = knn.fit(x_train_fold, y_train_fold.ravel())\n\nprint('When k=5, the testing score(accuracy) is: ')\nprint(Knn.score(x_test,y_test))","442bf985":"confusion_matrix(Knn, x_test, y_test)","8c5ca310":"from sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\n\nclassifier = SVC(gamma = 'auto')\nsvm_model = OneVsRestClassifier(classifier, n_jobs=1).fit(x_train, y_train)\n\nprint(svm_model.score(x_train,y_train))\nprint(svm_model.score(x_val,y_val))","2c0c571f":"#Tuning\n\n# https:\/\/medium.com\/all-things-ai\/in-depth-parameter-tuning-for-svc-758215394769\n# https:\/\/medium.com\/machine-learning-101\/chapter-2-svm-support-vector-machine-theory-f0812effc72\n\n#from sklearn.model_selection import GridSearchCV\n\n#parameters = {\"estimator__gamma\":[0.0001, 0.001, 0.01, 0.3, 0.5, 0.1, 2, 5, 10, 100]}\n#grid_search = GridSearchCV(svm_model, param_grid=parameters)\n#grid_search.fit(x_train, y_train)\n#print(grid_search.best_score_)\n#print(grid_search.best_params_)\n\naccuracy=[]\ngamma=[0.0001, 0.001, 0.005, 0.01, 0.1, 0.2, 0.3, 0.5, 0.1]\n\nfor a in gamma:\n    classifier = SVC(C=1, \n        kernel='rbf', \n        degree=2, \n        gamma=a, \n        coef0=1,\n        shrinking=True, \n        tol=0.5,\n        probability=False, \n        cache_size=200, \n        class_weight=None,\n        verbose=False, \n        max_iter=-1, \n        decision_function_shape=None, \n        random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1)\n    svm_model.fit(x_train, y_train)\n    predict=svm_model.predict(x_val)\n    accuracy.append(svm_model.score(x_val,y_val))\nprint(accuracy)\nplt.scatter(gamma, accuracy)","fef94133":"gamma=np.arange(0.0001, 0.005, 0.0003) \naccuracy=[]\n\nfor a in gamma:\n    classifier = SVC(C=1, \n        kernel='rbf', \n        degree=2, \n        gamma=a, \n        coef0=1,\n        shrinking=True, \n        tol=0.5,\n        probability=False, \n        cache_size=200, \n        class_weight=None,\n        verbose=False, \n        max_iter=-1, \n        decision_function_shape=None, \n        random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1)\n    svm_model.fit(x_train, y_train)\n    predict=svm_model.predict(x_val)\n    accuracy.append(svm_model.score(x_val,y_val))\nprint(accuracy)\nplt.scatter(gamma, accuracy)\nplt.scatter(gamma, accuracy)\nplt.title(\"Finding Gamma\")\nplt.xlabel(\"Gamma\")\nplt.ylabel(\"Accuracy Score\")\nplt.show()","dfc4336d":"accuracy=[]\nC=np.arange(1,10,1) \n\nfor a in C:\n    classifier = SVC(C=a, \n        kernel='rbf', \n        degree=2, \n        gamma=0.0013, \n        coef0=1,\n        shrinking=True, \n        tol=0.5,\n        probability=False, \n        cache_size=200, \n        class_weight=None,\n        verbose=False, \n        max_iter=-1, \n        decision_function_shape=None, \n        random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1)\n    svm_model.fit(x_train, y_train)\n    predict=svm_model.predict(x_val)\n    accuracy.append(svm_model.score(x_val,y_val))\nprint(accuracy)\nplt.scatter(C, accuracy)\nplt.title(\"Finding C\")\nplt.xlabel(\"C\")\nplt.ylabel(\"Accuracy Score\")\nplt.show()","85dcbfe0":"classifier = SVC(C=1, # Regularization parameter\n        kernel='rbf', # kernel type, rbf working fine here\n        degree=2, # default value\n        gamma=0.0013, # kernel coefficient\n        coef0=1, # change to 1 from default value of 0.0\n        shrinking=True, # using shrinking heuristics\n        tol=0.5, # stopping criterion tolerance \n        probability=False, # no need to enable probability estimates\n        cache_size=200, # 200 MB cache size\n        class_weight=None, # all classes are treated equally \n        verbose=False, # print the logs \n        max_iter=-1, # no limit, let it run\n        decision_function_shape=None, # will use one vs rest explicitly \n        random_state=None)\nsvm_model = OneVsRestClassifier(classifier, n_jobs=1).fit(x_train, y_train)\n\nprint(svm_model.score(x_train,y_train))\nprint(svm_model.score(x_val,y_val))","a721bb4e":"print(svm_model.score(x_test,y_test))","5e17bd85":"# Confusion Matrix\n\nconfusion_matrix(svm_model, x_test, y_test)","cb654168":"# Learning Curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(classifier, n_jobs=1), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve \\n C=1, gamma=0.0013\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","a8942f79":"# curse of dimensionality\n\n# one or two features are simple, but it cannot recognize and divide our categories. more features means\n# more evidence in different dimensions, but it could cause overfitting.\n\n# https:\/\/thispointer.com\/select-rows-columns-by-name-or-index-in-dataframe-using-loc-iloc-python-pandas\/\n\nd_train = []\nd_val = []\n\nfor i in range(1,23):\n    \n    X_train_index = X_train.iloc[: , 0:i]\n    X_val_index = X_val.iloc[: , 0:i]\n    \n    classifier = SVC(C=1, # Regularization parameter\n                    kernel='rbf', # kernel type, rbf working fine here\n                    degree=2, # default value\n                    gamma=0.0001, # kernel coefficient\n                    coef0=1, # change to 1 from default value of 0.0\n                    shrinking=True, # using shrinking heuristics\n                    tol=0.5, # stopping criterion tolerance \n                    probability=False, # no need to enable probability estimates\n                    cache_size=200, # 200 MB cache size\n                    class_weight=None, # all classes are treated equally \n                    verbose=False, # print the logs \n                    max_iter=-1, # no limit, let it run\n                    decision_function_shape=None, # will use one vs rest explicitly \n                    random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1).fit(X_train_index, Y_train)\n\n    d_train.append(svm_model.score(X_train_index, Y_train))\n    d_val.append(svm_model.score(X_val_index, Y_val))","dbf4a1f5":"plt.title('SVM Curse of Dimensionality')\nplt.plot(range(1,23),d_val,label=\"Validation\")\nplt.plot(range(1,23),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,23))\nplt.show()","4f314827":"# NB assumes that the features themselves are not correlated to each other. Therefore, if the collinearity of our features are low, the model will perform better.\n\nx = df[['NBA_DraftNumber', 'Age', 'WS', 'BPM']]\ny = df[['Nominal']]\n\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.25, random_state=1)","c7daf2b1":"# https:\/\/medium.com\/machine-learning-101\/chapter-1-supervised-learning-and-naive-bayes-classification-part-1-theory-8b9e361897d5\n# https:\/\/blog.csdn.net\/li8zi8fa\/article\/details\/76176597\n# GaussianNB,MultinomialNB, BernoulliNB\n\n# http:\/\/www.cnblogs.com\/lesliexong\/p\/6907642.html\n\n# Gaussian is for continous features\n\n#\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u79bb\u6563\u6837\u672c\u4e5f\u5c31\u662f\u57fa\u4e8e\u9891\u7387\u7684\u4f30\u8ba1\u4e2d\uff0c\u5982\u679c\u67d0\u4e2a\u7279\u5f81fn\u672a\u5728\u8bad\u7ec3\u96c6\u7684\u7c7b\u522bci\u4e2d\u51fa\u73b0\u8fc7\uff0c\u90a3\u4e48P(fn|ci)\u9879\u4e3a0\u4f1a\u5bfc\u81f4\u6574\u4e2a\u4f30\u8ba1\u4e3a0\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u7684\u7279\u5f81\u4fe1\u606f\u3002\n#\u8fd9\u6837\u7684\u4f30\u8ba1\u663e\u7136\u662f\u4e0d\u51c6\u786e\u7684\uff0c\u6240\u4ee5\u901a\u5e38\u9700\u8981\u5bf9\u4e8e\u6837\u672c\u8fdb\u884c\u6837\u672c\u4fee\u6b63\u4fdd\u8bc1\u4e0d\u4f1a\u67090\u6982\u7387\u51fa\u73b0\u3002\n#\u6bd4\u5982\u91c7\u7528laplace\u6821\u51c6\uff0c\u5bf9\u6ca1\u7c7b\u522b\u4e0b\u6240\u6709\u5212\u5206\u7684\u8ba1\u6570\u52a01\uff0c\u8fd9\u6837\u5982\u679c\u8bad\u7ec3\u6837\u672c\u96c6\u6570\u91cf\u5145\u5206\u5927\u65f6\uff0c\u5e76\u4e0d\u4f1a\u5bf9\u7ed3\u679c\u4ea7\u751f\u5f71\u54cd\u3002listone\u4fee\u6b63\u5219\u662f\u52a0\u4e00\u4e2a0-1\u4e4b\u95f4\u7684\u6570\u3002\n\n# \u548c\u591a\u5143\u6734\u7d20\u8d1d\u53f6\u65af\u4e2d\u901a\u8fc7\u7279\u5f81\u51fa\u73b0\u9891\u7387\u6765\u8ba1\u7b97P(fn|ci)\u4e0d\u540c\uff0c\u4f2f\u52aa\u5229\u6a21\u578b\u53ea\u8003\u8651\u51fa\u73b0\u4e0d\u51fa\u73b0\u7684\u4e8c\u503c\u95ee\u9898\u3002\n\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\nnb_model = gaussian.fit(x_train, y_train.values.ravel())\n\nprint(nb_model.score(x_train,y_train))","ae8ece8a":"train_score = []\nval_score = []\na = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1]\n\n#for i in np.arange(1,20):\nfor i in a:\n    gaussian = GaussianNB(priors=None, var_smoothing=i)\n    nb_model = gaussian.fit(x_train, y_train.values.ravel())\n    train_score.append(nb_model.score(x_train, y_train))\n    val_score.append(nb_model.score(x_val, y_val))","a33fc377":"plt.plot(a,train_score)\nplt.plot(a,val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Naive Bayes Tuning')\nplt.xlabel('Variance Smoothing')\nplt.ylabel('Accuracy')","b8c7ed2b":"gaussian = GaussianNB(priors=None, var_smoothing=0.1)\nnb_model = gaussian.fit(x_train, y_train.values.ravel())\n\nprint(nb_model.score(x_test, y_test))","f085c3b4":"# https:\/\/www.kaggle.com\/diegosch\/classifier-evaluation-using-confusion-matrix\n\n# 0 - edge players\n# 1 - normal players\n# 2 - all stars\n# 3 - superstars\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\ny_predict = nb_model.predict(x_test)\ncm = confusion_matrix(y_test, y_predict) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['edge players','normal players', 'all stars', 'superstars'], \n                     columns = ['edge players','normal players', 'all stars', 'superstars'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Naive Bayes \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","a7c3dc21":"# Learning Curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(GaussianNB(priors=None, var_smoothing=0.1)), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"NB Learning Curve \\n \")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","1d5fef72":"d_train = []\nd_val = []\n\nfor i in range(1,24):\n    \n    X_train_index = X_train.iloc[: , 0:i]\n    X_val_index = X_val.iloc[: , 0:i]\n    \n    classifier = GaussianNB(priors=None, var_smoothing=0.1)\n    nb_model = gaussian.fit(X_train_index, Y_train.values.ravel())\n\n    d_train.append(nb_model.score(X_train_index, Y_train))\n    d_val.append(nb_model.score(X_val_index, Y_val))","411629e7":"plt.title('Naive Bayes Curse of Dimensionality')\nplt.plot(range(1,24),d_val,label=\"Validation\")\nplt.plot(range(1,24),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,24))\nplt.show()","ef7cbfe9":"# https:\/\/blog.csdn.net\/app_12062011\/article\/details\/52136117\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree_model = DecisionTreeClassifier()\ndecision_tree_model.fit(x_train, y_train)\nprint(decision_tree_model.score(x_train,y_train))\nprint(decision_tree_model.score(x_val,y_val))","74273ac8":"plt.bar(range(len(x_train.columns.values)), decision_tree_model.feature_importances_)\nplt.xticks(range(len(x_train.columns.values)),x_train.columns.values, rotation= 45)\nplt.title('Feature Importance')","233a1170":"#Model Tuning\n\n#https:\/\/www.kaggle.com\/drgilermo\/stephen-curry-s-decision-tree\n\ntrain_score = []\nval_score = []\nfor depth in np.arange(1,20):\n    decision_tree = tree.DecisionTreeClassifier(max_depth = depth,min_samples_leaf = 5)\n    decision_tree.fit(x_train, y_train)\n    train_score.append(decision_tree.score(x_train, y_train))\n    val_score.append(decision_tree.score(x_val, y_val))\n\nplt.plot(np.arange(1,20),train_score)\nplt.plot(np.arange(1,20),val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Decision Tree Tuning')\nplt.xlabel('Depth')\nplt.ylabel('Accuracy')","72f2af17":"train_score = []\nval_score = []\nfor leaf in np.arange(1,30):\n    decision_tree = tree.DecisionTreeClassifier(max_depth = 5, min_samples_leaf = leaf)\n    decision_tree.fit(x_train, y_train)\n    train_score.append(decision_tree.score(x_train, y_train))\n    val_score.append(decision_tree.score(x_val, y_val))\n\nplt.plot(np.arange(1,30),train_score)\nplt.plot(np.arange(1,30),val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Decision Tree Tuning')\nplt.xlabel('Minimum Samples Leaf')\nplt.ylabel('Accuracy')","919bb3d1":"my_decision_tree_model = DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 6)\nmy_decision_tree_model.fit(x_train, y_train)\nprint(my_decision_tree_model.score(x_train,y_train))\nprint(my_decision_tree_model.score(x_val,y_val))","38f5781e":"print(my_decision_tree_model.score(x_test,y_test))","4fccc78b":"y_predict = my_decision_tree_model.predict(x_test)\ncm = confusion_matrix(y_test, y_predict) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['edge players','normal players', 'all stars', 'superstars'], \n                     columns = ['edge players','normal players', 'all stars', 'superstars'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Decision Tree \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","d18a16e4":"# Learning Curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 6)), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Decision Tree Learning Curve \\n \")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","0743afb2":"# Curse of Dimensionality\n\nd_train = []\nd_val = []\n\nfor i in range(1,24):\n    \n    X_train_index = X_train.iloc[: , 0:i]\n    X_val_index = X_val.iloc[: , 0:i]\n    \n    classifier = DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 6)\n    dt_model = classifier.fit(X_train_index, Y_train.values.ravel())\n\n    d_train.append(dt_model.score(X_train_index, Y_train))\n    d_val.append(dt_model.score(X_val_index, Y_val))","3f4fd9ea":"plt.title('Decision Tree Curse of Dimensionality')\nplt.plot(range(1,24),d_val,label=\"Validation\")\nplt.plot(range(1,24),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,24))\nplt.show()","7425e539":"# logistic regression (LR)\n\n#https:\/\/www.kaggle.com\/joparga3\/2-tuning-parameters-for-logistic-regression\n\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nlogistic_model = LogisticRegression()\nlogistic_model.fit(x_train, y_train.values.ravel())\n\nprint(logistic_model.score(x_train,y_train))\nprint(logistic_model.score(x_val,y_val))","10901638":"#https:\/\/www.kaggle.com\/joparga3\/2-tuning-parameters-for-logistic-regression\n\ntrain_score = []\nval_score=[]\n\nfor i in np.arange(1,80):\n    \n    logistic_model = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n    \n    logistic_model.fit(x_train,y_train.values.ravel()) \n    \n    train_score.append(logistic_model.score(x_train, y_train))\n    val_score.append(logistic_model.score(x_val,y_val))\n\n    \nplt.plot(np.arange(1,80),train_score)\nplt.plot(np.arange(1,80),val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Logistic Regression Tuning')\nplt.xlabel('C')\nplt.ylabel('Accuracy')","59615a7c":"my_logistic_regression_model = LogisticRegression(penalty = 'l2', C = 50, random_state = 0)\nmy_logistic_regression_model.fit(x_train, y_train)\nprint(my_logistic_regression_model.score(x_train,y_train))\nprint(my_logistic_regression_model.score(x_val,y_val))","1087f87e":"print(my_logistic_regression_model.score(x_test,y_test))","0ba76093":"y_predict = my_logistic_regression_model.predict(x_test)\ncm = confusion_matrix(y_test, y_predict) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['edge players','normal players', 'all stars', 'superstars'], \n                     columns = ['edge players','normal players', 'all stars', 'superstars'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Logistic Regression \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","8d4e3dec":"# Learning Curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(LogisticRegression(penalty = 'l2', C = 50, random_state = 0)), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Logistic Regression Learning Curve \\n \")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","78cdbaf5":"# Curse of Dimensionality\n\nd_train = []\nd_val = []\n\nfor i in range(1,24):\n    \n    X_train_index = X_train.iloc[: , 0:i]\n    X_val_index = X_val.iloc[: , 0:i]\n    \n    classifier = LogisticRegression(penalty = 'l2', C = 50, random_state = 0)\n    lr_model = classifier.fit(X_train_index, Y_train.values.ravel())\n\n    d_train.append(lr_model.score(X_train_index, Y_train))\n    d_val.append(lr_model.score(X_val_index, Y_val))","eb599f3e":"plt.title('Logistic Regression Curse of Dimensionality')\nplt.plot(range(1,24),d_val,label=\"Validation\")\nplt.plot(range(1,24),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,24))\nplt.show()","d49815d8":"## 2.1 Data Cleaning\n\nMissing values and outliers would make the modelling process difficult. Therefore, we use imputation method to fill in the missing values. In this case, we choose median number of the whole column to fill in the missing variables because a median value will not be influened by outliers.","a9b0543f":"# Contents\n\n1. Introduction & Questions\n\n2. Methods & Results\n    \n    2.1 Data Cleaning\n    \n    2.2 Exploratory Analysis\n        2.2.1 Descriptive Statistics\n        2.2.2 Statistical Inference\n        2.2.3 Probability Distributions\n    \n    2.3 Modelling\n        2.3.1 Feature Engineering\n            (1) Creating Features\n            (2) Pearson's R-Square Correlation\n            (3) Multicollinearity Analysis\n        2.3.2 Regression\n            (1) Measure of Goodness: RMSE\n            (2) Selection of Model: Multivariate, Cross Validation, and Bias\/Variance Trade-off\n            (3) Regularization: Ridge, Lasso, and ElasticNet\n        2.3.3 Classification\n            (1) Measure of Goodness: Accuracy Score and Confusion Matrix\n            (2) Selection of Model: KNN, SVM, Na\u00efve Bayes, Decision Tree, Logistic Regression\n            (3) Comparison: Model Tuning, Learning Curve and Curse of Dimensionality\n\n3. Recommendations & Discussions","22c85b95":"#### (2) Selection of Model: KNN, SVM, Na\u00efve Bayes, Decision Tree, Logistic Regression\n\nFive Models are selected to fit our dataset. Here we use KNN, SVM to fit the 'Binary' target variables, and use others to fit the 'Nominal' variables.","722bf50d":"***ElasticNet***","9440a397":"# 1. Introduction & Questions\n\nIn 2017-18 season, the salary cap and the luxury tax of NBA reached 99 million and 119 million dolars respectively (Di, 2018). This means that the managers and coaches of all 30 teams have to focus on finding those who are ability to put their teams to another level within their budgets. Therefore, a model which can predict players\u2019 salaries according to their performance data is necessary in the league.\n\nThis report chooses a dataset named NBA 2017-18 season players\u2019 salaries. It is oriented from Kaggle, the largest online machine learning and data science community (Narayanan, Shi & Rubinstein, 2011). We believe that the dataset is suitable because it focuses on not only on-ball stats, such as scores, rebounds, and assists, but also off-ball stats, which can capture those who have a big impact on the game without ball on the hand. Also, the identity information is captured, such as age, draftnumber, and so on.\n\nIn this report, the 3 key questions are: \n* What are the most important 4 features that influence the salary? \n* What are the most suitable regression and classification models to predict players\u2019 salaries? And how do the models work? \n* What recommendations can be made?\n\nIn order to answer the questions, data cleaning, exploratory analysis, and data modelling methods will be applied. Firstly, the dataset should be cleaned in order to make it easier to build model. Secondly, in order to get familiar with the dataset, this report will use descriptive statistics, statistical inference and concepts of probability distributions to make a description of the potential variables that we need to choose for moddelling. Thirdly, we will use regression and classification methods to select, build, and evaluate our models.","0a741926":"#### (3) Comparison: Model Tuning, Learning Curve and Curse of Dimensionality\n\nThe process of Model Tuning is similar to \"Bias-Variance Trade-off\", which is to find the balance that provide not only the high score of the training set, but also good ability to predict the testing set. After applying our models, it is essential to use the model tuning techniques to find the best parameters that fit our dataset.\n\nLearning Curve is the process to see the change of correctness within the quantity of data set. It is also a good way to overcome the adverse impact of overfitting, for a big-size dataset can make a complex model performs well than a small-size dataset.\n\nCurse of Dimensionality is one of the main reasons of overfitting. If the number of dimensions is similar to the amount of data, each or several samples may form one class, which may make the traing model performs well in the training set, but losing its ability to predict the testing set at the same time.","f9500176":"### 2.2.1 Descriptive Statistics\n\nIn order to understand our data, descriptive statistics can be used to get a well understand of the dataset. The methods here include mean, median, mode, standard deviation, interquatile range. The first three methods are to read the central tendency of data, and the rest are to describe the dispersion of datasets.","60f217eb":"#### (2) Pearson's R-Square Correlation\n\nIn order to choose features that are correlated to our target variables, the Pearson's R-Square Correlation can be applied to choose top 8 features that are most correlated to the salaries.","391289ac":"### 2.2.2 Statistical Inference\n\nIn order to select appropriate features for prediction, an independent t-test can be applied to calculate whether a feature is significant enough. Here we choose \"Country\" as our feature.\n* H0: There is no significant difference of salaries between players from USA and not.\n* H1: There is significant difference of salaries between players from USA and not.\n* (when alpha level = .05, two-tailed test)\n\nIf |t-statistics| < |t-critical|, we retain the null hypothesis. Conversely, we reject the null. In this case, the |t-statistics| = 0.7033 which is smaller than |t-critical|, then we prefer to think that there is no significant difference of salaries between players from USA and overseas.","f91f7902":"Apart from that, the histogram of salaries can make it clear to see the distribution of NBA salaries. From the positive-skewed histogram, we can read that more than 33% of NBA players' salaries are less than 3 million dollars, while only no more than 40 players' salaries are more than 25 million dollars. This also means that the salary problem needs to be paid attention by managers.","96a20769":"**---------Ending--------**","9ac21fe6":"##### Logistic Regression","b3da2b29":"#### (2) Selection of Model: Multivariate, Cross Validation, and Bias\/Variance Trade-off\n\nAt first, we use multivariate linear regression to build our initial model. Then we assume that our model does not cause overfitting or underfitting. In order to accept or reject our hypothesis, we use cross validation to separate our data into training set and validation set (8:2). Then we apply the bias\/variance trade-off graph to see whether the assumption is true or not.","aec9e6c2":"As it is shown on the Figure 5, when degree=1, both of training and validation set's RMSE are quite low. But when degree>=4, the difference between training set's RMSE and validation set's RMSE is obvious. Here we retain the null hypothesis that the 1nd-order polynomial model does not cause high-bias.\n\nAnd the performance of different models is shown above, where 2nd-order polynomial regression performs the best. And when it comes to 4nd-order polynomial regression, it causes overfiting.\n\nDifferent regularization methods perform differently. Focusing on coefficients and we can find that Ridge regularization drives parameters to smaller values. But if the multicollinearity exits, Lasso will turn its coefficients to 0, while Ridge will not erase any feature value. So if we want to do the feature selection, we can choose Lasso. But if we want to keep all features on the list, we prefer Ridge.\n\nIn all, although the effect of regularization is significant, it is much better to choose the correct parameters and features.","a48af47b":"***Ridge***","cf0b1b1b":"***Lasso***","e88ce759":"#### (1) Creating Features\n\nIn order to make classification prediction, it is neccessary to create discrete target variables according to players' salaries. Here we create two columns named 'Binary' and 'Nominal' as below:","253b136a":"### 2.3.2 Regression\n\n#### (1) Measure of Goodness: RMSE\n\nRoot Mean Squrare Error is a measure of how far the predicted points away from the real points. Compared with MSE and MAE, RMSE can provide the same dimensionality with target variables, and the sqaure function can make the measurement more precise than MAE when comparing different models.","842d8cb8":"## 2.2 Exploratory Analysis ","caa002be":"# 3 Recommendations & Discussions\n\n## Recommendations\n\n* What are the most important 4 features that influence the salary?\n\nThrough our modelling process, the most important 4 features are draft number, age, WS, and BPM.\n\n* What are the most suitable regression and classification models to predict players\u2019 salaries? And how do the models work?\n\nThe most suitable regression model is 2nd-order polynomial regression, which has the RMSE of about 4.1m. The most suitable binary classification model is SVM, taking the accuracy score of 0.896. Decision Tree model performs the best in Nominal target classification, with 0.691 accuracy score.\n\n* What recommendations can be made?\n\nFirstly, through our exploratory analysis and modelling process, the difference between salaries of overseas and USA players are not significant, but the number of USA players are almost 3 times more than that of overseas players. This difference can be an opportunity to recruit more overseas players from other countries for the promotion.\n\nSecondly, the correlations between players and their stats are not so strong, which means that there are the situations that players are overpaid or underpaid. In fact, this phenomenon is quite popular in the real NBA market. This should be noticed by the teams' managers.","f2ba2143":"### 2.3.1 Feature Engineering","6047fc8e":"##### Naive Bayes\n\nIn Naive Bayes, we assume that the features are independent from each other. We can try non-binary target variables.","578d591a":"The Regularization parameter tells the SVM optimization how much you want to avoid misclassifying each training example.\n\nFor large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.","b3b39ce4":"As expected, the OWS, DWS, and WS have a high variance inflation factor because they \"explain\" the same meaning. Also, the Age, BPM, USG%, VORP, MP and PER also share the similar high VIF, so some of them should be discarded. Therefore, we choose 'NBA_DraftNumber', 'Age', 'WS', 'BPM' as our features for modelling.","b814553c":"#### (3) Multicollinearity Analysis\n\nAs we have 8 features now, which may contain multicollinearity that make the model inaccurate and cause overfitting. Therefore, the VIF value can be chosen to detect the multicollinearity. If it is larger than 10, we think that the multicollinearity is very strong and the feature should not be included.","95f26648":"Through Model Tuning, Learning Curve, Curse of Dimensionality, and Confusion Matrix, we can get some knowledge about models' characteristics.\n\nFirstly, model tuning is quite silimar with Bias-Variance Trade-off. The most suitable point is not the highest point in training set, but a balanced point which performs \"not so bad\" in both training and validation sets. However, the values of some models' parameters are very large, while others are quite small, such as Naive Bayes, which should be paid attention to.\n\nSecondly, all models share the similar trends in the Learning Curve. When the size of training size is small, the score of training set is very high, but the score of vaidation set is very low, which causes overfitting. As the increase of data size, the score of training set becomes lower, and the validation set's score becomes higher, which means that the distance between these two groups are narrowing. However, as the size of this dataset only reaches 300, it cannot make sure that if the size is absolutly large (more than 10 thousands), how well will the curves perform.\n\nThridly, different models have different sensitivities to dimensionality. In our models, the curse of dimensionality is obvious in SVM, Naive Bayes, and Logistic Regression, where the high-dimensional features cause overfiting. While in other models, maybe it is because of the number of features are not enough, the \"curse\" does not appear.\nLastly, Confusion Matrix tells us the performance on different target groups. For example, SVM does better in predicting label \"0\", while KNN performs better in predicting label \"1\". The similar phenomenon happens in another comparison, which can be touted as an important way to see the details of our models' prediction.","e63f62e3":"# 2 Methods & Results","b010f4e0":"##### KNN","bcf8d413":"#### (3) Regularization: Ridge, Lasso, and ElasticNet\n\nThere are 3 ways to solve overfiting. The first way is to increase the size of dataset, the second way is to choose a suitable model complexity, and the third way is to use regularization to reduce the value of coefficient. In this part, we focus on regularization and select degree=4 to test the effectiveness of these three methods.\n\nThe meaning of regularization can be considered as 'punishiment'. When the model is too complex, the values of coefficients are very large. So we introduce the l to make the coefficients smaller than before.","921cc11a":"As it is shown on the Figure 5, when degree=1, both of training and validation set's RMSE are quite low. But when degree>=4, the difference between training set's RMSE and validation set's RMSE is obvious. Here we retain the null hypothesis that the 1nd-order polynomial model does not cause high-bias.","47414840":"## Discussions\n\nIn this mini-project, there are some technical limitations, such as normalization methods, model selections and stacking skills.\n\nFrom Andrew Ng's Open Course, Normalization can change the big-value-features into a small-value one, which may make the lost function more accurate. And maybe that is the reason why my model's RMSE reachs over million. But I have not applied it because I think this will affect the real data's meaning, and also I think it is meaningless because our RMSE is a relative value, not an absolute one.\n\nModel selection is important. At the beggining of this report, we planned to apply other advanced models, such as random forests, ADBoost, and so on. However, it is the mathematical concepts that let me realize that it is meaningless to apply them if I cannot understand the basic algorithm behind them. Therefore, I select Decision Tree by reading the slides, and select Logistic Regression by watching Andrew Ng's videos. Although I cannot apply these models this time, I believe I will understand them in the nearing future.\n\nAlso, multiple regression is the one that I cannot solve in programming language, because I cannot find any example in others' work. Similarly, within my ability, stacking skills is not approachable yet. But I believe that I will get them done in the future.","efae8105":"## 2.3 Modelling","b656dd88":"**Welcome to my Mini Project. This is NBA Salaries Prediction, version 3. I hope everybody can learn from it. If you think this is helpful or it has any problem, please upvote it and discuss your idea with me.**","b0873b50":"Probability Distribution can make it clear to realize the feature of our variables. Continuing the \"Country\" problem, a Bernoulli Distribution can be applied to see the difference between these two groups.","2ae7929b":"### 2.2.3 Probability Distributions\n\nBayes Theorem is the fundamental concept of probability. Here we can apply it to answer the question such as \u201cwhat is the probability that players\u2019 salaries are higher than 10 million dollars, given that the player is from USA?\u201d","53e29861":"##### Decision Tree\n\nThere are three ways to build a decision tree. CART is for binary target variables, ID3 is for nomial attributes, and C4.5 can be applied for continous features, whcih is the most suitable in our case.","916b459d":"##### SVM","fe8a8eb3":"### 2.3.3 Classification\n\n#### (1) Measure of Goodness: Accuracy Score and Confusion Matrix\n\nAccuracy Score is straight-forward, for it tells us the probability of the right answers that your model can predict. However, if we want to know the Accuracy Score of each target group, it is more suitable to use Confusion Matrix, which will show the comparison of predicted values and real values in each group.","f7c2f5ee":"## Reference\n\nDi, H. (2018). Value Creation: Comparative Netnographic Study of Two NBA Online Communities.\n\nNarayanan, A., Shi, E., & Rubinstein, B. I. (2011, July). Link prediction by de-anonymization: How we won the kaggle social network challenge. In Neural Networks (IJCNN), The 2011 International Joint Conference on (pp. 1825-1834). IEEE.\n\nRosen, J., Arcidiacono, P., & Kimbrough, K. (2016). Determining NBA Free Agent Salary from Player Performance.","bfa32bef":"The use of gamma is similar to k in KNN. The higher the gamma value it tries to exactly fit the training data set. And if the gamma value is too high, it will cause overfitting.\n\nThe gamma parameter defines how far the influence of a single training example reaches, with low values meaning \u2018far\u2019 and high values meaning \u2018close\u2019. In other words, with low gamma, points far away from plausible seperation line are considered in calculation for the seperation line. Where as high gamma means the points close to plausible line are considered in calculation.","25807cb5":"Also, players\u2019 age is one of the most important issues in NBA, for a player can make more profits if he can play longer. As it is a discrete value, we can build a probability mass function about age."}}