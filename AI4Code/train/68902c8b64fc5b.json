{"cell_type":{"52804fe2":"code","35509263":"code","7e807bcb":"code","76bc5466":"code","20dec33e":"code","7be50211":"code","b9c87383":"code","fbb2c98f":"code","d6183f0f":"code","3fba83cd":"code","dbf18d0d":"code","946a7c58":"code","5d55efc4":"code","965989f0":"code","5d8cc1c3":"code","dacda5e5":"code","47b732ce":"code","e0e2cb7d":"code","9a4de008":"code","9520e5b8":"code","49651a1f":"code","0f9486d7":"code","0dfce067":"code","5af4fe8f":"code","3b126e0f":"code","d96f4251":"code","54831617":"code","b1a2f4a8":"code","4d937aa7":"code","bec05afe":"code","3a057955":"code","44b60e52":"code","f16a7771":"code","a828a60c":"code","4ba22759":"code","885d600e":"code","383b4dd6":"code","ad1e4a65":"code","e22674b7":"code","11da7d0e":"code","9092a77f":"code","1edee2f0":"code","f03d602d":"code","287adfb6":"code","e9b4cdcb":"code","b2c99152":"code","df28d46b":"code","ae160d0c":"code","1349d962":"code","06411ac8":"code","48275bd7":"code","5e3f913c":"code","ed4148f4":"code","a6eec356":"code","3880bb6c":"code","ee3609ad":"code","28d250d8":"code","5aef0534":"code","c1e29071":"code","a1a053bf":"code","ef9b6533":"code","9423cbc7":"code","302b54c2":"code","078567d0":"code","6d74f529":"code","20a6f680":"code","a5045c99":"code","433ffefd":"code","cb68f03d":"code","52ef9ff0":"code","2556297d":"code","f9949c08":"markdown","22e93302":"markdown","77e43a11":"markdown","97f1994e":"markdown","180ccc62":"markdown","3386a404":"markdown","87c1e141":"markdown","0c09b408":"markdown","1e7a8726":"markdown","b7b2a615":"markdown","d8b92878":"markdown","22b8d9a7":"markdown","ba6af412":"markdown","397c65c2":"markdown","b81c18cb":"markdown","8b181657":"markdown","4747dc1f":"markdown","8a4c22dd":"markdown","0a559404":"markdown","11128570":"markdown","08259aa7":"markdown","feffe5d1":"markdown","ca5adad4":"markdown","a892072b":"markdown","a60b3eb8":"markdown","16b16ab8":"markdown","b6426c64":"markdown","98675cb9":"markdown","719f8c73":"markdown","c961292a":"markdown","e80e32e8":"markdown","ad94e024":"markdown","5a201c9d":"markdown","5f98fdff":"markdown","239eff81":"markdown"},"source":{"52804fe2":"#Import all packages needed\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport requests\nimport tweepy \nimport json ","35509263":"pip install tweepy","7e807bcb":"#Read CSV file \ntwitter_archive = pd.read_csv('..\/input\/weratedogs-twitterapi\/twitter-archive-enhanced.csv')","76bc5466":"twitter_archive.sort_values('timestamp')\ntwitter_archive.head()","20dec33e":"twitter_archive.info()","7be50211":"#URL downloaded programatically \nurl = \"https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/599fd2ad_image-predictions\/image-predictions.tsv\"\nresponse = requests.get(url)\n\nwith open('image-predictions.tsv', mode ='wb') as file:\n    file.write(response.content)\n\n#Read TSV file\nimage_prediction = pd.read_csv('image-predictions.tsv', sep='\\t' )","b9c87383":"#https:\/\/stackoverflow.com\/questions\/28384588\/twitter-api-get-tweets-with-specific-id\n\nauth = tweepy.OAuthHandler('5Uur0mo4ol2kB8yhtZ1VxXS0u', 'h8E7fSpXWiMoBel7G1ZOAeu4Mgru0v0MtxH5ehYE1RKM89SiBH')\nauth.set_access_token('303562412-ct9aNnU0FQR0UKJVn1i1W3Y8omqSewiQWUcRaygB', 'D3qslrbdOU5fqTOp951kOIuZbkeTPBodnjNYoEGFR63Ft')\napi = tweepy.API(auth, \n                 parser = tweepy.parsers.JSONParser(), \n                 wait_on_rate_limit = True, \n                 wait_on_rate_limit_notify = True)\n","fbb2c98f":"#Download Tweepy status object based on Tweet ID and store in list\nlist_of_tweets = []\n# Tweets that can't be found are saved in the list below:\ncant_find_tweets_for_those_ids = []\nfor tweet_id in twitter_archive['tweet_id']:   \n    try:\n        list_of_tweets.append(api.get_status(tweet_id))\n    except Exception as e:\n        cant_find_tweets_for_those_ids.append(tweet_id)","d6183f0f":"print(\"The list of tweets\" ,len(list_of_tweets))\nprint(\"The list of tweets no found\" , len(cant_find_tweets_for_those_ids))","3fba83cd":"#Then in this code block we isolate the json part of each tweepy \n#status object that we have downloaded and we add them all into a list\n\nmy_list_of_dicts = []\nfor each_json_tweet in list_of_tweets:\n    my_list_of_dicts.append(each_json_tweet)","dbf18d0d":"#we write this list into a txt file:\n\nwith open('tweet_json.txt', 'w') as file:\n        file.write(json.dumps(my_list_of_dicts, indent=4))","946a7c58":"#identify information of interest from JSON dictionaries in txt file\n#and put it in a dataframe called tweet JSON\nmy_demo_list = []\nwith open('tweet_json.txt', encoding='utf-8') as json_file:  \n    all_data = json.load(json_file)\n    for each_dictionary in all_data:\n        tweet_id = each_dictionary['id']\n        whole_tweet = each_dictionary['text']\n        only_url = whole_tweet[whole_tweet.find('https'):]\n        favorite_count = each_dictionary['favorite_count']\n        retweet_count = each_dictionary['retweet_count']\n        followers_count = each_dictionary['user']['followers_count']\n        friends_count = each_dictionary['user']['friends_count']\n        whole_source = each_dictionary['source']\n        only_device = whole_source[whole_source.find('rel=\"nofollow\">') + 15:-4]\n        source = only_device\n        retweeted_status = each_dictionary['retweeted_status'] = each_dictionary.get('retweeted_status', 'Original tweet')\n        if retweeted_status == 'Original tweet':\n            url = only_url\n        else:\n            retweeted_status = 'This is a retweet'\n            url = 'This is a retweet'\n\n        my_demo_list.append({'tweet_id': str(tweet_id),\n                             'favorite_count': int(favorite_count),\n                             'retweet_count': int(retweet_count),\n                             'followers_count': int(followers_count),\n                             'friends_count': int(friends_count),\n                             'url': url,\n                             'source': source,\n                             'retweeted_status': retweeted_status,\n                            })\n        tweet_json = pd.DataFrame(my_demo_list, columns = ['tweet_id', 'favorite_count','retweet_count', \n                                                           'followers_count', 'friends_count','source', \n                                                           'retweeted_status', 'url'])\n","5d55efc4":"tweet_json.head()","965989f0":"tweet_json.info()","5d8cc1c3":"twitter_archive","dacda5e5":"image_prediction","47b732ce":"tweet_json","e0e2cb7d":"twitter_archive.info()","9a4de008":"sum(twitter_archive['tweet_id'].duplicated())","9520e5b8":"twitter_archive.rating_numerator.value_counts()","49651a1f":"print(twitter_archive.loc[twitter_archive.rating_numerator == 204, 'text']) \nprint(twitter_archive.loc[twitter_archive.rating_numerator == 143, 'text']) \nprint(twitter_archive.loc[twitter_archive.rating_numerator == 666, 'text']) \nprint(twitter_archive.loc[twitter_archive.rating_numerator == 1176, 'text'])\nprint(twitter_archive.loc[twitter_archive.rating_numerator == 144, 'text'])","0f9486d7":"#print whole text in order to verify numerators and denominators\nprint(twitter_archive['text'][1120]) #17 dogs\nprint(twitter_archive['text'][1634]) #13 dogs\nprint(twitter_archive['text'][313]) #just a tweet to explain actual ratings, this will be ignored when cleaning data\nprint(twitter_archive['text'][189]) #no picture, this will be ignored when cleaning data\nprint(twitter_archive['text'][1779]) #12 dogs","0dfce067":"twitter_archive.rating_denominator.value_counts()","5af4fe8f":"print(twitter_archive.loc[twitter_archive.rating_denominator == 11, 'text']) \nprint(twitter_archive.loc[twitter_archive.rating_denominator == 2, 'text']) \nprint(twitter_archive.loc[twitter_archive.rating_denominator == 16, 'text']) \nprint(twitter_archive.loc[twitter_archive.rating_denominator == 15, 'text'])\nprint(twitter_archive.loc[twitter_archive.rating_denominator == 7, 'text'])","3b126e0f":"print(twitter_archive['text'][784]) #retweet - it will be deleted when delete all retweets\nprint(twitter_archive['text'][1068]) #actual rating 14\/10 need to change manually\nprint(twitter_archive['text'][1662]) #actual rating 10\/10 need to change manually\nprint(twitter_archive['text'][2335]) #actual rating 9\/10 need to change manually\nprint(twitter_archive['text'][1663]) # tweet to explain rating\nprint(twitter_archive['text'][342]) #no rating - delete\nprint(twitter_archive['text'][516]) #no rating - delete","d96f4251":"#   UDACITY REVIEWER POINT: \n#Check the numerator - in the source data there seems to be fractional numerators. Are those preserved \n#when exporting the data? Take a look at the id = 681340665377193000\n\n#TWITTER_ID NOT FOUND\n\ntwitter_archive[twitter_archive['tweet_id'] == 681340665377193000]","54831617":"# Reviewer point brought something to my attention that it was not picked up during the first assessment\n\nwith pd.option_context('max_colwidth', 200):\n    display(twitter_archive[twitter_archive['text'].str.contains(r\"(\\d+\\.\\d*\\\/\\d+)\")]\n            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])","b1a2f4a8":"image_prediction.sample(10)","4d937aa7":"image_prediction.info()","bec05afe":"sum(image_prediction.jpg_url.duplicated())","3a057955":"pd.concat(g for _, g in image_prediction.groupby(\"jpg_url\") if len(g) > 1)","44b60e52":"print(image_prediction.p1_dog.value_counts())\nprint(image_prediction.p2_dog.value_counts())\nprint(image_prediction.p3_dog.value_counts())","f16a7771":"image_prediction.img_num.value_counts()","a828a60c":"tweet_json.sample(10)","4ba22759":"tweet_json.info()","885d600e":"tweet_json.retweeted_status.value_counts()","383b4dd6":"tweet_json.source.value_counts()","ad1e4a65":"twitter_archive_clean = twitter_archive.copy()\nimage_prediction_clean = image_prediction.copy()\ntweet_json_clean = tweet_json.copy()","e22674b7":"#CODE: Delete retweets by filtering the NaN of retweeted_status_user_id\ntwitter_archive_clean = twitter_archive_clean[pd.isnull(twitter_archive_clean['retweeted_status_user_id'])]\n\n#TEST\nprint(sum(twitter_archive_clean.retweeted_status_user_id.value_counts()))","11da7d0e":"#get the column names of twitter_archive_clean\nprint(list(twitter_archive_clean))\n\n#CODE: Delete columns no needed\ntwitter_archive_clean = twitter_archive_clean.drop(['source',\n                                                    'in_reply_to_status_id',\n                                                    'in_reply_to_user_id',\n                                                    'retweeted_status_id',\n                                                    'retweeted_status_user_id', \n                                                    'retweeted_status_timestamp', \n                                                    'expanded_urls'], 1)\n","9092a77f":"#TEST\nlist(twitter_archive_clean)","1edee2f0":"#CODE: Melt the doggo, floofer, pupper and puppo columns to dogs and dogs_stage column\ntwitter_archive_clean = pd.melt(twitter_archive_clean, id_vars=['tweet_id',                                          \n                                                                'timestamp',\n                                                                'text',\n                                                                'rating_numerator',\n                                                                'rating_denominator',\n                                                                'name'],\n                               var_name='dogs', value_name='dogs_stage')\n\n#CODE: drop dogs\ntwitter_archive_clean = twitter_archive_clean.drop('dogs', 1)\n\n#CODE: Sort by dogs_stage then drop duplicated based on tweet_id except the last occurrence\ntwitter_archive_clean = twitter_archive_clean.sort_values('dogs_stage').drop_duplicates(subset='tweet_id', \n                                                                                        keep='last')\n                            ","f03d602d":"#TEST\ntwitter_archive_clean['dogs_stage'].value_counts()","287adfb6":"#CODE: convert timestamp to datetime\ntwitter_archive_clean['timestamp'] = pd.to_datetime(twitter_archive_clean['timestamp'])\n\n#extract year, month and day to new columns\ntwitter_archive_clean['year'] = twitter_archive_clean['timestamp'].dt.year\ntwitter_archive_clean['month'] = twitter_archive_clean['timestamp'].dt.month\ntwitter_archive_clean['day'] = twitter_archive_clean['timestamp'].dt.day\n\n#Finally drop timestamp column\ntwitter_archive_clean = twitter_archive_clean.drop('timestamp', 1)","e9b4cdcb":"#TEST\nlist(twitter_archive_clean)","b2c99152":"\ntwitter_archive_clean[['rating_numerator', 'rating_denominator']] = twitter_archive_clean[['rating_numerator','rating_denominator']].astype(float)\n\ntwitter_archive_clean.info()","df28d46b":"#CODE\n\n#First change numerator and denominators type int to float to allow decimals \ntwitter_archive_clean[['rating_numerator', 'rating_denominator']] = twitter_archive_clean[['rating_numerator','rating_denominator']].astype(float)\n\n#Update numerators\n\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 883482846933004288), 'rating_numerator'] = 13.5\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 786709082849828864), 'rating_numerator'] = 9.75\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 778027034220126208), 'rating_numerator'] = 11.27\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 681340665377193984), 'rating_numerator'] = 9.5\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 680494726643068929), 'rating_numerator'] = 11.26\n\n#TEST\nwith pd.option_context('max_colwidth', 200):\n    display(twitter_archive_clean[twitter_archive_clean['text'].str.contains(r\"(\\d+\\.\\d*\\\/\\d+)\")]\n            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])","ae160d0c":"#CODE: Update both numerators and denominators\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 740373189193256964), 'rating_numerator'] = 14\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 740373189193256964), 'rating_denominator'] = 10\n\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 682962037429899265), 'rating_numerator'] = 10\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 682962037429899265), 'rating_denominator'] = 10\n\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 666287406224695296), 'rating_numerator'] = 9\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 666287406224695296), 'rating_denominator'] = 10\n\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 722974582966214656), 'rating_numerator'] = 13\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 722974582966214656), 'rating_denominator'] = 10\n\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 716439118184652801), 'rating_numerator'] = 13.5\ntwitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 716439118184652801), 'rating_denominator'] = 10\n\n#CODE: Delete five tweets with no actual ratings\ntwitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 832088576586297345]\ntwitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 810984652412424192]\ntwitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 682808988178739200]\ntwitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 835246439529840640]\ntwitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 686035780142297088]\n\n#TEST: Left only the group dogs for programatically clean\nwith pd.option_context('max_colwidth', 200):\n    display(twitter_archive_clean[twitter_archive_clean['rating_denominator'] != 10][['tweet_id',\n                                                                                      'text',\n                                                                                      'rating_numerator',\n                                                                                      'rating_denominator']])","1349d962":"#CODE: Create a new column with rating in float type to avoid converting all int column to float\ntwitter_archive_clean['rating'] = 10 * twitter_archive_clean['rating_numerator'] \/ twitter_archive_clean['rating_denominator'].astype(float)\n\n#TEST\ntwitter_archive_clean.sample(5)","06411ac8":"#CODE: Delete duplicated jpg_url\nimage_prediction_clean = image_prediction_clean.drop_duplicates(subset=['jpg_url'], keep='last')\n\n#TEST\nsum(image_prediction_clean['jpg_url'].duplicated())","48275bd7":"#CODE: the first true prediction (p1, p2 or p3) will be store in these lists\ndog_type = []\nconfidence_list = []\n\n#create a function with nested if to capture the dog type and confidence level\n# from the first 'true' prediction\ndef image(image_prediction_clean):\n    if image_prediction_clean['p1_dog'] == True:\n        dog_type.append(image_prediction_clean['p1'])\n        confidence_list.append(image_prediction_clean['p1_conf'])\n    elif image_prediction_clean['p2_dog'] == True:\n        dog_type.append(image_prediction_clean['p2'])\n        confidence_list.append(image_prediction_clean['p2_conf'])\n    elif image_prediction_clean['p3_dog'] == True:\n        dog_type.append(image_prediction_clean['p3'])\n        confidence_list.append(image_prediction_clean['p3_conf'])\n    else:\n        dog_type.append('Error')\n        confidence_list.append('Error')\n\n#series objects having index the image_prediction_clean column.        \nimage_prediction_clean.apply(image, axis=1)\n\n#create new columns\nimage_prediction_clean['dog_type'] = dog_type\nimage_prediction_clean['confidence_list'] = confidence_list","5e3f913c":"#drop rows that has prediction_list 'error'\nimage_prediction_clean = image_prediction_clean[image_prediction_clean['dog_type'] != 'Error']\n\n#TEST: \nimage_prediction_clean.info()","ed4148f4":"#CODE: print list of image_prediction columns\nprint(list(image_prediction_clean))\n\n#Delete columns\nimage_prediction_clean = image_prediction_clean.drop(['img_num', 'p1', \n                                                      'p1_conf', 'p1_dog', \n                                                      'p2', 'p2_conf', \n                                                      'p2_dog', 'p3', \n                                                      'p3_conf', \n                                                      'p3_dog'], 1)\n\n#TEST\nlist(image_prediction_clean)","a6eec356":"#CODE:\ntweet_json_clean = tweet_json_clean[tweet_json_clean['retweeted_status'] == 'Original tweet']\n\n#TEST\ntweet_json_clean['retweeted_status'].value_counts()","3880bb6c":"#CODE: change tweet_id from str to int\ntweet_json_clean['tweet_id'] = tweet_json_clean['tweet_id'].astype(int)\n\n#TEST\ntweet_json_clean['tweet_id'].dtypes","ee3609ad":"#CODE: create a new dataframe that merge twitter_archive_clean and \n#image_prediction_clean\ndf_twitter1 = pd.merge(twitter_archive_clean, \n                      image_prediction_clean, \n                      how = 'left', on = ['tweet_id'])\n\n#keep rows that have picture (jpg_url)\ndf_twitter1 = df_twitter1[df_twitter1['jpg_url'].notnull()]\n\n#TEST\ndf_twitter1.info()","28d250d8":"#CODE: create a new dataframe that merge df_twitter and tweet_json_clean\ndf_twitter = pd.merge(df_twitter1, tweet_json_clean, \n                      how = 'left', on = ['tweet_id'])\n\n#TEST\ndf_twitter.info()","5aef0534":"df_twitter['rating_numerator'].value_counts()","c1e29071":"#Store the clean DataFrame in a CSV file\ndf_twitter.to_csv('twitter_archive_master.csv', \n                 index=False, encoding = 'utf-8')","a1a053bf":"df_twitter['dog_type'].value_counts()","ef9b6533":"df_dog_type = df_twitter.groupby('dog_type').filter(lambda x: len(x) >= 25)\n\ndf_dog_type['dog_type'].value_counts().plot(kind = 'barh')\nplt.title('Histogram of the Most Rated Dog Type')\nplt.xlabel('Count')\nplt.ylabel('Type of dog')\n\nfig = plt.gcf() \nfig.savefig('output.png',bbox_inches='tight');","9423cbc7":"df_dog_type_mean = df_twitter.groupby('dog_type').mean()\n","302b54c2":"df_dog_type_mean.head()","078567d0":"df_dog_type_sorted = df_dog_type_mean['rating'].sort_values()\n\ndf_dog_type_sorted","6d74f529":"print(df_twitter.loc[df_twitter.dog_type == 'Japanese_spaniel', 'url']) ","20a6f680":"df_twitter[df_twitter['dog_type'] == 'golden_retriever']","a5045c99":"df_dog_type_count = df_twitter.groupby('dog_type').count()\ndf_dog_type_count","433ffefd":"dog_type_count = df_dog_type_count['rating']\ndog_type_mean = df_dog_type_mean['rating']\ndog_type_mean","cb68f03d":"df = pd.DataFrame()\ndf['dog_type_count'] = dog_type_count\ndf['dog_type_mean'] = dog_type_mean\ndf","52ef9ff0":"df.plot(x='dog_type_count', y='dog_type_mean', kind='scatter')\nplt.xlabel('Number of Ratings of a Dog Type')\nplt.ylabel('Average Rating of Dog Type')\nplt.title('Average Rating of Dog Type by Number of Ratings of a Dog Type Scatter Plot')\n\nfig = plt.gcf()\n#plt.savefig('X:\/' + newName + '.png', \nfig.savefig('output2.png',bbox_inches='tight');","2556297d":"df_twitter.plot(x='retweet_count', y='rating', kind='scatter')\nplt.xlabel('Retweet Counts')\nplt.ylabel('Ratings')\nplt.title('Retweet Counts by Ratings Scatter Plot')\n\nfig = plt.gcf()\nfig.savefig('output3.png',bbox_inches='tight');","f9949c08":"<a id='clean'><\/a>\n## Cleaning Data","22e93302":"**7. Image_prediction** - Drop 66 jpg_url duplicated","77e43a11":"<a id='assess'><\/a>\n## Assessing data","97f1994e":"**1. Twitter archive** - keep original ratings (no retweets) that have images. \n\nBased on info, there are 181 values in retweeted_status_id and retweeted_status_user_id. Delete the retweets. Once I merge twitter_archive and image_prediction, I will only keep the ones with images.","180ccc62":"<a id='intro'><\/a>\n## Introduction\n\n","3386a404":"**12. Tidiness** - All tables should be part of one dataset","87c1e141":"### Programmatic assessment\n\nPandas' functions and\/or methods are used to assess the data.","0c09b408":"<a id='two'><\/a>\n### Insight two\nJapanese_spaniel has the lowest average rating\nClumber has the highest average rating","1e7a8726":"#### *b. Programatically* \nThese tweets with denominator not equal to 10 are multiple dogs. For example, tweet_id 713900603437621000 has numerator and denominators 99\/90 because there are 9 dogs in the picture https:\/\/t.co\/mpvaVxKmc1.  ","b7b2a615":"**5. Twitter_archive** - Correc numerators","d8b92878":"** 8. Image_prediction** - Create 1 column for image prediction and 1 column for confidence level\n\nCreate a function where I keep the first true prediction along the confidence level as new columns. ","22b8d9a7":"<a id='gather'><\/a>\n## Gathering data","ba6af412":"**6. Twitter_archive** - Correc denominators\n\n#### *a. Manually*\nFive tweets with denominator not equal to 10 for special circunstances. Update both numerators and denominators when necessary. Delete other five tweets because they do not have actual ratings.","397c65c2":"<a id='one'><\/a>\n### Insight one & visualization\n\nGolden retriever is the most common dog in this dataset.","b81c18cb":"**4. Twitter_archive** - Separate timestamp into day - month - year (3 columns)\n\nFirst convert *timestamp* to datetime. Then extract year, month and day to new columns. Finally drop *timestamp* column. ","8b181657":"**10. Tweet_json** - keep 2174 original tweets ","4747dc1f":"**2. Tweet image prediction**","8a4c22dd":"# Data wrangling WeRateDogs\n\n","0a559404":"### *tweet_json*\n1. Keep original tweets only\n","11128570":"### Visual assessment\n\nEach piece of gathered data is displayed in the Jupyter Notebook for visual assessment purposes.","08259aa7":"**9. Image_prediction** - Delete columns that won't be used for analysis","feffe5d1":"**3. Twitter API & JSON**\n\nhttps:\/\/stackoverflow.com\/questions\/47612822\/how-to-create-pandas-dataframe-from-twitter-search-api","ca5adad4":"1. **Twitter archive file:** download this file manually by clicking the following link: twitter_archive_enhanced.csv\n\n- **The tweet image predictions**, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/599fd2ad_image-predictions\/image-predictions.tsv\n\n- **Twitter API & JSON:** Each tweet's retweet count and favorite (\"like\") count at minimum, and any additional data you find interesting. Using the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. \n    Each tweet's JSON data should be written to its own line. Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count. ","a892072b":"<a id='three'><\/a>\n### Insight three & visualization\n\nDog_types with low number of ratings show a high variaty of mean ratings. ","a60b3eb8":"<a id='quality'><\/a>\n### Quality\n\n*Completeness, validity, accuracy, consistency (content issues)*","16b16ab8":"**3. Twitter_archive** - Erroneous datatypes (doggo, floofer, pupper and puppo columns)\n\nMelt the doggo, floofer, pupper and puppo columns to *dogs* and *dogs_stage* column. Then drop *dogs*. Sort by *dogs_stage* in order to then drop duplicated based on tweet_id except for the last occurrence.","b6426c64":"### *twitter_archive*\n1. Keep original ratings (no retweets) that have images\n- Delete columns that won't be used for analysis\n- Erroneous datatypes (doggo, floofer, pupper and puppo columns)\n- Separate timestamp into day - month - year (3 columns)\n- Correct numerators with decimals\n- Correc denominators other than 10: \n    \n    a. Manually (few examples assessed by individual print text).\n    \n    b. Programatically (Tweets with denominator not equal to 10 are usually multiple dogs). \n","98675cb9":"**2. Twitter archive** - Delete columns that won't be used for analysis","719f8c73":"<a id='four'><\/a>\n### Insight four & visualization\n\nThe highest ratings do not receive the most retweets. ","c961292a":"### *image_prediction*\n6. Drop 66 jpg_url duplicated\n7. Create 1 column for image prediction and 1 column for confidence level\n8. Delete columns that won't be used for analysis ","e80e32e8":"**11. Tidiness** - Change tweet_id to type int64 in order to merge with the other 2 tables","ad94e024":"The purpose of this project is to put in practice what I learned in data wrangling data section from Udacity Data Analysis Nanodegree program. The dataset that is wrangled is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. ","5a201c9d":"<a id='tidiness'><\/a>\n### Tidiness \n\n1. Change tweet_id to type int64 in order to merge with the other 2 tables\n- All tables should be part of one dataset","5f98fdff":"**1. Twitter archive file**","239eff81":"<a id='storing'><\/a>\n## Storing, Analyzing, and Visualizing Data"}}