{"cell_type":{"709ac594":"code","3fa0dc8c":"code","449c7bfd":"code","d4411e4c":"code","46cee9c3":"code","e49a11d9":"code","5d98f58e":"code","56e11ece":"code","c6b13e71":"code","7cb95586":"code","fa4129b5":"code","86d78466":"code","47905b14":"code","f96d5db0":"code","903557e1":"code","6c70e107":"code","96b5ac1f":"code","ae6ba03a":"code","7edbf064":"code","5a2424db":"code","9b152d01":"code","be1ee5ef":"code","f7cd8fa8":"code","39c74a53":"code","4c41ebab":"code","79d455f6":"code","fac26135":"code","95d910ac":"code","6d0be2fd":"code","09ff6f4f":"code","df427973":"code","c87af313":"code","75dc140f":"code","71342aef":"code","fac94284":"code","a107cb27":"code","151627ca":"code","474d41c2":"code","9afe1762":"code","14e8cd23":"code","0ac33050":"code","7b6b7e06":"code","5675c7dc":"code","3389a7d1":"code","8b86cfac":"code","a17a4100":"code","fa5d6619":"code","025ae528":"markdown","efb583ae":"markdown","87ec497d":"markdown","5e963a43":"markdown","0aa15077":"markdown","bb1e3435":"markdown","09add9d3":"markdown","e2a1d4fa":"markdown","489379c7":"markdown","14378973":"markdown","11e229d2":"markdown"},"source":{"709ac594":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3fa0dc8c":"heart=pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","449c7bfd":"heart","d4411e4c":"#Men having Heart Attack\nmen=0\nfor i in range(303):\n    if(heart['sex'][i]==1 and heart['output'][i]==1):\n        men=men+1\nmen","46cee9c3":"#Women having Heart Attack\nwomen=0\nfor i in range(303):\n    if(heart['sex'][i]==0 and heart['output'][i]==1):\n        women=women+1\nwomen","e49a11d9":"y = np.array([men,women])\nmylabels = ['Men  '+str((men\/(men+women))*100)+\"%\", 'Women  '+str((women\/(men+women))*100)+\"%\"]\nplt.pie(y, labels = mylabels)\nplt.show() ","5d98f58e":"#Factors having most impact on heart faliure\n\ncorr=heart.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr,vmax=1,square=True,annot=True)","56e11ece":"heart.describe()","c6b13e71":"heart.isnull().sum()","7cb95586":"idx=[i for i in range(1,304)]\nheart['id']=idx\nheart.head()","fa4129b5":"# Type of Test Pain Associated Mostly with Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nax = sns.barplot(\n    x = df_heartattack['cp'].value_counts().head(10).keys(), \n    y = df_heartattack['cp'].value_counts().head(10).values\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n\nplt.title('Type of Test Pain Associated Mostly with Heart Attack')\nplt.show()","86d78466":"# Resting BP Associated Mostly with Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nax = sns.barplot(\n    x = df_heartattack['trtbps'].value_counts().head(10).keys(), \n    y = df_heartattack['trtbps'].value_counts().head(10).values\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n\nplt.title('Resting BP Associated Mostly with Heart Attack')\nplt.show()","47905b14":"# Resting BP Not Associated Mostly with Heart Attack\n\ndf_heartattack= heart[heart['output']==0]\nax = sns.barplot(\n    x = df_heartattack['trtbps'].value_counts().head(10).keys(), \n    y = df_heartattack['trtbps'].value_counts().head(10).values\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n\nplt.title('Resting BP Not Associated Mostly with Heart Attack')\nplt.show()","f96d5db0":"# Fasting blood sugar Associated Mostly with Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nax = sns.barplot(\n    x = df_heartattack['fbs'].value_counts().head(10).keys(), \n    y = df_heartattack['fbs'].value_counts().head(10).values\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n\nplt.title('Fasting blood sugar Associated Mostly with Heart Attack')\nplt.show()","903557e1":"# number of major vessels Associated Mostly with Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nax = sns.barplot(\n    x = df_heartattack['caa'].value_counts().head(10).keys(), \n    y = df_heartattack['caa'].value_counts().head(10).values\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n\nplt.title('number of major vessels Associated Mostly with Heart Attack')\nplt.show()","6c70e107":"\n# Resting electrocardiographic results Associated Mostly with Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nax = sns.barplot(\n    x = df_heartattack['restecg'].value_counts().head(10).keys(), \n    y = df_heartattack['restecg'].value_counts().head(10).values\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n\nplt.title('Resting electrocardiographic results Associated Mostly with Heart Attack')\nplt.show()","96b5ac1f":"# Age Most Susceptible to Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nsns.histplot(df_heartattack['age'],kde = False)\n\nplt.title('Age Most Susceptible to Heart Attack')\nplt.show()","ae6ba03a":"# Cholestoral Level Most Susceptible to Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nsns.histplot(df_heartattack['chol'],kde = False)\n\nplt.title('Cholestoral Level Most Susceptible to Heart Attack')\nplt.show()","7edbf064":"# Max Heart Rate Most Susceptible to Heart Attack\n\ndf_heartattack= heart[heart['output']==1]\nsns.histplot(df_heartattack['thalachh'],kde = False)\n\nplt.title('Max Heart Rate Level Most Susceptible to Heart Attack')\nplt.show()","5a2424db":"from sklearn.model_selection import train_test_split","9b152d01":"#Getting All the columns\ncol=heart.columns","be1ee5ef":"# Data points on which classifier will be trained\nX=heart[col[:-2]]\nX","f7cd8fa8":"#Target Values\nY=heart['output'].values","39c74a53":"# Splitting into Test and Train data. Test data is used for evaluation\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","4c41ebab":"from sklearn.ensemble import RandomForestClassifier","79d455f6":"clf=RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X_train,y_train)","fac26135":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score","95d910ac":"ypred=clf.predict(X_test)\nacc=accuracy_score(y_test, ypred) # 86 % Not Bad. But Can We improve?\nprc=precision_score(y_test, ypred)\nrecall=recall_score(y_test, ypred)\nf1=f1_score(y_test, ypred)\nroc_auc=roc_auc_score(y_test, ypred)\nprint(\"Accuracy  \",acc)\nprint(\"Precision  \",prc)\nprint(\"Recall  \",recall)\nprint(\"F1  \",f1)\nprint(\"ROC_AUC  \",roc_auc)","6d0be2fd":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel","09ff6f4f":"lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train) # Usually applied on whole dataset!!!!!","df427973":"model = SelectFromModel(lsvc, prefit=True)","c87af313":"X_train_new=model.transform(X_train)\nX_test_new=model.transform(X_test)   # Transforming the dataset based on Selected feature","75dc140f":"print(\"Old Shape =\",X_train.shape,\" New Shape=\",X_train_new.shape)\nprint(\"Yippe we reduced the Dimesions From \",X_train.shape[1],\" to\",X_train_new.shape[1])","71342aef":"clf2=RandomForestClassifier(max_depth=2, random_state=0)\nclf2.fit(X_train_new,y_train)\nyprednew=clf2.predict(X_test_new)\nacc=accuracy_score(y_test, yprednew) # 86 % Not Bad. But Can We improve?\nprc=precision_score(y_test, yprednew)\nrecall=recall_score(y_test, yprednew)\nf1=f1_score(y_test, yprednew)\nroc_auc=roc_auc_score(y_test, yprednew)\nprint(\"Accuracy  \",acc)\nprint(\"Precision  \",prc)\nprint(\"Recall  \",recall)\nprint(\"F1  \",f1)\nprint(\"ROC_AUC  \",roc_auc)","fac94284":"X.shape\nY.shape","a107cb27":"import optuna","151627ca":"import optuna\nfrom sklearn.model_selection import train_test_split\n\ndef objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 255),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n        \"min_weight_fraction_leaf\": trial.suggest_float(\"min_weight_fraction_leaf\", 0.1, 0.5),\n        \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 2, 20),\n        \"random_state\": trial.suggest_int(\"random_state\", 0, 100),\n    }\n    clf=RandomForestClassifier(**params)\n    \n    model = clf.fit(X_train,y_train)\n    preds = clf.predict(X_test)\n    acc = accuracy_score(y_test, preds)\n    return acc\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n","474d41c2":"\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial: \", study.best_trial.params)","9afe1762":"params= {'n_estimators': 49, 'max_depth': 30, 'min_weight_fraction_leaf': 0.295797700627993, 'max_leaf_nodes': 7, 'random_state': 34}","14e8cd23":"clf=RandomForestClassifier(**params)\nclf.fit(X_train,y_train)\nypred=clf.predict(X_test)\nacc=accuracy_score(y_test, ypred) \nprc=precision_score(y_test, ypred)\nrecall=recall_score(y_test, ypred)\nf1=f1_score(y_test, ypred)\nroc_auc=roc_auc_score(y_test, ypred)\nprint(\"Accuracy  \",acc)\nprint(\"Precision  \",prc)\nprint(\"Recall  \",recall)\nprint(\"F1  \",f1)\nprint(\"ROC_AUC  \",roc_auc)\n","0ac33050":"import xgboost as xgb","7b6b7e06":"def objective(trial):\n    param = {\n        'eval_metric' : 'auc',\n        'booster' : 'gbtree',\n        #'tree_method' : 'gpu_hist' , \n        'use_label_encoder' : False , \n        'lambda' : trial.suggest_loguniform('lambda' , 1e-5 , 1.0),\n        'alpha' : trial.suggest_loguniform('alpha' , 1e-5 , 1.0),\n        'colsample_bytree' : trial.suggest_uniform('colsample_bytree' , 0 , 1.0),\n        'subsample' : trial.suggest_uniform('subsample' , 0 , 1.0),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0 , 0.02),\n        'n_estimators' : trial.suggest_int('n_estimators' , 1 , 9999),\n        'max_depth' : trial.suggest_int('max_depth' , 1 , 20),\n        'random_state' : trial.suggest_categorical('random_state' , [0,42,2021]),\n        'min_child_weight' : trial.suggest_int('min_child_weight' , 1 , 300),\n        'gamma' : trial.suggest_loguniform('gamma' , 1e-5 , 1.0)\n    }\n    model = xgb.XGBClassifier(**param)  \n    \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(X_test)\n    acc = accuracy_score(y_test, preds)\n    return acc\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)","5675c7dc":"print('number of the finished trials:' , len(study.trials))\nprint('the parametors of best trial:' , study.best_trial.params)\nprint('best value:' , study.best_value)","3389a7d1":"param={'lambda': 1.2809264958313406e-05, 'alpha': 1.4140161513536232e-05, 'colsample_bytree': 0.9598902643944417, 'subsample': 0.39423255537223423, 'learning_rate': 3.055933917538514e-05, 'n_estimators': 46, 'max_depth': 8, 'random_state': 2021, 'min_child_weight': 10, 'gamma': 0.9686255126368254}","8b86cfac":"param['eval_metric'] = 'auc'\nparam['booster'] = 'gbtree'\nparam['use_label_encoder'] = False\nparam","a17a4100":"model = xgb.XGBClassifier(**params)\nmodel.fit(X_train , y_train , eval_set = [(X_test , y_test)] , early_stopping_rounds = 100 , verbose = False)","fa5d6619":"ypred=model.predict(X_test)\nacc=accuracy_score(y_test, ypred) \nprc=precision_score(y_test, ypred)\nrecall=recall_score(y_test, ypred)\nf1=f1_score(y_test, ypred)\nroc_auc=roc_auc_score(y_test, ypred)\nprint(\"Accuracy  \",acc)\nprint(\"Precision  \",prc)\nprint(\"Recall  \",recall)\nprint(\"F1  \",f1)\nprint(\"ROC_AUC  \",roc_auc)\n","025ae528":"# Feature Engineering","efb583ae":"Not So Much here, (maybe in Nest dataset \ud83d\ude09), so definitely use it.\nHere also though the Evaluation Metrics More or Less remains same. The model complexity which was earlier e^13 has reduced to e^7 as Model complexity is exponentially proportional to the dimesion of the data used to train it. So in Networks with Large number of parameters, the first step after EDA should be Dimensionality reduction, keeping in minnd that we do not loose much information","87ec497d":"# Hyper Parameter Tuning","5e963a43":"# Random Forest Classification -- Baseline","0aa15077":"# Evluating The Model","bb1e3435":"##### IS feature selection worth it  **in this Dataset**.                     \nFocus on **In this dataset**, most of the time it is, and it reduces the model Complexity","09add9d3":"# Data Prepration To Feed into Classifier","e2a1d4fa":"#### L1 Based Feature Selection","489379c7":"# XGBOOST","14378973":"### Yoohoo! From 86 % to 91 %","11e229d2":"# EDA"}}