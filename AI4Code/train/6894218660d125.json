{"cell_type":{"db2705db":"code","97dad146":"code","ac450cf8":"code","230189c9":"code","dce77da5":"code","f90cdda9":"code","1d5591dc":"code","2dfa892d":"code","5b5af2f4":"code","5dd23ceb":"code","0c3600ce":"code","576c65c3":"code","ad17b54a":"code","d1f24de8":"code","d63277b1":"code","2c37f793":"code","0d0f93ea":"code","d34171e8":"code","a0a375c2":"code","fa8b2545":"code","9350cf68":"code","c61d7469":"code","6632ddc1":"code","816038e0":"code","d6fab0c8":"code","4655ec9b":"code","4428894d":"code","10c66ece":"code","28811e87":"code","85ac6316":"code","cbf1ab84":"code","db65ee8c":"code","f909d62b":"code","705f3969":"code","0f16cde7":"code","99cb32a0":"code","181c6f52":"code","b30aff59":"code","c0dc98a6":"code","6b9b6004":"code","6c626fe8":"code","29e706d3":"code","898a0794":"code","38f47a79":"code","a173bbb9":"code","96a47993":"code","590b0bd6":"code","d7f63766":"code","3d3212f5":"code","da73fb69":"code","3474f9ad":"code","fee3ad39":"code","b0d6f2d9":"code","c67a7eda":"code","de9c0227":"code","6e882cc9":"code","d40d7d5f":"code","a653bafb":"code","db5bce81":"code","43decfd1":"code","c4cc2717":"markdown","4dc7582e":"markdown","6b01e868":"markdown","86acb6a2":"markdown","3e35c1ed":"markdown","b8ce9af2":"markdown","fda92488":"markdown","b4f5e6e5":"markdown","ec16762c":"markdown","ce120596":"markdown","dd25d8ba":"markdown","3cfe1bc2":"markdown","c2dc3d5b":"markdown","141b3951":"markdown","7ca1207b":"markdown","37cc465d":"markdown","69a19c2f":"markdown","57db6135":"markdown","1abbf4eb":"markdown","123dafc8":"markdown"},"source":{"db2705db":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.base as skb\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.utils as sku\nimport sklearn.linear_model as sklm\nimport sklearn.neighbors as skn\nimport sklearn.ensemble as ske\nimport catboost as cb\nimport scipy.stats as sstats\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","97dad146":"!pip install pandas-profiling --quiet\nimport pandas_profiling as pp","ac450cf8":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","230189c9":"base = '\/kaggle\/input\/churn-risk-rate-hackerearth-ml\/'\ndata_file = base + \"train.csv\"\ndf = pd.read_csv(data_file)\ndf.head()","dce77da5":"data_file = base + \"test.csv\"\ndf_test = pd.read_csv(data_file)\ndf_test.head()","f90cdda9":"# set target feature\ntargetFeature='churn_risk_score'","1d5591dc":"# check dataset shape\ndatasetShape(df)","2dfa892d":"# remove ID from train data\ndf.drop(['customer_id'], inplace=True, axis=1)","5b5af2f4":"# check for duplicates\nprint(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","5dd23ceb":"df.info()","0c3600ce":"df_test.info()","576c65c3":"df.describe()","ad17b54a":"cont_features, cat_features = divideFeatures(df)\ncat_features.head()","d1f24de8":"# check target feature distribution\ndf[targetFeature].hist()\nplt.show()","d63277b1":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,16))\nfor i in range(len(cont_features.columns)):\n    fig.add_subplot(3, 3, i+1)\n    sns.boxplot(y=cont_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","2c37f793":"sns.pairplot(df)\nplt.show()","0d0f93ea":"# correlation heatmap for all features\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","d34171e8":"profile = pp.ProfileReport(df, title='Pandas Profiling Report', explorative=True)\nprofile.to_file(\"profile.html\")","a0a375c2":"profile.to_notebook_iframe()","fa8b2545":"skewed_features = cont_features.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","9350cf68":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nif df.isna().any().sum()>0:\n    missing, missing_perc = calc_missing(df)\n    missing.plot(kind='bar',figsize=(14,5))\n    plt.title('Missing Values')\n    plt.show()\nelse:\n    print(\"No Missing Values\")","c61d7469":"# remove all columns having no values\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"all\", inplace=True)\ndatasetShape(df)","6632ddc1":"def fillNan(df, col, value):\n    df[col].fillna(value, inplace=True)","816038e0":"# setting missing values to most occurring values\nfillNan(df, 'region_category', df['region_category'].mode()[0])\nfillNan(df_test, 'region_category', df['region_category'].mode()[0])\ndf['region_category'].isna().any()","d6fab0c8":"# setting missing values to most occurring values\nfillNan(df, 'points_in_wallet', df['points_in_wallet'].mean())\nfillNan(df_test, 'points_in_wallet', df['points_in_wallet'].mean())\ndf['points_in_wallet'].isna().any()","4655ec9b":"# setting missing values to most occurring values\nfillNan(df, 'preferred_offer_types', df['preferred_offer_types'].mode()[0])\nfillNan(df_test, 'preferred_offer_types', df['preferred_offer_types'].mode()[0])\ndf['preferred_offer_types'].isna().any()","4428894d":"# setting missing values to most occurring values\ndf['joined_through_referral'] = df['joined_through_referral'].apply(lambda x:'No' if x == '?' else x)\ndf_test['joined_through_referral'] = df_test['joined_through_referral'].apply(lambda x:'No' if x == '?' else x)\ndf['joined_through_referral'].unique()","10c66ece":"# setting missing values to most occurring values\ndf['medium_of_operation'] = df['medium_of_operation'].apply(lambda x:'Desktop' if x == '?' else x)\ndf_test['medium_of_operation'] = df_test['medium_of_operation'].apply(lambda x:'Desktop' if x == '?' else x)\ndf['medium_of_operation'].unique()","28811e87":"# setting target wrong value -1 to 1 assuming sign issue, \n# and setting 5 to 0 for training after prediction revert it back to 5\ndf['churn_risk_score'] = df['churn_risk_score'].apply(lambda x:1 if x == -1 else 0 if x == 5 else x)\ndf['churn_risk_score'].unique()","85ac6316":"# setting missing values to most occurring values\ndf['avg_frequency_login_days'] = df['avg_frequency_login_days'].apply(lambda x:0 if x == 'Error' else x)\ndf_test['avg_frequency_login_days'] = df_test['avg_frequency_login_days'].apply(lambda x:0 if x == 'Error' else x)\ndf['avg_frequency_login_days'] = pd.to_numeric(df['avg_frequency_login_days'])\ndf['avg_frequency_login_days'].describe()","cbf1ab84":"# remove non-useful features\ncolsToRemove = ['Name', 'security_no', 'referral_id', 'last_visit_time']\ndf.drop(colsToRemove, inplace=True, axis=1)\ndf_test.drop(colsToRemove, inplace=True, axis=1)\ndf.head()","db65ee8c":"print(\"Train Missing:\",df.isna().any().sum())\nprint(\"Test Missing:\",df_test.isna().any().sum())","f909d62b":"df['joining_date'] = pd.to_datetime(df['joining_date'])\ndf_test['joining_date'] = pd.to_datetime(df_test['joining_date'])","705f3969":"df['days_since_joined'] = df['joining_date'].apply(lambda x:(pd.Timestamp('today') - x).days)\ndf_test['days_since_joined'] = df_test['joining_date'].apply(lambda x:(pd.Timestamp('today') - x).days)\ndf.head()","0f16cde7":"df.drop(['joining_date'], inplace=True, axis=1)\ndf_test.drop(['joining_date'], inplace=True, axis=1)\ndf.head()","99cb32a0":"cont_features, cat_features = divideFeatures(df)\ncat_features","181c6f52":"# label encoding on categorical features\ndef mapFeature(data, f, data_test=None):\n    feat = data[f].unique()\n    feat_idx = [x for x in range(len(feat))]\n\n    data[f].replace(feat, feat_idx, inplace=True)\n    if data_test is not None:\n        data_test[f].replace(feat, feat_idx, inplace=True)","b30aff59":"for col in cat_features.columns:\n    mapFeature(df, col, df_test)\ndf_test.head()","c0dc98a6":"# # extract numerical and categorical for dummy and scaling later\n# custom_feat = ['feedback', 'complaint_status']\n# # custom_feat = ['complaint_status']\n# for feat in cat_features.columns:\n#     if len(df[feat].unique()) > 2 and feat in custom_feat:\n#         dummyVars = pd.get_dummies(df[feat], drop_first=True, prefix=feat+\"_\")\n#         df = pd.concat([df, dummyVars], axis=1)\n#         df.drop(feat, axis=1, inplace=True)\n# datasetShape(df)\n\n# df.head()","6b9b6004":"# # extract numerical and categorical for dummy and scaling later\n# for feat in cat_features.columns:\n#     if len(df_test[feat].unique()) > 2 and feat in custom_feat:\n#         dummyVars = pd.get_dummies(df_test[feat], drop_first=True, prefix=feat+\"_\")\n#         df_test = pd.concat([df_test, dummyVars], axis=1)\n#         df_test.drop(feat, axis=1, inplace=True)\n# datasetShape(df_test)\n\n# df_test.head()","6c626fe8":"# helper functions\n\ndef log1p(vec):\n    return np.log1p(abs(vec))\n\ndef expm1(x):\n    return np.expm1(x)\n\ndef clipExp(vec):\n    return np.clip(expm1(vec), 0, None)\n\ndef printScore(y_train, y_train_pred):\n    print(skm.f1_score(y_train, y_train_pred, average=\"macro\"))","29e706d3":"# shuffle samples\ndf_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ndf_y = df_shuffle.pop(targetFeature)\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.8, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","898a0794":"cont_features.drop(targetFeature, inplace=True, axis=1)\ncont_features.head()","38f47a79":"# reset index for X_train and X_test\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\nX_train.index[:5]","a173bbb9":"# scaler = skp.RobustScaler()\n# scaler = skp.MinMaxScaler()\nscaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train[cont_features.columns] = pd.DataFrame(scaler.fit_transform(X_train[cont_features.columns]), columns=cont_features.columns)\n\n# scale test data with transform()\nX_test[cont_features.columns] = pd.DataFrame(scaler.transform(X_test[cont_features.columns]), columns=cont_features.columns)\n\n# view sample data\nX_train.describe()","96a47993":"class_weights = sku.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\nclass_weights = dict(enumerate(class_weights))\nclass_weights","590b0bd6":"sample_weights = sku.class_weight.compute_sample_weight('balanced', y_train)\nsample_weights","d7f63766":"import catboost as cb\n\ncat_model = cb.CatBoostClassifier(verbose=0, iterations=100, \n#                                   eval_metric='F1', \n                                  class_weights=class_weights, \n#                                   use_best_model=True\n                                 )\ncat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\nprint(cat_model.best_score_)\n\ny_train_pred = cat_model.predict(X_train)\ny_test_pred = cat_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","3d3212f5":"rf_model = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n                                 n_estimators=100,max_depth=15, \n                                 min_samples_split = 5, min_samples_leaf = 1\n                                )\nrf_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = rf_model.predict(X_train)\ny_test_pred = rf_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","da73fb69":"import xgboost as xg","3474f9ad":"# # Grid used for parameter tuning\n# param_test1 = {\n#     'max_depth': np.arange(5, 12, 2),\n#     'learning_rate': np.arange(0.04, 0.07, 0.01)\n# }\n# xgb_cv1 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=100, objective='multi:softprob', nthread=4, seed=seed), \n#                              param_grid = param_test1, scoring='f1', n_jobs=4, \n#                              cv=5, verbose=1)\n# xgb_cv1.fit(X_train_small, y_train_small)\n# print(xgb_cv1.best_params_, xgb_cv1.best_score_)\n# # max_depth = 10\n# # learning_rate = 0.04","fee3ad39":"# # Grid used for parameter tuning\n# param_test2 = {\n#  'subsample': np.arange(0.5, 1, 0.1),\n#  'min_child_weight': range(1, 6, 1)\n# }\n# xgb_cv2 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=500, max_depth = 10, \n#                                                      objective= 'multi:softprob', nthread=4, seed=seed), \n#                             param_grid = param_test2, scoring='f1', n_jobs=4,\n#                             cv=5, verbose=1)\n# xgb_cv2.fit(X_train_small, y_train_small)\n# print(xgb_cv2.best_params_, xgb_cv2.best_score_)\n# print(xgb_cv2.best_estimator_)\n# # subsample = 0.5\n# # min_child_weight = 2","b0d6f2d9":"xgb_model = xg.XGBClassifier(objective ='multi:softprob', random_state=seed, verbose=0, scoring='f1', \n                             learning_rate=0.001, subsample=0.5, n_jobs=-1, \n                             n_estimators=100, max_depth = 10)\nxgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb_model.predict(X_train)\ny_test_pred = xgb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","c67a7eda":"import lightgbm as lgb\nlgb_model = lgb.LGBMClassifier(objective='multi', class_weight=class_weights, random_state=1, n_jobs=-1, \n                               learning_rate=0.15, \n                               n_estimators=100)\nlgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = lgb_model.predict(X_train)\ny_test_pred = lgb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","de9c0227":"# Generate Ensembles\n\ndef rmse_cv(model):\n    '''\n    Use this function to get quickly the rmse score over a cv\n    '''\n    rmse = np.sqrt(-skms.cross_val_score(model, X_train, y_train, \n                                         scoring=\"neg_mean_squared_error\", cv = 5, n_jobs=-1))\n    return rmse\n\nclass MixModel(skb.BaseEstimator, skb.RegressorMixin, skb.TransformerMixin):\n    '''\n    Here we will get a set of models as parameter already trained and \n    will calculate the mean of the predictions for using each model predictions\n    '''\n    def __init__(self, algs):\n        self.algs = algs\n\n    # Define clones of parameters models\n    def fit(self, X, y):\n        self.algs_ = [skb.clone(x) for x in self.algs]\n        \n        # Train cloned base models\n        for alg in self.algs_:\n            alg.fit(X, y)\n\n        return self\n    \n    # Average predictions of all cloned models\n    def predict(self, X):\n        predictions = np.column_stack([\n            stacked_model.predict(X) for stacked_model in self.algs_\n        ])\n        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=predictions)","6e882cc9":"mixed_model = MixModel(algs = [\n    cat_model,\n    rf_model,\n    xgb_model,\n    lgb_model\n])\n# score = rmse_cv(mixed_model)\n# print(\"\\nAveraged base algs score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nmixed_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = mixed_model.predict(X_train)\ny_test_pred = mixed_model.predict(X_test)\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","d40d7d5f":"def getTestResults():\n    df_final = df.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df_final.columns if targetFeature not in x]\n    df_final_test = df_test[test_cols]\n    df_y = df_final.pop(targetFeature)\n    df_X = df_final\n\n    scaler = skp.RobustScaler()\n#     scaler = skp.MinMaxScaler()\n#     scaler = skp.StandardScaler()\n\n    df_X[cont_features.columns] = pd.DataFrame(scaler.fit_transform(df_X[cont_features.columns]), columns=cont_features.columns)\n    df_final_test[cont_features.columns] = pd.DataFrame(scaler.transform(df_final_test[cont_features.columns]), columns=cont_features.columns)\n\n#     sample_weights = sku.class_weight.compute_sample_weight('balanced', df_y)\n    \n    model = MixModel(algs = [\n        cat_model,\n        rf_model,\n        xgb_model,\n        lgb_model\n    ])\n\n    model.fit(df_X, df_y)\n\n    # predict\n    y_train_pred = model.predict(df_X)\n    y_test_pred = model.predict(df_final_test)\n    print(\"Accuracy Score for Train:\",skm.accuracy_score(df_y, y_train_pred))\n    printScore(df_y, y_train_pred)\n    return y_test_pred\n\n# ML models\nresults = getTestResults()","a653bafb":"submission = pd.DataFrame({\n    'customer_id': df_test['customer_id'],\n    targetFeature: results.ravel(),\n})\nprint(submission[targetFeature].value_counts())","db5bce81":"# revert back 0 to 5 for predictions\nsubmission[targetFeature] = submission[targetFeature].apply(lambda x:5 if x == 0 else x)\nsubmission[targetFeature].value_counts()","43decfd1":"submission.to_csv('.\/submission_Ensemble4.csv', index=False)","c4cc2717":"### Skewness","4dc7582e":"# Step 2: EDA","6b01e868":"## Create Dummy Features","86acb6a2":"### Profiling for Whole Data","3e35c1ed":"### RandomForest","b8ce9af2":"# Predict the churn risk rate - HackerEarth ML","fda92488":"### One-Hot Encoding\nOne hot encoding didn't work well.","b4f5e6e5":"# Step 3: Data Preparation","ec16762c":"### Handle Missing","ce120596":"## Model Building","dd25d8ba":"### XGBoost","3cfe1bc2":"### Feature Scaling","c2dc3d5b":"## Derive Features","141b3951":"### LightGBM","7ca1207b":"# Step 5: Test Evaluation & Submission","37cc465d":"# Step 1: Reading and Understanding the Data","69a19c2f":"# Step 4: Data Modelling\n\n### Split Train-Test Data","57db6135":"### CatBoost","1abbf4eb":"With this ensemble of four best classifiers, 76.56 LB is scored.","123dafc8":"### Univariate Analysis"}}