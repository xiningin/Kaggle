{"cell_type":{"bbfaff78":"code","5ae392d4":"code","4b7c4fe1":"code","34321b2c":"code","d196495c":"code","f4233b3a":"code","241562bd":"code","457e6f4c":"code","1e0eff71":"code","b9f02fbd":"code","d2346766":"code","955b48a6":"code","0d4c2187":"code","6ea3d764":"code","a39276e9":"code","415d868e":"code","70fa3846":"code","f6bbbd99":"code","d3e70bfc":"code","2ef3729f":"code","71a24930":"code","a25bbe61":"code","5ac613e5":"code","8a14677c":"code","cbb5f4b0":"code","f7e5dd0f":"code","74ed17a5":"markdown","23b28189":"markdown","7f0b0af3":"markdown","3d58e996":"markdown","a056f538":"markdown","6e8874d5":"markdown","4f5d2cd5":"markdown","a1b2c913":"markdown","6828db10":"markdown","509927e0":"markdown","780d0a20":"markdown","3a52893d":"markdown","938caae1":"markdown","e7ffc9cf":"markdown"},"source":{"bbfaff78":"!git clone https:\/\/github.com\/tensorflow\/models\n    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n!cd models ","5ae392d4":"%%bash\ncd models\/research\n\n# Compile protos.\nprotoc object_detection\/protos\/*.proto --python_out=.\n\n# Install TensorFlow Object Detection API.\n# Note: I fixed the version of some dependencies to make it work on Kaggle notebook. In particular:\n# * scipy==1.6.3 to avoid the missing GLIBCXX_3.4.26 error\n# * tensorflow to 2.6.0 to make it compatible with the CUDA version preinstalled on Kaggle.\n# When Kaggle notebook upgrade to TF 2.7, you can use the default setup.py script:\n# cp object_detection\/packages\/tf2\/setup.py .\nwget https:\/\/storage.googleapis.com\/odml-dataset\/others\/setup.py\npip install -q --user .\n\n# Test if the Object Dectection API is working correctly\npython object_detection\/builders\/model_builder_tf2_test.py","4b7c4fe1":"import contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\n\nfrom PIL import Image, ImageDraw\n\n# Import the library that is used to submit the prediction result.\nINPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nsys.path.append(INPUT_DIR)\nimport greatbarrierreef","34321b2c":"# The notebook is supposed to run with TF 2.6.0\nprint(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","d196495c":"data_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\ndata_df.head()","f4233b3a":"TRAINING_RATIO = 0.8\n\ndata_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\n# Split the dataset so that no sequence is leaked from the training dataset into the validation dataset.\nsplit_index = int(TRAINING_RATIO * len(data_df))\nwhile data_df.iloc[split_index - 1].sequence == data_df.iloc[split_index].sequence:\n    split_index += 1\n\n# Shuffle both the training and validation datasets.\ntrain_data_df = data_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\nval_data_df = data_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\ntrain_positive_count = len(train_data_df[train_data_df.annotations != '[]'])\nval_positive_count = len(val_data_df[val_data_df.annotations != '[]'])\n\nprint('Training ratio (all samples):', \n      float(len(train_data_df)) \/ (len(train_data_df) + len(val_data_df)))\nprint('Training ratio (positive samples):', \n      float(train_positive_count) \/ (train_positive_count + val_positive_count))","241562bd":"\n\n# Take only the positive images for training and validation\ntrain_data_df = train_data_df[train_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for training:', len(train_data_df))\nval_data_df = val_data_df[val_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for validation:', len(val_data_df))\n\n","457e6f4c":"def image_with_annotation(video_id, video_frame, data_df, image_path):\n    \"\"\"Visualize annotations of a given image.\"\"\"\n    full_path = os.path.join(image_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    image = Image.open(full_path)\n    draw = ImageDraw.Draw(image)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            draw.rectangle((\n                annotation['x'], \n                annotation['y'],\n                (annotation['x'] + annotation['width']), \n                (annotation['y'] + annotation['height']),\n                ), outline=(255, 255, 0))\n        \n    buf = io.BytesIO()\n    image.save(buf, 'PNG')\n    data = buf.getvalue()\n\n    return data\n\n# Test visualization of a randomly selected image\nimage_path = os.path.join(INPUT_DIR, 'train_images')\ntest_index = 4\nvideo_id = train_data_df.iloc[test_index].video_id\nvideo_frame = train_data_df.iloc[test_index].video_frame\nIPython.display.Image(image_with_annotation(video_id, video_frame, data_df, image_path))","1e0eff71":"BytesList = tf.train.BytesList\nFloatList = tf.train.FloatList\nIntList = tf.train.Int64List\n\nfrom object_detection.utils import dataset_util\n\ndef image_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    return tf.train.Feature(\n        bytes_list=BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n    )\n\ndef bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    return tf.train.Feature(bytes_list=BytesList(value=[value.encode()]))\n\ndef bytes_feature_list(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    return tf.train.Feature(bytes_list=BytesList(value=value))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float \/ double.\"\"\"\n  return tf.train.Feature(float_list=FloatList(value=[value]))\n\n\ndef float_feature_list(value):\n    \"\"\"Returns a list of float_list from a float \/ double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n  return tf.train.Feature(int64_list=IntList(value=[value]))\n\ndef _int64_feature_list(value):\n  \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n  return tf.train.Feature(int64_list=IntList(value=value))\n\n\n\ndef create_tf_example(video_id, video_frame, data_df, image_dir_path):\n    full_path = os.path.join(image_dir_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    img = open(full_path,'rb').read()\n    filename = f'{video_id}:{video_frame}'.encode('utf8')\n    image = Image.open(full_path)\n    image_format = 'jpeg'.encode('utf8')\n    \n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    \n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n    \n    \n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            xmins.append(annotation['x'] \/ width) \n            xmaxs.append((annotation['x'] + annotation['width']) \/ width) \n            ymins.append(annotation['y'] \/ height) \n            ymaxs.append((annotation['y'] + annotation['height']) \/ height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n            \n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image\/height': dataset_util.int64_feature(height),\n      'image\/width': dataset_util.int64_feature(width),\n      'image\/filename': dataset_util.bytes_feature(filename),\n      'image\/source_id': dataset_util.bytes_feature(filename),\n      'image\/encoded': dataset_util.bytes_feature(img),\n      'image\/format': dataset_util.bytes_feature(image_format),\n      'image\/object\/bbox\/xmin': dataset_util.float_list_feature(xmins),\n      'image\/object\/bbox\/xmax': dataset_util.float_list_feature(xmaxs),\n      'image\/object\/bbox\/ymin': dataset_util.float_list_feature(ymins),\n      'image\/object\/bbox\/ymax': dataset_util.float_list_feature(ymaxs),\n      'image\/object\/class\/text': dataset_util.bytes_list_feature(classes_text),\n      'image\/object\/class\/label': dataset_util.int64_list_feature(classes),\n    }))\n    \n    return tf_example\n            \n# tf_example = create_tf_example(0, 13, data_df, os.path.join(INPUT_DIR, 'train_images'))\n\ndef convert_to_tfrecord(data_df, tf_record_file, image_dir_path):\n    with tf.io.TFRecordWriter(tf_record_file) as writer:\n        for index, row in data_df.iterrows():\n            if index % 500 == 0:\n                print('Processed {0} images.'.format(index))\n            tf_example = create_tf_example(row.video_id, row.video_frame, data_df, image_path)\n            writer.write(tf_example.SerializeToString())\n        print('Completed processing {0} images.'.format(len(data_df)))\n\n        \nimage_path = os.path.join(INPUT_DIR, 'train_images')\n!mkdir dataset\n\nprint('Converting TRAIN images...')\nconvert_to_tfrecord(train_data_df, 'dataset\/cots_train.tfrecord',image_path)\n\nprint('Converting validation images...')\nconvert_to_tfrecord(val_data_df, 'dataset\/cots_val.tfrecord',image_path)","b9f02fbd":"# Create a label map to map between label index and human-readable label name.\n\nlabel_map_str = \"\"\"item {\n  id: 1\n  name: 'COTS'\n}\"\"\"\n\nwith open('dataset\/label_map.pbtxt', 'w') as f:\n  f.write(label_map_str)\n\n!more dataset\/label_map.pbtxt","d2346766":"!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/efficientdet_d2_coco17_tpu-32.tar.gz\n!tar -xvzf efficientdet_d2_coco17_tpu-32.tar.gz","955b48a6":"!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/efficientdet_d0_coco17_tpu-32.tar.gz\n!tar -xvzf efficientdet_d0_coco17_tpu-32.tar.gz","0d4c2187":"example1 = \"efficientdet_d2_coco17_tpu-32\/pipeline.config\"\nfile1 = open(example1, \"r\") \nFileContent = file1.read()\nprint(FileContent)","6ea3d764":"example2 = \"efficientdet_d0_coco17_tpu-32\/pipeline.config\"\nfile1 = open(example2, \"r\") \nFileContent = file1.read()\nprint(FileContent)","a39276e9":"from string import Template\n\nconfig_file_template = \"\"\"\n# SSD with EfficientNet-b0 + BiFPN feature extractor,\n# shared box predictor and focal loss (a.k.a EfficientDet-d0).\n# See EfficientDet, Tan et al, https:\/\/arxiv.org\/abs\/1911.09070\n# See Lin et al, https:\/\/arxiv.org\/abs\/1708.02002\n# Initialized from an EfficientDet-D0 checkpoint.\n#\n# Train on GPU\n\nmodel {\n  ssd {\n    num_classes: 1\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    \n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 1280\n        max_dimension: 1280\n        pad_to_max_dimension: true\n      }\n    }\n    feature_extractor {\n      type: \"ssd_efficientnet-b2_bifpn_keras\"\n      conv_hyperparams {\n        regularizer {\n          l2_regularizer {\n            weight: 3.9999998989515007e-05\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            mean: 0.0\n            stddev: 0.029999999329447746\n          }\n        }\n        activation: SWISH\n        batch_norm {\n          decay: 0.9900000095367432\n          scale: true\n          epsilon: 0.0010000000474974513\n        }\n        force_use_bias: true\n      }\n      bifpn {\n        min_level: 3\n        max_level: 7\n        num_iterations: 5\n        num_filters: 112\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n              weight: 3.9999998989515007e-05\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              mean: 0.0\n              stddev: 0.01\n            }\n          }\n          activation: SWISH\n          batch_norm {\n            decay: 0.9900000095367432\n            scale: true\n            epsilon: 0.0010000000474974513\n          }\n          force_use_bias: true\n        }\n        depth: 112\n        num_layers_before_predictor: 3\n        kernel_size: 3\n        class_prediction_bias_init: -4.599999904632568\n        use_depthwise: true\n      }\n    }\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n        aspect_ratios: 0.5\n        scales_per_octave: 3\n      }\n    }\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 9.99999993922529e-09\n        iou_threshold: 0.5\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n    normalize_loss_by_num_matches: true\n    loss {\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 1.5\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    encode_background_as_zeros: true\n    normalize_loc_loss_by_codesize: true\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    add_background_class: false\n  }\n}\ntrain_config {\n  batch_size: 2\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_scale_crop_and_pad_to_square {\n      output_size: 1280\n      scale_min: 0.5\n      scale_max: 2.0\n    }\n  }\n  sync_replicas: false\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: 5e-3\n          total_steps: $training_steps\n          warmup_learning_rate: 5e-4\n          warmup_steps: $warmup_steps\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: \"efficientdet_d2_coco17_tpu-32\/checkpoint\/ckpt-0\"\n  num_steps: $training_steps\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 1\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: \"detection\"\n  use_bfloat16: false\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader: {\n  label_map_path: \"dataset\/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"dataset\/cots_train.tfrecord\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 1;\n}\n\neval_input_reader: {\n  label_map_path: \"dataset\/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"dataset\/cots_val.tfrecord\"\n  }\n}\n\n\"\"\"","415d868e":"# Define the training pipeline\n\nTRAINING_STEPS = 20000\nWARMUP_STEPS = 2000\nPIPELINE_CONFIG_PATH='dataset\/pipeline.config'\n\npipeline = Template(config_file_template).substitute(\n    training_steps=TRAINING_STEPS, warmup_steps=WARMUP_STEPS)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","70fa3846":"MODEL_DIR='cots_efficientdet_d2'\n!mkdir {MODEL_DIR}\n!python models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --alsologtostderr","f6bbbd99":"print(\"k\")","d3e70bfc":"!python models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --checkpoint_dir={MODEL_DIR} \\\n    --eval_timeout=0 \\\n    --alsologtostderr","2ef3729f":"!python models\/research\/object_detection\/exporter_main_v2.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --trained_checkpoint_dir={MODEL_DIR} \\\n    --output_directory={MODEL_DIR}\/output","71a24930":"!ls {MODEL_DIR}\/output","a25bbe61":"# Load the TensorFlow COTS detection model into memory.\nstart_time = time.time()\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(MODEL_DIR, 'output'), 'saved_model'))\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Elapsed time: ' + str(elapsed_time) + 's')","5ac613e5":"# Define some utils method for prediction.\n\ndef load_image_into_numpy_array(path):\n    \"\"\"Load an image from file into a numpy array.\n\n    Puts image into numpy array to feed into tensorflow graph.\n    Note that by convention we put it into a numpy array with shape\n    (height, width, channels), where channels=3 for RGB.\n\n    Args:\n    path: a file path (this can be local or on colossus)\n\n    Returns:\n    uint8 numpy array with shape (img_height, img_width, 3)\n    \"\"\"\n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(io.BytesIO(img_data))\n    (im_width, im_height) = image.size\n    \n    return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ndef detect(image_np):\n    \"\"\"Detect COTS from a given numpy image.\"\"\"\n\n    input_tensor = np.expand_dims(image_np, 0)\n    start_time = time.time()\n    detections = detect_fn_tf_odt(input_tensor)\n    return detections","8a14677c":"import greatbarrierreef\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","cbb5f4b0":"DETECTION_THRESHOLD = 0.3\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n    height, width, _ = image_np.shape\n    \n    # Run object detection using the TensorFlow model.\n    detections = detect(image_np)\n    \n    # Parse the detection result and generate a prediction string.\n    num_detections = detections['num_detections'][0].numpy().astype(np.int32)\n    predictions = []\n    for index in range(num_detections):\n        score = detections['detection_scores'][0][index].numpy()\n        if score < DETECTION_THRESHOLD:\n            continue\n\n        bbox = detections['detection_boxes'][0][index].numpy()\n        y_min = int(bbox[0] * height)\n        x_min = int(bbox[1] * width)\n        y_max = int(bbox[2] * height)\n        x_max = int(bbox[3] * width)\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    # Generate the submission data.\n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","f7e5dd0f":"# Remove the dataset files to save space.\n!rm -rf dataset\n!rm -rf train_images\n!rm tensorflow-great-barrier-reef.zip\n\n# Remove other data downloaded during training.\n!rm -rf models\n!rm efficientdet_d0_coco17_tpu-32.tar.gz","74ed17a5":"# Run inference on test images and create the submission file","23b28189":"## **tf_record**\nModels based on the TensorFlow object detection API need a special format for all input data, called TFRecord file format, Tensorflow\u2019s own binary storage format.\n\nIf you are working with large datasets, using a binary file format for storage of your data can have a significant impact on the performance of your import pipeline and as a consequence on the training time of your model. Binary data takes up less space on disk, takes less time to copy and can be read much more efficiently from disk.\n\nIt is optimized for use with Tensorflow in multiple ways. To start with, it makes it easy to combine multiple datasets and integrates seamlessly with the data import and preprocessing functionality provided by the library.Especially for datasets that are too large to be stored fully in memory this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed.\n\n#### A TFRecord file contains a sequence of records. The file can only be read sequentially.\n**[Official Tutorial](https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord)**","7f0b0af3":"Split the `train` folder into training dataset and validation dataset. ","3d58e996":"# Import dependencies","a056f538":"\n\nTo save time for demonstration purpose, here we'll only take the positive images (images that contain at least 1 starfish) for training. TensorFlow Object Detection API will take the areas in the images that aren't annotated as containing a starfish to use as negative samples.\n","6e8874d5":"### **Model configuration**\nWe downloaded and extracted a pre-trained model of our choice. Now we want to configure it.\n\n**Model configuration** is a process that lets us tailor model-related artifacts (e.g. hyperparameters, loss function, etc) so that it can be trained (fine-tuned) to tackle detection for the objects that we\u2019re interested in. That\u2019s it.\n\nThe TensorFlow Object Detection API allows model configuration via the **pipeline.config** file that goes along with the pre-trained model. ","4f5d2cd5":"# Clean up","a1b2c913":"# Prepare the training dataset","6828db10":"# Train an object detection model\n\n### **Model selection**\n\nOne of the coolest features of the TensorFlow Object Detection API is the opportunity to work with a set of state of the art models, pre-trained on the COCO dataset! We can fine-tune these models for our purposes and get great results.\n\nWe'll use [TensorFlow Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection) and an EfficientDet-D4 model and apply transfer learning to train a COTS detection model. You can probably increase accuracy by switch to using a larger EfficientDet model.","509927e0":"# Install TensorFlow Object Detection API\n\nPip may report some dependency errors. You can safely ignore these errors and proceed if all tests in `model_builder_tf2_test.py` passed. ","780d0a20":"Visualize one randomly selected image from the training data to see if the annotation looks correct.","3a52893d":"[Original Notebook](https:\/\/www.kaggle.com\/khanhlvg\/cots-detection-w-tensorflow-object-detection-api)\n\nThis notebook contains code to train a crown-of-thorns starfish (COTS) detection model to serve as a baseline model for [this competition](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/overview). We use [TensorFlow Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection) to apply transfer learning on an [EfficientDet-D0](https:\/\/arxiv.org\/abs\/1911.09070) pretrained model. ","938caae1":"# Evaluate the object detection model","e7ffc9cf":"# Export as SavedModel for inference"}}