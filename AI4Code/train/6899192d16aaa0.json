{"cell_type":{"6a4e88d0":"code","8e0d2eca":"code","32218610":"code","ae3f2afb":"code","6eb4ab3b":"code","3495cabb":"code","fd0c7607":"code","724bfd2a":"code","9ef67c37":"code","1370ae92":"code","5652dd17":"code","f80e4bfa":"code","a5e4fdef":"code","ac0b9579":"code","c111bce7":"code","f0489872":"code","76c2ee16":"code","78f51865":"code","5ec7e1ca":"code","3294d561":"code","c8dac8d5":"code","31356384":"code","6c111611":"code","c1430c1a":"code","663e0699":"code","2a3d66a7":"code","7d61802c":"code","ab8a6c73":"code","7be9296b":"code","9968286f":"code","cb2c4a2e":"code","0c22f06b":"code","9aa214bb":"code","d013f11c":"code","363105b7":"code","a885e56a":"code","53e84dd2":"code","ee3c0546":"code","cd3678f6":"code","33dda036":"code","e43d9a74":"code","3606a0f8":"code","a8a6afcb":"code","37783d8c":"code","0f63daf7":"code","2c259983":"code","e4205bf0":"code","f9292a3f":"code","d812073a":"code","157b6bdc":"code","c7547aff":"code","dab15c42":"code","5c0861a5":"code","c6b7809e":"code","6c5a628d":"code","17a3df7f":"code","c6d82374":"code","96dc1439":"code","b5a0ed59":"code","d83d1efe":"code","8011ac4f":"code","9c0f53aa":"code","5be7cb39":"code","0ad1b2cc":"code","91982786":"code","0a56ece2":"code","18d019ed":"code","d9f18783":"code","4f59b51b":"code","16db7ea9":"code","a779e303":"code","dfb0f85b":"code","6e560049":"code","48048f0b":"code","1d9e1d05":"code","445b19be":"code","90cbb2fb":"code","f95d9f32":"code","1ba9ef5f":"code","3fe696cc":"code","693da9a5":"code","bb2ba931":"code","812599fe":"code","345df26e":"code","134eb723":"code","7a3d30d7":"markdown","8084bb26":"markdown","b7827be9":"markdown","5b7d261f":"markdown","9992fa5d":"markdown","af079e11":"markdown","bf4f81f4":"markdown","29c64f70":"markdown","bd1a7972":"markdown","3588ce55":"markdown","c3c0b0ed":"markdown","722ab862":"markdown","e411ba4f":"markdown","e821184f":"markdown","e9029d74":"markdown","3d1ccd1d":"markdown","cc823861":"markdown","54b28f4f":"markdown","88ace5ed":"markdown","eab1b118":"markdown","19b71f66":"markdown","e453f9a1":"markdown","3b56da77":"markdown","96e97be2":"markdown","46d279d8":"markdown","f39ca891":"markdown","b9913394":"markdown","59fe536a":"markdown","e602b427":"markdown","08372a2b":"markdown","a611b800":"markdown","94e5dfc3":"markdown","0fecbfb3":"markdown","a184f50d":"markdown","b3cec50a":"markdown","fc2d7d3c":"markdown","f5964c12":"markdown","19c94ba4":"markdown","2a8819c5":"markdown","f07caba0":"markdown","c82d8476":"markdown","69cb987a":"markdown","1cf66209":"markdown","c3275a0b":"markdown","69e437e3":"markdown","241200b3":"markdown","02cc2008":"markdown","146f9d36":"markdown","d5c09abb":"markdown"},"source":{"6a4e88d0":"#Load ML libraries\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\n#Load DL libraries\nimport keras\nimport sys\nfrom keras.layers import LSTM, Dense, Dropout, Activation\nfrom keras.models import Sequential\n#from keras.layers.core import Dense, Dropout, Activation\nimport lightgbm as lgb\n\nfrom keras.preprocessing.sequence import TimeseriesGenerator\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8e0d2eca":"retail_club = pd.read_csv('\/kaggle\/input\/retailer-data\/0029977001001UA096_ST10.csv')\nretail_club.head()","32218610":"print(retail_club.iloc[0])","ae3f2afb":"print(retail_club.iloc[114])","6eb4ab3b":"retail_df = retail_club.iloc[0:115]\nprint('First date record of retail data: \\n', retail_df.head(1).StartDate)\nprint('\\n')\nprint('Last date record of retail data: \\n',retail_df.tail(1).StartDate)","3495cabb":"#Transform string to date\nretail_df['StartDate'] = pd.to_datetime(retail_df.StartDate, format=\"%d-%b-%y\")\n\n#Extracting Year\nretail_df['Year'] = retail_df['StartDate'].dt.year","fd0c7607":"f, ax = plt.subplots(1,1, figsize=(20,8))\nplot = sns.lineplot(x='StartDate', y='Base', data=retail_df)\nplot.set(title='Quarterly Sales')","724bfd2a":"retail_df.corr()","9ef67c37":"retail_data = retail_df[[\"StartDate\", \"Type - 1\", \"Type - 2 DNR\", \"Base\"]]\nretail_data.shape","1370ae92":"retail_data.isnull().sum()","5652dd17":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=2)\nretail_data_imputation = imputer.fit_transform(retail_data.drop(columns=[\"StartDate\"]))\nretail_imputed_df = pd.DataFrame(data=retail_data_imputation, columns=[\"Type - 1\", \"Type - 2 DNR\",\t\"Base\"])\n#retail_impute_df[\"Week\"] = retail_impute_df[\"Week\"].astype('int64')\nretail_imputed_df.head()","f80e4bfa":"print('After Imputation: \\n')\nprint(retail_imputed_df.isnull().sum(), '\\n')","a5e4fdef":"retail_imputed_df.describe()","ac0b9579":"sns.pairplot(retail_imputed_df, diag_kind='kde')","c111bce7":"## Univariate Analysis using boxplot\n## Checking the presence of outliers\npos = 1\nplt.figure(figsize=(12,8))\nfor i in retail_imputed_df.columns:\n    plt.subplot(2, 2, pos)\n    sns.boxplot(retail_imputed_df[i],color=\"red\")\n    pos += 1","f0489872":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(retail_imputed_df))\nprint('Z-score of column values:\\n', z)","76c2ee16":"# Setting threshold to identify an outlier\nthreshold = 3\nprint(np.where(z > 3))","78f51865":"print('Below are outliers in retail data: \\n')\nz[101][0], z[101][2], z[102][0], z[102][2], z[114][1]","5ec7e1ca":"print('Shape of retail data with outliers:', retail_imputed_df.shape)\nretail_z_nonout = retail_imputed_df[(z < 3).all(axis=1)]\nprint('Shape of retail data after removing outliers using z-score:', retail_z_nonout.shape)\nretail_z_nonout.info()","3294d561":"pos = 1\nplt.figure(figsize=(12,8))\nfor i in retail_z_nonout.columns:\n    plt.subplot(2, 2, pos)\n    sns.boxplot(retail_z_nonout[i],color=\"magenta\")\n    pos += 1","c8dac8d5":"## Detect outliers using IQR and Handling outliers\nQ1 = retail_z_nonout.quantile(0.25)\nQ3 = retail_z_nonout.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","31356384":"## Checking for outliers presence of data points with \"True\"\nbool_outs= (retail_z_nonout < (Q1 - 1.5 * IQR)) |(retail_z_nonout > (Q3 + 1.5 * IQR))\nprint(bool_outs)","6c111611":"## Removing outliers from dataframe\nretail_final = retail_z_nonout[~bool_outs.any(axis=1)]\nprint('Shape of dataframe with outliers: {}'.format(retail_z_nonout.shape))\nprint('Shape of dataframe without outliers: {}'.format(retail_final.shape))","c1430c1a":"# Checking correlation to remove unnecessary columns from dataset\nretail_final.corr()","663e0699":"retail_model = retail_final[[\"Type - 1\", \"Base\"]]\nretail_model.head()","2a3d66a7":"pos = 1\nplt.figure(figsize=(10,5))\nfor i in retail_model.columns:\n    plt.subplot(2, 2, pos)\n    sns.boxplot(retail_model[i],color=\"green\")\n    pos += 1","7d61802c":"# Lag features and Shift index\nfor i in range(1,3):\n    lag_i = 'lag_' + str(i)\n    retail_model[lag_i] = retail_model.Base.shift(i)\n    \n# Rolling window\nretail_model['rolling_mean'] = retail_model.Base.rolling(window=2).mean()\nretail_model['rolling_max'] = retail_model.Base.rolling(window=2).max()\nretail_model['rolling_min'] = retail_model.Base.rolling(window=2).min()","ab8a6c73":"# Correlation matrix with heatmap\ncorr = retail_model.corr()\nfig = plt.figure(figsize=(6,4))\nsns.heatmap(corr, linewidths=.5)","7be9296b":"retail_model.corr()","9968286f":"retail_model_final = retail_model[[\"Type - 1\", \"rolling_mean\",\t\"rolling_max\", \"rolling_min\", \"Base\"]]\nprint('Shape:', retail_model_final.shape)\nretail_model_final.head()","cb2c4a2e":"retail_model_final.mean()","0c22f06b":"retail_model_final.fillna(retail_model_final.mean(), inplace = True)\nretail_model_final.head()","9aa214bb":"sns.pairplot(retail_model_final, diag_kind = 'kde')","d013f11c":"retail_model_final.shape","363105b7":"# Split the time series data (Train-78, Test-20)\n#retail_data = retail_data.set_index('StartDate')\nprint('Total records in dataset:', len(retail_model_final))\nretail_train = retail_model_final.iloc[0:78]               \nretail_test = retail_model_final.iloc[78:]\n\nretail_pred_train = retail_model_final.iloc[0:78]               \nretail_pred_test = retail_model_final.iloc[78:]\nprint('Total records in Training set:', len(retail_train))\nprint('Total records in Test set:', len(retail_test))","a885e56a":"retail_train['Base'].plot(legend=True,label='TRAIN (80%)')\nretail_test['Base'].plot(legend=True,label='TEST(20%)',figsize=(12,8));","53e84dd2":"X_train = retail_train.drop(['Base'], axis=1)\ny_train = retail_train['Base'].values\n\nX_test = retail_test.drop(['Base'], axis=1)\ny_test = retail_test['Base'].values","ee3c0546":"X_pred_train = retail_pred_train.drop(['Base'], axis=1)\ny_pred_train = retail_pred_train['Base'].values\n\nX_pred_test = retail_pred_test.drop(['Base'], axis=1)\ny_pred_test = retail_pred_test['Base'].values","cd3678f6":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import ExtraTreesRegressor","33dda036":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\nses_model = SimpleExpSmoothing(retail_train['Base']).fit(smoothing_level=0.3)\nses_preds = ses_model.forecast(steps = 20) #ses_model.predict(start=retail_test.index[0], end=retail_test.index[len(y_test)-1])","e43d9a74":"ses_errors_df = retail_test[['Base']]\nses_errors_df['Predicted_Base'] = ses_preds\nses_errors_df['Error'] = ses_preds - y_test\nses_errors_df.insert(0, 'Modelname', 'Holtman- SES')\nses_errors_df.head()","3606a0f8":"#retail_train['Base'].plot(legend=True,label='TRAIN')\n#retail_test['Base'].plot(legend=True,label='TEST',figsize=(12,8))\n#ses_predict[\"Forecasted_Base\"].plot(legend=True,label='PREDICTIONS')\n#plt.title('Forecast using HoltWinter-SES model')","a8a6afcb":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\ndouble_model = ExponentialSmoothing(retail_train['Base'],trend='add').fit()\ndoublemodel_preds = double_model.forecast(20).rename('DES Forecast')","37783d8c":"des_errors_df = retail_test[['Base']]\ndes_errors_df['Predicted_Base'] = doublemodel_preds\ndes_errors_df['Error'] = doublemodel_preds - y_test\ndes_errors_df.insert(0, 'Modelname', 'Holtman- DES')\ndes_errors_df.head()","0f63daf7":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\ntriple_model = ExponentialSmoothing(retail_train['Base'],trend='add', seasonal= 'mul',seasonal_periods= 12).fit(use_boxcox=False, remove_bias=False)\ntes_forecast = triple_model.forecast(20).rename('TES_Forecast')\n#triplemodel_preds = pd.Series(triple_model.predict(start=retail_test.index[0], end=retail_test.index[len(y_test-1)]), \n                       #name='Forecast_Sales').reset_index()","2c259983":"tes_errors_df = retail_test[['Base']]\ntes_errors_df['Predicted_Base'] = tes_forecast\ntes_errors_df['Error'] = tes_forecast - y_test\ntes_errors_df.insert(0, 'Modelname', 'Holtman- TES')\ntes_errors_df.head()","e4205bf0":"# Evaluate predictions for Holt Winters-Double Exponential Smoothing\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(des_errors_df.index, des_errors_df['Predicted_Base'], label='Forecast - HW-DES')\nplt.legend(loc='best')\nplt.xlabel('StartDate')\nplt.ylabel('Base')\nplt.title('Forecast using Holt Winters-Double Exponential Smoothing')\nplt.show()","f9292a3f":"# Evaluate predictions for Holt Winters-Triple Exponential Smoothing\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(tes_errors_df.index, tes_errors_df['Predicted_Base'], label='Forecast - HW-TES')\nplt.legend(loc='best')\nplt.xlabel('StartDate')\nplt.ylabel('Base')\nplt.title('Forecast using Holt Winters-Triple Exponential Smoothing')\nplt.show()","d812073a":"def mae(err):\n    return np.mean(np.abs(err))\n\ndef rmse(err):\n    return np.sqrt(np.mean(err ** 2))\n\ndef mape(err, sales=tes_errors_df['Base']):\n    return np.sum(np.abs(err))\/np.sum(sales) * 100","157b6bdc":"# fit model\netr_model = ExtraTreesRegressor(n_estimators=100)\netr_model.fit(X_train, y_train)","c7547aff":"etr_preds = etr_model.predict(X_test)\nprint('Prediction is done..')","dab15c42":"print('Model Score at Train set: {:.2%}'.format(etr_model.score(X_train, y_train)))\nprint('Model Score at Test set: {:.2%}'.format(etr_model.score(X_test, y_test)))","5c0861a5":"etr_errors_df = retail_test[['Base']]\netr_errors_df['Predicted_Base'] = etr_preds\netr_errors_df['Error'] = etr_preds - y_test\netr_errors_df.insert(0, 'Modelname', 'ExtreeTreesRegressor')\netr_errors_df.head()","c6b7809e":"# Evaluate predictions for Extra Tree Regressor\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(etr_errors_df.index, etr_errors_df['Predicted_Base'], label='Forecast - ExtraTreesRegressor')\nplt.legend(loc='best')\nplt.xlabel('StartDate')\nplt.ylabel('Base')\nplt.title('Forecast using ExtraTreesRegressor model')\nplt.show()","6c5a628d":"fig = plt.figure(figsize=(14,7))\nplt.plot(etr_errors_df.index, etr_errors_df.Error, label='Error')\nplt.plot(etr_errors_df.index, etr_errors_df.Base, label='Actual Sales')\nplt.plot(etr_errors_df.index, etr_errors_df.Predicted_Base, label='Forecasted Sales')\nplt.legend(loc='best')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.title('Extra Tree Regressor forecasting with Actual sales vs errors')\nplt.show()","17a3df7f":"# fit model\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)","c6d82374":"lr_preds = lr_model.predict(X_test)\nprint('Prediction is done..')","96dc1439":"lr_errors_df = retail_test[['Base']]\nlr_errors_df['Predicted_Base'] = lr_preds\nlr_errors_df['Error'] = lr_preds - y_test\nlr_errors_df.insert(0, 'Modelname', 'Linear Regression')\nlr_errors_df.head()","b5a0ed59":"# Evaluate predictions for Linear Regression\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(lr_errors_df.index, lr_errors_df['Predicted_Base'], label='Forecast - Linear Regression')\nplt.legend(loc='best')\nplt.xlabel('StartDate')\nplt.ylabel('Base')\nplt.title('Forecast using Linear Regression')\nplt.show()","d83d1efe":"fig = plt.figure(figsize=(14,7))\nplt.plot(lr_errors_df.index, lr_errors_df.Error, label='Error')\nplt.plot(lr_errors_df.index, lr_errors_df.Base, label='Actual Sales')\nplt.plot(lr_errors_df.index, lr_errors_df.Predicted_Base, label='Forecasted-Sales')\nplt.legend(loc='best')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.title('Linear Regression Forecasting with Actual sales vs errors')\nplt.show()","8011ac4f":"arima_model = ARIMA(retail_model_final.Base, (6,1,1)).fit(disp=False)\nprint(arima_model.summary())","9c0f53aa":"# fit the model\nsarima_model = SARIMAX(retail_model_final.Base, order=(6, 1, 0), seasonal_order=(6, 1, 0, 7), \n                       enforce_invertibility=False, enforce_stationarity=False)\nsarima_fit = sarima_model.fit()\nsarima_test_df = retail_test[[\"Base\"]]\nsarima_test_df['Predicted_Base'] = sarima_fit.predict(start=retail_test.index[0],\n                                                 end=retail_test.index[-1], dynamic= True)\nplot = sarima_fit.plot_diagnostics(figsize=(14,7))\nplot","5be7cb39":"# Evaluation\nsarima_test_df['Error'] = sarima_test_df.Base - sarima_test_df.Predicted_Base\nsarima_test_df.insert(0, 'Modelname', 'SARIMA')\nsarima_test_df.head()","0ad1b2cc":"# Evaluate the predictions for Seasonal ARIMA model\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(sarima_test_df.index, sarima_test_df['Predicted_Base'], label='Forecast - SARIMA')\nplt.legend(loc='best')\nplt.xlabel('StartDate')\nplt.ylabel('Base')\nplt.title('Forecast using SARIMA')\nplt.show()","91982786":"plt.figure(figsize=(14,7))\nplt.plot(sarima_test_df.index, np.abs(sarima_test_df['Error']), label='errors')\nplt.plot(sarima_test_df.index, sarima_test_df['Base'], label='Actual sales')\nplt.plot(sarima_test_df.index, sarima_test_df['Predicted_Base'], label='Predicted Sales')\nplt.legend(loc='best')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.title('Seasonal ARIMA (SARIMA) forecasts with actual sales vs errors')\nplt.show()","0a56ece2":"import statsmodels.api as sm\n\n# Note the difference in argument order\nml_model = sm.OLS(retail_model_final.Base, retail_model_final.drop(columns= [\"Base\"])).fit() ## sm.OLS(output, input)\nml_preds = ml_model.predict(X_test)\n\n# Print out the statistics\nml_model.summary()","18d019ed":"ml_preds","d9f18783":"ml_errors_df = retail_test[['Base']]\nml_errors_df['Predicted_Base'] = ml_preds\nml_errors_df['Error'] = ml_preds - y_test\nml_errors_df.insert(0, 'Modelname', 'Multi Linear Regression')\nml_errors_df.head()","4f59b51b":"# Evaluate predictions for Linear Regression\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(ml_errors_df.index, ml_errors_df['Predicted_Base'], label='Forecast - Multi Linear Regression')\nplt.legend(loc='best')\nplt.xlabel('StartDate')\nplt.ylabel('Base')\nplt.title('Forecast using Multi Linear Regression')\nplt.show()","16db7ea9":"import sklearn\nsvr_model = sklearn.svm.SVR(kernel='linear', degree=3, C=1.0,).fit(X_train, y_train)\nsvr_preds = svr_model.predict(X_test)\nprint('Prediction is done...')\n# Errors calculation\nsvr_errors_df = retail_test[['Base']]\nsvr_errors_df['Predicted_Base'] = svr_preds\nsvr_errors_df['Error'] = svr_preds - y_test\nsvr_errors_df.insert(0, 'Modelname', 'Support Vector Regressor')\nsvr_errors_df.head()","a779e303":"# Evaluate predictions for Support Vector Regressor\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(svr_errors_df.index, svr_errors_df['Predicted_Base'], label='Forecast - Support Vector Regressor')\nplt.legend(loc='best')\nplt.xlabel('Intervals')\nplt.ylabel('Base')\nplt.title('Forecast using Support Vector Regressor')\nplt.show()","dfb0f85b":"result_df_svr = svr_errors_df.groupby('Modelname').agg(Total_Sales=('Base', 'sum'),\n                                          Total_Pred_Sales=('Predicted_Base', 'sum'),\n                                          Model_Overall_Error=('Error', 'sum'),\n                                          MAE=('Error', mae),\n                                          RMSE=('Error', rmse), \n                                          MAPE=('Error', mape))\nresult_df_svr","6e560049":"result_df_sarima = sarima_test_df.groupby('Modelname').agg(Total_Sales=('Base', 'sum'),\n                                          Total_Pred_Sales=('Predicted_Base', 'sum'),\n                                          Model_Overall_Error=('Error', 'sum'),\n                                          MAE=('Error', mae),\n                                          RMSE=('Error', rmse), \n                                          MAPE=('Error', mape))\nresult_df_sarima","48048f0b":"result_df_hw = tes_errors_df.groupby('Modelname').agg(Total_Sales=('Base', 'sum'),\n                                          Total_Pred_Sales=('Predicted_Base', 'sum'),\n                                          Model_Overall_Error=('Error', 'sum'),\n                                          MAE=('Error', mae),\n                                          RMSE=('Error', rmse), \n                                          MAPE=('Error', mape))\nresult_df_hw","1d9e1d05":"result_df_lr = lr_errors_df.groupby('Modelname').agg(Total_Sales=('Base', 'sum'),\n                                          Total_Pred_Sales=('Predicted_Base', 'sum'),\n                                          Model_Overall_Error=('Error', 'sum'),\n                                          MAE=('Error', mae),\n                                          RMSE=('Error', rmse), \n                                          MAPE=('Error', mape))\nresult_df_lr","445b19be":"result_df_etr = etr_errors_df.groupby('Modelname').agg(Total_Sales=('Base', 'sum'),\n                                          Total_Pred_Sales=('Predicted_Base', 'sum'),\n                                          Model_Overall_Error=('Error', 'sum'),\n                                          MAE=('Error', mae),\n                                          RMSE=('Error', rmse), \n                                          MAPE=('Error', mape))\nresult_df_etr","90cbb2fb":"result_df_mlr = ml_errors_df.groupby('Modelname').agg(Total_Sales=('Base', 'sum'),\n                                          Total_Pred_Sales=('Predicted_Base', 'sum'),\n                                          Model_Overall_Error=('Error', 'sum'),\n                                          MAE=('Error', mae),\n                                          RMSE=('Error', rmse), \n                                          MAPE=('Error', mape))\nresult_df_mlr","f95d9f32":"list_objs = [result_df_etr, result_df_mlr, result_df_svr, result_df_lr, result_df_hw, result_df_sarima]\nmetrics_table = pd.concat(list_objs)\nmetrics_table","1ba9ef5f":"print('Model Score at Train set: {:.2%}'.format(etr_model.score(X_train, y_train)))\nprint('Model Score at Test set: {:.2%}'.format(etr_model.score(X_test, y_test)))","3fe696cc":"# Create submission dataframe\nsubmission_etr_df = pd.DataFrame(data=etr_preds, columns=['Predicted_Base'])\nsubmission_etr_df.head()\n\n","693da9a5":"retail_pred_test.head()","bb2ba931":"retail_pred_test.drop(columns= ['Type - 1','rolling_mean','rolling_max','rolling_min'], inplace=True)\nretail_pred_test.head()","812599fe":"# Reset index \nretail_pred_test.reset_index(inplace=True)\nretail_pred_test.head()","345df26e":"retail_pred_test = retail_pred_test.drop(columns=[\"index\"])\nsubmission_results = pd.merge(retail_pred_test, submission_etr_df, left_index=True, right_index=True)\nsubmission_results.head()","134eb723":"## Saving file to csv\nsubmission_results.to_csv('\/kaggle\/working\/ETR_predictions.csv', index= False)\nprint('CSV file loaded with results in the path: \/content\/drive\/My Drive\/retail_forecast\/')","7a3d30d7":"# **Forecasting Model**\n\n![image.png](attachment:image.png)","8084bb26":"### **4. ARIMA\/SARIMA**","b7827be9":"#### **Plot sales using SARIMA model**","5b7d261f":"## **7. Descriptive Statistics**","9992fa5d":"#### **d. Plot sales using Holtman Forecast models**","af079e11":"### **6. KNNRegressor**","bf4f81f4":"#### **Plot forecast sales using MLR model**","29c64f70":"### **5. Multiple Linear Regression**","bd1a7972":"## **10. Correlation of features**","3588ce55":"**Handling Missing Data (Using KNN Imputation Method)**","c3c0b0ed":"**Inference**: Type - 1 and Type - 2 DNR carries Null values and need to be handled.","722ab862":"**Inference**: From above metrics it was clear that \"**Extra Trees Regressor**\" model performs well out of all 6 models. In addition model score is pretty accurate with (**100%** for Train data, **96.91**% for Test data). Hence, finalizing the model for predicting retail sales and obtaining results in CSV file.","e411ba4f":"**Errors calculated**","e821184f":"## **4. Feature Engineering (using todatetime)**","e9029d74":"**Errors calculated**","3d1ccd1d":"## **6. Find missing data and Handling Null\/NaN values**","cc823861":"#### **b. Double Exponential Smoothing**","54b28f4f":"## **9. Feature Engineering (Using rolling window calculations)**","88ace5ed":"#### **Plot forecast sales using SVR model**","eab1b118":"## **11. Split data (Train=80%, Test= 20%)**","19b71f66":"**Fill NaN with Mean values**","e453f9a1":"## **8. Detection and Handle Outliers**","3b56da77":"## **13. Modelling**","96e97be2":"**Inference**: \nAll columns are positively correlated now and the data is shown in bell curve shape which means data follows normal distribution according to Empirical rule.","46d279d8":"**Using IQR Method:**","f39ca891":"**Inference**: Still outliers exists in dataset. Hence proceeding with IQR way of method.","b9913394":"#### **b. Plot sales using Extra Trees Regressor**","59fe536a":"## **5. Plot Historical Data**","e602b427":"## **14. Calculate Metrics (MAE\/RMSE\/MAPE)**","08372a2b":"#### **Plot sales using Linear Regression**","a611b800":"#### **c. Triple Exponential**","94e5dfc3":"## **15. Table of Metrics for model comparision**","0fecbfb3":"#### **a. Calculating Scores for train and test sets**","a184f50d":"**Inference**: Base is less correlated with lag_1 and lag_2. Hence removing them from dataset.","b3cec50a":"**Inference**:\nBase is highly correlated with Type - 1 and is less correlated with Type - 2 DNR. Hence removing Type - 2 DNR from dataset.","fc2d7d3c":"## **16. Submission file with Prediction values**","f5964c12":"**Context**:\n\nDataset contains the demand shipment history of product at a store for multiple customers from global sites for the past two years.\n\n**Objective**:\n\nFind a best forecast model to predict product shipment demands for future with the given retailer data.","19c94ba4":"## **1. Import Python Libraries**","2a8819c5":"## **12. Plot Train and Test data**","f07caba0":"**Outliers**: First array contains the list of row numbers, second array are respective column numbers and each outlier have a Z-Score higher than 3.","c82d8476":"**Define Metrics**","69cb987a":"### **2. Extra Trees Regressor**","1cf66209":"## **3. Data Slicing**","c3275a0b":"### **3. Linear Regression**","69e437e3":"**Inference**: All columns have outliers.","241200b3":"**Using z-score**","02cc2008":"### **1. Holt Winter**","146f9d36":"## **2. Load Dataset using Pandas**","d5c09abb":"#### **a. Simple Exponential Smoothing**"}}