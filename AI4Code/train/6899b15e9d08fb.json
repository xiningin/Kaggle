{"cell_type":{"7fcd5b42":"code","e875a2e5":"code","1b37fa2e":"code","90334c6a":"code","94e78e49":"code","8d29fbdc":"code","a2f1ba2b":"code","7a20b5dc":"code","5a326c44":"code","26b588a1":"code","5f4edb37":"code","d8055316":"code","659606d0":"code","c6caa90b":"code","7d5bba27":"code","2b6484bd":"code","e86614a4":"code","9fa7e1ab":"code","0c512222":"code","9973dcca":"code","e40c783b":"code","740b3d3b":"code","1a60f381":"code","c0692764":"code","9bb66c17":"code","25e9f159":"code","0472b2e4":"code","b1317909":"code","ae14b755":"code","f6bced71":"code","7e601752":"code","686be02e":"code","ec793341":"code","dce62e40":"code","517c4ba4":"code","903ad034":"code","93385337":"code","95703ac3":"code","669b01ea":"code","8ebd75ef":"code","4549f64a":"code","53a33f78":"code","11b1a09b":"code","d096820f":"code","f87edff3":"code","5cfc8ca3":"code","6e3d52bb":"code","a01d1584":"code","491c9c7e":"code","7a64977b":"code","c810376b":"code","7b5da0b9":"code","42ea86a0":"code","cc38c5a9":"code","c179ffe9":"code","b2b7025b":"code","04999dd8":"code","66457840":"markdown","3e23122f":"markdown","9b8263c1":"markdown","ff453020":"markdown","a63fa50b":"markdown","2fbb8f0d":"markdown","7c71a817":"markdown","67a8ddda":"markdown","634a7681":"markdown","058baab3":"markdown","b4da2268":"markdown","17d9e917":"markdown","8fdb07aa":"markdown"},"source":{"7fcd5b42":"#Importing required numerical and data manipulation libraries\nimport numpy as np \nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#plotting libraries\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))","e875a2e5":"data = pd.read_csv(\"..\/input\/Wholesale customers data.csv\")\ndata.shape","1b37fa2e":"data.head()","90334c6a":"type(data)","94e78e49":"print('Descriptive Statastics of our Data:')\ndata.describe().T","8d29fbdc":"print('Showing Meta Data :')\ndata.info()","a2f1ba2b":"#Checking for missing values\npd.isnull(data).sum()","7a20b5dc":"data.Region.value_counts()","5a326c44":"data.Channel.value_counts()","26b588a1":"dataset = data.copy()","5f4edb37":"dataset['Channel'] = dataset['Channel'].map({1:'Horeca', 2:'Retail'})","d8055316":"dataset['Region'].replace([1,2,3],['Lisbon','Oporto','other'],inplace=True)","659606d0":"dataset.head()","c6caa90b":"def continous_data(i):\n    if dataset[i].dtype!='object':\n        print('--'*60)\n        sns.boxplot(dataset[i])\n        plt.title(\"Boxplot of \"+str(i))\n        plt.show()\n        plt.title(\"histogram of \"+str(i))        \n        dataset[i].plot.hist(bins = 20)\n        plt.show()\n        plt.clf()","7d5bba27":"sns.set() #Sets the default seaborn style\nj=['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen']\nfor k in j:\n    continous_data(i=k)","2b6484bd":"dataset.head()","e86614a4":"# Scale the data using the natural logarithm\nlog_data = np.log(dataset[['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen']].copy())","9fa7e1ab":"log_data.head()","0c512222":"def categorical_data(i):\n    dataset[i].value_counts().plot(kind='bar')\n\nj_1 = ['Channel','Region']\n\nfor k in j_1:\n    categorical_data(i=k)\n    plt.show()    ","9973dcca":"dataset.corr()","e40c783b":"print('Correlation Heat map of the data')\nplt.figure(figsize=(10,6))\nsns.heatmap(dataset.corr(),annot=True,fmt='.2f',vmin=-1,vmax=1,cmap='Spectral')\nplt.show()","740b3d3b":"def scatterplot(i,j):\n    sns.regplot(data=log_data,x=i,y=j)\n    plt.show()","1a60f381":"scatterplot(i='Milk',j='Grocery')","c0692764":"scatterplot(i='Milk',j='Detergents_Paper')","9bb66c17":"scatterplot(i='Detergents_Paper',j='Grocery')","25e9f159":"def categorical_multi(i,j):\n    pd.crosstab(dataset[i],dataset[j]).plot(kind='bar')\n    plt.show()\n    print(pd.crosstab(dataset[i],dataset[j]))\n\ncategorical_multi(i='Channel',j='Region')    ","0472b2e4":"list(log_data.columns)","b1317909":"# replacing the outliers with their Inner fences\nfor k in list(log_data.columns):\n    IQR = np.percentile(log_data[k],75) - np.percentile(log_data[k],25)\n    \n    Outlier_top = np.percentile(log_data[k],75) + 1.5*IQR\n    Outlier_bottom = np.percentile(log_data[k],25) - 1.5*IQR\n    \n    log_data[k] = np.where(log_data[k] > Outlier_top,Outlier_top,log_data[k])\n    log_data[k] = np.where(log_data[k] < Outlier_bottom,Outlier_bottom,log_data[k])","ae14b755":"def continous_data(i):\n    if log_data[i].dtype!='object':\n        print('--'*60)\n        sns.boxplot(log_data[i])\n        plt.title(\"Boxplot of \"+str(i))\n        plt.show()\n        plt.title(\"histogram of \"+str(i))\n        log_data[i].plot.kde()\n        plt.show()\n        plt.clf()\n\nfor k in j:\n    continous_data(i=k)        ","f6bced71":"sns.pairplot(log_data,diag_kind = 'kde')","7e601752":"dataset1 = log_data.copy()\nlist(dataset1.columns)","686be02e":"## replacing with median to treat the outliers\nfor k in list(dataset1.columns):\n    IQR=np.percentile(dataset1[k],75) - np.percentile(dataset1[k],25)\n    \n    Outlier_top=np.percentile(dataset1[k],75)+1.5*IQR\n    Outlier_bottom=np.percentile(dataset1[k],25)-1.5*IQR\n    \n    dataset1[k]=np.where(dataset1[k] > Outlier_top,np.percentile(dataset1[k],50),dataset1[k])\n    dataset1[k]=np.where(dataset1[k] < Outlier_bottom,np.percentile(dataset1[k],50),dataset1[k])","ec793341":"sns.pairplot(dataset1,diag_kind = 'kde')","dce62e40":"df  =  pd.concat([dataset[['Channel','Region']],log_data],axis=1)\ndf.head()","517c4ba4":"df = pd.get_dummies(df,columns=['Channel','Region'],drop_first=True)\ndf.head()","903ad034":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf_std = scaler.fit_transform(df)\ndf_std = pd.DataFrame(df_std,columns=df.columns)\ndf_std.head()","93385337":"from scipy.spatial.distance import pdist,squareform\nfrom scipy.cluster.hierarchy import linkage,dendrogram,cut_tree","95703ac3":"eu_d = pdist(df_std,metric='euclidean')\nclus = linkage(eu_d,method='average')\nnames = np.arange(0,df_std.shape[0]).tolist()","669b01ea":"plt.figure(figsize=[14,8])\ndendrogram(clus,labels=names)\nplt.xlabel('hclust')\nplt.ylabel('distance')\nplt.title('cluster dendogram')","8ebd75ef":"data_hier = data.copy()\ndata_hier.head()","4549f64a":"data_hier['clusters'] = cut_tree(clus,6)","53a33f78":"clust_profile = data_hier.groupby(['clusters'],as_index=False).mean()\nclust_profile","11b1a09b":"X = df_std.copy()\n\nfrom sklearn.cluster import KMeans\ncluster_range = range(1,20)\ncluster_wss=[] \nfor cluster in cluster_range:\n    model = KMeans(cluster)\n    model.fit(X)\n    cluster_wss.append(model.inertia_)","d096820f":"#PLotting Elbow curve for finding Optimal K value\nplt.figure(figsize=[10,6])\nplt.title('WSS curve for finding Optimul K value')\nplt.xlabel('No. of clusters')\nplt.ylabel('Inertia or WSS')\nplt.plot(list(cluster_range),cluster_wss,marker='o')\nplt.show()","f87edff3":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=6,random_state=0)\nmodel.fit(X)","5cfc8ca3":"dataset_final = data.copy()\ndataset_final.head()","6e3d52bb":"dataset_final['clusters']=model.predict(X)\ndataset_final.head()","a01d1584":"#cluster profiles\nclust_prof = dataset_final.groupby(['clusters'],as_index=False).mean()\nclust_prof","491c9c7e":"from sklearn.decomposition import PCA\npca2 = PCA(n_components=2)\npc = pca2.fit_transform(df_std)\npc_df = pd.DataFrame(pc)\npc_df.head()","7a64977b":"pca = pd.concat([pc_df,dataset_final['clusters']],axis=1)\npca.columns = ['pc1','pc2','clusters']\nprint(pca.shape)\npca.head()","c810376b":"pca.clusters.value_counts()","7b5da0b9":"plt.figure(figsize=[16,8])\nsns.scatterplot(x='pc1', y='pc2', hue= 'clusters', data=pca,palette='Set1')\nplt.show()","42ea86a0":"dataset_final.groupby('clusters').Fresh.mean().plot(kind='bar')","cc38c5a9":"dataset_final.groupby('clusters').Milk.mean().plot(kind='bar')","c179ffe9":"dataset_final.groupby('clusters').Grocery.mean().plot(kind='bar')","b2b7025b":"dataset_final.groupby('clusters').Frozen.mean().plot(kind='bar')","04999dd8":"dataset_final.groupby('clusters').Detergents_Paper.mean().plot(kind='bar')","66457840":"### PCA for getting the first 2 Principle components","3e23122f":"## Treatment of Outliers","9b8263c1":"# Feature Scaling","ff453020":"## Data Transformation","a63fa50b":"### Univarient Analysis","2fbb8f0d":"## Creating dummies for categorical varibles","7c71a817":"### Clustering Using K-means with K=6","67a8ddda":"## Exploratory Data Analysis(EDA)","634a7681":"Now,lets convert the channel and Region to categorical variable for only EDA purpose \nmapping channels as 1:Horeca 2:Retail and \nalso regions 1:lisbon 2:oporto 3:other","058baab3":"## Kmeans Clustering","b4da2268":"The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units on diverse product categories.\n\n### Attribute Information:\n- 1)\tFRESH: annual spending (m.u.) on fresh products (Continuous); \n- 2)\tMILK: annual spending (m.u.) on milk products (Continuous); \n- 3)\tGROCERY: annual spending (m.u.)on grocery products (Continuous); \n- 4)\tFROZEN: annual spending (m.u.)on frozen products (Continuous) \n- 5)\tDETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous) \n- 6)\tDELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous); \n- 7)\tCHANNEL: customers Channel - Horeca (Hotel\/Restaurant\/Cafe) or Retail channel (Nominal) \n- 8)\tREGION: customers Region - Lisbon, Oporto or Other (Nominal) \n\n### Region\tFrequency \nLisbon\t77 \nOporto\t47 \nOther Region\t316 \nTotal\t440 \n\n### Channel\tFrequency \nHoreca\t298 \nRetail\t142 \nTotal\t440 \n\nOur project goal is to use various clustering techniques to segment customers. Clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Thus, there is no outcome to be predicted, and the algorithm just tries to find patterns in the data.","17d9e917":"## Explain what you have done and how this segments customers","8fdb07aa":"# INTRODUCTION"}}