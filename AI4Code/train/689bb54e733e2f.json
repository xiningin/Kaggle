{"cell_type":{"cdbe4558":"code","9aa12fdd":"code","fdc01c1c":"code","9f08df25":"code","d7f784e6":"code","6b2b890e":"code","becae30d":"code","4ebcf957":"code","dd667252":"code","a4c63c75":"code","ee0fbb00":"code","6f646ea4":"code","d60bde64":"code","eeeead6e":"code","2dec19c2":"code","6de51bf2":"code","61e15616":"code","d59e80fc":"code","f4e21ef4":"code","cab990de":"code","a1178cbe":"code","2c45104b":"code","65c687f8":"code","5c311587":"code","64226759":"code","f12eb6fa":"code","92ab9c07":"code","07e78df6":"code","c8ffd527":"code","a061a184":"code","fc293e3f":"code","4aba6e73":"code","b90e2e8b":"code","8e5c4d0d":"code","34a3e230":"code","71874fca":"code","786e7552":"code","ff32b890":"code","684981e3":"code","2a646a3f":"code","26f29b52":"code","ae902e63":"code","35491d93":"code","32c5a130":"code","2fa3c5ee":"code","73f698a4":"code","403fe68e":"code","f02585de":"code","fd044aad":"code","6bac567e":"code","a4e583db":"code","95591ab7":"code","23668d47":"code","c6b267d2":"code","59253fd6":"code","b22c9e0d":"code","5bcbe29e":"code","3e19cb44":"code","a4795f50":"code","9b7de93d":"code","62e51164":"code","08f5eb9f":"code","e6f166e0":"code","8be9b4ce":"code","3d63a6d3":"code","635c272a":"code","d797737f":"code","3adbb68d":"code","02f3bda9":"code","7ea5a22a":"code","ef97ade2":"markdown","d6b72140":"markdown","8c779f96":"markdown","01b48544":"markdown","98eaf8d0":"markdown","7f075d51":"markdown","73293b03":"markdown","f51b9bff":"markdown","3fced7f0":"markdown","bcf644e6":"markdown","2874d711":"markdown","f43fdd61":"markdown","beed8b5d":"markdown","e2f93c25":"markdown","62563104":"markdown","cd781133":"markdown","352b0fd7":"markdown","07d85a33":"markdown","b4a5eb07":"markdown","14d9fc98":"markdown","f1ba3fe5":"markdown","ed3f9871":"markdown","e6e48065":"markdown","e0f34374":"markdown","24b4350a":"markdown","e2434509":"markdown","790e5d1a":"markdown","ba2a682a":"markdown","fc140e81":"markdown","9527d876":"markdown","4ac70a40":"markdown","ea871312":"markdown","096ec0f9":"markdown","26700d20":"markdown","16cd1ab9":"markdown","dcd48500":"markdown","49ce5b8b":"markdown","5b06ac49":"markdown","23638197":"markdown","1d4467d2":"markdown","e37497cd":"markdown","ded7891e":"markdown","8e761130":"markdown","159c90e3":"markdown","0a562543":"markdown"},"source":{"cdbe4558":"from IPython.display import Image\nImage(\"..\/input\/image\/image.jpg\")","9aa12fdd":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","fdc01c1c":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","9f08df25":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(df_train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(df_test.shape))\n\n#Save the 'Id' column\ntrain_ID = df_train['Id']\ntest_ID = df_test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(df_train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(df_test.shape))","d7f784e6":"plt.figure()\nplt.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'], alpha = 0.5)\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.title('Spot the outliers')\nplt.show()","6b2b890e":"df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<210000)].index)","becae30d":"plt.figure()\nplt.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'], alpha = 0.5)\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.title('Spot the outliers')\nplt.show()","4ebcf957":"sns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","dd667252":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","a4c63c75":"train_feature_ext = df_train['Exterior1st'].value_counts().index\ntest_feature_ext = df_test['Exterior1st'].value_counts().index\n\nprint( 'The number of catogries in the train set for the \"Extorior1st\" feature is:  {:.0f}'.format(len(train_feature_ext)))\nprint( '\\nThe number of catogries in the test set for the \"Extorior1st\" feature is:  {:.0f}'.format(len(test_feature_ext)))","ee0fbb00":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\ny_train = df_train['SalePrice']\n\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True) # all_data contains all the data from the train and test set expect for the SalePrice\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","6f646ea4":"nbr_missing = all_data.isnull().sum()\nall_data_na = (nbr_missing \/ len(all_data)) * 100\n\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\nnbr_missing = nbr_missing.drop(nbr_missing[nbr_missing == 0].index).sort_values(ascending=False)[:40]\n\nmissing_data = pd.DataFrame({'Percent missing' :all_data_na, 'Missing values' :nbr_missing})\nmissing_data.head(40)","d60bde64":"plt.figure(figsize=(20,10))\nsns.heatmap(all_data.isna(), cbar=False, cmap = 'plasma')\nplt.title('Visual representation of the missing values in the dataset')","eeeead6e":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass NaN_transformer(BaseEstimator, TransformerMixin):\n    '''\n    This transformer fill the missing values in the given dataset,\n    or delete the features with too much missing values.\n    '''\n\n    def fit( self, X, y=None ):\n        return self \n\n    def transform(self, X, y=None):\n        \n        X[\"PoolQC\"] = X[\"PoolQC\"].fillna(\"None\") #PoolQC : data description says NA means \"No Pool\"\n        X[\"MiscFeature\"] = X[\"MiscFeature\"].fillna(\"None\") #MiscFeature : data description says NA means \"no misc feature\"\n        X[\"Alley\"] = X[\"Alley\"].fillna(\"None\") #Alley : data description says NA means \"no alley access\"\n        X[\"Fence\"] = X[\"Fence\"].fillna(\"None\") #Fence : data description says NA means \"no fence\"\"\n        X[\"FireplaceQu\"] = X[\"FireplaceQu\"].fillna(\"None\") #FireplaceQu : data description says NA means \"no fireplace\"\n        \n        for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n            X[col] = X[col].fillna('None')\n        \n        X['GarageYrBlt'] = X['GarageYrBlt'].fillna(X['GarageYrBlt'].median()) # We fill the missing data with the medianvalue of the other variables\n        X['GarageCars'] = X['GarageCars'].fillna(0) # We fill the missing data with 0 cars\n        X = X.drop('GarageArea', axis = 1) #'GarageCars' and 'GarageArea' are correlated variables, we'll keep only one\n        \n        X[\"LotFrontage\"] = X.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n        \n        for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n            X[col] = X[col].fillna(0) # These samples are likely to have no basement, since the fill with zeros\n            \n        for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n            X[col] = X[col].fillna('None') # NaN means no basement\n        \n        X[\"MasVnrType\"] = X[\"MasVnrType\"].fillna(\"None\")\n        X[\"MasVnrArea\"] = X[\"MasVnrArea\"].fillna(0)\n        \n        X['MSZoning'] = X['MSZoning'].fillna(all_data['MSZoning'].mode()[0]) # Filling with the most common category\n        \n        X = X.drop(['Utilities'], axis=1)\n        \n        X[\"Functional\"] = X[\"Functional\"].fillna(\"Typ\") #data description says NA means typical\n        \n        X['Electrical'] = X['Electrical'].fillna(X['Electrical'].mode()[0]) # Filling with the most common category\n        X['KitchenQual'] = X['KitchenQual'].fillna(X['KitchenQual'].mode()[0])\n        X['Exterior1st'] = X['Exterior1st'].fillna(X['Exterior1st'].mode()[0])\n        X['Exterior2nd'] = X['Exterior2nd'].fillna(X['Exterior2nd'].mode()[0])\n        X['SaleType'] = X['SaleType'].fillna(X['SaleType'].mode()[0])\n        \n        X['MSSubClass'] =  X['MSSubClass'].fillna(\"None\")\n        \n        X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF'] # Adding a feature\n        \n        return(X)","2dec19c2":"tmp = all_data.copy() # make a copy of the dataframe to show an exemple\n\nna_tr = NaN_transformer() #We pass nothing inside this transformer\n\ntmp = na_tr.fit_transform(tmp)","6de51bf2":"plt.figure(figsize=(20,10))\nsns.heatmap(tmp.isna(), cbar=False, cmap = 'plasma')\nplt.title('Visual representation of the missing values in the dataset')","61e15616":"class ToCategorical_transformer(BaseEstimator, TransformerMixin):\n    '''\n    This transformer selects either numerical or categorical features.\n    In this way we can build separate pipelines for separate data types.\n    '''\n\n    def fit( self, X, y=None ):\n        return self \n\n    def transform(self, X, y=None):\n        \n        #MSSubClass=The building class\n        X['MSSubClass'] = X['MSSubClass'].apply(str)\n\n        #Changing OverallCond into a categorical variable\n        X['OverallCond'] = X['OverallCond'].astype(str)\n\n        #Year and month sold are transformed into categorical features.\n        X['YrSold'] = X['YrSold'].astype(str)\n        X['MoSold'] = X['MoSold'].astype(str)\n        \n        return(X)","d59e80fc":"tiny_tmp = all_data[['MSSubClass','OverallCond','YrSold','MoSold']]\ntiny_tmp.dtypes","f4e21ef4":"toCat_tr = ToCategorical_transformer()\n\ntiny_tmp = toCat_tr.fit_transform(tiny_tmp)\n\ntiny_tmp.dtypes","cab990de":"from sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nclass Personal_LabelEncoder(BaseEstimator, TransformerMixin):\n    '''\n    This transformer selects either numerical or categorical features.\n    In this way we can build separate pipelines for separate data types.\n    '''\n    def __init__(self, cols): #Now we need the __init__ method since we'll build variables we'll use in this transformer\n        self.cols = cols # This variable will contains all the columns that will be transformed\n        self.encoder = None # Since the encoder is already a transformer that exists, we'll define it in the fit method\n    \n    def fit( self, X, y=None ):\n        self.encoder = LabelEncoder() # We fit the encoder to the data\n        return self \n\n    def transform(self, X, y=None):\n        for c in self.cols:\n            X[c] = self.encoder.fit_transform(list(X[c].values)) # Here we actually do the transformation\n        \n        return(X)","a1178cbe":"tmp[['FireplaceQu', 'BsmtQual', 'BsmtCond']].head()","2c45104b":"encoder = Personal_LabelEncoder(cols = cols)\n\ntmp = encoder.fit_transform(tmp)\n\ntmp[['FireplaceQu', 'BsmtQual', 'BsmtCond']].head()","65c687f8":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass Dummy_transform(BaseEstimator, TransformerMixin):\n    '''\n    This transformer selects either numerical or categorical features.\n    In this way we can build separate pipelines for separate data types.\n    '''\n\n    def fit( self, X, y=None ):\n        return self \n\n    def transform(self, X, y=None):\n        X = pd.get_dummies(X)\n        return(X)","5c311587":"from sklearn.pipeline import Pipeline\n\nfull_pipe= Pipeline([\n                ('NaN transformer', NaN_transformer()),\n                ('To categorical Transform', ToCategorical_transformer()),\n                ('Label Encoder', Personal_LabelEncoder(cols = cols)),\n                ('Dummy transformation', Dummy_transform())\n            ])\n\nall_data_tr = all_data.copy()\n\nall_data_tr = full_pipe.fit_transform(all_data_tr)","64226759":"all_data_tr.head()","f12eb6fa":"all_data_tr.shape #We have way more columns due to the dummy transformation","92ab9c07":"X_train = all_data_tr[:ntrain]\nX_test = all_data_tr[ntrain:]","07e78df6":"X_train.head()","c8ffd527":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV, Lasso, LinearRegression, BayesianRidge\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error","a061a184":"# Setup cross validation folds\nkf = KFold(n_splits=6, random_state=42, shuffle=True) #Because there are no shuffle implemented with the cross validation, we define here the folds for the cross validation","fc293e3f":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X_train):\n    rmse = np.sqrt(-cross_val_score(model, X.values, y_train.values, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","4aba6e73":"linear = make_pipeline(RobustScaler(),LinearRegression())","b90e2e8b":"# Here the lambda parameter is called \"alpha\"\nlasso = make_pipeline(StandardScaler(), Lasso(alpha =0.0035, random_state=42)) # Here lasso_pipe has to be used just like a regular model.","8e5c4d0d":"# But...what if we want to try different scaler, how to use GridSearchCV with a pipeline ???\n# Check this out:\n\nlasso_pipe = Pipeline([ #We have to create a pipeline, just like before, but create a \"None\" value for the scaler that we will fill later.\n    ('scaler', None),\n    ('classifier', Lasso())\n])\n\nalpha_params = [0.0025,0.003,0.0035]\n\n# Now we have to define a list of dictionnary. Within each dictionnary we'll call a different scaler and the same alpha parameters\nparam_grid = [\n    {\n        'scaler': [StandardScaler()],\n        'classifier__alpha': alpha_params,\n    },\n    {\n        'scaler': [RobustScaler()],\n        'classifier__alpha': alpha_params,\n    },\n]\n\n#Then, well, you already know how to build a grid !\n\nlasso_grid = GridSearchCV(lasso_pipe, param_grid, cv=kf)\nlasso_grid.fit(X_train, y_train)\n\nprint('the best score obtained is {} with the parameters {}'.format(lasso_grid.best_score_,lasso_grid.best_params_))","34a3e230":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0045, l1_ratio=.9, random_state=3))","71874fca":"ENet_pipe = Pipeline([ #We have to create a pipeline, just like before, but create a \"None\" value for the scaler that we will fill later.\n    ('scaler', None),\n    ('classifier', ElasticNet())\n])\n\nalpha_params = [0.00045,0.0005,0.00055]\nl1_params = [0.85,0.86,0.87,0.88,0.89,0.9]\n\n# Now we have to define a list of dictionnary. Within each dictionnary we'll call a different scaler and the same alpha parameters\nparam_grid = [\n    {\n        'scaler': [StandardScaler()],\n        'classifier__alpha': alpha_params,\n        'classifier__l1_ratio': l1_params,\n    },\n    {\n        'scaler': [RobustScaler()],\n        'classifier__alpha': alpha_params,\n        'classifier__l1_ratio': l1_params,\n    },\n]\n\n#Then, well, you already know how to build a grid !\n\nENet_grid = GridSearchCV(ENet_pipe, param_grid, cv=kf)\nENet_grid.fit(X_train, y_train)\n\nprint('the best score obtained is {} with the parameters {}'.format(ENet_grid.best_score_,ENet_grid.best_params_))","786e7552":"# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)","ff32b890":"adar = AdaBoostRegressor(n_estimators=4000,\n                         learning_rate=0.01,\n                         loss='linear',\n                        random_state=42,)","684981e3":"param_grid={\n    'n_estimators':[4000],\n    'learning_rate':[0.001,0.005,0.01],\n    'loss':['linear','exponential']\n    \n    \n}\nadar_grid = RandomizedSearchCV(AdaBoostRegressor(random_state=42), param_grid, cv=kf) # Just use it like a regular GridSearchCV\nadar_grid.fit(X_train,y_train)","2a646a3f":"adar_grid.best_score_ # This is how to access the best score calculated","26f29b52":"adar_grid.best_params_ # This is how to get the parameters that leads to the best score","ae902e63":"model_adar = adar_grid.best_estimator_ # This is how to create a new model with the best parameters found in the grid.","35491d93":"score = cv_rmse(model_adar)\nprint(\"model_adar: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n#scores['model_adar'] = (score.mean(), score.std())","32c5a130":"gbr = GradientBoostingRegressor(n_estimators=3000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)","2fa3c5ee":"# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=7000,\n                       max_depth=8,\n                       min_child_weight=10,\n                       gamma=0.6,\n                       subsample=0.5,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       seed=42,\n                       reg_alpha=0.006,\n                       random_state=42)","73f698a4":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, # This is the maximum number of bins the features will be bucketed in.\n                       bagging_fraction=0.8,\n                       bagging_freq=4, # Means: perform the bagging every 4 iterations\n                       bagging_seed=42, # Equivalent of random_state but with bagging.\n                       feature_fraction=0.2, # As usual, the faction of features that will be randomly selected to build trees at each iteration\n                       feature_fraction_seed=42, # Equivalent of random_state but with the feature_fraction\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1, # To shut down the messages\n                       random_state=42)","403fe68e":"# Support Vector Regressor\nsvr = make_pipeline(StandardScaler(), SVR(C= 5, epsilon= 0.0003, gamma=0.0003))","f02585de":"svr_pipe = Pipeline([ #We have to create a pipeline, just like before, but create a \"None\" value for the scaler that we will fill later.\n    ('scaler', None),\n    ('classifier', SVR())\n])\n\nC_params = [4,5,6]\nepsilon_params = [0.0002,0.0003,0.0004,0.0005]\ngamma_params = [0.00005,0.0001,0.0002,0.0003]\n\nparam_grid ={\n        'scaler': [StandardScaler()],\n        'classifier__C': C_params,\n        'classifier__epsilon': epsilon_params,\n        'classifier__gamma': gamma_params,\n    }\n\nsvr_grid = GridSearchCV(svr_pipe, param_grid, cv=kf)\nsvr_grid.fit(X_train, y_train)\n\nprint('the best score obtained is {} with the parameters {}'.format(svr_grid.best_score_,svr_grid.best_params_))\n\nmodel_svr = svr_grid.best_estimator_","fd044aad":"score = cv_rmse(model_svr)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n#scores['xgboost'] = (score.mean(), score.std())","6bac567e":"#Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ENet, lasso, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","a4e583db":"scores = {}\n\nscore = cv_rmse(linear)\nprint(\"linear: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['linear'] = (score.mean(), score.std())","95591ab7":"score = cv_rmse(lasso)\nprint(\"lasso: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std())","23668d47":"score = cv_rmse(ENet)\nprint(\"ENet: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ENet'] = (score.mean(), score.std())","c6b267d2":"score = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lightgbm'] = (score.mean(), score.std())","59253fd6":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgboost'] = (score.mean(), score.std())","b22c9e0d":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","5bcbe29e":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","3e19cb44":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X_train), np.array(y_train))","a4795f50":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X_train, y_train)","9b7de93d":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X_train, y_train)","62e51164":"print('linear')\nlinear_model_full_data = linear.fit(X_train, y_train)","08f5eb9f":"print('lasso')\nlasso_model_full_data = lasso.fit(X_train, y_train)","e6f166e0":"print('Svr')\nsvr_model_full_data = svr.fit(X_train, y_train)","8be9b4ce":"print('RandomForest')\nrf_model_full_data = rf.fit(X_train, y_train )","3d63a6d3":"print('GradientBoosting')\ngbr_model_full_data = gbr.fit(X_train, y_train)","635c272a":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(x1,x2,x3,x4,x5,x6,x7,X):\n    assert(x1+x2+x3+x4+x5+x6+x7 == 1.0, 'Sum is not equal to 1')\n    return ((x1 * lasso_model_full_data.predict(X)) + \\\n            (x2 * svr_model_full_data.predict(X)) + \\\n            (x3 * gbr_model_full_data.predict(X)) + \\\n            (x4 * xgb_model_full_data.predict(X)) + \\\n            (x5 * lgb_model_full_data.predict(X)) + \\\n            (x6 * rf_model_full_data.predict(X)) + \\\n            (x7 * stack_gen_model.predict(np.array(X))))","d797737f":"x1, x2, x3, x4, x5, x6, x7 = 0.3, 0.1, 0.05, 0.1, 0.3, 0.1, 0.05","3adbb68d":"# Get final precitions from the blended model\nblended_score = rmsle(y_train, blended_predictions(x1,x2,x3,x4,x5,x6,x7,X_train))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","02f3bda9":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","7ea5a22a":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = np.expm1(blended_predictions(x1,x2,x3,x4,x5,x6,x7,X_test))\nsub.to_csv('submission_house_price.csv',index=False)","ef97ade2":"To make the prediction as accurate as possible, we'll mix the results of every models based on how well they performed lately. Since we are predicting prices (continuous variable) we can add part of the predictions as we want (the total of percentage coefficient must be 1 obviously). ","d6b72140":"## 3.5 Dummy transformation","8c779f96":"# House price competition: pipeline and models.\n\nIf you already know how to bluid basic-machine learning algorithms, observe and clean the data you are dealing with, you might want to go to the next level and start implementing pipelines in your codes.\nThe principal goal of building a pipeline is to make things easier, more understandable and it's a proper way to avoid data leakage.\n\nIn this tutorial, i'll try to present what I've learned to build simple and effective pipelines in order to finetune every kind of models.\n\nHowever, since I am not an expert myself, I'd be glad to hear your recommandations in the comment section !\n\nA huge part of this notebook comes from the excellent [Serigne notebook](http:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard). I highly suggest to check it first. My intend here is to re-use its work on the feature engineering to focus more on the creation of a pipeline.\n\nMoreover, since I'll build many different types of estimators through this notebook, I'll try to explain the main ideas of each so that you might learn more things !\n\nOne last thing: please consider leaving an upvote if you find this notebook useful in any ways !","01b48544":"Looks like this technique works well !","98eaf8d0":"Another way to visualize this lack of data is to display the whole dataset as followed:","7f075d51":"### ElasticNet regressor\n- The ElasticNet regression is a mix between the ridge and the lasso regression. (The good news is that we already know the main ideas behind lasso regression and have some insight on how ridge regression works).\n- It is a good choice when one have to many features (so many that one cannot interpret all of them). This is not the case here but why not try it and see what happen ?\n- The important thing here is that there are two penalty lambda parameters now (one like the lasso and one like the ridge). However in sklearn, these parameters are controlled with the parameters \"alpha\" and \"l1_ratio\".\n\na * L1 + b * L2 where alpha = a + b and l1_ratio = a \/ (a + b).","73293b03":"## 3.6 Putting everything together","f51b9bff":"We've seen how to use GridSearchCV on the previous exemples, now that models are becoming more and more sophisticated, it might be too long to wait for a full grid to run if we want to fine tune a model. One thing one can try is to use a RandomizedSearchCV that will try, randomly, a certain amount of combinaison. It can be a good choice to gain a first insight on the scale of the parameters you have to tune.","3fced7f0":"### Gradient Boosting Regressor\nIn order to understand the main ideas behind a GradientBoostingRegressor, it is best to know how AdaBoostRegressors work...what a chance, you know it now !\n\n- In contrast to AdaBoost, GradientBoost starts by making a single leaf and not a Stump (the first leaf is the average of the y values)\n- Then a tree is made from this leaf. Usually, this tree is larger than a Stump. And just like AdaBoost, each tree is made based on the errors of the previous one.","bcf644e6":"# <div id=\"chap6\">6. Submission","2874d711":"# <div id=\"chap2\">2. Data preprocessing\nFor now, we just want to preprocess the data a bit before starting to implement our pipeline.","f43fdd61":"### XGBoost regressor\nXGBoost stands for \"eXtreme Gradient Boosting\" (sounds fancy right ?), let's say it is just an advanced implementation of gradient boosting algorithm and let's focus on the parameters. One last thing important to know is that XGBoost algorithms are not in the SKlearn library, these algorithms come from a specific library (xgboost) but use the same API as SKlearn.\nThere are a lot of parameters to tune on the XGBoost, hence the difficulty to fine tune it well. Among these, you already know lot of them (learning_rate, n_estimators, max_depth etc. so I won't (re)cover them, let's focus on the new ones:\n- **min_child_weight**: Defines the minium sum of instances of all observations requiered in a child. In the particular case of linear regression, it correponds to the number of instances needed to be in each node. This value ranges from 0 to infinity.\n- **gamma**: A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction requiered to make a split.\n- **subsample**: This is the percentage of observations to be randomly samples for each tree.\n- **colsample_bytree**: the percentage of features to be considered when creating a new tree\n- **alpha**: This is just like in the lasso regression, the L1 regularization term on weight\n- **objective**: This defines the loss function to be minimized. Here we use the RMSE since it is a regression problem\n\nThese are just some of the parameters, there are a lot more but since it is difficult enough to fine tune it, I'll stick with these ones. For more information you can visit the XGBoost documentation [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html).","beed8b5d":"Time to rebuilt the train and test set now !","e2f93c25":"Let's check if it worked:","62563104":"### LightGradientBoosting regressor\nThis is again a kind of Boosting Regressors not directly implemented in SKlearn but that uses the same API. It is pretty similar to the basic gradient boosting regressor but is said to be faster in training, better in accuracy and uses less memory.\n\nFor more information, go to this [page](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html) that is the documentation of the LGBMRegressor.","cd781133":"### Lasso regressor\n- The idea behing lasso regression is to add a penalty to the sum of the least squared residuals using a lambda coefficient that multiplies the absolute value of the slope of the fitting line.\n- Usually, by decreasing the sum of the least squares we are getting a line that fits the data better. The thing is that this line might predict well the training data but badly the test data. By using a penalty lambda coefficient, we increase the bias for the training line but at the end of the day, we are decreasing the variance of the fit on the test data.\n- Lasso regression is very similar to ridge regression. The difference is that with ridge, we take the square of the slope and not the absolute value.\n- The lambda parameter is determined with a cross validation. If lambda = 0, the lasso line is the same as the least square line calculated with a Linear Regression.\n- The nice thing with using lasso regression is that we can get rid of useless features with a proper lambda.\nFor this competition, since we have plenty of features that might not be that usefull, lasso regression seems a better choice in comparison to ridge regression that doesn't get completly get rid of useless variables.\n\nNow that we have seen how to build and use a pipeline to clean the data you might want to know how to build a pipeline with models inside, well, it is as simple as before ! Sice we haven't scaled the data yet, we have to do it before calling the model. This is where the pipeline will be usefull. We'll build a \"new model\" that is a succession of a scaler and the model:","352b0fd7":"### AdaBoostRegressor\n\nTo uderstand how AdaBoostRegressors work, it is a good thing to compare it to a RandomForest models.\n- In comparison to RandomForest, where trees are developped to their maximum depth (or close to it), trees made with Adaboost are just composed of a node and two leaves (this is called a \"stump\"). AdaBoost models create a forest of stumps.\n- In RandomForest,each tree's vote is the same worth for the final prediction, this is not the case with AdaBoost where some stumps have more say in the final prediction. The amount of say is related to the ability of the stump to correct the errors made on previous ones.\n- In RandomForest, each trees are build independently, this is not the case in AdaBoost. The order of creation of the stumps is important. The error that one stump made influence the next stump and so on.\n- The **learning_rate** is used here to prevent the model to fit the data to well (to start overfitting the data).\n","07d85a33":"### Support Vector Regressor\nThis is regressor is part of the support vector machine algorithms. To really understand how support vector machine works you have to go deeper in the math. However, I'll try to give a vague idea of what you have to know. This kind of estimator relies on defining a loss function that ignores errors for all the points located inside a tube (a tube of k dimensions depending on the problem) that encapsulate the true value. The value **epsilon** we have to define as a parameter for the SVR is the width of the tube.\n- **C** is used for regularization, this is a L2 suared penalty.","b4a5eb07":"Now it is your turn to optimize these parameters to get the best results !","14d9fc98":"## 4.3 Defining the models\nwe'll define several models and select the ones that performs better","f1ba3fe5":"Nice and blue !\n\n## 3.3 Changing numericals to categoricals\n\nLet's turn some numerical values to categorical ones with another transformer:","ed3f9871":"## Fit the models to the whole dataset\nIf you are now satified with the root-mean-squared error of every model (once you fine tuned them for hours !) it is time to fit them to the whole dataset. Previously, all we did was just to find the best parameters for each models to compare them. Here things are getting serious !","e6e48065":"Let's see how well each model performed compared to the blended model:","e0f34374":"We can claerly see on this graph that the SalePrice distribution is not normal at all. This is bad news since some linear models perform better on normal data. One transformation one can use to tackle this problem is the log(1+x),let's use it:","24b4350a":"It worked too !","e2434509":"For instance, for the Extorior1st feature, we got 15 categories for the train set and 13 for the test_set. This means that if we use an encoder separatly on the train and test set, the categories won't be encoded the same way and will resulst in a bad prediction.","790e5d1a":"Perfect !","ba2a682a":"## 4.2 Defining usefull functions: score, cross_validation","fc140e81":"# <div id=\"chap1\">1. Set up\n## 1.1 Import the relevant libraries","9527d876":"The two dots on the right of the graphs might be some outliers. These are huge houses but also cheap ones, let's remove them now.","4ac70a40":"# <div id=\"chap4\">4. Modelling\n## 4.1 Import the relevant libraries","ea871312":"## Evaluation of the models\nNow time to see how well models performed. As a reminder, we are using a shuffled 6-fold cross validation to evaluate them.","096ec0f9":"## 3.2 Missing data\nOne big problem of this dataset is the missing data. Some columns are almost empty:","26700d20":"# This is already the end of this notebook, if you found it useful, please consider leaving an upvote. If you want more details or comments on one specific point, leave a comment, I'll try my best to give you an answer !","16cd1ab9":"### Linear regressor\nok this one, I'll assume you already know it !","dcd48500":"### Random Forest Regressor\nHere I'll assume you have some basic knowledge about RandomForest Regressor but just as a reminder (because we'll need this for other models):\n- **n_estimator**: Represents the number of trees that will be created.\n- **min_sample_split**: Specifies the minimum number of samples requiered to split an internal node;\n- **min_sample_leaf**: Specifies the minimum number of sample requiered to be a leaf node;\n- **max_depth**: Represents the depth of each tree in the forest. The deeper the trees, the more it captures information about the input data.\n- **max_features**: represents the number of features to consider when looking for the best split\n    - sqrt: max_feature = sqrt(n_features)\n    - None: max_feature = n_features\n    - log2: max_feature = log2(n_features)\n- **random_state**: Control both randomness of the boostrapping and the sampling of the features to consider when looking for the best split at each node.","49ce5b8b":"## <div id=\"SECTIONS\">SECTIONS<\/div>\n\n**<font size=\"2\"><a href=\"#chap1\">1. Set up<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap2\">2. Data preprocessing<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap3\">3. Feature Engineering<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap4\">4. Modelling<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap5\">5. Building the model<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap6\">6. Submission<\/a><\/font>**","5b06ac49":"The data have already been splitted into train and test set (apparently 50\/50).\nThe missing column in the test set is the SalePrice, obviously !","23638197":"This is way better ! However, one have to pay attention to the final prediction...it's ganna be the log(1 + SalePrice), we'll need to re-transform it later, before the submission !","1d4467d2":"Great !\n## 3.4 Label Encoding","e37497cd":"# <div id=\"chap3\">3. Feature Engineering\nHere we go for the pipeline ! One last thing we need to do before starting is to concatenate the train and test set. Why that ? Because I know from experience on this competition that for some features (columns) the train and test set doesn't have the exact same categorical values, let me show you !","ded7891e":"### Stacking models\nThis is the ultimate trick you want to know, to make our prediction more robust and to get the best of every single model we've made so far, we can use a StackingCVRegressor that will combine multiple models via a meta-regressor.\n\nHere we are using the StackingCVRegressor which is a more advanced ensemble technique implemented in SKlearn, StackingRegressor (but if you understand how this one works, you won't have any trouble for the easiest !).\n\nWith this stacking technique, all the models specified in **regressors** are trained on k-1 folds of the data (CV in the name obviously stands for CrossValidation, hence the spits into k folds). Once there are trained, the prediction is made based on the last fold and is used as input data for the meta-regressor. This is repeated on the entire dataset.\n\nHere, since **use_features_in_secondary**=True, the meta-regressor will be trained on both the predictions from the regressors and the original dataset.","8e761130":"**Time for some explications** (if you've never seen this, do not panic !)\n\nIf you are not very familiar with classes, here is the basic things you need to know: \"*class NaN_transformer(BaseEstimator, TransformerMixin)*\":\n   - **NaN_transformer** is the name of our new transformer.\n   - Once it is defined, we can use it as any other sklearn transformer...thanks to the heredity of the classes \"*BaseEstimator*\" and \"*TransformerMixin*\"\n   - *TransformerMixin* allows us to immediatly use the \"fit_transform\" method without defining it in our own transformer.\n   - *BaseEstimator* allows use to use immediatly \"set_params\" and \"get_params\" methods.\n    \n\"Why there isn't an \"__init__\" method defined here ?\" -Because, in this particular case, we don't need to pass any argument to our transformer.\n\nThe input data in this transformer is defined as X.\n\nThe variable \"y\" is \"none\" since we won't perform any transformation on the target data here.\n\nIf you want to know why missing data are filled this way, I highly suggest you to check this [notebook](http:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) and this [one](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) too.","159c90e3":"This kind of representation is usefull to recognize some pattern in the lack of data. We can clearly see that all the \"BsmtX\" features lack the same amount of data. Same for the \"GarageX\" features. It's now time to deal with them.","0a562543":"## 1.2 Load the data"}}