{"cell_type":{"ed446974":"code","a5c4ca9a":"code","21b624d3":"code","f33cee79":"code","a5ca5755":"code","6eae82f2":"code","a842702d":"code","b1e90c61":"code","9c2358d4":"code","0fcf61b6":"code","989341d4":"code","7913470c":"code","716aa20c":"code","ad75092b":"code","03eae216":"code","953873e7":"code","94d34046":"code","fda485af":"code","2e8754c2":"code","b636e323":"code","1553b94d":"code","7c231a5b":"code","0af13e9a":"code","c19346c7":"markdown","43f296c4":"markdown","06f44d6c":"markdown","008c6ea2":"markdown","fd7d16eb":"markdown","186f3a19":"markdown","5287d79a":"markdown","1a4e3b55":"markdown","cc404d42":"markdown","ae3281a8":"markdown","921d2b9b":"markdown"},"source":{"ed446974":"#!ls ..\/input\/padhaivisdata\n!ln -s ..\/input\/padhaivisdata\/ data\n!ls data\/","a5c4ca9a":"#reading the labels of data we uploaded\nwith open(\"data\/imagenet_labels.txt\") as f:\n    classes = eval(f.read())\n#type(classes)\nprint(list(classes.values())[0:5])","21b624d3":"import warnings\nwarnings.filterwarnings(\"ignore\")","f33cee79":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch \nimport torch.nn as nn\nimport torchvision\nimport torchvision.datasets as datasets\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms","a5ca5755":"# parameters\n\nbatch_size = 1 #batch size\ncuda = True","6eae82f2":"#defining the transformations for the data\n\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    #normalize the images with imagenet data mean and std\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])","a842702d":"#define the data we uploaded as evaluation data and apply the transformations\nevalset = torchvision.datasets.ImageFolder(root = \".\/data\/imagenet\", transform = transform)\n\n#create a data loader for evaluation\nevalloader = torch.utils.data.DataLoader(evalset, batch_size = batch_size, shuffle = True)","b1e90c61":"#looking at data using iter\ndataiter = iter(evalloader)\nimages, labels = dataiter.next()\n\n#shape of images bunch\nprint(images.shape)\n\n#label of the image\nprint(labels[0].item())","9c2358d4":"#for visualization we will use vgg16 pretrained on imagenet data\n\nmodel = models.vgg16(pretrained=True)\nif cuda: model.cuda()\n\nmodel.eval()","0fcf61b6":"def imshow(img, title):\n  \"\"\"Custom function to display the image using matplotlib\"\"\"\n  \n  #define std correction to be made\n  std_correction = np.asarray([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n  \n  #define mean correction to be made\n  mean_correction = np.asarray([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n  \n  #convert the tensor img to numpy img and de normalize \n  if cuda: img = img.cpu()\n  npimg = np.multiply(img.numpy(), std_correction) + mean_correction\n  \n  #plot the numpy image\n  plt.figure(figsize = (batch_size * 4, 4))\n  plt.axis(\"off\")\n  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n  plt.title(title)\n  plt.show()\n\n\ndef show_batch_images(dataloader):\n  \"\"\"custom function to fetch images from dataloader\"\"\"\n\n  images,_ = next(iter(dataloader))\n  if cuda: images = images.cuda()\n  \n  #run the model on the images\n  outputs = model(images)\n  if cuda: outputs = outputs.cpu()\n  \n  #get the maximum class \n  _, pred = torch.max(outputs.data, 1)\n  \n  #make grid\n  img = torchvision.utils.make_grid(images)\n  \n  #call the function\n  imshow(img, title=[classes[x.item()] for x in pred])\n  \n  return images, pred","989341d4":"images, pred = show_batch_images(evalloader)","7913470c":"#running inference on the images without occlusion\n\n#vgg16 pretrained model\nif cuda: images = images.cuda()\noutputs = model(images)\n\n#passing the outputs through softmax to interpret them as probability\noutputs = nn.functional.softmax(outputs, dim = 1)\n\n#getting the maximum predicted label\nprob_no_occ, pred = torch.max(outputs.data, 1)\n\n#get the first item\nprob_no_occ = prob_no_occ[0].item()\n\nprint(prob_no_occ)","716aa20c":"def occlusion(model, image, label, occ_size = 50, occ_stride = 50, occ_pixel = 0.5):\n    \"\"\"custom function to conduct occlusion experiments\"\"\"\n  \n    #get the width and height of the image\n    width, height = image.shape[-2], image.shape[-1]\n  \n    #setting the output image width and height\n    output_height = int(np.ceil((height-occ_size)\/occ_stride))\n    output_width = int(np.ceil((width-occ_size)\/occ_stride))\n  \n    #create a white image of sizes we defined\n    heatmap = torch.zeros((output_height, output_width))\n    \n    #iterate all the pixels in each column\n    for h in range(0, height):\n        for w in range(0, width):\n            \n            h_start = h*occ_stride\n            w_start = w*occ_stride\n            h_end = min(height, h_start + occ_size)\n            w_end = min(width, w_start + occ_size)\n            \n            if (w_end) >= width or (h_end) >= height:\n                continue\n            \n            input_image = image.clone().detach()\n            \n            #replacing all the pixel information in the image with occ_pixel(grey) in the specified location\n            input_image[:, :, w_start:w_end, h_start:h_end] = occ_pixel\n            if cuda: input_image = input_image.cuda()\n            \n            #run inference on modified image\n            output = model(input_image)\n            output = nn.functional.softmax(output, dim=1)\n            prob = output.tolist()[0][label]\n            \n            #setting the heatmap location to probability value\n            heatmap[h, w] = prob \n\n    return heatmap","ad75092b":"heatmap = occlusion(model, images, pred[0].item(), 32, 14)","03eae216":"#displaying the image using seaborn heatmap and also setting the maximum value of gradient to probability\nimgplot = sns.heatmap(heatmap, xticklabels=False, yticklabels=False, vmax=prob_no_occ)\nfigure = imgplot.get_figure()    \nfigure.savefig('svm_conf.png', dpi=400)","953873e7":"#for filter visualization, we will use alexnet pretrained with imagenet data\n\nalexnet = models.alexnet(pretrained=True)\n#if cuda: alexnet.cuda()\n\nprint(alexnet)","94d34046":"def plot_filters_single_channel_big(t):\n    \n    #setting the rows and columns\n    nrows = t.shape[0]*t.shape[2]\n    ncols = t.shape[1]*t.shape[3]\n    \n    \n    npimg = np.array(t.numpy(), np.float32)\n    npimg = npimg.transpose((0, 2, 1, 3))\n    npimg = npimg.ravel().reshape(nrows, ncols)\n    \n    npimg = npimg.T\n    \n    fig, ax = plt.subplots(figsize=(ncols\/10, nrows\/200))    \n    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)\n    \n\ndef plot_filters_single_channel(t):\n    \n    #kernels depth * number of kernels\n    nplots = t.shape[0]*t.shape[1]\n    ncols = 12\n    \n    nrows = 1 + nplots\/\/ncols\n    #convert tensor to numpy image\n    npimg = np.array(t.numpy(), np.float32)\n    \n    count = 0\n    fig = plt.figure(figsize=(ncols, nrows))\n    \n    #looping through all the kernels in each channel\n    for i in range(t.shape[0]):\n        for j in range(t.shape[1]):\n            count += 1\n            ax1 = fig.add_subplot(nrows, ncols, count)\n            npimg = np.array(t[i, j].numpy(), np.float32)\n            npimg = (npimg - np.mean(npimg)) \/ np.std(npimg)\n            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n            ax1.imshow(npimg)\n            ax1.set_title(str(i) + ',' + str(j))\n            ax1.axis('off')\n            ax1.set_xticklabels([])\n            ax1.set_yticklabels([])\n   \n    plt.tight_layout()\n    plt.show()\n    \n\ndef plot_filters_multi_channel(t):\n    \n    #get the number of kernals\n    num_kernels = t.shape[0]    \n    \n    #define number of columns for subplots\n    num_cols = 12\n    #rows = num of kernels\n    num_rows = num_kernels\n    \n    #set the figure size\n    fig = plt.figure(figsize=(num_cols,num_rows))\n    \n    #looping through all the kernels\n    for i in range(t.shape[0]):\n        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n        \n        #for each kernel, we convert the tensor to numpy \n        npimg = np.array(t[i].numpy(), np.float32)\n        #standardize the numpy image\n        npimg = (npimg - np.mean(npimg)) \/ np.std(npimg)\n        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n        npimg = npimg.transpose((1, 2, 0))\n        ax1.imshow(npimg)\n        ax1.axis('off')\n        ax1.set_title(str(i))\n        ax1.set_xticklabels([])\n        ax1.set_yticklabels([])\n        \n    plt.savefig('myimage.png', dpi=100)    \n    plt.tight_layout()\n    plt.show()\n    \ndef plot_weights(model, layer_num, single_channel = True, collated = False):\n  \n  #extracting the model features at the particular layer number\n  layer = model.features[layer_num]\n  \n  #checking whether the layer is convolution layer or not \n  if isinstance(layer, nn.Conv2d):\n    #getting the weight tensor data\n    weight_tensor = model.features[layer_num].weight.data\n    \n    if single_channel:\n      if collated:\n        plot_filters_single_channel_big(weight_tensor)\n      else:\n        plot_filters_single_channel(weight_tensor)\n        \n    else:\n      if weight_tensor.shape[1] == 3:\n        plot_filters_multi_channel(weight_tensor)\n      else:\n        print(\"Can only plot weights with three channels with single channel = False\")\n        \n  else:\n    print(\"Can only visualize layers which are convolutional\")","fda485af":"#visualize weights for alexnet - first conv layer\n\nplot_weights(alexnet, 0, single_channel = False)","2e8754c2":"#plotting single channel images\n\nplot_weights(alexnet, 0, single_channel = True)","b636e323":"#plot for 3rd layer -> 2nd conv layer\n\nplot_weights(alexnet, 3, single_channel = True)","1553b94d":"plot_weights(alexnet, 0, single_channel = True, collated = True)","7c231a5b":"plot_weights(alexnet, 3, single_channel = True, collated = True)","0af13e9a":"plot_weights(alexnet, 6, single_channel = True, collated = True)","c19346c7":"## 5 Filter visualisation","43f296c4":"plot_weights(model, 2, single_channel = True, collated = True)","06f44d6c":"## 1 Load dataset","008c6ea2":"## 3 Visualise image","fd7d16eb":"## 2 Load pretrained model","186f3a19":"# Visualization of CNN Layers and Filters","5287d79a":"## Outline\n1. Using torchvision.datasets with a custom folder of images\n2. Occlusion analysis with pretrained model\n3. Filter visualisation with pretrained model","1a4e3b55":"## 4 Occlusion analysis","cc404d42":"plot_weights(model, 0, single_channel = False, collated = False)","ae3281a8":"#for vgg16\n\nplot_weights(model, 0, single_channel = True, collated = True)","921d2b9b":"plot_weights(model, 5, single_channel = True, collated = True)"}}