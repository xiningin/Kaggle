{"cell_type":{"c2e9f804":"code","18a1e178":"code","05e9f5a1":"code","1b2c094d":"code","cfc4cd5b":"code","b4a0b02c":"code","0f1c8dfa":"code","2f98daee":"code","b0378364":"code","f3227a25":"code","e0d3c785":"code","dcc7dcbb":"code","b229435c":"code","097ef319":"code","d07b42d2":"code","bf8681e0":"code","67ff89f8":"code","92199950":"code","e89be22d":"code","c3107508":"code","74dd8d50":"code","67879d5a":"code","2287f6f8":"code","1874b316":"code","225ae4cd":"code","a285a747":"code","da6c880a":"code","64dda757":"code","349171b1":"code","a643b5ba":"code","1ef402af":"code","c7f0a5ce":"code","d6246eb6":"code","2c287236":"code","91606227":"code","e9a90657":"code","76d5bc76":"code","a257733e":"code","b4228c41":"code","97cd6d6e":"code","af6548b2":"code","e0156a3c":"code","d8fe959c":"code","a58630ba":"code","8be02d19":"code","7e5c74e7":"code","1376ec36":"code","6f97b733":"code","ec6d18f1":"code","cb2e60c0":"code","63c198ce":"code","178e6c9f":"code","0e0b9dc3":"code","7c9ee636":"code","1ebf6868":"code","7350603f":"code","785473b6":"code","f88f827b":"code","d32e4385":"code","4afa64c0":"code","76e6b36e":"code","60f88f5a":"code","dd262d44":"code","d3f77f47":"code","4a4d8b0a":"code","e1de0bfa":"code","3870c50a":"code","7902f30d":"code","d89c2e6a":"code","6773c9fc":"code","eb8015d3":"code","684c46c1":"code","10b64a44":"code","9dfd9df7":"markdown","e0e08fc5":"markdown","216ab4f4":"markdown","4dc7a2f6":"markdown","129185ca":"markdown","021db613":"markdown","56374133":"markdown","77d5935d":"markdown","39e723c4":"markdown","ae587440":"markdown","668574d1":"markdown","611c6468":"markdown","657a6674":"markdown"},"source":{"c2e9f804":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","18a1e178":"from collections import defaultdict\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nimport os\nfrom pprint import pprint\nimport warnings\nfrom fbprophet import Prophet\nfrom fbprophet.plot import add_changepoints_to_plot\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib\nfrom matplotlib.ticker import ScalarFormatter\n%matplotlib inline\nimport numpy as np\nimport optuna\noptuna.logging.disable_default_handler()\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport seaborn as sns\nfrom scipy.optimize import curve_fit\nfrom scipy.integrate import solve_ivp\nimport pystan.misc","05e9f5a1":"plt.style.use(\"seaborn-ticks\")\nplt.rcParams[\"xtick.direction\"] = \"in\"\nplt.rcParams[\"ytick.direction\"] = \"in\"\nplt.rcParams[\"font.size\"] = 11.0\nplt.rcParams[\"figure.figsize\"] = (9, 6)","1b2c094d":"warnings.simplefilter(\"ignore\")","cfc4cd5b":"path = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/\"","b4a0b02c":"train_data = pd.read_csv(path+\"train.csv\")\n#train_data = train_data[(train_data.Country_Region==\"US\") ]\ntrain_df = train_data\n\ntrain_df['area'] = [str(i)+str(' - ')+str(j) for i,j in zip(train_data['Country_Region'], train_data['Province_State'])]\ntrain_df['Date'] = pd.to_datetime(train_df['Date'])\nfull_data = train_df","0f1c8dfa":"#train_df.tail(50)","2f98daee":"today = full_data['Date'].max()+timedelta(days=1) \nprint (today)\nprint ('Last update of this dataset was ' + str(train_df.loc[len(train_df)-1]['Date']))\n","b0378364":"def get_country_data(train_df, area, metric):\n    country_data = train_df[train_df['area']==area]\n    country_data = country_data.drop(['Id','Province_State', 'Country_Region'], axis=1)\n    country_data = pd.pivot_table(country_data, values=['ConfirmedCases','Fatalities'], index=['Date'], aggfunc=np.sum) \n    country_data = country_data[country_data[metric]!=0]\n    tmp = country_data.shift(periods=1,fill_value=0)\n    country_data['prior_confirmed'] = tmp.ConfirmedCases\n    country_data['prior_deaths'] = tmp.Fatalities\n    country_data['DailyConfirmed'] = country_data.ConfirmedCases - country_data.prior_confirmed\n    country_data['DailyFatalities'] = country_data.Fatalities - country_data.prior_deaths\n    return country_data        ","f3227a25":"area_info = pd.DataFrame(columns=['area', 'cases_start_date', 'deaths_start_date', 'init_ConfirmedCases', 'init_Fatalities','init_DailyConfirmed','init_DailyFatalities'])\nfor i in range(len(train_df['area'].unique())):\n    area = train_df['area'].unique()[i]\n    area_cases_data = get_country_data(train_df, area, 'ConfirmedCases')\n    #print (area_cases_data)\n   \n    area_deaths_data = get_country_data(train_df, area, 'Fatalities')\n    cases_start_date = area_cases_data.index.min()\n    deaths_start_date = area_deaths_data.index.min()\n    if len(area_cases_data) > 0:\n        confirmed_cases = max(area_cases_data['ConfirmedCases'])\n        last = area_cases_data.tail(1)\n        last_daily = np.float(last['DailyConfirmed'])\n\n    else:\n        confirmed_cases = 0\n        last_daily = 0\n    if len(area_deaths_data) > 0:\n        fatalities = max(area_deaths_data['Fatalities'])\n        last = area_deaths_data.tail(1)\n        last_death = np.float(last['DailyFatalities'])\n    else:\n        fatalities = 0\n        last_death = 0\n    #print (last_daily)\n    #print (last_death)\n    area_info.loc[i] = [area, cases_start_date, deaths_start_date, confirmed_cases, fatalities,last_daily,last_death]\narea_info = area_info.fillna(pd.to_datetime(today))\narea_info['init_cases_day_no'] = pd.to_datetime(today)-area_info['cases_start_date']\narea_info['init_cases_day_no'] = area_info['init_cases_day_no'].dt.days.fillna(0).astype(int)\narea_info['init_deaths_day_no'] = pd.to_datetime(today)-area_info['deaths_start_date']\narea_info['init_deaths_day_no'] = area_info['init_deaths_day_no'].dt.days.fillna(0).astype(int)\n#area_info['init_DailyConfirmed'] = last_daily.astype(float)\n#area_info['init_DailyFatalities'] = last_death.astype(float)\narea_info.head()","e0d3c785":"def make_cdf (y):\n    cdf = []\n    for i in range(1,len(y)+1): \n        total = np.sum(y[:i])\n        #print (total)\n        cdf.append(total)\n        #print (cdf)\n    return cdf","dcc7dcbb":"from scipy.special import factorial\ndef gamma_pdf(x, k, lam, ymax):\n\n    k = np.float(k)\n    #print ('k is ' + str(k))\n    \n    num = ymax * (np.power(lam,k) * np.power(x,(k-1)) * np.exp(-lam*x))\n    if k < 0.5:\n        k = 1\n    else:\n         k = np.round(k)    \n    den = (factorial (k-1))\n    return num\/den\n\n    \ndef gamma_fit(train_df, area, metric,to_fit, est_count):\n    area_data = get_country_data(train_df, area, metric)\n    x_data = range(len(area_data.index))\n    y_data = area_data[to_fit]\n    x_data = np.array(x_data,dtype='float64')\n    y_data = np.array(y_data,dtype='float64')\n    #x_data = x_data.ravel()\n    #y_data = y_data.ravel()\n    #_data = np.asarray(x_data).ravel()\n    #y_data = np.asarray(y_data).ravel()\n    #print (y_data)\n    if len(y_data) < 5:\n        estimated_k = 6  \n        estimated_lam = 0.1 \n        ymax = np.float(est_count)\n    elif max(y_data) == 0:\n        estimated_k = 6  \n        estimated_lam = 0.1 \n        ymax = np.float(est_count)\n    else:\n        \n        p0_est=[6.0 ,0.1,est_count]\n        try:\n            popt, pcov = curve_fit(gamma_pdf, x_data, y_data,bounds=([0,0,0],100000000),p0=p0_est, maxfev=1000000)\n                                   #bounds=([0,0,0],100000000), p0=p0_est, maxfev=1000000)\n            estimated_k, estimated_lam, ymax = popt\n        except RuntimeError:\n            print(area)\n            print(\"Runtime Error - curve_fit failed\") \n            estimated_k = 6  \n            estimated_lam = 0.1 \n            ymax = est_count\n        #else:\n        #    print(area)\n        #    print(\"Catch all Error - curve_fit failed\") \n        #    estimated_k = 5  \n        #    estimated_lam = 0.1 \n        #    ymax = est_count\n\n    estimated_parameters = pd.DataFrame(np.array([[area, estimated_k, estimated_lam, ymax]]), columns=['area', 'k', 'lam', 'ymax'])\n    return estimated_parameters","b229435c":"def get_parameters(metric, to_fit):\n    parameters = pd.DataFrame(columns=['area', 'k', 'lam', 'ymax'], dtype=np.float)\n    for area in train_df['area'].unique():\n        #print ('Area fitting is ' + area)\n        if metric == 'ConfirmedCases':\n            init = area_info[area_info.area == area]['init_ConfirmedCases']\n        else:\n            init = area_info[area_info.area == area]['init_Fatalities']\n        init = init.astype(float)\n        #print (init)\n        # establish an initial guess for maxy\n        est_count = init * 4.0\n        #print (est_count)\n        estimated_parameters = gamma_fit(train_df, area, metric, to_fit, est_count)\n        parameters = parameters.append(estimated_parameters)\n    if True:\n        try:\n            parameters['k'] = pd.to_numeric(parameters['k'], downcast=\"float\")\n            parameters['lam'] = pd.to_numeric(parameters['lam'], downcast=\"float\")\n            parameters['ymax'] = pd.to_numeric(parameters['ymax'], downcast=\"float\")\n        except RuntimeError: \n            print ('run time error')\n        except TypeError:\n            print ('type error')\n        #else:\n        #    print (\"error on parameter conversion\")\n        #parameters = parameters.replace({'k': {-1: parameters[parameters['ymax']>0].median()[0]}, \n        #                                 'lam': {-1: parameters[parameters['ymax']>0].median()[1]}, \n        #                                 'ymax': {-1: parameters[parameters['ymax']>0].median()[2]}})\n    return parameters","097ef319":"cases_parameters = get_parameters('ConfirmedCases','DailyConfirmed')\ncases_parameters.tail(20)","d07b42d2":"deaths_parameters = get_parameters('Fatalities','DailyFatalities')\ndeaths_parameters.tail(20)","bf8681e0":"fit_df = area_info.merge(cases_parameters, on='area', how='left')\nfit_df = fit_df.rename(columns={\"k\": \"cases_k\", \"lam\": \"cases_lam\", \"ymax\": \"cases_ymax\"})\nfit_df = fit_df.merge(deaths_parameters, on='area', how='left')\nfit_df = fit_df.rename(columns={\"k\": \"deaths_k\", \"lam\": \"deaths_lam\", \"ymax\": \"deaths_ymax\"})\n\nfit_df.head()\n","67ff89f8":"test_data = pd.read_csv(path+\"test.csv\")\ntest_df = test_data.copy()\n#test_df = test_data[(test_data.Country_Region==\"US\") & (test_data.Province_State != 'x')].copy()\ntest_df['area'] = [str(i)+str(' - ')+str(j) for i,j in zip(test_df['Country_Region'], test_df['Province_State'])]\n\ntest_df = test_df.merge(fit_df, on='area', how='left')\ntest_df = test_df.merge(cases_parameters, on='area', how='left')\n#print (len(test_df))\n\n#test_df = test_df.rename(columns={\"k\": \"cases_k\", \"lam\": \"cases_lam\", \"ymax\": \"cases_ymax\"})\n#test_df = test_df.merge(deaths_parameters, on='area', how='left')\n#test_df = test_df.rename(columns={\"k\": \"deaths_k\", \"lam\": \"deaths_lam\", \"ymax\": \"deaths_ymax\"})\n#test_df.cases_k = test_df.cases_k.astype(float)\n#test_df.deaths_k = test_df.deaths_k.astype(float)\n#for i,t in test_df.iterrows():\n#    print (i)\n#    print (t.area)\n#    print (t.cases_k)\ntest_df['Date'] = pd.to_datetime(test_df['Date'])\ntest_df['cases_start_date'] = pd.to_datetime(test_df['cases_start_date'])\ntest_df['deaths_start_date'] = pd.to_datetime(test_df['deaths_start_date'])\n\ntest_df['cases_day_no'] = test_df['Date']-test_df['cases_start_date']\ntest_df['cases_day_no'] = test_df['cases_day_no'].dt.days.fillna(0).astype(int)\ntest_df['deaths_day_no'] = test_df['Date']-test_df['deaths_start_date']\ntest_df['deaths_day_no'] = test_df['deaths_day_no'].dt.days.fillna(0).astype(int)\ntest_df['DailyFatalities_fit'] = 0","92199950":"fit_df[(fit_df.area>'US') & (fit_df.area < 'UT')]","e89be22d":"#y = gamma_pdf(d, ['cases_k']), t['cases_lam'], t['cases_ymax'])\npred_yd = []\npred_yc = []\nfor (idx, df) in test_df.iterrows():\n    #print('for death day ' + str(df['deaths_day_no']))\n    y  = gamma_pdf(df['deaths_day_no'], df['deaths_k'], df['deaths_lam'], df['deaths_ymax'])\n    #print (y)\n    pred_yd.append([df.area,y])\n    #print ('for confirmed day ' + str(df['cases_day_no']))\n    yc = gamma_pdf(df['cases_day_no'], df['cases_k'], df['cases_lam'], df['cases_ymax'])\n    pred_yc.append([df.area,yc])\n    #test_df['DailyCases_pred'] = round(test_df['DailyConfirmed_fit']+test_df['DailyConfirmed_error'])\n\n    #test_df['DailyFatalities_pred'] = round(test_df['DailyFatalities_fit']+test_df['DailyFatalities_error'])\n","c3107508":"yd_df = pd.DataFrame( pred_yd)\nyc_df = pd.DataFrame( pred_yc)","74dd8d50":"yc_df.columns = ['Area','Predicted']\nyd_df.columns = ['Area','Predicted']","67879d5a":"def make_pred(df):\n    cdf_all = pd.DataFrame()\n    for a in df['Area'].unique():\n        tmp = df[df.Area==a]\n        cdf = make_cdf (tmp.Predicted)\n        cdf = pd.DataFrame(cdf)\n        cdf_all = pd.concat([cdf_all, cdf])\n    return cdf_all\n\ncdfc = make_pred(yc_df)\ncdfd = make_pred(yd_df)\ncdfc.columns =['Pred']\ncdfd.columns =['Pred']","2287f6f8":"test_df['DailyFatalities_fit'] = np.round(cdfd.Pred.values)\ntest_df['DailyCases_fit'] = np.round(cdfc.Pred.values)","1874b316":"# generate submission\nsubmission = pd.DataFrame(data={'ForecastId': test_df['ForecastId'], 'ConfirmedCases': test_df['DailyCases_fit'], 'Fatalities': test_df['DailyFatalities_fit']}).fillna(0.5)\n# submission.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","225ae4cd":"submission.head()","a285a747":"def line_plot(df, title, xlabel=None, ylabel=\"Cases\", h=None, v=None,\n              xlim=(None, None), ylim=(0, None), math_scale=True, y_logscale=False, y_integer=False):\n    \"\"\"\n    Show chlonological change of the data.\n    \"\"\"\n    ax = df.plot()\n    if math_scale:\n        ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n        ax.ticklabel_format(style=\"sci\",  axis=\"y\",scilimits=(0, 0))\n    if y_logscale:\n        ax.set_yscale(\"log\")\n    if y_integer:\n        fmt = matplotlib.ticker.ScalarFormatter(useOffset=False)\n        fmt.set_scientific(False)\n        ax.yaxis.set_major_formatter(fmt)\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xlim(*xlim)\n    ax.set_ylim(*ylim)\n    ax.legend(bbox_to_anchor=(1.02, 0), loc=\"lower left\", borderaxespad=0)\n    if h is not None:\n        ax.axhline(y=h, color=\"black\", linestyle=\"--\")\n    if v is not None:\n        if not isinstance(v, list):\n            v = [v]\n        for value in v:\n            ax.axvline(x=value, color=\"black\", linestyle=\"--\")\n    plt.tight_layout()\n    plt.show()","da6c880a":"def select_area(ncov_df, group=\"Date\", places=None, areas=None, excluded_places=None,\n                start_date=None, end_date=None, date_format=\"%d%b%Y\"):\n    \"\"\"\n    Select the records of the palces.\n    @ncov_df <pd.DataFrame>: the clean data\n    @group <str or None>: group-by the group, or not perform (None)\n    @area or @places:\n        if ncov_df has Country and Province column,\n            @places <list[tuple(<str\/None>, <str\/None>)]: the list of places\n                - if the list is None, all data will be used\n                - (str, str): both of country and province are specified\n                - (str, None): only country is specified\n                - (None, str) or (None, None): Error\n        if ncov_df has Area column,\n            @areas <list[str]>: the list of area names\n                - if the list is None, all data will be used\n                - eg. Japan\n                - eg. US\/California\n    @excluded_places <list[tuple(<str\/None>, <str\/None>)]: the list of excluded places\n        - if the list is None, all data in the \"places\" will be used\n        - (str, str): both of country and province are specified\n        - (str, None): only country is specified\n        - (None, str) or (None, None): Error\n    @start_date <str>: the start date or None\n    @end_date <str>: the start date or None\n    @date_format <str>: format of @start_date and @end_date\n    @return <pd.DataFrame>: index and columns are as same as @ncov_df\n    \"\"\"\n    # Select the target records\n    df = ncov_df.copy()\n    if (places is not None) or (excluded_places is not None):\n        c_series = df[\"Country\"]\n        p_series = df[\"Province\"]\n        if places is not None:\n            df = pd.DataFrame(columns=ncov_df.columns)\n            for (c, p) in places:\n                if c is None:\n                    raise Exception(\"places: Country must be specified!\")\n                if p is None:\n                    new_df = ncov_df.loc[c_series == c, :]\n                else:\n                    new_df = ncov_df.loc[(c_series == c) & (p_series == p), :]\n                df = pd.concat([df, new_df], axis=0)\n        if excluded_places is not None:\n            for (c, p) in excluded_places:\n                if c is None:\n                    raise Exception(\"excluded_places: Country must be specified!\")\n                if p is None:\n                    df = df.loc[c_series != c, :]\n                else:\n                    c_df = df.loc[(c_series == c) & (p_series != p), :]\n                    other_df = df.loc[c_series != c, :]\n                    df = pd.concat([c_df, other_df], axis=0)\n    if areas is not None:\n        df = df.loc[df[\"Area\"].isin(areas), :]\n    if group is not None:\n        df = df.groupby(group).sum().reset_index()\n    # Range of date\n    if start_date is not None:\n        df = df.loc[df[\"Date\"] >= datetime.strptime(start_date, date_format), :]\n    if end_date is not None:\n        df = df.loc[df[\"Date\"] <= datetime.strptime(end_date, date_format), :]\n    # Only use the records with Confirmed > 0\n    try:\n        df = df.loc[df[\"Confirmed\"] > 0, :]\n    except KeyError:\n        pass\n    # Aleart empty\n    if df.empty:\n        raise Exception(\"The output dataframe is empty!\")\n    return df","64dda757":"def show_trend(ncov_df, name=None, variable=\"Confirmed\", n_changepoints=2, **kwargs):\n    \"\"\"\n    Show trend of log10(@variable) using fbprophet package.\n    @ncov_df <pd.DataFrame>: the clean data\n    @variable <str>: variable name to analyse\n        - if Confirmed, use Infected + Recovered + Deaths\n    @n_changepoints <int>: max number of change points\n    @kwargs: keword arguments of select_area()\n    \"\"\"\n    # Data arrangement\n    df = select_area(ncov_df, **kwargs)\n    df = df.loc[:, [\"Date\", variable]]\n    df.columns = [\"ds\", \"y\"]\n    # Log10(x)\n    warnings.resetwarnings()\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        df[\"y\"] = np.log10(df[\"y\"]).replace([np.inf, -np.inf], 0)\n    # fbprophet\n    model = Prophet(growth=\"linear\", daily_seasonality=False, n_changepoints=n_changepoints)\n    model.fit(df)\n    future = model.make_future_dataframe(periods=0)\n    forecast = model.predict(future)\n    # Create figure\n    fig = model.plot(forecast)\n    _ = add_changepoints_to_plot(fig.gca(), model, forecast)\n    if name is None:\n        try:\n            name = f\"{kwargs['places'][0][0]}: \"\n        except Exception:\n            name = str()\n    else:\n        name = f\"{name}: \"\n    plt.title(f\"{name}log10({variable}) over time and chainge points\")\n    plt.ylabel(f\"log10(the number of cases)\")\n    plt.xlabel(\"\")","349171b1":"def create_target_df(ncov_df, total_population,\n                     confirmed=\"Confirmed\", recovered=\"Recovered\", fatal=\"Deaths\", **kwargs):\n    \"\"\"\n    Select the records of the places, calculate the number of susceptible people,\n     and calculate the elapsed time [day] from the start date of the target dataframe.\n    @ncov_df <pd.DataFrame>: the clean data\n    @total_population <int>: total population in the places\n    column names in @ncov_df:\n        @confirmed <str>: column name of the number of confirmed cases\n        @recovered <str>: column name of the number of recovered cases\n        @fatal <str>: column name of the number of fatal cases\n    @kwargs: keword arguments of select_area()\n    @return <tuple(2 objects)>:\n        - 1. first_date <pd.Timestamp>: the first date of the selected records\n        - 2. target_df <pd.DataFrame>:\n            - column T: elapsed time [min] from the start date of the dataset\n            - column Susceptible: the number of patients who are in the palces but not infected\/recovered\/died\n            - column Infected: the number of infected cases\n            - column Recovered: the number of recovered cases\n            - column Deaths: the number of death cases\n    \"\"\"\n    # Select the target records\n    df = select_area(ncov_df, **kwargs)\n    first_date = df.loc[df.index[0], \"Date\"]\n    # column T\n    df[\"T\"] = ((df[\"Date\"] - first_date).dt.total_seconds() \/ 60).astype(int)\n    # coluns except T\n    cols = [confirmed, recovered, fatal]\n    if not set(cols).issubset(set(df.columns)):\n        raise KeyError(f\"ncov_df must have {', '.join(cols)} column!\")\n    df[\"Susceptible\"] = total_population - df[confirmed]\n    df[\"Infected\"] = df[confirmed] - df[recovered] - df[fatal]\n    df[\"Recovered\"] = df[recovered]\n    df[\"Fatal\"] = df.loc[:, fatal]\n    response_variables = [\"Susceptible\", \"Infected\", \"Recovered\", \"Fatal\"]\n    # Return\n    target_df = df.loc[:, [\"T\", *response_variables]]\n    return (first_date, target_df)","a643b5ba":"def simulation(model, initials, step_n, **params):\n    \"\"\"\n    Solve ODE of the model.\n    @model <ModelBase>: the model\n    @initials <tuple[float]>: the initial values\n    @step_n <int>: the number of steps\n    @params: the paramerters of the model\n    \"\"\"\n    tstart, dt, tend = 0, 1, step_n\n    sol = solve_ivp(\n        fun=model(**params),\n        t_span=[tstart, tend],\n        y0=np.array(initials, dtype=np.float64),\n        t_eval=np.arange(tstart, tend + dt, dt),\n        dense_output=False  # True\n    )\n    t_df = pd.Series(data=sol[\"t\"], name=\"t\")\n    y_df = pd.DataFrame(data=sol[\"y\"].T.copy(), columns=model.VARIABLES)\n    sim_df = pd.concat([t_df, y_df], axis=1)\n    return sim_df","1ef402af":"class ModelBase(object):\n    NAME = \"Model\"\n    VARIABLES = [\"x\"]\n    PRIORITIES = np.array([1])\n    QUANTILE_RANGE = [0.3, 0.7]\n    MONOTONIC = [\"x\"]\n\n    @classmethod\n    def param_dict(cls, train_df_divided=None, q_range=None):\n        \"\"\"\n        Define parameters without tau. This function should be overwritten.\n        @train_df_divided <pd.DataFrame>:\n            - column: t and non-dimensional variables\n        @q_range <list[float, float]>: quantile rage of the parameters calculated by the data\n        @return <dict[name]=(min, max):\n            @min <float>: min value\n            @max <float>: max value\n        \"\"\"\n        param_dict = dict()\n        return param_dict\n\n    @staticmethod\n    def calc_variables(df):\n        \"\"\"\n        Calculate the variables of the model.\n        This function should be overwritten.\n        @df <pd.DataFrame>\n        @return <pd.DataFrame>\n        \"\"\"\n        return df\n\n    @staticmethod\n    def calc_variables_reverse(df):\n        \"\"\"\n        Calculate measurable variables using the variables of the model.\n        This function should be overwritten.\n        @df <pd.DataFrame>\n        @return <pd.DataFrame>\n        \"\"\"\n        return df\n\n    @classmethod\n    def create_dataset(cls, ncov_df, total_population, **kwargs):\n        \"\"\"\n        Create dataset with the model-specific varibles.\n        The variables will be divided by total population.\n        The column names (not include T) will be lower letters.\n        **kwargs: See the function named create_target_df()\n        @return <tuple(objects)>:\n            - start_date <pd.Timestamp>\n            - initials <tuple(float)>: the initial values\n            - Tend <int>: the last value of T\n            - df <pd.DataFrame>: the dataset\n        \"\"\"\n        start_date, target_df = create_target_df(ncov_df, total_population, **kwargs)\n        df = cls.calc_variables(target_df).set_index(\"T\") \/ total_population\n        df.columns = [n.lower() for n in df.columns]\n        initials = df.iloc[0, :].values\n        df = df.reset_index()\n        Tend = df.iloc[-1, 0]\n        return (start_date, initials, Tend, df)\n\n    def calc_r0(self):\n        \"\"\"\n        Calculate R0. This function should be overwritten.\n        \"\"\"\n        return None\n\n    def calc_days_dict(self, tau):\n        \"\"\"\n        Calculate 1\/beta [day] etc.\n        This function should be overwritten.\n        @param tau <int>: tau value [hour]\n        \"\"\"\n        return dict()","c7f0a5ce":"class SIRF(ModelBase):\n    NAME = \"SIR-F\"\n    VARIABLES = [\"x\", \"y\", \"z\", \"w\"]\n    PRIORITIES = np.array([1, 10, 10, 2])\n    MONOTONIC = [\"z\", \"w\"]\n\n    def __init__(self, theta, kappa, rho, sigma):\n        super().__init__()\n        self.theta = theta\n        self.kappa = kappa\n        self.rho = rho\n        self.sigma = sigma\n\n    def __call__(self, t, X):\n        # x, y, z, w = [X[i] for i in range(len(self.VARIABLES))]\n        # dxdt = - self.rho * x * y\n        # dydt = self.rho * (1 - self.theta) * x * y - (self.sigma + self.kappa) * y\n        # dzdt = self.sigma * y\n        # dwdt = self.rho * self.theta * x * y + self.kappa * y\n        dxdt = - self.rho * X[0] * X[1]\n        dydt = self.rho * (1 - self.theta) * X[0] * X[1] - (self.sigma + self.kappa) * X[1]\n        dzdt = self.sigma * X[1]\n        dwdt = self.rho * self.theta * X[0] * X[1] + self.kappa * X[1]\n        return np.array([dxdt, dydt, dzdt, dwdt])\n\n    @classmethod\n    def param_dict(cls, train_df_divided=None, q_range=None):\n        param_dict = super().param_dict()\n        q_range = super().QUANTILE_RANGE[:] if q_range is None else q_range\n        param_dict[\"theta\"] = (0, 1)\n        param_dict[\"kappa\"] = (0, 1)\n        if train_df_divided is not None:\n            df = train_df_divided.copy()\n            # rho = - (dx\/dt) \/ x \/ y\n            rho_series = 0 - df[\"x\"].diff() \/ df[\"t\"].diff() \/ df[\"x\"] \/ df[\"y\"]\n            param_dict[\"rho\"] = rho_series.quantile(q_range)\n            # sigma = (dz\/dt) \/ y\n            sigma_series = df[\"z\"].diff() \/ df[\"t\"].diff() \/ df[\"y\"]\n            param_dict[\"sigma\"] = sigma_series.quantile(q_range)\n            return param_dict\n        param_dict[\"rho\"] = (0, 1)\n        param_dict[\"sigma\"] = (0, 1)\n        return param_dict\n\n    @staticmethod\n    def calc_variables(df):\n        df[\"X\"] = df[\"Susceptible\"]\n        df[\"Y\"] = df[\"Infected\"]\n        df[\"Z\"] = df[\"Recovered\"]\n        df[\"W\"] = df[\"Fatal\"]\n        return df.loc[:, [\"T\", \"X\", \"Y\", \"Z\", \"W\"]]\n\n    @staticmethod\n    def calc_variables_reverse(df):\n        df[\"Susceptible\"] = df[\"X\"]\n        df[\"Infected\"] = df[\"Y\"]\n        df[\"Recovered\"] = df[\"Z\"]\n        df[\"Fatal\"] = df[\"W\"]\n        return df\n\n    def calc_r0(self):\n        try:\n            r0 = self.rho * (1 - self.theta) \/ (self.sigma + self.kappa)\n        except ZeroDivisionError:\n            return np.nan\n        return round(r0, 2)\n\n    def calc_days_dict(self, tau):\n        _dict = dict()\n        _dict[\"alpha1 [-]\"] = round(self.theta, 3)\n        if self.kappa == 0:\n            _dict[\"1\/alpha2 [day]\"] = 0\n        else:\n            _dict[\"1\/alpha2 [day]\"] = int(tau \/ 24 \/ 60 \/ self.kappa)\n        _dict[\"1\/beta [day]\"] = int(tau \/ 24 \/ 60 \/ self.rho)\n        if self.sigma == 0:\n            _dict[\"1\/gamma [day]\"] = 0\n        else:\n            _dict[\"1\/gamma [day]\"] = int(tau \/ 24 \/ 60 \/ self.sigma)\n        return _dict","d6246eb6":"class Estimator(object):\n    def __init__(self, model, ncov_df, total_population, name=None, places=None, areas=None,\n                 excluded_places=None, start_date=None, end_date=None, date_format=\"%d%b%Y\", **params):\n        \"\"\"\n        Set training data.\n        @model <ModelBase>: the model\n        @name <str>: name of the area\n        @params: fixed parameter of the model\n        @the other params: See the function named create_target_df()\n        \"\"\"\n        # Fixed parameters\n        self.fixed_param_dict = params.copy()\n        if None in params.values():\n            self.fixed_param_dict = {\n                k: v for (k, v) in params.items() if v is not None\n            }\n        # Register the dataset arranged for the model\n        dataset = model.create_dataset(\n            ncov_df, total_population, places=places, areas=areas,\n            excluded_places=excluded_places,\n            start_date=start_date, end_date=end_date, date_format=date_format\n        )\n        self.start_time, self.initials, self.Tend, self.train_df = dataset\n        self.total_population = total_population\n        self.name = name\n        self.model = model\n        self.param_dict = dict()\n        self.study = None\n        self.optimize_df = None\n\n    def run(self, n_trials=500):\n        \"\"\"\n        Try estimation (optimization of parameters and tau).\n        @n_trials <int>: the number of trials\n        \"\"\"\n        if self.study is None:\n            self.study = optuna.create_study(direction=\"minimize\")\n        self.study.optimize(\n            lambda x: self.objective(x),\n            n_trials=n_trials,\n            n_jobs=-1\n        )\n        param_dict = self.study.best_params.copy()\n        param_dict.update(self.fixed_param_dict)\n        param_dict[\"R0\"] = self.calc_r0()\n        param_dict[\"score\"] = self.score()\n        param_dict.update(self.calc_days_dict())\n        self.param_dict = param_dict.copy()\n        return param_dict\n\n    def history_df(self):\n        \"\"\"\n        Return the hsitory of optimization.\n        @return <pd.DataFrame>\n        \"\"\"\n        optimize_df = self.study.trials_dataframe()\n        optimize_df[\"time[s]\"] = optimize_df[\"datetime_complete\"] - \\\n            optimize_df[\"datetime_start\"]\n        optimize_df[\"time[s]\"] = optimize_df[\"time[s]\"].dt.total_seconds()\n        self.optimize_df = optimize_df.drop(\n            [\"datetime_complete\", \"datetime_start\", \"system_attrs__number\"], axis=1)\n        return self.optimize_df.sort_values(\"value\", ascending=True)\n\n    def history_graph(self):\n        \"\"\"\n        Show the history of parameter search using pair-plot.\n        \"\"\"\n        if self.optimize_df is None:\n            self.history_df()\n        df = self.optimize_df.copy()\n        sns.pairplot(df.loc[:, df.columns.str.startswith(\n            \"params_\")], diag_kind=\"kde\", markers=\"+\")\n        plt.show()\n\n    def objective(self, trial):\n        # Time\n        try:\n            tau = self.fixed_param_dict[\"tau\"]\n        except KeyError:\n            tau = trial.suggest_int(\"tau\", 1, 1440)\n        train_df_divided = self.train_df.copy()\n        train_df_divided[\"t\"] = (train_df_divided[\"T\"] \/ tau).astype(np.int64)\n        # Parameters\n        param_dict = self.model.param_dict(train_df_divided)\n        p_dict = {\"tau\": None}\n        p_dict.update(\n            {\n                k: trial.suggest_uniform(k, *v)\n                for (k, v) in param_dict.items()\n            }\n        )\n        p_dict.update(self.fixed_param_dict)\n        p_dict.pop(\"tau\")\n        # Simulation\n        t_end = train_df_divided.loc[train_df_divided.index[-1], \"t\"]\n        sim_df = simulation(self.model, self.initials, step_n=t_end, **p_dict)\n        return self.error_f(train_df_divided, sim_df)\n\n    def error_f(self, train_df_divided, sim_df):\n        \"\"\"\n        We need to minimize the difference of the observed values and estimated values.\n        This function calculate the difference of the estimated value and obsereved value.\n        \"\"\"\n        n = self.total_population\n        df = pd.merge(train_df_divided, sim_df, on=\"t\", suffixes=(\"_observed\", \"_estimated\"))\n        diffs = [\n            # Weighted Average: the recent data is more important\n            p * np.average(\n                abs(df[f\"{v}_observed\"] - df[f\"{v}_estimated\"]) \/ (df[f\"{v}_observed\"] * n + 1),\n                weights=df[\"t\"]\n            )\n            for (p, v) in zip(self.model.PRIORITIES, self.model.VARIABLES)\n        ]\n        return sum(diffs) * n\n\n    def compare_df(self):\n        \"\"\"\n        Show the taining data and simulated data in one dataframe.\n\n        \"\"\"\n        est_dict = self.study.best_params.copy()\n        est_dict.update(self.fixed_param_dict)\n        tau = est_dict[\"tau\"]\n        est_dict.pop(\"tau\")\n        observed_df = self.train_df.drop(\"T\", axis=1)\n        observed_df[\"t\"] = (self.train_df[\"T\"] \/ tau).astype(int)\n        t_end = observed_df.loc[observed_df.index[-1], \"t\"]\n        sim_df = simulation(self.model, self.initials, step_n=t_end, **est_dict)\n        df = pd.merge(observed_df, sim_df, on=\"t\", suffixes=(\"_observed\", \"_estimated\"))\n        df = df.set_index(\"t\")\n        return df\n\n    def compare_graph(self):\n        \"\"\"\n        Compare obsereved and estimated values in graphs.\n        \"\"\"\n        df = self.compare_df()\n        use_variables = [\n            v for (i, (p, v)) in enumerate(zip(self.model.PRIORITIES, self.model.VARIABLES))\n            if p != 0 and i != 0\n        ]\n        val_len = len(use_variables) + 1\n        fig, axes = plt.subplots(\n            ncols=1, nrows=val_len, figsize=(9, 6 * val_len \/ 2))\n        for (ax, v) in zip(axes.ravel()[1:], use_variables):\n            df[[f\"{v}_observed\", f\"{v}_estimated\"]].plot.line(\n                ax=ax, ylim=(0, None), sharex=True,\n                title=f\"{self.model.NAME}: Comparison of observed\/estimated {v}(t)\"\n            )\n            ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n            ax.ticklabel_format(style=\"sci\",  axis=\"y\", scilimits=(0, 0))\n            ax.legend(bbox_to_anchor=(1.02, 0),\n                      loc=\"lower left\", borderaxespad=0)\n        for v in use_variables:\n            df[f\"{v}_diff\"] = df[f\"{v}_observed\"] - df[f\"{v}_estimated\"]\n            df[f\"{v}_diff\"].plot.line(\n                ax=axes.ravel()[0], sharex=True,\n                title=f\"{self.model.NAME}: observed - estimated\"\n            )\n        axes.ravel()[0].axhline(y=0, color=\"black\", linestyle=\"--\")\n        axes.ravel()[0].yaxis.set_major_formatter(\n            ScalarFormatter(useMathText=True))\n        axes.ravel()[0].ticklabel_format(\n            style=\"sci\",  axis=\"y\", scilimits=(0, 0))\n        axes.ravel()[0].legend(bbox_to_anchor=(1.02, 0),\n                               loc=\"lower left\", borderaxespad=0)\n        fig.tight_layout()\n        fig.show()\n\n    def calc_r0(self):\n        \"\"\"\n        Calculate R0.\n        \"\"\"\n        est_dict = self.study.best_params.copy()\n        est_dict.update(self.fixed_param_dict)\n        est_dict.pop(\"tau\")\n        model_instance = self.model(**est_dict)\n        return model_instance.calc_r0()\n\n    def calc_days_dict(self):\n        \"\"\"\n        Calculate 1\/beta etc.\n        \"\"\"\n        est_dict = self.study.best_params.copy()\n        est_dict.update(self.fixed_param_dict)\n        tau = est_dict[\"tau\"]\n        est_dict.pop(\"tau\")\n        model_instance = self.model(**est_dict)\n        return model_instance.calc_days_dict(tau)\n\n    def predict_df(self, step_n):\n        \"\"\"\n        Predict the values in the future.\n        @step_n <int>: the number of steps\n        @return <pd.DataFrame>: predicted data for measurable variables.\n        \"\"\"\n        est_dict = self.study.best_params.copy()\n        est_dict.update(self.fixed_param_dict)\n        tau = est_dict[\"tau\"]\n        est_dict.pop(\"tau\")\n        df = simulation(self.model, self.initials, step_n=step_n, **est_dict)\n        df[\"Time\"] = (\n            df[\"t\"] * tau).apply(lambda x: timedelta(minutes=x)) + self.start_time\n        df = df.set_index(\"Time\").drop(\"t\", axis=1)\n        df = (df * self.total_population).astype(np.int64)\n        upper_cols = [n.upper() for n in df.columns]\n        df.columns = upper_cols\n        df = self.model.calc_variables_reverse(df).drop(upper_cols, axis=1)\n        return df\n\n    def predict_graph(self, step_n, name=None, excluded_cols=None):\n        \"\"\"\n        Predict the values in the future and create a figure.\n        @step_n <int>: the number of steps\n        @name <str>: name of the area\n        @excluded_cols <list[str]>: the excluded columns in the figure\n        \"\"\"\n        if self.name is not None:\n            name = self.name\n        else:\n            name = str() if name is None else name\n        df = self.predict_df(step_n=step_n)\n        if excluded_cols is not None:\n            df = df.drop(excluded_cols, axis=1)\n        r0 = self.param_dict[\"R0\"]\n        title = f\"Prediction in {name} with {self.model.NAME} model: R0 = {r0}\"\n        line_plot(df, title, v=datetime.today(), h=self.total_population)\n\n    def rmsle(self, compare_df):\n        \"\"\"\n        Return the value of RMSLE.\n        @param compare_df <pd.DataFrame>\n        \"\"\"\n        df = compare_df.set_index(\"t\") * self.total_population\n        score = 0\n        for (priority, v) in zip(self.model.PRIORITIES, self.model.VARIABLES):\n            if priority == 0:\n                continue\n            observed, estimated = df[f\"{v}_observed\"], df[f\"{v}_estimated\"]\n            diff = (np.log(observed + 1) - np.log(estimated + 1))\n            score += (diff ** 2).sum()\n        rmsle = np.sqrt(score \/ len(df))\n        return rmsle\n\n    def score(self):\n        \"\"\"\n        Return the value of RMSLE.\n        \"\"\"\n        rmsle = self.rmsle(self.compare_df().reset_index(\"t\"))\n        return rmsle\n\n    def info(self):\n        \"\"\"\n        Return Estimater information.\n        @return <tupple[object]>:\n            - <ModelBase>: model\n            - <dict[str]=str>: name, total_population, start_time, tau\n            - <dict[str]=float>: values of parameters of model\n        \"\"\"\n        param_dict = self.study.best_params.copy()\n        param_dict.update(self.fixed_param_dict)\n        info_dict = {\n            \"name\": self.name,\n            \"total_population\": self.total_population,\n            \"start_time\": self.start_time,\n            \"tau\": param_dict[\"tau\"],\n            \"initials\": self.initials\n        }\n        param_dict.pop(\"tau\")\n        return (self.model, info_dict, param_dict)","2c287236":"class Predicter(object):\n    \"\"\"\n    Predict the future using models.\n    \"\"\"\n    def __init__(self, name, total_population, start_time, tau, initials, date_format=\"%d%b%Y\"):\n        \"\"\"\n        @name <str>: place name\n        @total_population <int>: total population\n        @start_time <datatime>: the start time\n        @tau <int>: tau value (time step)\n        @initials <list\/tupple\/np.array[float]>: initial values of the first model\n        @date_format <str>: date format to display in figures\n        \"\"\"\n        self.name = name\n        self.total_population = total_population\n        self.start_time = start_time.replace(hour=0, minute=0, second=0, microsecond=0)\n        self.tau = tau\n        self.date_format = date_format\n        # Un-fixed\n        self.last_time = start_time\n        self.axvlines = list()\n        self.initials = initials\n        self.df = pd.DataFrame()\n        self.title_list = list()\n        self.reverse_f = lambda x: x\n        self.model_names = list()\n\n    def add(self, model, end_day_n=None, count_from_last=False, vline=True, **param_dict):\n        \"\"\"\n        @model <ModelBase>: the epidemic model\n        @end_day_n <int\/None>: day number of the end date (0, 1, 2,...), or None (now)\n            - if @count_from_last <bool> is True, start point will be the last date registered to Predicter\n        @vline <bool>: if True, vertical line will be shown at the end date\n        @**param_dict <dict>: keyword arguments of the model\n        \"\"\"\n        # Validate day number, and calculate step number\n        vline_yesterday = False\n        if end_day_n == 0:\n            end_day_n = 1\n            vline_yesterday = True\n        if end_day_n is None:\n            end_time = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n        else:\n            if count_from_last:\n                end_time = self.last_time + timedelta(days=end_day_n)\n            else:\n                end_time = self.start_time + timedelta(days=end_day_n)\n        if end_time <= self.last_time:\n            raise Exception(f\"Model on {end_time.strftime(self.date_format)} has been registered!\")\n        step_n = int((end_time - self.last_time).total_seconds() \/ 60 \/ self.tau) + 1\n        self.last_time = end_time.replace(hour=0, minute=0, second=0, microsecond=0)\n        # Perform simulation\n        new_df = simulation(model, self.initials, step_n=step_n, **param_dict)\n        new_df[\"t\"] = new_df[\"t\"] + len(self.df)\n        self.df = pd.concat([self.df, new_df.iloc[1:, :]], axis=0).fillna(0)\n        self.initials = new_df.set_index(\"t\").iloc[-1, :]\n        # For title\n        self.model_names.append(model.NAME)\n        if vline:\n            vline_date = end_time.replace(hour=0, minute=0, second=0, microsecond=0)\n            if vline_yesterday:\n                vline_date -= timedelta(days=1)\n            self.axvlines.append(vline_date)\n            r0 = model(**param_dict).calc_r0()\n            if len(self.axvlines) == 1:\n                self.title_list.append(f\"{model.NAME}(R0={r0}, -{vline_date.strftime(self.date_format)})\")\n            else:\n                if model.NAME == self.model_names[-1]:\n                    self.title_list.append(f\"({r0}, -{vline_date.strftime(self.date_format)})\")\n                else:\n                    self.title_list.append(f\"{model.NAME}({r0}, -{end_time.strftime(self.date_format)})\")\n        # Update reverse function (X, Y,.. to Susceptible, Infected,...)\n        self.reverse_f = model.calc_variables_reverse\n        return self\n\n    def restore_df(self, min_infected=1):\n        \"\"\"\n        Return the dimentional simulated data.\n        @min_infected <int>: if Infected < min_infected, the records will not be used\n        @return <pd.DataFrame>\n        \"\"\"\n        df = self.df.copy()\n        df[\"Time\"] = self.start_time + df[\"t\"].apply(lambda x: timedelta(minutes=x * self.tau))\n        df = df.drop(\"t\", axis=1).set_index(\"Time\") * self.total_population\n        df = df.astype(np.int64)\n        upper_cols = [n.upper() for n in df.columns]\n        df.columns = upper_cols\n        df = self.reverse_f(df).drop(upper_cols, axis=1)\n        df = df.loc[df[\"Infected\"] >= min_infected, :]\n        return df\n\n    def restore_graph(self, drop_cols=None, min_infected=1, **kwargs):\n        \"\"\"\n        Show the dimentional simulate data as a figure.\n        @drop_cols <list[str]>: the columns not to be shown\n        @min_infected <int>: if Infected < min_infected, the records will not be used\n        @kwargs: keyword arguments of line_plot() function\n        \"\"\"\n        df = self.restore_df(min_infected=min_infected)\n        if drop_cols is not None:\n            df = df.drop(drop_cols, axis=1)\n        today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n        axvlines = [today, *self.axvlines] if len(self.axvlines) == 1 else self.axvlines[:]\n        line_plot(\n            df,\n            title=f\"{self.name}: {', '.join(self.title_list)}\",\n            v=axvlines[:-1],\n            h=self.total_population,\n            **kwargs\n        )","91606227":"class Scenario(object):\n    \"\"\"\n    Class for scenario analysis.\n    \"\"\"\n    SUFFIX_DICT = defaultdict(lambda: \"th\")\n    SUFFIX_DICT.update({1: \"st\", 2: \"nd\", 3: \"rd\"})\n\n    def __init__(self, ncov_df, name, date_format=\"%d%b%Y\", **kwargs):\n        \"\"\"\n        @ncov_df <pd.DataFrame>: the cleaned data\n        @name <str>: name of the country\/area\n        @date_format <str>: string format of date\n        @kwargs: keyword arguments of select_area() function\n        \"\"\"\n        record_df = select_area(ncov_df, **kwargs)\n        record_df = record_df.set_index(\"Date\").resample(\"D\").last()\n        record_df = record_df.interpolate(method=\"linear\")\n        record_df = record_df.loc[:, [\"Confirmed\", \"Infected\", \"Deaths\", \"Recovered\"]]\n        self.record_df = record_df.reset_index()\n        self.name = name\n        self.date_format = date_format\n        self.phase_dict = dict()\n        self.estimator_dict = dict()\n        self.param_df = pd.DataFrame()\n        self.future_phase_dict = dict()\n        self.future_param_dict = dict()\n        self.phases_without_vline = list()\n        self.last_model = ModelBase\n\n    def show_record(self):\n        \"\"\"\n        Show the records.\n        \"\"\"\n        line_plot(\n            self.record_df.drop(\"Confirmed\", axis=1).set_index(\"Date\"),\n            f\"{self.name}: Cases over time\",\n            y_integer=True\n        )\n        return self.record_df\n\n    def growth_factor(self, days_to_predict=0, until_stopping=False, show_figure=True):\n        \"\"\"\n        Return growth factor group and the history of growth factor values.\n        @days_to_predict <int>: how many days to predict\n        @until_stopping <bool>:\n            if True and days_to_predict > 0,\n            calculate growth factor values until the group will shift stopping\n            after the last observation date\n        @show_figure <bool>: if True, show line plot of cases over time\n        \"\"\"\n        # Calculate growth factor\n        if days_to_predict <= 0:\n            # Value\n            records = self.record_df.set_index(\"Date\")[\"Confirmed\"]\n            growth = records.diff() \/ records.diff().shift(freq=\"D\")\n        else:\n            records = self.predict(days=days_to_predict, show_figure=False)\n            records = records[\"Confirmed\"].fillna(\"ffill\")\n            growth = records.diff() \/ records.diff().shift()\n        growth = growth.replace(np.inf, np.nan).fillna(1.0)\n        growth = growth.rolling(7).mean().dropna().round(2)\n        # Group\n        if days_to_predict > 0 and until_stopping:\n            last_observe_date = self.record_df[\"Date\"].max().round(\"D\")\n            df = pd.DataFrame(\n                {\"Date\": growth.index.round(\"D\"), \"Value\": growth}\n            )\n            df = df.set_index(\"Date\").resample(\"D\").last().reset_index()\n            df = df.loc[df[\"Date\"] > (last_observe_date - timedelta(days=8)), :]\n            date_df = df.loc[(df[\"Value\"] < 1).rolling(7).sum() >= 7, \"Date\"]\n            try:\n                calc_date = date_df.reset_index(drop=True)[0]\n            except IndexError:\n                calc_date = df[\"Date\"].max()\n            group = \"Stopping\"\n            growth = df.loc[df[\"Date\"] <= calc_date, :]\n            more_n = (growth[\"Value\"] > 1)[::-1].cumprod().sum()\n            less_n = (growth[\"Value\"] < 1)[::-1].cumprod().sum()\n            growth = growth.set_index(\"Date\")\n            date_str = calc_date.strftime(\"%d%b%Y\")\n            fig_title = f\"{self.name}: Growth factor over time with prediction until {date_str}\"\n        else:\n            more_n = (growth > 1)[::-1].cumprod().sum()\n            less_n = (growth < 1)[::-1].cumprod().sum()\n            calc_date = growth.index[-1]\n            group = \"Outbreaking\" if more_n >= 7 else \"Stopping\" if less_n >= 7 else \"Crossroad\"\n            fig_title = f\"{self.name}: Growth Factor over time\"\n        group_df = pd.DataFrame(\n            {\n                \"Date\": calc_date,\n                \"Group\": group,\n                \"GF > 1 [straight days]\": more_n,\n                \"GF < 1 [straight days]\": less_n\n            },\n            index=[self.name]\n        )\n        # Growth factor over time\n        if show_figure:\n            growth.plot(title=fig_title, legend=False)\n            plt.axhline(1.0, color=\"black\", linestyle=\"--\")\n            plt.xlabel(None)\n            today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n            plt.axvline(today, color=\"black\", linestyle=\"--\")\n            plt.show()\n        return group_df\n        \n    def trend(self, variables=[\"Confirmed\", \"Deaths\", \"Recovered\"], **kwargs):\n        \"\"\"\n        Perform trend analysis.\n        @variables <list[str]>: list of variables\n        @kwargs: keyword arguments of show_trend() function\n        \"\"\"\n        if \"variable\" in kwargs.keys():\n            raise KeyError(\"Please use variables argument rather than variable arugument.\")\n        for val in variables:\n            show_trend(self.record_df, name=self.name, variable=val, **kwargs)\n        return None\n\n    def set_phase(self, start_dates, population):\n        \"\"\"\n        Set phase for hyperparameter estimation.\n        @start_dates <list[str]>: list of start dates of the phases\n        @population <int or list[int]>: total population or list of total population\n        \"\"\"\n        end_dates = [\n            (datetime.strptime(s, self.date_format) - timedelta(days=1)).strftime(self.date_format)\n            for s in start_dates[1:]\n        ]\n        end_dates.append(None)\n        if isinstance(population, int):\n            population_values = [population for _ in range(len(start_dates))]\n        elif len(population) == len(start_dates):\n            population_values = population[:]\n        else:\n            raise Exception(\"start_date and population must have the same length!\")\n        self.phase_dict = {\n            self._num2str(n): {\"start_date\": s, \"end_date\": e, \"population\": p}\n            for (n, (s, e, p)) in enumerate(zip(start_dates, end_dates, population_values), 1)\n        }\n        self.estimator_dict = dict()\n        return pd.DataFrame.from_dict(self.phase_dict, orient=\"index\").fillna(\"-\")\n\n    def estimate(self, model, n_trials=100, same_tau=True):\n        \"\"\"\n        Perform hyperparameter estimation.\n        @model <ModelBase>: math model\n        @n_trials <int>: the number of trials\n        @same_tau <bool>:\n            whether apply the tau value of first phase to the following phases or not.\n        \"\"\"\n        if not self.phase_dict:\n            raise Exception(\"Please use Scenario.set_phase() at first.\")\n        tau = None\n        est_start_time = datetime.now()\n        for num in self.phase_dict.keys():\n            print(f\"Hyperparameter estimation of {num} phase.\")\n            target_dict = self.phase_dict[num]\n            while True:\n                # Create estimator\n                est_start_time_class = datetime.now()\n                self.estimator_dict[num] = Estimator(\n                    model, self.record_df, target_dict[\"population\"],\n                    name=self.name,\n                    start_date=target_dict[\"start_date\"],\n                    end_date=target_dict[\"end_date\"],\n                    date_format=self.date_format,\n                    tau=tau\n                )\n                print(\"\\tEstimator was created.\")\n                # Run trials\n                while True:\n                    print(f\"\\t\\t{n_trials} trials\", end=\" \")\n                    est_start_time_run = datetime.now()\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"ignore\")\n                        _ = self.estimator_dict[num].run(n_trials=n_trials)\n                    minutes, seconds = divmod(int((datetime.now() - est_start_time_run).total_seconds()), 60)\n                    print(f\"finished in {minutes} min {seconds} sec.\")\n                    # Check if estimated in (observed * 0.8, observed * 1.2)\n                    compare_df = self.estimator_dict[num].compare_df()\n                    targets = [\n                        (compare_df[f\"{val}_estimated\"], compare_df[f\"{val}_observed\"])\n                        for val in model.MONOTONIC\n                    ]\n                    max_ok = [obs.max() * 0.8 <= est.max() <= obs.max() * 1.2 for (est, obs) in targets]\n                    monotonic_ok = [target[0].is_monotonic for target in targets]\n                    elapsed = (datetime.now() - est_start_time_class).total_seconds()\n                    if all(max_ok) or not all(monotonic_ok) or elapsed > 60 * 3:\n                        break\n                if all(monotonic_ok) and all(max_ok):\n                    print(\"\\tSuccessfully estimated.\")\n                    break\n                vals = [val for (val, ok) in zip(model.MONOTONIC, monotonic_ok) if not ok]\n                try:\n                    print(f\"\\tEstimator will be replaced because estimated {vals[0]} is non-monotonic.\")\n                except IndexError:\n                    print(f\"\\tEstimator will be replaced because it is incapable of improvement.\")\n            tau = self.estimator_dict[num].param_dict[\"tau\"]\n        minutes, seconds = divmod(int((datetime.now() - est_start_time).total_seconds()), 60)\n        print(f\"Total: {minutes} min {seconds} sec.\")\n        self.show_parameters()\n        self.last_model = model\n\n    def accuracy_graph(self, phase_n=1):\n        \"\"\"\n        Show observed - estimated graph.\n        @phase_n <int>: phase number\n        \"\"\"\n        phase_numbers = self.estimator_dict.keys()\n        phase = self._num2str(phase_n)\n        if phase not in phase_numbers:\n            raise KeyError(f\"phase_n must be in {list(phase_numbers)[0]} - {list(phase_numbers)[-1]}\")\n        self.estimator_dict[phase].compare_graph()\n\n    def _num2str(self, num):\n        \"\"\"\n        Convert numbers to 1st, 2nd etc.\n        @num <int>: number\n        @return <str>\n        \"\"\"\n        q, mod = divmod(num, 10)\n        suffix = \"th\" if q == 1 else self.SUFFIX_DICT[mod]\n        return f\"{num}{suffix}\"\n\n    def show_parameters(self):\n        \"\"\"\n        Show the parameter values.\n        @retunr <pd.DataFrame>\n        \"\"\"\n        # Phase information\n        phase_dict = self.phase_dict.copy()\n        phase_dict.update(self.future_phase_dict)\n        df1 = pd.DataFrame.from_dict(phase_dict, orient=\"index\")\n        # Parameter information\n        _dict = {\n            k: estimator.param_dict\n            for (k, estimator) in self.estimator_dict.items()\n        }\n        _future_dict = {\n            k: {\n                \"tau\": _dict[\"1st\"][\"tau\"],\n                **param_dict,\n                \"R0\": self.last_model(**param_dict).calc_r0(),\n                \"score\": None,\n                **self.last_model(**param_dict).calc_days_dict(_dict[\"1st\"][\"tau\"])\n            }\n            for (k, param_dict) in self.future_param_dict.items()\n        }\n        _dict.update(_future_dict)\n        df2 = pd.DataFrame.from_dict(_dict, orient=\"index\")\n        # Rename R0 to Rt\n        df2 = df2.rename({\"R0\": \"Rt\"}, axis=1)\n        self.param_df = pd.concat([df1, df2], axis=1).fillna(\"-\")\n        return self.param_df\n\n    def param(self, phase, param_name):\n        \"\"\"\n        Return parameter value.\n        @phase <str>: phase name, like 1st, 2nd..., or last\n        @param_name <str>: name of parameter, like rho\n        \"\"\"\n        if phase == \"last\":\n            phase = list(self.phase_dict.items())[-1][0]\n        try:\n            estimator = self.estimator_dict[phase]\n        except KeyError:\n            raise KeyError(\"Please revise phase name (NOT iinclude future params). e.g. 1st, 2nd,... or last\")\n        try:\n            param_name = \"R0\" if param_name == \"Rt\" else param_name\n            return estimator.param_dict[param_name]\n        except KeyError:\n            raise KeyError(\"Please revise parameter name. e.g. rho, gamma, R0 or R0\")\n\n    def param_history(self, targets=None, box_plot=True, **kwargs):\n        \"\"\"\n        Show the ratio to 1st parameters as a figure (bar plot).\n        @targets <list[str] or str>: parameters to show (including Rt etc.)\n        @box_plot <bool>: if True, box plot. if False, line plot.\n        @kwargs: keword arguments of pd.DataFrame.plot or line_plot()\n        \"\"\"\n        _ = self.show_parameters()\n        targets = self.param_df.columns if targets is None else targets\n        targets = [targets] if isinstance(targets, str) else targets\n        if \"R0\" in targets:\n            targets = [t.replace(\"R0\", \"Rt\") for t in targets]\n        df = self.param_df.loc[:, targets]\n        df.index = self.param_df[[\"start_date\", \"end_date\"]].apply(\n            lambda x: f\"{x[0]}-{x[1].replace('-', 'today')}\",\n            axis=1\n        )\n        df = df \/ df.iloc[0]\n        if box_plot:\n            df.plot.bar(title=\"Ratio to 1st parameters\", **kwargs)\n            plt.xticks(rotation=0)\n            plt.show()\n        else:\n            _df = df.reset_index(drop=True)\n            _df.index = _df.index + 1\n            line_plot(\n                _df, title=\"Ratio to 1st parameters\",\n                xlabel=\"Phase\", ylabel=str(), math_scale=False,\n                **kwargs\n            )\n\n    def compare_estimated_numbers(self, phases=None):\n        \"\"\"\n        Compare the number of confimred cases estimated with the parameters and show graph.\n        @variable <str>: variable to compare\n        @phases <list[str]>: phase to show (if None, all)\n        \"\"\"\n        phases = list(self.phase_dict.keys()) if phases is None else phases\n        # Observed\n        df = pd.DataFrame(self.record_df.set_index(\"Date\")[\"Confirmed\"])\n        # Estimated\n        for (num, estimator) in self.estimator_dict.items():\n            model, info_dict, param_dict = estimator.info()\n            day_n = int((datetime.today() - info_dict[\"start_time\"]).total_seconds() \/ 60 \/ 60 \/ 24 + 1)\n            predicter = Predicter(**info_dict)\n            predicter.add(model, end_day_n=day_n, **param_dict)\n            # Calculate the number of confirmed cases\n            new_df = predicter.restore_df().drop(\"Susceptible\", axis=1).sum(axis=1)\n            new_df = new_df.resample(\"D\").last()\n            df = pd.concat([df, new_df], axis=1)\n        # Show graph\n        df = df.fillna(0).astype(np.int64)\n        df.columns = [\"Observed\"] + [f\"{phase}_param\" for phase in self.phase_dict.keys()]\n        df = df.loc[self.phase_dict[\"1st\"][\"start_date\"]: self.record_df[\"Date\"].max(), :]\n        for col in df.columns[1:]:\n            if col[:col.find(\"_\")] not in phases:\n                continue\n            line_plot(\n                df.replace(0, np.nan)[[\"Observed\", col]],\n                f\"Confirmed cases over time: Actual and predicted with {col}\",\n                y_integer=True\n            )\n\n    def clear_future_param(self):\n        \"\"\"\n        Clear the future parameters.\n        \"\"\"\n        self.future_param_dict = dict()\n        self.future_phase_dict = dict()\n        last_phase = list(self.phase_dict.items())[-1][0]\n        self.phase_dict[last_phase][\"end_date\"] = None\n        return self\n\n    def add_future_param(self, start_date, vline=True, **kwargs):\n        \"\"\"\n        Add parameters of the future.\n        @start_date <str>: the start date of the phase\n        @vline <bool>: if True, add vertical line in the figure of predicted number of cases\n        @kwargs: keword argument of parameters to change\n        \"\"\"\n        yesterday_of_start = (pd.to_datetime(start_date) - timedelta(days=1)).strftime(self.date_format)\n        # Last phase registered\n        phase_dict = self.phase_dict.copy()\n        phase_dict.update(self.future_phase_dict)\n        last_phase = list(phase_dict.items())[-1][0]\n        # Set the end date of the last phase\n        if self.future_phase_dict:\n            self.future_phase_dict[last_phase][\"end_date\"] = yesterday_of_start\n        else:\n            self.phase_dict[last_phase][\"end_date\"] = yesterday_of_start\n        # Set the new phase\n        try:\n            param_dict = self.estimator_dict[last_phase].info()[2]\n            population = self.phase_dict[last_phase][\"population\"]\n        except KeyError:\n            param_dict = self.future_param_dict[last_phase].copy()\n            population = self.future_phase_dict[last_phase][\"population\"]\n        param_dict.update(**kwargs)\n        new_phase = self._num2str(len(phase_dict) + 1)\n        self.future_param_dict[new_phase] = param_dict\n        self.future_phase_dict[new_phase] = {\n            \"start_date\": start_date,\n            \"end_date\": None,\n            \"population\": population\n        }\n        if not vline:\n            self.phases_without_vline.append(new_phase)\n        return pd.DataFrame.from_dict(self.future_param_dict, orient=\"index\")\n\n    def add_future_param_gradually(self, start_date, end_date, param, first, last):\n        \"\"\"\n        Set the future parameters. The value will be gradually (log-scale) changed.\n        @start_date <str>: the start date of change\n        @end_date <str>: the end date of change\n        @param <str>: parameter name\n        @first <float>: parameter value of the start date\n        @last <float>: parameter value of the end date\n        \"\"\"\n        start = (pd.to_datetime(start_date) - timedelta(days=1)).strftime(self.date_format)\n        dates = pd.date_range(start=start, end=end_date, freq=\"D\")\n        values = np.logspace(\n            start=np.log10(first), stop=np.log10(last), num=len(dates), base=10.0\n        )\n        for (d, v) in zip(dates[1:], values[1:]):\n            vline = True if d in dates[-2:] else False\n            self.add_future_param(d.strftime(self.date_format), vline=vline, **{param: v})\n        return pd.DataFrame.from_dict(self.future_param_dict, orient=\"index\")\n\n    def predict(self, days=1000, min_infected=1, show_figure=True):\n        \"\"\"\n        Predict the future.\n        @days <int or None>: how many days to predict from the last records date\n        @min_infected <int>: if Infected < min_infected, the records will not be used\n        @show_figure <bool>: if True, show line plot of cases over time\n        \"\"\"\n        if not isinstance(days, int):\n            raise TypeError(\"days must be integer!\")\n        # Create parameter dictionary\n        predict_param_dict = {\n            phase: self.estimator_dict[phase].info()[2]\n            for (phase, _) in self.phase_dict.items()\n        }\n        predict_param_dict.update(self.future_param_dict)\n        # Define phases\n        model, info_dict, _ = self.estimator_dict[\"1st\"].info()\n        predicter = Predicter(**info_dict)\n        phase_dict = self.phase_dict.copy()\n        phase_dict.update(self.future_phase_dict)\n        # Simulation with Predicter\n        for (phase, date_dict) in phase_dict.items():\n            start = pd.to_datetime(date_dict[\"start_date\"])\n            end = pd.to_datetime(date_dict[\"end_date\"])\n            if end is None:\n                day_n = days\n            elif start == end:\n                day_n = 0\n            else:\n                day_n = int((end - start).total_seconds() \/ 60 \/ 60 \/ 24) + 1\n            param_dict = predict_param_dict[phase].copy()\n            vline = False if phase in self.phases_without_vline else True\n            predicter.add(model, end_day_n=day_n, count_from_last=True, vline=vline, **param_dict)\n        # Restore\n        df = predicter.restore_df(min_infected=min_infected)\n        try:\n            df[\"Confirmed\"] = df[\"Infected\"] + df[\"Recovered\"] + df[\"Fatal\"]\n        except KeyError:\n            pass\n        # Graph: If max(other variables) < min(Susceptible), not show Susceptible\n        if show_figure:\n            without_s = df.drop(\"Susceptible\", axis=1).sum(axis=1).max()\n            drop_cols = [\"Susceptible\"] if without_s < df[\"Susceptible\"].min() else None\n            predicter.restore_graph(drop_cols=drop_cols, min_infected=min_infected, y_integer=True)\n        return df","e9a90657":"def log_curve(x, k, x_0, ymax):\n    return ymax \/ (1 + np.exp(-k*(x-x_0)))","76d5bc76":"def log_fit(train_df, area, metric):\n    area_data = select_area(train_df, areas=[area])\n    area_data = area_data.loc[area_data[metric] > 0, :]\n    x_data = range(len(area_data.index))\n    y_data = area_data[metric]\n    if len(y_data) < 5:\n        estimated_k = -1  \n        estimated_x_0 = -1 \n        ymax = -1\n    elif max(y_data) == 0:\n        estimated_k = -1  \n        estimated_x_0 = -1 \n        ymax = -1\n    else:\n        try:\n            popt, pcov = curve_fit(\n                log_curve, x_data, y_data, bounds=([0,0,0],np.inf),\n                p0=[0.3,100,10000], maxfev=1000000\n            )\n            estimated_k, estimated_x_0, ymax = popt\n        except RuntimeError:\n            print(area)\n            print(\"Error - curve_fit failed\") \n            estimated_k = -1  \n            estimated_x_0 = -1 \n            ymax = -1\n    estimated_parameters = pd.DataFrame(\n        np.array([[area, estimated_k, estimated_x_0, ymax]]), columns=['Area', 'k', 'x_0', 'ymax']\n    )\n    return estimated_parameters","a257733e":"def get_parameters(metric):\n    parameters = pd.DataFrame(columns=['Area', 'k', 'x_0', 'ymax'], dtype=np.float)\n    for area in train_df['Area'].unique():\n        estimated_parameters = log_fit(train_df, area, metric)\n        parameters = parameters.append(estimated_parameters)\n    parameters['k'] = pd.to_numeric(parameters['k'], downcast=\"float\")\n    parameters['x_0'] = pd.to_numeric(parameters['x_0'], downcast=\"float\")\n    parameters['ymax'] = pd.to_numeric(parameters['ymax'], downcast=\"float\")\n    parameters = parameters.replace({'k': {-1: parameters[parameters['ymax']>0].median()[0]}, \n                                     'x_0': {-1: parameters[parameters['ymax']>0].median()[1]}, \n                                     'ymax': {-1: parameters[parameters['ymax']>0].median()[2]}})\n    return parameters","b4228c41":"train_raw = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\ntest_raw = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\nsubmission_sample_raw = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/submission.csv\")\n# Population\npopulation_raw = pd.read_csv(\n    \"\/kaggle\/input\/covid19-global-forecasting-locations-population\/locations_population.csv\"\n)","97cd6d6e":"submission_sample_raw.head()","af6548b2":"df = pd.DataFrame(\n    {\n        \"Nunique_train\": train_raw.nunique(),\n        \"Nunique_test\": test_raw.nunique(),\n        \"Null_Train\": train_raw.isnull().sum(),\n        \"Null_Test\": test_raw.isnull().sum(),\n    }\n)\ndf.fillna(\"-\").T","e0156a3c":"population_raw.head()","d8fe959c":"df = population_raw.rename({\"Province.State\": \"Province\", \"Country.Region\": \"Country\"}, axis=1)\ndf[\"Country\/Province\"] = df[[\"Country\", \"Province\"]].apply(\n    lambda x: f\"{x[0]}\/{x[0]}\" if x[1] is np.nan else f\"{x[0]}\/{x[1]}\",\n    axis=1\n)\ndf = df.loc[:, [\"Country\/Province\", \"Population\"]]\n# Culculate total value of each country\/province\ndf = df.groupby(\"Country\/Province\").sum()\n# Global population\ndf.loc[\"Global\", \"Population\"] = df[\"Population\"].sum()\n# DataFrame to dictionary\npopulation_dict = df.astype(np.int64).to_dict()[\"Population\"]\npopulation_dict","a58630ba":"df = pd.merge(\n    train_raw.rename({\"Province_State\": \"Province\", \"Country_Region\": \"Country\"}, axis=1),\n    population_raw.rename({\"Province.State\": \"Province\", \"Country.Region\": \"Country\"}, axis=1),\n    on=[\"Country\", \"Province\"],\n    how=\"left\"\n)\n# Area: Country or Country\/Province\ndf[\"Area\"] = df[[\"Country\", \"Province\"]].apply(\n    lambda x: f\"{x[0]}\" if x[1] is np.nan else f\"{x[0]}\/{x[1]}\",\n    axis=1\n)\n# Date\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n# The number of cases\ndf = df.rename({\"ConfirmedCases\": \"Confirmed\", \"Fatalities\": \"Fatal\"}, axis=1)\ndf[[\"Confirmed\", \"Fatal\"]] = df[[\"Confirmed\", \"Fatal\"]].astype(np.int64)\n# Show data\ndf = df.loc[:, [\"Date\", \"Area\", \"Population\", \"Confirmed\", \"Fatal\"]]\ntrain_df = df.copy()\ntrain_df.tail()","8be02d19":"df = pd.merge(\n    test_raw.rename({\"Province_State\": \"Province\", \"Country_Region\": \"Country\"}, axis=1),\n    population_raw.rename({\"Province.State\": \"Province\", \"Country.Region\": \"Country\"}, axis=1),\n    on=[\"Country\", \"Province\"],\n    how=\"left\"\n)\ndf[\"Area\"] = df[[\"Country\", \"Province\"]].apply(\n    lambda x: f\"{x[0]}\" if x[1] is np.nan else f\"{x[0]}\/{x[1]}\",\n    axis=1\n)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf = df.loc[:, [\"ForecastId\", \"Date\", \"Area\", \"Population\"]]\ntest_df = df.copy()\ntest_df.tail()","7e5c74e7":"%%time\nconfirmed_param_df = get_parameters(\"Confirmed\")\nconfirmed_param_df.head()","1376ec36":"df = train_df.loc[train_df[\"Confirmed\"] > 0, [\"Date\", \"Area\"]].groupby(\"Area\").first()\ndf = df.rename({\"Date\": \"First_date\"}, axis=1).reset_index()\ndf = pd.merge(confirmed_param_df, df)\nconfirmed_df = df.copy()\nconfirmed_df.head()","6f97b733":"fatal_param_df = get_parameters(\"Fatal\")\nfatal_param_df.head()","ec6d18f1":"df = train_df.loc[train_df[\"Fatal\"] > 0, [\"Date\", \"Area\"]].groupby(\"Area\").first()\ndf = df.rename({\"Date\": \"First_date\"}, axis=1).reset_index()\ndf = pd.merge(fatal_param_df, df)\nfatal_df = df.copy()\nfatal_df.head()","cb2e60c0":"df = pd.merge(confirmed_df, fatal_df, on=\"Area\", suffixes=[\"_confirmed\", \"_fatal\"])\n# k\ndf[\"k\"] = df[\"k_confirmed\"]\n# x_0\ndf[\"First_date_recovered\"] = df[\"First_date_fatal\"]\ndf[\"x_0\"] = df[[\"x_0_confirmed\", \"x_0_fatal\"]].max(axis=1)\n# ymax\ndf[\"ymax\"] = df[\"ymax_confirmed\"] - df[\"ymax_fatal\"]\n# save\ndf = df.loc[:, [\"Area\", \"First_date_recovered\", \"k\", \"x_0\", \"ymax\"]]\nrecovered_df = df.copy()\nrecovered_df.head()","63c198ce":"df = train_df.loc[train_df[\"Confirmed\"] > 0, :]\ndf = pd.merge(df, recovered_df, on=\"Area\", how=\"left\")\ndf[\"date_diff\"] = (df[\"Date\"] - df[\"First_date_recovered\"]).dt.total_seconds() \/ 60 \/ 60 \/ 24\ndf.loc[(df[\"date_diff\"] < 0) | (df[\"date_diff\"].isnull()), \"date_diff\"] = -1\ndf[\"date_diff\"] = df[\"date_diff\"].astype(np.int64)\ndf[\"Recovered\"] = df[[\"date_diff\", \"k\", \"x_0\", \"ymax\"]].apply(\n    lambda x: 0 if x[0] < 0 else log_curve(x[0], x[1], x[2], x[3]),\n    axis=1\n).astype(np.int64)\ndf = df.rename({\"Fatal\": \"Deaths\"}, axis=1)\ndf[\"Infected\"] = df[\"Confirmed\"] - df[\"Recovered\"] - df[\"Deaths\"]\nncov_df = df.copy()\nncov_df.head()","178e6c9f":"scenario = Scenario(ncov_df, name=\"Global\")","0e0b9dc3":"scenario.show_record().tail()","7c9ee636":"scenario.growth_factor()","1ebf6868":"scenario.trend(variables=[\"Confirmed\"])","7350603f":"scenario.trend(variables=[\"Confirmed\"], start_date=\"07Mar2020\")","785473b6":"scenario.set_phase(\n    start_dates=[\"04Apr2020\"],\n    population=population_dict[\"Global\"]\n)","f88f827b":"scenario.estimate(SIRF)","d32e4385":"scenario.accuracy_graph(phase_n=1)","4afa64c0":"scenario.show_parameters()","76e6b36e":"days_to_predict = int((test_df[\"Date\"].max() - datetime.today()).total_seconds() \/ 3600 \/ 24 + 10)\ndays_to_predict","60f88f5a":"global_predict = scenario.predict(days=days_to_predict)\nglobal_predict.tail(7).style.background_gradient(axis=0)","dd262d44":"# Current record\ndf = ncov_df.copy()\ndf = df.loc[df[\"Date\"] == df[\"Date\"].max(), [\"Area\", \"Confirmed\", \"Deaths\"]]\ndf[\"Confirmed\"] = df[\"Confirmed\"] \/ df[\"Confirmed\"].sum()\ndf[\"Deaths\"] = df[\"Deaths\"] \/ df[\"Deaths\"].sum()\ncurrent_df = df.rename({\"Deaths\": \"Fatal\"}, axis=1)\ncurrent_df.tail()","d3f77f47":"df = global_predict.copy()\ndf[\"Date\"] = df.index.date\ndf[\"Confirmed\"] = df[\"Infected\"] + df[\"Recovered\"] + df[\"Fatal\"]\ndf = df.groupby(\"Date\").last().reset_index()[[\"Date\", \"Confirmed\", \"Fatal\"]]\nglobal_df = df.copy()\nglobal_df.tail()","4a4d8b0a":"record_df = pd.DataFrame()\n\nfor i in range(len(global_df)):\n    date, confirmed, fatal = global_df.iloc[i, :].tolist()\n    df = current_df.copy()\n    df[\"Date\"] = date\n    df[\"Confirmed\"] = (confirmed * df[\"Confirmed\"]).astype(np.int64)\n    df[\"Fatal\"] = (fatal * df[\"Fatal\"]).astype(np.int64)\n    record_df = pd.concat([record_df, df], axis=0)\n\nrecord_df[\"Date\"] = pd.to_datetime(record_df[\"Date\"])\nrecord_df = record_df.loc[:, [\"Date\", \"Area\", \"Confirmed\", \"Fatal\"]].reset_index(drop=True)\nrecord_df","e1de0bfa":"submission_sample_raw.shape","3870c50a":"df = pd.merge(record_df, test_df, on=[\"Date\", \"Area\"], how=\"right\")\ndf = df.sort_values(\"ForecastId\").reset_index()\ndf = df.groupby([\"Area\"]).fillna(method=\"bfill\")\ndf = df.loc[:, [\"ForecastId\", \"Confirmed\", \"Fatal\"]]\ndf = df.rename({\"Confirmed\": \"ConfirmedCases\", \"Fatal\": \"Fatalities\"}, axis=1)\ndf = df.fillna(0).astype(np.int64)\nsubmission_df = df.copy()\nsubmission_df","7902f30d":"submission_df.shape","d89c2e6a":"len(submission_df) == len(submission_sample_raw)","6773c9fc":"submission","eb8015d3":"submission_df","684c46c1":"merged = pd.merge(submission, submission_df, on=\"ForecastId\")\nmerged[\"ConfirmedCases\"] = (merged[\"ConfirmedCases_x\"] + merged[\"ConfirmedCases_y\"]) \/ 2\nmerged[\"Fatalities\"] = (merged[\"Fatalities_x\"] +  merged[\"Fatalities_y\"]) \/ 2\nmerged = merged.loc[:, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].astype(np.int64)\nmerged","10b64a44":"merged.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","9dfd9df7":"## Submission data","e0e08fc5":"## Trend Analysis","216ab4f4":"## Gamma Curve Fitting - Overview of This Approach\n\nThis notebook is a followup to some previous effort of Bill Holst (https:\/\/www.kaggle.com\/wjholst), Daner Ferhadi (https:\/\/www.kaggle.com\/dferhadi) and Dan Pearson (https:\/\/www.kaggle.com\/dan89pearson).\nSee:\n* https:\/\/www.kaggle.com\/dferhadi\/logistic-curve-fit-parameter-tuning\n* https:\/\/www.kaggle.com\/wjholst\/covid-19-growth-patterns-in-critical-countries\n* https:\/\/www.youtube.com\/watch?v=Kas0tIxDvrg&t=35s \n\n\nBoth of us used a logistic model to both predict and to identify critial inflection points in the growth model. You can observer from early track of the virus growth, that the initial rate is exponential. Eventually the curve tends to flatten and turn down. That is when the curve begins to take on the sigmoid properties.\n\nHowever, when you observe these events over time, the probability distribution does not look normal but rather skewed with a long right tail. That is why this model uses a gamma pdf, which can be tuned to more realistically fit the actual distributions. \n\nThis approach is not a machine learning effort, but rather employs the Python curve_fit library to find the closest fit for each population group. I use Daner's code base and use the gamma function to formulate the predictions.\n\n\n\n","4dc7a2f6":"# Calculate the mean value of 1. and 2.","129185ca":"## Area lebel","021db613":"## Data","56374133":"# Introduction\nTo predict the number of confirmed\/fatal cases with COVID-19, we will take the following steps.\n\n1. Predict the values using Gamma CDF curve fitting\n2. Predict the values using logistic curve fitting of recovered cases and SIR-F model\n3. Calculate the mean value of 1. and 2.","77d5935d":"## Prediction of the number of recovered cases with logistic curve","39e723c4":"## Prepare for SIR-F model hyperparameter optumization","ae587440":"### Curve fitting: Confirmed, Fatal","668574d1":"# Logistic curve fitting and SIR-F model\nUsing curve fitting method and SIR-F model, we will predict the number of confirmed cases and fatal cases with COVID-19 global data. SIR-F model was created in another notebook of an auther. Please refer to the references.  \n\nContents:\n* Preparation\n* Prediction of the number of recovered cases with logistic curve\n* Prameter estimation with SIR-F model\n* Prediction of global data\n* Data submission\n\nReferences:\n* [COVID-19 - Growth of Virus in Specific Countries](https:\/\/www.kaggle.com\/wjholst\/covid-19-growth-of-virus-in-specific-countries) by Bill Holst\n* [COVID-19 data with SIR model](https:\/\/www.kaggle.com\/lisphilar\/covid-19-data-with-sir-model) by Lisphilar","611c6468":"### Predict the number of recovered cases","657a6674":"## Tool"}}