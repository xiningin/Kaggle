{"cell_type":{"9017dae7":"code","24f37641":"code","f8bd3989":"code","280395b0":"code","9d2dfad7":"code","4f71177c":"code","1b07c97c":"code","ce9eae14":"code","4b34c9b7":"code","4611c03f":"code","78ce7c29":"code","a77fb4d9":"code","6ca71469":"code","733f5b8d":"code","013555c7":"code","669ca143":"markdown"},"source":{"9017dae7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24f37641":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]","f8bd3989":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","280395b0":"\ndf.info()","9d2dfad7":"df.isnull().sum()","4f71177c":"count_classes = pd.value_counts(df['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction Class Distribution\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","1b07c97c":"## Get the Fraud and the normal dataset \nfraud = df[df['Class']==1]\nnormal = df[df['Class']==0]","ce9eae14":"print(fraud.shape,normal.shape)","4b34c9b7":"# Take some sample of the data\n\ndf1= df.sample(frac = 0.1,random_state=1)\ndf1.shape","4611c03f":"df.shape","78ce7c29":"#Determine the number of fraud and valid transactions in the dataset\n\nFraud = df1[df1['Class']==1]\nValid = df1[df1['Class']==0]\noutlier_fraction = len(Fraud)\/float(len(Valid))","a77fb4d9":"#Create independent and Dependent Features\ncolumns = df1.columns.tolist()\n\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\n\nX = df1[columns]\nY = df1[target]\n\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\nprint(X.shape)\nprint(Y.shape)","6ca71469":"##Define the outlier detection methods\n\nclassifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=5, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction),\n    \"Support Vector Machine\":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n                                         max_iter=-1)\n   \n}","733f5b8d":"type(classifiers)","013555c7":"n_outliers = len(Fraud)\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Support Vector Machine\":\n        clf.fit(X)\n        y_pred = clf.predict(X)\n    else:    \n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))","669ca143":"\n### Observations : Isolation Forest detected 73 errors versus Local Outlier Factor detecting 97 errors vs. SVM detecting 8516 errors Isolation Forest has a 99.74% more accurate than LOF of 99.65% and SVM of 70.09 When comparing error precision & recall for 3 models , the Isolation Forest performed much better than the LOF as we can see that the detection of fraud cases is around 27 % versus LOF detection rate of just 2 % and SVM of 0%. So overall Isolation Forest Method performed much better in determining the fraud cases which is around 30%."}}