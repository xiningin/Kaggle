{"cell_type":{"f910bd4d":"code","9da2b7ad":"code","d976f23d":"code","e7bd7ad9":"code","7e94585e":"code","e0596339":"code","01206b0b":"code","d1754aef":"code","4b39c6c0":"markdown","77a7b7fd":"markdown","fa3c0cf4":"markdown"},"source":{"f910bd4d":"!pip install pytorch-tabnet\n\n# install develop branch\n# !pip install  \"git+https:\/\/github.com\/dreamquark-ai\/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade","9da2b7ad":"import numpy as np \nimport pandas as pd \nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\n\nimport plotly.express as px\nfrom matplotlib import pyplot as plt","d976f23d":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv')\ndf_sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n# set submission preds to 0\ndf_sub[[col for col in df_sub.columns if col.startswith(\"Class\")]] = 0","e7bd7ad9":"# Consider everything as numerical\n# CAT_COLS = [] \n# NUM_COLS = [c for c in df_train.columns if c.startswith(\"feature_\")]\n\n# Consider everything as categorical variables might be useful : this is the only trick of this notebook\nCAT_COLS = [c for c in df_train.columns if c.startswith(\"feature_\")] \nNUM_COLS = [] \n\nFEATURES = CAT_COLS + NUM_COLS\n\n\nencoders = {}\n# Categorical features need to be LabelEncoded\nfor cat_col in CAT_COLS:\n    label_enc = LabelEncoder()\n        \n    df_train[cat_col] = label_enc.fit_transform(df_train[cat_col])\n    encoders[cat_col] = label_enc\n    \n# Encode test set\nfor cat_col in CAT_COLS:\n    label_enc = encoders[cat_col]\n    le_dict = dict(zip(label_enc.classes_, label_enc.transform(label_enc.classes_)))\n    # Replace unknown values by the most common value\n    # Changing this to another value might make more sense\n    if le_dict.get(\"low_frequency\") is not None:\n        default_val = le_dict[\"low_frequency\"]\n    else:\n        default_val = df_train[cat_col].mode().values[0]\n    df_test[cat_col] = df_test[cat_col].apply(lambda x: le_dict.get(x, default_val ))\n    \n# Clip numerical features in test set to match training set\nfor num_col in NUM_COLS:\n    df_test[num_col] = np.clip(df_test[num_col], df_train[num_col].min(), df_train[num_col].max())","7e94585e":"cat_dims = df_train[CAT_COLS].nunique().to_list()\ncat_idxs = [FEATURES.index(cat_col) for cat_col in CAT_COLS]\ncat_emb_dims = np.ceil(np.log(cat_dims)).astype(np.int).tolist()\n# cat_emb_dims = np.ceil(np.clip((np.array(cat_dims)) \/ 2, a_min=1, a_max=50)).astype(np.int).tolist()\n# cat_emb_dims=1\n\nX = df_train[FEATURES].values\ny = df_train[\"target\"].values\n\nX_test = df_test[FEATURES].values","e0596339":"from pytorch_tabnet.pretraining import TabNetPretrainer\n\nN_D = 64 #64 # 32\nN_A = 64 # 32\nN_INDEP = 1 #2\nN_SHARED = 1 #2\nN_STEPS = 3 #2\nMASK_TYPE = \"sparsemax\"\nGAMMA = 1.2\nBS = 256\nMAX_EPOCH =  30\nPRETRAIN = True\n\n\nif PRETRAIN:\n    pretrain_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,  #0.2,\n                           n_independent=N_INDEP, n_shared=N_SHARED,\n                           cat_idxs=cat_idxs,\n                           cat_dims=cat_dims,\n                           cat_emb_dim=cat_emb_dims,\n                           gamma=GAMMA,\n                           lambda_sparse=0., optimizer_fn=torch.optim.Adam,\n                           optimizer_params=dict(lr=2e-2),\n                           mask_type=MASK_TYPE,\n                           scheduler_params=dict(mode=\"min\",\n                                                 patience=3,\n                                                 min_lr=1e-5,\n                                                 factor=0.5,),\n                           scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,                         \n                           verbose=1,\n                          )\n\n    pretrainer = TabNetPretrainer(**pretrain_params)\n\n    pretrainer.fit(X_train=X_test, \n                   eval_set=[X],\n                   max_epochs=MAX_EPOCH,\n                   patience=25, batch_size=BS, virtual_batch_size=BS, #128,\n                   num_workers=1, drop_last=True,\n                   pretraining_ratio=0.5 # The bigger your pretraining_ratio the harder it is to reconstruct\n                  )","01206b0b":"BS = 2048\nMAX_EPOCH =  100\nLAMBDA_SPARSE = 1e-5 #1e-5\n\nN_SPLITS = 5\nNB_FOLDS = 5 # max N_SPLITS\nskf = StratifiedKFold(n_splits=N_SPLITS, random_state=2021, shuffle=True)\n\n\nLR = 1e-1 # 5e-2\nfold_nb = 1\nfor train_index, valid_index in skf.split(X, y):\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n\n    tabnet_params = dict(n_d=N_D, \n                         n_a=N_A,\n                         n_steps=N_STEPS, gamma=GAMMA,\n                         n_independent=N_INDEP, n_shared=N_SHARED,\n                         lambda_sparse=LAMBDA_SPARSE,\n                         seed=0,\n                         clip_value=2,\n                         cat_idxs=cat_idxs,\n                         cat_dims=cat_dims,\n                         cat_emb_dim=cat_emb_dims,\n                         mask_type=MASK_TYPE,\n                         device_name='auto',\n                         optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=LR, weight_decay=1e-5),\n#                          scheduler_params=dict(max_lr=LR,\n#                                                steps_per_epoch=int(X_train.shape[0] \/ BS),\n#                                                epochs=MAX_EPOCH,\n#                                                #final_div_factor=100,\n#                                                is_batch_level=True),\n#                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                              scheduler_params=dict(mode='min',\n                                                    factor=0.5,\n                                                    patience=3,\n                                                    is_batch_level=False,),\n                              scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                         verbose=1)\n    # Defining TabNet model\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(X_train=X_train, y_train=y_train,\n              from_unsupervised=pretrainer if PRETRAIN else None,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_name=[\"train\", \"valid\"],\n              eval_metric=[\"logloss\"],\n              batch_size=BS,\n              virtual_batch_size=256,\n              max_epochs=MAX_EPOCH,\n              drop_last=True,\n              pin_memory=True,\n              patience=10,\n             )  \n    \n    test_preds = model.predict_proba(X_test)\n    df_sub[model.classes_] += test_preds\n    fold_nb+=1\n    \n    if fold_nb > NB_FOLDS:\n        break\n\ndf_sub[model.classes_] = df_sub[model.classes_] \/ NB_FOLDS\n","d1754aef":"df_sub.to_csv('submission.csv', index=None)","4b39c6c0":"## Actual training","77a7b7fd":"## Pretraining","fa3c0cf4":"# About this notebook\n\nThis is just a very simple baseline for TabNet usage as I saw some starters with very poor results.\n\nThere is nothing very specific to this competition in this notebook. This is a simple adaptation of a previous notebook I shared on an other competition : https:\/\/www.kaggle.com\/optimo\/tabnetregressor-baseline\n"}}