{"cell_type":{"b7cd3ab3":"code","7a882f12":"code","83e6afad":"code","91aff8cf":"code","adf582d6":"code","334a4d75":"code","a4cea4da":"code","46eb0014":"code","e21cb0c7":"code","2f093959":"code","771a8bea":"code","1ec42fbd":"code","914ef080":"code","3a7058d7":"code","04ac96bc":"code","ae5b0382":"code","ca914150":"code","c79208b8":"code","84c8f502":"code","fc61120d":"code","e6832c1e":"code","a2ae70aa":"code","12900b88":"code","961aefc5":"code","bc147eca":"code","1ed5c1f1":"code","d41d861e":"code","1cdba02b":"code","aa179eac":"code","0cd97628":"code","4c477dd1":"code","3215d115":"code","403ce8ea":"code","730c16a9":"code","271f784d":"code","cf165c38":"code","623240b8":"code","c55148bb":"code","b4f773ff":"code","92b78a68":"code","c2f4ccf7":"code","cab7c12c":"code","3f5812a8":"code","02f87a75":"code","6fc9c665":"markdown","18597193":"markdown","2ab7439a":"markdown","32e2cc07":"markdown","44365ce4":"markdown","ef137b2d":"markdown","e1ac43c3":"markdown","cb5a0ba0":"markdown","5afdf0b1":"markdown","ee5e2b3b":"markdown","be0ccdd0":"markdown","4f743c56":"markdown","f6cb48ce":"markdown","0b8cf023":"markdown","bec09222":"markdown","17629190":"markdown","9db0b055":"markdown","40f21f9d":"markdown","8a3d34c6":"markdown","0aa965cb":"markdown","9101be46":"markdown","4a1187b9":"markdown","61d4ff7a":"markdown","45d8a7d4":"markdown","378c08d7":"markdown","14cf5f20":"markdown","5b9f600e":"markdown","06f64220":"markdown"},"source":{"b7cd3ab3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport seaborn as sns\nimport scipy.stats as stats","7a882f12":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","83e6afad":"# Check number of observations in the train and test data\nprint(train.shape)\nprint(test.shape)","91aff8cf":"# check the columns in train and test\nprint(train.columns)\nprint(test.columns)\nprint('\\n')\nprint('Variables in test but not in train is: ', set(train.columns)-set(test.columns))","adf582d6":"# Check the first and last five observations of train and test data\ntrain.head()","334a4d75":"test.head()","a4cea4da":"# Check data type and NAN value\nprint(train.info())\nprint(test.info())","46eb0014":"# Check basic descriptive information for numeric features\ntrain.describe()","e21cb0c7":"sns.countplot(x=train['Survived'])\ntrain['Survived'].value_counts(normalize=True)","2f093959":"sns.countplot(train['Survived'], hue=train['Sex'])\n# Add contigency table for Sex by Survived\npd.crosstab(train['Sex'],train['Survived'], normalize='index')","771a8bea":"sns.countplot(train['Survived'], hue=train['Pclass'])\n# Add contigency table for Pclass by Survived\npd.crosstab(train['Pclass'],train['Survived'], normalize='index')","1ec42fbd":"plt.figure(figsize=(7,7))\nplt.hist(train['Age'].dropna(), bins=30)\n# We can see that there are some children and very senior people on the boat","914ef080":"print('The minimum age is: ', train['Age'].min())\nprint('The maximum age is: ', train['Age'].max())\n\ndef age_group(Age):\n    if Age<5:\n        return \"Group 1: < 5 Years old\"\n    elif Age<10:\n        return \"Group 2: 5-10 Years old\"\n    elif Age<20:\n        return \"Group 3: 10-20 Years old\"\n    elif Age<40:\n        return \"Group 4: 20-40 Years old\"\n    elif Age<60:\n        return \"Group 5: 40-60 Years old\"\n    else:\n        return \"Group 6: >= 60 Years old\"\ntrain['Age_Group']=train['Age'].apply(age_group)","3a7058d7":"plt.figure(figsize=(15,7))\nsns.countplot(train['Age_Group'], hue=train['Survived'])\npd.crosstab(train['Age_Group'],train['Survived'], normalize='index' )","04ac96bc":"# Check the number of siblings and spouse\nsns.countplot(train['SibSp'])\n# Most people do not have siblings or spouses on boat","ae5b0382":"sns.countplot(train['SibSp'], hue=train['Survived'])\npd.crosstab(train['SibSp'], train['Survived'], normalize='index')","ca914150":"# Check number of parents or children, \nprint(train['Parch'].value_counts())\nsns.countplot(train['Parch'])\n# Most people do not bring parents or children","c79208b8":"sns.countplot(train['Parch'], hue=train['Survived'])\npd.crosstab(train['Parch'], train['Survived'], normalize='index')","84c8f502":"sns.distplot(train['Fare'], kde=False)","fc61120d":"# Create categorical variables for Fare to check whether Survived has relationship with Fare Price\nprint(train['Fare'].min())\nprint(train['Fare'].max())\ndef fare_cat(Fare):\n    if Fare<50:\n        return \"C1: <50\"\n    elif Fare<100:\n        return \"C2: <100\"\n    elif Fare<200:\n        return \"C3: <200\"\n    elif Fare<300:\n        return \"C4: <300\"\n    else:\n        return \"C5: >=300\"\ntrain['Fare_Cat']=train['Fare'].apply(fare_cat)\nprint(pd.crosstab(train['Fare_Cat'], train['Survived']))\nprint(pd.crosstab(train['Fare_Cat'], train['Survived'], normalize='index'))","e6832c1e":"sns.countplot(train['Embarked'], hue=train['Survived'])\npd.crosstab(train['Embarked'],train['Survived'], normalize='index')","a2ae70aa":"# Drop variables not correlated\ntrain.drop(['Name','Ticket','PassengerId'], axis=1, inplace=True)\n\nTestId=test['PassengerId']\ntest.drop(['PassengerId', 'Name','Ticket'], axis=1, inplace=True)","12900b88":"train.drop(['Age_Group', 'Fare_Cat'], axis=1, inplace=True)","961aefc5":"# use seaborn.heatmap to check NAN values\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n# From the heatmap, Age has some NAN values and most of the Cabin are missing, Embarked has few missing values\n# Check missing percentage\ntrain_nan_pct=((train.isnull().sum())\/(train.isnull().count())).sort_values(ascending=False)\ntrain_nan_pct[train_nan_pct>0]","bc147eca":"# Drop the Cabin columns since too many NAN\ntrain.drop(['Cabin'], axis=1, inplace=True)\n\n# Since Age is skewed, impute with median\ntrain['Age'].fillna(train['Age'].median(), inplace=True)\n# Fill the numeric Embarked with mode\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","1ed5c1f1":"# use seaborn.heatmap to check NAN values for test data\nsns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n# From the heatmap, Age has some NAN values and most of the Cabin are missing, Fare has few missing values\n# Check missing percentage\ntest_nan_pct=((test.isnull().sum())\/(test.isnull().count())).sort_values(ascending=False)\ntest_nan_pct[test_nan_pct>0]","d41d861e":"test.drop(['Cabin'], axis=1, inplace=True)\ntest['Age'].fillna(test['Age'].median(), inplace=True)\ntest['Fare'].fillna(test['Fare'].median(), inplace=True)\n\n# Check whether impute Age by Pclass will imporve prediction performance","1cdba02b":"train=pd.get_dummies(train, drop_first=True)\ntest=pd.get_dummies(test, drop_first=True)","aa179eac":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val=train_test_split(train.drop(['Survived'], axis=1), train['Survived'], test_size=0.3, random_state=100)","0cd97628":"from sklearn.linear_model import LogisticRegression\ncs=[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]\nscore=[]\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfor c in cs:\n    lr=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=c, fit_intercept=True, intercept_scaling=1, class_weight=None)\n    lr.fit(X_train, y_train)\n    predicted=lr.predict(X_val)\n    score.append(accuracy_score(predicted, y_val))\nplt.scatter(x=cs, y=score)\nscore=pd.Series(score, index=cs)\nprint(score.argmax())\nprint(score.max())\n","4c477dd1":"from sklearn.linear_model import LogisticRegression\ncs=np.arange(6,12, 0.2)\nscore=[]\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfor c in cs:\n    lr=LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=c, fit_intercept=True, intercept_scaling=1, class_weight=None)\n    lr.fit(X_train, y_train)\n    predicted=lr.predict(X_val)\n    score.append(accuracy_score(predicted, y_val))\nplt.scatter(x=cs, y=score)\nscore=pd.Series(score, index=cs)\nprint(score.argmax())\nprint(\"The best accuracy score is: \", score.max())","3215d115":"lr=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=6, fit_intercept=True, intercept_scaling=1, class_weight=None)\nlr.fit(train.drop(['Survived'], axis=1), train['Survived'])\ntest_predicted=lr.predict(test)","403ce8ea":"submission=pd.DataFrame()\nsubmission['PassengerId']=TestId\nsubmission['Survived']=test_predicted\nsubmission.to_csv('submission.csv', index=False)","730c16a9":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler(copy=True, with_mean=True, with_std=True)\nsub_features=train[['Age','Fare']]\nsub_features\nscaler.fit(sub_features)\nscaler.fit_transform(sub_features)","271f784d":"train[['Age','Fare']]=scaler.fit_transform(sub_features)","cf165c38":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val=train_test_split(train.drop(['Survived'], axis=1), train['Survived'], test_size=0.3, random_state=100)","623240b8":"from sklearn.svm import SVC\nsvc=SVC(C=1, kernel='rbf', tol=0.001)\nsvc.fit(X_train, y_train)","c55148bb":"predicted=svc.predict(X_val)\nprint(confusion_matrix(y_val, predicted))\nprint('\\n')\nprint(classification_report(y_val, predicted))\nprint('\\n')\nprint('Accuracy score is: ', accuracy_score(y_val, predicted))\n# The accuracy score is not good by using the above parameters. We will tune the hyperparameters using GridSearchCV","b4f773ff":"# Create a dictionary called param_grid and fill out some parameters for C and gamma.\n# 'gamma':auto has  aaccuracy score of 0.8260\n#param_grid = {'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300], 'kernel': ['rbf'], 'gamma':['auto']}\n# 'gamma':[0.1] has better aaccuracy of 0.8271\nparam_grid = {'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300], 'kernel': ['rbf'], 'gamma': [10,1,0.1,0.01,0.001,0.0001]}\n\nfrom sklearn.model_selection import GridSearchCV\n# we can specify scoring='accuracy' (default), 'precision', 'f1', 'recall' to choose parameters\ngrid=GridSearchCV(SVC(), param_grid, verbose=1,  scoring='accuracy', refit=True)\ngrid.fit(train.drop(['Survived'], axis=1), train['Survived'])","92b78a68":"# The best hyperparameters chosen is\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint('Mean cross-validated score of the best_estimator: ', grid.best_score_)\nprint('The number of cross-validation splits (folds\/iterations): ', grid.n_splits_)","c2f4ccf7":"# Re-tune the hyperparameters based on previous results C=9, gamma=0.05, 0.8316498316498316\nparam_grid = {'C': np.arange(1,20), 'kernel': ['rbf'], 'gamma': [0.01,0.03,0.04, 0.05, 0.06, 0.07, 0.1,0.13,0.15,0.17,0.2,0.23,0.25,0.27,0.3]}\n\nfrom sklearn.model_selection import GridSearchCV\n# we can specify scoring='accuracy' (default), 'precision', 'f1', 'recall' to choose parameters\ngrid=GridSearchCV(SVC(), param_grid, verbose=1,  scoring='accuracy', refit=True)\ngrid.fit(train.drop(['Survived'], axis=1), train['Survived'])\n\n# The best hyperparameters chosen is\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint('Mean cross-validated score of the best_estimator: ', grid.best_score_)\nprint('The number of cross-validation splits (folds\/iterations): ', grid.n_splits_)","cab7c12c":"test_sub_features=test[['Age','Fare']]\ntest_sub_features\nscaler.fit(test_sub_features)\nscaler.fit_transform(test_sub_features)\ntest[['Age','Fare']]=scaler.fit_transform(test_sub_features)","3f5812a8":"test.head()","02f87a75":"svc=SVC(C=9, gamma=0.05, kernel='rbf', tol=0.001)\nsvc.fit(train.drop(['Survived'], axis=1), train['Survived'])\ntest_predicted=svc.predict(test)\nsubmission=pd.DataFrame()\nsubmission['PassengerId']=TestId\nsubmission['Survived']=test_predicted\nsubmission.to_csv('submission.csv', index=False)","6fc9c665":"### 7.3 GridSearchcv to tune hyperparameters\n\n* GridSearchCV exhaustive search over specified parameter values for an estimator.","18597193":"### 7.2 Model evaluation","2ab7439a":"### 3.5 Survived and number of Sibling and spouse\n\nsibsp: The dataset defines family relations in this way...\n* Sibling = brother, sister, stepbrother, stepsister\n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n(1) Most people do not have siblings or spouses on boat\n\n(2) People with 1 sibling or spouse has highest survival rate of 53.6%","32e2cc07":"## Part 5: Create dummy variables for categorical variables\n\nscipy.sklearn could not deal with categorical variables directly. So we need to first create dummy variables. We use drop_first=True to avoid multicolinearity","44365ce4":"### 3.6 Survived and number of Parent\/Child\n\nparch: The dataset defines family relations in this way...\n* Parent = mother, father\n* Child = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n(1) Most people do not bring parents or children","ef137b2d":"For this kernel, I will try different machine learning classification models to predict the survival in the Titanic tragety.\n\n1. The simple logistic regression has prediction score 0.74641, rank 9565, bad\n1. The Support Vector Classification with GridSearchCV  has scored 0.79, ranked up 2373\n1. The Support Vector Classification with with normalized numeric feature,   has scored 0.78947, ranked up 4448\n1. Random forest classifier","e1ac43c3":"### 7.1 Normalize the numeric features","cb5a0ba0":"## Part 2: Load the train and test data and check data","5afdf0b1":" ## Part 4: Check and impute missing values","ee5e2b3b":"### 3.2 Check whether the survival has any relationship with Gender\n* From the count plot and two way contigency table, 74% of female survive and only 19% of male survive.\n* Female are more likely to survive in the Titanic tragedy","be0ccdd0":"1. First we will use the numeric Age and Fare in the model\n2. Next we will use the Age_Group and Fare_Cat","4f743c56":"## Part 6: Build Logistic Regression to predict survival\n\nLogistic regression is a classifical statistical model for classification and is good for intepretation. For logistic regression, we build the hypothesis function log((p(y=1|X))\/(1-p(y=1|X)))=theta*X. So the probability of y=1 given X is p(y=1|X)=(exp(theta*X))\/(1+exp(theta*X)). ","f6cb48ce":"### 6.1 Train\/validation split the train data set","0b8cf023":"### 3.1 Count plot to check how many people died\n\n* Survival 0 = No, 1 = Yes\n* We can see that about 62% died and 38% alive","bec09222":"### 3.7 Survived and Fare\n\n* Most of the Fare price are under 50\n* People with Fare price >50 are more likelily to survive than people with Fare price <50. This is consistent with the Pclass.","17629190":"### 3.4 Check age information and Survival by Age\n\n* We can see that there are some children and very senior people on the boat\n* Children under 10 has more than 50% of survival rate\n* The survival rate decreases as age grow. Child under 5% has survival rate of 67.5% and senior people >60 has only survival rate of 27%","9db0b055":"### 7.2 Build simple SVC model\n\nGiven a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.","40f21f9d":"* Create age group and check the relationship between Survived and Age","8a3d34c6":"### Use all training data to fit model with the best parameters\n{'C': 9, 'gamma': 0.05, 'kernel': 'rbf'}\n","0aa965cb":"### Normalize the test features","9101be46":"### 3.9 Drop 'PassengerId', 'Name','Ticket' which seems not correlated with survival\n\n* Name, Ticket Number and PassengerId seems not correlated with Survived, we will drop them for now","4a1187b9":"* I am still working on improving this kernel. I will keep updating my tries and whether they work or not. \n* If you think my kernel is helpful, please give me a voteup. This is very important for new people like me. Thank you in advance.\n* If you have any question, please feel free to leave me a message, I will check every day. Thank you so much.\n","61d4ff7a":"## Part 7. SVM classifier","45d8a7d4":"### 6.2 Logistic Regression\n\n9565\/score=0.74641","378c08d7":"### 3.3 Check the relationship between Survival and Economic class\n\npclass: A proxy for socio-economic status (SES)\n* 1st = Upper\n* 2nd = Middle\n* 3rd = Lower\n\nFrom the Count plot and contigency table, \n* Upper class with Pclass=1 has survival rate 63%\n* Middle class with Pclass=2 has survival rate 47%\n* Lower class with Pclass=3 has survival rate of only 24%\n\nSo Higher Economic class people are more likely to survive than people in lower Economic class ","14cf5f20":"## Part 3: Data Visualization and feature selection","5b9f600e":"## Part 1: Import the libraries","06f64220":"### 3.8 Survived and Embarked\n\nPort of Embarkation \n* C = Cherbourg, \n* Q = Queenstown, \n* S = Southampton \n\nPeople embarked at Cherbourg has survival rate of 55% which is much higher than the other two ports. Possibly because people at Cherbourg are richer."}}