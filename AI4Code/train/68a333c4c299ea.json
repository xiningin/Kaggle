{"cell_type":{"ce3e0c2f":"code","d7cb6601":"code","eb400f9f":"code","d62620a5":"code","a73a5bf1":"code","d7c62401":"code","30852ebf":"code","f3779764":"markdown"},"source":{"ce3e0c2f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        pass","d7cb6601":"all_train_data = pd.read_csv('..\/input\/bri-data-hackathon-people-analytic\/train.csv')\nall_test_data = pd.read_csv('..\/input\/bri-data-hackathon-people-analytic\/test.csv')\nsample_submission = pd.read_csv('..\/input\/bri-data-hackathon-people-analytic\/sample_submission.csv')\ndropped_columns = ['year_graduated']\ny_train = all_train_data['Best Performance']\nX_train = all_train_data.drop('Best Performance',axis=1)\nfor col in dropped_columns:\n    X_train = X_train.drop(col,axis=1)\nX_test = all_test_data\nfor col in dropped_columns:\n    X_test = X_test.drop(col,axis=1)\nX_train_, X_test_, y_train_, y_test_ = train_test_split(X_train, y_train, test_size=0.2, random_state=38)\n","eb400f9f":"np.unique(X_test[cat])","d62620a5":"cat_columns = ['job_level', 'person_level', 'Employee_type', 'Employee_status', 'gender', 'marital_status_maried(Y\/N)', 'Education_level', 'achievement_target_1', 'achievement_target_2', 'achievement_target_3']\nfor cat in cat_columns:\n    labelencoder = LabelEncoder()\n    X_train_[cat] = labelencoder.fit_transform(X_train_[cat].astype(str))\n    X_test_[cat] = labelencoder.transform(X_test_[cat].astype(str))\nfor cat in cat_columns:\n    labelencoder = LabelEncoder()\n    X_train[cat] = labelencoder.fit_transform(X_train[cat].astype(str))\n    X_test[cat] = labelencoder.transform(X_test[cat].astype(str))        \n    ","a73a5bf1":"params = {'min_child_weight': 0.6715,\n                                              'max_depth': 12,\n                                              'num_leaves': 20,\n                                            'min_child_samples' :24,\n                                            'bagging_fraction' : 0.8538,\n                                            'lambda_l1' : 0.7467,\n                                            'lambda_l2' : 0.6911\n                                           }\nmodel_lgb = lgb.LGBMClassifier(**params)\nmodel_lgb.fit(X_train,y_train)\nmodel_lgb.predict_proba(X_test)[:,1]\nsample_submission['Best Performance'] = model_lgb.predict_proba(X_test)[:,1]\nsample_submission.to_csv('submission.csv')","d7c62401":"class lgbm_target :\n    def __init__(self, x_train, y_train, x_test, y_test) :\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_test = x_test\n        self.y_test = y_test\n        \n    def clean_param(self, param) :\n        params = {'boosting_type':'gbdt', 'class_weight':None, 'colsample_bytree':1.0, \n                  'importance_type':'split', 'learning_rate':0.1,\n                  'min_child_samples':20, 'min_split_gain':0.0, 'n_estimators':100, 'objective':None,\n                  'random_state':0, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n                  'subsample':1.0, 'subsample_for_bin':200000, 'subsample_freq':0}\n        params['num_leaves'] = int(param['num_leaves'])\n        params['min_child_weight'] = int(param['min_child_weight'])\n        params['max_depth'] = int(param['max_depth'])\n        params['learning_rate'] = 0.1\n        params['min_data_in_bin'] = 1\n        params['min_data'] = 1\n        \n        params['min_child_samples'] = int(param['min_child_samples'])\n        params['bagging_fraction'] = param['bagging_fraction']\n        params['lambda_l1'] = param['lambda_l1']\n        params['lambda_l2'] = param['lambda_l2']\n\n        return params\n        \n    def evaluate(self, min_child_weight, max_depth, num_leaves,\n                min_child_samples, bagging_fraction, lambda_l1, lambda_l2):\n        params = {'num_leaves':num_leaves, \n                  'min_child_weight':min_child_weight, \n                  'max_depth':max_depth,\n                 'min_child_samples':min_child_samples,\n                 'bagging_fraction' : bagging_fraction,\n                 'lambda_l1' : lambda_l1,\n                 'lambda_l2' : lambda_l2}\n        \n        params = self.clean_param(params)\n\n        lgbm_model = lgb.LGBMClassifier(**params)\n        lgbm_model.fit(self.x_train, self.y_train)\n        y_pred = lgbm_model.predict_proba(self.x_test)\n        predictions = y_pred[:,1]\n#         rmse = np.sqrt(mean_squared_error(self.y_test, predictions))\n#         return -1*rmse\n        acc = roc_auc_score(self.y_test,predictions)\n        return acc\n\nlt = lgbm_target(X_train_, y_train_, X_test_, y_test_)\nlgbmBO = BayesianOptimization(lt.evaluate, {'min_child_weight': (0.01, 1),\n                                              'max_depth': (7, 15),\n                                              'num_leaves': (5, 50),\n                                            'min_child_samples' :(10,50),\n                                            'bagging_fraction' : (0.5,1),\n                                            'lambda_l1' : (0,1),\n                                            'lambda_l2' : (0,1)\n                                           }, \n                             random_state=3)\n\nlgbmBO.maximize(init_points=30, n_iter=20)","30852ebf":"y_label_adv = np.zeros((X_train.shape[0]+ X_test.shape[0]))\ny_label_adv[:X_train.shape[0]] = 1\nadversarial_data = pd.concat((X_train,X_test))\nmodel_lgb = lgb.LGBMClassifier()\nX_train_, X_test_, y_train_, y_test_ = train_test_split(adversarial_data, y_label_adv, test_size=0.33, random_state=38)\n\nmodel_lgb.fit(X_train_,y_train_)\ny_pred  = model_lgb.predict(X_test_)\nroc_auc_score(y_pred,y_test_)","f3779764":"ADVERSARIAL VALIDATION"}}