{"cell_type":{"216e1c9a":"code","68597b76":"code","575ea6f5":"code","3d10deb9":"code","8c220355":"code","bdb8d14f":"code","4a295111":"code","c6c16ae6":"code","bb4e03a6":"code","fb75a8f6":"code","cb60e55b":"code","8efd1118":"code","1843f6a3":"code","4b958c8d":"code","ede19641":"code","02ed4126":"code","52bfd772":"code","5d62e190":"code","0b2d0329":"code","7799ccd2":"code","1a814263":"code","8695ff78":"code","fcf0ea9a":"code","fb181f13":"code","6039a605":"code","3f891a46":"code","0c8cd461":"code","481cfb69":"code","16552630":"code","782548ae":"code","e8f967a1":"code","cdfed3d4":"code","7dff8705":"code","198d47f2":"code","6e7b55a2":"code","bce9185d":"code","ca1379ae":"code","d9a4c7d3":"code","e776eb8b":"code","8b32a1f9":"code","a6e51e48":"code","820109ec":"code","2d18af65":"code","2158e8c3":"code","8dd42bea":"code","72cac4a6":"code","a85ff10c":"code","d9cccdf3":"code","ca4e7e3d":"code","132a7077":"code","b4bc3539":"code","08aa2801":"code","b547aabd":"code","6fe607cc":"markdown","a949f4b5":"markdown","e2668076":"markdown","920eb121":"markdown","b1c929e6":"markdown","a6025cfc":"markdown","5507d3af":"markdown","53bd4471":"markdown","22e48bc5":"markdown","08daa62a":"markdown","100fd11d":"markdown","974d5a71":"markdown","fff29e4e":"markdown","8b749086":"markdown","7aaa8eb1":"markdown","57fe9324":"markdown","58a576b0":"markdown","3618be00":"markdown","930402a7":"markdown","f205580f":"markdown","e7b921df":"markdown","81fdf39c":"markdown"},"source":{"216e1c9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport itertools as it # to avoid nested for-loops\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # to search for substrings using regular expressions\nimport scipy.stats as stats # for performing t-tests for statistically significant difference in mean values\nfrom sklearn.model_selection import KFold # for k-fold validation of models\nfrom sklearn import metrics as met # for model evaluation metrics\nimport sklearn.linear_model as lm # for linear models\nfrom sklearn import tree # decision tree used to discretize some continuous variables\nimport seaborn as sns # for vizualiations\nimport statsmodels.api as sm # Used to create logistic regression models to check for statistical significance of continuous variables. This package returns confidence intervals for predictors\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68597b76":"# Save test data as dataframe and preview in order to get a sense of the what the data look like.\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndisplay(train.head())","575ea6f5":"# Initial review shows that Pclass appears to be predictive\ndisplay(train[['Survived','Pclass']].groupby(['Pclass']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Pclass'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} \/ {total_rows} = {nulls\/total_rows}')","3d10deb9":"# The Names column contains too many unique values. We will investigate particular substrings.\npd.set_option(\"display.max_rows\", 20)\ndisplay(train['Name'])\nnulls = train['Name'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} \/ {total_rows} = {nulls\/total_rows}')","8c220355":"# It appears that all titles (i.e. 'Mr.', 'Mrs.',...) can be found between a space and a period, so we will create a set of unique titles by searching\n# for this pattern in every name\ntitles = set()\nfor i in train['Name']:\n    title = re.search('\\s([a-zA-Z]+)\\.', i)\n    titles.add(title.group(1))\nprint(titles)","bdb8d14f":"# Create new columns with an indicator for each title and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor title in titles:\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    display(copy[['Survived',title]].groupby([title]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')","4a295111":"# Many titles did not have sufficient frequency to use for predictions, but several may prove useful in our model\nfor title in titles.copy():\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    # After looking over the t-test results, we will keep all of the variables for which survival was significantly different amongs the two groups\n    if not p<0.05:\n        titles.remove(title)\ntitles = list(titles)\ndisplay(titles)\n# Add these new indicator columns to our actual dataset for these titles now that we have chosen them\nfor title in titles:\n    train[title] = np.where(copy['Name'].str.contains(title), 1, 0)","c6c16ae6":"# Display unique values and how they survived\ndisplay(train[['Survived','Sex']].groupby(['Sex']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Sex'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} \/ {total_rows} = {nulls\/total_rows}')\n# Perform a t-test to check for statistically significant difference in survival rates\nt, p = stats.ttest_ind(a = train[train['Sex'] == 'female']['Survived'], b = train[train['Sex'] == 'male']['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')","bb4e03a6":"# visualize distribution of ages\ntrain['Age'].hist()\n# check for null values\\\nnulls = train['Age'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} \/ {total_rows} = {nulls\/total_rows}')","fb75a8f6":"# first we will try imputing the mean age into the null values\ntrain['Age_mean_imputed'] = np.where(train['Age'].isnull(),np.mean(train['Age']),train['Age'])\n# Now lets fit a bivariate regression model\nY, X = train['Survived'], train['Age_mean_imputed']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])","cb60e55b":"# second method is to use a regression model to predict the missing ages\nsub_data = train[~train['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = train[train['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntrain['Age_lm'] = train['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = train[train['Age_lm'].isnull()].index)[0]\ntrain['Age_lm'].fillna(age_predictions_indexed, inplace = True)\n\n# Now we will see how this predictor does at predicting Survival\nY, X = train['Survived'], train['Age_lm']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])","8efd1118":"# Finally, let's use ROC AUC to compare which version is superior\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'lbfgs')\nROC_AUC_mean = []\nROC_AUC_lm = []\n# We will do 5-fold validation when calculating the ROC_AUC\nkfold = KFold(10, shuffle = True, random_state = 1)\nfor tr, te in kfold.split(train):\n    # capture metrics for the first variation of the age with mean imputing:\n    X, y = np.array(train['Age_mean_imputed']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_mean_imputed']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_mean.append(met.roc_auc_score(y_true, y_score = y_hat))\n    \n    # capture metrics for the second variation of the age with linear regression:\n    X, y = np.array(train['Age_lm']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_lm']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_lm.append(met.roc_auc_score(y_true, y_score = y_hat))\ndisplay(np.average(ROC_AUC_mean))\ndisplay(np.average(ROC_AUC_lm))","1843f6a3":"# Initial review shows that SibSp appears to be predictive\ndisplay(train[['Survived','SibSp']].groupby(['SibSp']).agg({'Survived':['count','sum','mean']}))\nnulls = train['SibSp'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} \/ {total_rows} = {nulls\/total_rows}')","4b958c8d":"# many values of SibSp have a very low frequency. Let's try simple indicator for any siblings or spouses\ntrain['SibSp_ind'] = np.where(train['SibSp'] > 0, 1, 0)\ndisplay(train[['Survived','SibSp_ind']].groupby(['SibSp_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['SibSp_ind'] == 1]['Survived'], b = train[train['SibSp_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')","ede19641":"# Initial review shows that Parch appears to be predictive\ndisplay(train[['Survived','Parch']].groupby(['Parch']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Parch'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} \/ {total_rows} = {nulls\/total_rows}')","02ed4126":"# Let's consider an indicator for Parch as well due to low frequency of high values\ntrain['Parch_ind'] = np.where(train['Parch'] > 0, 1, 0)\ndisplay(train[['Survived','Parch_ind']].groupby(['Parch_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['Parch_ind'] == 1]['Survived'], b = train[train['Parch_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')","52bfd772":"# The Ticket column contains too many unique values. We will investigate particular substrings.\n#pd.set_option(\"display.max_rows\", None)\ndisplay(train['Ticket'])\nnulls = train['Ticket'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} \/ {total_rows} = {nulls\/total_rows}')","5d62e190":"# let's look for patterns in the beginning letters of the ticket\nprefixes = set()\nfor i in train['Ticket']:\n    pref = re.search('([^\\s]+)\\s', i)\n    if pref is None: prefixes.add(\"None\")\n    else: prefixes.add(pref.group(1))\nprint(prefixes)","0b2d0329":"# Create new columns with an indicator for each prefix and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor pref in prefixes:\n    # Make sure to check for a space at the end of the prefix so we don't accidentally capture substrings\n    copy[pref] = np.where(copy['Ticket'].str.contains(f'{pref} '), 1, 0)\n    display(copy[['Survived',pref]].groupby([pref]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[pref] == 0]['Survived'], b = copy[copy[pref] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')","7799ccd2":"# There was a very low frequency of most prefixes. Let's compare prefixes to no prefix\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(tuple(prefixes)), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')","1a814263":"# as a whole, prefixes do not appear to be predictive. Let's just keep the two that were frequent enough and predictive on their own:\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(('C ','PC ')), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')","8695ff78":"# since these two prefixes are frequent enough and have sufficiently different survival rates, we'll keep them in for consideration\ntrain['Ticket_PC_C'] = np.where(train['Ticket'].str.startswith(('C ','PC ')), 1, 0)","fcf0ea9a":"copy['Ticket_num'] = copy.Ticket.str.extract('(^\\d*)')\ncopy['Ticket_num'] = copy['Ticket_num'].str.len()\ndisplay(copy[['Survived','Ticket_num']].groupby(['Ticket_num']).agg({'Survived':['count','sum','mean']}))","fb181f13":"train['Ticket_len5'] = train.Ticket.str.extract('(^\\d*)')\ntrain['Ticket_len5'] = np.where(train['Ticket_len5'].str.len() == 5, 1, 0)","6039a605":"# visualize distribution of ages\ntrain['Fare'].hist(bins = 40)\n# check for null values\\\nnulls = train['Fare'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} \/ {total_rows} = {nulls\/total_rows}')","3f891a46":"# since the data are highly skewed with some outliers, let's bin the values into buckets using a decision tree and use that instead of the original column\nDT = tree.DecisionTreeClassifier(min_samples_leaf = 0.08, max_depth = 3)\nFare_tree = DT.fit(train['Fare'].to_frame(), train['Survived'])\ntrain['Fare_binned'] = Fare_tree.predict_proba(train['Fare'].to_frame())[:,1]\ndisplay(train[['Survived','Fare_binned','Fare']].groupby(['Fare_binned']).agg({'Survived':['count','sum','mean'],'Fare':['min','max','mean']}))","0c8cd461":"# Initial review shows that Parch appears to be predictive\nnulls = train['Cabin'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} \/ {total_rows} = {nulls\/total_rows}')","481cfb69":"# Let's merge all cabins that start with the same letter\ncopy = train.copy()\ncopy['Cabin_group'] = copy['Cabin'].str[:1]\ncopy['Cabin_group'].fillna('None', inplace = True)\ndisplay(copy[['Survived','Cabin_group']].groupby(['Cabin_group']).agg({'Survived':['count','sum','mean']}))","16552630":"# Since there is still a low frequency in many groups and it's unclear how groups might be meaningfully merged, let's mean_encode this column\ntrain['Cabin_group'] = train['Cabin'].str[:1]\ntrain['Cabin_group'].fillna('None', inplace = True)\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Cabin_group'])","782548ae":"# Initial review shows that Embarked appears to be predictive\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Embarked'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} \/ {total_rows} = {nulls\/total_rows}')","e8f967a1":"# since there are only two null values for embarked, we will encode them with the mode\ntrain['Embarked'].fillna('S', inplace = True)\nprint(train['Embarked'].isnull().sum())\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))","cdfed3d4":"# let's mean encode this column as well so that we can have all numerical columns\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Embarked'])","7dff8705":"# create our final data set with just the columns we'll be keeping\ndata = train[['Survived','Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ndata.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ndata['Female'] = np.where(data['Sex'] == 'female', 1, 0)\ndata.drop('Sex', axis = 1, inplace = True)\ndisplay(data.head())\ndisplay(data.columns)","198d47f2":"X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create lists of parameters to try to figure out the optimal parameters\n# I started with much bigger ranges and narrowed it down to these\nmin_leafs = range(32,45)\nmax_depth = range(5,15)\n# create an empty dataframe in which to store ROC AUC score\ndf = pd.DataFrame(columns = max_depth, index = min_leafs)\n\n# for each combination of parameter values, figure out the ROC AUC using 10 fold validation\nfor leaf, depth in it.product(min_leafs, max_depth):\n    dtc = tree.DecisionTreeClassifier(min_samples_leaf = leaf, max_depth = depth)\n    kfold = KFold(10, shuffle = True, random_state = 1)\n    ROC_AUC_scores = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        # fit the data\n        dtc = dtc.fit(X_train, y_train)\n        # get predictions on the test set\n        y_pred = dtc.predict_proba(X_test)[:,1]\n        # get the ROC on the test set\n        ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n    # average the ROC from all folds\n    ROC_AUC = np.mean(ROC_AUC_scores)\n    # store average ROC AUC in dataframe\n    df.loc[leaf][depth] = float(ROC_AUC)","6e7b55a2":"# Looking at all of our parameter values, it looks like a max_depth = 7 and min_leaf_size = 38 are our best bets\ndf[df.columns] = df[df.columns].astype(float)\nsns.heatmap(df)","bce9185d":"np.max(df[7])","ca1379ae":"X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create an empty list in which to store ROC AUC score\nROC_AUC = []\npredictors = X.columns\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nkfold = KFold(10, shuffle = True, random_state = 1)\nselected = []\nmax_AUC = {}\n\n# starting with one predictor, figure out the ROC AUC in a bivariate model, then continuously add one predictor each time to maximize ROC AUC\nfor i in range(0, len(predictors)):\n    next_best = {}\n    for pred in [p for p in predictors if p not in selected]:\n        ROC_AUC_scores = []\n        X = data[selected + [pred]]\n        for tr, te in kfold.split(data):\n            X_train, X_test = X.loc[tr], X.loc[te]\n            y_train, y_test = y.loc[tr], y.loc[te]\n            fit = model.fit(X = X_train, y = y_train)\n            y_pred = fit.predict_proba(X_test)[:,1]\n            ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n        next_best[pred] = np.mean(ROC_AUC_scores)\n    selected.append(max(next_best, key = next_best.get))\n    max_AUC[', '.join(selected)] = next_best[max(next_best, key = next_best.get)]\nfor i in max_AUC.keys():\n    print(f'{i}: {max_AUC[i]}')","d9a4c7d3":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndisplay(test.head())","e776eb8b":"# Update names column\nfor title in ['Mrs', 'Mr', 'Miss', 'Master']:\n    test[title] = np.where(test['Name'].str.contains(title), 1, 0)\nlen(test[test['Name'].isnull()])","8b32a1f9":"#Update Sex columns\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\nlen(test[test['Sex'].isnull()])","a6e51e48":"#Update SibSp\ntest['SibSp_ind'] = np.where(test['SibSp'] > 0, 1, 0)\nlen(test[test['SibSp'].isnull()])\n# display(test.head())","820109ec":"#Update Parch\ntest['Parch_ind'] = np.where(test['Parch'] > 0, 1, 0)\nlen(test[test['Parch'].isnull()])\n# display(test.head())","2d18af65":"#Update ticket\ntest['Ticket_PC_C'] = np.where(test['Ticket'].str.startswith(('C ','PC ')), 1, 0)\nlen(test[test['Ticket_PC_C'].isnull()])\n\ntest['Ticket_len5'] = test.Ticket.str.extract('(^\\d*)')\ntest['Ticket_len5'] = np.where(test['Ticket_len5'].str.len() == 5, 1, 0)","2158e8c3":"#Update Fare\n#One null fare will be imputed with the median since outliers might skew the mean\ntest['Fare'].fillna(test['Fare'].median(), inplace = True)\n#Use the same decision tree we made before to impute the values\ntest['Fare_binned'] = Fare_tree.predict_proba(test['Fare'].to_frame())[:,1]\nlen(test[test['Fare'].isnull()])","8dd42bea":"#Update Cabin\ntest['Cabin_group'] = test['Cabin'].str[:1]\ntest['Cabin_group'].fillna('None', inplace = True)\n#Mean encode based on the original training data\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Cabin_group'])","72cac4a6":"#Update Embarked\nlen(test[test['Embarked'].isnull()])\n# mean encode\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Embarked'])","a85ff10c":"#Update Age column\nsub_data = test[~test['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = test[test['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntest['Age_lm'] = test['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = test[test['Age_lm'].isnull()].index)[0]\ntest['Age_lm'].fillna(age_predictions_indexed, inplace = True)\ndisplay(test.head())","d9cccdf3":"# create our final data set with just the columns we'll be keeping\ntest = test[['Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ntest.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\ntest.drop('Sex', axis = 1, inplace = True)\ndisplay(test.head())\ndisplay(test.columns)","ca4e7e3d":"model = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\nfit = model.fit(X = X, y = y)\n\ny_pred = fit.predict_proba(test[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']])[:,1]","132a7077":"sample = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission['Survived'] = y_pred\nsubmission = submission[['PassengerId','Survived']]\ndisplay(submission)","b4bc3539":"# In order to choose a cutoff value for survival, we will tune the cutoff on our original model to maximize the matthews correlation coefficient\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\ncutoffs = {}\n# try cutoff values from 0.1 to 0.9\nfor i in np.arange(0.3,0.7,0.001):\n    MCC = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        fit = model.fit(X = X_train, y = y_train)\n        y_pred = fit.predict_proba(X_test)[:,1]\n        y_pred = np.where(y_pred < i, 0, 1)\n        MCC.append(met.matthews_corrcoef(y_test, y_pred))\n    cutoffs[i] = np.mean(MCC)\ndisplay(max(cutoffs, key = cutoffs.get))","08aa2801":"submission['Survived'] = np.where(submission['Survived'] < 0.573, 0, 1)\nprint(submission['Survived'].mean())\ndisplay(submission.head(20))","b547aabd":"submission.to_csv('submission.csv', index=False)","6fe607cc":"### Embarked\nEmbarked appears to be related to persistence. Two null values will be mode encoded.","a949f4b5":"### Determine Prediction Cutoff\nA prediction cutoff value of 0.573 seems optimal for maximizing MCC.","e2668076":"### Fare\nWe will use a decision tree to transform Fare into bins because of several outliers and a highly skewed distribution.","920eb121":"# Getting Results","b1c929e6":"### Name\nHaving a title of 'Master', 'Mr', 'Mrs', or 'Miss' has a statistically significant relationship with persistence. We will create indicator columns for each of these to consider for our final model.","a6025cfc":"Neither variation of age shows a lot of lift on its own. We will keep the lm variation for now because it did provide slightly stronger ROC AUC lift, but it may or may not end up in the final model.","5507d3af":"### Final Data Cleanup","53bd4471":"### Logistic Regression\nThe ROC AUC esentially stops increasing with forward selection for the set of predictors including Female, Pclass, Master, Embarked, Cabin, Parch, Age, and Ticket_len5. This yields an ROC AUC of 0.870.","22e48bc5":"### Pclass\nPclass shows strong potential in differentiating passengers who survived vs. those who didn't.","08daa62a":"### Cabin\nCabins are mean encoded based on the survival rate of other passengers with the same letter portion of the cabin name. Those without a cabin are also mean encoded as a single group.","100fd11d":"### Ticket\nAfter exploring all of the ticket prefixes, only C and PC had high enough frequency and difference in survival rates to show a significant difference. We will transform this into an indicator of PC or C as a ticket prefix.","974d5a71":"# Import Data as DataFrame","fff29e4e":"Let's also investigate the length of the numeric portion of the ticket.","8b749086":"# Exploratory Data Analysis\nBefore creating any models, we will investigate each column's individual distribution and relationship with survival.","7aaa8eb1":"### SibSp\nHaving at least one sibling or spouse has a statistically significant relationship with survival. However, due to low frequency for many values, we will transform this into an indicator of at least one spouse or sibling.","57fe9324":"# Model Building","58a576b0":"About 1\/5 of the ages are missing. We will try a few methods for resolving this to see if we can still use this column.","3618be00":"It appears that passengers with exactly 5 numbers in their ticket survived at higher rates than those with a different number of numbers. Since most of the other categories have low frequency, we will combine them all.","930402a7":"### Sex\nFemales had a statistically significantly higher survival rate than males. We will keep this for consideration.","f205580f":"### Age\nThere are a number of null ages, too many for us to throw out those observations. After attempting to fill them with the mean and with values obtained through linear regression, we will use linear regression to impute them.","e7b921df":"### Parch\nHaving at least one parent or child has a statistically significant relationship with survival. However, due to low frequency for many values, we will transform this into an indicator of at least one parent or child.","81fdf39c":"### Decision Tree\nWith parameters set to min_samples_leaf = 32 and max_depth = 7, we achieved an average ROC AUC of 0.864 using 10-fold cross-validation"}}