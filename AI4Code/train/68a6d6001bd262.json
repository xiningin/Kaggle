{"cell_type":{"9934ff00":"code","c3b01e69":"code","783e8041":"code","41a50d93":"code","70ddcd84":"code","af02385c":"code","2dcff365":"code","a722309c":"code","e214bd27":"code","7801e2c2":"code","58d2f617":"code","0717bd99":"code","4573e0e6":"code","aab871e7":"code","b6eaa90d":"code","bf7e7d3e":"code","f019adcb":"code","cf16b5b6":"code","62975bd7":"code","8110ec2b":"code","44734443":"code","89d1fb69":"code","7fc7916f":"code","b02a7644":"code","5ff5c1fc":"code","671515cc":"code","67c04082":"code","21fd200c":"code","207eb857":"markdown","5b9d887c":"markdown","838de508":"markdown","c2d562ad":"markdown","7ed8a936":"markdown","857e305e":"markdown","a88256e1":"markdown","2991422a":"markdown","20537653":"markdown"},"source":{"9934ff00":"#importing the libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c3b01e69":"#loading the dataset and obtaining info about columns\ndf=pd.read_csv(\"..\/input\/housing.csv\")\nlist(df)","783e8041":"#description of the numerical columns\ndf.describe()","41a50d93":"#count the values of the columns\ndf.count()","70ddcd84":"#We have missing values in the column total_bedrooms. We can drop the null rows or replace the null value for the mean.\n#I choose to replace it with the mean\ndf['total_bedrooms'].fillna(df['total_bedrooms'].mean(), inplace=True)\n","af02385c":"#I want information about the column \"ocean_proximity\"\ndf['ocean_proximity'].value_counts()\n","2dcff365":"#Transform the variable into a numerical one.\ndef map_age(age):\n    if age == '<1H OCEAN':\n        return 0\n    elif age == 'INLAND':\n        return 1\n    elif age == 'NEAR OCEAN':\n        return 2\n    elif age == 'NEAR BAY':\n        return 3\n    elif age == 'ISLAND':\n        return 4\ndf['ocean_proximity'] = df['ocean_proximity'].apply(map_age)","a722309c":"#Obtaining info of the correlations with a heatmap\nplt.figure(figsize=(15,8))\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df.corr(), linewidths=.5,annot=True,mask=mask,cmap='coolwarm')\n","e214bd27":"#There is a high correlation between households and populati\ndf.drop('households', axis=1, inplace=True)\n","7801e2c2":"# let's create 2 more columns with the total bedrooms and rooms per population in the same block.\ndf['average_rooms']=df['total_rooms']\/df['population']\ndf['average_bedrooms']=df['total_bedrooms']\/df['population']\n","58d2f617":"#dropping the 2 columns we are not going to use\ndf.drop('total_rooms',axis=1,inplace=True)\ndf.drop('total_bedrooms',axis=1,inplace=True)","0717bd99":"#Obtaining info of the new correlations with a heatmap\nplt.figure(figsize=(15,8))\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df.corr(), linewidths=.5,annot=True,mask=mask,cmap='coolwarm')","4573e0e6":"#histogram to get the distributions of the different variables\ndf.hist(bins=70, figsize=(20,20))\nplt.show()","aab871e7":"#Finding Outliers\nplt.figure(figsize=(15,5))\nsns.boxplot(x=df['housing_median_age'])\nplt.figure()\nplt.figure(figsize=(15,5))\nsns.boxplot(x=df['median_house_value'])","b6eaa90d":"#removing outliers\ndf=df.loc[df['median_house_value']<500001,:]","bf7e7d3e":"#Choosing the dependant variable and the regressors. In this case we want to predict the housing price\nX=df[['longitude',\n 'latitude',\n 'housing_median_age',\n 'population',\n 'median_income',\n 'ocean_proximity',\n 'average_rooms',\n 'average_bedrooms']]\nY=df['median_house_value']","f019adcb":"#splitting the dataset into the train set and the test set\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state=0)","cf16b5b6":"#Training the model\nfrom sklearn.linear_model import LinearRegression\nregressor= LinearRegression()\nregressor.fit(X_train,Y_train)","62975bd7":"#Obtaining the predictions\ny_pred = regressor.predict(X_test)\n","8110ec2b":"#R2 score\nfrom sklearn.metrics import r2_score\nr2=r2_score(Y_test,y_pred)\nprint('the R squared of the linear regression is:', r2)","44734443":"#Graphically\ngrp = pd.DataFrame({'prediction':y_pred,'Actual':Y_test})\ngrp = grp.reset_index()\ngrp = grp.drop(['index'],axis=1)\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(20,10))\nplt.plot(grp[:120],linewidth=2)\nplt.legend(['Actual','Predicted'],prop={'size': 20})","89d1fb69":"import xgboost as xgb\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 1,eta=0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 2000)","7fc7916f":"xg_reg.fit(X_train,Y_train)\n\ny_pred2 = xg_reg.predict(X_test)","b02a7644":"#Graphically\ngrp = pd.DataFrame({'prediction':y_pred2,'Actual':Y_test})\ngrp = grp.reset_index()\ngrp = grp.drop(['index'],axis=1)\nplt.figure(figsize=(20,10))\nplt.plot(grp[:120],linewidth=2)\nplt.legend(['Actual','Predicted'],prop={'size': 20})","5ff5c1fc":"r2xgb=r2_score(Y_test,y_pred2)\nprint('the R squared of the xgboost method is:', r2xgb)","671515cc":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","67c04082":"#Doing cross validation to see the accuracy of the XGboost model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(regressor, X, Y, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","21fd200c":"#comparing the scores of both techniques \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt\n\nmae1 = mean_absolute_error(Y_test, y_pred)\nrms1 = sqrt(mean_squared_error(Y_test, y_pred))\nmae2 =mean_absolute_error(Y_test,y_pred2)\nrms2 = sqrt(mean_squared_error(Y_test, y_pred2))\n\nprint('Stats for the linea regression: \\n','mean squared error: ',rms1, '\\n R2:',r2,' \\n mean absolute error:',mae1 )\nprint('Stats xgboost: \\n','mean squared error: ',rms2, '\\n R2:',r2xgb,' \\n mean absolute error:',mae2 )","207eb857":"# XGBoost <a class=\"anchor\" id=\"Xgboost\"><\/a>","5b9d887c":"## Training the model <a class=\"anchor\" id=\"Training\"><\/a>","838de508":"## Evaluating the model <a class=\"anchor\" id=\"Evaluation2\"><\/a>","c2d562ad":"## Training the model <a class=\"anchor\" id=\"Training2\"><\/a>","7ed8a936":"# Linear Regression <a class=\"anchor\" id=\"Regression\"><\/a>","857e305e":"## Evaluating the model <a class=\"anchor\" id=\"Evaluation\"><\/a>","a88256e1":"## Preprocessing the data <a class=\"anchor\" id=\"preprocessing\"><\/a>","2991422a":"# Linear regression vs XGBoost <a class=\"anchor\" id=\"Comparison\"><\/a>","20537653":"# ****California Housing Prices (a linear regression and a XGboost)****\n\n## Table of Contents:\n* [1-Preprocessing the data](#preprocessing)\n* [2-Linear Regression](#Regression)\n    * [2.1-Training the model](#Training)\n    * [2.2-Evaluating the model](#Evaluation)\n* [3-XGBoost](#Xgboost)\n    * [3.1-Training the model](#Training2)\n    * [3.2-Evaluating the model](#Evaluation2)\n* [4-XGBoost vs Linear Regression](#Comparison)"}}