{"cell_type":{"0b398821":"code","4a365da6":"code","8d8c6526":"code","fd913d92":"code","7e854466":"code","e38804d7":"code","c875fdbf":"code","cfe18610":"code","15aaf000":"code","11b14905":"code","a1ffc784":"code","0c04ee0c":"code","912bfbab":"code","9a7d9906":"code","779b85f0":"code","adfaa18c":"code","b6641c41":"code","4e487df4":"code","c043bcb6":"code","f473f809":"code","b770d9fb":"code","fc0707d3":"code","dc70927a":"code","4af2c24f":"code","81db5a24":"code","1f3f2a84":"code","70ad67ff":"code","80d6ec60":"code","85d03a1e":"code","c2012177":"code","2072f9a1":"code","e3256bf1":"code","aafa3329":"code","31012b2f":"code","e1595533":"code","907fb60a":"code","84a34e53":"code","eab69205":"code","55e2754e":"code","bda483c1":"code","20d6f870":"markdown","8c1b4d7a":"markdown"},"source":{"0b398821":"!pip install glove_python","4a365da6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.tokenize import WordPunctTokenizer\nfrom collections import Counter\nfrom string import punctuation, ascii_lowercase\nimport regex as re\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras import Sequential\nfrom keras.layers import Conv1D, Flatten, MaxPooling1D, Dense, Dropout, Input, LSTM\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense, Input, Conv1D, Embedding, Dropout, Flatten, MaxPooling1D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom sklearn.metrics import mean_squared_log_error\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8d8c6526":"train_df = pd.read_csv(\"\/kaggle\/input\/nnfl-assignment-2\/final_train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nnfl-assignment-2\/final_test.csv\")","fd913d92":"train_df.head()","7e854466":"def textClean(text):\n    text = text.replace(\".\",\" \").replace(\",\",\" \").replace(\";\",\" \")\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = {'so', 'his', 't', 'y', 'ours', 'herself', \n             'your', 'all', 'some', 'they', 'i', 'of', 'didn', \n             'them', 'when', 'will', 'that', 'its', 'because', \n             'while', 'those', 'my', 'don', 'again', 'her', 'if',\n             'further', 'now', 'does', 'against', 'won', 'same', \n             'a', 'during', 'who', 'here', 'have', 'in', 'being', \n             'it', 'other', 'once', 'itself', 'hers', 'after', 're',\n             'just', 'their', 'himself', 'theirs', 'whom', 'then', 'd', \n             'out', 'm', 'mustn', 'where', 'below', 'about', 'isn',\n             'shouldn', 'wouldn', 'these', 'me', 'to', 'doesn', 'into',\n             'the', 'until', 'she', 'am', 'under', 'how', 'yourself',\n             'couldn', 'ma', 'up', 'than', 'from', 'themselves', 'yourselves',\n             'off', 'above', 'yours', 'having', 'mightn', 'needn', 'on', \n             'too', 'there', 'an', 'and', 'down', 'ourselves', 'each',\n             'hadn', 'ain', 'such', 've', 'did', 'be', 'or', 'aren', 'he', \n             'should', 'for', 'both', 'doing', 'this', 'through', 'do', 'had',\n             'own', 'but', 'were', 'over', 'not', 'are', 'few', 'by', \n             'been', 'most', 'no', 'as', 'was', 'what', 's', 'is', 'you', \n             'shan', 'between', 'wasn', 'has', 'more', 'him', 'nor',\n             'can', 'why', 'any', 'at', 'myself', 'very', 'with', 'we', \n             'which', 'hasn', 'weren', 'haven', 'our', 'll', 'only',\n             'o', 'before'}\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    \n    return(text)","e38804d7":"train_df[\"clean_text\"] = [textClean(t) for t in train_df.desc]\ntest_df[\"clean_text\"] = [textClean(t) for t in test_df.desc]","c875fdbf":"num_words = 50000\ntokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                   lower=True,split=' ')\ntokenizer.fit_on_texts(train_df['clean_text'].values)\nX = tokenizer.texts_to_sequences(train_df['clean_text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nmax_length_of_text = 200\nX = pad_sequences(X, maxlen=max_length_of_text)\n\nprint(word_index)","cfe18610":"y = train_df['rating']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","15aaf000":"embed_dim = 100 #Change to observe effects\n\ninputs = Input((max_length_of_text, ))\nx = Embedding(num_words, embed_dim)(inputs)    \nx = Conv1D(32, 3, activation = \"relu\", kernel_initializer=\"he_uniform\")(x)\nx = Conv1D(32, 3, activation = \"relu\", kernel_initializer=\"he_uniform\")(x)\nx = MaxPooling1D(2)(x)\nx = Dropout(0.5)(x)\nx = Conv1D(64, 3, activation = \"relu\", kernel_initializer=\"he_uniform\")(x)\nx = Conv1D(64, 3, activation = \"relu\", kernel_initializer=\"he_uniform\")(x)\nx = MaxPooling1D(2)(x)\nx = Dropout(0.5)(x)\nx = Conv1D(64, 3, activation = \"relu\", kernel_initializer=\"he_uniform\")(x)\nx = Conv1D(64, 3, activation = \"relu\", kernel_initializer=\"he_uniform\")(x)\nx = MaxPooling1D(2)(x)\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = Dense(128, activation = \"relu\", kernel_initializer=\"he_uniform\")(x)\nx = Dense(1)(x)\nmodel = Model(inputs, x)\nprint(model.summary())","11b14905":"model.compile(loss = 'mean_squared_logarithmic_error', optimizer='adam',metrics = ['accuracy'])\nhist = model.fit(X, y, validation_split=0.1 ,batch_size = 1032, epochs = 10)","a1ffc784":"history = pd.DataFrame(hist.history)\nplt.figure(figsize=(12,12))\nplt.plot(history[\"loss\"])\nplt.plot(history[\"val_loss\"])\nplt.title(\"Loss with pretrained word vectors\")\nplt.show()","0c04ee0c":"predict = model.predict(X_test)\npredict = [float(np.round(i)) if i<10.0 else 10.0 for i in predict]","912bfbab":"np.sqrt(mean_squared_log_error(y_test,predict))","9a7d9906":"x_test = tokenizer.texts_to_sequences(test_df.clean_text.values)\nx_test = pad_sequences(x_test, maxlen=max_length_of_text)","779b85f0":"pred = model.predict(x_test)","adfaa18c":"pred = [float(np.round(i)) if i<10.0 else 10.0 for i in pred]\nprint(pred)","b6641c41":"final_df = pd.DataFrame(list(zip(test_df[\"Id\"],pred)), columns = [\"Id\",\"Rating\"])\nfinal_df.head()","4e487df4":"final_df.to_csv(\"submission_cnn.csv\",index=False)","c043bcb6":"model.save_weights(\"model_cnn.h5\")","f473f809":"from IPython.display import FileLink","b770d9fb":"FileLink(\"submission_cnn.csv\")","fc0707d3":"FileLink(\"model_cnn.h5\")","dc70927a":"train = pd.read_csv(\"\/kaggle\/input\/nnfl-assignment-2\/final_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nnfl-assignment-2\/final_test.csv\")\n# train.head()\ntrain.clean_text = [textClean(i) for i in train.desc]\ntest.clean_text = [textClean(i) for i in test.desc]\nX_train = train.clean_text\ny_train = train.rating.values \nX_test = test.clean_text\n\n# sample_pred = np.zeros_like(test[\"rating\"], dtype=np.float32)","4af2c24f":"lines = list(X_train) + list(X_test)\nnew_lines = []\nfor i in lines:\n    new_lines.append(i.split(' '))","81db5a24":"X_train[0]","1f3f2a84":"tokenizer = Tokenizer(num_words=50000)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = pad_sequences(X_train, maxlen=200)\nx_test = pad_sequences(X_test, maxlen=200)","70ad67ff":"from glove import Corpus, Glove","80d6ec60":"corpus = Corpus()\n# print(x_train))\ncorpus.fit(new_lines, window=10)\n\nglove = Glove(no_components=200, learning_rate=0.05) \nglove.fit(corpus.matrix, epochs=10, no_threads=10, verbose=True)\nglove.add_dictionary(corpus.dictionary)\nglove.save('glove.model')","85d03a1e":"embeddings_index = {}\nfor i in glove.dictionary.keys():\n    embeddings_index[i] = glove.word_vectors[glove.dictionary[i]]","c2012177":"word_index = tokenizer.word_index\nnb_words = min(50000, len(word_index))\nembedding_matrix = np.zeros((nb_words, 200))\nfor word, i in word_index.items():\n    if i >= 50000: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","2072f9a1":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    \n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","e3256bf1":"inp = Input(shape=(200, ))\nx = Embedding(50000, 200, weights=[embedding_matrix],trainable = False)(inp)\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\nx = Bidirectional(CuDNNLSTM(128, return_sequences=False))(x)\n# x = Attention(100)(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(1, activation=\"relu\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss=\"mean_squared_logarithmic_error\",optimizer=\"adam\",metrics=['accuracy'])\nprint(model.summary())","aafa3329":"hist = model.fit(x_train, y_train, validation_split=0.1 ,batch_size = 2032, epochs = 10)","31012b2f":"history = pd.DataFrame(hist.history)\nplt.figure(figsize=(12,12))\nplt.plot(history[\"loss\"])\nplt.plot(history[\"val_loss\"])\nplt.title(\"Loss with pretrained word vectors\")\nplt.show()","e1595533":"pred = model.predict(x_test)","907fb60a":"# new_pred = pred*10\n# print(new_pred[:10])\nnew_pred = [max(1.0,min(float(np.round(i)),10.0)) for i in pred]\nprint(new_pred[:10])","84a34e53":"final_df = pd.DataFrame(list(zip(test_df[\"Id\"],new_pred)), columns = [\"Id\",\"Rating\"])\nfinal_df.head()","eab69205":"final_df.to_csv(\"submission_bilstm1.csv\",index=False)","55e2754e":"from IPython.display import FileLink","bda483c1":"FileLink(\"submission_bilstm1.csv\")","20d6f870":"# Training glove","8c1b4d7a":"# Tokenizer"}}