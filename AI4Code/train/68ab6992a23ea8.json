{"cell_type":{"fb1e2c49":"code","f4bbe77b":"code","d67961b7":"code","639c03bb":"code","fbffa6ca":"code","6643802c":"code","9a73b853":"code","8dcb6120":"code","c90690f1":"code","50efe0f4":"code","a883488c":"code","2d8ac555":"code","19ef2caa":"code","25796120":"code","9d12f8b1":"code","2edde19a":"code","28945878":"code","4a6c54e9":"code","0a252e2b":"markdown","b2efab19":"markdown","9f838ff4":"markdown","f0d5922c":"markdown","5fe31333":"markdown","8c097906":"markdown","5513a05e":"markdown","928a8611":"markdown","10ba0e59":"markdown","d72286ad":"markdown","c418486f":"markdown","300d8cba":"markdown","3a2753cf":"markdown","66d8dc39":"markdown"},"source":{"fb1e2c49":"!pip install efficientnet","f4bbe77b":"import math, re, os\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\n\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50, InceptionResNetV2, DenseNet201, ResNet50V2\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# import efficientnet.tfkeras as efn\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint\n\nimport efficientnet.tfkeras as efn","d67961b7":"AUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","639c03bb":"# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","fbffa6ca":"INPUT = \"\/kaggle\/input\/journey-springfield\"\nTRAIN_DIR = \"\/kaggle\/input\/journey-springfield\/train\/simpsons_dataset\"\nTEST_DIR = \"\/kaggle\/input\/journey-springfield\/testset\/testset\"\n\nEPOCHS = 25\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nIM_Z = 784","6643802c":"df = pd.read_csv(\"\/kaggle\/input\/journey-springfield\/sample_submission.csv\")\ntest_files = df[\"Id\"].values","9a73b853":"test_classes = {k:v for v, k in zip(sorted(os.listdir(TRAIN_DIR)), range(42))}\ntrain_classes = {v:k for v, k in zip(sorted(os.listdir(TRAIN_DIR)), range(42))}\n\ndirs = sorted(os.listdir(TRAIN_DIR))\ntrain_paths = np.array(([GCS_DS_PATH + \"\/train\/simpsons_dataset\/\" + i + \"\/\" +  j for i in dirs for j in os.listdir(TRAIN_DIR + \"\/\" + i)]))\ntmp_labels = np.array([train_classes[j.split(\"\/\")[-2]] for j in train_paths])\n\ntrain_labels = np.zeros((train_paths.shape[0], 42))\nfor i in range(train_paths.shape[0]):\n    train_labels[i, tmp_labels[i]] = 1\nprint(train_paths.shape)\n\ntest_paths = np.array(([GCS_DS_PATH + \"\/testset\/testset\/\" +  i for i in test_files]))\n\n# train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n#     train_paths, train_labels, test_size=0.1, random_state=2020)","8dcb6120":"train_labels = train_labels.astype(int)","c90690f1":"def decode_image(filename, label=None, image_size=(IM_Z, IM_Z)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_png(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n#     print(label)\n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n#     image = tf.image.adjust_brightness(image, delta=0.2)\n#     image = tf.image.adjust_contrast(image,2)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","50efe0f4":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\n# valid_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((valid_paths, valid_labels))\n#     .map(decode_image, num_parallel_calls=AUTO)\n#     .batch(BATCH_SIZE)\n#     .cache()\n#     .prefetch(AUTO)\n# )\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","a883488c":"LR_START = 0.00001\nLR_MAX = 0.0001 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 15\nLR_SUSTAIN_EPOCHS = 3\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\n# rng = [i for i in range(EPOCHS)]\n# y = [lrfn(x) for x in rng]\n# plt.plot(rng, y)\n# print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","2d8ac555":"with strategy.scope():\n    model_efnB7 = tf.keras.Sequential([\n            efn.EfficientNetB7(weights=\"imagenet\", include_top=False, input_shape=(IM_Z, IM_Z, 3)),\n\n            L.GlobalAveragePooling2D(),\n            L.Flatten(name=\"flatten\"),\n            L.Dense(256, activation=\"relu\"),\n            L.Dropout(0.5),\n            L.Dense(42, activation='softmax')\n    ])\n    \n    opt = Adam(lr=1e-4, decay=1e-4 \/ EPOCHS)\n\n    model_efnB7.compile(\n        optimizer=opt,\n        loss = 'categorical_crossentropy',\n        metrics=['categorical_accuracy']\n    )\n#     model.summary()","19ef2caa":"STEPS_PER_EPOCH = train_labels.shape[0] \/\/ BATCH_SIZE \n\nhistory = model_efnB7.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=[lr_callback],\n    \n#     validation_data=valid_dataset\n)","25796120":"# with strategy.scope():\n#     model_efnB6 = tf.keras.Sequential([\n#             efn.EfficientNetB6(weights=\"imagenet\", include_top=False, input_shape=(IM_Z, IM_Z, 3)),\n\n#             L.GlobalAveragePooling2D(),\n#             L.Flatten(name=\"flatten\"),\n#             L.Dense(256, activation=\"relu\"),\n#             L.Dropout(0.5),\n#             L.Dense(42, activation='softmax')\n#     ])\n    \n#     opt = Adam(lr=1e-4, decay=1e-4 \/ EPOCHS)\n\n#     model_efnB6.compile(\n#         optimizer=opt,\n#         loss = 'categorical_crossentropy',\n#         metrics=['categorical_accuracy']\n#     )\n# #     model.summary()","9d12f8b1":"# STEPS_PER_EPOCH = train_labels.shape[0] \/\/ BATCH_SIZE \n\n# history = model_efnB6.fit(\n#     train_dataset, \n#     epochs=EPOCHS, \n#     steps_per_epoch=STEPS_PER_EPOCH,\n#     callbacks=[lr_callback],\n# #     validation_data=valid_dataset\n# )","2edde19a":"# prob = (model_efnB7.predict(test_dataset, verbose=1) + model_efnB6.predict(test_dataset, verbose=1))\/2\nprob = model_efnB7.predict(test_dataset, verbose=1)","28945878":"df.Expected = np.array([test_classes[i] for i in np.argmax(prob, axis=1)])\ndf","4a6c54e9":"df.to_csv(\"submission.csv\", index=False)","0a252e2b":"## Get test image titles","b2efab19":"Train EfficientNetB7 model","9f838ff4":"# TPU","f0d5922c":"# Predict","5fe31333":"Create  EfficientNetB7 model","8c097906":"Create EfficientNetB6 model","5513a05e":"# Dataset","928a8611":"## Prepare data\n\ntest_classes - need to predict classes ({\"class_name\": 1,....\")\n\ntrain_classes - need to associate a image with classes ({1:\"class_name\",  .......})\n\ntrain_path - train image path to TPU \n\ntrain_labels - classes to TPU (one hot encoded ) \n\ntest_paths - test image path to TPU","10ba0e59":"Competition data access\n\nTPUs read data directly from Google Cloud Storage (GCS). This Kaggle utility will copy the dataset to a GCS bucket co-located with the TPU. If you have multiple datasets attached to the notebook, you can pass the name of a specific dataset to the get_gcs_path function. The name of the dataset is the name of the directory it is mounted in. Use !ls \/kaggle\/input\/ to list attached datasets.","d72286ad":"# Configuration","c418486f":"https:\/\/github.com\/qubvel\/efficientnet\n\nEfficientNets rely on AutoML and compound scaling to achieve superior performance without compromising resource efficiency. The AutoML Mobile framework has helped develop a mobile-size baseline network, EfficientNet-B0, which is then improved by the compound scaling method to obtain EfficientNet-B1 to B7.\n\n\n![](https:\/\/raw.githubusercontent.com\/tensorflow\/tpu\/master\/models\/official\/efficientnet\/g3doc\/params.png)\n\n","300d8cba":"# Models","3a2753cf":"Train EfficientNetB6 model","66d8dc39":"Labels must be integer not float"}}