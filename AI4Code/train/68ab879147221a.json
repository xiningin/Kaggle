{"cell_type":{"53abf33f":"code","8c2db917":"code","1fb0b46f":"code","7065bf18":"code","10cc139d":"code","5c86d1f0":"code","2bfa8131":"code","48e3c6c5":"code","25924eca":"code","e60996c4":"code","11e43d5f":"code","761d4376":"code","af2d1975":"code","5ed1f6c8":"code","2aaaa4c2":"code","5a01480d":"code","f60255e1":"code","7fc17ab7":"markdown","143b4558":"markdown","ac920db4":"markdown","c07cfbc8":"markdown","95a5aa52":"markdown","93569de1":"markdown","c8847914":"markdown","c47fb914":"markdown","6062064e":"markdown","df69134c":"markdown","2478e808":"markdown","76392bef":"markdown"},"source":{"53abf33f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","8c2db917":"img_train_path=os.path.abspath('..\/input\/train')\nimg_test_path=os.path.abspath('..\/input\/test')\n\ncsv_train_path=os.path.abspath('..\/input\/analytics-vidhya-loan-prediction\/train.csv')\ncsv_test_path=os.path.abspath('..\/input\/analytics-vidhya-loan-prediction\/test.csv')\ncsv_train_path\ncsv_test_path","1fb0b46f":"df_train=pd.read_csv(csv_train_path)\ndf_train.head()","7065bf18":"df_train.info()","10cc139d":"\ndf_train = df_train.drop(columns=['Loan_ID']) ## Dropping Loan ID\ncategory_col=['Gender','Married','Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\nprint(category_col)\nnumeric_col=['ApplicantIncome','CoapplicantIncome','LoanAmount']\nprint(numeric_col)\n","5c86d1f0":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfig,axes = plt.subplots(4,2,figsize=(12,15))\nfor idx,cat_col in enumerate(category_col):\n    row,col = idx\/\/2,idx%2\n    sns.countplot(x=cat_col,data=df_train,hue='Loan_Status',ax=axes[row,col])\n\n\nplt.subplots_adjust(hspace=1)","2bfa8131":"fig,axes = plt.subplots(1,3,figsize=(17,5))\nfor idx,cat_col in enumerate(numeric_col):\n    sns.boxplot(y=cat_col,data=df_train,x='Loan_Status',ax=axes[idx])\n\nprint(df_train[numeric_col].describe())\nplt.subplots_adjust(hspace=1)","48e3c6c5":"#### Encoding categrical Features: ##########\ndf_train_encoded = pd.get_dummies(df_train,drop_first=True)\ndf_train_encoded.head()","25924eca":"########## Split Features and Target Varible ############\nX = df_train_encoded.drop(columns='Loan_Status_Y')\ny = df_train_encoded['Loan_Status_Y']\n\n################# Splitting into Train -Test Data #######\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)\n############### Handling\/Imputing Missing values #############\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy='mean')\nimp_train = imp.fit(X_train)\nX_train = imp_train.transform(X_train)\nX_test_imp = imp_train.transform(X_test)","e60996c4":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score\n\n\ntree_clf = DecisionTreeClassifier()\ntree_clf.fit(X_train,y_train)\ny_pred = tree_clf.predict(X_train)\nprint(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\nprint(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","11e43d5f":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\ntree_depths = []\n\nfor depth in range(1,20):\n    tree_clf = DecisionTreeClassifier(max_depth=depth)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    tree_depths.append(depth)\n\nTuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","761d4376":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\nmin_samples_leaf = []\nimport numpy as np\nfor samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf \n    tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    min_samples_leaf.append(samples_leaf)\n    \n\nTuning_min_samples_leaf = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Min_Samples_leaf\": min_samples_leaf }\nTuning_min_samples_leaf_df = pd.DataFrame.from_dict(Tuning_min_samples_leaf)\n\nplot_df = Tuning_min_samples_leaf_df.melt('Min_Samples_leaf',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Min_Samples_leaf\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","af2d1975":"from sklearn.metrics import confusion_matrix\ntree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)\ntree_clf.fit(X_train,y_train)\ny_pred = tree_clf.predict(X_test_imp)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","5ed1f6c8":"#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict\n\ntrain_accuracies = []\ntrain_f1_scores = []\ntest_accuracies = []\ntest_f1_scores = []\nthresholds = []\nfor thresh in np.arange(0.1,0.9,0.1): ## Sweeping from threshold of 0.1 to 0.9\n    logreg_clf = LogisticRegression(solver='liblinear')\n    logreg_clf.fit(X_train,y_train)\n    \n    y_pred_train_thresh = logreg_clf.predict_proba(X_train)[:,1]\n    y_pred_train = (y_pred_train_thresh > thresh).astype(int)\n\n    train_acc = accuracy_score(y_train,y_pred_train)\n    train_f1 = f1_score(y_train,y_pred_train)\n    \n    y_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\n    y_pred_test = (y_pred_test_thresh > thresh).astype(int) \n    \n    test_acc = accuracy_score(y_test,y_pred_test)\n    test_f1 = f1_score(y_test,y_pred_test)\n    \n    train_accuracies.append(train_acc)\n    train_f1_scores.append(train_f1)\n    test_accuracies.append(test_acc)\n    test_f1_scores.append(test_f1)\n    thresholds.append(thresh)\n    \nThreshold_logreg = {\"Training Accuracy\": train_accuracies, \"Test Accuracy\": test_accuracies, \"Training F1\": train_f1_scores, \"Test F1\":test_f1_scores, \"Decision Threshold\": thresholds }\nThreshold_logreg_df = pd.DataFrame.from_dict(Threshold_logreg)\n\nplot_df = Threshold_logreg_df.melt('Decision Threshold',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Decision Threshold\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)\n","2aaaa4c2":"thresh = 0.4 ### Threshold chosen from above Curves\ny_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\ny_pred = (y_pred_test_thresh > thresh).astype(int) \nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","5a01480d":"#KNN Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier(n_neighbors=3)\nknn_clf.fit(X_train,y_train)\ny_pred = knn_clf.predict(X_train)\nprint(\"Train F1 Score \", f1_score(y_train,y_pred))\nprint(\"Train Accuracy \", accuracy_score(y_train,y_pred))\n\n","f60255e1":"#NAIVE BAYES CLASSIIFIER\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_train)\n\nprint(\"Train F1 Score \", f1_score(y_train,y_pred))\nprint(\"Train Accuracy \", accuracy_score(y_train,y_pred))","7fc17ab7":"Logistic Regression Confusion matrix is very similar to Decision Tree and Random Forest Classifier. In this analysis, we did extensive analysis of input data and were able to achieve Test Accuracy of 86 %","143b4558":"Plots above convey following things about the dataset:\nLoan Approval Status: About 2\/3rd of applicants have been granted loan.\nSex: There are more Men than Women (approx. 3x)\nMartial Status: 2\/3rd of the population in the dataset is Marred; Married applicants are more likely to be granted loans.\nDependents: Majority of the population have zero dependents and are also likely to accepted for loan.\nEducation: About 5\/6th of the population is Graduate and graduates have higher propotion of loan approval\nEmployment: 5\/6th of population is not self employed.\nProperty Area: More applicants from Semi-urban and also likely to be granted loans.\nApplicant with credit history are far more likely to be accepted.\nLoan Amount Term: Majority of the loans taken are for 360 Months (30 years).\nNow, let's also analyze Numerical Columns:","ac920db4":"**Preprocessing Data:**\nInput data needs to be pre-processed before we feed it to model. Following things need to be taken care:\n\nEncoding Categorical Features.\nImputing missing values","c07cfbc8":"Overfitting Problem\nWe can see from above metrics that Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning and see if it helps.","95a5aa52":"LOADING DATASET","93569de1":"For Numercical Columns, there is no significant relation to Loan approval status.","c8847914":"From above plot, we will choose Min_Samples_leaf to 35 to improve test accuracy.\n\nLet's use this Decision Tree classifier on unseen test data and evaluate Test Accuracy, F1 Score and Confusion Matrix","c47fb914":"DATA VISUALIZATION FOR ANALYSING DATA","6062064e":"From above graph, we can conclude that keeping 'Max_Depth' = 3 will yield optimum Test accuracy and F1 score Optimum Test Accuracy ~ 0.805; Optimum F1 Score: ~0.7","df69134c":"MODEL: Decision Tree","2478e808":"SUMMARIZING DATASET","76392bef":"Logistic Regression does slightly better than Decision Tree and Random Forest.\nBased on the above Test\/Train curves, we can keep threshold to 0.4.\nNow Finally let's look at Logistic Regression Confusion Matrix"}}