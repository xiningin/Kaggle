{"cell_type":{"adf32bbe":"code","5a8b8de6":"code","d0a1c173":"code","f4bf35f5":"code","fe4f1d35":"code","2f580d2b":"code","0e9079c1":"code","15cc2366":"code","088726a1":"code","de8d98f3":"code","1db3280b":"code","2e5337ed":"code","6351795d":"code","01e887bb":"code","1cfa7172":"code","2996c455":"code","275e5a0f":"code","14fb244c":"code","16d52b15":"code","071acb9b":"code","0f1dbba5":"code","2145b905":"code","6364c3fc":"code","84db8df0":"code","02520fd4":"code","34202936":"code","7320b4c6":"code","d5f8d346":"code","23066a28":"code","e3c14340":"code","2ee964e3":"code","cebee965":"code","88c09a3c":"code","7b25628f":"code","3886f1de":"code","cb46adf3":"code","e52b12df":"code","504eee21":"code","9aa59ec4":"code","3bd28c79":"code","30e85124":"code","4c83f451":"code","a5ca458d":"code","1dde28eb":"code","6189db58":"code","ac6b021f":"code","e377c514":"code","c5c62e01":"code","f80e56c4":"code","4b787dca":"code","37113ca4":"code","b7a969f0":"code","0597f999":"code","6a35fb6b":"code","556808a8":"code","e17b44b7":"code","268d4b1e":"code","f06d484d":"code","6452340e":"code","250448dd":"code","fb50ee5e":"code","50a11dbc":"code","ceec386d":"code","2dfdfe33":"code","be7bf68d":"code","2b764e83":"code","d33e0699":"code","7aa84ddf":"code","10eb63f8":"code","30adfba6":"code","b67f0997":"code","bfdda14a":"code","152c5137":"code","bdeef51b":"code","685f30c7":"code","9d7006c4":"code","dea90b40":"code","5c0a41f2":"code","caaf369b":"code","aabc656d":"code","f7583675":"code","1ef89fe5":"code","f81c6396":"code","b7048e88":"code","c6dc2464":"code","1f2bd155":"code","61a6b7d3":"code","2e6eea1c":"code","acf6529f":"code","4a06293f":"code","a931308d":"code","b216a079":"code","eadb2b49":"code","c6888907":"code","c159cb2c":"code","3ee56614":"code","c6693350":"code","b600440f":"code","f9de3904":"code","db8a8bbf":"code","611f1d37":"code","12e54630":"code","f9e8d51f":"code","df5d2900":"code","bd6cae96":"code","4b472d4f":"code","e43ad020":"code","9f76dde2":"code","aa0bc00e":"code","67c0d349":"code","13a2fa96":"code","21553145":"code","3a8f6a5b":"code","43952402":"code","535de1a8":"code","5138406e":"code","d4ec61f9":"code","8ac68636":"code","13c9a469":"code","db4b5987":"code","03d189d0":"code","83b2fc05":"markdown"},"source":{"adf32bbe":"# Widen the display of python output\n# This is done to avoid ellipsis appearing which restricts output view in row or column\nimport pandas as pd\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","5a8b8de6":"# Importing Signals Data file\n# This is same as the signals dataset provided\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\ndata = pd.read_csv('..\/input\/uci-semcom\/uci-secom.csv')\ndataset = pd.DataFrame(data)\ndataset.head()","d0a1c173":"#Dataset has 1,567 rows and 592 columns\ndataset.shape","f4bf35f5":"#Add a prefix to the column names for easeof understanding\ndataset.columns = 'feature_' + dataset.columns","fe4f1d35":"#Rename the time column and Pass_Fail column as they are not features \ndataset.rename(columns = {'feature_Time':'Time'}, inplace = True) \ndataset.rename(columns = {'feature_Pass\/Fail':'Pass_Fail'}, inplace = True)","2f580d2b":"#All variables except Pass\/Fail (Wether the process entity passed or not) is float. \n#Pass\/Fail is a integer variable\ndataset.dtypes","0e9079c1":"#5 number summary of the columns \n#The counts are varying for each feature so there might be issues with some of the values like missing values\n#Few variables like feature_586 etc have negative values as well\n#There also appears to be several features with outliers as there is significant \n#difference between 75% and max value\ndataset.describe().transpose()","15cc2366":"#Check for null values - Most of the features are having null values\ndataset.isnull().sum()","088726a1":"#Check for NA values - Most of the features are having NA values\ndataset.isna().sum()","de8d98f3":"#fill NA with zero of each column in signal dataset for missing value imputation\n#this shows that we did not have any signal from that feature\ndf = dataset.iloc[:,1:]\ndf = df.apply(lambda x: x.fillna(0),axis=0)","1db3280b":"df2 = dataset.iloc[:,0]\nresult = pd.concat([df, df2], axis=1).reindex(df.index)","2e5337ed":"#Some features seem to have the same values throughout for example feature_5\n#We would need to drop such features and those which are highly correlated to each other in future steps\nresult.head()","6351795d":"#We can see that all the NA values are removed now from the dataframe\nresult.isna().sum()","01e887bb":"#We can see that all the null values are also removed now from the dataframe\nresult.isnull().sum()","1cfa7172":"#The classes for Pass_Fail variable are not balanced - 93.4% observations are passed  \n#while 6.6% people are failed. \n#So if an alogrithm just assigns value as -1 to all observations, it will still achieve 93.4% accuracy, \n#So our selected model should have better accuracy than 93.4% to be called a good model \nresult[\"Pass_Fail\"].value_counts(normalize=True)","2996c455":"#The bar plot below also shows us that the classes are not balanced\nresult[\"Pass_Fail\"].value_counts().plot(kind=\"bar\");","275e5a0f":"# Get the correlation matrix\ncorr = result.corr()\n#sns.heatmap(corr,annot=True);\n#mask = np.zeros_like(corr)\n#mask[np.triu_indices_from(mask)] = True\n#with sns.axes_style(\"white\"):\n#    f, ax = plt.subplots(figsize=(20, 20))\n#    ax = sns.heatmap(corr, mask=mask, vmax=.3, annot=True, square=True);","14fb244c":"print(corr)","16d52b15":"#Since the correlation is very big to view here so have exported as csv file\ncorr.to_csv(\"correlation.csv\")","071acb9b":"#Based on the correlation excel, a lot of columns are having same value through and no variance\n#These are shown as blank values in the correlation excel\n#Removing such columns from the dataframe below - About 116 columns are blank\nresult.drop(['feature_5','feature_13','feature_42','feature_49','feature_52','feature_69','feature_97','feature_141',\n             'feature_149','feature_178','feature_179','feature_186','feature_189','feature_190','feature_191','feature_192',\n             'feature_193','feature_194','feature_226','feature_229','feature_230','feature_231','feature_232','feature_233',\n             'feature_234','feature_235','feature_236','feature_237','feature_240','feature_241','feature_242','feature_243',\n             'feature_256','feature_257','feature_258','feature_259','feature_260','feature_261','feature_262','feature_263',\n             'feature_264','feature_265','feature_266','feature_276','feature_284','feature_313','feature_314','feature_315',\n             'feature_322','feature_325','feature_326','feature_327','feature_328','feature_329','feature_330','feature_364',\n             'feature_369','feature_370','feature_371','feature_372','feature_373','feature_374','feature_375','feature_378',\n             'feature_379','feature_380','feature_381','feature_394','feature_395','feature_396','feature_397','feature_398',\n             'feature_399','feature_400','feature_401','feature_402','feature_403','feature_404','feature_414','feature_422',\n             'feature_449','feature_450','feature_451','feature_458','feature_461','feature_462','feature_463','feature_464',\n             'feature_465','feature_466','feature_481','feature_498','feature_501','feature_502','feature_503','feature_504',\n             'feature_505','feature_506','feature_507','feature_508','feature_509','feature_512','feature_513','feature_514',\n             'feature_515','feature_528','feature_529','feature_530','feature_531','feature_532','feature_533','feature_534',\n             'feature_535','feature_536','feature_537','feature_538'],axis=1,inplace=True)","0f1dbba5":"#Remove the highly collinear features from results dataframe\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model \n        to generalize and improves the interpretability of the model.\n\n    Inputs: \n        x: features dataframe\n        threshold: features with correlations greater than this value are removed\n\n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n\n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns=drops)\n\n    return x","2145b905":"#Remove columns having more than 70% correlation\n#Both positive and negative correlations are considered here\nresult = remove_collinear_features(result,0.70)","6364c3fc":"result.head()","84db8df0":"#After dropping the highly correlated variables we have 197 columns and 1,567 rows\nresult.shape","02520fd4":"#Observation is that most of the variables distribution are right skewed with long tails and outliers \n#\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    fig=plt.figure(figsize=(20,10))\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\n\n#test = pd.DataFrame(np.random.randn(30, 9), columns=map(str, range(9)))\n#Most the variables are approximately normally distributed except for feature_4, feature_7,\n#feature_11, feature_12, feature_15, feature_16\ndraw_histograms(result, result.iloc[:,0:15], 5, 3)","34202936":"result.Time.dtype","7320b4c6":"from datetime import datetime\nresult['year'] = pd.DatetimeIndex(result['Time']).year\nresult['month'] = pd.DatetimeIndex(result['Time']).month\nresult['date'] = pd.DatetimeIndex(result['Time']).day\nresult['week_day'] = pd.DatetimeIndex(result['Time']).weekday\nresult['start_time'] = pd.DatetimeIndex(result['Time']).time\nresult['hour'] = pd.DatetimeIndex(result['Time']).hour\nresult['min'] = pd.DatetimeIndex(result['Time']).minute","d5f8d346":"result.head()","23066a28":"#This consists of only year 2008\nresult.year.unique()","e3c14340":"#This consists of all the months of 2008\nresult.month.unique()","2ee964e3":"#All the dates of the month are not there, might be related to production on certain days only\nresult.date.unique()","cebee965":"#All the weekdays of the month are here, so production happens on all 7 days\n#0 stand for Sunday, 1 for Monday ... 6 for Saturday\nresult.week_day.unique()","88c09a3c":"#We see that the failures (Pass_Fail=1) peak in August which is also the peak for pass.\n#August and September are months with most product and most failures as well\n#The failures seem to subside from September onwards post some correction \n#(May-Aug we see more failures than passes)\nsns.distplot( result[result.Pass_Fail == -1]['month'], color = 'g');\nsns.distplot( result[result.Pass_Fail == 1]['month'], color = 'r');","7b25628f":"#The failures tend to decrease towards month end and is in close sync with pass population\nsns.distplot( result[result.Pass_Fail == -1]['date'], color = 'g');\nsns.distplot( result[result.Pass_Fail == 1]['date'], color = 'r');","3886f1de":"#Failures appear to be more towards start and end of the week rather than in the middle of the week\nsns.distplot( result[result.Pass_Fail == -1]['week_day'], color = 'g');\nsns.distplot( result[result.Pass_Fail == 1]['week_day'], color = 'r');","cb46adf3":"#There is no specific trend in terms of hours, it seems to be fairly distributed\nsns.distplot( result[result.Pass_Fail == -1]['hour'], color = 'g');\nsns.distplot( result[result.Pass_Fail == 1]['hour'], color = 'r');","e52b12df":"#There is no specific trend in terms of minutes, it seems to be fairly distributed\nsns.distplot( result[result.Pass_Fail == -1]['min'], color = 'g');\nsns.distplot( result[result.Pass_Fail == 1]['min'], color = 'r');","504eee21":"#Pairplot of the dataset without name and status columns\n#Most of the independent variables have a positive skew\n#sns.pairplot(dataset3);","9aa59ec4":"# Create a boxplot for all the features by target (Pass_Fail) column\n# One common observation is that almost all have outliers, so outlier removal\/correction \n# will be required in future steps\n# we see certain features with very less observations like feature 4, 8, 9, 10, 11\nresult.boxplot(column = ['feature_0',\n'feature_1',\n'feature_2',\n'feature_3',\n'feature_4',\n'feature_8',\n'feature_9',\n'feature_10',\n'feature_11'\n], by='Pass_Fail', figsize = (20,20));\n","3bd28c79":"#There appears to be very low variation in the features, so we can drop such features\nresult[\"feature_11\"].unique()","30e85124":"result_num = result.drop(['Pass_Fail','Time','start_time'],axis=1)","4c83f451":"#Contains all the features\nresult_num.head()","a5ca458d":"#Drop columns with very low standard deviation thresholds \nthreshold = 0.2\nresult_num.drop(result_num.std()[result_num.std() < threshold].index.values, axis=1)","1dde28eb":"result_num.shape","6189db58":"#The biggest challenge in the dataset seems to be presence of outliers in almost all variables\n#Another challenge is that except few variables there is not very good seperation between observations \n#having failure and those that have passed\n#There is also the challenge of domain knowledge as none of the features are named, so we are not able\n#to apply any intuitive understanding","ac6b021f":"#Create a copy of the dataset for maintain data after outlier removal\n#Here after identifying outliers we replace with median\npd_data = result_num.copy()\n#pd_data.head()\n\n#pd_data2 = pd_data.drop(columns=['name'],axis=1)\n#pd_data2 = pd_data2.apply(replace,axis=1)\nfrom scipy import stats\n\n#Define a function to remove outliers on max side\ndef outlier_removal_max(var):\n    var = np.where(var > var.quantile(0.75)+ stats.iqr(var),var.quantile(0.50),var)\n    return var\n\n#Define a function to remove outliers on min side\ndef outlier_removal_min(var):\n    var = np.where(var < var.quantile(0.25) - stats.iqr(var),var.quantile(0.50),var)\n    return var\n\n#Loop over the columns and remove the outliers on min and max side\nfor column in pd_data:\n    pd_data[column] = outlier_removal_max(pd_data[column])\n    pd_data[column] = outlier_removal_min(pd_data[column])\n    ","e377c514":"pd_data2 = pd_data.copy()","c5c62e01":"pd_data2[\"Pass_Fail\"] = result[\"Pass_Fail\"]","f80e56c4":"#Plotting sample boxplot to check if outliers are removed or not\n#They are removed from the boxplots so we can now go for PCA\npd_data2.boxplot(column = ['feature_64',\n'feature_67',\n'feature_71',\n'feature_72',\n'feature_74',\n'feature_75',\n'feature_76',\n'feature_77',\n'feature_78',\n'feature_79',\n'feature_80'\n], by='Pass_Fail', figsize = (20,10));","4b787dca":"#Drop columns with very low standard deviation thresholds \nthreshold = 0.2\npd_data.drop(pd_data.std()[pd_data.std() < threshold].index.values, axis=1)","37113ca4":"pd_data.shape","b7a969f0":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","0597f999":"scaler.fit(pd_data)","6a35fb6b":"#pd_data_scaled = scaler.transform(pd_data)\npd_data_scaled = pd_data.copy()","556808a8":"pd_data_scaled[pd_data_scaled.columns] = scaler.fit_transform(pd_data[pd_data.columns])","e17b44b7":"pd_data_scaled.head()","268d4b1e":"# PCA\n# Step 1 - Create covariance matrix\n\ncov_matrix = np.cov(pd_data_scaled.T)\nprint('Covariance Matrix \\n%s', cov_matrix)","f06d484d":"# Step 2- Get eigen values and eigen vector\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eig_vecs)\nprint('\\n Eigen Values \\n%s', eig_vals)","6452340e":"tot = sum(eig_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var_exp)","250448dd":"plt.plot(var_exp)","fb50ee5e":"# Ploting we see that PCA is not giving us much benefit\nplt.figure(figsize=(10 , 5))\nplt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","50a11dbc":"#result_z.describe().transpose()","ceec386d":"#Let us scale the data before plotting histogram or boxplot\n#This will help us visualize better since there are more than 200 variables\n#from scipy.stats import zscore\n#result2 = result.drop(\"Time\",axis=1)\n#result3 = result2.drop(\"Pass_Fail\",axis=1)\n#result_z = pd_data.apply(zscore)\n#result_z = pd.DataFrame(result_z , columns  = result_z.columns)\n#result_z.describe().transpose()","2dfdfe33":"result_z2 = pd_data_scaled","be7bf68d":"#Copy over the target column to the scaled datasets\nresult_z2[\"Pass_Fail\"] = result[\"Pass_Fail\"]","2b764e83":"#Check the shape of result_z dataset\n#It has 1,567 rows and 202 columns\nresult_z2.shape","d33e0699":"result_z2.head()","7aa84ddf":"%matplotlib inline\n\n\n# Numerical libraries\nimport numpy as np   \n\n# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import r2_score\n\n# to handle data in form of rows and columns \nimport pandas as pd    \n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\n#importing seaborn for statistical plots\nimport seaborn as sns","10eb63f8":"#result_z2.dropyear \tmonth \tdate \tweek_day \thour","30adfba6":"# separating the dependent and independent data\n\nx = result_z2.iloc[:,:201]\ny = result_z2[\"Pass_Fail\"]\n\n# getting the shapes of new data sets x and y\nprint(\"shape of x:\", x.shape)\nprint(\"shape of y:\", y.shape)","b67f0997":"def makeOverSamplesADASYN(X,y):\n #input DataFrame\n #X \u2192Independent Variable in DataFrame\\\n #y \u2192dependent Variable in Pandas DataFrame format\n from imblearn.over_sampling import ADASYN \n sm = ADASYN()\n X, y = sm.fit_sample(X, y)\n return(X,y)","bfdda14a":"x_samp, y_samp = makeOverSamplesADASYN(x, y)","152c5137":"y_samp.head().unique()","bdeef51b":"# splitting them into train test and split\n# 70% data is for training and 30% is for test\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_samp, y_samp, test_size = 0.3, random_state = 0)\n\n# getting the shapes - 70:30 split\nprint(\"shape of x_train: \", x_train.shape)\nprint(\"shape of x_test: \", x_test.shape)\nprint(\"shape of y_train: \", y_train.shape)\nprint(\"shape of y_test: \", y_test.shape)","685f30c7":"y_train.head()","9d7006c4":"x_train.head()","dea90b40":"lasso = Lasso(alpha=0.1)\nlasso.fit(x_train,y_train)\nprint (\"Lasso model:\", (lasso.coef_))","5c0a41f2":"#Finding optimal no. of clusters\nfrom scipy.spatial.distance import cdist\nclusters=range(1,10)\nmeanDistortions=[]\n\nfor k in clusters:\n    model=KMeans(n_clusters=k)\n    model.fit(result_z2)\n    prediction=model.predict(techSuppScaled)\n    meanDistortions.append(sum(np.min(cdist(techSuppScaled, model.cluster_centers_, 'euclidean'), axis=1))\n                           \/ techSuppScaled.shape[0])\n\n\nplt.plot(clusters, meanDistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average distortion')\nplt.title('Selecting k with the Elbow Method')","caaf369b":"# splitting them into train test and split\n# 70% data is for training and 30% is for test\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n\n# getting the shapes - 70:30 split\nprint(\"shape of x_train: \", x_train.shape)\nprint(\"shape of x_test: \", x_test.shape)\nprint(\"shape of y_train: \", y_train.shape)\nprint(\"shape of y_test: \", y_test.shape)","aabc656d":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n# Return the model statistics\ndef fit_n_print(model, X_train, X_test, y_train, y_test):  # take the model, and data as inputs\n    from sklearn import metrics\n    from sklearn.model_selection import cross_val_score\n    \n    model.fit(X_train, y_train)   # fit the model with the train data\n\n    pred = model.predict(X_test)  # make predictions on the test set\n\n    score = round(model.score(X_test, y_test), 3)   # compute accuracy score for test set\n    mae = mean_absolute_error(y_test, pred)\n    mse = mean_squared_error(y_test, pred)\n    r2 = r2_score(y_test, pred)\n   \n    return score, mae, mse, r2  # return all the metrics\n","f7583675":"#Function to display confusion matrix\ndef disp_confusion_matrix(model_name, model, X_test, y_test):\n    from sklearn.metrics import confusion_matrix\n    y_pred = model.predict(X_test)\n    conf_mat = confusion_matrix(y_test, y_pred)\n    df_conf_mat = pd.DataFrame(conf_mat)\n    #ax = plt.axes()\n    #plt.title()\n    plt.figure(figsize = (10,7))\n    plt.suptitle(\"Confusion matrix: \"+model_name)\n    sns.heatmap(df_conf_mat, annot=True,cmap='Blues', fmt='g')\n    #ax.set_title()\n    #plt.show();","1ef89fe5":"# Function to display roc curve and auc\ndef disp_roc_curve(model_name, model, X_test, y_test):    \n    from sklearn.metrics import roc_curve\n    from sklearn.metrics import roc_auc_score\n    from matplotlib import pyplot\n    # generate a no skill prediction (majority class)\n    ns_probs = [0 for _ in range(len(y_test))]\n    # predict probabilities\n    lr_probs = model.predict_proba(X_test)\n    #lr_probs = model.predict(X_test)\n    # keep probabilities for the positive outcome only\n    lr_probs = lr_probs[:, 1]\n    # calculate scores\n    ns_auc = roc_auc_score(y_test, ns_probs)\n    lr_auc = roc_auc_score(y_test, lr_probs)\n    # summarize scores\n    #print('Random: ROC AUC=%.3f' % (ns_auc))\n    print(model_name + ': ROC AUC=%.3f' % (lr_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n    # plot the roc curve for the model\n    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='Random')\n    pyplot.plot(lr_fpr, lr_tpr, marker='.', label=model_name)\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    # show the legend\n    pyplot.legend()\n    # show the plot\n    pyplot.show()","f81c6396":"#Install XGBoost if not installed\n!pip install xgboost","b7048e88":"from xgboost import XGBClassifier","c6dc2464":"#Define different classifiers including Logistic Regression, Random Forest an XG Boost\n#We have created a pipeline to do PCA first and then do the modeling part\n\n#from sklearn.calibration import CalibratedClassifierCV\n#from sklearn.linear_model import LogisticRegression\n#lr = LogisticRegression(random_state=0)\n#lr_model = CalibratedClassifierCV(lr) \n\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.decomposition import PCA \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline \n\n\npipe_lr = Pipeline([('pca', PCA(n_components=10)), ('lr', LogisticRegression(random_state=1))]) \npipe_lr.fit(x_train, y_train) \nprint('Test Accuracy - Logistic Regression: %.4f' % pipe_lr.score(x_test, y_test))\n\npipe_rf = Pipeline([('pca', PCA(n_components=10)), ('rf', RandomForestClassifier(n_estimators=50,\n                                                                                random_state=1))]) \npipe_rf.fit(x_train, y_train) \nprint('Test Accuracy - Random Forest: %.4f' % pipe_rf.score(x_test, y_test)) \n\npipe_xgb = Pipeline([('pca', PCA(n_components=10)), ('xg',XGBClassifier(random_state=1))]) \npipe_xgb.fit(x_train, y_train) \nprint('Test Accuracy - XG Boost: %.4f' % pipe_xgb.score(x_test, y_test)) \n\n#from sklearn.ensemble import StackingClassifier\n#estimators = [('dt', dt),('rf', rf),('bg', bg), ('gb', gb), ('ab', ab)]\n#estimators = [('lr', lr_model),('rf', rf_model),('xgb', xgb_model)]\n\n#reg = StackingClassifier(estimators=estimators)","1f2bd155":"#This javascript code disables autoscroll","61a6b7d3":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","2e6eea1c":"#We can see that even though Logistic and Random Forest have more accuracy, \n#they have not classified any observation in failure class correctly\n#XGBoost though slightly low accuracy has classified 1 observations in the failure class correctly\nfor model, model_name in zip([pipe_lr,pipe_rf, pipe_xgb], ['Logistic Regression','Random Forest', \n                                                      'XG Boost']):\n    disp_confusion_matrix(model_name, model, x_test, y_test);","acf6529f":"#Install imbalanced library if not installed\n!pip install -U imbalanced-learn","4a06293f":"#!pip install scikit-learn==0.23.1","a931308d":"from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler","b216a079":"# Count of records before oversampling\nprint(\"Before Upsampling, count of pass '-1':{}\".format(sum(y_train==-1)))\nprint(\"Before Upsampling, count of pass '1':{}\".format(sum(y_train==1)))","eadb2b49":"def makeOverSamplesSMOTE(X,y):\n #input DataFrame\n #X \u2192Independent Variable in DataFrame\\\n #y \u2192dependent Variable in Pandas DataFrame format\n from imblearn.over_sampling import SMOTE\n sm = SMOTE()\n X, y = sm.fit_sample(X, y)\n return X,y","c6888907":"def makeOverSamplesADASYN(X,y):\n #input DataFrame\n #X \u2192Independent Variable in DataFrame\\\n #y \u2192dependent Variable in Pandas DataFrame format\n from imblearn.over_sampling import ADASYN \n sm = ADASYN()\n X, y = sm.fit_sample(X, y)\n return(X,y)","c159cb2c":"#Use the SMOTE technique to oversample\nsm = SMOTE(sampling_strategy=1,k_neighbors=5,random_state = 1)","3ee56614":"sm_x_train,sm_y_train = sm.fit_sample(x_train, y_train)","c6693350":"# Count of records after oversampling\nprint(\"After Upsampling,counts of label '-1':{}\".format(sum(sm_y_train==-1)))\nprint(\"After Upsampling,counts of label '1':{}\".format(sum(sm_y_train==1)))","b600440f":"pipe_lr2 = Pipeline([('pca', PCA(n_components=10)), ('lr', LogisticRegression(random_state=1))]) \npipe_lr2.fit(sm_x_train, sm_y_train) \nprint('Test Accuracy - Logistic Regression: %.4f' % pipe_lr2.score(x_test, y_test))\n\npipe_rf2 = Pipeline([('pca', PCA(n_components=10)), ('rf', RandomForestClassifier(n_estimators=50,\n                                                                                random_state=1))]) \npipe_rf2.fit(sm_x_train, sm_y_train) \nprint('Test Accuracy - Random Forest: %.4f' % pipe_rf2.score(x_test, y_test)) \n\npipe_xgb2 = Pipeline([('pca', PCA(n_components=10)), ('xg',XGBClassifier(random_state=1))]) \npipe_xgb2.fit(sm_x_train, sm_y_train) \nprint('Test Accuracy - XG Boost: %.4f' % pipe_xgb2.score(x_test, y_test)) \n","f9de3904":"#We can see that Logistic Regression has accuracy of 73%,\n#While random forest and XG Boost has much better accuracy of 87% and 86% \n#Basis on the confusion matrix we see that more observations are classified under 1 now compared to earlier\n#13, 6 and 8 observation for logistic, random forest and XG boost, so Logistic is better even though \n#accuracy is lower\nfor model, model_name in zip([pipe_lr2,pipe_rf2, pipe_xgb2], ['Logistic Regression','Random Forest', \n                                                      'XG Boost']):\n    disp_confusion_matrix(model_name, model, x_test, y_test);","db8a8bbf":"#Using Cluster centroids method to undersample\nfrom collections import Counter\nfrom imblearn.under_sampling import ClusterCentroids \nprint('Original dataset shape {}'.format(Counter(y_train)))\ncc = ClusterCentroids(random_state=1)\nx_res, y_res = cc.fit_sample(x_train, y_train)","611f1d37":"# Count of records after downsampling\nprint(\"After Downsampling,counts of label '-1':{}\".format(sum(y_res==-1)))\nprint(\"After Downsampling,counts of label '1':{}\".format(sum(y_res==1)))","12e54630":"pipe_lr3 = Pipeline([('pca', PCA(n_components=10)), ('lr', LogisticRegression(random_state=1))]) \npipe_lr3.fit(x_res, y_res) \nprint('Test Accuracy - Logistic Regression: %.4f' % pipe_lr3.score(x_test, y_test))\n\npipe_rf3 = Pipeline([('pca', PCA(n_components=10)), ('rf', RandomForestClassifier(n_estimators=50,\n                                                                                random_state=1))]) \npipe_rf3.fit(x_res, y_res) \nprint('Test Accuracy - Random Forest: %.4f' % pipe_rf3.score(x_test, y_test)) \n\npipe_xgb3 = Pipeline([('pca', PCA(n_components=10)), ('xg',XGBClassifier(random_state=1))]) \npipe_xgb3.fit(x_res, y_res) \nprint('Test Accuracy - XG Boost: %.4f' % pipe_xgb3.score(x_test, y_test)) ","f9e8d51f":"#We can see that Logistic Regression has accuracy of 33%,\n#While random forest and XG Boost has much better accuracy of 44% and 49% \n#Basis on the confusion matrix we see that more observations are classified under 1 now compared to upsampling\n#20, 17 and 17 observation for logistic, random forest and XG boost, so Logistic is better even though \n#accuracy is lower\nfor model, model_name in zip([pipe_lr3,pipe_rf3, pipe_xgb3], ['Logistic Regression','Random Forest', \n                                                      'XG Boost']):\n    disp_confusion_matrix(model_name, model, x_test, y_test);","df5d2900":"#Try using KFold cross validation with Upsampling as we have more accuracy there\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import metrics\n\nnum_folds = 50\nseed = 1\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\n#model = LogisticRegression()\n\npipe_lr2 = Pipeline([('pca', PCA(n_components=10)), ('lr', LogisticRegression(random_state=1))]) \npipe_lr2.fit(sm_x_train, sm_y_train) \nprint('Test Accuracy - Logistic Regression: %.4f' % pipe_lr2.score(x_test, y_test))\nresults = cross_val_score(pipe_lr2, x, y, cv=kfold)\n#print(results)\n#pred = cross_val_predict(pipe_lr2,sm_x_train, sm_y_train, cv=kfold)\nprint(\"CV Accuracy - Logistic Regression: %.4f (%.4f)\" % (results.mean(), results.std()))\n\n\npipe_rf2 = Pipeline([('pca', PCA(n_components=10)), ('rf', RandomForestClassifier(n_estimators=50,\n                                                                                random_state=1))]) \npipe_rf2.fit(sm_x_train, sm_y_train) \nprint('Test Accuracy - Random Forest: %.4f' % pipe_rf2.score(x_test, y_test)) \nresults = cross_val_score(pipe_rf2, x, y, cv=kfold)\n#print(results)\n#pred = cross_val_predict(pipe_lr2,sm_x_train, sm_y_train, cv=kfold)\nprint(\"CV Accuracy - Random Forest: %.4f (%.4f)\" % (results.mean(), results.std()))\n\n\npipe_xgb2 = Pipeline([('pca', PCA(n_components=10)), ('xg',XGBClassifier(random_state=1))]) \npipe_xgb2.fit(sm_x_train, sm_y_train) \nprint('Test Accuracy - XG Boost: %.4f' % pipe_xgb2.score(x_test, y_test)) \nresults = cross_val_score(pipe_xgb2, x, y, cv=kfold)\n#print(results)\n#pred = cross_val_predict(pipe_lr2,sm_x_train, sm_y_train, cv=kfold)\nprint(\"CV Accuracy - XG Boost: %.4f (%.4f)\" % (results.mean(), results.std()))","bd6cae96":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom time import time\n\n#disp_confusion_matrix(model_name, model, x_test, y_test);","4b472d4f":"#Except XG Boost none of the models predicted a single observation for failure \nfor model, model_name in zip([pipe_lr2,pipe_rf2, pipe_xgb2], ['Logistic Regression','Random Forest', \n                                                      'XG Boost']):\n    y_pred = cross_val_predict(model, x, y, cv=kfold)\n    conf_mat = confusion_matrix(y, y_pred)\n    print(conf_mat)","e43ad020":"#Using Grid Search to search the hyper parameter space\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\nrf = RandomForestClassifier(n_estimators=50,random_state=1)                                                                             \ngrid_search = GridSearchCV(rf, param_grid=param_grid)\nstart = time()\ngrid_search.fit(sm_x_train, sm_y_train)","9f76dde2":"#Get the best parameters for Random Forest\ngrid_search.best_params_","aa0bc00e":"#Mean Test Scores across the models\ngrid_search.cv_results_['mean_test_score']","67c0d349":"#Best Model Parameters\ngrid_search.best_estimator_","13a2fa96":"#Construct RandomForest with the best model parameters\npipe_rf2 = Pipeline([('pca', PCA(n_components=10)), ('rf', RandomForestClassifier(bootstrap=False, max_features=3, min_samples_split=3,\n                       n_estimators=50, random_state=1))]) \npipe_rf2.fit(sm_x_train, sm_y_train) \nprint('Test Accuracy - Random Forest: %.4f' % pipe_rf2.score(x_test, y_test))","21553145":"#Able to classify 6 observations correctly\ndisp_confusion_matrix('Random Forest - Grid Search', pipe_rf2, x_test, y_test);","3a8f6a5b":"!pip install rfpimp","43952402":"#Identify which features are best in Random Forest Classifier\nfrom sklearn.metrics import r2_score\nfrom rfpimp import permutation_importances\n\ndef r2(rf, X_train, y_train):\n    return r2_score(y_train, rf.predict(X_train))\n\nperm_imp_rfpimp = permutation_importances(pipe_rf2, sm_x_train, sm_y_train, r2)","535de1a8":"#Get the feature importance\nperm_imp_rfpimp.Importance.plot(kind=\"bar\",figsize=(50,20))","5138406e":"#Feature Importance is as below\nperm_imp_rfpimp","d4ec61f9":"#Try OneClassSVM with oversampled train data with SMOTE\nfrom sklearn.svm import OneClassSVM\n\nmodel = OneClassSVM(kernel ='rbf', degree=3, gamma=0.1,nu=0.005, max_iter=-1)\n\nmodel.fit(sm_x_train, sm_y_train)\ny_pred = model.fit_predict(x_test)\naccuracy = (len(y_pred[y_pred == -1])\/len(y_pred))\nprint('Test Accuracy - OneClassSVM (Oversampled): %.4f' % accuracy)\n#print(len(y_pred[y_pred == -1]))\n#print(len(y_pred))\n#print(accuracy)","8ac68636":"# evaluating the model\n# printing the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm ,annot = True, cmap = 'summer')","13c9a469":"#Try OneClassSVM with undersampled train data\nfrom sklearn.svm import OneClassSVM\n\nmodel = OneClassSVM(kernel ='rbf', degree=3, gamma=0.1,nu=0.005, max_iter=-1)\n\nmodel.fit(x_res, y_res)\ny_pred = model.fit_predict(x_test)\naccuracy = (len(y_pred[y_pred == -1])\/len(y_pred))\nprint('Test Accuracy - OneClassSVM (Undersampled): %.4f' % accuracy)","db4b5987":"# evaluating the model\n# printing the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm ,annot = True, cmap = 'summer')","03d189d0":"#We have tried Logistic Regression, Random Forest and XG Boost algorithm for the imbalanced classes\n#Across methods Logisitc Regression performed the worst while Random Forest and XG Boost performed similarly\n#However on the good side Logistic was able to classify more observationsin failure compared to other two\n#algorithms.We have tried two sampling techniques -first one using SMOTE (oversampling) and second one \n#using centroid based method (undersampling), Oversampling gave better results than undersampling in \n#terms of accuracy. However undersampling classified more observations in minority class than oversampling\n#We did Z score scaling on both the datasets and took PCA with n_components as 10\n#We tried K-fold cross validation which helped improve the results a fair bit to about 93% accuracy\n#However it continues misclassifying the minority class\n#We used Grid search for hyper parameter tuning as well for random forest and checked results with 89% accuracy\n#Using feature importance, we found that feature_64, feature_55 and feature_45 are the top three important \n#features. Lastly we tried OneClassSVM as well on the undersampled and oversampled data with similar accuracy\n#of about 84%. However we were not able to achieve accuracy more than 93.4%, if we tried to improve the \n#classifier on the failure observations","83b2fc05":"Domain Semiconductor manufacturing process  \n\nBusiness Context  \nA complex modern semiconductor manufacturing process is normally under constant surveillance via the monitoring of signals\/variables collected from sensors and or process measurement points. However, not all of these signals are equally valuable in a specific monitoring system.  The measured signals contain a combination of useful information, irrelevant information as well as noise. Engineers typically have a much larger number of signals than are actually required. If we consider each type of signal as a feature, then feature selection may be applied to identify the most relevant signals. The Process Engineers may then use these signals to determine key factors contributing to yield excursions downstream in the process. This will enable an increase in process throughput, decreased time to learning and reduce the per unit production costs. These signals can be used as features to predict the yield type. And by analyzing and trying out different combinations of features, essential signals that are impacting the yield type can be identified.     \n\nObjective  \nWe will build a classifier to predict the Pass\/Fail yield of a particular process entity and analyze whether all the features are required to build the model or not.   \nDataset description  \n   \nProprietary content. \u00a9 Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited.                                                                        2  \nsensor-data.csv : (1567, 592)  \nThe data consists of 1567 examples each with 591 features.  The dataset presented in this case represents a selection of such features where each example represents a single production entity with associated measured features and the labels represent a simple pass\/fail yield for in house line testing. Target column \u201c \u20131\u201d corresponds to a pass and \u201c1\u201d corresponds to a fail and the data time stamp is for that specific test point.    \nSteps   \n1. Import the necessary liberraries and read the provided CSV as a dataframe and perform the below steps. ( 5 points)  a. Check a few observations and shape of the dataframe b. Check for missing values. Impute the missing values if there is any c. Univariate analysis - check frequency count of target column and distribution of the first few features (sensors) d. Perform bivariate analysis and check for the correlation  e. Drop irrelevant columns   \n2. Standardize the data ( 3 points)  3. Segregate the dependent column (\"Pass\/Fail\") from the data frame. And split the dataset into training and testing set ( 70:30 split) ( 2 points) 4. Build a logistic regression, random forest, and xgboost classifier model and print confusion matrix for the test data ( 10 points)  5. Apply sampling techniques to handle the imbalanced classes ( 5 points)  6. Build a logistic regression, random forest, and xgboost classifier model after resampling the data and print the confusion matrix for the test data ( 10 points) 7. Apply Grid Search CV to get the best hyper parameters for any one of the above model  ( 5 points) 8. Build a classifier model using the above best hyper parameters and check the accuracy and confusion matrix ( 5 points) 9. Report feature importance and mention your comments ( 2 points) \n   \nProprietary content. \u00a9 Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited.                                                                        3 \n10. Report your findings and inferences ( 3 points)     \nFurther Questions ( Optional) -  \n1. Check for outliers and impute them as required. 2. Apply PCA to get rid of redundant features and reduce dimension of the data 3. Try cross validation techniques to get better results 4. Try OneCLassSVM model to get better recall   \nLearning Outcomes  \n\u25cf Feature Importance \u25cf Sampling \u25cf SMOTE \u25cf Grid Search \u25cf Random Forest \u25cf Exploratory Data Analysis \u25cf Logistic Regression   "}}