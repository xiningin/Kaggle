{"cell_type":{"a3d33f4a":"code","cb41b3aa":"code","9bfa829d":"code","53904f5a":"code","98b45330":"code","ebef2327":"code","18c31991":"code","3af64839":"markdown","2bc0596d":"markdown","edaf2eaf":"markdown","4fb66783":"markdown","57c52dbc":"markdown","f2f3a3af":"markdown","2fe71661":"markdown","1cccf8fb":"markdown","0819f900":"markdown","4bedf001":"markdown","84f89909":"markdown","f57b1e18":"markdown","9c445161":"markdown","786304d0":"markdown","e36b2be2":"markdown"},"source":{"a3d33f4a":"# make a simple model to show ML explainability\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('..\/input\/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"\/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","cb41b3aa":"# show permutation importance\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","9bfa829d":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=my_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()","53904f5a":"feature_to_plot = 'Distance Covered (Kms)'\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","98b45330":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\ninter1  =  pdp.pdp_interact(model=my_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","ebef2327":"# extract single row of the dataset\nrow_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nmy_model.predict_proba(data_for_prediction_array)","18c31991":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","3af64839":"## Code example\nI used code in this [DanB's kernel ](https:\/\/www.kaggle.com\/dansbecker\/partial-plots) (I changed some variable names)","2bc0596d":"## Code example\n\nIn this kernel, I used a model that predict whether a soccer team will have the \"Man of the Game\" winner based on the team's statistics.  \nI used code in this [DanB's kernel](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance) ","edaf2eaf":"## When do we use SHAP values?\n\n- You can use SHAP values for making decision, especially for the case which is related to people's life.\n- You can show why your model output this result to users.\n- You can use SHAP values to improve your model, because you can find out why your model made a wrong decision.","4fb66783":"You can check interactions between features from 2D partial plots.","57c52dbc":"## When do we use permutation importance?\n\n- For shortening time to make model and prediction by removing less important features.\n- You can confirm that whether your feature has a big impact on predictions.","f2f3a3af":"# Why is machine learning explainability needed?\n\n- Machine learnig model tend to be a black box compared with statistical model.\n- If model can't explain why it output this result, users don't want to use ML model, especially for the case which is related to people's life.\n    - If it's not related to people's life, users want to know why ML model output this result in many cases.\n- Data scientists want to know how model predict in order to enhance and debug model.","2fe71661":"# Permutation Importance\n[DanB's permutation importance lesson](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n\n\n## Overview\n\n- Permutation importance is a method to confirm which feature have the biggest impact on predictions.\n\n## How it works\n\n1. Prepare trained model\n1. Shuffle one feature like below and make prediction. Calculate accuracy using prediction.\n1. repeat 2. for every feature\n    - accuracy is decreased significantly -> shuffled feature is much important.\n    - accuracy is almost same before shuffling -> shuffled feature is not important.\n\n![Shuffle](https:\/\/i.imgur.com\/h17tMUU.png)","1cccf8fb":"# What is machine learning explainability?\n\n- an ability to explain why machine learning model predict that results\n- an ability to show feature importance of machine learning model","0819f900":"## Code Examples\n\nI used code in this [DanB's kernel](https:\/\/www.kaggle.com\/dansbecker\/shap-values)","4bedf001":"# Methods to explain ML model\n\nI introduce three methods to explain machine learning model following DanB's lessons.\n\n- permutation importance\n- partial plots\n- SHAP values","84f89909":"I briefly summarized [DanB](https:\/\/www.kaggle.com\/dansbecker)'s great lessons about machine learning explainability for my study.  \nThis is **[Course Home Page](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability)**.","f57b1e18":"## When do we use partial plots?\n\n- You can find out how much detail we should collect data from partial plots.(It costs collecting detailed data...)\n    - For example, we found out that we don't have to collect \"Goal Scored\" data in detail. \"No Goal\" and \"More than one goal\" may be enough, because prediction do not change whether a team get 1 goal  or 2 goals.\n- You can confirm whether your feature has a big impact on predictions.\n    - If partial plots doesn't change, you can find out this feature is not important.","9c445161":"Thanks for reading my kernel.  \nIf you have any questions, please ask me.  \nIf you feel my English is weird, please give me some comments.  ","786304d0":"# SHAP values\n[DanB's SHAP values lesson](https:\/\/www.kaggle.com\/dansbecker\/shap-values)\n\n## Overview\n\n- SHAP values shows that which feature has a big impact on predictions for individual predictions.","e36b2be2":"# Partial Plots\n[DanB's Partial Plots lesson](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)\n\n## Overview\n\n- Partial plots is a method to confirm the change of predictions by altering one feature's value gradually\n\n## How it works\n\n1. Prepare a trained model\n1. Alter one feature's value gradually, and plot prediction changes"}}