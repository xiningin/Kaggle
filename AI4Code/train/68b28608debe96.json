{"cell_type":{"30b8210d":"code","1d26a41c":"code","c9246fea":"code","43053225":"code","f55f47ff":"code","243e2694":"code","a4dab3f7":"code","a8558a4c":"code","d321de11":"code","26b7413a":"code","7c2b96f3":"code","d5aa3bc4":"code","f1a6463b":"code","0f7d2029":"code","cfdfd9bc":"code","5449f59a":"code","8254f08e":"code","ca8390e9":"code","b63c8c2d":"code","6cd2038c":"code","4bd67544":"code","fd1a46fd":"code","56a8463f":"code","5f2fe542":"code","ba15dddb":"code","5b7bcb2c":"code","449b2c1c":"code","313a03ce":"code","16d5b25c":"code","1c2095d1":"code","cbfbcce4":"code","635f13af":"code","16da60e4":"code","7612471b":"code","2fc73c16":"code","edb219e8":"code","854969ea":"code","b7736011":"code","0b085ae6":"code","f906cd75":"code","29cb1e7f":"code","0763c3c2":"code","ef99e8b4":"code","ea62c74b":"code","83a8743c":"code","905ff200":"code","d6573298":"code","608f1fba":"code","1984eb95":"code","189066ab":"code","6f2dfb91":"code","00817156":"code","0d2559d1":"code","d6b8aa03":"code","88739f4a":"code","6731203e":"code","ab6a6529":"code","1009c62d":"code","983d8b9a":"code","de881ac4":"markdown","95db84cf":"markdown","9cbcba6d":"markdown","02e07fbd":"markdown","a28232ef":"markdown","fb07e87b":"markdown","9e3d7d15":"markdown","110c91ca":"markdown","80ba7309":"markdown","d6f65cd0":"markdown","0767683d":"markdown","3aada63b":"markdown","272c83f0":"markdown","8710d0c7":"markdown","ee62c0b7":"markdown","2eafef75":"markdown","16b90030":"markdown","2905e3b1":"markdown","46cd2274":"markdown","c500175a":"markdown","21af4f55":"markdown","cccb48b4":"markdown","e27aa902":"markdown","abb298f9":"markdown","5b264b7b":"markdown","a21d8fd8":"markdown","7e94c4b2":"markdown","6704718c":"markdown","2ada13d3":"markdown","2a0cf0d3":"markdown","9d400987":"markdown","853ca88c":"markdown","e00091d9":"markdown","37f37901":"markdown","72dc1ec2":"markdown","361dac5c":"markdown","3530a614":"markdown","92a89c6b":"markdown","4269e875":"markdown","e772814e":"markdown","a493810a":"markdown","ec1259e9":"markdown","9a6ba2f2":"markdown","cc44fe45":"markdown","480942ba":"markdown","79b81be9":"markdown","e67b0923":"markdown","c1cf166e":"markdown","c965f0e9":"markdown","d6bbdf26":"markdown","f2942dc2":"markdown","e846815a":"markdown","24879ff2":"markdown"},"source":{"30b8210d":"import os\nimport getpass\nif getpass.getuser() == 'root': # kaggle\n    %pip install -qU scikit-learn\n\nimport random\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nrandom.seed(64)\nnp.random.seed(64)\n\ndata_root = os.environ.get('KAGGLE_DIR', '..\/input')\ndf = pd.read_parquet(f'{data_root}\/tpsdec2021parquet\/train.pq')\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=64)\nn_jobs = min(cv.n_splits, os.cpu_count())\n\nsns.set(\n    style='darkgrid', context='notebook', rc={\n        'figure.frameon': False,\n        'figure.figsize': (16, 12),\n        'legend.frameon': False,\n    }\n)\n\ntree_method = 'gpu_hist' if torch.cuda.is_available() else 'hist'\n\ndf.info()","1d26a41c":"df.isna().sum().sum()","c9246fea":"px.bar(df.Cover_Type.value_counts(normalize=True))","43053225":"is_wilds = df.columns.str.startswith('Wilderness')\nwilds = df.columns[is_wilds]\ndf[wilds].sum(axis=1).value_counts()","f55f47ff":"d = df[df.columns[is_wilds | (df.columns == 'Cover_Type')]]\n\npx.bar(\n    df[wilds].mean(), title='mean(variable)'\n)","243e2694":"px.imshow(df[wilds].corr())\n","a4dab3f7":"\npx.bar(\n    d.groupby('Cover_Type', as_index=False).mean().melt(id_vars=['Cover_Type'], value_name='mean(variable)'),\n    x='Cover_Type', y='mean(variable)', color='variable', barmode='group'\n)\n","a8558a4c":"tree = DecisionTreeClassifier()\n\ncross_val_score(\n    tree, df[wilds], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","d321de11":"dummy = DummyClassifier(strategy='prior')\n\ncross_val_score(\n    dummy, df[wilds], df.Cover_Type, cv=cv, n_jobs=n_jobs,\n)","26b7413a":"cross_val_score(\n    tree,\n    pd.concat([df[wilds], df[wilds].sum(axis=1).rename('sum')], axis=1),\n    df.Cover_Type,\n    cv=cv,\n    n_jobs=n_jobs\n)\n","7c2b96f3":"is_soiltype = df.columns.str.startswith('Soil_')\nsoils = df.columns[is_soiltype]\n\npx.bar(df[soils].mean())","d5aa3bc4":"px.bar(\n    df[soils].sum(axis=1).value_counts(normalize=True)\n)\n","f1a6463b":"d = df[df.columns[is_soiltype | (df.columns == 'Cover_Type')]].groupby(\n    'Cover_Type', as_index=False\n).mean().melt(\n    id_vars=['Cover_Type'], value_name='mean(variable)'\n)\n\npx.bar(\n    d, x='variable', y='mean(variable)', facet_col='Cover_Type', facet_col_wrap=4\n)\n","0f7d2029":"cross_val_score(\n    DecisionTreeClassifier(max_depth=12), df[soils], df.Cover_Type, cv=cv, n_jobs=n_jobs,\n)","cfdfd9bc":"cross_val_score(\n    LogisticRegression(), df[soils], df.Cover_Type, cv=cv, n_jobs=n_jobs,\n)","5449f59a":"from xgboost import XGBClassifier\n\ncross_val_score(\n    XGBClassifier(tree_method=tree_method), df[soils], df.Cover_Type, cv=cv, n_jobs=1\n)\n","8254f08e":"conts = df.columns[(df.dtypes == np.float32) | (df.columns == 'Cover_Type')]\nconts = df[conts].melt(id_vars=['Cover_Type']).astype({'Cover_Type': 'category'})\n\nsns.displot(\n    data=conts, facet_kws={'sharey': False, 'sharex': False}, common_bins=False,\n    x='value', hue='Cover_Type', col='variable', col_wrap=3, bins=50,\n);","ca8390e9":"sns.displot(df.astype({'Cover_Type': 'category'}), x='Elevation', hue='Cover_Type', kind='ecdf', aspect=2);\n","b63c8c2d":"cross_val_score(tree, df[['Elevation']], df.Cover_Type, cv=cv, n_jobs=min(5, os.cpu_count()))\n","6cd2038c":"100 * df.Cover_Type.value_counts(normalize=True)\n","4bd67544":"df_test = pd.read_parquet(f'{data_root}\/tpsdec2021parquet\/test.pq')\n\nsns.displot(\n    pd.concat([df[['Elevation']].assign(set='train'), df_test[['Elevation']].assign(set='test')]).reset_index(),\n    x='Elevation', row='set', aspect=2, bins=100, facet_kws={'sharey': False}\n);\n","fd1a46fd":"sns.displot(\n    pd.concat([df[['Elevation']].assign(set='train'), df_test[['Elevation']].assign(set='test')]).reset_index(),\n    x='Elevation', hue='set', aspect=2, kind='ecdf',\n);\n","56a8463f":"df.groupby('Cover_Type')['Elevation'].agg(['mean', 'std'])\n","5f2fe542":"sns.histplot(x=df.loc[df.Cover_Type == 4, 'Elevation'], kde=True);\n","ba15dddb":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\ncross_val_score(\n    make_pipeline(StandardScaler(), LogisticRegression()),\n    df[['Elevation']], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","5b7bcb2c":"cross_val_score(\n    make_pipeline(StandardScaler(), LogisticRegression()),\n    df[['Elevation']].apply(np.log), df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","449b2c1c":"cross_val_score(\n    make_pipeline(StandardScaler(), LogisticRegression()),\n    df[['Elevation']].apply(np.sqrt), df.Cover_Type, cv=cv, n_jobs=n_jobs\n)","313a03ce":"cross_val_score(\n    make_pipeline(StandardScaler(), LogisticRegression()),\n    df[['Elevation']].apply(np.cbrt), df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","16d5b25c":"sns.displot(\n    x=df.Elevation, hue=df.Wilderness_Area4, kde=True, bins=50, aspect=2\n);\n","1c2095d1":"cross_val_score(\n    tree, df[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3']], df.Cover_Type, cv=cv, n_jobs=min(5, os.cpu_count())\n)\n","cbfbcce4":"from matplotlib import pyplot as plt\n\nfig, axes = plt.subplots(2, 2, sharex=True)\n\nfor i, ax in enumerate(axes.flat, 1):\n    sns.histplot(x=df.Elevation, hue=df[f'Wilderness_Area{i}'], kde=True, bins=50, ax=ax).set(title=f'Wilderness_Area{i}');","635f13af":"cross_val_score(\n    tree,\n    pd.concat([df[wilds], df[['Elevation']]], axis=1),\n    df.Cover_Type, cv=cv, n_jobs=n_jobs\n)","16da60e4":"cross_val_score(\n    make_pipeline(StandardScaler(), LogisticRegression()),\n    pd.concat([df[wilds], df[['Elevation']].apply(np.log)], axis=1),\n    df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","7612471b":"sns.displot(x=df.Aspect, kde=True, aspect=2, bins=100);\n","2fc73c16":"sns.displot(\n    x=df.Aspect, col=df.Cover_Type, kde=True, aspect=2, bins=50, col_wrap=4, facet_kws={'sharey': False}\n);\n","edb219e8":"cross_val_score(\n    tree, df[['Aspect']], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","854969ea":"sns.displot(\n    pd.concat([df[['Aspect']].assign(set='train'), df_test[['Aspect']].assign(set='test')]).reset_index(),\n    x='Aspect', row='set', bins=50, kde=True, aspect=2, facet_kws={'sharey': False}\n);","b7736011":"sns.displot(x=df.Slope, aspect=2, kde=True, bins=50);\n","0b085ae6":"sns.displot(\n    x=df.Slope, col=df.Cover_Type, kde=True, aspect=2, bins=50, col_wrap=4, facet_kws={'sharey': False}\n);","f906cd75":"cross_val_score(\n    tree, df[['Slope']], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","29cb1e7f":"sns.displot(x=df.Horizontal_Distance_To_Hydrology, kde=True, bins=100, aspect=2);\n","0763c3c2":"sns.displot(x=np.log(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Hydrology.min() + 1), kde=True, bins=100, aspect=2);\n","ef99e8b4":"sns.displot(\n    x=np.log(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Hydrology.min() + 1),\n    kde=True, bins=100, aspect=2, col=df.Cover_Type, col_wrap=4, facet_kws={'sharey': False}\n);","ea62c74b":"cross_val_score(\n    tree, df[['Horizontal_Distance_To_Hydrology']], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","83a8743c":"df.assign(\n    neg_distance=df.Horizontal_Distance_To_Hydrology < 0\n).groupby(\n    'Cover_Type'\n).neg_distance.mean()","905ff200":"sns.displot(df.Vertical_Distance_To_Hydrology, aspect=2, bins=100, kde=True);\n","d6573298":"sns.displot(\n    x=df.Vertical_Distance_To_Hydrology, col=df.Cover_Type, kde=True, aspect=2, bins=100, col_wrap=4, facet_kws={'sharey': False}\n);\n","608f1fba":"cross_val_score(\n    tree, df[['Vertical_Distance_To_Hydrology']], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","1984eb95":"sns.displot(df.Horizontal_Distance_To_Roadways, aspect=2, bins=100, kde=True);\n","189066ab":"sns.displot(\n    x=df.Horizontal_Distance_To_Roadways, col=df.Cover_Type, kde=True, aspect=2, bins=100, col_wrap=4, facet_kws={'sharey': False}\n);\n","6f2dfb91":"sns.displot(\n    x=np.log(1 + df.Horizontal_Distance_To_Roadways - df.Horizontal_Distance_To_Roadways.min()),\n    col=df.Cover_Type, kde=True, aspect=2, bins=100, col_wrap=4, facet_kws={'sharey': False}\n);","00817156":"cross_val_score(\n    tree, df[['Horizontal_Distance_To_Roadways']], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)","0d2559d1":"shades = df[df.columns[(df.columns == 'Cover_Type') | df.columns.str.startswith('Hillshade')]]\nd = shades.melt(id_vars=['Cover_Type'])\n\nsns.displot(\n    d, x='value', col='variable', row='Cover_Type', kde=True, bins=100, facet_kws={'sharey': False}\n);\n","d6b8aa03":"cross_val_score(\n    tree, df[df.columns[df.columns.str.startswith('Hillshade')]], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","88739f4a":"sns.displot(\n    x=df.Horizontal_Distance_To_Fire_Points, bins=100, aspect=2, kde=True\n);\n","6731203e":"sns.displot(\n    x=df.Horizontal_Distance_To_Fire_Points, bins=100, aspect=2, kde=True, col=df.Cover_Type, col_wrap=4, facet_kws={'sharey': False}\n);\n","ab6a6529":"sns.displot(\n    x=np.log(1 + df.Horizontal_Distance_To_Fire_Points - df.Horizontal_Distance_To_Fire_Points.min()),\n    bins=100, aspect=2, kde=True, col=df.Cover_Type, col_wrap=4, facet_kws={'sharey': False}\n);","1009c62d":"sns.displot(\n    x=np.sqrt(1 + df.Horizontal_Distance_To_Fire_Points - df.Horizontal_Distance_To_Fire_Points.min()),\n    bins=100, aspect=2, kde=True, col=df.Cover_Type, col_wrap=4, facet_kws={'sharey': False}\n);\n","983d8b9a":"cross_val_score(\n    tree, df[['Horizontal_Distance_To_Fire_Points']], df.Cover_Type, cv=cv, n_jobs=n_jobs\n)\n","de881ac4":"Well, that's almost as good as the decision tree. What if we take the log or the cube root?\n","95db84cf":"This also doesn't seem useful, let's check if the tree can make use of it to beat `DummyClassifier`:\n","9cbcba6d":"Right, certainly easy to see why Elevation makes a difference. A lot of these don't look like they're normally distributed, so it might be interesting to do some transforms for linear models here.\n\nLet's look at them one at a time.\n\nElevation\n==\n\nElevation is super-important:\n","02e07fbd":"Right, on their own, these are not much better than `DummyClassifier`. It's plausible that some other model could find them valuable:","a28232ef":"It seems to me that there are differences. The test set has higher representation at lower Elevation. This probably also implies a higher amount of the class imbalances for low-elevation classes, just to take the mean:\n","fb07e87b":"No, this is still a lot better than the `DummyClassifier` -- but maybe that's also correlation with `Elevation`. Let's check:","9e3d7d15":"Are they correlated?","110c91ca":"That looks almost categorical in nature here. And it also looks like something that could benefit from being transformed. Let's how this distributes with the labels:","80ba7309":"Right, these just aren't very strong features, it would appear.\n\nContinuous features\n==\n\nThese were considered by far the most important features by the boosters we've already trained, in particular `Elevation` was used very actively.\n\nThere aren't that many of these, so we'll check these out one at a time. But first, an overview plot:\n","d6f65cd0":"And stronger linear models too:","0767683d":"There are no nulls\n==\n\nI already know this, but just to have it out of the way:","3aada63b":"Still more features go get through, though. Let's move on!\n\nAspect\n==\n\nThe aspect is in degrees azimuth, let's review it's full distribution:\n","272c83f0":"These appear to be quite sparse. Could they be some one-hot encoding of some other kind of feature?","8710d0c7":"Ah, this seems more promising. But, even though these distributions are distinct, they wouldn't be easy to separate. Maybe we need to have this in combination with something else for it to make sense? I don't think we can easily fix this with some numerical trick:","ee62c0b7":"But I don't think this makes it a lot more compelling as a feature. Let's try it:\n","2eafef75":"The train set contains less than 1% of these samples, but the test set likely has much more, with its increased density below 2150 elevation.\n\nFeature transformations for Elevation\n--\n\nNot sure what makes sense here, exactly. But I think we can get away with just scaling it, even for linear classifiers:","16b90030":"`Wilderness_Area4` depends on `Elevation`\n--\n\nOkay, bingo. We were getting 60% accuracy by just classifying based on the `Wilderness_Area` variables, was that all `Wilderness_Area4`?\n","2905e3b1":"We have pretty strong correlation between `Wilderness_Area1` and `Wilderness_Area3`. Let's check if they can tell us anything about the likely target value:\n","46cd2274":"Nope, the decision tree can perfectly capture all that's being expressed here without it, so it seems unlikely we'll have much to gain by doing feature engineering on these.\n\nLet's check out the boolean columns that were not `Wilderness_Area`\n\nSoil_Type\n==\n\nThere are lots and lots of boolean soil type columns:\n","c500175a":"Let's recheck the label proportions:\n","21af4f55":"Label distribution\n==\n\nThis data set is wildly imbalanced:","cccb48b4":"Well, perhaps it used to be in degrees in the original dataset, but this doesn't look exactly like degrees.\n\nThe bins look strange to me, almost as if this feature was categorical in part. How does this feature distribute for the various `Cover_Type` values?","e27aa902":"This feature feels unimportant at this point, and I'm not going to worry about it much.\n\nSlope\n==\n\nAlso in degrees, let's check it out:\n","abb298f9":"Again, not beating the dummy classifier.\n\nFeature inspection summary\n==\n\nThat means we only found very few features that are strong enough to stand on their own -- `Elevation` doing nearly 90% accuracy on its own, and the rest being very underwhelming on their own. If we were to try out some feature engineering, lots of the continuous features could probably benefit from being transformed, if we were using linear models. Other than that, `Elevation` seems most exciting, or perhaps dealing with the major imbalances.\n\nAt this point, it would be interesting to see if there's any combination of features that helps, and does not contain `Elevation` -- and looking into different ways of encoding the bools may also be useful. It's probably going to be a lot of feature engineering work to make a good linear model here.\n","5b264b7b":"This looks like it might have some value -- to me, there's this particular thing where `Wilderness_Area1` is almost never set outside of `Cover_Type = 1 | 2 | 7` and `Wilderness_Area4` is almost only seen in those cases.\n\nI bet that these alone could give us a reasonable baseline:\n","a21d8fd8":"At a glance, perhaps it's only `Wilderness_Area4`. Using the wilderness features together with elevation results in stronger trees:","7e94c4b2":"This has a tail, and we'd probably benefit from doing some sort of transform to it, for our linear models:\n","6704718c":"Initial EDA\n==\n\nIn [TPS202112 - Parquet](https:\/\/www.kaggle.com\/kaaveland\/tps202112-parquet), I converted the competition CSV files to parquet, to facilitate rapid testing\/exploring, and we'll be build off from that dataset.\n\nIn [TPS202112 - XGBoost Baseline](https:\/\/www.kaggle.com\/kaaveland\/tps202112-xgboost-baseline) and [TPS202112 - LGBM  feature importance](https:\/\/www.kaggle.com\/kaaveland\/tps202112-lgbm-feature-importance) we establish that boosters do pretty well on this problem, easily achieving LB scores of over 95.3% without looking at the data.\n\nIn this notebook, we'll try to gain some advantage from... actually looking at the data.","2ada13d3":"It looks like it's going to perfectly separate several of the cover types on its own:\n","2a0cf0d3":"This doesn't look important at first glance. Can the decision tree make any use of it at all?\n","9d400987":"Test set probably has a higher density of Cover_Type = 4\n==\n\nThis finding could be important later on. We can try to make use of this, for example by assigning higher class weight to this Cover_Type.\n","853ca88c":"Now, in the real world, it makes perfect sense to have 0 distance to Hydrology, but probably not negative distance to it? Does that imply direction?\n\nAnyway, let's check if this explains anything with the labels:\n","e00091d9":"We'll keep it in mind that a log transformation here could help linear models, and move on -- hold on, actually, I want to test one thing. I seem to remember `Wilderness_Area4` being very important for `Cover_Type in (4, 6)`. Might that just be a proxy for a certain kind of elevation?\n","37f37901":"That's patently false -- it's possible for all of these variables to be true at the same time. It's possibly interesting to sum them like this as a feature. Let's take a look at how commonly these are positive:","72dc1ec2":"Well, no, not really. The decision tree couldn't make much use of them, and neither could a logistic regression. Let's quickly check a booster too:\n","361dac5c":"None of these seem particularly compelling to me, on their own, they seem mostly to not be strongly connected to the label. Can our tree make use of these?\n","3530a614":"Well, no, not really.\n\nHorizontal_Distance_To_Fire_Points\n==\n\nAlso supposed to be meters, let's check it out:","92a89c6b":"So, this feature alone gets us to 89% accuracy. That is rather quite strong. This makes it very interesting to check whether the elevation distribution is roughly the same in train and test, so let's do that:\n","4269e875":"At a glance, this too, doesn't seem so useful. I'm going to throw the tree at it, and move on:\n","e772814e":"Right, beats the dummy classifier:\n","a493810a":"Let's check if summing these features could make an extra feature of some value:","ec1259e9":"To me, it doesn't look like there are massive differences between these, other than in the Cover_Types that have too few samples. Let's check with the tree:\n","9a6ba2f2":"It's quite possible that it's a good idea to try to model some of these separately. Eg. if we can get close to 100% accuracy on the question: \"is it either 1 or 2?\", then maybe we could solve the others separately.\n\nLet's start looking at features.\n\nUnderstanding Wilderness_Area\n==\n\nLet's start by looking into `Wilderness_Area`. I think there's a good chance that this is actually a single one-hot\nencoded variable. Let's test that theory quickly:","cc44fe45":"It seems like maybe these could help us separate some of the rarer classes, but these are very sparse in `Cover_Type <= 3`.\n\nLet's how far a model gets by only looking at these. For this, I'm going to regularize the tree a bit, to prevent it from growing very deep:\n","480942ba":"Right, it can't. As a last check, does negative distance mean anything?","79b81be9":"Possibly a _very_ weak correlation with Cover_Type = 4, but more likely just because we have so few. Let's leave this here.\n\nVertical_Distance_To_Hydrology\n==\n\nAlso supposed to be meters:","e67b0923":"That seems not helpful, unfortunately.\n\nHillshade_*\n==\n\nThese seem similar enough, that I'm going to try to do all three at once. This seems to be some sort of measurement of shade, but I can't work out what the unit is.\n\nOh well, we can have negative shade again!","c1cf166e":"Okay, it's definitely not the case that this is a single one-hot encoding. Let's try to see if there's anything obviously connecting the various features here with the label:\n","c965f0e9":"Seems on par with the dummy classifier to me, so we can probably move on.\n\nHorizontal_Distance_To_Roadways\n==\n\nThis one was rated as important by the boosters we trained earlier. It's supposed to be distance in meters again -- and I suppose it would be a good measurement of how likely something is to be impacted by human activity, such as logging or construction. Let's check this out:\n","d6bbdf26":"Right, this too can be negative. Let's check out if this impacts the label:\n","f2942dc2":"On eyeballing it, this doesn't seem too promising either. It does look like maybe we could apply some transformation to this as well, to make it look more centered:\n","e846815a":"That performed as well as the `DummyClassifier` -- on its own, this feature doesn't explain much. Let's check whether the test distribution is about the same as the train distribution:\n","24879ff2":"No, doesn't look like it. Let's move on.\n\nHorizontal_Distance_To_Hydrology\n==\n\nThis is distance to the nearest water source:\n"}}