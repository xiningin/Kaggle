{"cell_type":{"9e6ed480":"code","cde55ee0":"code","683423ac":"code","b7cced96":"code","558b7f7f":"code","fb7b2507":"code","f1d0a736":"code","4e440a54":"code","b2db03a0":"code","a5824d83":"code","222f405a":"code","dd6ce3b4":"code","41aef8f9":"code","076c871c":"code","3aa9539a":"code","145a75ab":"code","77b01489":"code","69753c0a":"code","6fa281f2":"code","abb602ad":"code","2ca4fe2f":"code","193273c4":"code","512212f5":"code","8d2705ad":"code","0b61491e":"code","5efb88a7":"code","3a706f6a":"code","02e8645a":"code","4506e114":"code","f2504c29":"code","7fac9565":"code","639d099f":"code","b18152f9":"code","06a72961":"code","d350d0c5":"code","14e2f432":"markdown","b38c3e0f":"markdown","eb2320f1":"markdown","5c0cf8a5":"markdown","24b9b077":"markdown","7855f258":"markdown","0b73e4c3":"markdown","e269b11f":"markdown","c78bb268":"markdown","4af02c36":"markdown","43306113":"markdown","ee41649d":"markdown","3c9c5771":"markdown","fe07e679":"markdown","3a8f81e2":"markdown","0cd58524":"markdown"},"source":{"9e6ed480":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\n\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max.columns', None)","cde55ee0":"df = pd.read_csv(\"..\/input\/startup-logistic-regression\/50_Startups.csv\")\ndf.head()","683423ac":"sns.distplot(df['R&D Spend'])","b7cced96":"sns.distplot(df['Administration'])","558b7f7f":"sns.distplot(df['Marketing Spend'])","fb7b2507":"sns.distplot(df['Profit'])","f1d0a736":"sns.barplot(x = 'State', y = 'R&D Spend', data = df)","4e440a54":"sns.barplot(x = 'State', y = 'Administration', data = df)","b2db03a0":"sns.countplot(x = 'State', data = df)","a5824d83":"sns.boxplot(x = 'State', y = 'R&D Spend', data = df)","222f405a":"sns.boxplot(x = 'State', y = 'Administration', data = df)","dd6ce3b4":"sns.boxplot(x = 'State', y = 'Marketing Spend', data = df)","41aef8f9":"sns.boxplot(x = 'State', y = 'Profit', data = df)","076c871c":"sns.stripplot(x ='State', y= 'R&D Spend', data = df)","3aa9539a":"sns.stripplot(x ='State', y= 'Administration', data = df)","145a75ab":"sns.stripplot(x ='State', y= 'Marketing Spend', data = df)","77b01489":"sns.stripplot(x ='State', y= 'Profit', data = df)","69753c0a":"sns.pairplot(df, hue = 'State')","6fa281f2":"df.info()","abb602ad":"df.isnull().sum()","2ca4fe2f":"df.State.describe()","193273c4":"df = pd.get_dummies(df, columns=['State'])\ndf.head()","512212f5":"df.corr()['Profit'].sort_values()","8d2705ad":"f, ax = plt.subplots(figsize=(10,8))\ncorr = df.corr()\nsns.heatmap(corr, annot=True, mask=np.zeros_like(corr, dtype=np.bool),\n           cmap = sns.diverging_palette(240, 10, as_cmap = True), \n           square = True, ax = ax)","0b61491e":"std = StandardScaler()\ndf_std = std.fit_transform(df)\ndf_std = pd.DataFrame(df_std, columns = df.columns)","5efb88a7":"X = df_std.drop(['Profit'], axis = 1)\ny = df_std.Profit","3a706f6a":"X_train, X_test, y_train, y_test = train_test_split(X, y\n                                                    ,test_size = 0.3\n                                                    ,random_state = 42)","02e8645a":"X_train.shape","4506e114":"regressor = sm.OLS(y_train, X_train).fit()\nprint(regressor.summary())\n\nX_train_dropped = X_train.copy()","f2504c29":"while True:\n    if max(regressor.pvalues) > 0.05:\n        drop_variable = regressor.pvalues[regressor.pvalues == max(regressor.pvalues)]\n        print(\"Dropping \" + drop_variable.index[0] + \" and running regression again because pvalue is: \" + str(drop_variable[0]))\n        X_train_dropped = X_train_dropped.drop(columns = [drop_variable.index[0]])\n        regressor = sm.OLS(y_train, X_train_dropped).fit()\n    else:\n        print(\"All p values less than 0.05\")\n        break","7fac9565":"print(regressor.summary())","639d099f":"column_names = df.drop(columns = ['Profit']).columns\nno_of_features = []\nr_squared_train = []\nr_squared_test = []\n\n\nfor k in range(1, 6):\n    selector = SelectKBest(f_regression, k = k)\n    X_train_transformed = selector.fit_transform(X_train, y_train)\n    X_test_transformed = selector.transform(X_test)\n    regressor = LinearRegression()\n    regressor.fit(X_train_transformed, y_train)\n    no_of_features.append(k)\n    r_squared_train.append(regressor.score(X_train_transformed, y_train))\n    r_squared_test.append(regressor.score(X_test_transformed, y_test))\n    \nsns.lineplot(x = no_of_features, y = r_squared_train, legend = 'full')\nsns.lineplot(x = no_of_features, y = r_squared_test, legend = 'full')","b18152f9":"### Best score k = 1, look at orange line\nselector = SelectKBest(f_regression, k = 1)\nX_train_transformed = selector.fit_transform(X_train, y_train)\nX_test_transformed = selector.transform(X_test)\ncolumn_names[selector.get_support()]","06a72961":"def regression_model(model):\n    \"\"\"\n    Will fit the regression model passed and will return the regressor object and the score\n    \"\"\"\n    regressor = model\n    regressor.fit(X_train_transformed, y_train)\n    score = regressor.score(X_test_transformed, y_test)\n    return regressor, score","d350d0c5":"model_performance = pd.DataFrame(columns = [\"Features\", \"Model\", \"Score\"])\n\nmodels_to_evaluate = [LinearRegression(), Ridge(), Lasso(), SVR(), RandomForestRegressor(), MLPRegressor()]\n\nfor model in models_to_evaluate:\n    regressor, score = regression_model(model)\n    model_performance = model_performance.append({\"Features\": \"Linear\",\"Model\": model, \"Score\": round(score,2)}, ignore_index=True)\n\nmodel_performance","14e2f432":"# Data preprocessing","b38c3e0f":"## Countplot","eb2320f1":"## Boxplot\n\nWe look at outliers","5c0cf8a5":"## OneHotEncoder","24b9b077":"## Barplot","7855f258":"# EDA\n\n## Distplot\n\nWe look at the distribution","0b73e4c3":"# Import Libs","e269b11f":"## Train split test","c78bb268":"### Uses SelectKBest","4af02c36":"# Data loading and overview","43306113":"Feature distribution is not normal","ee41649d":"### The function removes features with high p-value","3c9c5771":"## Striplot\n\nFeature scattering","fe07e679":"## Matrix","3a8f81e2":"### Thanks for watching!\n### If you liked the notebook then upvoted it or write your opinion.","0cd58524":"## Modeling\n\nWe uses scalling"}}