{"cell_type":{"d387e1fa":"code","9c337827":"code","f2265879":"code","8be706c4":"code","a8850912":"code","460aaf81":"code","cabb3367":"code","29e32c1b":"code","573bff53":"code","a3f25b9f":"code","b1bd06ae":"code","eee95f7a":"code","689b1b22":"code","6a5abf8f":"code","6cf7aa68":"code","9f484fd2":"code","aff3827e":"code","0418e7c0":"code","04bac80d":"code","448f0b1c":"code","b25f568d":"code","3f05dac1":"code","dfa782d9":"code","b8b37f65":"code","d1aabadc":"code","8b30c25d":"code","e0f6edf6":"markdown","326a816a":"markdown","0554c58d":"markdown","09cc8c36":"markdown","9aa7c4bb":"markdown"},"source":{"d387e1fa":"seed_value= 42\n\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\nimport random\nrandom.seed(seed_value)\n\nimport numpy as np\nnp.random.seed(seed_value)\n\nimport tensorflow as tf\ntf.random.set_seed(seed_value)","9c337827":"!mkdir Dataset\n!mkdir Dataset\/class1\n!mkdir Dataset\/class2\n!mkdir Dataset\/class3\n\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class1\/subclass1'\/* .\/Dataset\/class1\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class1\/subclass2'\/* .\/Dataset\/class1\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class1\/subclass6'\/* .\/Dataset\/class1\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class2\/subclass3'\/* .\/Dataset\/class2\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class2\/subclass7'\/* .\/Dataset\/class2\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class2\/subclass8'\/* .\/Dataset\/class2\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class3\/subclass4'\/* .\/Dataset\/class3\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class3\/subclass5'\/* .\/Dataset\/class3\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/class3\/subclass9'\/* .\/Dataset\/class3","f2265879":"data_dir = '.\/Dataset'\nvgg16weight = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nresnet50weight = '..\/input\/keras-pretrained-models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'","8be706c4":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.optimizers import RMSprop, SGD,Adam\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\nimport cv2\nimport requests\nimport os\nimport keras\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a8850912":"img_width, img_height = 200, 200\n\ntrain_data_dir = data_dir\nvalidation_data_dir = data_dir\nnb_train_samples = 93\nnb_validation_samples = 25\nepochs = 300\nbatch_size = 16\nnumclasses = 3","460aaf81":"def convert_to_jpg(path):\n    from PIL import Image\n    im = Image.open(path)\n    im.save(path.split('.jfif')[0] + '.jpg')","cabb3367":"import glob\n\nfor sub_class in glob.glob('.\/Dataset\/*'):\n    for file in glob.glob(sub_class + '\/*'):\n        if 'jfif' in file:\n            convert_to_jpg(file)","29e32c1b":"train_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n#     zoom_range = 0.1, # Randomly zoom image \n#     width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n#     height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n#     shear_range=0.2,\n    vertical_flip=False,\n    horizontal_flip=False)\n\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')","573bff53":"if K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)","a3f25b9f":"def resnet50(input_shape, outclass, sigma='sigmoid'):\n    \n    base_model = None\n    base_model = keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    \n    top_model = Sequential()\n    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n    for i in range(2):\n        top_model.add(Dense(4096, activation='relu'))\n        top_model.add(Dropout(0.5))\n    top_model.add(Dense(outclass, activation=sigma))\n\n    model = None\n    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n    \n    return model\n","b1bd06ae":"model = resnet50(input_shape, numclasses, 'softmax')\nlr = 1e-5\ndecay = 1e-7 \noptimizer = RMSprop(lr=lr, decay=decay)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\nmcp_save = ModelCheckpoint('.\/model.hdf5', save_weights_only=True, \n                           save_best_only=True, monitor='val_acc', mode='max')","eee95f7a":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_samples \/\/ batch_size,\n    epochs=epochs,\n    callbacks = [mcp_save],\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples \/\/ batch_size)","689b1b22":"model.save('model.hdf5')","6a5abf8f":"model.load_weights('.\/model.hdf5')","6cf7aa68":"training_loss = history.history['loss']\ntraining_acc = history.history['accuracy']\n\nepoch_count = range(1, len(training_loss) + 1)\n\nfig=plt.figure(figsize=(12, 4))\n\nfig.add_subplot(121)\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, training_acc, 'b-')\nplt.legend(['Training Loss', 'Training Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss\/Acc')\n\n\nval_acc = history.history['val_accuracy']\ntraining_acc = history.history['accuracy']\n\n\nepoch_count = range(1, len(val_acc) + 1)\n\n\nfig.add_subplot(122)\nplt.plot(epoch_count, val_acc, 'r--')\nplt.plot(epoch_count, training_acc, 'b-')\nplt.legend(['Validation Accuracy', 'Training Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\n\nplt.show();","9f484fd2":"predictions = model.predict(validation_generator)","aff3827e":"predictions = predictions.argmax(axis=1)","0418e7c0":"cm = confusion_matrix(validation_generator.classes, predictions)","04bac80d":"df_cm = pd.DataFrame(cm, index = ['Class1', 'Class2', 'Class3'],\n                  columns = ['Class1', 'Class2', 'Class3'])\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True)","448f0b1c":"!mkdir test_data\n!cp -r '..\/input\/face-dataset\/training model - cropped\/training model - cropped\/testimages' .\/test_data","b25f568d":"for file in glob.glob('.\/test_data\/*'):\n    if 'jfif' in file:\n        convert_to_jpg(file)","3f05dac1":"test_generator = test_datagen.flow_from_directory(\n    '.\/test_data',\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')","dfa782d9":"model.load_weights('.\/model.hdf5')","b8b37f65":"predictions = model.predict_generator(test_generator)","d1aabadc":"predictions = predictions.argmax(axis=1)","8b30c25d":"test_imgs = glob.glob('.\/test_data\/testimages\/*')\n\nfor test in test_imgs:\n    test_img = test\n    img = image.load_img(test_img, target_size=(img_width, img_height))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x \/= 255.\n    classes = model.predict(x)\n    result = np.squeeze(classes)\n    result_indices = np.argmax(result)\n    \n    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.axis('off')\n    plt.title(\"class {}, probability {:.2f}%\".format(predictions[result_indices]+1, result[result_indices]*100))\n    plt.imshow(img)\n    plt.show()\n    #print(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))","e0f6edf6":"# 3. Train","326a816a":"## Confusion Matrix","0554c58d":"# 2. Model","09cc8c36":"# 1. Prepare Data","9aa7c4bb":"# 4. Evaluation"}}