{"cell_type":{"b6ab34c5":"code","4c5c06de":"code","cefbc1da":"code","4451ef0c":"code","c8b86f29":"code","cf04af07":"code","8c2c9166":"code","759fe0d3":"code","2a717842":"code","5c730e03":"code","7235dc4a":"code","1a349461":"code","1318415d":"code","b1329aa3":"code","20089a36":"code","56c448ef":"code","8ef490db":"code","da50a358":"code","1d4169ba":"code","2f1cec8a":"code","6d56cb03":"code","18f1ce8d":"code","c215a5e4":"code","f4fad2b5":"code","5ec18a36":"code","268037d7":"code","d55b118c":"code","5f9a149d":"code","85a8e5f6":"code","3cadfa43":"code","75369e91":"code","71a9a1f6":"code","d0a51838":"code","c95f2276":"code","3ef7ca4c":"code","a77c84a7":"code","1c344904":"code","42ec3750":"code","db658b68":"markdown","102dc5ec":"markdown","96d137f8":"markdown","825b3391":"markdown","20b4c80e":"markdown","84bac9e8":"markdown"},"source":{"b6ab34c5":"import torchvision\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\nfrom fastai import *\nimport cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport fastai","4c5c06de":"import fastprogress\nfastprogress.fastprogress.NO_BAR = True\nmaster_bar, progress_bar = fastprogress.force_console_behavior()\nfastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\nfastai.basic_data.master_bar, fastai.basic_data.progress_bar = master_bar, progress_bar\ndataclass.master_bar, dataclass.progress_bar = master_bar, progress_bar\n\nfastai.core.master_bar, fastai.core.progress_bar = master_bar, progress_bar\nseed = 10","cefbc1da":"os.listdir('..\/input')\n!tar -xf \/kaggle\/input\/make-pneumothorax-classifer-data\/classifier_data.tar.gz -C .\n!tar -xf \/kaggle\/input\/make-pneumothorax-classifer-data\/test_data.tar.gz -C .","4451ef0c":"ls","c8b86f29":"tfms = get_transforms(do_flip=True, flip_vert=False, max_lighting=0.1, max_zoom=1.05,\n                      max_warp=0.,\n                      xtra_tfms=[rand_crop(), rand_zoom(1, 1.5),\n                                 symmetric_warp(magnitude=(-0.2, 0.2))])\n\n\npath = '\/kaggle\/working\/classifier_data'\npath_test = '\/kaggle\/working\/test'\n\ndata = (ImageList.from_folder(path)\n        .split_by_rand_pct(seed=seed)\n        .label_from_folder()\n        .transform(tfms, size=224)\n        .databunch().normalize(imagenet_stats))","cf04af07":"data.show_batch(rows=3, figsize=(12,9))","8c2c9166":"# class names and number of classes\n# print(data.classes)\nlen(data.classes),data.c","759fe0d3":"!pip install pretrainedmodels\nimport pretrainedmodels","2a717842":"from torch import nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1., gamma=2.):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets, **kwargs):\n        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n        return F_loss.mean()","5c730e03":"def resnext50_32x4d(pretrained=False):\n    pretrained = 'imagenet' if pretrained else None\n    model = pretrainedmodels.se_resnext50_32x4d(pretrained=pretrained)\n    return nn.Sequential(*list(model.children()))","7235dc4a":"learn = cnn_learner(data, resnext50_32x4d, pretrained=True, cut=-2,\n                    split_on=lambda m: (m[0][3], m[1]), \n                    metrics=[accuracy])\nlearn.loss_fn = FocalLoss()","1a349461":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","1318415d":"learn.fit_one_cycle(32, max_lr=slice(2e-2), wd=1e-5)","b1329aa3":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","20089a36":"learn.save('resnext50_32x4d_1');\nlearn.unfreeze();\nlearn = learn.clip_grad();","56c448ef":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","8ef490db":"learn.load('resnext50_32x4d_1');\nlearn.unfreeze();\nlearn = learn.clip_grad();","da50a358":"lr = [3e-3\/100, 3e-3\/20, 3e-3\/10]\nlearn.fit_one_cycle(36, lr, wd=1e-7)\n","1d4169ba":"learn.save('resnext50_32x4d_2');","2f1cec8a":"SZ = 224\ncutout_frac = 0.20\np_cutout = 0.75\ncutout_sz = round(SZ*cutout_frac)\ncutout_tfm = cutout(n_holes=(1,1), length=(cutout_sz, cutout_sz), p=p_cutout)\n\ntfms = get_transforms(do_flip=True, max_rotate=15, flip_vert=False, max_lighting=0.1,\n                      max_zoom=1.05, max_warp=0.,\n                      xtra_tfms=[rand_crop(), rand_zoom(1, 1.5),\n                                 symmetric_warp(magnitude=(-0.2, 0.2)), cutout_tfm])","6d56cb03":"data = (ImageList.from_folder(path)\n        .split_by_rand_pct(seed=seed)\n        .label_from_folder()\n        .transform(tfms, size=224)\n        .databunch().normalize(imagenet_stats))\n\nlearn.data = data\nlearn.bs = 32\ndata.train_ds[0][0].shape","18f1ce8d":"learn.load('resnext50_32x4d_2');\nlearn.freeze();\nlearn = learn.clip_grad();","c215a5e4":"learn.loss_func = FocalLoss()","f4fad2b5":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","5ec18a36":"learn.fit_one_cycle(24, slice(3e-3), wd=5e-6)","268037d7":"learn.save('resnext50_32x4d_3');\nlearn.load('resnext50_32x4d_3');","d55b118c":"learn.unfreeze();\nlearn = learn.clip_grad();","5f9a149d":"lr = [1e-3\/200, 1e-3\/20, 1e-3\/10]\nlearn.fit_one_cycle(32, lr)","85a8e5f6":"learn.save('resnext50_32x4d_4');\nlearn.load('resnext50_32x4d_4');","3cadfa43":"learn.export('\/kaggle\/working\/fastai_resnet.pkl');","75369e91":"learn = load_learner('\/kaggle\/working\/','fastai_resnet.pkl', ImageList.from_folder(path_test))\npreds,_ = learn.get_preds(ds_type=DatasetType.Test)\ncls_pred = F.softmax(preds,1).argmax(1).cpu().numpy()","71a9a1f6":"paths = list(map(str,list(learn.data.test_ds.x.items)))\nall_test_paths = [p.split('\/')[-1][:-4] for p in paths]\n\ndf_preds = pd.DataFrame()\ndf_preds['test_paths'] = all_test_paths\ndf_preds['class_pred'] = cls_pred\n\ndf_preds.set_index('test_paths',inplace=True)","d0a51838":"df_preds.head()","c95f2276":"no_dis_idx = df_preds[df_preds.class_pred==1].index\nprint(len(no_dis_idx))","3ef7ca4c":"sub = pd.read_csv('\/kaggle\/input\/unet-plus-plus-with-efficientnet-encoder\/submission.csv'\n                  ,index_col='ImageId')\nsub.head()","a77c84a7":"sub.loc[no_dis_idx] = ' -1'","1c344904":"sub.to_csv('sub_classifier_correction.csv')","42ec3750":"!rm -r *\/","db658b68":"# Making the training set and dataloader","102dc5ec":"# Size 224","96d137f8":"# Stage 1 training with size 128","825b3391":"In this kernel i train a classifier using fastai. The classifieer data used for training is genereated [here](https:\/\/www.kaggle.com\/meaninglesslives\/make-pneumothorax-classifer-data). I use pretrained imagenet weights for seresnext50.\n\nI use the trained classifier to modify the submission in [my efficientnet kernel](https:\/\/www.kaggle.com\/meaninglesslives\/unet-plus-plus-with-efficientnet-encoder). The idea is that the trained unet may predict masks even when there is no pneumothorax. Zeroing out wrongly predicted masks will help us get a better performance.","20b4c80e":"# Loading Libraries","84bac9e8":"# Predicting on the test set"}}