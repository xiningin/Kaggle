{"cell_type":{"b4743e48":"code","35a9e352":"code","196dc0cd":"code","e8e733e1":"code","7cd9db75":"code","c56ef1b5":"code","368536d7":"code","eabaed53":"code","03f2f1eb":"code","315314cc":"code","a9573c99":"code","01dab932":"code","3b2647b1":"code","a9d3393b":"code","a1ddfa3f":"code","a5f5688a":"code","cc51fea4":"code","ccbfb9a1":"code","3d30ee31":"code","bb33db2d":"code","247173c0":"code","05399e0e":"code","102bb1e4":"code","3c488f23":"code","81d56aae":"code","fc95ca97":"code","d864d30a":"code","5f149a38":"code","348384c8":"code","8b8ce2f3":"code","7459a1d0":"code","2b8076a6":"code","a23da559":"code","84972000":"markdown","8a4f178c":"markdown","1c6d2bab":"markdown","fd1e3254":"markdown","73d89d87":"markdown","39e33d5d":"markdown","dfc5f54b":"markdown","0969a5a9":"markdown","89745af4":"markdown","5026d134":"markdown","e58e51cf":"markdown","501912d4":"markdown","94d5318a":"markdown","196cedbc":"markdown","05df9a40":"markdown"},"source":{"b4743e48":"# package install\n!pip uninstall -q -y autogluon.core\n# !pip uninstall -q -y autogluon\n# !pip uninstall -q -y mxnet\n!pip install --upgrade -q pip\n!pip install -q category_encoders==2.2.2\n!pip install -q imblearn\n!pip install -q scikit-learn==0.23.2\n!pip install -q xgboost==1.3.3\n!pip install -q lightgbm==2.3.1\n!pip install -q catboost==0.24.3\n!pip install -q mlxtend\n!pip install -q ipython-autotime\n%load_ext autotime","35a9e352":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n    \nimport time\nimport math\nfrom collections import Counter\nimport itertools\nimport scipy.stats as ss\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 100) # Setting pandas to display a N number of columns\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.axes._axes import _log as matplotlib_axes_logger\nimport matplotlib.gridspec as gridspec\nmatplotlib_axes_logger.setLevel('ERROR')\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold, GridSearchCV, cross_val_predict\nfrom sklearn.metrics import accuracy_score, precision_score, roc_auc_score, f1_score, fbeta_score, matthews_corrcoef, make_scorer, roc_curve\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom sklearn.tree import export_graphviz\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nimport mlxtend\nfrom mlxtend.classifier import StackingCVClassifier\n\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline, make_pipeline\n\nimport category_encoders\nfrom category_encoders import WOEEncoder\n\nimport xgboost\nfrom xgboost import XGBClassifier\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nimport catboost\nfrom catboost import CatBoostClassifier\n\nprint(\"Python version:\", sys.version)\nprint(\"Version info.:\", sys.version_info)\nprint(\"pandas version:\", pd.__version__)\nprint(\"numpy version:\", np.__version__)\nprint(\"seaborn version: \", sns.__version__)\nprint(\"skearn version:\", sklearn.__version__)\nprint(\"mlxtend version: \", mlxtend.__version__)\nprint(\"imblearn version: \", imblearn.__version__)\nprint(\"XGBoost version: \", xgboost.__version__)\nprint(\"lightgbm version: \", lgb.__version__)\nprint(\"catboost version: \", catboost.__version__)\nprint(\"category_encoders version: \", category_encoders.__version__)","196dc0cd":"link = \"..\/input\/bank-marketing-data-set\/bank-additional-full.csv\"\ndf_bank = pd.read_csv(link, sep=';')\n\n# Drop duplicated \ndf_bank.drop_duplicates(inplace=True) \n\ndisplay(df_bank.shape, df_bank.head(10))","e8e733e1":"print(df_bank['y'].describe())\n\nplt.figure(figsize=(10, 5))\nsns.countplot(df_bank['y']) \nplt.grid() \nplt.title(\"Distribution of the class labels\", weight=\"semibold\") \nplt.show()","7cd9db75":"# Pick out the categorical, nominal, ordinal and numercial columns\ncat_cols = np.array(['job', 'marital', 'education', 'default', 'housing', 'loan', \n                     'contact', 'month', 'day_of_week', 'poutcome', 'y'])\nord_cols = np.array(['default', 'housing', 'loan', 'contact', 'poutcome', 'y'])\nnom_cols = np.setdiff1d(cat_cols, ord_cols)\nnum_cols = np.setdiff1d(df_bank.columns, cat_cols)\ncat_cols = np.setdiff1d(cat_cols, ['y'])\nord_cols = np.setdiff1d(ord_cols, ['y'])\n\nprint(\"Categorical columns:\", cat_cols)\nprint(\"Nominal columns:\", ord_cols)\nprint(\"Ordinal columns:\", nom_cols)\nprint(\"Numercial columns:\", num_cols)\n\ndf_bank.info()","c56ef1b5":"for x in cat_cols:\n    print(\"Attribute \\\"%s\\\":\" % (x), df_bank[x].unique())","368536d7":"pd.set_option('display.max_colwidth', 500) # Setting pandas to display a N number of columns\n\nprint(df_bank.shape)\ndisplay(df_bank.head(5))\ndisplay(df_bank.describe(include='all').T)\ndisplay(pd.DataFrame(df_bank.apply(lambda col: col.sort_values(ascending=True).unique()))) # unique values of each column","eabaed53":"# Plot the distribbution of categorical attributes\ndef chunks(l, n):\n    return [l[i:i + n] for i in range(0, len(l), n)]\n\ndef histplot(df, cols, ncols):\n    for lst in chunks(cols, ncols):\n        sns.set(font_scale = 1)\n        fig, axes = plt.subplots(nrows=1, ncols=ncols, figsize=(20, 4), dpi=600)\n        for idx in range(0, len(lst)):\n            attr = lst[idx]\n            data = df[attr]\n            sns.set_palette('Paired',30)\n            g = sns.countplot(x=attr, data=df, ax=axes[idx])\n            for item in g.get_xticklabels():\n                item.set_rotation(75)\n        plt.tight_layout()\n\nhistplot(df_bank[cat_cols], cat_cols, 5)","03f2f1eb":"# Define Functions\ndef conditional_entropy(x, y):\n    \"\"\"\n    Calculates the conditional entropy of x given y: S(x|y)\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Conditional_entropy\n    :param x: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of measurements\n    :param y: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of measurements\n    :return: float\n    \"\"\"\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0.0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y\/p_xy)\n    return entropy\n\ndef theils_u(x, y):\n    \"\"\"\n    Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association.\n    This is the uncertainty of x given y: value is on the range of [0,1] - \n    where 0 means y provides no information about\n    x, and 1 means y provides full information about x.\n    This is an asymmetric coefficient: U(x,y) != U(y,x)\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Uncertainty_coefficient\n    :param x: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of categorical measurements\n    :param y: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of categorical measurements\n    :return: float\n        in the range of [0,1]\n    \"\"\"\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) \/ s_x","315314cc":"def corr_matrix_plot(matrix, title=\"Correlation matrix between attributes\"):\n    # Plot the correlation matrix\n    fig, ax = plt.subplots(figsize=(10, 10))\n    plt.title(title, weight=\"semibold\")\n    ax.title.set_fontsize(10)\n\n    # Build the Color Correlation Matrix\n    mask = np.zeros_like(matrix, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    sns.set(font_scale = 1)\n    g = sns.heatmap(matrix, cmap='Blues', fmt = '.2f', square = True, \n                  mask=mask, annot=True, annot_kws={\"size\":10}, linewidths=1.0)\n\n    for text in g.texts:\n        t = float(text.get_text())\n        if ((t) < 0.1):\n            text.set_text('')\n        else:\n            text.set_text(round(t, 2))\n\n    # Build the Values Correlation Matrix\n    mask[np.triu_indices_from(mask)] = False\n    mask[np.tril_indices_from(mask)] = True\n    g = sns.heatmap(matrix, cmap=ListedColormap(['white']), square = True, fmt = '.2f', \n                  linewidths=1.0, mask=mask, annot=True, annot_kws={\"size\":8}, cbar=False);\n    g.set_xticklabels(g.get_xticklabels(), rotation=60, ha=\"right\")\n\n    # the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n    print(\"Display first 10 pairs of features with high correlation:\")\n    print(matrix.where(np.triu(np.ones(matrix.shape), k=1).astype(np.bool)).stack().sort_values(ascending=False)[:10])","a9573c99":"## Correlation matrix for categorical attributes\ncorrelationMatrix = pd.DataFrame(index=cat_cols, columns=cat_cols)\n\nfor col in cat_cols:\n    for row in cat_cols:\n        temp = theils_u(df_bank[cat_cols][col], df_bank[cat_cols][row])\n        correlationMatrix[row][col] = temp\n        correlationMatrix[col][row] = temp\n        \ncorrelationMatrix = correlationMatrix.astype(float)\ncorr_matrix_plot(correlationMatrix, title=\"Correlation matrix between categorical attributes\")","01dab932":"# Plot the distribbution of numerical attributes    \ndef chunks(l, n):\n    return [l[i:i + n] for i in range(0, len(l), n)]\n\ndef distplot(df, cols, ncols):\n    for lst in chunks(cols, ncols):\n        sns.set(font_scale = 1)\n        fig, axes = plt.subplots(nrows=1, ncols=ncols, figsize=(20, 4), dpi=600)\n        for idx in range(0, len(lst)):\n            attr = lst[idx]\n            data = df[attr]\n            g = sns.distplot(data.dropna(), color='navy', ax=axes[idx], kde_kws={'bw': 1.5})\n        plt.tight_layout()\n\ndistplot(df_bank[num_cols], num_cols, 5)","3b2647b1":"# Plot the boxplot of each attribute \ndef chunks(l, n):\n    return [l[i:i + n] for i in range(0, len(l), n)]\n\ndef boxplot(df, cols, ncols):\n    for lst in chunks(cols, ncols):\n        sns.set(font_scale = 1)\n        fig, axes = plt.subplots(nrows=1, ncols=ncols, figsize=(20, 4), dpi=600)\n        for idx in range(0, len(lst)):\n            attr = lst[idx]\n            data = df[attr]\n            sns.set_palette('Paired',30)\n            g = sns.boxplot(data=df[attr], ax=axes[idx], fliersize=0.5, linewidth=0.5, color='navy').set_title(attr)\n        plt.tight_layout()\n\nboxplot(df_bank[num_cols], num_cols, 5)","a9d3393b":"## CORRELATION MATRIX FOR NUMERICAL ATTRIBUTES\n\ncorrelationMatrix = df_bank[num_cols].dropna().corr()\ncorr_matrix_plot(correlationMatrix, title=\"Correlation matrix between numerical attributes\")","a1ddfa3f":"g = sns.pairplot(df_bank, hue='y', palette = 'seismic', size=1.8, diag_kind = 'kde', \n                 plot_kws=dict(s=10), diag_kws={'bw': 0.2, 'shade': True})\ng.set(xticklabels=[])","a5f5688a":"# convert the ordinal to numerical \nfor x in ord_cols:\n    print(x, df_bank[x].unique())\n\ndf_bank['contact'].replace({'telephone':0, 'cellular':1}, inplace=True)\ndf_bank['default'].replace({'no':-1, 'unknown':0, 'yes':1}, inplace=True)\ndf_bank['housing'].replace({'no':-1, 'unknown':0, 'yes':1}, inplace=True)\ndf_bank['loan'].replace({'no':-1, 'unknown':0, 'yes':1}, inplace=True)\ndf_bank['poutcome'].replace({'failure':-1, 'nonexistent':0, 'success':1}, inplace=True)\n\nfor x in ord_cols:\n    print(x, df_bank[x].unique())","cc51fea4":"TEST_SIZE = 0.3\nRAND = 42\nearlystopping = 50\n\nX_temp = df_bank.drop(['y'], axis=1)\ny_temp = df_bank['y'].replace({'yes':1, 'no':0})\n\nX_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, \n                                                    test_size = TEST_SIZE, \n                                                    random_state=RAND)\ndisplay(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train.head())","ccbfb9a1":"def transform_values(df):\n    return df.values","3d30ee31":"# The initial experiment on columns transformation, SMOTE for imbalanced classifcation and Decision Trees\nWOE_encoder = WOEEncoder(cols = nom_cols)\nWOE_encoder.fit(X_train, y_train)\nX_train_woe = WOE_encoder.transform(X_train, y_train).values\nX_test_woe = WOE_encoder.transform(X_test, y_test).values\n\nsm = SMOTE(random_state=RAND)\nX_train_woe_SMOTE, y_train_woe_SMOTE = sm.fit_resample(X_train_woe, y_train)\n\nprint(\"Training dataset after WOE encoder and SMOTE:\")\ndisplay(X_train_woe_SMOTE.shape)\ndisplay(pd.DataFrame(X_train_woe_SMOTE, columns=X_train.columns).head())\n\ndec_tree = tree.DecisionTreeClassifier()\ndec_tree.fit(X_train_woe_SMOTE, y_train_woe_SMOTE.squeeze())\n\n#calculate and print scores for the model for top 15 features\ny_train_preds = dec_tree.predict(X_train_woe_SMOTE)\ny_test_preds = dec_tree.predict(X_test_woe)\n\nprint('Decision Tree accuracy for train: %.3f: test: %.3f' % (accuracy_score(y_train_woe_SMOTE, y_train_preds), \n                                                              accuracy_score(y_test, y_test_preds)))\nprint('Decision Tree precision for train: %.3f: test: %.3f' % (precision_score(y_train_woe_SMOTE, y_train_preds), \n                                                              precision_score(y_test, y_test_preds)))\nprint('Decision Tree AUC for train: %.3f: test: %.3f' % (roc_auc_score(y_train_woe_SMOTE, y_train_preds), \n                                                         roc_auc_score(y_test, y_test_preds)))\nprint('Decision Tree F1 score for train: %.3f: test: %.3f' % (f1_score(y_train_woe_SMOTE, y_train_preds, average='macro'), \n                                                              f1_score(y_test, y_test_preds, average='macro')))\nprint('Decision Tree F-Beta 0.5 score for train: %.3f: test: %.3f' % (fbeta_score(y_train_woe_SMOTE, y_train_preds, average='macro', beta=0.5), \n                                                                      fbeta_score(y_test, y_test_preds, average='macro', beta=0.5)))\nprint('Decision Tree Matthews correlation coefficient for train: %.3f: test: %.3f' % (matthews_corrcoef(y_train_woe_SMOTE, y_train_preds), \n                                                                                      matthews_corrcoef(y_test, y_test_preds)))\n\n#get feature importances from the model\ndc_feature_importances_ = pd.DataFrame(dec_tree.feature_importances_, index = X_train.columns, \n                                       columns=[\"Decision Tree\"]).sort_values(by=\"Decision Tree\", ascending=False)\n\nprint(\"Feature importances (Decision Tree):\")\ndisplay(dc_feature_importances_.T)\n\n#plot feature importances\nplt.figure(figsize=(15, 5))\nx_pos = np.arange(0, len(dc_feature_importances_))\nplt.bar(x_pos, dc_feature_importances_[\"Decision Tree\"])\nplt.xticks(x_pos, dc_feature_importances_.index, rotation=45, ha=\"right\")\nplt.title(\"Feature importances (Decision Tree)\")\nplt.show()","bb33db2d":"# Implement the transformation pipeline, StratifiedKFold cv and Decision Trees\nimba_pipeline = Pipeline([(\"WOEEncoder\", WOEEncoder(cols = nom_cols)), \n                          (\"ValueTrans\", FunctionTransformer(transform_values)),\n                          (\"SMOTE\", SMOTE(random_state=RAND, n_jobs=-1)), \n                          (\"model\", tree.DecisionTreeClassifier())\n                         ])\n\nskf = StratifiedKFold(n_splits=5, random_state=RAND, shuffle=True)\n\ncross_validate(imba_pipeline, X_train, y_train, n_jobs=-1, scoring=\"f1_macro\", cv=skf)","247173c0":"# Implement the transformation pipeline, StratifiedKFold cv, Decision Trees with gridsearch\nimba_pipeline = Pipeline([(\"WOEEncoder\", WOEEncoder(cols = nom_cols)), \n                          (\"ValueTrans\", FunctionTransformer(transform_values)),\n                          (\"SMOTE\", SMOTE(random_state=RAND, n_jobs=-1)), \n                          (\"model\", tree.DecisionTreeClassifier())\n                         ])\n\nparam_grid = {\"model__max_depth\": [16, 18, 20], \n              \"model__criterion\": [\"gini\", \"entropy\"],\n              \"model__min_samples_leaf\": [5, 50, 100],\n              \"model__min_samples_split\": [2, 3, 4]\n             }\n\nskf = StratifiedKFold(n_splits=5, random_state=RAND, shuffle=True)\n\ngrid_imba = GridSearchCV(estimator=imba_pipeline, param_grid=param_grid, \n                         n_jobs=-1, pre_dispatch='2*n_jobs',cv=skf, \n                         scoring=\"f1_macro\", refit='f1_macro', return_train_score=True\n                         )\n\ngrid_imba.fit(X_train, y_train)\nprint(grid_imba.best_params_, grid_imba.best_score_)\n\ny_test_predict = grid_imba.predict(X_test)\nprint(accuracy_score(y_test, y_test_predict))\nprint(precision_score(y_test, y_test_predict))\nprint(roc_auc_score(y_test, y_test_predict))\nprint(f1_score(y_test, y_test_predict, average='macro'))\nprint(fbeta_score(y_test, y_test_predict, average='macro', beta=0.5))\nprint(matthews_corrcoef(y_test, y_test_predict))","05399e0e":"def batch_classify(X_train, Y_train, X_test, Y_test, dict_models, \n                   dict_classifiers, pipeline_list, no_classifiers = 1, verbose = True):\n    \"\"\"\n    This function take input data, pipeline and list of classifiers and its hyperparameters. \n    Then store the trained models into list, which could be reused later or export to file for further uses. \n    \"\"\"\n    \n    for classifier_name, classifier in list(dict_classifiers.items())[:no_classifiers]:\n        t_start = time.time()\n        if verbose:\n            print(\"Start {c} at {ct}\".format(c=classifier_name, ct=time.ctime()))\n        \n        # pipeline: \n        temp_pipeline_list = pipeline_list.copy()\n        temp_pipeline_list.append((\"SMOTE\", SMOTE(random_state=RAND, n_jobs=-1))) \n        temp_pipeline_list.append((\"model\", classifier.get(\"model\")))\n        imba_pipeline = Pipeline(temp_pipeline_list)\n\n        # get param grid\n        new_params = {'model__' + key: classifier.get(\"param_grid\")[key] for key in classifier.get(\"param_grid\")}\n        \n        # params for XGBoost and LightGBM early stopping\n        if (classifier_name == \"XGB\") or (classifier_name == \"LightGBM\"):\n            eval_pipe = Pipeline(pipeline_list)\n            eval_pipe.fit(X_train, Y_train)\n            X_test_val = eval_pipe.transform(X_test)\n            if classifier_name == \"XGB\":\n                fit_params = {\"model__early_stopping_rounds\": earlystopping, \"model__verbose\": False,\n                              \"model__eval_set\": [[X_test_val, Y_test]]}\n            else:\n                fit_params = {\"model__early_stopping_rounds\": earlystopping, \"model__verbose\": False, \n                              \"model__eval_set\": (X_test_val, Y_test)}\n        else:\n            fit_params = {}\n        \n        # Stratified K-Folds Cross-Validation\n        skf = StratifiedKFold(n_splits=5, random_state=RAND, shuffle=True)\n        \n        # GrivSearchCV to find best parameters \n        grid_imba = GridSearchCV(estimator=imba_pipeline, param_grid=new_params, n_jobs=-1, pre_dispatch='2*n_jobs',\n                                 cv=skf, scoring=\"f1_macro\", refit=\"f1_macro\", return_train_score=True)\n        grid_imba.fit(X_train, Y_train, **fit_params)        \n        Y_train_preds = grid_imba.predict(X_train)\n        Y_test_preds = grid_imba.predict(X_test)\n\n        t_end = time.time()\n        t_diff = (t_end - t_start)\/60\n        \n        # scoring and update dict_models\n        dict_models[classifier_name] = {'model': grid_imba.best_estimator_,\n                                        'train_accuracy': accuracy_score(Y_train, Y_train_preds), \n                                        'test_accuracy': accuracy_score(Y_test, Y_test_preds), \n                                        'train_precision': precision_score(Y_train, Y_train_preds, average='macro'), \n                                        'test_precision': precision_score(Y_test, Y_test_preds, average='macro'), \n                                        'train_roc_auc': roc_auc_score(Y_train, Y_train_preds, average='macro'), \n                                        'test_roc_auc':roc_auc_score(Y_test, Y_test_preds, average='macro'),\n                                        'train_mcc':matthews_corrcoef(Y_train, Y_train_preds), \n                                        'test_mcc':matthews_corrcoef(Y_test, Y_test_preds), \n                                        'train_fbeta':fbeta_score(Y_train, Y_train_preds, average='macro', beta=0.5), \n                                        'test_fbeta':fbeta_score(Y_test, Y_test_preds, average='macro', beta=0.5), \n                                        'train_f1':f1_score(Y_train, Y_train_preds, average='macro'), \n                                        'test_f1':f1_score(Y_test, Y_test_preds, average='macro'), \n                                        'train_time': t_diff}\n\n        if verbose:\n            print(\"Trained {c} in {f:.2f} min and finished at {ct}\".format(c=classifier_name, f=t_diff, ct=time.ctime()))\n    return dict_models","102bb1e4":"def export_dict_models(dict_models, sort_by='test_f1'):\n    cls = [key for key in dict_models.keys()]\n    \n    training_acc = [dict_models[key]['train_accuracy'] for key in cls]\n    test_acc = [dict_models[key]['test_accuracy'] for key in cls]\n    training_pre = [dict_models[key]['train_precision'] for key in cls]\n    test_pre = [dict_models[key]['test_precision'] for key in cls]\n    training_roc = [dict_models[key]['train_roc_auc'] for key in cls]\n    test_roc = [dict_models[key]['test_roc_auc'] for key in cls]\n    training_mcc = [dict_models[key]['train_mcc'] for key in cls]\n    test_mcc = [dict_models[key]['test_mcc'] for key in cls]\n    training_fb = [dict_models[key]['train_fbeta'] for key in cls]\n    test_fb = [dict_models[key]['test_fbeta'] for key in cls]\n    training_f1 = [dict_models[key]['train_f1'] for key in cls]\n    test_f1 = [dict_models[key]['test_f1'] for key in cls]\n    training_t = [dict_models[key]['train_time'] for key in cls]\n\n    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),14)), columns = ['classifier', \n                                                                     'train_accuracy', 'test_accuracy', \n                                                                     'train_precision', 'test_precision', \n                                                                     'train_roc_auc', 'test_roc_auc', 'train_mcc', 'test_mcc', \n                                                                     'train_fbeta', 'test_fbeta', 'train_f1', 'test_f1',\n                                                                     'train_time'])\n    for ii in range(0,len(cls)):\n        df_.loc[ii, 'classifier'] = cls[ii]\n        df_.loc[ii, 'train_accuracy'] = training_acc[ii]\n        df_.loc[ii, 'test_accuracy'] = test_acc[ii]\n        df_.loc[ii, 'train_precision'] = training_pre[ii]\n        df_.loc[ii, 'test_precision'] = test_pre[ii]\n        df_.loc[ii, 'train_roc_auc'] = training_roc[ii]\n        df_.loc[ii, 'test_roc_auc'] = test_roc[ii]\n        df_.loc[ii, 'train_mcc'] = training_mcc[ii]\n        df_.loc[ii, 'test_mcc'] = test_mcc[ii]\n        df_.loc[ii, 'train_fbeta'] = training_fb[ii]\n        df_.loc[ii, 'test_fbeta'] = test_fb[ii]\n        df_.loc[ii, 'train_f1'] = training_f1[ii]\n        df_.loc[ii, 'test_f1'] = test_f1[ii]\n        df_.loc[ii, 'train_time'] = training_t[ii]\n    \n    return df_.sort_values(by=sort_by, ascending=False)","3c488f23":"# Set up models and grid parameters \ndict_classifiers = {\n        \"Logistic Regression\": {\"model\": LogisticRegression(random_state=RAND), \n                                \"param_grid\": {\"C\": [0.3, 0.6, 0.7], \"penalty\": [\"l1\",\"l2\"], \n                                               \"solver\": [\"saga\"]\n                                              }\n                               },\n        \"Nearest Neighbors\": {\"model\": KNeighborsClassifier(), \n                              \"param_grid\": {\"n_neighbors\": [21, 23, 25]}\n                             },\n#         \"Linear SVM\": {\"model\": SVC(), \n#                         \"param_grid\": {'gamma':[0.1, 0.001]}\n#                       },\n        \"Naive Bayes\": {\"model\": GaussianNB(), \n                        \"param_grid\": {\"var_smoothing\": np.logspace(0,-9, num=10)}\n                       },\n        \"Neural Net\": {\"model\": MLPClassifier(random_state=RAND, early_stopping=True),\n                        \"param_grid\": {\"alpha\": [0.00001, 0.000001], \n                                       \"activation\": [\"tanh\"], # \"relu\"], \n                                       \"hidden_layer_sizes\": [(10, 150, 10), (10, 180, 10)]\n                                      }\n                      },\n        \"QDA\": {\"model\": QuadraticDiscriminantAnalysis(), \n                        \"param_grid\": {\"reg_param\": (0.01, 0.1), \"store_covariance\": (True, False), \n                                       \"tol\": (0.01, 0.1)}\n               },\n        \"Decision Tree\": {\"model\": tree.DecisionTreeClassifier(random_state=RAND), \n                          \"param_grid\": {\"max_depth\": [16, 18, 20], \n                                         \"criterion\": [\"gini\", \"entropy\"],\n                                         \"min_samples_leaf\": [5, 50, 100],\n                                         \"min_samples_split\": [2, 3, 4]}\n                         },\n}\n\n# Train and compare with multiple models\ndict_models = {}\nimba_pipeline_list = [(\"WOEEncoder\", WOEEncoder(cols = nom_cols)), \n                      (\"ValueTrans\", FunctionTransformer(transform_values)), \n                      (\"StandardScaler\", StandardScaler()),\n                     ]\n\nbatch_classify(X_train, y_train, X_test, y_test, dict_models, dict_classifiers, imba_pipeline_list, no_classifiers = 6)\nmodels_comp = export_dict_models(dict_models)\ndisplay(models_comp)","81d56aae":"display(dict_models)","fc95ca97":"cls = [key for key in dict_models.keys()]\nvoting_list = [(key, dict_models[key]['model']['model']) for key in cls]\n\ndict_classifiers = {\n        \"Voting Hard Classifiers\": {\"model\": VotingClassifier(estimators=voting_list, n_jobs=-1), \n                                    \"param_grid\": {'voting': ['hard']}\n                                   },\n        \"Voting Soft Classifiers\": {\"model\": VotingClassifier(estimators=voting_list, n_jobs=-1), \n                                    \"param_grid\": {'voting': ['soft']}\n                                   },\n}\n\nimba_pipeline_list = [(\"WOEEncoder\", WOEEncoder(cols = nom_cols)), \n                      (\"ValueTrans\", FunctionTransformer(transform_values)), \n                      (\"StandardScaler\", StandardScaler()),\n                     ]\n\nbatch_classify(X_train, y_train, X_test, y_test, dict_models, dict_classifiers, imba_pipeline_list, no_classifiers = 2)\nmodels_comp = export_dict_models(dict_models)\ndisplay(models_comp)","d864d30a":"cls = [key for key in dict_models.keys()]\nselect_models = ['Nearest Neighbors'] #['Logistic Regression', 'Nearest Neighbors', 'Naive Bayes', 'Neural Net', 'QDA', 'Decision Tree']\nbagging_list = [dict_models[key]['model']['model'] if key in select_models else None for key in cls]\n\ndict_classifiers = {\n        \"Extra Trees\": {\"model\": ExtraTreesClassifier(random_state=RAND, n_jobs=-1), \n                        \"param_grid\": {'criterion': ['entropy', 'gini'], 'max_depth': [6, 8],\n                                       'min_samples_leaf': [5, 50, 100, 120, 150]\n                                      }\n                       },\n        \"Random Forest\": {\"model\": RandomForestClassifier(random_state=RAND, n_jobs=-1), \n                          \"param_grid\": {'min_samples_leaf': [5, 50, 100],\n                                         'n_estimators': [250, 500], \n                                         'max_depth': [6, 8]\n                                        }\n                         },\n        \"Bagging Classifer\": {\"model\": BaggingClassifier(random_state=RAND, n_jobs=-1), \n                              \"param_grid\": {'base_estimator': bagging_list, \n                                             'n_estimators': [250],\n                                             'max_samples': [0.5], # X_train.shape[0]\/\/2], \n                                             'max_features': [0.5], # X_train.shape[1]\/\/2]\n                                            }\n                             }\n}\n\nimba_pipeline_list = [(\"WOEEncoder\", WOEEncoder(cols = nom_cols)), \n                      (\"ValueTrans\", FunctionTransformer(transform_values))\n                     ]\n\nbatch_classify(X_train, y_train, X_test, y_test, dict_models, dict_classifiers, imba_pipeline_list, no_classifiers = 3)\nmodels_comp = export_dict_models(dict_models)\ndisplay(models_comp)","5f149a38":"dict_classifiers = {\n        \"AdaBoost\": {\"model\": AdaBoostClassifier(), \n                     \"param_grid\": {\n                                    'n_estimators': [250, 500], \n                                    'learning_rate' : [0.01, 0.1]\n                                   }\n                    },\n        \"Gradient Boosting\": {\"model\": GradientBoostingClassifier(random_state=RAND, n_iter_no_change=10), \n                              \"param_grid\": {\n                                             'n_estimators': [250, 500], \n                                             'max_depth': [6, 8], \n                                             'subsample' : [0.7], \n                                             'learning_rate' : [0.01]\n                                            }\n                             },\n        \"XGB\": {\"model\": XGBClassifier(seed=RAND, verbosity=0, \n#                                        n_jobs=-1), \n                                       tree_method='gpu_hist', predictor='gpu_predictor'), \n                \"param_grid\": {\"objective\": [\"binary:logistic\"], \n                               \"n_estimators\": [250, 500], \n                               \"max_depth\": [6, 8], \n                               \"subsample\": [0.6, 0.9],  \n                               \"min_child_weight\": [1, 5, 8],\n                               \"colsample_bytree\": [0.6, 0.9],\n                               \"learning_rate\": [0.001, 0.01]\n                              }\n               },\n        \"LightGBM\": {\"model\": LGBMClassifier(random_state=RAND, verbose=-1, \n#                                              n_jobs=-1),\n                                             device = \"gpu\", gpu_platform_id = 0, gpu_device_id = 0),\n                     \"param_grid\": {\"objective\": [\"binary\"], \"verbose\": [-1], \n                                    \"n_estimators\": [250, 500], \n                                    \"max_depth\": [6, 8], \"num_leaves\": [32],\n                                    \"subsample\" : [0.6, 0.9],\n                                    \"min_child_weight\": [1, 5, 8],\n                                    \"colsample_bytree\": [0.6, 0.9],\n                                    \"learning_rate\" : [0.001, 0.01]\n                                   }\n                    },\n        \"CatBoost\": {\"model\": CatBoostClassifier(random_seed=RAND, verbose=False, early_stopping_rounds=50, \n#                                                  thread_count=-1),\n                                                 task_type=\"GPU\", bootstrap_type=\"Poisson\"), \n                     \"param_grid\": {\n                                    \"n_estimators\": [250, 500], \n                                    \"max_depth\": [6, 8],\n                                    \"subsample\": [0.6, 0.7, 0.9],\n                                    \"l2_leaf_reg\": [1, 5, 10], \n                                    \"learning_rate\": [0.001, 0.01]\n                                   }\n                    },\n}\n\nimba_pipeline_list = [(\"WOEEncoder\", WOEEncoder(cols = nom_cols)), \n                      (\"ValueTrans\", FunctionTransformer(transform_values)), \n                     ]\n\nbatch_classify(X_train, y_train, X_test, y_test, dict_models, dict_classifiers, imba_pipeline_list, no_classifiers = 5)\nmodels_comp = export_dict_models(dict_models)\ndisplay(models_comp)","348384c8":"# Using MLX package\n# We consider 3 boosting models XGB, LightGBM and CatBoost\nstacking_list = [dict_models[\"XGB\"]['model']['model'], dict_models[\"LightGBM\"]['model']['model'], dict_models[\"CatBoost\"]['model']['model']]\n\nlr = LogisticRegression(n_jobs=-1)\nprint(\"Estimators \", stacking_list)\nprint(\"Final Estimator \", lr)\n\ndict_classifiers = {\n        \"Stacking Classifer MLX\": {\"model\": StackingCVClassifier(classifiers = stacking_list, \n                                                                 meta_classifier = lr, n_jobs=-1), \n                                   \"param_grid\": {\n                                                  \"meta_classifier__C\": [0.01, 0.1], \n                                                  \"meta_classifier__penalty\": [\"l1\", \"l2\"],\n                                                  \"meta_classifier__solver\": [\"saga\"]\n                                                 }\n                                  }\n}\n\nimba_pipeline_list = [(\"WOEEncoder\", WOEEncoder(cols = nom_cols)), \n                      (\"ValueTrans\", FunctionTransformer(transform_values)), \n                     ]\n\nbatch_classify(X_train, y_train, X_test, y_test, dict_models, dict_classifiers, imba_pipeline_list, no_classifiers = 1)\nmodels_comp = export_dict_models(dict_models)\ndisplay(models_comp)","8b8ce2f3":"print(dict_models)","7459a1d0":"def feature_imp_viz(name, model, models_feat_imp, X_train):\n    #get feature importances from the model\n    plt.figure(figsize=(15, 3))\n    \n    feature_importances_ = pd.DataFrame(model[\"model\"].feature_importances_, \n                                        index = X_train.columns, columns=[name]).sort_values(by=name, ascending=False)\n\n    print(\"Feature importance\", name)\n    display(feature_importances_.T)\n    \n    #plot feature importances\n    x_pos = np.arange(0, len(feature_importances_))\n    plt.bar(x_pos, feature_importances_[name])\n    plt.xticks(x_pos, feature_importances_.index, rotation=45, ha=\"right\")\n    plt.title('Feature importances from '+ name)\n    plt.show()\n    \n    # export the ranking of feature importance table\n    for x in feature_importances_.index: feature_importances_.loc[x][name] = len(feature_importances_) - len(feature_importances_[:x])\n    models_feat_imp = feature_importances_.join(models_feat_imp)\n    return models_feat_imp","2b8076a6":"# only get top 5 models within boosting models\nselect_models = ['XGB', 'LightGBM', 'CatBoost', 'Random Forest', 'AdaBoost', 'Gradient Boosting']\ntop_models = models_comp.loc[models_comp[\"classifier\"].isin(select_models)].nlargest(5, \"test_f1\")[\"classifier\"].values\nmodels_feat_imp = pd.DataFrame()\n\nfor key in top_models:\n    models_feat_imp = feature_imp_viz(key, dict_models[key]['model'], models_feat_imp, X_train)","a23da559":"# We use a simple ranking mechanism on feature importance of each model - higher number is more important. \nmodels_feat_imp[\"Sum\"] = models_feat_imp.sum(axis=1)\nmodels_feat_imp.sort_values(by=\"Sum\", ascending=False)","84972000":"<a id=\"voting_classifiers\"><\/a>\n\n# Voting Classifiers\n[Back To Table of Contents](#top_section)","8a4f178c":"<a id=\"Read_and_explore_data\"><\/a>\n\n# Read and explore data\n[Back To Table of Contents](#top_section)","1c6d2bab":"<a id=\"feature_imp\"><\/a>\n\n# Extra on Feature Importance\n[Back To Table of Contents](#top_section)","fd1e3254":"<a id=\"eda\"><\/a>\n\n# EDA\n[Back To Table of Contents](#top_section)","73d89d87":"<a id=\"Feature_Engineering\"><\/a>\n\n# Feature Engineering\n[Back To Table of Contents](#top_section)","39e33d5d":"For the other catergorical features, we will use the Weight of Evidence Encoder to transform to numerical. ","dfc5f54b":"<a id=\"basic_classifiers\"><\/a>\n\n# Some Basic Classifiers\n[Back To Table of Contents](#top_section)","0969a5a9":"<a id=\"Introduction\"><\/a>\n# Introduction\nThe context of the Bank Marketing Dataset: The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y). Further information about the dataset and features details can be found in [https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing)\n\n* The purposes of this notebook are to experiment on the following topics:\n    - Imbalanced Binary Classification.\n    - Solving Data Leakage via Cross-Validation\n    - Training pipeline\n    - Hyperparameter optimization\n    - Ensemble Learning Binary Classification Models (Voting, Bagging, Boosting and Stacking Classifiers)\n* Full Model Training pipeline:\n    - WOE Encoding for categorical features (and Feature Scaling)\n    - SMOTE technique for Imbalanced Classification\n    - Stratified K-Folds Cross-Validation\n    - GridSearchCV with experiments on different single classifiers and ensemble learning classifiers\n    - Trained models will be stored as list as it could be reused within the notebook or model export for future uses.\n* The output will compare each model with different metrics such as Accuracy, Precision, ROC AUC, F1, Fbeta, and Matthews Correlation Coefficient (MCC).\n* Some factors should be considered as the model should minimize the false positive (customers who have not deposited but classify as yes) as the telesales would miss the potential customers. However, the other perspective is that if you keep calling the customers, who already have their deposit, it is aslo costing the bank - depending on the bank's financial cost model.\n* In short, the F1 macro score will be the primary comparison metric as it would be suitable for the imbalanced binary classification problem. Other metrics will be considered depending on different preferences.\n\nThe notebook was built up in a modular way, which could be expanded for further different experiments such as other Categorical Encoders, Sampling techniques, or various hyperparameter optimizations such as Random search, Bayesian, etc.","89745af4":"There are no missing values in the dataset.","5026d134":"<a id=\"stacking_classifiers\"><\/a>\n\n# Stacking Classifiers\n[Back To Table of Contents](#top_section)","e58e51cf":"<a id=\"Init_exp\"><\/a>\n\n# Initial Experiment\n[Back To Table of Contents](#top_section)","501912d4":"<a id=\"top_section\"><\/a>\n<div align='center'><font size=\"6\" color=\"#000000\"><b>Bank Marketing - Ensemble Learning Pipeline<\/b><\/font><\/div>\n<hr>\n<div align='center'><font size=\"4\" color=\"#000000\">Ensemble Learning Pipeline Experiments on Imbalanced Binary Classification Problem<\/font><\/div>\n<hr>\n\n# Table of Contents\n* [Introduction](#Introduction)\n* [Read and explore data](#Read_and_explore_data)\n* [EDA](#eda)\n* [Feature Engineering](#Feature_Engineering)\n* [Initial Experiment](#Init_exp)\n* [Some Basic Classifiers](#basic_classifiers)\n* [Voting Classifiers](#voting_classifiers)\n* [Bagging Classifiers](#bagging_classifiers)\n* [Boosting Classifiers](#boosting_classifiers)\n* [Stacking Classifiers](#stacking_classifiers)\n* [Extra on Feature Importanceg](#feature_imp)\n* [References](#References)","94d5318a":"<a id=\"References\"><\/a>\n\n# References:\n* https:\/\/www.kaggle.com\/janiobachmann\/bank-marketing-campaign-opening-a-term-deposit\n* https:\/\/www.kaggle.com\/tilii7\/hyperparameter-grid-search-with-xgboost\n* http:\/\/ataspinar.com\/2017\/05\/26\/classification-with-scikit-learn\/\n* https:\/\/machinelearningmastery.com\/tour-of-evaluation-metrics-for-imbalanced-classification\/\n* https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/ \n* https:\/\/kiwidamien.github.io\/how-to-do-cross-validation-when-upsampling-data.html\n* https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/EnsembleVoteClassifier\/ \n* https:\/\/coderzcolumn.com\/tutorials\/machine-learning\/scikit-learn-sklearn-ensemble-learning-bagging-and-random-forests\n\n#### I really appreciate your feedbacks, there would be some areas can be fixed and improved.\n### If you liked my work please Upvote!\n[Back To Table of Contents](#top_section)","196cedbc":"<a id=\"bagging_classifiers\"><\/a>\n\n# Bagging Classifiers\n[Back To Table of Contents](#top_section)","05df9a40":"<a id=\"boosting_classifiers\"><\/a>\n\n# Boosting Classifiers\n[Back To Table of Contents](#top_section)"}}