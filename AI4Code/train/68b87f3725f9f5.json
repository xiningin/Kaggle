{"cell_type":{"9fe8809f":"code","258c6431":"code","fe722272":"code","1fdfa214":"code","f8608295":"code","7ff161c3":"code","f7ba5c87":"code","354cf1da":"code","ec83a729":"code","152dc322":"code","d0b5e180":"code","7147e63d":"code","10e45dab":"code","228a588c":"code","c4dcb36d":"code","3aae3e44":"code","bf67d36e":"code","5b53fb61":"code","0eb1ee84":"code","41a7c761":"code","ddf30c61":"code","8e079a55":"code","61af1b33":"code","f37bdd5a":"code","44b8aeb9":"code","0adf081d":"code","27d738d7":"code","7f9dde51":"code","0494cb8e":"markdown","8e34bc12":"markdown","ec5ce818":"markdown","a6bcb8b5":"markdown","b5d9ec40":"markdown","b106c3d1":"markdown","3a870e35":"markdown","d31e80ef":"markdown","4205861e":"markdown","42058dd9":"markdown","f6288b84":"markdown","1fa40706":"markdown","5c25a427":"markdown","21088886":"markdown","a80c651f":"markdown","921573c6":"markdown","5459ab77":"markdown","4fca6152":"markdown"},"source":{"9fe8809f":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom itertools import combinations\nimport cv2\nimport os\nimport random\nfrom time import time\nfrom math import sqrt\nfrom tqdm import tqdm\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom keras.models import load_model\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\nfrom seaborn import kdeplot","258c6431":"prerun_data = pd.read_csv(\"..\/input\/calculating-dlib-facial-landmarks-for-rfiw-images\/facial_landmark_coordinates.csv\")\nprerun_data.head()","fe722272":"point_names = [\"jaw\"+str(i) for i in range(17)]+[\"browL\"+str(i) for i in range(5)]+\\\n            [\"browR\"+str(i) for i in range(5)]+[\"nRidge\"+str(i) for i in range(4)]+\\\n            [\"nTip\"+str(i) for i in range(5)]+[\"eyeL\"+str(i) for i in range(6)]+\\\n            [\"eyeR\"+str(i) for i in range(6)]+[\"lipOut\"+str(i) for i in range(12)]+\\\n            [\"lipIn\"+str(i) for i in range(8)]\n\nfor i in point_names:\n    x_mean = prerun_data[i +'X'].mean()\n    y_mean = prerun_data[i +'Y'].mean()\n    col_name = \"d2Avg(\" + i + \")\"\n    prerun_data[col_name] = np.sqrt(((prerun_data[i+'X']-x_mean)**2)+((prerun_data[i+'Y']-y_mean)**2))\n    \nprerun_data.iloc[0:5,137:205]","1fdfa214":"features = ['jaw0','jaw8','jaw16','browL0','browL4','browR0',\n            'browR4','nRidge3','nTip0','nTip2','nTip4',\n            'eyeL0','eyeL3','eyeR0','eyeR3','lipOut0','lipOut3',\n            'lipOut6','lipOut9']\nfeatures = list(combinations(features, 2))\n\nfor i in range(0, len(features)):\n    name1 = features[i][0]\n    name2 = features[i][1]\n    col_name = \"dist(\" + name1 + \",\" + name2 + \")\"\n    \n    x1 = prerun_data[name1+'X']\n    y1 = prerun_data[name1+'Y']\n    x2 = prerun_data[name2+'X']\n    y2 = prerun_data[name2+'Y']\n    \n    prerun_data[col_name] = np.sqrt(((x1-x2)**2) + ((y1-y2)**2))\n    \nprerun_data.iloc[0:5,205:376]","f8608295":"model_path = '..\/input\/facenet-keras\/facenet_keras.h5'\nfacenet_model = load_model(model_path)","7ff161c3":"def prewhiten(x):\n    if x.ndim == 4:\n        axis = (1, 2, 3)\n        size = x[0].size\n    elif x.ndim == 3:\n        axis = (0, 1, 2)\n        size = x.size\n    else:\n        raise ValueError('Dimension should be 3 or 4')\n\n    mean = np.mean(x, axis=axis, keepdims=True)\n    std = np.std(x, axis=axis, keepdims=True)\n    std_adj = np.maximum(std, 1.0\/np.sqrt(size))\n    y = (x - mean) \/ std_adj\n    return y\n\ndef l2_normalize(x, axis=-1, epsilon=1e-10):\n    output = x \/ np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n    return output\n\ndef load_and_align_images(filepaths, image_size = 160):\n    \n    aligned_images = []\n    for filepath in filepaths:\n        img = imread(filepath)\n        aligned = resize(img, (image_size, image_size), mode='reflect')\n        aligned_images.append(aligned)\n            \n    return np.array(aligned_images)\n\ndef calc_embs(filepaths, margin=10, batch_size=512):\n    pd = []\n    for start in tqdm(range(0, len(filepaths), batch_size)):\n        aligned_images = prewhiten(load_and_align_images(filepaths[start:start+batch_size]))\n        pd.append(facenet_model.predict_on_batch(aligned_images))\n    embs = l2_normalize(np.concatenate(pd))\n\n    return embs\n\nprint(\"Helper functions compiled succesfully.\")","f7ba5c87":"facenet_embs = calc_embs(prerun_data[\"path\"])\nfacenet_embs = pd.DataFrame(facenet_embs, columns=[\"facenet_\"+str(i) for i in range(128)])\nprerun_data = pd.concat([prerun_data, facenet_embs], axis=1)\nprerun_data.iloc[0:5,376:504]","354cf1da":"def calc_distance(embs_img1, embs_img2):\n    dists = distance.euclidean(embs_img1, embs_img2)\n    return dists","ec83a729":"!pip install git+https:\/\/github.com\/rcmalli\/keras-vggface.git","152dc322":"from keras_vggface.vggface import VGGFace\nvggface_model = VGGFace(include_top=False, input_shape=(160, 160, 3), pooling='avg')","d0b5e180":"def calc_embs_vggface(filepaths, margin=10, batch_size=64):\n    pd = []\n    for start in tqdm(range(0, len(filepaths), batch_size)):\n        aligned_images = prewhiten(load_and_align_images(filepaths[start:start+batch_size]))\n        pd.append(vggface_model.predict_on_batch(aligned_images))\n    embs = l2_normalize(np.concatenate(pd))\n\n    return embs\n\nprint(\"Helper functions successfully compiled.\")","7147e63d":"vggface_embs = calc_embs_vggface(prerun_data[\"path\"])\nvggface_embs = pd.DataFrame(vggface_embs, columns=[\"vggface_\"+str(i) for i in range(512)])\nprerun_data = pd.concat([prerun_data, vggface_embs], axis=1)\nprerun_data.iloc[0:5,505:1017]","10e45dab":"f = plt.figure(figsize=(13,3))\nax1 = f.add_subplot(131)\nax2 = f.add_subplot(132)\nax3 = f.add_subplot(133)\n\nax1.hist(prerun_data['browL2X'].dropna(), 25, alpha=0.75)\nax1.set_title('Left Brow, Point 2, X Coord')\nax2.hist(prerun_data['dist(jaw0,jaw8)'].dropna(),25, alpha=0.75)\nax2.set_title('Dist. from Jaw0 to Jaw8')\nax3.hist(prerun_data['facenet_12'].dropna(), 25, alpha=0.75)\nax3.set_title('Facenet Embedding 12')\nplt.show()","228a588c":"scaler = preprocessing.StandardScaler()\n\nscaled_data = scaler.fit_transform(prerun_data.iloc[:,1:])\nscaled_data = pd.DataFrame(scaled_data, columns = prerun_data.columns[1:])\nscaled_data.index = prerun_data.index\nscaled_data.insert(loc=0, column=\"path\", value=prerun_data[\"path\"])","c4dcb36d":"random.seed(6242)\n\ntrain_folder = '..\/input\/recognizing-faces-in-the-wild\/train\/'\nfamily_names = sorted(os.listdir(train_folder))\nkin_pairs = pd.read_csv('..\/input\/recognizing-faces-in-the-wild\/train_relationships.csv')\n\nkin_files = []\nfor i in range(0, len(kin_pairs.index)):\n    if os.path.exists(train_folder+kin_pairs.iloc[i,0]) and os.path.exists(train_folder+kin_pairs.iloc[i,1]):\n        pair = [kin_pairs.iloc[i,0], kin_pairs.iloc[i,1], 1]\n        kin_files.append(pair)\nfor i in range(0, len(kin_files), 2):\n    x = kin_files[i][0]\n    kin_files[i][0] = kin_files[i][1]\n    kin_files[i][1] = x\n        \nrandom_files = []\nfor i in range(len(kin_files)):\n    x, y = 0, 0\n    while x == y:\n        x = random.randint(0,469)\n        y = random.randint(0,469)\n\n    fam1 = family_names[x]\n    fam2 = family_names[y]\n\n    fam1_folder = train_folder + fam1 + '\/'\n    fam1_members = sorted(os.listdir(fam1_folder))\n    m = random.randint(0,len(fam1_members)-1)\n    pers1 = fam1_members[m]\n\n    fam2_folder = train_folder + fam2 + '\/'\n    fam2_members = sorted(os.listdir(fam2_folder))\n    n = random.randint(0,len(fam2_members)-1)\n    pers2 = fam2_members[n]\n\n    pair = [fam1+'\/' +pers1, fam2+'\/'+pers2, 0]\n    random_files.append(pair)\n\nimage_files = kin_files + random_files\nprint(\"There are \", len(image_files), \" pairs of individuals in the training data\")","3aae3e44":"print(\"There are\", pd.isnull(scaled_data).sum()[1], \"incorrectly detected images.\")  ","bf67d36e":"col_names = []\nimages_per_person = 3\nlandmark_data = pd.DataFrame(\n    columns = [name + \"_A\" for name in scaled_data.columns[1:]] + \\\n        [name + \"_B\" for name in scaled_data.columns[1:]] + \\\n        [\"dist(Facenet)\", \"dist(VGGFace)\", \"Kin\"])\nemb_cols = [\"facenet_\" + str(i) for i in range(128)]\nemb_cols_vggface = [\"vggface_\" + str(i) for i in range(512)]\n\nfor i in range(len(image_files)):\n    rowEntry = []\n    pers1 = image_files[i][0]\n    pers2 = image_files[i][1]\n    images1 = sorted(os.listdir(train_folder+pers1+'\/'))\n    images2 = sorted(os.listdir(train_folder+pers2+'\/'))\n    num_imgs1 = len(images1)\n    num_imgs2 = len(images2)\n    \n    if num_imgs1 == 0:\n        continue\n    elif num_imgs1 > images_per_person:\n        num_imgs1 = images_per_person\n        \n    if num_imgs2 == 0:\n        continue\n    elif num_imgs2 > images_per_person:\n        num_imgs2 = images_per_person\n    \n    for j in range(0,num_imgs1):\n        for k in range(j, num_imgs2):\n            img1 = train_folder+pers1+'\/'+images1[j]\n            img2 = train_folder+pers2+'\/'+images2[k]\n            points1 = scaled_data.loc[scaled_data['path'] == img1]\n            points2 = scaled_data.loc[scaled_data['path'] == img2]\n            points1 = points1.values.tolist()[0][1:]\n            points2 = points2.values.tolist()[0][1:]\n            \n            embs_img1 = prerun_data.loc[prerun_data[\"path\"]==img1,emb_cols]\n            embs_img2 = prerun_data.loc[prerun_data[\"path\"]==img2,emb_cols]\n            emb_dist = calc_distance(embs_img1,embs_img2)\n            \n            embs_img1_vgg = prerun_data.loc[prerun_data[\"path\"] == img1, emb_cols_vggface]\n            embs_img2_vgg = prerun_data.loc[prerun_data[\"path\"] == img2, emb_cols_vggface]\n            emb_dist_vggface = calc_distance(embs_img1_vgg, embs_img2_vgg)\n            \n            rowEntry = points1 + points2 + [emb_dist, emb_dist_vggface] + [image_files[i][2]]\n            if len(rowEntry) == len(landmark_data.columns):\n                landmark_data.loc[len(landmark_data)] = rowEntry\n\ndist_scaler = preprocessing.StandardScaler()\ndistance_col = np.asarray(landmark_data[\"dist(Facenet)\"].tolist()).reshape(-1,1)\nscaled_dists = dist_scaler.fit_transform(distance_col)\nlandmark_data[\"dist(Facenet)\"] = scaled_dists\n\nvggface_dist_scaler = preprocessing.StandardScaler()\ndistance_col = np.asarray(landmark_data[\"dist(VGGFace)\"].tolist()).reshape(-1,1)\nscaled_dists = vggface_dist_scaler.fit_transform(distance_col)\nlandmark_data[\"dist(VGGFace)\"] = scaled_dists\n\nprint(\"We now have\", len(landmark_data.index), \"pairs of faces.\")","5b53fb61":"lm_size = len(landmark_data.index)\nlandmark_data = landmark_data.dropna()\nprint(\"There are now\", len(landmark_data.index), \"pairs left, a loss of\", lm_size-len(landmark_data.index))","0eb1ee84":"start = time()\nlogReg = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", random_state=8119)\nlogReg.fit(landmark_data.drop(\"Kin\", axis=1), landmark_data[\"Kin\"].astype(\"int\"))\nend = time()\n\nall_coord_time = end - start","41a7c761":"print(\"That took about\", int(all_coord_time\/60), \"minutes.\", sum(logReg.coef_.reshape(len(landmark_data.columns)-1,) == 0), \"coefficients were reduced to zero.\")\nprint(\"Lets train it again with only 250 features and see if we can reduce that time.\")","ddf30c61":"coefficients = logReg.coef_.reshape(len(landmark_data.columns)-1,)\nk = len(coefficients) - 250\ndrop_cols = np.argpartition(coefficients, k)\ndrop_cols = drop_cols[:k]\n\ntop250_data = landmark_data.drop(landmark_data.columns[(drop_cols)], axis=1)\n\nstart = time()\nlogReg250 = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", random_state=8119)\nlogReg250.fit(top250_data.drop(\"Kin\", axis=1), top250_data[\"Kin\"].astype(\"int\"))\nend = time()\n\ncoord_time_250 = end - start","8e079a55":"print(\"That took about\", int(coord_time_250\/60), \"minutes, much faster. Let's see how it scores.\")","61af1b33":"test_pairs = pd.read_csv('..\/input\/recognizing-faces-in-the-wild\/sample_submission.csv')\ntest_pairs_t250 = test_pairs.copy()\n\ntest_folder = '..\/input\/recognizing-faces-in-the-wild\/test\/'\ndetection_errors = 0\n\nfor i in range(len(test_pairs)):\n    \n    rowEntry = []\n    missing_vals = 0\n    \n    pair = test_pairs.iloc[i,0]\n    \n    pers1 = pair.split('-')[0]\n    pers2 = pair.split('-')[1]\n    \n    pers1 = test_folder+pers1\n    points1 = scaled_data.loc[scaled_data['path'] == pers1]\n    points1 = points1.drop('path', axis=1)\n    points1.reset_index(drop=True, inplace=True)\n    if points1.isnull().values.any():\n        missing_vals = 1\n    \n    pers2 = test_folder+pers2\n    points2 = scaled_data.loc[scaled_data['path'] == pers2]\n    points2 = points2.drop('path', axis=1)\n    points2.reset_index(drop=True, inplace=True)\n    if points2.isnull().values.any():\n        missing_vals = 1\n    \n    embs_img1 = prerun_data.loc[prerun_data[\"path\"] == pers1, emb_cols]\n    embs_img2 = prerun_data.loc[prerun_data[\"path\"] == pers2, emb_cols]\n    emb_dist = calc_distance(embs_img1, embs_img2)\n    emb_dist = (emb_dist - dist_scaler.mean_[0])\/dist_scaler.scale_[0]\n    emb_dist = pd.DataFrame([emb_dist], columns=[\"dist(Facenet)\"])\n    \n    embs_img1_vgg = prerun_data.loc[prerun_data[\"path\"] == img1, emb_cols_vggface]\n    embs_img2_vgg = prerun_data.loc[prerun_data[\"path\"] == img2, emb_cols_vggface]\n    emb_dist_vggface = calc_distance(embs_img1_vgg, embs_img2_vgg)\n    emb_dist_vggface = (emb_dist_vggface - vggface_dist_scaler.mean_[0])\/vggface_dist_scaler.scale_[0]\n    emb_dist_vggface = pd.DataFrame([emb_dist_vggface], columns=[\"dist(VGGFace)\"])\n    \n    rowEntry = pd.concat([points1, points2, emb_dist, emb_dist_vggface], axis=1, ignore_index = True)\n    rowEntry.columns = [name + \"_A\" for name in scaled_data.columns[1:]] + \\\n                        [name + \"_B\" for name in scaled_data.columns[1:]] + \\\n                        [\"dist(Facenet)\", \"dist(VGGFace)\"]\n    \n    rowEntry_t250 = rowEntry.drop(rowEntry.columns[(drop_cols)], axis=1)\n        \n    if missing_vals == 0:\n        test_pairs.iloc[i,1] = logReg.predict_proba(rowEntry)[0][0]\n        test_pairs_t250.iloc[i,1] = logReg250.predict_proba(rowEntry_t250)[0][0]\n    else:\n        detection_errors += 1\n        test_pairs.iloc[i,1] = 0.5\n        test_pairs_t250.iloc[i,1] = 0.5\n    \nif(logReg.classes_[0] == 0): # Probabilities depend on the order of the classes in the model\n    test_pairs[\"is_related\"] = [1-x for x in test_pairs[\"is_related\"]]\n    test_pairs_t250[\"is_related\"] = [1-x for x in test_pairs_t250[\"is_related\"]]\n    \nprint(detection_errors, \"pairs have been lost due to errors with dLib's facial detection.\")\nprint(\"All missing cases (\", round(detection_errors\/len(test_pairs.index),3),\"%) have been set to 0.5.\")","f37bdd5a":"test_pairs.to_csv('submission_file.csv', index=False)\ntest_pairs_t250.to_csv('submission_file_t250.csv', index=False)","44b8aeb9":"feature_coeffs = pd.DataFrame()\nfeature_coeffs[\"feature\"] = landmark_data.columns[:-1]\nfeature_coeffs[\"coefficient\"] = logReg.coef_[0]\nfeature_coeffs.to_csv('feature_coefficients.csv', index=False)\n\nsorted_coeffs = feature_coeffs.reindex(feature_coeffs.coefficient.abs().sort_values(ascending=False).index)\n\nzero_coeffs = feature_coeffs[feature_coeffs[\"coefficient\"]==0]\nbest_coeffs = sorted_coeffs.iloc[0:250,:].reset_index(drop=True)\nmiddle_coeffs = sorted_coeffs.iloc[250:len(feature_coeffs.index)-len(zero_coeffs.index),:]","0adf081d":"objects = (\"Zero\", \"Middle\", \"Top-250\")\n\nf = plt.figure(figsize=(13,8))\nax = f.add_subplot(231)\nax2 = f.add_subplot(232)\nax3 = f.add_subplot(233)\nax4 = f.add_subplot(234)\nax5 = f.add_subplot(235)\n\ndata = [sum([\"X_\" in name or \"Y_\" in name for name in zero_coeffs[\"feature\"]]),\n        sum([\"X_\" in name or \"Y_\" in name for name in middle_coeffs[\"feature\"]]),\n        sum([\"X_\" in name or \"Y_\" in name for name in best_coeffs[\"feature\"]])]\nax.bar(objects, data, align='center', alpha=0.75)\nax.set_title('Raw Coordinates')\n\ndata = [sum([\"d2Avg(\" in name for name in zero_coeffs[\"feature\"]]),\n        sum([\"d2Avg(\" in name for name in middle_coeffs[\"feature\"]]),\n        sum([\"d2Avg(\" in name for name in best_coeffs[\"feature\"]])]\nax2.bar(objects, data, align='center', alpha=0.75)\nax2.set_title('Dist. to Average Point')\n\ndata = [sum([\"dist(\" in name for name in zero_coeffs[\"feature\"]]),\n        sum([\"dist(\" in name for name in middle_coeffs[\"feature\"]]),\n        sum([\"dist(\" in name for name in best_coeffs[\"feature\"]])]\nax3.bar(objects, data, align='center', alpha=0.75)\nax3.set_title('Point to Point Dist.')\n\ndata = [sum([\"facenet_\" in name for name in zero_coeffs[\"feature\"]]),\n        sum([\"facenet_\" in name for name in middle_coeffs[\"feature\"]]),\n        sum([\"facenet_\" in name for name in best_coeffs[\"feature\"]])]\nax4.bar(objects, data, align='center', alpha=0.75)\nax4.set_title('Facenet Embeddings')\n\ndata = [sum([\"vggface_\" in name for name in zero_coeffs[\"feature\"]]),\n        sum([\"vggface_\" in name for name in middle_coeffs[\"feature\"]]),\n        sum([\"vggface_\" in name for name in best_coeffs[\"feature\"]])]\nax5.bar(objects, data, align='center', alpha=0.75)\nax5.set_title('VGGFace Embeddings')","27d738d7":"feature_coeffs.iloc[2030:2032,:]","7f9dde51":"best_coeffs[abs(best_coeffs[\"coefficient\"]) > 1]","0494cb8e":"#### The Best of The Best\n\nHere are the features with a coefficient greater than 1:","8e34bc12":"<div id=\"conc\"><hr><\/div>\n\n## Conclusion\n\n### Results\n\nThere are a number of simple improvements that could be made to this kernel to improve its standings in the competition. From the beginning, errors with Dlib's pretrained models have forced me to throw away valuable data and blindly guess on around 1% of the test cases. By implementing a more accurate facial recognition model, that problem could be solved. I noted earlier that my data collection process was flawed; a more robust method could be devised. I declined to focus on these issues and others because they effect more the competition score rather than my high level analysis of feature generation. With this kernel I seek not to win the competition but to learn and create.\n\nRegardless of the overall competition score, the question is: which features were strong and worth investigating further and which features are of little use. Here I will show and discuss general trends and only a few of the top features, but I will create a .csv file with all of the features and their respective coefficients for further examination.\n\n#### Coefficient Spread\n\nI examine the features by dividing them into three groups: those that were reduced to zero by Lasso, those in the top-250 that were selected to train on the secondary model, and those in the middle that contribute only weakly. I define the 'influence' or 'strength' of a feature by the absolute value of its coefficient because the coefficient determines the feature's effect on the model's prediction. Below I create those three divisions as well as write a .csv file with all the features and coefficients for your further reference.","ec5ce818":"Due the errors with the Dlib's facial recognition, a number of the training pairs have missing values. Let's remove those now.","a6bcb8b5":"Now that we have an indication of what features are useful, lets train the model again with only the top 250 features. Hopefully we will only have a limited drop in score for a large gain in speed!","b5d9ec40":"#### Point to Point Distance\nIdeally I would calculate the distance between all possible pairs of the 68 points, but that would result in 2278 features, too many for the scope of this kernel. I also believe that many of those features would be trivial. For that reason I will select a number of points that I think will provide the most valuable information. Those points are the tips and bottom of the jawline, ends of both eyebrows, corners of the eyes, the top of the nasal bridge, the ends and middle of the nasal tip, and the top, bottom, and corners of the outer lips, for a total of 19 points. 19 points creates 171 combinations of two, meaning 171 features per face and 342 per pair.","b106c3d1":"# Feature Generation and Analysis with Facial Landmarks and Pretrained Model Embeddings\n\n<div id=\"intro\"><hr><\/div>\n\n## Introduction\n\nIn this kernel I generate a large number of features using Dlib's facial landmark detector and the pretrained Facenet and VGGFace models in order to identify a smaller group of highly influential features. After using a series of simple transformations to create new features, I use a logistic regression model with a l1 regularization parameter to indentify the most useful features. Feel free to skip to the conclusion where I discuss the results of the kernel and to use any of the new features in your own models. The kernel is organized as follows.\n\n* [Introduction](#intro)\n* [Feature Generation](#featgen)\n* [Feature Selection](#featsel)\n* [Submission](#sub)\n* [Conclusion](#conc)\n\n<div id=\"featgen\"><hr><\/div>\n\n## Feature Generation\n\n### Facial Landmarks\n\nThe first set of features will stem from facial landmark coordinates calculated using Dlib's face detector and facial landmark predictor. I initialized the predictor with weights pretrained on the iBUG 300-W dataset, available for research purposes only, which can be downloaded [here](http:\/\/dlib.net\/files\/shape_predictor_68_face_landmarks.dat.bz2). In order to speed up the execution and elevate the readability of this kernel I have calculated the coordinates for every image in the training and testing data in a seperate kernel. For more information on DLib and the example code that I used, visit the [facial landmark tutorial from pyimagesearch](https:\/\/www.pyimagesearch.com\/2017\/04\/03\/facial-landmarks-dlib-opencv-python\/). Beyond the raw coordinates, I will create features based on the distance to the average point, distances from points on a person's face, and distances from points across faces.\n\n#### Raw Coordinates\n\nDLib's facial landmark predictor generates 68 points across a person's face. There are 9 specific landmarks: the jawline, left eyebrow, right eyebrow, nasal ridge, nasal tip, left eye, right eye, outer lips, and inner lips, with a variable number of points for each. I have named each point in a landmark-point-axis format (Ex: jaw0X, lipOut11Y), with point numbers in the order shown below (image courtesy of [ibug](https:\/\/ibug.doc.ic.ac.uk\/resources\/facial-point-annotations\/)):\n<img src=\"https:\/\/ibug.doc.ic.ac.uk\/media\/uploads\/images\/annotpics\/figure_68_markup.jpg\" width=\"50%\"\/>\n<br>\nI have already calculated the landmark coordinates for every image so here I load the required packages and read in the csv file. With 68 points we have 136 features per face and 272 per pair.","3a870e35":"### Image Pairing\n\nThe competition provided a list of individuals who have a kinship relation, but obviously more work needs to be done before our data are ready to be modeled. First, I format the given relationships and then generate an equal number of non-kinship pairs by randomly selecting individuals from different families. This method has a few flaws, including the fact that it will never create a pair of individuals from within a family who do not have a kinship relation. If I were attempting to compete for first place I would devise a more thorough data-cleaning pipeline, but my aim is just to provide a high level look at what features seem to be powerful so I will stick with this more simplistic method for now.","d31e80ef":"Over half of the best features are point-to-point distances and most of the rest are raw coordinates. The odd feature out is the Facenet embedding distance. The presence of point-to-point features does not suprise me, as I chose points that had a high likelihood of providing valuable information, but having such a large presence of raw coordinates is intriguing, as I intended for them to be the building blocks of more sophisticated features rather than intergral parts of the model. This odd collection of features demonstrates how differently machine learning models think from humans.\n\nIf you read through my whole kernel or even skipped to the end, I appreciate and thank you for your interest. Feel free to use any of the features I created or try to create some of your own! This was my first major kernel on Kaggle and would love your feedback in the comments or with an upvote.\n\n### Sources\n\n* [Facial landmark tutorial from pyimagesearch](https:\/\/www.pyimagesearch.com\/2017\/04\/03\/facial-landmarks-dlib-opencv-python\/)\n* [ibug](https:\/\/ibug.doc.ic.ac.uk\/resources\/facial-point-annotations\/)\n* [Original Facenet Article](https:\/\/arxiv.org\/abs\/1503.03832)\n* [Facenet Keras Dataset from Khoi Nguyen](https:\/\/www.kaggle.com\/suicaokhoailang\/facenet-keras)\n* [Facenet Baseline in Keras from Khoi Nguyen](https:\/\/www.kaggle.com\/suicaokhoailang\/facenet-baseline-in-keras-0-749-lb)\n* [Alexander Teplyuk's VGGFace Kernel](https:\/\/www.kaggle.com\/ateplyuk\/vggface-baseline-in-keras\/data)\n* [rcmalli's github repository](https:\/\/github.com\/rcmalli\/keras-vggface\/releases)\n* [Feature Scaling from Ben Alex Keen](http:\/\/benalexkeen.com\/feature-scaling-with-scikit-learn\/)","4205861e":"The plots below show the distribution of coefficients across the three catagories for each type of feature. The middle-ground group has 1384 coefficients while the top has 250 and the bottom has 398, so we should expect each graph to have a large middle bar. That is clearly not the case, however. While the 'Distance to the Avergae Point' features and VGGFace embeddings have largely mediocre strengths, the raw coordinates and point to point distances have a lot of bad features and a lot of strong features. The Facenet embeddings mostly fall in the bad and middle sections.","42058dd9":"### Pretrained Models\n\n#### Facenet Embeddings\n\nFacenet is a powerful facial recognition and clustering model that has been trained on the Labeled Faces in the Wild dataset and has proven to be effective in this competition. Based on the popular kernel from Khoi Nguyen, I use the pretrained facenet model to calculate embeddings for each image. The computed embeddings are 128-dimensional vectors, meaning 128 features for each face and 256 for each pair. ","f6288b84":"Given the success of pretrained-models in the competition, it is odd that the calculated embeddings perform so poorly. That said, collecting all the embeddings into a single face-to-face distance feature proved to be more successful. The Facenet embedding distance is one of the top features, and significantly more influential than the VGGFace distance feature. This discrepancy is especially interesting when you consider how poorly the Facenet embeddings performed on their own.","1fa40706":"Now that I have the pairs of individuals, I collect the features for each image into a single observation, effectively doubling the number of features. Individuals may have more than one picture, so the training data has many more observations than the number of pairs I created above. Within the code below I also calculate the Facenet and VGGFace embedding distance for each image pair and scale the data using the StandardScaler. Another issue with the data is that Dlib's facial recognition model occasionaly cannot detect a face in the images, meaning some pairs will have to be discarded as the logistic regression model cannot handle missing values.","5c25a427":"### Modeling\n\nOur data is finally wrangled and it is time to train the logistic regression model. Training sklearn's logistic regression model will provide coefficients for each feature that will estimate how important each individual feature is. To demonstrate the benefits of using a concentrated group of features, I will train another model with the top-250 most influential coefficients and post the results in the comments at the bottom of the kernel.","21088886":"#### Distance to Average Point\n\nHere I find the average X and Y values for each of the 68 points that have been calculated by DLib's facial landmark predictor. Then for every face I find the distance of their points to the corresponding average. An average for each point leads to 68 new features per face and 136 per pair.","a80c651f":"### Facenet Embedding Distance\nMany high-scoring submissions have been as simple as finding the distance between faces based on embeddings. Here I do the same thing, but calculating the distance between two images requires the embeddings of both images, meaning that we need to create our training pairs first. I create the pairs in the 'Feature Selection' section of the kernel, so I will just write a function that can be used for both model training and generating the submissions. This will only create 1 new feature per pair.","921573c6":"### VGGFace Embeddings and Distance\n\nThe VGGFace model has been another popular model in this competition and seen great success, such as [this kernel](https:\/\/www.kaggle.com\/ateplyuk\/vggface-baseline-in-keras\/data) from Alexander Teplyuk which was my inspiration for much of this section. Just like the Facenet model, I calculate the embeddings for each face as well as the distance between image pairs. I have put the pretrained model into a private Kaggle dataset, but you can easily download it from [rcmalli's github repository](https:\/\/github.com\/rcmalli\/keras-vggface\/releases), as I did. The distance function created in the Facenet section can be used for the VGGFace model as well. The embeddings are 512-dimensional vectors, meaning 512 features per face and 1024 per pair, as well as another single feature per pair from the embedding distance.","5459ab77":"<div id=\"featsel\"><hr><\/div>\n\n## Feature Selection\n\nWe have a lot of features and lot of observations. Cutting down the computational time and power needed to train a model may be essential for industry applications or to beat Kaggle's 6 hour runtime limit. Using fewer observations is one way to accomplish this, but doing so may cause models to overfit. Trimming features, however, can be done in such a way that model performance is largely unaffected, with much lower computational requirements. A smaller, concentrated group of powerful features may be just as effective as a larger, noisier group. I will use Lasso regularization within a logistic regression model to identify meaningful features. According to the [Stanford Statistics Department](http:\/\/statweb.stanford.edu\/~tibs\/lasso.html):\n> Lasso is a shrinkage and selection method for linear regression. It minimizes the usual sum of squared errors, with a bound on the sum of the absolute values of the coefficients. \n\nI have chosen Lasso because it tends to prefer solutions with fewer non-zero features compared to other regularization techniques like Ridge or Elastic-Net. Before we can train our model we need to complete just a few data-cleaning tasks.\n\n### Scaling\n\nLogistic regression models benefit greatly from normalized data, so it is neccessary that I scale the features I have created. Most of the features are already fairly normal, as you can see below, so I will use a basic z-score scaler in scikit's StandardScaler. ","4fca6152":"<div id=\"sub\"><hr><\/div>\n\n## Submission\n\nNow that the models have been trained a competition submission can be created. This code is very similar to that in the feature selection section except that the pairs come from the provided sample submission file."}}