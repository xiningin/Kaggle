{"cell_type":{"6309672a":"code","cc7fa698":"code","2825eaf0":"code","f5dd0a9e":"code","b232658a":"code","9ddd61e9":"code","cff8dbff":"code","d0dbae2c":"code","56e6c592":"code","77172910":"code","c6a27040":"code","9f4122f5":"code","eac7e39e":"code","d1b140b3":"code","21dbc1ae":"code","8360cc9c":"code","fc200a6a":"code","01cb7c89":"code","90d74ae1":"code","83e76218":"code","6d69f96e":"code","77f26e47":"code","1e742e00":"code","5cf746be":"code","7a8a7d11":"code","875c3fb1":"markdown","58c41a55":"markdown","ab6ced98":"markdown","1c5091d6":"markdown","bf12efdd":"markdown","7d95fc46":"markdown","9030bf39":"markdown"},"source":{"6309672a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc7fa698":"import tensorflow as tf\nimport matplotlib.pyplot as plt","2825eaf0":"real = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nfake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\n\nfake.head()","f5dd0a9e":"from wordcloud import WordCloud, STOPWORDS\nfake_words = \"\"\nstopwords = set(STOPWORDS)\nstopwords.add(\"wa\")\nstopwords.add(\"thi\")\nfor text in fake.text.values:\n    text = str(text)\n    words = text.split()\n    fake_words += \" \".join([(i.lower() + \" \") for i in words])","b232658a":"fake_cloud = WordCloud(width = 500, height = 500, background_color = 'white', stopwords = stopwords, min_font_size = 10)\nfake_cloud.generate(fake_words)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(fake_cloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title(\"Fake news\")\n  \nplt.show() ","9ddd61e9":"real_words = \"\"\nfor text in real.text.values:\n    text = str(text)\n    words = text.split()\n    real_words += \" \".join([(i.lower() + \" \") for i in words])\nreal_cloud = WordCloud(width = 500, height = 500, background_color = 'white', stopwords = stopwords, min_font_size = 10)\nreal_cloud.generate(real_words)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(real_cloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title(\"Real news\")\n  \nplt.show() ","cff8dbff":"real[\"fake?\"] = np.zeros(len(real))\nfake[\"fake?\"] = np.ones(len(fake))\nfake.head()","d0dbae2c":"from sklearn.utils import shuffle\n\nnews = real.append(fake)\nnews = shuffle(news)\nnews","56e6c592":"import string\ndef clean_text(text):\n    words = str(text).split()\n    words = [i.lower() + \" \" for i in words]\n    words = \" \".join(words)\n    words = words.translate(words.maketrans('', '', string.punctuation))\n    return words\n\nnews['text'] = news['text'].apply(clean_text)\nnews.head()","77172910":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(news)\ntrain, validation = train_test_split(train, test_size = 0.2)\nprint(len(train), len(validation), len(test) )","c6a27040":"train","9f4122f5":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 10000\ntrunc_type = \"post\"\npad_type = \"post\"\noov_tok = \"<OOV>\"\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train.text)\nword_index = tokenizer.word_index","eac7e39e":"training_sequences = tokenizer.texts_to_sequences(np.array(train.text))\ntraining_padded = pad_sequences(training_sequences,truncating=trunc_type, padding=pad_type)\n\nmax_length = len(training_padded[0])\n\nvalidation_sequences = tokenizer.texts_to_sequences(np.array(validation.text))\nvalidation_padded = pad_sequences(validation_sequences, padding=pad_type, truncating=trunc_type, maxlen = max_length)","d1b140b3":"train_x = np.copy(training_padded)\nvalidate_x = np.copy(validation_padded)\ntrain_y = train['fake?'].values\nvalidate_y = validation['fake?'].values","21dbc1ae":"print(len(train_x), len(train_y))","8360cc9c":"!pip install -U keras-tuner","fc200a6a":"from kerastuner.tuners import RandomSearch\n\ndef build_model(hp):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, hp.Int('units', min_value = 5, max_value = 200, step = 25), input_length=max_length),\n        tf.keras.layers.Conv1D(16, 5, activation='relu'),\n        tf.keras.layers.GlobalMaxPooling1D(),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy',\n                      optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate',\n                      values=[1e-2, 1e-3, 1e-4])), \n                      metrics=['accuracy'])\n    return model\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3)\n\n# history = model.fit(train_x, train_y, epochs = 30, validation_data = (validate_x, validate_y),\n#                    callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])\n","01cb7c89":"tuner.search(train_x, train_y, epochs = 3,verbose = 2,validation_data = (validate_x, validate_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=2)])","90d74ae1":"tuner.results_summary()","83e76218":"# model = tf.keras.Sequential([\n#     tf.keras.layers.Embedding(vocab_size, 16, input_length=max_length),\n#     tf.keras.layers.Conv1D(16, 5, activation='relu'),\n#     tf.keras.layers.GlobalMaxPooling1D(),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n# model.compile(loss='binary_crossentropy',\n#                   optimizer=tf.keras.optimizers.Adam(), \n#                   metrics=['accuracy'])\n# history = model.fit(train_x, train_y, verbose = 2, epochs = 3, validation_data = (validate_x, validate_y),\n#                    callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])\nmodel = tuner.get_best_models()[0]\nhistory = model.fit(train_x, train_y, verbose = 2, epochs = 3, validation_data = (validate_x, validate_y),\n                   callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","6d69f96e":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","77f26e47":"test_sequences = tokenizer.texts_to_sequences(np.array(test.text))\ntest_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen = max_length)","1e742e00":"preds = np.round(model.predict(test_padded))\n","5cf746be":"len(preds)","7a8a7d11":"acc = np.sum(1 if i==j else 0 for i,j in zip(preds, test[\"fake?\"].values)) \/ len(test)\nprint(\"Accuracy: \", acc )","875c3fb1":"Tokenize & pad text to create input data","58c41a55":"Merge real & fake data, clean it, and split it into train\/test\/validation.","ab6ced98":"## Real news word cloud","1c5091d6":"First, I'm going to make some wordclouds to take a look at the data","bf12efdd":"## Fake news word cloud","7d95fc46":"Classify test set and calculate accuracy","9030bf39":"Use KerasTuner for Hyperparameter tuning for our Sequential Tensorflow model."}}