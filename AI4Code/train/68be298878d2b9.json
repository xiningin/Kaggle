{"cell_type":{"25e5dee9":"code","201bfec5":"code","d5230118":"code","49cae423":"code","5c1e86e2":"code","2a638f52":"code","acc848df":"code","097c8ebc":"code","465e8572":"code","ae7482a3":"code","f64d4025":"code","287e01cb":"code","54474b74":"code","30d156aa":"code","7ce2119d":"code","6ab5325b":"code","1ee4751e":"code","da2a8493":"code","e9ae5159":"code","6af81a24":"code","def83de7":"code","4b649a88":"code","7e57d401":"code","570c8f03":"markdown","dfb2bddc":"markdown","067c6dc6":"markdown","48818f43":"markdown","efc157d4":"markdown","b30acab5":"markdown","5fd2d0e5":"markdown","509a3e7a":"markdown","db651dce":"markdown","38b29f09":"markdown","8192f3f3":"markdown","fac503c3":"markdown","eb791fcb":"markdown","a263182c":"markdown","c968553e":"markdown","98f1be3c":"markdown","fb534361":"markdown","321d9764":"markdown","f41affde":"markdown","e3c71585":"markdown","14b29006":"markdown"},"source":{"25e5dee9":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)","201bfec5":"#Read files\ntrain = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/train.csv')\ntest = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/test.csv')\n\n#Check shapes\nprint('Train Shape: ', train.shape) \nprint('Test Shape: ', test.shape)\n\ntrain.head() #First 5 rows.","d5230118":"y_train = train['target'] #Assign the y target value for potential models.\nX_train = train.drop(['target', 'id'], axis=1) #Drop target and ID for X_train\n\nX_test = test.drop(['id'], axis=1) #Drop ID from the test set.","49cae423":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","5c1e86e2":"log = linear_model.LogisticRegression(solver='liblinear')\nridge = linear_model.Ridge()\nSGD = linear_model.SGDRegressor()\nelastic = linear_model.ElasticNet()\nlars = linear_model.Lars()\nlasso = linear_model.Lasso()\nlassolars = linear_model.LassoLars()\northo = linear_model.OrthogonalMatchingPursuit()\nARD = linear_model.ARDRegression()\nbaye = linear_model.BayesianRidge()","2a638f52":"def cv_scores(model):\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc') #5 folds\n    print('Model: ', model)\n    print('CV Mean: ', np.mean(scores)) #Mean of the 5 scores\n    print('STD: ', np.std(scores)) #Standard deviation of the 5 scores\n    print('\\n')","acc848df":"models = [log, ridge, SGD, elastic, lars, lasso, lassolars, ortho, ARD, baye]\n\nfor model in models:\n    cv_scores(model)","097c8ebc":"n_nonzero_coefs = np.arange(1, 50, 1)\ntol = [None, 1, 2, 5, 8, 15, 25, 35]\nfit_intercept = [True, False]\nnormalize = [True, False]\nprecompute = [True, False]\n\nfrom sklearn.model_selection import StratifiedKFold\n\nparameters = dict(n_nonzero_coefs = n_nonzero_coefs,\n             tol = tol,\n             fit_intercept = fit_intercept,\n             normalize = normalize,\n             precompute = precompute)\n\ngrid = GridSearchCV(estimator = ortho, param_grid = parameters, scoring = 'roc_auc', verbose = 1, n_jobs=-1) #n_jobs use all proccessors\ngridresult = grid.fit(X_train, y_train)\n\nprint('The best score was {:.5f} with parameters of {}'.format(gridresult.best_score_, gridresult.best_params_))","465e8572":"ortho = linear_model.OrthogonalMatchingPursuit(n_nonzero_coefs = 8, fit_intercept = True, normalize = False, precompute = True,\n                                              tol = None)\ncv_scores(ortho)","ae7482a3":"predict = ortho.fit(X_train, y_train).predict(X_test)\n\nsubmission = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/sample_submission.csv')\nsubmission['target'] = predict\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","f64d4025":"solver = ['liblinear', 'saga'] #both handle l1 and l2 penalty\npenalty = ['l1', 'l2']\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nclass_weight = ['balanced']\n\nparameters = dict(solver = solver,\n             penalty = penalty,\n             C = C,\n             class_weight = class_weight)\n\ngrid = GridSearchCV(estimator = log, param_grid = parameters, scoring = 'roc_auc', verbose = 1, n_jobs=-1) #n_jobs use all proccessors\ngridresult = grid.fit(X_train, y_train)\n\nprint('The best score was {:.5f} with parameters of {}'.format(gridresult.best_score_, gridresult.best_params_))","287e01cb":"log = linear_model.LogisticRegression(C = 1, class_weight = {1: 0.5, 0: 0.5}, penalty = 'l1', solver = 'liblinear')\ncv_scores(log)","54474b74":"predict = log.fit(X_train, y_train).predict(X_test)\n\nsubmission = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/sample_submission.csv')\nsubmission['target'] = predict\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","30d156aa":"n_iter = np.arange(1,501,100)\nverbose = [True, False]\nalpha_1 = (1e-9, 1.0, 'log-uniform')\nalpha_2 = (1e-9, 1.0, 'log-uniform')\nlambda_1 = (1e-9, 1000, 'log-uniform')\nlambda_2 = (1e-9, 1000, 'log-uniform')\n\nparameters = dict(n_iter = n_iter,\n                 verbose = verbose,\n                 alpha_1 = alpha_1,\n                 alpha_2 = alpha_2,\n                 lambda_1 = lambda_1,\n                 lambda_2 = lambda_2)\n\ngrid = GridSearchCV(estimator = ARD, param_grid = parameters, scoring = 'roc_auc', verbose = 1, n_jobs=-1) #n_jobs use all proccessors\ngridresult = grid.fit(X_train, y_train)\n\nprint('The best score was {:.5f} with parameters of {}'.format(gridresult.best_score_, gridresult.best_params_))\n\n#randomsearch = RandomizedSearchCV(estimator = ARD, param_distributions = parameters, scoring = 'roc_auc', verbose = 1, \n                                  #n_jobs= -1)\n#searchresult = randomsearch.fit(X_train, y_train)\n\n#print('The best score was {:.5f} with parameters of {}'.format(searchresult.best_score_, searchresult.best_params_))","7ce2119d":"ARD = linear_model.ARDRegression(alpha_1=1e-09, alpha_2 = 1.0, lambda_1 = 1e-09, lambda_2 = 1e-09, n_iter = 1, verbose = True)\n\ncv_scores(ARD)","6ab5325b":"predict = ARD.fit(X_train, y_train).predict(X_test)\n\nsubmission = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/sample_submission.csv')\nsubmission['target'] = predict\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","1ee4751e":"from skopt import BayesSearchCV\n\nn_iter = np.arange(1,501,100)\nalpha_1 = (1e-9, 1.0, 'log-uniform')\nalpha_2 = (1e-9, 1.0, 'log-uniform')\nlambda_1 = (1e-9, 1000, 'log-uniform')\nlambda_2 = (1e-9, 1000, 'log-uniform')\n\n\nparams = dict(n_iter = n_iter,\n             alpha_1 = alpha_1,\n             alpha_2 = alpha_2,\n             lambda_1 = lambda_1,\n             lambda_2 = lambda_2,\n             )\n\n#bayes = BayesSearchCV(estimator = baye, search_spaces = params, scoring='roc_auc', verbose=1, n_jobs=-1, n_iter=12)\n#bayesresult = bayes.fit(X_train, y_train)\n#print('The best score was {:.5f} with parameters of {}'.format(bayesresult.best_score_, bayesresult.best_params_))\n\ngrid = GridSearchCV(estimator = baye, param_grid = params, scoring='roc_auc', verbose=1, n_jobs=-1)\ngridresult = grid.fit(X_train, y_train)\nprint('The best score was {:.5f} with parameters of {}'.format(gridresult.best_score_, gridresult.best_params_))","da2a8493":"baye = linear_model.BayesianRidge(alpha_1=1.0, alpha_2=1.0, lambda_1=1e-09, lambda_2=1e-09, n_iter=1)\ncv_scores(baye)","e9ae5159":"predict = baye.fit(X_train, y_train).predict(X_test)\n\nsubmission = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/sample_submission.csv')\nsubmission['target'] = predict\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","6af81a24":"penalty = ['l1', 'l2', 'elasticnet']\nalpha = [1, 10, 100, 1000]\nlearning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\neta0 = [1, 10, 100]\n\nparams = dict(\n                           penalty=penalty,\n                           alpha=alpha,\n                           learning_rate=learning_rate,\n                           eta0=eta0)\n\ngrid = GridSearchCV(estimator = SGD, param_grid = params, scoring='roc_auc', verbose=1, n_jobs=-1)\ngridresult = grid.fit(X_train, y_train)\nprint('The best score was {:.5f} with parameters of {}'.format(gridresult.best_score_, gridresult.best_params_))","def83de7":"SGD = linear_model.SGDRegressor(alpha=10, eta0=1, learning_rate='adaptive', penalty='l2')\ncv_scores(SGD)","4b649a88":"predict = SGD.fit(X_train, y_train).predict(X_test)\n\nsubmission = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/sample_submission.csv')\nsubmission['target'] = predict\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","7e57d401":"models = []\nsub_score = []\n\nmodels = ['Orthogonal Matching', 'Linear Regression', 'ARD Regression', 'Bayesian Ridge', 'SGD Regressor']\nsub_score = [0.839, 0.735, 0.733, 0.740, 0.746]\n\nfor i in range(len(models)):\n    print(models[i], 'with a score of: ', sub_score[i])","570c8f03":"# Logistic Regression","dfb2bddc":"# Final order of submission scores.","067c6dc6":"Read the training and test files, check the shape and read the first 5 rows of the training set","48818f43":"Now we can list all the models that we will use. \n\nSeeing as we are predicting probability, we will use regression models.","efc157d4":"Using this model we can now make predictions on the test data and save it into a submission csv:","b30acab5":"### Score of 0.839 when submitted. Good score!","5fd2d0e5":"From the first rows we see that 'target' is obviously the target and the ID column can be dropped. Seeing as we don't have any target values in our test set, we can split the training set and run potential models on this.\n\nNext we set the y and X values.","509a3e7a":"# ARD Regression","db651dce":"### Produces a score of 0.733.","38b29f09":"First import the modules that will be used","8192f3f3":"# SGD Regressor","fac503c3":"Seeing as we have different ranges of values, we will benefit from scaling the data.","eb791fcb":"We define a function to check the cross validation scores for individual models. \"cross_val_score\" splits the data into say 5 folds. Then for each fold it fits the data on 4 folds and scores the 5th fold. Then it gives you the 5 scores from which you can calculate a mean and variance for the score.","a263182c":"# **Basic Attempt at Don't Overfit 2**","c968553e":"### Score of 0.740.","98f1be3c":"# Bayesian Ridge","fb534361":"By picking a range of hyper parameters and running a grid search, we can find the best possible parameters for the Orthogonal Matching Model on a 5 fold cross validation splitting strategy on our training data.","321d9764":"1. Orthogonal Matching is the best model\n2. Followed by Logistic Regression\n3. ARD Regression\n4. Bayesian Ridge\n5. SGD Regressor\n\nLets try and tune the hyperparameters for each one of these models, and then run again.","f41affde":"# Orthogonal Matching Model","e3c71585":"### Score of 0.746.","14b29006":"### Score of 0.735."}}