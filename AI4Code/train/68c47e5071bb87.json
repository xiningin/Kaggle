{"cell_type":{"b68b3892":"code","bee0fff1":"code","6d3efcc5":"code","48847c0f":"code","cf347cdc":"code","63f1668a":"code","1f10a12c":"code","e9394e42":"code","6a178a56":"code","70d306b4":"code","cc41a66b":"code","f7c87f13":"code","a1986fd2":"code","cf56fea0":"code","45de31a7":"code","41c5e5f7":"code","46e02927":"code","bc7c39fb":"code","8cc8e703":"code","073fad8d":"code","f0612bb8":"markdown","a43fc478":"markdown","c6b81bcc":"markdown","cfca9943":"markdown","2d2fe132":"markdown","1c168e13":"markdown","5fab9459":"markdown","4356eea5":"markdown","d0c08fd5":"markdown","4c67e2db":"markdown","c6395f37":"markdown"},"source":{"b68b3892":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bee0fff1":"df=pd.read_csv(\"..\/input\/data.csv\")\ndf.head()","6d3efcc5":"\n#we dont need id coloum and last coloum unnamed\ndf.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)\ndf.describe(include=\"all\")","48847c0f":"import seaborn as sns\nsns.set(rc={'figure.figsize':(5,5)})\nsns.heatmap(df.isnull())","cf347cdc":"import matplotlib.pyplot as plt\nsns.set(rc={'figure.figsize':(20,20)})\nsns.heatmap( df.corr(),annot=True)\n","63f1668a":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\npositive_to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\npositive_to_drop=np.asarray(positive_to_drop)\n\nnegative_to_drop = [column for column in upper.columns if any(upper[column] < -0.90)]\nnegative_to_drop=np.asarray(negative_to_drop)\n\nfor i in positive_to_drop:\n    df.drop(i,axis=1,inplace=True)\n    \nfor i in negative_to_drop:\n    df.drop(i,axis=1,inplace=True)\n    ","1f10a12c":"sns.set(rc={'figure.figsize':(20,20)})\nsns.heatmap( df.corr(),annot=True)","e9394e42":"df[\"diagnosis\"].replace(\"M\",1,inplace=True)\ndf[\"diagnosis\"].replace(\"B\",0,inplace=True)\ndf=df.apply(pd.to_numeric)","6a178a56":"fig = plt.figure(figsize = (20, 25))\nj = 0\n#Droping_Characters and string coloums because graph donot support them\n\nfor i in df.columns:\n    plt.subplot(7, 7, j+1)\n    j += 1\n    sns.distplot(df[i][df['diagnosis']==1], color='r', label = 'malignant')\n    sns.distplot(df[i][df['diagnosis']==0], color='g', label = 'benign')\n    plt.legend(loc='best')\nfig.suptitle('Breast Cancer ')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","70d306b4":"#outliers\nsns.set(rc={'figure.figsize':(5,5)})\nfor column in df:\n    plt.figure()\n    sns.boxplot(x=df[column])","cc41a66b":"\ndf_X=df.drop(\"diagnosis\",axis=1)\nQ1 = df_X.quantile(0.25)\nQ3 = df_X.quantile(0.75)\nIQR = Q3 - Q1\ndf_X = df_X[~((df_X < (Q1 - 1.5 * IQR)) |(df_X > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(\"With Outliers Valuse\",len(df))\nprint(\"Removed Outliers Valuse\",len(df_X))\nprint(\"Values Removed \",str(len(df)-len(df_X)))","f7c87f13":"\ndf_y=df[\"diagnosis\"]\nnew_df=df_X.merge(df_y.to_frame(), left_index=True, right_index=True)\n\nnew_df.head(10)","a1986fd2":"from sklearn.model_selection import train_test_split\nX=new_df.drop(\"diagnosis\",axis=1)\ny=new_df[\"diagnosis\"]\n\n\nprint(X.shape)\nprint(y.shape)\nX_train, X_test, y_train, y_test = train_test_split(\n   X,y, test_size=0.1, random_state=0)\n\nX_train=np.asarray(X_train)\nX_test=np.asarray(X_test)\ny_train=np.asarray(y_train)\ny_test=np.asarray(y_test)\n\nprint(type(X_train))\nprint(type(y_train))\nprint(X_train.shape)\nprint(y_train.shape)","cf56fea0":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=20)\n\nresults=cross_val_score(clf,X_train ,y_train , cv=10)\nprint(results)\nAverage=sum(results) \/ len(results) \nprint(\"Average Accuracy :\",Average)","45de31a7":"len(df_X.columns)","41c5e5f7":"clf.fit(X_train,y_train)\nimportance_frame = pd.DataFrame()\nimportance_frame['Features'] = X.columns\nimportance_frame['Importance'] = clf.feature_importances_\nimportance_frame = importance_frame.sort_values(by=['Importance'], ascending=True)\nplt.barh([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], importance_frame['Importance'], align='center', alpha=0.5)\nplt.yticks([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], importance_frame['Features'])\nplt.xlabel('Importance')\nplt.title('Feature Importances')\nplt.show()","46e02927":"from sklearn import ensemble\nfrom sklearn import gaussian_process\nfrom sklearn import linear_model\nfrom sklearn import naive_bayes\nfrom sklearn import neighbors\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import discriminant_analysis\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier \nfrom sklearn import metrics","bc7c39fb":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train, y_train)\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\nMLA_compare\n#MLA_predict","8cc8e703":"from sklearn.ensemble import VotingClassifier\nvote_clf = VotingClassifier(estimators=[\n                                        \n                                        (\"LR\",linear_model.LogisticRegressionCV()),\n                                        (\"RC\",linear_model.RidgeClassifierCV()),\n                                        (\"SVC\",svm.SVC(probability=True)),\n                                        (\"XGB\",XGBClassifier())\n                                        ])\nvote_clf = vote_clf.fit(X_train, y_train)","073fad8d":"\nfrom sklearn.metrics import confusion_matrix\nprediction=vote_clf.predict(X_test)\ny_pred=[]\n\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, prediction).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\n\nAccuracy=(true_positive+true_negative)\/(true_positive+false_positive+true_negative+false_negative)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","f0612bb8":"**Data Distribution**","a43fc478":"M = malignant is Cancerous Tumor\nB = benign  is non Cancerous Tumor\n\n* Replacing M==1 and B==1 \n\nBecause computer are good with numeric Values","c6b81bcc":"**Introduction:**\n* What is dataset?\n* Visualization\n        * Missing Values\n        * Correlation Analysis\n        * Data Distribution        \n        * Outlier Analysis\n* Machine Learning\n    * Random Forest\n    * Checking 20 Different Classifiers\n    * Ensemble Technique (Bagging)\n* Evaluation Measures   ","cfca9943":"**We have no missing Values**","2d2fe132":"**most of the coloum distribution are intersacting with eachother.\nbut concavity worst and concavity mean and compactness mean I think these colums are most usefull**","1c168e13":"**Checking Other Classifiers**","5fab9459":"**Machine Learning**\n","4356eea5":"**Outlier Analysis**","d0c08fd5":"**Removing Outliers**","4c67e2db":"Co-related Coloum gives us the same information. So that it is good for us to drop 1 of the coloum to make our data small so that our machine learning algorithms take less time to compute the results. \nNow we are going to drop those coloum which are highly co-realated greater than value  where co-relation is greater than( 0.90)\n\n1. radius_mean and area_worst = 0.94\n2. readius_mean and parameter_worst=0.97\n3. radius_mean and radius_worst=0.97\n4. radius_mean and area_mean=0.99\n5. radius_mean and parameter_mean=1\n6. Texture_mean and Texture_worth=0.91\n7. area_mean and parameter_mean=0.99\n8. area_mean and radius_mean=0.99\n9. radius_se and parameter_se=0.97\n10. radius_se and area_se=0.94\n11. area_mean and radius_worst=0.96\n12. radius_Worst and parameter_worst=0.99\n13. radius_Worst and area_worst=0.98\n14. concavePoint_worst and concave_point_mean=0.91\n\n\n","c6395f37":"**Droping Coloums which are highly Co-related**"}}