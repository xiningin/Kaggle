{"cell_type":{"94388224":"code","d5d1a61b":"code","56970d2d":"code","71656eab":"code","32ea573c":"code","0701d03a":"code","f5de0b94":"code","1f3b7a71":"code","53fdc1af":"code","ecfbd1d5":"code","bd15931f":"code","98c5bec0":"code","90998f7b":"code","699fac69":"code","1b6aaa60":"code","ae17b0ce":"code","587f9e4e":"code","692aa2b8":"code","5012ad97":"code","85ab2fc9":"code","7c7cc434":"code","045a7682":"code","c728b9a9":"code","2fd1e26f":"code","94196d95":"code","caaadff6":"code","70521f35":"code","4b8cc843":"code","0a33d2c3":"code","58e75889":"code","4e678d58":"code","2ea9a5e4":"code","0fc3e38f":"code","af031242":"code","dd48bc2d":"code","8507e6e3":"code","2c200ed0":"code","e64ed8e8":"code","21664dcb":"code","47810275":"code","52768c53":"code","6ed3d5b0":"code","e4c74d59":"code","412725e2":"code","25f265dd":"code","b68fd7b0":"code","5b55fe00":"code","5b955ddf":"code","c2561a52":"code","4e01bd72":"code","8feb6771":"markdown","5f1d5f68":"markdown","0ab2cbbf":"markdown","b10911fe":"markdown","d9c1ab7c":"markdown","77f9522c":"markdown","24806bd0":"markdown","f77a93d5":"markdown","4cc8ea10":"markdown","f463464c":"markdown","3cb443a9":"markdown","37caa01b":"markdown","08d01573":"markdown","aee5f227":"markdown","a0096ee1":"markdown","da7c9e68":"markdown","1c4e38a4":"markdown","7f50c5dd":"markdown","cf324e19":"markdown","2e2c8a64":"markdown","863d33c0":"markdown"},"source":{"94388224":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en')\n\nimport nltk\nfrom nltk.corpus import treebank\nnltk.download('treebank')\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\n# for Sentiment Analysis\nfrom textblob import TextBlob","d5d1a61b":"filename = '..\/input\/export-dashboard-tsla\/export_dashboard_tsla.xlsx'\ntslaTweets = pd.read_excel(filename,sheet_name='Stream')\ntslaTweets","56970d2d":"# pre-processing fuction\n\nwpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')\n\ndef normalize_document(doc):\n    # lower case and remove special characters\\whitespaces\n    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n    doc = doc.lower()\n    doc = doc.strip()\n    # tokenize document\n    tokens = wpt.tokenize(doc)\n    # filter stopwords out of document\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    # re-create document from filtered tokens\n    doc = ' '.join(filtered_tokens)\n    return doc\n\nnormalize_corpus = np.vectorize(normalize_document)","71656eab":"# call the normalize_document function to pre-process the training corpus\n\nnorm_content = normalize_corpus(tslaTweets['Tweet content'])\nnorm_content","32ea573c":"tslaTweets.shape","0701d03a":"norm_content.shape","f5de0b94":"tslaTweets['cleaned'] = norm_content\ntslaTweets.head()","1f3b7a71":"# Filter the contents \nLANG = 'en'\nHASH_TAGS = '#Tesla'\n\n# Filter with the langauge \ntslaTweetsFiltered = tslaTweets[(tslaTweets['Tweet language (ISO 639-1)'] == LANG)]\n\n# Filter with the langauge and hastags (note: use contains for hashtags)\n#tslaTweetsFiltered = tslaTweets[(tslaTweets['Tweet language (ISO 639-1)'] == LANG) & (tslaTweets['Hashtags'].str.contains(HASH_TAGS))]\n\ntslaTweetsFiltered.reset_index(drop=True, inplace=True)\ntslaTweetsFiltered.head()","53fdc1af":"tslaTweetsFiltered.columns","ecfbd1d5":"# Look at the subset of useful columns for the sentiment trading\nCOLUMNS = ['Date', 'cleaned', 'Followers']\ntslaTweetsSubset = tslaTweetsFiltered[COLUMNS]\n\n# Convert Date string to datetime to match with the stock daily change later\ntslaTweetsSubset['Date'] = pd.to_datetime(tslaTweetsSubset['Date'])\ntslaTweetsSubset.head()","bd15931f":"tslaTweetsSubset.shape","98c5bec0":"tslaTweetsSubset = tslaTweetsSubset.dropna()\ntslaTweetsSubset","90998f7b":"# Use TextBlob to run the tweets sentiment polarity\ntslaTweetsSubset['sentiment'] = tslaTweetsSubset['cleaned'].apply(lambda x: TextBlob(x).polarity)\n\n# Weight the tweets sentiment importance by the number of followings\ntslaTweetsSubset['sentiment_weighted'] = tslaTweetsSubset['sentiment'] * tslaTweetsSubset['Followers']\ntslaTweetsSubset","699fac69":"# Check Sentiment assessment\nINDEX = 0\n\ntext = tslaTweetsSubset.iloc[INDEX]['cleaned']\nprint(text)\n\nTextBlob(text).sentiment_assessments","1b6aaa60":"# Group the weighted sentiment by Date for matching the stock daily change\naggregateSentiments = tslaTweetsSubset.groupby(['Date']).sum()[['sentiment_weighted']]\naggregateSentiments","ae17b0ce":"# get stocks daily data (OHLCV) from Yahoo\nimport pandas_datareader.data as web\nfrom datetime import datetime\n\nstart = datetime(2016, 4, 2) \nend = datetime(2016, 6, 15) \nstock= web.DataReader('TSLA', 'yahoo', start=start, end=end)\nstock.head()","587f9e4e":"# calculate the stock daily change\nstock['change'] = (stock['Close'] - stock['Open']) \/ stock['Open']\nstock.head()","692aa2b8":"start = datetime(2016, 4, 2) \nend = datetime(2016, 6, 15) \nNDX= web.DataReader('^NDX', 'yahoo', start=start, end=end)\nNDX.head()","5012ad97":"# calculate the NASDAQ 100 daily change\nNDX['NDX change'] = (NDX['Close'] - NDX['Open']) \/ NDX['Open']\nNDX.head()","85ab2fc9":"# Merge the daily stock price info with the sentiments\nmerged = stock.merge(aggregateSentiments, on='Date', how='left')[['change', 'sentiment_weighted']]\nmerged.head()","7c7cc434":"# Scale the unit to -1 to 1\nscaler = MinMaxScaler((-1, 1))\nmerged['changes'] = scaler.fit_transform(merged[['change']])\nmerged['sentiments'] = scaler.fit_transform(merged[['sentiment_weighted']])\nscaled = merged[['changes', 'sentiments']]\nscaled = scaled.dropna()\nscaled.head()","045a7682":"scaled.plot(figsize=(10, 6))","c728b9a9":"# shows the correlation\ncorr_data = scaled.corr()\ncorr_data.style.background_gradient(cmap='coolwarm', axis=None)","2fd1e26f":"# Try sentiments with different date lags\n\n# Sentiment shift backwards -> Current day sentiments predicts next day stock price change (predictive)\nscaled['sentiment-1'] = merged['sentiments'].shift(-1)\n\n# Sentiment shift forwards -> Current day sentiments reflects yesterday's price change (reactive)\nscaled['sentiment+1'] = merged['sentiments'].shift(1)\nscaled.head()","94196d95":"scaled.plot(figsize=(10, 6))","caaadff6":"corr_data = scaled.corr()\ncorr_data.style.background_gradient(cmap='coolwarm', axis=None)","70521f35":"stock.head()","4b8cc843":"# Look at the subset of useful columns for the NASDAQ 100\nCOLUMNS = ['Open', 'Close', 'NDX change']\nNDX = NDX[COLUMNS]\nNDX.head()","0a33d2c3":"# Look at the subset of useful columns for the Tesla Stock\nCOLUMNS = ['Open', 'Close', 'change']\nstock = stock[COLUMNS]\nstock.head()","58e75889":"#combine the information of NASDAQ 100 and Tesla\ndata_df = pd.merge(stock,NDX,on='Date',how='left')\ndata_df.head()","4e678d58":"#calculate the difference of between Tesla change and NASDAQ 100 change\ndata_df['difference'] = data_df['change'] - data_df['NDX change']\ndata_df.head()","2ea9a5e4":"#Add tag to all dates, if the tag is 1 tesla stock return is higher than NASDAQ 100 on that day otherwise the tag is 0\n\ndata_df['tag']=[1 if x>0 else 0 for x in data_df['difference'] ]\ndata_df.head()","0fc3e38f":"data_df = pd.merge(data_df,tslaTweetsSubset,on='Date',how='left')\ndata_df.head()","af031242":"from sklearn.model_selection import train_test_split \n\nX = np.array(data_df['cleaned'].fillna(' ')) \ny = np.array(data_df['tag'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","dd48bc2d":"from sklearn.feature_extraction.text import CountVectorizer\n   \n### build BOW features on train_corpus using the CountVectortizer\n\n### transform the test_corpus using the transform method\n\ntransfer = CountVectorizer()\n\nX_train = transfer.fit_transform(X_train)\n\nX_test = transfer.transform(X_test)","8507e6e3":"# Na\u00efve Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n# fit the model with the y_train as the target and the BOW featurs on the train_corpus as input\n\nestimator = MultinomialNB()\n\nestimator.fit(X_train,y_train)","2c200ed0":"from sklearn.metrics import confusion_matrix\n# Predict the testing output using the BOW features on the test_corpus\n\n#compare the real value with predicted value\ny_predict = estimator.predict(X_test)\n\nlabels = np.unique(y_test)\n\ncm = confusion_matrix(y_test, y_predict, labels=labels)\n\npd.DataFrame(cm, index=labels, columns=labels)","e64ed8e8":"# calculate the accuracy, precison, recall and F1 - score\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_predict, y_test)\nprint('Accuracy: %f' % accuracy)\n\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_predict, y_test)\nprint('Precision: %f' % precision)\n\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_predict, y_test)\nprint('Recall: %f' % recall)\n\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_predict, y_test)\nprint('F1 score: %f' % f1)","21664dcb":"# Compute ROC curve and AUC for the predicted class \"1\"\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\n\ny_score = estimator.predict_proba(X_test)[:,1]\n\n# Compute ROC curve and AUC for the predicted class \"1\"\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfpr, tpr, threshold = roc_curve(y_test, y_score)\n\n# Compute Area Under the Curve (AUC) using the trapezoidal rule\nestimator_roc_auc = auc(fpr, tpr)","47810275":"plt.plot(fpr, tpr)\nprint(\"AUC score = {}\".format(estimator_roc_auc))","52768c53":"# verify\n\nfpr, tpr, threshold = roc_curve(y_test, y_score)\nestimator_roc_auc = auc(fpr, tpr)\n\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111)\nax.set_title('Receiver Operating Characteristic')\nax.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % estimator_roc_auc)\nax.legend(loc = 'lower right')\nax.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nax.set_ylabel('True Positive Rate')\nax.set_xlabel('False Positive Rate')","6ed3d5b0":"from sklearn.model_selection import train_test_split \n\nX = np.array(data_df['cleaned'].fillna(' ')) \ny = np.array(data_df['tag'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e4c74d59":"from sklearn.feature_extraction.text import TfidfVectorizer\n### build BOW features on train_corpus using the TfidfVectortizer\n### transform the test_corpus using the transform method\n\ntransfer = TfidfVectorizer()\n\nX_train = transfer.fit_transform(X_train)\n\nX_test = transfer.transform(X_test)","412725e2":"# Na\u00efve Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n# fit the model with the y_train as the target and the BOW featurs on the train_corpus as input\n\nestimator = MultinomialNB()\n\nestimator.fit(X_train,y_train)","25f265dd":"# Predict the testing output using the BOW features on the test_corpus\n\n#compare the real value with predicted value\ny_predict = estimator.predict(X_test)\nprint(\"y_predict:\",y_predict)\nprint(\"The boolen matrix of prediction:\",y_test==y_predict)   #True means the model predicting correctly on that article!\n\n#precison\nscore = estimator.score(X_test,y_test)","b68fd7b0":"# Evaluate the model using the confusion matrix\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\n\nlabels = np.unique(y_test)\ncm = confusion_matrix(y_test, y_predict, labels=labels)\n\npd.DataFrame(cm, index=labels, columns=labels)","5b55fe00":"# calculate the accuracy, precison, recall and F1 - score\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_predict, y_test)\nprint('Accuracy: %f' % accuracy)\n\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_predict, y_test)\nprint('Precision: %f' % precision)\n\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_predict, y_test)\nprint('Recall: %f' % recall)\n\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_predict, y_test)\nprint('F1 score: %f' % f1)","5b955ddf":"# Compute ROC curve and AUC for the predicted class \"1\"\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\n\ny_score = estimator.predict_proba(X_test)[:,1]\n\n# Compute ROC curve and AUC for the predicted class \"1\"\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfpr, tpr, threshold = roc_curve(y_test, y_score)\n\n# Compute Area Under the Curve (AUC) using the trapezoidal rule\nestimator_roc_auc = auc(fpr, tpr)","c2561a52":"plt.plot(fpr, tpr)\nprint(\"AUC score = {}\".format(estimator_roc_auc))","4e01bd72":"# verify\n\nfpr, tpr, threshold = roc_curve(y_test, y_score)\nestimator_roc_auc_roc_auc = auc(fpr, tpr)\n\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111)\nax.set_title('Receiver Operating Characteristic')\nax.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % estimator_roc_auc_roc_auc)\nax.legend(loc = 'lower right')\nax.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nax.set_ylabel('True Positive Rate')\nax.set_xlabel('False Positive Rate')","8feb6771":"#Step 9e: Model Evaluation using ROC\/AUC","5f1d5f68":"#STEP 8: Tag Dataset","0ab2cbbf":"#STEP 3: Weight sentiment importance","b10911fe":"#STEP 7: Check the correlation ","d9c1ab7c":"#Step 10e: Model Evaluation using ROC\/AUC","77f9522c":"#Step 10b: using TFIDF to represent the document","24806bd0":"#Step 9a: Split the data into training\/testing","f77a93d5":"#Step 9d: Model Evaluation using Confustion Matrix","4cc8ea10":"#STEP 6: Merge dataset","f463464c":"#Step 9c: Using Naive Bayes Classifiers to train a Text Classification Model","3cb443a9":"#STEP 9: Using Bow - TF","37caa01b":"#STEP 2: Filter the contents","08d01573":"# The tweets dataset is downloaded from http:\/\/followthehashtag.com\/datasets\/nasdaq-100-companies-free-twitter-dataset\/","aee5f227":"#Step 9b: Using BoW - TF to represent the document X","a0096ee1":"#Step 10a: Split the data into training\/testing","da7c9e68":"#Step 10c: Using Naive Bayes Classifiers to train a Text Classification Model","1c4e38a4":"#Step 10: using TFIDF to represent the document","7f50c5dd":"#STEP 1: Data Preprocessing","cf324e19":"#Step 10d: Model Evaluation using Confustion Matrix","2e2c8a64":"#STEP 5: Get the daily stock data","863d33c0":"#STEP 4: Group Sentiment Data"}}