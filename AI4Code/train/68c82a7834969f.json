{"cell_type":{"c1e81d6c":"code","11c8ca93":"code","c20398d1":"code","4360eb8f":"code","1c6a279b":"code","fb49dec4":"code","109a13db":"code","582147b7":"code","ea7be5f9":"code","26ae84eb":"code","21c8c9b8":"code","74742138":"code","4fe2f27b":"code","0d81ee41":"code","bcc995ae":"code","bc8eaa4e":"code","95e7182a":"code","b29c35c5":"code","17e4cb7a":"code","41197e24":"code","8f625729":"code","74f3dc8d":"code","74839ca0":"code","e536bc12":"code","86115bc4":"code","25381bc0":"code","55728c74":"code","e706e86e":"code","b4a304a7":"code","33052ae1":"code","e86768e7":"code","65be68d9":"code","ebf61038":"code","aef58117":"code","9cfa33b9":"code","2ab556ae":"code","fdf2693c":"code","76468ab2":"code","8bca84bf":"code","3da629d0":"code","cc7cdd5d":"code","97a8b11d":"markdown","a3f80c4f":"markdown","963742dd":"markdown","84ca8347":"markdown","b6f3b323":"markdown","de3088a7":"markdown","93e19193":"markdown","e8976d05":"markdown","fab7083e":"markdown","0620f2cd":"markdown","e89c5392":"markdown","3fa05f07":"markdown","8cabf158":"markdown","1962ec66":"markdown","0bb6b451":"markdown","e48e4817":"markdown","0ec4a427":"markdown","ca4d8087":"markdown","484e48d1":"markdown","98ad3609":"markdown","45f8e3ad":"markdown","cd75c588":"markdown","e602224c":"markdown","3d74753a":"markdown","a6ef4797":"markdown","eeaeb446":"markdown","e13ae1b9":"markdown","72a6632c":"markdown","c12cbc58":"markdown","2d78254f":"markdown","9b2c753d":"markdown","c44ef73e":"markdown","3cadcd52":"markdown","0066fb3f":"markdown","b7042946":"markdown","9c98c6a5":"markdown","110f57d0":"markdown","d316747f":"markdown","3a17c194":"markdown","58016036":"markdown","353183c4":"markdown","1549793c":"markdown","a8dc1149":"markdown"},"source":{"c1e81d6c":"# IMPORT LIBRARIES\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss, CondensedNearestNeighbour, EditedNearestNeighbours, TomekLinks, OneSidedSelection, NeighbourhoodCleaningRule\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV,RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier","11c8ca93":"# READ DATA\ntrain_data = pd.read_csv('..\/input\/highly-unbalanced-multiclass6-dataset\/spenddata.csv')\ntrain_data = train_data[train_data.columns[1:]]\ntrain_data.head()","c20398d1":"plt.figure(figsize=(15,6))\nsns.countplot(train_data.pov6, palette='Set2')\nplt.show()","4360eb8f":"train_data.info()","1c6a279b":"train_data.pov6","fb49dec4":"# READ TEST DATA\ntest_data = pd.read_csv('..\/input\/highly-unbalanced-multiclass6-dataset\/testdata.csv')\ntest_data = test_data[test_data.columns[1:]]\ntest_data.info()","109a13db":"train_data.drop_duplicates()","582147b7":"# STORE ALL THE COLUMNS THAT HAS NULL VALUES \ntrain_null_columns = []\n\n# ITERATE THROUGH TRAINING DATA COLUMNS AND CHECK WHETHER IT HAS NULL VALUES OR NOT\nfor index, rows in pd.DataFrame(train_data.isna().any()).iterrows():\n    if rows[0] == True:\n        # COLUMNS WITH NULL VALUES MORE THAN 11000\n        if train_data[index].isna().sum() >= 11000:\n            train_null_columns.append(index)\n        \ntrain_null_columns","ea7be5f9":"# STORE ALL THE COLUMNS THAT HAS NULL VALUES \ntest_null_columns = []\n\n# ITERATE THROUGH TEST DATA COLUMNS AND CHECK WHETHER IT HAS NULL VALUES OR NOT\nfor index, rows in pd.DataFrame(test_data.isna().any()).iterrows():\n    if rows[0] == True:\n        # COLUMNS WITH NULL VALUES MORE THAN 11000\n        if test_data[index].isna().sum() >= 4000:\n            test_null_columns.append(index)\n        \ntest_null_columns    ","26ae84eb":"# COLUMNS THAT HAVE MOSTLY NULL VALUES IN TRAIN DATA BUT NOT IN TEST DATA\nset(train_null_columns) - set(test_null_columns)","21c8c9b8":"# COLUMNS THAT HAVE MOSTLY NULL VALUES TEST DATA BUT NOT IN TRAIN DATA\nset(test_null_columns) - set(train_null_columns)","74742138":"test_data['totshopping.rep'].isna().sum()","4fe2f27b":"train_data['totshopping.rep'].isna().sum()","0d81ee41":"# TOTAL NULL COLUMNS TO BE DELETED\ntot_cols = train_null_columns + list(set(test_null_columns) - set(train_null_columns))\n\nprint('TOTAL COLUMNS TO BE DELETED', len(tot_cols))\n\n# DELETE THE NULL VALUED COLUMNS FROM TRAINING DATA \nfiltered_data = train_data.drop(tot_cols, axis=1)\n\nprint('COLUMNS BEFORE', len(train_data.columns))\nprint('COLUMNS AFTER REMOVAL', len(filtered_data.columns))","bcc995ae":"# DELETE THE NULL VALUED COLUMNS FROM TEST DATA \nfiltered_test_data = test_data.copy()\n\nfor col in tot_cols:\n    try:\n        filtered_test_data.drop([col], axis=1, inplace=True)\n    except:\n        print('{} COLUMN not in TEST DATA '.format(col))\n\nprint()\nprint('COLUMNS BEFORE', len(test_data.columns))\nprint('COLUMNS AFTER REMOVAL', len(filtered_test_data.columns))","bc8eaa4e":"if set(filtered_data.columns.tolist()) == set(filtered_test_data.columns.tolist()):\n    print('THEY BOTH HAVE SAME COLUMNS')\nelse:\n    print(set(filtered_data.columns.tolist()) - set(filtered_test_data.columns.tolist()))\n    print(set(filtered_test_data.columns.tolist()) - set(filtered_data.columns.tolist()))","95e7182a":"# MAKE LABEL\nlabel = pd.DataFrame(train_data.pov6)","b29c35c5":"# SELECT COLUMNS WITH OBJECT DATATYPE\nfiltered_data.columns[filtered_data.dtypes=='object']","17e4cb7a":"filtered_data['var9'].head()","41197e24":"filtered_data['respondent.id'].head()","8f625729":"# ENCODE STRING COLUMNS \npd.get_dummies(filtered_data.var9)","74f3dc8d":"# CONCAT THE DUMMY DATA WITH THE TRAINING DATA\nfiltered_data = pd.concat([filtered_data, pd.get_dummies(filtered_data.var9)], axis=1 )\n\n# REMOVE UNWANTED STRING COLUMN\nfiltered_data.drop(['respondent.id','var9'], axis=1, inplace=True)\nlen(filtered_data.columns)","74839ca0":"# CONCAT THE DUMMY DATA WITH THE TEST DATA\nfiltered_test_data = pd.concat([filtered_test_data, pd.get_dummies(filtered_test_data.var9)], axis=1 )\n\n# REMOVE UNWANTED STRING COLUMN\nfiltered_test_data.drop(['respondent.id','var9'], axis=1, inplace=True)\nlen(filtered_test_data.columns)","e536bc12":"# BEFORE REMOVING NULL VALUES\nfiltered_data.head()","86115bc4":"# FILL ALL THE NULL VALUES WITH MEAN\nfiltered_data.fillna(filtered_data.mean(), inplace=True)\nfiltered_data.head()","25381bc0":"# FILL ALL THE NULL VALUES WITH MEAN\nfiltered_test_data.fillna(filtered_test_data.mean(), inplace=True)\nfiltered_test_data.head()","55728c74":"# INITIALISE SCALER\nscaler = MinMaxScaler()\n\n# CREATE DATAFRAME OF THE SCALED DATA\nscaled_train_data = pd.DataFrame(scaler.fit_transform(filtered_data), index=filtered_data.index, columns=filtered_data.columns)\nscaled_train_data.head()","e706e86e":"# CREATE DATAFRAME OF THE SCALED DATA\nscaled_test_data = pd.DataFrame(scaler.fit_transform(filtered_test_data), index=filtered_test_data.index, columns=filtered_test_data.columns)\nscaled_test_data.head()","b4a304a7":"# PLOT DISTRIBUTION OF THE TARGET CLASSES\nplt.figure(figsize=(15,6))\nsns.countplot(label.pov6, palette='Set2')\nplt.show()\n\n# SUMMARIZE DISTRIBUTION\ncounter = Counter(label.pov6)\nfor k,v in counter.items():\n    per = v \/ len(label) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))","33052ae1":"# PERFORM OVERSAMPLING USING SMOTE ( SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE )\noversample = SMOTE()\nX_over, y_over = oversample.fit_resample(scaled_train_data, label)\n\n# PLOT THE NEW DATA\nplt.figure(figsize=(10,6))\nsns.countplot(y_over.pov6, palette='Wistia')\nplt.show()\n\n# SUMMARIZE DISTRIBUTION\ncounter = Counter(y_over.pov6)\nfor k,v in counter.items():\n    per = v \/ len(y_over) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))","e86768e7":"undersample = NearMiss(version=1, n_neighbors=3)\n# transform the dataset\nX_under, y_under = undersample.fit_resample(scaled_train_data, label)\n\n# PLOT THE NEW DATA\nplt.figure(figsize=(10,6))\nsns.countplot(y_under.pov6, palette='autumn')\nplt.show()\n\n# SUMMARIZE DISTRIBUTION\ncounter = Counter(y_under.pov6)\nfor k,v in counter.items():\n    per = v \/ len(y_under) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))","65be68d9":"# CREATE PIPELINE OF ALL THE POSSIBLE CLASSIFIERS\nest =[]\n\nest.append(('SVC', Pipeline([('SVC', SVC(gamma='scale', class_weight='balanced'))])))\nest.append(('GradientBoosting', Pipeline([('GradientBoosting',GradientBoostingClassifier())])))\nest.append(('AdaBoost', Pipeline([ ('AdaBoost', AdaBoostClassifier())])))\nest.append(('ExtraTree', Pipeline([('ExtraTrees', ExtraTreesClassifier())])))\nest.append(('RandomForest', Pipeline([('RandomForest', RandomForestClassifier())]))) \nest.append(('Bagging', Pipeline([('Bagging', BaggingClassifier())])))\nest.append(('KNeighbors', Pipeline([('KNeighbors', KNeighborsClassifier())])))\nest.append(('DecisionTree', Pipeline([('DecisionTree', DecisionTreeClassifier())])))\n# est.append(('XGB', Pipeline([('XGB', XGBClassifier())])))","ebf61038":"import warnings\nwarnings.filterwarnings(action='ignore')\n\nseed = 4\nsplits = 5\n\n# CALCULATE F1 SCORE WITH MORE WEIGHT ON RECALL\nf_score = make_scorer(fbeta_score, beta=2, average='macro')\nmodels_score =[]\n\nfor i in est:\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n    results = cross_val_score(i[1], X_under, y_under, cv=cv, scoring=f_score)\n    models_score.append({i[0] : '{} +\/- {}'.format(results.mean(), results.std())})","aef58117":"print(\"F1 SCORES OF DIFFERENT CLASSIFIERS\")\nmodels_score","9cfa33b9":"model = GradientBoostingClassifier()\nmodel.fit(X_under, y_under.pov6)\n# PREDICT ON THE TEST DATA\npredict_under = model.predict(scaled_test_data)","2ab556ae":"model_ = GradientBoostingClassifier()\nmodel_.fit(X_over, y_over.pov6)\n# PREDICT ON THE TEST DATA\npredict_over = model_.predict(scaled_test_data)","fdf2693c":"l = [predict_under, predict_over, train_data.pov6]\npal = [\"autumn\",'Wistia','Set2']\ntitles =['UNDERSAMPLED', 'OVERSAMPLED', 'TRAINING']\n\nplt.figure(figsize=(25,6))\nplt.rc('font', size=15)\nfor i in range(3):\n    plt.subplot(1,3, i+1)\n    sns.countplot(l[i], palette=pal[i])\n    plt.title(titles[i])\n    plt.xlabel('CLASSES')\n    plt.ylabel('COUNT')\nplt.show()\n\n# SUMMARIZE DISTRIBUTION\nprint('DISTRIBUTION OF PREDICTED UNDERSAMPLED CLASSES')\ncounter = Counter(predict_under)\nfor k,v in counter.items():\n    per = v \/ len(predict_under) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n    \nprint()\nprint('DISTRIBUTION OF PREDICTED OVERSAMPLED CLASSES')\ncounter = Counter(predict_over)\nfor k,v in counter.items():\n    per = v \/ len(predict_over) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))","76468ab2":"# NEAR MISS VERSION 2\nundersample = NearMiss(version=2, n_neighbors=3)\n# transform the dataset\nX_near2, y_near2 = undersample.fit_resample(scaled_train_data, label)\n\n# NEAR MISS VERSION 3\nundersample = NearMiss(version=3, n_neighbors=3)\n# transform the dataset\nX_near3, y_near3 = undersample.fit_resample(scaled_train_data, label)\n\n# CONDENSED NEAREST NEIGHBOUR\nundersample = CondensedNearestNeighbour(n_neighbors=1)\n# transform the dataset\nX_cnn, y_cnn = undersample.fit_resample(scaled_train_data, label)\n\n# TOMEK LINKS\nundersample = TomekLinks()\n# transform the dataset\nX_tomek, y_tomek = undersample.fit_resample(scaled_train_data, label)\n\n# EDITED NEAREST NEIGHBOURS\nundersample = EditedNearestNeighbours(n_neighbors=3)\n# transform the dataset\nX_enn, y_enn = undersample.fit_resample(scaled_train_data, label)\n\n# ONE SIDED SELECTION \nundersample = OneSidedSelection(n_neighbors=1, n_seeds_S=200)\n# transform the dataset\nX_oss, y_oss = undersample.fit_resample(scaled_train_data, label)\n\n# NEIGHBOURHOOD CLEANING RULE\nundersample = NeighbourhoodCleaningRule(n_neighbors=3, threshold_cleaning=0.5)\n# transform the dataset\nX_ncr, y_ncr = undersample.fit_resample(scaled_train_data, label)","8bca84bf":"model = GradientBoostingClassifier()\n\n# FIT ON NEAR MISS VERSION 2\nmodel.fit(X_near2, y_near2)\n# PREDICT ON THE TEST DATA\npredict_near2 = model.predict(scaled_test_data)\n\n# NEAR MISS VERSION 3\nmodel.fit(X_near3, y_near3)\npredict_near3 = model.predict(scaled_test_data)\n\n# CONDENSED NEAREST NEIGHBOUR\nmodel.fit(X_cnn, y_cnn)\npredict_cnn = model.predict(scaled_test_data)\n\n# TOMEK LINKS\nmodel.fit(X_tomek, y_tomek)\npredict_tomek = model.predict(scaled_test_data)\n\n# EDITED NEAREST NEIGHBOURS\nmodel.fit(X_enn, y_enn)\npredict_enn = model.predict(scaled_test_data)\n\n# ONE SIDED SELECTION\nmodel.fit(X_oss, y_oss)\npredict_oss = model.predict(scaled_test_data)\n\n# NEIGHBOURHOOD CLEANING RULE\nmodel.fit(X_ncr, y_ncr)\npredict_ncr = model.predict(scaled_test_data)","3da629d0":"pred_list = [predict_near2, predict_near3, predict_cnn, predict_tomek, predict_enn, predict_oss, predict_ncr]\npal = ['autumn','Blues','Reds_r', 'icefire', 'Wistia','Purples', 'Greens']\ntitles =['NEAR_2', 'NEAR_3', 'Condensed Nearest Neighbour', 'TOMEK_LINKS', 'Edited Nearest Neighbour', 'One Sided Selection', 'Neighborhood Cleaning Rule']\n\nplt.figure(figsize=(25,25))\nplt.rc('font', size=15)\nfor i in range(len(pred_list)):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(pred_list[i], palette=pal[i])\n    plt.title(titles[i])\n    plt.xlabel('CLASSES')\n    plt.ylabel('COUNT')\nplt.show()","cc7cdd5d":"l = [predict_under, predict_near2]\npal = ['Set1','Set2']\ntitles =['NearMiss 1', 'NearMiss 2']\n\nplt.figure(figsize=(25,6))\nplt.rc('font', size=15)\nfor i in range(2):\n    plt.subplot(1,2, i+1)\n    sns.countplot(l[i], palette=pal[i])\n    plt.title(titles[i])\n    plt.xlabel('CLASSES')\n    plt.ylabel('COUNT')\nplt.show()\n\nprint('NearMiss Version 1')\ncounter = Counter(predict_under)\nfor k,v in counter.items():\n    per = v \/ len(predict_under) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n\nprint()\nprint()\nprint('NearMiss Version 2')\ncounter = Counter(predict_near2)\nfor k,v in counter.items():\n    per = v \/ len(predict_near2) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))","97a8b11d":"## COMMON NULL COLUMNS TO BE DELETED","a3f80c4f":"## STRING FEATURE VALUES\n\nList all the features columns that has string values.","963742dd":"## CONCATENATE THE ENCODED DATA WITH TRAINING DATA \n\nAfter concatenation , remove both the string columns","84ca8347":"## FIT THE BEST MODEL ON THE UNDERSAMPLED DATA","b6f3b323":"## TEST MULTIPLE CLASSIFICATION ALGORITHMS ON UNDERSAMPLED DATA\n\nWe will test multiple classification algorithms on undersampled data and choose the algorithm with high F beta score. F beta score is a generalized form of F1 score. F1 score has precision and recall equal weightage. In Fbeta score, you can give weightage to precision and recall using parameter beta. We will give Recall higher preference. \n\nThen the best algorithm will be fit on both UNDERSAMPLED and OVERSAMPLED data and we will compare which resampling technique gives the best results.","de3088a7":"## Alert ! Following code takes a lot of time ","93e19193":"Now the data is balanced.","e8976d05":"## BALANCE THE LABEL DATA BY OVERSAMPLING & UNDERSAMPLING ","fab7083e":"As we can see columns **'pov6' and 'totshopping.rep'** are having most of the null values in test data. 'pov6' is our label or target columns so we can delete this column from the test data and after storing the values of column 'pov6' in another variable named \"label\", we can delete it from the training data as well.\n\nLets analyse **'totshopping.rep'** whether we should delete it or not. ","0620f2cd":"## CLASSIFY HIGHLY UNBALANCED DATA\n\nThis Notebook is a step by step approach to make a classifier on a highly unbalanced dataset with 6 classes in which 77 % of the data falls in Class 1.\n\nI have resampling technique to overcome this unbalanced issue. As you know, there are two ways to handle it either by OVERSAMPLING or UNDERSAMPLING. I have tried both the techniques to see which will perform better.\n\nFor OVERSAMPLING - I used SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line. \n\nFor UNDERSAMPLING - Undersampling techniques falls under two category - \n\n**\"Select To Keep or Select To Delete\"**\n\nSELECT TO KEEP - Techniques under this category selects some of the values from the majority class applying some algorithms. e.g. NearMiss 1,2,3 and Condensed Nearest Neighbor\n\nSELECT TO DELETE - It applies algorithm (KNN) to delete some of the values from the majority class. e.g. Tomek Links, Edited Nearest Neighbors\n\n****\n\n## CONCLUSION\n\nBecause the data is highly dimensional that is it has lots of feature , in that case, oversampling techniques like SMOTE performs very bad, whereas undersampling techniques performed better. \n\nHence we can conclude SMOTE does not perform well on the multidimensional data.","e89c5392":"DATA has 18379 DATA POINTS and 300 FEATURES and the TARGET FEATURE is **'pov6'**","3fa05f07":"As test data has 4595 datapoints we will check for the columns that has null values more than 4000","8cabf158":"As we can see column **respondent.id** is not of any use, so we can delete it. We need to encode column **var9** to numerical values, we can do one hot encoding using **pandas.get_dummies**","1962ec66":"## UNDERSAMPLED vs OVERSAMPLED PREDICTED DISTRIBUTION COMPARISON","0bb6b451":"## Train the UNDERSAMPLED data with different classifiers","e48e4817":"So here we can see that column **'totshopping.rep'** has all the values null in test data but no values null in the training data. Then the question is \"WE SHOULD DELETE IT OR NOT\"?\n\nANSWER IS \"YES\", because test data is what on we need to test our model and if the features are not present in the test data, it is totally irrelevant to include it in the training data.\n\n****\nSo the TOTAL NULL COLUMNS TO BE DELETED are \n\n    train_null_columns + set(test_null_columns) - set(train_null_columns)\n    \n    i.e. train_null_columns + {'pov6', 'totshopping.rep'}\n****","0ec4a427":"## FIT THE BEST MODEL ON THE OVERSAMPLED DATA","ca4d8087":"## MAKE LABEL\n\n'pov6' column as LABEL data \n\n'filtered_data' is our FEATURE data.","484e48d1":"#### NO DUPLICATES FOUND ","98ad3609":"## SAME FOR THE TEST DATA","45f8e3ad":"## FILL NULL VALUES WITH MEAN\n\nFill the null values of each column with mean value of that particular column.","cd75c588":"It has removed 46 columns with most of the null values. It has reduced features from 300 to 254.","e602224c":"## DROP DUPLICATES","3d74753a":"### As we can see OVERSAMPLED data retains the same distribution of the classes same as the training data. Thus we can conclude that the OVERSAMPLING technique is not good.\n\n### Whereas UNDERSAMPLED data, gives a good result as the classes are not skewed. Let's experiment with some more undersampled techniques.","a6ef4797":"## COMPARISON BETWEEN NearMiss 1 vs NearMiss 2","eeaeb446":"# REMOVE NULL VALUES\n\n\nWe will make a dictionary with column as KEY and total number of null values in that column as VALUE. Then we will sort that dictionary in descending order by VALUE and remove those columns that have more than 11000 null values.","e13ae1b9":"## CHECK LABEL OR TARGET COLUMN\n\nCheck the distribution of the 6 classes among the dataset","72a6632c":"As we can see, it is highly unbalanced data with 77 % of the data in class 1 and rest of the data falls in other classes. So we will use resampling technique. \n\nResampling techniques include undersampling and oversampling. **Undersampling** means reducing the majority classes to the minority class and **Oversampling** means increasing the minority classes to the majority class.\n\nWe will not use Undersampling because minority class has 133 datapoints and reducing to such a less number will reduce our datapoints which is not good for the classification. Hence we will go for Oversampling.","c12cbc58":"We will use Repeated Stratified K-Fold model validation technique to train the model. It repeats Stratified K-Fold n times with different randomization in each repetition. Stratified K-Fold cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class. \n\nIt will make sure that model is overfitted.","2d78254f":"## DIFFERENT UNDERSAMPLED TECHNIQUES\n\n### * NearMiss 2\n     NearMiss(version=2, n_neighbors=3)\n### * NearMiss 3\n     NearMiss(version=3, n_neighbors=3)\n### * Condensed Nearest Neighbour\n     CondensedNearestNeighbour(n_neighbors=1)\n### * Tomek Links\n     TomekLinks()\n### * Edited Nearest Neighbours\n     EditedNearestNeighbours(n_neighbors=3)\n\n## COMBINED TECHNIQUES \n\n### * One Sided Selection   (Tomek Links and the Condensed Nearest Neighbor (CNN))\n     OneSidedSelection(n_neighbors=1, n_seeds_S=200)\n### * Neighborhood Cleaning Rule (Condensed Nearest Neighbor & Edited Nearest Neighbors )\n     NeighbourhoodCleaningRule(n_neighbors=3, threshold_cleaning=0.5)","9b2c753d":"### We can see from the prediction plot except Near Miss version 2, no one has given non skewed values","c44ef73e":"## CLASSIFICATION ","3cadcd52":"As we can see GradientBoosting (89%) , Bagging (84%) has great Fbeta scores. We will choose the one with the highest Fbeta score. ","0066fb3f":"## SIMILARLY CHECK FOR THE NULL VALUES IN TEST DATA","b7042946":"## Both have given non-skewed values for this dataset, hence we can conclude that for multidimensional datasets using Undersampling techniques are better.","9c98c6a5":"## NearMiss 1","110f57d0":"## SAME FOR THE TEST DATA","d316747f":"## SMOTE ","3a17c194":"## SCALE THE FEATURES BETWEEN 0-1\n\nScaling is very important to range the columns into same range. MinMaxScaler ranges the data between 0 to 1.","58016036":"## TEST DATA","353183c4":"## SAME FOR TEST DATA","1549793c":"## CHECK WHETHER TEST DATA AND TRAINING DATA BOTH HAVE SAME COLUMNS OR NOT","a8dc1149":"Similarly it has reduced test data features from 299 to 254"}}