{"cell_type":{"26da7c3e":"code","7d0ac7c4":"code","4b19e1ce":"code","53e3c2ba":"code","09d0e323":"code","d171a5f3":"code","77eefdd8":"code","38e32c9e":"code","1effe0e8":"code","04bbdb0a":"code","4f0ffc8f":"code","282520d1":"code","b26b87de":"code","8b7e5724":"code","c06035a7":"code","b22e5efc":"code","6c856dc9":"code","cd282cf2":"code","fd473a01":"code","eac38c29":"code","568a98c8":"code","33504e53":"code","35dd8129":"code","78358fc6":"code","02d30ea7":"code","06670460":"code","68c6d4bd":"code","897d6b85":"code","45c0193a":"code","adb97a84":"code","eeb04796":"code","1601dc8d":"code","63820f38":"code","2b9676c4":"code","f3cbe1fe":"code","3bfc231d":"code","3daffe12":"code","71e99411":"code","ac323517":"code","d06500a4":"code","8a1e96ae":"code","82a1f697":"code","1b9e8ba0":"code","3400bdc0":"code","f3ffc657":"code","8cde1dfa":"code","af7701d8":"code","90b782b4":"code","7dbb0c9f":"code","237b6228":"code","dd481c7d":"code","674723be":"code","3050551e":"code","98e68e4b":"code","1f232687":"code","cd27ba40":"code","e5e5486c":"code","eb66aa6c":"code","9a74ca63":"code","a64d634c":"code","4fde486c":"code","37be30b1":"code","8897d63f":"code","9928177f":"code","879bb615":"code","221a6a89":"code","50e7e5f9":"code","3cce9708":"markdown","402542de":"markdown","d8e5d3cf":"markdown","0fc5e2e7":"markdown","610c9a9a":"markdown","79bcd5a2":"markdown","80597b42":"markdown","ed368233":"markdown","d22fb736":"markdown","1f4d5195":"markdown","813bc6df":"markdown","15cb0f2a":"markdown","69c312b6":"markdown","e14528dd":"markdown","63deffa0":"markdown","6c87517e":"markdown","3fe57d9e":"markdown","aa26102f":"markdown","41ee03f5":"markdown","ca6f3db1":"markdown","555dd811":"markdown","bedb6cd1":"markdown","a897ca21":"markdown","54727b00":"markdown","d483e34f":"markdown","e42b56b4":"markdown","340fbe93":"markdown","8e30d626":"markdown","4205af05":"markdown","449f9281":"markdown","442dc192":"markdown","79bea331":"markdown","65f1b0f7":"markdown","9006e2a4":"markdown","86342d60":"markdown","397d91f4":"markdown","58ed897c":"markdown","1b89176a":"markdown","af90da1a":"markdown","c8d0287c":"markdown","8fa9f38d":"markdown","567d889d":"markdown","f093c239":"markdown","7169eff7":"markdown","4ec8805f":"markdown","f78681df":"markdown","888e45c9":"markdown","f32367a7":"markdown","93a98c4a":"markdown","fb9ddf17":"markdown","bc9b84e4":"markdown","808ef7c3":"markdown","28271946":"markdown","9030a185":"markdown"},"source":{"26da7c3e":"# Import libraries\nimport numpy as np\nprint('numpy version\\t:', np.__version__)\nimport pandas as pd\nprint('pandas version\\t:', pd.__version__)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint('seaborn version\\t:', sns.__version__)\nfrom scipy import stats\n\nimport os\n\npd.set_option('display.max_columns', 200) # to display all the columns\npd.set_option('display.max_rows',150) # to display all rows of df series\npd.options.display.float_format = '{:.4f}'.format #set it to convert scientific noations such as 4.225108e+11 to 422510842796.00\n\nimport warnings\nwarnings.filterwarnings('ignore') # if there are any warning due to version mismatch, it will be ignored\n\nimport random","7d0ac7c4":"# # Sample data to overcome Memory Error\n# # Less RAM: Reduce the data: It's completely fine to take a sample of the data to work on this case study\n# # Random Sampling to get a random sample of data from the complete data\n# filename = \"application_data.csv\"# This file is available is the same location as the jupyter notebook\n\n# # Count the number of rows in my file\n# num_lines = sum(1 for i in open(filename))\n# # The number of rows that I wanted to load\n# size = num_lines\/\/2\n\n# # Create a random indices between these two numbers\n\n# random.seed(10)\n# skip_id = random.sample(range(1, num_lines), num_lines-size)\n\n# df_app = pd.read_csv(filename, skiprows = skip_id)","4b19e1ce":"# read data\ndf_app = pd.read_csv('..\/input\/credit-card\/application_data.csv')","53e3c2ba":"# get shape of data (rows, columns)\nprint(df_app.shape)","09d0e323":"df_app.dtypes.value_counts()","d171a5f3":"# get some insights of data\ndf_app.head()","77eefdd8":"df_app.info()","38e32c9e":"# get the count, size and Unique value in each column of application data\ndf_app.agg(['count','size','nunique'])","1effe0e8":"# funcion to get null value\ndef column_wise_null_percentage(df):\n    output = round(df.isnull().sum()\/len(df.index)*100,2)\n    return output","04bbdb0a":"# get missign values of all columns\nNA_col = column_wise_null_percentage(df_app)\nNA_col","4f0ffc8f":"# identify columns only with null values\nNA_col = NA_col[NA_col>0]\nNA_col","282520d1":"# grafical representation of columns having % null values\nplt.figure(figsize= (20,4),dpi=300)\nNA_col.plot(kind = 'bar')\nplt.title (' columns having null values')\nplt.ylabel('% null values')\nplt.show()\n# plt.savefig('filename.png', dpi=300)","b26b87de":"# Get the column with null values more than 50%\nNA_col_50 = NA_col[NA_col>50]\nprint(\"Number of columns having null value more than 50% :\", len(NA_col_50.index))\nprint(NA_col_50)","8b7e5724":"# removed 41 columns having null percentage more than 50%.\ndf_app = df_app.drop(NA_col_50.index, axis =1)\ndf_app.shape","c06035a7":"# Get columns having <15% null values\nNA_col_15 = NA_col[NA_col<15]\nprint(\"Number of columns having null value less than 15% :\", len(NA_col_15.index))\nprint(NA_col_15)","b22e5efc":"NA_col_15.index","6c856dc9":"# understand the insight of missing columns having <15% null values\ndf_app[NA_col_15.index].describe()","cd282cf2":"# identify unique values in the colums having <15% null value \ndf_app[NA_col_15.index].nunique().sort_values(ascending=False)","fd473a01":"# Box plot for continuious variable\nplt.figure(figsize=(12,4))\nsns.boxplot(df_app['EXT_SOURCE_2'])\nplt.show()","eac38c29":"plt.figure(figsize=(12,4))\nsns.boxplot(df_app['AMT_GOODS_PRICE'])\nplt.show()","568a98c8":"# identify maximum frequency values\nprint('Maximum Frequncy categorical values are,')\nprint('NAME_TYPE_SUITE: ',df_app['NAME_TYPE_SUITE'].mode()[0])\nprint('OBS_30_CNT_SOCIAL_CIRCLE:', df_app['OBS_30_CNT_SOCIAL_CIRCLE'].mode()[0])\nprint('DEF_30_CNT_SOCIAL_CIRCLE:', df_app['DEF_30_CNT_SOCIAL_CIRCLE'].mode()[0])\nprint('OBS_60_CNT_SOCIAL_CIRCLE:', df_app['OBS_60_CNT_SOCIAL_CIRCLE'].mode()[0])\nprint('DEF_60_CNT_SOCIAL_CIRCLE:', df_app['DEF_60_CNT_SOCIAL_CIRCLE'].mode()[0])\n","33504e53":"# Remove unwanted columns from application dataset for better analysis.\n\nunwanted=['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE','FLAG_PHONE', 'FLAG_EMAIL',\n          'REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','FLAG_EMAIL','CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n          'REGION_RATING_CLIENT_W_CITY','FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4',\n          'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6','FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10',\n          'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n          'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18','FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n          'FLAG_DOCUMENT_21','EXT_SOURCE_2','EXT_SOURCE_3','YEARS_BEGINEXPLUATATION_AVG','FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_MODE',\n          'FLOORSMAX_MODE','YEARS_BEGINEXPLUATATION_MEDI','FLOORSMAX_MEDI','TOTALAREA_MODE','EMERGENCYSTATE_MODE']\n\ndf_app.drop(labels=unwanted,axis=1,inplace=True)","35dd8129":"df_app.shape","78358fc6":"df_app.head()","02d30ea7":"# For Code Gender column\n\nprint('CODE_GENDER: ',df_app['CODE_GENDER'].unique())\nprint('No of values: ',df_app[df_app['CODE_GENDER']=='XNA'].shape[0])\n\nXNA_count = df_app[df_app['CODE_GENDER']=='XNA'].shape[0]\nper_XNA = round(XNA_count\/len(df_app.index)*100,3)\n\nprint('% of XNA Values:',  per_XNA)\n\nprint('maximum frequency data :', df_app['CODE_GENDER'].describe().top)","06670460":"# Dropping the XNA value in column 'CODE_GENDER' with \"F\" for the dataset\n\ndf_app = df_app.drop(df_app.loc[df_app['CODE_GENDER']=='XNA'].index)\ndf_app[df_app['CODE_GENDER']=='XNA'].shape","68c6d4bd":"# For Organization column\nprint('No of XNA values: ', df_app[df_app['ORGANIZATION_TYPE']=='XNA'].shape[0])\n\nXNA_count = df_app[df_app['ORGANIZATION_TYPE']=='XNA'].shape[0]\nper_XNA = round(XNA_count\/len(df_app.index)*100,3)\n\nprint('% of XNA Values:',  per_XNA)\n\ndf_app['ORGANIZATION_TYPE'].describe()\n","897d6b85":"# # Dropping the rows have 'XNA' values in the organization type column\n\n# df_app = df_app.drop(df_app.loc[df_app['ORGANIZATION_TYPE']=='XNA'].index)\n# df_app[df_app['ORGANIZATION_TYPE']=='XNA'].shape","45c0193a":"df_app.head()","adb97a84":"# Casting variable into numeric in the dataset\n\nnumeric_columns=['TARGET','CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','REGION_POPULATION_RELATIVE',\n                 'DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','HOUR_APPR_PROCESS_START',\n                 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY',\n                'DAYS_LAST_PHONE_CHANGE']\n\ndf_app[numeric_columns]=df_app[numeric_columns].apply(pd.to_numeric)\ndf_app.head(5)\n","eeb04796":"# Converting '-ve' values into '+ve' Values\ndf_app['DAYS_BIRTH'] = df_app['DAYS_BIRTH'].abs()\ndf_app['DAYS_EMPLOYED'] = df_app['DAYS_EMPLOYED'].abs()\ndf_app['DAYS_REGISTRATION'] = df_app['DAYS_REGISTRATION'].abs()\ndf_app['DAYS_ID_PUBLISH'] = df_app['DAYS_ID_PUBLISH'].abs()\ndf_app['DAYS_LAST_PHONE_CHANGE'] = df_app['DAYS_LAST_PHONE_CHANGE'].abs()","1601dc8d":"# describe numeric columns\ndf_app[numeric_columns].describe()","63820f38":"# Box plot for selected columns\nfeatures = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','DAYS_EMPLOYED', 'DAYS_REGISTRATION']\n\nplt.figure(figsize = (20, 15), dpi=300)\nfor i in enumerate(features):\n    plt.subplot(3, 2, i[0]+1)\n    sns.boxplot(x = i[1], data = df_app)\nplt.show()","2b9676c4":"bins = [0,100000,200000,300000,400000,500000,10000000000]\nslot = ['<100000', '100000-200000','200000-300000','300000-400000','400000-500000', '500000 and above']\n\ndf_app['AMT_INCOME_RANGE']=pd.cut(df_app['AMT_INCOME_TOTAL'],bins,labels=slot)","f3cbe1fe":"bins = [0,100000,200000,300000,400000,500000,600000,700000,800000,900000,10000000000]\nslot = ['<100000', '100000-200000','200000-300000','300000-400000','400000-500000', '500000-600000',\n        '600000-700000','700000-800000','850000-900000','900000 and above']\n\ndf_app['AMT_CREDIT_RANGE']=pd.cut(df_app['AMT_CREDIT'],bins,labels=slot)","3bfc231d":"# Dividing the dataset into two dataset of  target=1(client with payment difficulties) and target=0(all other)\n\ntarget0_df=df_app.loc[df_app[\"TARGET\"]==0]\ntarget1_df=df_app.loc[df_app[\"TARGET\"]==1]","3daffe12":"# insights from number of target values\n\npercentage_defaulters= round(100*len(target1_df)\/(len(target0_df)+len(target1_df)),2)\n\npercentage_nondefaulters=round(100*len(target0_df)\/(len(target0_df)+len(target1_df)),2)\n\nprint('Count of target0_df:', len(target0_df))\nprint('Count of target1_df:', len(target1_df))\n\n\nprint('Percentage of people who paid their loan are: ', percentage_nondefaulters, '%' )\nprint('Percentage of people who did not paid their loan are: ', percentage_defaulters, '%' )","71e99411":"# Calculating Imbalance percentage\n    \n# Since the majority is target0 and minority is target1\n\nimb_ratio = round(len(target0_df)\/len(target1_df),2)\n\nprint('Imbalance Ratio:', imb_ratio)","ac323517":"# Count plotting in logarithmic scale\n\ndef uniplot(df,col,title,hue =None):\n    \n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n    plt.rcParams[\"axes.labelsize\"] = 14\n    plt.rcParams['axes.titlesize'] = 16\n    plt.rcParams['axes.titlepad'] = 14\n    \n    \n    temp = pd.Series(data = hue)\n    fig, ax = plt.subplots()\n    width = len(df[col].unique()) + 7 + 4*len(temp.unique())\n    fig.set_size_inches(width , 8)\n    plt.xticks(rotation=45)\n    plt.yscale('log')\n    plt.title(title)\n    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue) \n        \n    plt.show()","d06500a4":"# Categoroical Univariate Analysis in logarithmic scale\n\nfeatures = ['AMT_INCOME_RANGE', 'AMT_CREDIT_RANGE','NAME_INCOME_TYPE','NAME_CONTRACT_TYPE']\nplt.figure(figsize = (20, 15))\n\nfor i in enumerate(features):\n    plt.subplot(2, 2, i[0]+1)\n    plt.subplots_adjust(hspace=0.5)\n    sns.countplot(x = i[1], hue = 'TARGET', data = df_app)\n    \n    plt.rcParams['axes.titlesize'] = 16\n    \n    plt.xticks(rotation = 45)\n    plt.yscale('log')\n    ","8a1e96ae":"# Categoroical Univariate Analysis in Value scale\n\nfeatures = ['CODE_GENDER','FLAG_OWN_CAR']\nplt.figure(figsize = (20, 10))\n\nfor i in enumerate(features):\n    plt.subplot(2, 2, i[0]+1)\n    plt.subplots_adjust(hspace=0.5)\n    sns.countplot(x = i[1], hue = 'TARGET', data = df_app)\n     \n    plt.rcParams['axes.titlesize'] = 16\n    plt.xticks(rotation = 45)\n#     plt.yscale('log')","82a1f697":"# Univariate Analysis for continous variable\n\nfeatures = ['AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_LAST_PHONE_CHANGE','DAYS_ID_PUBLISH']\nplt.figure(figsize = (15, 20))\n\nfor i in enumerate(features):\n    plt.subplot(3, 2, i[0]+1)\n    plt.subplots_adjust(hspace=0.5)\n    sns.boxplot(x = 'TARGET', y = i[1], data = df_app)\n    ","1b9e8ba0":"# Box plotting for Credit amount\n\nplt.figure(figsize=(16,12))\nplt.xticks(rotation=45)\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_CREDIT', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Credit amount vs Education Status')\nplt.show()","3400bdc0":"# Box plotting for Income amount in logarithmic scale\n\nplt.figure(figsize=(16,12))\nplt.xticks(rotation=45)\nplt.yscale('log')\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_INCOME_TOTAL', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Income amount vs Education Status')\nplt.show()","f3ffc657":"# Box plotting for credit amount\n\nplt.figure(figsize=(15,10))\nplt.xticks(rotation=45)\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_CREDIT', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Credit Amount vs Education Status')\nplt.show()","8cde1dfa":"# Box plotting for Income amount in logarithmic scale\n\nplt.figure(figsize=(16,12))\nplt.xticks(rotation=45)\nplt.yscale('log')\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_INCOME_TOTAL', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Income amount vs Education Status')\nplt.show()","af7701d8":"# Top 10 correlated variables: target 0 dataaframe\n\ncorr = target0_df.corr()\ncorrdf = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\ncorrdf = corrdf.unstack().reset_index()\ncorrdf.columns = ['Var1', 'Var2', 'Correlation']\ncorrdf.dropna(subset = ['Correlation'], inplace = True)\ncorrdf['Correlation'] = round(corrdf['Correlation'], 2)\ncorrdf['Correlation'] = abs(corrdf['Correlation'])\ncorrdf.sort_values(by = 'Correlation', ascending = False).head(10)","90b782b4":"# Top 10 correlated variables: target 1 dataaframe\n\ncorr = target1_df.corr()\ncorrdf = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\ncorrdf = corrdf.unstack().reset_index()\ncorrdf.columns = ['Var1', 'Var2', 'Correlation']\ncorrdf.dropna(subset = ['Correlation'], inplace = True)\ncorrdf['Correlation'] = round(corrdf['Correlation'], 2)\ncorrdf['Correlation'] = abs(corrdf['Correlation'])\ncorrdf.sort_values(by = 'Correlation', ascending = False).head(10)","7dbb0c9f":"# Reading the dataset of previous application\n\ndf_prev=pd.read_csv('..\/input\/credit-card\/previous_application.csv')","237b6228":"#explore the dataset\ndf_prev.columns","dd481c7d":"# get shape of data (rows, columns)\ndf_prev.shape","674723be":"# get the type of dataset\ndf_prev.dtypes","3050551e":"# displaying the informtion of previous application dataset\ndf_prev.info()","98e68e4b":"# Describing the previous application dataset\ndf_prev.describe()","1f232687":"# Finding percentage of null values columns\nNA_col_pre = column_wise_null_percentage(df_prev)","cd27ba40":"# identify columns only with null values\nNA_col_pre = NA_col_pre[NA_col_pre>0]\nNA_col_pre","e5e5486c":"# grafical representation of columns having % null values\nplt.figure(figsize= (20,4),dpi=300)\nNA_col_pre.plot(kind = 'bar')\nplt.title (' columns having null values')\nplt.ylabel('% null values')\nplt.show()","eb66aa6c":"# Get the column with null values more than 50%\nNA_col_pre = NA_col_pre[NA_col_pre>50]\nprint(\"Number of columns having null value more than 50% :\", len(NA_col_pre.index))\nprint(NA_col_pre)","9a74ca63":"# removed 4 columns having null percentage more than 50%.\ndf_prev = df_prev.drop(NA_col_pre.index, axis =1)\ndf_prev.shape","a64d634c":"# Merging the Application dataset with previous appliaction dataset\n\ndf_comb = pd.merge(left=df_app,right=df_prev,how='inner',on='SK_ID_CURR',suffixes='_x')\ndf_comb.shape","4fde486c":"df_comb.head()","37be30b1":"# Renaming the column names after merging from combined df\n\ndf_comb = df_comb.rename({'NAME_CONTRACT_TYPE_' : 'NAME_CONTRACT_TYPE','AMT_CREDIT_':'AMT_CREDIT','AMT_ANNUITY_':'AMT_ANNUITY',\n                         'WEEKDAY_APPR_PROCESS_START_' : 'WEEKDAY_APPR_PROCESS_START',\n                         'HOUR_APPR_PROCESS_START_':'HOUR_APPR_PROCESS_START','NAME_CONTRACT_TYPEx':'NAME_CONTRACT_TYPE_PREV',\n                         'AMT_CREDITx':'AMT_CREDIT_PREV','AMT_ANNUITYx':'AMT_ANNUITY_PREV',\n                         'WEEKDAY_APPR_PROCESS_STARTx':'WEEKDAY_APPR_PROCESS_START_PREV',\n                         'HOUR_APPR_PROCESS_STARTx':'HOUR_APPR_PROCESS_START_PREV'}, axis=1)\n","8897d63f":"# Removing unwanted columns from cmbined df for analysis\n\ndf_comb.drop(['SK_ID_CURR','WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START','REG_REGION_NOT_LIVE_REGION', \n              'REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n              'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY','WEEKDAY_APPR_PROCESS_START_PREV',\n              'HOUR_APPR_PROCESS_START_PREV', 'FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY'],axis=1,inplace=True)","9928177f":"# Distribution of contract status in logarithmic scale\n# Distribution of contract status in logarithmic scale\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\nplt.figure(figsize=(10,25),dpi = 300)\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams['axes.titlesize'] = 22\nplt.rcParams['axes.titlepad'] = 30\nplt.xticks(rotation=90)\nplt.xscale('log')\nplt.title('Distribution of contract status with purposes')\nax = sns.countplot(data = df_comb, y= 'NAME_CASH_LOAN_PURPOSE', \n                   order=df_comb['NAME_CASH_LOAN_PURPOSE'].value_counts().index,hue = 'NAME_CONTRACT_STATUS') ","879bb615":"# Distribution of contract status\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\nplt.figure(figsize=(10,30),dpi = 300)\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams['axes.titlesize'] = 22\nplt.rcParams['axes.titlepad'] = 30\nplt.xticks(rotation=90)\nplt.xscale('log')\nplt.title('Distribution of purposes with target ')\nax = sns.countplot(data = df_comb, y= 'NAME_CASH_LOAN_PURPOSE', \n                   order=df_comb['NAME_CASH_LOAN_PURPOSE'].value_counts().index,hue = 'TARGET') ","221a6a89":"# Box plotting for Credit amount in logarithmic scale\n\nplt.figure(figsize=(20,15),dpi = 300)\nplt.xticks(rotation=90)\nplt.yscale('log')\nsns.boxplot(data =df_comb, x='NAME_CASH_LOAN_PURPOSE',hue='NAME_INCOME_TYPE',y='AMT_CREDIT_PREV',orient='v')\nplt.title('Prev Credit amount vs Loan Purpose')\nplt.show()","50e7e5f9":"# Box plotting for Credit amount prev vs Housing type in logarithmic scale\n\nplt.figure(figsize=(15,15),dpi = 150)\nplt.xticks(rotation=90)\nsns.barplot(data =df_comb, y='AMT_CREDIT_PREV',hue='TARGET',x='NAME_HOUSING_TYPE')\nplt.title('Prev Credit amount vs Housing type')\nplt.show()","3cce9708":"From the above we can conclude some points-\n\nThe credit amount of Loan purposes like 'Buying a home','Buying a land','Buying a new car' and'Building a house' is higher.\nIncome type of state servants have a significant amount of credit applied\nMoney for third person or a Hobby is having less credits applied for.","402542de":"# 6. Conclusion\/Recomendation:","d8e5d3cf":"#### 2.b. Identify and remove columns with high missing percentage (>50%)","0fc5e2e7":"#### 2.a. Find the percentage of missing values of the columns","610c9a9a":"**1. Banks should focus more on contract type \u2018Student\u2019 ,\u2019pensioner\u2019 and \u2018Businessman\u2019 with housing \u2018type other than \u2018Co-op apartment\u2019 for successful payments.**\n\n**2. Banks should focus less on income type \u2018Working\u2019 as they are having most number of unsuccessful payments.**\n\n**3. In loan purpose \u2018Repairs\u2019:**\n\n> a. Although having higher number of rejection in loan purposes with 'Repairs' there are observed difficulties in payment on time.<br> \nb. There are few places where loan payment is delay is significantly high.<br> \nc. Bank should keep continue to caution while giving loan for this purpose.\n\n**4. Bank should avoid giving loans to the housing type of co-op apartment as they are having difficulties in payment.**\n\n**5. Bank can focus mostly on housing type \u2018with parents\u2019 , \u2018House\\apartment\u2019 and \u2018municipal apartment\u2019 for successful payments.**\n","79bcd5a2":"Points to be concluded from above plot:\n\nMost rejection of loans came from purpose 'repairs'.\nFor education purposes we have equal number of approves and rejection\nPayign other loans and buying a new car is having significant higher rejection than approves.","80597b42":"### 3. Analysis:","ed368233":"#### 3.a Univariate analysis","d22fb736":"* There is also have some similarity with Target0, \n* Education type 'Higher education' the income amount is mostly equal with family status. \n* Less outlier are having for Academic degree but there income amount is little higher that Higher education. \n* Lower secondary are have less income amount than others.","1f4d5195":"#### 3.b. Bivariate analysis for numerical variables","813bc6df":"###  1. Data Importing","15cb0f2a":"* The first quartile almost missing for CNT_CHILDREN that means most of the data are present in the first quartile.\n\n* There is single high value data point as outlier present in AMT_INCOME_TOTAL and Removal this point will dtrasticaly impact the box plot for further analysis.\n\n* The first quartiles is slim compare to third quartile for AMT_CREDIT,AMT_ANNUITY, DAYS_EMPLOYED, DAYS_REGISTRATION. This mean data are skewed towards first quartile.","69c312b6":"Inference:\n* Days_Birth: The people having higher age are having higher probability of repayment.\n* Some outliers are observed in In 'AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_EMPLOYED', DAYS_LAST_PHONE_CHANGE in the dataset.\n* Less outlier observed in Days_Birth and DAYS_ID_PUBLISH\n* 1st quartile is smaller than third quartile in In 'AMT_ANNUITY','AMT_GOODS_PRICE', DAYS_LAST_PHONE_CHANGE.\n* In DAYS_ID_PUBLISH: people changing ID in recent days are relativelty prone to be default.\n* There is single high value data point as outlier present in DAYS_EMPLOYED. Removal this point will drastically impact the box plot for further analysis. ","e14528dd":"* Family status of 'civil marriage', 'marriage' and 'separated' of Academic degree education are having higher number of credits than others. \n* Also, higher education of family status of 'marriage', 'single' and 'civil marriage' are having more outliers. Civil marriage for Academic degree is having most of the credits in the third quartile.","63deffa0":"**For Target 1**","6c87517e":"Following age\/days columns are having -ve value, which needs to converted to  +ve value.\n\n```\n'DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE',\n```","3fe57d9e":"* In Education type 'Higher education' the income amount is mostly equal with family status. It does contain many outliers. \n* Less outlier are having for Academic degree but there income amount is little higher that Higher education. \n* Lower secondary of civil marriage family status are have less income amount than others.","aa26102f":"#### 2.d. Check the data type of all the columns and changed the data type.","41ee03f5":"##### Insights: \n> CODE_GENDER:\n    * The % of defaulters are more in Male than Female\n\n\n> FLAG_OWN_CAR:\n    * The person owning car is having higher percentage of defaulter.\n","ca6f3db1":"Inference from box plot:\n* for 'EXT_SOURCE_2' there is no outliers present. And there is no significant diffence observed between mean and median. However data look to be right skewed. So missing values can be imputed with median value: 0.565\n* for 'AMT_GOODS_PRICE' there is significant number of outlier present in the data. SO data should be imputed with median value: 450000\n","555dd811":"##### Insights:<br>\n\n> AMT_INCOME_RANGE : \n    * The people having 100000-200000 are havign higher number of loan and also having higher in defaulter\n    * The income segment having >500000 are having less defaulter.\n\n> AMT_CREDIT_RANGE:\n    * The people having <100000 loan are less defaulter.\n    * income having more thatn >100000 are almost equal % of loan defaulter\n\n> NAME_INCOME_TYPE:\n    * Student pensioner and business have higher percentage of loan repayment.\n    * Working, State servent and Commercial associates have higher default percentage.\n    * Maternity category is significantly higher problem in replayement.\n\n> NAME_CONTRACT_TYPE\n    * For contract type \u2018cash loans\u2019 is having higher number of credits than \u2018Revolving loans\u2019 contract type.\n    * From the above graphs we can see that the Revolving loans are small amount compared to Cash loans but the % of non payment for the revolving loans are comapritvely high.","bedb6cd1":"* Droped all columns from Dataframe for which missing value percentage are more than 50%.\n``````    \n    'AMT_DOWN_PAYMENT', 'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED'\n``````","a897ca21":"#### 2.f. Bin Creation","54727b00":"#### 2.e Checking the outlier for numerical variables:","d483e34f":"The Imbalance ratio is 11.48","e42b56b4":"Creating bins for continous variable categories column 'AMT_INCOME_TOTAL' and 'AMT_CREDIT'","340fbe93":"Categorical Univariate Analysis in logarithmic scale for target=0 (client with no payment difficulties)","8e30d626":"### 2. Data Quality Check and Missing Values","4205af05":"* The columns having null values less than 15% are,\n\n> 'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE', 'EXT_SOURCE_2','OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n       'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE','AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',\n       'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'\n\n* These columns shall be imputed with suitable values which shall be explained subsequently. ","449f9281":"So, for column 'ORGANIZATION_TYPE', we have total count of 153755 rows of which 27737 rows are having 'XNA' values. Which means 18% of the column is having this values.","442dc192":"From the above box plot and descibe analysis we found that following are the numeric columns are having outliers:\n~~~~~~~~~\nCNT_CHILDREN, AMT_INCOME_TOTAL,AMT_CREDIT,AMT_ANNUITY,DAYS_EMPLOYED, DAYS_REGISTRATION\n~~~~~~~~","79bea331":"* Observations are Quite similar with Target 0 \n* Family status of 'civil marriage', 'marriage' and 'separated' of Academic degree education are having higher number of credits than others. \n* Most of the outliers are from Education type 'Higher education' and 'Secondary'. \n* Civil marriage for Academic degree is having most of the credits in the third quartile.","65f1b0f7":"###  II. Import Libraries and set required parameters","9006e2a4":"**For Target 0**","86342d60":"* Droped all columns from Dataframe for which missing value percentage are more than 50%.\n\n`````````````\n       'OWN_CAR_AGE', 'EXT_SOURCE_1', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n       'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n       'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG',\n       'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n       'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BUILD_MODE',\n       'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMIN_MODE',\n       'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n       'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI',\n       'BASEMENTAREA_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n       'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI',\n       'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI',\n       'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE',\n       'WALLSMATERIAL_MODE'\n```````````","397d91f4":"# Bank Loan Exploratory Data Analysis\n****\nBy: Santh Raul and Ramlal Naik","58ed897c":"##### Continous variable:","1b89176a":"### 4. Read Previous Application data and merging with application data","af90da1a":"### 3.c. Correlation:","c8d0287c":"There are some columns where the value is mentioned as 'XNA' which means 'Not Available'. So we have to find the number of rows and columns.","8fa9f38d":"For categorical vriable the value which should be imputed with maximum in frequency.<br>\nSo the value to be imputed are:<br>\nNAME_TYPE_SUITE:  Unaccompanied<br>\nOBS_30_CNT_SOCIAL_CIRCLE: 0.0 <br>\nDEF_30_CNT_SOCIAL_CIRCLE: 0.0<br>\nOBS_60_CNT_SOCIAL_CIRCLE: 0.0<br>\nDEF_60_CNT_SOCIAL_CIRCLE: 0.0<br>\n","567d889d":"Getting top 10 correlation between variables","f093c239":"#### Univariate analysis Continuious variables:","7169eff7":"#### Categorical variables:","4ec8805f":"Get some insights of data","f78681df":"#### 2.c. identify columns with less missing missing values (<15%)","888e45c9":"Here for Housing type, office appartment is having higher credit of target 0 and co-op apartment is having higher credit of target 1. So, we can conclude that bank should avoid giving loans to the housing type of co-op apartment as they are having difficulties in payment. Bank can focus mostly on housing type with parents or House\\appartment or miuncipal appartment for successful payments.","f32367a7":"**Bivariate analysis**","93a98c4a":"** Performing univariate analysis**","fb9ddf17":"* **For analysis of imputation selecetd 7 varibles.**\n<br>Continuious variables:\n``````\n> 'EXT_SOURCE_2','AMT_GOODS_PRICE'\n``````\nCategorical variables:\n`````````\n> 'OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','NAME_TYPE_SUITE'\n`````````\n","bc9b84e4":"# I. Problem Statement:\n* If the applicant is likely to repay the loan, then not approving the loan results in a loss of business to the company\n* If the applicant is not likely to repay the loan, i.e. he\/she is likely to default, then approving the loan may lead to a financial loss for the company.\n\nThe company wants to understand the driving factors (or driver variables) behind the loan default. i.e the variables which are strong in loan default.\n","808ef7c3":"\n* From the above correlation analysis it is infered that the highest corelation (1.0) is between (OBS_60_CNT_SOCIAL_CIRCLE with OBS_30_CNT_SOCIAL_CIRCLE) and (FLOORSMAX_MEDI with FLOORSMAX_AVG) which is same for both the data set.","28271946":"Since, Female is having the majority and only 2 rows are having XNA values, we can impute those with Gender 'F' as there will be no impact on the dataset. Also there will no impact if we drop those rows.","9030a185":"Few points we can conclude from abpve plot:\n\nLoan purposes with 'Repairs' are facing more difficulites in payment on time.\nThere are few places where loan payment is significant higher than facing difficulties. They are 'Buying a garage', 'Business developemt', 'Buying land','Buying a new car' and 'Education' Hence we can focus on these purposes for which the client is having for minimal payment difficulties."}}