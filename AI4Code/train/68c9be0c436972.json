{"cell_type":{"0647fe87":"code","52af2778":"code","2ac6c06f":"code","468ff76e":"code","cbd727f5":"code","ef7101f2":"code","4290e940":"code","1e8bcbc0":"code","bfc430b1":"code","76185946":"code","933cbbd2":"code","c3cef097":"code","b34128e8":"code","d80809da":"code","b04ae15e":"code","2023a21e":"code","add95292":"code","1157b921":"code","3c9fbd75":"code","71037838":"code","1cd90f29":"code","39f59c43":"code","80f18c27":"code","833d192b":"code","54780a02":"code","a45917a0":"code","58c56d0f":"code","2bfab53c":"code","9881479b":"code","f5566b6c":"code","7dd01dcb":"code","1814ef8a":"code","8e68f9f8":"code","3a0febfb":"code","71fbe4fd":"code","8e055d57":"code","b36bdc5e":"code","1ec654bc":"markdown","019c5fb0":"markdown","b2d1e495":"markdown","87010a7b":"markdown","b2abc72c":"markdown","498e4202":"markdown","261240fd":"markdown","150d7d7b":"markdown","4c42c9b2":"markdown","68f1e245":"markdown","2da98fe9":"markdown","068a5e77":"markdown","e22273dc":"markdown","d506cccd":"markdown","1e4c6a3f":"markdown","13110961":"markdown","04df2fe9":"markdown","d13d968e":"markdown","86eb20da":"markdown","92fbb509":"markdown","4f963036":"markdown","9cea95fb":"markdown","12e7cf1a":"markdown","e88116c4":"markdown","b523695f":"markdown","095c5c75":"markdown","b3855b92":"markdown"},"source":{"0647fe87":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n","52af2778":"data =  pd.read_csv('..\/input\/parkinsons-data-set\/parkinsons.data', sep=\",\")","2ac6c06f":"#data shape \nprint(f'Shape of the dataset {data.shape}')","468ff76e":"#columns in dataset\ndata.columns","cbd727f5":"pd.set_option('max_columns', 25)\ndata.head()","ef7101f2":"data.tail()","4290e940":"# descriptive statistics for numerical columns\ndata.describe().style.background_gradient(cmap = 'copper')","1e8bcbc0":"# lets check the boxplots for the columns where we suspect for outliers\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('fivethirtyeight')\n\n\nplt.subplot(1, 3, 1)\nsns.boxplot(data['MDVP:Fo(Hz)'], color = 'red')\n\n\nplt.subplot(1, 3, 2)\nsns.boxplot(data['MDVP:Fhi(Hz)'], color = 'red')\n\nplt.subplot(1, 3, 3)\nsns.boxplot(data['MDVP:Flo(Hz)'], color = 'red')\n\nplt.suptitle('Box Plot', fontsize = 20)\nplt.show()","bfc430b1":"data = data[data['MDVP:Fhi(Hz)'] < 336]","76185946":"# descriptive statistics for catagorical column\ndata.describe(include = 'object')","933cbbd2":"data.drop('name', axis=1, inplace = True)","c3cef097":"data.isna().mean().round(4)*100","b34128e8":"print(f'Number of duplicate rows = {data.duplicated().sum()}')","d80809da":"plt.rcParams['figure.figsize'] = (10, 5)\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1, 2, 1)\nsns.countplot(data['status'])\n\nplt.xlabel('Healthy or Not healthy', fontsize = 12)\n\nplt.subplot(1, 2, 2)\ndata['status'].value_counts().plot(kind = 'pie', explode = [0, 0.1], \n                                   autopct = '%.2f%%', startangle = 90, labels = ['1','0'])\n\nplt.suptitle('Target Class Balance', fontsize = 15)\nplt.show()","b04ae15e":"plt.rcParams['figure.figsize'] = (15, 4)\nplt.subplot(131)\nsns.kdeplot(data.loc[(data['status']==1), 'MDVP:Fo(Hz)'], color='r', shade=True, Label='1')\nsns.kdeplot(data.loc[(data['status']==0), 'MDVP:Fo(Hz)'], color='g', shade=True, Label='0')\nplt.xlabel('Average vocal fundamental frequency', fontsize=12)\n\nplt.subplot(132)\nsns.kdeplot(data.loc[(data['status']==1), 'MDVP:Fhi(Hz)'], color='r', shade=True, Label='1')\nsns.kdeplot(data.loc[(data['status']==0), 'MDVP:Fhi(Hz)'], color='g', shade=True, Label='0')\nplt.xlabel('Maximum vocal fundamental frequency', fontsize=12)\n\nplt.subplot(133)\nsns.kdeplot(data.loc[(data['status']==1), 'MDVP:Flo(Hz)'], color='r', shade=True, Label='1')\nsns.kdeplot(data.loc[(data['status']==0), 'MDVP:Flo(Hz)'], color='g', shade=True, Label='0')\nplt.xlabel('Minimum vocal fundamental frequency', fontsize=12)\n\nplt.suptitle('Average, Maximum and Minimum vocal fundamental frequency')\nplt.show()","2023a21e":"plt.rcParams['figure.figsize'] = (15,4)\nplt.subplot(131)\nsns.boxenplot(data['status'], data['spread1'], palette = 'copper')\n\nplt.subplot(132)\nsns.boxenplot(data['status'], data['spread2'], palette = 'copper')\n\nplt.subplot(133)\nsns.boxenplot(data['status'], data['PPE'], palette = 'copper')\n\nplt.show()","add95292":"plt.rcParams['figure.figsize'] = (15, 4)\nsns.pairplot(data,hue = 'status', vars = ['MDVP:Jitter(%)','MDVP:Jitter(Abs)','MDVP:RAP','MDVP:PPQ', 'Jitter:DDP'] )\nplt.show()","1157b921":"plt.rcParams['figure.figsize'] = (15, 4)\nsns.pairplot(data,hue = 'status', vars = ['MDVP:Shimmer','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ','Shimmer:DDA'] )\nplt.show()\n","3c9fbd75":"plt.rcParams['figure.figsize'] = (15,4)\nplt.subplot(121)\nsns.boxplot(data['status'], data['RPDE'], palette = 'gray')\n\nplt.subplot(122)\nsns.boxplot(data['status'], data['D2'], palette = 'gray')\n\nplt.show()","71037838":"plt.rcParams['figure.figsize']=18,10\n\nsns.heatmap(data.corr(), annot=True)","1cd90f29":"x=data.drop('status', axis=1)\ny=data['status']\n\nfrom xgboost import XGBClassifier\nfrom boruta import BorutaPy\n\nmodel=XGBClassifier()\n\nboruta = BorutaPy(estimator=model, n_estimators = 'auto',\n                 max_iter = 100, random_state = 0)\n\nboruta.fit(np.array(x), np.array(y))\n\nbest_features = x.columns[boruta.support_].to_list()\nprint(best_features)","39f59c43":"data_copy = data.copy()\ndata = data[['MDVP:Fo(Hz)', 'MDVP:Shimmer(dB)', 'HNR', 'spread1', 'PPE', 'status']]","80f18c27":"from imblearn.over_sampling import SMOTE\n\nx_resample, y_resample  = SMOTE().fit_resample(data.iloc[:,:-1], data['status'])\n\n# lets print the shape of x and y after resampling it\nprint(x_resample.shape)\nprint(y_resample.shape)","833d192b":"print(\"Before Resampling :\")\nprint(y.value_counts())\n\nprint(\"After Resampling :\")\nprint(np.bincount(y_resample))","54780a02":"from sklearn.model_selection import train_test_split\n#train and test data \nX_train, X_test, y_train, y_test = train_test_split(x_resample, y_resample, test_size=0.25, random_state=0)","a45917a0":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scl = scaler.fit_transform(X_train)\n\nX_test_scl = scaler.transform(X_test)","58c56d0f":"from sklearn.neighbors import KNeighborsClassifier","2bfab53c":"#let's find optimized n_neighbour value\n\nerror_rate = []\nfor i in range(1,25):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_scl,y_train)\n    pred_i = knn.predict(X_test_scl)\n    error_rate.append(np.mean(pred_i != y_test))","9881479b":"plt.figure(figsize=(9,5))\nplt.plot(range(1,25),error_rate, marker='o',\n         markerfacecolor='black', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","f5566b6c":"knn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train_scl,y_train)","7dd01dcb":"y_test_pred = knn.predict(X_test_scl)","1814ef8a":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score","8e68f9f8":"plt.rcParams['figure.figsize']=5,5\nsns.heatmap(confusion_matrix(y_test,y_test_pred), annot=True)","3a0febfb":"print(accuracy_score(y_test, y_test_pred))\nprint(classification_report(y_test,y_test_pred))","71fbe4fd":"#from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nXG = XGBClassifier()\n#RF = RandomForestClassifier()\nXG.fit(X_train, y_train)\n\ny_test_pred2 = XG.predict(X_test)","8e055d57":"plt.rcParams['figure.figsize']=5,5\nsns.heatmap(confusion_matrix(y_test,y_test_pred2), annot=True)","b36bdc5e":"print(accuracy_score(y_test, y_test_pred2))\nprint(classification_report(y_test,y_test_pred2))","1ec654bc":"### Descriptive Statistics","019c5fb0":"### Duplicates checker","b2d1e495":"It's clear that we have no duplicate records in our data","87010a7b":"We can see that if the number of neighbors value increases more than 7, the error rate also increases. So we are choosing 7 as the n_neighbors value.","b2abc72c":"from this graphs we can see that vocal frequency for healthy people is genaraly between 100 to 300. But for the Parkinson people most of the records are between 100 to 200 only","498e4202":"### Accuracy matrics of KNN","261240fd":"# XG Boost model building","150d7d7b":"### Data splitting","4c42c9b2":"### Imbalance treatment","68f1e245":"We can clearly see from the result that we have no missing values in our dataset","2da98fe9":"We can easily see a high\u00a0imbalance in target feature\u00a0\"status\"\u00a0and the classes in target feature\u00a0have to be balanced.\u00a0 We'll\u00a0have very low results, which'll\u00a0totally biassed in relation to class with a higher distribution, when we use machine learning models with imbalanced data.","068a5e77":"It is clearly shown that the \"name\" column have 100% uniquness. So it'll not going to contribute much for building the model.","e22273dc":"### Examining the dataset","d506cccd":"# KNN Model building ","1e4c6a3f":"### Accuracy matrics of XGB","13110961":"It is clearly shown that most of the featurs have no outlies in the dataset, but we are suspencting 3 feature ('MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)'). So let's plot these 3 features and check for outliers in it","04df2fe9":"### Feature Selection","d13d968e":"### Target feature analysis","86eb20da":"These 3 box plots showing that the feature \"MDVP:Fo(Hz)\" have no outliers \n\nIn the feature \"MDVP:Flo(Hz)\", we can see some points after the Max Value, which can be termed to be as Outliers. We do not need to remove these values, as the values are not very far and Huge.\n\nIn the feature \"MDVP:Fhi(Hz)\", we can see some noticable outliers. So let's remove those from the dataset","92fbb509":"From the above pair plot we can understand that all these fundamental frequencies are highly correlated with eachother.","4f963036":"It clearly shown that nonlinear measures of fundamental frequency variation of healthy people is always lower than the parkinson affected people","9cea95fb":"### Feature Selection using Boruta\n\n* Boruta can be used on any tree based models\n* it creates new shadow features for every orginal features. \n* Shadow feature means randomly select some feature and shuffle it's values and creating new feature with it\n* It will train and calculate the importance of the features.\n* If orginal feature performed better than it's shadow feature, then it will mark it as important feature or else it will not","12e7cf1a":"From the above pair plot we can understand that all these measures variation in amplitude are highly correlated with eachother.","e88116c4":"### Missing value Treatment","b523695f":"* name - ASCII subject name and recording number\n* MDVP:Fo(Hz) - Average vocal fundamental frequency\n* MDVP:Fhi(Hz) - Maximum vocal fundamental frequency\n* MDVP:Flo(Hz) - Minimum vocal fundamental frequency\n* MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency\n* MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\n* NHR,HNR - Two measures of ratio of noise to tonal components in the voice\n* status - Health status of the subject (one) - Parkinson's, (zero) - healthy\n* RPDE,D2 - Two nonlinear dynamical complexity measures\n* DFA - Signal fractal scaling exponent\n* spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation","095c5c75":"### Data description","b3855b92":"* In this correlation heatmap, we can see that many independent features are highly correlated with eachother. So, to omit multicolinearity let's work on features. "}}