{"cell_type":{"3fbcd866":"code","b5523fd1":"code","cac53ddd":"code","6e023ba8":"code","398cd1d5":"code","1ebc8124":"code","6df515b9":"code","1f89f963":"code","0628ab11":"code","06add601":"code","c68ed4d7":"code","bf699e78":"code","8c44a7bc":"code","d440ce7b":"code","f86e1ce5":"code","1f996bf8":"code","5d8875f0":"code","e33e7ef6":"code","0cee91d3":"code","27f21bae":"code","c71b929d":"code","300fdfdb":"code","e2f1ab12":"code","ea6825d9":"code","e4138b89":"markdown","569a7f3c":"markdown","c975ce1c":"markdown","161994b0":"markdown","711c5130":"markdown","90c69ad6":"markdown","cb7ea97a":"markdown","135e98fc":"markdown"},"source":{"3fbcd866":"import tensorflow as tf\n\nimport os\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.image import extract_patches_2d\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('Agg')     \n%matplotlib inline\n\nimport pandas as pd\n\nimport numpy as np\nimport cv2","b5523fd1":"base_dir = '..\/input\/Kannada-MNIST\/'","cac53ddd":"L = 10 # labels\nS = 28 # dimensions (H, W)\nC = 1  # depth","6e023ba8":"# Load the Kannada MNIST training dataset downloaded from Kaggle\n# into a Pandas DataFrame\nX_df = pd.read_csv(os.path.join(base_dir, 'train.csv'))","398cd1d5":"# Plot pixel intensities\nX_df.head()","1ebc8124":"# Extract dataset samples pixel intensities\nX = X_df.iloc[:, 1:]\nX = X.values \n\n# Extract labels\ny = X_df.iloc[:, 0]\ny = y.values\n\n# Plot labels\npd.DataFrame(y)\n\ndel X_df","6df515b9":"# Reshape dataset samples to (num_samples, height[rows], width[cols])\n# to do some plotting\nX = X.reshape(X.shape[0], S, S)\nX.shape","1f89f963":"# Plot a sample of the dataset\nplt.imshow(X[420])\nplt.title(label='Image at position 420 (blaze it)')\nplt.show()","0628ab11":"# Plot more samples\n\nfig, ax = plt.subplots(\n    nrows=6,\n    ncols=5,\n    figsize=[6, 8]\n)\n\nfor index, axi in enumerate(ax.flat):\n    axi.imshow(X[index])\n    axi.set_title(f'Image #{index}')\n\nplt.tight_layout(True)\nplt.show()","06add601":"# Normalize pixel intensities to range [0, 1]\nX = X.astype('float32') \/ 255.0","c68ed4d7":"# Reshape the samples from the dataset\n# to meet Keras requirements (num_samples, height, width, num_channels)\nX = X.reshape(-1, 28, 28, 1)\nX.shape","bf699e78":"# One-hot encode labels from integers to vectors\ny = to_categorical(y, num_classes=L)\npd.DataFrame(y)","8c44a7bc":"# Split the dataset into 80\/20 train\/test sets\n(X_train, X_ttest, y_train, y_ttest) = train_test_split(X, y, test_size = 0.2, random_state=42)\nprint(X_train.shape) # <- num of train samples\nprint(X_ttest.shape) # <- num of test  samples\n\ndel X","d440ce7b":"# Data Augmentation: Set the image generator\n\ndaug = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=12,\n    zoom_range = 0.3, \n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=False,\n    vertical_flip=False\n)","f86e1ce5":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, LeakyReLU\n\nclass CustomNet(object):\n    @staticmethod\n    def build(width, height, num_classes, depth=3):\n        model = Sequential()\n        input_shape = (height, width, depth)\n        chan_dim = -1\n        \n        # (Conv => LReLU => BN) * 3 => POOL => DO\n        model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        \n        # (Conv => LReLU => BN) * 3 => POOL => DO\n        model.add(Conv2D(96, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(96, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(96, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.2))\n        \n        # (Conv => LReLU => BN) * 3 => POOL => DO\n        model.add(Conv2D(128, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(128, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(128, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.2))\n        \n        # (Conv => LReLU => BN) * 3 => POOL => DO\n        model.add(Conv2D(256, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(256, (3, 3), padding='same'))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.2))\n        \n        # (FC => LReLU => BN) * 2 => DO\n        model.add(Flatten())\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n        model.add(Dense(128))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n        \n        # Softmax\n        model.add(Dense(num_classes))\n        model.add(Activation('softmax'))\n        \n        print(model.summary())\n        \n        return model","1f996bf8":"net = CustomNet()\n\nmodel = net.build(\n    width=S,\n    height=S,\n    num_classes=L,\n    depth=C)","5d8875f0":"num_epochs = 80","e33e7ef6":"init_lr = 0.002\npower = 2.0\n\nadam_opt = Adam(\n    lr=init_lr,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-08,\n    decay=0.0\n)\n\n\ndef polynomial_decay(epoch):\n    max_epochs = num_epochs\n    \n    return init_lr * (1 - (epoch \/ float(max_epochs))) ** power","0cee91d3":"# Plot polynomial decay\nx = np.linspace(0, num_epochs)\nfx = [init_lr * (1 - (i \/ float(num_epochs))) ** power for i in range(len(x))]\nplt.plot(x, fx)\nplt.title(label=f'Polynomial decay, power {power}')\nplt.show()","27f21bae":"# Ignore checkpointing on Kaggle kernel\n\n'''\ncheckpointHandler = ModelCheckpoint(\n    os.path.join(base_dir, 'best_weights.hdf5'),\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n'''\n\ncallbacks = [\n    LearningRateScheduler(polynomial_decay),\n    # checkpointHandler\n]","c71b929d":"batch_size = 128\n\nprint('# Compiling the model...')\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=adam_opt,\n    metrics=['accuracy']\n)\n\nprint('# Training the network...')\nh = model.fit_generator(\n    daug.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_ttest, y_ttest),\n    epochs=num_epochs,\n    steps_per_epoch=len(X_train) \/\/ batch_size,\n    callbacks=callbacks,\n    verbose=1\n)","300fdfdb":"label_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\nprint('Confusion matrix:')\npreds = model.predict(X_ttest, batch_size=batch_size)\nprint(classification_report(y_ttest.argmax(axis=1),\npreds.argmax(axis=1), target_names=label_names))","e2f1ab12":"plt.style.use('ggplot')\nplt.figure(figsize=(10, 7))\nplt.plot(np.arange(0, num_epochs), h.history['loss'], label='train_loss')\nplt.plot(np.arange(0, num_epochs), h.history['val_loss'], label='val_loss')\nplt.plot(np.arange(0, num_epochs), h.history['accuracy'], label='train_accuracy')\nplt.plot(np.arange(0, num_epochs), h.history['val_accuracy'], label='val_accuracy')\n\nplt.title('Training Loss and Accuracy')\nplt.ylabel('Loss\/Accuracy')\nplt.xlabel('Epoch #')\nplt.legend()\n\nplt.show()","ea6825d9":"'''\nPushing a submission to Kaggle.\n\nFirst of, load the test set, normalize it and reshape it.\nThen, make predictions via the trained model and build the\nsubmission csv file.\n'''\n\nsub_X_test = pd.read_csv(os.path.join(base_dir, 'test.csv'))     # Load CSV\nsub_X_test = sub_X_test.drop('id', axis=1)                       # Drop the ID column\nsub_X_test = sub_X_test.iloc[:,:].values                         # Get raw pixel intensities\nsub_X_test = sub_X_test.reshape(sub_X_test.shape[0], S, S, C)    # Reshape to meet Keras requirements\nsub_X_test = sub_X_test \/ 255.0                                  # Normalize to range [0, 1]\n\npreds = model.predict_classes(sub_X_test, batch_size=batch_size)        # Make predictions and build the submission file\n\n# https:\/\/www.kaggle.com\/c\/Kannada-MNIST\/discussion\/110586\nid_col = np.arange(preds.shape[0])\nsubmission = pd.DataFrame({'id': id_col, 'label': preds})\nprint(pd.DataFrame(submission))\nsubmission.to_csv('submission.csv', index = False)","e4138b89":"# Training time!","569a7f3c":"# Build the model","c975ce1c":"# Setup","161994b0":"# Kaggle","711c5130":"# Load dataset","90c69ad6":"# Visualize curves","cb7ea97a":"# Evaluate","135e98fc":"# Set the optimizer and hyperparameters"}}