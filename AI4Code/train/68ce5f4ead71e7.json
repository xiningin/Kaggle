{"cell_type":{"934d8585":"code","c6735c0a":"code","da493d67":"code","250effe0":"code","2845ad06":"code","0ede16da":"code","d77f81b8":"code","975eda31":"code","df33cef2":"code","4b132fff":"code","869e8320":"code","a20b6319":"code","27346087":"code","1c7b471c":"code","7705a9ce":"code","36ae6682":"code","5975d863":"code","a5b8c2a4":"code","1239ffb1":"code","dad84ef1":"code","9f9c708a":"code","273fd578":"code","4923f2fc":"markdown","9dab0228":"markdown","3c38a9b9":"markdown","d9724b3a":"markdown","da719c9e":"markdown","23288fe6":"markdown","19e9235f":"markdown","ef3ea8c3":"markdown"},"source":{"934d8585":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6735c0a":"import warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata.head()","da493d67":"data=data.drop('Unnamed: 32', axis=1)\ny=data.diagnosis\nX=data.drop('diagnosis', axis=1)\nX.info()","250effe0":"X.isnull().sum()","2845ad06":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ny=le.fit_transform(y)\nX=X.drop('id', axis=1)\nfrom sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=42)\nX_val1,X_test,y_val1,y_test=train_test_split(X_val,y_val,test_size=0.5, random_state=42)\nprint('train set: ', X_train.shape,y_train.shape)\nprint('test set: ', X_test.shape, y_test.shape)\nprint('val1 set: ', X_val1.shape, y_val1.shape)","0ede16da":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Activation, Input, Dense, Dropout,Add, BatchNormalization ","d77f81b8":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","975eda31":"callbacks=[EarlyStopping(patience=200,\n                        min_delta=0.00001,\n                        restore_best_weights=True),\n          ReduceLROnPlateau(factor=0.5, patience=200)]","df33cef2":"input_tensor=Input(shape=(30,))\nD1=Dense(512, input_shape=(30,))(input_tensor)\nA1=Activation('relu')(D1)\nA1=Dropout(0.5)(A1)\nA1=BatchNormalization()(A1)\nD2=Dense(512)(A1)\nA2=Activation('relu')(D2)\nA2=Dropout(0.5)(A2)\nA2=BatchNormalization()(A2)\nD3=Dense(512)(A2)\nA3=Activation('relu')(D3)\nA3=Dropout(0.5)(A3)\nA3=BatchNormalization()(A3)\nD4=Dense(512)(A3)\nD4=Add()([D4,A1])\nA4=Activation('relu')(D4)\nA4=Dropout(0.5)(A4)\nA4=BatchNormalization()(A4)\n\n\nD5=Dense(256)(A4)\nA5=Activation('relu')(D5)\nA5=Dropout(0.2)(A5)\nA5=BatchNormalization()(A5)\nD6=Dense(256)(A5)\nA6=Activation('relu')(D6)\nA6=Dropout(0.2)(A6)\nA6=BatchNormalization()(A6)\noutput_tensor=Dense(1, activation='sigmoid')(A6)\n\nfunctional_model=Model(inputs= input_tensor,\n                      outputs=output_tensor)\n","4b132fff":"functional_model.compile(loss='binary_crossentropy',\n                        optimizer='adam',\n                        metrics=['binary_accuracy'])","869e8320":"history=functional_model.fit(X_train,y_train,\n                    validation_data=(X_val1,y_val1),\n                    epochs=500,\n                    batch_size=128,\n                    callbacks=callbacks)","a20b6319":"history_df=pd.DataFrame(history.history)\nhistory_df[['loss','val_loss']].plot()","27346087":"history_df[['binary_accuracy','val_binary_accuracy']].plot();","1c7b471c":"from sklearn.metrics import mean_squared_error\npreds=functional_model.predict(X_test)\nnp.sqrt(mean_squared_error(preds,y_test))","7705a9ce":"error=0.16612618888540173","36ae6682":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nxgb=XGBClassifier()\nxgb.fit(X_train,y_train)\npreds=xgb.predict(X_val)\nprint('XGB Accuracy: ',np.sqrt(mean_squared_error(preds,y_val)))","5975d863":"grid_params={'n_estimators': [100,200,300,400,50],\n            'learning_rate': [0.010,0.001,0.0001,1],\n            'max_depth':[5,6,4,7]}\ngrid_model=GridSearchCV(estimator=XGBClassifier(),param_grid=grid_params,cv=3)\ngrid_model.fit(X_train,y_train)\ngrid_model.best_params_","a5b8c2a4":"xgb_optimized=XGBClassifier(learning_rate= 0.01, max_depth= 4, n_estimators=400)\nxgb_optimized.fit(X_train,y_train)\npreds=xgb_optimized.predict(X_val)\nerror2=np.sqrt(mean_squared_error(preds,y_val))\nprint('Error of the optimized XGBClassifier model: ', str(error2))","1239ffb1":"from sklearn.ensemble import RandomForestClassifier","dad84ef1":"grid_params={'n_estimators': [100,200,300,400,50],\n            'max_depth':[5,6,4,7]}\ngrid_model=GridSearchCV(estimator=rf,param_grid=grid_params,cv=3)\ngrid_model.fit(X_train,y_train)\ngrid_model.best_params_\n\n\n","9f9c708a":"rf_optimized=RandomForestClassifier(max_depth= 7, n_estimators= 100)\nrf_optimized.fit(X_train,y_train)\npreds=rf_optimized.predict(X_val)\nerror3=np.sqrt(mean_squared_error(preds,y_val))\nprint('Error of the Optimized Random Forest Model: ', error3)\n","273fd578":"plt.figure(figsize=(10,6))\nsns.barplot(x=['XGBClassifier','RandomForest','Functional ANN',],y=[error2,error3,error])\nplt.title('Error Rates for Different Predictor Models')\nplt.xlabel('$Models$')\nplt.ylabel('$Error Rates$')\nplt.show()","4923f2fc":"# References:\n","9dab0228":"# 2. XGBoost","3c38a9b9":"# 2.1.Boosting the Accuracy","d9724b3a":"# 3.RandomForest Classifier","da719c9e":"<img src= \"https:\/\/i.imgur.com\/tHiVFnM.png\" alt =\"Curves\" style='width: 500px;'>\n\nFigure 1","23288fe6":"[1] https:\/\/www.lexico.com\/definition\/overfitting","19e9235f":"# 1. ANN","ef3ea8c3":"\n**Overfitting:** The production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.[1]\nThe curve of the model can be an example of overfitting.(See Figure 1). Lower epochs (15-20) would probably improve the accuracy of the model."}}