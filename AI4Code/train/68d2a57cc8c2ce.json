{"cell_type":{"478c3b36":"code","5757656a":"code","4797e6a3":"code","cd966931":"code","97493bd0":"code","5954a603":"code","671517f1":"code","da7a5ad9":"code","7c64b00d":"code","9251e072":"code","35c623f6":"code","edeedf35":"code","ae427e52":"code","fe4fdbf5":"code","96738860":"code","1e58a8ad":"code","c86113f3":"code","abeac061":"code","8285725a":"code","7ff7c697":"code","4545ff84":"code","8519d87e":"code","38c917d2":"code","7c33826a":"code","8452a168":"code","378ea628":"code","ec36f1c9":"code","d809ac4a":"code","11ce39af":"code","97868953":"code","4fa0ce8a":"code","90ffc9d2":"code","b63efbb0":"code","6129ba68":"code","7ed3b33e":"code","d7abdc75":"code","16317805":"code","c0b75201":"code","516cde9a":"code","642a2348":"code","ae35bcd2":"markdown","65976182":"markdown","d25503b2":"markdown","6ec5aa81":"markdown","6371f555":"markdown","51421d63":"markdown","a4fb6efe":"markdown"},"source":{"478c3b36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        pass\n\nwarnings.filterwarnings(\"ignore\")\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5757656a":"train_df = pd.read_csv('\/kaggle\/input\/corrected-train-csv-feedback-prize\/corrected_train.csv')\n\ntrain_df.columns","4797e6a3":"test_names, test_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/test'))):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('..\/input\/feedback-prize-2021\/test\/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n# test_texts['text'] = test_texts['text'].apply(lambda x:x.split())\ntest_texts.head()","cd966931":"test_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))[:]):\n    test_names.append(f.replace('.txt', ''))\n    train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n# train_texts['text'] = test_texts['text'].apply(lambda x:x.split())\ntrain_text_df.head()","97493bd0":"train_df.head()","5954a603":"all_entities = []\nfor i in tqdm(train_text_df.iterrows()):\n    total = i[1]['text'].split().__len__()\n#     entities = []\n    entities = [\"O\" for i in range(total)]\n    for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n        discourse = j[1]['discourse_type']\n        list_ix = j[1]['new_predictionstring'].split()\n        for li in list_ix[1:]:\n#             print(li, entities)\n            entities[int(li)] = f\"I-{discourse}\"\n        entities[int(list_ix[0])] = f\"B-{discourse}\"\n    all_entities.append(entities)","671517f1":"# train_text_df = pd.read_csv(\"\/kaggle\/input\/feedback-prize-ner-tagged-data\/feedback_prize_ner_tagged_data.csv\")\n# train_text_df.head()","da7a5ad9":"# import ast\n# train_text_df['entities'] = train_text_df['entities'].apply(lambda x:ast.literal_eval(x))\n\n# print(train_text_df['entities'].values[0])","7c64b00d":"train_text_df['entities'] = all_entities\n\ntrain_text_df.head()","9251e072":"# train_text_df[train_text_df.apply(lambda x: x['text'].split().__len__() == len(x['entities']), axis=1)]","35c623f6":"train_text_df = train_text_df[:]","edeedf35":"import datasets\nfrom transformers import Trainer, TrainingArguments, DataCollatorWithPadding\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom torch import cuda\nimport torch","ae427e52":"config = {'model_name': '\/kaggle\/input\/allenailongformerbase4096\/longformer\/',\n         'max_length': 1024,\n         'train_batch_size':4,\n         'valid_batch_size':8,\n         'epochs':5,\n         'learning_rate':5e-05,\n         'max_grad_norm':10,\n          'warmup':0.1,\n          \"grad_acc\":8,\n          \"model_save_path\":\"long-former\",\n         'device': 'cuda' if cuda.is_available() else 'cpu'}\n\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}","fe4fdbf5":"train_text_df['labels'] = train_text_df['entities'].apply(lambda x: [labels_to_ids[i] for i in x])","96738860":"train_text_df.head()","1e58a8ad":"tokenizer = AutoTokenizer.from_pretrained(config['model_name'], add_prefix_space=True)\nmodel = AutoModelForTokenClassification.from_pretrained(config['model_name'],\n                                                     num_labels=len(output_labels))","c86113f3":"converted = tokenizer(train_text_df.loc[0].values[1].split(),\n                      is_split_into_words=True)","abeac061":"print(converted, converted.word_ids())","8285725a":"len(converted.word_ids()), len(train_text_df.loc[0].values[1].split())","7ff7c697":"print(tokenizer.convert_ids_to_tokens(converted['input_ids'][:]), len(tokenizer.convert_ids_to_tokens(converted['input_ids'])))","4545ff84":"def tokenizer_data(example):\n    encoding = tokenizer(example['text'].split(),\n                         is_split_into_words=True,\n                         truncation=True,\n                         padding='max_length',\n                         max_length=config['max_length'])\n#     i = 0\n    labels = example['labels']\n    encoded_labels = np.ones(len(encoding[\"input_ids\"]), dtype=int) * -100\n    previous_idx = None\n    word_idx = encoding.word_ids()\n    for i,idx in enumerate(word_idx):\n        if idx != previous_idx and idx is not None:\n#             print(idx)\n            encoded_labels[i] = labels[idx]\n#             i += 1\n        previous_idx = idx\n    item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n    item['labels'] = torch.as_tensor(encoded_labels)\n    return item","8519d87e":"dataset = datasets.Dataset.from_pandas(train_text_df)","38c917d2":"dataset = dataset.train_test_split(test_size=0.1)\ndataset","7c33826a":"text = dataset['train'][0]\n\n# print(text['text'], text['entities'], text['labels'])","8452a168":"print(text['text'].split())","378ea628":"converted = tokenizer_data(text)\n\nprint(converted)","ec36f1c9":"converted\n\ni=0\nfor token, label in zip(tokenizer.convert_ids_to_tokens(converted[\"input_ids\"]), \n                        converted[\"labels\"]):\n#     print(token)\n    if label != -100:\n        print(token, '@@',text['text'].split()[i], ids_to_labels[label.item()],'@@', text['entities'][i])\n        i+=1\n    if i == 15:\n        break","d809ac4a":"dataset = dataset.map(tokenizer_data)\n\ndataset","11ce39af":"dataset.set_format(type='torch', columns=['input_ids', 'attention_mask',\n                                         'labels'])\n\ndataset","97868953":"for a in dataset['train']:\n    if a['input_ids'].shape[0] != a['attention_mask'].shape[0]:\n        print(a)\n        break","4fa0ce8a":"trainer_args = TrainingArguments('test_trainer',\n                                report_to='none',\n                                 num_train_epochs=config['epochs'],\n                                evaluation_strategy ='epoch',\n                                per_device_train_batch_size=config['train_batch_size'],\n                                per_device_eval_batch_size=config['valid_batch_size'],\n                                fp16=True,\n                                save_strategy = \"epoch\",\n                                 warmup_ratio= config['warmup'],\n                                 gradient_accumulation_steps=config['grad_acc'],\n                                 logging_strategy=\"epoch\",\n                                 save_total_limit=1\n                                )\n\ntrainer = Trainer(model=model,\n                  args=trainer_args, \n                  train_dataset = dataset['train'],\n                  eval_dataset=dataset['test'],\n#                   data_collator = data_collator,\n                  tokenizer=tokenizer)","90ffc9d2":"trainer.train()","b63efbb0":"device = config['device']","6129ba68":"trainer.model.eval()\ndef inference(sentence):\n    inputs = tokenizer(sentence.split(),\n                        is_split_into_words=True,\n                        padding='max_length', \n                        truncation=True, \n                        max_length=4096,\n                        return_tensors=\"pt\")\n\n    # move to gpu\n    ids = inputs[\"input_ids\"].to(device)\n    mask = inputs[\"attention_mask\"].to(device)\n    # forward pass\n    outputs = trainer.model(input_ids=ids, attention_mask=mask, return_dict=False)\n#     print(outputs)\n    logits = outputs[0]\n    \n    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n    print(logits.shape, active_logits.shape, flattened_predictions.shape)\n    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n    token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n    prediction = []\n    out_str = []\n    previous_idx = None\n    for idx, mapping in enumerate(inputs.word_ids()):\n#         print(mapping, token_pred[1], token_pred[0],\"####\")\n\n#         only predictions on first word pieces are important\n        if mapping is not None and mapping != previous_idx:\n#             print(mapping, token_pred[1], token_pred[0])\n            prediction.append(wp_preds[idx][1])\n            out_str.append(wp_preds[idx][0])\n#         else:\n#             if idx == 1:\n#                 prediction.append(wp_preds[idx][1])\n#                 out_str.append(wp_preds[idx][0])\n#             continue\n        previous_idx = mapping\n    return prediction, out_str","7ed3b33e":"te = test_texts.iloc[3]['text']","d7abdc75":"len(inference(te)[0]), len(te.split())","16317805":"final_preds = []\nimport pdb\nfor i in tqdm(range(len(test_texts))):\n    idx = test_texts.id.values[i]\n    pred, _ = inference(test_texts.text.values[i])\n#     print(idx.split(), pred)\n    pred = [x.replace('B-','').replace('I-','') for x in pred]\n    preds = []\n    j = 0\n    while j < len(pred):\n        cls = pred[j]\n        if cls == 'O':\n            j += 1\n        end = j + 1\n        while end < len(pred) and pred[end] == cls:\n            end += 1\n            \n        if cls != 'O' and cls != '' and end - j > 7:\n            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n        \n        j = end\n        \n# print(final_preds[1])","c0b75201":"len(final_preds)","516cde9a":"test_df = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\ntest_df\n\nsub = pd.DataFrame(final_preds)\nsub.columns = test_df.columns\n\nsub.head()","642a2348":"sub.to_csv(\"submission.csv\", index=False)","ae35bcd2":"### Tokenize the data\n\n* Load data into huggingface datasets.\n* Make sure you take care of sub-word tokenizing problem when adding labels to tokenized data.\n* use .map to map tokenizer_data function to data\n* Look at one sample to see whether mapping is done correctly or not\n","65976182":"### Check the shape of inputs_ids and attention_mask are same or not","d25503b2":"### Define config and Load model, tokenizer","6ec5aa81":"### Model trained on corrected train.csv check out https:\/\/www.kaggle.com\/nbroad\/corrected-train-csv-feedback-prize detailed notebook on how to get the file.\n\n* Loads datafrom dataframe to Datasets\n* Uses the Longformer using with 1024 tokens\n* Trainer API with fp16 enabled so as to optimize the training processes.\n \n\n## Do Upvote if you find it usefull, It keeps me motivated to do more quality work, Thanks!\n","6371f555":"### Define training argument and train the model","51421d63":"### Test to make sure is_split_into_words, return_offsets parameters are working properly","a4fb6efe":"### Write inference function, loop through the test_text and dump into submission file"}}