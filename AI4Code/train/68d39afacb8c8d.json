{"cell_type":{"7c172f88":"code","6517d9a7":"code","714dcf0f":"code","f3456b47":"code","cb35098f":"code","737077d1":"code","6a00c965":"code","b29c37cd":"code","1d9e7995":"code","e184399b":"code","cd34f307":"code","029ad955":"code","f0bf8a6f":"code","c3bd13a2":"code","15ae05b0":"code","caa4ba40":"markdown","209d4626":"markdown","e91e37c8":"markdown","d15670b3":"markdown","ec8ce2cf":"markdown","58434407":"markdown","f8349587":"markdown","5b2d7f84":"markdown","cde2f188":"markdown"},"source":{"7c172f88":"%%capture\n!pip install wandb","6517d9a7":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n# Imports for augmentations. \nfrom albumentations import (\n    Compose, HorizontalFlip, VerticalFlip, Rotate\n)","714dcf0f":"import wandb\nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)","f3456b47":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/mledu-datasets\/cats_and_dogs_filtered.zip \\\n    -O \/tmp\/cats_and_dogs_filtered.zip","cb35098f":"import zipfile\n\nlocal_zip = '\/tmp\/cats_and_dogs_filtered.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp')\nzip_ref.close()","737077d1":"base_dir = '\/tmp\/cats_and_dogs_filtered'\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\n\n# Directory with our training cat pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\n\n# Directory with our training dog pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\n\n# Directory with our validation cat pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\n\n# Directory with our validation dog pictures\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\n\nprint('total training cat images:', len(os.listdir(train_cats_dir)))\nprint('total training dog images:', len(os.listdir(train_dogs_dir)))\nprint('total validation cat images:', len(os.listdir(validation_cats_dir)))\nprint('total validation dog images:', len(os.listdir(validation_dogs_dir)))","6a00c965":"IMG_HEIGHT = 224\nIMG_WIDTH = 224\n\n# Define the augmentation policies. Note that they are applied sequentially with some probability p.\ntransforms = Compose([\n            Rotate(limit=30),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5)\n        ])\n\n\n# Apply augmentation policies.\ndef aug_fn(image):\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    \n    return aug_img\n\n# Augmentation policies\ndef apply_augmentation(image, label):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n    aug_img.set_shape((IMG_HEIGHT, IMG_WIDTH, 3))\n    \n    return aug_img, label","b29c37cd":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# Preprocess image\ndef preprocess_data(image, label):\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = image\/255.0\n    image = tf.squeeze(image, 0)\n    \n    label = tf.squeeze(label, -1)\n    \n    return image, label\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir, labels='inferred', label_mode='binary', color_mode='rgb', batch_size=1, image_size=(IMG_HEIGHT, IMG_WIDTH), shuffle=True\n).map(preprocess_data, num_parallel_calls=AUTOTUNE).map(apply_augmentation, num_parallel_calls=AUTOTUNE).batch(32).prefetch(AUTOTUNE)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    validation_dir, labels='inferred', label_mode='binary', color_mode='rgb', batch_size=1, image_size=(IMG_HEIGHT, IMG_WIDTH), shuffle=True, seed=42\n).map(preprocess_data, num_parallel_calls=AUTOTUNE).batch(32).prefetch(AUTOTUNE)","1d9e7995":"def show_batch(image_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(25):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.axis('off')\n    \nimage_batch, image_label = next(iter(train_ds))\nshow_batch(image_batch)","e184399b":"def catdogmodel():\n  inp = keras.layers.Input(shape=(224,224,3))\n  vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,\n                                            input_shape=(224,224,3))\n  vgg.trainable = False\n  \n  x = vgg.get_layer('block5_pool').output\n  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  x = tf.keras.layers.Dense(64, activation='relu')(x)\n  output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n  model = tf.keras.models.Model(inputs = inp, outputs=output)\n\n  return model\n\nkeras.backend.clear_session()\nmodel = catdogmodel()\nmodel.summary()","cd34f307":"## Reference: https:\/\/www.pyimagesearch.com\/2020\/03\/09\/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning\/\nclass GradCAM:\n  def __init__(self, model, layerName):\n    self.model = model\n    self.layerName = layerName\n    \n    self.gradModel = keras.models.Model(inputs=[self.model.inputs], \n                                        outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n    \n  def compute_heatmap(self, image, classIdx, eps=1e-8):\n    \n    with tf.GradientTape() as tape:\n      tape.watch(self.gradModel.get_layer(self.layerName).variables)\n      inputs = tf.cast(image, tf.float32)\n      (convOutputs, predictions) = self.gradModel(inputs)\n\n      if len(predictions)==1:\n        # Binary Classification\n        loss = predictions[0]\n      else:\n        loss = predictions[:, classIdx]\n    \n    grads = tape.gradient(loss, convOutputs)\n    \n    castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n    castGrads = tf.cast(grads > 0, \"float32\")\n    guidedGrads = castConvOutputs * castGrads * grads\n    \n    convOutputs = convOutputs[0]\n    guidedGrads = guidedGrads[0]\n    \n    weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n    cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n    \n    (w, h) = (image.shape[2], image.shape[1])\n    heatmap = cv2.resize(cam.numpy(), (w, h))\n    \n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer \/ denom\n    heatmap = (heatmap * 255).astype(\"uint8\")\n    \n    return heatmap\n    \n  def overlay_heatmap(self, heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_HOT):\n    heatmap = cv2.applyColorMap(heatmap, colormap)\n    output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n    \n    return (heatmap, output)","029ad955":"class GRADCamLogger(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data, layer_name):\n      super(GRADCamLogger, self).__init__()\n      self.validation_data = validation_data\n      self.layer_name = layer_name\n\n    def on_epoch_end(self, logs, epoch):\n      images = []\n      grad_cam = []\n\n      ## Initialize GRADCam Class\n      cam = GradCAM(model, self.layer_name)\n\n      for image in self.validation_data:\n        image = np.expand_dims(image, 0)\n        pred = model.predict(image)\n        classIDx = np.argmax(pred[0])\n  \n        ## Compute Heatmap\n        heatmap = cam.compute_heatmap(image, classIDx)\n        \n        image = image.reshape(image.shape[1:])\n        image = image*255\n        image = image.astype(np.uint8)\n\n        ## Overlay heatmap on original image\n        heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1]))\n        (heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5)\n\n        images.append(image)\n        grad_cam.append(output)\n\n      wandb.log({\"images\": [wandb.Image(image)\n                            for image in images]})\n      wandb.log({\"gradcam\": [wandb.Image(cam)\n                            for cam in grad_cam]})","f0bf8a6f":"## Prepare sample images to run your GradCam on. \nsample_images, sample_labels = next(iter(val_ds))\nsample_images.shape, sample_labels.shape","c3bd13a2":"earlystoper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=5, verbose=0, mode='min',\n    restore_best_weights=True\n)","15ae05b0":"# Initialize model\nkeras.backend.clear_session()\nmodel = catdogmodel()\n\n# Compile model\nmodel.compile('adam', 'binary_crossentropy', ['acc'])\n\n# Intialize W&B run\nwandb.init(project='gradcam')\n\n# Train model\nhistory = model.fit(train_ds,\n                    epochs=20,\n                    validation_data=val_ds,\n                    callbacks=[WandbCallback(data_type=\"image\", validation_data=(sample_images, sample_labels)),\n                               GRADCamLogger(sample_images, layer_name='block5_conv3')])\n\n# Close W&B run\nwandb.finish()","caa4ba40":"# \u2663\ufe0f Callback","209d4626":"# \ud83d\udcc0 Download Cat and Dog Dataset","e91e37c8":"### \ud83d\udccc About this kernel\n\n* This kernel shows the implementation of Gradient based Class Activation Map(CAM) as a custom Keras Callback. \n* It uses a toy Cat Vs Dog dataset(Binary Classification).\n* This can be slighly modified to be used in a multi-class and multi-label setting. \n\n### \u231b Upcoming\n\nWorking on a kernel to use this callback for multi-label classification of HPA dataset.\n\n### \ud83d\udca1 GradCAM visualization\n\n![](https:\/\/i.imgur.com\/LWFcT1Y.gif)","d15670b3":"# \ud83d\udc8e GradCAM Callback","ec8ce2cf":"# \ud83c\udf0b Prepare Dataloader","58434407":"# \ud83d\udc24 Model","f8349587":"# \ud83d\ude8b Train with W&B","5b2d7f84":"# \ud83e\uddf0 Imports, Installations and Setups","cde2f188":"# \ud83c\udf85 Gradient Based Class Activation Map(GradCAM): Intro\n\n### [More on CAM and GradCAM here $\\rightarrow$](https:\/\/wandb.ai\/ayush-thakur\/interpretability\/reports\/Interpretability-in-Deep-Learning-With-W-B-CAM-and-GradCAM--Vmlldzo5MTIyNw)\n\nThus the authors of [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https:\/\/arxiv.org\/pdf\/1610.02391.pdf), a really amazing paper, came up with modifications to CAM and previous approaches. Their approach uses the gradients of any target prediction flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the class of the image. \n\nThus Grad-CAM is a strict generalization over CAM. Besides overcoming the limitations of CAM it's applicable to different deep learning tasks involving CNNs:\n\n* CNNs with fully-connected layers (e.g. VGG) without any modification to the network.\n* CNNs used for structured outputs like image captioning.\n* CNNs used in tasks with multi-modal inputs like visual Q&A or reinforcement learning, without architectural changes or re-training.\n\n![image.png](attachment:image.png)\n([Source](https:\/\/arxiv.org\/pdf\/1610.02391.pdf))"}}