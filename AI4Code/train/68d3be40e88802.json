{"cell_type":{"c64200f7":"code","8dbd9741":"code","d91c1c41":"code","4c703786":"code","d0b19362":"code","77b16e81":"code","2ba88bf5":"code","a1a0adf6":"code","ae1aaa41":"code","013f77eb":"code","0c29b85e":"code","d9d36033":"code","f9ead8da":"code","96fc66bb":"code","4a7cedab":"code","4b22a1ee":"code","5edd3266":"code","7d4d27d5":"code","d239de74":"code","0eeea303":"code","73ac5338":"code","e88ea8c2":"code","87a8a062":"code","725e40aa":"code","4459a748":"code","543354c2":"code","f34fe6ed":"code","acbb4336":"code","1a556a2a":"code","58ec748c":"code","66390d4b":"code","ed9c2c09":"code","9a86a22c":"code","542511df":"code","576b435a":"code","4db42c72":"code","86bd1ff3":"code","50f47a82":"code","1e5ebe50":"code","ea9b9f38":"code","d7c6627a":"code","7dbd2940":"code","5ab57b1d":"code","e2a40a74":"code","72e3530b":"code","b17d2626":"code","d8425497":"code","8ba80203":"code","8a7d3ec6":"code","eb5a41c1":"code","70316428":"code","3d170139":"code","dc201061":"code","d798a4eb":"code","7b94a1b4":"code","31b6bcc1":"code","9c2f65c5":"code","f60ee442":"code","4700e56f":"code","5ef29cf5":"code","2917df47":"code","c5e228a6":"code","efeea769":"code","03495ae1":"code","645d5d64":"code","b7767d2b":"code","5e3ffef0":"code","e5d448cc":"code","4ac64554":"code","78fa73fd":"code","0bf27d5c":"code","55d3ccc2":"code","1e23696f":"code","d41857b6":"code","4fb812e4":"code","4722d98c":"code","5aa8f921":"code","3ef02885":"code","4041f0e0":"code","ba7660bd":"code","da880ba1":"code","4a5ebaef":"code","9d7b6ee6":"code","cd5bc9ec":"code","19904c5f":"code","6bb9911e":"code","dcac1a9a":"code","1a5bbbe5":"code","0796d765":"code","66416ef7":"code","ebbf46b7":"code","6a3a5616":"code","fd83647c":"code","6498bb46":"code","7b522493":"code","4a5141ef":"code","7ee1842a":"code","0b353b94":"code","8d0afdc7":"code","277050f7":"code","5ad5a54a":"code","f39c64df":"code","29ffa8f8":"code","11b2e1fa":"code","7681a64c":"code","fd285024":"code","458c75a7":"code","6bb3b63c":"code","69eddef1":"code","86c3918f":"code","bd4b1d19":"markdown","15f716ba":"markdown","fed8bfee":"markdown","875107a0":"markdown","8c7300df":"markdown","4773f33c":"markdown","d87cf0f5":"markdown","46491e67":"markdown","ca36bb5e":"markdown","f1a29e0a":"markdown","14968afe":"markdown","cd40f93a":"markdown","024bbb44":"markdown","1d070fc1":"markdown","a1a34390":"markdown","46720bdb":"markdown","65e12c33":"markdown","08368e1e":"markdown","7ac326e4":"markdown","b0b4db71":"markdown","c7cdeb69":"markdown","670e6bbd":"markdown","62f2eb38":"markdown","3042b599":"markdown","89d274d0":"markdown","2d41bd30":"markdown","d9569ba4":"markdown","3e2696e2":"markdown","6dc8f66b":"markdown","324b5e91":"markdown","1328444e":"markdown","9ebce844":"markdown","fbed3dd0":"markdown","34cab5fc":"markdown","5b90e17e":"markdown","07416c4f":"markdown","bb612fa3":"markdown","1c6010c3":"markdown"},"source":{"c64200f7":"# Data Analysis\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#import umap.plot\n#from yellowbrick.text import UMAPVisualizer\n\n# Text Processing\nimport re\nimport itertools\nimport spacy\nimport string\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_web_sm\nfrom collections import Counter\n\n### Dimensionality reduction and embedding\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nimport umap\n\n# Machine Learning packages\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport sklearn.cluster as cluster\nfrom sklearn.preprocessing import LabelEncoder\n\n# Ignore noise warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Export data\nimport pickle as pkl\nfrom scipy import sparse\nfrom numpy import asarray\nfrom numpy import savetxt\n\npd.set_option(\"display.max_column\", None)","8dbd9741":"mbti_df = pd.read_csv(\"..\/input\/mbti-type\/mbti_1.csv\")","d91c1c41":"type = [\"type\"]\nposts = [\"posts\"]\ncolumns = [*type, *posts]","4c703786":"mbti_df_raw = mbti_df\nmbti_df_raw[type] = mbti_df[type].fillna(\"\")\nmbti_df_raw[posts] = mbti_df[posts].fillna(\"\")\nmbti_df_raw.head()","d0b19362":"def clean_url(str_text_raw):\n    \"\"\"This function eliminate a string URL in a given text\"\"\"\n    str_text = re.sub(\"url_\\S+\", \"\", str_text_raw)\n    str_text = re.sub(\"email_\\S+\", \"\", str_text)\n    str_text = re.sub(\"phone_\\S+\", \"\", str_text)\n    return(re.sub(\"http[s]?:\/\/\\S+\", \"\", str_text))\n    \ndef clean_punctuation(str_text_raw):\n    \"\"\"This function replace some of the troublemaker puntuation elements in a given text\"\"\"\n    return(re.sub(\"[$\\(\\)\/|{|\\}#~\\[\\]^#;:!?\u00bf]\", \" \", str_text_raw))\n\ndef clean_unicode(str_text_raw):\n    \"\"\"This function eliminate non-unicode text\"\"\"\n    str_text = re.sub(\"&amp;\", \"\", str_text_raw)\n    return(re.sub(r\"[^\\x00-\\x7F]+\",\" \", str_text))\n                      \ndef clean_dot_words(str_text_raw):\n    \"\"\"This function replace dots between words\"\"\"\n    return(re.sub(r\"(\\w+)\\.+(\\w+)\", r\"\\1 \\2\",str_text_raw))\n\ndef clean_text(str_text_raw):\n    \"\"\"This function sets the text to lowercase and applies previous cleaning functions \"\"\"\n    str_text = str_text_raw.lower()\n    str_text = clean_dot_words(clean_punctuation(clean_unicode(clean_url(str_text))))\n    return(str_text)","77b16e81":"tokens_to_drop=[\"+\"]\n\ndef string_to_token(string, str_pickle = None):\n    \"\"\"\n    This function takes a sentence and returns the list of tokens and all their information\n    * Text: The original text of the lemma.\n    * Lemma: Lemma.\n    * Orth: The hash value of the lemma.\n    * is alpha: Does the lemma consist of alphabetic characters?\n    * is digit: Does the lemma consist of digits?\n    * is_title: Is the token in titlecase? \n    * is_punct: Is the token punctuation?\n    * is_space: Does the token consist of whitespace characters?\n    * is_stop: Is the token part of a \u201cstop list\u201d?\n    * is_digit: Does the token consist of digits?\n    * lang: Language of the token\n    * tag: Fine-grained part-of-speech. The complete list is in: \n    https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html, also using: spacy.explain(\"RB\")\n    * pos: Coarse-grained part-of-speech.\n    * has_vector: A boolean value indicating whether a word vector is associated with the token.\n    * vector_norm: The L2 norm of the token\u2019s vector representation.\n    * is_ovv: \"\"\"\n    doc = nlp(string)\n    l_token = [[token.text, token.lemma_, token.orth, token.is_alpha, token.is_digit, token.is_title, token.lang_, \n        token.tag_, token.pos_, token.has_vector, token.vector_norm, token.is_oov]\n        for token in doc if not token.is_punct | token.is_space | token.is_stop | token.is_digit | token.like_url \n               | token.like_num | token.like_email & token.is_oov]\n    pd_token = pd.DataFrame(l_token, columns=[\"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\",\n                                          \"tag\", \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\"])\n    #drop problematic tokens\n    pd_token = pd_token[~pd_token[\"text\"].isin(tokens_to_drop)]\n    #Convert plural text to singular\n    pd_token[\"text_to_singular\"] = np.where(pd_token[\"tag\"].isin([\"NNPS\", \"NNS\"]), pd_token[\"lemma\"], pd_token[\"text\"])\n    if(str_pickle!=None):\n        pd_token.to_pickle(f\"{str_pickle}.pkl\")\n    del l_token\n    return(pd_token)\n\ndef apply_cleaning(string):\n    \"\"\"\n    This function takes a sentence and returns a clean text\n    \"\"\"\n    doc = nlp(clean_text(string))\n    l_token = [token.text for token in doc if not token.is_punct | token.is_space | token.is_stop | \n               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n    return \" \".join(l_token)\n\ndef apply_lemma(string):\n    \"\"\"\n    This function takes a sentence and returns a clean text\n    \"\"\"\n    doc = nlp(clean_text(string))\n    l_token = [token.lemma_ for token in doc if not token.is_punct | token.is_space | token.is_stop | \n               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n    return \" \".join(l_token)\n\ndef list_to_bow(l_words):\n    \"\"\"\n    This function takes a list of words and create the bag of words ordered by desc order\n    \"\"\"\n    cv = CountVectorizer(l_words)\n    # show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n    count_vector=cv.fit_transform(l_words)\n    word_freq = Counter(l_words)\n    print(f\"Bag of words size: {count_vector.shape}\\nUnique words size: {len(word_freq)}\")\n    dict_word_freq = dict(word_freq.most_common())\n    return(dict_word_freq)","2ba88bf5":"mbti_df_clean = pd.DataFrame(mbti_df_raw[[\"type\", \"posts\"]])\nfor c in columns:\n    mbti_df_clean[c] = mbti_df_raw[c].apply(lambda row: clean_text(row))\nmbti_df_clean[\"posts\"] = mbti_df_clean[posts].apply(lambda x: \" \".join(x), axis=1)\nmbti_df_clean.head()","a1a0adf6":"#raise SystemExit(\"This is a very consumming memory process, with average wall time: ~ 20 min. If you don't want to wait please go to the next step\")","ae1aaa41":"nlp = spacy.load(\"en_core_web_sm\", disable = [\"ner\", \"parser\"]) \nnlp.max_length = 33000000","013f77eb":"mbti_df_clean.shape","0c29b85e":"mbti_df_clean_first = mbti_df_clean.iloc[:2169]","d9d36033":"mbti_df_clean_second = mbti_df_clean[2169:4338]","f9ead8da":"mbti_df_clean_third = mbti_df_clean.iloc[4338:6507]","96fc66bb":"mbti_df_clean_fourth = mbti_df_clean.iloc[6507:8675]","4a7cedab":"%%time\nfor column in columns:    \n    str_bow_column_first = \" \".join(mbti_df_clean_first[column])\n    pd_token_first = string_to_token(str_bow_column_first, f\"token_first_{column}\")\n    print(f\"Length of {column} column: {len(str_bow_column_first)}\")\n    print(f\"Number of tokens created: {pd_token_first.shape[0]}\\n\")","4b22a1ee":"%%time\nfor column in columns:    \n    str_bow_column_second = \" \".join(mbti_df_clean_second[column])\n    pd_token_second = string_to_token(str_bow_column_second, f\"token_second_{column}\")\n    print(f\"Length of {column} column: {len(str_bow_column_second)}\")\n    print(f\"Number of tokens created: {pd_token_second.shape[0]}\\n\")","5edd3266":"%%time\nfor column in columns:    \n    str_bow_column_third = \" \".join(mbti_df_clean_third[column])\n    pd_token_third = string_to_token(str_bow_column_third, f\"token_third_{column}\")\n    print(f\"Length of {column} column: {len(str_bow_column_third)}\")\n    print(f\"Number of tokens created: {pd_token_third.shape[0]}\\n\")","7d4d27d5":"%%time\nfor column in columns:    \n    str_bow_column_fourth = \" \".join(mbti_df_clean_fourth[column])\n    pd_token_fourth = string_to_token(str_bow_column_fourth, f\"token_fourth_{column}\")\n    print(f\"Length of {column} column: {len(str_bow_column_fourth)}\")\n    print(f\"Number of tokens created: {pd_token_fourth.shape[0]}\\n\")","d239de74":"%%time\npd_token_first = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\nfor column in columns:\n    pd_temp = pd.read_pickle(f\"token_first_{column}.pkl\") #Modified\n    pd_temp[\"column\"] = column\n    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n    pd_token_first = pd.concat([pd_token_first, pd_temp])\nprint(f\"Total rows loaded: {pd_token_first.shape[0]}\")","0eeea303":"%%time\npd_token_second = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\nfor column in columns:\n    pd_temp = pd.read_pickle(f\"token_second_{column}.pkl\") #Modified\n    pd_temp[\"column\"] = column\n    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n    pd_token_second = pd.concat([pd_token_second, pd_temp])\nprint(f\"Total rows loaded: {pd_token_second.shape[0]}\")","73ac5338":"%%time\npd_token_third = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\nfor column in columns:\n    pd_temp = pd.read_pickle(f\"token_third_{column}.pkl\") #Modified\n    pd_temp[\"column\"] = column\n    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n    pd_token_third = pd.concat([pd_token_third, pd_temp])\nprint(f\"Total rows loaded: {pd_token_third.shape[0]}\")","e88ea8c2":"%%time\npd_token_fourth = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\nfor column in columns:\n    pd_temp = pd.read_pickle(f\"token_fourth_{column}.pkl\") #Modified\n    pd_temp[\"column\"] = column\n    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n    pd_token_fourth = pd.concat([pd_token_fourth, pd_temp])\nprint(f\"Total rows loaded: {pd_token_fourth.shape[0]}\")","87a8a062":"pd_token_first.head()\n","725e40aa":"pd_token_first.tail()","4459a748":"mbti_df_clean['type_clean'] = mbti_df_clean['type'].apply(lambda x: apply_cleaning(x))\nmbti_df_clean['posts_clean']   = mbti_df_clean['posts'].apply(lambda x: apply_cleaning(x))\nmbti_df_clean['type_lemma'] = mbti_df_clean['type'].apply(lambda x: apply_lemma(x))\nmbti_df_clean['posts_lemma']   = mbti_df_clean['posts'].apply(lambda x: apply_lemma(x))","543354c2":"mbti_df_clean.shape","f34fe6ed":"mbti_df_clean.to_pickle('mbti_clean_text.pkl')\n\"\"\"\nThis pickle is too heavy to upload it in the GitHub repository and generating it in the previous cells is quite demmanding. \n\"\"\"","acbb4336":"mbti_df_clean = pd.read_pickle(\"mbti_clean_text.pkl\")","1a556a2a":"mbti_df_clean.head()","58ec748c":"mbti_text = mbti_df[[\"type\",\"posts\"]].copy()","66390d4b":"mbti_text = mbti_text.fillna(\"\")\ntext_columns = mbti_text[[\"type\"]]\ntext_columns[\"text\"] = mbti_text.iloc[:,1:].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)","ed9c2c09":"text_columns.head()","9a86a22c":"text_columns = pd.DataFrame()\ntext_columns[\"type\"] = mbti_df_clean[[\"type_lemma\"]].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)\ntext_columns[\"text\"] = mbti_df_clean[[\"posts_lemma\"]].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)","542511df":"text_columns.head()","576b435a":"text_columns['text'].isnull().sum()","4db42c72":"corpus = text_columns['text']\nvectorizer = TfidfVectorizer()\ntfidf = vectorizer.fit_transform(corpus)","86bd1ff3":"dictionary = dict(zip(vectorizer.get_feature_names(), tfidf))","50f47a82":"tfidf.shape","1e5ebe50":"sparse.save_npz(\"tfidf.npz\", tfidf)","ea9b9f38":"tfidf_df = pd.DataFrame(tfidf)","d7c6627a":"tfidf_df.to_csv(\"tfidf_df.csv\")","7dbd2940":"tfidf_df.head()","5ab57b1d":"possible_types= [\"infj\", \"entp\", \"intp\", \"intj\", \"entj\", \"enfj\", \"infp\", \"enfp\", \"isfp\", \"istp\", \"isfj\", \"istj\", \"estp\", \"esfp\", \"estj\", \"esfj\"]\nlab_encoder = LabelEncoder().fit(possible_types)","e2a40a74":"def encode_personalities(text_columns):\n\n    list_personality = []\n    len_data = len(text_columns)\n    i=0\n    \n    for row in text_columns.iterrows():\n        i+=1\n        if i % 500 == 0:\n            print(\"%s | %s rows\" % (i, len_data))\n\n        ##### Remove and clean comments\n        type_labelized = lab_encoder.transform([row[1].type])[0]\n        list_personality.append(type_labelized)\n\n    #del data\n    list_personality = np.array(list_personality)\n    return list_personality\n\nlist_personality = encode_personalities(text_columns)","72e3530b":"savetxt(\"personality.csv\", list_personality, delimiter=\",\")\nlist_personality.shape","b17d2626":"svd = TruncatedSVD(n_components=100, n_iter=10, random_state=42)\nsvd_vec = svd.fit_transform(tfidf)\n\nprint(\"TSNE\")\nX_tsne = TSNE(n_components=3, verbose=1, perplexity=40).fit_transform(svd_vec)","d8425497":"svd_vec.shape","8ba80203":"np.amin(svd_vec)","8a7d3ec6":"svd_vec_positive = svd_vec-np.amin(svd_vec)","eb5a41c1":"text_columns_svd_vec = pd.DataFrame(svd_vec_positive)","70316428":"col = list_personality\n\nplt.figure(0, figsize=(18,10))\nplt.scatter(X_tsne[:,0], X_tsne[:,1], c=col, cmap=plt.get_cmap('tab20') , s=12)\nplt.savefig(\"TSNE0.png\")\n\nplt.figure(1, figsize=(18,10))\nplt.scatter(X_tsne[:,0], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\nplt.savefig(\"TSNE1.png\")\n\nplt.figure(2, figsize=(18,10))\nplt.scatter(X_tsne[:,1], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\nplt.savefig(\"TSNE2.png\")\n\nsns.set_context(\"talk\")\nplt.show()","3d170139":"embedding = umap.UMAP(metric='hellinger', random_state=42).fit_transform(tfidf)","dc201061":"np.amin(embedding)","d798a4eb":"embedding_positive = embedding-np.amin(embedding)","7b94a1b4":"text_columns_umap = pd.DataFrame(embedding_positive)","31b6bcc1":"plt.figure(figsize=(18,10))\nplt.scatter(embedding_positive[:, 0], embedding_positive[:, 1], c=col, cmap='Spectral', s=12)\n\n\nsns.set_context(\"talk\")\nplt.savefig(\"UMAP_embedding_positive.png\")\nplt.show()","9c2f65c5":"embedding_svd = umap.UMAP(metric='hellinger', random_state=42).fit_transform(svd_vec_positive)","f60ee442":"np.amin(embedding_svd)","4700e56f":"embedding_svd_positive = embedding_svd-np.amin(embedding_svd)","5ef29cf5":"text_columns_umap_svd = pd.DataFrame(embedding_svd_positive)","2917df47":"plt.figure(figsize=(18,10))\nplt.scatter(embedding_svd_positive[:, 0], embedding_svd_positive[:, 1], c=col, cmap='Spectral', s=12)\n\n\nsns.set_context(\"talk\")\nplt.savefig(\"UMAP_embedding_svd_positive.png\")\nplt.show()","c5e228a6":"def var_row(row):\n    lst = []\n    for word in row.split(\"|||\"):\n        lst.append(len(word.split()))\n    return np.var(lst)\n\nmbti_df[\"words_per_comment\"] = mbti_df[\"posts\"].apply(lambda x: len(x.split())\/50)\nmbti_df[\"variance_of_word_counts\"] = mbti_df[\"posts\"].apply(lambda x: var_row(x))","efeea769":"type_dummies = pd.get_dummies(text_columns[\"type\"])\ntext_columns.drop([\"text\"], axis=1, inplace=True)\ntext_columns = pd.concat([text_columns, mbti_df[\"words_per_comment\"], mbti_df[\"variance_of_word_counts\"], \n                          type_dummies], axis=1,levels=None ,sort=False)","03495ae1":"map1 = {\"i\": 0, \"e\": 1}\nmap2 = {\"n\": 0, \"s\": 1}\nmap3 = {\"t\": 0, \"f\": 1}\nmap4 = {\"j\": 0, \"p\": 1}\ntext_columns[\"i-e\"] = text_columns[\"type\"].astype(str).str[0]\ntext_columns[\"i-e\"] = text_columns[\"i-e\"].map(map1)\ntext_columns[\"n-s\"] = text_columns[\"type\"].astype(str).str[1]\ntext_columns[\"n-s\"] = text_columns[\"n-s\"].map(map2)\ntext_columns[\"t-f\"] = text_columns[\"type\"].astype(str).str[2]\ntext_columns[\"t-f\"] = text_columns[\"t-f\"].map(map3)\ntext_columns[\"j-p\"] = text_columns[\"type\"].astype(str).str[3]\ntext_columns[\"j-p\"] = text_columns[\"j-p\"].map(map4)","645d5d64":"text_columns.head()","b7767d2b":"tfidf.shape","5e3ffef0":"tfidf_T = np.transpose(tfidf)","e5d448cc":"tfidf_T.shape","4ac64554":"list_personality.shape","78fa73fd":"train_array_T = sparse.vstack((list_personality, tfidf_T), format=\"csr\")\ntrain_array_types = np.transpose(train_array_T)","0bf27d5c":"train_array_types.shape","55d3ccc2":"sparse.save_npz(\"train_array_types.npz\", train_array_types)","1e23696f":"dimensions_array = text_columns[[\"i-e\", \"n-s\", \"t-f\", \"j-p\"]].to_numpy()","d41857b6":"dimensions_array.shape","4fb812e4":"savetxt(\"dimensions.csv\", dimensions_array, delimiter=\",\")","4722d98c":"train_array_dimensions = sparse.hstack((dimensions_array, tfidf), format=\"csr\")","5aa8f921":"train_array_dimensions.shape","3ef02885":"sparse.save_npz(\"train_array_dimensions.npz\", train_array_dimensions)","4041f0e0":"#text_columns.drop([\"type\"], axis=1, inplace=True)\nresult_svd_vec_dimensions = pd.concat([text_columns, text_columns_svd_vec], axis=1,levels=None ,sort=False)","ba7660bd":"result_svd_vec_dimensions.drop([\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\", \n                             \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1, inplace=True)","da880ba1":"result_svd_vec_dimensions.to_csv(\"result_svd_vec_dimensions.csv\")","4a5ebaef":"result_svd_vec_dimensions.head()","9d7b6ee6":"result_svd_vec_types = pd.concat([text_columns, text_columns_svd_vec], axis=1,levels=None ,sort=False)","cd5bc9ec":"result_svd_vec_types.drop([\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1, inplace=True)","19904c5f":"result_svd_vec_types.to_csv(\"result_svd_vec_types.csv\")","6bb9911e":"result_svd_vec_types.head()","dcac1a9a":"result_umap_dimensions = pd.concat([text_columns, text_columns_umap], axis=1,levels=None ,sort=False)","1a5bbbe5":"result_umap_dimensions.drop([\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\", \n                             \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1, inplace=True)","0796d765":"result_umap_dimensions.to_csv(\"result_umap_dimensions.csv\")","66416ef7":"result_umap_dimensions.head()","ebbf46b7":"result_umap_types = pd.concat([text_columns, text_columns_umap], axis=1,levels=None ,sort=False)","6a3a5616":"result_umap_types.drop([\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1, inplace=True)","fd83647c":"result_umap_types.to_csv(\"result_umap_types.csv\")","6498bb46":"result_umap_types.head()","7b522493":"result_umap_dimensions = pd.concat([text_columns, text_columns_umap], axis=1,levels=None ,sort=False)","4a5141ef":"result_umap_dimensions.drop([\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\", \n                             \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1, inplace=True)","7ee1842a":"result_umap_dimensions.to_csv(\"result_umap_dimensions.csv\")","0b353b94":"result_umap_dimensions.head()","8d0afdc7":"result_umap_types = pd.concat([text_columns, text_columns_umap], axis=1,levels=None ,sort=False)","277050f7":"result_umap_types.drop([\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1, inplace=True)","5ad5a54a":"result_umap_types.to_csv(\"result_umap_types.csv\")","f39c64df":"result_umap_types.head()","29ffa8f8":"result_umap_svd_dimensions = pd.concat([text_columns, text_columns_umap_svd], axis=1,levels=None ,sort=False)","11b2e1fa":"result_umap_svd_dimensions.drop([\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\", \n                             \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1, inplace=True)","7681a64c":"result_umap_svd_dimensions.to_csv(\"result_umap_svd_dimensions.csv\")","fd285024":"result_umap_svd_dimensions.head()","458c75a7":"result_umap_svd_types = pd.concat([text_columns, text_columns_umap_svd], axis=1,levels=None ,sort=False)","6bb3b63c":"result_umap_svd_types.drop([\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1, inplace=True)","69eddef1":"result_umap_svd_types.to_csv(\"result_umap_svd_types.csv\")","86c3918f":"result_umap_svd_types.head()","bd4b1d19":"#### UMAP on TSVD","15f716ba":"<img src=\"https:\/\/bit.ly\/2VnXWr2\" width=\"100\" align=\"left\">","fed8bfee":"##### Dimensions","875107a0":"#### Declare preprocessing functions","8c7300df":"##### Tokenization and lemmatization functions","4773f33c":"### Dimensionlity reduction","d87cf0f5":"#### UMAP","46491e67":"#### End cleaning and tokenize rows using SpaCy","ca36bb5e":"### Merge","f1a29e0a":"#### Label encoding","14968afe":"### Cleaning text","cd40f93a":"##### Types","024bbb44":"##### Types","1d070fc1":"#### Truncated SVD and TSNE","a1a34390":"#### UMAP","46720bdb":"##### Dimensions","65e12c33":"### Get new numerical columns","08368e1e":"#### Truncated SVD","7ac326e4":"##### Types","b0b4db71":"I will merge the new numerical columns I created with the tfidf and different embedding results","c7cdeb69":"##### Types","670e6bbd":"# Final project: NLP to predict Myers-Briggs Personality Type","62f2eb38":"##### Dimensions","3042b599":"##### Types","89d274d0":"#### UMAP","2d41bd30":"#### UMAP on TSVD","d9569ba4":"### TF-IDF","3e2696e2":"## 2. Data preprocessing","6dc8f66b":"##### Cleaning functions","324b5e91":"#### Add cleaned info to the dataset and store it into a pickle","1328444e":"#### Clean data","9ebce844":"#### Load the pickles into Dataframe","fbed3dd0":"#### Matrixes","34cab5fc":"I will get the original dataframe with its posts lengths and variances, dummies from every type and dummies on every type dimension axis.","5b90e17e":"<img src=\"https:\/\/www.nicepng.com\/png\/detail\/148-1486992_discover-the-most-powerful-ways-to-automate-your.png\" width=\"1000\"> ","07416c4f":"##### Dimensions","bb612fa3":"## Imports","1c6010c3":"##### Dimensions"}}