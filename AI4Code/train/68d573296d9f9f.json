{"cell_type":{"d06b1847":"code","8b71d935":"code","856fcc7c":"code","b3003c2a":"code","4341142a":"code","5e785ad2":"code","970b4cd1":"code","c766ca00":"code","dc26f76f":"code","ce660669":"code","0c83d782":"code","62752e08":"code","e7fb423d":"code","1ac4819d":"code","ef0e5f1d":"code","ca1f3465":"code","a82bfdf2":"code","ce610d8e":"code","bfdec357":"code","3c262fc0":"code","f32c7873":"code","7ffd2a38":"code","3b4a7f0d":"code","4e02ea6f":"code","082da75b":"code","3db64940":"code","a8451f3f":"code","88263d8c":"code","c086db54":"code","91d04485":"code","31ee9243":"code","03ca139f":"code","bd04eceb":"code","6120775b":"code","00039f0c":"code","f194c920":"code","799b76cf":"code","654fd8dd":"code","7f7fd273":"code","775593f2":"code","9603a700":"code","9d5ae3dd":"code","775eb8e0":"code","d83402da":"code","1d8e3015":"code","7031cf28":"code","d7b6ceb7":"code","26edb979":"code","0da2add8":"code","69e44164":"code","3bdbd2bf":"code","b6c2b7c3":"code","637c60fa":"code","9ae77c03":"code","67fc8d9a":"code","af52a1be":"code","d36f8737":"code","589f9cf2":"code","2cc0fba1":"code","7447a254":"code","89be39dc":"code","d278d3b3":"code","a19fc1a1":"code","a9ecf593":"code","6c93ab37":"code","d3c2caa8":"code","85bce39f":"code","68ffffa1":"code","046e3128":"code","e9b6c18d":"code","09e43e23":"code","d3b59112":"code","19db1a48":"markdown","d0dd56f4":"markdown","4fdbf891":"markdown","75301dd9":"markdown","00399f4d":"markdown","5e62d9f4":"markdown","3ce733e1":"markdown","f03b69e1":"markdown","1116cac6":"markdown","e474dae4":"markdown","1a4e0b52":"markdown","d0ff51bb":"markdown","8a01fbac":"markdown","15bbd235":"markdown","122ea256":"markdown","32f553dd":"markdown","1af952b0":"markdown","aec250c0":"markdown","caef0fa9":"markdown","dddad37f":"markdown","8906dd02":"markdown","deb79a28":"markdown","ba90cdd2":"markdown","ca122478":"markdown","0f4cde71":"markdown","2f49a25b":"markdown","658768a7":"markdown","1c5b746d":"markdown","ffdbdbd6":"markdown","71ff5111":"markdown","889b7f94":"markdown","8cd69494":"markdown","61ac0c0b":"markdown","2d3a24db":"markdown","594464dd":"markdown","1b560275":"markdown","4af6ccba":"markdown","12c0881f":"markdown","aff438b5":"markdown","e305e6b2":"markdown","09e0b92a":"markdown","b69c8741":"markdown","d913f6b2":"markdown","52d30662":"markdown","a7f74a82":"markdown","c7f9cd11":"markdown","42e2b32f":"markdown","4aa9f65f":"markdown","136bc365":"markdown","ab1b7bdb":"markdown","21b1bd73":"markdown","4af4ff61":"markdown","20f4a69a":"markdown","177e807b":"markdown","262639ba":"markdown","151225c2":"markdown","764f99d3":"markdown","136ebf1e":"markdown","0f1a6f2d":"markdown","5f400356":"markdown","1c08a0b9":"markdown","92877a74":"markdown","0934aff3":"markdown","9ce7cdbb":"markdown","70af73f1":"markdown","f1a1316c":"markdown","317c0507":"markdown","6673a851":"markdown","27e115b1":"markdown","bc51ae72":"markdown","5a0a8eeb":"markdown","64250dfb":"markdown","3df72a34":"markdown","76d57ec0":"markdown","fb1a8b73":"markdown","bf6c6c11":"markdown","29805c4a":"markdown","ac4edfc1":"markdown","84e4c68e":"markdown","947e2423":"markdown"},"source":{"d06b1847":"import math \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nimport pickle\nfrom joblib import dump, load\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nfrom tensorflow.keras import Sequential, callbacks\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\n\npd.set_option('display.max_rows',30)\n%matplotlib inline","8b71d935":"df = pd.read_csv('..\/input\/craigslist-carstrucks-data\/vehicles.csv')","856fcc7c":"df.describe()","b3003c2a":"df.info()","4341142a":"df.head()","5e785ad2":"fig, ax = plt.subplots(figsize=(10,2))\nax.set_title('Box plot of the prices')\nsns.boxplot(x='price', data = df)","970b4cd1":"Q1 = df['price'].quantile(0.25)\nQ3 = df['price'].quantile(0.75)\nIQR = Q3 - Q1\nfilter = (df['price'] >= Q1 - 1.5 * IQR) & (df['price'] <= Q3 + 1.5 *IQR)\ninit_size = df.count()['id']\ndf = df.loc[filter]  \nfiltered_size = df.count()['id']\nprint(init_size-filtered_size,'(', '{:.2f}'.format(100*(init_size-filtered_size)\/init_size), '%',')', 'outliers removed from dataset')","c766ca00":"fig, ax = plt.subplots(figsize=(10,5))\nax.set_title('Distribution of the prices')\nsns.distplot(df['price'], bins=30, kde=False)","dc26f76f":"df = df[df['price']>600]","ce660669":"fig, axs = plt.subplots(2, figsize=(20,10))\nsns.distplot(df['odometer'], ax = axs[0])\naxs[0].set_title('Distribution of the odometer')\naxs[1].set_title('Box plot of the odometer')\nsns.boxplot(x='odometer', data = df, ax=axs[1])","0c83d782":"Q1 = df['odometer'].quantile(0.25)\nQ3 = df['odometer'].quantile(0.75)\nIQR = Q3 - Q1\nfilter = (df['odometer'] <= Q3 + 3 *IQR)\ninit_size = df.count()['id']\ndf = df.loc[filter]  \nfiltered_size = df.count()['id']\nprint(init_size-filtered_size,'(', '{:.2f}'.format(100*(init_size-filtered_size)\/init_size), '%',')', 'outliers removed from dataset')","62752e08":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Geographical distribution of the cars colored by prices')\nsns.scatterplot(x= 'long', y='lat', data = df, hue = 'price', ax=ax )","e7fb423d":"fig, ax = plt.subplots(figsize=(15,10))\nax.set_title('Mean car price of each state')\ndf.groupby(['state']).mean()['price'].plot.bar(ax=ax)","1ac4819d":"df = df.drop(columns = ['id', 'url', 'region', 'region_url', 'title_status', 'vin', 'image_url', 'description', 'county', 'state', 'long', 'lat'])","ef0e5f1d":"df.head()","ca1f3465":"df_man = df['manufacturer'].to_frame()","a82bfdf2":"df = df.drop(columns = ['manufacturer'])","ce610d8e":"fig, ax = plt.subplots(figsize=(8,6))\nax.set_title('Distribution of the missing values (yellow records)')\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","bfdec357":"df = df.drop(columns = ['size'])","3c262fc0":"rm_rows = ['year', 'model', 'fuel', 'transmission', 'drive', 'type', 'paint_color']\nfor column in rm_rows:\n    df = df[~df[column].isnull()]","f32c7873":"df = df.replace(np.nan, 'null', regex=True)","7ffd2a38":"df.info()\nfig, ax = plt.subplots(figsize=(8,6))\nax.set_title('Distribution of the missing values (yellow records)')\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","3b4a7f0d":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Scatter plot between mileages and prices')\nsns.scatterplot(x='odometer', y='price', data=df)","4e02ea6f":"df = df[(df['price']+df['odometer'])>5000]","082da75b":"df = df[df['year']>1960]","3db64940":"df_man['manufacturer'].value_counts()","a8451f3f":"rm_brands = ['harley-davidson', 'alfa-romeo', 'datsun', 'tesla', 'land rover', 'porche', 'aston-martin', 'ferrari']\nfor brand in rm_brands:\n    df_man = df_man[~(df_man['manufacturer'] == brand)]","88263d8c":"df = df.groupby('model').filter(lambda x: len(x) > 50)","c086db54":"df['model'].value_counts()","91d04485":"df.info()","31ee9243":"fig, axs = plt.subplots(2, figsize=(14, 10))\naxs[0].set_title('Box plot of the prices')\nsns.boxplot(x='price', data = df, ax = axs[0])\naxs[1].set_title('Distribution of the prices')\nsns.distplot(df['price'], ax=axs[1], bins=30, kde=False)","03ca139f":"fig, axs = plt.subplots(2, figsize=(14, 10))\nsns.distplot(df['odometer'], ax = axs[1], bins=30, kde=False)\naxs[1].set_title('Box plot of the odometer')\nsns.boxplot(x='odometer', data = df, ax=axs[0])\naxs[0].set_title('Distribution of the odometer')","bd04eceb":"df['paint_color'].value_counts()","6120775b":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Box plot of the prices on each color')\nsns.boxplot(x='paint_color', y='price', data = df)","00039f0c":"print(df['type'].value_counts())\nfig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Box plot of the prices on each car type')\nsns.boxplot(x='type', y='price', data = df)","f194c920":"print('Condition:')\nprint(df['condition'].value_counts())\nprint('\\nCylinders:')\nprint(df['cylinders'].value_counts())\nprint('\\nFuel:')\nprint(df['fuel'].value_counts())\nprint('\\nTransmission:')\nprint(df['transmission'].value_counts())\nprint('\\nDrive:')\nprint(df['drive'].value_counts())\n\nfig=plt.figure(figsize=(25,37))\nfig.add_subplot(3, 2, 1)\n \nsns.boxplot(x='condition', y='price', data = df)\nfig.add_subplot(3, 2, 2)\nsns.boxplot(x='cylinders', y='price', data = df)\nfig.add_subplot(3, 2, 3)\nsns.boxplot(x='fuel', y='price', data = df)\nfig.add_subplot(3, 2, 4)\nsns.boxplot(x='transmission', y='price', data = df)\nfig.add_subplot(3, 2, 5)\nsns.boxplot(x='drive', y='price', data = df)","799b76cf":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Scatter plot of the prices in each year, colored by transmission type')\n#sns.scatterplot(x='year', y='price', data=df[(df['transmission'] =='manual') | (df['transmission'] =='other')], hue = 'transmission')\nsns.scatterplot(x='year', y='price', data=df, hue = 'transmission')","654fd8dd":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Scatter plot of the prices in each year, colored by drive type')\nsns.scatterplot(x='year', y='price', data=df, hue = 'drive')","7f7fd273":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Scatter plot of the prices in each year, colored by fuel type')\nsns.scatterplot(x='year', y='price', data=df, hue = 'fuel')","775593f2":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Scatter plot of the prices in each year, colored by cylinder type')\nsns.scatterplot(x='year', y='price', data=df, hue = 'cylinders')","9603a700":"fig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Scatter plot of the odometer of the cars in each year, colored by car condition')\nsns.scatterplot(x='year', y='odometer', data=df, hue = 'condition')","9d5ae3dd":"fig, ax = plt.subplots(figsize=(35,15))\nax.set_title('Count plot of all cars group by manufacturer')\nsns.countplot(x='manufacturer', data=df_man)","775eb8e0":"fig, ax = plt.subplots(figsize=(12,10))\nax.set_title('Person correlations among prices, years and mileages')\nsns.heatmap(df.corr())","d83402da":"cate_Columns = ['model', 'condition', 'cylinders', 'fuel', 'transmission', 'drive', 'type', 'paint_color']\nfor column in cate_Columns:\n    column = pd.get_dummies(df[column],drop_first=True)\n    df = pd.concat([df,column],axis=1)\ndf = df.drop(columns = cate_Columns)","1d8e3015":"df.head()","7031cf28":"std_scaler = StandardScaler()\n\nfor column in ['year', 'odometer']:\n    df[column] = std_scaler.fit_transform(df[column].values.reshape(-1,1))","d7b6ceb7":"df.head()","26edb979":"X_train, X_test, y_train, y_test = train_test_split(df.drop('price',axis=1), \n                                                    df['price'], test_size=0.30, \n                                                    random_state=141)","0da2add8":"model_score = pd.DataFrame(columns=('r2', 'rmse'))","69e44164":"lrmodel = LinearRegression()\nlrmodel.fit(X_train,y_train)","3bdbd2bf":"lr_predict = lrmodel.predict(X_test)\n\nlr_r2 = metrics.r2_score(y_test, lr_predict)\nlr_rmse = math.sqrt(metrics.mean_squared_error(y_test, lr_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[lr_r2], 'rmse':[lr_rmse]}, index = ['Linear Regression']))\n\nprint('For the linear regressor, the root mean square error for the testing set is:', lr_rmse)\nprint('The r2 score for the testing set is:', lr_r2)\n\nfig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Comparison between predicted prices and actual prices in testing set, linear regrssion')\nplt.scatter(y_test, lr_predict)","b6c2b7c3":"lr_predict_train = lrmodel.predict(X_train)\n\nlr_r2_train = metrics.r2_score(y_train, lr_predict_train)\nlr_rmse_train = math.sqrt(metrics.mean_squared_error(y_train, lr_predict_train))\n\nprint('For the linear regressor, the root mean square error for the training set is:', lr_rmse_train)\nprint('The r2 score for the testing set is:', lr_r2_train)\n\nfig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Comparison between predicted prices and actual prices in training set, linear regrssion')\nplt.scatter(y_train, lr_predict_train)","637c60fa":"alphas = np.logspace(-4,4,12)\nlasso = LassoCV(max_iter=10**6, alphas=alphas)\nlasso.fit(X_train, y_train)","9ae77c03":"lasso_predict = lasso.predict(X_test)\n\nlasso_r2 = metrics.r2_score(y_test, lasso_predict)\nlasso_rmse = math.sqrt(metrics.mean_squared_error(y_test, lasso_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[lasso_r2], 'rmse':[lasso_rmse]}, index = ['Lasso Regression']))\n\nprint('For the Lasso linear regressor, the root mean square error for the testing set is:', lasso_rmse)\nprint('The r2 score for the testing set is:', lasso_r2)\n\nfig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Comparison between predicted prices and actual prices in testing set, Lasso regrssion')\nplt.scatter(y_test, lasso_predict)","67fc8d9a":"callback = callbacks.EarlyStopping(monitor='loss', patience=3)\nnn_model = Sequential()\nnn_model.add(Dense(input_dim = X_train.shape[1], units = 2000, activation = 'relu'))\n#nn_model.add(Dropout(0.3)) There seems to be no overfitting problem, so no need for dropout.\nnn_model.add(Dense(units = 2000, activation = 'relu'))\nnn_model.add(Dense(units=1))\nnn_model.compile(loss='mean_squared_error', optimizer = 'adam', metrics=['mae', 'mse'])","af52a1be":"#nn_model.fit(X_train, y_train, batch_size=5000, epochs=800, callbacks=[callback], verbose=0)   Here in Kaggle we use the trained model to save time\nnn_model = load_model(\"..\/input\/models-needed\/nn0.917\") #Here in Kaggle we use the trained model to save time","d36f8737":"nn_predict = nn_model.predict(X_test)\n\nnn_rmse = math.sqrt(metrics.mean_squared_error(y_test, nn_predict))\nnn_r2 = metrics.r2_score(y_test, nn_predict)\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[nn_r2], 'rmse':[nn_rmse]}, index = ['MLP']))\n\nprint('For the MLP model, the root mean square error for the testing set is:', nn_rmse)\nprint('The r2 score for the testing set is:', nn_r2)\n\nfig, ax = plt.subplots(figsize=(20,15))\nax.set_title('Comparison between predicted prices and actual prices in testing set, MLP')\nplt.scatter(y_test, nn_predict)","589f9cf2":"knnReg = KNeighborsRegressor()\n\nparam_grid = [\n     {\n         'weights':['uniform'],\n         'n_neighbors':[i for i in range(1,7)]\n     }]\n\ngrid_search_knn = GridSearchCV(knnReg, param_grid,n_jobs=-1,verbose=2)\ngrid_search_knn.fit(X_train, y_train) ","2cc0fba1":"knn_best = grid_search_knn.best_estimator_\nknn_best","7447a254":"knn_predict = knn_best.predict(X_test)\n\nknn_r2 = metrics.r2_score(y_test, knn_predict)\nknn_rmse = math.sqrt(metrics.mean_squared_error(y_test, knn_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[knn_r2], 'rmse':[knn_rmse]}, index = ['K - Nearest Neighbor']))\n\nprint('For the K-NN regressor, the root mean square error for the testing set is:', knn_rmse)\nprint('The r2 score for the testing set is:', knn_r2)\nfig, ax = plt.subplots(figsize=(20,15))\nplt.scatter(y_test, knn_predict)","89be39dc":"dt_model = DecisionTreeRegressor(random_state=0)\ndt_model.fit(X_train, y_train)","d278d3b3":"dt_predict = dt_model.predict(X_test)\n\ndt_r2 = metrics.r2_score(y_test, dt_predict)\ndt_rmse = math.sqrt(metrics.mean_squared_error(y_test, dt_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[dt_r2], 'rmse':[dt_rmse]}, index = ['Decision Tree']))\n\nprint('For the decision tree regressor, the root mean square error for the testing set is:', dt_rmse)\nprint('The r2 score for the testing set is:', dt_r2)\nfig, ax = plt.subplots(figsize=(20,15))\nplt.scatter(y_test, dt_predict)","a19fc1a1":"ranF_model = RandomForestRegressor(max_depth=8, random_state=0)\nranF_model.fit(X_train, y_train)","a9ecf593":"ranF_predict = ranF_model.predict(X_test)\n\nranF_r2 = metrics.r2_score(y_test, ranF_predict)\nranF_rmse = math.sqrt(metrics.mean_squared_error(y_test, ranF_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[ranF_r2], 'rmse':[ranF_rmse]}, index = ['Random Forest']))\n\nprint('For the random forest regressor, the root mean square error for the testing set is:', ranF_rmse)\nprint('The r2 score for the testing set is:', ranF_r2)\nfig, ax = plt.subplots(figsize=(20,15))\nplt.scatter(y_test, ranF_predict)","6c93ab37":"svr_model = SVR(C = 1, epsilon = 0.2, kernel = 'rbf', max_iter=10000)\nsvr_model.fit(X_train, y_train)","d3c2caa8":"svr_predict = svr_model.predict(X_test)\n\nsvr_r2 = metrics.r2_score(y_test, svr_predict)\nsvr_rmse = math.sqrt(metrics.mean_squared_error(y_test, svr_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[svr_r2], 'rmse':[svr_rmse]}, index = ['SVM_gaus']))\n\nprint('For the support vector regressor with gaussian kernel, the root mean square error for the testing set is:', svr_rmse)\nprint('The r2 score for the testing set is:', svr_r2)\nfig, ax = plt.subplots(figsize=(20,15))\nplt.scatter(y_test, svr_predict)","85bce39f":"svr_model2 = SVR(C = 1, epsilon = 0.2, kernel = 'linear', max_iter=10000)\nsvr_model2.fit(X_train, y_train)\nsvr_predict2 = svr_model2.predict(X_test)\n\nsvr2_r2 = metrics.r2_score(y_test, svr_predict2)\nsvr2_rmse = math.sqrt(metrics.mean_squared_error(y_test, svr_predict2))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[svr2_r2], 'rmse':[svr2_rmse]}, index = ['SVM_linear']))\n\nprint('For the support vector regressor with linear kernal, the root mean square error for the testing set is:', svr2_rmse)\nprint('The r2 score for the testing set is:', svr2_r2)\nfig, ax = plt.subplots(figsize=(20,15))\nplt.scatter(y_test, svr_predict2)","68ffffa1":"gb_model = GradientBoostingRegressor(random_state=0)\ngb_model.fit(X_train, y_train)","046e3128":"gb_predict = gb_model.predict(X_test)\n\ngb_r2 = metrics.r2_score(y_test, gb_predict)\ngb_rmse = math.sqrt(metrics.mean_squared_error(y_test, gb_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[gb_r2], 'rmse':[gb_rmse]}, index = ['GBDT']))\n\nprint('For the gradient boosting regressor, the root mean square error for the testing set is:', gb_rmse)\nprint('The r2 score for the testing set is:', gb_r2)\nfig, ax = plt.subplots(figsize=(20,15))\nplt.scatter(y_test, gb_predict)","e9b6c18d":"xgb_model = XGBRegressor()\n\ndf_noDuplicate = df.loc[:,~df.columns.duplicated()]\n\nX_train_nd, X_test_nd, y_train_nd, y_test_nd = train_test_split(df_noDuplicate.drop('price',axis=1), \n                                                    df['price'], test_size=0.30, \n                                                    random_state=141)\n\n\nxgb_model.fit(X_train_nd, y_train_nd)","09e43e23":"xgb_predict = xgb_model.predict(X_test_nd)\n\nxgb_r2 = metrics.r2_score(y_test_nd, xgb_predict)\nxgb_rmse = math.sqrt(metrics.mean_squared_error(y_test_nd, xgb_predict))\n\nmodel_score = model_score.append(pd.DataFrame({'r2':[xgb_r2], 'rmse':[xgb_rmse]}, index = ['XGBoost']))\n\nprint('For the XGboosting regressor, the root mean square error for the testing set is:', xgb_rmse)\nprint('The r2 score for the testing set is:', xgb_r2)\nfig, ax = plt.subplots(figsize=(20,15))\nplt.scatter(y_test_nd, xgb_predict)","d3b59112":"model_score.sort_values(by=['r2'], ascending=False)","19db1a48":"Now there is **no missing values** in our dataset.","d0dd56f4":"From this basic statistical infomation of the dataset, we can find that:\n1. There are about **540,000** samples in the dataset.\n2. There are **\"free cars\"** in the dataset, whose prices are 0. These samples should be removed.\n3. The maximal price is **4 billion**. it is an outlier, no matter it is real or not.\n4. The minimal year is **0**, which is a value with error.\n5. The maximal odometer read is $6.4*10^7$ miles, which should also be an outlier.\n6. \"county\" is a total **empty column**.\n\nWe will talk about the remaining categorical features in the next section. Nevertheless, according to our finding above, this dataset **must be cleaned** before we start applying any analysis on it.","4fdbf891":"<a id=\"head\"><\/a>\n# Used Car Price Prediction Project  \n### with Data Cleaning, Visualization and Machine Learning      \n  \n****\n  \n\nThe dataset is from [Kaggle](https:\/\/www.kaggle.com\/austinreese\/craigslist-carstrucks-data \"Kaggle\"), collected by [Austin Reese](https:\/\/www.kaggle.com\/austinreese \"austinreese\"). which contains more than 500,000 used cars selling records in the US from [craigslist](http:\/\/craigslist.org\/ \"craigslist\").","75301dd9":"It is obvious to see that **diesel cars tend to be more expensive than gas cars**. One of the main reason could be diesel engines are usually more expensive and are **mostly equipped by high power vehicles such as trucks, buses and upmarket SUVs**. A strange phenomenon is that although diesel engines appear far earlier than gas engines, **we can hardly see them in the used cars older than 1990**.","00399f4d":"Now let's check the distribution of the null values among all columns.","5e62d9f4":"<a id=\"5\"><\/a>\n## 5. Model preparation\n[Back to top](#head)\n\nBefore feeding those data to the machine learning models to predict prices, we still need to finish the following preparation tasks.","3ce733e1":"Now let's see whether there are outliers to remove in the **odometer column**.","f03b69e1":"<a id=\"1\"><\/a>\n### 1. Import necessary libraries:\n[Back to top](#head)","1116cac6":"Within expectation, the result is very close to the previous linear regression model.\n\nWith **r2 = 0.801, it seems like the best a linear model can do**, unless we do some deeper processing with the dataset itself.","e474dae4":"From these five figures we conclude the following reasonable phenomena:\n- New and like-new cars tend to be more expensive, while cars with fair and salvage conditions tend to be much cheaper. \n- Cars with 6, 8 or 10 cylinders tend to be more expensive,  while 4 and 5-cylinder cars are cheaper.\n- Diesel cars are more expensive than gas and hybrid cars.\n- Cars with \"other\" transmission (possibly CVT) tend to be the most expensive. Cars with automatic transmission tend to be more expensive than those with manual transmission.\n- Cars equipped with all-wheel drive tend to be more expensive than those with front-wheel drive.","1a4e0b52":"<a id=\"3.5\"><\/a>\n### 3.5 Remove insufficient records","d0ff51bb":"**As locations do not have major correlation to the prices**, we choose to **remove 'long', 'lat' and 'state'** as well.","8a01fbac":"This is another way to see the **high correlations among years mileages and prices**","15bbd235":"Let's first see the relationship between **prices** and **mileages (odometer)**.","122ea256":"For both cases, the models are forced to stop with our maximal iteration setting, or they will run forever... As a result, they both perform badly.","32f553dd":"With 3% data loss, we now get a much better distribution of prices.\n\nBut when we draw the distribution plot of the prices:","1af952b0":"<a id=\"5.1\"><\/a>\n### 5.1 Encoding categorical features\n\nWe transform the string values in all categorical features into numeric values using **One Hot Encoding scheme**. Note that the \"*drop_first*\" parameter in \"*get_dummies*\" is to remove the first column for each feature to avoid the collinearity problem because of the correlations among each newly created columns.","aec250c0":"**Pickup cars, trucks and buses have higher prices** as they cost higher for new. The prices for **sedan, wagon, hatchback and mini-van are more stable**.","caef0fa9":"Now the feature columns are all we believe **correlated to the prices**.","dddad37f":"Some brands have **too few samples**.\n\nWe remove the manufacturers which hold **less than 100 records**.","8906dd02":"There indeed are **some extremely large prices** in our box plot. They are too large for us to actually see the \"box\". We have to remove them. The common way to remove the outliers is using **interquartile range** showing below.","deb79a28":"XGBoost performs pretty well, just behind neuron network.","ba90cdd2":"## Table of Contents\n1. [Import necessary libraries](#1)<br><br>\n2. [Read dataset and first glance](#2)<br><br>\n3. [Data cleaning](#3)\n  - [Outliers](#3.1)\n  - [Uncorrelated columns](#3.2)\n  - [Filling null values](#3.3)\n  - [Remove samples based on common sense](#3.4)\n  - [Remove insufficient records](#3.5)<br><br>\n4. [Data visualization](#4)<br><br>\n5. [Model preparation](#5)\n  - [Encoding categorical features](#5.1)\n  - [Normalization](#5.2)\n  - [Split training and testing set](#5.3)<br><br>\n6. [Models and tuning](#6)\n  - [Linear regression](#6.1)\n  - [Lasso regression](#6.2)\n  - [Multi-layer perceptron](#6.3)\n  - [K-NearestNeighbor](#6.4)\n  - [Decision tree](#6.5)\n  - [Random forest](#6.6)\n  - [Support vector machine](#6.7)\n  - [Gradient Boosting Decision Tree](#6.8)\n  - [XGBoost](#6.9)<br><br>\n7. [Evaluate and conclude](#7)<br><br>","ca122478":"<a id=\"5.2\"><\/a>\n### 5.2 Normalization","0f4cde71":"Next we need to drop some uncorrelated columns. They are columns that we are pretty sure having no correlations to the car prices. This include the **id, url, region, region_url, title_status, VIN, image_url, description, county** (as it is an empty column). As for its **state and cordinate**, we need to make sure they don't have high correlation to the price.","2f49a25b":"The goal of this project is to explore the dataset and discuss some interesting observations through visualizations and train machine learning models to fit and predict the prices of the used cars using supervised learning.  \n\nIt is also a great dataset to practice my data cleaning and transformation skills, as there are many natural problems to solve before analysis in the dataset, such as outliers, null values, categorical features and imbalanced features. As part of the project, we need to clean the data to make it easier to use.","658768a7":"<a id=\"6.2\"><\/a>\n### 6.2 Lasso Regression\n\nLasso Regression is a linear regression model with **L1 regularization** factor to eliminate the errors caused by the collinearity problem and overfitting. We choose **12 regularization coefficents** and choose the best via **cross validation** method.\n\nAlthough we see from above that the poor performance of the linear model is **not caused by overfitting**, we still decide to give Lasso a try.","1c5b746d":"<a id=\"3\"><\/a>\n## 3. Data Cleaning\n[Back to top](#head)","ffdbdbd6":"From the dataframe above, we conclude that **the neuron network performs the best** in this dataset. But even so, the rmse is still around 2600, which is **pretty large compared to the actual prices**.","71ff5111":"We see that **most of the old cars (older than 1990) are rear wheel drive**. **Front wheel drive and all wheel drive started to get popular since 1990**. **Cars with AWD are obviously more expensive than cars with FWD**, which is quite reasonable. There are still some expensive cars with RWD, I guess it is because RWD are equipped by many sports car nowadays.","889b7f94":"Many columns have **nothing to do with the prices of the car**, such as \"id\", \"url\", \"image_url\". We will remove columns like this in the next section.","8cd69494":"We see that KNN model does achive a pretty high accuracy.","61ac0c0b":"With **r2 = 0.917** and **rmse = 2476** on the training set, this MLP regressor is the best model for now.","2d3a24db":"<a id=\"6.3\"><\/a>\n### 6.3 MLP (Fully Connected Neuron Network)\nHere we train a fully connected neuron network for regression using Keras.  \n\nTo change the model from solving classification problem to regression problem, we first change the output layer to be a single cell. Then we change the loss function from cross-entropy to be **MSE**.","594464dd":"In the decision tree model, we did not set the maximal depth or leaf nodes.","1b560275":"We can see that **most of the used cars are being sold in the US**, some others are in other parts of the world. Although the high price cars are not seperated uniformly, **the prices don't seem to vary much geographically**, indicating that there are not significant correlations between prices and coordinates.","4af6ccba":"Based on the situation that there are plenty of null values in our dataset, and the fact that these missing values are **hard to fill with proper guesses**. we decide to take the following three actions:\n1. For columns that have **too much** missing values, we **remove the whole column**. (for column \"size\")\n2. For columns that have **very few** missing values, we **remove the corresponding rows**. (for column \"year\", \"manufacturer\", \"model\", \"fuel\", \"transmission\", \"drive\", \"type\", \"paint_color\")\n3. For columns that have **intermediate number of missing values**, such as \"condition\" and \"cylinders\", we choose to **keep the missing values and assign them another unique category called \"null\"**.","12c0881f":"**Higher prices are more likely to be seen in newer cars**. Older cars tend to be cheaper (but not always).\n\nAlthough there are more and more cars with automatic transmission, manual types are still very poplular. Also, there appears to be **more cars with \"other\" transimission since 2010**. I guess they include new type gearboxes such as **CVT** and DCT.","aff438b5":"<a id=\"6.7\"><\/a>\n### 6.7 Support Vector Machine","e305e6b2":"<a id=\"5.3\"><\/a>\n### 5.3 Split training and testing set\n\nWe set **70%** of the data to be the training set, leaving the remaining for testing.","09e0b92a":"**The prices don't differ very much in each states in the US** either. But we do find that the used car prices are slightly higher in the states neighboured on Canada (such as **Alaska, Idaho, Washington, Montana, North Dakota**). See the figure above, where blue indicates higher average prices, while yellow indicates lower average prices. The second figure is drawn in Power BI.","b69c8741":"<a id=\"6\"><\/a>\n## 6. Models and tuning\n[Back to top](#head)\n\nIn this section, we are going to create and train several machine learning models to see their performance in this used car dataset for price prediction.","d913f6b2":"There are also a lot of **missing values** among features.","52d30662":"<a id=\"6.9\"><\/a>\n### 6.9 XGBoost\n\nFinally, we adapt the advanced version of GBDT -- XGBoost into the dataset.","a7f74a82":"<a id=\"2\"><\/a>\n## 2. Read dataset and first glance\n[back to top](#head)","c7f9cd11":"From the table we see that the top five popular brands in the used car market are **Ford, Chevrolet, Toyota, Nissan and Jeep**.","42e2b32f":"As it is a regression problem, we use **R2 score** and **root mean squared error** as the way to evaluate our models.","4aa9f65f":"Cars that are too old (let's say **earier than 1960**) will increase uncertainty to our data prediction, because of the insufficient amount and probably unstable prices (some of them can be regarded as antiques). So we remove the samples **older than 1960**.","136bc365":"**The best number of neighbors for our knn model is 4**.","ab1b7bdb":"There are many reason for this. For example, we did not play very much with the parameters in the models above, indicating the models **could do better with proper parameters**. Also, the dataset may **not be clean enough**. There are some particular values in the categorical features that are insufficent (e.g. there are **only six 12-cylinder cars** and **165 electric cars** in total). **Some columns are still correlated**. A simple example is that most Subaru models (in the \"model\") are also AWD (in the \"drive\"). But most importantly, according to the scatter plot between the predicted prices and real prices, **there are always some points that have very large differences between the two**. I think the future job to make these model better is to **output these outliers and analze the causes of this huge bias**.","21b1bd73":"The random forest model does not give a good performance. It is even worse than the decision tree. Maybe the tree is not deep enough.","4af4ff61":"![states](https:\/\/steinsgate.gq\/Capture.jpg)","20f4a69a":"**Higher odometer tends to have lower prices**, while lower odometer tends to be more expensive. Also, a funny observation is that people tend to set their prices easy to remember such as 5000, 5100, 8999 (there are **many horizontal straight lines** in the scatter plot above).\n\nHowever, there are some **relatively new cars were sold nearly for free**, which is against common sense. Thus we need to do some intervention to it.","177e807b":"<a id=\"3.2\"><\/a>\n### 3.2 Uncorrelated columns","262639ba":"<a id=\"4\"><\/a>\n## 4. Data visualization\n[Back to top](#head)","151225c2":"**Higher odometer are more likely to be seeing in cars around 2005**. It is reasonable as **newer cars have not yet to accumulate high mileage**, while **older cars need to have lower mileage to be sold in a desent price in the used car market**. If a car has both high age and mileage, then the owner may directly sell the parts to the repair shop rather than selling it as a used car.\n\nAlso, we **can not see straight forward correlation between \"condition\" and \"odometer\" (or \"year\")**, which is a surprise. Myabe different people hold different views toward \"condition\" as it is quite subjective.","764f99d3":"Sometimes KNN can achieve high accuracy, with the cost of time. As it is a type of \"lazy study\" model, the predicting time is very long even if the training is done.\n\nWe use ***GridSearchCV()*** to find the **best number of neighbors** via cross validation.","136ebf1e":"<a id=\"6.1\"><\/a>\n### 6.1 Linear Regression\n\nIn Scikit Learn, ***LinearRegression()*** is using **Ordinary Least Squares** to calculate the coefficients in the linear regression model, without the \"learning\" process through gradient descent.","0f1a6f2d":"Besides customized colors, there are **11** different common colors in the dataset. It seems that **white, black, orange and yellow** cars are the top 4 colors ranked by their median prices. By contrast, **green and purple** are the least welcome colors. Note that due to relatively fewer samples for purple, yellow and orange, the statement above may not totally correct.","5f400356":"We can see that there are a large amount of wierd \"free cars\" in our dataset. We have to remove them as well.\n\nHere we set a threshhold of **$600**.","1c08a0b9":"Now let's head to the car models. For the precision of our future model, we choose to remove the **car models which have less than 50 samples**. It will narrow the capability of our model, but in return lower the bias and variance.","92877a74":"<a id=\"6.5\"><\/a>\n### 6.5 Decision Tree","0934aff3":"We can see that linear regression model does not perform well in the dataset. To show whether it is overfitting, we calculate the score for the **training set**:","9ce7cdbb":"We also need to **normalize the values** in the numerical features (\"year\" and \"odometer\"), as they do not have the same scale as the other newly created columns.","70af73f1":"<a id=\"3.1\"><\/a>\n### 3.1 Outliers","f1a1316c":"<a id=\"3.4\"><\/a>\n### 3.4 Remove samples based on common sense","317c0507":"We see that after the data cleaning, the distribution of the prices looks better. Most of the cars are sold in price between","6673a851":"<a id=\"6.8\"><\/a>\n### 6.8 Gradient Boosting Decision Tree\nWe use Scikit Learn build-in GBDT model with default setting.","27e115b1":"Now let's see how differnt factors influence the car prices","bc51ae72":"<a id=\"6.6\"><\/a>\n### 6.6 Random Forest","5a0a8eeb":"With similar **rmse** and **r2 score**, it seems that overfitting is not the probelm.","64250dfb":"<a id=\"7\"><\/a>\n## 7. Evaluate and conclude\n[Back to top](#head)","3df72a34":"We can see that **most of the cars have 8 cylinders before 1990**. In recent years, **upmarket is occupied by 6 and 8-cylinder cars** and **relatively cheap cars are mainly 4-cylinder type**.","76d57ec0":"Now that we have column 'model', there is no need for the column 'manufacturer' for modeling (as normally one car model only belongs to one manufacturer). But as it is still needed for further visualization, we store this column into a new data frame (**df_man**) before removing it.","fb1a8b73":"We tried two different kernel for the support vector machine, **Gaussian** and **linear**.","bf6c6c11":"<a id=\"3.3\"><\/a>\n### 3.3 Filling null values","29805c4a":"Same problem happens to the odometer column. The outliers could come from two situations:\n\n1. **Mistakes in the samples** (such as the sample larger than 6*10^7 miles)\n2. Most of the used cars have relatively low mileage, which **lower the median**, while some other cars actally run that far.\n\nHowever, both situations cause uncertainty to the prediction of our future model, thus we remove these outliers using the same method as above. The only differece is our filter:\n\n1. **Extremly small odoemter is OK** to keep, as they tend to be new cars.\n2. The upper threshold is set to be higher, accepting more samples.","ac4edfc1":"Although we give up **18%** of our data, the odometer column now should look normal.","84e4c68e":"Fist we need to remove the outliers of the **prices**. See the output from df.describe(), the price range between 25% to 75% quantile of our dataset is 4400 ~ 17926, but the maximum price is 4.3*10^9, which is too high for a price!","947e2423":"<a id=\"6.4\"><\/a>\n### 6.4 K-Nearest Neighbor"}}