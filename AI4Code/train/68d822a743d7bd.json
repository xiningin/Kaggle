{"cell_type":{"4a45fae3":"code","97cbec01":"code","c3e54b7b":"code","d6beeec6":"code","49b2d4e2":"code","98b1a783":"code","9781b945":"code","071dc8c9":"code","e7acc99e":"code","8a373ffa":"code","2537e271":"code","f6bc60fb":"code","d3ed32d6":"code","bf111d25":"code","7922a45a":"code","d6db3d35":"code","64ec190e":"code","d97cb741":"code","8c8544cc":"code","24aebd38":"code","d470dcc8":"code","87cc7652":"code","d4d36eba":"code","c6ea28b1":"code","cde3d4f1":"code","fa3cb1da":"markdown","6ad99af4":"markdown","945d3a07":"markdown","b32f0102":"markdown","d9ff8479":"markdown","b2bf6e1c":"markdown","fca15af5":"markdown","89794724":"markdown","1cdaa102":"markdown","6f6f3ff5":"markdown","b336fa87":"markdown","3a19a549":"markdown"},"source":{"4a45fae3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error #Check the r2 error\nfrom sklearn.metrics import r2_score #Check the r2 error\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler #Perform data scaling\nfrom sklearn.model_selection import cross_val_score, GridSearchCV #Cross valdiation scores\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97cbec01":"train_df=pd.read_csv(os.path.join(dirname, filenames[1]))\ntrain_df.head(3)","c3e54b7b":"test_df=pd.read_csv(os.path.join(dirname, filenames[2]))\ntest_df.head(3)","d6beeec6":"len(test_df)","49b2d4e2":"from nltk.corpus import stopwords\nimport spacy\nimport timeit\nimport re\n\n\nnlp = spacy.load('en')\npunct=\";|!|:|;|,|-|'\"\nstop=set(stopwords.words('english'))\n\ndef preprocess_dataframe(df):\n    #Set a unique Numbering for each exerpt\n    df=df.reset_index()  \n    #Average excerpt length\n    train_df['excerpt_length']=train_df['excerpt'].str.len()\n    avg_excerpt_len=train_df['excerpt_length'].mean().round(0) #Avg. excerpt length\n    #Convert all text to lowecase\n    df['excerpt_preprocess']=df['excerpt'].str.lower()         \n    #FEATURE ENGINEERING: Get the legth of each excerpt\n    df['excerpt_actual_length']=df['excerpt_preprocess'].str.len()\n    #Remove common words from excerpt\n    df['excerpt_preprocess']=df['excerpt_preprocess'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n    #FEATURE ENGINEERING: Get the legth of the preprocessed excerpt\n    df['excerpt_preprocessed_length']=df['excerpt_preprocess'].str.len()\n    #FEATURE ENGINEERING: Percent frequent words\n    df['excerpt_stopword_freq']=(df['excerpt_actual_length']-df['excerpt_preprocessed_length'])\/df['excerpt_actual_length']\n    #FEATURE ENGINEERING: Get count of punctuations in the excerpt\n    df['excerpt_punct_count']=df['excerpt'].apply(lambda x: len(re.findall(punct, x)))\n    #Convert excerpt into setences\n    df['excerpt_sentence'] = df['excerpt_preprocess'].apply(lambda x: list(nlp(x).sents))\n    #Convert each setence of the exerpt into a pandas row\n    df=df.explode('excerpt_sentence')\n    #Convert spacy object to string object\n    df['excerpt_sentence']=df['excerpt_sentence'].apply(lambda x: x.text)    \n    ##FEATURE ENGINEERING: Get sentence length\n    df['sentence_length']=df['excerpt_sentence'].str.len()\n    ##FEATURE ENGINEERING: Get word count\n    df['totalwords'] = df['excerpt_sentence'].str.split().map(len)\n    ##FEATURE ENGINEERING: Get normalized word count\n    df['normalized_word_count'] = round(df['sentence_length']\/df['totalwords'],2)\n    ##FEATURE ENGINEERING: Get normalized stopword frequency\n    df['normalized_stopword_freq']=round(df['excerpt_stopword_freq']*avg_excerpt_len,1)\n    ##FEATURE ENGINEERING: Get average senetence length\n    df['avg sent length']=df[['sentence_length', 'index']].groupby(['index']).agg(['median'])\n    ##FEATURE ENGINEERING: Get average senetence length\n    df=df[['index','id','excerpt','excerpt_preprocess','avg sent length','normalized_word_count','normalized_stopword_freq']].drop_duplicates(subset ='index').set_index('index')\n    return df","98b1a783":"from datetime import datetime\n\nnow = datetime.now()\ntarget=train_df['target']\ntrain_df=preprocess_dataframe(train_df)\ntrain_df['target']=target\nlater = datetime.now()\ndifference = int((later - now).total_seconds())\n\nprint(\"Execution Time: \",difference)\nprint(\"Dataframe length: \",len(train_df))","9781b945":"train_df.head(3)","071dc8c9":"train_df['target'].head()","e7acc99e":"len(train_df)","8a373ffa":"from sklearn.feature_extraction.text import TfidfVectorizer\nv = TfidfVectorizer()\nx= v.fit_transform(train_df['excerpt'])\ndf1 = pd.DataFrame(x.toarray())\ntrain_df_x=train_df[['avg sent length','normalized_word_count','normalized_stopword_freq']]\ntrain_df_x = pd.concat([train_df_x, df1], axis = 1)","2537e271":"scaler = MinMaxScaler()\ntrain_df_x = scaler.fit_transform(train_df_x)","f6bc60fb":"train_df_x","d3ed32d6":"from sklearn.ensemble import RandomForestRegressor\n\nnow = datetime.now()\nregr = RandomForestRegressor(random_state=0)","bf111d25":"regr","7922a45a":"regr.fit(train_df_x, train_df['target'])","d6db3d35":"#Get the predicted target scores\ny_train_predict=regr.predict(train_df_x)","64ec190e":"#Cross validation r2 score\nscores = cross_val_score(regr, y_train_predict.reshape(-1, 1), train_df['target'], cv=3, scoring='neg_root_mean_squared_error')\nscores","d97cb741":"#r2_score(y_train_predict, train_df['target'])\nround(np.sqrt(mean_squared_error(y_train_predict, train_df['target'])),3)","8c8544cc":"r2_score(y_train_predict, train_df['target'])","24aebd38":"later = datetime.now()\ndifference = int((later - now).total_seconds())\nprint(\"Sklearn execution time: \",difference)","d470dcc8":"now = datetime.now()\ntest_df=preprocess_dataframe(test_df)\nlater = datetime.now()\ndifference = int((later - now).total_seconds())\n\nprint(\"Execution Time: \",difference)\nprint(\"Dataframe length: \",len(test_df))","87cc7652":"x_test= v.transform(test_df['excerpt'])\ndf1 = pd.DataFrame(x_test.toarray())\ntest_df_x=test_df[['avg sent length','normalized_word_count','normalized_stopword_freq']]\ntest_df_x = pd.concat([test_df_x, df1], axis = 1)\ntest_df_x = scaler.transform(test_df_x)","d4d36eba":"y_test_predict = regr.predict(test_df_x)\nids = test_df['id']\n\nprint(y_test_predict.shape)\nprint(type(y_test_predict))","c6ea28b1":"submission_df = pd.DataFrame({'id': ids, 'target': y_test_predict})\nsubmission_df.to_csv('\/kaggle\/working\/submission.csv', index=False)","cde3d4f1":"submission_df","fa3cb1da":"**Preprocess the test data and get the test scores**","6ad99af4":"## Notebook Overview\n\nThe goal of the notebook is to clean CLRP Data, vectorize the excerpt data and add additional features to the data. The main packages used here are NLTK, regex, and pandas, and sklearn to achieve this.\n\nHere are the various processes in this notebook:\n\n1. Reading Data\n2. Clean the Data\n3. Feature Engineering\n4. Vectorize data\n5. Build a model & create baseline predictions","945d3a07":"**Normalize the data**","b32f0102":"Call the above function to - clean and generate features for the train data","d9ff8479":"**Process 5: Model building & Prediction**\n\nDefine the baseline model for the prediction.","b2bf6e1c":"**Get the execution time of the prediction model**","fca15af5":"**Fit the model using the features and the target score**","89794724":"**Process 1: Reading the data**","1cdaa102":"**Get the rmse Score of the model.**","6f6f3ff5":"Verify that the data import and explore the data by printing the top 3 rows in the dataframe.","b336fa87":"**Process 4: Vectorize the Data**\n\nConvert the excerpt into a sparse matrix using TFIDF.","3a19a549":"**Process 2: Clean the Data**\n\nConvert the excerpt into lower case(so that an accurate count of the words can be obtained).\nThen remove the most frequently occurring words like - 'an', 'the', and 'on'. This list of frequently occurring can be obtained from the NLTK library.\n\n**Process 3: Feature engineering**\n\nThree new features were created:\n\n    1.Average sentence length of the Excerpt \n    2.Normalized word count\n    3.Normalized stopword frequency"}}