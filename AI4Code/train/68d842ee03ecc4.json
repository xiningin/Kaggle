{"cell_type":{"6104a0ec":"code","99dbabe3":"code","9129ff4d":"code","3808947a":"code","7705ae36":"code","5c6740b9":"code","a3a30ce2":"code","0da1646a":"code","f8bb8427":"code","2836139c":"code","d6442ebc":"code","c193ba99":"code","5cc56f58":"code","5a4e1565":"code","db4fe1dd":"markdown","d3376840":"markdown","8da35382":"markdown","8c10398c":"markdown","37162f02":"markdown","0c7ba3f4":"markdown","7d9a4bb3":"markdown","75ed35a7":"markdown","1844b92b":"markdown","590c3a0e":"markdown","e33b2831":"markdown"},"source":{"6104a0ec":"\"\"\"\nThe dataset contains several parameters which are considered important during \nthe application for Masters Programs in India.\nThe parameters included are : \n1. GRE Scores ( out of 340 ) \n2. TOEFL Scores ( out of 120 ) \n3. University Rating ( out of 5 ) \n4. Statement of Purpose\n5. Letter of Recommendation Strength ( out of 5 ) \n6. Undergraduate GPA ( out of 10 ) \n7. Research Experience ( either 0 or 1 ) \n8. Chance of Admit ( ranging from 0 to 1 )\n\nI borrowed some of my ideas for data cleaning and exploration from Jonas Erthal's pipeline notebook at\nhttps:\/\/www.kaggle.com\/apollopower\/grad-admissions-full-ml-pipeline-96-accuracy\/notebook\n\"\"\"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","99dbabe3":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\ndf.head()","9129ff4d":"df.dtypes","3808947a":"df.set_index('Serial No.', inplace=True)\ndf.rename(index=str, columns={\"Chance of Admit \": \"Chance_of_Admit\",\n                             \"GRE Score\": \"GRE_Score\",\n                             \"TOEFL Score\": \"TOEFL_Score\",\n                             \"University Rating\": \"University_Rating\",\n                             \"SOP\": \"Statement_of_Purpose\",\n                             \"LOR \": \"Letter_of_Recommendation\",\n                             \"CGPA\": \"College_GPA\"}, inplace=True)\ndf.head()","7705ae36":"df['Chance_of_Admit'].describe()","5c6740b9":"df.hist(column=\"Chance_of_Admit\", figsize=(8,4))","a3a30ce2":"plt.scatter(df['GRE_Score'], df['Chance_of_Admit'])\nplt.title('Chance of Admit vs GRE Score')\nplt.show()\nplt.scatter(df['TOEFL_Score'], df['Chance_of_Admit'])\nplt.title('Chance of Admit vs TOEFL Score')\nplt.show()\nplt.scatter(df['University_Rating'], df['Chance_of_Admit'])\nplt.title('Chance of Admit vs University Rating')\nplt.show()\nplt.scatter(df['College_GPA'], df['Chance_of_Admit'])\nplt.title('Chance of Admit vs CGPA')\nplt.show()","0da1646a":"for col in df.columns[:-1]:\n    print('Chance of Admit vs {} : {}'.format(col, round(df['Chance_of_Admit'].corr(df[col]), 4)))","f8bb8427":"corr_matrix = df.corr()\n\nsns.heatmap(corr_matrix)","2836139c":"sns.lmplot('College_GPA', 'Chance_of_Admit', data=df, hue='Research')","d6442ebc":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","c193ba99":"features = list(df.columns[:-1])\n\ntrain_x = df[features]\ntrain_y = df['Chance_of_Admit']\n\nprint(features)","5cc56f58":"linear_regression_model = LinearRegression()\n\nlinear_regression_model.fit(train_x, train_y)","5a4e1565":"pd.DataFrame(linear_regression_model.coef_, train_x.columns, columns=['Coefficient'])","db4fe1dd":"### Modeling the data\n\nMost of the data visualizations show an approximately linear relationship between the predictors (or features) and the target (or label) variable, so let's start by trying to fit a basic Linear Regression model to the data.","d3376840":"As we might have guessed, the predictors with the highest correlation were Undergraduate GPA, GRE Score, and TOEFL Score. Since these predictors correlated so strongly with the target variable, we would expect them to have a significant impact on any models we might create to predict Chance of Admission. ","8da35382":"In the results, we see that College GPA was the strongest predictor by a factor of 5, followed by Research Experience then Letter of Recommendation strength. We expected GPA from the beginning, and digging a little deeper into our data, we also found Research Experience to be a possible strong predictor. What was not expected was Letter of Recommendation strength. Data Science is an iterative process, so let's go back and look at Letter of Recommendation strength's histogram and scatter plot.","8c10398c":"We can get an idea of the correlation between all of the variables using a correlation matrix. In the above heatmap created by seaborn, we can clearly see that research experience has the lowest overall correlation, and our strongest correlation is between GRE and TOEFL Scores, College GPA, and Chance of Admit, as we saw with our above calculations.\n\nClearly we have our strong variables, but if we look closely at the relationship between Research, College_GPA (the variable it's most strongly correlated with), and Chance_of_Admit, we see an interesting correlation.","37162f02":"In this step, I'll separate my feature\/predictor variables from my target\/label variable by creating a list of df's columns from index 0 to the last column, exclusive. After that, I'll train the Linear Regression model!","0c7ba3f4":"Visually, GPA increases for students with research experience correlate to higher chances of admission, as we can see from the regression line included on the graph. Our orange dots back up that story, showing that the vast majority of students with Chance of Admission over 80% have research experience.","7d9a4bb3":"All of these charts suggest a positive linear correlation between the variables and chance of admission, some more than others. Let's take a peek at what our actual correlation values are.","75ed35a7":"I think it is a sound hypothesis that there will be a correlation between GRE scores and Chance of Admit and it will be a positive correlation, but let's test that hypothesis with GRE scores and some of the other values.","1844b92b":"As good data scientists, we will want to know how each of the predictors affect the model. For that, we can create a DataFrame with our predictors as one column and their coefficient values as the other.","590c3a0e":"Before we delve too far into the data, let's try making our lives a little easier by setting an index column, removing the extra space at the end of the 'Chance of Admit ' column name, and standardizing the names to snake_case.","e33b2831":"Now that our DataFrame looks a little bit neater and won't be surprising us with any Missing Index errors, we can start exploring the Chance of Admit variable, which will be our target variable. Below you'll find the summary statistics and the default histogram, which shows somewhat of a bell-shaped (Normal) distribution centered around 75% and is skewed to the left. This is supported by the large difference between our min value of 34 and our 25% value of 64, which is a 30 point difference, where the others are around 10% different from each other."}}