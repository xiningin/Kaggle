{"cell_type":{"02aed10e":"code","90468e30":"code","db5b4e48":"code","6e9953a0":"code","392a9952":"code","bf942889":"code","d9b71adb":"code","8c7243f6":"code","f24ed45a":"code","4a2130e3":"code","4bb21aaa":"code","9b49832e":"code","062b6fd4":"code","04cf1487":"code","ac4d59dd":"code","e656edf7":"code","fa090794":"code","dbd766e1":"code","3dba6df6":"code","4d4333a9":"code","71bb53b7":"markdown","466d6d2b":"markdown","f57e2ef3":"markdown","d58035dd":"markdown","53329460":"markdown","3830d051":"markdown","10577a5a":"markdown"},"source":{"02aed10e":"!pip install -U sentence-transformers","90468e30":"from sentence_transformers import SentenceTransformer\n\n#there are about 10 pretrained models\n#roberta-large-nli-stsb-mean-tokens - returns 1024 dimentional vector\n#distilbert-base-nli-stsb-mean-tokens - returns 768 dimentional vector\n\nPRETRAINED_MODEL='roberta-large-nli-stsb-mean-tokens'    # 'distilbert-base-nli-stsb-mean-tokens'        \nmodel = SentenceTransformer(PRETRAINED_MODEL)  \n","db5b4e48":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path \nfrom sklearn import preprocessing\nimport os\nfrom timeit import default_timer\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport regex as re\nimport lightgbm as lgbm","6e9953a0":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","392a9952":"DATA_ROOT = Path(\"..\") \/ \"\/kaggle\/input\/sentiment-analysis-on-movie-reviews\"\ntrain = pd.read_csv(DATA_ROOT \/ 'train.tsv.zip', sep=\"\\t\")\ntest = pd.read_csv(DATA_ROOT \/ 'test.tsv.zip', sep=\"\\t\")\nprint(train.shape,test.shape)\ntrain.head()","bf942889":"test.head()","d9b71adb":"\n#add two simple features - number of chars and words\ndef add_features (df):\n    df['nwords'] = df.Phrase.apply(lambda text: len(re.findall(r'\\w+', text)))\n    df['nchars'] = df.Phrase.apply(lambda text: len(text))","8c7243f6":"add_features (test)\nadd_features (train)\ntest.head()","f24ed45a":"#Do it with GPU !!!!!\n#on CPU 80 times slower\n\nTRANSFORMER_BATCH=128\n\ndef count_embedd (df):\n    idx_chunk=list(df.columns).index('Phrase')\n    embedd_lst = []\n    for index in range (0, df.shape[0], TRANSFORMER_BATCH):\n        embedds = model.encode(df.iloc[index:index+TRANSFORMER_BATCH, idx_chunk].values, show_progress_bar=False)\n        embedd_lst.append(embedds)\n    return np.concatenate(embedd_lst)\n","4a2130e3":"# sentence embeddings for TRAIN dataset, 1024 dimentions each\nstart_time = default_timer()\ntrain_embedd = count_embedd(train)\nprint(\"Train embeddings: {}: in: {:5.2f}s\".format(train_embedd.shape, default_timer() - start_time))","4bb21aaa":"# sentence embeddings for TEST dataset, 1024 dimentions each\nstart_time = default_timer()\ntest_embedd = count_embedd(test)\nprint(\"Test embeddings: {}: in: {:5.2f}s\".format(test_embedd.shape, default_timer() - start_time))\n","9b49832e":"X_train = np.array(train_embedd)\nX_test = np.array(test_embedd)\nX_train = np.concatenate((X_train, train[['nchars','nwords']].values), axis=1)\nX_test = np.concatenate((X_test, test[['nchars','nwords']].values), axis=1)\n\nX_train.shape, X_test.shape","062b6fd4":"#convert labels into 5-dimentional vector\n\nenc = preprocessing.OneHotEncoder()\nlabel = train['Sentiment'].values.reshape ((-1,1))\nenc.fit(label)\ny_train = enc.transform(label).toarray()\ny_train.shape","04cf1487":"\n\nKERAS_VALIDATION_SPLIT=0.05\nKERAS_EPOCHS=10\nKERAS_BATCH_SIZE=128\n\n# Create and train Keras model\nn_features=X_train.shape[1]\nn_labels = y_train.shape[1]\n\nstart_time=default_timer()\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(2048, input_dim=n_features, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(n_labels, activation='softmax')\n])\n\nLR=0.0001\nadam = keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999, amsgrad=False)\n\nmodel.compile(optimizer=adam, \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=KERAS_EPOCHS, batch_size=KERAS_BATCH_SIZE, validation_split=KERAS_VALIDATION_SPLIT)\n\nprint(\"Training. Dataset size: {} {:5.2f}s\".format(X_train.shape, default_timer() - start_time))","ac4d59dd":"y_preds = model.predict(X_test)\ny_preds.shape","e656edf7":"sample_submission = pd.read_csv(DATA_ROOT \/ 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(y_preds,axis=1)\nsample_submission.to_csv(\"predictions.csv\", index=False)","fa090794":"params = {\n    'objective': 'multiclass',\n    'num_class':y_train.shape[1]\n    #'metric': 'multi_logloss'\n}\nlgbm_model = lgbm.LGBMClassifier(objective='multiclass')\nlgbm_model.fit(X_train, label)\n\ny_preds = lgbm_model.predict_proba(X_test)\n\nsample_submission = pd.read_csv(DATA_ROOT \/ 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(y_preds,axis=1)\nsample_submission.to_csv(\"predictions_lgbm.csv\", index=False)","dbd766e1":"test.tail()","3dba6df6":"sample_submission.tail()","4d4333a9":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='predictions_lgbm.csv')\n","71bb53b7":"Compare records in sampleSubmission.csv and results","466d6d2b":"\n## Sentiment analysis with Sentence Transformers\n\nSentence Transformers: Sentence Embeddings using BERT \/ RoBERTa \/ DistilBERT \/ ALBERT \/ XLNet with PyTorch\nhttps:\/\/github.com\/UKPLab\/sentence-transformers\n\n\nIdea from this notebook \nhttps:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta\n\nMaximilien Roberti trains BERT-like transformers and get **0.7** score in Sentiment analisys competition.\n\nHere I use pre-trained models to get sentence embeddings and feed them into simple classifier.\nMy result is **0.67177** - not so bad and much faster.\n\nUpdate:\nPlus two features - num words in sentence and num chars - **0.67569**. It would be 10th result 6 years ago\n\n","f57e2ef3":"## Sentiment Analysis on Movie Reviews\nhttps:\/\/www.kaggle.com\/c\/sentiment-analysis-on-movie-reviews\/overview\n\nThe sentiment labels are:\n\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive\n\n\nRecords contains parts of reviews, some of them just one letter 'A'.\n**We will ignore fields PhraseId, SentenceId so result will be far from 100%**","d58035dd":"Model was loaded. No need to format or tokenize text. All is done inside.\n\n**Should be done on GPU**","53329460":"## Train\n\nWe can use any kind of classifier","3830d051":"## Libraries and external models.\n","10577a5a":"## LightGBM\n\nonly **0.64979** scores"}}