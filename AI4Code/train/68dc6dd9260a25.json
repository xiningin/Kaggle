{"cell_type":{"7be8fb99":"code","04b6f67f":"code","12e78c47":"code","509a7c0b":"code","67da0102":"code","df6d9f82":"code","e18616b0":"code","1ddac5c4":"code","9b23ca98":"code","ee912ca3":"code","7c122270":"code","fe6e713a":"code","f184431c":"code","289967b3":"code","2efa4e67":"code","ed6dc1c2":"code","b192c7b4":"code","d17af273":"code","412e671f":"code","ff679eb7":"code","e43ec6ad":"code","88781196":"code","6c5a14d2":"code","23d5ef00":"code","5ba2ad35":"code","f49045d5":"code","2aa1f363":"code","def3bafa":"code","1ef4e202":"code","5a4fad87":"code","4cb7da07":"code","e3ba1181":"code","59e6ac61":"code","cdbb5ce6":"code","8bc70d28":"code","4da5d3f7":"code","843698b5":"code","84e18752":"code","cf92d031":"code","955fb9f9":"code","724b9b40":"code","0708ed91":"code","7e118c2c":"code","c5d0b908":"code","d4cf67a9":"code","6b4d3301":"code","9f2efcda":"code","677fb089":"code","5109514e":"code","4abb2778":"code","4c7c2950":"code","5d93ec21":"code","fda676f4":"code","4fe0331a":"code","77d42cdc":"code","a618d029":"code","b2b1b342":"code","77dda7bf":"code","2fac6856":"code","d033f353":"code","b224d6c5":"code","344c51b4":"code","3438c1a7":"code","f51b5702":"code","7238af58":"code","c86eb292":"code","bf14ba85":"code","c2c69fb9":"code","3ffc7636":"code","b1e6262e":"code","83a9a7df":"code","724bdf94":"code","454ef6d8":"code","9705eaae":"code","f49a8d7b":"markdown","00bdb448":"markdown","51dba4fe":"markdown","facbb0ed":"markdown","5ac93f47":"markdown","a31928ba":"markdown","ce1eb3c8":"markdown","28a5686b":"markdown","ef4099b6":"markdown","2ca5fac8":"markdown","6283a552":"markdown","28dcc18b":"markdown","03396e79":"markdown","9a1e91c7":"markdown","bfe05e6d":"markdown","fedce01d":"markdown","be23c211":"markdown","88a5eec2":"markdown","4cc3a143":"markdown","9636c162":"markdown","f4d12bdb":"markdown","7c5f001f":"markdown","54799fb4":"markdown","50953f53":"markdown","b99c7aec":"markdown","379b98c9":"markdown","acde7b7e":"markdown"},"source":{"7be8fb99":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#import pandas \nimport pandas as pd\n\n#import numpy\nimport numpy as np\n\n#import seaborn for visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\n\n#import statsmodel \nimport statsmodels.api as sm \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#import sklearn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score,f1_score,roc_curve,roc_auc_score","04b6f67f":"# Changing to show more rows for visual analysis\npd.set_option('display.max_rows', 500)\n# Changing display format to not show scientific notation\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n# Displaying all columns\npd.set_option('display.max_columns', 500)","12e78c47":"#import data\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","509a7c0b":"data.head()","67da0102":"data.shape","df6d9f82":"data.describe()","e18616b0":"#Function to calculate the missing value percent in DataFrame columns - As we are goining to do it frequently\ndef missingValues(df):\n   missingcontent=round(df.isnull().sum()\/len(df) *100,2)\n   print(\"Total Missing Value Percentage in dataframe: \",round(missingcontent.mean(),2))\n   print(missingcontent[missingcontent>0].sort_values(ascending=False))","1ddac5c4":"#null value percentage in data\nmissingValues(data)","9b23ca98":"data.drop(columns=['id','Unnamed: 32'],inplace=True)","ee912ca3":"#null value percentage in data\nmissingValues(data)","7c122270":"# 'Diagnosis' count details\ndata['diagnosis'].value_counts()","fe6e713a":"sns.barplot(y=data['diagnosis'].value_counts(),x=data['diagnosis'].unique(),palette=\"pastel\")","f184431c":"plt.figure(figsize=(20,20))\nsns.heatmap(data.corr(),cmap='YlGnBu',annot=True)\nplt.show()","289967b3":"# generate a pair plot with the \"mean\" columns alone\ncols = ['diagnosis',\n        'radius_mean', \n        'texture_mean', \n        'perimeter_mean', \n        'area_mean', \n        'smoothness_mean', \n        'compactness_mean', \n        'concavity_mean',\n        'concave points_mean', \n        'symmetry_mean', \n        'fractal_dimension_mean']\n\nsns.pairplot(data=data[cols], hue='diagnosis', palette='RdBu')","2efa4e67":"#cols to be dropped inorder to handle the multicollinearity between the variables\ncols= ['perimeter_se', 'area_se',\n       'perimeter_mean','area_mean',\n       'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst',\n       'concavity_mean',\n       'concave points_mean',\n       'concavity_se', 'concave points_se']","ed6dc1c2":"data.drop(columns=cols,axis=1,inplace=True)","b192c7b4":"#Our final columns for our model\ndata.columns","d17af273":"#Binary map of 'M' & 'B' values in the diagnosis column \ndata['diagnosis'] = data['diagnosis'].map({'B':0,'M':1})","412e671f":"data.describe()","ff679eb7":"y = data.pop('diagnosis')\nX= data","e43ec6ad":"X.head()","88781196":"# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, random_state = 100)","6c5a14d2":"print('X_Train Dataset: ',X_train.shape)\nprint('y_Train Dataset: ',y_train.shape)\nprint('X_Test Dataset: ',X_test.shape)\nprint('y_Test Dataset: ',y_test.shape)","23d5ef00":"#data normalization using sklearn MinMaxScaler\nscaler = MinMaxScaler()","5ba2ad35":"cols = X_train.columns","f49045d5":"X_train[cols] = scaler.fit_transform(X_train[cols])\nX_test[cols] = scaler.transform(X_test[cols])","2aa1f363":"X_train.describe()","def3bafa":"# Logistic regression model using statsmodel\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","1ef4e202":"#Function to calculate VIF values\ndef VIF_values(X_train):\n    vif = pd.DataFrame()\n    X= X_train\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    print(vif)","5a4fad87":"VIF_values(X_train)","4cb7da07":"#dropping the 'smoothness_mean' from the model\nX_train = X_train.drop(columns='smoothness_mean',axis=1)","e3ba1181":"X_train_sm = sm.add_constant(X_train)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","59e6ac61":"VIF_values(X_train)","cdbb5ce6":"#dropping the 'compactness_mean' from the model\nX_train = X_train.drop(columns='compactness_mean',axis=1)","8bc70d28":"X_train_sm = sm.add_constant(X_train)\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","4da5d3f7":"# feature variables and their corresponding VIFs\nVIF_values(X_train)","843698b5":"#dropping the 'symmetry_mean' from the model\nX_train = X_train.drop(columns='symmetry_mean',axis=1)","84e18752":"X_train_sm = sm.add_constant(X_train)\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","cf92d031":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF_values(X_train)","955fb9f9":"#dropping the 'compactness_se' from the model\nX_train = X_train.drop(columns='compactness_se',axis=1)","724b9b40":"X_train_sm = sm.add_constant(X_train)\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","0708ed91":"#dropping the 'symmetry_se' from the model\nX_train = X_train.drop(columns='symmetry_se',axis=1)","7e118c2c":"X_train_sm = sm.add_constant(X_train)\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","c5d0b908":"#dropping the 'smoothness_se' from the model\nX_train = X_train.drop(columns='smoothness_se',axis=1)","d4cf67a9":"X_train_sm = sm.add_constant(X_train)\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","6b4d3301":"#dropping the 'texture_se' from the model\nX_train = X_train.drop(columns='texture_se',axis=1)","9f2efcda":"X_train_sm = sm.add_constant(X_train)\nlogm8 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm8.fit()\nres.summary()","677fb089":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF_values(X_train)","5109514e":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","4abb2778":"y_train_pred","4c7c2950":"y_train_pred_final = pd.DataFrame({'Diagnosis':y_train.values, 'Diagnosis_Prob':y_train_pred})","5d93ec21":"#predicted values above 0.5 is considered to be Malignant i.e 1\ny_train_pred_final.Diagnosis_Prob = y_train_pred_final.Diagnosis_Prob.map(lambda x: 1 if x>0.5 else 0)","fda676f4":"y_train_pred_final.head()","4fe0331a":"#build confusion matrix using confusion_matrix from sklearn.metrics\nconfusion = confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob)","77d42cdc":"plt.figure(figsize=(10,5))\ncategories = ['Beingn', 'Malignant']\nsns.heatmap(confusion,annot=True,fmt='d', cmap='Blues',linewidths=1,xticklabels=categories,yticklabels=categories,cbar=False)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","a618d029":"print('Accuracy Score : ',accuracy_score(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob))\nprint('f1 Score : ',f1_score(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob))","b2b1b342":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n\nprint('Sensitivity : ',TP \/ float(TP+FN))\nprint('Specificity : ',TN \/ float(TN+FP))","77dda7bf":"# Calculate false postive rate - predicting Malignant when patient does have beingn\nprint(FP\/ float(TN+FP))","2fac6856":"# positive predictive value  and  Negative predictive value\n\nprint('positive predictive value: ',TP \/ float(TP+FP))\nprint('Negative predictive value: ',TN \/ float(TN+ FN))","d033f353":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","b224d6c5":"draw_roc(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob)","344c51b4":"#filtering out the columns based on our final model \n\nX_test = X_test[X_train.columns]","3438c1a7":"#add constant to the X_test data\n\nX_test_sm = sm.add_constant(X_test)","f51b5702":"#predict the y_test values\n\ny_test_pred = res.predict(X_test_sm)","7238af58":"# forming new dataframe holding y_test and y_test_pred values\n\ny_pred_final = pd.concat([pd.DataFrame(y_test),pd.DataFrame(y_test_pred)],axis=1)","c86eb292":"y_pred_final.head()","bf14ba85":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'diagnosis_Prob'})","c2c69fb9":"y_pred_final['diagnosis_Prob'] = y_pred_final['diagnosis_Prob'].map(lambda x: 1 if x>0.5 else 0)","3ffc7636":"print('Accuracy Score : ',accuracy_score(y_pred_final.diagnosis, y_pred_final.diagnosis_Prob))\nprint('f1 Score : ',f1_score(y_pred_final.diagnosis, y_pred_final.diagnosis_Prob))","b1e6262e":"#build confusion matrix using confusion_matrix from sklearn.metrics\nconfusion = confusion_matrix(y_pred_final.diagnosis, y_pred_final.diagnosis_Prob)","83a9a7df":"plt.figure(figsize=(10,5))\ncategories = ['Beingn', 'Malignant']\nsns.heatmap(confusion,annot=True,fmt='d', cmap='Blues',linewidths=1,xticklabels=categories,yticklabels=categories,cbar=False)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","724bdf94":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","454ef6d8":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","9705eaae":"# Let us calculate specificity\nTN \/ float(TN+FP)","f49a8d7b":"Observation:\n    \n- We have built a model with `Accuracy score` - `92.74%` and of `f1_score` - `89.71%`","00bdb448":"Let's see the sensitivity and specificity values ","51dba4fe":"### Confusion Matrix","facbb0ed":"Observation:\n    -`texture_se` has p_value of `0.253`","5ac93f47":"### Predictions on Test Data","a31928ba":"Observation:\n   - `compactness_se` has high VIF`10.16`","ce1eb3c8":"#### Plotting ROC Curve","28a5686b":"Lets' drop the cols, which we have stated above as highly correlated","ef4099b6":"finding correlations using the heatmap","2ca5fac8":"## Breast Cancer Prediction - Using Logistic regression\n\nHello Kagglers,\n\nWelcome to my first kernel on Kaggle. In this notebook, I analyse the Breast Cancer dataset and develop a Logistic Regression model to try classifying suspected cells to either Benign or Malignant.","6283a552":"Observation:\n   - `smoothness_se` has P_value of `0.394`","28dcc18b":"#### Splitting the data to X,y DataSets","03396e79":"Observation:\n    \n   - all the `worst` scenerios data is highly correlated with the `mean` data\n   - Ex: `radius_mean` is highly correlated with `radius_worst` \n   - similarly `radius_se` with 'perimeter_se' & 'area_se'\n   - High correlation exists between many variables\n   ","9a1e91c7":"#### confusion Matrix","bfe05e6d":"Observation:\n   - `symmetry_se` has high P-value of `0.995`","fedce01d":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","be23c211":"creating a dataframe to hold `diagnosis`and the predicted `diagnosis_prob` of train dataset","88a5eec2":"dropping 'Unnamed: 32' - since its holding null values 100% and 'id' as well - since doesn't hold any significance","4cc3a143":"Observation:\n    -`compactness_mean` has high VIF ","9636c162":"Test and Train Dataset Split","f4d12bdb":"#### Feature Rescaling\n\nNormalising the numerical columns using Min Max scaler","7c5f001f":"Observation: \n - `symmetry_mean` has high VIF of 15.07","54799fb4":"Observation:\n    \n   - Let's rescale the values later inorder to have cofficients of same scale","50953f53":"Observation:\n    \n   - `radius_mean` is highly correlated with 'perimeter_mean', 'area_mean'\n   - `compactness_mean` is similar to 'concavity_mean' & 'concave_points_mean'","b99c7aec":"Observation:\n    \n   - `smoothness_mean` has high VIF indicating the multicollinearity","379b98c9":"### Logistic Regression model","acde7b7e":"### Data Cleaning and preparation"}}