{"cell_type":{"2b180bc5":"code","11f18c91":"code","0963773d":"code","fb9cb4c9":"code","e31d5897":"code","3fce7827":"code","44da0c97":"code","01f065c3":"code","31c84eaa":"code","3772d34b":"code","cb60c254":"code","6c56a940":"code","c615b91e":"code","f0b18c69":"code","b655b2e3":"code","fa157f8e":"code","cb8f8810":"code","b7c69e08":"code","7cc36f95":"markdown","29475ef3":"markdown"},"source":{"2b180bc5":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random, datetime, math\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy.stats import ks_2samp\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","11f18c91":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","0963773d":"########################### Vars\n#################################################################################\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = True\nMAKE_MODEL_TEST = True\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","fb9cb4c9":"########################### Model params\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.05,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':80000,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","e31d5897":"########################### Model\nimport lightgbm as lgb\n\ndef make_test_predictions(tr_df, tt_df, target, lgb_params, NFOLDS=2):\n    \n    new_columns = set(list(train_df)).difference(base_columns + remove_features)\n    features_columns = base_columns + list(new_columns)\n    \n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n\n    for col in list(X):\n        if X[col].dtype=='O':\n            X[col] = X[col].fillna('unseen_before_label')\n            P[col] = P[col].fillna('unseen_before_label')\n\n            X[col] = train_df[col].astype(str)\n            P[col] = test_df[col].astype(str)\n\n            le = LabelEncoder()\n            le.fit(list(X[col])+list(P[col]))\n            X[col] = le.transform(X[col])\n            P[col]  = le.transform(P[col])\n\n            X[col] = X[col].astype('category')\n            P[col] = P[col].astype('category')\n        \n    tt_df = tt_df[['TransactionID',target]]    \n    predictions = np.zeros(len(tt_df))\n\n    tr_data = lgb.Dataset(X, label=y)\n    vl_data = lgb.Dataset(P, label=P_y) \n    estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n    pp_p = estimator.predict(P)\n    predictions += pp_p\/NFOLDS\n\n    if LOCAL_TEST:\n        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n        print(feature_imp)\n        \n    tt_df['prediction'] = predictions\n    \n    return tt_df\n## -------------------","3fce7827":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_pickle('..\/input\/ieee-data-minification\/train_transaction.pkl')\n\nif LOCAL_TEST:\n    \n    # Convert TransactionDT to \"Month\" time-period. \n    # We will also drop penultimate block \n    # to \"simulate\" test set values difference\n    train_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n    test_df = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n    train_df = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n    del train_df['DT_M'], test_df['DT_M']\n    \nelse:\n    test_df = pd.read_pickle('..\/input\/ieee-data-minification\/test_transaction.pkl')\n    \nprint('Shape control:', train_df.shape, test_df.shape)","44da0c97":"########################### Features\n#################################################################################\n# Add list of feature that we will\n# remove later from final features list\nremove_features = [\n    'TransactionID','TransactionDT', # These columns are pure noise right now\n    TARGET,\n    ]\n\n# Let's also remove all V columns for tests\nremove_features += ['V'+str(i) for i in range(1,340)]\n\nbase_columns = [col for col in list(train_df) if col not in remove_features]","01f065c3":"#### Let's make baseline model \nif MAKE_MODEL_TEST:\n    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n####","31c84eaa":"########################### Let's find nan groups in V columns\nnans_groups = {}\nnans_df = pd.concat([train_df, test_df]).isna()\n\ni_cols = ['V'+str(i) for i in range(1,340)]\nfor col in i_cols:\n    cur_group = nans_df[col].sum()\n    if cur_group>0:\n        try:\n            nans_groups[cur_group].append(col)\n        except:\n            nans_groups[cur_group]=[col]\n            \nfor n_group,n_members in nans_groups.items():\n    print(n_group, len(n_members))","3772d34b":"# I like group 68825\n# 68825 nans -> 23 \"similar\" columns\n# I will choose it for exeriments\ntest_group = nans_groups[68825]\nprint(test_group)","cb60c254":"########################### Let's look on values\nperiods = ['TransactionDT']\ntemp_df = pd.concat([train_df[test_group+periods], test_df[test_group+periods]])\nfor period in periods:\n    for col in test_group:\n        for df in [temp_df]:\n            df.set_index(period)[col].plot(style='.', title=col, figsize=(15, 3))\n            plt.show()","6c56a940":"########################### Let's check p values \nfeatures_check = []\n\nfor col in test_group:\n    features_check.append(ks_2samp(train_df[col], test_df[col])[1])\n    \nfeatures_check = pd.Series(features_check, index=test_group).sort_values() \nprint(features_check)","c615b91e":"########################### Lest try model with these features\nif MAKE_MODEL_TEST:\n    remove_features = [col for col in remove_features if col not in test_group]\n    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n    remove_features += test_group\n####","f0b18c69":"########################### Try to compact values\ntrain_df['group_sum'] = train_df[test_group].to_numpy().sum(axis=1)\ntrain_df['group_mean'] = train_df[test_group].to_numpy().mean(axis=1)\n    \ntest_df['group_sum'] = test_df[test_group].to_numpy().sum(axis=1)\ntest_df['group_mean'] = test_df[test_group].to_numpy().mean(axis=1)\n\ncompact_cols = ['group_sum','group_mean']\n\n# check\nfeatures_check = []\n\nfor col in compact_cols:\n    features_check.append(ks_2samp(train_df[col], test_df[col])[1])\n    \nfeatures_check = pd.Series(features_check, index=compact_cols).sort_values() \nprint(features_check)","b655b2e3":"########################### Model few only new features\nif MAKE_MODEL_TEST:\n    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n    remove_features += compact_cols\n####","fa157f8e":"########################### Let's apply PCA\nfrom sklearn.preprocessing import StandardScaler\nfor col in test_group:\n    sc = StandardScaler()\n    sc.fit(train_df[[col]].fillna(0))\n    train_df[col+'_sc'] = sc.transform(train_df[[col]].fillna(0))\n    test_df[col+'_sc'] = sc.transform(test_df[[col]].fillna(0))\n    \nsc_test_group = [col+'_sc' for col in test_group]  \n\n# check -> same obviously\nfeatures_check = []\n\nfor col in sc_test_group:\n    features_check.append(ks_2samp(train_df[col], test_df[col])[1])\n    \nfeatures_check = pd.Series(features_check, index=sc_test_group).sort_values() \nprint(features_check)\n\nfrom sklearn.decomposition import PCA\npca = PCA(random_state=SEED)\npca.fit(train_df[sc_test_group])\nprint(len(sc_test_group), pca.transform(train_df[sc_test_group]).shape[-1])\ntrain_df[sc_test_group] = pca.transform(train_df[sc_test_group])\ntest_df[sc_test_group] = pca.transform(test_df[sc_test_group])\n\nsc_variance =pca.explained_variance_ratio_\nprint(sc_variance)\n\n# check\nfeatures_check = []\n\nfor col in sc_test_group:\n    features_check.append(ks_2samp(train_df[col], test_df[col])[1])\n    \nfeatures_check = pd.Series(features_check, index=sc_test_group).sort_values() \nprint(features_check)","cb8f8810":"########################### Model few only new features\nif MAKE_MODEL_TEST:\n    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n####","b7c69e08":"########################### Model few only new features\nif MAKE_MODEL_TEST:\n    remove_features += sc_test_group[5:]\n    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n####","7cc36f95":"----","29475ef3":"----\n\n## NOTES!\n\n1. I'm not telling that grouping by the nans is the best choice (p value\/corr\/max-min values\/etc can be applyed to find similar columns)\n\n2. I'm just showing options how to transform and check effect on score\n\n3. You have to have stable CV to do this\n\n4. This is just an example.\n"}}