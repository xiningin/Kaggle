{"cell_type":{"7aef1bed":"code","2b938672":"code","34b0773f":"code","da90236a":"code","56bb1335":"code","3d0809c2":"code","a9e5d078":"code","dd5145b9":"code","17c1d58b":"code","a18dc91d":"code","e448870a":"code","35624e85":"code","c63227d6":"code","18e8d154":"code","dd6a5433":"code","e6e63304":"code","bc6d31e5":"code","0717d479":"code","108887a4":"markdown","033eec16":"markdown","15109f49":"markdown","e2281273":"markdown","554fe5a0":"markdown","bb8fc69b":"markdown","3e6152bb":"markdown","4d1838a7":"markdown","d7592bfe":"markdown","f79bab9b":"markdown","8b1da7fa":"markdown","dbc15675":"markdown","77dd41c9":"markdown","0aec5ff8":"markdown","6f350e05":"markdown","723f8adc":"markdown","a95395a4":"markdown","27742221":"markdown","592b0c93":"markdown","762dafe0":"markdown","d2353c5f":"markdown","3c438bae":"markdown","79bd4882":"markdown","4ac7d16f":"markdown","1b4f4c01":"markdown","d8a74419":"markdown","eb46068c":"markdown","a8505a26":"markdown"},"source":{"7aef1bed":"import numpy as np \nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom wordcloud import WordCloud, STOPWORDS\nimport glob\nimport random","2b938672":"train_data = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\ntrain_data.head()","34b0773f":"test_data = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\ntest_data.head()","da90236a":"sample_sub = pd.read_csv(\"..\/input\/shopee-product-matching\/sample_submission.csv\")\nsample_sub.head()","56bb1335":"train_data.info()","3d0809c2":"print(f\"Training Dataset Shape: {train_data.shape}\")\nprint(f\"Test Dataset Shape: {test_data.shape}\")","a9e5d078":"for col in train_data.columns:\n    print(col + \":\" + (str(len(train_data[col].unique()))))","dd5145b9":"train_jpg_directory = '..\/input\/shopee-product-matching\/train_images'\ntest_jpg_directory = '..\/input\/shopee-product-matching\/test_images'\ndef getImagePaths(path):\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names\ntrain_images_path = getImagePaths(train_jpg_directory)\ntest_images_path = getImagePaths(test_jpg_directory)\nprint(f\"Number of train images: {len(train_images_path)}\")\nprint(f\"Number of test images:  {len(test_images_path)}\")","17c1d58b":"def display_img(images_paths, rows, cols):\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8) )\n    for ind,image_path in enumerate(images_paths):\n        image=cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","a18dc91d":"display_img(train_images_path[:50], 5, 5)","e448870a":"display_img(test_images_path, 1, 3)","35624e85":"top10_names = train_data['label_group'].value_counts().index.tolist()[:15]\ntop10_values = train_data['label_group'].value_counts().tolist()[:15]\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=top10_names, y=top10_values)\nplt.xticks(rotation=45)\nplt.xlabel(\"Label Group\")\nplt.ylabel(\"Image Count\")\nplt.title(\"Top-15 Label Groups by Image Count\")\nplt.show()","c63227d6":"groups = train_data.label_group.value_counts()\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(len(groups)),groups.values)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Index of Unique Item',size=14)\nplt.title('Duplicate Count vs. Unique Item Count',size=16)\nplt.show()\n\nplt.figure(figsize=(20,5))\nplt.bar(groups.index.values[:50].astype('str'),groups.values[:50])\nplt.xticks(rotation = 45)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Label Group',size=14)\nplt.title('Top 50 Duplicated Items',size=16)\nplt.show()","18e8d154":"import cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport nltk\nfrom cuml.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom wordcloud import WordCloud,STOPWORDS\nfrom tensorflow.keras.applications import ResNet101\nprint('TF',tf.__version__)\nprint('RAPIDS',cuml.__version__)","dd6a5433":"# Load Data\ntrain_data = cudf.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntrain_data.head(2)","e6e63304":"model = TfidfVectorizer(stop_words='english', binary=True)\ntext_embeddings = model.fit_transform(train_data.title).toarray()\nprint('text embeddings shape is',text_embeddings.shape)","bc6d31e5":"# Find similar 'Titles' with RAPIDS KNN\nKNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(text_embeddings)\ndistances, indices = model.kneighbors(text_embeddings)","0717d479":"for k in range(5):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-')\n    plt.title('Text Distance From Train Row %i to Other Train Rows'%k,size=16)\n    plt.ylabel('Distance to Train Row %i'%k,size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n    plt.show()\n    \n    print( train_data.loc[cupy.asnumpy(indices[k,:10]),['title','label_group']] )","108887a4":"<h2>Libraries<\/h2>","033eec16":"<h4> Matching Images Usings RAPIDS... In Progress<\/h4>","15109f49":"<h3>Test Data <\/h3>","e2281273":"<h5>Display Images<\/h5>","554fe5a0":"<h2>Evaluation Metric<\/h2>\n\nThis competition will be judged on F1 score evaluation. The major difference between \u2018accuracy\u2019 and \u2018F1\u2019 is that accuracy is dependent on \u2018True Positives\u2019 and \u2018True Negatives\u2019 while F1 is also dependent on \u2018False Positives\u2019 and \u2018False Negatives\u2019.\n","bb8fc69b":"<h5>Test Images<\/h5>","3e6152bb":"<h2>RAPIDS<\/h2","4d1838a7":"<h3> Finding Similar Titles using RAPIDS <\/h3>","d7592bfe":"<h5> Column-wise Unique values <\/h5>","f79bab9b":"<h4>Extract Text Embeddings with RAPIDS TfidfVectorizer<\/h4>\nTfidfVectorizer returns a cupy sparse matrix. Afterward we convert to a cupy dense matrix and feed that into RAPIDS cuML KNN.","8b1da7fa":"<h5>Except posting_id column all columns have duplicate values <\/h5>","dbc15675":"<h5> Duplicate Count per Label<\/h5>","77dd41c9":"To find similar items in train data using only the title's text, first we will extract text embeddings using RAPIDS cuML's TfidfVectorizer. This will turn every title into a one-hot-encoding of the words present. We will then compare one-hot-encodings with RAPIDS cuML KNN to find title's that are similar.","0aec5ff8":"<h5>Data Information<\/h5>","6f350e05":"<h2>Let\u2019s understand the problem first-<\/h2>\n\n\nShopee is an e-commerce platform which provides \u2018Lowest Price Guaranteed\u2019 feature to thousands of products listed. To ensure lowest price Shopee must find the duplicate\/similar items listed in other retailer\u2019s websites. To perform these matches automatically we have to a ML algorithm which can cluster similar items irrespective of different images, titles, description etc. It is given that we can find at most 49 similar products to a given product.\n","723f8adc":"<h2>Load Data<\/h2>","a95395a4":"<h3> Train Data <\/h3","27742221":"<h3>Exploratory Data Analysis<\/h3>","592b0c93":"<h5>Train Images<\/h5>","762dafe0":"<h5>There is no 'null value' in the dataset:)<\/h5>","d2353c5f":"<h4> So we have gathered a good in-dpeth knowledge about Data. Lets try Model now <\/h4>","3c438bae":"<h3> Sample Submission<\/h3>","79bd4882":"<center><h1>Shopee - Price Match Guarantee<\/h1><\/center>","4ac7d16f":"<h5> Image Label Groups by No. of Images <\/h5>","1b4f4c01":"<h5> Train & Test Image Count <\/h5>","d8a74419":"<h5> Dataset Size <\/h5>","eb46068c":"<h4>Refrences<\/h4>\n\nhttps:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-tfidfvectorizer-and-knn\n\n\nhttps:\/\/www.kaggle.com\/ishandutta\/v7-shopee-indepth-eda-one-stop-for-all-your-needs","a8505a26":"<h2>Let\u2019s Understand the DATA now-<\/h2>\n\n\nThe training data provided has following features - posting_id, image name, image, image_phash (perceptual hash of the image), the title of the image and image group. Image group is basically ID code for all postings that map to the same product. \nThe test data has -  3 samples but the model will be evaluated on more samples (about 70K images) privately when submitted. The submission file should consist of 2 rows:\n\n\n\u2022posting_id: The Posting Id of the image (taken from the test file)\n\n\u2022matches: All the different matches to the current image by their posting id. Keep in mind, all images are a self-match \nfor first (i.e: all images also match themselves, so you would have to include that in your entry too). Different posting ids will be separated by space.\n"}}