{"cell_type":{"d679d167":"code","1cd42fe8":"code","c9c1c01e":"code","3f89c923":"code","1ba462e3":"code","404140e3":"code","28233613":"code","6da4fcea":"code","e8fcb58c":"code","b24a82d7":"code","3eb8608b":"code","083d6837":"code","5d8f090b":"code","8f0e0426":"code","7f61e5ea":"code","19004135":"code","f1542eb5":"code","5aebbde3":"code","2f8bb3c2":"code","d2d05eef":"code","640798df":"code","155ed552":"code","e9f067f5":"code","386bba88":"code","ef87ea2f":"code","90c2f7fd":"code","8bd4aba6":"markdown","5b12cdce":"markdown","d6e77dae":"markdown","41adc7ad":"markdown","5b57d9a1":"markdown","0da9f007":"markdown","8cf34d37":"markdown","ee1e1852":"markdown","79941dff":"markdown","c0b72371":"markdown","98dd7386":"markdown","cf2053c7":"markdown"},"source":{"d679d167":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport imgaug.augmenters as iaa\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1cd42fe8":"df = pd.read_csv('..\/input\/train.csv')","c9c1c01e":"df.head()","3f89c923":"df.info()","1ba462e3":"X = df.iloc[:, 1:]\nY = df.iloc[:, 0]","404140e3":"X.head(2)","28233613":"Y.head()","6da4fcea":"X = np.array(X)\nY = np.array(Y)","e8fcb58c":"X = X\/255.0","b24a82d7":"def plot_images(X, Y, shape):\n    for i in range(20):\n        plt.subplot(5, 4, i+1)\n        plt.tight_layout()\n        plt.imshow(X[i].reshape((shape,shape)), cmap='gray')\n        plt.title('Digit:{}'.format(Y[i]))\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()","3eb8608b":"plot_images(X[:20], Y[:20], 28)","083d6837":"class Util:\n    def __init__(self):\n        self.name = \"Util\"\n        \n    def sigmoid(self, z):\n        return 1 \/ (1 + np.exp(-z))\n    \n    def sigmoid_prime(self, z):\n        return -np.exp(-z)\/(1 + np.exp(-z))**2\n    \n    def softmax(self, z):\n        expA = np.exp(z)\n        return expA\/expA.sum(axis=1, keepdims=True)\n        \n    def softmax_prime(self, z):\n        s = self.softmax(z)\n        return s*(1-s)\n    \n    def tanh(self, z):\n        return np.tanh(z)\n    \n    def tanh_prime(self, z):\n        return 1 - np.tanh(z)**2\n    \n    def relu(self, z):\n        return np.where(z>0, z, 0)\n    \n    def relu_prime(self, z):\n        return np.where(z>0, 1, 0)\n    \n    def sigmoid_cost_function(self, P, T):\n        m = P.shape[0]\n        return -np.sum(T*np.log(P) + (1-T)*np.log(1-P))\/m\n    \n    def softmax_cost_function(self, P, T, parameters, lambd):\n        m = P.shape[0]\n        sum_weights = 0\n        L = len(parameters)\/\/2\n        for l in range(L):\n            sum_weights += np.sum(parameters[\"W\" + str(l + 1)])\n        J = (lambd \/ (2*m)) * (sum_weights**2)\n        J += -np.sum(T*np.log(P))\/m\n        return J\n    \n    def accuracy(self, P, Y):\n        Yhat = np.argmax(P, axis=1)\n        return np.mean(Yhat==Y)\n    \n    def yEncode(self, Y):\n        T = np.zeros((len(Y), len(set(Y))))\n        T[np.arange(Y.size), Y] = 1\n        return T","5d8f090b":"class ImageAugmenter:\n    def __init__(self):\n        self.name = 'ImageAugmenter'\n        \n    def reshape_images(self, img_arr, shape):\n        return img_arr.reshape(shape)\n    \n    def transform_images(self, seq, img_arr, shape):\n        X_img = self.reshape_images(img_arr, (img_arr.shape[0], shape, shape))\n        X_aug = seq.augment_images(X_img)\n        X_aug = self.reshape_images(X_aug, (img_arr.shape[0], shape*shape))\n        return X_aug\n        \n    def fliplr(self, X, shape):\n        seq = iaa.Sequential([\n            iaa.Fliplr(1)\n        ])\n        return self.transform_images(seq, X, shape)\n    \n    def flipud(self, X, shape):\n        seq = iaa.Sequential([\n            iaa.Flipud(1)\n        ])\n        return self.transform_images(seq, X, shape)\n    \n    def scale(self, X, shape):\n        seq = iaa.Sequential([\n            iaa.Affine(\n                scale={\"x\":(0.5, 1.5), \"y\":(0.5, 1.5)}\n            )\n        ])\n        return self.transform_images(seq, X, shape)\n    \n    def translate(self, X, shape):\n        seq = iaa.Sequential([\n            iaa.Affine(\n                translate_percent={\"x\":(-0.2, 0.2), \"y\":(-0.2, 0.2)}\n            )\n        ])\n        return self.transform_images(seq, X, shape)\n    \n    def rotate(self, X, shape):\n        seq = iaa.Sequential([\n            iaa.Affine(\n                rotate=(-45, 45)\n            )\n        ])\n        return self.transform_images(seq, X, shape)\n    \n    def shear(self, X, shape):\n        seq = iaa.Sequential([\n            iaa.Affine(\n                shear=(-10, 10)\n            )\n        ])\n        return self.transform_images(seq, X, shape)\n    \n    def compose(self, X, shape):\n        seq = iaa.Sequential([\n            iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n            rotate=(-25, 25),\n            shear=(-8, 8)\n            ),\n            iaa.Pepper(1e-5)\n        ])\n        return self.transform_images(seq, X, shape)\n    \n    def augment(self, count, X, Y):\n        X_out = np.copy(X)\n        for i in range(count):\n            X_scale = self.scale(X, 28)\n            X_rotate = self.rotate(X, 28)\n            X_trans = self.translate(X, 28)\n            X_shear = self.shear(X, 28)\n            X_compose = self.compose(X, 28)\n            X_out = np.concatenate((X_out, X_scale), axis = 0)\n            X_out = np.concatenate((X_out, X_rotate), axis = 0)\n            X_out = np.concatenate((X_out, X_trans), axis = 0)\n            X_out = np.concatenate((X_out, X_shear), axis = 0)\n            X_out = np.concatenate((X_out, X_compose), axis = 0)\n        Y = np.repeat(Y, (count*5)+1, axis = 0)\n        return X_out, Y","8f0e0426":"class DNN_Core:\n    def __init__(self, layer_dims):\n        self.layer_dims = layer_dims\n        \n    def initialise_parameters(self):\n        parameters = {}\n        for l in range(1, len(self.layer_dims)):\n            parameters[\"W\" + str(l)] = np.random.randn(self.layer_dims[l-1], self.layer_dims[l])*0.01\n            parameters[\"b\" + str(l)] = np.random.randn(self.layer_dims[l], 1)*0.01\n        return parameters\n        \n    def linear_forward(self, A, W, b):\n        z = np.dot(A, W) + b.T\n        return z, (A, W, b)\n    \n    def activation_forward(self, A, W, b, activation):\n        util = Util()\n        z, linear_cache = self.linear_forward(A, W, b)\n        if activation == 'sigmoid':\n            A = util.sigmoid(z)\n        elif activation == 'softmax':\n            A = util.softmax(z)\n        elif activation == 'tanh':\n            A = util.tanh(z)\n        elif activation == 'relu':\n            A = util.relu(z)\n        activation_cache = z\n        return A, (linear_cache, activation_cache)\n    \n    def feed_forward(self, X, parameters):\n        caches = []\n        A = X\n        L = len(parameters)\/\/2\n        for l in range(1, L):\n            A_prev = A\n            A, cache = self.activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], 'relu')\n            caches.append(cache)\n        P, cache = self.activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], 'softmax')\n        caches.append(cache)\n        return P, caches\n    \n    def linear_backward(self, dZ, linear_cache):\n        A_prev, W, b = linear_cache\n        m = A_prev.shape[0]\n        dW = np.dot(A_prev.T, dZ)\/m\n        db = np.sum(dZ, axis=0, keepdims=True)\/m\n        dA_prev = np.dot(dZ, W.T)\/m\n        return dA_prev, dW, db\n    \n    def activation_backward(self, dA, cache, activation):\n        util = Util()\n        linear_cache, activation_cache = cache\n        if activation == 'sigmoid':\n            dZ = dA * util.sigmoid_prime(activation_cache)\n        elif activation == 'softmax':\n            dZ = dA\n        elif activation == 'tanh':\n            dZ = dA * util.tanh_prime(activation_cache)\n        elif activation == 'relu':\n            dZ = dA * util.relu_prime(activation_cache)\n        dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n        return dA_prev, dW, db.T\n    \n    def back_propogation(self, P, T, caches):\n        gradients = {}\n        dA = P-T\n        L = len(caches)\n        dA_prev, dW, db = self.activation_backward(dA, caches[L-1], 'softmax')\n        gradients['dW'+str(L)] = dW\n        gradients['db'+str(L)] = db\n        gradients['dA'+str(L-1)] = dA_prev\n        for l in reversed(range(L-1)):\n            dA_prev, dW, db = self.activation_backward(dA_prev, caches[l], 'relu')\n            gradients['dW'+str(l+1)] = dW\n            gradients['db'+str(l+1)] = db\n            gradients['dA'+str(l)] = dA_prev\n        return gradients\n    \n    def update_patameters(self, parameters, gradients, alpha, lambd, m):\n        L = len(parameters)\/\/2\n        for l in range(L):\n            parameters['W'+str(l+1)] -= alpha*gradients['dW'+str(l+1)] \n            parameters['W'+str(l+1)] -= lambd*parameters['W'+str(l+1)]\n            parameters['b'+str(l+1)] -= alpha*gradients['db'+str(l+1)]\n        return parameters","7f61e5ea":"class SGD:\n    def __init__(self, layer_dims):\n        self.layer_dims = layer_dims\n        \n    def SGD(self, X, X_val, Y, Y_val, epochs, alpha, lambd, batch_size):\n        J_history = []\n        J_val_history = []\n        acc_history = []\n        acc_val_history = []\n        core = DNN_Core(self.layer_dims)\n        util = Util()\n        augmenter = ImageAugmenter()\n        T = util.yEncode(Y)\n        T_val = util.yEncode(Y_val)\n        parameters = core.initialise_parameters()\n        for i in range(epochs):\n            start = 0\n            s = np.arange(X.shape[0])\n            np.random.shuffle(s)\n            X = X[s]\n            T = T[s]\n            Y = Y[s]\n            while start < X.shape[0]:\n                end = start + batch_size\n                if end > X.shape[0]:\n                    end = X.shape[0]\n                x = X[start:end, :]\n                t = T[start:end, :]\n                #x, t = augmenter.augment(x, t)\n                P, caches = core.feed_forward(x, parameters)\n                gradients = core.back_propogation(P, t, caches)\n                parameters = core.update_patameters(parameters, gradients, alpha, lambd, t.shape[0])\n                start = end\n            # Validate at the end of 10 epochs\n            if i%20 == 0:\n                P, _ = core.feed_forward(X, parameters)\n                P_val, _ = core.feed_forward(X_val, parameters)\n                J = util.softmax_cost_function(P, T, parameters, lambd)\n                J_val = util.softmax_cost_function(P_val, T_val, parameters, lambd)\n                J_history.append(J)\n                J_val_history.append(J_val)\n                acc = util.accuracy(P, Y)\n                acc_val = util.accuracy(P_val, Y_val)\n                acc_history.append(acc)\n                acc_val_history.append(acc_val)\n                print('Epoch: ' + str(i), 'Cost: {0:.2f} Valid_Cost: {1:.2f} Accuracy: {2:.7f} Valid_Accuracy: {3:.7f}'.format(J, J_val, acc, acc_val))\n        return P, parameters, J_history, J_val_history, acc_history, acc_val_history","19004135":"from sklearn.model_selection import  train_test_split\nlayer_dims = [X.shape[1], 30, 20, 10]\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.05, random_state=28) \nsgd = SGD(layer_dims)\nP, parameters, J_history, J_val_history, acc_history, acc_val_history = sgd.SGD(X_train, X_val, Y_train, Y_val, 500, 1e1, 1e-4, 100)","f1542eb5":"plt.plot(J_history, color='blue')\nplt.plot(J_val_history, color='red')\nplt.show()","5aebbde3":"plt.plot(acc_history, color='blue')\nplt.plot(acc_val_history, color='red')\nplt.show()","2f8bb3c2":"core = DNN_Core(layer_dims)\nutil = Util()\nP_test, _ = core.feed_forward(X_val, parameters)\nprint(util.accuracy(P_test, Y_val))","d2d05eef":"df_test = pd.read_csv('..\/input\/test.csv')","640798df":"df_test.head()","155ed552":"df_test.shape","e9f067f5":"X_out = np.array(df_test)\nX_out = X_out\/255.0\nP_out, _ = core.feed_forward(X_out, parameters)","386bba88":"Y_out = np.argmax(P_out, axis=1)\nY_out[1:5]","ef87ea2f":"df_submission = pd.read_csv('..\/input\/sample_submission.csv')\ndf_submission['Label'] = Y_out\ndf_submission.head()","90c2f7fd":"df_submission.to_csv('output.csv', index=False)","8bd4aba6":"# Create output","5b12cdce":"# Plot images","d6e77dae":"# Fit the model","41adc7ad":"# Validation dataset prediction","5b57d9a1":"# Core:\n\n1. Initialise Parameters\n2. Linear Forward\n3. Activation Forward\n4. Feed Forward\n5. Linear Backward\n6. Activation Backward\n7. Back Propagation","0da9f007":"# Augment Images using ImgAug \n\nPerforms the following image Augmentation with ImgAug:\n* Flip\n* Scale\n* Translate\n* Rotate\n* Shear\n* Composite","8cf34d37":"# Stochastic Gradient Descent \n** Batch size of 100 **\n\n** Shuffled dataset for each epoch **","ee1e1852":"# Plot the cost and accuracy","79941dff":"Normalize X. Since we are delaing with pixels and they are going to be between 0 to 255, lets normalize the input features by dividing by 255","c0b72371":"# Read input","98dd7386":"# Util class \nWill contain the activation functions along with their derivatives, cost functions and accuracy","cf2053c7":"# Seperate input and output features"}}