{"cell_type":{"d21a2c2f":"code","a7bd8122":"code","dab12fe8":"code","22d50f77":"code","b019c917":"code","a0fcaf05":"code","1b20eec4":"code","96671a23":"code","6b95189a":"code","9bdcdbf0":"code","b52282e9":"code","7697b466":"code","3ee01f87":"code","03c79c21":"code","5d016a82":"code","e8984972":"code","64884bef":"code","d0840b4c":"code","12d1b872":"code","a7b9dda4":"code","52faeed5":"code","835d807c":"code","dfb306a6":"code","4131c8ba":"code","0769adab":"code","2b86e2b3":"code","0f96f361":"code","4b195593":"code","d811ebc6":"code","8fe0ad15":"code","e34808b3":"code","6b2ea6c5":"code","9a85bef8":"code","b218e866":"code","9fef0754":"code","5e16b476":"code","23c34686":"code","54c2ee8f":"code","34ffdc2c":"code","4e712da2":"code","7340f640":"code","a482d3a2":"code","53ffae5c":"code","71a0efed":"code","7aa375bd":"code","8f9a6d1e":"code","f8b2572f":"code","c2b64284":"code","0247cc97":"code","2f40637b":"code","4bf76e17":"code","49e46db4":"code","4fd68df4":"code","7b72562a":"code","2f1fb947":"code","a2b58259":"code","85a723f2":"code","bd547ab3":"code","c64430c8":"code","88588e72":"code","6856d3ad":"code","33b18e1a":"code","54e4b8d3":"code","3f8afa4a":"code","dd7341fe":"code","29a6fd1a":"code","dcc8d320":"code","3d23b7ef":"code","8f0f22b0":"code","783024fd":"code","8410b40d":"code","c3410193":"code","c568d508":"code","03bcb71d":"code","9d3abbf9":"code","354f76d9":"code","d3c11d51":"code","454d9519":"code","18b26fec":"code","abaa47c6":"code","91ab40a9":"code","d287cf49":"code","2ed51705":"code","ff8cc071":"code","3dacb206":"code","bf4dbea4":"code","92aa59d2":"code","bb038684":"code","83e39157":"code","43119284":"code","9e7bbdab":"code","0338e7cf":"code","ee415fd4":"code","dec07130":"code","e407458b":"code","83753988":"code","8ac8b771":"code","84b3fa80":"code","7e1d801c":"code","aa6d828c":"code","858e38e6":"code","ed46afac":"code","4f7485ab":"code","8cdb4400":"code","f7344fae":"code","06fc3f60":"code","edd3f2cb":"code","3bd65171":"markdown","1b2b38f6":"markdown","839d8114":"markdown","33b8f374":"markdown","1d5b2309":"markdown","dabb5de4":"markdown","0afd1efa":"markdown","25f8300c":"markdown","934c63bf":"markdown","9ea112a9":"markdown","ffc0894a":"markdown","c03db4be":"markdown","8acf3a3d":"markdown","8502eaee":"markdown","26939bf4":"markdown","735ee255":"markdown","a166a372":"markdown","2551fbb8":"markdown","8fd935d1":"markdown","af4041c6":"markdown","f88b6290":"markdown","63c16b76":"markdown","ba48d5b2":"markdown","3fd7a0ae":"markdown","56f83ee2":"markdown","9af2343c":"markdown","4fbaa350":"markdown","23334c95":"markdown","4955dd77":"markdown","7f9f3922":"markdown","a0fa0321":"markdown","54c9de5c":"markdown","7e907f62":"markdown","0cf435b9":"markdown","33a72bdc":"markdown","6a04581e":"markdown","e0c79a40":"markdown","60de5b32":"markdown","7fa28e80":"markdown","38ef9b81":"markdown","b0a3c378":"markdown"},"source":{"d21a2c2f":"import gc\nimport os\nimport json\nimport re\nimport glob\nfrom joblib import Parallel, delayed\n\nimport scipy as sp\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 500)\n\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom collections import Counter\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nnp.random.seed(1029)\n\nfrom tqdm import tqdm, tqdm_notebook\n\nimport cv2\nfrom keras.applications.densenet import preprocess_input, DenseNet121\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\n\nfrom scipy import stats\n\nfrom PIL import Image","a7bd8122":"N_FOLDS = 4\nFOLDS = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)","dab12fe8":"def get_chi2(obs, exp):\n    diff = set(exp) - set(obs)\n    f_obs = obs.value_counts()\n    f_exp = exp.value_counts()\n    if diff:\n        for i in diff:\n            f_obs[i] = 0\n    f_obs = f_obs.sort_index()\n    f_exp = f_exp.sort_index()\n    chi2, _ = stats.chisquare(f_obs.values,f_exp.values)\n    return chi2\n\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    rater_a = y_true\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)\n\ndef get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n    ysort = np.sort(y)\n    predsort = np.sort(y_pred)\n    bounds = []\n    for ibound in range(N-1):\n        iy = len(ysort[ysort <= ibound])\n        if (ibound == 0) and (class0_fraction >= 0.0) :\n            iy = int(class0_fraction * iy)\n        bounds.append(predsort[iy])\n    return bounds\n\ndef assign_class(y_pred, boundaries):\n    y_classes = np.zeros(len(y_pred))\n    for iclass, bound in enumerate(boundaries):\n        y_classes[y_pred >= bound] = iclass + 1\n    return y_classes.astype(int)\n\ndef get_init_coefs(y_test_pred, y_test):\n    kappas = []\n    coefs = []\n    cl0fracs = np.array(np.arange(0.01,30,0.01))\n    for cl0frac in cl0fracs:\n        coef = get_class_bounds(y_test, y_test_pred, class0_fraction=cl0frac)\n        coefs.append(coef)\n        y_test_k = assign_class(y_test_pred, coef)\n        kappa = cohen_kappa_score(y_test, y_test_k, weights='quadratic')\n        kappas.append(kappa)\n    ifmax = np.array(kappas).argmax()\n    best_frac = cl0fracs[ifmax]\n    best_coef = coefs[ifmax] \n    return best_coef\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","22d50f77":"class OptimizedRounder(object):\n    def __init__(self,initial_coefs = None):\n        if(initial_coefs == None):\n            self.initial_coefs = [1.775, 2.1057, 2.4438, 2.7892]\n        else:\n            self.initial_coefs = initial_coefs.copy()\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coefs, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n\nclass OptimizedRounder_v2(object):\n    def __init__(self, initial_coefs = None):\n        if(initial_coefs == None):\n            self.initial_coefs = [1.775, 2.1057, 2.4438, 2.7892]\n        else:\n            self.initial_coefs = initial_coefs.copy()\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        ll = cohen_kappa_score(y, X_p, weights = 'quadratic')    \n        chi2 =  get_chi2(X_p, y)\n        ll = ll - chi2 * (1.0 \/ 25000)\n        return -ll\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coefs, method = 'nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']\n\n\nclass OptimizedRounder_v3(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef, len_0=410):\n        X_p = np.copy(X)\n        temp = sorted(list(X_p))\n        threshold = temp[int(0.9*len_0)-1]\n        for i, pred in enumerate(X_p):\n            if pred < threshold:\n                X_p[i] = 0\n            elif pred >= threshold and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","b019c917":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","a0fcaf05":"train = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/test.csv\")\n\nbreeds = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/breed_labels.csv\")\ncolors = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/color_labels.csv\")\nstates = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/state_labels.csv\")","1b20eec4":"origin_train = train[list(train.columns)]\norigin_test = test[list(test.columns)]","96671a23":"breedid_map = dict(zip(breeds['BreedID'], breeds['BreedName'].map(lambda x:x.lower())))\ncolor_map = dict(zip(colors['ColorID'], colors['ColorName'].map(lambda x:x)))\nstate_map = dict(zip(states['StateID'], states['StateName'].map(lambda x:x)))","6b95189a":"train_id = train['PetID']\ntest_id = test['PetID']","9bdcdbf0":"def sentiment_feature(data, ids, path):\n    doc_sent_mag = []\n    doc_sent_score = []\n    doc_sent_len = []\n    doc_sent_mags = []\n    doc_sent_scores = []\n\n    doc_entity_len = []\n    doc_entity_sali = []\n\n    nf_count = 0\n\n    for pet in ids:\n        try:\n            with open('..\/input\/petfinder-adoption-prediction\/%s\/' % path + pet + '.json', 'r') as f:\n                sentiment = json.load(f)\n            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n            doc_sent_score.append(sentiment['documentSentiment']['score'])\n            \n            doc_sent_len.append(len(sentiment['sentences']))\n            if len(sentiment['sentences']) == 0:\n                doc_sent_mags.append([-999])\n                doc_sent_scores.append([-999])\n            else:\n                doc_sent_mags.append([sent['sentiment']['magnitude'] for sent in sentiment['sentences']])\n                doc_sent_scores.append([sent['sentiment']['score'] for sent in sentiment['sentences']])\n            \n            doc_entity_len.append(len(sentiment['entities']))\n            if len(sentiment['entities']) == 0:\n                doc_entity_sali.append([-999])\n            else:\n                doc_entity_sali.append([entity['salience'] for entity in sentiment['entities']])\n        except FileNotFoundError:\n            nf_count += 1\n            doc_sent_mag.append(-1)\n            doc_sent_score.append(-1)\n            doc_sent_len.append(-1)\n            doc_sent_mags.append([-1000])\n            doc_sent_scores.append([-1000])\n            doc_entity_len.append(-1)\n            doc_entity_sali.append([-1000])\n\n    data.loc[:, 'doc_sent_mag'] = doc_sent_mag\n    data.loc[:, 'doc_sent_score'] = doc_sent_score\n\n    return data\n\ntrain = sentiment_feature(train, train_id, 'train_sentiment')\ntest = sentiment_feature(test, test_id, 'test_sentiment')","b52282e9":"def gen_meta_f(df, ids, meta_path):\n    vertex_xs = []\n    vertex_ys = []\n    bounding_confidences = []\n    bounding_importance_fracs = []\n    dominant_blues = []\n    dominant_greens = []\n    dominant_reds = []\n    dominant_pixel_fracs = []\n    dominant_scores = []\n    \n    dominant_blues1 = []\n    dominant_greens1 = []\n    dominant_reds1 = []\n    dominant_pixel_fracs1 = []\n    dominant_scores1 = []\n\n    label_descriptions = []\n    label_descriptions1 = []\n    label_descriptions2 = []\n    label_descriptions3 = []\n    \n    label_scores = []\n    label_scores1 = []\n    label_scores2 = []\n    label_scores3 = []\n    \n    nf_count = 0\n    nl_count = 0\n    label_data = {}\n    for idx, pet in enumerate(ids):\n        try:\n            with open('..\/input\/petfinder-adoption-prediction\/%s\/' % meta_path + pet + '-1.json', 'r') as f:\n                data = json.load(f)\n            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n            vertex_xs.append(vertex_x)\n            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n            vertex_ys.append(vertex_y)\n            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n            bounding_confidences.append(bounding_confidence)\n            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n            bounding_importance_fracs.append(bounding_importance_frac)\n            # 0\n            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n            dominant_blues.append(dominant_blue)\n            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n            dominant_greens.append(dominant_green)\n            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n            dominant_reds.append(dominant_red)\n            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n            dominant_pixel_fracs.append(dominant_pixel_frac)\n            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n            dominant_scores.append(dominant_score)\n            # 1\n            if len(data['imagePropertiesAnnotation']['dominantColors']['colors']) > 1 and len(data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']) == 3:\n                dominant_blue1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['blue']\n                dominant_blues1.append(dominant_blue1)\n                dominant_green1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['green']\n                dominant_greens1.append(dominant_green1)\n                dominant_red1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['red']\n                dominant_reds1.append(dominant_red1)\n                dominant_pixel_frac1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['pixelFraction']\n                dominant_pixel_fracs1.append(dominant_pixel_frac1)\n                dominant_score1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['score']\n                dominant_scores1.append(dominant_score1)\n        \n            else:\n                dominant_blues1.append(-1)\n                dominant_greens1.append(-1)\n                dominant_reds1.append(-1)\n                dominant_pixel_fracs1.append(-1)\n                dominant_scores1.append(-1)\n                \n            if data.get('labelAnnotations'):\n                label_description = data['labelAnnotations'][0]['description']\n                label_descriptions.append(label_description)\n                label_score = data['labelAnnotations'][0]['score']\n                label_scores.append(label_score)\n\n                if len(data['labelAnnotations']) > 1:\n                    label_description1 = data['labelAnnotations'][1]['description']\n                    label_descriptions1.append(label_description1)\n                    label_score1 = data['labelAnnotations'][1]['score']\n                    label_scores1.append(label_score1)\n                else:\n                    label_descriptions1.append('nothing')\n                    label_scores1.append(-1)\n                \n                if len(data['labelAnnotations']) > 2:\n                    label_description2 = data['labelAnnotations'][2]['description']\n                    label_descriptions2.append(label_description2)\n                    label_score2 = data['labelAnnotations'][2]['score']\n                    label_scores2.append(label_score2)\n                else:\n                    label_descriptions2.append('nothing')\n                    label_scores2.append(-1)\n\n                if len(data['labelAnnotations']) > 3:\n                    label_description3 = data['labelAnnotations'][3]['description']\n                    label_descriptions3.append(label_description3)\n                    label_score3 = data['labelAnnotations'][3]['score']\n                    label_scores3.append(label_score3)\n                else:\n                    label_descriptions3.append('nothing')\n                    label_scores3.append(-1)\n\n            else:\n                nl_count += 1\n                label_descriptions.append('nothing')\n                label_descriptions1.append(label_description1)\n                label_descriptions2.append(label_description2)\n                label_descriptions3.append(label_description3)\n                \n                label_scores.append(-1)\n                label_scores1.append(-1)\n                label_scores2.append(-1)\n                label_scores3.append(-1)\n                                                            \n        except FileNotFoundError:\n            nf_count += 1\n            vertex_xs.append(-1)\n            vertex_ys.append(-1)\n            bounding_confidences.append(-1)\n            bounding_importance_fracs.append(-1)\n            dominant_blues.append(-1)\n            dominant_greens.append(-1)\n            dominant_reds.append(-1)\n            dominant_pixel_fracs.append(-1)\n            dominant_scores.append(-1)\n            \n            dominant_blues1.append(-1)\n            dominant_greens1.append(-1)\n            dominant_reds1.append(-1)\n            dominant_pixel_fracs1.append(-1)\n            dominant_scores1.append(-1)\n\n            label_descriptions.append('nothing')\n            label_descriptions1.append('nothing')\n            label_descriptions2.append('nothing')\n            label_descriptions3.append('nothing')\n            label_scores.append(-1)\n            label_scores1.append(-1)\n            label_scores2.append(-1)\n            label_scores3.append(-1)\n\n    prefix = 'meta_'\n    df.loc[:, prefix+'vertex_x'] = vertex_xs\n    df.loc[:, prefix+'vertex_y'] = vertex_ys\n    df.loc[:, prefix+'bounding_confidence'] = bounding_confidences\n    df.loc[:, prefix+'bounding_importance'] = bounding_importance_fracs\n    df.loc[:, prefix+'dominant_blue'] = dominant_blues\n    df.loc[:, prefix+'dominant_green'] = dominant_greens\n    df.loc[:, prefix+'dominant_red'] = dominant_reds\n    df.loc[:, prefix+'dominant_pixel_frac'] = dominant_pixel_fracs\n    df.loc[:, prefix+'dominant_score'] = dominant_scores\n    \n    df.loc[:, prefix+'label_description'] = label_descriptions\n    df.loc[:, prefix+'label_description1'] = label_descriptions1\n    df.loc[:, prefix+'label_description2'] = label_descriptions2\n\n    df.loc[:, prefix+'label_score'] = label_scores\n    df.loc[:, prefix+'label_score1'] = label_scores1\n    df.loc[:, prefix+'label_score2'] = label_scores1\n    cate_cols = [prefix+col for col in ['label_description','label_description1','label_description2']]\n    df.loc[:, cate_cols] = df[cate_cols].astype('category')\n\ngen_meta_f(train, train_id, 'train_metadata')\ngen_meta_f(test, test_id, 'test_metadata')","7697b466":"def rescue_feature(df):\n    rescue_count = df.groupby('RescuerID')['Quantity'].count()\n    rescue_count.name = 'rescue_count'\n    rescue_num = df.groupby('RescuerID')['Quantity'].sum()\n    rescue_num.name = 'rescue_num'\n    rescue_unique_type = df.drop_duplicates(['RescuerID', 'Type']).groupby('RescuerID')['RescuerID'].count()\n    rescue_unique_type.name = 'rescue_unique_type'\n    df = df.join(rescue_count, on='RescuerID')\n    df = df.join(rescue_num, on='RescuerID') \n    df['rescue_rank'] = df.RescuerID.map(df.RescuerID.value_counts().rank()\/df.RescuerID.unique().shape[0])\n    return df\n\ndef pure_breed_encode(data):\n    data['pure_breed1'] = np.where((data['Breed1'] != 307) , '0', '1')\n    data['pure_breed2'] = np.where((data['Breed2'] == 0) , '0', np.where(data['Breed2'] != 307, '1', '2'))\n    data['pure_breed3'] = (data['pure_breed1'] + data['pure_breed2'])\n    data['pure_animal_pure_breed4'] = np.where((data['Type'].astype(np.str)=='1') & (data['pure_breed3']=='00'), '100', np.where((data['Type'].astype(np.str)=='2') & (data['pure_breed3']=='00'), '200', '333'))\n    for col in ['pure_breed1', 'pure_breed2', 'pure_breed3', 'pure_animal_pure_breed4']:\n        data[col] = data[col].astype('category')\n    del data['pure_animal_pure_breed4']\n    return data\n\ndef call_name_f(data):\n    is_call_name = []\n    for name, desc in zip(data['Name'], data['Description']):\n        clean_desc = str(desc).lower()\n        clean_name = str(name).lower()\n        if clean_name == 'nan':\n            is_call_name.append(0)\n        else:\n            num = len(clean_desc.split(clean_name))\n            is_call_name.append(num)\n    data['call_name_num'] = is_call_name\n    return data\n\ntrain = rescue_feature(train)\ntest = rescue_feature(test)\n\ntrain = pure_breed_encode(train)\ntest = pure_breed_encode(test)","3ee01f87":"def language_type(desc):\n    desc = str(desc)\n    if desc=='nan':\n        return 0\n    zhmodel = re.compile(u'[\\u4e00-\\u9fa5]')\n    enmodel = re.compile(u'[a-zA-Z]')\n    zhmatch = zhmodel.search(desc)\n    enmatch = enmodel.search(desc)\n    if zhmatch and enmatch:\n        return 3\n    elif zhmatch:\n        return 3\n    elif enmatch:\n        return 2\n    else:\n        return 1\n\ndef malaiyu_type(desc):\n    desc = str(desc)\n    malai = [' la x ' , ' nk ',' nie ', ' umur ', ' di ', 'teruk', ' satu ',' dh ', ' ni ',' tp ', ' yg ', 'mmg', 'msj', ' utk ' ,'neh' ]\n    for ma_tag in malai:\n        if desc.find(ma_tag) > -1:\n            return ma_tag,1\n    \n    return \"\", 0\n\nlang_prefix = 'lang_'\ntrain[lang_prefix+'language_type'] = train.Description.map(lambda x:language_type(x))\ntrain[lang_prefix+'malaiyu_type'] = train.Description.map(lambda x:malaiyu_type(x)[1])\n\ntest[lang_prefix+'language_type'] = test.Description.map(lambda x:language_type(x))\ntest[lang_prefix+'malaiyu_type'] = test.Description.map(lambda x:malaiyu_type(x)[1])","03c79c21":"def obtain_text(df):\n    breed1_text = df['Breed1'].map(lambda x:breedid_map.get(x, 'unknown_breed'))\n    breed2_text = df['Breed2'].map(lambda x:breedid_map.get(x, 'unknown_breed'))\n    color1_text = df['Color1'].map(lambda x:color_map.get(x, 'unknown_color'))\n    color2_text = df['Color2'].map(lambda x:color_map.get(x, 'unknown_color'))\n    color3_text = df['Color3'].map(lambda x:color_map.get(x, 'unknown_color'))\n\n    text = df['Name'].fillna(\"none\") + \" \" \\\n           + breed1_text  + \" \" \\\n           + breed2_text + \" \" \\\n           + color1_text + \" \" \\\n           + color2_text + \" \" \\\n           + color3_text + \" \" \\\n           + df['Description'].fillna(\"none\")\n    \n    return text\n\ntrain_desc = train.Description.fillna(\"none\").values\ntest_desc = test.Description.fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=10000,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\ntfv.fit(list(train_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\ncomponents = 120\nsvd = TruncatedSVD(n_components=components)\nsvd.fit(X)\n\nX = svd.transform(X)\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(components)])\ntrain = pd.concat((train, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(components)])\ntest = pd.concat((test, X_test), axis=1)","5d016a82":"def nmf_lda_feature(train, test, train_text, test_text):\n    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n\n    tfv.fit(list(train_text)+list(test_text))\n    X =  tfv.transform(train_text)\n    X_test = tfv.transform(test_text)\n\n    components = 20\n    nmf = NMF(n_components=components, random_state=100).fit(X)\n    nmf_x = nmf.transform(X)\n    nmf_x = pd.DataFrame(nmf_x, columns=['nmf_{}'.format(i) for i in range(components)])\n    train = pd.concat((train, nmf_x), axis=1)\n    nmf_x_test = nmf.transform(X_test)\n    nmf_x_test = pd.DataFrame(nmf_x_test, columns=['nmf_{}'.format(i) for i in range(components)])\n    test = pd.concat((test, nmf_x_test), axis=1)\n\n    components = 12\n    lda = LatentDirichletAllocation(n_components=components, max_iter=120, n_jobs=-1)\n    lda.fit(X)\n    lda_x = lda.transform(X)\n    lda_x = pd.DataFrame(lda_x, columns=['lda_{}'.format(i) for i in range(components)])\n    train = pd.concat((train, lda_x), axis=1)\n    lda_x_test = lda.transform(X_test)\n    lda_x_test = pd.DataFrame(lda_x_test, columns=['lda_{}'.format(i) for i in range(components)])\n    test = pd.concat((test, lda_x_test), axis=1)\n    \n    return train, test\n\ntrain_text = obtain_text(train)\ntest_text = obtain_text(test)\n\ntrain, test = nmf_lda_feature(train, test, train_text, test_text)","e8984972":"train_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")\ntest_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\ntest_pet_ids = test_df['PetID'].values\ntrain_pet_ids = train_df['PetID'].values\ntarget = train_df['AdoptionSpeed'].values","64884bef":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D,Dense,Dropout\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.applications.densenet import preprocess_input, DenseNet121\nfrom keras.applications.resnet50 import preprocess_input as res_preprocess, ResNet50","d0840b4c":"batch_size = 128\ndef BASE_MODEL():\n    inp = Input((128,128,3))\n    backbone = ResNet50(input_tensor = inp, \n                           weights=\"..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n                           include_top = False)\n    x = backbone.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512)(x)\n    x = Dropout(0.5)(x)\n    output = Dense(1,activation='linear')(x)\n    return Model(inp,output)\n\ndef new_load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    try:\n        new_image = cv2.resize(image,(128,128))\n    except:\n        new_image = np.zeros((128,128,3))\n    new_image = res_preprocess(new_image)\n    return new_image\n\n\ndef train_gen(batch_size=128, shuffle=True, pet_list=None, pet_labels=None, use_labels=True):\n    images_df = pd.DataFrame({'img_id':pet_list,'label':pet_labels})\n    while True:\n        if shuffle:\n            images_df = images_df.sample(frac=1.0).reset_index(drop=True)\n        for start in range(0, len(images_df), batch_size):\n            x_batch = []\n            y_batch = []\n            end = min(start + batch_size,len(images_df))\n            for _id in range(start,end):\n                image_row = images_df.iloc[_id]\n                image_id = image_row['img_id']\n                img = new_load_image(\"..\/input\/petfinder-adoption-prediction\/train_images\/\", image_id)\n                if use_labels:\n                    img_label = image_row['label']\n                    y_batch.append(img_label)\n                else:\n                    y_batch.append(-1.0)\n                x_batch.append(img)\n            yield np.array(x_batch),np.array(y_batch)\n            \ndef test_gen(batch_size=128,shuffle=True,pet_list=None,pet_labels=None,use_labels=True):\n    images_df = pd.DataFrame({'img_id':pet_list,'label':pet_labels})\n    while True:\n        if shuffle:\n            images_df = images_df.sample(frac=1.0).reset_index(drop=True)\n        for start in range(0, len(images_df), batch_size):\n            x_batch = []\n            y_batch = []\n            end = min(start + batch_size,len(images_df))\n            for _id in range(start,end):\n                image_row = images_df.iloc[_id]\n                image_id = image_row['img_id']\n                img = new_load_image(\"..\/input\/petfinder-adoption-prediction\/test_images\/\", image_id)\n                if use_labels:\n                    img_label = image_row['label']\n                    y_batch.append(img_label)\n                else:\n                    y_batch.append(-1.0)\n                x_batch.append(img)\n            yield np.array(x_batch),np.array(y_batch)","12d1b872":"test_img_prob = np.zeros(shape=(test_df.shape[0],1))\ntrain_img_prob = np.zeros(shape=(train_df.shape[0],1))\n\nfor tr_idx,te_idx in FOLDS.split(train_pet_ids, target):\n    gen_tr = train_gen(batch_size=batch_size,\n                       shuffle=True,\n                       pet_list=train_pet_ids[tr_idx],\n                       pet_labels=target[tr_idx])\n    gen_te = train_gen(batch_size=batch_size,\n                       shuffle=False,\n                       pet_list=train_pet_ids[te_idx],\n                       pet_labels=target[te_idx])\n    gen_test = test_gen(batch_size=batch_size,\n                        shuffle=False,\n                        pet_list=test_pet_ids,\n                        pet_labels=None,\n                        use_labels=False)\n    \n    model = BASE_MODEL()\n    model.compile(optimizer='adam', loss='mse')\n    \n    model.fit_generator(gen_tr,\n                        steps_per_epoch=int(np.ceil(len(tr_idx)*1.0\/batch_size)),\n                        epochs=3,verbose=1,\n                        validation_data=gen_te,\n                        validation_steps=int(np.ceil(len(te_idx)*1.0\/batch_size)))\n    _test_prob = model.predict_generator(gen_test,\n                                         steps=int(np.ceil(len(test_df)*1.0\/(batch_size))))\n    _val_prob = model.predict_generator(gen_te,\n                                        steps=int(np.ceil(len(te_idx)*1.0\/(batch_size))))\n    \n    train_img_prob[te_idx,:] = _val_prob \n    test_img_prob += _test_prob\n\ntest_img_prob \/= N_FOLDS","a7b9dda4":"img_size = 256\nbatch_size = 16\n\ntrain_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")\npet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/train_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n\ntrain_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]","52faeed5":"test_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\n\npet_ids = test_df['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/test_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n        \ntest_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]\n\ntest_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntrain_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats.head()\ntrain = pd.merge(train, train_feats, how='left', on='PetID')\ntest = pd.merge(test, test_feats, how='left', on='PetID')","835d807c":"train_feats_256 = train_feats.copy()\ntest_feats_256 = test_feats.copy()","dfb306a6":"breed_id_map = dict(zip(breeds.BreedID.values,breeds.BreedName.values))\nbreed_type_map = dict(zip(breeds.BreedID.values,breeds.Type.values))\ncolor_id_map = dict(zip(colors.ColorID.values,colors.ColorName.values))","4131c8ba":"train['Breed1_text'] = train['Breed1'].map(lambda x:breed_id_map.get(x,'UNK_Breed1'))\ntrain['Breed2_text'] = train['Breed2'].map(lambda x:breed_id_map.get(x,'UNK_Breed2'))\ntrain['Color1_text'] = train['Color1'].map(lambda x:color_id_map.get(x,'UNK_Color1'))\ntrain['Color2_text'] = train['Color2'].map(lambda x:color_id_map.get(x,'UNK_Color2'))\ntrain['Color3_text'] = train['Color3'].map(lambda x:color_id_map.get(x,'UNK_Color3'))\n\ntest['Breed1_text'] = test['Breed1'].map(lambda x:breed_id_map.get(x,'UNK_Breed1'))\ntest['Breed2_text'] = test['Breed2'].map(lambda x:breed_id_map.get(x,'UNK_Breed2'))\ntest['Color1_text'] = test['Color1'].map(lambda x:color_id_map.get(x,'UNK_Color1'))\ntest['Color2_text'] = test['Color2'].map(lambda x:color_id_map.get(x,'UNK_Color2'))\ntest['Color3_text'] = test['Color3'].map(lambda x:color_id_map.get(x,'UNK_Color3'))","0769adab":"train['raw_text'] =  train['Name'] + ' ' \\\n                    + train['Breed1_text'] + ' ' + train['Breed2_text'] + ' ' \\\n                    + train['Color1_text'] + ' ' + train['Color2_text'] + ' ' \\\n                    + train['Color3_text'] + ' ' \\\n                    + train['Description']\n\ntest['raw_text'] =  test['Name'] + ' ' \\\n                    + test['Breed1_text'] + ' ' + test['Breed2_text'] + ' ' \\\n                    + test['Color1_text'] + ' ' + test['Color2_text'] + ' ' \\\n                    + test['Color3_text'] + ' ' \\\n                    + test['Description']","2b86e2b3":"gzf_prefix = 'gzf_'\n\ntrain[gzf_prefix+'RescureID_rank'] = train.RescuerID.map(train.RescuerID.value_counts().rank()\/train.RescuerID.unique().shape[0])\ntrain[gzf_prefix+'Description_len'] = train.Description.map(lambda x:len(x) if type(x)!=float else 0)\ntrain[gzf_prefix+'Description_word_len'] = train.Description.map(lambda x:len(x.strip().split()) if type(x)!=float else 0)\ntrain[gzf_prefix+'Description_distinct_word_len'] = train.Description.map(lambda x:len(set(x.lower().strip().split())) if type(x)!=float else 0)\ntrain[gzf_prefix+'Description_distinct_word_ratio'] = train[gzf_prefix+'Description_distinct_word_len'] \/ (train[gzf_prefix+'Description_word_len'] + 1.0)\n\ntest[gzf_prefix+'RescureID_rank'] = test.RescuerID.map(test.RescuerID.value_counts().rank()\/test.RescuerID.unique().shape[0])\ntest[gzf_prefix+'Description_len'] = test.Description.map(lambda x:len(x) if type(x)!=float else 0)\ntest[gzf_prefix+'Description_word_len'] = test.Description.map(lambda x:len(x.strip().split()) if type(x)!=float else 0)\ntest[gzf_prefix+'Description_distinct_word_len'] = test.Description.map(lambda x:len(set(x.lower().strip().split())) if type(x)!=float else 0)\ntest[gzf_prefix+'Description_distinct_word_ratio'] = test[gzf_prefix+'Description_distinct_word_len'] \/ (test[gzf_prefix+'Description_word_len'] + 1.0)","0f96f361":"X = pd.concat([train,test],axis=0,ignore_index=True)\nlen_train = len(train)","4b195593":"X[gzf_prefix+'is_pure'] = ((X.Breed1!=307) & (X.Breed2!=307) & (X.Breed2!=0)).astype(float)\nX[gzf_prefix+'is_pure_breed1'] = (X.Breed1!=307).astype(float)\nX[gzf_prefix+'is_pure_breed2'] = ((X.Breed2!=307) & (X.Breed2!=0)).astype(float)","d811ebc6":"agg_num_feature = ['Age','Health','PhotoAmt','Quantity',\n                   'doc_sent_mag', 'doc_sent_score', \n                   'meta_dominant_score', 'meta_label_score',gzf_prefix+'Description_len']\nagg_rescureid_1 = X.groupby(['RescuerID'])[agg_num_feature].mean()\nagg_rescureid_1.columns = ['Age_id','Health_id','PhotoAmt_id','Quantity_id',\n                   'doc_sent_mag_id', 'doc_sent_score_id', \n                   'dominant_score_id', 'label_score_id','Description_len_id']\nagg_rescureid_2 = X.groupby(['RescuerID'])['Breed1'].aggregate({'307_ratio':lambda x:(x==307).mean()})\nagg_rescureid = pd.concat([agg_rescureid_1,agg_rescureid_2],axis=1)\nagg_rescureid.columns = [gzf_prefix+x for x in agg_rescureid.columns ]\nX = pd.merge(X,agg_rescureid,left_on='RescuerID',right_index=True,how='left')","8fe0ad15":"SVD_FEATURES = 120\nNMF_FEATURES = 20\nLDA_FEATURES = 12\n\ndesc = X.raw_text.fillna(\"none\").values\ntfidf = TfidfVectorizer(min_df=3,  max_features=10000,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\nX_tfidf = tfidf.fit_transform(list(desc))\n\nsvd = TruncatedSVD(n_components=SVD_FEATURES)\nsvd.fit(X_tfidf)\nX_svd = svd.fit_transform(X_tfidf)\n\nX_svd = pd.DataFrame(X_svd, columns=[gzf_prefix+'sdv_{}'.format(i) for i in range(SVD_FEATURES)])\nX = pd.concat((X, X_svd), axis=1)\n\nnmf = NMF(n_components=NMF_FEATURES)\nnmf.fit(X_tfidf)\nX_nmf = nmf.fit_transform(X_tfidf)\n\nX_nmf = pd.DataFrame(X_nmf, columns=[gzf_prefix+'mnf_{}'.format(i) for i in range(NMF_FEATURES)])\nX = pd.concat((X, X_nmf), axis=1)\n\nlda = LatentDirichletAllocation(n_components=LDA_FEATURES, n_jobs=-1,max_iter=120)\nlda.fit(X_tfidf)\nX_lda = lda.fit_transform(X_tfidf)\n\nX_lda = pd.DataFrame(X_lda, columns=[gzf_prefix+'lad_{}'.format(i) for i in range(LDA_FEATURES)])\nX = pd.concat((X, X_lda), axis=1)\n","e34808b3":"cat_cols = ['Health',\n 'Breed1', 'Breed2',\n 'Type', 'Gender',\n 'Color3', 'Color2', 'Color1',\n 'Vaccinated','Sterilized',  'Dewormed',\n 'MaturitySize', 'FurLength',\n 'State','meta_label_description','meta_label_description1','meta_label_description2']\nX.loc[:, cat_cols] = X[cat_cols].astype('category')","6b2ea6c5":"foo = train.dtypes\ncat_feature_names = foo[foo == \"category\"].index.values\ncat_features = [i for i in range(X.shape[1]) if X.columns[i] in cat_feature_names]","9a85bef8":"train = X[:len_train]\ntest = X[len_train:]\ntrain.index = range(len_train)\ntest.index = range(test.shape[0])\n\ntarget = train['AdoptionSpeed']\nrescue_id = train['RescuerID']\n\ntrain.shape, target.shape","b218e866":"def obtain_train_mse_and_kappa(train_predictions, target):\n    optR = OptimizedRounder()\n    optR.fit(train_predictions, target)\n    coefficients_ = optR.coefficients()\n    rmse_score1 = rmse(target, train_predictions)\n    train_predictions = optR.predict(train_predictions, optR.coefficients()).astype(int)\n    qwk_score = quadratic_weighted_kappa(target, train_predictions)\n    rmse_score2 = rmse(target, train_predictions)\n    \n    return rmse_score1, rmse_score2, qwk_score\n\ndef run_cv_model(train, test, target, weight, model_fn, params={}, eval_fn=None, label='model'):\n    kf = FOLDS\n    n_splits = N_FOLDS\n    \n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], n_splits))\n    pred_test = np.zeros((origin_test.shape[0], n_splits))\n    \n    all_coefficients = np.zeros((n_splits, 4))\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '\/{}'.format(n_splits))\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n            dev_weight, val_weight = weight[dev_index], weight[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n            dev_weight, val_weight = weight[dev_index], weight[val_index]\n            \n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, dev_weight, val_weight, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        pred_test[:, i-1] = pred_test_y.reshape(-1)\n        \n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        i += 1\n    train_rmse1,  train_rmse2, train_qwk = obtain_train_mse_and_kappa([r[0] for r in pred_train], target)\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean        RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv recalculate RMSE1 score : {}'.format(label, train_rmse1))\n    print('{} cv recalculate RMSE2 score : {}'.format(label, train_rmse2))\n    print('{} cv std RMSE score : {}'.format(label, np.std(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean        QWK score : {}'.format(label, np.mean(qwk_scores)))\n    print('{} cv recalculate QWK score : {}'.format(label, train_qwk))\n    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test \/ float(n_splits)\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test, 'test_value':pred_test,\n                'cv': cv_scores, 'qwk': qwk_scores,\n               'coefficients': all_coefficients}\n    return results\n\ndef runLGB(train_X, train_y, test_X, test_y, dev_weight, val_weight, test_X2, params):\n    print('Prep LGB')\n\n    d_train = lgb.Dataset(train_X, label=train_y, weight=dev_weight)\n    d_valid = lgb.Dataset(test_X, label=test_y, weight=val_weight)\n    watchlist = [d_train, d_valid]\n    print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    print('Predict 1\/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    importances = model.feature_importance()\n    optR = OptimizedRounder_v3()\n    len_0 = test_y[test_y==0].shape[0]\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients, len_0)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2\/2')\n    return np.array(pred_test_y).reshape(-1, 1), np.array(pred_test_y2).reshape(-1, 1), importances, coefficients, qwk","9fef0754":"def get_cols(totals, prefixs):\n    if isinstance(prefixs, list):\n        cols = []\n        for prefix in prefixs:\n            cols += [col for col in totals if col.find(prefix) > -1]\n        return cols\n    else:\n        return [col for col in totals if col.find(prefixs) > -1]\n\norigin_cols = [\n    \"Type\",\"Age\",\n    \"Breed1\",\"Breed2\",\"Gender\",\n    \"Color1\",\"Color2\",\"Color3\",\n    \"MaturitySize\",\"FurLength\",\n    \"Vaccinated\",\"Dewormed\",\"Sterilized\",\"Health\",\n    \"Quantity\",\"Fee\",\"State\",\n    \"VideoAmt\",\"PhotoAmt\"\n]\n\ndoc_cols = get_cols(train.columns, 'doc_')\nmeta_cols = get_cols(train.columns, 'meta_')\npure_cols = get_cols(train.columns, 'pure_')\nrescue_cols = get_cols(train.columns, 'rescue_')\nlang_cols = get_cols(train.columns, 'lang_')\nsml_cols = get_cols(train.columns, ['svd_', 'lda_', 'nmf_'])\npic_cols = get_cols(train.columns, 'pic_')","5e16b476":"train['ResNet_meta'] = train_img_prob.flatten()\ntest['ResNet_meta'] = test_img_prob.flatten()","23c34686":"gzf_cols = doc_cols + lang_cols + origin_cols + pic_cols + [\n                'meta_dominant_blue', 'meta_dominant_green','meta_dominant_pixel_frac', \n                'meta_dominant_red', 'meta_dominant_score', 'meta_label_score', \n                'meta_vertex_x', 'meta_vertex_y'] + [\n                gzf_prefix+'RescureID_rank',gzf_prefix+'Description_len',\n                gzf_prefix+'Description_word_len',gzf_prefix+'Description_distinct_word_len',\n                gzf_prefix+'Description_distinct_word_ratio',\n                gzf_prefix+'is_pure',gzf_prefix+'is_pure_breed1',gzf_prefix+'is_pure_breed2',\n                gzf_prefix+'Quantity_id',gzf_prefix+'307_ratio'\n                ] + [gzf_prefix+'sdv_{}'.format(i) for i in range(SVD_FEATURES)] \\\n                  + [gzf_prefix+'mnf_{}'.format(i) for i in range(NMF_FEATURES)] \\\n                  + [gzf_prefix+'lad_{}'.format(i) for i in range(LDA_FEATURES)] \\\n                  + ['ResNet_meta']","54c2ee8f":"zkr_cols = origin_cols + doc_cols + meta_cols + pure_cols + rescue_cols + lang_cols + sml_cols + pic_cols + ['ResNet_meta']","34ffdc2c":"train_gzf = train[gzf_cols]\ntest_gzf = test[gzf_cols]\n\ntrain_zkr = train[zkr_cols]\ntest_zkr = test[zkr_cols]\n\nprint(train_gzf.shape, test_gzf.shape, train_zkr.shape, test_zkr.shape)","4e712da2":"params = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 80,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.9,\n          'bagging_freq': 3,\n          'feature_fraction': 0.85,\n          'min_split_gain': 0.01,\n          'min_child_samples': 150,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'early_stop': 100,\n          'verbose_eval': 500,\n          'num_rounds': 5000\n         }\n\nweight = pd.Series(np.where(train['Type']==2, 1.0, 1.0))\nlgb_gzf = run_cv_model(train[gzf_cols], test[gzf_cols], target, weight, runLGB, params, rmse, 'lgb')","7340f640":"params = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 80,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.9,\n          'bagging_freq': 3,\n          'feature_fraction': 0.84,\n          'min_split_gain': 0.01,\n          'min_child_samples': 150,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'verbose_eval': 500,\n          'num_rounds': 1500,\n         }\n\nweight = pd.Series(np.where(train['Type']==2, 1.0, 1.0))\nlgb_zkr = run_cv_model(train[zkr_cols], test[zkr_cols], target, weight, runLGB, params, rmse, 'lgb')","a482d3a2":"del train, test\ngc.collect()\n\ntrain = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/test.csv\")","53ffae5c":"train['Color'] = train.Color1 * 100 + train.Color2 * 10 + train.Color3\ntrain.drop(['Color1', 'Color2', 'Color3'], axis=1, inplace=True)\n\ntest['Color'] = test.Color1 * 100 + test.Color2 * 10 + test.Color3\ntest.drop(['Color1', 'Color2', 'Color3'], axis=1, inplace=True)","71a0efed":"target = train['AdoptionSpeed']\ntrain_id = train['PetID']\ntest_id = test['PetID']","7aa375bd":"doc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in train_id:\n    try:\n        with open('..\/input\/petfinder-adoption-prediction\/train_sentiment\/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntrain.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntrain.loc[:, 'doc_sent_score'] = doc_sent_score\ntrain[\"doc_sentiment\"] = train.doc_sent_mag * train.doc_sent_score\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in test_id:\n    try:\n        with open('..\/input\/petfinder-adoption-prediction\/test_sentiment\/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntest.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntest.loc[:, 'doc_sent_score'] = doc_sent_score\ntest[\"doc_sentiment\"] = test.doc_sent_mag * test.doc_sent_score","8f9a6d1e":"n_components = 150\n\ntrain_desc = train.Description.fillna(\"none\").values\ntest_desc = test.Description.fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words='english')\n\ntfv.fit(list(train_desc))\nX = tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\nsvd = TruncatedSVD(n_components=n_components)\nsvd.fit(X)\nX = svd.transform(X)\n\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(n_components)])\ntrain = pd.concat((train, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(n_components)])\ntest = pd.concat((test, X_test), axis=1)","f8b2572f":"img_xs = []\nimg_ys = []\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        im = Image.open('..\/input\/petfinder-adoption-prediction\/train_images\/%s-1.jpg' % pet)\n        width, height = im.size\n        img_xs.append(width)\n        img_ys.append(height)\n        with open('..\/input\/petfinder-adoption-prediction\/train_metadata\/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        img_xs.append(-1)\n        img_ys.append(-1)\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\ntrain.loc[:, 'img_x'] = img_xs\ntrain.loc[:, 'img_y'] = img_ys\ntrain.loc[:, 'vertex_x'] = vertex_xs\ntrain.loc[:, 'vertex_y'] = vertex_ys\ntrain.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain.loc[:, 'dominant_blue'] = dominant_blues\ntrain.loc[:, 'dominant_green'] = dominant_greens\ntrain.loc[:, 'dominant_red'] = dominant_reds\ntrain.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain.loc[:, 'dominant_score'] = dominant_scores\ntrain.loc[:, 'label_description'] = label_descriptions\ntrain.loc[:, 'label_score'] = label_scores\n\nimg_xs = []\nimg_ys = []\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        im = Image.open('..\/input\/petfinder-adoption-prediction\/test_images\/%s-1.jpg' % pet)\n        width, height = im.size\n        img_xs.append(width)\n        img_ys.append(height)\n        with open('..\/input\/petfinder-adoption-prediction\/test_metadata\/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        img_xs.append(-1)\n        img_ys.append(-1)\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\ntest.loc[:, 'img_x'] = img_xs\ntest.loc[:, 'img_y'] = img_ys\ntest.loc[:, 'vertex_x'] = vertex_xs\ntest.loc[:, 'vertex_y'] = vertex_ys\ntest.loc[:, 'bounding_confidence'] = bounding_confidences\ntest.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest.loc[:, 'dominant_blue'] = dominant_blues\ntest.loc[:, 'dominant_green'] = dominant_greens\ntest.loc[:, 'dominant_red'] = dominant_reds\ntest.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest.loc[:, 'dominant_score'] = dominant_scores\ntest.loc[:, 'label_description'] = label_descriptions\ntest.loc[:, 'label_score'] = label_scores","c2b64284":"train[\"vertex_x_ratio\"] = train.vertex_x \/ train.img_x\ntrain[\"vertex_y_ratio\"] = train.vertex_y \/ train.img_y\n\ntest[\"vertex_x_ratio\"] = test.vertex_x \/ test.img_x\ntest[\"vertex_y_ratio\"] = test.vertex_y \/ test.img_y","0247cc97":"train.Name = train.Name.fillna('')\ntest.Name = test.Name.fillna('')\ntrain[\"Name\"] = train.Name.apply(lambda x: str(x).lower())\ntest[\"Name\"] = test.Name.apply(lambda x: str(x).lower())\n\ntrain[\"name_length\"] = train.Name.apply(lambda x: len(str(x)))\ntest[\"name_length\"] = test.Name.apply(lambda x: len(str(x)))","2f40637b":"all_data = pd.concat((train, test))\n\nname_idx, name_val = all_data.Name.value_counts().index, all_data.Name.value_counts().values\nname_map = dict()\nfor idx, val in zip(name_idx, name_val):\n    name_map.update({idx: val})\n\ntrain[\"name_cnt\"] = train.Name.map(name_map)\ntest[\"name_cnt\"] = test.Name.map(name_map)","4bf76e17":"train['Description'] = train['Description'].fillna('')\ntest['Description'] = test['Description'].fillna('')\n\ntrain['desc_length'] = train['Description'].apply(lambda x: len(x))\ntrain['desc_words'] = train['Description'].apply(lambda x: len(x.split()))\n\ntest['desc_length'] = test['Description'].apply(lambda x: len(x))\ntest['desc_words'] = test['Description'].apply(lambda x: len(x.split()))","49e46db4":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '^']\n\ndef lexical_density(x):\n    for punct in puncts:\n        x = x.replace(punct, \"\")\n    li = x.split(\" \")\n    return len(set(li)) \/ len(li) if len(li) != 0 else 0\n\ntrain[\"desc_lexical_density\"] = train.Description.apply(lambda x: lexical_density(x))\ntest[\"desc_lexical_density\"] = test.Description.apply(lambda x: lexical_density(x))","4fd68df4":"def sentences_count(x):\n    return len(re.split(r'[.!?]+', x))\n\ntrain[\"sentences_count\"] = train.Description.apply(lambda x: sentences_count(x))\ntest[\"sentences_count\"] = test.Description.apply(lambda x: sentences_count(x))","7b72562a":"def find_capitals(x):\n    return len(re.findall('[A-Z]', x))\n\ntrain[\"desc_capitals\"] = train.Description.apply(lambda x: find_capitals(x))\ntest[\"desc_capitals\"] = test.Description.apply(lambda x: find_capitals(x))","2f1fb947":"rescuer_idx, rescuer_val = all_data.RescuerID.value_counts().index, all_data.RescuerID.value_counts().values\nrescuer_map = dict()\nfor idx, val in zip(rescuer_idx, rescuer_val):\n    rescuer_map.update({idx: val})\n\ntrain[\"rescuer_cnt\"] = train.RescuerID.map(rescuer_map)\ntest[\"rescuer_cnt\"] = test.RescuerID.map(rescuer_map)","a2b58259":"# state GDP: https:\/\/en.wikipedia.org\/wiki\/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# state population: https:\/\/zh.wikipedia.org\/wiki\/%E9%A9%AC%E6%9D%A5%E8%A5%BF%E4%BA%9A\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\n# state area\nstate_area = {\n    41336: 19.210,\n    41325: 9.500,\n    41367: 15.099,\n    41401: 0.243,\n    41415: 0.091,\n    41324: 1.664,\n    41332: 6.686,\n    41335: 36.137,\n    41330: 21.035,\n    41380: 2.31541,\n    41327: 0.821,\n    41345: 73.631,\n    41342: 124.450,\n    41326: 8.104,\n    41361: 13.035\n}\n\ntrain[\"state_gdp\"] = train.State.map(state_gdp)\ntrain[\"state_population\"] = train.State.map(state_population)\ntrain[\"state_area\"] = train.State.map(state_area)\ntest[\"state_gdp\"] = test.State.map(state_gdp)\ntest[\"state_population\"] = test.State.map(state_population)\ntest[\"state_area\"] = test.State.map(state_area)","85a723f2":"# {\"Domestic Long Hair\": 264, \"Domestic Medium Hair\": 265, \"Domestic Short Hair\": 266, \"Mixed Breed\": 307}\n\ntrain['Pure_breed'] = 1\ntrain.loc[train['Breed2'] != 0, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 264, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 265, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 266, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 307, 'Pure_breed'] = 0\n\ntest['Pure_breed'] = 1\ntest.loc[test['Breed2'] != 0, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 264, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 265, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 266, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 307, 'Pure_breed'] = 0","bd547ab3":"# drop some not so impantance features\n\ntrain.drop(['vertex_x', 'vertex_y', 'bounding_confidence'], axis=1, inplace=True)\ntest.drop(['vertex_x', 'vertex_y', 'bounding_confidence'], axis=1, inplace=True)","c64430c8":"n_img_features = 128\n\nimg_size = 256\nbatch_size = 16\n\ninp = Input((img_size, img_size, 3))\nbackbone = DenseNet121(input_tensor=inp, \n                       weights=\"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(1024\/\/n_img_features)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","88588e72":"pet_ids = train_id.values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/train_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n        \ntrain_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.head()","6856d3ad":"pet_ids = test_id.values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/test_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n        \ntest_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.head()","33b18e1a":"train_feats.columns = [\"img_feat{}\".format(i) for i in range(n_img_features)]\ntest_feats.columns = [\"img_feat{}\".format(i) for i in range(n_img_features)]\n\ntrain_feats[\"PetID\"] = train_feats.index\ntest_feats[\"PetID\"] = test_feats.index\n\ntrain = pd.merge(train, train_feats, on=\"PetID\")\ntest = pd.merge(test, test_feats, on=\"PetID\")\n\nprint(train.shape, test.shape)","54e4b8d3":"train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\ntest.drop(['PetID'], axis=1, inplace=True)","3f8afa4a":"train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\ntest.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n\n# rearrange columns again\nc = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'VideoAmt', 'PhotoAmt', 'Color'] +  [\"img_feat{}\".format(i) for i in range(n_img_features)] + ['doc_sent_mag', 'doc_sent_score'] + ['svd_{}'.format(i) for i in range(n_components)] + ['img_x', 'img_y', 'bounding_importance', 'dominant_blue', 'dominant_green', 'dominant_red', 'dominant_pixel_frac', 'dominant_score','label_description', 'label_score', 'vertex_x_ratio', 'vertex_y_ratio', 'name_length', 'name_cnt', 'desc_length', 'desc_words', 'desc_lexical_density', 'sentences_count', 'desc_capitals', 'rescuer_cnt', 'state_gdp', 'state_population', 'Pure_breed']\ntrain = train[c]\ntest = test[c]\n\nnumeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'doc_sent_mag', 'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', 'dominant_red', 'dominant_green', 'dominant_blue', 'bounding_importance', 'img_x', 'img_y', 'vertex_x_ratio', 'vertex_y_ratio', 'label_score', 'desc_length', 'desc_words', 'desc_lexical_density', 'sentences_count', 'desc_capitals', 'rescuer_cnt', 'state_gdp', 'state_population', 'Pure_breed', 'name_length', 'name_cnt'] + ['svd_{}'.format(i) for i in range(n_components)] + [\"img_feat{}\".format(i) for i in range(n_img_features)]\ncat_cols = list(set(train.columns) - set(numeric_cols))\n\ntrain.loc[:, cat_cols] = train[cat_cols].astype('category')\ntest.loc[:, cat_cols] = test[cat_cols].astype('category')\n\nfoo = train.dtypes\ncat_feature_names = foo[foo == \"category\"]\ncat_features = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]","dd7341fe":"del run_cv_model\ngc.collect()\n\ndef run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = FOLDS\n    n_splits = N_FOLDS\n    \n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], n_splits))\n    all_coefficients = np.zeros((n_splits, 4))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '\/' + str(n_splits))\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = train.columns.values\n        fold_importance_df['importance'] = importances\n        fold_importance_df['fold'] = i\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        i += 1\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test \/ float(n_splits)\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test,\n                'cv': cv_scores, 'qwk': qwk_scores,\n               'importance': feature_importance_df,\n               'coefficients': all_coefficients}\n    return results","29a6fd1a":"del runLGB\ngc.collect()\n\ndef runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n    d_train = lgb.Dataset(train_X, label=train_y)\n    d_valid = lgb.Dataset(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n    print('Train LGB')\n    try:\n        num_rounds = params.pop('num_rounds')\n    except:\n        pass\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=10000,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      callbacks=[lgb.reset_parameter(learning_rate=[0.005]*1000+[0.003]*1000+[0.001]*8000)],\n                      early_stopping_rounds=early_stop)\n\n    print('Predict 1\/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    init_coef = get_init_coefs(pred_test_y, test_y)\n    optR = OptimizedRounder_v2(initial_coefs=init_coef)\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    chi2 = get_chi2(pred_test_y_k, test_y)\n    print(\"Valid Counts = {}\".format(Counter(test_y)))\n    print(\"Predicted Counts = {}\".format(Counter(pred_test_y_k)))\n    print(\"Coefficients = {}\".format(coefficients))\n    print(\"Chi2 = {}\".format(chi2))\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n    print(\"QWK = {}\".format(qwk))\n    print('Predict 2\/2')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk","dcc8d320":"param = {'application': 'regression',\n         'boosting': 'gbdt', \n         'metric': 'rmse', \n         'num_leaves': 149, \n         'max_depth': 11, \n         'max_bin': 37, \n         'bagging_fraction': 0.975419815153193, \n         'bagging_freq': 1, \n         'feature_fraction': 0.2705570927694394, \n         'min_split_gain': 0.7636472013417633, \n         'min_child_samples': 29, \n         'min_child_weight': 0.13126728393897313, \n         'lambda_l2': 0.841358003322472, \n         'verbosity': -1, \n         'data_random_seed': 1029, \n         'early_stop': 100, \n         'verbose_eval': 2000, \n         'num_rounds': 10000}","3d23b7ef":"lgb_zyl = run_cv_model(train, test, target, runLGB, param, rmse, 'lgb')","8f0f22b0":"del train, test\ngc.collect()\n\ntrain = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\n\nlabels_breed = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')","783024fd":"all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]","8410b40d":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures_df = pd.concat([train_feats_256, test_feats_256], axis=0)\nfeatures = features_df[[f'pic_{i}' for i in range(256)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\n\nimg_features = pd.concat([all_ids, svd_col], axis=1)","c3410193":"train_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_sentiment\/*.json'))\n\ntest_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_sentiment\/*.json'))","c568d508":"split_char = '\/'","03bcb71d":"train_df_ids = train[['PetID']]\n\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)","9d3abbf9":"test_df_ids = test[['PetID']]\n\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)","354f76d9":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):        \n        self.debug = debug\n        self.sentence_sep = ' '        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)       \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        file_keys = list(file.keys())        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')        \n        return df_metadata\n    \ndef extract_additional_features(pet_id, mode='train'):\n    sentiment_filename = f'..\/input\/petfinder-adoption-prediction\/{mode}_sentiment\/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob(f'..\/input\/petfinder-adoption-prediction\/{mode}_metadata\/{pet_id}*.json'))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_json_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]    \n    return dfs\n\npet_parser = PetFinderParser()","d3c11d51":"train_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","454d9519":"aggregates = ['sum', 'mean', 'var']\nsent_agg = ['sum']\n\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","18b26fec":"train_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","abaa47c6":"train_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)","91ab40a9":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)","d287cf49":"X_temp = X.copy()\n\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID']","2ed51705":"rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","ff8cc071":"for i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","3dacb206":"X_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')","bf4dbea4":"X_temp['Length_Description'] = X_text['Description'].map(len)\nX_temp['Length_metadata_annots_top_desc'] = X_text['metadata_annots_top_desc'].map(len)\nX_temp['Lengths_sentiment_entities'] = X_text['sentiment_entities'].map(len)","92aa59d2":"n_components = 16\ntext_features = []\n\nfor i in X_text.columns:\n    print(f'generating features from: {i}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))    \n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)\n\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","bb038684":"X_temp = X_temp.merge(img_features, how='left', on='PetID')","83e39157":"train_df_ids = train[['PetID']]\ntest_df_ids = test[['PetID']]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\ntrain_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\ntrain_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\ntrain_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\ntest_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\ntest_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\ntest_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n\naggs = {\n    'image_size': ['sum', 'mean', 'var'],\n    'width': ['sum', 'mean', 'var'],\n    'height': ['sum', 'mean', 'var'],\n}\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\nagg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)","43119284":"X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')\n\nX_temp = X_temp.drop(to_drop_columns, axis=1)","9e7bbdab":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","0338e7cf":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)\nX_train_non_null['ResNet_meta'] = train_img_prob.flatten()         # ADD IMG ResNet50 metafeature\nX_test_non_null['ResNet_meta'] = test_img_prob.flatten()           # ADD IMG ResNet50 metafeature","ee415fd4":"X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()\nX_train_non_null.shape, X_test_non_null.shape","dec07130":"xgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'eta': 0.0123,\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}","e407458b":"def run_xgb(params, X_train, X_test):\n    kf = FOLDS\n    n_splits = N_FOLDS\n    \n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test","83753988":"model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)","8ac8b771":"xgb_453_train_pred = oof_train\nxgb_453_test_pred = np.mean(oof_test, axis=1)\nxgb_453_train_pred.shape, xgb_453_test_pred.shape","84b3fa80":"gzf_lgb = lgb_gzf[\"test\"].reshape(-1)\nzkr_lgb = lgb_zkr[\"test\"].reshape(-1)\nzyl_lgb = lgb_zyl[\"test\"].reshape(-1)\n\ndfa = pd.DataFrame({\"gzf_lgb\":gzf_lgb, \n                    \"zkr_lgb\":zkr_lgb, \n                    \"zyl_lgb\":zyl_lgb, \n                    \"453_xgb\":xgb_453_test_pred})\ndfa.corr()","7e1d801c":"gzf_lgb_train_pred = np.mean(lgb_gzf['train'], axis=1)\ngzf_lgb_test_pred = np.mean(lgb_gzf['test'], axis=1)\n\nzkr_lgb_train_pred = np.mean(lgb_zkr['train'], axis=1)\nzkr_lgb_test_pred = np.mean(lgb_zkr['test'], axis=1)\n\nzyl_lgb_train_pred = np.mean(lgb_zyl['train'], axis=1)\nzyl_lgb_test_pred = np.mean(lgb_zyl['test'], axis=1)\n\n\ntrain_meta = np.concatenate([gzf_lgb_train_pred.reshape(-1,1),\n                             zkr_lgb_train_pred.reshape(-1,1),\n                             zyl_lgb_train_pred.reshape(-1,1),\n                             xgb_453_train_pred.reshape(-1,1)\n                            ], axis=1)\ntest_meta = np.concatenate([gzf_lgb_test_pred.reshape(-1,1),\n                            zkr_lgb_test_pred.reshape(-1,1),\n                            zyl_lgb_test_pred.reshape(-1,1),\n                            xgb_453_test_pred.reshape(-1,1)\n                           ], axis=1)","aa6d828c":"from sklearn.linear_model import Ridge","858e38e6":"clf = Ridge(alpha=0.1)\n\nclf.fit(train_meta, target)\ntrain_pred = clf.predict(train_meta)","ed46afac":"print(clf.coef_)","4f7485ab":"init_coef = get_init_coefs(train_pred,  target)\noptR = OptimizedRounder_v2(initial_coefs=init_coef)\noptR.fit(train_pred, target)\ncoefficients = optR.coefficients()\nprint(\"coefficients: \", coefficients, \"\\n\")\n\nprint(\"True Counter: \", Counter(target))\n\noptR = OptimizedRounder_v2()\ntrain_predictions = optR.predict(train_pred, coefficients).astype(int)\nprint(\"Train Counter: \", Counter(train_predictions))\n\nprint(\"\\nTrain QWK: \", quadratic_weighted_kappa(target, train_predictions))\nprint(\"Train RMSE: \", rmse(target, train_pred))","8cdb4400":"predictions = clf.predict(test_meta)\n\noptR = OptimizedRounder_v3()\ntest_predictions = optR.predict(predictions, coefficients, 110).astype(int)\nprint(\"Test Counter: \", Counter(test_predictions), \"\\n\")\n\nprint(\"True Distribution:\")\nprint(pd.value_counts(target, normalize=True).sort_index())\nprint(\"Train Predicted Distribution:\")\nprint(pd.value_counts(train_predictions, normalize=True).sort_index())\nprint(\"Test Predicted Distribution:\")\nprint(pd.value_counts(test_predictions, normalize=True).sort_index())","f7344fae":"submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\nsubmission.head(10)","06fc3f60":"submission.to_csv('submission.csv', index=False)","edd3f2cb":"!head submission.csv","3bd65171":"## color","1b2b38f6":"## sentiment data","839d8114":"## load data","33b8f374":"## image metadata","1d5b2309":"# Feature table 3\n\nhttps:\/\/www.kaggle.com\/ranjoranjan\/single-xgboost-model","dabb5de4":"## description","0afd1efa":"# image processing functions","25f8300c":"# Feature table 3","934c63bf":"## all raw text","9ea112a9":"# Feature table 1","ffc0894a":"# Corr","c03db4be":"## colors","8acf3a3d":"## rescuer","8502eaee":"We have 3 feature tables and one from public kernel, and just merging them together.","26939bf4":"## image feature","735ee255":"### DenseNet121 extracted 256 dim image features","a166a372":"## common feature","2551fbb8":"## NMF LDA","8fd935d1":"## origin feature","af4041c6":"### ResNet50 meta feature","f88b6290":"## features","63c16b76":"## TFIDF NMF LDA","ba48d5b2":"# Feature table 2","3fd7a0ae":"## LGB","56f83ee2":"## XGB","9af2343c":"## description feature","4fbaa350":"# Training FT 1 and 2","23334c95":"# metric functions and OptimizedRounder(s)","4955dd77":"## state","7f9f3922":"# nfolds","a0fa0321":"## TFIDF","54c9de5c":"## pure breed","7e907f62":"# Stacking","0cf435b9":"## reload","33a72bdc":"# imports","6a04581e":"## reload","e0c79a40":"## LGB","60de5b32":"## DenseNet121 extracted 128 dim image features","7fa28e80":"# training functions for FT 1 and 2","38ef9b81":"## name","b0a3c378":"## rescuer, breeds and description"}}