{"cell_type":{"a1d449b4":"code","0c4bb40e":"code","63dbedc2":"code","cf05952c":"code","bbf2738f":"code","def22aab":"code","aa64c31c":"code","9e3d0e95":"code","e9c3eb7d":"code","c2413a82":"code","3e93977d":"code","7e42bde3":"code","0017f5de":"code","367ff917":"code","3c0746be":"code","f3a10649":"code","278357f5":"code","1650a5e6":"code","6e4f28d6":"code","d36dbbc0":"code","0622f239":"code","47f9179d":"markdown","718331f9":"markdown"},"source":{"a1d449b4":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"..\/input\/titanic\/train_and_test2.csv\") \ndf.head() ","0c4bb40e":"# drop the columns we don't use\ndf.drop([\"zero\",\"zero.1\",\"zero.2\",\"zero.3\",\"zero.4\",\"zero.5\",\"zero.6\",\"zero.7\",\"zero.8\",\"zero.9\",\n        \"zero.10\",\"zero.11\",\"zero.12\",\"zero.13\",\"zero.14\",\"zero.15\",\"zero.16\",\"zero.17\",\"zero.18\"], axis=1,inplace=True)","63dbedc2":"# find there are two nan data in \"Embarked\" columns\ndf.info()","cf05952c":"# clean data\ndf = df.dropna()","bbf2738f":"# Exploratory Data Analysis\nfeatures=['Sex', 'sibsp', 'Parch', 'Pclass',\n       'Embarked', '2urvived']\nfig=plt.subplots(figsize=(15,15))\nfor i, j in enumerate(features):\n    plt.subplot(4, 2, i+1)\n    plt.subplots_adjust(hspace = 1.0)\n    sns.countplot(x=j,data = df)\n    plt.title(\"EDA\")","def22aab":"# built features and label\n# \u5efa\u69cb\u7279\u5fb5\u8207\u6a19\u7c64\ny = df[\"2urvived\"].values\nX = df.drop(['2urvived'], axis = 1)","aa64c31c":"# split training and testing data\n# \u5340\u5206\u8a13\u7df4\u548c\u6e2c\u8a66\u7528\u8cc7\u6599\nfrom sklearn.model_selection import train_test_split  \nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=0)","9e3d0e95":"# feature scaling\n# \u7279\u5fb5\u7e2e\u653e\nfrom sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","e9c3eb7d":"# variant of score method\n# \u9664\u4e86\u6a21\u7d44\u5167\u7684score\u5916,\u53e6\u5916\u518d\u4ee5f1\u53caconfusion matrix\u4f86\u8a55\u50f9\nfrom sklearn.metrics import (f1_score, confusion_matrix) ","c2413a82":"from sklearn.linear_model import LogisticRegression # LogisticRegression # \u908f\u8f2f\u56de\u6b78\nlr = LogisticRegression() \nlr.fit(X_train,y_train) \ny_pred = lr.predict(X_test) \nlr_acc = lr.score(X_test,y_test)*100 \nlr_f1 = f1_score(y_test, y_pred)*100 \nprint(\"Accuracy of Logistic Regression\uff1a {:.2f}%\".format(lr_acc))\nprint(\"F1 score of Logistic Regression: {:.2f}%\".format(lr_f1))\nprint('Confusion matrix of Logistic Regression:\\n', confusion_matrix(y_test,y_pred))","3e93977d":"from sklearn.neighbors import KNeighborsClassifier # KNN\nk = 5 \nknn = KNeighborsClassifier(n_neighbors = k)  \nknn.fit(X_train, y_train) \ny_pred = knn.predict(X_test) \nknn_acc = knn.score(X_test,y_test)*100 \nknn_f1 = f1_score(y_test, y_pred)*100 \nprint(\"Accuracy of {}NN: {:.2f}%\".format(k, knn_acc))\nprint(\"F1 score of {}NN : {:.2f}%\".format(k, knn_f1))\nprint('Confusion matrix of KNN :\\n', confusion_matrix(y_test,y_pred))","7e42bde3":"# find the best k value\nf1_score_list = []\nacc_score_list = []\nfor i in range(1,15): \n    kNN = KNeighborsClassifier(n_neighbors = i)  \n    kNN.fit(X_train, y_train)\n    acc_score_list.append(kNN.score(X_test, y_test))\n    y_pred = kNN.predict(X_test) \n    f1_score_list.append(f1_score(y_test, y_pred))\nindex = np.arange(1,15,1)\n# plot\nplt.plot(index,acc_score_list,c='blue',linestyle='solid')\nplt.plot(index,f1_score_list,c='red',linestyle='dashed')\nplt.legend([\"Accuracy\", \"F1 Score\"])\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.grid('false')\nplt.show()\nkNN_acc = max(f1_score_list)*100\nprint(\"Maximum kNN Score is {:.2f}%\".format(kNN_acc))","0017f5de":"from sklearn.svm import SVC # SVM\nsvm = SVC(random_state = 1) \nsvm.fit(X_train, y_train) \ny_pred = svm.predict(X_test) \nsvm_acc = svm.score(X_test,y_test)*100 \nsvm_f1 = f1_score(y_test, y_pred)*100 \nprint(\"Accuracy of SVM : {:.2f}%\".format(svm_acc))\nprint(\"F1 score of SVM : {:.2f}%\".format(svm_f1))\nprint('Confusion matrix of SVM :\\n', confusion_matrix(y_test,y_pred))","367ff917":"from sklearn.naive_bayes import GaussianNB # naive bayes\nnb = GaussianNB() \nnb.fit(X_train, y_train) \ny_pred = nb.predict(X_test) \nnb_acc = nb.score(X_test,y_test)*100 \nnb_f1 = f1_score(y_test, y_pred)*100 \nprint(\"Accuracy of naive bayes: {:.2f}%\".format(nb_acc))\nprint(\"F1 score of naive bayes: {:.2f}%\".format(nb_f1))\nprint('Confusion matrix of naive bayes:\\n', confusion_matrix(y_test,y_pred))","3c0746be":"from sklearn.tree import DecisionTreeClassifier # Decision Tree\ndt = DecisionTreeClassifier() \ndt.fit(X_train, y_train) \ny_pred = dt.predict(X_test) \ndt_acc = dt.score(X_test,y_test)*100 \ndt_f1 = f1_score(y_test, y_pred)*100 \nprint(\"Accuracy of Decision Tree: {:.2f}%\".format(dt_acc))\nprint(\"F1 score of Decision Tree: {:.2f}%\".format(dt_f1))\nprint(\"Confusion matrix of Decision Tree:\\n\", confusion_matrix(y_test,y_pred))","f3a10649":"from sklearn.ensemble import RandomForestClassifier # RandomForest\nrf = RandomForestClassifier(n_estimators = 550, random_state = 1) \nrf.fit(X_train, y_train) \ny_pred = rf.predict(X_test) \nrf_acc = rf.score(X_test,y_test)*100 \nrf_f1 = f1_score(y_test, y_pred)*100 \nprint(\"Accuracy of Random Forest: {:.2f}%\".format(rf_acc))\nprint(\"F1 score of Random Forest: {:.2f}%\".format(rf_f1))\nprint(\"Confusion matrix of Random Forest:\\n\", confusion_matrix( y_test,y_pred))","278357f5":"# comparison of F1 score\nimport seaborn as sns\nmethods = [\"Logistic Regression\", \"KNN\", \"SVM\", \n           \"Naive Bayes\", \"Decision Tree\", \"Random Forest\"]\nf1 = [lr_f1, knn_f1, svm_f1, nb_f1, dt_f1, rf_f1]\ncolors = [\"orange\",\"red\",\"purple\", \"magenta\", \"green\",\"blue\"]\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylim((0,80))\nplt.ylabel(\"F1 Score\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=methods, y=f1, palette=colors)\n# plt.grid(b=None)\nplt.show()","1650a5e6":"# # comparison of Accurancy\nimport seaborn as sns\nmethods = [\"Logistic Regression\", \"KNN\", \"SVM\", \n           \"Naive Bayes\", \"Decision Tree\", \"Random Forest\"]\nf1 = [lr_acc, knn_acc, svm_acc, nb_acc, dt_acc, rf_acc]\ncolors = [\"orange\",\"red\",\"purple\", \"magenta\", \"green\",\"blue\"]\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylim((60,100))\nplt.ylabel(\"accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=methods, y=f1, palette=colors)\n# plt.grid(b=None)\nplt.show()","6e4f28d6":"# use deep learning to predict\nfrom tensorflow import keras \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense \nann = Sequential() # ANN(Artifical Neural Network)\nann.add(Dense(units=8, input_dim=8, activation = 'relu')) \nann.add(Dense(units=16, activation = 'relu')) \nann.add(Dense(units=24, activation = 'relu')) \nann.add(Dense(units=1, activation = 'sigmoid')) \nann.summary() \n\nann.compile(optimizer = 'adam',           \n            loss = 'binary_crossentropy',   \n            metrics = ['acc'])       ","d36dbbc0":"history = ann.fit(X_train, y_train, \n                  epochs=30,        \n                  batch_size=64,    \n                  validation_data=(X_test, y_test)) ","0622f239":"# plot the learning curve\ndef show_history(history): \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure(figsize=(12,4))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\nshow_history(history) ","47f9179d":"## This is a basic practice and also a comparison of variant methods of ML.\n\n+ EDA\n+ Logistic Regression\n+ KNN\n+ SVM\n+ Naive Bayes\n+ Decision Tree\n+ Random Forest\n+ Deap Learning - ANN\n\n","718331f9":"## Conclusion\n\nIt seems like in the traditional machine learning field, SVM and Random Forest perform better than other methods in this dataset. And using ANN to analyze can also get a pretty good result.(All without Hyperparameter adjustment)\n"}}