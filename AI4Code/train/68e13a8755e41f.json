{"cell_type":{"46160cc8":"code","1926b229":"code","633a4355":"code","a703e8a0":"code","2de1a942":"code","8198a05c":"code","0cfce979":"code","2c121b82":"code","d98b5889":"code","9504f1f8":"code","424d6d50":"code","2e22f938":"code","14992bd6":"code","b894ac56":"code","d5d32ee3":"code","1fe7767c":"code","f2aeecbb":"code","94cf25f9":"code","09314233":"code","0fb554c2":"code","69d2dd40":"code","4b2fa47d":"code","0439126b":"code","71279bc8":"code","ee0367c0":"code","043292a2":"code","b71284d1":"code","613e11f8":"code","a0f37ab7":"code","746763b1":"code","40eebc98":"markdown","6c3b6744":"markdown","2a22d5f3":"markdown","d36cebbb":"markdown","03b7cfc2":"markdown","3918296c":"markdown","6793509a":"markdown","5eef3cd5":"markdown","1942eff7":"markdown","84039f4f":"markdown","3f279f94":"markdown","f4952fe8":"markdown","5438f9a9":"markdown","a50106a2":"markdown","d639a2fe":"markdown","a8a3ccde":"markdown","52dbe413":"markdown","f12a9152":"markdown","eeb1f962":"markdown","6824c78b":"markdown","11b73e33":"markdown","7d95d356":"markdown","2506f5ee":"markdown"},"source":{"46160cc8":"# Data Manipulation and Linear Algebra\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_val_predict, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore')","1926b229":"data = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\n\nplacement = data.copy()\nplacement","633a4355":"placement.info()","a703e8a0":"placement.isnull().sum()","2de1a942":"placement['salary'].fillna(value=0, inplace=True)\n\nplacement.salary.isnull().sum()","8198a05c":"placement.drop(['sl_no','ssc_b','hsc_b'], axis = 1,inplace=True) \nplacement","0cfce979":"sns.set_style(\"white\")\nfig, axes = plt.subplots(figsize=(15, 10), dpi=80, nrows=2, ncols=2)\n\nsns.boxplot(y=placement['ssc_p'], ax=axes[0,0], width=0.15)\naxes[0,0].set_title('Secondary school percentage')\n\nsns.boxplot(y=placement['hsc_p'], ax=axes[0,1], width=0.15)\naxes[0,1].set_title('Higher Secondary school percentage')\n\nsns.boxplot(y=placement['degree_p'], ax=axes[1,0], width=0.15)\naxes[1,0].set_title('UG Degree percentage')\n\nsns.boxplot(y=placement['etest_p'], ax=axes[1,1], width=0.15)\naxes[1,1].set_title('Employability percentage')","2c121b82":"Q1 = placement['hsc_p'].quantile(0.25)\nQ3 = placement['hsc_p'].quantile(0.75)\nIQR = Q3 - Q1    #IQR is interquartile range. \n\nprint(IQR)","d98b5889":"outlier_filter = (placement['hsc_p'] >= Q1 - 1.5 * IQR) & (placement['hsc_p'] <= Q3 + 1.5 *IQR)\n\nfiltered_placement = placement.loc[outlier_filter]","9504f1f8":"sns.set_style(\"white\")\nfig, axes = plt.subplots(figsize=(15, 6), dpi=80, nrows=1, ncols=2)\n\nsns.boxplot(y=placement['hsc_p'], ax=axes[0], width=0.15)\naxes[0].set_title('Before removing outliers(hsc_p)')\n\nsns.boxplot(y=filtered_placement['hsc_p'], ax=axes[1], width=0.15)\naxes[1].set_title('After removing outliers(hsc_p)')","424d6d50":"plt.figure(figsize = (14, 8), dpi=80)\n\ncols = [\"specialisation\", \"workex\", \"degree_t\", \"gender\", \"hsc_s\", \"status\"]\ncolor_palette = [\"hls\", \"husl\", \"Set2\", \"flare\", \"mako\", \"crest\"]\n\nsubplot_num = 231\nfor index, col in enumerate(cols):    \n    plt.subplot(subplot_num)\n    ax=sns.countplot(x=col, data=filtered_placement, palette=sns.color_palette(color_palette[index], 3))\n    \n    subplot_num += 1","2e22f938":"num_cols = filtered_placement.select_dtypes(include=np.number).columns.tolist()\ncolors = [\"#a86432\", \"#a8a232\", \"#a8327b\", \"#32a871\", \"#328da8\", \"#8532a8\"]\n\nfig, axes = plt.subplots(figsize = (15, 15), dpi=80, nrows=4, ncols=3)\n\nfor i in range(0, 3):\n    sns.distplot(filtered_placement[num_cols[i]], color=colors[i], bins=20, ax=axes[0, i])\n    \nfor i in range(0, 3):\n    sns.boxplot(filtered_placement[num_cols[i]], color=colors[i], ax=axes[1, i], width=0.4)\n    \nfor i in range(0, 3):\n    j = i + 3\n    sns.distplot(filtered_placement[num_cols[j]], color=colors[j], bins=20, ax=axes[2, i])\n    \nfor i in range(0, 3):\n    j = i + 3\n    sns.boxplot(filtered_placement[num_cols[i]], color=colors[i], ax=axes[3, i], width=0.4)","14992bd6":"plt.figure(figsize=(15, 6), dpi=80)\n\nplt.subplot(121)\nsns.countplot(x=\"workex\", hue=\"status\", data=data, palette=['#5bde54',\"#de5454\"])\n\nplt.subplot(122)\nplt.pie(data.status.value_counts(), autopct='%.1f%%', colors=['#5bde54',\"#de5454\"], explode=[0, 0.05])\nplt.title(\"Count of Placed and Not Placed\")\n\nplt.show()","b894ac56":"plt.figure(dpi=80, figsize=(8, 6))\nsns.violinplot(x=\"specialisation\", y=\"salary\", hue=\"gender\", data=filtered_placement)\nplt.show()","d5d32ee3":"plt.figure(dpi=80, figsize=(8, 6))\nsns.heatmap(filtered_placement.corr(), cmap=\"viridis\", annot=True)\nplt.show()","1fe7767c":"object_cols=['gender','workex','specialisation']\n\nlabel_encoder = LabelEncoder()\n\nfor col in object_cols:\n    filtered_placement[col] = label_encoder.fit_transform(filtered_placement[col])\n    \nfiltered_placement.head()","f2aeecbb":"ohe_features = pd.get_dummies(filtered_placement.drop(\"status\", axis=1))\n\nfull_data = pd.concat([ohe_features, filtered_placement.status], axis=1).reset_index(drop=True)\nfull_data.head()","94cf25f9":"full_data[\"status\"].replace({\"Placed\": 1, \"Not Placed\": 0}, inplace=True)","09314233":"# Dropping salary column\nfull_data.drop(\"salary\", axis=1, inplace=True)","0fb554c2":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(full_data, full_data['status']):\n    train = full_data.loc[train_index]\n    test = full_data.loc[test_index]","69d2dd40":"X_train = train.drop(\"status\", axis=1)\ny_train = train[\"status\"]\n\nX_test = test.drop(\"status\", axis=1)\ny_test = test[\"status\"]","4b2fa47d":"MLA_compare = pd.DataFrame()\n\ndef MLA_testing(MLA, X_train, X_test):\n    row_index = 0\n    for classifier in MLA:\n        # Training The Model\n        classifier.fit(X_train, y_train)\n\n        # KFold Accuracies on Training Data\n        kfold_accuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs=-1)\n        \n        # Prediction on Testing Data\n        y_pred = cross_val_predict(estimator = classifier, X = X_test, y = y_test, cv = 10, n_jobs=-1)\n        \n        # Accuracy for y_test and y_pred\n        classifier_accuracy_score = accuracy_score(y_test, y_pred)\n\n        # Saving Data in Dataframe\n        MLA_name = classifier.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'Accuracy Score'] = classifier_accuracy_score*100\n        MLA_compare.loc[row_index, 'K-Fold Accuracy'] = kfold_accuracy.mean()*100\n\n        print(MLA_name, \"Done\")\n        row_index+=1","0439126b":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.LinearSVC(), \n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    XGBClassifier(eval_metric=\"logloss\"),\n    CatBoostClassifier(silent=True)  \n    ]\n\nMLA_testing(MLA=MLA, X_train=X_train, X_test=X_test)","71279bc8":"MLA_compare = MLA_compare.sort_values(by=\"K-Fold Accuracy\", ascending=False).reset_index(drop=True)[:10]\nMLA_compare","ee0367c0":"plt.figure(figsize=(10, 6), dpi=80)\nsns.barplot(x=\"K-Fold Accuracy\", y=\"MLA Name\", data=MLA_compare)","043292a2":"plt.figure(figsize=(10, 6), dpi=80)\nsns.barplot(x=\"Accuracy Score\", y=\"MLA Name\", data=MLA_compare)","b71284d1":"def classification(classifier):\n    # Training The Model\n    classifier.fit(X_train, y_train)\n\n    # KFold Accuracies on Training Data\n    kfold_accuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs=-1)\n    print(\"K-Fold Accuracy Score:\\n\", kfold_accuracy, \"\\n\")\n    print(\"Avg K-Fold Accuracy Score:\", kfold_accuracy.mean(), \"\\n\")\n\n    # Prediction on Testing Data\n    y_pred = cross_val_predict(estimator = classifier, X = X_test, y = y_test, cv = 10, n_jobs=-1)\n            \n    # Accuracy for y_test and y_pred\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score:\", accuracy, \"\\n\")\n\n    # Confusion Matrix\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n\n    # Classification Report\n    print(classification_report(y_test, y_pred))","613e11f8":"rf_clf = ensemble.RandomForestClassifier()\n\nclassification(rf_clf)","a0f37ab7":"# Feature Importance Score DataFrame\nfeature_imp = pd.DataFrame()\n\n# Getting the Feature Names and Their Importance Scores\nfeature_imp[\"Features\"] = [\n    'ssc_p', 'degree_p', 'hsc_p', 'mba_p', 'etest_p', 'workex', 'gender',\n    'specialisation', 'hsc_s_Science', 'degree_t_Sci&Tech', 'hsc_s_Commerce',\n    'degree_t_Comm&Mgmt', 'degree_t_Others', 'hsc_s_Arts']\nfeature_imp[\"Features Importance Score\"] = rf_clf.feature_importances_\n\n# Sorting\nfeature_imp = feature_imp.sort_values(by=\"Features Importance Score\", ascending=False).reset_index(drop=True)\n\nfeature_imp","746763b1":"plt.figure(figsize=(10, 8), dpi=80)\nsns.barplot(x=\"Features Importance Score\", y=\"Features\", data=feature_imp)\nplt.title(\"Feature Importance Score Comparison\")","40eebc98":"# Preparing Data","6c3b6744":"## Label Encoding","2a22d5f3":"## RandomForestClassifier","d36cebbb":"## Count Plots","03b7cfc2":"## Specialisation vs Salary","3918296c":"## Distribution of Numberical Features","6793509a":"## Correlation Heatmap","5eef3cd5":"## Viewing Important Features for Prediction by RandomForestClassifier","1942eff7":"## Finding Outliers and Removing Them","84039f4f":"# Modeling","3f279f94":"## Checking for Null Values","f4952fe8":"# Importing Libraries and EDA","5438f9a9":"## Dataframe to store all the accuracy scores for Comparison and Analysis","a50106a2":"## Handling Missing Values","d639a2fe":"## Stratified Train Test Split\n### Evenly Spreading the Dependent Variable \"status\" in Train and Test set","a8a3ccde":"# Data Cleaning","52dbe413":"## Importing Libraries","f12a9152":"## Dropping Unwanted Features","eeb1f962":"## Outliers","6824c78b":"## Loading Dataset","11b73e33":"## OneHot Encoding","7d95d356":"## Comparing Models\n## Viewing Top 10 Best Performing Models","2506f5ee":"# Data Visualization"}}