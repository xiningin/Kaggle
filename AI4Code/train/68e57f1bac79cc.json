{"cell_type":{"726d6d2d":"code","6a9ca448":"code","ac7a50ed":"code","4ee61d42":"code","214d7b02":"code","af6b5dcd":"code","ad8b11b9":"code","d2d6cd3f":"code","e76b2b46":"code","c4ad2ce3":"code","bc0b7879":"code","5c088be3":"code","ab99a453":"code","e486393b":"code","9efddb2c":"code","0fdb3ade":"code","35a3a8f1":"code","c48df409":"code","cf2f0856":"code","332baeaa":"code","48c57c4f":"code","05f38cb2":"code","cb54a86f":"code","9f2bfe9b":"code","8c47c789":"code","4596c8e0":"code","d0ee5781":"markdown","125a3d49":"markdown","4f1c2627":"markdown","00dece0f":"markdown","fafd6d22":"markdown","63c45beb":"markdown","72fb84c3":"markdown","2bff5564":"markdown","0cd15d29":"markdown","02827afd":"markdown","8efc9f3e":"markdown","51891cc4":"markdown","f3c58061":"markdown","cbafa8b3":"markdown","25f62e40":"markdown","cbbc90ab":"markdown"},"source":{"726d6d2d":"# load the file with the top results\nimport numpy as np\nimport pandas as pd\npd.set_option('max_colwidth', 150)\ncancer_paper = pd.read_csv('..\/input\/cancer-output\/oncology_paper.csv')\n\ncancer_paper[['cord_uid','title']].head(10)","6a9ca448":"# load all the required libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n\n# load the raw data\npaper = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')","ac7a50ed":"# for toubleshooting if there's problem with the pre-written cleaning steps\npaper.head()","4ee61d42":"length_of_df = len(paper)\nprint(f'Before cleaning, there are {length_of_df} entries in the metadata.csv file.')","214d7b02":"# cleaning pipline\n''' 1. keeping the columns: cord_uid, title, abstract\n    2. remove duplication in the title column, because they may have different cord_uid, \n       and not being recognized as duplicated entry\n    3. remove NA in the abstract column, because the analysis will use the words from the \n       abstract column\n    4. reset the index of the dataframe'''\n\npaper.drop(columns=['sha','source_x','doi','pmcid','pubmed_id','license', 'publish_time',\n                    'authors', 'journal','Microsoft Academic Paper ID',\n                    'WHO #Covidence', 'has_pdf_parse','has_pmc_xml_parse',\n                    'full_text_file', 'url'], \n           inplace = True)\n\npaper.drop_duplicates(subset=['title'], keep = 'first', inplace = True)\n\npaper.dropna(subset = ['abstract'], inplace = True)\n\npaper.reset_index(drop=True)","af6b5dcd":"length_of_df = len(paper)\nprint(f'After cleaning, there are {length_of_df} entries in dataframe, \"paper\".')","ad8b11b9":"# need to install en_core_sci_lg, as it is not pre-installed on Kaggle.\n\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","d2d6cd3f":"# import the natural language processing library and PhraseMatcher\nimport spacy\nimport en_core_sci_lg\nfrom spacy.matcher import PhraseMatcher\n\n# load large science tokenizer, tagger, parser NER and word vectors\nnlp = en_core_sci_lg.load()\n\n# define the matcher\nmatcher = PhraseMatcher(nlp.vocab)","e76b2b46":"Oncology_list = ['oncology', \n            'cancer',\n            'biomarker',\n            'turnaround time',\n            'clinical lab'\n           ]\ncovid_list = ['COVID-19',\n              'novel coronavirus 2019',\n              '2019 novel coronavirus',\n              'new coronavirus pneumonia',\n              'SARS-CoV-2',\n              'coronavirus disease 2019'\n             ]\n\n# convert the phrase to a Doc object\nphrase_pattern_Oncology = [nlp(text) for text in Oncology_list]\nphrase_pattern_covid = [nlp(text) for text in covid_list]\n\n# pass the phrase_pattern to the matcher\nmatcher.add('Oncology', None, *phrase_pattern_Oncology)\nmatcher.add('COVID', None, *phrase_pattern_covid)","c4ad2ce3":"def matchID_to_matchString(match_output):\n    '''this function coverts the result of Phrasematcher, match ID into the match string.\n       \n       input: Phrasematcher output, list of number with match ID, the location of the match\n       string in the text. The function will use the first entry of each item in the list, \n       and convert it back to the string.\n       \n       output: a list of unqiue match string, which are the sub-topics (repeated matches are removed)\n    '''\n    \n    match_string_list = list()\n    \n    for i in range(0, len(match_output)):\n        match_string = nlp.vocab.strings[match_output[i][0]]\n        \n        if match_string not in match_string_list:\n            match_string_list.append(match_string)\n            \n    return match_string_list\n\n\ndef match_all_keywords(abstract_nlp, match_ouput):\n    '''this function covert the match output to individual keywords.\n       \n       input: tokenized abstract and the match output\n       output: a list of keywords found in the abstract\n    '''\n    \n    keywords = list()\n    \n    for i in range(0, len(match_ouput)):\n        start_id, start_end = match_ouput[i][1], match_ouput[i][2]\n        keywords.append(abstract_nlp[start_id:start_end].text)\n        \n    return keywords\n\n\ndef find_unique_keywords(all_keywords):\n    '''this function find the unique keywords from the all_keywords list\n    \n       input: a list of all keywords\n       output: a list of unique keywords\n    '''\n    \n    unique_list = list()\n    \n    for words in all_keywords:\n        if words not in unique_list:\n            unique_list.append(words)\n            \n    return unique_list\n\ndef one_hot_encoding(df, col_name, topics):\n    '''this function is a one_hot_encoding process to convert a list of match_string or unique_keywords into\n       columns with 0\/1 with the column name representing the sub-topic or keywords.\n       \n       input: \n             df:the dataframe that the new columns will be added\n             col_name: it can be the columns match_string if sub-topic is intented to be unpacked; or \n                       unique_keywords if indivdiual keywords is intented to be unpacked\n             word_list:  a list of sub-topics in match_string or a list of unique_keywords,\n                        it should be each entry of the column: 'match_string' or 'unique_keywords'\n       ouput: the dataframe with new column\n       \n    '''\n    def unpack_topic(word_list, topic):\n        \n        is_topic = list()\n        \n        for item in word_list:\n            is_topic.append(topic in item)\n        return sum(is_topic)\n\n    for topic in topics:\n        df[topic] = df[col_name].apply(lambda x: unpack_topic(x, topic))\n    \n    return df","bc0b7879":"# additional functions for analysis on individual keywords level\n\ndef count_keywords(unique_keywords):\n    '''this function count the unique keywords of the whole dataframe'''\n    \n    keyword_count = dict()\n    keywords_list = list()\n    \n    for row in unique_keywords:\n        keywords_list.extend(row)\n        \n    for word in keywords_list:\n        if word in keyword_count:\n            keyword_count[word] += 1\n        else:\n            keyword_count[word] = 1\n\n    return keyword_count\n\ndef remove_covid(count_list):\n    for word in covid_list:\n        count_list.pop(word)\n    return count_list\n\ndef output_title(df, keyword):\n    new_df = df.loc[df[keyword] == 1].copy()\n    new_df.sort_values(by = ['no_unique_keywords'], ascending = False, inplace = True)\n    return new_df.filter(items = ['cord_uid', 'title'])","5c088be3":"# tokenizer the text in abstract\npaper['abstract_nlp'] = paper['abstract'].apply(lambda x: nlp(x))","ab99a453":"# match the phrases to the tokenized abstract\npaper['match_output'] = paper['abstract_nlp'].apply(lambda x: matcher(x))\n\n# convert the phrasematcher output into strings\npaper['match_string'] = paper['match_output'].apply(matchID_to_matchString)\n\n# find all the keywords\/phrase in the string output of the phrasematcher\npaper['all_keywords'] = paper.apply(lambda x: match_all_keywords(x['abstract_nlp'],x['match_output']), axis = 'columns')\n\n# find the unique keywords, ie. remove duplications\npaper['unique_keywords'] = paper['all_keywords'].apply(find_unique_keywords)\n\n# count the number of topics and the number of unique keywords\npaper['no_topics'] = paper['match_string'].apply(len)\npaper['no_unique_keywords'] = paper['unique_keywords'].apply(len)\n\n# remove unwanted columns\npaper.drop(columns = ['abstract_nlp','match_output'], inplace = True)","e486393b":"# one-hot-encoding to covert the data in list to individual columns\nsub_topics = ['Oncology', 'COVID']\none_hot_encoding(paper, 'match_string', sub_topics)","9efddb2c":"# subset the papers related to COVID-19\ncovid_paper = paper.loc[paper['COVID'] > 0].copy()","0fdb3ade":"# count number of unique keywords that is present (not counting keywords related to COVID-19)\ncovid_oncology_list = count_keywords(covid_paper['unique_keywords'])\ncovid_oncology_list = remove_covid(covid_oncology_list)\nkeywords_stat = pd.DataFrame.from_dict(covid_oncology_list, orient = 'index', columns = ['counts'])\nkeywords_stat = keywords_stat.sort_values(by = ['counts'], ascending = False)","35a3a8f1":"# adding each keywords to the dataframe\nkeywords_list = keywords_stat.index\none_hot_encoding(covid_paper, 'unique_keywords', keywords_list)","c48df409":"# add a column with the number of ethic topics\ncovid_paper['oncology_topics'] = covid_paper['no_topics'].apply(lambda x: x-1)\n\n# add a column with the number of ethic topics\ncovid_paper['no_oncology_keywords'] = covid_paper.apply(lambda x: sum(x[keywords_list]), axis = 1)\n\n# sort the dataframe with the number of topic, and the number of unique keywords\ncovid_paper.sort_values(by = ['oncology_topics', 'no_oncology_keywords'], ascending = False, inplace = True)\n\n# output the COVID-19 papers \ncovid_paper.to_csv('covid_paper.csv')\n\n# subset the oncology papers\noncology_paper = covid_paper.loc[covid_paper['Oncology'] > 0].copy()\n\n# output the oncology papers\noncology_paper.to_csv('oncology_paper.csv')","cf2f0856":"oncology_paper.head(n=20)","332baeaa":"length_of_covid = len(covid_paper)\nprint(f'There are {length_of_covid} research papers related to COVID-19.')","48c57c4f":"length_of_oncology = len(oncology_paper)\nprint(f'There are {length_of_oncology} research papers related to oncology.')","05f38cb2":"plt.style.use('ggplot')\nplt.figure(figsize = [10, 8])\nplot4 = keywords_stat.plot(kind = 'barh',legend=False)\nplot4.set_xlabel('Number of research papers')\nplot4.set_ylabel('search phrases\/keywords')","cb54a86f":"pd.set_option('max_colwidth', 150)\noutput_title(oncology_paper,'turnaround time')","9f2bfe9b":"output_title(oncology_paper,'biomarker')","8c47c789":"output_title(oncology_paper,'oncology')","4596c8e0":"output_title(oncology_paper,'cancer')","d0ee5781":"## Outline:\n\n* [Summary](#summary)\n* [Strategy](#strategy)\n* [Data processing](#data_processing)\n* [Results](#results)\n    - [Visualizations](#visualizations)\n    - [research papers on oncology](#oncology)      ","125a3d49":"<a id=\"visualizations\"><\/a>\n### Visualizations","4f1c2627":"#### Research paper that contain the keywords, \"turnaround time\"","00dece0f":"# Research paper related to oncology and COVID-19","fafd6d22":"![Screenshot%202020-04-09%20at%2020.32.41.png](attachment:Screenshot%202020-04-09%20at%2020.32.41.png)","63c45beb":"<a id=\"data_processing\"><\/a>\n## Data processing","72fb84c3":"### Top 10 research papers on oncology","2bff5564":"<a id=\"oncology\"><\/a>\n### Research papers on oncology","0cd15d29":"<a id=\"results\"><\/a>\n## Results","02827afd":"#### Research papers that contain the keyword, \"biomarker\"","8efc9f3e":"<a id=\"summary\"><\/a>\n## Summary:\nOut of 48105 research papers available in the database, 4565 papers contain keywords related to COVID-19, within those only 98 papers contains keywords related to oncology ","51891cc4":"Cancer, oncology and biomarker are the most found keywords.","f3c58061":"#### Research papers that contan the keyword, \"cancer\"","cbafa8b3":"<a id=\"strategy\"><\/a>\n## Strategy","25f62e40":"This study takes advandage of the Phrasematcher from the NLP library Spacy, not only individual keywords, but also tokenized phrases can be use to find matched within a tokenized text. In this study, the abstract of the reseachers papers will be tokenized to be searched with custom defined keywords and search phrases.\n\n* Oncology: 'oncology', 'cancer','biomarker','turnaround time','clinical lab'\n\n* COVID-19: 'COVID-19', 'novel coronavirus 2019', '2019 novel coronavirus', 'new coronavirus pneumonia', 'SARS-CoV-2', 'coronavirus disease 2019'\n            \nAs we are interested in the research papers related to the current COVID-19 outbreak, the papers with COVID-19 related will be first selected. The papers are ranked by the number of unique keywords and the the number of sub-topics found in the papers. \n\nIt is possible that a paper can contain keywords\/phrases in multiple sub-topics, which means that this paper covers a broad range of topic of our interest. On the other hand, it is possible that a paper contain multiple keywords or phrases within a particular sub-topics. The higher the number of unique keywords\/phrases, the more likely that paper is relavant to the questions we are interested in. ","cbbc90ab":"#### Research papers that contan the keyword, \"oncology\""}}