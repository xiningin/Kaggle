{"cell_type":{"aeda3150":"code","cc977446":"code","021fc236":"code","2864260a":"code","33f3ac22":"code","a6bd499b":"code","cf704497":"code","41b93a97":"code","6e38fac3":"code","5e40cddb":"code","fc2dbb1e":"code","53409588":"code","3bddce5f":"code","050641c2":"code","66e6824d":"code","e53cf427":"code","08e45ac7":"code","810b5425":"code","bb428bee":"code","86fae214":"code","02d7f777":"code","eac7ede1":"code","a820f442":"code","bc50e0f0":"code","50560dba":"code","6b16cb00":"code","a2d96571":"markdown","af8af210":"markdown","86f70455":"markdown","b1ab6ba1":"markdown","b45b75d6":"markdown","fe596ced":"markdown","54127dbb":"markdown","b851e9b3":"markdown","eb09974c":"markdown","001668eb":"markdown","91b255f4":"markdown","c0a87310":"markdown","24fac46d":"markdown","5ab67bd9":"markdown","51a30695":"markdown","57733d40":"markdown","e3e23b86":"markdown"},"source":{"aeda3150":"SEED = 666\nDEBUG = False\n\n# For Fetaure imprtance - Only if not GRID_SEARCH :\nMAKE_VALUE_PERMUTATION = [\"Elevation\", \"Hydrology_Elevation\", \"slope_o_elevation\", \"firep_m_elev\", \"h_hydro_eps_p_road\"\n    , \"h_hydro_eps_p_fire\", \"Aspect2\", \"Aspect_sin\", \"Slope\", \"Hillshade_Sum\", \"Aspect\", \"Aspect_mod_360\"\n    , \"Hillshade_3pm_clipped\", \"slope_m_elevation\"]\n\nFOLDS = 2 if DEBUG else 5\nMAX_ELEM_IN_RANDOM_GRID_SEARCH = 2 if DEBUG else 16 # time control","cc977446":"# V8\nGRID = {\"learning_rate\":[.15], \"subsample\":[.25], \"l2_leaf_reg\":[50, 100], \"max_leaves\":[223, 255]\n        # low value of Max_bin for better generalization, except for for Elevation and HydrologyxElevation which are very important\n        # 19 is the index of Hydrology_Elevation in the train.columns, \n    , \"border_count\":[96], 'per_float_feature_quantization':[['0:border_count=256', '19:border_count=256']]\n    , \"depth\":[1000]\n    , \"drop_feats\":[\n        ['slope_o_elevation', 'firep_m_elev', 'Hillshade_Noon_clipped_upper', 'Hillshade_3pm_clipped_upper'\n        , 'Hillshade_9am_clipped_upper', 'Hillshade_Noon', 'Hillshade_3pm', 'Hillshade_9am']]}\n\n# V9 # V10\nGRID = {\"learning_rate\":[.15], \"subsample\":[.25], \"l2_leaf_reg\":[100], \"max_leaves\":[255]\n    # low value of Max_bin for better generalization, except for for Elevation and HydrologyxElevation which are very important\n    # 21 is the index of Hydrology_Elevation in the train.columns, \n    , \"border_count\":[96], 'per_float_feature_quantization':[['0:border_count=256', '21:border_count=256']]\n    , \"depth\":[1000]\n    , \"drop_feats\":[['slope_o_elevation', 'firep_m_elev', 'Hillshade_Noon_clipped_upper', 'Hillshade_3pm_clipped_upper'\n            , 'Hillshade_9am_clipped_upper', 'Hillshade_Noon', 'Hillshade_3pm', 'Hillshade_9am']]}\n\nfrom itertools import product\n\n_, values = zip(*GRID.items())\nGRID_SEARCH = len(list(product(*values))) > 1","021fc236":"import os, gc, random, pickle\n\nimport numpy as np \n\nimport pandas as pd\npd.set_option('max_columns', 100)\npd.set_option('max_rows', 200)\n\nimport datatable as dt\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\nimport seaborn as sns\nsns.set(style='darkgrid', context='notebook', rc={'figure.figsize': (16, 12), 'figure.frameon': False})\n\nfrom sklearn.model_selection import StratifiedKFold\nsplits = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom catboost import CatBoostClassifier, Pool","2864260a":"train_data = [[1, 4, 5, 6],\n              [4, 5, 6, 7],\n              [30, 40, 50, 60]]\n\ntrain_labels = [0, 1, 0]\nmodel = CatBoostClassifier(**{\"task_type\": \"GPU\", \"random_seed\": SEED,\n    \"loss_function\": \"MultiClass\", 'eval_metric': \"Accuracy\", \n    \"grow_policy\": \"Lossguide\", \"iterations\":10, \"max_leaves\":255})  \nmodel.fit(train_data,\n          train_labels,\n          verbose=False)\nprint(model.get_all_params())\nprint(model.get_all_params()[\"depth\"])","33f3ac22":"%%time\ntrain = dt.fread('..\/input\/tabular-playground-series-dec-2021\/train.csv').to_pandas()\ntest = dt.fread('..\/input\/\/tabular-playground-series-dec-2021\/test.csv').to_pandas()\nsub = test[[\"Id\"]].copy()\npseudo = dt.fread('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv').to_pandas()","a6bd499b":"for df in [train, test, pseudo]:\n    \n    df[\"Sum_Wilderness_Area\"] = df[[f\"Wilderness_Area{i+1}\" for i in range(4)]].sum(axis=1)\n    df[\"Sum_Soil_Type\"] = df[[f\"Soil_Type{i+1}\" for i in range(40)]].sum(axis=1)\n    \n    # I won't use those features like that\n    df.drop([f\"Wilderness_Area{i+1}\" for i in range(4)] + [f\"Soil_Type{i+1}\" for i in range(40)] + [\"Id\"]\n           , inplace=True, axis=1)\n    \ngc.collect()","cf704497":"def categ_feats(feats, file, name_col):\n    \n    feats.update({...:None})\n    \n    df = dt.fread(file, columns=feats)\n    df.to_csv(\"temp.csv\")\n    \n    #  \";\" to interpret all colomns like a unique column\n    new_col = dt.fread(\"temp.csv\", sep=\";\").to_pandas() \n    new_col.columns=[name_col]\n    new_col[name_col] = new_col[name_col].astype('category')\n    \n    return new_col\n\n\nfor feats, new_col in zip([\n    {f\"Wilderness_Area{i+1}\":f\"Wilderness_Area{i+1}\" for i in range(4)}\n    , {f\"Soil_Type{i+1}\":f\"Soil_Type{i+1}\" for i in range(10)}\n    , {f\"Soil_Type{i+11}\":f\"Soil_Type{i+11}\" for i in range(10)}\n    , {f\"Soil_Type{i+21}\":f\"Soil_Type{i+21}\" for i in range(10)}\n    , {f\"Soil_Type{i+31}\":f\"Soil_Type{i+31}\" for i in range(10)}\n    ], [\"all_wilderness_area\", \"ST_1_10\", \"ST_11_20\", \"ST_21_30\", \"ST_31_40\"]):\n    \n    train = pd.concat([train, categ_feats(feats, '..\/input\/tabular-playground-series-dec-2021\/train.csv', new_col)], axis=1)\n    test = pd.concat([test, categ_feats(feats, '..\/input\/tabular-playground-series-dec-2021\/test.csv', new_col)], axis=1)\n    pseudo = pd.concat([pseudo, categ_feats(feats, '..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv', new_col)], axis=1)","41b93a97":"train.head()","6e38fac3":"for f in [\"all_wilderness_area\", \"ST_1_10\", \"ST_11_20\", \"ST_21_30\", \"ST_31_40\"]:\n    print(f\"Unique values in train for the new categorical feature {f} : {train[f].nunique()}\")","5e40cddb":"def reduce_mem_usage(df, verbose = True):\n    \n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","fc2dbb1e":"def start_at_eps(series, eps=1e-10): return series - series.min() + eps\n\nwilderness = test.columns[test.columns.str.startswith('Wilderness')]\nsoil_type = test.columns[test.columns.str.startswith('Soil_Type')]\nhillshade = test.columns[test.columns.str.startswith('Hillshade')]\n\nall_df = pd.concat([train.assign(ds=0), pseudo.assign(ds=1), test.assign(ds=2)])\n\npos_h_hydrology = start_at_eps(all_df.Horizontal_Distance_To_Hydrology)\npos_v_hydrology = start_at_eps(all_df.Vertical_Distance_To_Hydrology)\n\nall_df = pd.concat([\n        all_df,\n\n#        all_df[wilderness].sum(axis=1).rename('Wilderness_Sum').astype(np.float32),\n#        all_df[soil_type].sum(axis=1).rename('Soil_Type_Sum').astype(np.float32),\n    \n        (all_df.Aspect % 360).rename('Aspect_mod_360'),\n        (all_df.Aspect * np.pi \/ 180).apply(np.sin).rename('Aspect_sin').astype(np.float32),\n        (all_df.Aspect - 180).where(all_df.Aspect + 180 > 360, all_df.Aspect + 180).rename('Aspect2'),\n\n        (all_df.Elevation - all_df.Vertical_Distance_To_Hydrology).rename('Hydrology_Elevation'),\n        all_df.Vertical_Distance_To_Hydrology.apply(np.sign).rename('Water_Vertical_Direction'),\n\n        (pos_h_hydrology + pos_v_hydrology).rename('Manhatten_positive_hydrology').astype(np.float32),\n        (all_df.Horizontal_Distance_To_Hydrology.abs() + all_df.Vertical_Distance_To_Hydrology.abs()).rename('Manhattan_abs_hydrology'),\n        (pos_h_hydrology ** 2 + pos_v_hydrology ** 2).apply(np.sqrt).rename('Euclidean_positive_hydrology').astype(np.float32),\n        (all_df.Horizontal_Distance_To_Hydrology ** 2 + all_df.Vertical_Distance_To_Hydrology ** 2).apply(np.sqrt).rename('Euclidean_hydrology'),\n\n        all_df[hillshade].clip(upper=255).add_suffix('_clipped_upper'),\n        all_df[hillshade].clip(lower=0, upper=255).add_suffix('_clipped'),\n        all_df[hillshade].sum(axis=1).rename('Hillshade_sum'),  \n\n        (all_df.Slope \/ all_df.Elevation).rename('slope_o_elevation'), # ALDPARIS\n        (all_df.Slope * all_df.Elevation).rename('slope_m_elevation'), # ALDPARIS\n        (all_df.Horizontal_Distance_To_Fire_Points * all_df.Elevation).rename('firep_m_elev'), # ALDPARIS\n        (all_df.Horizontal_Distance_To_Roadways * all_df.Elevation).rename('road_m_elev'),\n        (all_df.Vertical_Distance_To_Hydrology * all_df.Elevation).rename('vhydro_elevation'),\n        (all_df.Elevation - all_df.Horizontal_Distance_To_Hydrology * .2).rename('elev_sub_.2_h_hydro').astype(np.float32),\n\n        (all_df.Horizontal_Distance_To_Hydrology + all_df.Horizontal_Distance_To_Fire_Points).rename('h_hydro_p_fire'),\n        (start_at_eps(all_df.Horizontal_Distance_To_Hydrology) + start_at_eps(all_df.Horizontal_Distance_To_Fire_Points)).rename('h_hydro_eps_p_fire').astype(np.float32),\n        (all_df.Horizontal_Distance_To_Hydrology - all_df.Horizontal_Distance_To_Fire_Points).rename('h_hydro_s_fire'),\n        (start_at_eps(all_df.Horizontal_Distance_To_Hydrology) + start_at_eps(all_df.Horizontal_Distance_To_Roadways)).rename('h_hydro_eps_p_road').astype(np.float32),\n\n        (all_df.Horizontal_Distance_To_Fire_Points + all_df.Horizontal_Distance_To_Roadways).abs().rename('abs_h_fire_p_road'),\n        (all_df.Horizontal_Distance_To_Fire_Points - all_df.Horizontal_Distance_To_Roadways).abs().rename('abs_h_fire_s_road'),\n        ], axis=1)\n    \ntypes = {'Cover_Type': np.int8}\ntrain = all_df.loc[all_df.ds == 0].astype(types).drop(columns=['ds'])\npseudo = all_df.loc[all_df.ds == 1].astype(types).drop(columns=['ds'])\ntest = all_df.loc[all_df.ds == 2].drop(columns=['Cover_Type', 'ds'])\n    \ndel all_df, pos_h_hydrology, pos_v_hydrology","53409588":"train = train[train[\"Cover_Type\"] != 5]\n\nif DEBUG:\n    train = train.sample(frac=.05, random_state=SEED)\n    pseudo = pseudo.sample(frac=.1, random_state=SEED)\n    \nle = LabelEncoder()\ny = le.fit_transform(train.Cover_Type)\ny_pseudo = le.transform(pseudo.Cover_Type)\n\ngc.collect()\n    \ntrain = reduce_mem_usage(train)\npseudo = reduce_mem_usage(pseudo)\ntest = reduce_mem_usage(test)\n    \ngc.collect()\n\n# All possible categorical features for catboost\ncat_features = [\"Sum_Wilderness_Area\", \"all_wilderness_area\", \"ST_1_10\", \"ST_11_20\", \"ST_21_30\", \"ST_31_40\"]","3bddce5f":"print(\"Index of Elevation : {}\".format(list(train.columns).index(\"Elevation\")))\nprint(\"Index of Hydrology_Elevation : {}\".format(list(train.columns).index(\"Hydrology_Elevation\")))","050641c2":"def make_trainset(X, X_pseudo, feats, seed = SEED):\n    \n    np.random.seed(seed)\n    \n    ix = np.arange(X.shape[0] + X_pseudo.shape[0])\n    np.random.shuffle(ix)\n    \n    X_trn = pd.concat([X[feats], X_pseudo[feats]], axis=0)\n    y_trn = pd.concat([X[\"Cover_Type\"], X_pseudo[\"Cover_Type\"]], axis=0)\n    \n    return X_trn.iloc[ix], y_trn.iloc[ix]","66e6824d":"def cv_catboost(params, drop_feats=[], early_stopping_rounds=30):\n\n    oof_proba = np.zeros((train.shape[0], len(le.classes_)), dtype=np.float32)\n\n    feats = list(set(list(test.columns)) - set(drop_feats))\n    cat_features_pos = [feats.index(f) for f in cat_features if f in feats]\n    \n    test_proba = np.zeros((test.shape[0], len(le.classes_)), dtype=np.float32)\n\n    accs = [] ; feat_imp = {} ; df_feat_imp = pd.DataFrame(index=feats)\n    \n    if not GRID_SEARCH:\n        fig, ax = plt.subplots(nrows = splits.n_splits, ncols = 3, figsize=(20, 5 * splits.n_splits))\n        plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n\n    for fold, (trn_idx, val_idx) in enumerate(splits.split(train, train[\"Cover_Type\"])): \n    \n        X_trn, y_trn = make_trainset(train.iloc[trn_idx], pseudo, feats, seed = SEED + fold)\n        X_val, y_val = train.iloc[val_idx][feats], train.iloc[val_idx][\"Cover_Type\"]\n    \n        model = CatBoostClassifier(**params)  \n    \n        os.environ['PYTHONHASHSEED'] = str(SEED)\n\n        model.fit(Pool(data = X_trn, label=y_trn, cat_features = cat_features_pos),\n            use_best_model=True,\n            verbose = 100,\n            plot = False,\n            eval_set=Pool(data = X_val, label=y_val, cat_features = cat_features_pos),\n            early_stopping_rounds=early_stopping_rounds)\n    \n        oof_proba[val_idx] = model.predict_proba(X_val)\n        accs.append( accuracy_score(y_val, le.inverse_transform(oof_proba[val_idx].argmax(axis=1))) )\n        print(\"Fold {} - Accuracy {:.5f} - Best iteration : {}\".format(\n            fold + 1, accs[fold], model.get_best_iteration()))\n    \n        del X_trn, y_trn\n        gc.collect()\n        \n        # Test prediction and feature importance by value permutation\n        if not GRID_SEARCH:\n    \n            # Test prediction\n            test_proba += model.predict_proba(test[feats]) \/ splits.n_splits\n        \n            # Feature importance with CatBoost\n            df_temp = pd.DataFrame({'value': model.feature_importances_}, index=feats).sort_values(\"value\", ascending = False)\n            df_temp[-10:].plot.barh(ax=ax[fold, 0])\n            ax[fold, 0].set_title(f\"Catboost less important features - fold n\u00b0{fold+1}\")\n            ax[fold, 0].set_xlabel('Importance')\n            df_feat_imp = pd.concat([df_feat_imp, df_temp.rename({\"value\":f'Fold{fold+1}'}, axis=1)], axis=1)\n\n            # Learning history\n            df_hist = pd.concat([pd.DataFrame(model.evals_result_[\"learn\"]).rename(\n                    {\"Accuracy\":\"Train Accuracy\", \"MultiClass\":\"Train Loss\"}, axis=1)\n                , pd.DataFrame(model.evals_result_[\"validation\"]).rename(\n                    {\"Accuracy\":\"Valid Accuracy\", \"MultiClass\":\"Valid Loss\"}, axis=1)], axis=1)\n            df_hist.index.names=[\"Iteration\"]\n            df_hist=df_hist[:-early_stopping_rounds]\n\n            df_hist[[\"Train Loss\", \"Valid Loss\"]].plot(ax=ax[fold, 1])\n            df_hist[[\"Train Accuracy\", \"Valid Accuracy\"]].plot(ax=ax[fold, 2])\n            for p, t, l in zip(list(range(2)), [\"Loss\", \"Accuracy\"], [[0, .2], [.8, 1.]]):\n                ax[fold, p+1].set_title(f\"CatBoost fit History - fold n\u00b0{fold + 1}\")\n                ax[fold, p+1].set_xlabel('Iteration')\n                ax[fold, p+1].set_ylabel(t)\n                ax[fold, p+1].set(ylim=(l[0], l[1]))\n\n            # Feature importance with value permutation\n            for f, feat in enumerate(feats):\n                if feat in MAKE_VALUE_PERMUTATION or MAKE_VALUE_PERMUTATION == \"all\":\n                    if feat not in feat_imp.keys(): feat_imp[feat] = {}\n                    temp_df = X_val.copy()\n                    temp_df[feat] = np.random.permutation(temp_df[feat])\n                    y_temp = model.predict_proba(temp_df)\n                    y_temp = le.inverse_transform(y_temp.argmax(axis=1))\n                    feat_imp[feat][fold] = accs[fold] - accuracy_score(y_val, y_temp)\n                \n            # Pickle save for later blend of models\n            pickle.dump(oof_proba, open(\"oof_proba.pkl\", \"wb\" ))\n            pickle.dump(test_proba, open(\"test_proba.pkl\", \"wb\" ))\n                    \n            # End if not grid search\n                    \n        del X_val, y_val\n        if \"temp_df\" in locals(): del temp_df, y_temp\n        \n        gc.collect()\n        \n        # Feature importance\n        if GRID_SEARCH and fold == 1: break # beacause it takes many times...\n        # End loop\n                \n    print(\"Mean Accuracy {:.5f} - Std Accuracy {:.5f} - OOF Accuracy {:.5f}\".format(\n        np.mean(accs), np.std(accs), \n            accuracy_score(train[\"Cover_Type\"], le.inverse_transform(oof_proba.argmax(axis=1)))))\n    \n    return {\"acc\":np.mean(accs), \"test_proba\":test_proba, \"oof_proba\":oof_proba, \"feat_imp\":feat_imp, \"df_feat_imp\":df_feat_imp}","e53cf427":"params = {\n    \"task_type\": \"GPU\", \n    \"random_seed\": SEED, # Useless : catboost is not deterministic with GPU...\n    \"loss_function\": \"MultiClass\",\n    'eval_metric': \"Accuracy\", \n    \"grow_policy\": \"Lossguide\",\n    'iterations': 1000, \n    \"learning_rate\" : .2,\n    \"subsample\": .2,\n    \"bootstrap_type\": 'Poisson',\n    \"l2_leaf_reg\": 25,\n    \"max_leaves\": 255,\n}","08e45ac7":"#%%time \n#cv_res = cv_catboost(params)","810b5425":"%%time\n\nall_cv_res = []\n\nkeys, values = zip(*GRID.items())\nlist_comb = list(product(*values))\n\nprint(\"With all GRID ({} combinations) : almost {} hours\".format(len(list_comb), (len(list_comb)*18)\/\/60+1))\nfinal_list_comb = random.sample(list_comb, k = min(MAX_ELEM_IN_RANDOM_GRID_SEARCH, len(list_comb)))\nprint(\"With only {} combinations : almost {} hours\".format(len(final_list_comb), (len(final_list_comb)*18)\/\/60+1))\n\nfor i, comb in enumerate(final_list_comb):\n\n    boost_params = params.copy()\n\n    d = dict(zip(keys, comb))\n    all_cv_res.append(d.copy())\n    d.pop(\"drop_feats\")\n    boost_params.update(d)\n\n    cv_res = cv_catboost(params = boost_params, drop_feats = all_cv_res[i][\"drop_feats\"])\n    \n    y_pred = le.inverse_transform(cv_res[\"oof_proba\"].argmax(axis=1))\n    print(classification_report(train[\"Cover_Type\"], y_pred, digits = 3))\n\n    all_cv_res[i].update({\"acc\":cv_res[\"acc\"], \"oof_acc\":accuracy_score(train[\"Cover_Type\"], y_pred)})\n    print(\"\\nMean CV Accuracy {:.5f} (std:{:.5f}) | OOF Accuracy : {:.5f}\".format(\n        np.mean(cv_res[\"acc\"]), np.std(cv_res[\"acc\"]), accuracy_score(train[\"Cover_Type\"], y_pred)))\n    \nprint(all_cv_res)","bb428bee":"df_all_res_cv = pd.DataFrame(all_cv_res).sort_values(\"acc\", ascending = False)\nprint(df_all_res_cv.iloc[0][\"drop_feats\"])\npickle.dump(df_all_res_cv, open(\"df_all_res_cv.pkl\", \"wb\" ))\n\ndf_all_res_cv","86fae214":"if cv_res[\"df_feat_imp\"].shape[1] > 0:\n    cv_res[\"df_feat_imp\"][\"mean\"] = cv_res[\"df_feat_imp\"].mean(axis=1)\n    cv_res[\"df_feat_imp\"][[\"mean\"]].sort_values(\"mean\", ascending=True).plot(\n        kind = \"barh\", figsize = (15,15), title = \"Mean importance with CatBoost\"\n        , xlabel=\"Importance\", legend = False)","02d7f777":"def plot_feat_imp(feat_imp, drop=[\"Elevation\", \"Hydrology_Elevation\"]):\n    \n    if feat_imp == {}: return \n    \n    nf = len(feat_imp[list(feat_imp.keys())[0]])\n    \n    feat_imp_df = pd.DataFrame(feat_imp).transpose()\n    scores_decrease = [i for i in range(nf)]\n    scores_decrease_sign = [f\"sign_{i}\" for i in range(nf)]\n\n    feat_imp_df[scores_decrease_sign] = 0\n    for score_decrease, score_decrease_sign in zip(scores_decrease, scores_decrease_sign):\n        feat_imp_df.loc[feat_imp_df[score_decrease]>0, score_decrease_sign] = 1\n    \n    feat_imp_df[\"nb_folds\"] = feat_imp_df[scores_decrease_sign].sum(axis=1)\n        \n    feat_imp_df[\"mean\"] = feat_imp_df[list(range(nf))].mean(axis=1)\n    feat_imp_df.sort_values(\"mean\", ascending = False, inplace=True)\n    \n    # drop to much important features\n    if drop is not None:\n        print(\"Very important feature dropped from plot :\")\n        for f in drop:\n            print(\"{} : {:.6f}\".format(f, feat_imp_df.loc[f, \"mean\"]))\n        feat_imp_df.drop(drop, axis=0, inplace = True)\n    \n    fig = plt.figure(figsize = (15, int(feat_imp_df.shape[0] * 2\/3)), constrained_layout=False)\n    gs = fig.add_gridspec(nrows=20, ncols=5, left=0.05, right=0.95,\n                        wspace=0.1, hspace=.1)\n    ax1 = fig.add_subplot(gs[:, 0])\n    ax2 = fig.add_subplot(gs[:, 2:])\n\n    sns.barplot(x=\"nb_folds\",\n            y=feat_imp_df.index,\n            data=feat_imp_df, ax=ax1, color = \"green\")\n    ax1.set_title(\"N folds where feature is important\")\n    ax1.set_xlabel('Nb folds')\n\n    sns.barplot(x = \"mean\", y = feat_imp_df.index ,data = feat_imp_df, ax = ax2, color = \"green\")\n    ax2.set_title(\"Accuracy decrease after values permutation\")\n    \n    ax2.set_xlabel('Accuracy difference after values permutations')","eac7ede1":"plot_feat_imp(cv_res[\"feat_imp\"])","a820f442":"def plot_confusion_matrix(y_true, y_pred, labels):\n    \n    cm = pd.DataFrame(confusion_matrix(y_true, y_pred, labels = labels), index = labels, columns = labels)\n        \n    plots = {\n        'Count':[cm, ',d'],\n        'Accuracy on diagonal (sum of matrix = 100%)': [cm \/ cm.sum().sum(), '.1%'],\n        'Precision on diagonal (sum of each column = 100%)': [cm \/ cm.sum(axis = 0), '.1%'],\n        'Recall on diagonal (sum of each row = 100%)' : [(cm.transpose() \/ cm.sum(axis = 1)).transpose(), '.1%'],\n        # Recall : I don't trust cm \/ cm.sum(axis = 1) from https:\/\/www.kaggle.com\/teckmengwong\/dcnv2-softmaxclassification\n    }\n    \n    fig, ax = plt.subplots(2,2, tight_layout = True, figsize = (15,15))\n    plt.subplots_adjust(hspace = 0.7, wspace = 0.6)\n    ax = ax.flatten()\n\n    for idx, (title, (numbers, fmt)) in enumerate(plots.items()):\n\n        sns.heatmap(\n            data = numbers,\n            cmap = sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar = False,\n            lw = 0.25,\n            annot = True,\n            fmt = fmt,\n            ax = ax[idx]\n        )\n        \n        ax[idx].set_title(title, fontweight='bold')\n        ax[idx].set_ylabel('True label')\n        ax[idx].set_xlabel('Predicted label')","bc50e0f0":"plot_confusion_matrix(train[\"Cover_Type\"], y_pred, le.classes_)","50560dba":"if not GRID_SEARCH:\n    \n    sub[\"Cover_Type\"] = le.inverse_transform(cv_res[\"test_proba\"].argmax(axis=1))\n    sub.to_csv('submission.csv', index=False)","6b16cb00":"if not GRID_SEARCH:\n\n    plt.figure(figsize=(10,3))\n    plt.hist(train[\"Cover_Type\"], bins=np.linspace(0.5, 7.5, 8), density=True, label='Train labels')\n    plt.hist(sub['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, rwidth=0.7, label='Test predictions')\n    plt.xlabel('Cover_Type')\n    plt.ylabel('Frequency')\n    plt.gca().yaxis.set_major_formatter(PercentFormatter(1.0))\n    plt.legend()\n    plt.show();","a2d96571":"# 3. CATBOOST\nInspired by https:\/\/www.kaggle.com\/kaaveland\/tps202112-reasonable-xgboost-model<br>\nI'm using pandas.concat to preserve dtype of features (integer for all cat_features).","af8af210":"## What are default values for CatBoost parameters ?","86f70455":"## Libraries","b1ab6ba1":"# 5. FEATURES IMPORTANCE\n## CatBoost importances features","b45b75d6":"# 1. LIBRARIES & PARAMETERS","fe596ced":"Distribution of the test predictions<br>\nInspired by https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart","54127dbb":"# 4. RANDOMIZED GRID SEARCH","b851e9b3":"# 6. CONFUSION MATRIX \ninspired by :\n* https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart\n* and https:\/\/www.kaggle.com\/teckmengwong\/dcnv2-softmaxclassification (my recall compute is different)","eb09974c":"# 7. SUBMISSION","001668eb":"## Feature Enginnnering","91b255f4":"# 2. DATA & FEATURE ENGINEERING\n## Read data","c0a87310":"<font size=6>Tabular Playgroung Series - december 2021 - with CatBoost and pseudo labels<\/font><br>\nInspired by \n* https:\/\/www.kaggle.com\/kaaveland\/tps202112-reasonable-xgboost-model (thank you rkaveland)\n* https:\/\/www.kaggle.com\/remekkinas\/tps-12-nn-tpu-pseudolabeling-0-95690 (thank you Remek Kinas)\n------------\nIn this notebook I tried to use CatBoost, but even if results are beginning to be decent, there are not as high as with XGBoost.\nWhy CatBoost ? because Soil_Type and Wilderness features could be seen as categorical features.\n- - - - - - -\nWhat's new :<br>\n* CatBoost instead of XGBoost\n* randomized grid search\n* feature importance with value permutation, for a set of features because it takes times. Only if ```GRID_SEARCH == FALSE```\n* degugging mode (small train set for fast execution)\n* some other plots\n____________\nTo do by cross validation :<br>\n* test more parameters\n* drop useless features\n- - - - - - -\n\n|Version|CV accuracy|n folds|Public LB|Note|\n|:--:|--:|--:|--:|--:|\n|V7||2||DEBUG mode. A grid search for max_leaves and l2_leaf_reg|\n|V8||2||A grid search for max_leaves and l2_leaf_reg|\n|V9|0.96188|5|0.95620|max_leaves 255 & l2_leaf_reg 100 **with** a features selction|\n|V10||5||max_leaves 255 & l2_leaf_reg 100 **without** features selction|","24fac46d":"## Grid of parameters\n","5ab67bd9":"## Importance with value permutation for a sample of features","51a30695":"## Reduce memory usage","57733d40":"Create categorical features for Soil_Type and Wilderness_Area features.<br>\nCatboost will run faster with those features instead of original One hot Encoded features.<br>\nScore is quite the same.","e3e23b86":"## Feature engineering for numerci features\nInspired by https:\/\/www.kaggle.com\/kaaveland\/tps202112-reasonable-xgboost-model"}}