{"cell_type":{"3e8575ca":"code","74290458":"code","411b5578":"code","d9ca80b1":"code","6a0973cf":"code","bb71af3e":"code","0c7a3d40":"code","02955615":"code","c13387f2":"code","6ac8e59f":"code","07bfbcb7":"code","dcf32a74":"code","5e3f8f9d":"code","8ee9f80a":"code","ae11149f":"code","844e6ec0":"code","f4318c57":"code","9d8789a4":"code","c3b281f6":"code","f6b062b6":"code","1ca352d5":"code","4cd2f75c":"code","c8adfea1":"code","889ae0c3":"code","69672e54":"code","199b6b83":"code","dfe0b6fc":"code","5657f2e9":"code","2dcf7998":"code","080aa5f0":"markdown","31d70553":"markdown","26558cf6":"markdown","bf13344b":"markdown","c0591452":"markdown","1d7c1024":"markdown","f3ba3a8d":"markdown","e213df26":"markdown","5402deb2":"markdown","cfb318e9":"markdown","102050e9":"markdown","663472da":"markdown","d4487b2d":"markdown","c8e84e79":"markdown","68a1fa35":"markdown","51f1249e":"markdown","1e4dc089":"markdown","ac71210a":"markdown","f989d5e9":"markdown","4c2bf0a4":"markdown","45b7cee5":"markdown","2d746620":"markdown","b9774589":"markdown","8c02461d":"markdown"},"source":{"3e8575ca":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Load Data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n #Concatenate train & test\ntrain_objs_num = len(train)\ny = train['Survived']\ndataset = pd.concat(objs=[train.drop(columns=['Survived']), test], axis=0)\ntrain.head()\ndataset.head()","74290458":"dataset.shape","411b5578":"dataset['Age'].count()","d9ca80b1":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","6a0973cf":"percent.head()","bb71af3e":"dataset.head()","0c7a3d40":"print(dataset.Age.value_counts().sort_values().to_string()) #to print the complete datset","02955615":"# Plot histogram using seaborn\nplt.subplots(figsize=(15,8))\nsns.distplot(dataset.Age, bins =30)","c13387f2":"df = dataset\n# Will drop all features with missing values \ndf.dropna(inplace = True)\ndataset.shape","6ac8e59f":"df1 = dataset\n# Will drop the rows only if all of the values in the row are missing\ndf1.dropna(how = 'all',inplace = True)","07bfbcb7":"df = dataset\n# Will drop a feature that has some missing values.\ndf.dropna(axis = 1,inplace = True)","dcf32a74":"df = dataset\n# Keep only the rows with at least 4 non-na values\ndf.dropna(thresh = 0.4,inplace = True)\ndf.shape","5e3f8f9d":"df = dataset\n#for back fill\ndf.fillna(method='bfill',inplace=True)\n#for forward-fill\ndf.fillna(method='ffill',inplace=True)\ndf.info()","8ee9f80a":"# Replace with a constant value\n# dataframe.Column_Name.fillna(-99,inplace=True)","ae11149f":"df3 = train\ndf3['Age'].isnull().sum()","844e6ec0":"df3.shape","f4318c57":"df3['Age'].mean()","9d8789a4":"df3['Age'].replace(np.NaN,df3['Age'].mean()).head(15)\ndf3.head(15)","c3b281f6":"df4 = train\ndf4['Age'].fillna(df4['Age'].median(),inplace=True)\ndf4.head()","f6b062b6":"data_cat=train\ndata_cat['Embarked'].fillna(data_cat['Embarked'].mode()[0], inplace=True)\ndata_cat.head()","1ca352d5":"data_unique = train\ndata_unique['Cabin'].head(10)","4cd2f75c":"data_unique['Cabin'].fillna('ground').head(10)","c8adfea1":"import missingno as msno","889ae0c3":"msno.matrix(train.sample(150))","69672e54":"msno.heatmap(train)","199b6b83":"msno.bar(train.sample(880))","dfe0b6fc":"msno.dendrogram(train)","5657f2e9":"import pandas_profiling ","2dcf7998":"train.profile_report()","080aa5f0":"You can also select to drop the rows only if all of the values in the row are missing.","31d70553":"## Matrix:\nVisualising missing values for a sample of 150\nUsing this matrix you can very quickly find the pattern of missingness in the dataset.","26558cf6":"### 4. Replacing With Mean\/Median\/Mode\n\nThis strategy can be applied on a feature which has numeric data like the age of a person or the ticket fare. We can calculate the mean, median or mode of the feature and replace it with the missing values. This is an approximation which can add variance to the data set. But the loss of the data can be negated by this method which yields better results compared to removal of rows and columns. Replacing with the above three approximations are a statistical approach of handling the missing values. This method is also called as** leaking the data while training**. Another way is to approximate it with the deviation of neighbouring values. This works better if the data is linear.  \n\n**MEAN:** Suitable for continuous data without outliers","bf13344b":"In summary it is always better to keep data than to delete them.\n\n**The only case that it may worth deleting a variable is when its missing values are more than 60% of the observations but only if that variable is insignificant.Taking this into consideration, imputation is always a preferred choice over deleting variables.**\n\n### Pros:\n- Complete removal of data with missing values results in robust and highly accurate model\n- Deleting a particular row or a column with no specific information is better, since it does not have a high weightage\n\n### Cons:\n- Loss of information and data\n- Works poorly if the percentage of missing values is high (say 30%), compared to the whole dataset\n\n### 2. Back-fill or Forward-fill\nBack-fill or forward-fill to propagate next or previous values respectively.","c0591452":"### Pros:\n* This is a better approach when the data size is small\n* It can prevent data loss which results in removal of the rows and columns\n\n### Cons:\n- Imputing the approximations add variance and bias\n- Works poorly compared to other multiple-imputations method\n\n\n**Note:** Mean, Median and Mode imputation diminishes any correlations involving the variable(s) that are imputed. This is because we assume that there is no relationship between the imputed variable and any other measured variables. Thus, those imputations have some attractive properties for univariate analysis but become problematic for multivariate analysis.\n\n### 5. Assigning An Unique Category\n\nA categorical feature will have a definite number of possibilities, such as gender, for example. Since they have a definite number of classes, we can assign another class for the missing values. Here, the features Cabin and Embarked have missing values which can be replaced with a new category, say, U for \u2018unknown\u2019. This strategy will add more information into the dataset which will result in the change of variance. Since they are categorical, we need to find one hot encoding to convert it to a numeric form for the algorithm to understand it. ","1d7c1024":"## Heatmap\n* The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:","f3ba3a8d":"** Key Note:** NaN value will remain even after forward filling or back filling if a next or previous value isn\u2019t available or it is also a NaN value.\n\n### 3. Replace with constant value\n\nReplace with some constant value outside fixed value range-999,-1 etc\nThis method is useful as it gives the possibility to group missing values as a separate category represented by a constant value. It is a preferred option when it doesn\u2019t make sense to try and predict a missing value. The downside is that performance of linear models can suffer.Use a global constant to fill in for missing values.\nFor example, in the titanic dataset filling in the missing value of the Embarked feature with the most common Port of Embarkation might not really makes sense as opposed to using something like \u201cN\/A\u201d.","e213df26":"As you see above in the fifth row the mean value is replaced in place of NaN.\n\n**MEDIAN :** Suitable for continuous data with outliers","5402deb2":"## Summary\n\nThe take-way from this kernel is:\n\n* The approach to deal with missing values is heavily dependent on the nature of the dataset\n* It is always useful to ask, why there are missing values?Understand the business context.\n* Different ways of handling missing data have difference effects on the performance of models.\n* Useful ready made tools for quicker view of missing values using missingno and Pandas Profiling\n\n# I hope you find this kernel useful in your day to day work.\n# Please do leave your comments and if you like this kernel greatly appreciate an UPVOTE .","cfb318e9":"# Out of Shelve Tools:\n\n## 1. missingno:\nIn the case of a real-world dataset, it is very common that some values in the dataset are missing. We represent these missing values as NaN (Not a Number) values. But to build a good machine learning model our dataset should be complete. That\u2019s why we use some imputation techniques to replace the NaN values with some probable values. But before doing that we need to have a good understanding of how the NaN values are distributed in our dataset.\n\n**Missingno** library offers a very nice way to visualize the distribution of NaN values. Missingno is a Python library and compatible with Pandas.","102050e9":"Sometimes, we may just want to drop a feature that has some missing values.","663472da":"Now let us find out the missing values.","d4487b2d":"Let us look now at some sample observations in the dataset to see how the data is looking like .","c8e84e79":"**Mode:** For categorical feature we can select to fill in the missing values with the most common value(mode) as illustrated below.","68a1fa35":"In the above sample it is observed that NaN are present.\n\nIn general missing values could be: NaN, empty string, ?,-1,-99,-999 and so on. In order to understand if -1 is a missing value or not we could draw a histogram. If this variable has a uniform distribution between 0 and 1 and it has a small peak at -1 then -1 is actually a missing value.\n\nMissing values can be hidden from us and by hidden mean replaced by some other value beside NaN. Therefore, it is always beneficial to plot a histogram in order to identify those values.\n\nSo as an example let us plot a histogram for Age feature as it is a numerical feature.","51f1249e":"## 2.PandasProfiling \nPython package to help understand data quickly","1e4dc089":"## Missing values Treatment Techniques\n\n#### 1.Deleting Data \nThis method is the most commonly used to handle the null values. Here, we either delete a particular record if it has a null value for a particular feature ,and a particular feature if it has more than 70-75% of missing values. This method is advised only when there are enough samples in the data set. \n\nNote : One has to make sure that after we have deleted the data, there is no addition of bias. Removing the data will lead to loss of information which will not give the expected results while predicting the output.\n\nImagine we drop one whole observation just because one of the features had a missing value, even if the rest of the features are perfectly filled and informative!","ac71210a":"### Pros:\n\n* Less possibilities with one extra category, resulting in low variance after one hot encoding \u2014 since it is categorical\n* Negates the loss of data by adding an unique category\n\n### Cons:\n\n* Adds less variance\n* Adds another feature to the model while encoding, which may result in poor performance\n","f989d5e9":"Firstly we cannot simply ignore missing values in a dataset. We must handle them in some way for the very practical reason that most algorithms do not accept missing values.\n\n\"Common sense\" is not sensible here.From my experience the 2 most commonly recommended ways of dealing with missing data actually are not accurate .\n\nThey are:\n\n1. Dropping observations that have missing values\n2. Imputing the missing values based on other observations\n\nDropping missing values is sub-optimal because when we drop observations, we drop information.\n\nThe fact that the value was missing may be informative in itself.We need to understand business deeper to uncover why this information is missing in real world problems.In real time problems we need to make predictions even if some of the features are missing !!!.\n\n\nImputing missing values is sub-optimal because the value was originally missing but you filled it in, which always leads to a loss in information, no matter how sophisticated our imputation method is.\n\n\"<b>Missingness<\/b>\" is almost always informative in itself, and we should tell our algorithm if a value was missing.Even if we build a model to impute our values, we are not adding any real information. You\u2019re just reinforcing the patterns already provided by other features.\n\nBasically, **there are three categories of missing data**. We assume that each record or observation can be divided into an \"observable component\" and an \"unobservable component\". We also assume that the records are independent and identically distributed.\n\n1. **MCAR (Missing Completely At Random)** where the pattern of missinginess is statistically independent of the data record. Example: you have a data set on a piece of paper and you spill coffee on the paper destroying part of the data.\n\n2. **MAR (Missing At Random)** where the probability distribution of the pattern of missingness is functionally dependent upon the observable component in the record. MCAR is a special case of MAR. Example: you have a question on a survey asking if the survey participant is a drug addict and another question which asks if the survey participant has less than one alcoholic drink per year. Assume the answer to the alcoholic drink question is always observable, then the probability that someone fails to answer the drug addict question is most likely functionally dependent upon their answer to the alcoholic drink question.\n\n3. **MNAR (Missing Not at Random)** which is defined as the case which is NOT MAR. In the MNAR case, you can have situations where both the drug addict and alcoholic drink questions are absent in the same record. Another example, is a case where the question is: \"What is your gender?\" Suppose that females are less likely to answer this question than males. This is another example of an MNAR question because the probability that the answer is observable is conditionally dependent upon the unobservable component of the data record.\n\nNow that we know the basic terminology,I like to share some strategies and recommendations\n\n### Important Strategies:\n\n1. We should never insert mean, mode, median, max, min or anything else for missing values. That is, avoid deterministic imputation even though it is widely used and available in most software packages. It underestimates and distorts the statistical regularities (e.g., underestimates variance is one example) present in the data sample. \n\n2. If the data records are MCAR Then you can delete records with missing data.\n\n3. If the data records are MCAR, then sometimes you can stochastically impute the missing values rather than deterministically impute them. So this means that if you specify the marginal probability distribution of a missing value as Gaussian with some known mean and some known variance then you can sample from that distribution to impute values into the data set. We need to be careful and do some additional research  and analysis on data ,understand the business completely and take a judicious decision.\n\n4. If the data is MAR then an algorithm such as Expectation Maximization can be used to handle the missing observations.\n\n5. If the data is MNAR we can include binary indicators in the data record which explicitly identify when a variable is not observable. The challenge with this approach is that a highly nonlinear model needs to be designed to properly integrate this information in an appropriate manner. This might work in a machine learning algorithm where the binary indicators \"disconnect\" the influence of predictors which are not observable. Consequently, the MNAR theory (i.e., the theory of the joint distribution of the complete data record and missing data pattern) is instantiated in the learning machine's probabilistic model of its statistical environment.\n\n\n### Missing numeric data\n\nFor missing numeric data, we should flag and fill the values.\n\n1. Flag the observation with an indicator variable of missingness.\n2. Secondly fill the original missing value with 0 just to meet the technical requirement of no missing values.\n\nBy using this technique of flagging and filling, we are essentially allowing the algorithm to estimate the optimal constant for missingness, instead of just filling it in with the mean.\n\n### Missing categorical data\n\nThe best way to handle missing data for categorical features is to simply label them as \u2019Missing\u2019!\n\n1. We are essentially adding a new class for the feature.\n2. This tells the algorithm that the value was missing.\n3. This also gets around the technical requirement for no missing values.","4c2bf0a4":"# A Simple Tutorial - How to Handle Missing Data ?\n![](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/03\/How-to-Handle-Missing-Values-with-Python.jpg)\n\n## Data in real world are rarely clean and homogeneous. Typically this is because of the following reasons\n\n\n| |Primary Reasons for Missing Data|  \n|-||--|\n|1||Corrupt data|  \n|2||Failure to load information|  \n|3||Incomplete extraction|  \n|4||Noisy, and inconsistent|\n|5||Incomplete data|\n\n \nSo it is an important task of a Data scientist to prepossess the data by filling missing values because making the right decision on how to handle it generates robust data models. It is important to be handled as they could lead to wrong prediction or classification for any given model being used. The goal of this article is to cover the basic techniques for handling missing values in a dataset.\n\nReal-world data often has missing values. \n\nLet us look at the titanic dataset example as a good starting point to under this concept in detail.\n\n\n# Key Objectives\n\nSpecifically, after completing this tutorial you will know:\n\n- How to marking invalid or corrupt values as missing in your dataset.\n- How to remove rows with missing data from your dataset.\n- How to impute missing values with mean values in your dataset.\n\n\nLet\u2019s get started.\n\nFirst let us load the data ","45b7cee5":"Suppose we want to keep only the rows with at least 4 non-na values","2d746620":"#### Bar Chart :\nThis bar chart gives you an idea about how many missing values are there in each column.","b9774589":"We can easily understand from above that features like \u2018Age\u2019, \u2018Fare\u2019, \u2018Cabin\u2019 and \u2018Embarked\u2019 contains missing values. Lets analyse deeper to look at in detail.","8c02461d":"## Dendogram:\nThe dendrogram uses a hierarchical clustering algorithm (courtesy of scipy) to bin variables against one another by their nullity correlation (measured in terms of binary distance). At each step of the tree the variables are split up based on which combination minimizes the distance of the remaining clusters. The more monotone the set of variables, the closer their total distance is to zero, and the closer their average distance (the y-axis) is to zero.\n\nTo interpret this graph, read it from a top-down perspective. Cluster leaves which linked together at a distance of zero fully predict one another's presence\u2014one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record.\n\nCluster leaves which split close to zero, but not at it, predict one another very well, but still imperfectly. If your own interpretation of the dataset is that these columns actually are or ought to be match each other in nullity , then the height of the cluster leaf tells you, in absolute terms, how often the records are \"mismatched\" or incorrectly filed\u2014that is, how many values you would have to fill in or drop, if you are so inclined.\n\nAs with matrix, only up to 50 labeled columns will comfortably display in this configuration. However the dendrogram more elegantly handles extremely large datasets by simply flipping to a horizontal configuration."}}