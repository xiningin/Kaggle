{"cell_type":{"bbb986d1":"code","1acd6b06":"code","89b0012b":"code","b775defc":"code","f83c72b2":"code","6efbfbae":"code","819baa00":"code","cbc7e640":"code","a13a97f9":"code","ffd3166d":"code","7f61c8ff":"code","819ad8cb":"code","68e6a04d":"code","784903ef":"code","1b9c0da7":"code","e276acad":"code","451fc64c":"markdown","f907c6da":"markdown","5f305f9b":"markdown","210dc57b":"markdown","cfd60629":"markdown","ff8c00e0":"markdown","c6ed985b":"markdown","433fbd0e":"markdown","8320c8e5":"markdown","0c418e7e":"markdown","3de3249a":"markdown","8bcefc16":"markdown","8f6d6411":"markdown","cec4bb3a":"markdown","fad8cde0":"markdown","a6209000":"markdown","e59baf79":"markdown","c9106f60":"markdown"},"source":{"bbb986d1":"import pandas as pd\n\n# load the training dataset\ndata = pd.read_csv('..\/input\/microsoftchallenge3\/wine.csv')\ndata.sample(10)","1acd6b06":"data.describe()","89b0012b":"data.isnull().sum()","b775defc":"feature_name = ['Alcohol','Malic_acid','Ash','Alcalinity','Magnesium','Phenols','Flavanoids','Nonflavanoids','Proanthocyanins','Color_intensity','Hue','OD280_315_of_diluted_wines','Proline']\nlabel_name = 'WineVariety'","f83c72b2":"from matplotlib import pyplot as plt\n%matplotlib inline\n\nfor col in feature_name:\n    data.boxplot(column=col,by=label_name,figsize=(6,6))\n    plt.title(col)\n\nplt.show()","6efbfbae":"X, y = data[feature_name].values, data[label_name].values","819baa00":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_nrm = scaler.fit_transform(X)\nprint(X_nrm)","cbc7e640":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_nrm, y, test_size=0.30, random_state=0)\nprint ('Training cases: %d\\nTest cases: %d' % (X_train.shape[0], X_test.shape[0]))","a13a97f9":"### LOGISTICS REGRESSION ###\n\nfrom sklearn.linear_model import LogisticRegression\n\nreg = 0.1\nmodel = LogisticRegression(C=1\/reg, solver='lbfgs', multi_class='auto').fit(X_train, y_train)\nprint(model)","ffd3166d":"from sklearn. metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npredictions = model.predict(X_test)\n\nprint(\"Overall Accuracy:\",accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\",precision_score(y_test, predictions, average='macro'))\nprint(\"Overall Recall:\",recall_score(y_test, predictions, average='macro'))\n\ncm = confusion_matrix(y_test, predictions)\n\nclasses = ['Variety A','Variety B','Variety C']\n\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\n\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.title('Confusion Matrix')\nplt.xlabel(\"Predicted Variety\")\nplt.ylabel(\"Actual Variety\")\n\nplt.show()","7f61c8ff":"X, y = data[feature_name].values, data[label_name].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\nreg = 0.1\nmodel = LogisticRegression(C=1\/reg, solver='lbfgs', multi_class='auto', max_iter = 10000).fit(X_train, y_train)\nprint(model)\n\npredictions = model.predict(X_test)\n\nprint(\"Overall Accuracy:\",accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\",precision_score(y_test, predictions, average='macro'))\nprint(\"Overall Recall:\",recall_score(y_test, predictions, average='macro'))\n\ncm = confusion_matrix(y_test, predictions)\n\nclasses = ['Variety A','Variety B','Variety C']\n\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\n\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.title('Confusion Matrix')\nplt.xlabel(\"Predicted Variety\")\nplt.ylabel(\"Actual Variety\")\n\nplt.show()","819ad8cb":"### DECISSION TREE ###\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)\nprint(model)\n\npredictions = model.predict(X_test)\n\nprint(\"Overall Accuracy:\",accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\",precision_score(y_test, predictions, average='macro'))\nprint(\"Overall Recall:\",recall_score(y_test, predictions, average='macro'))\n\ncm = confusion_matrix(y_test, predictions)\n\nclasses = ['Variety A','Variety B','Variety C']\n\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\n\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.title('Confusion Matrix')\nplt.xlabel(\"Predicted Variety\")\nplt.ylabel(\"Actual Variety\")\n\nplt.show()","68e6a04d":"### SUPPORT VECTOR MACHINE (Linear Kernel) ###\n\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\nprint(model)\n\npredictions = model.predict(X_test)\n\nprint(\"Overall Accuracy:\",accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\",precision_score(y_test, predictions, average='macro'))\nprint(\"Overall Recall:\",recall_score(y_test, predictions, average='macro'))\n\ncm = confusion_matrix(y_test, predictions)\n\nclasses = ['Variety A','Variety B','Variety C']\n\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\n\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.title('Confusion Matrix')\nplt.xlabel(\"Predicted Variety\")\nplt.ylabel(\"Actual Variety\")\n\nplt.show()","784903ef":"### K-NEAREST NEIGHBOURS (K = 5) ###\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors = 5).fit(X_train, y_train)\nprint(model)\n\npredictions = model.predict(X_test)\n\nprint(\"Overall Accuracy:\",accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\",precision_score(y_test, predictions, average='macro'))\nprint(\"Overall Recall:\",recall_score(y_test, predictions, average='macro'))\n\ncm = confusion_matrix(y_test, predictions)\n\nclasses = ['Variety A','Variety B','Variety C']\n\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\n\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.title('Confusion Matrix')\nplt.xlabel(\"Predicted Variety\")\nplt.ylabel(\"Actual Variety\")\n\nplt.show()","1b9c0da7":"### NAIVE BAYES ###\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB().fit(X_train, y_train)\nprint(model)\n\npredictions = model.predict(X_test)\n\nprint(\"Overall Accuracy:\",accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\",precision_score(y_test, predictions, average='macro'))\nprint(\"Overall Recall:\",recall_score(y_test, predictions, average='macro'))\n\ncm = confusion_matrix(y_test, predictions)\n\nclasses = ['Variety A','Variety B','Variety C']\n\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\n\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.title('Confusion Matrix')\nplt.xlabel(\"Predicted Variety\")\nplt.ylabel(\"Actual Variety\")\n\nplt.show()","e276acad":"model = LogisticRegression(C=1\/reg, solver='lbfgs', multi_class='auto', max_iter = 10000).fit(X_train, y_train)\nx_new = np.array([[13.72,1.43,2.5,16.7,108,3.4,3.67,0.19,2.04,6.8,0.89,2.87,1285],\n                  [12.37,0.94,1.36,10.6,88,1.98,0.57,0.28,0.42,1.95,1.05,1.82,520]])\npredictions = model.predict(x_new)\nfor prediction in predictions:\n    print(prediction, '(' + classes[prediction] +')')","451fc64c":"### Normalize Data","f907c6da":"### What if I use original data","5f305f9b":"### Predict New Data","210dc57b":"Your challenge is to explore the data and train a classification model that achieves an overall *Recall* metric of over 0.95 (95%).","cfd60629":"### Try Another Model","ff8c00e0":"WHAT?! Something must be wrong... Hmm","c6ed985b":"# Answer","433fbd0e":"### Train Model","8320c8e5":"Lebih rasional... akurasi model sebesar 98%","0c418e7e":"Secara umum, terlihat seluruh feature\/variabel berpengaruh pada perbedaan WineVariety.","3de3249a":"# Classification Challenge\n\nBy: Muhammad Alwy Shihab (Fresh Graduate of Statistics, Unpad)\n\nWine experts can identify wines from specific vineyards through smell and taste, but the factors that give different wines their individual charateristics are actually based on their chemical composition.\n\nIn this challenge, you must train a classification model to analyze the chemical and visual features of wine samples and classify them based on their cultivar (grape variety).\n\n> **Citation**: The data used in this exercise was originally collected by Forina, M. et al.\n>\n> PARVUS - An Extendible Package for Data Exploration, Classification and Correlation.\nInstitute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno,\n16147 Genoa, Italy.\n>\n> It can be downloaded from the UCI dataset repository (Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository]([http:\/\/archive.ics.uci.edu\/ml). Irvine, CA: University of California, School of Information and Computer Science). ","8bcefc16":"## Explore the data\n\nRun the following cell to load a CSV file of wine data, which consists of 12 numeric features and a classification label with the following classes:\n\n- **0** (*variety A*)\n- **1** (*variety B*)\n- **2** (*variety C*)","8f6d6411":"## Use the model with new data observation\n\nWhen you're happy with your model's predictive performance, save it and then use it to predict classes for the following two new wine samples:\n\n- \\[13.72,1.43,2.5,16.7,108,3.4,3.67,0.19,2.04,6.8,0.89,2.87,1285\\]\n- \\[12.37,0.94,1.36,10.6,88,1.98,0.57,0.28,0.42,1.95,1.05,1.82,520\\]\n","cec4bb3a":"### Split Data","fad8cde0":"## Train and evaluate a model\n\nAdd markdown and code cells as required to to explore the data, train a model, and evaluate the model's predictive performance.","a6209000":"Setelah mencoba beberapa model, didapat model dengan menggunakan Regresi Logistik dan Support Vector Machine mendapat akurasi model paling tinggi, yakni sebesar 98%.","e59baf79":"### Evaluate Model","c9106f60":"### Data Exploration"}}