{"cell_type":{"916b73ed":"code","3d4eb3e8":"code","877d58fb":"code","20270dfd":"code","cb2a36ec":"code","9bd3a89b":"code","52c32783":"code","0a9b8a32":"code","3f21f016":"code","23adbda9":"code","64d9bf58":"code","53d69952":"code","06c8c80b":"code","c80cdcf9":"code","a8ac755b":"code","36a69a18":"code","4e3b0edc":"code","bc0d30fb":"code","fc84ec10":"code","207335b7":"code","8466a654":"code","84afeeeb":"code","1de16a3d":"code","e29ba350":"code","96204acd":"code","077874b4":"code","f59d47e2":"code","958f84df":"code","7bcb6ed5":"code","786f7a14":"code","c4a2311f":"code","10a26cc4":"code","77f94054":"code","cb3f4592":"code","dc97ce29":"code","22210eee":"code","7b500b53":"code","20e154d7":"code","038375f4":"code","62cc7b4c":"code","06501e5f":"code","bdf954ba":"code","de14d175":"code","f5da2a6d":"code","ae467035":"code","51f694a6":"code","e64e113e":"code","b539c883":"code","9ed493c0":"code","4f8f7ffd":"code","99fe691f":"code","1eb25590":"code","0c01e35e":"code","5045740e":"code","6b856349":"code","3fe7a3f4":"code","56ae71be":"code","eee5762f":"code","f7934bed":"code","ac6c6e56":"code","50faf112":"code","164c6707":"code","cafe6bc1":"code","ed9ece69":"code","dddfa4b9":"code","f695cbd1":"code","182dc53c":"code","d3b4f0a1":"code","319942ab":"code","245c8d91":"code","576a0d71":"code","2b0984eb":"code","b26cc7dc":"code","8ae68970":"code","9af73cf6":"code","9cce62bb":"code","ccb256c2":"code","41bf3994":"code","75a35d62":"code","45fca1ce":"code","52293dcf":"code","8c6916bc":"code","89feea44":"code","eb87f61b":"code","2524abd2":"code","ab61e06a":"code","2dd8f8e2":"code","d8f82728":"markdown","a0de3c8c":"markdown","daf68b2a":"markdown","4d0db62f":"markdown","6244ec2a":"markdown","061bdd51":"markdown","b6d91c4e":"markdown","430d6dba":"markdown","98667a6f":"markdown","d52b2425":"markdown","d59b9752":"markdown","26695edd":"markdown","26688628":"markdown","2413f941":"markdown","2e047612":"markdown","7241d8c3":"markdown","e62a3495":"markdown","39887597":"markdown","575dbe18":"markdown","1cb8cfa5":"markdown","099e2ff8":"markdown","5e6cb5de":"markdown","f96b1bb3":"markdown","b0c2445d":"markdown","5e24d5b4":"markdown","c347f3e2":"markdown","7d1271e0":"markdown","28c79373":"markdown","8a740578":"markdown","4f382a68":"markdown","f9b84d4b":"markdown","89be9a1d":"markdown","5336026f":"markdown","3cba1855":"markdown","af880b00":"markdown","6ed94636":"markdown","26d90725":"markdown","06240798":"markdown","43563ccf":"markdown","b2db64ca":"markdown","ed12e15c":"markdown","72a0d73f":"markdown","3b83c6e3":"markdown","8243ce0f":"markdown","a42c3fb0":"markdown","b657a5d9":"markdown","35e9634c":"markdown","dcbd0aa9":"markdown","b4a0185e":"markdown","215c9400":"markdown","6bb7ad78":"markdown","0ea44f3d":"markdown"},"source":{"916b73ed":"# Load the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, log_loss\nfrom sklearn import decomposition\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import auc\nimport math\nimport joblib","3d4eb3e8":"data = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","877d58fb":"data.head()","20270dfd":"data['Churn'].value_counts().plot.bar()\n# We can see that the data set is imbalanced, i.e., not many people have churned.","cb2a36ec":"# Get the percentage of values in 'Churn' column\n(data['Churn'].value_counts()\/data['Churn'].count())*100","9bd3a89b":"# Check if data has missing values\ndata.isnull().sum()","52c32783":"data.dtypes","0a9b8a32":"data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')\ndata.isnull().sum()","3f21f016":"# Separate out the churned and existing customers\nchurned = data.loc[data['Churn']=='Yes']\nexisting = data.loc[data['Churn']=='No']","23adbda9":"g = sns.catplot(x=\"gender\", col=\"Churn\",\n                data=data, kind=\"count\",\n                height=5, aspect=0.8,palette='husl' );","64d9bf58":"g = sns.catplot(x=\"SeniorCitizen\", col=\"Churn\",\n                data=data, kind=\"count\",\n                height=5, aspect=0.8, palette='YlOrBr');","53d69952":"g = sns.catplot(x=\"Partner\", col=\"Churn\",\n                data=data, kind=\"count\",\n                height=5, aspect=0.8);","06c8c80b":"# Tenure vs Churn\nplt.figure(figsize=(9, 4))\nsns.boxplot(x=\"Churn\", y=\"tenure\",\n            hue=\"Churn\", palette=\"pastel\",\n            data=data).set_title(\"tenure vs Churn\", fontsize=15)\nsns.despine(offset=30, trim=True)","c80cdcf9":"# MonthlyCharges vs Churn\nplt.figure(figsize=(9, 4))\nsns.boxplot(x=\"Churn\", y=\"MonthlyCharges\",\n            hue=\"Churn\", palette='Blues',\n            data=data, notch=True).set_title(\"MonthlyCharges vs Churn\", fontsize=15)\nsns.despine(offset=30, trim=True)","a8ac755b":"# TotalCharges vs Churn\nplt.figure(figsize=(9, 6))\nsns.boxplot(x=\"Churn\", y=\"TotalCharges\",\n            hue=\"Churn\", palette=\"muted\",\n            data=data, notch=True).set_title(\"TotalCharges vs Churn\", fontsize=15)\nsns.despine(offset=30, trim=True)","36a69a18":"churned_PS = churned[\"PhoneService\"].value_counts()\nexisting_PS = existing[\"PhoneService\"].value_counts()\ncolors = ['#99ff99','#ffcc99']\nexplode = (0, 0.1)\nfig1, axs = plt.subplots(1, 2)\n\naxs[0].pie(churned_PS, labels=churned_PS.index, autopct='%1.1f%%', shadow=True, startangle=100, colors=colors, explode=explode)\naxs[0].set_title('Churned customers')\n\ncolors = ['#ff9999','#66b3ff']\naxs[1].pie(existing_PS, labels=existing_PS.index, autopct='%1.1f%%', shadow=True, startangle=100, colors=colors, explode=explode)\naxs[1].set_title('Existing customers')\n\nplt.show()","4e3b0edc":"churned_IS = churned[\"InternetService\"].value_counts()\nexisting_IS = existing[\"InternetService\"].value_counts()\n\nfig, axs = plt.subplots(1, 2)\nfig.tight_layout()\n\naxs[0].pie(churned_IS, labels=churned_IS.index, autopct='%1.2f%%', shadow=None)\naxs[0].set_title('Churned customers')\n\naxs[1].pie(existing_IS, labels=existing_IS.index, autopct='%1.2f%%', shadow=None)\naxs[1].set_title('Existing customers')\n\nplt.show()","bc0d30fb":"churned_IS","fc84ec10":"fig, ax =plt.subplots(nrows=1,ncols=3,figsize = (14,4))\nfig.tight_layout(pad=3.0)\nsns.countplot(x ='MultipleLines', hue = \"Churn\", data = data, ax=ax[0],palette=\"rocket\")\nsns.countplot(x ='OnlineBackup', hue = \"Churn\", data = data, ax=ax[1],palette=\"rocket\")\nsns.countplot(x ='OnlineSecurity', hue = \"Churn\", data = data, ax=ax[2],palette=\"rocket\")\n\nfig, ax =plt.subplots(nrows=1,ncols=4,figsize = (14,4))\nfig.tight_layout(pad=3.0)\nsns.countplot(x ='DeviceProtection', hue = \"Churn\", data = data, ax=ax[0],palette=\"Set2\")\nsns.countplot(x ='TechSupport', hue = \"Churn\", data = data, ax=ax[1],palette=\"Set2\")\nsns.countplot(x ='StreamingTV', hue = \"Churn\", data = data, ax=ax[2],palette=\"Set2\")\nsns.countplot(x ='StreamingMovies', hue = \"Churn\", data = data, ax=ax[3],palette=\"Set2\")","207335b7":"!pip install squarify","8466a654":"import squarify\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nplt.subplots_adjust(top=0.95, bottom=0.05, left=0.05, right=0.95, hspace=0.35)\n\nx1 = churned.groupby(['Contract']).MonthlyCharges.count().sort_values(ascending=False)\naxes[0].set_title('Churned customers contract lengths')\nsquarify.plot(sizes=x1.tolist()[:3], label=x1.index.tolist()[:3], alpha=0.6, ax=axes[0])\naxes[0].axis('off')\nx2 = existing.groupby(['Contract']).MonthlyCharges.count().sort_values(ascending=False)\naxes[1].set_title('Existing customers contract lengths')\nsquarify.plot(sizes=x2.tolist()[:3], label=x2.index.tolist()[:3], alpha=0.6, ax=axes[1])\naxes[1].axis('off')\nplt.show()","84afeeeb":"#fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nchurned_paymethods=churned[\"PaymentMethod\"].value_counts()\nexisting_paymethods=existing[\"PaymentMethod\"].value_counts()\n    \nnames=churned_paymethods.index.tolist()\nplt.pie(churned_paymethods, labels=names)\n#plt.show()\n\n# add a circle at the center\nmy_circle=plt.Circle((0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title(\"Churned customer payment methods\")\nplt.show()","1de16a3d":"data.isnull().sum()","e29ba350":"# Split into test and training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\ndata = data.drop(['customerID'], axis=1)\nx = data.drop(['Churn'], axis=1)\ny = data['Churn']\nX_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state=42)\n\n# Replace the null values in 'TotalCharges' with mean value\ntrain_mean = X_train['TotalCharges'].mean()\nX_train['TotalCharges'].fillna(train_mean, inplace=True)\n\ntest_mean = X_test['TotalCharges'].mean()\nX_test['TotalCharges'].fillna(test_mean, inplace=True)","96204acd":"corr = X_train[['TotalCharges', 'MonthlyCharges', 'tenure']].corr()\n# plot the heatmap\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","077874b4":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler()),\n    ])\n\nnum_attribs = ['tenure', 'MonthlyCharges', 'TotalCharges']\ncat_attribs = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService',\n               'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies',\n               'Contract','PaperlessBilling','PaymentMethod']\n\n# handle_unknown = 'ignore' is needed to produce transformed test data with same dimensions as the transformed training data.\nfull_pipeline = ColumnTransformer([\n        ('num', num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(handle_unknown = 'ignore'), cat_attribs),\n    ])","f59d47e2":"data_prepared = full_pipeline.fit_transform(X_train)\ntest_data = full_pipeline.fit_transform(X_test) ","958f84df":"# Install imblearn\nimport sys\n!{sys.executable} -m pip install imblearn\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n\n# summarize class distribution\ncounter = Counter(y_train)\nprint(counter)\n# transform the dataset\noversample = BorderlineSMOTE(random_state=123)\nX, y = oversample.fit_resample(data_prepared, y_train)\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)","7bcb6ed5":"pca = decomposition.PCA(n_components=3)\npca.fit(data_prepared)\ndata_pca = pca.transform(data_prepared)\nfig, ax = plt.subplots()\ncolors = {'Yes':'red','No':'blue'}\nax.scatter(data_pca[:,0], data_pca[:,1], c=y_train.map(colors))\nplt.show()","786f7a14":"pca = decomposition.PCA(n_components=3)\npca.fit(X)\nX_pca = pca.transform(X)\nfig, ax = plt.subplots()\ncolors = {'Yes':'red','No':'blue'}\ny_series = pd.Series(y)\nax.scatter(X_pca[:,0], X_pca[:,1], c=y_series.map(colors))\nplt.show()","c4a2311f":"# Load the models\nRF_model = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_RF_model.pkl\")\nknn_model = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_knn_model.pkl\")\nlog_model = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_logistic_model.pkl\")\nnb_model = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_naive_bayes_model.pkl\")\n\nRF_basemodel = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_RF_basemodel.pkl\")\nknn_basemodel = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_knn_basemodel.pkl\")\nlog_basemodel = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_logistic_basemodel.pkl\")\nnb_basemodel = joblib.load(\"..\/input\/pre-trained-models-for-customer-churn\/telco_naive_bayes_basemodel.pkl\")","10a26cc4":"def tune_svm(x,y):\n  param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001],'kernel': ['linear', 'rbf']}\n  grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n  grid.fit(x,y)\n  return grid\n\nsvm_model = tune_svm(X,y)","77f94054":"joblib.dump(svm_model, \"telco_svm_model.pkl\")","cb3f4592":"y_pred = svm_model.predict(test_data)\nprint (\"best train accuracy\", svm_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","dc97ce29":"print(classification_report(y_test, y_pred))","22210eee":"grid = tune_svm(data_prepared, y_train)","7b500b53":"y_pred = grid.predict(test_data)\nprint (\"best train accuracy\", grid.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","20e154d7":"print(classification_report(y_test, y_pred))","038375f4":"# knn = KNeighborsClassifier()\n# k_range = list(range(1, 31))\n# param_grid = dict(n_neighbors=k_range)\n# grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n# grid.fit(X, y)","62cc7b4c":"# knn_model = grid\n# import joblib\n# joblib.dump(knn_model, \"telco_knn_model.pkl\")","06501e5f":"knn_model.best_params_","bdf954ba":"y_pred = knn_model.predict(test_data)\nprint (\"best train accuracy\", knn_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","de14d175":"print(classification_report(y_test, y_pred))","f5da2a6d":"# knn = KNeighborsClassifier()\n# k_range = list(range(1, 31))\n# param_grid = dict(n_neighbors=k_range)\n# grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n# grid.fit(data_prepared, y_train)","ae467035":"y_pred = knn_basemodel.predict(test_data)\nprint (\"best train accuracy\", knn_basemodel.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","51f694a6":"print(classification_report(y_test, y_pred))","e64e113e":"def tune_log_reg(x,y): \n  logistic = linear_model.LogisticRegression(solver='liblinear')\n\n  # Create regularization penalty space\n  penalty = ['l1', 'l2']\n\n  # Create regularization hyperparameter space\n  C = [100, 10, 1.0, 0.1, 0.01]\n\n  # Create hyperparameter options\n  hyperparameters = dict(C=C, penalty=penalty)\n\n  clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n  best_model = clf.fit(x, y)\n  return best_model\n","b539c883":"#best_model = tune_log_reg(X,y)","9ed493c0":"# logistic_model = best_model\n# import joblib\n# joblib.dump(logistic_model, \"telco_logistic_model.pkl\")","4f8f7ffd":"log_model.best_params_","99fe691f":"y_logistic_pred = log_model.predict(test_data)\nprint (\"best train accuracy\", log_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","1eb25590":"print(classification_report(y_test, y_logistic_pred))","0c01e35e":"#best_model = tune_log_reg(data_prepared,y_train)","5045740e":"y_pred = log_basemodel.predict(test_data)\nprint (\"best train accuracy\", log_basemodel.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","6b856349":"print(classification_report(y_test, y_pred))","3fe7a3f4":"# #Create a Gaussian Classifier\n# model = GaussianNB()\n\n# # Train the model using the training sets\n# model.fit(X,y)","56ae71be":"y_naive_pred = nb_model.predict(test_data)\naccuracy_score(y_test, y_naive_pred)","eee5762f":"print(classification_report(y_test, y_naive_pred))","f7934bed":"# #Create a Gaussian Classifier\n# model = GaussianNB()\n\n# # Train the model using the training sets\n# model.fit(data_prepared,y_train)","ac6c6e56":"y_naive_pred = nb_basemodel.predict(test_data)\naccuracy_score(y_test, y_naive_pred)","50faf112":"print(classification_report(y_test, y_naive_pred))","164c6707":"def tune_random_forest(x,y):\n  model_params = {\n      'n_estimators': [50, 150, 250],\n      'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n      'min_samples_split': [2, 4, 6]\n  }\n\n  # create random forest classifier model\n  rf_model = RandomForestClassifier(random_state=1, oob_score = True)\n\n  # set up grid search meta-estimator\n  clf = GridSearchCV(rf_model, model_params, cv=5)\n\n  # train the grid search meta-estimator to find the best mode\n  clf.fit(x, y)\n  return clf\n\n#RF_model = tune_random_forest(X,y)","cafe6bc1":"RF_model.best_params_","ed9ece69":"y_forest_pred = RF_model.predict(test_data)\nprint (\"best train accuracy\", RF_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","dddfa4b9":"print(classification_report(y_test, y_forest_pred))","f695cbd1":"#RF_basemodel = tune_random_forest(data_prepared,y_train)","182dc53c":"y_forest_pred = RF_basemodel.predict(test_data)\nprint (\"best train accuracy\", RF_basemodel.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","d3b4f0a1":"print(classification_report(y_test, y_forest_pred))","319942ab":"def FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes):\n    layers = []\n\n    nodes_increment = (last_layer_nodes - first_layer_nodes)\/ (n_layers-1)\n    nodes = first_layer_nodes\n    for i in range(1, n_layers+1):\n        layers.append(math.ceil(nodes))\n        nodes = nodes + nodes_increment\n\n    return layers\n\ndef createmodel(n_layers, first_layer_nodes, last_layer_nodes, activation_func):\n    model = Sequential()\n    n_nodes = FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes)\n    for i in range(1, n_layers):\n        if i==1:\n            model.add(Dense(first_layer_nodes, input_dim=X.shape[1], activation=activation_func))\n        else:\n            model.add(Dense(n_nodes[i-1], activation=activation_func))\n\n    #Finally, the output layer should have a single node in binary classification\n    model.add(Dense(1, activation=activation_func))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics = [\"accuracy\"]) #note: metrics could also be 'mse'\n\n    return model\n\n#Wrap model into scikit-learn\nmodel =  KerasClassifier(build_fn=createmodel, verbose = False)\n\nactivation_funcs = ['sigmoid', 'relu', 'tanh']\n#loss_funcs = ['binary_crossentropy','hinge']\nparam_grid = dict(n_layers=[2,3], first_layer_nodes = [64,32,16], last_layer_nodes = [4], \n                  activation_func = activation_funcs, batch_size = [100], epochs = [20,60])\nMLP = GridSearchCV(estimator = model, param_grid = param_grid)\nMLP.fit(X,y)","245c8d91":"MLP.best_params_","576a0d71":"y_mlp_pred = MLP.predict(test_data)\nprint (\"best train accuracy\", MLP.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","2b0984eb":"print(classification_report(y_test, y_mlp_pred))","b26cc7dc":"#Wrap model into scikit-learn\nmodel =  KerasClassifier(build_fn=createmodel, verbose = False)\n\nactivation_funcs = ['sigmoid', 'relu', 'tanh']\n#loss_funcs = ['binary_crossentropy','hinge']\nparam_grid = dict(n_layers=[2,3], first_layer_nodes = [64,32,16], last_layer_nodes = [4], \n                  activation_func = activation_funcs, batch_size = [100], epochs = [20,60])\nbase_mlp = GridSearchCV(estimator = model, param_grid = param_grid)\nbase_mlp.fit(data_prepared, y_train)","8ae68970":"# Classification accuracy on test data\ny_pred = base_mlp.predict(test_data)\nprint (\"best train accuracy\", base_mlp.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","9af73cf6":"# Create the SVM model with the best parameters again, as 'probability=True' was not there while hyperparameter tuning,\n# which is needed for the function 'predict_proba'. \nprint (svm_model.best_params_)\nprint (grid.best_params_)","9cce62bb":"from sklearn.svm import SVC\nsvm = SVC(kernel='rbf', C=1, gamma=1, probability=True).fit(X,y)\nsvm_base = SVC(kernel='rbf', C=10, gamma=0.01, probability=True).fit(data_prepared,y_train)","ccb256c2":"y_svm_pred = svm.predict(test_data)\naccuracy_score(y_test, y_svm_pred)","41bf3994":"y_svm_pred = svm_base.predict(test_data)\naccuracy_score(y_test, y_svm_pred)","75a35d62":"pred_prob1 = RF_model.predict_proba(test_data)\npred_prob2 = knn_model.predict_proba(test_data)\npred_prob3 = log_model.predict_proba(test_data)\npred_prob4 = nb_model.predict_proba(test_data)\npred_prob5 = svm.predict_proba(test_data)\npred_prob6 = MLP.predict_proba(test_data)\n\npred_baseprob1 = RF_basemodel.predict_proba(test_data)\npred_baseprob2 = knn_basemodel.predict_proba(test_data)\npred_baseprob3 = log_basemodel.predict_proba(test_data)\npred_baseprob4 = nb_basemodel.predict_proba(test_data)\npred_baseprob5 = svm_base.predict_proba(test_data)\npred_baseprob6 = base_mlp.predict_proba(test_data)","45fca1ce":"lr_precision1, lr_recall1, _ = precision_recall_curve(y_test, pred_prob1[:,1], pos_label='Yes')\nlr_precision2, lr_recall2, _ = precision_recall_curve(y_test, pred_prob2[:,1], pos_label='Yes')\nlr_precision3, lr_recall3, _ = precision_recall_curve(y_test, pred_prob3[:,1], pos_label='Yes')\nlr_precision4, lr_recall4, _ = precision_recall_curve(y_test, pred_prob4[:,1], pos_label='Yes')\nlr_precision5, lr_recall5, _ = precision_recall_curve(y_test, pred_prob5[:,1], pos_label='Yes')\nlr_precision6, lr_recall6, _ = precision_recall_curve(y_test, pred_prob6[:,1], pos_label='Yes')\n\nlr_precision7, lr_recall7, _ = precision_recall_curve(y_test, pred_baseprob1[:,1], pos_label='Yes')\nlr_precision8, lr_recall8, _ = precision_recall_curve(y_test, pred_baseprob2[:,1], pos_label='Yes')\nlr_precision9, lr_recall9, _ = precision_recall_curve(y_test, pred_baseprob3[:,1], pos_label='Yes')\nlr_precision10, lr_recall10, _ = precision_recall_curve(y_test, pred_baseprob4[:,1], pos_label='Yes')\nlr_precision11, lr_recall11, _ = precision_recall_curve(y_test, pred_baseprob5[:,1], pos_label='Yes')\nlr_precision12, lr_recall12, _ = precision_recall_curve(y_test, pred_baseprob6[:,1], pos_label='Yes')\n\nfig, axes = plt.subplots(2, 1, figsize=(15,20))\naxes[0].plot(lr_recall1, lr_precision1, linestyle='--',color='orange', label='Random Forest')\naxes[0].plot(lr_recall2, lr_precision2, linestyle='--',color='green', label='KNN')\naxes[0].plot(lr_recall3, lr_precision3, linestyle='--',color='blue', label='Logistic Regression')\naxes[0].plot(lr_recall4, lr_precision4, linestyle='--',color='red', label='Naive Bayes')\naxes[0].plot(lr_recall5, lr_precision5, linestyle='--',color='yellow', label='SVM')\naxes[0].plot(lr_recall6, lr_precision6, linestyle='--',color='brown', label='Neural Network')\naxes[0].set_ylim([0.0, 1])\n# title\naxes[0].set_title('SMOTE PR curve')\n# x label\naxes[0].set_xlabel('Recall')\n# y label\naxes[0].set_ylabel('Precision')\naxes[0].legend(loc='best')\n\naxes[1].plot(lr_recall7, lr_precision7, linestyle='--',color='orange', label='Random Forest')\naxes[1].plot(lr_recall8, lr_precision8, linestyle='--',color='green', label='KNN')\naxes[1].plot(lr_recall9, lr_precision9, linestyle='--',color='blue', label='Logistic Regression')\naxes[1].plot(lr_recall10, lr_precision10, linestyle='--',color='red', label='Naive Bayes')\naxes[1].plot(lr_recall11, lr_precision11, linestyle='--',color='yellow', label='SVM')\naxes[1].plot(lr_recall12, lr_precision12, linestyle='--',color='brown', label='Neural Network')\n\n# title\naxes[1].set_title('PR curve')\n# x label\naxes[1].set_xlabel('Recall')\n# y label\naxes[1].set_ylabel('Precision')\naxes[1].legend(loc='best')","52293dcf":"# reliability diagram\nfop1, mpv1 = calibration_curve(y_test, pred_baseprob1[:,1], n_bins=10, normalize=True)\nfop2, mpv2 = calibration_curve(y_test, pred_baseprob2[:,1], n_bins=10, normalize=True)\n# plot perfectly calibrated\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot model reliability\npyplot.plot(mpv1, fop1, marker='.')\npyplot.plot(mpv2, fop2, marker='.')\npyplot.show()","8c6916bc":"rf_cal = RF_basemodel.best_estimator_\nrf_calibrated = CalibratedClassifierCV(rf_cal, method='sigmoid', cv=5)\nrf_calibrated.fit(data_prepared, y_train)\n# predict probabilities\nprobs_rf = rf_calibrated.predict_proba(test_data)[:, 1]\n# reliability diagram\nfop3, mpv3 = calibration_curve(y_test, probs_rf, n_bins=10, normalize=True)\n\nsvm_cal = svm_base\nsvm_calibrated = CalibratedClassifierCV(svm_cal, method='sigmoid', cv=5)\nsvm_calibrated.fit(data_prepared, y_train)\n# predict probabilities\nprobs_svm = svm_calibrated.predict_proba(test_data)[:, 1]\n# reliability diagram\nfop4, mpv4 = calibration_curve(y_test, probs_svm, n_bins=10, normalize=True)\n\n# plot perfectly calibrated\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot model reliability\npyplot.plot(mpv3, fop3, marker='.')\npyplot.plot(mpv4, fop4, marker='.')\npyplot.show()","89feea44":"lr_precision7, lr_recall7, _ = precision_recall_curve(y_test, probs_rf, pos_label='Yes')\nlr_precision8, lr_recall8, _ = precision_recall_curve(y_test, pred_baseprob2[:,1], pos_label='Yes')\nlr_precision9, lr_recall9, _ = precision_recall_curve(y_test, pred_baseprob3[:,1], pos_label='Yes')\nlr_precision10, lr_recall10, _ = precision_recall_curve(y_test, pred_baseprob4[:,1], pos_label='Yes')\nlr_precision11, lr_recall11, _ = precision_recall_curve(y_test, probs_svm, pos_label='Yes')\nlr_precision12, lr_recall12, _ = precision_recall_curve(y_test, pred_baseprob6[:,1], pos_label='Yes')\n\n#fig, axes = plt.subplots(2, 1, figsize=(12, 16))\nplt.figure(figsize=(15,10))\nplt.plot(lr_recall7, lr_precision7, linestyle='--',color='orange', label='Random Forest')\nplt.plot(lr_recall8, lr_precision8, linestyle='--',color='green', label='KNN')\nplt.plot(lr_recall9, lr_precision9, linestyle='--',color='blue', label='Logistic Regression')\nplt.plot(lr_recall10, lr_precision10, linestyle='--',color='red', label='Naive Bayes')\nplt.plot(lr_recall11, lr_precision11, linestyle='--',color='yellow', label='SVM')\nplt.plot(lr_recall12, lr_precision12, linestyle='--',color='brown', label='Neural Network')\nplt.ylim([0.0, 1])\n# title\nplt.title('PR curve')\n# x label\nplt.xlabel('Recall')\n# y label\nplt.ylabel('Precision')\nplt.legend(loc='best')","eb87f61b":"a = y_test.to_list()\npred_base1 = RF_basemodel.predict(test_data)\npred_base2 = knn_basemodel.predict(test_data)\npred_base3 = log_basemodel.predict(test_data)\npred_base4 = nb_basemodel.predict(test_data)\npred_base5 = svm_base.predict(test_data)\npred_base6 = base_mlp.predict(test_data)\n\nauc_score7 = round(auc(lr_recall7, lr_precision7),2)\nrf_f1 = round(f1_score(a, pred_base2, pos_label='Yes'),2)\n\nauc_score8 = round(auc(lr_recall8, lr_precision8),2)\nknn_f1 = round(f1_score(a, pred_base2, pos_label='Yes'),2)\n\nauc_score9 = round(auc(lr_recall9, lr_precision9),2)\nlog_reg_f1 = round(f1_score(a, pred_base3, pos_label='Yes'),2)\n\nauc_score10 = round(auc(lr_recall10, lr_precision10),2)\nnb_f1 = round(f1_score(a, pred_base4, pos_label='Yes'),2)\n\nauc_score11 = round(auc(lr_recall11, lr_precision11),2)\nsvm_f1 = round(f1_score(a, pred_base5, pos_label='Yes'),2)\n\nauc_score12 = round(auc(lr_recall12, lr_precision12),2)\nmlp_f1 = round(f1_score(a, pred_base6, pos_label='Yes'),2)","2524abd2":"voting_clf = VotingClassifier(estimators=[('RF', RF_model.best_estimator_), ('knn', knn_model.best_estimator_), ('LogReg', log_model.best_estimator_), ('NB', nb_model),\n                                          ('SVM', svm_base)], voting='soft')\nvoting_clf.fit(data_prepared, y_train)\npreds = voting_clf.predict(test_data)\n\na = y_test.to_list()\nb = preds.tolist()\nacc = accuracy_score(a, b)\nf1 = round(f1_score(a, b, pos_label='Yes'),2)\n\n# PR AUC score\nprobs = voting_clf.predict_proba(test_data)\nmodel_probs = probs[:, 1]\n# calculate the precision-recall auc\nprecision, recall, _ = precision_recall_curve(y_test, model_probs, pos_label='Yes')\nauc_score = round(auc(recall, precision),2)\nprint (\"Statistics for the voting classsifier:-\")\nprint(\"Accuracy is: \" + str(acc))\nprint(\"F1 Score is: \" + str(f1))\nprint('Voting classifier PR AUC: %.3f' % auc_score)","ab61e06a":"models = ['Random Forest', 'KNN', 'Log Reg', 'Naive Bayes', 'SVM', 'Neural Network', 'Ensemble model']\nF1_scores = [rf_f1, knn_f1, log_reg_f1, nb_f1, svm_f1, mlp_f1, f1]\npr_auc = [auc_score7, auc_score8, auc_score9, auc_score10, auc_score11, auc_score12, auc_score]\n\nx = np.arange(len(models))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(12,8))\nrects1 = ax.bar(x - width\/2, F1_scores, width, label='F1 scores')\nrects2 = ax.bar(x + width\/2, pr_auc, width, label='PR AUC')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\n#fig.figure()\nax.set_ylim([0.0, 1])\nax.set_ylabel('Scores')\nax.set_title('Scores by F1 score and PR AUC')\nax.set_xticks(x)\nax.set_xticklabels(models)\nax.legend(loc='best')\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\n\nfig.tight_layout()\n\n#plt.show()","2dd8f8e2":"y_mlp_pred = base_mlp.predict(test_data)\n\nprint(\"Statistics for the neural network model:-\")\nprint(\"Accuracy is: \" + str(accuracy_score(y_test, y_mlp_pred)))\nprint(\"F1 Score is: \" + str(mlp_f1))\nprint('Voting classifier PR AUC: %.3f' % auc_score12)","d8f82728":"# Precision Recall AUC plot","a0de3c8c":"# Exploratory Data Analysis","daf68b2a":"### Both the existing and churned customers have neraly equal distribution for phone service.","4d0db62f":"### Most of the customers who have churned, did not have a partner.","6244ec2a":"Perform the SMOTE oversampling","061bdd51":"# ## Since it is more important to classify the customers who might churn, we will focus more on precision and recall of the models, and select the final model based on that.","b6d91c4e":"### RF without SMOTE","430d6dba":"### We have 11 null values in 'TotalCharges' column. To impute them, we must first:-\n1. Split into test and training.\n2. Impute any missing values in training set.\n3. Then fit the test data accordingly.","98667a6f":"# Random Forest Classifier","d52b2425":"1. The customer information are plotted w.r.t. to the first two principal components. We can see that there are very few customers who churned(red points).\n2. We also see 2 distinct clusters for customers of both kind. This suggests that there must be some very specific behaviors belonging to customers in the clusters. ","d59b9752":"### We see that there are 11 missing values in the TotalCharges column. When fitting models, we will:-\n1. First split this data into test and train sets\n2. Then impute the missing values in TotalCharges in the train set(if present in training set)\n3. Fill the missing values in test set(if any)","26695edd":"1. Tenure has a very positive correlation with TotalCharges, but also has almost no correlation with MonthlyCharges.","26688628":"### Plot the original training data","2413f941":"### SVM without SMOTE","2e047612":"## Precision Recall Curves","7241d8c3":"## Out of all the 7 models trained, the neural network model is providing the best combination of PR AUC, and F1 scores. Hence, we can finalize that as the best algorithm for this dataset.","e62a3495":"### We can see that the models are performing better when we are NOT doing BorderlineSMOTE sampling.\n### Hence, we will continue with the models trained on the regular dataset(basemodels).","39887597":"# Neural Network Classifier","575dbe18":"# Data Pre-processing","1cb8cfa5":"### Also, the probabilities generated by SVM and Random forest models can be calibrated by the 'CalibratedClassifierCV' method from sklearn.","099e2ff8":"Transform the training data and test data","5e6cb5de":"### Logistic regression without SMOTE sampling","f96b1bb3":"There are no null values in the dataset.","b0c2445d":"### The genders are uniformally distributed in both churning and existing coustomers. Hence, gender cannot be considered a factor which determines whether a customer will churn","5e24d5b4":"# Naive Bayes Classifier","c347f3e2":"# Logistic Regression","7d1271e0":"## Borderline-SMOTE oversampling","28c79373":"### After calibration, we can see that parts of the curves are both above and below the base curve, making them much more balanced.\n### Let's make a new PR curve with the calibrated probabilities:-","8a740578":"## We can see that the dataset is a little imbalanced. \n1. Churned customers = 26.53%\n2. Existing customers = 73.46%\n\n## We shall use SMOTE when fitting models for classification, and see if it provides any improvement.","4f382a68":"## Voting Classifier","f9b84d4b":"1. Almost 70% of the churned customers were using fiber optics for their internet service. Could this have to do something with the reason for their leaving the company services?\n2. Also, nearly 94% customers who left, had even internet services. Could it be that they were dissatisfied by some of the services being provided over the internet?  ","89be9a1d":"### We can see that the both curves are mostly under the curve, meaning that the models have over-forecast i.e., the probabilites are too large.","5336026f":"Create the transformation pipeline","3cba1855":"### Our training data is well balanced now, containing equal samples from both classes(churned and existing customers)","af880b00":"### In the dataset, the 'TotalCharges' column has numerical values, but its type is 'object'. So we will convert it to float.","6ed94636":"### Many of the customers who left, were having high monthly charges. 109 customers are even outliers in Total Charges. \nThere is probably some service which costed more, but did not provide the quality as expected by the customers. \n","26d90725":"## Principal Component Analysis to plot data","06240798":"### We can see that a very large portion of the customers who left were mostly using month-to-month contracts. This probably made it easier for them to leave, as they were not bound to the company with long term contracts.","43563ccf":"### Plot the oversampled data","b2db64ca":"1. Many customers who left, did not have online backup, security, even though 94% had inernet services.\n2. Device Protection and tech support was also not being used in the same proportion as the customers who have stayed.","ed12e15c":"## K nearest neighbors","72a0d73f":"### Loading the models that I had already trained, so that they do not have to run again on Kaggle.","3b83c6e3":"### We can also employ an ensemble learning method, and compare its performance with the previous models.","8243ce0f":"### KNN Without SMOTE sampling","a42c3fb0":"### Plot the F1 scores and Precision-Recall AUC value:-","b657a5d9":"# Model Fitting for classification","35e9634c":"## Support Vector Machine classifier","dcbd0aa9":"### We can see that 75% of the customers who churned(in the right boxplot), have stayed with the company for less than 30 months. \n### This is less than the median of existing customers group, which is around 37 months.\n1. This tells us that most customers who left, did so relatively early.\n2. There are some customers who left after long time, around 70 months(5.8 years). There are even some outliers, exceeding even that mark.","b4a0185e":"1. We can see that the churned customers(red points) are in much larger number now.\n2. Also, notice that the red points are generated in those places where they were in much larger density in the first plot. Hence, Borderline SMOTE makes sure not to generate outlier like samples, as it will increase noise in the dataset.","215c9400":"### Neural Network without SMOTE","6bb7ad78":"## Analyze the numerical columns","0ea44f3d":"### Naive Bayes without SMOTE"}}