{"cell_type":{"b07bb897":"code","cae0903c":"code","0ed16b6c":"code","da3d493d":"code","2b285003":"code","c97bf6f3":"code","225f034b":"code","67d038dc":"code","fccef87d":"code","0f38e176":"code","8821083f":"code","87db4cc3":"code","bee53046":"code","472b5af3":"code","49a1b989":"code","64f5d18b":"code","ba8d39c2":"code","99eff7a8":"code","91bb3bec":"code","9d06bb6d":"code","e4e330d9":"code","6cd34484":"code","313c3ed7":"code","ab2e2912":"code","3f462268":"code","d7aecc5a":"code","e98aca2d":"code","98bbfe6e":"code","b26063bc":"code","1febe14d":"code","df438d12":"code","e19af8eb":"code","0e234171":"code","3c02b731":"code","33a95e0f":"code","fbfcd397":"code","93fc561d":"code","82db29da":"code","86343490":"code","8d50783b":"code","eda5b90a":"code","1f96cddd":"code","bdfa2e7a":"code","d988e927":"code","f1b4a69a":"code","8236a150":"code","cb9f9372":"code","d643339a":"code","5d35b6a6":"code","2f2d0db3":"code","057735db":"code","a602b8c5":"code","f21f6871":"code","dfc645ae":"code","a7b08a27":"code","7bee3d97":"code","b50291f7":"code","b553ed71":"code","59910646":"code","965eda03":"code","c84d8cfe":"code","674267b2":"code","56319c67":"code","f350060f":"code","209abe8b":"markdown","a2c1c1a4":"markdown","b542d30f":"markdown","a2a372b3":"markdown","d8d8985a":"markdown","e759441f":"markdown","a6527926":"markdown","6bd7712b":"markdown","59b06a37":"markdown","046c9cf1":"markdown","cf710fcb":"markdown","c1a55f0e":"markdown","9cfb0c48":"markdown","1d572ec7":"markdown","f6316e0c":"markdown","251eff22":"markdown","ad2cb45b":"markdown","60487d6d":"markdown","14e720c2":"markdown","617fb11b":"markdown","5afe191e":"markdown","3ac8a947":"markdown","49f9dd41":"markdown","6cdf5120":"markdown","b5f0ec55":"markdown","78f6d2c3":"markdown","1e7b0083":"markdown","30c24535":"markdown","0797a46a":"markdown","41090401":"markdown","d455603c":"markdown","2e0e95a9":"markdown","b6ec26a6":"markdown","25e7d484":"markdown","078f4a41":"markdown","c5b54b6b":"markdown","975b31fa":"markdown","3aa5655b":"markdown","3bcfe9a4":"markdown","4ae4e4c9":"markdown","9065508d":"markdown","89c78b3e":"markdown","69fad1fc":"markdown","70624b8a":"markdown","cc8ad955":"markdown","54e10e2e":"markdown","06464d88":"markdown","33f18889":"markdown","1248f794":"markdown","6c3169c0":"markdown","1c21ddb1":"markdown","3bad8f33":"markdown","3e0aa4d6":"markdown","bcaae799":"markdown","e70e58c4":"markdown"},"source":{"b07bb897":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cae0903c":"#We will start will importing all the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score\n\n#we will import warnings\nimport warnings\nwarnings.filterwarnings('ignore')","0ed16b6c":"# We will read our dataset\n#and check for first 5 records.\n\nbike=pd.read_csv(\"\/kaggle\/input\/boombikes\/day.csv\")\nbike.head()","da3d493d":"#check for shape(how many rows and columns we have)\nbike.shape","2b285003":"#check for info)(what are the datatypes)\nbike.info()","c97bf6f3":"#check for columns\nbike.columns","225f034b":"#check for some statistics\nbike.describe()","67d038dc":"#check for null values if any\nbike.isnull().sum()","fccef87d":"g=sns.PairGrid(bike, vars=[\"casual\", \"registered\",\"cnt\"])\ng = g.map(plt.scatter)","0f38e176":"# Drop 'casual' and 'registered'.\nbike.drop('casual',axis=1,inplace=True)\nbike.drop('registered',axis=1,inplace=True)","8821083f":"#It was unnessary there in the data so I drop the instant column and date column to remove the redundancy\nbike.drop('instant',axis=1,inplace=True)\nbike.drop('dteday',axis=1,inplace=True)","87db4cc3":"bike.head()","bee53046":"# Plotting heatmap\nplt.figure(figsize=(16,10))\nax=sns.heatmap(bike.corr(),annot=True,cmap='YlGnBu')\nbottom,top=ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nplt.show()","472b5af3":"# Drop temp\nbike.drop('temp',axis=1,inplace=True)","49a1b989":"#check bike.head()\nbike.head()","64f5d18b":"# we will make pairplot to visualize the numerical variable and checking some linear relation.\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(bike,vars=[\"atemp\",\"hum\",\"windspeed\",\"cnt\"])\nplt.show()","ba8d39c2":"map_season={1:'spring', 2:'summer', 3:'fall', 4:'winter'}\nbike['season']=bike['season'].map(map_season)\nbike.season.value_counts()","99eff7a8":"map_mnth={1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June', 7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'}\nbike['mnth']=bike['mnth'].map(map_mnth)\nbike.mnth.value_counts()","91bb3bec":"map_weekday= {0: 'Sunday', 1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', 5: 'Friday', 6: 'Saturday'}\nbike['weekday']=bike['weekday'].map(map_weekday)\nbike.weekday.value_counts()","9d06bb6d":"map_weathersit={1: 'Clear', 2: 'Mist + Cloudy', 3: 'Light Snow', 4: 'Heavy Rain + Ice Pallets'}\nbike['weathersit']=bike['weathersit'].map(map_weathersit)\nbike.weathersit.value_counts()","e4e330d9":"# to visualize categorical and continous variable we will see boxplots.\nplt.figure(figsize=(20,12))\nplt.subplot(3,3,1)\nsns.boxplot(x='season',y='cnt',data=bike)\nplt.subplot(3,3,2)\nsns.boxplot(x='yr',y='cnt',data=bike)\nplt.subplot(3,3,3)\nsns.boxplot(x='mnth',y='cnt',data=bike)\nplt.subplot(3,3,4)\nsns.boxplot(x='holiday',y='cnt',data=bike)\nplt.subplot(3,3,5)\nsns.boxplot(x='weekday',y='cnt',data=bike)\nplt.subplot(3,3,6)\nsns.boxplot(x='workingday',y='cnt',data=bike)\nplt.subplot(3,3,7)\nsns.boxplot(x='weathersit',y='cnt',data=bike)\nplt.show()","6cd34484":"# we will do one-hot encoding for season\nseason_dum=pd.get_dummies(bike['season'],drop_first=True)\nseason_dum.head()","313c3ed7":"#  we will do one-hot encoding for mnth\nmnth_dum=pd.get_dummies(bike['mnth'],drop_first=True)\nmnth_dum.head()","ab2e2912":"# we will do one-hot encoding for weekday\nweekday_dum=pd.get_dummies(bike['weekday'],drop_first=True)\nweekday_dum.head()","3f462268":"weathersit_dum=pd.get_dummies(bike['weathersit'],drop_first=True)\nweathersit_dum.head()","d7aecc5a":"# after that we will concatenate all the three with our original dataset.\nbike=pd.concat([bike,season_dum,mnth_dum,weekday_dum,weathersit_dum],axis=1)\nbike.head()","e98aca2d":"# As we have dummified columns for mnth,season, weekday,weathersit so now will drop them from our dataset.\nbike.drop('mnth',axis=1,inplace=True)\nbike.drop('season',axis=1,inplace=True)\nbike.drop('weekday',axis=1,inplace=True)\nbike.drop('weathersit',axis=1,inplace=True)","98bbfe6e":"bike.head()","b26063bc":"bike.info()","1febe14d":"# Perform Train and Test split\nbike_train,bike_test=train_test_split(bike,train_size=0.7,random_state=80)\nprint(bike_train.shape)\nprint(bike_test.shape)","df438d12":"#we will do scaling so initialise scaler\nscaler=MinMaxScaler()","e19af8eb":"# It has to be performed on numerical value so will fit_transform on our train data.\nnum_vars=['atemp','hum','windspeed','cnt']\nbike_train[num_vars]=scaler.fit_transform(bike_train[num_vars])\nbike_train.head()","0e234171":"# Describe the numerical variables.\nbike_train[num_vars].describe()","3c02b731":"# Divide train data into X and y.\nX_train=bike_train\ny_train=bike_train.pop('cnt')","33a95e0f":"#Initialise model Instance\nlm=LinearRegression()","fbfcd397":"# we will fit() on train data\nlm.fit(X_train,y_train)","93fc561d":"# Pass model instance and no. of variables to RFE to let the model select features on its own.\nrfe=RFE(lm,15)\nrfe.fit(X_train,y_train)","82db29da":"# theses are the features that model has selected.\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","86343490":"#columns that RFE is supportiing\ncol=X_train.columns[rfe.support_]\ncol","8d50783b":"#columns that RFE does not support.\nX_train.columns[~rfe.support_]","eda5b90a":"# Now will train the model using features that RFE has selected.\nX_train_rfe=X_train[col]\nX_train_rfe=sm.add_constant(X_train_rfe) # adding constant\nlm=sm.OLS(y_train,X_train_rfe).fit()    # Running linear mode;\nprint(lm.summary())","1f96cddd":"#checking VIF of all the variables.\nvif=pd.DataFrame()\nX=X_train_rfe\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","bdfa2e7a":"X_train_new1= X_train_rfe.drop('const',axis=1)","d988e927":"#checking VIF of all the variables.\nvif=pd.DataFrame()\nX=X_train_new1\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","f1b4a69a":"#Dropping Februry and training the model\nX_train_new2=X_train_new1.drop([\"February\"],axis=1)\nX_train_lm2=sm.add_constant(X_train_new2)\nlm=sm.OLS(y_train,X_train_lm2).fit()\nprint(lm.summary())","8236a150":"#check VIF again\nvif=pd.DataFrame()\nX=X_train_new2\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","cb9f9372":"# dropping holiday and training the model.\nX_train_new3=X_train_new2.drop([\"hum\"],axis=1)\nX_train_lm3=sm.add_constant(X_train_new3)\nlm=sm.OLS(y_train,X_train_lm3).fit()\nprint(lm.summary())","d643339a":"#checking VIF\nvif=pd.DataFrame()\nX=X_train_new3\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","5d35b6a6":"#dropping hum and training the model\nX_train_new4=X_train_new3.drop([\"holiday\"],axis=1)\nX_train_lm4=sm.add_constant(X_train_new4)\nlm=sm.OLS(y_train,X_train_lm4).fit()\nprint(lm.summary())","2f2d0db3":"#checking VIF again to check multicollinearity.\nvif=pd.DataFrame()\nX=X_train_new4\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","057735db":"# I decided to drop January also on the basis of p value.\nX_train_new5=X_train_new4.drop([\"January\"],axis=1)\nX_train_lm5=sm.add_constant(X_train_new5)\nlm=sm.OLS(y_train,X_train_lm5).fit()\nprint(lm.summary())","a602b8c5":"#checking VIF\nvif=pd.DataFrame()\nX=X_train_new5\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","f21f6871":"# ro reduce the number of extra variables I am dropping July on the basis of p value.\nX_train_new6=X_train_new5.drop([\"July\"],axis=1)\nX_train_lm6=sm.add_constant(X_train_new6)\nlm=sm.OLS(y_train,X_train_lm6).fit()\nprint(lm.summary())","dfc645ae":"#checking VIF \nvif=pd.DataFrame()\nX=X_train_new6\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","a7b08a27":"#calculting Predicted value of y for train set.\ny_train_pred=lm.predict(X_train_lm6)\ny_train_pred","7bee3d97":"#visualizing error terms.\nfig=plt.figure()\nres=y_train - y_train_pred\nsns.distplot(res,bins=20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)     ","b50291f7":"#So performing scaling on Test by Transform().\nnum_vars=['atemp','hum','windspeed','cnt']\nbike_test[num_vars]=scaler.transform(bike_test[num_vars])\nbike_test.head()","b553ed71":"#Describing numerical variables for the test set.\nbike_test[num_vars].describe()","59910646":"# Decide X and y for the test set.\nX_test=bike_test\ny_test=bike_test.pop('cnt')","965eda03":"# adding constant as our final trained model also having constant.\nX_test_new=X_test[X_train_new6.columns]\nX_test_new=sm.add_constant(X_test_new)\nX_test_new.head()","c84d8cfe":"#predicting Y values on our unseen data(Test data)\ny_test_pred=lm.predict(X_test_new)","674267b2":"#checking R2 for our predicted model.\nr2=r2_score(y_true=y_test,y_pred=y_test_pred)\nprint(r2)","56319c67":"#checking adjusted R squared for our predicted model.\nadj_r2 = 1 - (1-r2)*(len(bike) - 1) \/ (len(bike) - (bike.shape[1] - 1) - 1)\nprint(adj_r2)","f350060f":"#Evaluating the final model.\nfig = plt.figure()\nplt.scatter(y_test,y_test_pred)\nfig.suptitle('y_test vs y_tet_pred', fontsize=20)           # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","209abe8b":"Now If we see the data, \"instant\" is index column which we already have and \"dteday\" is date column but we have \"yr\" and \"mnth\" separately.\nSo drop \"instant\" and \"dtedy\".","a2c1c1a4":"#### Heatmap","b542d30f":"here p value < 0.05 but VIF of hum is highest so Drop \"hum\".","a2a372b3":"we can see that equation of you best fitted line is:\n    \ncnt      =    0.26 * yr  +  0.41* atemp  -  0.13*windspeed  -  0.14*spring  +  0.07*winter  -  0.07*December - 0.07*November                     +                     0.05*September -   0.27* Light Snow  -   0.08*(Mist + Cloudy)","d8d8985a":"#### 4.weathersit","e759441f":"### Step 7.2 Dividing into X_test and y_test","a6527926":"## Importing Required Libraries","6bd7712b":"#### Model 4","59b06a37":"We see that R2 is 84.5% and Adj R2 is 84%.\nwhen we see all the features \"February\" is having p-value >0.05. So we need to drop it.\nwe will see VIF also.","046c9cf1":"## Step 2. Visualising the Data","cf710fcb":"Dropping \"January\" on the basis of p values","c1a55f0e":"## Step 2.1 Encoding the Labels","9cfb0c48":"High Temperature positively impact bike rentals.\n\nAlso Yearly Bike rentals are also increasing.","1d572ec7":"-For mnth we can see that \"cnt\" is first increasing then it start decreasing.\n\n-For \"weekday\" It is almost same.\n\n-Also in year 2019 it seems high.","f6316e0c":"**Thank You for Reading !**\n\n**Your insight would be much appreciated :)**","251eff22":"## Step 4. Splitting the Data into Training and Testing Sets","ad2cb45b":"## Step 1.3 Dropping Redundant columns","60487d6d":"## Step 3. Data Preparation","14e720c2":"whatever preprocessing we have performed on train set, we have to perform on Test set.","617fb11b":"#### VIF < 5 for all the 10 variables.","5afe191e":"## Step 6. Residual Analysis of the train data","3ac8a947":"#### Train model -\n\nR_squared             -   0.831\n\nAdjusted R_squared    -   0.827\n              \n              \n#### Test model -\n\nR_squared             -   0.801\n\nAdjusted R_squared    -   0.793","49f9dd41":"### Step 4.2 Divide data into X and y","6cdf5120":"### Step 5.1 Using RFE for feature Selection","b5f0ec55":"#### 3.weekday","78f6d2c3":"### Step 7.1 Applying the scaling on the test sets","1e7b0083":"### Step 4.1 Scaling the features","30c24535":"Dropping \"July\" on the basis of p value","0797a46a":"#### Adjusted R squared","41090401":"### Step 3.2 Concatenating the Dataframes","d455603c":"## Step 8. Model Evaluation","2e0e95a9":"#### Model 6","b6ec26a6":"We see that the casual and registered have strong correlation with cnt also if we see the dataset we will be able to see clearly that casual + registered = cnt\nso let's drop the \"casual\" and \"registered\".","25e7d484":"#### 2. mnth","078f4a41":"#### Model 1","c5b54b6b":"## Step 7. Making Predictions","975b31fa":"### step3.1 One Hot Encoding","3aa5655b":"Dropping Const","3bcfe9a4":"#### Model 5","4ae4e4c9":"# Problem Statement:\n\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demands\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n# Business Goal:\n\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","9065508d":"checking VIF again","89c78b3e":"#### Model 2","69fad1fc":"## Step 5. Building a linear model","70624b8a":"Here we see that \"atemp\" is highly correlated with \"temp\".\nSo drop one of them, I decided to drop \"temp\"","cc8ad955":"## Step 1.Reading and Understanding Data","54e10e2e":"#### This comes out as my Final Model with 10 variables each with p value <0.05 and seems highly significant.","06464d88":"#### Model 3","33f18889":"#### Pairplot","1248f794":"#### Error terms are normally distributed with zero mean.","6c3169c0":"Dropping \"holiday\" on the basis of p values","1c21ddb1":"#### R Squared","3bad8f33":"#### The top 5 variables that are seen effecting and benefitting the Bike Rental count are as follows:\n1. Temperature : 0.4081\n2. Light Snow  : -0.2717\n3. year        : 0.2601\n4. Spring      : -0.1384\n5. windspeed   : -0.1254","3e0aa4d6":"#### 1. Season","bcaae799":"#### Boxplot","e70e58c4":"## Step 1.2 check for missing values"}}