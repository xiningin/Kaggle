{"cell_type":{"dd2f6048":"code","9594c07d":"code","789c1d81":"code","9f06fc81":"code","816c7257":"code","a14306aa":"code","5bb2d502":"code","c1a667ba":"code","ce7a5d58":"code","97923c19":"code","b24dfba1":"code","f9eaa283":"code","9e106b19":"code","7759d75f":"code","77166bf2":"code","adfd593a":"code","1a6e9e8d":"code","37d972fe":"code","af669115":"code","09f88dac":"code","6fb5b202":"code","0e741637":"markdown","2452c515":"markdown","2f39d404":"markdown","26650f07":"markdown","142dbfa6":"markdown","e5924c73":"markdown","e570f90c":"markdown","96f485f3":"markdown","421f1557":"markdown","aacfb850":"markdown","349bee98":"markdown"},"source":{"dd2f6048":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9594c07d":"from __future__ import division\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\nfrom tqdm import tqdm_notebook \n\nfrom IPython.display import clear_output\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\nimport math\nimport time\nimport tqdm\n\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\n\nimport cv2\n\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","789c1d81":"def to_cpu(tensor):\n    return tensor.detach().cpu()\n\n\ndef load_classes(path):\n    \"\"\"\n    Loads class labels at 'path'\n    \"\"\"\n    fp = open(path, \"r\")\n    names = fp.read().split(\"\\n\")[:-1]\n    return names\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n\ndef rescale_boxes(boxes, current_dim, original_shape):\n    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n    orig_h, orig_w = original_shape\n    # The amount of padding that was added\n    pad_x = max(orig_h - orig_w, 0) * (current_dim \/ max(original_shape))\n    pad_y = max(orig_w - orig_h, 0) * (current_dim \/ max(original_shape))\n    # Image height and width after padding is removed\n    unpad_h = current_dim - pad_y\n    unpad_w = current_dim - pad_x\n    # Rescale bounding boxes to dimension of original image\n    boxes[:, 0] = ((boxes[:, 0] - pad_x \/\/ 2) \/ unpad_w) * orig_w\n    boxes[:, 1] = ((boxes[:, 1] - pad_y \/\/ 2) \/ unpad_h) * orig_h\n    boxes[:, 2] = ((boxes[:, 2] - pad_x \/\/ 2) \/ unpad_w) * orig_w\n    boxes[:, 3] = ((boxes[:, 3] - pad_y \/\/ 2) \/ unpad_h) * orig_h\n    return boxes\n\n\ndef xywh2xyxy(x):\n    y = x.new(x.shape)\n    y[..., 0] = x[..., 0] - x[..., 2] \/ 2\n    y[..., 1] = x[..., 1] - x[..., 3] \/ 2\n    y[..., 2] = x[..., 0] + x[..., 2] \/ 2\n    y[..., 3] = x[..., 1] + x[..., 3] \/ 2\n    return y\n\n\ndef ap_per_class(tp, conf, pred_cls, target_cls):\n    \"\"\" Compute the average precision, given the recall and precision curves.\n    Source: https:\/\/github.com\/rafaelpadilla\/Object-Detection-Metrics.\n    # Arguments\n        tp:    True positives (list).\n        conf:  Objectness value from 0-1 (list).\n        pred_cls: Predicted object classes (list).\n        target_cls: True object classes (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    \"\"\"\n\n    # Sort by objectness\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes = np.unique(target_cls)\n\n    # Create Precision-Recall curve and compute AP for each class\n    ap, p, r = [], [], []\n    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n        i = pred_cls == c\n        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n        n_p = i.sum()  # Number of predicted objects\n\n        if n_p == 0 and n_gt == 0:\n            continue\n        elif n_p == 0 or n_gt == 0:\n            ap.append(0)\n            r.append(0)\n            p.append(0)\n        else:\n            # Accumulate FPs and TPs\n            fpc = (1 - tp[i]).cumsum()\n            tpc = (tp[i]).cumsum()\n\n            # Recall\n            recall_curve = tpc \/ (n_gt + 1e-16)\n            r.append(recall_curve[-1])\n\n            # Precision\n            precision_curve = tpc \/ (tpc + fpc)\n            p.append(precision_curve[-1])\n\n            # AP from recall-precision curve\n            ap.append(compute_ap(recall_curve, precision_curve))\n\n    # Compute F1 score (harmonic mean of precision and recall)\n    p, r, ap = np.array(p), np.array(r), np.array(ap)\n    f1 = 2 * p * r \/ (p + r + 1e-16)\n\n    return p, r, ap, f1, unique_classes.astype(\"int32\")\n\n\ndef compute_ap(recall, precision):\n    \"\"\" Compute the average precision, given the recall and precision curves.\n    Code originally from https:\/\/github.com\/rbgirshick\/py-faster-rcnn.\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    \"\"\"\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef get_batch_statistics(outputs, targets, iou_threshold):\n    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n    batch_metrics = []\n    for sample_i in range(len(outputs)):\n\n        if outputs[sample_i] is None:\n            continue\n\n        output = outputs[sample_i]\n        pred_boxes = output[:, :4]\n        pred_scores = output[:, 4]\n        pred_labels = output[:, -1]\n\n        true_positives = np.zeros(pred_boxes.shape[0])\n\n        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n        target_labels = annotations[:, 0] if len(annotations) else []\n        if len(annotations):\n            detected_boxes = []\n            target_boxes = annotations[:, 1:]\n\n            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n\n                # If targets are found break\n                if len(detected_boxes) == len(annotations):\n                    break\n\n                # Ignore if label is not one of the target labels\n                if pred_label not in target_labels:\n                    continue\n\n                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n                if iou >= iou_threshold and box_index not in detected_boxes:\n                    true_positives[pred_i] = 1\n                    detected_boxes += [box_index]\n        batch_metrics.append([true_positives, pred_scores, pred_labels])\n    return batch_metrics\n\n\ndef bbox_wh_iou(wh1, wh2):\n    wh2 = wh2.t()\n    w1, h1 = wh1[0], wh1[1]\n    w2, h2 = wh2[0], wh2[1]\n    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n    return inter_area \/ union_area\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    \"\"\"\n    Returns the IoU of two bounding boxes\n    \"\"\"\n    if not x1y1x2y2:\n        # Transform from center and width to exact coordinates\n        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] \/ 2, box1[:, 0] + box1[:, 2] \/ 2\n        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] \/ 2, box1[:, 1] + box1[:, 3] \/ 2\n        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] \/ 2, box2[:, 0] + box2[:, 2] \/ 2\n        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] \/ 2, box2[:, 1] + box2[:, 3] \/ 2\n    else:\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n\n    # get the corrdinates of the intersection rectangle\n    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n    # Intersection area\n    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n        inter_rect_y2 - inter_rect_y1 + 1, min=0\n    )\n    # Union Area\n    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n\n    iou = inter_area \/ (b1_area + b2_area - inter_area + 1e-16)\n\n    return iou\n\n\ndef non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n    \"\"\"\n    Removes detections with lower object confidence score than 'conf_thres' and performs\n    Non-Maximum Suppression to further filter detections.\n    Returns detections with shape:\n        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n    \"\"\"\n\n    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n    output = [None for _ in range(len(prediction))]\n    for image_i, image_pred in enumerate(prediction):\n        # Filter out confidence scores below threshold\n        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n        # If none are remaining => process next image\n        if not image_pred.size(0):\n            continue\n        # Object confidence times class confidence\n        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n        # Sort by it\n        image_pred = image_pred[(-score).argsort()]\n        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n        # Perform non-maximum suppression\n        keep_boxes = []\n        while detections.size(0):\n            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n            label_match = detections[0, -1] == detections[:, -1]\n            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n            invalid = large_overlap & label_match\n            weights = detections[invalid, 4:5]\n            # Merge overlapping bboxes by order of confidence\n            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) \/ weights.sum()\n            keep_boxes += [detections[0]]\n            detections = detections[~invalid]\n        if keep_boxes:\n            output[image_i] = torch.stack(keep_boxes)\n\n    return output\n\n\ndef build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n\n    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n\n    nB = pred_boxes.size(0)\n    nA = pred_boxes.size(1)\n    nC = pred_cls.size(-1)\n    nG = pred_boxes.size(2)\n\n    # Output tensors\n    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n\n    # Convert to position relative to box\n    target_boxes = target[:, 2:6] * nG\n    gxy = target_boxes[:, :2]\n    gwh = target_boxes[:, 2:]\n    # Get anchors with best iou\n    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n    best_ious, best_n = ious.max(0)\n    # Separate target values\n    b, target_labels = target[:, :2].long().t()\n    gx, gy = gxy.t()\n    gw, gh = gwh.t()\n    gi, gj = gxy.long().t()\n    # Set masks\n    obj_mask[b, best_n, gj, gi] = 1\n    noobj_mask[b, best_n, gj, gi] = 0\n\n    # Set noobj mask to zero where iou exceeds ignore threshold\n    for i, anchor_ious in enumerate(ious.t()):\n        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n\n    # Coordinates\n    tx[b, best_n, gj, gi] = gx - gx.floor()\n    ty[b, best_n, gj, gi] = gy - gy.floor()\n    # Width and height\n    tw[b, best_n, gj, gi] = torch.log(gw \/ anchors[best_n][:, 0] + 1e-16)\n    th[b, best_n, gj, gi] = torch.log(gh \/ anchors[best_n][:, 1] + 1e-16)\n    # One-hot encoding of label\n    tcls[b, best_n, gj, gi, target_labels] = 1\n    # Compute label correctness and iou at best anchor\n    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n\n    tconf = obj_mask.float()\n    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf","9f06fc81":"# Building Block Layers","816c7257":"class EmptyLayer(nn.Module):\n    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n\n    def __init__(self):\n        super(EmptyLayer, self).__init__()","a14306aa":"class YOLOLayer(nn.Module):\n    \"\"\"Detection layer\"\"\"\n\n    def __init__(self, anchors, num_classes, img_dim=416):\n        super(YOLOLayer, self).__init__()\n        self.anchors = anchors\n        self.num_anchors = len(anchors)\n        self.num_classes = num_classes\n        self.ignore_thres = 0.5\n        self.mse_loss = nn.MSELoss()\n        self.bce_loss = nn.BCELoss()\n        self.obj_scale = 1\n        self.noobj_scale = 100\n        self.metrics = {}\n        self.img_dim = img_dim\n        self.grid_size = 0  # grid size\n\n    def compute_grid_offsets(self, grid_size, cuda=True):\n        self.grid_size = grid_size\n        g = self.grid_size\n        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n        self.stride = self.img_dim \/ self.grid_size\n        # Calculate offsets for each grid\n        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n        self.scaled_anchors = FloatTensor([(a_w \/ self.stride, a_h \/ self.stride) for a_w, a_h in self.anchors])\n        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n\n    def forward(self, x, targets=None, img_dim=None):\n\n        # Tensors for cuda support\n        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n\n        self.img_dim = img_dim\n        num_samples = x.size(0)\n        grid_size = x.size(2)\n\n        prediction = (\n            x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)\n            .permute(0, 1, 3, 4, 2)\n            .contiguous()\n        )\n\n        # Get outputs\n        x = torch.sigmoid(prediction[..., 0])  # Center x\n        y = torch.sigmoid(prediction[..., 1])  # Center y\n        w = prediction[..., 2]  # Width\n        h = prediction[..., 3]  # Height\n        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n\n        # If grid size does not match current we compute new offsets\n        if grid_size != self.grid_size:\n            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n\n        # Add offset and scale with anchors\n        pred_boxes = FloatTensor(prediction[..., :4].shape)\n        pred_boxes[..., 0] = x.data + self.grid_x\n        pred_boxes[..., 1] = y.data + self.grid_y\n        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n\n        output = torch.cat(\n            (\n                pred_boxes.view(num_samples, -1, 4) * self.stride,\n                pred_conf.view(num_samples, -1, 1),\n                pred_cls.view(num_samples, -1, self.num_classes),\n            ),\n            -1,\n        )\n\n        if targets is None:\n            return output, 0\n        else:\n            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n                pred_boxes=pred_boxes,\n                pred_cls=pred_cls,\n                target=targets,\n                anchors=self.scaled_anchors,\n                ignore_thres=self.ignore_thres,\n            )\n\n            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n\n            # Metrics\n            cls_acc = 100 * class_mask[obj_mask].mean()\n            conf_obj = pred_conf[obj_mask].mean()\n            conf_noobj = pred_conf[noobj_mask].mean()\n            conf50 = (pred_conf > 0.5).float()\n            iou50 = (iou_scores > 0.5).float()\n            iou75 = (iou_scores > 0.75).float()\n            detected_mask = conf50 * class_mask * tconf\n            precision = torch.sum(iou50 * detected_mask) \/ (conf50.sum() + 1e-16)\n            recall50 = torch.sum(iou50 * detected_mask) \/ (obj_mask.sum() + 1e-16)\n            recall75 = torch.sum(iou75 * detected_mask) \/ (obj_mask.sum() + 1e-16)\n\n            self.metrics = {\n                \"loss\": to_cpu(total_loss).item(),\n                \"x\": to_cpu(loss_x).item(),\n                \"y\": to_cpu(loss_y).item(),\n                \"w\": to_cpu(loss_w).item(),\n                \"h\": to_cpu(loss_h).item(),\n                \"conf\": to_cpu(loss_conf).item(),\n                \"cls\": to_cpu(loss_cls).item(),\n                \"cls_acc\": to_cpu(cls_acc).item(),\n                \"recall50\": to_cpu(recall50).item(),\n                \"recall75\": to_cpu(recall75).item(),\n                \"precision\": to_cpu(precision).item(),\n                \"conf_obj\": to_cpu(conf_obj).item(),\n                \"conf_noobj\": to_cpu(conf_noobj).item(),\n                \"grid_size\": grid_size,\n            }\n\n            return output, total_loss","5bb2d502":"def pad_to_square(img, pad_value):\n    c,h, w = img.shape\n    dim_diff = np.abs(h - w)\n    # (upper \/ left) padding and (lower \/ right) padding\n    pad1, pad2 = dim_diff \/\/ 2, dim_diff - dim_diff \/\/ 2\n    # Determine padding\n    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n    # Add padding\n    img = F.pad(img, pad, \"constant\", value=pad_value)\n\n    return img, pad","c1a667ba":"def resize(image, size):\n    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n    return image","ce7a5d58":"img = transforms.ToTensor()(Image.open('..\/input\/Image\/1\/0.jpg').convert('RGB'))\nprint(img.size())\nimg,pad = pad_to_square(img,0)\nimg= resize(img,416)\nprint(img.shape)","97923c19":"image = np.transpose(img, (1,2,0))\nplt.imshow(image)","b24dfba1":"cla = []\ndic_class = {}\nfor i in range(4703):\n    filename = '..\/input\/Annotation\/1\/' + str(i) + '.txt'\n#     print(filename)\n    file = open(filename, 'r',encoding=\"utf8\")\n    lines = file.read().split('\\n')\n    lines = [line for line in lines if len(line)>0]\n#     print(lines)\n    for line in lines:\n      cla.append(line.split()[-1])\n    \ncla = list(set(cla))   \nfor j , val in enumerate(cla):\n    if val not in list(dic_class.keys()):\n        dic_class[val] = j\nprint(len(dic_class))\n","f9eaa283":"def labelsprocess(path):\n    file = open(path, 'r',encoding=\"utf8\")\n    lines = file.read().split('\\n')\n    lines = [line for line in lines if len(line)>0]\n    \n    labels = []\n    for i in lines:\n        data = i.split()\n        cl = dic_class[data[-1]]\n        data = data[:-1]\n        data  = list(map(float,data))\n        x1 = min(data[0:4])\n        y1 = min(data[4:8])\n        w = max(data[0:4])-min(data[0:4])\n        h = max(data[4:8])-min(data[4:8])\n        labels.append([cl,x1,y1,w,h])\n        \n    return labels","9e106b19":"tar = labelsprocess('..\/input\/Annotation\/1\/0.txt')","7759d75f":"fr = cv2.imread(\"..\/input\/Image\/1\/0.jpg\")\nframe_rgb = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\nfor i in tar:\n    frame_rgb = cv2.rectangle(frame_rgb, (int(i[1]), int(i[2])), (int(i[1])+int(i[3]), int(i[2])+int(i[4])), (255, 0, 0), 5)\nplt.imshow(frame_rgb)","77166bf2":"def dataprocess(image_path,targets_path):\n    \n    # Image processing\n    \n    img = transforms.ToTensor()(Image.open(image_path).convert('RGB'))\n    _, h, w = img.shape\n    \n    img, pad = pad_to_square(img, 0)\n    _, padded_h, padded_w = img.shape\n        \n    img = resize(img, 416)\n    \n    h_factor,w_factor = (1,1)\n    # label processing\n    targets = None\n    \n    labels = labelsprocess(targets_path)\n    boxes = torch.tensor(labels)\n    \n    x1 = boxes[:,1]\n    y1 = boxes[:,2]\n    x2 = boxes[:,1] + boxes[:,3]\n    y2 = boxes[:,2] + boxes[:,4]\n    \n    # Adjust for added padding\n    x1 += pad[0]\n    y1 += pad[2]\n    x2 += pad[1]\n    y2 += pad[3]\n\n    # Returns (x, y, w, h)\n    boxes[:, 1] = ((x1 + x2) \/ 2) \/ padded_w\n    boxes[:, 2] = ((y1 + y2) \/ 2) \/ padded_h\n    boxes[:, 3] *= w_factor \/ padded_w\n    boxes[:, 4] *= h_factor \/ padded_h\n    \n    \n    targets = torch.zeros((len(boxes), 6))\n    targets[:, 1:] = boxes\n    \n    return img, targets","adfd593a":"yololayer1 = YOLOLayer([(81, 82), (135, 169), (344, 319)],2299,416)\nyololayer2 = YOLOLayer([(23, 27), (37, 58), (81, 82)],2299,416)","1a6e9e8d":"class yolomini(nn.Module):\n    \n    def __init__(self):\n        \n        super(yolomini,self).__init__()\n        \n        self.conv1 = nn.Sequential(\n                        \n                        nn.Conv2d(3,16,3,1,1,bias =False),  # 0\n                        nn.BatchNorm2d(16),\n                        nn.LeakyReLU(0.1 , inplace=True),\n            \n                        )\n        \n                        \n        self.pool1  =   nn.MaxPool2d(2,2,0)                 # 1\n        \n        self.conv2  = nn.Sequential(\n            \n                        nn.Conv2d(16,32,3,1,1,bias=False),  # 2\n                        nn.BatchNorm2d(32),\n                        nn.LeakyReLU(0.1,inplace=True),\n            \n                        )\n        \n        self.pool2 =    nn.MaxPool2d(2,2,0)                #3\n            \n        self.conv3 =  nn.Sequential(\n                        nn.Conv2d(32,64,3,1,1,bias=False), #4\n                        nn.BatchNorm2d(64),\n                        nn.LeakyReLU(0.1,inplace=True),\n            \n                        )\n        \n        self.pool3 =    nn.MaxPool2d(2,2,0)               #5\n            \n        self.conv4 =  nn.Sequential(\n                        nn.Conv2d(64,128,3,1,1,bias=False), #6\n                        nn.BatchNorm2d(128),\n                        nn.LeakyReLU(0.1,inplace=True),\n                        )\n            \n            \n        self.pool4 =  nn.MaxPool2d(2,2,0)               #7\n                        \n        self.conv5 =  nn.Sequential(             \n                        nn.Conv2d(128,256,3,1,1,bias=False),#8\n                        nn.BatchNorm2d(256),\n                        nn.LeakyReLU(0.1,inplace=True),\n                        )\n            \n        self.pool5 =  nn.MaxPool2d(2,2,0)               #9\n            \n        self.conv6 =  nn.Sequential(\n                        nn.Conv2d(256,512,3,1,1,bias=False),#10\n                        nn.BatchNorm2d(512),\n                        nn.LeakyReLU(0.1,inplace=True),\n                        )\n            \n        self.pool6 = nn.Sequential(\n                        nn.ZeroPad2d((0, 1, 0, 1)),\n                        nn.MaxPool2d(2,1,0),               #11\n                        )\n                        \n        self.conv7 = nn.Sequential(\n                        nn.Conv2d(512,1024,3,1,1,bias=False),#12\n                        nn.BatchNorm2d(1024),\n                        nn.LeakyReLU(0.1,inplace=True),\n                        )\n                        \n        self.conv8 = nn.Sequential(\n                        nn.Conv2d(1024,256,1,1,bias=False),#13\n                        nn.BatchNorm2d(256),\n                        nn.LeakyReLU(0.1,inplace=True),\n                        )\n            \n        self.conv9 = nn.Sequential(\n                        nn.Conv2d(256,512,3,1,1,bias=False),#14\n                        nn.BatchNorm2d(512),\n                        nn.LeakyReLU(0.1,inplace=True),\n                        )\n        \n        self.conv10 =   nn.Conv2d(512,6912,1,1)   # 15\n                        \n        \n        self.yolo1 = yololayer1  #16\n          \n        self.route1 = EmptyLayer()  #17\n        \n        self.conv11 = nn.Sequential(\n            \n                        nn.Conv2d(256,128,1,1,bias=False),#18\n                        nn.BatchNorm2d(128),\n                        nn.LeakyReLU(0.1,inplace=True),\n        \n                        )\n        \n        self.upsample1 = nn.Upsample(scale_factor=2,mode='nearest') #19\n        \n        self.route2 = EmptyLayer() #20\n        \n        self.conv12 = nn.Sequential(\n                        \n                        nn.Conv2d(384,256,3,1,1), #21\n                        nn.BatchNorm2d(256),\n                        nn.LeakyReLU(0.1,inplace=True),\n                        ) \n                        \n        self.conv13 = nn.Conv2d(256,6912,1,1) #22\n                        \n        self.yolo2 = yololayer2 #23\n        \n        \n    def forward(self,X,targets = None):\n        img_dim = X.shape[2]\n#         print(img_dim)\n        loss = 0\n        \n        x1 = self.conv1(X)\n        x2 = self.pool1(x1)\n        x3 = self.conv2(x2)\n        x4 = self.pool2(x3)\n        x5 = self.conv3(x4)\n        x6 = self.pool3(x5)\n        x7 = self.conv4(x6)\n#         print(\"x7\",x7.shape)\n        x8 = self.pool4(x7)\n#         print(\"x8\",x8.shape)\n        x9 = self.conv5(x8)\n#         print(\"x9\",x9.shape)\n        x10 = self.pool5(x9)\n#         print(\"x10\",x10.shape)\n        x11 = self.conv6(x10)\n#         print(\"x11\",x11.shape)\n        x12 = self.pool6(x11)\n#         print(\"x12\",x12.shape)\n        x13 = self.conv7(x12)\n#         print(\"x13\",x13.shape)\n        x14 = self.conv8(x13)\n#         print(\"x14\",x14.shape)\n        x15 = self.conv9(x14)\n        x16 = self.conv10(x15)\n        x17,layer_loss = self.yolo1(x16,targets,img_dim)\n        loss += layer_loss\n        x18 = x14\n        x19 = self.conv11(x18)\n        x20 = self.upsample1(x19)\n#         print(x20.shape,x9.shape,x19.shape)\n        x21 = torch.cat([x20,x9],1)\n        x22 = self.conv12(x21)\n        x23 = self.conv13(x22)\n#         print(\"x23\",x23.shape)\n        x24,layer_loss = self.yolo2(x23,targets,img_dim)\n        loss += layer_loss\n        \n        yolo_outputs = to_cpu(torch.cat([x17,x24],1))\n        \n        return yolo_outputs if targets is None else (loss,yolo_outputs)\n        \n        ","37d972fe":"def createbatch(batch_size,batch_start):\n    batch_end = batch_start + batch_size\n    if batch_end <= 4703 :\n        batch = list(range(batch_start,batch_end))\n    else:\n        temp = batch_size - (4703-batch_start)\n        batch = list(range(batch_start,4703)) + list(range(0,temp))\n    images = []\n    boxes =  []\n#     print(batch)\n    for i in batch:\n        image_path  = '..\/input\/Image\/1\/' + str(i) + '.jpg'\n        target_path = '..\/input\/Annotation\/1\/' + str(i) + '.txt'\n#     print(image_path,target_path)\n        img,targets = dataprocess(image_path,target_path)\n        images.append(img)\n        boxes.append(targets)\n#     print(type(boxes))\n    for j, b in enumerate(boxes):\n            b[:, 0] = j\n    targets = torch.cat(boxes, 0)\n    imgs = torch.stack(images)\n    \n    return imgs, targets","af669115":"imgs,targets = createbatch(10,4703)","09f88dac":"#Training Model","6fb5b202":"Epoch = 100\nbatch_size = 2\nbatch_start = np.random.randint(0,4703)\nmodel = yolomini()\nmodel.to(device)\noptimizer = optim.SGD(model.parameters(),lr=0.0001)\nloss_epoch = []\nfor num_epoch in tqdm_notebook(range(Epoch), total=Epoch, unit=\"Epoch\"):\n    Loss_batch = []\n    N_batch = 1000\/\/batch_size\n    for num_batch in tqdm_notebook(range(N_batch),total=N_batch , unit =\"Batch\"):\n        imgs,targets = createbatch(batch_size,batch_start)\n        imgs = imgs.to(device)\n        targets = targets.to(device)\n        loss,output = model(imgs,targets)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        Loss_batch.append(loss)\n        batch_start = batch_start + batch_size\n        torch.cuda.empty_cache()\n    clear_output(wait=True)\n    plt.plot(Loss_batch)\n    plt.title(\"Loss v\/s batches\")\n    plt.xlabel(\"Bat\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n    break\n    loss_epoch.append(loss)\nplt.plot(loss_epoch)\nplt.title(\"loss v\/s epoch\")\nplt.xlabel('epochs')\nplt.ylabel('Loss')\nplt.show()","0e741637":" Preprocessing the images","2452c515":"converting Annotation data as per yolo format\n","2f39d404":"**Yolo3 Network**\n\nObject Detection is one of key application for different real time uses such as Auto Driving , AI based monitoring at airports etc , Object classification. \nThere has been many Deep learning networks developed by the Data scientist such Fast RCNN , Retina net , Yolt , YOLO etc. You can check out the list below:\nhttps:\/\/github.com\/amusi\/awesome-object-detection \n\nEven though the real work done by the these scientist , however it is still difficult to comprehend the entire code and diffcult to apply the same for custom data set. \nHere I have tried to break down the complexity of the YOLO3 network and write into a simple form which can be applied to custom data set.\n\nTo understand Yolo on conceptual level please visit :\nhttps:\/\/blog.paperspace.com\/how-to-implement-a-yolo-object-detector-in-pytorch\/\n\nHowever it will not guide you to train your model from scarch with custom data. \n\nI have followed the work of Erik LinderNoren and used some code from his github repository:\nhttps:\/\/github.com\/eriklindernoren\/PyTorch-YOLOv3 \n\nThe Original paper can be checked at \nYOLOv3: An Incremental Improvement by Joseph Redmon, Ali Farhadi 2018 Journal arXiv\n","26650f07":"Required Libraries","142dbfa6":"Create Data Loader","e5924c73":"Here I have developed a pytorch implementation of mini yolo3 netowork . \n\nIf you want to apply yolo3 to your custom data you need to check the number of classes in you data set and what is the image dimention. \n\nAlthough we can develop yolo3 network for any size of image however in our case we need to have a image with square dimention also pixel size divisible by 32. Why?\n\nThe image dimentions are being reduce 32 times in yolo network and at one stage the out put of one layer is being concatenated with another layer output. Before Concatenation the output of last layer dimentions has been upsample to meet the dimention of layers output from which concatenation has to be done. Suppose you choose 600 size . After 8th times reduction the size will 75. After that if we tried to reduce it further the system will take round value of 37. Now if we upsample this 37 size to 2times it will be 74 which will create problem in concatenation. \n\nTherefore you need to resize the image at 416 or multiple of 32. \n\nthe Annotation should be like this :\n[class Xmid Ymid w h ]\n\nthe layer before yolo layer must have filters as 3*(5+ number of classes) also yolo layer takes classes as input\n\nSo in my yolo3 layer if you want to modify it you need to make changes in all yolo layers and all 1 layer before yolo layer.\n\n\n","e570f90c":"Check Image processing","96f485f3":"Utility functions","421f1557":"Creating class list from Annotation data","aacfb850":"Model Development","349bee98":"# "}}