{"cell_type":{"8246cf2b":"code","b7bbafc9":"code","cc592534":"code","4c50fac1":"code","08eafb27":"code","4e21c338":"code","36c532d6":"code","620fe105":"code","4a0fcf2f":"code","eff660de":"code","65d691a4":"code","31b21ad4":"code","c2988b62":"code","19dab47a":"code","8f7e7f63":"code","6ca7cbe8":"code","a8c27ff5":"code","e8a90d9d":"code","8aeadf7a":"markdown","f16eacc4":"markdown"},"source":{"8246cf2b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom keras import preprocessing\nfrom keras.models import Sequential\nfrom tensorflow.keras import layers\n#from keras.layers import Conv2D,Dropout,Dense,Flatten,Conv2DTranspose,BatchNormalization,LeakyReLU,Reshape\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Progbar\n\n# Any results you write to the current directory are saved as output.","b7bbafc9":"TEST_SET_SIZE = 5000\nBUFFER_SIZE = TEST_SET_SIZE\nBATCH_SIZE = 128\nNOISE_DIM = 100\nNUM_EXAMPLES_TO_GENERATE = 16\nEXAMPLE_SEED = tf.random.normal([NUM_EXAMPLES_TO_GENERATE, NOISE_DIM])\nEPOCHS = 300  #set epoch according to your training dataset size,i had chosen 50k images hence epochs are high as 300...\nBATCH_SIZE = 128","cc592534":"def decode_img(file_path):\n    file = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(file, channels=1)\n    return img   \n\ndef process_path(file_path):\n    img = decode_img(file_path)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.central_crop(img, 0.7)\n    \n    img = tf.image.crop_to_bounding_box(\n        img, 30, 10, 115, 115\n    )\n    img = tf.image.resize_with_pad(img, 64,64)\n    return img\n\n\nimg_path = \"\/kaggle\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/\"\nds_train_paths = tf.data.Dataset.list_files(str(img_path + '*.jpg'))\nds_train_paths = ds_train_paths.take(TEST_SET_SIZE)\n\nds_train = ds_train_paths.map(process_path).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","4c50fac1":"unknown_image_batch = next(iter(ds_train))","08eafb27":"tf.math.reduce_mean(\n    unknown_image_batch[1], axis=None, keepdims=False, name=None\n)","4e21c338":"img = unknown_image_batch[0]\nplt.imshow(tf.concat([img,img,img], 2))","36c532d6":"def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((8, 8, 256)))\n    assert model.output_shape == (None, 8, 8, 256) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 16, 16, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 32, 32, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 64, 64, 1)\n\n    return model\n\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1,activation='sigmoid'))\n\n    return model\n\ngenerator = make_generator_model()\ndiscriminator = make_discriminator_model()","620fe105":"def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(4*4*512, use_bias = False, input_shape = (100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    model.add(layers.Reshape((4, 4, 512)))\n    \n    model.add(layers.Conv2DTranspose(256, (5, 5), strides = (2,2), padding = \"same\", use_bias = False))\n    assert model.output_shape == (None, 8, 8, 256)\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    \n    model.add(layers.Conv2DTranspose(128, (5,5), strides = (2,2), padding = 'same', use_bias = False))\n    assert model.output_shape == (None, 16, 16, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    \n    model.add(layers.Conv2DTranspose(64, (5,5), strides = (2,2), padding = 'same', use_bias = False))\n    assert model.output_shape == (None, 32, 32, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    \n    model.add(layers.Conv2DTranspose(1, (5,5), strides = (2,2), padding = 'same', use_bias = False, activation = 'tanh'))\n    assert model.output_shape == (None, 64, 64, 1)\n    \n    return model\n\n\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 1]))\n    model.add(layers.LeakyReLU())\n    #model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    #model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1,activation='sigmoid'))\n\n    return model\n\ngenerator = make_generator_model()\ndiscriminator = make_discriminator_model()","4a0fcf2f":"gan = tf.keras.models.Sequential([generator, discriminator])\n\ndiscriminator.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(0.0002, beta_1=0.5))\ndiscriminator.trainable = False\ngan.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(0.0002, beta_1=0.5))","eff660de":"gan.summary()","65d691a4":"def generate_and_save_images(generator, epoch, test_input=EXAMPLE_SEED, samples=NUM_EXAMPLES_TO_GENERATE):\n    \n    fake_faces = generator.predict(test_input)\n    \n    fig = plt.figure(figsize=(10,10))\n\n    for k in range(samples):\n        plt.subplot(4, 4, k+1)\n        img = fake_faces[k]\n        plt.imshow(tf.concat([img,img,img], 2))\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","31b21ad4":"history = []\n\nwith tf.device('\/gpu:0'):\n    for epoch in range(EPOCHS):\n\n        progress_bar = Progbar(TEST_SET_SIZE, stateful_metrics=['gen_loss','disc_loss'])\n        print(\"\\nepoch {}\/{}\".format(epoch+1,EPOCHS))\n\n        for X_batch in ds_train:\n            #train the disceriminator\n            train_label=tf.ones([BATCH_SIZE], tf.int32)\n            discriminator.trainable = True\n            discriminator_real_loss = discriminator.train_on_batch(X_batch,train_label)\n\n            noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n            fake_images = generator.predict_on_batch(noise)\n            train_label=tf.zeros([BATCH_SIZE], tf.int32)\n            discriminator_fake_loss = discriminator.train_on_batch(fake_images,train_label)\n\n            #train the generator\n            noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n            train_label=np.ones(shape=(BATCH_SIZE,1))\n            discriminator.trainable = False\n            generator_loss = gan.train_on_batch(noise, train_label)\n\n            values=[('gen_loss',generator_loss), ('disc_loss',discriminator_real_loss+discriminator_fake_loss)]\n            progress_bar.add(BATCH_SIZE, values=values)\n\n        history.append({'gen_loss': generator_loss, \n                        'disc_loss_fake': discriminator_fake_loss, \n                        'disc_loss_real': discriminator_real_loss})\n        \n        if epoch % 5 == 0:\n             generate_and_save_images(generator,epoch)","c2988b62":"generate_and_save_images(generator,1)","19dab47a":"cross_entropy = tf.keras.losses.BinaryCrossentropy()\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam()\ndiscriminator_optimizer = tf.keras.optimizers.Adam()","8f7e7f63":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","6ca7cbe8":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    for image_batch in dataset:\n      train_step(image_batch)\n    print(epoch)\n    generate_and_save_images(generator,\n                             epoch + 1)\n\n","a8c27ff5":"train(ds_train, 100)","e8a90d9d":"import imageio\nimport glob\n\nanim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n    filenames = glob.glob('image*.png')\n    filenames = sorted(filenames)\n    last = -1\n    for i,filename in enumerate(filenames):\n        frame = 2*(i**0.5)\n        if round(frame) > round(last):\n            last = frame\n        else:\n            continue\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)\n\nimport IPython\nif IPython.version_info > (6,2,0,''):\n    display.Image(filename=anim_file)","8aeadf7a":"# Training V2","f16eacc4":"# **Training**"}}