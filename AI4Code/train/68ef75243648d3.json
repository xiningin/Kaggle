{"cell_type":{"5d9f619a":"code","53f31ed2":"code","2b61b46e":"code","be16d750":"code","eadefc62":"code","4fde029d":"code","e543922a":"code","ef10f22c":"code","1dff3a23":"code","8973058f":"code","acc92f53":"code","89751cbf":"code","3edb5cf4":"code","485aaf00":"code","23f74900":"code","0d401d74":"code","7e06267a":"code","8e368e7a":"code","b315fc13":"code","f8fbb0d8":"code","3244a202":"code","1629f5d6":"code","e53875c9":"code","d7fbd57f":"code","1354a693":"code","9e9e8e97":"markdown","a5e9363f":"markdown","58f0ac5e":"markdown","b6108285":"markdown","53b2b2cc":"markdown","c82f387f":"markdown","e233a23b":"markdown","85741585":"markdown","d6075005":"markdown"},"source":{"5d9f619a":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53f31ed2":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","2b61b46e":"df=pd.read_csv(\"..\/input\/companydata\/companydata.txt\",index_col=0)","be16d750":"df.head()","eadefc62":"x=df.drop('TARGET CLASS',axis=1)","4fde029d":"from sklearn.preprocessing import StandardScaler","e543922a":"scal=StandardScaler()","ef10f22c":"scal.fit(x)","1dff3a23":"scale=scal.transform(x)","8973058f":"x=pd.DataFrame(scale,columns=df.columns[:-1])","acc92f53":"x.head()","89751cbf":"sns.pairplot(df,hue='TARGET CLASS')","3edb5cf4":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(x,df['TARGET CLASS'],test_size=0.30)","485aaf00":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=1)","23f74900":"knn.fit(xtrain,ytrain)","0d401d74":"pred=knn.predict(xtest)","7e06267a":"from sklearn.metrics import confusion_matrix,classification_report\nprint('Confusion Matrix')\nprint(confusion_matrix(ytest,pred))","8e368e7a":"print('Classification report')\nprint(classification_report(ytest,pred))","b315fc13":"from sklearn.model_selection import cross_val_score","f8fbb0d8":"acc=[]\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    score=cross_val_score(knn,x,df['TARGET CLASS'],cv=10)\n    acc.append(score.mean())\n    ","3244a202":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),acc,linestyle='dashed',marker='o',markersize=10,markerfacecolor='red')\nplt.xlabel('K-neighbor')\nplt.ylabel('Accuracy rate')","1629f5d6":"err=[]\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    score=cross_val_score(knn,x,df['TARGET CLASS'],cv=10)\n    err.append(1-score.mean())\n    ","e53875c9":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),err,linestyle='dashed',marker='o',markersize=10,markerfacecolor='red')\nplt.xlabel('K-neighbor')\nplt.ylabel('Error rate')","d7fbd57f":"knn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(xtrain,ytrain)\npred = knn.predict(xtest)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(ytest,pred))\nprint('\\n')\nprint(classification_report(ytest,pred))","1354a693":"knn = KNeighborsClassifier(n_neighbors=23)\n\nknn.fit(xtrain,ytrain)\npred = knn.predict(xtest)\n\nprint('WITH K=23')\nprint('\\n')\nprint(confusion_matrix(ytest,pred))\nprint('\\n')\nprint(classification_report(ytest,pred))","9e9e8e97":"# Choosing the K-neighbour****","a5e9363f":"# # Comparing k=1 and k=23 there will be increase in accuracy****","58f0ac5e":"From the above figure we can say that after k=23 there is no increase in error.So we can take the value as k=23","b6108285":"# K-Nearest Neighbour  ****\n**We use this algorithm to classify the two target class**","53b2b2cc":"# **Standardize the variables**","c82f387f":"Here we can see that that after arouns K>23 the accuracy rate just tends to hover around 0.06-0.05 Let's retrain the model with that and check the classification report!****","e233a23b":"# Train test split****","85741585":"**From the above diagram,the target classes are overlapped and it can be classified using K-nearest neighbour algorithm.**","d6075005":"**Standardisation is important in KNN because the prediction of model is based on the distance between the neighbour points so all the features values should be on the standard scale,otherwise it will lead to wrong prediction**"}}