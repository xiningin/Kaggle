{"cell_type":{"ad555d46":"code","265ba89d":"code","b79e02ee":"code","c7ffc512":"code","de917590":"code","834f02ab":"code","7ddb0b00":"code","fbb1eb3a":"code","1d1e3c13":"code","667a2e02":"code","6917a8d3":"code","be73c638":"code","9adcc6f3":"code","1f879475":"code","da51bc8a":"code","f64c1f19":"code","36c115ca":"code","dd5d3fed":"code","7e730757":"code","e666ec00":"code","35a652b6":"code","543a84d0":"code","325cebbf":"code","42825fd5":"code","35ba419b":"code","bd5c7ef3":"code","cca4b426":"code","9d135008":"code","95c6d279":"code","c19a9dc6":"code","e6969dd4":"code","54a13e68":"code","5265234a":"code","244ac50e":"code","87f0e18b":"code","1f2aadc9":"code","a89e1191":"code","37a2360f":"code","fe5e872a":"code","e6d7c579":"code","6db47954":"code","2e1d9c65":"code","657323a9":"code","409f6ce7":"code","88eac5ee":"code","ff47bb88":"code","31b9a68a":"code","a319677c":"code","699619ba":"code","aa58b239":"code","218f3175":"markdown","5f5969fe":"markdown","8bb3129b":"markdown","00fadeb4":"markdown","295b818f":"markdown","0d3a9a1b":"markdown","47c72730":"markdown"},"source":{"ad555d46":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport sqlite3\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom wordcloud import WordCloud\nimport re\nimport os\nfrom sqlalchemy import create_engine # database connection\nimport datetime as dt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom skmultilearn.adapt import mlknn\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom skmultilearn.problem_transform import LabelPowerset\nfrom sklearn.naive_bayes import GaussianNB\nfrom datetime import datetime","265ba89d":"# Loading the data into a pandas dataframe\ndf = pd.read_csv(\"\/kaggle\/input\/facebook-recruiting-iii-keyword-extraction\/Train.zip\")\ndf.head()","b79e02ee":"df.columns","c7ffc512":"df = df.drop_duplicates(['Title', 'Body', 'Tags'])","de917590":"df[\"tag_count\"] = df[\"Tags\"].apply(lambda row : len(str(row).split(\" \")))","834f02ab":"df.head()","7ddb0b00":"df.tag_count.value_counts()","fbb1eb3a":"df.dropna(inplace=True)","1d1e3c13":"vectorizer = CountVectorizer(tokenizer= lambda text : text.split(\" \"))\ntag_dtm = vectorizer.fit_transform(df[\"Tags\"])","667a2e02":"tags = vectorizer.get_feature_names()\ntags[:10]","6917a8d3":"freqs = tag_dtm.sum(axis=0).A1\nresult = dict(zip(tags,freqs))","be73c638":"tag_df = pd.DataFrame(result.items(), columns=[\"Tags\", \"Counts\"])","9adcc6f3":"tag_df.head()","1f879475":"tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\ntag_counts = tag_df_sorted[\"Counts\"].values","da51bc8a":"plt.plot(tag_counts[:100])\nplt.scatter(x= list(range(0,100,5)), y = tag_counts[0:100:5], c= 'orange',label = \"Quantiles with 5 % intervals\")\nplt.scatter(x= list(range(0,100,25)), y = tag_counts[0:100:25], c = \"red\", label = \"Quantiles with 25th % intervals\")\nplt.grid()\nplt.xlabel(\"Tag Number\")\nplt.ylabel(\"Number of times the tag Appear\")\nplt.show()","f64c1f19":"dict(result.items())","36c115ca":"wordcloud = WordCloud(background_color='black',\n         width = 1600,\n         height = 800).generate_from_frequencies(result)\nplt.figure(figsize=(30,20))\nplt.imshow(wordcloud)\nplt.show()","dd5d3fed":"i = np.arange(30)\ntag_df_sorted.head(30).plot(kind='bar')\nplt.xticks(i, tag_df_sorted['Tags'][:30])\nplt.show()","7e730757":"def striphtml(data):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr,' ',str(data))\n    return cleantext\n\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer('english')\n","e666ec00":"df.shape","35a652b6":"random_df = df.sample(50000)","543a84d0":"random_df.head()","325cebbf":"random_df.shape","42825fd5":"start = datetime.now()\npreprocessed_data_list=[]\nquestions_with_code=0\nlen_pre=0\nlen_post=0\nquestions_proccesed = 0\nprepared_df = pd.DataFrame(columns=['question','code','tags','words_pre','words_post','is_code'])\nfor row in random_df.iterrows():\n\n    is_code = 0\n\n    #As title seems very important feature Hence increasing title weight by adding it 3 times\n    title, question, tags = 3*(' ' +row[1][1]), row[1][2], row[1][3]\n\n    if '<code>' in question:\n        questions_with_code+=1\n        is_code = 1\n    x = len(question)+len(title)\n    len_pre+=x\n\n    code = str(re.findall(r'<code>(.*?)<\/code>', question, flags=re.DOTALL))\n\n    question=re.sub('<code>(.*?)<\/code>', '', question, flags=re.MULTILINE|re.DOTALL)\n    question=striphtml(question.encode('utf-8'))\n\n    title=title.encode('utf-8')\n\n    question=str(title)+\" \"+str(question)\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n\n    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n\n    len_post+=len(question)\n    processed_di = {\n        \"question\": question,\n        \"code\": code,\n        \"tags\": tags,\n        \"words_pre\": x,\n        \"words_post\": len(question),\n        \"is_code\" : is_code\n    }\n    \n    prepared_df.loc[len(prepared_df.index)] = [question,code,tags,x,len(question),is_code]\n    questions_proccesed += 1\n    if (questions_proccesed%100000==0):\n        print(\"number of questions completed=\",questions_proccesed)\n\nno_dup_avg_len_pre=(len_pre*1.0)\/questions_proccesed\nno_dup_avg_len_post=(len_post*1.0)\/questions_proccesed\n\nprint( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\nprint( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\nprint (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)\/questions_proccesed))\n\nprint(\"Time taken to run this cell :\", datetime.now() - start)","35ba419b":"prepared_df.head()","bd5c7ef3":"preprocessed_data = prepared_df[[\"question\",\"tags\"]]","cca4b426":"preprocessed_data.head()","9d135008":"vectorizer = CountVectorizer(tokenizer= lambda text : text.split(), binary=True)\nmultilabel_y = vectorizer.fit_transform(preprocessed_data[\"tags\"])","95c6d279":"multilabel_y.get_shape()","c19a9dc6":"def tags_to_choose(n):\n    t = multilabel_y.sum(axis=0).tolist()[0]\n    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n    return multilabel_yn\n\ndef questions_explained_fn(n):\n    multilabel_yn = tags_to_choose(n)\n    x= multilabel_yn.sum(axis=1)\n    return (np.count_nonzero(x==0))","e6969dd4":"question_explained = []\ntotal_tags = multilabel_y.shape[1]\ntotal_qs = preprocessed_data.shape[0]\n\nfor i in range(500, total_tags, 100):\n    question_explained.append(np.round(((total_qs-questions_explained_fn(i))\/total_qs)*100,3))","54a13e68":"fig, ax = plt.subplots()\nax.plot(question_explained)\nxlabel = list(500+np.array(range(-50,450,50))*50)\nax.set_xticklabels(xlabel)\nplt.xlabel(\"Number of tags\")\nplt.ylabel(\"Number Questions coverd partially\")\nplt.grid()\nplt.show()\n# you can choose any number of tags based on your computing power, minimun is 50(it covers 90% of the tags)\nprint(\"with \",5500,\"tags we are covering \",question_explained[50],\"% of questions\")","5265234a":"multilabel_yx = tags_to_choose(5500)\nprint(\"number of questions that are not covered :\", questions_explained_fn(5500),\"out of \", total_qs)","244ac50e":"multilabel_yx.get_shape()","87f0e18b":"print(\"Number of tags in sample :\", multilabel_y.shape[1])\nprint(\"number of tags taken :\", multilabel_yx.shape[1],\"(\",(multilabel_yx.shape[1]\/multilabel_y.shape[1])*100,\"%)\")","1f2aadc9":"total_size=preprocessed_data.shape[0]\ntrain_size=int(0.80*total_size)\n\nx_train=preprocessed_data.head(train_size)\nx_test=preprocessed_data.tail(total_size - train_size)\n\ny_train = multilabel_yx[0:train_size,:]\ny_test = multilabel_yx[train_size:total_size,:]","a89e1191":"print(\"Number of data points in train data :\", y_train.shape)\nprint(\"Number of data points in test data :\", y_test.shape)","37a2360f":"tfidf_vect = TfidfVectorizer(min_df=0.00009,max_features=200000,smooth_idf=True,norm='l2',\\\n               tokenizer=lambda x : x.split(),sublinear_tf=False, ngram_range=(1,3) )","fe5e872a":"x_train_vectors = tfidf_vect.fit_transform(x_train['question'])\nx_test_vectors = tfidf_vect.transform(x_test['question'])","e6d7c579":"print(\"Dimensions of train data X:\",x_train_vectors.shape, \"Y :\",y_train.shape)\nprint(\"Dimensions of test data X:\",x_test_vectors.shape,\"Y:\",y_test.shape)","6db47954":"classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\nclassifier.fit(x_train_vectors,y_train)","2e1d9c65":"predictions = classifier.predict(x_test_vectors)","657323a9":"print(\"accuracy \", metrics.accuracy_score(y_test,predictions))\nprint(\"macro f1 score \",metrics.f1_score(y_test,predictions, average='macro'))\nprint(\"micro f1 score \", metrics.f1_score(y_test, predictions, average='micro'))\nprint(\"hamming loss \", metrics.hamming_loss(y_test,predictions))\n\n# print(\"Precision classification report \\n\", metrics.classification_report(y_test,predictions))\n","409f6ce7":"# cls_sgd = OneVsRestClassifier(SGDClassifier(), n_jobs=-1)","88eac5ee":"# cls_sgd.get_params()","ff47bb88":"# params = {\n# \"estimator__loss\" : [\"log\", \"hinge\"],\n# \"estimator__penalty\" : ['l1','l2'], \n# \"estimator__alpha\" : [0.01, 0.1]     \n# }","31b9a68a":"# clf_grid = GridSearchCV(cls_sgd,param_grid=params)","a319677c":"# clf_grid.fit(x_train_vectors,y_train)","699619ba":"# clf_grid.best_estimator_","aa58b239":"# clf_grid.best_score_","218f3175":"Observations:\nA look at the word cloud shows that \"c#\", \"java\", \"php\", \"asp.net\", \"javascript\", \"c++\" are some of the most frequent tags.","5f5969fe":"# Featurizing data","8bb3129b":"# Applying Logistic Regression with OneVsRest Classifier","00fadeb4":"Converting tags for multilabel problems","295b818f":"Split the data into test and train (80:20)","0d3a9a1b":"# 4. Machine Learning Models","47c72730":"1. # Applying Linear SVM with hinge loss and logistic regression in single shot with OneVsRest Classifier"}}