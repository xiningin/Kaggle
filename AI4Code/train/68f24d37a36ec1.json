{"cell_type":{"28f16efb":"code","7ba52b7a":"code","0f8b17c7":"code","e6ed1014":"code","b3ab63e9":"code","7aabc2e1":"code","44e48fda":"code","304662f8":"code","22eb1b06":"code","9ad6f6be":"code","dcca6681":"code","515b3d84":"code","43a9e80e":"code","435d5a6e":"code","9157c29b":"code","c3d73e65":"code","a029a754":"code","05ee547e":"code","20be6b09":"code","f548dc0c":"code","3686ee56":"code","71f15162":"code","fc0323e5":"code","fc4c7c46":"code","b47dc055":"code","10c3efa0":"code","5979dbaf":"code","fa32e178":"code","30bd05f1":"code","a5d8c618":"code","8b5e42b6":"code","79248260":"code","972f78cc":"code","b5248e79":"code","0cf81c30":"code","17562b34":"code","f10a9896":"code","b653e475":"code","89a83e75":"code","94d9ceb3":"code","58ee28e8":"code","c8f90554":"code","064e9d59":"code","9b4e32d1":"code","d00eb7c6":"code","218fe0f2":"code","bdd6ad24":"code","89f3d2b6":"code","cb1cd1f9":"code","d13096d2":"code","2c126b8a":"code","47b121b3":"code","4b991694":"code","dd2eb86f":"code","1d750319":"code","77e5e743":"code","3595b283":"code","868ed3c5":"code","9ad71d75":"code","2dda7e9b":"code","eb4b71b1":"code","0dd54c3d":"code","65561bf9":"code","55aeb630":"code","3304784f":"code","f879ac50":"markdown","187380fa":"markdown","6abe432b":"markdown","da6c4a30":"markdown","2e83209a":"markdown","e63f0648":"markdown","3c63d24d":"markdown","7620253a":"markdown","20554c8d":"markdown","499710db":"markdown","ff396b6c":"markdown","b4e65aa9":"markdown","8007c95c":"markdown","cfa6813d":"markdown","8d479f92":"markdown","dbbe2d7a":"markdown","c5b71eb3":"markdown","ca884134":"markdown","be0049ad":"markdown","e3d31343":"markdown","e38783b3":"markdown","4e4ce98c":"markdown"},"source":{"28f16efb":"import numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)  ","7ba52b7a":"training = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntesting = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","0f8b17c7":"training.head()","e6ed1014":"training.describe()","b3ab63e9":"training.shape","7aabc2e1":"training.keys()","44e48fda":"correlations = training.corr()\ncorrelations = correlations[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[1:6]\ncorrelations","304662f8":"training_null = pd.isnull(training).sum()\ntesting_null = pd.isnull(testing).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])","22eb1b06":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #not as much missing values","9ad6f6be":"null_many","dcca6681":"#you can find these features on the description data file provided\n\nnull_has_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]","515b3d84":"for i in null_has_meaning:\n    training[i].fillna(\"None\", inplace=True)\n    testing[i].fillna(\"None\", inplace=True)","43a9e80e":"#from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer\n\n#imputer = Imputer(strategy=\"median\")\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')","435d5a6e":"training_null = pd.isnull(training).sum()\ntesting_null = pd.isnull(testing).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])","9157c29b":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few missing values","c3d73e65":"null_many","a029a754":"training.drop(\"LotFrontage\", axis=1, inplace=True)\ntesting.drop(\"LotFrontage\", axis=1, inplace=True)","05ee547e":"null_few","20be6b09":"training[\"GarageYrBlt\"].fillna(training[\"GarageYrBlt\"].median(), inplace=True)\ntesting[\"GarageYrBlt\"].fillna(testing[\"GarageYrBlt\"].median(), inplace=True)\ntraining[\"MasVnrArea\"].fillna(training[\"MasVnrArea\"].median(), inplace=True)\ntesting[\"MasVnrArea\"].fillna(testing[\"MasVnrArea\"].median(), inplace=True)\ntraining[\"MasVnrType\"].fillna(\"None\", inplace=True)\ntesting[\"MasVnrType\"].fillna(\"None\", inplace=True)","f548dc0c":"types_train = training.dtypes #type of each feature in data: int, float, object\nnum_train = types_train[(types_train == int) | (types_train == float)] #numerical values are either type int or float\ncat_train = types_train[types_train == object] #categorical values are type object\n\n#we do the same for the test set\ntypes_test = testing.dtypes\nnum_test = types_test[(types_test == int) | (types_test == float)]\ncat_test = types_test[types_test == object]","3686ee56":"#we should convert num_train and num_test to a list to make it easier to work with\nnumerical_values_train = list(num_train.index)\nnumerical_values_test = list(num_test.index)","71f15162":"print(numerical_values_train)","fc0323e5":"fill_num = []\n\nfor i in numerical_values_train:\n    if i in list(null_few.index):\n        fill_num.append(i)","fc4c7c46":"print(fill_num)","b47dc055":"for i in fill_num:\n    training[i].fillna(training[i].median(), inplace=True)\n    testing[i].fillna(testing[i].median(), inplace=True)","10c3efa0":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","5979dbaf":"print(categorical_values_train) ","fa32e178":"fill_cat = []\n\nfor i in categorical_values_train:\n    if i in list(null_few.index):\n        fill_cat.append(i)","30bd05f1":"print(fill_cat) ","a5d8c618":"def most_common_term(lst):\n    lst = list(lst)\n    return max(set(lst), key=lst.count)\n#most_common_term finds the most common term in a series\n\nmost_common = [\"Electrical\", \"Exterior1st\", \"Exterior2nd\", \"Functional\", \"KitchenQual\", \"MSZoning\", \"SaleType\", \"Utilities\", \"MasVnrType\"]\n\ncounter = 0\nfor i in fill_cat:\n    most_common[counter] = most_common_term(training[i])\n    counter += 1","8b5e42b6":"most_common_dictionary = {fill_cat[0]: [most_common[0]], fill_cat[1]: [most_common[1]], fill_cat[2]: [most_common[2]], fill_cat[3]: [most_common[3]],\n                          fill_cat[4]: [most_common[4]], fill_cat[5]: [most_common[5]], fill_cat[6]: [most_common[6]], fill_cat[7]: [most_common[7]],\n                          fill_cat[8]: [most_common[8]]}\nmost_common_dictionary","79248260":"counter = 0\nfor i in fill_cat:  \n    training[i].fillna(most_common[counter], inplace=True)\n    testing[i].fillna(most_common[counter], inplace=True)\n    counter += 1","972f78cc":"training_null = pd.isnull(training).sum()\ntesting_null = pd.isnull(testing).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\nnull[null.sum(axis=1) > 0]","b5248e79":"sns.distplot(training[\"SalePrice\"])","0cf81c30":"sns.distplot(np.log(training[\"SalePrice\"]))","17562b34":"training[\"TransformedPrice\"] = np.log(training[\"SalePrice\"])","f10a9896":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","b653e475":"print(categorical_values_train)","89a83e75":"for i in categorical_values_train:\n    feature_set = set(training[i])\n    for j in feature_set:\n        feature_list = list(feature_set)\n        training.loc[training[i] == j, i] = feature_list.index(j)\n\nfor i in categorical_values_test:\n    feature_set2 = set(testing[i])\n    for j in feature_set2:\n        feature_list2 = list(feature_set2)\n        testing.loc[testing[i] == j, i] = feature_list2.index(j)","94d9ceb3":"training.head()","58ee28e8":"testing.head()","c8f90554":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.cross_validation import cross_val_score, KFold\nfrom sklearn.model_selection import KFold, cross_val_score ","064e9d59":"X_train = training.drop([\"Id\", \"SalePrice\", \"TransformedPrice\"], axis=1).values\ny_train = training[\"TransformedPrice\"].values\nX_test = testing.drop(\"Id\", axis=1).values","9b4e32d1":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","d00eb7c6":"linreg = LinearRegression()\nparameters_lin = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False]}\ngrid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\ngrid_linreg.fit(X_training, y_training)\n\nprint(\"Best LinReg Model: \" + str(grid_linreg.best_estimator_))\nprint(\"Best Score: \" + str(grid_linreg.best_score_))","218fe0f2":"linreg = grid_linreg.best_estimator_\nlinreg.fit(X_training, y_training)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","bdd6ad24":"scores_lin = cross_val_score(linreg, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))","89f3d2b6":"lasso = Lasso()\nparameters_lasso = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"precompute\" : [True, False], \"copy_X\" : [True, False]}\ngrid_lasso = GridSearchCV(lasso, parameters_lasso, verbose=1, scoring=\"r2\")\ngrid_lasso.fit(X_training, y_training)\n\nprint(\"Best Lasso Model: \" + str(grid_lasso.best_estimator_))\nprint(\"Best Score: \" + str(grid_lasso.best_score_))","cb1cd1f9":"lasso = grid_lasso.best_estimator_\nlasso.fit(X_training, y_training)\nlasso_pred = lasso.predict(X_valid)\nr2_lasso = r2_score(y_valid, lasso_pred)\nrmse_lasso = np.sqrt(mean_squared_error(y_valid, lasso_pred))\nprint(\"R^2 Score: \" + str(r2_lasso))\nprint(\"RMSE Score: \" + str(rmse_lasso))","d13096d2":"scores_lasso = cross_val_score(lasso, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lasso)))","2c126b8a":"ridge = Ridge()\nparameters_ridge = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False], \"solver\" : [\"auto\"]}\ngrid_ridge = GridSearchCV(ridge, parameters_ridge, verbose=1, scoring=\"r2\")\ngrid_ridge.fit(X_training, y_training)\n\nprint(\"Best Ridge Model: \" + str(grid_ridge.best_estimator_))\nprint(\"Best Score: \" + str(grid_ridge.best_score_))","47b121b3":"ridge = grid_ridge.best_estimator_\nridge.fit(X_training, y_training)\nridge_pred = ridge.predict(X_valid)\nr2_ridge = r2_score(y_valid, ridge_pred)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\nprint(\"R^2 Score: \" + str(r2_ridge))\nprint(\"RMSE Score: \" + str(rmse_ridge))","4b991694":"scores_ridge = cross_val_score(ridge, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))","dd2eb86f":"dtr = DecisionTreeRegressor()\nparameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n                  \"max_features\" : [\"auto\", \"log2\"]}\ngrid_dtr = GridSearchCV(dtr, parameters_dtr, verbose=1, scoring=\"r2\")\ngrid_dtr.fit(X_training, y_training)\n\nprint(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\nprint(\"Best Score: \" + str(grid_dtr.best_score_))","1d750319":"dtr = grid_dtr.best_estimator_\ndtr.fit(X_training, y_training)\ndtr_pred = dtr.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","77e5e743":"scores_dtr = cross_val_score(dtr, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","3595b283":"rf = RandomForestRegressor()\nparemeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n                 \"max_features\" : [\"auto\", \"log2\"]}\ngrid_rf = GridSearchCV(rf, paremeters_rf, verbose=1, scoring=\"r2\")\ngrid_rf.fit(X_training, y_training)\n\nprint(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\nprint(\"Best Score: \" + str(grid_rf.best_score_))","868ed3c5":"rf = grid_rf.best_estimator_\nrf.fit(X_training, y_training)\nrf_pred = rf.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","9ad71d75":"scores_rf = cross_val_score(rf, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","2dda7e9b":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Ridge\", \"Lasso\", \"Decision Tree Regressor\", \"Random Forest Regressor\"],\n    \"Best Score\" : [grid_linreg.best_score_,  grid_ridge.best_score_, grid_lasso.best_score_, grid_dtr.best_score_, grid_rf.best_score_],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_ridge)[0:5], str(r2_lasso)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_ridge)[0:8], str(rmse_lasso)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8]]\n})\nmodel_performances.round(4)\n\nprint(\"Sorted by Best Score:\")\nmodel_performances.sort_values(by=\"Best Score\", ascending=False)","eb4b71b1":"print(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)","0dd54c3d":"print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)","65561bf9":"rf.fit(X_train, y_train)","55aeb630":"submission_predictions = np.exp(rf.predict(X_test))","3304784f":"submission = pd.DataFrame({\n        \"Id\": testing[\"Id\"],\n        \"SalePrice\": submission_predictions\n    })\n\nsubmission.to_csv(\"prices.csv\", index=False)\nprint(submission.shape)","f879ac50":"Now, the features with a lot of missing values have been taken care of! Let's move on to the features with fewer missing values.","187380fa":"## Imputing \"Real\" NaN Values","6abe432b":"### Categorical Imputing\n\nSince these are categorical values, we can't impute with median or mean. We can, however, use mode. We'll impute with the most common term that appears in the entire list.\n","da6c4a30":"# 3. Imputing Null Values ","2e83209a":"These are all the numerical features in our data. ","e63f0648":"## 5. Creating, Training, Evaluating, Validating, and Testing ML Models ","3c63d24d":"This shows the most common term for each of the categorical features that we're working with. We'll replace the null values with these.","7620253a":"## 6. Submission ","20554c8d":"## 1. Importing Packages ","499710db":"## 2. Loading and Inspecting Data","ff396b6c":"Good! That should take care of the last couple of missing values. Let's check our work by looking at how many null values remain. If we are successful, the code below should print an empty table.","b4e65aa9":"### Numerical Imputing\n\nWe'll impute with median since the distributions are probably very skewed.\n","8007c95c":"GarageYrBlt, MasVnrArea, and MasVnrType all have a fairly decent amount of missing values. MasVnrType is categorical so we can replace the missing values with \"None\", as we did before. We can fill the others with median.","cfa6813d":"These are the categorical features in the data that have missing values in them. We'll impute with the most common term below. ","8d479f92":"## 4. Feature Engineering","dbbe2d7a":"These are all the categorical features in our data ","c5b71eb3":"LotFrontage has too many Null values and it is a numerical value so it may be better to just drop it.","ca884134":"For numerical imputing, we would typically fill the missing values with a measure like median, mean, or mode. For categorical imputing, I chose to fill the missing values with the most common term that appeared from the entire column.\n\n\n### Places Where NaN Means Something\n\nThis means that if a value is NaN, the house might not have that certain attribute, which will affect the price of the house. Therefore, it is better to not drop, but fill in the null cell with a value called \"None\" which serves as its own category.","be0049ad":"## Evaluation Our Models ","e3d31343":"Let's inspect the correlations to get a better idea of which columns correlate the most with the Sale Price of the house. If there are features that don\u00b4t do a good job predicting the Sale Price, we can just eliminate them and not use them in our model. ","e38783b3":"These are the numerical features in the data that have missing values in them. We will impute these features with a for-loop below.","4e4ce98c":"## House Prices: Context\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home."}}