{"cell_type":{"65a6981c":"code","24cc678b":"code","5cbb9092":"code","6d488a75":"code","37fcf42d":"code","bf68ccd9":"code","8a901790":"code","3e00dcd6":"code","03f324b8":"code","ce7bc2f9":"code","9f430f8f":"code","a9e996f4":"code","93ea2f94":"code","91eb0240":"code","9ebf793b":"code","118d336e":"code","621d5042":"code","99d98a3f":"code","08bfd1b2":"code","8408d979":"code","7174a96b":"code","5689ed0c":"code","0810d142":"code","972c2458":"code","8531efeb":"code","6902a6f8":"code","079606c8":"code","b5a32d41":"code","2add77ac":"code","530d4ed7":"code","af69bb88":"code","dec42f41":"code","1a7b8c63":"code","f2de3fb4":"code","79f02137":"code","43e3f384":"code","77360e9b":"code","b307e54a":"code","6e663988":"code","0d524dcf":"code","63c05543":"code","d1fe9514":"code","97b20fd5":"code","472b09e0":"code","c57d8cd5":"code","bfee3ca4":"code","b1481664":"code","f853c2ea":"code","749b03e2":"code","75efb0a7":"code","5084c961":"code","4a6c172a":"code","7bc4bae5":"code","a71078c8":"markdown","bd19d9e5":"markdown","53514008":"markdown","e5cde220":"markdown","98ea1708":"markdown","0d6ddd22":"markdown","38f57430":"markdown","8eda7531":"markdown","c5134e2d":"markdown","d58929aa":"markdown","bf5ad3d5":"markdown","e3722bb4":"markdown","8fba2e7d":"markdown","367f819c":"markdown","9a026c83":"markdown","a12a15db":"markdown","da3e39c1":"markdown","04ae94f7":"markdown","6f8fae18":"markdown","eedf0fe0":"markdown","17b650dc":"markdown","60710ced":"markdown","b5619e17":"markdown"},"source":{"65a6981c":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","24cc678b":"from catboost import CatBoostClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom mlxtend.preprocessing import minmax_scaling\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn import preprocessing","5cbb9092":"import tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\n\nfrom keras.models import Model\nimport gc","6d488a75":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","37fcf42d":"df1 = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ndf2 = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsam = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","bf68ccd9":"MV1 = df1.isnull().sum()\nprint(f'Missing Value 1:\\n{MV1[MV1 > 0]}\\n')\n\nMV2 = df2.isnull().sum()\nprint(f'Missing Value 2:\\n{MV2[MV2 > 0]}\\n')","8a901790":"# display(df1.info(), df2.info())\ndisplay(df1, df1.describe().transpose())\ndisplay(df2, df2.describe().transpose())","3e00dcd6":"df1['target'].value_counts().plot(kind='bar')","03f324b8":"df1['target'].value_counts().plot(kind='pie')\ndf1['target'].value_counts(normalize=True)","ce7bc2f9":"data1 = df1.copy()\ndata1['target'] = data1['target'].str.slice(start=6).astype(int) - 1\n\n# dic = {'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3, 'Class_5':4, 'Class_6':5, 'Class_7':6, 'Class_8':7, 'Class_9':8 }\n# data1['target'].replace(dic, inplace=True)\ndisplay(df1['target'], data1['target'])","9f430f8f":"X = data1.drop(columns = ['id','target'])\ndisplay(X)","a9e996f4":"y = data1.target\ndisplay(y)","93ea2f94":"data2 = df2.copy()\nXX = data2.drop(columns = ['id'])\ndisplay(XX)","91eb0240":"train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.50, random_state=123) \nval_X.to_csv(\"val_X.csv\",index=False)\nval_y.to_csv(\"val_y.csv\",index=False)","9ebf793b":"X_scaled = minmax_scaling(X, columns=X.columns)\n# display(X_scaled)","118d336e":"train_Xs, val_Xs, train_ys, val_ys = train_test_split(X_scaled, y, test_size=0.50, random_state=123) ","621d5042":"XXs = minmax_scaling(XX, columns=XX.columns)\n# display(XXs)","99d98a3f":"yf = df1.target\nXf = df1.drop(columns = ['id', 'target'])\n\n# Label encoding for categoricals\nfor colname in Xf.select_dtypes(\"object\"):    \n    Xf[colname], _ = Xf[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = Xf.dtypes == int","08bfd1b2":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=123)    \n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)                         \n    mi_scores = mi_scores.sort_values(ascending=False)                          \n    return mi_scores\n\nmi_scores = make_mi_scores(Xf, yf, discrete_features)                          \ndisplay(mi_scores)","8408d979":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\nplt.figure(dpi=100, facecolor='lightgray', figsize=(10, 4))\nplot_mi_scores(mi_scores.head(15))","7174a96b":"plt.figure(dpi=100, facecolor='lightgray', figsize=(4, 4))\nplot_mi_scores(mi_scores.tail(15))","5689ed0c":"def drop_uninformative(df, mi_scores):    \n    return df.loc[:, mi_scores > 0.003]\n\nXff = drop_uninformative(Xf, mi_scores)\n# display(Xff)","0810d142":"train_Xf, val_Xf, train_yf, val_yf = train_test_split(Xff, yf, test_size=0.50, random_state=123) ","972c2458":"XXf = XX.drop(columns = ['feature_36'])\n# display(XXf)","8531efeb":"model1v = HistGradientBoostingClassifier(max_iter=250,\n                                         validation_fraction=None, \n                                         learning_rate=0.01, \n                                         max_depth=10, \n                                         min_samples_leaf=24, \n                                         max_leaf_nodes=60,\n                                         random_state=123)\n\nmodel1v.fit(train_X, train_y)\noof_pred1 = model1v.predict_proba(val_X)\nlog_loss(val_y, oof_pred1)","6902a6f8":"predictions1 = model1v.predict(val_X)\naccuracy1 = accuracy_score(val_y, predictions1)\ndisplay(accuracy1) ","079606c8":"model1 = HistGradientBoostingClassifier(max_iter=250,\n                                        validation_fraction=None, \n                                        learning_rate=0.01, \n                                        max_depth=10, \n                                        min_samples_leaf=24, \n                                        max_leaf_nodes=60,\n                                        random_state=123,\n                                        verbose=1)\n\nmodel1.fit(X, y)\npred1 = model1.predict_proba(XX)\ndisplay(pred1, pred1.shape) ","b5a32d41":"sub1 = sam.copy()\ndisplay(sub1)","2add77ac":"sub1.iloc[:, 1:] = pred1.data\ndisplay(sub1)\n\n# sub1['Class_1'] = pred1[:,0] \n# sub1['Class_2'] = pred1[:,1] \n# sub1['Class_3'] = pred1[:,2] \n# sub1['Class_4'] = pred1[:,3] \n# sub1['Class_5'] = pred1[:,4] \n# sub1['Class_6'] = pred1[:,5] \n# sub1['Class_7'] = pred1[:,6] \n# sub1['Class_8'] = pred1[:,7] \n# sub1['Class_9'] = pred1[:,8] ","530d4ed7":"sub1.to_csv(\"submission1.csv\",index=False)\n# Public Score: 1.75770\n!ls","af69bb88":"model2v = CatBoostClassifier(depth=8,\n                             iterations=1000,\n                             learning_rate=0.02,                            \n                             eval_metric='MultiClass',\n                             loss_function='MultiClass', \n                             bootstrap_type= 'Bernoulli',\n                             leaf_estimation_method='Gradient',\n                             random_state=123,\n                             task_type='GPU')                        \n\nmodel2v.fit(train_X, train_y, verbose=100)\noof_pred2 = model2v.predict_proba(val_X)\nlog_loss(val_y, oof_pred2)","dec42f41":"predictions2 = model2v.predict(val_X)\naccuracy2 = accuracy_score(val_y, predictions2)\ndisplay(accuracy2) ","1a7b8c63":"model2v.feature_importances_","f2de3fb4":"model2 = CatBoostClassifier(depth=8,\n                            iterations=1000,\n                            learning_rate=0.02,                            \n                            eval_metric='MultiClass',\n                            loss_function='MultiClass', \n                            bootstrap_type= 'Bernoulli',\n                            leaf_estimation_method='Gradient',\n                            random_state=123,\n                            task_type='GPU')   \n\nmodel2.fit(X, y, verbose=100)\npred2 = model2.predict_proba(XX)\ndisplay(pred2, pred2.shape) ","79f02137":"sub2 = sam.copy()\n# display(sub2)","43e3f384":"sub2.iloc[:, 1:] = pred2.data\ndisplay(sub2)","77360e9b":"sub2.to_csv(\"submission2.csv\",index=False)\n# Public Score: 1.75011\n!ls","b307e54a":"train = df1.copy()\ntest = df2.copy()\nsubmission = sam.set_index('id')\ntargets = pd.get_dummies(df1['target'])","6e663988":"def custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\ncce = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=1e-05, patience=8, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.7, patience=2, verbose=0)","0d524dcf":"def get_model():\n    inputs = layers.Input(shape = (75,))\n    \n    embed = layers.Embedding(360, 8)(inputs)\n    embed = layers.Flatten()(embed)\n    \n    hidden = layers.Dropout(0.2)(embed)\n    hidden = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='selu', kernel_initializer=\"lecun_normal\"))(hidden)\n    \n    output = layers.Dropout(0.2)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='relu'))(output) \n    \n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='elu'))(output) \n    output = layers.Dense(9, activation = 'softmax')(output)\n    \n    model = keras.Model(inputs=inputs, outputs=output, name=\"res_nn_model\")\n    \n    return model","63c05543":"EPOCH = 50\nSEED = 123\nN_FOLDS = 10\n\nNN_a_train_preds = []\nNN_a_test_preds  = []\nNN_a_oof_pred3 = []\n\noof_NN_a = np.zeros((train.shape[0],9))\npred_NN_a = np.zeros((test.shape[0],9))\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)","d1fe9514":"for fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n\n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n    K.clear_session()\n    \n    model_attention = get_model()\n\n    model_attention.compile(loss='categorical_crossentropy', \n                            optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n                            metrics=custom_metric)\n    \n    model_attention.fit(X_train, y_train,\n                        batch_size = 256, epochs = EPOCH,\n                        validation_data=(X_test, y_test),\n                        callbacks=[es, plateau],\n                        verbose = 0)\n    \n    pred_a = model_attention.predict(X_test) \n    oof_NN_a[ts_idx] += pred_a \n    score_NN_a = log_loss(y_test, pred_a)\n    print(f\"\\nFOLD {fold} Score NN Attention model: {score_NN_a}\")\n    pred_NN_a += model_attention.predict(test.iloc[:,1:]) \/ N_FOLDS \n    \n    NN_a_train_preds.append(oof_NN_a[ts_idx])\n    NN_a_oof_pred3.append(model_attention.predict(val_X))\n    NN_a_test_preds.append(model_attention.predict(test.iloc[:,1:]))\n    \nscore_a = log_loss(targets, oof_NN_a)\nprint('=' * 60)\nprint(f\"\\n===== FINAL SCORE ATTENTION MODEL : {score_a} =====\\n\")  \nprint('=' * 60)\ndisplay(oof_NN_a, oof_NN_a.shape)","97b20fd5":"oof_pred3 = sum(np.array(NN_a_oof_pred3)\/N_FOLDS)\nlog_loss(val_y, oof_pred3)","472b09e0":"pred3 = sum(np.array(NN_a_test_preds)\/N_FOLDS)\ndisplay(pred3, pred3.shape)","c57d8cd5":"sub3 = sam.copy()\n# display(sub3)","bfee3ca4":"sub3.iloc[:, 1:] = pred3.data\ndisplay(sub3)","b1481664":"sub3.to_csv(\"submission3.csv\",index=False)\n# Public Score: 1.74587\n!ls","f853c2ea":"def generate(main, support, coeff):\n    \n    g = main.copy()    \n    for i in main.columns[1:]:\n        \n        res = []\n        lm, Is = [], []        \n        lm = main[i].tolist()\n        ls = support[i].tolist()  \n        \n        for j in range(len(main)):\n            res.append((lm[j] * coeff) + (ls[j] * (1.- coeff)))            \n        g[i] = res\n        \n    return g","749b03e2":"sub4 = pd.read_csv('..\/input\/tabular-residual-network\/submission.csv')\nsub4.to_csv(\"submission4.csv\",index=False)\n# Public Score: 1.74522\n# display(sub4)","75efb0a7":"sub5 = pd.read_csv('..\/input\/tps-june-21-eda-models\/Sol.csv')\nsub5.to_csv(\"submission5.csv\",index=False)\n# Public Score: 1.74456\n# display(sub5)","5084c961":"sub6 = pd.read_csv('..\/input\/tps6-74442\/TPS6_74442.csv')\nsub6.to_csv(\"submission6.csv\",index=False)\n# Public Score: 1.74442\n# display(sub6)","4a6c172a":"sub = generate(sub2, sub1, 0.85)\n\nsub = generate(sub3, sub , 0.85)\n\nsub = generate(sub4, sub , 0.85)\n\nsub = generate(sub5, sub , 0.85)\n\nsub = generate(sub6, sub , 0.55)\n\ndisplay(sub)","7bc4bae5":"sub.to_csv(\"submission.csv\",index=False)\n# Public Score: \n!ls","a71078c8":"<div class=\"alert alert-success\">  \n<\/div>","bd19d9e5":"<div class=\"alert alert-success\">  \n<\/div>","53514008":"Thanks to: @oxzplvifi https:\/\/www.kaggle.com\/oxzplvifi\/tabular-residual-network","e5cde220":"<div class=\"alert alert-success\">  \n<\/div>","98ea1708":"## Data Set","0d6ddd22":"## Ensembling","38f57430":"## Split","8eda7531":"Thanks to: @bhavikjain https:\/\/www.kaggle.com\/bhavikjain\/tps-june-21-eda-models","c5134e2d":"## HistGradientBoostingClassifier\n\n### Validation Model","d58929aa":"### Model 2","bf5ad3d5":"## Neural Networks\n\nThanks to: @pourchot https:\/\/www.kaggle.com\/pourchot\/blending-neural-networks-weights-optimization\n\n### Model 3","e3722bb4":"Thanks to: @fusioncenter https:\/\/www.kaggle.com\/fusioncenter\/residual-network-for-tabular-data","8fba2e7d":"<div class=\"alert alert-success\">  \n<\/div>","367f819c":"## Import","9a026c83":"## CatBoostClassifier\n\n### Validation Model","a12a15db":"<div class=\"alert alert-success\">  \n<\/div>","da3e39c1":"<div>\n    <h1 align=\"center\">Tabular Playground Series - Jun 2021<\/h1>\n    <h2 align=\"center\">HistGradientBoosting & Catboost & Neural Networks<\/h2>\n    <h4 align=\"center\">By: Somayyeh Gholami & Mehran Kazeminia<\/h4>\n<\/div>","04ae94f7":"<div class=\"alert alert-success\">\n    <h1 align=\"center\">If you find this work useful, please don't forget upvoting :)<\/h1>\n<\/div>","6f8fae18":"### Model 1","eedf0fe0":"<div class=\"alert alert-success\">  \n<\/div>","17b650dc":"## Scaling","60710ced":"## Features","b5619e17":"<div class=\"alert alert-success\">  \n<\/div>"}}