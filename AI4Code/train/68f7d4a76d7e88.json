{"cell_type":{"a2785bbe":"code","6cc72d9a":"code","77184d58":"code","3d48b563":"code","1a3e28b5":"code","ac6ef1d3":"code","0ed331bd":"code","3229fd94":"code","822fc2da":"code","ff03ea3b":"code","24b80ce2":"code","e00e7ca2":"code","e45b5842":"code","6ffce07f":"code","54ef574f":"code","8e7e69ae":"code","4f320a3f":"code","4b7c9606":"code","e5ef30a9":"code","0707617c":"code","611187fc":"code","3c968c83":"code","f362f4b1":"code","dbcdf5e1":"markdown","4b93b0b9":"markdown","066ad24b":"markdown","c13e63fa":"markdown","42085b0f":"markdown","8c6e7376":"markdown","f50a5b25":"markdown","4a25a3c1":"markdown","acc801dc":"markdown","027343f5":"markdown","bc11749a":"markdown","98ca8ac2":"markdown","3c7cf40d":"markdown","d5c02209":"markdown","b574ce62":"markdown","30dbe6ba":"markdown","ebe79d5d":"markdown","fa31341c":"markdown","68613d67":"markdown","027ca808":"markdown","a8c3d84f":"markdown"},"source":{"a2785bbe":"import pandas as pd\nimport numpy as np\nimport json, os\nimport matplotlib.pyplot as plt\n%matplotlib inline","6cc72d9a":"# Check the revision log\nwith open('\/kaggle\/input\/ArTaxOr\/revision history.txt', 'r') as f:\n    print(f.read())","77184d58":"import glob\n\npfiles=glob.glob('\/kaggle\/input\/ArTaxOr\/**\/*.vott', recursive=True)\ndf=pd.DataFrame()\nfor f in pfiles:\n    with open(f) as file:\n        pdata=json.load(file)\n        df=df.append(pd.DataFrame(list(pdata['assets'].values())), ignore_index=True)\ndf['path']=df['path'].str.replace('file:F:\/','')\ndf.head()","3d48b563":"tags=pd.DataFrame(list(pdata['tags']))\npattern=r'[A-Z]'\nlabels=tags[tags.name.str.match(pattern)]\nlabels","1a3e28b5":"import seaborn as sns\n\nps=np.zeros(len(df))\nfor i in range(len(df)):\n    ps[i]=df['size'][i]['width'] * df['size'][i]['height']\/1e6\nsns.distplot(ps, bins=21,kde=False).set_title('Image resolution in Mpix (total {})'.format(len(df)));","ac6ef1d3":"%%time\nanno=pd.DataFrame(columns=['label', 'label_idx', 'xres', 'yres', 'height', 'width', 'left', 'top', \n                           'right', 'bottom', 'area', 'xcenter', 'ycenter', 'blurred',\n                           'occluded', 'truncated', 'file', 'id'])\nfor i in range(len(df)):\n    p=df['path'][i].split('\/')\n    p='\/'.join(p[:2])\n    afile='\/kaggle\/input\/'+p+'\/annotations\/'+df['id'][i]+'-asset.json'\n    if os.path.isfile(afile):\n        with open(afile) as file:\n            adata=json.load(file)\n        xres,yres=adata['asset']['size']['width'],adata['asset']['size']['height'] \n        for j in range(len(adata['regions'])):\n            h=adata['regions'][j]['boundingBox']['height']\/yres\n            w=adata['regions'][j]['boundingBox']['width']\/xres\n            tags=adata['regions'][j]['tags']\n            anno=anno.append({'label': tags[0],\n                              'label_idx': labels[labels.name==tags[0]].index[0],\n                              'xres': xres,\n                              'yres': yres,\n                              'height': h,\n                              'width': w,                              \n                              'left': adata['regions'][j]['boundingBox']['left']\/xres,\n                              'top': adata['regions'][j]['boundingBox']['top']\/yres,\n                              'right': adata['regions'][j]['boundingBox']['left']\/xres+w,\n                              'bottom': adata['regions'][j]['boundingBox']['top']\/yres+h, \n                              'area': h*w,\n                              'xcenter': adata['regions'][j]['boundingBox']['left']\/xres+0.5*w,\n                              'ycenter': adata['regions'][j]['boundingBox']['top']\/yres+0.5*h,\n                              'blurred': int(any(ele == '_blurred' for ele in tags)),\n                              'occluded': int(any(ele == '_occluded' for ele in tags)),\n                              'truncated': int(any(ele == '_truncated' for ele in tags)),\n                              'file': adata['asset']['path'].replace('file:F:\/',''),\n                              'id': adata['asset']['id'],}, ignore_index=True)","0ed331bd":"anno.sample(5)","3229fd94":"sns.relplot(x=\"width\", y=\"height\", hue=\"label\", col=\"label\", data=anno);","822fc2da":"sns.jointplot(x=\"width\", y=\"height\", data=anno.loc[anno['label'] == 'Lepidoptera']);","ff03ea3b":"sns.relplot(x=\"xcenter\", y=\"ycenter\", hue=\"label\", col=\"label\", data=anno);","24b80ce2":"sns.set(rc={'figure.figsize':(12,6)})\nsns.violinplot(x=anno['label'],y=anno['area']);","e00e7ca2":"graph=sns.countplot(data=anno, x='label')\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=90)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","e45b5842":"df2=anno[['label', 'blurred']]\ndf2=df2.loc[df2['blurred'] == 1]\nsns.set(rc={'figure.figsize':(10,6)})\nsns.countplot(x='blurred', hue='label', data=df2);","6ffce07f":"df2=anno[['label', 'occluded']]\ndf2=df2.loc[df2['occluded'] == 1]\nsns.countplot(x='occluded', hue='label', data=df2);","54ef574f":"df2=anno[['label', 'truncated']]\ndf2=df2.loc[df2['truncated'] == 1]\nsns.countplot(x='truncated', hue='label', data=df2);","8e7e69ae":"def attribution(fname):\n    img = Image.open(fname)\n    exif_data = img._getexif()\n    img.close()\n    if len(exif_data[315]) > 0:\n        s='Photo: '+exif_data[315]\n    else:\n        s=exif_data[37510][8:].decode('ascii')\n    return s\n\ndef plot_img(axes, idf, highlight=True):\n    f='\/kaggle\/input\/'+idf.iloc[0].file\n    im = Image.open(f)\n    im.thumbnail((300,300),Image.ANTIALIAS)\n    draw = ImageDraw.Draw(im)\n    xres, yres = im.size[0], im.size[1]\n    for i in range(len(idf)):\n        if highlight==True:\n            color=(255, 0, 0) if i == 0 else (128, 128, 128)          \n        else:\n            color=labels[labels.name == idf.iloc[i].label].color.iloc[0]\n        draw.rectangle([int(idf.iloc[i]['left']*xres),\n                        int(idf.iloc[i]['top']*yres),\n                        int(idf.iloc[i]['right']*xres),\n                        int(idf.iloc[i]['bottom']*yres)], outline=color, width=2)\n    plt.setp(axes, xticks=[], yticks=[])\n    axes.set_title(idf.iloc[0].label+'\\n'+attribution(f))\n    plt.imshow(im)","4f320a3f":"from PIL import Image, ImageDraw\n\nfig = plt.figure(figsize=(16,26))\nfor i in range(len(labels)):\n    ldf=anno[anno.label == labels.name[i]].nlargest(3, 'area')\n    for j in range (3):\n        axes = fig.add_subplot(len(labels), 3, 1+i*3+j)\n        plot_img(axes, anno[anno.id == ldf.iloc[j].id].sort_values(by=['area'], ascending=False), highlight=True)","4b7c9606":"fig = plt.figure(figsize=(16,26))\n\nfor i in range(len(labels)): \n    a=anno[anno.label == labels.name[i]]['id'].value_counts()\n    for j in range (3):\n        ldf=anno[anno.id == a.index[j]]\n        axes = fig.add_subplot(len(labels), 3, 1+i*3+j)\n        plot_img(axes, anno[anno.id == ldf.iloc[j].id], highlight=False)","e5ef30a9":"fig = plt.figure(figsize=(20,18))\nfor i in range (3):\n    ldf=anno.sample(n=3)\n    for j in range(3):\n        axes = fig.add_subplot(3, 3, 1+i*3+j)\n        plot_img(axes, anno[anno.id == ldf.iloc[j].id], highlight=False)","0707617c":"header = ['file', 'label', 'height', 'width', 'left', 'top', 'right', 'bottom'] # change as required\nanno.to_csv('.\/ArTaxOr.csv', index=False, columns = header) ","611187fc":"import sys\n!{sys.executable} -m pip install pascal_voc_writer\nfrom pascal_voc_writer import Writer\n\nif not os.path.exists('voc'):\n    os.mkdir('voc')\n\n#for i in range(len(df)):\nfor i in range(10): # use above line for full dataset\n    ldf=anno[anno.id == df.id[i]].reset_index()\n    p=df.path[i].split('\/') \n    width, height = ldf.xres[0], ldf.yres[0]\n    writer = Writer(df.path[i], width, height)\n    for j in range(len(ldf)):\n        writer.addObject(ldf.label[j], \n                         int(ldf.left[j]*width), \n                         int(ldf.top[j]*height), \n                         int(ldf.right[j]*width),\n                         int(ldf.bottom[j]*height))\n    writer.save('.\/voc\/'+p[2].replace('.jpg','.xml'))\nprint(os.listdir(\".\/voc\"))","3c968c83":"if not os.path.exists('labels'):\n    os.mkdir('labels')\n\n#for i in range(len(df)):\nfor i in range(10): # use above line for full dataset\n    ldf=anno[anno.id == df.id[i]].reset_index()\n    p=df.path[i].split('\/') \n    file=open('.\/labels\/'+p[2].replace('.jpg','.txt'),'w')\n    for j in range(len(ldf)):\n        l=labels[labels.name == ldf.label[j]].index.to_list()\n        file.write('{} {} {} {} {}\\n'.format(l[0], ldf.xcenter[j], ldf.ycenter[j], ldf.width[j], ldf.height[j]))\n    file.close()\nprint(os.listdir(\".\/labels\"))","f362f4b1":"labels.to_pickle('.\/ArTaxOr_labels.pkl')\ndf.to_pickle('.\/ArTaxOr_filelist.pkl')\nanno.to_pickle('.\/ArTaxOr_objects.pkl')","dbcdf5e1":"## Object data import\nWe will now import all the object data from the json files into a dataframe. In the process, we convert object positions to relative values. This step might take some time.","4b93b0b9":"How are the centers of each object distributed? Spiders (Araneae) are most centered in the image while ants, bees and wasps (Hymenoptera) are most spread.","066ad24b":"# Metadata export\nFinally, we can export metadata to various formats for object detection model training. Let's start with a .csv file.","c13e63fa":"That's it! ","42085b0f":"## Pascal VOC\nPascal VOC files are xml format, and there is one xml file per image file, with same name.","8c6e7376":"The violin plot gives another view on object size distribution:","f50a5b25":"Extract the labels for later use:","4a25a3c1":"For each label, view the images that have the largest objects (relative image size).","acc801dc":"OK, let's take a look at how the object size (relative image size) distribution compares between labels:","027343f5":"## Image resolution\nPlot the distribution of image size - there is a peak around 3Mpix.","bc11749a":"Next, how many objects are there for each label? There should be at least 2000, and reasonably balanced (less than 2:1 rato between highest and lowest count).","98ca8ac2":"## TensorFlow TFRecords\nTFRecords are created in a [separate notebook](https:\/\/www.kaggle.com\/mistag\/tensorflow-tfrecords-demystified).","3c7cf40d":"Let's look at how many objects are blurred, occluded or truncated.","d5c02209":"For each label, view the images that have the most objects:","b574ce62":"## Metadata import\nThere are multiple image directories, each with a VoTT project. The project file contains a list of image files. Each image is associated with a separate json file that contains all the object boundary boxes. ","30dbe6ba":"## Pickle\nFinally, store labels, file list and object bounding boxes in pickle files for later use.","ebe79d5d":"## Darknet YOLOv3\nDarknet expects one annotation file per image file. Each object is described by:  \n`class x_center y_center width height`  ","fa31341c":"# ArTaxOr Data Exploration\nThe Arthropod Taxonomy Orders dataset is a collection of highres images annotated with labels from the taxanomy rank [order](https:\/\/basicbiology.net\/biology-101\/taxonomy). Annotations have been made with [VoTT](https:\/\/github.com\/microsoft\/VoTT). VoTT stores all metadata in json files. In this kernel we will import all the metadata into DataFrames, do some statistical exploration, visualize images and their objects and finally export metadata to a variety of formats for object detection model training.  \nThe dataset is distributed under CC BY-NC-SA 4.0","68613d67":"Clearly, the object detection model must handle both very large and very small objects!  \nFinally, view some random images (re-run cell for a new selection).","027ca808":"## Image exploration\nTo understand the dataset, we will plot some images with object bounding boxes on top.","a8c3d84f":"The Seaborn jointplot gives more details:"}}