{"cell_type":{"206272ad":"code","4ccb235c":"code","9542dd89":"code","c5cabe21":"code","2fe6bdc4":"code","c2f42108":"code","01da47f4":"code","7fbe99ee":"code","1be4aedc":"code","fb6e4730":"code","d938b167":"code","d81f35d5":"code","76140db7":"code","8c743ad4":"code","6824f6d9":"code","f03bd9d3":"code","da5bd757":"code","03fdd5d7":"code","bca5737b":"code","724c18cd":"code","db824a38":"code","9adfde16":"code","4be4e000":"code","e8846716":"code","1c98d688":"code","cd00ed17":"code","742466b1":"code","b2ef2bce":"code","53b630aa":"code","96289a47":"code","95155bc3":"code","c6657a73":"code","c0838abe":"markdown","bcabbe0f":"markdown","a3a82195":"markdown","da70d5d4":"markdown","efd34398":"markdown","1936c0c5":"markdown","3c5e71de":"markdown"},"source":{"206272ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ccb235c":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport re\nimport string\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\nfrom keras.layers import Conv1D, GlobalMaxPool1D\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau","9542dd89":"treino = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","c5cabe21":"treino","2fe6bdc4":"treino.drop(columns=['id','keyword','location'], inplace=True)","c2f42108":"treino","01da47f4":"print(treino.target.value_counts())\ntreino.target.value_counts().plot(kind='bar')","7fbe99ee":"treino.isnull().sum()","1be4aedc":"abreviacao = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\",\n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how does\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \" u \": \" you \",\n    \" ur \": \" your \",\n    \" n \": \" and \",\n    \"won't\": \"would not\",\n    'dis': 'this',\n    'bak': 'back',\n    'brng': 'bring'}","fb6e4730":"treino.text = treino.text.apply(lambda x: x.lower())\ntreino.text = treino.text.replace(regex=abreviacao) \ntreino.text = treino.text.apply(lambda x: re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', x))\ntreino.text = treino.text.apply(lambda x: re.sub('(@[A-Za-z]+[A-za-z0-9-_]+)', '', x))\ntreino.text = treino.text.apply(lambda x: re.sub('rt','',x))\ntreino.text = treino.text.apply(lambda x: re.sub('\\<http.+?\\>', '', x))\ntreino.text = treino.text.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\ntreino.text = treino.text.apply(lambda x: re.sub(' +', ' ', x))","d938b167":"tm = []\nfor t in treino.text:\n    tm.append(len(t.split(' ')))\ntamanho = np.max(tm)\nprint('Tamanho m\u00e1ximo da senten\u00e7a:', tamanho)","d81f35d5":"# N\u00famero m\u00e1ximo de palavras usadas\nmax_palavra = 15000\n\n# Tamanho da dimens\u00e3o\nembedding_dim = 20\n\ntk = Tokenizer(num_words=max_palavra)\ntk.fit_on_texts(treino.text.values)\npl_unicas = tk.word_index\nprint('Quantidade de palavras \u00fanicas: ', len(pl_unicas)+1)","76140db7":"# Contando palavras\nminimo = 2\n\ncontagem = 0 \ntotal_contagem = 0\nfrequencia = 0\ntotal_frequencia = 0\n\nfor key, value in tk.word_counts.items():\n    total_contagem += 1\n    total_frequencia = total_frequencia + value\n    if (value < minimo):\n        contagem += 1\n        frequencia = frequencia + value\n        \nprint(\"% de palavras raras:\",(contagem\/total_contagem)*100)\nprint(\"Total de palavras raras:\",(frequencia\/total_frequencia)*100)\nprint('Palavras raras: ',contagem)\nprint('Tamanho do vocabul\u00e1rio: ',total_contagem)\nprint('Palavras mais comuns: ', total_contagem-contagem)","8c743ad4":"tk = Tokenizer(total_contagem-contagem)\ntk.fit_on_texts(list(treino.text))\npl_unicas = tk.word_index\nprint('Quantidade de palavras \u00fanicas: ', len(pl_unicas)+1)","6824f6d9":"x = tk.texts_to_sequences(treino.text.values)\nx = pad_sequences(x, maxlen=tamanho, padding='post')\nprint('Quantidade de dados de x: ', x.shape)","f03bd9d3":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ny = lb.fit_transform(treino.target.values)\n\n#y = pd.get_dummies(treino.target.values)\n#print('Quantidade de dados de y: ', y.shape)","da5bd757":"y","03fdd5d7":"x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size=0.2, stratify=y)\nprint(x_treino.shape, y_treino.shape)\nprint(x_teste.shape, y_teste.shape)","bca5737b":"batch_size = 128\n\nparada = EarlyStopping(monitor='val_accuracy', mode='min', patience=10, baseline=0.4, min_delta=.0001)\nredutor = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.0001)","724c18cd":"modelo_ngram = Sequential()\nmodelo_ngram.add(Embedding(10000,30))\nmodelo_ngram.add(Conv1D(filters=15, kernel_size=2, padding='same', activation='relu'))\nmodelo_ngram.add(Conv1D(filters=15, kernel_size=3, padding='same', activation='relu'))\nmodelo_ngram.add(Conv1D(filters=15, kernel_size=4, padding='same', activation='relu'))\nmodelo_ngram.add(GlobalMaxPool1D())\nmodelo_ngram.add(Dense(128, activation='relu'))\nmodelo_ngram.add(Dropout(0.1))\nmodelo_ngram.add(Dense(1, activation='sigmoid'))\nmodelo_ngram.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodelo_n = modelo_ngram.fit(x_treino, y_treino, epochs=100, batch_size=batch_size, verbose=1,\n                  validation_data=(x_teste, y_teste),callbacks=[redutor, parada])\n\nloss, acc = modelo_ngram.evaluate(x_teste, y_teste)","db824a38":"fig, axes = plt.subplots(1, 2, figsize=(15,5))\naxes[0].plot(modelo_n.history['accuracy'])\naxes[0].plot(modelo_n.history['val_accuracy'])\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Acur\u00e1cia')\naxes[0].legend(['Acur\u00e1cia em Treino','Acur\u00e1cia em Teste'])\naxes[0].grid(True)\n\naxes[1].plot(modelo_n.history['loss'])\naxes[1].plot(modelo_n.history['val_loss'])\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('Erro')\naxes[1].legend(['Erro em Treino','Erro em Teste'])\naxes[1].grid(True)","9adfde16":"teste = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","4be4e000":"teste","e8846716":"teste.drop(columns=['id','keyword','location'], inplace=True)\n\nteste.text = teste.text.apply(lambda x: x.lower())\nteste.text = teste.text.replace(regex=abreviacao) \nteste.text = teste.text.apply(lambda x: re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', x))\nteste.text = teste.text.apply(lambda x: re.sub('(@[A-Za-z]+[A-za-z0-9-_]+)', '', x))\nteste.text = teste.text.apply(lambda x: re.sub('rt','',x))\nteste.text = teste.text.apply(lambda x: re.sub('\\<http.+?\\>', '', x))\nteste.text = teste.text.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\nteste.text = teste.text.apply(lambda x: re.sub(' +', ' ', x))","1c98d688":"tm = []\nfor t in teste.text:\n    tm.append(len(t.split(' ')))\ntamanho = np.max(tm)\nprint('Tamanho m\u00e1ximo da senten\u00e7a:', tamanho)","cd00ed17":"#t = Tokenizer(total_contagem-contagem)\n#t.fit_on_texts(list(treino.text))\n#pl_unicas = t.word_index\n#print('Quantidade de palavras \u00fanicas: ', len(pl_unicas)+1)\n\npre = tk.texts_to_sequences(teste.text.values)\npre = pad_sequences(pre, maxlen=38, padding='post')","742466b1":"previsto = modelo_ngram.predict(pre)","b2ef2bce":"previsto = (np.round(previsto)).astype(int)\nprevisto","53b630aa":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","96289a47":"submission['target'] = previsto","95155bc3":"submission","c6657a73":"submission.to_csv('submission.csv', index=False)","c0838abe":"# Carregando os Dados","bcabbe0f":"# Padding","a3a82195":"# Limpando os dados","da70d5d4":"# N-Gram","efd34398":"# Deletando Colunas","1936c0c5":"# Tokens","3c5e71de":"# Visualizando os Dados"}}