{"cell_type":{"83c44c94":"code","f44f5914":"code","58e2f265":"code","38481a5d":"code","41b3f42b":"code","280f802a":"code","e8071375":"code","733b43eb":"code","30eb7da1":"markdown","61d58a00":"markdown","1edae69e":"markdown","917fab06":"markdown","fc963a25":"markdown","3ef5f439":"markdown"},"source":{"83c44c94":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f44f5914":"data = pd.read_csv('\/kaggle\/input\/melbourne-housing-market\/MELBOURNE_HOUSE_PRICES_LESS.csv')\nprint(data.columns)\ndata.head()","58e2f265":"data.drop(columns={'Suburb', 'Address','Type', 'Method', 'SellerG',\n       'Date', 'Postcode', 'Regionname', 'Propertycount', 'Distance',\n       'CouncilArea'},inplace=True)\ndata.head()               #Taking necessary data columns for examples","38481a5d":"from sklearn.preprocessing import scale\nprint(data['Price'][:5])\nprint(\"\\n Scaled Data:\")\nscale(data['Price'][:5])","41b3f42b":"X = data['Price'][:5]\n(X - np.mean(X)) \/ (np.std(X))    #the output is same","280f802a":"from sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\nX = data['Price'][:5]\nX=np.array(X.values.tolist())\nY=X.reshape(-1, 1)\nprint(data['Price'][:5])\nprint(\"\\n Scaled Data:\")\nprint(std_scaler.fit(Y))\nprint(std_scaler.transform(Y))","e8071375":"from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()            #by default the range is set (0,1)\nX = data['Price'][:5]\nX=np.array(X.values.tolist())\nY=X.reshape(-1, 1)\nprint(data['Price'][:5])\nprint(\"\\n Scaled Data:\")\nprint(mm_scaler.fit(Y))                      \nprint(mm_scaler.transform(Y))","733b43eb":"X = data['Price'][:5]\n(X - np.min(X)) \/ (np.max(X) - np.min(X))   #we can see that the output is similar to the one from using MinMaxScaler()","30eb7da1":"The math behind MinMaxScaler():\n\nAssume **D** to be the data points.\nThe scale function does this math :\n\n## $\\frac{D_{i} - {min(D)} }{{max(D)} - {min(D)} }$","61d58a00":"# scale()\nStandardize a dataset along any axis.\nCenter to the mean and component wise scale to unit variance.\n\nAssume **D** to be the data points.\nThe scale function does this math :\n\n## $\\frac{D_{i} - \\bar{D} }{\\sigma{(D)}}$\n\n$Where,$\n\n $\\bar{D} : Mean $\n\n $\\sigma{(D)} : Standard Deviation $","1edae69e":"# Why Feature Scaling ?\nWhen a feature in the dataset is too large when compared to other, then in models which use algorithms, where Euclidean distance is measured this big scaled feature becomes dominating and needs to be normalized.\n\nExample: In the above data columns selected, Price is way large when compared to Room column data points.  \n\nTo solve this issue, Feature Scaling is used.\n**sklearn** provides feature scaling functions in its **preprocessing** module. ","917fab06":"# MinMaxScaler()\n\nTransform features by scaling each feature to a given range.This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.","fc963a25":"# StandardScaler()\nDoes the same thing as scale() function but preprocessing.StandardScaler() is a class supporting the Transformer API.","3ef5f439":"#### Sources:\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html?highlight=minmax%20scaler#sklearn.preprocessing.MinMaxScaler\n- https:\/\/www.kaggle.com\/vin1234\/why-feature-scaling-is-important\n- https:\/\/stackoverflow.com\/questions\/46257627\/scikit-learn-preprocessing-scale-vs-preprocessing-standardscalar"}}