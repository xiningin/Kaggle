{"cell_type":{"a2116528":"code","1e604605":"code","9f60b0ed":"code","77efe35c":"code","a5060635":"code","d43156ff":"code","4699a239":"code","cefff288":"code","f8ea257f":"code","7a8289f8":"code","c8ad90c7":"code","073bb147":"code","e7db4574":"code","b0818b12":"code","5761ece9":"code","a5934ba9":"code","ed11ea93":"code","fcec651c":"code","fd510e9e":"code","349a4326":"code","bcbbab46":"code","231f499d":"code","14819218":"code","264656d0":"code","0c7861a0":"code","8380ba93":"code","7680fd22":"markdown","5ace530c":"markdown","283a2e2e":"markdown","1b80dbb2":"markdown","4b8e923e":"markdown","60caec64":"markdown","6119ce5b":"markdown","693231e7":"markdown","b07c311d":"markdown","72f7566f":"markdown","57226841":"markdown","814257fb":"markdown","b1208e45":"markdown"},"source":{"a2116528":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e604605":"pip install prince","9f60b0ed":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#Train data\nfrom sklearn.model_selection import train_test_split\n#FAMD\nimport prince\n#Normalization\nfrom sklearn.preprocessing import StandardScaler\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Evaluation\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas_profiling as pp\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","77efe35c":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndata.info()","a5060635":"sns.countplot(x=\"target\", data=data, palette=\"bwr\")\nplt.xlabel(\"Heart Disease (0= No, 1= Yes)\")\nplt.ylabel('Number')\nplt.show()","d43156ff":"plt.figure(figsize=(20,6))\nsns.countplot(x='age', data=data, hue='target')\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.legend([\"Haven't Disease\", \"Have Disease\"])","4699a239":"plt.figure(figsize=(14,10))\n\nsns.countplot(x='sex', data=data, hue='target', palette='rainbow')\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.ylabel('Frequency')\nplt.legend([\"Haven't Disease\", \"Have Disease\"])","cefff288":"plt.figure(figsize=(14,10))\nsns.countplot(x='cp',data=data, hue='target',palette='rainbow')\nplt.title('Heart Disease Frequency According to The chest pain experienced')\nplt.xlabel('The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)')\nplt.ylabel('Frequency')\nplt.legend([\"Haven't Disease\", \"Have Disease\"])","f8ea257f":"plt.figure(figsize=(14,10))\nsns.countplot(x='fbs',data=data, hue='target',palette='Set1')\nplt.title('Heart Disease Frequency According To FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg\/dl) (1 = true; 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","7a8289f8":"plt.figure(figsize=(14,10))\nsns.countplot(x='slope',data=data, hue='target',palette='Set1')\nplt.title('Heart Disease Frequency According To Slope')\nplt.xlabel('The slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","c8ad90c7":"plt.figure(figsize=(14,10))\nsns.countplot(x='restecg',data=data, hue='target',palette='rainbow')\nplt.title('Heart Disease Frequency According To Resting electrocardiographic measurement ')\nplt.xlabel(\"Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\")\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","073bb147":"plt.figure(figsize=(14,10))\nsns.countplot(x='exang',data=data, hue='target',palette='bwr')\nplt.title('Heart Disease Frequency According To Exercise induced angina')\nplt.xlabel(\"Exercise induced angina (1 = yes; 0 = no)\")\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","e7db4574":"#Define numeric and categorical type of data\ndata.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n\ndata['sex'][data['sex'] == 0] = 'female'\ndata['sex'][data['sex'] == 1] = 'male'\n\ndata['chest_pain_type'][data['chest_pain_type'] == 1] = 'typical angina'\ndata['chest_pain_type'][data['chest_pain_type'] == 2] = 'atypical angina'\ndata['chest_pain_type'][data['chest_pain_type'] == 3] = 'non-anginal pain'\ndata['chest_pain_type'][data['chest_pain_type'] == 4] = 'asymptomatic'\n\ndata['fasting_blood_sugar'][data['fasting_blood_sugar'] == 0] = 'lower than 120mg\/ml'\ndata['fasting_blood_sugar'][data['fasting_blood_sugar'] == 1] = 'greater than 120mg\/ml'\n\ndata['rest_ecg'][data['rest_ecg'] == 0] = 'normal'\ndata['rest_ecg'][data['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndata['rest_ecg'][data['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\ndata['exercise_induced_angina'][data['exercise_induced_angina'] == 0] = 'no'\ndata['exercise_induced_angina'][data['exercise_induced_angina'] == 1] = 'yes'\n\ndata['st_slope'][data['st_slope'] == 1] = 'upsloping'\ndata['st_slope'][data['st_slope'] == 2] = 'flat'\ndata['st_slope'][data['st_slope'] == 3] = 'downsloping'\n\ndata['thalassemia'][data['thalassemia'] == 1] = 'normal'\ndata['thalassemia'][data['thalassemia'] == 2] = 'fixed defect'\ndata['thalassemia'][data['thalassemia'] == 3] = 'reversable defect'","b0818b12":"data.dtypes","5761ece9":"#Data Stratification Holdout (80\/20)\nX = data.iloc[:,0:13]  #independent columns\ny = data.iloc[:,-1]    #target column \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0, stratify=y)\nprint(' y_train type count:\\n',y_train.value_counts())\nprint(' y_test type count:\\n', y_test.value_counts())","a5934ba9":"#FAMD\nfamd = prince.FAMD(n_components=24,n_iter=3)\nX_train_fit= famd.fit(X_train)\nX_train_famd=X_train_fit.row_coordinates(X_train)\n","ed11ea93":"sns.heatmap(X_train_famd.corr(),cmap=\"YlGnBu\")","fcec651c":"X_train_fit.plot_row_coordinates(X_train)","fd510e9e":"#Initial 20 derived features contribution to the proposed model, MIFH.\nx= [i for i in range(20)]\nplt.xticks(x)\nplt.bar(x,famd.explained_inertia_[0:20], )","349a4326":"#Normalization\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nsc.fit(X_train_famd)\n\nX_train_norm = sc.transform(X_train_famd)\nX_train_norm = pd.DataFrame(X_train_norm,index=X_train_famd.index)\nX_train_norm\n","bcbbab46":"#Test data\nfamd = prince.FAMD(n_components=24,n_iter=3)\nX_test_fit = famd.fit(X_test)\nX_test_famd=X_test_fit.row_coordinates(X_test)\n\nsc.fit(X_test_famd)\nX_test_norm = sc.transform(X_test_famd)\nX_test_norm = pd.DataFrame(X_test_norm,index=X_test_famd.index)\nX_test_norm","231f499d":"#Logistic Regression Hyperparameter Tuning\ntrain_acc_lr=[]\ntest_acc_lr=[]\nlist_para_lr=[]\nfor i in range(2,24):\n    X_train_final=X_train_norm.iloc[:,0:i]\n    X_test_final = X_test_norm.iloc[:,0:i]\n    parameters=[\n    {\n        'penalty':['l1','l2'],\n        'C':[0.1,0.5,1],\n        'solver':['saga','liblinear']\n        },\n    ]\n    \n    lr_cv = GridSearchCV(LogisticRegression(), parameters, scoring=\"accuracy\", n_jobs=-1, verbose=1,cv=3)\n    lr_cv.fit(X_train_final, y_train)\n    best_params = lr_cv.best_params_\n    test_score = accuracy_score(y_test, lr_cv.predict(X_test_final)) * 100\n    train_score = accuracy_score(y_train, lr_cv.predict(X_train_final)) * 100\n    train_acc_lr.append(train_score)\n    test_acc_lr.append(test_score)\n    list_para_lr.append(best_params)\n        \ntuning_lr_dic={'Parameters': list_para_lr,'Train Score': train_acc_lr, 'Test Score': test_acc_lr}\ntuning_lr_df=pd.DataFrame(tuning_lr_dic)\ntuning_lr_df","14819218":"#Decision Tree Hyperparameter Tuning\ntrain_acc_dt=[]\ntest_acc_dt=[]\nlist_para_dt=[]\nfor i in range(2,24):\n  X_train_final =X_train_norm.iloc[:,0:i]\n  X_test_final = X_test_norm.iloc[:,0:i]\n  parameters = [\n  {\n    \"criterion\":[\"gini\", \"entropy\"], \n    \"splitter\":[\"best\", \"random\"],\n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20))\n    \n      }\n  ]\n  dt_clf = DecisionTreeClassifier(random_state=42)\n  dt_cv = GridSearchCV(dt_clf, parameters, scoring=\"accuracy\", n_jobs=-1, verbose=1,cv=3)\n  dt_cv.fit(X_train_final, y_train)\n  best_params = dt_cv.best_params_\n  dt_clf = DecisionTreeClassifier(**best_params)\n  dt_clf.fit(X_train_final, y_train)\n  test_score = accuracy_score(y_test, dt_clf.predict(X_test_final)) * 100\n  train_score = accuracy_score(y_train, dt_clf.predict(X_train_final)) * 100\n  train_acc_dt.append(train_score)\n  test_acc_dt.append(test_score)\n  list_para_dt.append(best_params)\n\ntuning_dt_dic={'Parameters': list_para_dt,'Train Score': train_acc_dt, 'Test Score': test_acc_dt}\ntuning_dt_df=pd.DataFrame(tuning_dt_dic)\ntuning_dt_df  \n","264656d0":"#Random Forest Hyperparameter Tuning\ntrain_acc_rf=[]\ntest_acc_rf=[]\nlist_para_rf=[]\nfor i in range(2,24):\n  X_train_final =X_train_norm.iloc[:,0:i]\n  X_test_final = X_test_norm.iloc[:,0:i]\n  \n  parameters = {\n      'criterion': ['gini','entropy'],\n      'max_depth': np.arange(1, 5),\n      'min_samples_split': np.arange(2, 10),\n      'n_estimators': np.arange(10, 20)\n              }\n  rf_clf = RandomForestClassifier(random_state=42)\n  rf_cv = GridSearchCV(rf_clf, parameters, scoring=\"accuracy\", n_jobs=-1, verbose=1,cv=3)\n  rf_cv.fit(X_train_final, y_train)\n  best_params = rf_cv.best_params_\n  rf_clf = RandomForestClassifier(**best_params)\n  rf_clf.fit(X_train_final, y_train)\n  test_score = accuracy_score(y_test, rf_clf.predict(X_test_final)) * 100\n  train_score = accuracy_score(y_train, rf_clf.predict(X_train_final)) * 100\n  train_acc_rf.append(train_score)\n  test_acc_rf.append(test_score)\n  list_para_rf.append(best_params)\n\ntuning_rf_dic={'Parameters': list_para_rf,'Train Score': train_acc_rf, 'Test Score': test_acc_rf}\ntuning_rf_df=pd.DataFrame(tuning_rf_dic)\ntuning_rf_df  \n","0c7861a0":"#Support Vector Machine Hyperparameter Tuning\ntrain_acc_svc=[]\ntest_acc_svc=[]\nlist_para_svc=[]\nfor i in range(2,24):\n  X_train_final =X_train_norm.iloc[:,0:i]\n  X_test_final = X_test_norm.iloc[:,0:i]\n  \n  parameters = {\n      'C':[0.1, 0.5, 1, 2, 5,10],\n      \"kernel\":['linear', 'poly', 'rbf'],\n      \"gamma\":['scale','auto']\n              }\n  svc_clf = SVC(random_state=42)\n  svc_cv = GridSearchCV(svc_clf, parameters, scoring=\"accuracy\", n_jobs=-1, verbose=1,cv=3)\n  svc_cv.fit(X_train_final, y_train)\n  best_params = svc_cv.best_params_\n  svc_clf = SVC(**best_params)\n  svc_clf.fit(X_train_final, y_train)\n  test_score = accuracy_score(y_test, svc_clf.predict(X_test_final)) * 100\n  train_score = accuracy_score(y_train, svc_clf.predict(X_train_final)) * 100\n  train_acc_svc.append(train_score)\n  test_acc_svc.append(test_score)\n  list_para_svc.append(best_params)\n\ntuning_svc_dic={'Parameters': list_para_svc,'Train Score': train_acc_svc, 'Test Score': test_acc_svc}\ntuning_svc_df=pd.DataFrame(tuning_svc_dic)\ntuning_svc_df  \n","8380ba93":"#K-nearest neighbors Hyperparameter Tuning\ntrain_acc_knn=[]\ntest_acc_knn=[]\nlist_para_knn=[]\nfor i in range(2,24):\n  X_train_final =X_train_norm.iloc[:,0:i]\n  X_test_final = X_test_norm.iloc[:,0:i]\n  \n  parameters = {\n      'n_neighbors':np.arange(2,80),\n      'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n      'weights':['uniform', 'distance']\n              }\n  knn_clf = KNeighborsClassifier()\n  knn_cv = GridSearchCV(knn_clf, parameters, scoring=\"accuracy\", n_jobs=-1, verbose=1,cv=3)\n  knn_cv.fit(X_train_final, y_train)\n  best_params = knn_cv.best_params_\n  knn_clf = KNeighborsClassifier(**best_params)\n  knn_clf.fit(X_train_final, y_train)\n  test_score = accuracy_score(y_test, knn_clf.predict(X_test_final)) * 100\n  train_score = accuracy_score(y_train, knn_clf.predict(X_train_final)) * 100\n  train_acc_knn.append(train_score)\n  test_acc_knn.append(test_score)\n  list_para_knn.append(best_params)\n\ntuning_knn_dic={'Parameters': list_para_knn,'Train Score': train_acc_knn, 'Test Score': test_acc_knn}\ntuning_knn_df=pd.DataFrame(tuning_knn_dic)\ntuning_knn_df  ","7680fd22":"# NORMALIZATION","5ace530c":"# 3. Random Forest","283a2e2e":"# FAMD","1b80dbb2":"* `Data stratification`: to divide the dataset into training and validation sets to eliminate the unbalancing effect of disease classes. \n* `Feature extraction`: Factor analysis of mixed data (FAMD) is a factorial method dedicated to both types of features and principal components analysis (PCA): categorical and multiple correspondence analysis (MCA): numeric features => 24 FEATURES.\n* `Features normalization`: In this way, total of 22 new training datasets are created with varying feature F = {F1, F2, \u00b7 \u00b7 \u00b7 , Fm}, where 2 \u2264 m \u2264 24. Using unit mean and zero standard deviation.","4b8e923e":"1. Preprocessing \n   \n   1.1. Data Stratification\n    \n   1.2. FAMD (Factor analysis of mixed data)\n    \n   1.3. Normalization\n \n2. Models Hyperparameter Tuning\n   \n   2.1. Logistic Regression\n   \n   2.2. Decision Tree\n   \n   2.3. Random Forest","60caec64":"# 5. KNN","6119ce5b":"1. `age` - age in years\n2. `sex` - (1 = male; 0 = female)\n3. `cp` - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. `trestbps` - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. `chol` - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. `fbs` - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. `restecg` - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. `thalach` - maximum heart rate achieved\n9. `exang` - exercise induced angina (1 = yes; 0 = no)\n10. `oldpeak` - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. `slope` - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. `ca` - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. `thal` - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. `target` - have disease or not (1=yes, 0=no) (= the predicted attribute)","693231e7":"# 4. Support Vector Machine","b07c311d":"# DATA STRATIFICATION\n","72f7566f":"# 2. Decision Tree","57226841":"# VISUALIZATION","814257fb":"# 1. Logistic Regression","b1208e45":"# MODELS HYPERPARAMETER TUNING"}}