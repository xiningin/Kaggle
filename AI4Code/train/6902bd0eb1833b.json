{"cell_type":{"c225cd6f":"code","5d99933c":"code","61f26ac3":"code","03d8b8ff":"code","f8c206be":"code","1bc34b3f":"code","8aa4e38c":"code","bdf29131":"code","5320471f":"code","feb3b585":"code","97e71a5d":"code","299fa0c3":"code","1a23b710":"code","61486c40":"code","53c9c826":"code","ff2db50d":"code","66cff599":"code","9f1b7df6":"code","d670646e":"code","6c2d0e64":"code","55793c9b":"code","8ed1105e":"code","1aea28f8":"code","53acf4ee":"code","7820e0ae":"code","e18b6a33":"code","79c5bc69":"code","908fa0d2":"code","431e8317":"code","ea00b113":"code","5fcd5c1b":"code","93b961db":"code","05289891":"code","7b80a6c2":"code","5d761d9a":"code","9aa2c68c":"code","5a76789e":"code","63930142":"code","204c0433":"code","0055cf74":"code","66bdd87d":"code","fa04721b":"code","2bad31fd":"code","b7df9f9f":"code","1d0d1c28":"code","0138b411":"code","9e168a40":"code","44e6c8b7":"code","55e9656e":"code","89f58a6b":"code","62c67a1d":"code","764e39d2":"code","0bb04a22":"code","43e9538d":"code","32688e38":"code","23cbb2b1":"code","436bbf8f":"code","3a14e0c7":"code","369f1eeb":"markdown","01cf5b11":"markdown","3f6e19c3":"markdown","2deaa790":"markdown","907670f8":"markdown","b12e7709":"markdown","71fe7382":"markdown","e8e3edc7":"markdown","0d3897e3":"markdown","caf7c025":"markdown","7ef64001":"markdown","30edd11a":"markdown","28db3acc":"markdown","793b6ecb":"markdown","b5c565d3":"markdown","41585771":"markdown","faf1f6df":"markdown","c5b4cf17":"markdown","ce130b7d":"markdown","faec51c7":"markdown","fafdcb3e":"markdown","3f63b5d1":"markdown","0698759d":"markdown","844250b8":"markdown","a851ccfc":"markdown"},"source":{"c225cd6f":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","5d99933c":"companies= pd.read_csv(\"..\/input\/company-stocks\/company_stocks.csv\")","61f26ac3":"companies.head()","03d8b8ff":"stock_diff=companies.iloc[:,1:964]\n#stock_diff=companies.loc[:,companies.columns != 'Company']\nCompany_Name=companies.iloc[:,0]","f8c206be":"from sklearn.preprocessing import StandardScaler\nstandard=StandardScaler()","1bc34b3f":"scale_fit_transform = StandardScaler().fit_transform(stock_diff)\npd.DataFrame(scale_fit_transform)[:5]","8aa4e38c":"cov_mat = np.cov(scale_fit_transform)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)","bdf29131":"print('\\nEigenvalues \\n%s' %eig_vals)\nprint('Eigenvectors \\n%s' %eig_vecs)","5320471f":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nimport sklearn.cluster as cluster","feb3b585":"pca = PCA()","97e71a5d":"X_r = pca.fit(scale_fit_transform).transform(scale_fit_transform)\nprint('\\nEigenvalues \\n%s' %pca.explained_variance_)\nprint('Eigenvectors \\n%s' %pca.components_)","299fa0c3":"pca = PCA(n_components=18)","1a23b710":"principal_components = pca.fit_transform(scale_fit_transform)","61486c40":"pca.explained_variance_ratio_","53c9c826":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","ff2db50d":"import matplotlib.pyplot as plt","66cff599":"def scree_plot():\n    from matplotlib.pyplot import figure, show\n    from matplotlib.ticker import MaxNLocator\n    plt.style.use(\"fivethirtyeight\")\n    ax = figure(figsize=(15,8)).gca()\n    ax.plot(pca.explained_variance_)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Principal Component')\n    plt.ylabel('Eigenvalue')\n    plt.title('Scree Plot of PCA: Component Eigenvalues',fontsize=15)\n    show()\nscree_plot()","9f1b7df6":"def var_explained():\n    import numpy as np\n    from matplotlib.pyplot import figure, show\n    from matplotlib.ticker import MaxNLocator\n    ax = figure(figsize=(15,8)).gca()\n    ax.plot(np.cumsum(pca.explained_variance_ratio_))\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    #plt.axvline(x=16, linewidth=1, color='r', alpha=0.5)\n    plt.title('Explained Variance of PCA by Component',fontsize=15)\n    show()\nvar_explained()","d670646e":"plt.figure(figsize=(15,8))\nplt.style.use(\"fivethirtyeight\")\nplt.scatter(x=[i+1 for i in range(len(pca.explained_variance_ratio_))],\n            y=pca.explained_variance_ratio_,\n           s=200, alpha=0.75,c='orange',edgecolor='k')\nplt.grid(True)\nplt.title(\"Explained variance ratio of the fitted principal component vector\\n\",fontsize=15)\nplt.xlabel(\"Principal components\",fontsize=15)\nplt.xticks([i+1 for i in range(len(pca.explained_variance_ratio_))],fontsize=15)\nplt.yticks(fontsize=15)\nplt.ylabel(\"Explained variance ratio\",fontsize=15)\nplt.show()","6c2d0e64":"pca.explained_variance_ratio_","55793c9b":"principal_df = pd.DataFrame(data = principal_components, \\\n                            columns = ['PC_1', 'PC_2','PC_3','PC_4','PC_5','PC_6','PC_7','PC_8','PC_9','PC_10','PC_11',\\\n                                       'PC_12','PC_13','PC_14','PC_15','PC_16','PC_17','PC_18'])\nprincipal_df.head()                     ","8ed1105e":"plt.figure(figsize=(15,7))\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(1,19), np.cumsum(pca.explained_variance_ratio_), linestyle=\":\", marker='o', label='Cumulative')\nplt.plot(range(1,19), pca.explained_variance_ratio_, marker='o', label='Proportion')\nplt.xlabel(\"Number of Principal Components\")\nplt.ylabel(\"Explained Variance\")\nplt.title(\"Scree Plot\",fontsize=15)\nplt.plot([16]*10, np.linspace(0,1,10), \":\")\nplt.text(16.1, 0.6, \"optimal number of components = 16\")\nplt.legend(loc='best')\nplt.show()","1aea28f8":"from sklearn.metrics import silhouette_score\nsse_=[]\nfor k in range(2,20):\n    kmeans=KMeans(n_clusters=k).fit(scale_fit_transform)\n    sse_.append([k,silhouette_score(scale_fit_transform,kmeans.labels_)])","53acf4ee":"plt.figure(figsize=(15,7))\nplt.xlabel(\"Number of K \",fontsize=10)\nplt.ylabel(\"Silouette Scores\",fontsize=10)\nplt.title(\"Cluster versus Silhouette Score\",fontsize=15)\nplt.plot(pd.DataFrame(sse_)[0],pd.DataFrame(sse_)[1])\nplt.show()","7820e0ae":"ssq = []\nfor K in range(1,20):\n    kmeans_model = KMeans(n_clusters=K, random_state=123)\n    kmeans_model.fit(scale_fit_transform)\n    ssq.append(kmeans_model.inertia_)","e18b6a33":"plt.figure(figsize=(15,7))\nplt.style.use(\"Solarize_Light2\")\nplt.plot(range(1,20), ssq, marker='o')\nplt.xlabel(\"Number of clusters(k)\")\nplt.ylabel(\"Within-cluster SSQ(Inertia)\")\nplt.title(\"Scree Plot\")\nplt.plot([5]*35000, range(1,35001), \":\")\nplt.text(5.1, 35000, \"optimal number of clusters = 5\")\nplt.show()","79c5bc69":"import sklearn.metrics as metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom yellowbrick.cluster.elbow import kelbow_visualizer\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom yellowbrick.cluster import silhouette_visualizer","908fa0d2":"# for i, exp_var in enumerate(exp_var_ratio.cumsum()):\n#     if exp_var >= 0.9:\n#         n_comps = i + 1\n#         break\n# print(\"Number of components:\", n_comps)\n# pca = PCA(n_components=n_comps)\n# pca.fit(X)\n# scores_pca = pca.transform(X)","431e8317":"model=KMeans()","ea00b113":"plt.figure(figsize=(15,8))\nplt.style.use(\"bmh\")\nfor i in range(2,18):\n    labels=cluster.KMeans(n_clusters=i,random_state=123).fit(principal_df).labels_\nvisualizer = KElbowVisualizer(model, k=(2,18), metric='calinski_harabasz', timings=True)\nvisualizer.fit(principal_df)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","5fcd5c1b":"plt.figure(figsize=(15,8))\nplt.style.use(\"fivethirtyeight\")\nfor i in range(2,18):\n    labels=cluster.KMeans(n_clusters=i,random_state=123).fit(principal_df).labels_\nkelbow_visualizer(KMeans(random_state=4), principal_df, k=(2,18))\n    #print (\"Silhoutte score for k= \"+str(i)+\" is \"+str(metrics.silhouette_score(principal_df,labels,metric=\"euclidean\",random_state=200)))","93b961db":"#silhouette_visualizer(KMeans(5, random_state=42), principal_df, colors='yellowbrick')\nplt.figure(figsize=(15,8))\nplt.style.use(\"ggplot\")\nmodel = KMeans(5, random_state=42)\nvisualizer = SilhouetteVisualizer(model, colors='yellowbrick')\nvisualizer.fit(principal_df)        # Fit the data to the visualizer\nvisualizer.show()   \n#silhouette_visualizer(KMeans(5, random_state=42), principal_df, colors='yellowbrick')","05289891":"km = KMeans(n_clusters=5, random_state=123)\nkm.fit(principal_df)","7b80a6c2":"predicted_cluster = km.predict(principal_df)","5d761d9a":"predicted_cluster","9aa2c68c":"principal_df['category'] = pd.Series(predicted_cluster, index=companies.index)\ncompanies.index.name = \"Company Number\"","5a76789e":"df = pd.DataFrame({'labels':predicted_cluster,'companies':Company_Name})\ndf.sort_values('labels').head()","63930142":"import pylab\nimport scipy.cluster.hierarchy\nfrom scipy.cluster import hierarchy \nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.cluster.hierarchy import set_link_color_palette\nfrom sklearn.cluster import AgglomerativeClustering","204c0433":"set_link_color_palette(['yellow'])\nlink = linkage(principal_df, method='ward', metric='euclidean')","0055cf74":"principal_df.head()","66bdd87d":"fig = pylab.figure(figsize=(18,20))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.ylabel(\"Euclidean distance\")\n\ndendrogram(link,\n           leaf_rotation=0,  # rotates the x axis labels\n           leaf_font_size=10,\n          orientation = 'right')  # font size for the x axis labels\nplt.show()","fa04721b":"model = AgglomerativeClustering(n_clusters=16,linkage = 'complete')","2bad31fd":"model.fit(principal_df)","b7df9f9f":"predicted_cluster2 =  model.labels_","1d0d1c28":"silhouette_score(principal_df, model.labels_)","0138b411":"fig = pylab.figure(figsize=(12,8))\nsilhouette_visualizer(KMeans(16, random_state=42), principal_df, colors='yellowbrick')","9e168a40":"reduced_data = PCA(n_components = 3).fit_transform(principal_df)","44e6c8b7":"kmeans = KMeans(n_clusters=16)\nkmeans.fit(reduced_data)\nlabels = kmeans.predict(reduced_data)","55e9656e":"# create DataFrame aligning labels & companies\ndf2 = pd.DataFrame({'labels': labels, 'companies': Company_Name})","89f58a6b":"df2.head()","62c67a1d":"df1=pd.DataFrame(reduced_data)","764e39d2":"df1=pd.concat([df2,df1],axis=1)","0bb04a22":"df1=df1.set_axis([\"labels\",\"companies\",\"PC_1\",\"PC_2\",\"PC_3\"],axis=1, inplace=False)\ndf1","43e9538d":"from yellowbrick.cluster import InterclusterDistance\nvisualizer = InterclusterDistance(kmeans)\nvisualizer.fit(reduced_data)        # Fit the data to the visualizer\nvisualizer.show() ","32688e38":"# Scatter plot on Principal components to visualize the spread of the data\nx=int(input(\"Enter the Cluster value :\"))\ndf1[df1['labels']==x]","23cbb2b1":"plt.scatter(x=df2['labels'], y=df2['companies'], c='#d62728')","436bbf8f":"# Scatter plot on Principal components to visualize the spread of the data\n\nfig, axes = plt.subplots(1,2, figsize=(15,8))\n\nsns.scatterplot(x='PC_1',y='PC_2',hue='labels',legend='full',palette=\"Set1\",data=df1,ax=axes[0])\nsns.scatterplot(x='PC_1',y='PC_3',hue='labels',legend='full',palette=\"Set1\",data=df1,ax=axes[1])","3a14e0c7":"from sklearn.metrics import silhouette_score, davies_bouldin_score\nkm_scores= []\nkm_silhouette = []\ndb_score = []\nfor i in range(2,10):\n    km = KMeans(n_clusters=i, random_state=0).fit(principal_df)\n    preds = km.predict(principal_df)\n    print(\"Score for number of cluster(s) {}: {}\".format(i,km.score(principal_df)))\n    km_scores.append(-km.score(principal_df))\n    silhouette = silhouette_score(principal_df,preds)\n    km_silhouette.append(silhouette)\n    print(\"Silhouette score for number of cluster(s) {}: {}\".format(i,silhouette))\n    db = davies_bouldin_score(principal_df,preds)\n    db_score.append(db)\n    print(\"Davies Bouldin score for number of cluster(s) {}: {}\".format(i,db))\n    print(\"-\"*100)","369f1eeb":"#### Curse of Dimensionality\nWhen data does not have enough features, the model is likely to underfit, and when data has too many features, it is likely to overfit. Hence it is called the curse of dimensionality. \n\n**Principal Component Analysis (PCA)** Defined as the transformation of any high number of variables into a smaller number of uncorrelated variables called principal components (PCs), developed to capture as much of the data\u2019s variance as possible.","01cf5b11":"**By default, the parameter locate_elbow is set to True, which automatically find the \u201celbow\u201d which likely corresponds to the optimal value of k using the \u201cknee point detection algorithm\u201d.**","3f6e19c3":"Among 18 components we will choose those column which has cumulative variance ratio above 80%","2deaa790":"**The scoring parameter metric is set to distortion, which computes the sum of squared distances from each point to its assigned center. However, two other metrics can also be used with the KElbowVisualizer \u2013 silhouette and calinski_harabasz. The silhouette score calculates the mean Silhouette Coefficient of all samples, while the calinski_harabasz score computes the ratio of dispersion between and within clusters.**\n\n**The KElbowVisualizer also displays the amount of time to train the clustering model per K as a dashed green line, but is can be hidden by setting timings=False. In the following example, we\u2019ll use the calinski_harabasz score and hide the time to fit the model.**","907670f8":"**Limitations:**\nk-means algorithm will often be ineffective if the clusters have complicated geometries. In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries.\n\nSimplicity of k-means is a big advantage for fast processing of large scale data. But this very simplicity also leads to practical challenges in its application.\n\nIn particular, the non-probabilistic nature of k-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations.\n\nGaussian mixture models (GMMs), can be viewed as an extension of the ideas behind k-means, but can also be a powerful tool for estimation beyond simple clustering.","b12e7709":"The AgglomerativeClustering performs a hierarchical clustering using a bottom up approach. The linkage criteria determines the metric used for the merge strategy:\n\n-   Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n-   Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n-   Average linkage minimizes the average of the distances between all observations of pairs of clusters.","71fe7382":"In agglomerative clustering, at each iteration, the algorithm must update the distance matrix to reflect the distance of the newly formed cluster with the remaining clusters in the forest. \nThe following methods are supported in Scipy for calculating the distance between the newly formed cluster and each:\n```\n- single\n- complete\n- average\n- weighted\n- centroid\n```","e8e3edc7":"**Since the aim of PCA is data reduction, we need some criteria or we will have the same number of variables after a PCA.One rule of thumb is that a component should not be retained unless it has an eigenvalue greater than or equal to one (the \u2018Kaiser\u2019 criterion).**","0d3897e3":"#### The Explained Variance","caf7c025":"Intercluster distance maps display an embedding of the cluster centers in 2 dimensions with the distance to other centers preserved. the closer to centers are in the visualization, the closer they are in the original feature space. The clusters are sized according to a scoring metric. By default, they are sized by membership, e.g. the number of instances that belong to each center. This gives a sense of the relative importance of clusters.","7ef64001":"#### Predicting the Companies Cluster","30edd11a":"#### The historical stock price movement for 39 companies. This stock price movement is defined as the difference in dollar amount between the closing and opening prices for each trading day. The data was originally collected from Yahoo Finance for 2010 to 2015 duration. Each column represents a trading day and each row represents a company. Refer to the header and index for details about the trading day and companies.\n\nApply any of the clustering methods to partition companies into clusters. What insights can you gather from these clusters? ","28db3acc":"**Finding Covariance & Eigen Values**","793b6ecb":"### Agglomerative Clustering","b5c565d3":"### Explained variance plot","41585771":"The above plot means that the $1^{st}$ principal component explains about 17% of the total variance in the data and the $2^{nd}$ component explians further 9%. Therefore, if we just consider first two components, they together explain 26% of the total variance.","faf1f6df":"### Optimal Number of Prncipal component with Cumulative Explained variance gr. than 80","c5b4cf17":"#### Another important Metrics for measuring the Clustering Score is David Bouldin Score which says lower Davies-Bouldin index relates to a model with better separation between the clusters.Values closer to zero indicate a better partition. \n\nFrom the Scores we can see that for Cluster 4 the Scores is closer to zero and So No. of Cluster=4 is best for this model.\n\nReference: https:\/\/esigma6.wordpress.com\/2018\/11\/03\/2-3-9-7-davies-bouldin-index\/","ce130b7d":"### Intercluster Distance Maps\n","faec51c7":"### Dendrogram","fafdcb3e":"#### Fitting the components","3f63b5d1":"### Scree Plot","0698759d":"**Explained Variance**","844250b8":"### Scaling our Data by the help of Standard Scaler","a851ccfc":"### Pre-Analysisng the Clusters and Scores by Visualization"}}