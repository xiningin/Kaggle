{"cell_type":{"a4073873":"code","3965d3f1":"code","ecdeeaff":"code","bf1800ea":"code","561cb0e1":"code","6fb3858a":"code","8089c004":"code","61d06510":"code","80894138":"code","91b2fc3c":"code","9e0bb607":"code","25a13999":"code","a58b09c9":"code","6b70f74a":"code","33678a81":"code","e86935c2":"code","77f33d75":"code","7ba5b78e":"code","58b158e0":"code","f5654c5d":"code","61a148a2":"code","5633ee57":"code","f8c812b4":"code","fe457158":"code","da68facd":"code","ae3eb81c":"code","63ff814b":"code","4bc3b5e9":"code","c39a42aa":"code","46818fa1":"code","ca33986d":"code","8f107432":"code","983f7e4d":"code","e274c542":"code","4249bd00":"code","3bbae08b":"code","42fbdde1":"code","e2369f65":"code","afff580f":"code","b9f66f07":"code","76383a85":"code","de53a065":"code","dc8558a8":"code","1d05a42d":"code","84b4ef7d":"code","127a8723":"code","b64385ae":"code","1b7fd72e":"code","643e1279":"code","52e4d8d4":"code","2fe60912":"code","832ce71f":"code","eb4d714e":"code","93f3a52b":"code","dadded9d":"code","fd764d08":"code","aa799b05":"code","a82c05f2":"code","c882fe0e":"code","6bca783e":"code","5f5e026a":"code","86a69476":"code","ad6c6c8d":"code","a2b3e960":"code","04e7dba9":"code","34b3007b":"code","aada4f3c":"code","a4ee5de5":"markdown","24dcc0b7":"markdown","691ef4f3":"markdown","36fae59e":"markdown","060280d6":"markdown","ddf59b04":"markdown","2537da19":"markdown","5022c8c9":"markdown","22c14aa0":"markdown","4d6d6356":"markdown","a768948f":"markdown","16a51741":"markdown","b88452e4":"markdown","fb602072":"markdown","f2f01f52":"markdown","ad441a8a":"markdown","f3e6ca78":"markdown","e52527bc":"markdown","d439e2f0":"markdown","0ad4ddd8":"markdown","f399d143":"markdown","7566d7ef":"markdown","ce96ddfe":"markdown","d991dfb0":"markdown","a94ddd74":"markdown","aa8ac2be":"markdown","9e682792":"markdown","b290cf07":"markdown","b77be75e":"markdown","bafd9ce3":"markdown","95cd33ec":"markdown"},"source":{"a4073873":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\n\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nfrom PIL import Image\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom pandas import Series, DataFrame\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nimport warnings\n\nimport cv2\n\nimport time\n","3965d3f1":"im = cv2.imread('\/kaggle\/input\/totaldata\/Project Data\/combo\/02dd6098c5f9f51af380feca22578e02.png')\nim.shape\n","ecdeeaff":"device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#Creation of the Dataframe\ntrain_df_1 = pd.DataFrame(columns=[\"img_name\",\"label\"])\ntrain_df_2 = pd.DataFrame(columns=[\"img_name\",\"label\"])\n\ntrain_df_1[\"img_name\"] = os.listdir('\/kaggle\/input\/totaldata\/Project Data\/Disease\/')\ntrain_df_2[\"img_name\"] = os.listdir('\/kaggle\/input\/totaldata\/Project Data\/NotDisease\/')\n\ntrain_df = pd.concat([train_df_1, train_df_2])\n\nn = len(train_df_1)\n\nfor i in range(len(train_df)):\n    if i < n:\n        train_df['label'][i] = 1\n    else:\n        train_df['label'][i] = 0\n\n\n\n#To view the dataframe created\nprint(train_df)\n\n#Create a csv file out of our dataframe\ntrain_df.to_csv (r'train_csv.csv', index = False, header=True)\n","bf1800ea":"classes=['NoROP','ROP']\nfor label_number in range(2):\n    print('Label',label_number,'corresponds to',classes[label_number])","561cb0e1":"ListOfLabels=train_df[\"label\"].to_list()\n\n#Verification of the list\n#print(ListOfLabels)\n#print(len(ListOfLabels))\n\n#Here we are gonna count how many images for each stages we have\nNoROP_Size=ListOfLabels.count(0)\nprint('Number of NoROP images', NoROP_Size)\n\n#ROP\nStage1_Size=ListOfLabels.count(1)\nprint('Number of ROP images', Stage1_Size)\n\n#Check the images count\n#print(NoROP_Size+Stage1_Size+Stage2_Size+Stage3_Size)\n\nprint('the size of our dataset is',len(train_df[\"label\"]))\nn = len(train_df[\"label\"])","6fb3858a":"transform = transforms.Compose([transforms.Resize((60, 80)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5,0.5,0.5),\n                                                     (0.5,0.5,0.5))])\n    ","8089c004":"\n# Creation of the dataset we will use for our deep learning model\nclass Stage1NoROP(Dataset):\n    def __init__(self, root_dir, annotation_file, transform=None):\n        self.root_dir = root_dir\n        self.annotations = pd.read_csv(annotation_file)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        pic_id = self.annotations.iloc[index, 0]\n        pic = Image.open(os.path.join(self.root_dir, pic_id)).convert(\"RGB\")\n        y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n\n        if self.transform is not None:\n            pic = self.transform(pic)\n\n        return (pic, y_label)","61d06510":"n*.15,n*.7","80894138":"2930+2931+13676-n","91b2fc3c":"dataset = Stage1NoROP('..\/input\/totaldata\/Project Data\/combo\/',\"train_csv.csv\",transform=transform)\n\nsplit = np.array([.7,.15,.15])\ntrain_set, test_set, validation_set = torch.utils.data.random_split(dataset,[13676, 2930, 2931])\n","9e0bb607":"#Unnormalize the object given as input\nclass UnnormalizingPicture(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor\n    \nUnnormalized = UnnormalizingPicture(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))","25a13999":"def imshow(img):\n    img=img\/2+0.5\n    npimg=img.numpy()\n    plt.imshow(np.transpose(npimg,(1,2,0)))","a58b09c9":"#Define Parameters and create figure as an empty \"table\"\nClass=len(classes)\ncolumn=3\ncolors = ('red', 'green', 'blue')\nfig=plt.figure(figsize=(20,30))\n\n#Complete the table by displaying pictures \n#and creating their associated histogram\ni=0\nfor i in range(Class):\n    #Loading Train Set\n    dataiter_train=iter(train_set)\n    image_train, idx_train=next(x for x in dataiter_train if (x[1]==i))\n    \n    #Loading Test set\n    dataiter_test=iter(test_set)\n    image_test, idx_test=next(x for x in dataiter_test if (x[1]==i))\n    \n    #Loading Validation set\n    dataiter_test=iter(validation_set)\n    image_validation, idx_validation=next(x for x in dataiter_test if (x[1]==i))\n    \n    #Creating first colum of the grid - One train set picture from each classe\n    ax=fig.add_subplot(Class,column,i*column+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.title.set_text(str(classes[i]+' from train set'))\n    #Display one picture of each class in each row of first colum\n    imshow(image_train)\n    print(image_train.shape)\n    \n    #Creating third Column of the grid - One test set picture from each classe\n    ax2=fig.add_subplot(Class,column,i*column+3)\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    ax2.title.set_text(str(classes[i]+' from validation set'))\n    #Display one picture of each class in each row of first colum\n    imshow(image_validation)\n    \n    \n    #Creating third Column of the grid - One test set picture from each classe\n    ax3=fig.add_subplot(Class,column,i*column+2)\n    ax3.set_xticks([])\n    ax3.set_yticks([])\n    ax3.title.set_text(str(classes[i]+' from test set'))\n    #Display one picture of each class in each row of first colum\n    imshow(image_test)\n    ","6b70f74a":"#Get the list of indexes \ndef GetListofIndexes(Dataset):\n    indexes=[]\n    i=0\n    while len(indexes)!=(len(Dataset)):\n        image,label=Dataset[i]\n        indexes.append(label)\n        i=i+1\n    return(indexes)","33678a81":"# Verification of GetListofIndexes function\nClassesTrain,Train_count = np.unique(GetListofIndexes(train_set), return_counts=True)\nprint(ClassesTrain)\nprint(Train_count)\nprint(sum(Train_count))\nClassesTest,Test_count = np.unique(GetListofIndexes(test_set), return_counts=True)\nprint(ClassesTest)\nprint(Test_count)\nprint(sum(Test_count))\nClassesV,V_count = np.unique(GetListofIndexes(validation_set), return_counts=True)\nprint(ClassesV)\nprint(V_count)\nprint(sum(V_count))","e86935c2":"plt.figure(figsize=(10,10))\nplt.title('Our data distribution')\n#Create x axis with each class (10 classes)\nclasses=['NorROP','ROP']\nplt.xticks(range(2), classes) \n# axis labels\nplt.xlabel('Classe label')\nplt.ylabel('Number of samples')\n#Bar creation\nplt.bar(range(len(Train_count)), Train_count, label='Train')\nplt.bar(range(len(Test_count)), Test_count, label='Test', bottom=Train_count)\nplt.bar(range(len(V_count)), V_count, label='Validation', bottom=[i+j for i,j in zip(Train_count, Test_count)])\nplt.legend()\nplt.show()","77f33d75":"learning_rate = 0.001\nbatch_size = 40\nshuffle = True\npin_memory = True\nnum_workers = 0","7ba5b78e":"train_loader = torch.utils.data.DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)","58b158e0":"#Parameters\nWeight=32\nHeight=32\nTotalNumberofClasses=2\n\n#Check GPU availability\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\n\n#Creation of our Model :  Le Net model\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3,6, kernel_size=5)\n        self.dropout = nn.Dropout(p=0.2) \n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.fc1 = nn.Linear(3264, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, TotalNumberofClasses)\n        \n\n    def forward(self, x):\n        #print(x.size())\n        x = F.relu(self.conv1(x))\n        #print(x.size())\n        x = F.max_pool2d(x, 2)\n        #print(x.size())\n        x = F.relu(self.conv2(x))\n        #print(x.size())\n        x = F.max_pool2d(x, 2)\n        #print(x.size())\n        x=F.relu(x)\n        #print(x.size())\n        x=self.dropout(x)\n        x = x.view(x.size(0),-1)\n        #print(x.size())\n        x = F.relu(self.fc1(x))\n\n        #print(x.size())\n        x = F.relu(self.fc2(x))\n        #print(x.size())\n        x = self.fc3(x)\n        return x\n    \n\nour_CNN = LeNet()\n\n# If GPU available, move the model to GPU.\nif cuda:\n    our_CNN.cuda()","f5654c5d":"!mkdir saved_models","61a148a2":"#Display our model architecture\nprint(our_CNN)","5633ee57":"im.shape","f8c812b4":"#Loss function chosen : Mean Squared error loss function\nLoss = nn.MSELoss()\n\n#Optimizer = AdamOptimizer\nOptimizer = torch.optim.Adam(our_CNN.parameters(), learning_rate)","fe457158":"start = time.time()\nnum_epochs = 75\n\ntrain_epoch_loss = []\nvalidation_epoch_loss = []\n\nif cuda:\n    print(\"Using GPU\")\nelse:\n    print(\"Not using GPU\")\n\n\nfor epoch in range(num_epochs):\n    train_loss = []\n    validation_loss = []\n    \n    for batch_index, (train_image, train_label) in enumerate(train_loader):\n        # If GPU is available, move the data to the GPU for faster computation.\n        if cuda:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            our_CNN.train()\n            train_image=train_image.cuda()\n            train_label=train_label.cuda()\n            train_label_predicted = our_CNN(train_image)\n            one_hot_labels_train = torch.nn.functional.one_hot(train_label.long(), num_classes=2).to(torch.float)\n            #one_hot_labels_train = torch.nn.functional.one_hot(train_label, num_classes=3).to(torch.float)\n            #print(train_label_predicted.size())\n            loss = Loss(train_label_predicted, one_hot_labels_train)\n        \n            train_loss.append(loss.cuda().data.item())\n\n            # reset the gradient \n            Optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            Optimizer.step()\n\n            #######################################################\n            ###################### Validation #####################\n            #######################################################\n            # Set the model to evaluation mode so that parameters are fixed.\n            our_CNN.eval()\n            \n            #Extract the batch validation images and validation labels from the validation loader\n            (image_validation_torch, label_validation_torch) = next(iter(validation_loader))\n\n            validation_label_predicted = our_CNN(image_validation_torch.cuda())\n            \n            one_hot_label_validation = torch.nn.functional.one_hot(label_validation_torch.cuda().long(), num_classes=2).to(torch.float)\n\n            loss = Loss(validation_label_predicted, one_hot_label_validation) #Custom MSE loss function\n            validation_loss.append(loss.cuda().data.item())\n        \n        # If GPU is not available.\n        else:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            our_CNN.train()\n\n            train_label_predicted = our_CNN(train_image)\n            \n            one_hot_labels_train = torch.nn.functional.one_hot(train_label.long(), num_classes=2).to(torch.float)\n\n            # compute the loss\n            loss = Loss(train_label_predicted,one_hot_labels_train ) #Custom MSE loss function\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient\n            Optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            Optimizer.step()\n\n            #######################################################\n            ###################### Validation #####################\n            #######################################################\n            # Set the model to evaluation mode so that parameters are fixed.\n            our_CNN.eval()\n            \n            #Extract the batch validation images and validation labels from the validation loader\n            (image_validation_torch, label_validation_torch) = next(iter(validation_loader))\n\n            validation_label_predicted = our_CNN(image_validation_torch)\n            \n            one_hot_label_validation = torch.nn.functional.one_hot(label_validation_torch.long(), num_classes=2).to(torch.float)\n\n            loss = Loss(validation_label_predicted, one_hot_label_validation) #Custom MSE loss function\n            validation_loss.append(loss.cpu().data.item())\n\n    train_epoch_loss.append(np.mean(train_loss))\n    validation_epoch_loss.append(np.mean(validation_loss))\n\n    torch.save(our_CNN.state_dict(), '.\/saved_models\/checkpoint_epoch_%s.pth' % (epoch))\n\n    end = time.time()\n    rt = end - start\n    tl = (num_epochs - epoch - 1)*rt\/(epoch+1)\n    print('runtime {}:{}'.format(int(rt\/\/60), int(rt%60\/\/1)))\n    print('complete in {}:{}'.format(int(tl\/\/60), int(tl%60\/\/1)))\n\n    print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))","da68facd":"# Learning curve creation\nplt.figure(figsize = (12, 8))\nplt.plot(train_epoch_loss, '-o', label = 'training loss', markersize = 3)\nplt.plot(validation_epoch_loss, '-o', label = 'validation loss', markersize = 3)\nplt.legend(loc = 'upper right');","ae3eb81c":"#Find the best epoch\nbest_epoch = np.argmin(train_epoch_loss)\nprint('best epoch: ', best_epoch)","63ff814b":"state_dict = torch.load('.\/saved_models\/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\nour_CNN.load_state_dict(state_dict)","4bc3b5e9":"#Definition of the function that predicts the labels for every pictures\ndef predict(Model, Dataloader):\n    Labels = []\n    Predicted_labels = []\n    \n    for batch_index, (input_image, input_label) in enumerate(Dataloader):\n        # If GPU is available, move the data to the GPU for faster computation.\n        if cuda:\n            Model.eval()\n            \n            Labels.append(input_label.detach().cpu().tolist())\n            \n            Predicted_label = Model(input_image.cuda())\n            Predicted_label_probability, Predicted_label_idx = torch.max(Predicted_label.data, 1)\n            \n            for current_prediction in Predicted_label_idx:\n                Predicted_labels.append(current_prediction.detach().cpu().numpy().item())\n                \n        else:\n            Model.eval()\n            \n            Labels.append(input_label.detach().cpu().tolist())\n            \n            Predicted_label = Model(input_image)\n            Predicted_label_probability, Predicted_label_idx = torch.max(Predicted_label.data, 1)\n            \n            for current_prediction in Predicted_label_idx:\n                Predicted_labels.append(current_prediction.detach().cpu().numpy().item())\n                \n    Labels = [item for sublist in Labels for item in sublist]\n    return Predicted_labels, Labels","c39a42aa":"(test_predicted_labels, test_labels) = predict(our_CNN, test_loader)","46818fa1":"Accuracy = accuracy_score(test_labels, test_predicted_labels)\n\nprint(\"The accuracy is :\", Accuracy * 100, \"%\")","ca33986d":"Confusion_Matrix = confusion_matrix(test_labels, test_predicted_labels)\n\nplt.figure(figsize = (12,10))\nsns.heatmap(Confusion_Matrix, annot = True, annot_kws = {\"size\": 10})\nplt.ylim([0, 4]);\nplt.ylabel('Labels');\nplt.xlabel('Predicted labels');\nplt.xticks([i+0.5 for i in range(2)], classes)\nplt.yticks([i+0.5 for i in range(2)], classes)\n\nplt.show()","8f107432":"tn, fp, fn, tp = np.reshape(Confusion_Matrix,4)\nfpr = (fp) \/ (fp + tn)\nfnr = fn \/ (fn + tp)\nfpr, fnr","983f7e4d":"from PIL import Image, ImageEnhance\n\nim = Image.open('..\/input\/totaldata\/Project Data\/Disease\/0016252a3fa0e1a758ff72b122e94264.jpg')\n\nenhancer = ImageEnhance.Sharpness(im)\n\nfactor = 1\nim_s_1 = enhancer.enhance(factor)\nim_s_1.save('original-image-1.png');\n\nfactor = 0.05\nim_s_2 = enhancer.enhance(factor)\nim_s_2.save('blurred-image.png');\n\nfactor = 2\nim_s_3 = enhancer.enhance(factor)\nim_s_3.save('sharpened-image.png');","e274c542":"transform = transforms.Compose([transforms.Resize((60, 80)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5,0.5,0.5),\n                                                     (0.5,0.5,0.5))])\n    ","4249bd00":"class Sharpen(object):\n    def __init__(self, root_dir, annotation_file, transform = None, max_center=4):\n        self.identity = np.array([[0, 0, 0],\n                                  [0, 1, 0],\n                                  [0, 0, 0]])\n        self.sharpen = np.array([[ 0, -1,  0],\n                                [-1,  4, -1],\n                                [ 0, -1,  0]]) \/ 4\n        self.root_dir = root_dir\n        self.annotations = pd.read_csv(annotation_file)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.annotations)\n    def __getitem__(self, index):\n        pic_id = self.annotations.iloc[index, 0]\n        pic = Image.open(os.path.join(self.root_dir, pic_id)).convert(\"RGB\")\n        y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n\n        if self.transform is not None:\n            pic = self.transform(pic)\n\n    def __call__(self, pic, y_abel):\n\n        sharp = self.sharpen * np.random.random() * self.max_center\n        kernel = self.identity + sharp\n\n        pic = cv2.filter2D(pic, -1, kernel)\n        return (pic, y_label)","3bbae08b":"\n# Creation of the dataset we will use for our deep learning model\nclass Sharp(Dataset):\n    def __init__(self, root_dir, annotation_file, transform=None, max_center = 4):\n        self.identity = np.array([[0, 0, 0],\n                                  [0, 1, 0],\n                                  [0, 0, 0]])\n        self.sharpen = np.array([[ 0, -1,  0],\n                                [-1,  4, -1],\n                                [ 0, -1,  0]]) \/ 4\n        self.root_dir = root_dir\n        self.annotations = pd.read_csv(annotation_file)\n        self.transform = transform\n        self.max_center = max_center\n\n    def __len__(self):\n        return len(self.annotations)\n    \n    def __call__(self, X, Y):\n\n        sharp = self.sharpen * np.random.random() * self.max_center\n        kernel = self.identity + sharp\n\n        X = cv2.filter2D(X, -1, kernel)\n        return X, Y\n\n    def __getitem__(self, index):\n               \n        \n        pic_id = self.annotations.iloc[index, 0]\n        pic = Image.open(os.path.join(self.root_dir, pic_id)).convert(\"RGB\")\n        y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n\n        if self.transform is not None:\n            pic = self.transform(pic)\n\n        return (pic, y_label)","42fbdde1":"\nclass Sharpen(object):\n    def __init__(self, max_center=4):\n        self.identity = np.array([[0, 0, 0],\n                                  [0, 1, 0],\n                                  [0, 0, 0]])\n        self.sharpen = np.array([[ 0, -1,  0],\n                                [-1,  4, -1],\n                                [ 0, -1,  0]]) \/ 4\n\n    def __call__(self, X, Y):\n\n        sharp = self.sharpen * np.random.random() * self.max_center\n        kernel = self.identity + sharp\n\n        X = cv2.filter2D(X, -1, kernel)\n        return X, Y","e2369f65":"dataset = Sharp('..\/input\/totaldata\/Project Data\/combo\/',\"train_csv.csv\",transform=transform)\ndataset","afff580f":"split = np.array([.7,.15,.15])\ntrain_set, test_set, validation_set = torch.utils.data.random_split(dataset,[13676, 2930, 2931])","b9f66f07":"#Unnormalize the object given as input\nclass UnnormalizingPicture(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor\n    \nUnnormalized = UnnormalizingPicture(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))","76383a85":"def imshow(img):\n    img=img\/2+0.5\n    npimg=img.numpy()\n    plt.imshow(np.transpose(npimg,(1,2,0)))","de53a065":"#Define Parameters and create figure as an empty \"table\"\nClass=len(classes)\ncolumn=3\ncolors = ('red', 'green', 'blue')\nfig=plt.figure(figsize=(20,30))\n\n#Complete the table by displaying pictures \n#and creating their associated histogram\ni=0\nfor i in range(Class):\n    #Loading Train Set\n    dataiter_train=iter(train_set)\n    image_train, idx_train=next(x for x in dataiter_train if (x[1]==i))\n    \n    #Loading Test set\n    dataiter_test=iter(test_set)\n    image_test, idx_test=next(x for x in dataiter_test if (x[1]==i))\n    \n    #Loading Validation set\n    dataiter_test=iter(validation_set)\n    image_validation, idx_validation=next(x for x in dataiter_test if (x[1]==i))\n    \n    #Creating first colum of the grid - One train set picture from each classe\n    ax=fig.add_subplot(Class,column,i*column+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.title.set_text(str(classes[i]+' from train set'))\n    #Display one picture of each class in each row of first colum\n    imshow(image_train)\n    print(image_train.shape)\n    \n    #Creating third Column of the grid - One test set picture from each classe\n    ax2=fig.add_subplot(Class,column,i*column+3)\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    ax2.title.set_text(str(classes[i]+' from validation set'))\n    #Display one picture of each class in each row of first colum\n    imshow(image_validation)\n    \n    \n    #Creating third Column of the grid - One test set picture from each classe\n    ax3=fig.add_subplot(Class,column,i*column+2)\n    ax3.set_xticks([])\n    ax3.set_yticks([])\n    ax3.title.set_text(str(classes[i]+' from test set'))\n    #Display one picture of each class in each row of first colum\n    imshow(image_test)\n    ","dc8558a8":"#Get the list of indexes \ndef GetListofIndexes(Dataset):\n    indexes=[]\n    i=0\n    while len(indexes)!=(len(Dataset)):\n        image,label=Dataset[i]\n        indexes.append(label)\n        i=i+1\n    return(indexes)","1d05a42d":"# Verification of GetListofIndexes function\nClassesTrain,Train_count = np.unique(GetListofIndexes(train_set), return_counts=True)\nprint(ClassesTrain)\nprint(Train_count)\nprint(sum(Train_count))\nClassesTest,Test_count = np.unique(GetListofIndexes(test_set), return_counts=True)\nprint(ClassesTest)\nprint(Test_count)\nprint(sum(Test_count))\nClassesV,V_count = np.unique(GetListofIndexes(validation_set), return_counts=True)\nprint(ClassesV)\nprint(V_count)\nprint(sum(V_count))","84b4ef7d":"plt.figure(figsize=(10,10))\nplt.title('Our data distribution')\n#Create x axis with each class (10 classes)\nclasses=['NorROP','ROP']\nplt.xticks(range(2), classes) \n# axis labels\nplt.xlabel('Classe label')\nplt.ylabel('Number of samples')\n#Bar creation\nplt.bar(range(len(Train_count)), Train_count, label='Train')\nplt.bar(range(len(Test_count)), Test_count, label='Test', bottom=Train_count)\nplt.bar(range(len(V_count)), V_count, label='Validation', bottom=[i+j for i,j in zip(Train_count, Test_count)])\nplt.legend()\nplt.show()","127a8723":"learning_rate = 0.001\nbatch_size = 40\nshuffle = True\npin_memory = True\nnum_workers = 0","b64385ae":"train_loader = torch.utils.data.DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)","1b7fd72e":"#Parameters\nWeight=32\nHeight=32\nTotalNumberofClasses=2\n\n#Check GPU availability\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\n\n#Creation of our Model :  Le Net model\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3,6, kernel_size=5)\n        self.dropout = nn.Dropout(p=0.2) \n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.fc1 = nn.Linear(3264, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, TotalNumberofClasses)\n        \n\n    def forward(self, x):\n        #print(x.size())\n        x = F.relu(self.conv1(x))\n        #print(x.size())\n        x = F.max_pool2d(x, 2)\n        #print(x.size())\n        x = F.relu(self.conv2(x))\n        #print(x.size())\n        x = F.max_pool2d(x, 2)\n        #print(x.size())\n        x=F.relu(x)\n        #print(x.size())\n        x=self.dropout(x)\n        x = x.view(x.size(0),-1)\n        #print(x.size())\n        x = F.relu(self.fc1(x))\n\n        #print(x.size())\n        x = F.relu(self.fc2(x))\n        #print(x.size())\n        x = self.fc3(x)\n        return x\n    \n\nour_CNN = LeNet()\n\n# If GPU available, move the model to GPU.\nif cuda:\n    our_CNN.cuda()","643e1279":"!mkdir saved_models_2","52e4d8d4":"#Display our model architecture\nprint(our_CNN)","2fe60912":"#Loss function chosen : Mean Squared error loss function\nLoss = nn.MSELoss()\n\n#Optimizer = AdamOptimizer\nOptimizer = torch.optim.Adam(our_CNN.parameters(), learning_rate)","832ce71f":"start = time.time()\nnum_epochs = 75\n\ntrain_epoch_loss = []\nvalidation_epoch_loss = []\n\nif cuda:\n    print(\"Using GPU\")\nelse:\n    print(\"Not using GPU\")\n\n\nfor epoch in range(num_epochs):\n    train_loss = []\n    validation_loss = []\n    \n    for batch_index, (train_image, train_label) in enumerate(train_loader):\n        # If GPU is available, move the data to the GPU for faster computation.\n        if cuda:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            our_CNN.train()\n            train_image=train_image.cuda()\n            train_label=train_label.cuda()\n            train_label_predicted = our_CNN(train_image)\n            one_hot_labels_train = torch.nn.functional.one_hot(train_label.long(), num_classes=2).to(torch.float)\n            #one_hot_labels_train = torch.nn.functional.one_hot(train_label, num_classes=3).to(torch.float)\n            #print(train_label_predicted.size())\n            loss = Loss(train_label_predicted, one_hot_labels_train)\n        \n            train_loss.append(loss.cuda().data.item())\n\n            # reset the gradient \n            Optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            Optimizer.step()\n\n            #######################################################\n            ###################### Validation #####################\n            #######################################################\n            # Set the model to evaluation mode so that parameters are fixed.\n            our_CNN.eval()\n            \n            #Extract the batch validation images and validation labels from the validation loader\n            (image_validation_torch, label_validation_torch) = next(iter(validation_loader))\n\n            validation_label_predicted = our_CNN(image_validation_torch.cuda())\n            \n            one_hot_label_validation = torch.nn.functional.one_hot(label_validation_torch.cuda().long(), num_classes=2).to(torch.float)\n\n            loss = Loss(validation_label_predicted, one_hot_label_validation) #Custom MSE loss function\n            validation_loss.append(loss.cuda().data.item())\n        \n        # If GPU is not available.\n        else:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            our_CNN.train()\n\n            train_label_predicted = our_CNN(train_image)\n            \n            one_hot_labels_train = torch.nn.functional.one_hot(train_label.long(), num_classes=2).to(torch.float)\n\n            # compute the loss\n            loss = Loss(train_label_predicted,one_hot_labels_train ) #Custom MSE loss function\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient\n            Optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            Optimizer.step()\n\n            #######################################################\n            ###################### Validation #####################\n            #######################################################\n            # Set the model to evaluation mode so that parameters are fixed.\n            our_CNN.eval()\n            \n            #Extract the batch validation images and validation labels from the validation loader\n            (image_validation_torch, label_validation_torch) = next(iter(validation_loader))\n\n            validation_label_predicted = our_CNN(image_validation_torch)\n            \n            one_hot_label_validation = torch.nn.functional.one_hot(label_validation_torch.long(), num_classes=2).to(torch.float)\n\n            loss = Loss(validation_label_predicted, one_hot_label_validation) #Custom MSE loss function\n            validation_loss.append(loss.cpu().data.item())\n\n    train_epoch_loss.append(np.mean(train_loss))\n    validation_epoch_loss.append(np.mean(validation_loss))\n\n    torch.save(our_CNN.state_dict(), '.\/saved_models_2\/checkpoint_epoch_%s.pth' % (epoch))\n\n    end = time.time()\n    rt = end - start\n    tl = (num_epochs - epoch - 1)*rt\/(epoch+1)\n    print('runtime {}:{}'.format(int(rt\/\/60), int(rt%60\/\/1)))\n    print('complete in {}:{}'.format(int(tl\/\/60), int(tl%60\/\/1)))\n\n    print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))","eb4d714e":"# Learning curve creation\nplt.figure(figsize = (12, 8))\nplt.plot(train_epoch_loss, '-o', label = 'training loss', markersize = 3)\nplt.plot(validation_epoch_loss, '-o', label = 'validation loss', markersize = 3)\nplt.legend(loc = 'upper right');","93f3a52b":"#Find the best epoch\nbest_epoch = np.argmin(train_epoch_loss)\nprint('best epoch: ', best_epoch)","dadded9d":"state_dict = torch.load('.\/saved_models\/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\nour_CNN.load_state_dict(state_dict)","fd764d08":"#Definition of the function that predicts the labels for every pictures\ndef predict(Model, Dataloader):\n    Labels = []\n    Predicted_labels = []\n    \n    for batch_index, (input_image, input_label) in enumerate(Dataloader):\n        # If GPU is available, move the data to the GPU for faster computation.\n        if cuda:\n            Model.eval()\n            \n            Labels.append(input_label.detach().cpu().tolist())\n            \n            Predicted_label = Model(input_image.cuda())\n            Predicted_label_probability, Predicted_label_idx = torch.max(Predicted_label.data, 1)\n            \n            for current_prediction in Predicted_label_idx:\n                Predicted_labels.append(current_prediction.detach().cpu().numpy().item())\n                \n        else:\n            Model.eval()\n            \n            Labels.append(input_label.detach().cpu().tolist())\n            \n            Predicted_label = Model(input_image)\n            Predicted_label_probability, Predicted_label_idx = torch.max(Predicted_label.data, 1)\n            \n            for current_prediction in Predicted_label_idx:\n                Predicted_labels.append(current_prediction.detach().cpu().numpy().item())\n                \n    Labels = [item for sublist in Labels for item in sublist]\n    return Predicted_labels, Labels","aa799b05":"(test_predicted_labels, test_labels) = predict(our_CNN, test_loader)","a82c05f2":"Accuracy = accuracy_score(test_labels, test_predicted_labels)\n\nprint(\"The accuracy is :\", Accuracy * 100, \"%\")","c882fe0e":"Confusion_Matrix = confusion_matrix(test_labels, test_predicted_labels)\n\nplt.figure(figsize = (12,10))\nsns.heatmap(Confusion_Matrix, annot = True, annot_kws = {\"size\": 10})\nplt.ylim([0, 4]);\nplt.ylabel('Labels');\nplt.xlabel('Predicted labels');\nplt.xticks([i+0.5 for i in range(2)], classes)\nplt.yticks([i+0.5 for i in range(2)], classes)\n\nplt.show()","6bca783e":"tn, fp, fn, tp = np.reshape(Confusion_Matrix,4)\nfpr = (fp) \/ (fp + tn)\nfnr = fn \/ (fn + tp)\nfpr, fnr","5f5e026a":"transform = transforms.Compose(\n        [\n            \n            transforms.RandomCrop((30, 30)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )","86a69476":"train_loader = torch.utils.data.DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)","ad6c6c8d":"import torch.nn as nn\nimport torchvision.models as models\nfrom tqdm import tqdm\n\nclass CNN(nn.Module):\n    def __init__(self, train_CNN=False, num_classes=2):\n        super(CNN, self).__init__()\n        self.train_CNN = train_CNN\n        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n        self.inception.fc = nn.Linear(self.inception.fc.in_features, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, images):\n        features = self.inception(images)\n        return self.sigmoid(self.dropout(self.relu(features))).squeeze(1)","a2b3e960":"num_epochs = 5\nlearning_rate = 0.00001\ntrain_CNN = False\nbatch_size = 1\nshuffle = True\npin_memory = True\nnum_workers = 1","04e7dba9":"model = CNN().to(device)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor name, param in model.inception.named_parameters():\n    if \"fc.weight\" in name or \"fc.bias\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = train_CNN","34b3007b":"def check_accuracy(loader, model):\n    if loader == train_loader:\n        print(\"Checking accuracy on training data\")\n    else:\n        print(\"Checking accuracy on validation data\")\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n            scores = model(x)\n            predictions = torch.tensor([1.0 if i >= 0.5 else 0.0 for i in scores]).to(device)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n    return f\"{float(num_correct)\/float(num_samples)*100:.2f}\"\n    print(\n        f\"Got {num_correct} \/ {num_samples} with accuracy {float(num_correct)\/float(num_samples)*100:.2f}\"\n    )\n    model.train()","aada4f3c":"def train():\n    model.train()\n    for epoch in range(num_epochs):\n        loop = tqdm(train_loader, total = len(train_loader), leave = True)\n        if epoch % 2 == 0:\n            loop.set_postfix(val_acc = check_accuracy(validation_loader, model))\n        for imgs, labels in loop:\n            imgs = imgs.to(device)\n            labels = labels.to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            loop.set_description(f\"Epoch [{epoch}\/{num_epochs}]\")\n            loop.set_postfix(loss = loss.item())\n\nif __name__ == \"__main__\":\n    train()","a4ee5de5":"1. Image display for each classes","24dcc0b7":"2. Quantitative visualization of the dataset, repartition of pictures\n","691ef4f3":"# Our files","36fae59e":"Chose because MSE is the default loss function for most Pytorch regression problems.","060280d6":"# Dataset visualization","ddf59b04":"transform_augmentation = transforms.Compose([\n    transforms.Resize((1000, 1000)),\n    transforms.RandomHorizontalFlip(),\n    #transforms.RandomVerticalFlip(p=1),\n    transforms.ToTensor(),\n    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n    transforms.Normalize((0.5,0.5,0.5),\n                         (0.5,0.5,0.5))\n])\n","2537da19":"# Libraries","5022c8c9":"train_loader = torch.utils.data.DataLoader(dataset=train_set_augmentation, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_set_augmentation, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\ntest_loader = torch.utils.data.DataLoader(dataset=test_set_augmentation, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)","22c14aa0":"1. Image display for each classes","4d6d6356":"We want to know **how many images we have, in total, and for each stage.**","a768948f":"# Verification of GetListofIndexes function\nClassesTrain,Train_count = np.unique(GetListofIndexes(train_set_augmentation), return_counts=True)\nprint(ClassesTrain)\nprint(Train_count)\nprint(sum(Train_count))\nClassesTest,Test_count = np.unique(GetListofIndexes(test_set_augmentation), return_counts=True)\nprint(ClassesTest)\nprint(Test_count)\nprint(sum(Test_count))\nClassesV,V_count = np.unique(GetListofIndexes(validation_set_augmentation), return_counts=True)\nprint(ClassesV)\nprint(V_count)\nprint(sum(V_count))","16a51741":"The following commented code can be uncommented to get the name and file path of every images we have. ","b88452e4":"https:\/\/medium.com\/analytics-vidhya\/implementing-cnn-in-pytorch-with-custom-dataset-and-transfer-learning-1864daac14cc code used\n","fb602072":"# Verification of GetListofIndexes function\nClassesTrain,Train_count = np.unique(GetListofIndexes(train_set_augmentation), return_counts=True)\nprint(ClassesTrain)\nprint(Train_count)\nprint(sum(Train_count))\nClassesTest,Test_count = np.unique(GetListofIndexes(test_set_augmentation), return_counts=True)\nprint(ClassesTest)\nprint(Test_count)\nprint(sum(Test_count))\nClassesV,V_count = np.unique(GetListofIndexes(validation_set_augmentation), return_counts=True)\nprint(ClassesV)\nprint(V_count)\nprint(sum(V_count))","f2f01f52":"# Let's try Data Augmentation","ad441a8a":"dataset_augmentation = Stage1NoROP('\/kaggle\/input\/ropstages-reviewed\/NewROPDataset_Sample_justtotry\/',\"train_csv.csv\",transform=transform_augmentation)\ntrain_set_augmentation, test_set_augmentation, validation_set_augmentation = torch.utils.data.random_split(dataset,[60,15,16])","f3e6ca78":"2. Quantitative visualization of the dataset, repartition of pictures\n","e52527bc":"# Let's try Data Augmentation","d439e2f0":"# Creation of the dataset","0ad4ddd8":"# Introduction\nOur Algorithm here aims at classifying babies'fundus into 4 different classes to determine wether the baby will need treatment or not. In this first part, our neural network classifies the pictures into 4 stages : No ROP, Stage 1, Stage 2, Stage 3.","f399d143":"# CNN definition","7566d7ef":"# another model that worked?","ce96ddfe":"**Creation of the dataset used for our training**","d991dfb0":"# Dataset visualization","a94ddd74":"# CNN definition","aa8ac2be":"Here is the list the different libraries we are going to use and we import them.","9e682792":"transform_augmentation = transforms.Compose([\n    transforms.Resize((1000, 1000)),\n    transforms.RandomHorizontalFlip(),\n    #transforms.RandomVerticalFlip(p=1),\n    transforms.ToTensor(),\n    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n    transforms.Normalize((0.5,0.5,0.5),\n                         (0.5,0.5,0.5))\n])\n","b290cf07":"train_loader = torch.utils.data.DataLoader(dataset=train_set_augmentation, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_set_augmentation, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\ntest_loader = torch.utils.data.DataLoader(dataset=test_set_augmentation, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)","b77be75e":"In the following lines, you can understand how our labels are constructed. ","bafd9ce3":"Chose because MSE is the default loss function for most Pytorch regression problems.","95cd33ec":"dataset_augmentation = Stage1NoROP('\/kaggle\/input\/ropstages-reviewed\/NewROPDataset_Sample_justtotry\/',\"train_csv.csv\",transform=transform_augmentation)\ntrain_set_augmentation, test_set_augmentation, validation_set_augmentation = torch.utils.data.random_split(dataset,[60,15,16])"}}