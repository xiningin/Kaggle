{"cell_type":{"dd257c5f":"code","ba803f4a":"code","d61f4c49":"code","40a5c24a":"code","dc2e2767":"code","23bbfa29":"code","4239be30":"code","60067d32":"code","a4ff9936":"code","505e0d14":"code","adaf7591":"code","f0eb4b48":"code","90a3ac0b":"code","47068e45":"code","70cfbc60":"code","9dd026de":"code","3f0b8e01":"code","ae5e5ea5":"code","1c274890":"code","242e2691":"code","11ef9844":"code","4ec76755":"code","a7ecee08":"code","d84ced96":"code","f3eee878":"code","041be131":"code","ab9bd0f1":"code","44399678":"code","83ffa7f3":"code","b20b812f":"code","723004ba":"code","7e19f5c3":"code","a35b5892":"code","f82a23c2":"code","2b6345ed":"code","ccfb0c51":"code","321d985a":"code","8653190f":"code","72ceb4e7":"code","ce8f1554":"code","c84050cd":"code","94867d1d":"code","e71fa2d1":"code","50010d0f":"code","c3364b38":"code","fb06906f":"code","027b20c7":"code","5ce03fd1":"code","6bee5b7d":"code","03c33b71":"code","5465f112":"code","84284086":"code","37111258":"code","90d756f9":"code","150025b0":"code","76e2624c":"code","0aeca0f2":"code","8086e2a4":"code","3295c4e6":"code","4e6a3a90":"code","03178cae":"code","c76ca453":"code","915a6436":"code","4856cfb6":"code","413c0233":"code","2110efa9":"code","a58d0cf8":"code","c1015066":"code","f586d4fd":"code","e69add8d":"code","dff68b9c":"code","7abe1225":"code","f64a423f":"code","1dfce6b0":"code","398632f9":"code","9683d4d1":"code","8cfd467f":"code","af0b77eb":"code","7637f979":"code","96a04e97":"code","24f0ba7b":"code","d1331d41":"code","0b04431a":"code","044f55eb":"code","2eacc665":"code","c7237d3d":"code","8d47070b":"code","b289926a":"code","af180bae":"code","0426148c":"code","7a790f67":"code","42e56d34":"code","c7b7a745":"code","e7c3e395":"code","d4c2dcb4":"code","58ed15bb":"code","f2922830":"code","08dcae52":"code","7e765eec":"code","38a61c72":"code","44556cfb":"code","bf7911a7":"code","1fd4693b":"code","623c5f3e":"code","1bd24e1f":"code","89b522aa":"code","d0f81a1c":"code","7a327d47":"code","250a017e":"code","bb79fcfb":"code","d02de066":"code","4d6bd503":"code","8decac1c":"code","6651f81b":"code","7f79c11a":"code","b4c78887":"code","470db2f6":"code","3a2e5464":"code","e69e4523":"code","273395c9":"code","13ab9a83":"code","4dd4a8c1":"code","dbb92940":"code","c733f339":"code","7ec2612a":"code","b892b6fd":"code","098085ab":"code","3aec7fcb":"code","fe184714":"code","6d414e25":"code","1bf8bd62":"code","4a4e6bf7":"code","3313f399":"code","cdce0bd4":"code","765fd65b":"code","151973d7":"code","99b7b1f2":"code","60ad1e73":"code","3f11102e":"code","d09fbecd":"code","ea131001":"code","290350df":"code","a34b9a0b":"code","7d87714e":"code","b80ec73c":"code","b1526f54":"code","4351d34d":"code","aa9ff90a":"code","c1b1492c":"code","6becdf87":"code","b3bb932f":"code","eff07002":"code","b0eac172":"code","34860490":"code","ca89a889":"code","0c0e33b8":"code","b8e6a0fb":"code","a4acc4e7":"code","a1f84bcb":"code","4928794e":"code","9bf98268":"code","0fc4fb75":"markdown","e21ec805":"markdown","123ff01a":"markdown","ef4f8c19":"markdown","113b9f1c":"markdown","879f674e":"markdown","2ec6a76b":"markdown","46a4e7b8":"markdown","21f291ee":"markdown","5572298e":"markdown","9cccfd71":"markdown","2be4a8b3":"markdown","cac25fdf":"markdown","e5f904a4":"markdown","64292d8d":"markdown","78840622":"markdown","5eb3ba5c":"markdown","2b719179":"markdown","7160a243":"markdown","a7596f12":"markdown","7d384a72":"markdown","65d44b15":"markdown","127efc2a":"markdown","3a408654":"markdown","5ad37995":"markdown"},"source":{"dd257c5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba803f4a":"import seaborn as sns\nimport matplotlib.pyplot as matplot\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import show\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import zscore\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\n\npd.set_option(\"display.max_rows\",None)\npd.set_option(\"display.max_columns\",None)","d61f4c49":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nsub_df=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","40a5c24a":"train_df.head()","dc2e2767":"train_df.dtypes","23bbfa29":"test_df.head()","4239be30":"print(train_df.shape)\nprint(test_df.shape)\n","60067d32":"# Checking for null values\ntrain_df.isnull().sum()","a4ff9936":"test_df.isnull().sum()","505e0d14":"train_df.drop(\"Cabin\",axis=1,inplace=True)\ntest_df.drop(\"Cabin\",axis=1,inplace=True)","adaf7591":"train_df[train_df['Embarked'].isnull()]","f0eb4b48":"train_df['Embarked'].fillna(train_df[train_df['Pclass']==1]['Embarked'].mode()[0],inplace=True)","90a3ac0b":"train_df.sort_values(by='Ticket')","47068e45":"train_df[train_df['Ticket']=='CA. 2343']","70cfbc60":"train_df.sort_values(by='Age')","9dd026de":"train_df[train_df['Age'].isnull()].sort_values(by='Ticket')","3f0b8e01":"train_df[(train_df['Age'].isnull()) & (train_df['Parch']!=0)]","ae5e5ea5":"train_df['Age'].fillna(train_df['Age'].median(),inplace=True) # imputing with mdeian as there are some outlier, since only few people have travelled whose age is close to eighty. using mean will not be suitable","1c274890":"test_df['Age'].fillna(test_df['Age'].median(),inplace=True) # imputing with mdeian as there are some outlier, since only few people have travelled whose age is close to eighty. using mean will not be suitable","242e2691":"train_df.drop(['Name'],axis=1,inplace=True)\ntest_df.drop(['Name'],axis=1,inplace=True)","11ef9844":"train_df.describe().transpose()","4ec76755":"#sns.pairplot(train_df,hue='Survived')","a7ecee08":"\ntotal = float(len(train_df))\nax = sns.countplot(train_df['Survived']) # for Seaborn version 0.7 and more\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format((height\/total)*100),\n            ha=\"center\") \nshow()","d84ced96":"total = float(len(train_df))\nax = sns.countplot(train_df['Pclass']) # for Seaborn version 0.7 and more\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format((height\/total)*100),\n            ha=\"center\") \nshow()","f3eee878":"sns.distplot(train_df['Age'])","041be131":"total = float(len(train_df))\nax = sns.countplot(train_df['Sex']) # for Seaborn version 0.7 and more\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format((height\/total)*100),\n            ha=\"center\") \nshow()","ab9bd0f1":"total = float(len(train_df))\nax = sns.countplot(train_df['Embarked']) # for Seaborn version 0.7 and more\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format((height\/total)*100),\n            ha=\"center\") \nshow()","44399678":"total = float(len(train_df))\nax = sns.countplot(train_df['SibSp']+train_df['Parch']) # for Seaborn version 0.7 and more\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format((height\/total)*100),\n            ha=\"center\") \nshow()","83ffa7f3":"sns.distplot(train_df['Fare'])","b20b812f":"train_df.head()","723004ba":"sns.countplot(x='Survived',data=train_df)\ntotal = float(len(train_df))\nax = sns.countplot(x='Survived',hue='Pclass',data=train_df) # for Seaborn version 0.7 and more\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format((height\/total)*100),\n            ha=\"center\") \nshow()","7e19f5c3":"p_cont1 = pd.crosstab(train_df['Pclass'], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Pclass'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Pclass',  data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Pclass', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate Passenger class wise\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","a35b5892":"p_cont1 = pd.crosstab(train_df['Sex'], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Sex'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Sex',  data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Sex', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate Genderwise\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","f82a23c2":"p_cont1 = pd.crosstab(train_df['Embarked'], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Embarked'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Embarked',  data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Embarked', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate based on Embarked\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","2b6345ed":"train_df[train_df['Fare']==0]","ccfb0c51":"def personclassifier(x):\n    age, sex = x\n    if age <=12:\n        return 'Child'\n    if age > 12 and age <=19:\n        return 'Teenager'\n    if age >19 and age <=30:\n        return 'Young Adult'\n    if age >30 and age <= 45:\n        return 'Mid aged Adult'\n    if age >45:\n        return 'Old Adults'\n    \n    \n    ","321d985a":"def singletraveller(x):\n    single  = x\n    if single ==0:\n        return 'Yes'\n    else:\n        return 'No'   ","8653190f":"train_df['Single_Traveller'] = train_df['SibSp']+train_df['Parch']","72ceb4e7":"train_df['Single_Traveller'] = train_df['Single_Traveller'].apply(singletraveller)","ce8f1554":"test_df['Single_Traveller'] = test_df['SibSp']+test_df['Parch']","c84050cd":"test_df['Single_Traveller'] = test_df['Single_Traveller'].apply(singletraveller)","94867d1d":"train_df['Person'] = train_df[['Age','Sex']].apply(personclassifier,axis=1)","e71fa2d1":"test_df['Person'] = test_df[['Age','Sex']].apply(personclassifier,axis=1)","50010d0f":"p_cont1 = pd.crosstab(train_df['Person'], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Person'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Person',  data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Person', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate Age Group wise\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","c3364b38":"p_cont1 = pd.crosstab([train_df['Person'],train_df['Sex']], train_df['Survived'], normalize='index') * 100\np_cont1","fb06906f":"p_cont1 = pd.crosstab([train_df['Person'],train_df['Sex']], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Person'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Person', hue='Sex', data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Person', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate By Age Group by Gender \", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","027b20c7":"p_cont1 = pd.crosstab([train_df['Person'],train_df['Pclass']], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Person'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Person', hue='Pclass', data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Person', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate by age group by class\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","5ce03fd1":"p_cont1 = pd.crosstab([train_df['Person'],train_df['Pclass']], train_df['Survived'], normalize='index') * 100\np_cont1","6bee5b7d":"test_df.isnull().sum()","03c33b71":"test_df['Fare'].fillna(test_df['Fare'].median(),inplace=True)","5465f112":"p_cont1 = pd.crosstab(train_df['Single_Traveller'], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Single_Traveller'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Single_Traveller', data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Single_Traveller', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate by Single Traveller\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","84284086":"p_cont1 = pd.crosstab([train_df['Single_Traveller'],train_df['Sex']], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Single_Traveller'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Single_Traveller', hue='Sex', data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Single_Traveller', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate by Single traveller by gender\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","37111258":"p_cont1 = pd.crosstab([train_df['Single_Traveller'],train_df['Sex']], train_df['Survived'], normalize='index') * 100\np_cont1","90d756f9":"p_cont1 = pd.crosstab([train_df['Pclass'],train_df['Single_Traveller']], train_df['Survived'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Dead', 1:'Survived'}, inplace=True)\n\n\nlistv = []\nfor var in train_df['Pclass'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Pclass', hue='Single_Traveller', data=train_df,order=listv,palette=\"Set1\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Pclass', y='Survived', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Survived\", fontsize=12)\n\ng1.set_title(\"Survival Rate by class by single traveller\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","150025b0":"p_cont1 = pd.crosstab([train_df['Pclass'],train_df['Single_Traveller']], train_df['Survived'], normalize='index') * 100\np_cont1","76e2624c":"train_df.head()","0aeca0f2":"train_df.drop('Ticket',axis=1,inplace=True)\ntest_df.drop('Ticket',axis=1,inplace=True)\n#train_df.drop(['SibSp','Parch'],axis=1,inplace=True)\n#test_df.drop(['SibSp','Parch'],axis=1,inplace=True)","8086e2a4":"col = []\nfor c in train_df.columns:\n    if train_df[c].dtypes=='object':\n        col.append(c)\n        \n\ntrain_df_dummies = pd.get_dummies(train_df , columns=col, drop_first=True)","3295c4e6":"corr_mat = train_df_dummies.corr()\n","4e6a3a90":"train_df_dummies.corr()","03178cae":"# Getting the columns that are having multi collinearity\n# Creating a dataframe with correlated column, the correlation value and the source column to which it is correlated\n# Filtering only those that are correlated more than 96%\nmulti_col_df = pd.DataFrame(columns=['corr_col','corr_val','source_col'])\nfor i in corr_mat:\n    temp_df = pd.DataFrame(corr_mat[corr_mat[i]>0.9][i])\n    temp_df = temp_df.reset_index()\n    temp_df['source_col'] = i\n    temp_df.columns = ['corr_col','corr_val','source_col']\n    multi_col_df = pd.concat((multi_col_df,temp_df),axis=0)","c76ca453":"multi_col_df","915a6436":"X = train_df_dummies.drop(['Survived','PassengerId'],axis=1)\nX_id = train_df_dummies['PassengerId']\ny = train_df_dummies['Survived']","4856cfb6":"X_trainval, X_test, y_trainval, y_test = train_test_split(X,y, test_size=0.20,random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.20,random_state=1)","413c0233":"X_trainval_z =X_trainval.apply(zscore)\nX_train_z =X_train.apply(zscore)\nX_val_z =X_val.apply(zscore)\nX_test_z =X_test.apply(zscore)\nX_z = X.apply(zscore)","2110efa9":"# Grid Search based on Max_features, Min_Samples_Split and Max_Depth\nparam_grid = [\n{\n'n_neighbors': list(range(1,50)),\n'algorithm': ['auto', 'ball_tree', 'kd_tree','brute'],\n'leaf_size': [10,15,20,30],\n'n_jobs': [-1], \n'weights' : ['uniform','distance']\n}\n]\n\n","a58d0cf8":"import multiprocessing\ngs = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, scoring='accuracy', n_jobs=multiprocessing.cpu_count(),cv=3)\ngs.fit(X_train_z, y_train)","c1015066":"gs.best_estimator_","f586d4fd":"gs.best_score_","e69add8d":"knn_clfr = KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n                     metric_params=None, n_jobs=-1, n_neighbors=6, p=2,\n                     weights='uniform')","dff68b9c":"sfs1 = sfs(knn_clfr, k_features=8, forward=False, scoring='accuracy', cv=10,n_jobs=-1)","7abe1225":"sfs1 = sfs(knn_clfr, k_features=8, forward=False, scoring='accuracy', cv=10,n_jobs=-1)\nsfs1 = sfs1.fit(X_train_z.values, y_train.values)","f64a423f":"X_train_z.head()","1dfce6b0":"sfs1.get_metric_dict()","398632f9":"sfs1 = sfs(knn_clfr, k_features=8, forward=False, scoring='accuracy', cv=10,n_jobs=-1)\nsfs1 = sfs1.fit(X_train_z.values, y_train.values)\nsfs1.get_metric_dict()\ncolumnList = list(X_train_z.columns)\nfeat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)","9683d4d1":"sfs1 = sfs(knn_clfr, k_features=8, forward=False, scoring='accuracy', cv=10,n_jobs=-1)\nsfs1 = sfs1.fit(X_train_z.values, y_train.values)\nsfs1.get_metric_dict()\ncolumnList = list(X_train_z.columns)\nfeat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)\nsubsetColumnList = [columnList[i] for i in feat_cols] \nprint(subsetColumnList)","8cfd467f":"train_df_dummies.dtypes","af0b77eb":"train_df_dummies_knn = train_df_dummies.drop(['Age','Parch','Embarked_Q','Single_Traveller_Yes'],axis=1)\n","7637f979":"train_df_dummies_knn.head()","96a04e97":"X_knn = train_df_dummies_knn.drop(['PassengerId','Survived'],axis=1)\nX_id_knn = train_df_dummies_knn['PassengerId']\ny_knn = train_df_dummies_knn['Survived']","24f0ba7b":"from imblearn.combine import SMOTETomek\nsmk = SMOTETomek(random_state=1)\nX_res,y_res=smk.fit_sample(X_knn,y_knn)","d1331d41":"X_trainval_knn, X_test_knn, y_trainval_knn, y_test_knn = train_test_split(X_res,y_res, test_size=0.20,random_state=1)\nX_train_knn, X_val_knn, y_train_knn, y_val_knn = train_test_split(X_trainval_knn,y_trainval_knn, test_size=0.20,random_state=1)","0b04431a":"X_trainval_z_knn =X_trainval_knn.apply(zscore)\nX_train_z_knn =X_train_knn.apply(zscore)\nX_val_z_knn =X_val_knn.apply(zscore)\nX_test_z_knn =X_test_knn.apply(zscore)\nX_z_knn = X_res.apply(zscore)","044f55eb":"import multiprocessing\ngs = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, scoring='accuracy', n_jobs=multiprocessing.cpu_count(),cv=10)\ngs.fit(X_train_z_knn, y_train_knn)","2eacc665":"gs.best_estimator_","c7237d3d":"gs.best_score_","8d47070b":"knnclfr = KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n                     metric_params=None, n_jobs=-1, n_neighbors=6, p=2,\n                     weights='uniform')","b289926a":"knnclfr.fit(X_train_z_knn, y_train_knn)","af180bae":"y_predict_knn = knnclfr.predict(X_val_z_knn)","0426148c":"print(knnclfr.score(X_train_z_knn,y_train_knn))\nprint(knnclfr.score(X_val_z_knn,y_val_knn))\nprint(metrics.classification_report(y_val_knn,y_predict_knn))\nprint(metrics.confusion_matrix(y_val_knn,y_predict_knn))\n","7a790f67":"knnclfr.fit(X_trainval_z_knn, y_trainval_knn)","42e56d34":"y_predict_knn = knnclfr.predict(X_test_z_knn)","c7b7a745":"print(knnclfr.score(X_trainval_z_knn, y_trainval_knn))\nprint(knnclfr.score(X_test_z_knn,y_test_knn))\nprint(metrics.classification_report(y_test_knn,y_predict_knn))\nprint(metrics.confusion_matrix(y_test_knn,y_predict_knn))\n","e7c3e395":"knnclfr.fit(X_z_knn,y_res)","d4c2dcb4":"test_df.head()","58ed15bb":"test_df['Fare'].fillna(test_df['Fare'].median(),inplace=True)","f2922830":"col = []\nfor c in test_df.columns:\n    if test_df[c].dtypes=='object':\n        col.append(c)\n        \n\ntest_df_dummies = pd.get_dummies(test_df , columns=col, drop_first=True)","08dcae52":"test_df_dummies.head()","7e765eec":"test_df_dummies_knn = test_df_dummies.drop(['Age','Parch','Embarked_Q','Single_Traveller_Yes'],axis=1,inplace=True)\n","38a61c72":"test_df_dummies_knn","44556cfb":"X_test_knn = test_df_dummies.drop(['PassengerId'],axis=1)\nX_test_id_knn = test_df_dummies['PassengerId']\n","bf7911a7":"X_test_knn =X_test_knn.apply(zscore)\n","1fd4693b":"y_predict = knnclfr.predict(X_test_knn)","623c5f3e":"final_pred_df = pd.DataFrame(y_predict)\nfinal_pred_df.columns = ['Survived']\nX_test_id_knn = pd.DataFrame(X_test_id_knn)\nfinal_pred = X_test_id_knn.merge(final_pred_df, left_index=True, right_index=True)\nfinal_pred.shape\nfinal_pred.to_csv('csv_to_submit2602-2.csv', index = False)","1bd24e1f":"from sklearn import preprocessing\n\nlab_enc = preprocessing.LabelEncoder()\n","89b522aa":"lab_enc = preprocessing.LabelEncoder()","d0f81a1c":"col = []\nfor c in train_df.columns:\n    if train_df[c].dtypes=='object':\n        train_df[c] = lab_enc.fit_transform(train_df[c])\n        print(\"Column {} has been encoded\".format(c))","7a327d47":"col = []\nfor c in test_df.columns:\n    if test_df[c].dtypes=='object':\n        test_df[c] = lab_enc.fit_transform(test_df[c])\n        print(\"Column {} has been encoded\".format(c))","250a017e":"X = train_df.drop(['Survived','PassengerId'],axis=1)\nX_id = train_df['PassengerId']\ny = train_df['Survived']","bb79fcfb":"from imblearn.combine import SMOTETomek\nsmk = SMOTETomek(random_state=1)\nX_res,y_res=smk.fit_sample(X,y)","d02de066":"X_trainval, X_test, y_trainval, y_test = train_test_split(X_res,y_res, test_size=0.20,random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.20,random_state=1)","4d6bd503":"X_trainval_z =X_trainval.apply(zscore)\nX_train_z =X_train.apply(zscore)\nX_val_z =X_val.apply(zscore)\nX_test_z =X_test.apply(zscore)\nX_z = X_res.apply(zscore)","8decac1c":"# Grid Search based on Max_features, Min_Samples_Split and Max_Depth\nparam_grid = [\n{\n'n_neighbors': list(range(1,50)),\n'algorithm': ['auto', 'ball_tree', 'kd_tree','brute'],\n'leaf_size': [10,15,20,30],\n'n_jobs': [-1], \n'weights' : ['uniform','distance']\n}\n]","6651f81b":"import multiprocessing\ngs = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, scoring='accuracy', n_jobs=multiprocessing.cpu_count(),cv=3)\ngs.fit(X_train_z, y_train)","7f79c11a":"gs.best_estimator_","b4c78887":"gs.best_score_","470db2f6":"knn_clfr = KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n                     weights='uniform')","3a2e5464":"train_df.shape","e69e4523":"sfs1 = sfs(knn_clfr, k_features=6, forward=True, scoring='accuracy', cv=10,n_jobs=-1)","273395c9":"sfs1 = sfs1.fit(X_train_z.values, y_train.values)","13ab9a83":"sfs1.get_metric_dict()","4dd4a8c1":"columnList = list(X_train_z.columns)\nfeat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)","dbb92940":"subsetColumnList = [columnList[i] for i in feat_cols] \nprint(subsetColumnList)","c733f339":"train_df.head(1)","7ec2612a":"train_df_bkp = train_df.copy()\ntest_df_bkp = test_df.copy()","b892b6fd":"X_knn = train_df[['Pclass', 'Sex', 'Age', 'Fare', 'Single_Traveller', 'Person']]\nX_id_knn = train_df['PassengerId']\ny_knn = train_df['Survived']","098085ab":"from imblearn.combine import SMOTETomek\nsmk = SMOTETomek(random_state=1)\nX_res,y_res=smk.fit_sample(X_knn,y_knn)","3aec7fcb":"X_trainval_knn, X_test_knn, y_trainval_knn, y_test_knn = train_test_split(X_res,y_res, test_size=0.20,random_state=1)\nX_train_knn, X_val_knn, y_train_knn, y_val_knn = train_test_split(X_trainval_knn,y_trainval_knn, test_size=0.20,random_state=1)","fe184714":"X_trainval_z_knn =X_trainval_knn.apply(zscore)\nX_train_z_knn =X_train_knn.apply(zscore)\nX_val_z_knn =X_val_knn.apply(zscore)\nX_test_z_knn =X_test_knn.apply(zscore)\nX_z_knn = X_res.apply(zscore)","6d414e25":"X_train_z_knn.shape","1bf8bd62":"gs = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, scoring='accuracy', n_jobs=multiprocessing.cpu_count(),cv=10)\ngs.fit(X_train_z_knn, y_train_knn)","4a4e6bf7":"gs.best_estimator_","3313f399":"gs.best_score_","cdce0bd4":"knn_clfr = KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n                     metric_params=None, n_jobs=-1, n_neighbors=12, p=2,\n                     weights='distance')","765fd65b":"knn_clfr.fit(X_train_z_knn, y_train_knn)","151973d7":"y_predict = knn_clfr.predict(X_val_z_knn)","99b7b1f2":"print(knn_clfr.score(X_train_z_knn,y_train_knn))\nprint(knn_clfr.score(X_val_z_knn,y_val_knn))\nprint(metrics.classification_report(y_val_knn,y_predict))\nprint(metrics.confusion_matrix(y_val_knn,y_predict))","60ad1e73":"knn_clfr.fit(X_trainval_knn, y_trainval_knn)","3f11102e":"y_predict = knn_clfr.predict(X_test_z_knn)","d09fbecd":"print(knn_clfr.score(X_trainval_knn, y_trainval_knn))\nprint(knn_clfr.score(X_test_z_knn,y_test_knn))\nprint(metrics.classification_report(y_test_knn,y_predict))\nprint(metrics.confusion_matrix(y_test_knn,y_predict))","ea131001":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [10,50,150,175,200,250]   # returns evenly spaced 10 numbers\n# Number of features to consider at every split\nmax_features = ['auto']\n# Maximum number of levels in tree\nmax_depth = [6,7,12,15,18,22,25]  # returns evenly spaced numbers can be changed to any\n#max_depth.append(None)\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nsubsample = [0.7,0.8,0.9]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               \n               'bootstrap': bootstrap,\n               'subsample': subsample\n              }\n\nprint(random_grid)","290350df":"# Use the random grid to search for best hyperparameters\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nrf_random = GridSearchCV(estimator=XGBClassifier(), param_grid=random_grid,\n                                \n                              cv = 3, verbose=0,  n_jobs=1,\n                              return_train_score=True)\n\n# Fit the random search model\nrf_random.fit(X_train_z, y_train);\n#currently running","a34b9a0b":"rf_random.best_estimator_","7d87714e":"rf_random.best_score_","b80ec73c":"model = XGBClassifier(base_score=0.5, booster=None, bootstrap=True, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.300000012, max_delta_step=0, max_depth=7,\n              max_features='auto', min_child_weight=1, \n              monotone_constraints=None, n_estimators=10, n_jobs=0,\n              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.9,\n              tree_method=None, validate_parameters=False, verbosity=None)","b1526f54":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nresults = cross_val_score(model, X_train_z, y_train, cv=10)\nprint(results)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))","4351d34d":"sfs1 = sfs(model, k_features=8, forward=True, scoring='accuracy', cv=10,n_jobs=-1)","aa9ff90a":"sfs1 = sfs1.fit(X_train_z.values, y_train.values)","c1b1492c":"sfs1.get_metric_dict()","6becdf87":"columnList = list(X_train_z.columns)\nfeat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)","b3bb932f":"subsetColumnList = [columnList[i] for i in feat_cols] \nprint(subsetColumnList)","eff07002":"X = train_df_dummies[['Pclass', 'Age', 'SibSp', 'Fare', 'Sex_male', 'Person_Mid aged Adult', 'Person_Teenager', 'Person_Young Adult']]\nx_id = train_df_dummies['PassengerId']\ny = train_df_dummies['Survived']","b0eac172":"from imblearn.combine import SMOTETomek\nsmk = SMOTETomek(random_state=1)\nX_res,y_res=smk.fit_sample(X,y)","34860490":"X_trainval, X_test, y_trainval, y_test = train_test_split(X_res,y_res, test_size=0.20,random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.20,random_state=1)","ca89a889":"X_trainval_z =X_trainval.apply(zscore)\nX_train_z =X_train.apply(zscore)\nX_val_z =X_val.apply(zscore)\nX_test_z =X_test.apply(zscore)\nX_z = X_res.apply(zscore)","0c0e33b8":"model.fit(X_train_z, y_train)","b8e6a0fb":"y_predict = model.predict(X_val_z)","a4acc4e7":"print(model.score(X_train_z,y_train))\nprint(model.score(X_val_z,y_val))\nprint(metrics.classification_report(y_val,y_predict))\nprint(metrics.confusion_matrix(y_val,y_predict))","a1f84bcb":"model.fit(X_trainval_z, y_trainval)","4928794e":"y_predict = model.predict(X_test)","9bf98268":"print(model.score(X_trainval_z, y_trainval))\nprint(model.score(X_test_z,y_test))\nprint(metrics.classification_report(y_test,y_predict))\nprint(metrics.confusion_matrix(y_test,y_predict))","0fc4fb75":"The age and Embarked columns has null values in training dataset which needs to be imputed. \nSimilarly Age and Fare dataset has null values. ","e21ec805":"The training dataset has 891 records and test_df has 418 records. The test dataset has 1 column in short since the target column is excluded","123ff01a":"The nummber female single traveller are very less.","ef4f8c19":"Univariant analysis, Bivariant and multivariant analysis","113b9f1c":"Most of the traveller(72.50%) have embarked at S ","879f674e":"Creating the model without any outlier treatment.","2ec6a76b":"Children in class 2 have 100% survival rate and old adults in class 3 have the least survival rate with just 5.55%","46a4e7b8":"In every age group the count of male is higher whereas in teenager group alone female traveller are slightly higher. Breaking the survival rate by age group by sex, children seem to have a farely same survival rate in both the gender, where as in all age group female seem to have hiher survival rate where in the teenager male having the least survival rate (less than 10%) and females in the Old Adult group have the higher survival rate with 86.66%.","21f291ee":"The above graph clearly shows that, young adults are the majority population and the survival rate is much lesser in that group and children having the highest survival rate.","5572298e":"The graph above shows only 38.38% of people have survided","9cccfd71":"The graph above shows more than 50 percentage of the people have travelled in class 3","2be4a8b3":"Overall only 38.38 percentage of the travellers survived and the percentage split with respect to Pclass is shown.","cac25fdf":"In the total passenger 64.76% were male and 35.24% people were female. ","e5f904a4":"Both the records belong to same ticket, which means they are travelling together and we can see they were in class 1. \nimputing the column with most repeated value using mode.","64292d8d":"The column Cabin has more than 70% of the records with null values in both train and test dataset\nImputing this column will not be helpful, so deleting this column","78840622":"The fare column shows the same prize shows something interesting. We do have fare starting from 0 to 500. The highest probability \nfor the reason for the fare to be zero is they might be the crew members of the ship and the highest fare might be due to large nummber of people travelling together, as the fare displayed is at the itinerary level. ","5eb3ba5c":"60% of the people have travelled alone. This cannot be exact number as mentione in the data description children who have travelled with their nanny Parch is mentioned as 0. But we can say that a majority of the people have travelled alone.","2b719179":"The above graph shows, the survival rate is much higher in the class 1 travellers and very low for class 3","7160a243":"The count of single traveller seems to be high,but only 30 percentage of the single traveller survived where as more than 50% of the  traveller who travelled with someone have survived. ","a7596f12":"By all means single traveller has less survival rate","7d384a72":"The above graph shows that the number of male passengers were almost double but more than 70% of the female passengers survived whereas only 20 % of the men survived. ","65d44b15":"Interestingly for all the passengers who have 0 as fare has embarked at S, they are all male, all are travelling alone and none of them have survived. possibillly  they could be the crew members. All the travller with ticket mentioned as Line are in Class 3. They could be lower level crew member","127efc2a":"The distribution plot shows that the majority of the people were in the age group 20 to 40. ","3a408654":"We can remove the columns id and Name as it will not be helpful for our analysis ","5ad37995":"Though the number of single traveller female is very less almost 80% of them have survived, whereas only 15% of male single traveller have survived. "}}