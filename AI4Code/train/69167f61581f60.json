{"cell_type":{"753f89db":"code","871ff111":"code","0beb69fd":"code","0e28bcaa":"code","f62d1055":"code","ea019cdc":"code","75ac6b34":"code","6ce25df8":"code","51d69339":"code","c7f8237b":"code","5f3904e7":"code","d64502d9":"code","28395e16":"code","320ad7f5":"code","4dc13d5b":"code","ffe34aa4":"code","08c424a2":"code","a9d445c9":"code","6837b9b4":"markdown","96061aa9":"markdown","24171ba3":"markdown","12fb4ae6":"markdown","7800701d":"markdown","66969494":"markdown","6d5af89b":"markdown","97305731":"markdown","8658a798":"markdown","c9784cfe":"markdown","bf5e95c4":"markdown","4f8a0e28":"markdown","ceb2e31b":"markdown","1881394d":"markdown","eaa4773e":"markdown","a3d0562a":"markdown","09f5c472":"markdown","c176c750":"markdown","dd21823c":"markdown","db10a98b":"markdown","f1f9768e":"markdown","7ecf6088":"markdown","a3ebde90":"markdown","144f444a":"markdown","d753444b":"markdown"},"source":{"753f89db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","871ff111":"import pandas as pd\nimport matplotlib.pyplot as plt\ndata =  pd.read_csv(\"..\/input\/pump-sensor-data\/sensor.csv\")","0beb69fd":"data.head()","0e28bcaa":"# Let's convert the data type of timestamp column to datatime format\ndata['datetime'] = pd.to_datetime(data['timestamp'])\ndata.drop(['timestamp', 'Unnamed: 0'], axis=1, inplace=True)\ndata.head()","f62d1055":"# Extract the readings from the BROKEN state of the pump\nbroken = data[data['machine_status']=='BROKEN']\nrecovering = data[data['machine_status']=='RECOVERING']\n# Extract the names of the numerical columns\nsensors_to_plot = data.columns[0:5:2]\n# Plot time series for each sensor with BROKEN state marked with X in red color\nfor name in sensors_to_plot:\n    plt.figure(figsize=(18,3))\n    plt.plot(broken['datetime'], broken[name], linestyle='none', marker='X', color='red', markersize=12, label='broken')\n    plt.plot(recovering['datetime'], recovering[name], linestyle='none', marker='X', color='orange', markersize=6, label='recovering')\n    plt.plot(data['datetime'], data[name], color='blue', label='working')\n    plt.title(name)\n    plt.legend()\n    plt.show()","ea019cdc":"data['machine_status'].value_counts()","75ac6b34":"perc_nans = data.isnull().sum().sort_values(ascending=False)\/len(data)\nperc_nans.head(10)","6ce25df8":"df_tidy = data.drop_duplicates()\ndf_tidy.drop(['sensor_15', 'sensor_50', 'sensor_51', 'sensor_00'], axis=1, inplace=True)","51d69339":"from sklearn.cluster import KMeans","c7f8237b":"X_train = df_tidy.drop(['machine_status', 'datetime'], axis=1)\nX_train -= X_train.min()\nX_train \/= X_train.max()\nX_train.head()","5f3904e7":"X_train.fillna(method='ffill', inplace=True)","d64502d9":"inertia = []\n \nfor k in range(1, 15):\n    # Building and fitting the model with k clusters\n    kmeanModel = KMeans(n_clusters=k).fit(X_train)\n    inertia.append(kmeanModel.inertia_)","28395e16":"K = range(1, 15)\nplt.figure(figsize=(7,5))\nplt.plot(K, inertia, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method using Inertia')\nplt.show()","320ad7f5":"kmeans = KMeans(n_clusters=5, random_state=42)\nkmeans.fit(X_train)\nlabels = kmeans.predict(X_train)\n\nunique_elements, counts_elements = np.unique(labels, return_counts=True)\nclusters = np.asarray((unique_elements, counts_elements))\ndf_tidy['cluster'] = labels","4dc13d5b":"colors = ['limegreen', 'orange', 'red', 'yellow', 'cyan']\ncolors_plot = [colors[i] for i in df_tidy['cluster'].values]\nsensors_to_plot = ['sensor_01']\nfor name in sensors_to_plot:\n    plt.figure(figsize=(18,3))\n    plt.plot(df_tidy['datetime'], df_tidy[name], color='blue', label='sensor data')\n    plt.vlines(df_tidy['datetime'], 32, 55, color=colors_plot, alpha=0.5)\n    plt.title(name)\n    plt.legend()\n    plt.show()","ffe34aa4":"from sklearn.manifold import TSNE\n\nsubsampling_step = 500\nX_subset = X_train.loc[::subsampling_step]\ncolors_plot = [colors[i] for i in df_tidy['cluster'].loc[::subsampling_step]]\nX_embedded = TSNE(perplexity = 30, random_state=42).fit_transform(X_subset)\nplt.figure(figsize=(7,5))\nplt.scatter(X_embedded[:,0], X_embedded[:,1], color=colors_plot)","08c424a2":"centroid_distance = np.abs(kmeans.cluster_centers_[0] - kmeans.cluster_centers_[4])\nchanged_sensors = np.argsort(centroid_distance)","a9d445c9":"for sensor_idx in changed_sensors[-3:]:\n    sensor_data = df_tidy.iloc[:, sensor_idx]\n    plt.figure(figsize=(18,3))\n    plt.plot(df_tidy['datetime'], sensor_data, color='blue', label='changed_sensor')\n    plt.title(f'Sensor {df_tidy.columns[sensor_idx]}')\n    plt.legend()\n    plt.show()","6837b9b4":"**Question 4** \\\nWhat other signal processing techniques would you use for this problem?","96061aa9":"Let's now import pandas and pyplot, and load the dataset.","24171ba3":"We can use t-SNE to project the clusters onto 2d and plot them, to have a rough idea of their geometrical relationships","12fb4ae6":"A few observations:\n- Sensor measurements vary a lot in range\n- Sensors are likely to be refering to very different quantities (some temperature, some pressure, etc)\n\nWe will need to rescale all the sensor data separately, so that values along each column are in the same range (0 to 1).","7800701d":"Now we can plot the data and quickly analyse some of the patterns","66969494":"# Data exploration and cleaning","6d5af89b":"Time of each event is recorded in the timestamp column. Since the format is string, we create a new columns where time is registered as pd.Timestamp.\n\nWe will also drop the 'Unnamed: 0' column since it is just a row count","97305731":"To fill time series missing values, we can use pandas ffill","8658a798":"For each sensor, we want now to print the percentage of missing data","c9784cfe":"**Elbow method**","bf5e95c4":"In this Notebook we will analyse data from a water pump which experienced frequent failures in the period spring\/summer 2018.\n\nAs input we have time series data from 52 sensors which measure different physical properties of the system (like temperature and pressure). We will try to extract the different working modes of the pump and highlight possible early warning signals of breakage.\n\nAs always, we will start with some (brief) exploratory analysis, with the aim of examining missing or redundant data.","4f8a0e28":"The elbow method is a heuristic method to decide how many parameters to use for kmeans clustering. ","ceb2e31b":"We have found different regimes of the water pump. What are the sensors that most change in between these different regimes?","1881394d":"Let's have a first look at the data","eaa4773e":"We create X_train, a new dataset which comprises only of sensor data (no labels, no timestamps). \n\nThen, we will rescale the sensor data so that they lay in a similar range. If we subtract the minimum across the column and divide by the maximum, all the values will be between 0 and 1","a3d0562a":"We will create a new dataframe called df_tidy and drop all duplicates and all sensors with >3% of missing data. We will also drop 'Unnamed: 0' cause it is just a row count ","09f5c472":"We now want to fill the missing values for each column. \n\n**Question 2** \\\nWhen filling time series null values, which approach would you use? \\\ne.g. you have the temperature values in Munich for April, but you are missing 10 minutes in one of the days. What do you use to fill those values?\n\na) copy the last registered temperature before missing values \\\nb) average temperature across the day \\\nc) copy the temperature from previous day at the same time \\\nd) average temperature in April","c176c750":"**Question 1** \\\nIn order to detect early signals of pump failure, and given the data distrubution, what do you think would be the best approach\n\na) Supervised learning, with classification to predict failure events \\\nb) Supervised learning with regression \\\nc) Unsupervised learning and anomaly detection \\\nd) None of the above: we have too little data","dd21823c":"The next steps would be to characterise each of these regimes, and then do the anomaly detection. In the case of this dataset, we do not have enough data to describe the 'normal' working mode of the system, so it is very difficult to label anything as anomalous. Knowledge of what the sensors refer to, or more information on why the pump sensors shift so often would certainly help.","db10a98b":"# Back to sensors: centroid analysis","f1f9768e":"# K-means clustering and operating modes","7ecf6088":"# Cluster visualisation","a3ebde90":"Let's count explicitly the number of falures of the pump","144f444a":"Since we do not have enough data for supervised learning, we will explore the different working regimes of the pump with unsupervised learning. We will be using Kmeans clustering","d753444b":"**Question 3** \\\nUsing the elbow method, from the previous graph what would you use as the number of clusters for k-means?\n\na) 2 \\\nb) 4 \\\nc) 5 \\\nd) 6"}}