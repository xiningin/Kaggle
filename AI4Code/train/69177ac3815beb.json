{"cell_type":{"43f6ab1d":"code","ec453040":"code","c1cdbdc9":"code","7bcbe986":"code","52dcf7eb":"code","9c825f6b":"code","d9768902":"code","a7b62500":"code","c52a049f":"code","747ee212":"code","40c49368":"code","97dd33b9":"code","ebc3492a":"code","24466381":"code","5ab6c1c7":"code","f3ef3062":"code","d22f1e56":"code","82498d87":"code","70653029":"code","0f1b21b5":"code","ddb273cb":"markdown","1840b614":"markdown","91850054":"markdown","48033a7b":"markdown","3e82a517":"markdown","3a709ec3":"markdown","cb85fe01":"markdown","0bbb3a1f":"markdown","6853a420":"markdown","90c8a632":"markdown","447771bd":"markdown","75663551":"markdown","e434d46d":"markdown","3e6f2725":"markdown","676b4bf5":"markdown","7a896886":"markdown","e1b5423e":"markdown","8bdb5f93":"markdown","1007085b":"markdown","6d4d149b":"markdown","6d77ee9b":"markdown","a1bdb640":"markdown"},"source":{"43f6ab1d":"import numpy as np\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd ","ec453040":"df = pd.read_csv('..\/input\/datamahedi\/data.csv',delimiter=';')\ndf.columns","c1cdbdc9":"df.head(7)","7bcbe986":"df.isnull().sum()","52dcf7eb":"df = df.dropna()","9c825f6b":"df.describe(include='all')","d9768902":"#import seaborn as sns \n#sns.pairplot(df)","a7b62500":"df['Y']","c52a049f":"x=df.drop(['Y'], axis=1)\ny=df['Y'].values\ny\n#if you write the code y = df['y'] you will get a series of values, but if you add (.values) you will get an array of values which can be reshaped after that. ","747ee212":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.25,shuffle=False)","40c49368":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\ny= y.reshape(-1,1)\ny=sc.fit_transform(y)","97dd33b9":"from keras import Sequential\nfrom keras.layers import Dense\ndef build_regressor():\n    regressor = Sequential()\n    regressor.add(Dense(units=380, input_dim=380))\n    regressor.add(Dense(units=1))\n    regressor.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae','accuracy'])\n    return regressor","ebc3492a":"from keras.wrappers.scikit_learn import KerasRegressor\nregressor = KerasRegressor(build_fn=build_regressor, batch_size=32,epochs=100)","24466381":"results=regressor.fit(X_train,y_train)","5ab6c1c7":"y_pred= regressor.predict(X_test)","f3ef3062":"fig, ax = plt.subplots()\nax.scatter(y_test, y_pred)\nax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()","d22f1e56":"\"\"\"Import the required modules\"\"\"\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import r2_score","82498d87":"reg = MLPRegressor(hidden_layer_sizes=(380,100,10),activation=\"relu\" ,random_state=1, max_iter=2000).fit(X_train,y_train)","70653029":"y_pred=reg.predict(X_test)\nprint(\"The Score with \", (r2_score(y_pred, y_test)))","0f1b21b5":"fig, ax = plt.subplots()\nax.scatter(y_test, y_pred)\nax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\n\nax.set_ylabel('Predicted')\nplt.show()","ddb273cb":"![image.png](attachment:image.png)","1840b614":"Creating the neural network for the regressor. We have the input nodes, we create one hidden layer with the nodes and an output layer.\n\nAs this a regression problem, the loss function we use is mean squared error and the metrics against which we evaluate the performance of the model is mean absolute error and accuracy.\nMean absolute error is the absolute difference between the predicted value and the actual value.\n\nwe define a function build_regressor to use these wrappers. build_regressor creates and returns the Keras sequential model.","91850054":"### Importing the dataset","48033a7b":"Creating the training and test dataset","3e82a517":"In addition to \u201cRELU\u201d activation, MLPRegressor supports the \u201csigmoid\u201d and \u201chyperbolic tan\u201d function.\nthe trained model is used to predict the target values of the reserved test dataset, which model has not seen before.","3a709ec3":"# NN for regression ","cb85fe01":"All input features are numerical so we need to scale them. StandardScaler works well when the data is normally distributed. Based on the pair plot we see that the data is not normally distributed. Hence we use MinMaxScaler to scale the data","0bbb3a1f":"### Feature Scaling","6853a420":"(MLPs) can be used successfully for classification, albeit that state-of-the-art methods may yield better performance for some datasets.\n\nand it can also be used for a regression problem.","90c8a632":"### Dropping the missing values ","447771bd":"### Importing the libraries","75663551":"you play with the hidden layer sizes to get a better r2_score !\nhere few notes to take into consideration : \n* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n* The number of hidden neurons should be 2\/3 the size of the input layer, plus the size of the output layer.\n* The number of hidden neurons should be less than twice the size of the input layer.\n* see this https:\/\/www.heatonresearch.com\/2017\/06\/01\/hidden-layers.html\n","e434d46d":"Salient points of Multilayer Perceptron (MLP) in Scikit-learn\n* There is no activation function in the output layer.\n* For regression scenarios, the square error is the loss function, and cross-entropy is the loss function for the classification\n* It can work with single as well as multiple target values regression.\n* Unlike other popular packages, likes Keras the implementation of MLP in Scikit doesn\u2019t support GPU.\n* We cannot fine-tune the parameters like different activation functions, weight initializers etc. for each layer.","3e6f2725":"### Splitting the dataset into the Training set and Test set","676b4bf5":"We pass build_regressor function to the build_fn argument when constructing the KerasRegressor class. Batch_size is 32 and we run 100 epochs","7a896886":"We do a pairplot for all the variable sin the dataset","e1b5423e":"In the below code, three hidden layers are modelled, with 380 neurons in each layer. Considering the input and output layer, we have a total of 5 layers in the model. In case any optimiser is not mentioned then \u201cAdam\u201d is the default optimiser and it can manage pretty large dataset.","8bdb5f93":"We create input features and target variables","1007085b":"# Stacking Ensemble of Deep earning Neural Networks","6d4d149b":"## Part 1 - Data Preprocessing","6d77ee9b":"Black broken line is the predicted values and we can see that it encompasses most of the values\n\nCouple of tips for better accuracy\n* Always standardize both input features and target variable. If we only standardize input feature then we will get incorrect predictions\n* Data may not be always normally distributed so check the data and then based on the distribution apply StandardScaler, MinMaxScaler, Normalizer or RobustScaler","a1bdb640":"# **MLP for regression **"}}