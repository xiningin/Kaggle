{"cell_type":{"28f393d9":"code","7693ea03":"code","428f2dd0":"code","8ea06783":"code","05c583c5":"code","960b08ed":"code","f8c43ac3":"code","86e46202":"code","4693d55e":"code","a8ff2cb6":"code","8e74e364":"code","779df403":"code","c0a42cff":"code","a215be14":"code","c47c4080":"code","690b20f0":"code","8391da13":"code","cbb4f1c0":"code","19586067":"code","e528f0e8":"code","3bd5182e":"code","76466696":"code","ab53883a":"code","4a328ca1":"code","edfbafbf":"code","0f6de28c":"code","8afc3776":"code","6aa76c5f":"code","de7206d3":"code","85561f5f":"code","27584b1f":"code","89fd148a":"code","23b74662":"code","a9400327":"code","5e9dfff2":"code","0ffad52c":"code","8c4f0918":"code","63d44090":"code","ae325765":"code","6ab0a9d1":"code","3abcd3b0":"code","d6cceae5":"code","0c3e4eb9":"code","0ff164ac":"code","a89a6ab8":"code","4d862282":"code","11320b19":"code","e6833642":"code","b7eb70f5":"code","638d56f1":"code","0f679e44":"code","f45bb8b5":"code","562aedf8":"markdown","17add6e9":"markdown","af662b5c":"markdown"},"source":{"28f393d9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","7693ea03":"DT=1\noptimize_model=False\nMake_submission=True\nn_estimators=150 #400 #500  #1500\nmax_depth=2 #4 #12  #8","428f2dd0":"train=pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\")\ntrain.head()","8ea06783":"train[train['Province_State'].notna()].groupby(['Country_Region'], sort=False)['Province_State'].nunique()","05c583c5":"def add_location(df_old):\n    df=df_old.copy()\n    df['Date']=pd.to_datetime(df['Date'])\n    df['Country_Region']=df['Country_Region'].fillna('')\n    df['Province_State']=df['Province_State'].fillna('')\n    df['location']=df['Province_State'].astype('str')+\" \"+df['Country_Region'].astype('str')\n    return df","960b08ed":"train=add_location(train)","f8c43ac3":"max_cases_old=train[train['Date']<'2020-03-22'].groupby(['location'], sort=False)['ConfirmedCases'].max()\nmax_cases=train.groupby(['location'], sort=False)['ConfirmedCases'].max()\nprint(\"Now: {}\\r\\nSeven Days ago: {}\".format(len(max_cases[max_cases<5]),len(max_cases_old[max_cases_old<5])))","86e46202":"train.set_index('location',inplace=True)\n\ntrain['day_of_year']=train['Date'].dt.dayofyear\ntrain['day_of_week']=train['Date'].dt.dayofweek\n\n\n\nfirst_day=train[(train['ConfirmedCases']>0)].groupby(['location'], sort=False)['day_of_year'].min()\nfirst_day.rename('first_day',inplace=True)","4693d55e":"def add_days_passed(df_old,first_day):\n    df=df_old.copy()\n    df=pd.concat([df,first_day],axis=1,join='inner')\n    df['days_passed']=df['day_of_year']-df['first_day']\n    df.drop(columns=['first_day'],inplace=True)\n    df['location']=df.index\n    df.set_index('Id',inplace=True)\n    df['Id']=df.index\n    return df\n\n \n","a8ff2cb6":"train=add_days_passed(train,first_day)\n\ntrain.head()","8e74e364":"country_stat=pd.read_csv('..\/input\/countryinfo\/covid19countryinfo.csv')\ncountry_stat = country_stat[country_stat['region'].isnull()] \n\ndef add_country_stat(old_df,country_stat):\n    df=old_df.copy()\n    df=df.merge(country_stat[['country','pop','medianage','sex65plus','lung','smokers','density']],left_on=['Country_Region'],right_on=['country'],how='left')\n    df.drop(columns=['country'],inplace=True)\n    \n    df['pop']=df['pop'].fillna(1000)\n    df['pop']=df['pop'].apply(lambda x: int(str(x).replace(',', '')))\n    #df['gdp2019']=df['gdp2019'].fillna(0)\n    #df['gdp2019']=df['gdp2019'].apply(lambda x: int(str(x).replace(',', '')))\n    #df['gdp2019']=df['gdp2019']\/df['pop']\n    \n    \n    df['density']=df['density'].fillna(0)\n    df['medianage']=df['medianage'].fillna(0)\n    #df['sexratio']=df['sexratio'].fillna(1)\n    df['sex65plus']=df['sex65plus'].fillna(1)\n    df['lung']=df['lung'].fillna(24)\n    df['smokers']=df['smokers'].fillna(24)\n    #df['lung']=df['lung']*df['pop']\n    \n    return df\n    \n\ntrain=add_country_stat(train,country_stat)","779df403":"country_stat.info()","c0a42cff":"border_info=pd.read_csv(\"https:\/\/raw.githubusercontent.com\/geodatasource\/country-borders\/master\/GEODATASOURCE-COUNTRY-BORDERS.CSV\")\nborder_info.drop(columns=[\"country_code\",\"country_border_code\"],inplace=True)\nborder_info.replace({'United States of America':'US',\n                    'United Kingdom of Great Britain and Northern Ireland':'United Kingdom',\n                    'Bolivia (Plurinational State Of)':'Bolivia',\n                    'Brunei Darussalam':'Brunei',\n                    'Gambia (the)':'Gambia',\n                     'Congo':'Congo (Kinshasa)',\n                     'Cote d\u2019Ivoire':\"Cote d'Ivoire\",\n                     \"Iran (Islamic Republic of)\":'Iran',\n                     \"Korea (the Republic of)\":'Korea, South',\n                    \"Lao People's Democratic Republic\":'Laos',\n                     \"Moldova (the Republic of)\":'Moldova',\n                     \"Russian Federation\":'Russia',\n                    \"Syrian Arab Republic\":'Syria',\n                     \"Taiwan (Province of China)\":'Taiwan*',\n                    \"Tanzania (the United Republic of)\":'Tanzania',\n                     \"Venezuela (Bolivarian Republic of)\":'Venezuela',\n                     \"Viet Nam\":'Vietnam'},inplace=True)\nborder_info=border_info.fillna(\"\")\nborder_info.to_csv(\"border_info.csv\")\n","a215be14":"from itertools import product as it_product\ndef expand_grid(data_dict):\n  rows = it_product(*data_dict.values())\n  return pd.DataFrame.from_records(rows, columns=data_dict.keys())","c47c4080":"skel=expand_grid({'Index':border_info.index,'Date':train['Date'].unique()})\nskel.info()","690b20f0":"country_info=train.groupby(['Date','Country_Region'])['ConfirmedCases'].sum()","8391da13":"skel=expand_grid({'Index':border_info.index,'Date':train['Date'].unique()})\n\nskel=skel.merge(border_info, how='inner', left_on=['Index'],right_index=True)\nskel=skel.merge(country_info, how='inner', \n                left_on=['Date','country_border_name'],right_on=['Date','Country_Region'])","cbb4f1c0":"from datetime import timedelta\nskel['Date']=skel['Date']+timedelta(days=DT)\nborder_cases=skel.groupby(['country_name','Date'])['ConfirmedCases'].sum()\nlen(skel['country_name'].unique())","19586067":"train=train.merge(border_cases, how='left', left_on=['Country_Region','Date'],right_on=['country_name','Date'])\ntrain['ConfirmedCases_y']=train['ConfirmedCases_y'].fillna(0)\ntrain.rename(columns={'ConfirmedCases_y':'ConfirmedCases_neighbors','ConfirmedCases_x':'ConfirmedCases'},inplace=True)","e528f0e8":"big_train = pd.concat([train,pd.get_dummies(train['location'], prefix='loc')],axis=1)\nbig_train['ConfirmedCases_neighbors']=np.log1p(big_train['ConfirmedCases_neighbors'])\nbig_train.reset_index(inplace=True)\nbig_train.drop(columns=[\"Id\"],inplace=True)","3bd5182e":"big_train.shape","76466696":"def df_add_deltas(df_old):\n    df=df_old.copy()\n    df=df.sort_values(by=['location', 'Date'])\n    df['d_ConfirmedCases'] = df.groupby(['location'])['ConfirmedCases'].diff()\n    df['d_Fatalities'] = df.groupby(['location'])['Fatalities'].diff()\n    df.loc[df['d_Fatalities']<0,'d_Fatalities']=0\n    df.loc[df['d_ConfirmedCases']<0,'d_ConfirmedCases']=0\n    \n    df['prev_ConfirmedCases']=df['ConfirmedCases']-df['d_ConfirmedCases']\n    df['prev_Fatalities']=df['Fatalities']-df['d_Fatalities']\n    \n    df['prev_ConfirmedCases']=np.log1p(df['prev_ConfirmedCases'])\n    df['prev_Fatalities']=np.log1p(df['prev_Fatalities'])\n    \n    df['prev5_ConfirmedCases'] = df.groupby(['location'])['ConfirmedCases'].shift(5).fillna(0)\n    df['prev5_Fatalities'] = df.groupby(['location'])['Fatalities'].shift(5).fillna(0)\n    \n    df['prev5_ConfirmedCases']=np.log1p(df['prev5_ConfirmedCases'])\n    df['prev5_Fatalities']=np.log1p(df['prev5_Fatalities'])\n    \n    first_day_stat=df[df['Date']=='2020-01-22']\n    df.drop(df[df['Date']=='2020-01-22'].index, inplace=True)\n    \n    return df,first_day_stat\n    ","ab53883a":"big_train,first_day_stat=df_add_deltas(big_train)","4a328ca1":"#big_train.head()","edfbafbf":"big_train.reset_index(inplace=True,drop=True)","0f6de28c":"X=big_train.drop(columns=['Province_State','Country_Region','Date','ConfirmedCases','Fatalities','location',\n                          'd_ConfirmedCases','d_Fatalities'])\n\ny=big_train['d_ConfirmedCases']\ny_2=big_train['d_Fatalities']","8afc3776":"max_day=X['day_of_year'].max()\nmask_train=X['day_of_year']<max_day-DT+1\nmask_test=X['day_of_year']>=max_day-DT+1","6aa76c5f":"X_train=X[mask_train]\nX_test=X[mask_test]\n\n\ny_train=y[mask_train]\ny_test=y[mask_test]\n\ny_train_2=y_2[mask_train]\ny_test_2=y_2[mask_test]","de7206d3":"X_test['day_of_year'].nunique()","85561f5f":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\ncorr = big_train[['d_ConfirmedCases','d_Fatalities','days_passed','ConfirmedCases_neighbors','pop',\n                  'medianage','sex65plus','lung','smokers','density','prev_ConfirmedCases','prev_Fatalities']].corr(\"spearman\")\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(12,12))\n    ax = sns.heatmap(corr, annot=True,cmap=\"YlGnBu\",vmax=.3, square=True, linewidths=.3)\nplt.show()","27584b1f":"X_train.drop(columns=['day_of_year'],inplace=True)  #including day of year makes things worse RMLSE goes up from 0.49 to 0.7\nX_test.drop(columns=['day_of_year'],inplace=True)   #including day of year makes things worse RMLSE goes up from 0.49 to 0.7\n\nX_train.drop(columns=['day_of_week'],inplace=True)  #including day of week makes things worse RMLSE goes up from 0.49 to 0.57\nX_test.drop(columns=['day_of_week'],inplace=True)   #including day of week makes things worse RMLSE goes up from 0.49 to 0.57\n\nX.drop(columns=['day_of_year'],inplace=True)  \nX.drop(columns=['day_of_week'],inplace=True)   \n\n","89fd148a":"X.drop(columns=['index'],inplace=True)   \nX_train.drop(columns=['index'],inplace=True)\nX_test.drop(columns=['index'],inplace=True)","23b74662":"# Best: -0.252369 using {'max_depth': 6, 'n_estimators': 1500}\n\n# Best: -1.051575 using {'max_depth': 6, 'n_estimators': 500} - predict shifts log\n\n# Best: -278.598983 using {'max_depth': 10, 'n_estimators': 500} - predict values\n\n# Best: -1.111758 using {'max_depth': 6, 'n_estimators': 500} - predict shifts log, knowing prev log \n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nif optimize_model:\n\n    model = xgb.XGBRegressor(random_state=42)\n    n_estimators_grid = [500, 750,1000]\n    max_depth_grid = [4, 6, 8]\n    param_grid = dict(max_depth=max_depth_grid, n_estimators=n_estimators_grid)\n    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=[(X[mask_train].index,X[mask_test].index)], verbose=1)\n    grid_result = grid_search.fit(X,np.log1p(y))\n    # summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    print(grid_result.cv_results_)","a9400327":"# Best: -0.211438 using {'max_depth': 5, 'n_estimators': 2500}\n\n# Best: -0.974302 using {'max_depth': 5, 'n_estimators': 400} - predict shifts log\n\n# Best: -274.964946 using {'max_depth': 12, 'n_estimators': 500}\n\n# Best: -1.064197 using {'max_depth': 5, 'n_estimators': 400}\n\n\n\nif optimize_model:\n\n    model = xgb.XGBRegressor(random_state=42)\n    n_estimators_grid = [400,500,600]\n    max_depth_grid = [3,4,5]\n    param_grid = dict(max_depth=max_depth_grid, n_estimators=n_estimators_grid)\n    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=[(X[mask_train].index,X[mask_test].index)], verbose=1)\n    grid_result = grid_search.fit(X,np.log1p(y))\n    # summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    print(grid_result.cv_results_)","5e9dfff2":"#Best: -0.211107 using {'max_depth': 5, 'n_estimators': 3000}\n\n#Best: -0.940498 using {'max_depth': 4, 'n_estimators': 400}\n\n#Best: -274.964946 using {'max_depth': 12, 'n_estimators': 500}\n\n#Best: -0.861262 using {'max_depth': 2, 'n_estimators': 200}\n\n#Best: -0.834393 using {'max_depth': 2, 'n_estimators': 150}\n\n\nif optimize_model:\n\n    model = xgb.XGBRegressor(random_state=42)\n    n_estimators_grid = [125,150,175]\n    max_depth_grid = [1,2,3]\n    param_grid = dict(max_depth=max_depth_grid, n_estimators=n_estimators_grid)\n    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=[(X[mask_train].index,X[mask_test].index)], verbose=1)\n    grid_result = grid_search.fit(X,np.log1p(y))\n    # summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    print(grid_result.cv_results_)","0ffad52c":"reg = xgb.XGBRegressor(n_estimators=n_estimators,max_depth=max_depth,random_state=42)\nreg_2 = xgb.XGBRegressor(n_estimators=n_estimators,max_depth=max_depth,random_state=42)","8c4f0918":"reg.fit(X_train,np.log1p(y_train))","63d44090":"plot = xgb.plot_importance(reg, max_num_features=10)","ae325765":"y_pred = reg.predict(X_test)","6ab0a9d1":"from sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(y_pred,np.log1p(y_test)))","3abcd3b0":"X_train_2=X_train.copy()\nX_train_2['d_confirmed']=y_train  #0.4412899060661785 <- without , with - 0.4463  \nX_test_2=X_test.copy()\nX_test_2['d_confirmed']=y_pred","d6cceae5":"reg_2.fit(X_train_2,np.log1p(y_train_2))","0c3e4eb9":"plot = xgb.plot_importance(reg_2, max_num_features=10)","0ff164ac":"y_pred_2 = reg_2.predict(X_test_2)","a89a6ab8":"np.sqrt(mean_squared_error(y_pred_2,np.log1p(y_test_2)))","4d862282":"test=pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv\")\ntest.rename(columns={'ForecastId':'Id'},inplace=True)\ntest=add_location(test)\n\ntest.set_index('location',inplace=True)\n\ntest['day_of_year']=test['Date'].dt.dayofyear\ntest['day_of_week']=test['Date'].dt.dayofweek\ntest=add_days_passed(test,first_day)\ntest=add_country_stat(test,country_stat)","11320b19":"days_to_predict=test['Date'].unique()\ndays_to_predict.sort()","e6833642":"test.head()","b7eb70f5":"big_train=big_train.drop(columns=[\"index\"])\n","638d56f1":"known=big_train['Date'].unique()\nprint(known)","0f679e44":"if Make_submission==True:\n    results=[]\n\n    for d in days_to_predict:\n        print(\"Predicting {}\".format(d))\n        if d in known:\n            print(\"Data Known\")\n        \n            X=big_train.drop(columns=['Province_State','Country_Region','ConfirmedCases','Fatalities','location','Date',\n                                  'day_of_year','day_of_week','d_ConfirmedCases','d_Fatalities'])\n\n            y=big_train['d_ConfirmedCases']\n            y_2=big_train['d_Fatalities']\n        \n            mask_train=big_train['Date']<d\n            mask_val=big_train['Date']==d\n        \n            X_train=X[mask_train]\n            y_train=y[mask_train]\n            y_train_2=y_2[mask_train]\n        \n            X_val=X[mask_val]\n            y_val=y[mask_val]\n            y_val_2=y_2[mask_val]\n        \n            reg = xgb.XGBRegressor(n_estimators=n_estimators,max_depth=max_depth,random_state=42)\n            reg_2 = xgb.XGBRegressor(n_estimators=n_estimators,max_depth=max_depth,random_state=42)\n        \n            reg.fit(X_train,np.log1p(y_train))\n        \n            y_pred = reg.predict(X_val)\n            print(\"MSLE {}\".format(mean_squared_error(y_pred,np.log1p(y_val))))\n        \n            X_train_2=X_train.copy()\n            X_train_2['d_ConfirmedCases']=np.log1p(y_train)  #0.4412899060661785 <- without , with - 0.4463  \n            X_val_2=X_val.copy()\n            X_val_2['d_ConfirmedCases']=y_pred\n        \n            reg_2.fit(X_train_2,np.log1p(y_train_2))\n        \n            y_pred_2 = reg_2.predict(X_val_2)\n        \n            print(\"MSLE {}\".format(mean_squared_error(y_pred_2,np.log1p(y_val_2))))\n        \n        #result=X_test[['']]\n        elif d-np.timedelta64(86400000000000,'ns') in known:\n            print(\"Data Known\")\n        \n            X=big_train.drop(columns=['Province_State','Country_Region','ConfirmedCases','Fatalities','location','Date',\n                                  'day_of_year','day_of_week','d_ConfirmedCases','d_Fatalities'])\n\n            y=big_train['d_ConfirmedCases']\n            y_2=big_train['d_Fatalities']\n        \n            mask_train=big_train['Date']<d\n        \n            X_train=X[mask_train]\n            y_train=y[mask_train]\n            y_train_2=y_2[mask_train]\n        \n        \n            reg = xgb.XGBRegressor(n_estimators=n_estimators,max_depth=max_depth,random_state=42)\n            reg_2 = xgb.XGBRegressor(n_estimators=n_estimators,max_depth=max_depth,random_state=42)\n        \n            reg.fit(X_train,np.log1p(y_train))\n        \n            X_train_2=X_train.copy()\n            X_train_2['d_ConfirmedCases']=np.log1p(y_train)  #0.4412899060661785 <- without , with - 0.4463  \n            \n            reg_2.fit(X_train_2,np.log1p(y_train_2))\n        \n        \n        \n        X_test=test[test['Date']==d]\n    \n        day=X_test['day_of_year'].iloc[0]\n    \n        country_info=big_train[big_train['day_of_year']==day-1].groupby(['Country_Region'])['ConfirmedCases'].sum()\n    \n        border_cases=border_info.merge(country_info, how='inner', \n                left_on=['country_border_name'],right_on=['Country_Region'])\n    \n        border_cases=border_cases.groupby(['country_name'])['ConfirmedCases'].sum()\n        border_cases=border_cases.rename('ConfirmedCases_neighbors')\n    \n        X_test=X_test.merge(border_cases, how='left', left_on=['Country_Region'],right_on=['country_name'])\n        X_test['ConfirmedCases_neighbors']=X_test['ConfirmedCases_neighbors'].fillna(0)\n    \n        X_test = pd.concat([X_test,pd.get_dummies(X_test['location'], prefix='loc')],axis=1)\n        X_test['ConfirmedCases_neighbors']=np.log1p(X_test['ConfirmedCases_neighbors'])\n        \n        X_test=X_test.merge(big_train[big_train['day_of_year']==day-1][['location','ConfirmedCases','Fatalities']], how='left', \n                 left_on=['location'],right_on=['location'])\n        X_test.rename(columns={'ConfirmedCases':'prev_ConfirmedCases','Fatalities':'prev_Fatalities'},inplace=True)\n        \n        X_test['prev_ConfirmedCases']=np.log1p(X_test['prev_ConfirmedCases'])\n        X_test['prev_Fatalities']=np.log1p(X_test['prev_Fatalities'])\n        \n        X_test=X_test.merge(big_train[big_train['day_of_year']==day-5][['location','ConfirmedCases','Fatalities']], how='left', \n                 left_on=['location'],right_on=['location'])\n        X_test.rename(columns={'ConfirmedCases':'prev5_ConfirmedCases','Fatalities':'prev5_Fatalities'},inplace=True)\n        \n        X_test['prev_ConfirmedCases']=np.log1p(X_test['prev_ConfirmedCases'])\n        X_test['prev_Fatalities']=np.log1p(X_test['prev_Fatalities'])\n        \n    \n        X_test.set_index('Id',inplace=True)\n    \n    #print(X_test.head(5))\n    \n        y_test=reg.predict(X_test.drop(columns=['Province_State','Country_Region','location','Date','day_of_year','day_of_week']))\n    \n    #print(y_test)\n    \n        X_test['d_ConfirmedCases']=y_test\n    \n        y_test=reg_2.predict(X_test.drop(columns=['Province_State','Country_Region','location','Date',\n                                            'day_of_year','day_of_week']))\n    \n        X_test['d_Fatalities']=y_test\n    \n    #print(X_test.shape)\n    \n        X_test['Id']=X_test.index\n    \n        X_test=X_test.merge(big_train[big_train['day_of_year']==day-1][['location','ConfirmedCases','Fatalities']], how='left', \n                 left_on=['location'],right_on=['location'])\n    \n    #print(X_test.head(5))\n    \n    #X_test.set_index('Id',inplace=True)\n    \n    #print(X_test.shape)\n    \n        X_test.set_index('Id',inplace=True)\n    \n        print(X_test.head(5))\n        \n        X_test['d_ConfirmedCases']=np.expm1(X_test['d_ConfirmedCases'])\n        X_test['d_Fatalities']=np.expm1(X_test['d_Fatalities'])\n    \n        X_test['ConfirmedCases']+=X_test['d_ConfirmedCases']\n        X_test['Fatalities']+=X_test['d_Fatalities']\n       \n    \n    \n        results.append(X_test[['ConfirmedCases','Fatalities']])\n    \n        if not d in known: #Needed to correctly get data on neighbors         \n            big_train=pd.concat([big_train,X_test])\n    \n    \n    \n    \n    \n    \n    \n    \n        \n        \n        \n        ","f45bb8b5":"if Make_submission==True:\n    submission=pd.read_csv(\"..\/input\/covid19-global-forecasting-week-2\/submission.csv\")\n    submission.drop(columns=['ConfirmedCases','Fatalities'],inplace=True)\n    submission=submission.merge(pd.concat(results),left_on=['ForecastId'],right_index=True).clip(lower=0)\n    submission.to_csv('submission.csv',index=False)","562aedf8":"Grouping country names and provinces into variable - location","17add6e9":"For some countries we also have data for individual regions","af662b5c":"Locations with less than 5 cases:"}}