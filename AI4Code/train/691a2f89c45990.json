{"cell_type":{"19ece2b2":"code","7b63baee":"code","37df7b41":"code","d9e419f7":"code","269d756a":"code","6128d994":"code","f41d6369":"code","82c992c5":"code","95518b3a":"code","6330c8bd":"code","fc93699e":"code","838ead54":"code","a3c527cf":"code","0116289c":"code","df25fba3":"code","3c8bf89b":"code","38eccd70":"code","f355ddec":"code","5e553399":"code","f18159f6":"code","3b56e27c":"code","a1f016b9":"code","029fa2be":"markdown","10dfc72a":"markdown","23a71686":"markdown","60d6d26c":"markdown","571d93b0":"markdown","ef1a104a":"markdown","58b5ec50":"markdown","9326f465":"markdown","a56b392c":"markdown","4411024b":"markdown","def28bdc":"markdown","39edd187":"markdown","32094f80":"markdown","8f7088a9":"markdown","9fc93723":"markdown","6312d54c":"markdown","ae8d8d5f":"markdown","783bd470":"markdown","bb917ff5":"markdown","fe37b0ae":"markdown","3e5feb0d":"markdown","4f24810a":"markdown","239493e7":"markdown","77351aa9":"markdown","31b126d2":"markdown","58cc14d1":"markdown","44f283de":"markdown","dccf0f11":"markdown"},"source":{"19ece2b2":"from nltk.corpus import reuters\nimport nltk\nnltk.download('wordnet')\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom yellowbrick.text import TSNEVisualizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7b63baee":"train_documents, train_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('training\/')])\ntest_documents, test_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('test\/')])","37df7b41":"print(\"Number of training documents:\", len(train_documents))\nprint(\"Number of testing documents:\", len(test_documents))","d9e419f7":"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\ntrain_labels = mlb.fit_transform(train_categories)\ntest_labels = mlb.transform(test_categories)","269d756a":"trainData = {\"content\": train_documents}\ntestData = {\"content\": test_documents}\ntrainDf = pd.DataFrame(trainData, columns=[\"content\"])\ntestDf = pd.DataFrame(testData, columns=[\"content\"])","6128d994":"wordnet_lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\nstopwords = set(w.rstrip() for w in open(\"..\/input\/reuters\/reuters\/reuters\/stopwords\"))\n\ndef tokenize_lemma_stopwords(text):\n    text = text.replace(\"\\n\", \" \")\n    tokens = nltk.tokenize.word_tokenize(text.lower()) # split string into words (tokens)\n    tokens = [t for t in tokens if t.isalpha()] # keep strings with only alphabets\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n    tokens = [stemmer.stem(t) for t in tokens]\n    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n    cleanedText = \" \".join(tokens)\n    return cleanedText\n\ndef dataCleaning(df):\n    data = df.copy()\n    data[\"content\"] = data[\"content\"].apply(tokenize_lemma_stopwords)\n    return data","f41d6369":"cleanedTrainData = dataCleaning(trainDf)\ncleanedTestData = dataCleaning(testDf)","82c992c5":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\n\nvectorizer = TfidfVectorizer()\nvectorised_train_documents = vectorizer.fit_transform(cleanedTrainData[\"content\"])\nvectorised_test_documents = vectorizer.transform(cleanedTestData[\"content\"])","95518b3a":"from yellowbrick.text import FreqDistVisualizer\nfeatures = vectorizer.get_feature_names()\nvisualizer = FreqDistVisualizer(features=features, orient='v')\nvisualizer.fit(vectorised_train_documents)\nvisualizer.show()","6330c8bd":"tsne = TSNEVisualizer()\ntsne.fit(vectorised_train_documents)\ntsne.show()","fc93699e":"from yellowbrick.text import UMAPVisualizer\nfrom sklearn.cluster import KMeans\n\numap = UMAPVisualizer(metric=\"cosine\")\numap.fit(vectorised_train_documents)\numap.show()","838ead54":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n\nModelsPerformance = {}\n\ndef metricsReport(modelName, test_labels, predictions):\n    accuracy = accuracy_score(test_labels, predictions)\n\n    macro_precision = precision_score(test_labels, predictions, average='macro')\n    macro_recall = recall_score(test_labels, predictions, average='macro')\n    macro_f1 = f1_score(test_labels, predictions, average='macro')\n\n    micro_precision = precision_score(test_labels, predictions, average='micro')\n    micro_recall = recall_score(test_labels, predictions, average='micro')\n    micro_f1 = f1_score(test_labels, predictions, average='micro')\n    hamLoss = hamming_loss(test_labels, predictions)\n    print(\"------\" + modelName + \" Model Metrics-----\")\n    print(\"Accuracy: {:.4f}\\nHamming Loss: {:.4f}\\nPrecision:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nRecall:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nF1-measure:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\"\\\n          .format(accuracy, hamLoss, macro_precision, micro_precision, macro_recall, micro_recall, macro_f1, micro_f1))\n    ModelsPerformance[modelName] = micro_f1\n","a3c527cf":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nknnClf = KNeighborsClassifier()\n\nknnClf.fit(vectorised_train_documents, train_labels)\nknnPredictions = knnClf.predict(vectorised_test_documents)\nmetricsReport(\"knn\", test_labels, knnPredictions)","0116289c":"from sklearn.tree import DecisionTreeClassifier\n\ndtClassifier = DecisionTreeClassifier()\ndtClassifier.fit(vectorised_train_documents, train_labels)\ndtPreds = dtClassifier.predict(vectorised_test_documents)\nmetricsReport(\"Decision Tree\", test_labels, dtPreds)","df25fba3":"from sklearn.ensemble import BaggingClassifier\n\nbagClassifier = OneVsRestClassifier(BaggingClassifier(n_jobs=-1))\nbagClassifier.fit(vectorised_train_documents, train_labels)\nbagPreds = bagClassifier.predict(vectorised_test_documents)\nmetricsReport(\"Bagging\", test_labels, bagPreds)","3c8bf89b":"from sklearn.ensemble import RandomForestClassifier\nrfClassifier = RandomForestClassifier(n_jobs=-1)\nrfClassifier.fit(vectorised_train_documents, train_labels)\nrfPreds = rfClassifier.predict(vectorised_test_documents)\nmetricsReport(\"Random Forest\", test_labels, rfPreds)\n","38eccd70":"from sklearn.ensemble import GradientBoostingClassifier\n\nboostClassifier = OneVsRestClassifier(GradientBoostingClassifier())\nboostClassifier.fit(vectorised_train_documents, train_labels)\nboostPreds = boostClassifier.predict(vectorised_test_documents)\nmetricsReport(\"Boosting\", test_labels, boostPreds)\n","f355ddec":"from sklearn.naive_bayes import MultinomialNB\n\nnbClassifier = OneVsRestClassifier(MultinomialNB())\nnbClassifier.fit(vectorised_train_documents, train_labels)\n\nnbPreds = nbClassifier.predict(vectorised_test_documents)\nmetricsReport(\"Multinomial NB\", test_labels, nbPreds)","5e553399":"from sklearn.svm import LinearSVC\n\nsvmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\nsvmClassifier.fit(vectorised_train_documents, train_labels)\n\nsvmPreds = svmClassifier.predict(vectorised_test_documents)\nmetricsReport(\"SVC Sq. Hinge Loss\", test_labels, svmPreds)","f18159f6":"from skmultilearn.problem_transform import LabelPowerset\n\npowerSetSVC = LabelPowerset(LinearSVC())\npowerSetSVC.fit(vectorised_train_documents, train_labels)\n\npowerSetSVCPreds = powerSetSVC.predict(vectorised_test_documents)\nmetricsReport(\"Power Set SVC\", test_labels, powerSetSVCPreds)","3b56e27c":"print(\"  Model Name \" + \" \"*10 + \"| Micro-F1 Score\")\nprint(\"-------------------------------------------\")\nfor key, value in ModelsPerformance.items():\n    print(\"  \" + key, \" \"*(20-len(key)) + \"|\", value)\n    print(\"-------------------------------------------\")","a1f016b9":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_labels, svmPreds))","029fa2be":"* MultinomialNB performance is poor. There may be correlation between features. MultinomialNB is highly sensitive to that","10dfc72a":"# Create TF-IDF Vectorizer","23a71686":"# Pre-process the input data","60d6d26c":"* It seems pretty weak on Multi-label problem","571d93b0":"## Creating Dataframe for applying transformations","ef1a104a":"# Visualize the dataset with T-SNE","58b5ec50":"## Lets's look at the classification report for LinearSVC Classifier(best so far)","9326f465":"## Decision Tree Classifier\nTree-based methods are simple and useful for interpretation.\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.","a56b392c":"* As you can see here Bagging Classifier gives improved performance over Decision tree in terms of both Macro and Micro averages","4411024b":"* Linear SVC with Squarred hinge loss seems to outperfom all the previously seen models\n* SVCs are much more robust against Correlated features","def28bdc":"* It seems to perform well on overall documents but seems to fail on few classes","39edd187":"## KNN Classifier\nk-nearest neighbors algorithm (kNN) is a non-parametric technique used for classification.\n\nGiven a test document x, the KNN algorithm finds the k nearest neighbors of x among all the\ndocuments in the training set, and scores the category candidates based the class of k neighbors.\nThe similarity of x and each neighbor\u2019s document could be the score of the category of the neighbor\ndocuments. Multiple KNN documents may belong to the same category; in this case, the summation of\nthese scores would be the similarity score of class k with respect to the test document x. After sorting\nthe score values, the algorithm assigns the candidate to the class with the highest score from the test\ndocument x","32094f80":"# Train and Evaluate Classifiers","8f7088a9":"## Import required packages","9fc93723":"* It performs as good as Bagging Classifier","6312d54c":"## Naive Bayes Classifier\n\nNaive Bayes Classifier (NBC) is generative model which is widely used in Information Retrieval.\nThe multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.","ae8d8d5f":"# Comparison on different models based on their Micro-F1 score","783bd470":"# Visualize Word Frequency Distribution","bb917ff5":"# Visualizaing corpus with Uniform Manifold Approximation and Projection (UMAP)","fe37b0ae":"## Boosting\nRecall that bagging involves creating multiple copies of the original train-\ning data set using the bootstrap, fitting a separate decision tree to each\ncopy, and then combining all of the trees in order to create a single predic-\ntive model. Notably, each tree is built on a bootstrap data set, independent\nof the other trees. Boosting works in a similar way, except that the trees are\ngrown sequentially: each tree is grown using information from previously\ngrown trees. Boosting does not involve bootstrap sampling; instead each\ntree is fit on a modified version of the original data set.","3e5feb0d":"* This shows high similarity of the documents globally","4f24810a":"* It give average performance on documents but fails miserably on Macro average measure\n* Reason for failure in some labels may be due to correlation between features","239493e7":"## Bagging\n\n* The decision trees suffer from high variance.\n* This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different.\n* Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.","77351aa9":"## Random Forrest\nRandom forests provide an improvement over bagged trees by way of a\nsmall tweak that decorrelates the trees. As in bagging, we build a number\nof decision trees on bootstrapped training samples. But when building these\ndecision trees, each time a split in a tree is considered, a random sample of\nm predictors is chosen as split candidates from the full set of p predictors.\nThe split is allowed to use only one of those m predictors. A fresh sample of \u221am predictors is taken at each split, and typically we choose m \u2248 p\u2014that\nis, the number of predictors considered at each split is approximately equal\nto the square root of the total number of predictors\n\nIn other words, in building a random forest, at each split in the tree,\nthe algorithm is not even allowed to consider a majority of the available\npredictors.","31b126d2":"# Convert the categorical labels to Multi Label Encodings","58cc14d1":"## Support Vector Machine (Linear SVC)\n* Support Vector Machine (SVM), an approach for classification that was developed in the computer science community in\nthe 1990s and that has grown in popularity since then. \n* SVMs have been shown to perform well in a variety of settings, and are often considered one of the best \u201cout of the box\u201d classifiers.\n* The support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier","44f283de":"## Getting train and test dataset from nltk reuters corpus","dccf0f11":"## Things we have done so far:\n* Here We have compared various Machine learning based classifiers for Multi-label text classification with TF-IDF feature extraction\n* We also have used inherent multi-label Classifiers like Random-Forest, Decision Tree, Knn and also used various methods like OneVsRest and Label Power Set\n\n## Next Steps:\n* Use Word Embeddings\n* Deep learning based approaches (RNNs, CNNs, Transformers)"}}