{"cell_type":{"8395222b":"code","e025c749":"code","1274cec5":"code","58cda9a9":"code","b1cfb553":"code","06cd16bd":"code","9f122a68":"code","1e0a7e29":"code","fff9c492":"code","bdf11cc7":"code","b32c5779":"code","919bcf3e":"code","e8e3a090":"code","d95c8904":"code","2eadb9b9":"code","98c1a8f1":"code","315a0fc4":"code","e00bf91a":"code","9b62edc0":"code","98b34dcb":"code","43ed8dac":"code","6465ba09":"code","6ecc992e":"code","2b3d4d4b":"code","da96a18f":"code","39190495":"code","8afa4f5b":"code","6e07102d":"code","d478ee33":"code","3c1b6447":"code","e001ae47":"code","4975238d":"code","57c7e69c":"code","95a6bf48":"code","efecc197":"code","f571adb1":"code","aed29ad6":"code","f5757d6f":"code","8791abef":"code","edac382f":"code","d656eec9":"code","479f2bed":"code","371fb426":"code","bbb0d9d6":"code","2dbe8c62":"code","d79ecdac":"code","cccf287d":"code","a0b2dce5":"code","c9d7f461":"code","422ff722":"code","36cdb006":"code","47bc0da6":"code","8ff7fc6b":"code","520f0250":"code","9aefff67":"code","2e207900":"code","47784f7b":"code","0e6b5fd4":"code","268efca3":"code","6d0ecc03":"code","992614c5":"code","751fa849":"code","84e6821b":"code","1234a7e3":"code","2afa6e11":"code","896f8c6a":"code","4c3afd4c":"code","370ed143":"code","ea226a67":"code","ce7c9f1a":"code","b2d460ba":"code","b6b74c60":"code","bb07efa6":"code","adb5eae3":"code","45678c20":"code","f407a5d3":"code","bedc2906":"code","a6bb36fc":"code","d2cec360":"code","6598d25f":"code","9949b714":"code","580a3776":"code","367537f3":"code","d03f6fe1":"code","598c1284":"code","3cae2aae":"code","ec2ede20":"code","546bd9db":"code","dda069f0":"code","c92786a9":"code","2af4390b":"markdown","0c5ec084":"markdown","a67bec78":"markdown","16b5bc03":"markdown","b0a09e28":"markdown","f212881e":"markdown","8652c333":"markdown","f6ad777e":"markdown","7dd5de26":"markdown","24a47d88":"markdown","c6ae29e8":"markdown","24c0025f":"markdown","61ae2112":"markdown","fc44d212":"markdown","c7d69bce":"markdown","c5c07c9f":"markdown","6741ff3a":"markdown","3168bd2c":"markdown","2ab83b4e":"markdown","6d9458d5":"markdown","27e5d3ab":"markdown","2fd04b43":"markdown","108b2cae":"markdown","2d1d84b6":"markdown"},"source":{"8395222b":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nfrom IPython.display import HTML\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text \nfrom sklearn.decomposition import PCA\n\nfrom tensorflow.python.keras.models import Sequential, load_model\nfrom tensorflow.python.keras.layers import Dense, Dropout\nfrom tensorflow.python.keras import optimizers\n\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import words\nfrom nltk.corpus import wordnet \nallEnglishWords = words.words() + [w for w in wordnet.words()]\nallEnglishWords = np.unique([x.lower() for x in allEnglishWords])\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","e025c749":"path = \"\/kaggle\/input\/aclimdb\/aclImdb\/\"\npositiveFiles = [x for x in os.listdir(path+\"train\/pos\/\") if x.endswith(\".txt\")]\nnegativeFiles = [x for x in os.listdir(path+\"train\/neg\/\") if x.endswith(\".txt\")]\ntestFiles = [x for x in os.listdir(path+\"test\/\") if x.endswith(\".txt\")]","1274cec5":"positiveReviews, negativeReviews, testReviews = [], [], []\nfor pfile in positiveFiles:\n    with open(path+\"train\/pos\/\"+pfile, encoding=\"latin1\") as f:\n        positiveReviews.append(f.read())\nfor nfile in negativeFiles:\n    with open(path+\"train\/neg\/\"+nfile, encoding=\"latin1\") as f:\n        negativeReviews.append(f.read())\nfor tfile in testFiles:\n    with open(path+\"test\/\"+tfile, encoding=\"latin1\") as f:\n        testReviews.append(f.read())","58cda9a9":"reviews = pd.concat([\n    pd.DataFrame({\"review\":positiveReviews, \"label\":1, \"file\":positiveFiles}),\n    pd.DataFrame({\"review\":negativeReviews, \"label\":0, \"file\":negativeFiles}),\n    pd.DataFrame({\"review\":testReviews, \"label\":-1, \"file\":testFiles})\n], ignore_index=True).sample(frac=1, random_state=1)\nreviews.head()","b1cfb553":"reviews.tail()","06cd16bd":"reviews = reviews[[\"review\", \"label\", \"file\"]].sample(frac=1, random_state=1)\ntrain = reviews[reviews.label!=-1].sample(frac=0.6, random_state=1)\nvalid = reviews[reviews.label!=-1].drop(train.index)\ntest = reviews[reviews.label==-1]","9f122a68":"print(train.shape)\nprint(valid.shape)\nprint(test.shape)","1e0a7e29":"HTML(train.review.iloc[0])","fff9c492":"class Preprocessor(object):\n    ''' Preprocess data for NLP tasks. '''\n\n    def __init__(self, alpha=True, lower=True, stemmer=True, english=False):\n        self.alpha = alpha\n        self.lower = lower\n        self.stemmer = stemmer\n        self.english = english\n        \n        self.uniqueWords = None\n        self.uniqueStems = None\n        \n    def fit(self, texts):\n        texts = self._doAlways(texts)\n\n        allwords = pd.DataFrame({\"word\": np.concatenate(texts.apply(lambda x: x.split()).values)})\n        self.uniqueWords = allwords.groupby([\"word\"]).size().rename(\"count\").reset_index()\n        self.uniqueWords = self.uniqueWords[self.uniqueWords[\"count\"]>1]\n        if self.stemmer:\n            self.uniqueWords[\"stem\"] = self.uniqueWords.word.apply(lambda x: PorterStemmer().stem(x)).values\n            self.uniqueWords.sort_values([\"stem\", \"count\"], inplace=True, ascending=False)\n            self.uniqueStems = self.uniqueWords.groupby(\"stem\").first()\n        \n        #if self.english: self.words[\"english\"] = np.in1d(self.words[\"mode\"], allEnglishWords)\n        print(\"Fitted.\")\n            \n    def transform(self, texts):\n        texts = self._doAlways(texts)\n        if self.stemmer:\n            allwords = np.concatenate(texts.apply(lambda x: x.split()).values)\n            uniqueWords = pd.DataFrame(index=np.unique(allwords))\n            uniqueWords[\"stem\"] = pd.Series(uniqueWords.index).apply(lambda x: PorterStemmer().stem(x)).values\n            uniqueWords[\"mode\"] = uniqueWords.stem.apply(lambda x: self.uniqueStems.loc[x, \"word\"] if x in self.uniqueStems.index else \"\")\n            texts = texts.apply(lambda x: \" \".join([uniqueWords.loc[y, \"mode\"] for y in x.split()]))\n        #if self.english: texts = self.words.apply(lambda x: \" \".join([y for y in x.split() if self.words.loc[y,\"english\"]]))\n        print(\"Transformed.\")\n        return(texts)\n\n    def fit_transform(self, texts):\n        texts = self._doAlways(texts)\n        self.fit(texts)\n        texts = self.transform(texts)\n        return(texts)\n    \n    def _doAlways(self, texts):\n        # Remove parts between <>'s\n        texts = texts.apply(lambda x: re.sub('<.*?>', ' ', x))\n        # Keep letters and digits only.\n        if self.alpha: texts = texts.apply(lambda x: re.sub('[^a-zA-Z0-9 ]+', ' ', x))\n        # Set everything to lower case\n        if self.lower: texts = texts.apply(lambda x: x.lower())\n        return texts  ","bdf11cc7":"train.head()","b32c5779":"train.shape\ntrain.info()","919bcf3e":"preprocess = Preprocessor(alpha=True, lower=True, stemmer=True)","e8e3a090":"%%time\ntrainX = preprocess.fit_transform(train.review)\nvalidX = preprocess.transform(valid.review)","d95c8904":"trainX.head()","2eadb9b9":"trainX.shape","98c1a8f1":"trainX2 = pd.concat([trainX, train[['label', 'file']]], axis = 1, sort = False) ","315a0fc4":"trainX2.head()","e00bf91a":"print(preprocess.uniqueWords.shape)\npreprocess.uniqueWords[preprocess.uniqueWords.word.str.contains(\"disappoint\")]","9b62edc0":"print(preprocess.uniqueStems.shape)\npreprocess.uniqueStems[preprocess.uniqueStems.word.str.contains(\"disappoint\")]","98b34dcb":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","43ed8dac":"text = \" \".join(review for review in trainX2['review'])\nprint (\"There are {} words in the combination of all review.\".format(len(text)))","6465ba09":"stopwords = set(STOPWORDS)\nstopwords.update([\"Nan\",\"Negative\",\"etc\", \"br\", 'movie', 'film', 'one', 'make', 'even'])","6ecc992e":"wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=100).generate(text)","2b3d4d4b":"import matplotlib.pyplot as plt\n#% matplotlib inline","da96a18f":"plt.figure(figsize=(10,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","39190495":"positive = trainX2[trainX2['label'] == 1]\npositive.head()","8afa4f5b":"positive.shape[0]\nprint('There are ' + str(positive.shape[0]) + ' positive reviews')","6e07102d":"text = \" \".join(review for review in positive['review'])\nprint (\"There are {} words in the combination of all review.\".format(len(text)))","d478ee33":"stopwords.update([\"Nan\",\"Negative\",\"etc\", \"br\", 'film', 'movie', 'one'])\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=100).generate(text)\nplt.figure(figsize=(10,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","3c1b6447":"negative = trainX2[trainX2['label'] == 0]\nnegative.head()","e001ae47":"print('There are ' + str(negative.shape[0]) + ' negative reviews')","4975238d":"text = \" \".join(review for review in negative['review'])\nprint (\"There are {} words in the combination of all review.\".format(len(text)))","57c7e69c":"stopwords = set(STOPWORDS)\nstopwords.update([\"Nan\",\"Negative\",\"etc\", \"br\", 'film', 'movie', 'one', 'even'])\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=100).generate(text)\nplt.figure(figsize=(10,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","95a6bf48":"all_words = trainX2['review'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the training dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","efecc197":"first_text = negative.review.values[1]\nprint(first_text)\nprint(\"=\"*90)\nprint(first_text.split(\" \"))","f571adb1":"first_text_list = nltk.word_tokenize(first_text)\nprint(first_text_list)","aed29ad6":"stopwords = nltk.corpus.stopwords.words('english')\nlen(stopwords)","f5757d6f":"first_text_list_cleaned = [word for word in first_text_list if word.lower() not in stopwords]\nprint(first_text_list_cleaned)\nprint(\"=\"*90)\nprint(\"Length of original list: {0} words\\n\"\n      \"Length of list after stopwords removal: {1} words\"\n      .format(len(first_text_list), len(first_text_list_cleaned)))","8791abef":"stemmer = nltk.stem.PorterStemmer()","edac382f":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))","d656eec9":"from sklearn.feature_extraction.text import CountVectorizer","479f2bed":"def print_top_words(model, feature_names, n_top_words):\n    for index, topic in enumerate(model.components_):\n        message = \"\\nTopic #{}:\".format(index)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n        print(message)\n        print(\"=\"*70)","371fb426":"lemm = WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))","bbb0d9d6":"# Storing the entire training text in a list\ntext = list(negative.review.values)\n# Calling our overwritten Count vectorizer\ntf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\ntf = tf_vectorizer.fit_transform(text)","2dbe8c62":"from sklearn.decomposition import LatentDirichletAllocation","d79ecdac":"lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)","cccf287d":"lda.fit(tf)","a0b2dce5":"n_top_words = 40\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","c9d7f461":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]\nfourth_topic = lda.components_[3]\nfifth_topic = lda.components_[4]\nseventh_topic = lda.components_[6]","422ff722":"first_topic.shape","36cdb006":"first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\nsecond_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\nthird_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\nfourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]\nfifth_topic_words = [tf_feature_names[i] for i in fifth_topic.argsort()[:-50 - 1 :-1]]\nseventh_topic_words = [tf_feature_names[i] for i in seventh_topic.argsort()[:-50 - 1 :-1]]","47bc0da6":"stopwords = set(STOPWORDS)\nstopwords.update([\"Nan\",\"Negative\",\"etc\", \"br\", 'film', 'movie', 'one', 'johnny', 'cage', 'good'])\nfirstcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=100).generate(\" \".join(seventh_topic_words))\nplt.figure(figsize=(10,8))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","8ff7fc6b":"# Storing the entire training text in a list\ntext = list(positive.review.values)\n# Calling our overwritten Count vectorizer\ntf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\ntf = tf_vectorizer.fit_transform(text)","520f0250":"lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)","9aefff67":"lda.fit(tf)","2e207900":"n_top_words = 40\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","47784f7b":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]\nfourth_topic = lda.components_[3]\nfifth_topic = lda.components_[4]\ntenth_topic = lda.components_[9]","0e6b5fd4":"first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\nsecond_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\nthird_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\nfourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]\nfifth_topic_words = [tf_feature_names[i] for i in fifth_topic.argsort()[:-50 - 1 :-1]]\ntenth_topic_words = [tf_feature_names[i] for i in tenth_topic.argsort()[:-50 - 1 :-1]]","268efca3":"stopwords = set(STOPWORDS)\nstopwords.update([\"Nan\",\"Negative\",\"etc\", \"br\", 'film', 'movie', 'one'])\nfirstcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=100).generate(\" \".join(tenth_topic_words))\nplt.figure(figsize=(10,8))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","6d0ecc03":"tfidf = TfidfVectorizer(min_df=2, max_features=10000, stop_words=stopwords) #, ngram_range=(1,3)","992614c5":"%%time\ntrainX = tfidf.fit_transform(trainX).toarray()\nvalidX = tfidf.transform(validX).toarray()","751fa849":"print(trainX.shape)\nprint(validX.shape)","84e6821b":"trainY = train.label\nvalidY = valid.label","1234a7e3":"print(trainX.shape, trainY.shape)\nprint(validX.shape, validY.shape)","2afa6e11":"from scipy.stats.stats import pearsonr","896f8c6a":"getCorrelation = np.vectorize(lambda x: pearsonr(trainX[:,x], trainY)[0])\ncorrelations = getCorrelation(np.arange(trainX.shape[1]))\nprint(correlations)","4c3afd4c":"allIndeces = np.argsort(-correlations)\nbestIndeces = allIndeces[np.concatenate([np.arange(1000), np.arange(-1000, 0)])]","370ed143":"vocabulary = np.array(tfidf.get_feature_names())\nprint(vocabulary[bestIndeces][:15])\nprint(vocabulary[bestIndeces][-15:])","ea226a67":"trainX = trainX[:,bestIndeces]\nvalidX = validX[:,bestIndeces]","ce7c9f1a":"print(trainX.shape, trainY.shape)\nprint(validX.shape, validY.shape)","b2d460ba":"DROPOUT = 0.5\nACTIVATION = \"tanh\"\n\nmodel = Sequential([    \n    Dense(int(trainX.shape[1]\/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n    Dropout(DROPOUT),\n    Dense(int(trainX.shape[1]\/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n    Dropout(DROPOUT),\n    Dense(int(trainX.shape[1]\/4), activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(100, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(20, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(5, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(1, activation='sigmoid'),\n])","b6b74c60":"model.compile(optimizer=optimizers.Adam(0.00005), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","bb07efa6":"EPOCHS = 30\nBATCHSIZE = 1500","adb5eae3":"model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCHSIZE, validation_data=(validX, validY))","45678c20":"x = np.arange(EPOCHS)\nhistory = model.history.history\n\ndata = [\n    go.Scatter(x=x, y=history[\"acc\"], name=\"Train Accuracy\", marker=dict(size=5), yaxis='y2'),\n    go.Scatter(x=x, y=history[\"val_acc\"], name=\"Valid Accuracy\", marker=dict(size=5), yaxis='y2'),\n    go.Scatter(x=x, y=history[\"loss\"], name=\"Train Loss\", marker=dict(size=5)),\n    go.Scatter(x=x, y=history[\"val_loss\"], name=\"Valid Loss\", marker=dict(size=5))\n]\nlayout = go.Layout(\n    title=\"Model Training Evolution\", font=dict(family='Palatino'), xaxis=dict(title='Epoch', dtick=1),\n    yaxis1=dict(title=\"Loss\", domain=[0, 0.45]), yaxis2=dict(title=\"Accuracy\", domain=[0.55, 1]),\n)\npy.iplot(go.Figure(data=data, layout=layout), show_link=False)","f407a5d3":"train[\"probability\"] = model.predict(trainX)\ntrain[\"prediction\"] = train.probability-0.5>0\ntrain[\"truth\"] = train.label==1\ntrain.tail()","bedc2906":"print(model.evaluate(trainX, trainY))\nprint((train.truth==train.prediction).mean())","a6bb36fc":"valid[\"probability\"] = model.predict(validX)\nvalid[\"prediction\"] = valid.probability-0.5>0\nvalid[\"truth\"] = valid.label==1\nvalid.tail()","d2cec360":"print(model.evaluate(validX, validY))\nprint((valid.truth==valid.prediction).mean())","6598d25f":"trainCross = train.groupby([\"prediction\", \"truth\"]).size().unstack()\ntrainCross","9949b714":"validCross = valid.groupby([\"prediction\", \"truth\"]).size().unstack()\nvalidCross","580a3776":"truepositives = valid[(valid.truth==True)&(valid.truth==valid.prediction)]\nprint(len(truepositives), \"true positives.\")\ntruepositives.sort_values(\"probability\", ascending=False).head(3)","367537f3":"truepositives.review.iloc[1]","d03f6fe1":"truenegatives = valid[(valid.truth==False)&(valid.truth==valid.prediction)]\nprint(len(truenegatives), \"true negatives.\")\ntruenegatives.sort_values(\"probability\", ascending=True).head(3)","598c1284":"truenegatives.review.iloc[1]","3cae2aae":"falsepositives = valid[(valid.truth==True)&(valid.truth!=valid.prediction)]\nprint(len(falsepositives), \"false positives.\")\nfalsepositives.sort_values(\"probability\", ascending=True).head(3)","ec2ede20":"falsepositives.review.iloc[1]","546bd9db":"falsenegatives = valid[(valid.truth==False)&(valid.truth!=valid.prediction)]\nprint(len(falsenegatives), \"false negatives.\")\nfalsenegatives.sort_values(\"probability\", ascending=False).head(3)","dda069f0":"falsenegatives.review.iloc[1]","c92786a9":"HTML(valid.loc[22148].review)","2af4390b":"## Cleaning the dataset of all non english words","0c5ec084":"# Part 1 - Importing the dataset\nFirst, we need to import the data.\nWe create a new pandas dataframe with all the reviews as seperate rows.","a67bec78":"### Creating confusion matrices to check the accuracy of our model on the validation set","16b5bc03":"In this notebook we are going to analyze reviews or the most prominent topics.\nThe reviews have positive and negative topics and we will try to identify which reviews are positive and negative.","b0a09e28":"## Topic modelling and word cloud for the positive reviews","f212881e":"## With this we conclude the Topic Modelling and Sentiment Analysis of the Movie Review dataset.","8652c333":"# Part 5 - Sentiment Analysis\nIn the next part we will analyze the sentiment of the reviews.","f6ad777e":"## Word Cloud for negative reviews","7dd5de26":"# Sentiment Analysis on Movie Reviews","24a47d88":"# Part 4 - Topic detection","c6ae29e8":"# Part 3 - Creating different wordclouds\nCreating a word cloud for all words","24c0025f":"Selecting 90 words from the first review.","61ae2112":"## Topic modelling and word cloud for the negative reviews","fc44d212":"## Part 5a - Feature Selection\nNext, we take the 10k dimensional tfidf's as input, and keep the 2000 dimensions that correlate the most with our sentiment target. The corresponding words - see below - make sense.","c7d69bce":"## Word Cloud for positive reviews","c5c07c9f":"This is the review that got predicted as positive most certainly - while being labeled as negative. However, we can easily recognize it as a poorly labeled sample.","6741ff3a":"# Part 2 - Data Preprocessing\nThe next step is data preprocessing. The following class behaves like your typical SKLearn vectorizer.\n\nIt can perform the following operations.\n* Discard non alpha-numeric characters\n* Set everything to lower case\n* Stems all words using PorterStemmer, and change the stems back to the most occurring existent word.\n* Discard non-Egnlish words (not by default).","3168bd2c":"With everything centralized in 1 dataframe, we now perform train, validation and test set splits.","2ab83b4e":"## There are 5 parts in this notebook\n## Part 1 - Importing the dataset\n## Part 2 - Cleaning and preprocessing the dataset\n## Part 3 - Creating different wordclouds\n## Part 4 - Topic detection\n## Part 5 - Sentiment Analysis","6d9458d5":"## Most frequently used words in the trainX set","27e5d3ab":"## Part 5b - Defining Model Architecture\nWe choose a very simple dense network with 6 layers, performing binary classification.","2fd04b43":"## Appending other columns to the cleaned dataset","108b2cae":"## Part 5c - Model Training\nTraining the model using the TrainX dataset.","2d1d84b6":"## Part 5d -  Model Evaluation\nAdding the probabilities and predictions columns to the original train and validation dataframes."}}