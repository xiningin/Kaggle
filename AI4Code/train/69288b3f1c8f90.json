{"cell_type":{"6caa43f5":"code","0570ac9a":"code","b8ebf30d":"code","f768154a":"code","fd6c0a5a":"code","4514484b":"markdown"},"source":{"6caa43f5":"from sklearn import cluster, datasets, mixture\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport os.path\n\nimport warnings\nwarnings.filterwarnings('ignore')","0570ac9a":"# install DenMune clustering algorithm using pip command from the offecial Python repository, PyPi\n# from https:\/\/pypi.org\/project\/denmune\/\n!pip install denmune\n\n# then import it\nfrom denmune import DenMune","b8ebf30d":"# Denmune's Paramaters\nknn = 25 # k-nearest neighbor, the only parameter required by the algorithm\ndata_scale = []\n\nfor n in range(1000, 100000, 1000):\n    n_samples = n\n    noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n    \n    data= noisy_circles[0]\n    data_labels = noisy_circles[1]\n    dm = DenMune(train_data=data, k_nearest=knn, rgn_tsne=True)\n    labels, validity = dm.fit_predict(show_noise=True, show_analyzer=False, show_plots=False)\n    time_exec = dm.analyzer['exec_time']['DenMune']\n    data_scale.append([n, time_exec ])\n\n    print('data size:',n ,  'time:' , round(time_exec,4), 'seconds')","f768154a":"# creating moving average to smoth the curve\nx, y = zip(*data_scale)\nwindow = 5\ncumsum, moving_aves = [0], []\n\nfor i, n in enumerate(y, 1):\n    cumsum.append(cumsum[i-1] + n)\n    if i>=window:\n        moving_ave = (cumsum[i] - cumsum[i-window])\/window\n        #can do stuff with moving_ave here\n        moving_aves.append(moving_ave)\ny = moving_aves        ","fd6c0a5a":"# Creating figure and axis objects using subplots()\nfig, ax = plt.subplots(figsize=[9, 7])\nax.plot(x[:-window+1], y, marker='.', linewidth=2, label='DenMune Scalability')\nplt.xticks(rotation=60)\nax.set_xlabel('Dataset size')\nax.set_ylabel('Time in seconds')\nplt.legend()\nplt.show()","4514484b":"![measuring scalabitiy](https:\/\/raw.githubusercontent.com\/egy1st\/images\/main\/clustering\/scalability_1.jpg)\n\nwe are going to measure the scalability of a simple algorithm on working a sunthestic datasets of 1000000 samples with an increment of 1000 for each cycle"}}