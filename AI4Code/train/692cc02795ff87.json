{"cell_type":{"cb2304d7":"code","ffee6c31":"code","172f1f3a":"code","3c380a45":"code","51a86670":"code","e746dbbd":"code","e01400a5":"code","a5c42b70":"code","5723654e":"code","9f2ccf77":"code","da530afb":"code","d3649f2f":"code","f0c945db":"code","2355d86e":"code","f84ce5c5":"code","b1d4daeb":"code","c9ec7b79":"code","cc3ee8d4":"code","48abb8e3":"markdown","aba6dc4c":"markdown","29438fd4":"markdown","545c274e":"markdown"},"source":{"cb2304d7":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ffee6c31":"raw = pd.read_csv('..\/input\/news-summary\/news_summary_more.csv', encoding='iso-8859-1')","172f1f3a":"text = raw.iloc[0,1]\ntext","3c380a45":"import torch\nimport json \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config","51a86670":"model = T5ForConditionalGeneration.from_pretrained('t5-small')\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\ndevice = torch.device('cpu')","e746dbbd":"preprocess_text = text.strip().replace(\"\\n\",\"\")\nt5_prepared_Text = \"summarize: \"+preprocess_text\nprint (\"original text preprocessed: \\n\", preprocess_text)","e01400a5":"tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)","a5c42b70":"summary_ids = model.generate(tokenized_text,\n                                    num_beams=14,\n                                    no_repeat_ngram_size=2,\n                                    min_length=30,\n                                    max_length=100,\n                                    early_stopping=True)","5723654e":"output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)","9f2ccf77":"print (\"Summarized text: \\n\",output)","da530afb":"summary = pd.read_csv('..\/input\/news-summary\/news_summary.csv', encoding='iso-8859-1')\nsummary.head()","d3649f2f":"text = summary.iloc[0,5]\ntext","f0c945db":"preprocess_text = text\nt5_prepared_Text = \"summarize: \"+preprocess_text\nprint (\"original text preprocessed: \\n\", preprocess_text)\nprint (\"original length: \\n\", len(preprocess_text))","2355d86e":"tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)","f84ce5c5":"summary_ids = model.generate(tokenized_text,\n                                    num_beams=10,\n                                    no_repeat_ngram_size=3,\n                                    min_length=100,\n                                    max_length=250,\n                                    early_stopping=True)","b1d4daeb":"output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)","c9ec7b79":"output","cc3ee8d4":"print (\"Summary length: \\n\", len(output))","48abb8e3":"**Preprocessing the text**","aba6dc4c":"**Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability.**\n\nTo know more about it visit this link - https:\/\/huggingface.co\/blog\/how-to-generate","29438fd4":"**Summary of the above text**","545c274e":"**Generating the model**"}}