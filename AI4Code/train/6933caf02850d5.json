{"cell_type":{"aa414ac6":"code","201ba0fc":"code","c878e01b":"code","b6030514":"code","c2b6c5f5":"code","ca871882":"code","8420ec99":"code","92f7d7db":"code","a749eec9":"code","12b21fc3":"code","f57bed71":"code","1df2628a":"code","dcf2d6ba":"code","d3373e2f":"code","d26d9c99":"code","48c84f74":"code","cc843351":"code","f76be1c9":"code","c2356ce3":"code","175b2d9d":"code","4bd02dd8":"code","f9683b5a":"code","bc792118":"code","553c5516":"code","1faa9b00":"code","d7d29cc2":"code","92ef36b6":"code","11e26e7d":"code","757aaebf":"code","70ffd9cb":"code","29c3a6e8":"code","7ee4ccc7":"code","a72356c1":"code","2f76316e":"code","f0d3ccc5":"code","e24ff4cf":"code","e6fc6850":"code","f50310a4":"code","725b43e3":"markdown","27c969b5":"markdown","683336bd":"markdown","fcd60f8e":"markdown","199ae432":"markdown","28de7418":"markdown","d3e05306":"markdown","ff8432ad":"markdown","b6ff1557":"markdown","43332c51":"markdown","b6d2b204":"markdown","f212ddac":"markdown","59e7bf08":"markdown","4f944229":"markdown","db2ca162":"markdown","d4f8e712":"markdown","ad452a2c":"markdown","b1a8d57a":"markdown","fd0b89c1":"markdown"},"source":{"aa414ac6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport time\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport matplotlib.pyplot as plt\n\nimport matplotlib.cm as cm\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nfrom covid19_functions import *","201ba0fc":"corona_data = pd.read_csv(\"..\/input\/kagglecovid19\/kaggle_covid-19.csv\")\ncorona_data = corona_data.drop(columns=['abstract'])\ncorona_data = corona_data.fillna(\"Unknown\")\ncorona_data['risk_label'] = 'Unlabelled'","c878e01b":"coronas_d2v_model = Doc2Vec.load(\"..\/input\/covidvectors\/COVID_MEDICAL_DOCS_w2v_MODEL.model\")","b6030514":"doc_folder = {\"risk\": return_doc_index(\"risk\", corona_data),\n\n              \"preg\": return_doc_index(\"pregnant\", corona_data),\n\n               \"smoking\": return_doc_index(\"smoking\", corona_data),\n\n               \"co_infection\": return_doc_index(\"co infection\", corona_data),\n\n                \"neonates\": return_doc_index(\"neonates\", corona_data),\n\n               \"transmission\": return_doc_index(\"transmission dynamics\", corona_data),\n\n                \"high_risk\": return_doc_index(\"high-risk patient\", corona_data)\n             }","c2b6c5f5":"print(f\"Number of Documents that Mention Risk: {len(doc_folder['risk'][0])}\")\n\nprint(f\"Number of Documents that Mention Pregnancy: {len(doc_folder['preg'][0])}\")\n\nprint(f\"Number of Documents that Mention Smoking: {len(doc_folder['smoking'][0])}\")\n\nprint(f\"Number of Documents that Mention Neonates: {len(doc_folder['neonates'][0])}\")\n\nprint(f\"Number of Documents that Mention Transmission Dynamics: {len(doc_folder['transmission'][0])}\")\n\nprint(f\"Number of Documents that Mention High Risk Patients: {len(doc_folder['high_risk'][0])}\")","ca871882":"doc_folder['risk'][0]","8420ec99":"corona_data = assign_label(doc_folder['risk'][0], corona_data, \"risk\")\ncorona_data = assign_label(doc_folder['preg'][0], corona_data, \"preg\")\ncorona_data = assign_label(doc_folder['smoking'][0], corona_data, \"smoking\")\ncorona_data = assign_label(doc_folder['neonates'][0], corona_data, \"neonates\")\ncorona_data = assign_label(doc_folder['transmission'][0], corona_data, \"transmission\")\ncorona_data = assign_label(doc_folder['high_risk'][0], corona_data, \"high_risk\")","92f7d7db":"le = LabelEncoder()\ncorona_data['risk_label_encode'] = le.fit_transform(corona_data['risk_label'])","a749eec9":"corona_data['title_vector'] = corona_data['title'].apply(create_body_vector, args=[coronas_d2v_model])","12b21fc3":"vectors = [x for x in corona_data['title_vector']]","f57bed71":"vec_df = pd.DataFrame(vectors)","1df2628a":"corona_data = corona_data.drop(columns=['title_vector'])","dcf2d6ba":"scaler = MinMaxScaler()\nvec_df_s = scaler.fit_transform(vec_df)","d3373e2f":"# First we need to normalise the feature vectors before clustering,\n\nsilhouette_plot(vec_df_s, 2, 20)","d26d9c99":"best_cents = return_opt_weights(vec_df_s)","48c84f74":"kmeans_optimised = KMeans(n_clusters=15, init=best_cents, max_iter=20)\nkmeans_optimised.fit(vec_df_s)","cc843351":"corona_data['cluster_labels'] = kmeans_optimised.labels_","f76be1c9":"print(f\"Instance 0 CLuster Label: {corona_data['cluster_labels'][0]}\")\nprint(f\"Instance 156 CLuster Label: {corona_data['cluster_labels'][9875]}\")\nprint(f\"Instance 5689 CLuster Label: {corona_data['cluster_labels'][5689]}\")\nprint(f\"Instance 12 CLuster Label: {corona_data['cluster_labels'][12]}\")","c2356ce3":"\nreshaped_list = [\n    \n    (0, vec_df_s[0].reshape(-1, 1).T),\n    (9875, vec_df_s[9875].reshape(-1, 1).T),\n    (5689, vec_df_s[5689].reshape(-1, 1).T),\n    (12, vec_df_s[12].reshape(-1, 1).T)\n    \n]\n\n\n\nfor r in reshaped_list:\n    print('\\n------------------------\\n')\n    ind_arr = list(kmeans_optimised.transform(r[1]))\n    print(f\"Instance {r[0]} Distance Array:\\n\\n{ind_arr}\\n\\n\")\n    print(f\"Instance Actual CLuster Label: {corona_data['cluster_labels'][r[0]]}\")\n    print(f\"Instance Lowest Distance Index: {np.argmin(ind_arr)}\")\n    print('\\n-----------------------\\n')\n\n","175b2d9d":"cluster_features = create_cluster_df(kmeans_optimised, vec_df_s)","4bd02dd8":"cluster_features = rename_cluster_cols(cluster_features)","f9683b5a":"vec_df = rename_vec_df(vec_df)","bc792118":"num_covidDoc_repe = pd.concat([cluster_features, vec_df], axis=1)","553c5516":"doc_id_series = pd.Series(corona_data['doc_id'])\ndoc_source_series = pd.Series(corona_data['source'])\n\nnum_covidDoc_repe[\"doc_id\"] = num_covidDoc_repe.insert(0, \"doc_id\", doc_id_series)\nnum_covidDoc_repe[\"source\"] = num_covidDoc_repe.insert(0, \"source\", doc_id_series)\nnum_covidDoc_repe[\"cluster_label\"] = num_covidDoc_repe.insert(0, \"cluster_label\", doc_id_series)","1faa9b00":"num_covidDoc_repe[\"doc_id\"] = corona_data[\"doc_id\"]\nnum_covidDoc_repe['source'] = corona_data['source']\nnum_covidDoc_repe['cluster_label'] = corona_data['cluster_labels']","d7d29cc2":"clust_0_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==0]\nclust_1_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==1]\nclust_2_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==2]\nclust_3_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==3]\nclust_4_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==4]\nclust_5_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==5]\nclust_6_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==6]\nclust_7_d = num_covidDoc_repe[num_covidDoc_repe['cluster_label']==7]\n\nind_0 = list(clust_0_d.index)\nind_1 = list(clust_1_d.index)\nind_2 = list(clust_2_d.index)\nind_3 = list(clust_3_d.index)\nind_4 = list(clust_4_d.index)\nind_5 = list(clust_5_d.index)\nind_6 = list(clust_6_d.index)\nind_7 = list(clust_7_d.index)\n","92ef36b6":"# We can see that cluster 1 documents deal with virus modelling. \n\n# print_doc_title(corona_data, ind_0)","11e26e7d":"# CLuster 1 appears to deal with EPidemiology \n\n# print_doc_title(corona_data, ind_1)","757aaebf":"# Think cluister 2 appears to be about co-infection \n\n# print_doc_title(corona_data, ind_2)","70ffd9cb":"# cLUSTER 3 appears to be about bio-checmical interactions\n\n# print_doc_title(corona_data, ind_3)","29c3a6e8":"# CLuster 4 appears to deal with viral evolution. \n\n# print_doc_title(corona_data, ind_4)","7ee4ccc7":"# Unsure about cluster 5, maybe some domain knowldge would help. \n\n# print_doc_title(corona_data, ind_5)","a72356c1":"# Cluster 6 is mostly about detection \n\n# print_doc_title(corona_data, ind_6)","2f76316e":"# CLuster 7 appears to be mostly about Transmission data. \n\n# print_doc_title(corona_data, ind_7)","f0d3ccc5":"pca = PCA(n_components=3)\nthree_d_vectors = pca.fit_transform(vec_df_s)","e24ff4cf":"pca_df = pd.DataFrame()\npca_df['pca_one'] = three_d_vectors[:, 0]\npca_df['pca_two'] = three_d_vectors[:, 1]\npca_df['pca_three'] = three_d_vectors[:, 2]","e6fc6850":"plot_vectors(pca_df, 'cluster', corona_data)","f50310a4":"plot_vectors(pca_df, 'risk_label', corona_data)","725b43e3":"<h3><ins>Cluster Contents<\/ins><\/h3><br>\n\nTo get a rough idea of the contents of each cluster we grouped each of the instances together based on `cluster label`. Then, using the `print title` function above, we examined the subject of each document in the grouping. Rough topics are outlined below, however LDA or HDP will allow for automatic topic modelling and the assignment of meaningful labels to each grouping of documents. \n\n<ins>Estimated Topics<\/ins>\n\n1. <i>Virus Modelling<\/i>\n\n2. <i>Epidemiology<\/i>\n\n3. <i>Co-Infection<\/i>\n\n4. <i>Bio-Chemical Interactions<\/i>\n\n5. <i>Viral Evolution<\/i>\n\n6. <i>Unclear On This<\/i>\n\n7. <i>Virus Detection<\/i>\n\n\n<i>Please note these are rough estimations of the topics covered in each cluster.<\/i>\n\n-----------------------------\n\n","27c969b5":"<h3><ins>1. Custom Functions<\/ins><\/h3><br>\n\nAll custom functions imported from our utility script. \n\n--------------------------\n\n","683336bd":"------------------------\n\nAlso loaded is the doc2vec model, trained on the whole corpus of documents. This model is used to create our vectored features. \n\n-----------------------\n\n<i>To Note: Different embedding models may improve cluster silhouette score<\/i>\n\n------------------------\n","fcd60f8e":"<h1><ins>Task: Create Vector and Cluster Features<\/ins><\/h1><br>\n\nHere we aim to use a pretrained doc2vec model to engineer features from medical documents that will allow a neural network to decide whether they contain information relating to coronavirus risk. \n\nThe notebook runs as follows:\n\n1. Set up custom functions.\n2. Load the data. \n3. Testing lookup function.\n4. K-Means cluster evalutaion: Silhouette Score.\n5. K-Means feature engineering.\n6. Cluster contents.\n7. Embedding visualisation coloured on cluster label. \n\n--------------------------------------","199ae432":"<h3><ins>3. Lookup Function<\/ins><\/h3><br>\n\n\nAs a means of assessing the clustering we have manually assigned labels to the data in a prevous notebook. These labels are based on the simple lookup function demonstrated below. \n\nOnce vectorised and clustered, the instances will be plotted in 3d (based on their vectors, reduced using PCA). \n\nTwo plots are created; one in which instances are coloured according to cluster label, and the other coloured according to the manually assigned label. \n\nThrough comparison of these two plots we can assess how well K-Means has clustered our data. \n\n------------------------------------------------------\n","28de7418":"<h3><ins>5. K-Means feature engineering<\/ins><\/h3><br>\n\n\nFollowing from creating our cluster model we can now assign new features to the dataset. \n\n1. <i>Cluster Label:<\/i> The cluster to which each instance belongs. \n\n\n2. <i>Distance To Cluser Centroid:<\/i> Distance of each cluster to every other cluster centroid.\n\n\n---------------------------------------------","d3e05306":"<b><ins>Create Cluster Distance Features<\/ins><\/b>\n\nUsing the function we can now create a dataframe consisting of cluster distances and combine this with the vector dataframe for a full set of features. \n\n----------------------","ff8432ad":"<b><ins>Testing The Distance Array<\/ins><\/b><br>\n\nThe K-Means object provides a handy method for returning an instances distance to each cluster centroid. To ensure this is working correctly I chose four random instances. \n\nUsing `.transform()` I obtained an array with the distance to each centroid.\n\nWe then compared the index that held the lowest value (the cluster centroid closest to the instance) with the actual cluster label. This ensured that the list returned by `.transform()` was a true representation of the cluster space. \n\n-------","b6ff1557":"-------------------------\n\n<b><ins>Optimal Weights<\/ins><\/b><br>\n\nWe find the optimal weights by using a similiar technique, only this time measuring the inertia of a small subset and saving the weight initialisations. Once an optimum weighting has been found, this is returned and used to train the final K-Means model on all the data.  \n\n-------------------------------","43332c51":"-----------------------------\n\nA quick sanity check ensures the lookup function actually found some documents. \n\n-----------------------------------------","b6d2b204":"----------------\n\nLastly, we encode the manually assigned labels to allow the 3d plot generator to colour instances based on this attribute.\n\n---------------","f212ddac":"<b><ins>Assign Cluster Labels<\/ins><\/b><br>\n\nFirst we assign the labels to an attribute in the original dataframe. \n\n----------------------","59e7bf08":"------------------------\n\nWe use `MinMax Scaling` before clustering\n\n------------------------------","4f944229":"<h3><ins>2. Load The Data<\/ins><\/h3><br>\n\nData is loaded from the covid `.csv` file created in a previous kernel.\n\n---------------------------------------","db2ca162":"-------------------\n\nOnce we're done with the `title vector` column we can just drop it. \n\n\n---------------------------\n","d4f8e712":"---------------------------\n\n<b><ins>Silhouette Plot<\/ins><\/b>\n\nThen we call the function to start the silhouette analysis. \n\nWe iteratively try a range of clusters. \n\nFirst it will output the silhouette score for the model, then two plots follow;\n\n1. <i>Silhouette Diagram:<\/i> The dashed red line represents the mean silhouette score for that model. When most instances have a lower coefficient than this line it indicates poor clustering, meaning the instances are too close together. \n\n\n2. <i>Visualisation Data:<\/i> Each instance visualised in cluster space, coloured on cluster label and highlighting each cluster centroid.\n\n\nUsing these graphs it was determined that `8 clusters` was optimal. \n\n-------------------------------","ad452a2c":"-----------------------------------------\n\nFirst, the lookup function finds all documents containing a keyword. \n\n\n-------------------------------------","b1a8d57a":"<h3><ins>4. K-Means: Silhouette Score<\/ins><\/h3><br>\n\nIn order to properly conduct K-Means clustering we have to find out the optimum number of clusters. While the elbow method is one way to calculate this value, a better technique is to use the silhouette score. \n\n\nThe silhouette score of a model is the mean silhouette coefficient over all instances. An instances silhouette coefficient can be calucalted as:\n\n`(b - a) \/ max(a, b)`\n\nWhere:\n\n`a` = The intra-cluster distance (the mean distance to other instances in the same cluster).\n\n`b` = The nearest-cluster distance (the mean distance to other instances in the next nearest cluster).\n\nSilhouette coefficients can vary from -1 to 1, with a score closer to 1 meaning that instances are well within their own cluster and far from other clusters. A score closer to 0 means that it is close to the cluster boundary, while a score of -1 may mean an instance has been assigned to the wrong cluster. \n\nFirst, we create the vectors using the doc2vec model and build them into their own dataframe. \n\n---------------------------------------------------","fd0b89c1":"<h3><ins>7. Plot The Results<\/ins><\/h3><br>\n\nAs a means of visualising how well the algorithm has clustered the data, we reduced the dimensions to visualise the instance vectors. \n\nUsing this approach we created two plots:\n\n1. <i>Coloured On Manual Label<\/i>\n\n\n2. <i> Coloured On Cluster Label<\/i>\n\n\nAs can be seen from the graph, compared with our manual approach, and the cursory glance at the cluster contents, the algorithm has grouped the documents quite well. \n\nFurther analysis to automatically model the topics in these clusters is required. \n\n------------------------------------"}}