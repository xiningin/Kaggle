{"cell_type":{"25d370c3":"code","bf5d9640":"code","f6f34dfd":"code","69b09abf":"code","20b04439":"code","21e93f13":"code","89f0aab7":"code","ba203435":"code","0763e692":"code","51dc01d1":"code","1b4c7b19":"code","5384376b":"code","5f4bc6ea":"code","03e9e557":"code","7cd5da6d":"code","3d06b6d0":"code","efda263c":"code","874227b8":"code","8b890e17":"code","ce63d004":"code","4623f94e":"code","c573de22":"code","c9e1a284":"code","2184acd7":"code","3a20c84a":"code","e36112f5":"code","fb98fd82":"code","e80baff3":"code","4330af54":"code","5ed2c61a":"code","961d99be":"code","d2c2e5de":"markdown","4de6f6c7":"markdown","657ddc65":"markdown","46f57394":"markdown","5ad34697":"markdown","5a4abad6":"markdown","7d97d907":"markdown","188aa53b":"markdown","0d85801b":"markdown","41e7c8d5":"markdown","a6f075fe":"markdown","6150bc73":"markdown","967e5519":"markdown","0807cfaa":"markdown","9bdead9b":"markdown","584697fd":"markdown","172e4e18":"markdown","d310e87e":"markdown"},"source":{"25d370c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf5d9640":"!pip install seaborn --upgrade \n!pip install pyod\n","f6f34dfd":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pyod.models.copod import COPOD\nfrom scipy import stats\nfrom scipy.stats import f_oneway\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport optuna\nfrom optuna.samplers import TPESampler","69b09abf":"mtcars_data=pd.read_csv('..\/input\/mt-cars\/Lesson 3 Practice\/mtcars.csv')","20b04439":"mtcars_data.head()","21e93f13":"mtcars_data.isna().sum()","89f0aab7":"sns.displot(mtcars_data, x=\"hp\", kind=\"kde\")\n","ba203435":"ContV= pd.concat([mtcars_data['mpg'],mtcars_data['hp'],mtcars_data['disp'],mtcars_data['drat'],mtcars_data['wt'],mtcars_data['qsec'],mtcars_data['cyl']], axis=1)","0763e692":"sns.pairplot(ContV, hue=\"cyl\", height=2.5)\n","51dc01d1":"plt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(mtcars_data.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","1b4c7b19":"train=mtcars_data.drop(['model'],axis=1)","5384376b":"clf = COPOD(contamination=0.1)\nclf.fit(train)","5f4bc6ea":"cluster = clf.predict(train)\ntrain['cluster'] = cluster","03e9e557":"g = sns.PairGrid(train, hue=\"cluster\", corner=True)\ng.map_lower(sns.kdeplot, hue=None, levels=5, color=\".2\")\ng.map_lower(sns.scatterplot)\ng.map_diag(sns.histplot, element=\"step\", linewidth=0)\ng.add_legend(frameon=True)\ng.legend.set_bbox_to_anchor((.61, .6))","7cd5da6d":"train[train['cluster']==1]\ntrain=train.drop(train[train['cluster']==1].index,axis=0)","3d06b6d0":"def stepwise_lm(X,y):\n    model = sms.OLS(y,X).fit()\n    model1=model\n    X1=X\n    while model.bic>=model1.bic:\n        X=X1\n        model=model1\n        model.pvalues.idxmax()\n        X1=X.drop([model.pvalues.idxmax()],axis=1)\n        model1 = sms.OLS(y,X1).fit()\n    return(X,model)\n","efda263c":"# Independant variable\nX = train.drop(['cluster','hp'],axis=1)        # All rows & columns exclude Target features\n\n# Dependant variable\ny = train['hp'].values  ","874227b8":"X,model=stepwise_lm(X,y)","8b890e17":"X","ce63d004":"model.summary()","4623f94e":"sns.pairplot(X)\n","c573de22":"lr=LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\nlr.fit(X_train,y_train)\n","c9e1a284":"print('The mean absolute score is :',mean_absolute_error(lr.predict(X_test),y_test))\n","2184acd7":"sampler = TPESampler(seed=0)\n\ndef create_modelLGBM(trial):\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 31)\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n    max_depth = trial.suggest_int('max_depth', 3, 8)\n    min_child_samples = trial.suggest_int('min_child_samples', 100, 1200)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.99)\n    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 5, 90)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.0001, 1.0)\n    feature_fraction = trial.suggest_uniform('feature_fraction', 0.0001, 1.0)\n    model = LGBMRegressor(\n        num_leaves=num_leaves,\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        min_child_samples=min_child_samples, \n        min_data_in_leaf=min_data_in_leaf,\n        learning_rate=learning_rate,\n        feature_fraction=feature_fraction,\n        random_state=0\n    )\n    return model\ndef objectiveLGBM(trial):\n    model = create_modelLGBM(trial)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = mean_absolute_error(y_test, preds)\n    return score","3a20c84a":"studyLGBM = optuna.create_study(direction=\"minimize\", sampler=sampler)\nstudyLGBM.optimize(objectiveLGBM, n_trials=500)\nparamsLGBM = studyLGBM.best_params","e36112f5":"paramsLGBM","fb98fd82":"lgb = LGBMRegressor(**paramsLGBM)\nlgb.fit(X_train, y_train)\n\npred_LGBM = lgb.predict(X_test)\nprint('Simple LGBM accuracy: ', mean_absolute_error(y_test, pred_LGBM))","e80baff3":"sampler = TPESampler(seed=0)\n\ndef create_modelXGB(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 2000)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    model = XGBRegressor(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth, gamma=gamma, random_state=0)\n\n    return model\ndef objectiveXGB(trial):\n    model = create_modelXGB(trial)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = mean_absolute_error(y_test, preds)\n    return score","4330af54":"studyXGB = optuna.create_study(direction=\"minimize\", sampler=sampler)\nstudyXGB.optimize(objectiveXGB, n_trials=200)\nparamsXGB = studyXGB.best_params","5ed2c61a":"paramsXGB","961d99be":"XGBr = XGBRegressor(**paramsXGB)\nXGBr.fit(X_train, y_train)\ny_xpred=XGBr.predict(X_test)\nprint('Optimized XGBClassifier accuracy: ', mean_absolute_error(y_test, y_xpred))","d2c2e5de":"Our outlier detection method will be a copula-based method. For a deeper reading [link](https:\/\/arxiv.org\/pdf\/2009.09463.pdf).","4de6f6c7":"It is obvious that the behavior or the dependence of the data on the number of cylinders, especially when we have 8 cylinders, but it is not as important as I would expect. Next step of our analysis is study the correlation between variables.","657ddc65":"We start plotting the kernel density estimation  of hp variable.","46f57394":"Firts we define a stepwise function that allows us to determine the most relevant features for the model. The fucntion will slect the features trying to reduce the BIC score. We will remove the feature with the biggest p-value of the F-test. Technically we can choose any features, as long as it fulfills the null hypothesis, since under the null hypothesis the p-value follows a uniform distribution.","5ad34697":"Parameter selection of the LightGBM model","5a4abad6":"<h2 style='background:lightblue; border:0; color:black'><center>Advanced models: LightGBM and XGBoost<\/center><\/h2>\n","7d97d907":"<h1 style='background:lightblue; border:0; color:black'><center>Mt Cars<\/center><\/h1>\n","188aa53b":"Parameter selection of the XGBoost model","0d85801b":"Probably one of the most important characteristic of a car is the number of cylinder. To watch this we will plot all continuous variables separated by number of cylinders.","41e7c8d5":"<h2 style='background:lightblue; border:0; color:black'><center>Study of the hp<\/center><\/h2>\n","a6f075fe":"<center><img src=\"https:\/\/images8.alphacoders.com\/568\/thumb-1920-568490.jpg\"><\/center>","6150bc73":"All the variables are highly correlated. This would be a problem when we build our model because can appear confounding variables.","967e5519":"<h2 style='background:lightblue; border:0; color:black'><center>Linear regression model<\/center><\/h2>\n","0807cfaa":"<h2 style='background:lightblue; border:0; color:black'><center>Outlier detection<\/center><\/h2>\n","9bdead9b":"We remove the outlier detected.","584697fd":"To analyze the behavior of our outliers we will make the next relplot","172e4e18":"Number of NaN values ","d310e87e":"Overview f the dataset"}}