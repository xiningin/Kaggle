{"cell_type":{"436969ef":"code","5f5d3b47":"code","9736b27d":"code","296ebfb4":"code","d60c64ae":"code","dcf444ca":"code","8d3d156a":"code","c36cc0fb":"code","6b96992e":"code","f1fb7acf":"code","3813c2d8":"code","481ebc3a":"code","83a7cabb":"code","664f826a":"code","52b6791a":"code","88f1416a":"code","9afd8b29":"code","b15a837c":"markdown","77a1ec05":"markdown","b301414b":"markdown","deb0f8e3":"markdown","f277c2c9":"markdown","abb95514":"markdown","07b1b041":"markdown","472281ec":"markdown","1d352daf":"markdown","a67ecd84":"markdown","5414ea84":"markdown","f7d67802":"markdown"},"source":{"436969ef":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing feature selection and processing packages\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,PowerTransformer\nfrom sklearn.decomposition import PCA\n\n# importing modelling packages\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier","5f5d3b47":"# using datatable for faster loading\n\ntrain = pd.read_csv(r'..\/input\/tabular-playground-series-oct-2021\/train.csv',nrows=10000)\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-oct-2021\/test.csv',nrows=10000)","9736b27d":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64','float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                else:\n                    df[col] = df[col].astype(np.float32)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","296ebfb4":"train = reduce_mem_usage(train)\ntest  = reduce_mem_usage(test)","d60c64ae":"X = train.drop(columns=[\"id\", \"target\"]).copy()\ny = train[\"target\"].copy()\ntest_for_model = test.drop(columns=[\"id\"]).copy()\n\n# freeing up some memory\n\ndel train\ndel test","dcf444ca":"# using baseline xgb and catboost models - on gpu\n\ncat_params = {\"task_type\": \"GPU\"}\nxgb_params = {'tree_method': 'gpu_hist','predictor': 'gpu_predictor'}","8d3d156a":"folds = StratifiedKFold(n_splits = 10, random_state = 2021, shuffle = True)\n\npredictions_cb = np.zeros(len(test_for_model))\npredictions_xgb = np.zeros(len(test_for_model))\n\ncat_oof = np.zeros(X.shape[0])\nxgb_oof = np.zeros(X.shape[0])\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n    print(f\"Fold: {fold+1}\")\n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model_cb =  CatBoostClassifier(**cat_params,verbose=0,random_state=2021)\n    model_xgb = XGBClassifier(**xgb_params,random_state=2021)\n    \n    model_cb.fit(X_train, y_train)\n    pred_cb = model_cb.predict_proba(X_val)[:,1]\n    cat_oof[val_idx] = pred_cb\n    print('ROC of CB: ',roc_auc_score(y_val,pred_cb))\n    \n    model_xgb.fit(X_train, y_train)\n    pred_xgb = model_xgb.predict_proba(X_val)[:,1]\n    xgb_oof[val_idx] = pred_xgb\n    print('ROC of XGB: ',roc_auc_score(y_val,pred_xgb))\n    \n    print(\"-\"*50)\n    \n    predictions_cb += model_cb.predict_proba(test_for_model)[:,1] \/ folds.n_splits\n    predictions_xgb += model_xgb.predict_proba(test_for_model)[:,1] \/ folds.n_splits","c36cc0fb":"# calculating appropriate weights for ensemble\n\nimport scipy\ndef class_optimizer(X, a0, a1):\n    oof = X[0]*a0 + (1-X[0])*a1\n    return (1-roc_auc_score(y, oof))\n\nres = scipy.optimize.minimize(\n    fun=class_optimizer,\n    x0=[0.5],\n    args=tuple([cat_oof, xgb_oof]),\n    method='BFGS',\n    options={'maxiter': 1000})\n\nprint(res)\nprint(f\"coef0 {res.x[0]}, coef1 {1-res.x[0]}\")","6b96992e":"ensemble_pred = res.x[0] * predictions_cb  + (1-res.x[0]) * predictions_xgb\nsub['target'] = ensemble_pred\nsub.to_csv('submission_simple_ensemble.csv',index = False)","f1fb7acf":"# creating stack datasets for our meta-classifier\n\ncat_train = pd.DataFrame(cat_oof,columns=['CAT_train'])\nxgb_train = pd.DataFrame(xgb_oof,columns=['XGB_train'])\ncat_test = pd.DataFrame(predictions_cb,columns=['CAT_train'])\nxgb_test = pd.DataFrame(predictions_xgb,columns=['XGB_train'])\n\nstack_x_train = pd.concat((cat_train,xgb_train), axis = 1)\nstack_x_test = pd.concat((cat_test,xgb_test), axis = 1)","3813c2d8":"stk = StratifiedKFold(n_splits = 10, random_state = 42)\n\ntest_pred = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train.iloc[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train.iloc[valid_index], y[valid_index]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 1000, max_iter = 1000)\n    lr.fit(x_train, y_train)\n    \n    valid_pred = lr.predict_proba(x_valid)[:,1]\n    test_pred += lr.predict_proba(stack_x_test)[:,1]\n    auc = roc_auc_score(y_valid, valid_pred)\n    total_auc += auc \/ 10\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","481ebc3a":"# creating stack datasets for our meta-classifier\n\nstack_x_train['pred'] = stack_x_train.mean(axis=1)\nstack_x_test['pred'] = stack_x_test.mean(axis=1)\nstack_x_train = pd.DataFrame(stack_x_train['pred'])\nstack_x_test = pd.DataFrame(stack_x_test['pred'])","83a7cabb":"stk = StratifiedKFold(n_splits = 10, random_state = 42)\n\ntest_pred = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train.iloc[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train.iloc[valid_index], y[valid_index]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 1000, max_iter = 1000)\n    lr.fit(x_train, y_train)\n    \n    valid_pred = lr.predict_proba(x_valid)[:, 1]\n    test_pred += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred)\n    total_auc += auc \/ 10\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","664f826a":"# adding whole data to the predictions\n\nstack_x_train = pd.concat((X,cat_train, xgb_train), axis = 1)\nstack_x_test = pd.concat((test_for_model,cat_test, xgb_test), axis = 1)","52b6791a":"stk = StratifiedKFold(n_splits = 10, random_state = 42)\n\ntest_pred = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train.iloc[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train.iloc[valid_index], y[valid_index]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 1000, max_iter = 1000)\n    lr.fit(x_train, y_train)\n    \n    valid_pred = lr.predict_proba(x_valid)[:, 1]\n    test_pred += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred)\n    total_auc += auc \/ 10\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","88f1416a":"imp_features= [\"f22\", \"f179\", \"f69\", \"f58\", \"f214\", \"f78\", \"f136\", \"f156\",\n               \"f8\", \"f3\", \"f77\", \"f200\", \"f92\", \"f185\", \"f142\", \"f115\", \"f284\"]\nX_new = X[imp_features]\ntest_for_model_new = test_for_model[imp_features]\n\nstack_x_train = pd.concat((X_new,cat_train, xgb_train), axis = 1)\nstack_x_test = pd.concat((test_for_model_new,cat_test, xgb_test), axis = 1)","9afd8b29":"stk = StratifiedKFold(n_splits = 10, random_state = 42)\n\ntest_pred = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train.iloc[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train.iloc[valid_index], y[valid_index]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 1000, max_iter = 1000)\n    lr.fit(x_train, y_train)\n    \n    valid_pred = lr.predict_proba(x_valid)[:, 1]\n    test_pred += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred)\n    total_auc += auc \/ 10\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","b15a837c":"<div style=\"background-color:rgba(108, 29, 21, 0.5);\">\n    <h1><center>Initialising Baseline Models<\/center><\/h1>\n<\/div>","77a1ec05":"<div style=\"background-color:rgba(108, 29, 21, 0.5);\">\n    <h1><center>Data Splitting<\/center><\/h1>\n<\/div>","b301414b":"<div style=\"background-color:rgba(108, 29, 21, 0.5);\">\n    <h1><center>Importing Libraries and Data<\/center><\/h1>\n<\/div>","deb0f8e3":"# Stacking using whole data+predictions separately","f277c2c9":"# Stacking using the 2 predictions separately","abb95514":"<div style=\"background-color:rgba(108, 29, 21, 0.5);\">\n    <h1><center>Use for your own experiments and do upvote. Thanks :)<\/center><\/h1>\n<\/div>","07b1b041":"# Stacking using important features+predictions separately","472281ec":"This is a sandpit of sorts, where I explore different methods of stacking which came to my mind.\nI am aware of the fact that adding more levels to stacking may improve accuracy further.\n\nAfter getting weights for my simple ensemble, I am trying **different methods of stacking**-\n1. Taking the predictions as separate and using them for training my meta-classifier\n2. Aggregating the two predictions (mean) and using that for my meta.\n3. Adding the predictions to whole training data.\n4. Adding the predictions to a subset of the whole data, which has most 'important' features.\n\nI have calculated total average AUCs for these 4 methods.\n\nI hae compared the results as well and I hope it gives you an idea for trying out a different method of stacking as well. Note that **I have used only 10000 rows of the data**, to demonstrate, for speed and memory.\n\n**Do upvote if you find this notebook useful :)**","1d352daf":"# Summary\n\n1. Taking the predictions as separate and using them for training my meta-classifier - 0.8394 - BEST\n2. Aggregating the two predictions (mean) and using that for my meta - 0.8345\n3. Adding the predictions to whole training data. - 0.8301 - WORST (Weird)\n4. Adding the predictions to a subset of the whole data, which has most 'important' features - 0.8384","a67ecd84":"# Stacking using average of the 2 predictions","5414ea84":"# Simple Ensembling","f7d67802":"<div style=\"background-color:rgba(108, 29, 21, 0.5);\">\n    <h1><center>Memory Reduction<\/center><\/h1>\n<\/div>"}}