{"cell_type":{"c2d7b538":"code","61e5f9eb":"code","99d94046":"code","254eca8a":"code","67fbd43b":"code","aac026a5":"code","7c7f7994":"code","ae90f682":"code","d1842c00":"code","321ac0b2":"code","4252c87b":"code","947c8cd0":"code","2128975b":"code","fdb9f7ee":"code","e6b2aba6":"code","7843bd99":"code","eb7b4b1c":"code","50a89151":"code","3db8062a":"code","a5a76660":"code","9519fc11":"code","c20aa4f5":"code","cd6f0fd6":"code","6e836871":"code","068a77c8":"code","a7bcf419":"code","6fac779d":"code","3b7356ef":"markdown","823bef3f":"markdown","ca090871":"markdown","a500885c":"markdown","0c6513ee":"markdown","034ea963":"markdown","f8865f4f":"markdown","fb6899a6":"markdown","52960349":"markdown","0ca30056":"markdown","3d0108de":"markdown","aa701b35":"markdown","e4a623ac":"markdown","e2d2dd8a":"markdown","779d7ca3":"markdown","b0a50443":"markdown"},"source":{"c2d7b538":"import os\nimport cv2\nimport glob\nimport h5py\nimport shutil\nimport imgaug as aug\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport imgaug.augmenters as iaa\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom pathlib import Path\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom keras.models import Sequential, Model, load_model\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom keras import backend as K\nimport tensorflow as tf\n\n\ncolor = sns.color_palette()\n%matplotlib inline\n%config InlineBackend.figure_format=\"svg\"","61e5f9eb":"# Set the seed for hash based operations in python\nos.environ['PYTHONHASHSEED'] = '0'\n\nseed=1234\n\n# Set the numpy seed\nnp.random.seed(seed)\n\n# Set the random seed in tensorflow at graph level\ntf.set_random_seed(seed)\n\n# Make the augmentation sequence deterministic\naug.seed(seed)","99d94046":"# As usual, define some paths first to make life simpler\ntraining_data = Path('..\/input\/training\/training\/') \nvalidation_data = Path('..\/input\/validation\/validation\/') \nlabels_path = Path('..\/input\/monkey_labels.txt')","254eca8a":"labels_info = []\n\n# Read the file\nlines = labels_path.read_text().strip().splitlines()[1:]\nfor line in lines:\n    line = line.split(',')\n    line = [x.strip(' \\n\\t\\r') for x in line]\n    line[3], line[4] = int(line[3]), int(line[4])\n    line = tuple(line)\n    labels_info.append(line)\n    \n# Convert the data into a pandas dataframe\nlabels_info = pd.DataFrame(labels_info, columns=['Label', 'Latin Name', 'Common Name', \n                                                 'Train Images', 'Validation Images'], index=None)\n# Sneak peek \nlabels_info.head(10)","67fbd43b":"# Create a dictionary to map the labels to integers\nlabels_dict= {'n0':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'n5':5, 'n6':6, 'n7':7, 'n8':8, 'n9':9}\n\n# map labels to common names\nnames_dict = dict(zip(labels_dict.values(), labels_info[\"Common Name\"]))\nprint(names_dict)","aac026a5":"# Creating a dataframe for the training dataset\ntrain_df = []\nfor folder in os.listdir(training_data):\n    # Define the path to the images\n    imgs_path = training_data \/ folder\n    \n    # Get the list of all the images stored in that directory\n    imgs = sorted(imgs_path.glob('*.jpg'))\n    \n    # Store each image path and corresponding label \n    for img_name in imgs:\n        train_df.append((str(img_name), labels_dict[folder]))\n\n\ntrain_df = pd.DataFrame(train_df, columns=['image', 'label'], index=None)\n# shuffle the dataset \ntrain_df = train_df.sample(frac=1.).reset_index(drop=True)\n\n####################################################################################################\n\n# Creating dataframe for validation data in a similar fashion\nvalid_df = []\nfor folder in os.listdir(validation_data):\n    imgs_path = validation_data \/ folder\n    imgs = sorted(imgs_path.glob('*.jpg'))\n    for img_name in imgs:\n        valid_df.append((str(img_name), labels_dict[folder]))\n\n        \nvalid_df = pd.DataFrame(valid_df, columns=['image', 'label'], index=None)\n# shuffle the dataset \nvalid_df = valid_df.sample(frac=1.).reset_index(drop=True)\n\n####################################################################################################\n\n# How many samples do we have in our training and validation data?\nprint(\"Number of traininng samples: \", len(train_df))\nprint(\"Number of validation samples: \", len(valid_df))\n\n# sneak peek of the training and validation dataframes\nprint(\"\\n\",train_df.head(), \"\\n\")\nprint(\"=================================================================\\n\")\nprint(\"\\n\", valid_df.head())","7c7f7994":"# some constants(not truly though!) \n\n# dimensions to consider for the images\nimg_rows, img_cols, img_channels = 224,224,3\n\n# batch size for training  \nbatch_size=8\n\n# total number of classes in the dataset\nnb_classes=10","ae90f682":"# Augmentation sequence \nseq = iaa.OneOf([\n    iaa.Fliplr(), # horizontal flips\n    iaa.Affine(rotate=20), # roatation\n    iaa.Multiply((1.2, 1.5))]) #random brightness","d1842c00":"def data_generator(data, batch_size, is_validation_data=False):\n    # Get total number of samples in the data\n    n = len(data)\n    nb_batches = int(np.ceil(n\/batch_size))\n\n    # Get a numpy array of all the indices of the input data\n    indices = np.arange(n)\n    \n    # Define two numpy arrays for containing batch data and labels\n    batch_data = np.zeros((batch_size, img_rows, img_cols, img_channels), dtype=np.float32)\n    batch_labels = np.zeros((batch_size, nb_classes), dtype=np.float32)\n    \n    while True:\n        if not is_validation_data:\n            # shuffle indices for the training data\n            np.random.shuffle(indices)\n            \n        for i in range(nb_batches):\n            # get the next batch \n            next_batch_indices = indices[i*batch_size:(i+1)*batch_size]\n            \n            # process the next batch\n            for j, idx in enumerate(next_batch_indices):\n                img = cv2.imread(data.iloc[idx][\"image\"])\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                label = data.iloc[idx][\"label\"]\n                \n                if not is_validation_data:\n                    img = seq.augment_image(img)\n                \n                img = cv2.resize(img, (img_rows, img_cols)).astype(np.float32)\n                batch_data[j] = img\n                batch_labels[j] = to_categorical(label,num_classes=nb_classes)\n            \n            batch_data = preprocess_input(batch_data)\n            yield batch_data, batch_labels","321ac0b2":"#training data generator \ntrain_data_gen = data_generator(train_df, batch_size)\n\n# validation data generator \nvalid_data_gen = data_generator(valid_df, batch_size, is_validation_data=True)","4252c87b":"# simple function that returns the base model\ndef get_base_model():\n    base_model = VGG16(input_shape=(img_rows, img_cols, img_channels), weights='imagenet', include_top=True)\n    return base_model","947c8cd0":"# get the base model\nbase_model = get_base_model()\n\n#  get the output of the second last dense layer \nbase_model_output = base_model.layers[-2].output\n\n# add new layers \nx = Dropout(0.7,name='drop2')(base_model_output)\noutput = Dense(10, activation='softmax', name='fc3')(x)\n\n# define a new model \nmodel = Model(base_model.input, output)\n\n# Freeze all the base model layers \nfor layer in base_model.layers[:-1]:\n    layer.trainable=False\n\n# compile the model and check it \noptimizer = RMSprop(0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()","2128975b":"# always user earlystopping\n# the restore_best_weights parameter load the weights of the best iteration once the training finishes\nes = EarlyStopping(patience=10, restore_best_weights=True)\n\n# checkpoint to save model\nchkpt = ModelCheckpoint(filepath=\"model1\", save_best_only=True)\n\n# number of training and validation steps for training and validation\nnb_train_steps = int(np.ceil(len(train_df)\/batch_size))\nnb_valid_steps = int(np.ceil(len(valid_df)\/batch_size))\n\n# number of epochs \nnb_epochs=100","fdb9f7ee":"# train the model \nhistory1 = model.fit_generator(train_data_gen, \n                              epochs=nb_epochs, \n                              steps_per_epoch=nb_train_steps, \n                              validation_data=valid_data_gen, \n                              validation_steps=nb_valid_steps,\n                              callbacks=[es,chkpt])","e6b2aba6":"# let's plot the loss and accuracy \n\n# get the training and validation accuracy from the history object\ntrain_acc = history1.history['acc']\nvalid_acc = history1.history['val_acc']\n\n# get the loss\ntrain_loss = history1.history['loss']\nvalid_loss = history1.history['val_loss']\n\n# get the number of entries\nxvalues = np.arange(len(train_acc))\n\n# visualize\nf,ax = plt.subplots(1,2, figsize=(10,5))\nax[0].plot(xvalues, train_loss)\nax[0].plot(xvalues, valid_loss)\nax[0].set_title(\"Loss curve\")\nax[0].set_xlabel(\"Epoch\")\nax[0].set_ylabel(\"loss\")\nax[0].legend(['train', 'validation'])\n\nax[1].plot(xvalues, train_acc)\nax[1].plot(xvalues, valid_acc)\nax[1].set_title(\"Accuracy\")\nax[1].set_xlabel(\"Epoch\")\nax[1].set_ylabel(\"accuracy\")\nax[1].legend(['train', 'validation'])\n\nplt.show()","7843bd99":"# What is the final loss and accuracy on our validation data?\nvalid_loss, valid_acc = model.evaluate_generator(valid_data_gen, steps=nb_valid_steps)\nprint(f\"Final validation accuracy: {valid_acc*100:.2f}%\")","eb7b4b1c":"# select all the layers for which you want to visualize the outputs and store it in a list\noutputs = [layer.output for layer in model.layers[1:18]]\n\n# Define a new model that generates the above output\nvis_model = Model(model.input, outputs)\n\n# check if we have all the layers we require for visualization \nvis_model.summary()","50a89151":"# store the layer names we are interested in\nlayer_names = []\nfor layer in outputs:\n    layer_names.append(layer.name.split(\"\/\")[0])\n\n    \nprint(\"Layers going to be used for visualization: \")\nprint(layer_names)","3db8062a":"def get_CAM(processed_image, predicted_label):\n    \"\"\"\n    This function is used to generate a heatmap for a sample image prediction.\n    \n    Args:\n        processed_image: any sample image that has been pre-processed using the \n                       `preprocess_input()`method of a keras model\n        predicted_label: label that has been predicted by the network for this image\n    \n    Returns:\n        heatmap: heatmap generated over the last convolution layer output \n    \"\"\"\n    # we want the activations for the predicted label\n    predicted_output = model.output[:, predicted_label]\n    \n    # choose the last conv layer in your model\n    last_conv_layer = model.get_layer('block5_conv3')\n    \n    # get the gradients wrt to the last conv layer\n    grads = K.gradients(predicted_output, last_conv_layer.output)[0]\n    \n    # take mean gradient per feature map\n    grads = K.mean(grads, axis=(0,1,2))\n    \n    # Define a function that generates the values for the output and gradients\n    evaluation_function = K.function([model.input], [grads, last_conv_layer.output[0]])\n    \n    # get the values\n    grads_values, conv_ouput_values = evaluation_function([processed_image])\n    \n    # iterate over each feature map in yout conv output and multiply\n    # the gradient values with the conv output values. This gives an \n    # indication of \"how important a feature is\"\n    for i in range(512): # we have 512 features in our last conv layer\n        conv_ouput_values[:,:,i] *= grads_values[i]\n    \n    # create a heatmap\n    heatmap = np.mean(conv_ouput_values, axis=-1)\n    \n    # remove negative values\n    heatmap = np.maximum(heatmap, 0)\n    \n    # normalize\n    heatmap \/= heatmap.max()\n    \n    return heatmap","a5a76660":"def show_random_sample(idx):\n    \"\"\"\n    This function is used to select a random sample from the validation dataframe.\n    It generates prediction for the same. It also stores the heatmap and the intermediate\n    layers activation maps.\n    \n    Arguments:\n        idx: random index to select a sample from validation data\n    \n    Returns:\n        activations: activation values for intermediate layers\n    \"\"\"\n    # select the sample and read the corresponding image and label\n    sample_image = cv2.imread(valid_df.iloc[idx]['image'])\n    sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n    sample_image = cv2.resize(sample_image, (img_rows, img_cols))\n    sample_label = valid_df.iloc[idx][\"label\"]\n    \n    # pre-process the image\n    sample_image_processed = np.expand_dims(sample_image, axis=0)\n    sample_image_processed = preprocess_input(sample_image_processed)\n    \n    # generate activation maps from the intermediate layers using the visualization model\n    activations = vis_model.predict(sample_image_processed)\n    \n    # get the label predicted by our original model\n    pred_label = np.argmax(model.predict(sample_image_processed), axis=-1)[0]\n    \n    # choose any random activation map from the activation maps \n    sample_activation = activations[0][0,:,:,32]\n    \n    # normalize the sample activation map\n    sample_activation-=sample_activation.mean()\n    sample_activation\/=sample_activation.std()\n    \n    # convert pixel values between 0-255\n    sample_activation *=255\n    sample_activation = np.clip(sample_activation, 0, 255).astype(np.uint8)\n    \n    \n    \n    # get the heatmap for class activation map(CAM)\n    heatmap = get_CAM(sample_image_processed, pred_label)\n    heatmap = cv2.resize(heatmap, (sample_image.shape[0], sample_image.shape[1]))\n    heatmap = heatmap *255\n    heatmap = np.clip(heatmap, 0, 255).astype(np.uint8)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    super_imposed_image = heatmap * 0.5 + sample_image\n    super_imposed_image = np.clip(super_imposed_image, 0,255).astype(np.uint8)\n\n    f,ax = plt.subplots(2,2, figsize=(15,8))\n    ax[0,0].imshow(sample_image)\n    ax[0,0].set_title(f\"True label: {sample_label} \\n Predicted label: {pred_label}\")\n    ax[0,0].axis('off')\n    \n    ax[0,1].imshow(sample_activation)\n    ax[0,1].set_title(\"Random feature map\")\n    ax[0,1].axis('off')\n    \n    ax[1,0].imshow(heatmap)\n    ax[1,0].set_title(\"Class Activation Map\")\n    ax[1,0].axis('off')\n    \n    ax[1,1].imshow(super_imposed_image)\n    ax[1,1].set_title(\"Activation map superimposed\")\n    ax[1,1].axis('off')\n    plt.show()\n    \n    return activations","9519fc11":"# Get the intermediate activations and plot the heatmap first\nactivations= show_random_sample(123)","c20aa4f5":"def visualize_intermediate_activations(layer_names, activations):\n    \"\"\"\n    This function is used to visualize all the itermediate activation maps\n    \n    Arguments:\n        layer_names: list of names of all the intermediate layers we chose\n        activations: all the intermediate activation maps \n    \"\"\"\n    assert len(layer_names)==len(activations), \"Make sure layers and activation values match\"\n    images_per_row=16\n    \n    for layer_name, layer_activation in zip(layer_names, activations):\n        nb_features = layer_activation.shape[-1]\n        size= layer_activation.shape[1]\n\n        nb_cols = nb_features \/\/ images_per_row\n        grid = np.zeros((size*nb_cols, size*images_per_row))\n\n        for col in range(nb_cols):\n            for row in range(images_per_row):\n                feature_map = layer_activation[0,:,:,col*images_per_row + row]\n                feature_map -= feature_map.mean()\n                feature_map \/= feature_map.std()\n                feature_map *=255\n                feature_map = np.clip(feature_map, 0, 255).astype(np.uint8)\n\n                grid[col*size:(col+1)*size, row*size:(row+1)*size] = feature_map\n\n        scale = 1.\/size\n        plt.figure(figsize=(scale*grid.shape[1], scale*grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(grid, aspect='auto', cmap='viridis')\n    plt.show()","cd6f0fd6":"# visualize all the activation maps for this sample\nvisualize_intermediate_activations(activations=activations, layer_names=layer_names)","6e836871":"sample_idx=200\nactivations= show_random_sample(sample_idx)","068a77c8":"sample_idx=10\nactivations= show_random_sample(sample_idx)","a7bcf419":"sample_idx=55\nactivations= show_random_sample(sample_idx)","6fac779d":"sample_idx=70\nactivations= show_random_sample(sample_idx)","3b7356ef":"<a id='Imports'><\/a>\n## 1. Import the required libraries","823bef3f":"We will read the `monkey_labels.txt` file to extract the information about the labels. We can store this information in a list which then can be converted into a `pandas` dataframe.  ","ca090871":"<a id='reproducibility'><\/a>\n## 2. Do everything we can to make our results reproducible\n\nThumb rule: **Always set the seed!**","a500885c":"**Pretty cool!!**, isn't it? Let's check the **CAM** for few more samples now ","0c6513ee":"<a id=\"augmentation\"><\/a>\n## 4. Data Augmentation\n\nWhen you have limited data, deep models don't do very well. Deep learning models are **data hungry**. The more data you provide to a deep learning model, the more it performance improves (until unless your algorithm has reached a limit). This is where `data augmentation` really comes handy. We will be using [imgaug](https:\/\/github.com\/aleju\/imgaug), a very powerful library for augmenting our images. We will define a sequence of augmentations and for each image, one of these augmentations will be applied to the image during training ","034ea963":"<a id=\"model\"><\/a>\n## 6. Modelling\n\nWe will be doing **transfer learning** here and I am choosing `vgg16` as the base network. You can choose whichever network you want.\nAlso, as the dataset is very small and very very similar to Imagenet, we would make minimal changes in the network to keep the trainable parameters as few as possible  ","f8865f4f":"The labels are `n0, n1, n2, ...`. We will create a mapping of these labels where each class will be represented by an integer starting from 0 to number of classes. We will also create a mapping for the names corresponding to a class. We will be using `Common Name` for the last part","fb6899a6":"<a id=\"infer_results\"><\/a>\n## 8. Can we or should we use CAM for model interpretability?\nIf you look at the above results of CAM, you will find that in most of the class activation maps **face** of a monkey seems to be highly activated.\nThis suggests that for our model **at least** face is a very discriminative feature. You can even say that the first feature that is far by the most important to our model is the face of a monkey. \n\nThere are some advantages and disadvantages when using CAM. I am listing all of which I am aware of:<br>\n### Advantages:\n* CAM is a weak supervision technique and it works well out of the box\n* It is easy and simple to implement\n* It gives a fair idea of the dominant features for which a model is looking for \n* It is one of the easiest methods to explain the outputs of a CNN to anyone, including even a non-technical person.\n\n### Disadvantages:\n* Even though CAM gives a fair idea about dominant features but it lacks to provide specific details. For example, if you look at the examples above, there are some cases where we have the activation heatmap ranging from **neck to face**. How do you infer that for that particular class if the neck was more discriminative feature or face is?\n* Though it gives you an idea of discriminative features, it fails to capture the high-level semantic relationship within the hidden layers. For more details, check out this excellent [article](https:\/\/distill.pub\/2018\/building-blocks\/) ","52960349":"<a id='dataloading'><\/a>\n## 3. Load dataset and process it","0ca30056":"<a id=\"data-gen\"><\/a>\n## 5. Data Generator","3d0108de":"This is a very small dataset. You can load the data into `numpy arrays` which then can be directly used for training. But this isn't always the scenario. Most of the time you won't be able to load the entire dataset in the memory. This is why I always store information about the dataset in dataframes and then use a generator to load the data on the fly. We will be doing the same thing here. ","aa701b35":"Perfect! Store the names of all these intermediate layers in a list. We will use the names during visualizations of these activation maps ","e4a623ac":"<a id='interpretability'><\/a>\n## 7. Model Interpretability\n\nNow starts the most important part. How do we explain the output of our model? For example, given an image, what does the network consider important when classifying the image? Can we get info about it? Can we make our model a little `grey box`? LOL!\n\nAnswering the above question is a bit difficult. Although there have been many advancements in explaining the activations\/outputs of a neural network, for example, check [this](https:\/\/distill.pub\/2018\/building-blocks\/), a lot more has to be done in this direction.\n\nHere, we will explore two methods that are very simple to use and can give some good insights about model predictions. These are:\n* **Visualizing the intermediate layers outputs**\n* **[Class Activation Mapping](http:\/\/cnnlocalization.csail.mit.edu\/)** \n\n\nHere are all the steps we are going to do: \n* We will start by selecting all the layers up to last `convolution block` in VGG16, excluding the `Input layer`.\n* Define a new model, `vis_model`, that takes the same input as our model's input but outputs activations of all the intermediate layers we have selected. This will be used for displaying all the activation maps of each convolution block for any sample image from our validation data. Check `visualize_intermediate_activations()` function for more details. \n*  For `CAM`, we will take the same sample image and get the output of the `last convolution layer`. We also compute the gradients for this layer which we will use to generate a heatmap. See the `get_CAM()` function for details.\n","e2d2dd8a":"<a id=\"conclusion\"><\/a>\n## 9. Conclusion\n\nMachine learning models aren't pure black box. You have to figure out how you can make the predictions interpretable. It is not an easy task at all but the on-going research in this area will lead to better ways of interpretability in the nearby future. \n\nI hope you enjoyed the kernel. Let me know if you have any suggestions. Happy Kaggling!","779d7ca3":"<a id=\"contents\"><\/a>\n# Contents\n1. [Import the required libraries](#imports).\n2. [Do everything we can to make our results reproducible](#reproducibility) <br>\n3. [Load dataset and process it](#dataloading)<br>\n4. [Data Augmentation](#augmentation) <br>\n5. [Data Generator](#data-gen)<br>\n6. [Modelling](#modelling)<br>\n7. [Model Interpretability](#interpretability)<br>\n8. [Can we or should we use CAM for model interpretability?](#infer_results)\n9. [Conclusion](#conclusion)<br>","b0a50443":"Hello Kagglers! It has been a while since I made a new kernel. The last kernel was on [Kaggle Survey Challenge 2018](https:\/\/www.kaggle.com\/aakashnain\/is-it-better-than-2017\/). \n\nToday, we are going to deep dive into `Model Interpretability`. Machine learning models, especially **Deep Learning** models are often considered as a black box and hard to interpret. Well, this statement is neither completely true not it is completely false. It is a fact that debugging a deep learning model is way harder than other machine learning models but there are ways by which you can get `insights` about your model and to an extent, you can see what is happening.  Let's get started.   \n\n![Do you have any idea what are you doin?](http:\/\/media.giphy.com\/media\/SRx5tBBrTQOBi\/giphy.gif)"}}