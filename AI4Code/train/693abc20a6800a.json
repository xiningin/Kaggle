{"cell_type":{"a765b1dc":"code","eb3a0c42":"code","dd52ec84":"code","f6196ac9":"code","dcfb3c51":"code","7a5361d7":"code","e58018db":"code","cbe4c606":"code","f5b928d7":"code","4567757c":"code","8bc3a022":"code","6ca18293":"code","0275e6e8":"code","ae252fb6":"code","83c0c873":"code","3f49cdb1":"code","04bd2abb":"code","3af8e3d3":"code","ea15dfd2":"code","1c5d6fe5":"code","48f326e0":"code","95dba6ca":"code","114c3441":"code","ef55b83d":"code","58cec44f":"code","d90b6fb7":"code","491457ab":"code","2a223788":"code","b4132829":"code","854d570e":"code","269ab1b1":"code","0fa0e979":"markdown","96ebe85d":"markdown","cbe1cee7":"markdown","168739e3":"markdown","0e27beca":"markdown","cae40f15":"markdown","124394ca":"markdown","247dc94a":"markdown","c2387a07":"markdown","7ac49839":"markdown","c74e9b14":"markdown","380f37c5":"markdown","a2375aab":"markdown","9ab8dfd1":"markdown","4b4dca84":"markdown","25f86c02":"markdown","2682bcb1":"markdown","32ef1f4c":"markdown","b0d27e49":"markdown","7bb0e954":"markdown","87bd6e2c":"markdown"},"source":{"a765b1dc":"import warnings\nwarnings.filterwarnings('ignore')\n# To  collect garbage (delete files)\nimport gc\n# To save dataset as pcikle file for future use\nimport pickle\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# for basic math operations like sqrt\nimport math\n\n\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import plot_importance\nfrom sklearn.linear_model import LinearRegression\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\n            \nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nprint(os.listdir(\"..\/input\/rapido\"))","eb3a0c42":"input_data = pd.read_csv(\"..\/input\/rapido-rides\/ct_rr.csv\")\ninput_data.shape","dd52ec84":"print(\"Data size before removing: \",input_data.shape)\n\n# Check duplicated rows in train set\ndf = input_data[input_data.duplicated()]  # checks duplicate rows considering all columns\nprint(\"Number of duplicate observations: \", len(df))\ndel df\ngc.collect();\n\n#Dropping duplicates and keeping first occurence only\ninput_data.drop_duplicates(keep = 'first', inplace = True)\n\nprint(\"Data size after removing: \",input_data.shape)","f6196ac9":"input_data['ts'] = pd.to_datetime(input_data['ts'], format='%Y%m%')\ninput_data['ts'] = input_data['ts'].dt.date   # converting column Month to desired Year and month format and having datetime datatype","dcfb3c51":"input_data['ts'] = pd.to_datetime(input_data['ts'])","7a5361d7":"input_data.head()","e58018db":"ab = input_data.groupby(['ts']).number.count()\nab = ab.reset_index()\nab.head()","cbe4c606":"ab.tail()","f5b928d7":"ab = ab[ab['ts']<'2019-04-01']\nprint(ab.shape)\nab['number'].plot.line()","4567757c":"ab = ab.set_index('ts').resample('D').ffill()\nab = ab.reset_index()","8bc3a022":"ab['number'].iloc[10:40].plot.line()","6ca18293":"rides = ab[['number']]","0275e6e8":"log_ridership = np.log(rides)\nlog_ridership.plot.line()","ae252fb6":"# 1st order differencing\nrider_single_diff = (log_ridership.diff()).dropna()  # 1st term will be NAN\n\n#NOTE: diff(diff(X)) is 2nd order differencing \nrider_double_diff = (rider_single_diff.diff()).dropna()  \n\n#seasonal differencing of order 1\nrider_single_seasonal_diff = (rider_single_diff.diff(periods=7)).dropna()  # 1st term will be NAN\nrider_double_seasonal_diff = (rider_double_diff.diff(periods=7)).dropna()  # 1st term will be NAN\n\nrider_single_diff.plot.line()\nplt.title('1st Order Diff')\nplt.show()\n\nrider_double_diff.plot.line()\nplt.title('2nd Order Diff')\nplt.show()\n\nrider_single_seasonal_diff.plot.line()\nplt.title('1st Order Seasonal Diff with seasonality of 7 days')\nplt.show()","83c0c873":"#Perform Dickey\u2013Fuller test:\nprint('Results of Dickey Fuller Test:')\ndftest = adfuller(rider_double_seasonal_diff.number, autolag='AIC') #Note: the input should not be a dataframe but a panda series\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","3f49cdb1":"#ACF and PACF plots:\n\n# Here d=1, D=1 and log transformed\nimport statsmodels.api as sm\nsm.graphics.tsa.plot_acf(rider_single_seasonal_diff.values.squeeze(), lags=40)\nplt.title('ACF')\nplt.show()\n\nsm.graphics.tsa.plot_pacf(rider_single_seasonal_diff.values.squeeze(), lags=40)\nplt.title('PACF')\nplt.show()","04bd2abb":"#for our model we need dates as indexes\nab = ab.set_index('ts')\n\n#doing log transformation on main data\nab['number'] = np.log(ab[['number']])","3af8e3d3":"!pip install pmdarima","ea15dfd2":"from pmdarima.arima import auto_arima\nstepwise_model = auto_arima(ab['number'], start_p=1, start_q=1,\n                           max_p=2, max_q=1, m=7,\n                           start_P=1,max_P=2, seasonal=True,\n                           d=1, D=1, max_d = 2, max_D=2,trace=True,\n                           error_action='ignore',  \n                           suppress_warnings=True, \n                           stepwise=True)\nprint(stepwise_model.aic())","1c5d6fe5":"# Applying Seasonal ARIMA model to forcast the data \nmod = sm.tsa.SARIMAX(ab['number'], trend='n', order=(0,1,1), seasonal_order=(1,1,1,7))  #also play with \"trend\" argument\nresults = mod.fit()\nprint(results.summary())\n\nresults.plot_diagnostics(figsize=(15,12))\nplt.show()","48f326e0":"ab['number'].shape","95dba6ca":"ab['forecast'] = results.predict(start = 345, end= 359, dynamic= True)  \nab[['number', 'forecast']].iloc[-60:].plot(figsize=(12, 8))\nplt.show()","114c3441":"def forcasting_future_months(df, no_of_periods):\n    df_predict = df.reset_index()\n    mon = df_predict['ts']\n    mon = mon + pd.DateOffset(days = no_of_periods)\n    future_dates = mon[-no_of_periods -1:]\n    df_predict = df_predict.set_index('ts')\n    future = pd.DataFrame(index=future_dates, columns= df_predict.columns)\n    df_predict = pd.concat([df_predict, future])\n    df_predict['forecast'] = results.predict(start = 359, end = 390, dynamic= True)  \n    df_predict[['number', 'forecast']].iloc[-60:].plot(figsize=(12, 8))\n    plt.show()\n    return df_predict[-no_of_periods:]","ef55b83d":"predicted = forcasting_future_months(ab,30)  #insert dataframe name and number of period for which to forecast","58cec44f":"ab = ab.apply(np.exp)\nforecast = predicted.apply(np.exp)\nfinal = ab.append(forecast)\nfinal[['number', 'forecast']].plot(figsize=(12, 8))","d90b6fb7":"final.tail(4)","491457ab":"# Sum of num of rides in April\napril_rides = final.loc[final.index>'2019-03-31'].forecast.sum()\nprint(round(april_rides))","2a223788":"per_month_rides =ab.groupby([(ab.index.year),(ab.index.month)]).number.sum()","b4132829":"rides_till_march = pd.Series(per_month_rides.values)\nrides_till_april = rides_till_march.append(pd.Series(april_rides))\nrides_till_april = rides_till_april.reset_index(drop = True) ","854d570e":"rides_till_april","269ab1b1":"plt.scatter(rides_till_april.index, rides_till_april)\nplt.show()","0fa0e979":"Note that there might be cases that observations for many dates is not available. We need to have continuous data therefore we will fill those data using previous\/forward value and then only perform further analysis.","96ebe85d":"Observations from above graphs:\n\n1) ACF: ACF plot helps us to determine MA(q). We can see that most of the correlations are just noises(major is for lag-1, therefore q=1). There is a significant spike at lag 7 which indicates Q=1 for seasonality of 7 days.\n\n2) PACF: It helps to determine AR(p). It has same story here as well, p=1\/2 and P=1. Note that correlation around around 14 are also significant therefore we can try P=2.\n\nTherefore combinations to try:\n\np=1, P=1, q=1, Q=1, d=1, D=1\n\np=1, P=2, q=1, Q=1, d=1, D=1\n\np=2, P=1, q=1, Q=1, d=1, D=1\n\np=2, P=2, q=1, Q=1, d=1, D=1","cbe1cee7":"From above log we can see the best paramteres are ARIMA(0,1,1)x(1,1,1,7) which produces minimum AIC. Now we put those parameters in SARIMAX package to get few more evaluation criteria.","168739e3":"From the above graph it can be inferred that the seasonality is of 7 days.","0e27beca":"This log transformation has reduced variance and now upward trend is better visible.","cae40f15":"# To generate future forecasts","124394ca":"# Forecasting","247dc94a":"For using auto arima, you need to install \"pmdarima package\" instead of older version \"pyramid\". Note: Enable Internet On option to install it if using Kaggle notebooks.\n\nWhy am I using auto arima?\n\nThere are different combinations to be tried and compare the results. Auto-arima makes that very easy.","c2387a07":"From the above Q-Q plot we can see that there is deviation from line at start and end but the data points are very less. It might be that the data isnot correct. The p-values of coff. is very low which implied the coeff. are significantly different than zero.\n\nNote that the Prob(Q) is 0 which says that it rejects null hypothesis that  there is no correlation in the data. Ideally from the selected paramters our model should have high Prob(Q), for now we will make peace with it.","7ac49839":"Observations from above 3 graphs:\n\nGraph A: 1st order differencing We can see that it has removed upward trend. However, the series is still not stationary as it is showing seasonal behavior. Therefore, we will have to remove it.\n\nGraph B: 2nd order differencing There is not much difference w.r.t 1st order. So we will keep our model relatively simple and go aheaf with 1st order diff i.e d=1\n\nGraph C: Taking seasonal difference of 1st order diff series has removed both upward trend and seasonality. Therefore, d=1 and D=1 and m=7(seasonal order). Note even if you don't take seasonal difference, the series might pass stationarity test but by this method would yield better results.\n\nNow we have to check if it passes the Dickey-Fuller test. I am hopeful it will!","c74e9b14":"# Log transformation\n\nTo reduce variance","380f37c5":"# Periodicity and Autocorrelation\n\nAuto correlation is the most famous way to understand seasonal variation till now. We can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an autocorrelation.In this plot vertical axis is represented by the following equations:-\n\nCn=\u2211n\u2212ht=1(y(t)\u2212y^)(y(t+n)\u2212y^)\/n \n\nC0=\u2211nt=1(y(t)\u2212y^)2\/n \n\nHorizontal axis represents time lag(previous time steps) h.\n\nNote that the ACF and PACF plots help us to find (p,q)X(P,Q). Therefore we will use log transformed series with d=1, D=1. However, if you want to confirm the seasonality order then you can skip doing seasonal difference and then you will see seasonal patterns in those plots.","a2375aab":"# Analyzing month wise sales","9ab8dfd1":"Finding monthly sales by grouping by month and year available in index","4b4dca84":"Input previously saved data from part 1 of this assignment. This data contains basic features like distance, time etc.","25f86c02":"The #rides for April seems correctly following the trend.\n\nThis result is better than results obtained using XGboost and Neural networks. To see those results click: https:\/\/www.kaggle.com\/jatinmittal0001\/rapido-2","2682bcb1":"I will make time series model at daily level since then I will have decent number of observations to build on. I am not making at monthly level since then there will be only 13 observations. ","32ef1f4c":"Great! We can see that p-value is very small. Smaller than inverse of speed of light!\n\nTherefore, I am happy to declare that our series is now stationary. So we proceed to model it.\n\nNow there are different packages available in python for that. Following are two most commonly used ones:\n\n1) ARIMA(): In this you need to pass a series and order p, d, q. It will fit and generate forecast. To generate forecasts, here we first explicitly need to create dates on which to make future predictions. Also, It doesn't have in-bult option to take seasonal difference therefore you have to first take that difference and then pass it and after generating predictions, reverse engineer the difference part which is a little lengthy process.\n\n2) SARIMAX(): In this you need to pass series and order (p,d,q)x(P,D,Q)x(m). It will fit and generate forecasts. Here we have flexibility to simply define the number of period over which future predictions are to be made w\/o manually creating dates at initial stages.\n\nNOTE: In both, since they don't have in-built option to do log transformation, we have to pass log transformed series and later reverse engineer the predictions to get meaningful values.","b0d27e49":"# Trend Removal\n\nNow we see an upward trend, so we will use most common method of differenceing(order 1) i.e. with previous term to remove trend.","7bb0e954":"We have 366 observations to build model on. There is very slight upward trend in the data. Also, the variance in laster months is large than in initial months. Although we are building single model on entire data. One way to have better accurates could be to build 2\/3 models for different portions and generate forecats for desired months and can take mean value of forecast. Here we restrict ourselves to build only 1 model.\n\nThe seasonality period is clearly visible by patterns but the time period for that is not clear. One way to get quick idea about seasonality period is to plot a portion of the data.","87bd6e2c":"The data for April is not complete, therefore I will not use April data at all to prevent the possibility of incorrect data."}}