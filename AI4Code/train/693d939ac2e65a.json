{"cell_type":{"4e49d5a1":"code","0f9f9b3a":"code","f676e9e3":"code","778f784d":"code","011ca0c1":"code","e760423d":"code","befb2ec8":"code","bb33d1b0":"code","8a3541aa":"code","f4632859":"code","8e3b53ed":"code","25faecff":"code","aef30df4":"code","e4b49e54":"code","f1ed2d8d":"code","e8dd602a":"code","1041d338":"code","cd589875":"code","c9bbaf76":"code","f7d1de0e":"code","827715a6":"code","f618e912":"code","4f1f7fab":"code","a47b220f":"code","e8f62d2a":"code","e33ba0e1":"code","a605e6af":"code","ae25475c":"code","ef9aec72":"code","85894708":"code","2d51d119":"code","4d69a614":"code","378650f1":"code","ae254a33":"code","f278b971":"code","eb65712a":"code","e57da684":"code","e9872790":"code","4853365b":"code","056c1175":"code","bc83288a":"code","0003c3f7":"code","e5d24070":"code","2cceedb6":"code","6fc2c3bd":"code","8b892785":"code","5be45493":"code","72b08c54":"code","ef63ff4d":"code","48a7d82a":"code","7c2541d3":"code","2c57275a":"code","c190a3a7":"code","44683188":"code","e6d63514":"code","7bead67b":"code","05d9cc59":"code","74d6d39d":"code","d49510fe":"code","1a03f482":"code","5bfda933":"code","e55a1d3c":"markdown","0f9b5c15":"markdown","cfa3b9f1":"markdown","4f0e9130":"markdown","6c323df0":"markdown","d651f804":"markdown","297e8153":"markdown","ab3ad66e":"markdown","9cfc437e":"markdown","03aacf9d":"markdown","2dabba02":"markdown","f73e1a58":"markdown","85d59206":"markdown","85a1fb4c":"markdown","9eaa5e7b":"markdown","0b915b9b":"markdown","ba2e5e3a":"markdown","44b1b159":"markdown","fc3e53dc":"markdown","519583ad":"markdown","eb1e4ce4":"markdown","3d548f0c":"markdown","4b86c3f2":"markdown","f10d85b6":"markdown","611a7a7e":"markdown","2fa258c8":"markdown","1873cf6e":"markdown","1ef0fbb3":"markdown","2be5b62d":"markdown","801799b9":"markdown"},"source":{"4e49d5a1":"%matplotlib inline\nRANDOM_STATE = 0\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\npd.set_option('display.float_format', '{:.3f}'.format)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, ParameterGrid, StratifiedKFold\nfrom sklearn import metrics\nfrom tqdm import tqdm\n\nimport eli5 \nfrom eli5.sklearn import PermutationImportance\n\nimport itertools\n\nimport catboost as cb\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool","0f9f9b3a":"def summary(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary","f676e9e3":"def plot_cf_matrix_and_roc(model, \n                           X_train, \n                           y_train,\n                           X_test, \n                           y_test,\n                           y_pred, \n                           classes=[0,1],\n                           normalize=False,\n                           cmap=plt.cm.Blues):\n    metrics_list = []\n    \n    # the main plot\n    plt.figure(figsize=(15,5))\n\n    # the confusion matrix\n    plt.subplot(1,2,1)\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    \n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        plt.title(\"Normalized confusion matrix\")\n    else:\n        plt.title('Confusion matrix')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, format(cm[i, j]),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    # the result metrix\n    summary_df = pd.DataFrame([[str(np.unique( y_pred )),\n                               str(round(metrics.precision_score(y_test, y_pred.round()),3)),\n                               str(round(metrics.accuracy_score(y_test, y_pred.round()),3)),\n                               str(round(metrics.recall_score(y_test, y_pred.round(), average='binary'),3)),\n                               str(round(metrics.roc_auc_score(y_test, y_pred.round()),3)),\n                                str(round(metrics.cohen_kappa_score(y_test, y_pred.round()),3)),\n                               str(round(metrics.f1_score(y_test, y_pred.round(), average='binary'),3))]], \n                              columns=['Class', 'Precision', 'Accuracy', 'Recall', 'ROC-AUC', 'Kappa', 'F1-score'])\n    # print the metrics\n    print(\"\\n\");\n    print(summary_df);\n    print(\"\\n\");\n    \n    plt.show()","778f784d":"def cross_val(X, y, param, cat_features='', class_weights = '', n_splits=3):\n    results = []\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    for tr_ind, val_ind in skf.split(X, y):\n        X_train_i = X.iloc[tr_ind]\n        y_train_i = y.iloc[tr_ind]\n        \n        X_valid_i = X.iloc[val_ind]\n        y_valid_i = y.iloc[val_ind]\n        \n        if class_weights == '' :\n            clf = CatBoostClassifier(iterations=param['iterations'],\n                            loss_function = param['loss_function'],\n                            depth=param['depth'],\n                            l2_leaf_reg = param['l2_leaf_reg'],\n                            eval_metric = param['eval_metric'],\n                            leaf_estimation_iterations = 10,\n                            use_best_model=True,\n                            logging_level='Silent',\n                            od_type=\"Iter\",\n                            early_stopping_rounds=param['early_stopping_rounds']\n            )\n        else:\n            clf = CatBoostClassifier(iterations=param['iterations'],\n                            loss_function = param['loss_function'],\n                            depth=param['depth'],\n                            l2_leaf_reg = param['l2_leaf_reg'],\n                            class_weights = class_weights,\n                            eval_metric = param['eval_metric'],\n                            leaf_estimation_iterations = 10,\n                            use_best_model=True,\n                            logging_level='Silent',\n                            od_type=\"Iter\",\n                            early_stopping_rounds=param['early_stopping_rounds']\n            )\n        \n        \n        if cat_features == '' :\n            clf.fit(X_train_i, \n                    y_train_i,\n                    eval_set=(X_valid_i, y_valid_i)\n            )\n        else:\n            clf.fit(X_train_i, \n                    y_train_i,\n                    cat_features=cat_features,\n                    eval_set=(X_valid_i, y_valid_i)\n            )\n        \n        # predict\n        y_pred = clf.predict(X_valid_i)\n        \n        # select the right metric\n        if(param['eval_metric'] == 'Recall'):\n            metric = metrics.recall_score(y_valid_i, y_pred)\n        elif(param['eval_metric'] == 'Accuracy'):\n            metric = metrics.accuracy_score(y_valid_i, y_pred)\n        elif(param['eval_metric'] == 'F1'):\n            metric = metrics.f1_score(y_valid_i, y_pred)\n        elif(param['eval_metric'] == 'AUC'):\n            metric = metrics.roc_auc_score(y_valid_i, y_pred)\n        elif(param['eval_metric'] == 'Kappa'):\n            metric = metrics.cohen_kappa_score(y_valid_i, y_pred)\n        else:\n            metric = metrics.accuracy_score(y_valid_i, y_pred)\n        \n        # append the metric\n        results.append(metric)\n        \n        print('Classes: '+str(np.unique( y_pred )))\n        print('Precision: '+str(round(metrics.precision_score(y_valid_i, y_pred.round()),3)))\n        print('Accuracy: '+str(round(metrics.accuracy_score(y_valid_i, y_pred.round()),3)))\n        print('Recall: '+str(round(metrics.recall_score(y_valid_i, y_pred.round(), average='binary'),3)))\n        print('Roc_Auc: '+str(round(metrics.roc_auc_score(y_valid_i, y_pred.round()),3)))\n        print('F1 score: '+str(round(metrics.f1_score(y_valid_i, y_pred.round(), average='binary'),3)))\n        print('Mean for '+param['eval_metric']+' OOF prediction: ',np.mean(results))\n        print('Standard deviation for '+param['eval_metric']+' OOF prediction: ',np.std(results))\n        print(\"\\n\")\n    return sum(results)\/n_splits","011ca0c1":"def catboost_GridSearchCV(X, y, params, cat_features='', class_weights='', n_splits=5):\n    ps = {'score':0,'param': []}\n    for prms in tqdm(list(ParameterGrid(params)), ascii=True, desc='Params Tuning:'):\n        score = cross_val(X, y, prms, cat_features, class_weights, n_splits)\n        if score > ps['score']:\n            ps['score'] = score\n            ps['param'] = prms\n    print('Score: '+str(ps['score']))\n    print('Params: '+str(ps['param']))\n    return ps['param']","e760423d":"def check_target(df, target):\n    sns.countplot(df[target])\n    count_no = len(df[df[target]==0])\n    count_yes = len(df[df[target]==1])\n    pct_of_no_sub = count_no\/(count_no+count_yes)*100\n    pct_of_sub = count_yes\/(count_no + count_yes)*100\n    print('{} {} % YES '.format(count_yes, pct_of_sub))\n    print('{} {} % NO '.format(count_no, pct_of_no_sub))","befb2ec8":"def num_vs_ctr(df, var1, var2):\n    ctr = df[[var1, var2]].groupby(var1, as_index=False).mean().sort_values(var2, ascending=False)\n    count = df[[var1, var2]].groupby(var1, as_index=False).count().sort_values(var2, ascending=False)\n    merge = count.merge(ctr, on=var1, how='left')\n    merge.columns=[var1, 'count', 'ctr%']\n    return merge\n\ndef crosstab(df, features, target, label_cutoff = 'none'):\n    for feature in features:\n        if(label_cutoff != 'none' and label_cutoff > 0):\n            # how many uninques\n            unique_elements = data[feature].nunique()\n            \n            # if we have more uniques then the cutoff\n            if(unique_elements > label_cutoff):\n                # select the number most common values\n                most_common_values = df.groupby(feature)[target].count().sort_values(ascending=False).nlargest(label_cutoff)\n                # add another value \"Other\"\n                df[feature] = np.where(df[feature].isin(most_common_values.index), df[feature], 'Other')\n        \n        # plot the crosstab\n        pd.crosstab(df[feature],df[target]).plot(kind='bar', figsize=(20,5), stacked=True)\n        plt.title(feature+' \/ '+target)\n        plt.xlabel(feature)\n        plt.ylabel(feature+' \/ '+target)\n            \n        # display the table obove each chart \n        return num_vs_ctr(df, feature, target)   \n        ","bb33d1b0":"## Import the data file\ndata = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","8a3541aa":"data.head()","f4632859":"summary(data)","8e3b53ed":"data['Pregnancies'].describe()","25faecff":"crosstab(data, ['Pregnancies'], 'Outcome')","aef30df4":"data['Was_pregnant'] = 'No'\ndata.loc[data.Pregnancies > 0, 'Was_pregnant'] = 'Yes'","e4b49e54":"crosstab(data, ['Was_pregnant'], 'Outcome')","f1ed2d8d":"data['Glucose'].describe()","e8dd602a":"plot = sns.boxplot('Glucose',data=data,orient = 'v',color='green')","1041d338":"data.Glucose = data.Glucose.replace(0,data.Glucose.mean())","cd589875":"plot = sns.boxplot('Glucose',data=data,orient = 'v',color='green')","c9bbaf76":"crosstab(data, ['Glucose'], 'Outcome')","f7d1de0e":"data['Glucose_Mmol'] = data['Glucose']*0.0555","827715a6":"crosstab(data, ['Glucose_Mmol'], 'Outcome')","f618e912":"data['Glucose_Mmol_Range'] = 'Normal'\ndata.loc[data.Glucose_Mmol < 3.9, 'Glucose_Mmol_Range'] = 'Hypoglycemia'\ndata.loc[((data.Glucose_Mmol >= 3.9) & (data.Glucose_Mmol < 7)), 'Glucose_Mmol_Range'] = 'Normal'\ndata.loc[((data.Glucose_Mmol >= 7)), 'Glucose_Mmol_Range'] = 'Hyperglycemia'","4f1f7fab":"crosstab(data, ['Glucose_Mmol_Range'], 'Outcome')","a47b220f":"data['BloodPressure'].describe()","e8f62d2a":"plot = sns.boxplot('BloodPressure',data=data,orient = 'v',color='green')","e33ba0e1":"data.BloodPressure = data.BloodPressure.replace(0,data.BloodPressure.mean())","a605e6af":"plot = sns.boxplot('BloodPressure',data=data,orient = 'v',color='green')","ae25475c":"crosstab(data, ['BloodPressure'], 'Outcome')","ef9aec72":"data['BloodPressure_Range'] = 'Normal'\ndata.loc[data.BloodPressure < 80, 'BloodPressure_Range'] = 'Optimal'\ndata.loc[((data.BloodPressure >= 80) & (data.BloodPressure <= 84)), 'BloodPressure_Range'] = 'Normal'\ndata.loc[((data.BloodPressure >= 85) & (data.BloodPressure <= 89)), 'BloodPressure_Range'] = 'High normal'\ndata.loc[((data.BloodPressure >= 90) & (data.BloodPressure <= 99)), 'BloodPressure_Range'] = 'Grade 1 hypertension'\ndata.loc[((data.BloodPressure >= 100) & (data.BloodPressure <= 109)), 'BloodPressure_Range'] = 'Grade 2 hypertension'\ndata.loc[((data.BloodPressure >= 110)), 'BloodPressure_Range'] = 'Grade 3 hypertension'","85894708":"crosstab(data, ['BloodPressure_Range'], 'Outcome')","2d51d119":"crosstab(data, ['SkinThickness'], 'Outcome')","4d69a614":"data.SkinThickness = data.SkinThickness.replace(0,data.SkinThickness.mean())","378650f1":"crosstab(data, ['SkinThickness'], 'Outcome')","ae254a33":"crosstab(data, ['Insulin'], 'Outcome')","f278b971":"crosstab(data, ['DiabetesPedigreeFunction'], 'Outcome')","eb65712a":"data['BMI'].describe()","e57da684":"#data.BMI = data.BMI.replace(0,data.BMI.mean())","e9872790":"data['BMI_Range'] = 'Underweight'\ndata.loc[data.BMI < 18.5, 'BMI_Range'] = 'Underweight'\ndata.loc[((data.BMI >= 18.5) & (data.BMI < 24.9)), 'BMI_Range'] = 'Normal weight'\ndata.loc[((data.BMI >= 24.9) & (data.BMI < 29.9)), 'BMI_Range'] = 'Overweight'\ndata.loc[data.BMI >= 30, 'BMI_Range'] = 'Obese'","4853365b":"crosstab(data, ['BMI_Range'], 'Outcome')","056c1175":"crosstab(data, ['Age'], 'Outcome')","bc83288a":"summary(data)","0003c3f7":"#data['Age_BMI_Range']=data['Age'].astype('str')+'|'+data['BMI'].astype('str')\ndata['Age_Glucose_Mmol_Range']=data['Age'].astype('str')+'|'+data['Glucose_Mmol_Range'].astype('str')\ndata['Was_pregnant_BMI_Range']=data['Was_pregnant'].astype('str')+'|'+data['BMI_Range'].astype('str')\ndata['Was_pregnant_Insulin']=data['Was_pregnant'].astype('str')+'|'+data['Insulin'].astype('str')\n#data['Was_pregnant_SkinThickness']=data['Was_pregnant'].astype('str')+'|'+data['SkinThickness'].astype('str')\n#data['Age_Was_pregnant']=data['Age'].astype('str')+'|'+data['Was_pregnant'].astype('str')\n#data['Age_Insulin']=data['Age'].astype('str')+'|'+data['Insulin'].astype('str')\n#data['Age_SkinThickness']=data['Age'].astype('str')+'|'+data['SkinThickness'].astype('str')\n#data['Was_pregnant_Glucose']=data['Was_pregnant'].astype('str')+'|'+data['Glucose'].astype('str')\n#data['Age_BloodPressure_Range']=data['Age'].astype('str')+'|'+data['BloodPressure_Range'].astype('str')\n","e5d24070":"data = data.drop(columns=[\n    'Was_pregnant', \n    'Glucose'\n ])","2cceedb6":"X = data.drop('Outcome', 1)\ny = data['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=47)\nX_train.shape, X_test.shape","6fc2c3bd":"train_df = pd.concat([X_train, y_train], axis=1)\ncheck_target(train_df, 'Outcome')","8b892785":"test_df = pd.concat([X_test, y_test], axis=1)\ncheck_target(test_df, 'Outcome')","5be45493":"summary(data)","72b08c54":"cat_features=[i for i in X_train.columns if ((X_train.dtypes[i]!='int64') & (X_train.dtypes[i]!='float64'))]\ncat_features","ef63ff4d":"bool_features=[i for i in X_train.columns if ((X_train.dtypes[i]=='int64') & (len(X_train[i].unique()) == 2))]\nbool_features","48a7d82a":"num_features=[i for i in X_train.columns if ((X_train.dtypes[i]=='int64') & (len(X_train[i].unique()) > 2))]\nnum_features","7c2541d3":"from sklearn.utils import class_weight\ncw = list(class_weight.compute_class_weight('balanced',\n                                             np.unique(data['Outcome']),\n                                             data['Outcome']))","2c57275a":"params = {'depth':[2, 3, 4, 5],\n          'iterations':[1500],\n          'loss_function': ['Logloss'],\n          'l2_leaf_reg':np.logspace(-19,-20,3),\n          'early_stopping_rounds': [500],\n          'learning_rate':[0.01],\n          'eval_metric':['F1']\n}\n\n# parameter tuning\n#param = catboost_GridSearchCV(X_train, y_train, params, cat_features, cw)\n#param","c190a3a7":"# pre-optimized parameters\nparam = {'depth': 3,\n 'early_stopping_rounds': 500,\n 'eval_metric': 'F1',\n 'iterations': 1500,\n 'l2_leaf_reg': 1e-19,\n 'learning_rate': 0.01,\n 'loss_function': 'Logloss',\n 'leaf_estimation_iterations': 10\n}\n\n# create the model\nclf2 = CatBoostClassifier(iterations=param['iterations'],\n                        loss_function = param['loss_function'],\n                        depth=param['depth'],\n                        l2_leaf_reg = param['l2_leaf_reg'],\n                        eval_metric = param['eval_metric'],\n                        leaf_estimation_iterations = param['leaf_estimation_iterations'],\n                        use_best_model=True,\n                        early_stopping_rounds=param['early_stopping_rounds'],\n                        class_weights = cw\n)\n\n# train the model\nclf2.fit(X_train, \n        y_train,\n        cat_features=cat_features,\n        logging_level='Silent',\n        eval_set=(X_test, y_test)\n)","44683188":"feature_score = pd.DataFrame(list(zip(X_train.dtypes.index, clf2.get_feature_importance(Pool(X_train, label=train_df['Outcome'], cat_features=cat_features)))),\n                columns=['Feature','Score'])\n\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\nplt.rcParams[\"figure.figsize\"] = (15,8)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\n\nrects = ax.patches\n\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 0.35, label, ha='left', va='bottom')\nplt.xticks(rotation=85)\n\nplt.gca().invert_xaxis()\n\nplt.show()\nprint(feature_score)\n","e6d63514":"pred_catboost2_train = clf2.predict(X_train)","7bead67b":"plot_cf_matrix_and_roc(clf2, X_train, y_train, X_train, y_train, pred_catboost2_train , classes=['NO','YES'])","05d9cc59":"print(metrics.classification_report(y_train, pred_catboost2_train))","74d6d39d":"pred_catboost2_train = clf2.predict(X_test)","d49510fe":"plot_cf_matrix_and_roc(clf2, X_train, y_train, X_test, y_test, pred_catboost2_train , classes=['NO','YES'])","1a03f482":"print(metrics.classification_report(y_test, pred_catboost2_train))","5bfda933":"score = cross_val(X_train, y_train, param, cat_features, cw, 10)","e55a1d3c":"All the features in the data set are numerical. We will take deeper look at them and check for outliers, distribution and missing values.","0f9b5c15":"* Hypoglycemia: Blood sugar level < 3.9 mmol\/L (Source: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Hypoglycemia))\n* Hyperglycemia: Blood sugar level > 7 mmol\/L (Source: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Hyperglycemia))\n","cfa3b9f1":"The feature also contains values with 0, but these are not necessarily outlier because there are women who are not yet pregnant.","4f0e9130":"### Context\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n### Content\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n### Features\n* Pregnancies: Number of times pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skin fold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U\/ml)\n* BMI: Body mass index (weight in kg\/(height in m)^2)\n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: Class variable (0 or 1)","6c323df0":"### Check the distribution of the income for train, test and validation","d651f804":"## BloodPressure_Range\nLet's create a new feature for the BP levels, based on the ranges in Wikipedia for adults","297e8153":"### BMI\n* Underweight: BMI is less than 18.5\n* Normal weight: BMI is 18.5 to 24.9\n* Overweight: BMI is 25 to 29.9\n* Obese: BMI is 30 or more","ab3ad66e":"## Glucose_Mmol_Range\nLet's create a new feature for the glucose levels, based on the ranges in Wikipedia for adults","9cfc437e":"Let's check the correlation with the target variable 'Outcome'","03aacf9d":"## Was pregnant\nLet's create a new feature for if the women was pregnant or not. This feature could be helpful in interaction with other features like the BMI or Insuline.","2dabba02":"## SkinThickness\nwhat is this for a feature??","f73e1a58":"### **Interpretation**\n\nThe unit of measurement for the 2-hour OGTT in this dataset is assumed to be in milligrams per deciliter (mg\/dl). It can be converted to Milimoles per liter (mmol\/l) so that we may apply a qualitative test result to the numeric results. Multiplying the current results by 0.0555 will convert them to be measured in mmol\/l.","85d59206":"**Interpretation**\n* Women with pregnancy has a bit higher risk for diabetes\n* Also the data set is very unbalanced, because we have 5 times more women with pregnancy","85a1fb4c":"## Insulin\nhow is this feature defined","9eaa5e7b":"The model seems not so stable. The standard deviation for F1 is 0.07 - not so good! ","0b915b9b":"## Pregnancies","ba2e5e3a":"It seems that there are some outliers with values equal to 0. We need to fix this and replace the 0 values with the mean value.","44b1b159":"### **Interpretation**\n\n- The data shows very clear, that people with Hyperglycemia has very high risk to develop diabetes","fc3e53dc":"We don't have any information if this is a Systolic BP or Diastolic BP. Becuase the range of the BP'a are between the 40 and 120, we assume, that this is Diastolic BP.","519583ad":"### **Interpretation**\n\nThe data set includes only females at least 21 years old of Pima Indian heritage. During the pregnancy women can develop **gestational diabetes**. It is high blood sugar (glucose) that develops during pregnancy and usually disappears after giving birth.\nIt can happen at any stage of pregnancy, but is more common in the second or third trimester.\n\nIn women with gestational diabetes, blood sugar usually returns to normal soon after delivery. But if you've had gestational diabetes, you have a higher risk of getting type 2 diabetes. You'll need to be tested for changes in blood sugar more often.\n\nIn our data set we see, that women with much more pregnancy has higher chance to develop diabetes. ","eb1e4ce4":"### **Interpretation**\n\n- The higher the level of blood sugar, the higher the chance of diabetes","3d548f0c":"It seems that there are some outliers with values equal to 0. We need to fix this and replace the 0 values with the mean value.","4b86c3f2":"## Cross validate the model","f10d85b6":"**Interpretation**","611a7a7e":"## BloodPressure\nBlood pressure (BP) is the pressure of circulating blood against the walls of blood vessels. Most of this pressure results from the heart pumping blood through the circulatory system. When used without qualification, the term \"blood pressure\" refers to the pressure in the large arteries. Blood pressure is usually expressed in terms of the systolic pressure (maximum pressure during one heartbeat) over diastolic pressure (minimum pressure between two heartbeats) in the cardiac cycle. \n\nSource [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Blood_pressure)\n\n","2fa258c8":"## DiabetesPedigreeFunction\nwhat is this??","1873cf6e":"Normal blood glucose level (tested while fasting) for non-diabetics is between 3.9 and 7.1 mmol\/L (70 to 130 mg\/dL). The global mean fasting plasma blood glucose level in humans is about 5.5 mmol\/L (100 mg\/dL); however, this level fluctuates throughout the day. Blood sugar levels for those without diabetes and who are not fasting should be below 6.9 mmol\/L (125 mg\/dL). The blood glucose target range for diabetics, according to the American Diabetes Association, should be 5.0\u20137.2 mmol\/l (90\u2013130 mg\/dL) before meals, and less than 10 mmol\/L (180 mg\/dL) two hours after meals (as measured by a blood glucose monitor). \nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Blood_sugar_level)","1ef0fbb3":"## Glucose\nThe blood sugar level, blood sugar concentration, or blood glucose level is the concentration of glucose present in the blood of humans and other animals.","2be5b62d":"### **Interpretation**\n\n- The data shows very clear, that people with hypertension 1, 2 or 3 has very high risk to develop diabetes","801799b9":"## Split train and test"}}