{"cell_type":{"eddf9c83":"code","4f7fc203":"code","2b83b9ff":"code","2c7c7d54":"code","36d9596a":"code","fc61fa0a":"code","4aab21ff":"code","7855c8de":"code","3536f0ce":"code","116e2c96":"code","099a6866":"code","387739ab":"code","c622a09f":"code","f9f9e265":"code","32546d20":"code","027213d9":"code","f048a1cb":"code","6feb4cc0":"code","59bcde39":"code","1685a612":"code","e71bb4be":"code","116de34b":"code","6afa7e6b":"code","0ad26776":"code","77fbd846":"code","c7ef8acf":"code","b1e95698":"code","2ca8e52f":"code","ce66a2a5":"code","dab9d65e":"code","1c5d7087":"code","ad886e6e":"code","9538c164":"code","364abc31":"code","5e2b30e8":"code","29cce77d":"code","b672e01d":"code","7b57f25b":"code","a809304c":"code","ed9474fb":"code","bdf49817":"code","0ba10297":"code","8937e5c2":"code","31db94b9":"code","62749514":"code","97396112":"code","94617e00":"code","6bdf4f25":"code","b7b7d77c":"code","5e3e2fae":"code","6698a63c":"code","318261ee":"code","3a3a0b39":"code","9a3332aa":"code","be66528e":"code","a556bb06":"code","fed4eb29":"code","746313ee":"code","ed008db8":"code","a762a17a":"code","249e1c1e":"markdown","43de85ad":"markdown","a582d2df":"markdown","ee96f61b":"markdown","939a7c0e":"markdown","b49b1af8":"markdown","98bf224e":"markdown","378a5cee":"markdown","988eb9b3":"markdown","a964ab79":"markdown","65669d0c":"markdown","e991c7a5":"markdown","5d98c86d":"markdown","9b7f3822":"markdown","d85fbc81":"markdown","f8211af5":"markdown","1688f93f":"markdown","37678a50":"markdown","d597f716":"markdown","33049f69":"markdown","c07b364b":"markdown","e6c9f5c8":"markdown","d1890c76":"markdown","8e69f1a5":"markdown","b28bb063":"markdown","c9c18a5f":"markdown","194e51ba":"markdown","60ea4cd5":"markdown"},"source":{"eddf9c83":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re, string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nimport spacy\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier","4f7fc203":"data1 = pd.read_csv('\/kaggle\/input\/consumer-reviews-of-amazon-products\/1429_1.csv')\ndata1.head()","2b83b9ff":"data1.info()","2c7c7d54":"data1['reviews.rating'].value_counts()","36d9596a":"data2 = pd.read_csv(\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\")\ndata2 = data2[['reviews.rating' , 'reviews.text']]\ndata2 = data2[data2[\"reviews.rating\"]<=3]\n\ndata3 = pd.read_csv(\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\ndata3 = data3[['reviews.rating' , 'reviews.text']]\n","fc61fa0a":"data2['reviews.rating'].value_counts()","4aab21ff":"data3['reviews.rating'].value_counts()","7855c8de":"data2 = data2[data2[\"reviews.rating\"]<=3]\ndata3 = data3[data3[\"reviews.rating\"]<=3]","3536f0ce":"data=pd.concat([data1, data2, data3])","116e2c96":"df=pd.concat([data['reviews.text'],data['reviews.rating']], axis=1)\ndf.head()","099a6866":"df","387739ab":"df.info()","c622a09f":"df['reviews.rating'].value_counts()","f9f9e265":"df.dropna(inplace=True)","32546d20":"df=df.reset_index()","027213d9":"df.info()","f048a1cb":"df","6feb4cc0":"df.drop(columns=['index'],inplace=True)","59bcde39":"df","1685a612":"df['reviews.rating'].value_counts()","e71bb4be":"sentiment = {1: 0,\n            2: 0,\n            3: 0,\n            4: 1,\n            5: 1}\ndf['sentiment']=df['reviews.rating'].map(sentiment)","116de34b":"df['sentiment'].value_counts()","6afa7e6b":"counts=df['sentiment'].value_counts()\nplt.bar(counts.index, counts.values)\nplt.show()","0ad26776":"df[df['reviews.rating']==4]['reviews.text']","77fbd846":"list(df[df['reviews.rating']==3]['reviews.text'])[0:5]","c7ef8acf":"words = pd.Series(' '.join(df['reviews.text']).split())","b1e95698":"wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(words))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Most Used Words\")\nplt.axis(\"off\")\nplt.show()","2ca8e52f":"positiveWords=words = pd.Series(' '.join(df[df['sentiment']==1]['reviews.text']).split())","ce66a2a5":"wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(positiveWords))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Most Positive Words Used \")\nplt.axis(\"off\")\nplt.show()","dab9d65e":"negativeWords=words = pd.Series(' '.join(df[df['sentiment']==0]['reviews.text']).split())","1c5d7087":"wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(negativeWords))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Most Negative Words Used \")\nplt.axis(\"off\")\nplt.show()","ad886e6e":"def cleanString(text):\n    text = text.lower()\n    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(' \\d+', ' ', text)\n    return text\ncleanString('This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease.')\n","9538c164":"stopword_list = stopwords.words('english')\nsuitable_stopwords=[]\nl =[\"n'\",'nor','no','not']\nfor i in stopword_list:\n    if not any(word in i for word in l):\n        suitable_stopwords.append(i)\nprint(stopword_list)\nprint(suitable_stopwords)","364abc31":"suitable_stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n                    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n                    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n                    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n                    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n                    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because',\n                    'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n                    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n                    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n                    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only',\n                    'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\",\n                    'd', 'll', 'ma', 'm', 'o', 're', 've', 'y']\nreplace_list=['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'mightn', 'mustn', 'needn',\n              'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',\"n't\",\"n'\"]","5e2b30e8":"\ndef remove_stopwords(stmt):\n    filtered_sentence = []\n    words = word_tokenize(stmt)\n    \n    for w in words:\n        if w not in suitable_stopwords:\n            if w not in replace_list:\n                filtered_sentence.append(w)\n            else:\n                filtered_sentence.append('not')\n    return \" \".join(filtered_sentence)\nremove_stopwords('this product so far has not disappointed my children love to use it and i like the ability to monitor control what content they see with ease ')","29cce77d":"# This is a helper function to map NTLK position tags\n# Full list is available here: https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \ndef lemmatize(text):\n    # Initialize the lemmatizer\n    wl = WordNetLemmatizer()\n    lemmatized_sentence = []\n    # Tokenize the sentence\n    words = word_tokenize(text)\n    # Get position tags\n    word_pos_tags = nltk.pos_tag(words)\n    # Map the position tag and lemmatize the word\/token\n    for idx, tag in enumerate(word_pos_tags):\n        lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n\n    return \" \".join(lemmatized_sentence)\n    \nlemmatize('product far not disappointed children love use like ability monitor control content see ease')","b672e01d":"df['text']=df['reviews.text'].apply(cleanString)","7b57f25b":"df['text']=df['text'].apply(remove_stopwords)","a809304c":"df['text']=df['text'].apply(lemmatize)","ed9474fb":"df['reviews.text'].iloc[0]","bdf49817":"df['text'].iloc[0]","0ba10297":"vectorizer = TfidfVectorizer(max_features=700)\nvectorizer.fit(df['text'])\nfeatures = vectorizer.transform(df['text'])\n\nfeatures.toarray()","8937e5c2":"df.iloc[0]","31db94b9":"tf_idf = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())","62749514":"tf_idf.head()","97396112":"tf_idf.iloc[0]['ease']","94617e00":"X_train, X_test, y_train, y_test = train_test_split(tf_idf, df['sentiment'], test_size=0.2, random_state=42)","6bdf4f25":"print ('Train Set Shape\\t\\t:{}\\nTest Set Shape\\t\\t:{}'.format(X_train.shape, X_test.shape))","b7b7d77c":"counts=y_test.value_counts()\nplt.title(\"Test Classes count\")\nplt.bar(counts.index, counts.values)\nplt.show()","5e3e2fae":"counts=y_train.value_counts()\nplt.title(\"Train Classes count\")\nplt.bar(counts.index, counts.values)\nplt.show()","6698a63c":"yy=pd.DataFrame(y_train)","318261ee":"train_data = pd.concat([X_train,yy],axis=1)","3a3a0b39":"train_data.head()","9a3332aa":"target_count = train_data['sentiment'].value_counts()\nnegative_class = train_data[train_data['sentiment'] == 0]\npositive_class = train_data[train_data['sentiment'] == 1]\nnegative_over = negative_class.sample(target_count[1], replace=True)\ndf_train_over = pd.concat([positive_class, negative_over], axis=0)\ndf_train_over = shuffle(df_train_over)\ndf_train_over.head()","be66528e":"df_train_over['sentiment'].value_counts()","a556bb06":"counts=df_train_over['sentiment'].value_counts()\nplt.title(\"Train Classes count after Oversampling\")\nplt.bar(counts.index, counts.values)\nplt.show()","fed4eb29":"X_train=df_train_over.iloc[:,:-1]\ny_train=df_train_over['sentiment']","746313ee":"def modeling(Model, Xtrain = X_train, Xtest = X_test):\n    \"\"\"\n    This function apply countVectorizer with machine learning algorithms. \n    \"\"\"\n    \n    # Instantiate the classifier: model\n    model = Model\n    \n    # Fitting classifier to the Training set (all features)\n    model.fit(Xtrain, y_train)\n    \n    global y_pred\n    # Predicting the Test set results\n    y_pred = model.predict(Xtest)\n    \n    # Assign f1 score to a variable\n    print(classification_report(y_test, y_pred))\n    print ('AUC ',roc_auc_score(y_test, y_pred))\n    #cm = confusion_matrix(y_test, y_pred)\n    confusion_matrix = pd.crosstab(index=y_test, columns=np.round(y_pred), rownames=['True'], colnames=['predictions']).astype(int)\n    plt.figure(figsize = (8,8))\n\n    '''\n    cmapGR = LinearSegmentedColormap.from_list(\n        name='test', \n        colors=['red','green']\n    )\n    '''\n    sns.heatmap(confusion_matrix, annot=True,annot_kws={\"fontsize\":12}, fmt='.2f', cmap='YlGnBu').set_title('Confusion Matrix') \n","ed008db8":"modeling(MultinomialNB())","a762a17a":"modeling(XGBClassifier());","249e1c1e":"## 2.XGBoost","43de85ad":"check indeses","a582d2df":"Leave oversampling after edit text","ee96f61b":"## Text Preprocessing","939a7c0e":"## Feature Engineering and Selection","b49b1af8":"## Modeling","98bf224e":"### See the most Used words for both classes","378a5cee":"#### Overall","988eb9b3":"## Oversampling","a964ab79":"### Lemmatize Text","65669d0c":"### Create TF-IDF","e991c7a5":"### Splitting Dataset into Train and Test Set","5d98c86d":"### Import necessary libraries","9b7f3822":"#### Train Classes count","d85fbc81":"# Sentiment Analysis (**TF-IDF**) Using Naive Bayes and XGBoost","f8211af5":"### Read the Dataset","1688f93f":"There is still big diffrence between them","37678a50":"#### Test Classes count","d597f716":"## 1.Naive Bayes","33049f69":"## Data Preprocessing","c07b364b":"So there is a huge imbalnce in the data to the high rate classes","e6c9f5c8":"### Clean Text","d1890c76":"> **Solution:** Adding more data with low rate classes","8e69f1a5":"#### Negative Class","b28bb063":"> need to reset index","c9c18a5f":"# Natural Language Processing","194e51ba":"### Remove Stopwords","60ea4cd5":"#### Positive Class"}}