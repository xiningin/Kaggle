{"cell_type":{"8393dc7c":"code","92198000":"code","1d8fa74e":"code","72c3e4ef":"code","64f73761":"code","fd840aa0":"code","6bc2e17d":"code","006c024f":"code","2c453fdf":"code","76644bef":"code","e9344797":"code","7aea3bee":"code","00dc9904":"code","48df1754":"code","454f4234":"code","79b8ed36":"code","e7fb8779":"code","ef9d2a6b":"code","bb96d06d":"code","6da0cc78":"code","de3b1601":"code","a47598c5":"code","6d6aaabd":"code","f2acb6a1":"markdown","0648bd79":"markdown","2df5d55f":"markdown","1941ef8c":"markdown","a2fb488c":"markdown","0d49d591":"markdown","3db0e5dd":"markdown","4b754974":"markdown","230a69b2":"markdown","99a6fe32":"markdown","f1e8f1f6":"markdown","f0cb7e3e":"markdown","f9cc4de7":"markdown","51851790":"markdown","78618e43":"markdown","be34641a":"markdown","d2c09646":"markdown","a024fad4":"markdown","3ce6710d":"markdown","54b32d64":"markdown","6fa1122b":"markdown","7a363daa":"markdown","b64a8887":"markdown","881fd115":"markdown","c47acd6b":"markdown","d55a8797":"markdown","14ca1b39":"markdown","b29f2bfd":"markdown"},"source":{"8393dc7c":"!pip install -q nlpretext loguru","92198000":"import os\nimport gc\nimport copy\nimport time\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport transformers\nfrom transformers import T5Tokenizer, T5EncoderModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom loguru import logger\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom nlpretext import Preprocessor\nfrom nlpretext.basic.preprocess import (normalize_whitespace, remove_punct, \n                                        remove_eol_characters, remove_stopwords, \n                                        lower_text, unpack_english_contractions)\n\nfrom colorama import Fore\nb_ = Fore.BLUE\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1d8fa74e":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.head()","72c3e4ef":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df.head()","64f73761":"preprocessor = Preprocessor()\npreprocessor.pipe(unpack_english_contractions)\npreprocessor.pipe(remove_eol_characters)\npreprocessor.pipe(lower_text)\npreprocessor.pipe(normalize_whitespace)","fd840aa0":"train_df['excerpt'] = train_df['excerpt'].apply(preprocessor.run)","6bc2e17d":"excerpt_lenghts = train_df['excerpt'].apply(lambda x: len(x.split()))\nmax(excerpt_lenghts)","006c024f":"class CONFIG:\n    seed = 42\n    max_len = 205\n    model_name = 't5-base'\n    hidden_state = 768\n    hidden_state_fixed = 768 # ONLY CHANGE WHEN CHANGING THE MODEL\n                             # 512 for t5-small, 768 for t5-base, 1024 for t5-large   \n    train_batch_size = 32\n    valid_batch_size = 32\n    epochs = 20\n    learning_rate = 1e-5\n    n_accumulate = 1\n    folds = 10\n    tokenizer = T5Tokenizer.from_pretrained(model_name)\n    tokenizer.save_pretrained('.\/tokenizer')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","2c453fdf":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","76644bef":"def create_folds(df, n_s=5, n_grp=None):\n    df['kfold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s, random_state=CONFIG.seed)\n        target = df.target\n    else:\n        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG.seed)\n        df['grp'] = pd.cut(df.target, n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'kfold'] = fold_no\n    return df","e9344797":"df = create_folds(train_df, n_s=CONFIG.folds, n_grp=12)\ndf.head()","7aea3bee":"class T5Dataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }","00dc9904":"def criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))","48df1754":"class T5Pooler(nn.Module):\n    def __init__(self, hidden_size, activation=nn.Tanh()):\n        super().__init__()\n        self.dense = nn.Linear(CONFIG.hidden_state_fixed, hidden_size)\n        self.activation = activation\n        \n    def forward(self, hidden_states):\n        # We simply take the mean of the hidden states\n        mean_tensor = torch.mean(hidden_states, dim=1)\n        pooled_output = self.dense(mean_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output","454f4234":"class T5Model(nn.Module):\n    def __init__(self):\n        super(T5Model, self).__init__()\n        self.t5 = T5EncoderModel.from_pretrained(CONFIG.model_name)\n        self.pooler = T5Pooler(CONFIG.hidden_state, nn.LeakyReLU())\n        self.fc = nn.Linear(CONFIG.hidden_state, 1)\n    \n    def forward(self, ids, mask):\n        outputs = self.t5(ids, attention_mask=mask)\n        pooled_outputs = self.pooler(outputs.last_hidden_state)\n        outputs = self.fc(pooled_outputs)\n        return outputs\n\nmodel = T5Model()\nmodel.to(CONFIG.device);","79b8ed36":"def train_one_epoch(model, optimizer, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(ids, mask)\n            loss = criterion(outputs, targets)\n            loss = loss \/ CONFIG.n_accumulate\n            \n        scaler.scale(loss).backward()\n        \n        if (step + 1) % CONFIG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # zero the parameter gradients\n            optimizer.zero_grad()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","e7fb8779":"@torch.no_grad()\ndef valid_one_epoch(model, optimizer, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        outputs = model(ids, mask)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        PREDS.extend(outputs.cpu().detach().numpy().tolist())\n        TARGETS.extend(targets.cpu().detach().numpy().tolist())\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n        \n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    gc.collect()\n    \n    return epoch_loss, val_rmse","ef9d2a6b":"@logger.catch\ndef run(model, optimizer, scheduler, device, num_epochs):    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, dataloader=train_loader, \n                                           device=CONFIG.device, epoch=epoch)\n        \n        valid_epoch_loss, valid_epoch_rmse = valid_one_epoch(model, optimizer,\n                                                       dataloader=valid_loader, \n                                                       device=CONFIG.device, epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n        history['Valid RMSE'].append(valid_epoch_rmse)\n        \n        print(f'Valid RMSE: {valid_epoch_rmse}')\n        \n        if scheduler is not None:\n            scheduler.step()\n        \n        # deep copy the model\n        if valid_epoch_rmse <= best_rmse:\n            print(f\"{b_}Validation RMSE Improved ({best_rmse} ---> {valid_epoch_rmse})\")\n            best_rmse = valid_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"RMSE{:.4f}_epoch{:.0f}.bin\".format(best_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_rmse))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","bb96d06d":"def prepare_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = T5Dataset(df_train, CONFIG.tokenizer, CONFIG.max_len)\n    valid_dataset = T5Dataset(df_valid, CONFIG.tokenizer, CONFIG.max_len)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch_size, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch_size, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","6da0cc78":"train_loader, valid_loader = prepare_data(fold=0)","de3b1601":"# Defining Optimizer with weight decay to params other than bias and layer norms\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]\n\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\n\n# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=CONFIG.epochs\n)","a47598c5":"model, history = run(model, optimizer, scheduler, device=CONFIG.device, num_epochs=CONFIG.epochs)","6d6aaabd":"epochs = list(range(1, CONFIG.epochs + 1))\nfig = go.Figure()\ntrace1 = go.Scatter(x=epochs, y=history['Train Loss'],\n                    mode='lines+markers',\n                    name='Train Loss')\ntrace2 = go.Scatter(x=epochs, y=history['Valid Loss'],\n                    mode='lines+markers',\n                    name='Valid Loss')\nlayout = go.Layout(template=\"plotly_dark\", title='Loss Curve', \n                   xaxis=dict(title='Epochs'), yaxis=dict(title='Loss'))\nfig = go.Figure(data = [trace1, trace2], layout = layout)\nfig.show()","f2acb6a1":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Visualizations \ud83d\udcc9<\/h1>","0648bd79":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">We will use <i>NLPretext<\/i> library for preprocessing our text<\/span>","2df5d55f":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Define Optimizer and Scheduler<\/span>","1941ef8c":"![](https:\/\/github.com\/artefactory\/NLPretext\/raw\/master\/references\/logo_nlpretext.png)","a2fb488c":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Run<\/h1>","0d49d591":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Required Libraries \ud83d\udcda<\/h1>","3db0e5dd":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Install Required Libraries<\/h1>","4b754974":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Let's Try T5<\/h1>\n<br>","230a69b2":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Validation Function<\/h1>","99a6fe32":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Loss Function<\/h1>","f1e8f1f6":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Code taken from <a href=\"https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?rvi=1&scriptVersionId=52551118&cellId=6\">https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?rvi=1&scriptVersionId=52551118&cellId=6<\/a><\/span>","f0cb7e3e":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Function<\/h1>","f9cc4de7":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Folds<\/h1>","51851790":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset Class<\/h1>","78618e43":"![](https:\/\/1.bp.blogspot.com\/-o4oiOExxq1s\/Xk26XPC3haI\/AAAAAAAAFU8\/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ\/s640\/image3.gif)","be34641a":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Configuration \u2699\ufe0f<\/h1>","d2c09646":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by <b>introducing a unified framework that converts all text-based language problems into a text-to-text format<\/b>. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ''Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.<\/span> <br>\n<br>\n<br>\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"><i>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer: <a href='https:\/\/arxiv.org\/abs\/1910.10683'>https:\/\/arxiv.org\/abs\/1910.10683<\/a><\/i><\/span>","a024fad4":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Read the Data \ud83d\udcd6<\/h1>","3ce6710d":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Preprocessing<\/h1>","54b32d64":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Model<\/h1>","6fa1122b":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","7a363daa":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Maximum Length of Text present in the Dataset<\/span>","b64a8887":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Create Dataloaders<\/span>","881fd115":"<h3>\ud83d\udccc BERT Baseline Notebook:<\/h3> <h4><a href='https:\/\/www.kaggle.com\/debarshichanda\/pytorch-commonlit-readability-bert-baseline\/'>https:\/\/www.kaggle.com\/debarshichanda\/pytorch-commonlit-readability-bert-baseline<\/a><\/h4>","c47acd6b":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Set Seed for Reproducibility<\/h1>","d55a8797":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">In this kernel we will be using the <code>T5EncoderModel<\/code> provided by Huggingface transformers library to get an encoded representation of the text and then use a <code>Linear<\/code> layer on top of it for getting our output<\/span>","14ca1b39":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">We need <code>T5Pooler<\/code> to pool the outputs of the model<\/span><br>\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">This is simply done by taking the mean of the hidden states <\/span>","b29f2bfd":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Train Fold: 0<\/h1>"}}