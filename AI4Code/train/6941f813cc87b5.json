{"cell_type":{"45a740ba":"code","563bd911":"code","d0436e45":"code","d1c8eb89":"code","7e042531":"code","04675f9e":"code","13716422":"code","84ed82db":"code","12f4a035":"code","61c1e21c":"code","47007344":"code","54d68b22":"code","9e90d993":"code","8923d2d0":"code","a0ca5d57":"code","5fac91d9":"code","702ae1c7":"code","ae195272":"code","8625b23e":"code","e76c1685":"code","29e39349":"code","ec980ddd":"code","647b7361":"code","8935fc3a":"code","3f66bcdd":"code","d19479c5":"code","ba3967d2":"code","39ec1a17":"code","6e2bbabb":"code","98431bc8":"code","0b0ce377":"code","14bbf0d7":"code","87b82f2d":"code","dbd12a81":"code","4048ee76":"code","8a0a2b98":"code","3a3f7e62":"code","46e941ad":"code","30ae9cb1":"code","c4821b04":"code","559dd380":"markdown","31910c94":"markdown","ca4e8105":"markdown","6dfc8fac":"markdown","8c2c3b14":"markdown","fc6d95c4":"markdown","6e933b2f":"markdown","6074c38e":"markdown","bcc5f743":"markdown","28b67425":"markdown","c0ae1653":"markdown","ae0394f3":"markdown","404184f9":"markdown","e1b2c46f":"markdown","7bcc7709":"markdown","749cd304":"markdown","cefb4da8":"markdown","486bddc3":"markdown","d3189274":"markdown","1bec08dc":"markdown","d66e1818":"markdown","fcd6b008":"markdown","8cc8bd7d":"markdown","32cbd8e3":"markdown","aa29b5ad":"markdown","d4e0f6f1":"markdown"},"source":{"45a740ba":"#import libaries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","563bd911":"df=pd.read_csv('..\/input\/titanic\/train.csv')","d0436e45":"df.head()","d1c8eb89":"df.shape # contains 819 rows and 12columns","7e042531":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=df)","04675f9e":"sns.set_style(\"whitegrid\")\nsns.countplot(x='Survived',hue='Sex',data=df)","13716422":"sns.factorplot('Pclass','Survived',hue='Sex',data=df)\nplt.show()","84ed82db":"pd.crosstab([df.Sex,df.Survived],df.Pclass,margins=True).style.background_gradient(cmap='summer_r')","12f4a035":"df.groupby('Sex')['Survived'].mean()","61c1e21c":"df.groupby('Pclass')['Survived'].mean()","47007344":"df.isnull().sum()","54d68b22":"sns.heatmap(df.isnull(),cmap='viridis')","9e90d993":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Pclass',y='Age',data=df,palette='winter')","8923d2d0":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","a0ca5d57":"df['Age'] = df[['Age','Pclass']].apply(impute_age,axis=1)","5fac91d9":"sns.heatmap(df.isnull(),cmap='viridis')","702ae1c7":"df[df['Embarked'].isnull()]","ae195272":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=df)\nplt.show()","8625b23e":"most_frequent=df['Embarked'].value_counts()\nmost_frequent","e76c1685":"df['Embarked'].fillna('S',inplace=True)","29e39349":"df.drop('Cabin',axis=1,inplace=True)","ec980ddd":"pd.crosstab([df.SibSp],df.Survived).style.background_gradient(cmap='summer_r')","647b7361":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='SibSp',data=df)","8935fc3a":"pd.crosstab(df.SibSp,df.Pclass).style.background_gradient(cmap='summer_r')","3f66bcdd":"pd.crosstab(df.Parch,df.Pclass).style.background_gradient(cmap='summer_r')","d19479c5":"df=df.drop(['Name','Ticket','Fare'],axis=1)","ba3967d2":"df.info()","39ec1a17":"pd.get_dummies(df['Embarked'],drop_first=True).head()","6e2bbabb":"sex = pd.get_dummies(df['Sex'],drop_first=True)\nembark = pd.get_dummies(df['Embarked'],drop_first=True)","98431bc8":"df.drop(['Sex','Embarked',],axis=1,inplace=True)","0b0ce377":"df = pd.concat([df,sex,embark],axis=1)","14bbf0d7":"df.head()","87b82f2d":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure","dbd12a81":"X=df.drop(['Survived'],axis=1)\nY=df['Survived']","4048ee76":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","8a0a2b98":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3a3f7e62":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train,Y_train)\nprediction7=model.predict(X_test)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,Y_test))","46e941ad":"model = LogisticRegression()\nmodel.fit(X_train,Y_train)\nprediction3=model.predict(X_test)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,Y_test))","30ae9cb1":"model=DecisionTreeClassifier()\nmodel.fit(X_train,Y_train)\nprediction4=model.predict(X_test)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,Y_test))","c4821b04":"model=KNeighborsClassifier() \nmodel.fit(X_train,Y_train)\nprediction5=model.predict(X_test)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction5,Y_test))","559dd380":"### The number of people died was from 3rd class and maximum people saved was from 1st class","31910c94":"# K-Nearest Neighbours","ca4e8105":"# Exploratory Data Analysis (Titanic )","6dfc8fac":"### The column Name , Ticket and Fare is of less importance so we will drop these columns.","8c2c3b14":"## Observation:\n### The barplot and crosstab shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase. Surprisingly the survival for families with 5-8 members is 0%.","fc6d95c4":"## Observation:\n### 1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass.\n\n### 2)Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.\n\n### 3)Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3","6e933b2f":"## Removing Null values from  Age column.","6074c38e":"#### Here we can see that out of 819 , around 350 survived , lets try to compare other feature with survival rate","bcc5f743":"\n## THANK YOU SO MUCH FOR HAVING A LOOK AT THIS NOTEBOOK. IF YOU FOUND IT USEFUL PLEASE UPVOTE.","28b67425":"### As most of us must be familier with the Titanic incident.On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. \n### As a beginner ,this is a very good dataset to start the data science journey.\n### The objective of this notebook is to find the meaningful information from the dataset by using seaborn and matplotlib and remove missing values. As i am new to this community , my methods may be naive.","c0ae1653":"## Observation:\n### The catagorical feature need to be handled properly by coverting it into numeric form. So here we used get_dummies such that we get the three possible combination of embarked column. The first  combination can be ignored. Same can be done for Sex and embark column.","ae0394f3":"## Observation:\n### From factorplot  we can easily see that 95% of the women from class 1 were saved","404184f9":"# Random forest","e1b2c46f":"## Observation:\n### From above two statement we can identify the columns containing null values , cabin contains 687 null values ,age contains 177 null values and embarked contains only 2 null values.","7bcc7709":"## From different models we can see that Random Forest has given the maximum accuracy score .Hence, we can use Random Forest to predict the values.","749cd304":"# Predictive Modeling\n### We have gained some insights from the EDA part. But with that, we cannot accurately   predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:\n\n1)Logistic Regression\n\n2)Random Forest\n\n3)K-Nearest Neighbours\n\n4)Decision Tree","cefb4da8":"# Handeling Catagorical features","486bddc3":"# Decision Tree","d3189274":"# Observations:\n### Sex: The chance of survival for women is high as compared to men.\n\n### Pclass:There is a visible trend that being a 1st class passenger gives you better chances of survival. The survival rate for Pclass3 is very low. For women, the chance of survival from Pclass1 is almost 1 and is high too for those from Pclass2. \n\n\n### Embarked: This is a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Passengers at Q were all from Pclass3.\n\n### Parch+SibSp: Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you.","1bec08dc":"## Observation:\n### we can see that we have 177 null values in age column and 687 null values in cabin column and only 2 null values in embarked column.","d66e1818":"### As only 2 rows have null values ,we can insert the most frequent value .","fcd6b008":"### Contents of the Notebook:\u00b6\n### Part1: Exploratory Data Analysis(EDA):\n###        1)Analysis of the features.\n###        2)Finding any relations or trends considering multiple features.\n### Part2: Feature Engineering and Data Cleaning:\n###        1)Adding any few features.\n###        2)Removing redundant features.\n###        3)Converting features into suitable form for modeling","8cc8bd7d":"# Logistic Regression","32cbd8e3":"### From the boxplot we can see the average age from wach class ,  the average age from 1st class is 37 year , from class 2 its 29 years and from 3rd class its 24 years .\n### These three values can be used to remove null values from age column. So next we will create a function to insert these values","aa29b5ad":"## Observation:\n### we can see that number of men died is almost 4 times the women. The survival rate of women almost twice the rate of men","d4e0f6f1":"### After  removing null values from Age column, now we need to remove null values from the embarked column"}}