{"cell_type":{"28d51553":"code","a5ad4056":"code","8ba0fea5":"code","3846d28e":"code","cf92b066":"code","e0914922":"code","8ce4137f":"code","833a0ea8":"code","4762f9e4":"code","655d976c":"code","7d1ba352":"code","884f12e6":"code","2d41be6d":"code","d5d11572":"code","e742f5fa":"code","422ce3bb":"code","42d4c35c":"code","621ef8fb":"code","85417675":"code","24bdf245":"code","bcacf091":"code","2c337333":"code","8170d794":"code","d062c895":"code","ac2e2176":"code","97939262":"code","59a23d31":"code","9dde9d5b":"code","fa89d524":"code","88e4c3d1":"code","35a69928":"code","87abed6e":"code","b1c184cc":"code","7c4081ce":"code","dd97851f":"code","8c3aef9b":"code","e2495521":"code","45e2214d":"code","d3a1c32b":"code","852a78a0":"code","9b381dc2":"code","5cf4ad30":"code","297fa8ca":"code","5775bdbe":"code","71c5f135":"code","757697b1":"code","b1052052":"code","0595e6b5":"code","ded7c47a":"code","c60cd38e":"code","0336b04f":"code","fb3cff84":"code","b4f64248":"code","9a1d723d":"code","7436274c":"code","6c32320f":"code","b683d29a":"code","b6813577":"code","4d9028aa":"code","873c0138":"code","fd416fdf":"code","8a0dbdcb":"code","a8a27b2f":"code","0db931b9":"code","6a0642be":"code","b783d664":"code","25ac68f5":"code","8d863388":"code","d78b6bd6":"code","1ee6fc7e":"code","f9b0aec1":"code","07d3e4fd":"code","89f73fb0":"code","92d92612":"code","37b7d1c9":"code","b3b499e2":"code","eec3d6f3":"code","fa41fe0f":"code","62a68ef9":"code","7aa39f87":"code","8b820087":"code","2a3f1a20":"code","b4bacd03":"code","2ed1f903":"code","e15101fb":"code","69af194d":"code","d9ffb76b":"code","aa5eb674":"code","ffcb5673":"markdown","1943af29":"markdown","b8c16245":"markdown","9b05f3c9":"markdown","e4cdf20f":"markdown","9f5e2843":"markdown","1b8bb203":"markdown","af2a29cf":"markdown","4f769c9b":"markdown","226054a8":"markdown","81b21ef6":"markdown","4a5f8ff6":"markdown","a7251a6b":"markdown","ca7f0fde":"markdown","616e866f":"markdown","6ba5a1b1":"markdown","358a4bd7":"markdown","89cac871":"markdown","ce1df729":"markdown","1809c3cc":"markdown","cc791b52":"markdown","ca05d15d":"markdown","32a38228":"markdown","a9ca282a":"markdown","01a21df0":"markdown","c52c0ee3":"markdown","bc810600":"markdown","3cd2312c":"markdown"},"source":{"28d51553":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5ad4056":"import sklearn\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve","8ba0fea5":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam","3846d28e":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import word_tokenize \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords","cf92b066":"!pip install emot","e0914922":"\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport scipy\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport re\nfrom collections import Counter\nfrom emot.emo_unicode import UNICODE_EMO, EMOTICONS","8ce4137f":"train = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding=\"ISO-8859-1\", low_memory=False)\ntest = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding=\"ISO-8859-1\", low_memory=False) ","833a0ea8":"df = train.append(test, sort = False) #getting all together.","4762f9e4":"df","655d976c":"df.describe()","7d1ba352":"duplicatedRow = df[df.duplicated()]\nprint(duplicatedRow[:5]) #remove dublicated rows","884f12e6":"df.shape","2d41be6d":"df.info()","d5d11572":"display(train.isnull().sum().sort_values(ascending=False))","e742f5fa":"df['Location'].fillna(value='unknown', inplace=True) #filling missing values","422ce3bb":"encoding = {'Extremely Negative': 'Negative',\n            'Extremely Positive': 'Positive'\n           }\n\nlabels = ['Negative', 'Positive']\n           \n\ndf['Sentiment'].replace(encoding, inplace=True) #less label","42d4c35c":"df[\"sentiment\"] = LabelEncoder().fit_transform(df[\"Sentiment\"])\ndisplay(df[[\"Sentiment\", \"sentiment\"]].head(5))","621ef8fb":"df['CleanTweet'] = df['OriginalTweet'].copy()\ndisplay(df.head(5))","85417675":"a = df.corr()\nplt.figure(figsize=(9,9))\nsns.heatmap(a, linewidth=.5, annot=True, fmt=\".2f\", annot_kws={\"size\":10}, cmap=\"viridis\", vmin =0, vmax=1)","24bdf245":"def before_lowercase(tweet):\n    tweet = re.sub(r\" usa \", \" America \", tweet)\n    tweet = re.sub(r\" USA \", \" America \", tweet)\n    tweet = re.sub(r\" u s \", \" America \", tweet)\n    tweet = re.sub(r\" uk \", \" England \", tweet)\n    tweet = re.sub(r\" UK \", \" England \", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"the US\", \"America\", tweet)\n    tweet = re.sub(r\"Coronavirus\", \" covid \", tweet)\n    tweet = re.sub(r\"Covid19\", \" covid \", tweet)\n    return str(tweet)\n#before lowercase I replaced some important words.","bcacf091":"df['CleanTweet'] = df['CleanTweet'].apply(before_lowercase)\ndisplay(df['CleanTweet'].head(5))","2c337333":"# Function for url's\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'link', text)\n\nfrom bs4 import BeautifulSoup\n#Function for removing html\ndef html(text):\n    return BeautifulSoup(text, \"lxml\").text","8170d794":"df['CleanTweet'] = df['CleanTweet'].apply(remove_urls)\ndf['CleanTweet'] = df['CleanTweet'].apply(html)\n","d062c895":"df['CleanTweet'] = df['CleanTweet'].str.lower()\ndisplay(df['CleanTweet'].head(5))","ac2e2176":"char_list = [\"don\", \"ain\", \"ain't\", \"aren\", \"arent\", \"aren't\", \"cannot\", \"cant\", \"can't\", \"couldn\", \"couldnt\", \"couldn't\", \"didn\",\n               \"didn't\", \"doesn\", \"doesn't\", \"don\", \"don't\", \"hadn\", \"hadn't\", \"hasn\", \"hasnt\", \"hasn't\", \"haven\", \"haven't\", \"mightn\", \"mightn't\",\n               \"isn\", \"isn't\",  \"mustn\", \"mustn't\", \"needn\", \"needn't\", \"nt\", \"shouldn\", \"shouldn't\",  \"wasn\", \"wasnt\", \"wasn't\", \"don't\"]\n\ndef before_lowercase(tweet0):\n    tweet0 =  re.sub(r\"|\".join(char_list), \"not\", tweet0) \n    return str(tweet0)\n\ndf['CleanTweet'] = df['CleanTweet'].apply(before_lowercase)\ndisplay(df['CleanTweet'].head(15))","97939262":"# Function for converting emojis into word\ndef convert_emojis(text):\n    for emot in UNICODE_EMO:\n        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n    return text","59a23d31":"# Function for converting emoticons into word\ndef convert_emoticons(text):\n    for emot in EMOTICONS:\n        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n    return text\n# Example\ntext = \"Hello :-) :-)\"\nconvert_emoticons(text)","9dde9d5b":"df['CleanTweet'] = df['CleanTweet'].apply(convert_emojis)\ndf['CleanTweet'] = df['CleanTweet'].apply(convert_emoticons)\ndisplay(df['CleanTweet'].head(15))","fa89d524":"my_stopwords = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \n                     \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"after\", \"afterwards\", \"ag\", \"again\", \"ah\", \"aj\", \"al\", \"all\",\n                      \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\",  \"an\", \"and\", \"announce\", \n                      \"ao\", \"ap\", \"apparently\", \"appear\",  \"appropriate\", \"to\",\n                     \"approximately\", \"ar\", \"are\",  \"arise\", \"around\", \"as\", \"a's\", \"aside\",  \"associated\", \"at\", \"au\", \"auth\", \"av\",  \"aw\", \"away\", \"ax\", \"ay\", \n                     \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\",\n                     \"beginnings\", \"begins\", \"behind\", \"being\",  \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \n                     \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\",  \"cc\", \"cd\", \"ce\", \n                      \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\",\n                     \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\",\n                     \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\",  \"different\", \"dj\",\n                     \"dk\", \"dl\", \"do\", \"does\", \"doing\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \n                     \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"entirely\", \"eo\", \"ep\", \"eq\",\n                     \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"ey\", \"f\", \"f2\",\n                     \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\",\n                     \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \n                     \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \n                     \"h3\", \"had\", \"happens\", \"hardly\", \"has\",  \"have\",  \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\",\n                     \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\",  \"hopefully\", \"how\", \"howbeit\", \"however\", \n                     \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\",  \"ih\", \"ii\", \"ij\",\n                     \"il\", \"i'll\", \"im\", \"i'm\", \"in\", \"inasmuch\", \"inc\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"interest\", \"into\", \"invention\",\n                     \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\",  \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \n                     \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\",  \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\",\n                     \"le\", \"les\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\",\n                     \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"mill\", \"million\", \"mine\", \n                     \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"mug\",  \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \n                     \"nc\", \"nd\", \"ne\", \"near\", \"nearly\",\"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"nos\", \"noted\",  \"novel\", \"now\", \"nr\", \"ns\",  \"ny\", \"o\", \"oa\", \"ob\", \n                     \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\",  \"onto\", \n                     \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\",  \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\",  \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\",\n                     \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \n                      \"plus\", \"pm\", \"pn\", \"po\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\",  \"promptly\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\",\n                      \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\",  \"ref\", \"refs\", \"regarding\",  \"related\", \"relatively\", \"research-articl\", \"respectively\",\n                      \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\",\n                     \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \n                     \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\",  \"should've\",  \"si\", \"side\", \"significant\",\n                     \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\",\n                     \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\",\n                      \"sub\", \"substantially\", \"sup\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\",  \"that\",\n                     \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\",\n                     \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\",\n                     \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \n                     \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\",  \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\",  \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\",\n                     \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\",  \"uses\", \"using\",\"ut\",\n                     \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\",  \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\",  \"way\", \"we\", \n                     \"wed\", \"we'd\",  \"went\", \"were\", \"we're\",  \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\",\n                     \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\",\n                     \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"with\", \"within\",  \"wo\",  \"words\", \"world\", \"would\",  \"www\", \"x\", \"x1\", \"x2\",\n                     \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\",\n                     \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&','\/', '[', ']', '>', '%', '=', '#', '*', '+', \n                '\\\\', '\u2022',  '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', \n                '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', \n                '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', \n                '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2',\n                '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', \n                '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', \n                '\u00b9', '\u2264', '\u2021', '\u221a', '\u00ab', '\u00bb', '\u00b4', '\u00ba', '\u00be', '\u00a1', '\u00a7', '\u00a3', '\u20a4']\n# for adding multiple words\nprint(len(my_stopwords))","88e4c3d1":"def remove_swords(text,s_list):\n    a=[]\n    for s in text.split():\n        if s not in my_stopwords:\n            a.append(s)\n            #remove_swords(text ,my_stopwords)\n    return a     \n","35a69928":"b=[]\nfor t in df['CleanTweet']:\n    \n    b.append(remove_swords(t ,my_stopwords))","87abed6e":"df['CleanTweet2'] = b\ndf['CleanTweet2'].head()","b1c184cc":"# 2.8 Combine individual words\ndef combine_text(input):\n    combined = ' '.join(input)\n    return combined\ndf['CleanTweet'] = df['CleanTweet2'].apply(combine_text)\ndf['CleanTweet']","7c4081ce":"def clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n       \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    \n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    \n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    tweet = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'mentioned', tweet)\n    tweet = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'referance', #Replace URLs with 'httpaddr'\n                     tweet)\n    tweet = re.sub(r'\u00a3|\\$', 'money', tweet) #Replace money symbols with 'moneysymb'\n    tweet = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', #Replace phone numbers with 'numbers'\n                   ' ', tweet)\n    tweet = re.sub(r'\\d+(\\.\\d+)?', ' ', tweet)  #Replace numbers with 'numbr'\n    tweet = re.sub(r'[^\\w\\d\\s]', ' ', tweet)\n    tweet = re.sub(r'\\s+', ' ', tweet)\n    tweet = re.sub(r'^\\s+|\\s+?$', '', tweet.lower())\n    \n    \n    # Contractions\n   \n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"luv\", \"love\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n   \n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n  \n   \n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n   \n    tweet = re.sub(r\"e-mail\", \"email\", tweet)\n    tweet = re.sub(r\"\\s{2,}\", \" \", tweet)\n    tweet = re.sub(r\"quikly\", \"quickly\", tweet)\n    \n    \n    \n    tweet = re.sub(r\" iPhone \", \" phone \", tweet)\n    tweet = re.sub(r\"\\0rs \", \" rs \", tweet) \n    \n    tweet = re.sub(r\"ios\", \"operating system\", tweet)\n  \n    tweet = re.sub(r\"programing\", \"programming\", tweet)\n    tweet = re.sub(r\"bestfriend\", \"best friend\", tweet)\n    \n    \n    tweet = re.sub(r\" J K \", \" JK \", tweet)\n    tweet = re.sub(r\"coronavirus\", \" covid19\", tweet)\n    tweet = re.sub(r\"covid\", \" covid19\", tweet)\n    tweet = re.sub(r\"corrona\", \" covid19 \", tweet)\n    tweet = re.sub(r\"covid1919\", \" covid19 \", tweet)\n    tweet = re.sub(r\"_\", \"  \", tweet)\n    \n    # Urls\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n        \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ') \n        \n    \n        \n        \n    return str(tweet)","dd97851f":"df['CleanTweet'] = df['CleanTweet'].apply(clean)\ndisplay(df['CleanTweet'].head(15))","8c3aef9b":"!pip install pyspellchecker","e2495521":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        elif word not in misspelled_words:\n            corrected_text.append(word)\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"raed\"\ncorrect_spellings(text)","45e2214d":"#df['CleanTweet'] = df['CleanTweet'].apply(SpellChecker)\ndisplay(df['CleanTweet'].head(5))","d3a1c32b":"import string\nregular_punct = list(string.punctuation)\n#all_punct = list(set(regular_punct+ my_stopwords ))\ndef remove_punctuation(text,punct_list):\n    for punc in punct_list:\n        if punc in text:\n            text = text.replace(punc, ' ')\n    return text.strip()\ntext =\" advice talk to your neighbours family to excha..\"\nremove_punctuation(text ,regular_punct)","852a78a0":"df.groupby('Sentiment').describe(include=['O']).T","9b381dc2":"temp = df.groupby('Sentiment').count()['CleanTweet'].reset_index().sort_values(by='CleanTweet',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","5cf4ad30":"plt.figure(figsize=(12,6))\nsns.countplot(x='Sentiment',data=df)","297fa8ca":"plt.figure(figsize=(9,6))\nsns.countplot(y=df.Location, order = df.Location.value_counts().iloc[:25].index)\nplt.title('Top 25 locations')\nplt.show()","5775bdbe":"#Optional Step: Looking into data\ndisplay(df.sample(2)) #Sample rows of dataframe\n\nprint ( '\\nSample Tweet Positive :\\n-------------------------------')\nprint ( df[df['Sentiment']=='Positive'].CleanTweet.values[0] )\n\nprint ( '\\nSample Tweet Negative :\\n--------------------------------------')\nprint ( df[df['Sentiment']=='Negative'].CleanTweet.values[0] )\n\nprint ( '\\nSample Tweet Neutral:\\n--------------------------------------')\nprint ( df[df['Sentiment']=='Neutral'].CleanTweet.values[0] )\n\nprint ( '\\nTweets distribution for Disaster Tweets (1)  and Non-Disaster Tweets (0)\\n------------------------------------------------------------------------')\ndf['Sentiment'].hist() ;","71c5f135":"from collections import Counter\ncnt = Counter()\nfor text in df[\"CleanTweet\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)","757697b1":"def get_n_words(corpus, direction, n):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    if direction == \"top\":\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    else:\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=False)\n    return words_freq[:n]","b1052052":"from sklearn.feature_extraction.text import CountVectorizer\ncommon_words = get_n_words(df['CleanTweet'], \"top\", 15)\nrare_words = get_n_words(df['CleanTweet'], \"bottom\", 15)","0595e6b5":"common_words = dict(common_words)\nnames = list(common_words.keys())\nvalues = list(common_words.values())\nplt.subplots(figsize = (15,10))\nbars = plt.bar(range(len(common_words)),values,tick_label=names)\nplt.title('15 most common words:')\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + .01, yval)\nplt.show()","ded7c47a":"# Get all the pozitive and negative tweets\nPositive = df[df.Sentiment =='Positive']\nNegative = df[df.Sentiment=='Negative']\nNeutral = df[df.Sentiment=='Neutral']\n# Create numpy list to visualize using wordcloud\npositive_text = \" \".join(Positive.CleanTweet.to_numpy().tolist())\nnegative_text = \" \".join(Negative.CleanTweet.to_numpy().tolist())\nneutral_text = \" \".join(Neutral.CleanTweet.to_numpy().tolist())","c60cd38e":"# wordcloud of pozitive messages\nfont_path = 'path\/to\/font'\nmask = np.array(Image.open('..\/input\/wordcloud-mask\/indir.png'))\npositive_cloud = WordCloud(width =520, height =260, stopwords=my_stopwords,max_font_size=80,\n                           contour_width=5, contour_color='pink', max_words=100, background_color='white',\n                           colormap='Set2', collocations=False, mask=mask).generate(positive_text)\nplt.figure(figsize=(16,10))\nplt.imshow(positive_cloud, interpolation='bilinear')\nplt.axis('off') # turn off axis\nplt.show()","0336b04f":"# wordcloud of neutral messages\nfont_path = 'path\/to\/font'\nmask = np.array(Image.open('..\/input\/wordcloud-mask\/indir.png'))\npositive_cloud = WordCloud(width =520, height =260, stopwords=my_stopwords,max_font_size=80, \n                           contour_width=5, contour_color='orange', max_words=100,\n                            background_color='purple',colormap='Set2', collocations=False, mask=mask).generate(negative_text)\nplt.figure(figsize=(16,10))\nplt.imshow(positive_cloud, interpolation='bilinear')\nplt.axis('off') # turn off axis\nplt.show()","fb3cff84":"# wordcloud of negative messages\n\nfont_path = 'path\/to\/font'\nmask = np.array(Image.open('..\/input\/wordcloud-mask\/indir.png'))\npositive_cloud = WordCloud(width =520, height =260, stopwords=my_stopwords,max_font_size=80, \n                           contour_width=5, contour_color='red', max_words=100,\n                            background_color='black',colormap='Set2', collocations=False, mask=mask).generate(negative_text)\nplt.figure(figsize=(16,10))\nplt.imshow(positive_cloud, interpolation='bilinear')\nplt.axis('off') # turn off axis\nplt.show()","b4f64248":"\ndf['text_length'] = df['CleanTweet'].apply(len)\n#Calculate average length by label types\nlabels = df.groupby('Sentiment').mean()\nlabels","9a1d723d":"sns.catplot(x=\"sentiment\", y=\"text_length\",hue=\"sentiment\", data=df);","7436274c":"sns.jointplot(x=df['text_length'], y=df['sentiment']);","6c32320f":"df['CleanTweet'] = df['CleanTweet'].apply(word_tokenize)\ndisplay(df['CleanTweet'].head(5))","b683d29a":"lem = WordNetLemmatizer()\ndef lemma_wordnet(input):\n    return [lem.lemmatize(w) for w in input]\ndf['CleanTweet'] = df['CleanTweet'].apply(lemma_wordnet)","b6813577":"display(df['CleanTweet'].head(5))","4d9028aa":"# 2.8 Combine individual words\ndef combine_text(input):\n    combined = ' '.join(input)\n    return combined\ndf['CleanTweet'] = df['CleanTweet'].apply(combine_text)\ndf['CleanTweet']","873c0138":"train, test = train_test_split(df)","fd416fdf":"# Bag of words\n\ncv = CountVectorizer()\ncv.fit(train)\nX_train_bow = cv.fit_transform(train['CleanTweet']) #X_train\nX_test_bow = train['sentiment'] #Y_train\nY_train_bow = cv.transform(test['CleanTweet']) #X_test\nY_test = test['sentiment'] # Y_test\n\n# 3.2 TF-IDF\n\nvectorizer = TfidfVectorizer(norm = None)\nvectorizer.fit(train)\nX_train_tfidf = vectorizer.fit_transform(train['CleanTweet'])\nX_test_tfidf = train['sentiment'] #\nY_train_tdidf =vectorizer.transform(test['CleanTweet']) #vectorizer.fit_transform\n\n# 3.3 Hashing\n\nhv = HashingVectorizer()\nhv.fit(train)\nX_train_hash = hv.fit_transform(train['CleanTweet'])\nX_test_hash = train['sentiment']\nY_train_hash = hv.transform(test['CleanTweet']) ","8a0dbdcb":"display(\"Bow-TF:IDF :\", X_train_bow.shape)\ndf_tfidf = pd.DataFrame(X_train_bow.toarray(), columns=cv.get_feature_names())\ndisplay(df_tfidf.head())","a8a27b2f":"# Rigde with bag of word\nfrom sklearn import linear_model\nalpha = [80.0, 90.0, 100.0, 110.0, 120.0] \nfor a in alpha:\n    ridge = linear_model.RidgeClassifier(a)\n    scores = sklearn.model_selection.cross_val_score(ridge, X_train_bow, X_test_bow, cv=5)#scoring='f1' kaldirdim multiclass hatasina karsilik\n    print(\"alpha: \",a)\n    print(scores)\n    print(np.mean(scores))\n    print('\\n')","0db931b9":"# MultinomialNB with bag of word\nfrom sklearn.naive_bayes import MultinomialNB\nalpha = [1e-10, 1e-5, 0.1, 1.0, 2.0, 5.0]\nfor a in alpha:\n    mnb = MultinomialNB(a)\n    scores = sklearn.model_selection.cross_val_score(mnb, X_train_bow, X_test_bow, cv=5)\n    print('alpha: ', a)\n    print(scores)\n    print(np.mean(scores))\n    print('\\n')","6a0642be":"# MultinomialNB with TF-IDF\nalpha = [175.0, 200.0, 225.0, 250.0, 300.0]\nfor a in alpha:\n    mnb = MultinomialNB(a)\n    scores = sklearn.model_selection.cross_val_score(mnb, X_train_tfidf, X_test_tfidf, cv=5)\n    print('alpha: ', a)\n    print(scores)\n    print(np.mean(scores))\n    print('\\n')","b783d664":"# Rigde with Hash\nalpha = [1.1, 1.2, 1.3, 1.4, 1.5, 2.0]\nfor a in alpha:\n    ridge = linear_model.RidgeClassifier(a)\n    scores = sklearn.model_selection.cross_val_score(ridge, X_train_hash, X_test_hash, cv=5)\n    print(\"alpha: \",a)\n    print(scores)\n    print(np.mean(scores))\n    print('\\n')","25ac68f5":"# Rigde with TF-IDF\nalpha = [500.0, 1500.0, 2500.0, 3000.0]\nfor a in alpha:\n    ridge = linear_model.RidgeClassifier(a)\n    scores = sklearn.model_selection.cross_val_score(ridge, X_train_tfidf, X_test_tfidf, cv=5)\n    print(\"alpha: \",a)\n    print(scores)\n    print(np.mean(scores))\n    print('\\n')","8d863388":"from sklearn.metrics import accuracy_score\nridge = linear_model.RidgeClassifier(1.4)\nridge.fit(X_train_hash, X_test_hash)\ntest['sentiment_pred'] = ridge.predict(Y_train_hash)\ny_true = test['sentiment']\ny_pred = test['sentiment_pred']\naccuracy_score(y_true, y_pred)","d78b6bd6":"from sklearn.metrics import classification_report,confusion_matrix \nprint(classification_report(y_true, y_pred, target_names = ['Negative Tweets','Neutral Tweets', 'Positive Tweets']))","1ee6fc7e":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize = (9,9))\nsns.heatmap(cm,cmap= \"Blues\", \n            linecolor = 'black', \n            linewidth = 1, \n            annot = True, \n            fmt='', \n            xticklabels = ['Negative Tweets','Neutral Tweets', 'Positive Tweets'], \n            yticklabels = ['Negative Tweets','Neutral Tweets', 'Positive Tweets'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","f9b0aec1":"\ndtclassifier=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None)\ndtclassifier.fit(X_train_bow,train['sentiment'])\npreddt = dtclassifier.predict(Y_train_bow)","07d3e4fd":"accuracy= accuracy_score(preddt,Y_test)\nprint(accuracy)","89f73fb0":"dtclassifier=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None) \ndtclassifier.fit(X_train_tfidf,train['sentiment'])\npreddt = dtclassifier.predict(Y_train_tdidf) ","92d92612":"accuracy= accuracy_score(preddt,Y_test)\nprint(accuracy)","37b7d1c9":"dtclassifier=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None)\ndtclassifier.fit(X_train_hash,train['sentiment'])\npreddt = dtclassifier.predict(Y_train_hash) ","b3b499e2":"from sklearn.feature_extraction.text import TfidfVectorizer\n# Create feature vectors\nvectorizer = TfidfVectorizer(min_df = 5,\n                             max_df = 0.8,\n                             sublinear_tf = True,\n                             use_idf = True)\nvectors = vectorizer.fit_transform(df['CleanTweet']) #all data bunu boyle yaptik cunku matrrislerde uyumsuzluk olsun isteemiyoruz. islemde sorun yasayabilirim cunku.\n#test_vectors = vectorizer.transform(al['Sentiment'])","eec3d6f3":"test_vectors = vectors[40000:]\ntrain_vectors = vectors[:40000]","fa41fe0f":"# Perform classification with SVM, kernel=linear\nimport time\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report\nclassifier_linear = svm.SVC(kernel='linear')\nt0 = time.time()\nclassifier_linear.fit(train_vectors, df['Sentiment'][:40000])\nt1 = time.time()\nprediction_linear = classifier_linear.predict(test_vectors)\nt2 = time.time()\ntime_linear_train = t1-t0\ntime_linear_predict = t2-t1\n\n# results\nprint(\"Results for SVC(kernel=linear)\")\nprint(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\nreport = classification_report(df['Sentiment'][40000:], prediction_linear, output_dict=True)\nprint('positive: ', report['Positive'])\nprint('negative: ', report['Negative'])\nprint('notr: ', report['Neutral'])","62a68ef9":"review = \"\"\"I can help\"\"\"\nreview_vector = vectorizer.transform([review]) # vectorizing\nprint(classifier_linear.predict(review_vector))","7aa39f87":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(solver='lbfgs', multi_class=\"ovr\")\nlog_reg.fit(X_train_hash, X_test_hash)","8b820087":"train_accuracy = log_reg.score(X_train_hash, X_test_hash)\ntest_accuracy = log_reg.score(Y_train_hash, Y_test)\n\nprint('One-vs.-Rest', '-'*30, \n      'Accuracy on Train Data : {:.2f}'.format(train_accuracy), \n      'Accuracy on Test Data  : {:.2f}'.format(test_accuracy), sep='\\n')","2a3f1a20":"log_reg_mnm = LogisticRegression(multi_class='multinomial', solver='lbfgs')\nlog_reg_mnm.fit(X_train_hash, X_test_hash)\n\ntrain_accuracy = log_reg_mnm.score(X_train_hash, X_test_hash)\ntest_accuracy = log_reg_mnm.score(Y_train_hash, Y_test)\n\nprint('Multinomial (Softmax)', '-'*20, \n      'Accuracy on Train Data : {:.2f}'.format(train_accuracy), \n      'Accuracy on Test Data  : {:.2f}'.format(test_accuracy), sep='\\n')","b4bacd03":"\nC_values = [0.001,0.01, 0.1,1,10,100, 1000]\n\naccuracy_values = pd.DataFrame(columns=['C_values', 'Train Accuracy', 'Test Accuracy'])\n\nfor c in C_values:\n    # Apply logistic regression model to training data\n    lr = LogisticRegression(penalty = 'l2', C = c, random_state = 0, solver='lbfgs', multi_class='multinomial')\n    lr.fit(X_train_hash, X_test_hash)\n    accuracy_values = accuracy_values.append({'C_values': c,\n                                              'Train Accuracy': lr.score(X_train_hash, X_test_hash),\n                                              'Test Accuracy': lr.score(Y_train_hash, Y_test)\n                                             }, ignore_index=True)\ndisplay(accuracy_values)    ","2ed1f903":"parameters = {\"C\": [10 ** x for x in range (-5, 5, 1)],\n              \"penalty\": ['l1', 'l2']\n             }","e15101fb":"from sklearn.model_selection import GridSearchCV\n\ngrid_cv = GridSearchCV(estimator=log_reg,\n                       param_grid = parameters,\n                       cv = 10\n                      )\n\ngrid_cv.fit(X_train_hash, X_test_hash)","69af194d":"print(\"Best Parameters : \", grid_cv.best_params_)\nprint(\"Best Score      : \", grid_cv.best_score_)","d9ffb76b":"\n%time results = grid_cv.cv_results_\n\ndf1 = pd.DataFrame(results)\ndisplay(df1.head(35))\ndf1.info()","aa5eb674":"df1 = df1[['param_penalty','param_C', 'mean_test_score']]\ndf1 = df1.sort_values(by='mean_test_score', ascending = False)\ndf1","ffcb5673":"\nWe do the train\/test split before the CountVectorizer to properly simulate the real world where our future data contains words we have not seen before After you train your data and chose the best model, you would then train on all of your data before predicting actual future data to maximize learning.\n\n* vect.fit(train) learns the vocabulary of the training data\n* vect.transform(train) uses the fitted vocabulary to build a document-term matrix from the training data\n* vect.transform(test) uses the fitted vocabulary to build a document-term matrix from the testing data (and ignores tokens it hasn't seen before)","1943af29":"# **Logistic Regression**","b8c16245":"# **MultinomialNB with bag of word**","9b05f3c9":"# **Tokenize**","e4cdf20f":"# **Remove URL**","9f5e2843":"I created my own stopwords list.","1b8bb203":"# **Get length column for each text**","af2a29cf":"# **Grid Search**","4f769c9b":"# **Explore the Data**","226054a8":"# **Most Common Words**","81b21ef6":"# **MultinomialNB with TF-IDF**","4a5f8ff6":"# **SpellChecker**","a7251a6b":"# **Rigde with bag of word**","ca7f0fde":"# **Rigde with TF-IDF**","616e866f":"# **Wordclouds**","6ba5a1b1":"# **Stopwords**","358a4bd7":"**Covid19 Tweets Sentiment Analysis | NLP**\n\nSentiment analysis is one of the Natural Language Processing fields, dedicated to the exploration of subjective opinions or feelings collected from various sources about a particular subject.\n\nThis is a detailed example of a Text Classification project. It includes a variety of visualizations, rare libraries, and different algorithms. The tweets have been pulled from Twitter and manual tagging has been done then. After a detailed text cleaning, tweets have been classified as \"positive\", \"negative\" and \"notr\". Sentiments have predicted with different algorithms. The best algorithm has an 83% f-1 score.\n\nContent\n\nText Cleaning\nRemove URL\nLowercase\nConverting Emojis&Emoticons into Word\nStopwords\nRemove Special Characters\nSpellChecker\nRemove Punctuations\nExplore the Data\nMost Common Words\nWordclouds\nGet length column for each text\nTokenize\nLemmatizer\nVectorizer\nRigde with bag of words\nMultinomialNB with bag of word\nMultinomialNB with TF-IDF\nRigde with Hash\nRigde with TF-IDF\nRidge Classifier with Hash\nDecisionTreeClassifier\nSVM\nLogistic Regression\nGrid Search\n\n**Prerequisites**\n\n It could be useful to check the packages&libraries documentaries which I used. sklearn, nltk, emot, re, BeautifulSoup, pyspellchecker, Counter etc.\n \nEnjoy!","89cac871":"# **Rigde with Hash**","ce1df729":"# **Lemmatizer**","1809c3cc":"# **Remove Punctuations**","cc791b52":"# **SVM**","ca05d15d":"# **Vectorizer**","32a38228":"# **Remove Special Characters**","a9ca282a":"# **DecisionTreeClassifier**","01a21df0":"# **Lowercase**","c52c0ee3":"# **Text Cleaining**","bc810600":"# **Ridge Classifier with Hash**","3cd2312c":"# **Converting Emojis&Emoticons into Word**"}}