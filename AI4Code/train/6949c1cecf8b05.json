{"cell_type":{"4ec4df40":"code","32f3efe7":"code","021973ba":"code","784aaab9":"code","a2f84239":"code","470819f8":"code","6e5bfd7f":"code","73cfc78e":"code","efb8916d":"code","9057e038":"code","068bbc3d":"code","f6d9d73d":"code","a39382a7":"code","d9be3bca":"code","b44fe135":"code","d8a7c958":"code","88dc93c7":"code","9bc546e0":"code","b3b3ad57":"code","40ce3cc1":"code","08f019f7":"code","55ee3f2e":"code","0b739944":"code","6f0a63c5":"code","b9cc3356":"code","d0dc9c41":"code","a97b31ed":"code","e538fedb":"code","c14eab91":"code","339d7d4f":"code","37393be4":"code","0f62cea5":"code","79dcbb74":"code","c7e3bb86":"code","c2d8e4a8":"code","08ce8f54":"code","3bed7cc5":"code","88f01a2e":"code","f28a84d2":"code","c41f76b7":"code","8ce62b38":"code","1c2c74ec":"code","a77bcb7d":"code","3b622251":"code","824f3170":"code","b414f4f3":"code","ef74ff7d":"code","69c79e68":"code","cdb061f7":"code","d35a1089":"code","4d61ae7c":"code","4cdb0f37":"code","6b4ec977":"code","a276c74d":"code","71cea515":"markdown","f62d6e36":"markdown","fef32fb2":"markdown"},"source":{"4ec4df40":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# Visualizations\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\n%matplotlib inline\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","32f3efe7":"df= pd.read_csv('..\/input\/adult-census-income\/adult.csv')\ndf.head()","021973ba":"df.shape","784aaab9":"#Mapping binary values to the expected output\n\ndf['income']=df['income'].map({'<=50K': 0, '>50K': 1})","a2f84239":"#Replacing question marks in dataset with null values\n\ndf.replace('?',np.nan )","470819f8":"#Finding what percentage of data is missing from the dataset\n\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent_1 = df.isnull().sum()\/df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","6e5bfd7f":"#Since a very small amount of data is missing, we can replace the null values with the mode of each column\n\ndf['occupation'].describe()","73cfc78e":"#Since mode is Prof-specialty, replacing null values with it\n\ndf['occupation'] = df['occupation'].fillna('Prof-specialty')","efb8916d":"df['workclass'].describe()","9057e038":"#Since mode is Private, replacing null values with it\n\ndf['workclass'] = df['workclass'].fillna('Private')","068bbc3d":"df['native.country'].describe()","f6d9d73d":"#Since mode is United-States, replacing null values with it\n\ndf['native.country'] = df['native.country'].fillna('United-States')","a39382a7":"#Mean, Median, Minimum , Maximum values etc can be found\n\ndf.describe()","d9be3bca":"df.describe(include=[\"O\"])","b44fe135":"#Visualizing the numerical features of the dataset using histograms to analyze the distribution of those features in the dataset\n\nrcParams['figure.figsize'] = 12, 12\ndf[['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']].hist()\n\n#Can visualise that data such as capital gain, capitaln loss, fnlwgt is right skewed an other columns can be grouped for better visualisation","d8a7c958":"#Ploting the correlation between the output(income) and individual features\n\nplt.matshow(df.corr())\nplt.colorbar()\nplt.xticks(np.arange(len(df.corr().columns)), df.corr().columns.values, rotation = 45) \nplt.yticks(np.arange(len(df.corr().columns)), df.corr().columns.values) \nfor (i, j), corr in np.ndenumerate(df.corr()):\n    plt.text(j, i, '{:0.1f}'.format(corr), ha='center', va='center', color='white', fontsize=14)\n","88dc93c7":"#Since it has 0 correlation, it can be dropped\n\ndf.drop(['fnlwgt'], axis = 1, inplace = True)","9bc546e0":"dataset=df.copy()","b3b3ad57":"#Distributing Age column in 3 significant parts and plotting it corresponding to the output feature(income)\n\ndataset['age'] = pd.cut(dataset['age'], bins = [0, 25, 50, 100], labels = ['Young', 'Adult', 'Old'])","40ce3cc1":"sns.countplot(x = 'income', hue = 'age', data = dataset)","08f019f7":"#Capital gain and capital loss can be combined and transformed into a feature capital difference. Plotting the new feature corresponding to income\n\ndataset['Capital Diff'] = dataset['capital.gain'] - dataset['capital.loss']\ndataset.drop(['capital.gain'], axis = 1, inplace = True)\ndataset.drop(['capital.loss'], axis = 1, inplace = True)","55ee3f2e":"dataset['Capital Diff'] = pd.cut(dataset['Capital Diff'], bins = [-5000, 5000, 100000], labels = ['Minor', 'Major'])\nsns.countplot(x = 'income', hue = 'Capital Diff', data = dataset)","0b739944":"#Dividing hours of week in 3 major range and plotting it corresponding to the income\n\ndataset['Hours per Week'] = pd.cut(dataset['hours.per.week'], \n                                   bins = [0, 30, 40, 100], \n                                   labels = ['Lesser Hours', 'Normal Hours', 'Extra Hours'])","6f0a63c5":"sns.countplot(x = 'income', hue = 'Hours per Week', data = dataset)\n","b9cc3356":"#Plotting workclass corresponding to the income\n\nsns.countplot(x = 'income', hue = 'workclass', data = dataset)","d0dc9c41":"#Plot of education corresponding to income\n\nsns.countplot(x = 'income', hue = 'education', data = dataset)","a97b31ed":"#Combining the lower grades of education together\n\ndf.drop(['education.num'], axis = 1, inplace = True)\ndf['education'].replace(['11th', '9th', '7th-8th', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'],\n                             ' School', inplace = True)\ndf['education'].value_counts()","e538fedb":"sns.countplot(x = 'income', hue = 'education', data = df)","c14eab91":"#Plot of occupation corresponding to the income\n\nplt.xticks(rotation = 45)\nsns.countplot(x = 'income', hue = 'occupation', data = dataset)","339d7d4f":"sns.countplot(x = 'income', hue = 'race', data = dataset)","37393be4":"#Since majority of race is white, the rest of races can be combined together to form a new group\n\ndf['race'].unique()\ndf['race'].replace(['Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'],' Other', inplace = True)","0f62cea5":"#Plot of sex corresponding to income\n\nsns.countplot(x = 'income', hue = 'sex', data = dataset)","79dcbb74":"count = dataset['native.country'].value_counts()\ncount","c7e3bb86":"#Plot of Country corresponding to income\n\n\nplt.bar(count.index, count.values)\nplt.xlabel('Countries')\nplt.ylabel('Count')\nplt.title('Count from each Country')","c2d8e4a8":"#Combining all other into one class\n\ncountries = np.array(dataset['native.country'].unique())\ncountries = np.delete(countries, 0)","08ce8f54":"dataset['native.country'].replace(countries, 'Other', inplace = True)\ndf['native.country'].replace(countries, 'Other', inplace = True)","3bed7cc5":"sns.countplot(x = 'native.country', hue = 'income', data = dataset)","88f01a2e":"#Splitting the data set into features and outcome\n\nX = df.drop(['income'], axis=1)\nY = df['income']","f28a84d2":"X.head()","c41f76b7":"#Splitting the data into test data and training data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)","8ce62b38":"from sklearn import preprocessing\n\ncategorical = ['workclass','education', 'marital.status', 'occupation', 'relationship','race', 'sex','native.country']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n","1c2c74ec":"#Using StandardScalar to normalise the dataset\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)","a77bcb7d":"X_train.head()","3b622251":"#Applying the random forest algorithm\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","824f3170":"#Applying the Logistic Regression algorithm\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)","b414f4f3":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train) \nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)","ef74ff7d":"#Applying the GaussianNB algorithm\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)","69c79e68":"#Applying the Support Vector Machine algorithm\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)","cdb061f7":"#Applying the Decision Tree algorithm\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n","d35a1089":"#Plotting the accuracy of the used algorithms to find the best fit\n\nresults = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(7)","4d61ae7c":"#Finding significance of each feature in t5he best fit model\n\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","4cdb0f37":"#Plotting the significance of each feautre\n\nimportances.plot.bar()","6b4ec977":"#Since they hardly have any significance, can drop these columns to avoid overfitting\n\ndf  = df.drop(\"sex\", axis=1)\ndf  = df.drop(\"race\", axis=1)\ndf  = df.drop(\"native.country\", axis=1)","a276c74d":"#The accuracy remains the same even after dropping the columns\n\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","71cea515":"Thus, in this project, we cleaned the data, carried out EDA extensively and understood all the parameters in this process and successfully applied multiple classification algorithms to predict the required output!","f62d6e36":"Problem statement:\n\nGiven the data of citizens, we have to understand the data extensively, perform EDA to better understand the features and predict if an individual's income is above 50k or no by using the relevant features\n","fef32fb2":"Importing the necessary libraries"}}