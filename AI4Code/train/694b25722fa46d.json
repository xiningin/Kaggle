{"cell_type":{"dab3db14":"code","90ff9c50":"code","61158242":"code","56728821":"code","f2ac7971":"code","eaa08b48":"code","32cd028f":"code","dbffe8e9":"code","57276427":"code","29544ac6":"code","44e910ac":"code","3b408181":"code","c981c2ca":"code","89df001f":"code","162a4cb7":"code","7a75447f":"code","233686de":"code","9045bcb4":"code","bcd89417":"code","6d5cc104":"code","09377f24":"code","456120fa":"code","ddad9369":"code","3c679105":"code","30bfaa10":"code","3d3c0f56":"code","4478ef8c":"code","37b44436":"code","520bcf5b":"code","8d526383":"code","a92325ac":"code","8f882fad":"code","e6c64f25":"code","564a9b19":"code","5ddd0a15":"code","1faac87c":"code","4a9034d9":"code","dbbba200":"code","6bb60f6e":"code","cd427ad3":"code","bfd652c5":"code","99933810":"code","253c6bf3":"code","9e5eecc5":"code","fd3002ed":"code","3d44b14c":"code","596aee5c":"code","76f915f4":"code","0e628865":"code","da03f743":"code","7df44c66":"code","ab08a434":"code","d5208083":"markdown","5b175ae6":"markdown","0dc60005":"markdown","9315e722":"markdown"},"source":{"dab3db14":"#import files that are required for reading the data. \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n#plt.figure(figsize=(16,5))\n\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nimport os\n#print(os.listdir('..\/input'))","90ff9c50":"# create datafile\n\ndf= pd.read_csv('..\/input\/electric-motor-temperature\/pmsm_temperature_data.csv')\ndf.head()\n","61158242":"df.info()","56728821":"# function to filter dataframe based on profile id. \n\ndef profile_id_df(dataframe, prof_id):\n    '''\n    Input:\n    dataframe = Pandas dataframe \n    profile id = # profile id number out of df['profile_id'].unique()\n    \n    Output:\n    filtered dataframe for a given profile id\n    '''\n       \n    \n    return dataframe.loc[dataframe['profile_id'] == prof_id]","f2ac7971":"profile_list = df['profile_id'].unique()\nprofile_list\n\n\n#lets find out how many rows of data is there for each profile id. To estimate complexity of decision tree analysis\n\nrows = []\n\nfor n in profile_list:\n    rows.append((profile_id_df(df,n)).shape[0])\n#print(rows)\n\n\n## create df using above two lists\n\nd_dict = {'profile_id': profile_list, 'rows': rows}\n\ndf_shape = pd.DataFrame(d_dict)\n\ndf_shape.head()","eaa08b48":"#filter the df_shape dataframe with rows >=35000 (The number 15000 is an arbitrary # picked, just to reduce the computing power)\n\ndf_short = df_shape.loc[df_shape['rows'] >= 30000]\ndf_short.reset_index(drop=True, inplace = True)\ndf_short.head()","32cd028f":"df_short.nlargest(10, 'rows')['profile_id'].values","dbffe8e9":"#top 10 profile ids which can be used for classifying the data. \ndf_short","57276427":"# filter the dataframe based on top 10 profile id's\nprof_id = list(df_short['profile_id'].values)\n\ndf_prof = df[df.profile_id.isin(prof_id)]\nprint(df_prof.shape)\n","29544ac6":"#Data prep. Split the data into X and y input values\n\nX_filt= df_prof.drop(['profile_id'],axis =1)\ny_filt = df_prof['profile_id']","44e910ac":"## Classification analysis","3b408181":"#import files\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score,precision_score\nfrom sklearn.metrics import recall_score,balanced_accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, train_test_split","c981c2ca":"X_train, X_test, y_train, y_test = train_test_split(X_filt, y_filt, test_size = 0.3, random_state = 0)","89df001f":"## Functions to run evulation analysis. ","162a4cb7":"# function to create input dataframe and compare against various classification models. This serves as input. \n\ndef labels_list(y_test):\n    '''\n    Input:- \n    y_train with labels of classification\n    \n    Output\n    Sorted list of labels arranged in ascending order to be used in for confusion matrix. \n    \n    '''\n    df_test = pd.DataFrame(y_test.value_counts())\n    df_test.sort_index(ascending=True, inplace= True)\n    \n    return df_test","7a75447f":"# function to capture the diagonal values of the confusion matrix\n\ndef true_pred(model, confusion_matrix, y_test):\n    '''\n    Input:- \n    confusion_matrix = is a np.ndarray\n    model = string name of the model. \n    y_test = classification label series\n    \n    Output:- \n    diagonal values of confusion matrix (true predictions) in dataframe \n    \n    '''\n    test = np.matrix(confusion_matrix)\n    n,m = test.shape\n    \n    # get label names from y_test column\n    labels = sorted(y_test.unique())\n    #print(labels)\n    # list of values \n    values = []\n    \n    if n == len(labels):\n        for i in range(m):\n            values.append(test[i,i])\n    else :\n        print('The lengths of y_test does not match with confusion matrix shape')\n    \n    #print(values)\n    data = { model: values}\n    #print(data)\n    df= pd.DataFrame(data=data, index= labels)\n    \n    return df","233686de":"#function to capture all the numerical scores of the model. \n\ndef algor_scores(model, y_pred, y_test, time_name):\n    acc_score = accuracy_score(y_pred, y_test)\n    bal_acc_score= balanced_accuracy_score(y_pred, y_test)\n\n    \n    row_label = ['accuracy', 'balance_accuracy', 'time']\n    row_values = [acc_score, bal_acc_score, time_name ]\n    \n    data_score = { model: row_values}\n    \n    df_data = pd.DataFrame(data= data_score, index= row_label)\n    \n    return df_data\n","9045bcb4":"#raw data of y_test, each model should have values of y_pred as close as to raw_data. The accuracy and precision scores depend upon these predication along with confusion matrix. \nRaw_data = labels_list(y_test)\nRaw_data","bcd89417":"## KNN","6d5cc104":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nimport time, timeit","09377f24":"# hyperparameter tuning to find out optimium value for n_neighbors. Initialy the range was high, due to run time the range reduced to 10. \nerror_rate = []\nacc_score = []\n# Will take some time\nfor i in range(1,10):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    acc_score.append(accuracy_score(y_test, pred_i))","456120fa":"plt.figure(figsize=(10,6))\nplt.plot(range(1,10),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=2)\n\nplt.title('Error Rate vs. n_neighbour Value')\nplt.xlabel('n_neighbour')\nplt.ylabel('Error Rate')","ddad9369":"plt.plot(range(1,10),acc_score,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=2)\nplt.title('Acc_Score vs. n_neighbour Value')\nplt.xlabel('n_neighbour')\nplt.ylabel('Acc_Score')","3c679105":"# it is clear that the n_neighbour=1 would produce great results of accuracy and evulation. Lets run the knn analysis. ","30bfaa10":"## Knn Model analysis\n\n\nstart_time = time.time()\n\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train, y_train)\n\ny_pred_knn= knn.predict(X_test)\n\nknn_time = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","3d3c0f56":"## knn Model evaluation\n\nknn = true_pred('KNN', confusion_matrix(y_pred_knn, y_test), y_test)\ndf_knn_score = algor_scores('knn', y_pred_knn, y_test, knn_time)\ndf_knn_score","4478ef8c":"## Decision Tree (max_depth= None)","37b44436":"dec_tree = DecisionTreeClassifier()\nscaler = StandardScaler()","520bcf5b":"# Decision Tree model analysis\n\nstart_time = time.time()\n\npipe = Pipeline( steps = [('Standardscaler', scaler), ('DecisionTree', dec_tree)])\n\npipe.fit(X_train, y_train)\n\ny_pred_dtree = pipe.predict(X_test)\n\ndec_tree_time = time.time() - start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","8d526383":"#Decision Tree evaluation\n\ndec_tree = true_pred('dec_tree',confusion_matrix(y_test, y_pred_dtree), y_test)\ndf_dec_tree_score = algor_scores('dec_tree', y_pred_dtree, y_test, dec_tree_time)\ndf_dec_tree_score","a92325ac":"# Ada Boost Classifier","8f882fad":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier","e6c64f25":"ada_boost = AdaBoostClassifier(n_estimators= 100, learning_rate= 1, algorithm= 'SAMME', random_state= 0)\n","564a9b19":"#AdaBoost model analysis\nstart_time = time.time()\n\n\nada_boost.fit(X_train, y_train)\ny_pred_ad = ada_boost.predict(X_test)\n\n\nada_boost_time = time.time() - start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","5ddd0a15":"#AdaBoost evaulation\n\nAda_boost = true_pred('Ada_boost', confusion_matrix(y_pred_ad, y_test), y_test)\ndf_ada_boost_score = algor_scores('Ada_boost', y_pred_ad, y_test, ada_boost_time)\ndf_ada_boost_score","1faac87c":"## RandomForest Classification","4a9034d9":"## uses decision tree for classification. The output should be similiar to Decision tree (max depth= None. )\n\nrandom = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, max_features='auto', bootstrap=True, random_state=0, verbose=0)","dbbba200":"#RandomForest analysis\nstart_time= time.time()\n\nrandom.fit(X_train, y_train)\ny_pred_rand = random.predict(X_test)\n\nrandom_time = time.time()- start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","6bb60f6e":"#RandomForest evaulation\n\nRandom_forest= true_pred('Random_forest', confusion_matrix(y_pred_rand, y_test), y_test)\ndf_randforest_score = algor_scores('Random_forest', y_pred_rand, y_test, random_time)\ndf_randforest_score","cd427ad3":"#Bagging classifier \nTook nearly 35988.0761680603 seconds. Plus the accuracy was really low, hence not considered for analysis. ","bfd652c5":"# Extree Classifier","99933810":"ext_tree_clf = ExtraTreesClassifier()","253c6bf3":"# Model analysis\nstart_time= time.time()\n\next_tree_clf.fit(X_train, y_train)\n\ny_pred_extree = ext_tree_clf.predict(X_test)\n\nextree_time = time.time()- start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","9e5eecc5":"#Extree Model evaluation\n\next_tree = true_pred('ext_tree', confusion_matrix(y_pred_extree, y_test), y_test)\ndf_xtree_score = algor_scores('extree', y_pred_extree, y_test, extree_time)\ndf_xtree_score","fd3002ed":"## KNN Bagging classifier\nfrom sklearn.","3d44b14c":"# if the baggig classifier has 1 neighbor, the values will be same as knn =1, hence trying out with knn=3. \nfrom sklearn.ensemble import BaggingClassifier\nbag_clf_knn = BaggingClassifier(base_estimator= KNeighborsClassifier(n_neighbors=3),\n                            n_estimators=100, random_state = 0)","596aee5c":"# KNN Bagging classifier\nstart_time= time.time()\n\nbag_clf_knn.fit(X_train, y_train)\n\ny_pred_knn_bag = bag_clf_knn.predict(X_test)\n\nknn_bag_time = time.time()- start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","76f915f4":"#model evaluation\n\nBag_class_knn = true_pred('Bag_class_knn', confusion_matrix(y_pred_knn_bag, y_test), y_test)\ndf_bag_knn_score = algor_scores('Bag_knn', y_pred_knn_bag, y_test, knn_bag_time)\ndf_bag_knn_score","0e628865":"## Summary of classifiers","da03f743":"label = [4, 6, 20, 27, 53, 56, 58, 65, 66, 79]\n\ndf_summary= pd.concat([Raw_data,Random_forest,Ada_boost,dec_tree,knn,ext_tree,Bag_class_knn], axis=1, sort=True)\n\ndf_summary['label'] = label\n\ndf_summary.rename(columns={\"profile_id\": \"rawdata_rows#\"}, inplace= True)\ndf_summary= df_summary.sort_values(by=58, ascending= True, axis=1)\ndf_summary","7df44c66":"df_score= pd.concat([df_knn_score, df_dec_tree_score, df_ada_boost_score, df_randforest_score, df_xtree_score,df_bag_knn_score], axis=1, sort=True)\ndf_score = df_score.sort_values(by='accuracy', ascending= True, axis=1)\ndf_score","ab08a434":"## Conclusions\n\n1. Extree classifier provides high accuracy, recall scores. It is also relatively faster. \n2. knn provides faster analysis with reasonable accuracy. \n\n","d5208083":"## Data Exploration and cleaning","5b175ae6":"## Classification Analysis Objective","0dc60005":"1. Using train data change algorithm classify based on profile id's?\n2. Identify critical hyperparameters for KNN similarly analysis can be conducted for other algorithms\n3. Compare algorithms using confusion matrix, time taken to run analysis, accuracy, balanced accuracy. \n4. recommend algorithm for classification analysis. ","9315e722":"# Context\nThe dataset comprises several sensor data collected from a permanent magnet synchronous motor (PMSM) deployed on a test bench. The PMSM represents a german OEM's prototype model. Test bench measurements were collected by the LEA department at Paderborn University. This dataset is mildly anonymized.\nContent\nAll recordings are sampled at 2 Hz. The dataset consists of multiple measurement sessions, which can be distinguished from each other by column \"profile_id\". A measurement session can be between one and six hours long.\nThe motor is excited by hand-designed driving cycles denoting a reference motor speed and a reference torque. Currents in d\/q-coordinates (columns \"i_d\" and i_q\") and voltages in d\/q-coordinates (columns \"u_d\" and \"u_q\") are a result of a standard control strategy trying to follow the reference speed and torque. Columns \"motor_speed\" and \"torque\" are the resulting quantities achieved by that strategy, derived from set currents and voltages.\nMost driving cycles denote random walks in the speed-torque-plane in order to imitate real world driving cycles to a more accurate degree than constant excitations and ramp-ups and -downs would.\nAcknowledgements\nSeveral publications leveraged the setup of the PMSM in the Paderborn University Lab:\n\n\nInspiration\nThe most interesting target features are rotor temperature (\"pm\"), stator temperatures (\"stator_*\") and torque. Especially rotor temperature and torque are not reliably and economically measurable in a commercial vehicle.\nBeing able to have strong estimators for the rotor temperature helps the automotive industry to manufacture motors with less material and enables control strategies to utilize the motor to its maximum capability. A precise torque estimate leads to more accurate and adequate control of the motor, reducing power losses and eventually heat build-up.\n\n(https:\/\/www.kaggle.com\/wkirgsn\/electric-motor-temperature)"}}