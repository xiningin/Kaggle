{"cell_type":{"64da73a4":"code","4689bd3c":"code","8d0f11bd":"code","a1f0cb1a":"code","11e028f7":"code","bc43a7e1":"code","c1311235":"markdown","c4ef9088":"markdown","04babb93":"markdown","541ff358":"markdown","df33d884":"markdown","460217f6":"markdown"},"source":{"64da73a4":"import tensorflow as tf\nimport numpy as np\nfrom sklearn.metrics import f1_score","4689bd3c":"data_path = '\/kaggle\/input\/data-without-drift\/'\ntrain_data_file = data_path + 'train_clean.csv'\ntest_data_file = data_path + 'test_clean.csv'\n\ndef get_data(filename, train=True):\n  \n    if(train):\n        with open(filename) as training_file:\n            split_size = 10\n            data = np.loadtxt(training_file, delimiter=',', skiprows=1)\n            signal = data[:,1]\n            channels = data[:,2]\n            signal = np.array_split(signal, split_size)\n            channels = np.array_split(channels, split_size)\n            data = None\n        return np.array(signal), np.array(channels)\n    else:\n       with open(filename) as training_file:\n            split_size = 4\n            data = np.loadtxt(training_file, delimiter=',', skiprows=1)\n            signal = data[:,1]\n            signal = np.array_split(signal, split_size)\n            data = None\n       return np.array(signal)\n\ntrain_signal , train_channels = get_data(train_data_file)\ntest_signal = get_data(test_data_file, train=False)\n\ntest_model_signal = np.zeros((5,1000000))\ntest_model_channel = np.zeros((5,1000000))\ntest_model_signal[0][:500000] = train_signal[0].flatten()\ntest_model_signal[0][500000:] = train_signal[1].flatten()\ntest_model_signal[1][:500000] = train_signal[2].flatten()\ntest_model_signal[1][500000:] = train_signal[6].flatten()\ntest_model_signal[2][:500000] = train_signal[3].flatten()\ntest_model_signal[2][500000:] = train_signal[7].flatten()\ntest_model_signal[3][:500000] = train_signal[4].flatten()\ntest_model_signal[3][500000:] = train_signal[9].flatten()\ntest_model_signal[4][:500000] = train_signal[5].flatten()\ntest_model_signal[4][500000:] = train_signal[8].flatten()\n\n\ntest_model_channel[0][:500000] = train_channels[0].flatten()\ntest_model_channel[0][500000:] = train_channels[1].flatten()\ntest_model_channel[1][:500000] = train_channels[2].flatten()\ntest_model_channel[1][500000:] = train_channels[6].flatten()\ntest_model_channel[2][:500000] = train_channels[3].flatten()\ntest_model_channel[2][500000:] = train_channels[7].flatten()\ntest_model_channel[3][:500000] = train_channels[4].flatten()\ntest_model_channel[3][500000:] = train_channels[9].flatten()\ntest_model_channel[4][:500000] = train_channels[5].flatten()\ntest_model_channel[4][500000:] = train_channels[8].flatten()","8d0f11bd":"from sklearn.svm import SVC\nmodels = []\n\nspecs = [[1.2,1],[0.1,1],[0.5,1],[7,0.01],[10,0.1]]\n\nfor k in range (5):\n    print(\"starting training model no: \", k)\n    x = test_model_signal[k].flatten()\n    y = test_model_channel[k].flatten()\n    y = np.array(y).astype(int)\n    x = np.expand_dims(np.array(x),-1)\n    model = SVC(kernel = 'rbf', C=specs[k][0],gamma = specs[k][1])\n    samples= 400000\n    #trains by splitting into 10 batches for faster training\n    for i in range(10):\n        model.fit(x[i*samples\/\/10:(i+1)*samples\/\/10],y[i*samples\/\/10:(i+1)*samples\/\/10])\n    y_pred = model.predict(x[400000:500000])\n    y_true = y[400000:500000]\n    print(f1_score(y_true, y_pred, average=None))\n    print(f1_score(y_true, y_pred, average='macro'))\n    models.append(model)","a1f0cb1a":"model_ref = [0,2,4,0,1,3,4,3,0,2,0,0,0,0,0,0,0,0,0,0]\ny_pred_all = np.zeros((2000000))\nfor pec in range(20):\n  print(\"starting prediction of test batch no: \", pec)\n  x_test = test_signal.flatten()[pec*100000:(pec+1)*100000]\n  x_test = np.expand_dims(np.array(x_test),-1)\n  test_pred = models[model_ref[pec]].predict(x_test)\n  y_pred_1 = np.array(test_pred).astype(int)\n  y_pred_all[pec*100000:(pec+1)*100000] = y_pred_1\n\ny_pred_all = np.array(y_pred_all).astype(int)","11e028f7":"model_ref = [0,0,1,2,3,4]\ny_valid = np.zeros((1000000))\ny_pred = np.zeros((1000000))\nfor k in range(6):\n  x = train_signal[k].flatten()\n  y = train_channels[k].flatten()\n  y = np.array(y).astype(int)\n  x = np.expand_dims(np.array(x),-1)\n  model = models[model_ref[k]]\n  y_pred[k*100000:(k+1)*100000] = model.predict(x[400000:500000])\n  y_valid[k*100000:(k+1)*100000]=y[400000:500000]\n\nprint(f1_score(y_valid, y_pred, average=None))\nprint(f1_score(y_valid, y_pred, average='macro'))","bc43a7e1":"import pandas as pd\nsub = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsub.iloc[:,1] = y_pred_all\nsub.to_csv('submission.csv',index=False,float_format='%.4f')\nprint(\"saved the file\")","c1311235":"The following is the testing process. Each batch is of length 100000, which can be easily seen from plotting the signal values. The model for each batch can be manually determined, or by calculating the average of all the entries on each batch and matching the same with the average of training batches.","c4ef9088":"The following is a good estimation of LB. As it is known that the first 600000 or the first 6 batches of testing data are used for the public leaderboard. So, we evaluate the results for 6 batches of validation data from similar models.","04babb93":"Specs below refers to specifications of SVM model, namely C and gamma. You need to have a basic understanding of what an SVM is to understand the math behind the specifications. These were evaluated using a grid search for hyperparameter tuning. Refer to documentation of sklearn.svm.svc for more details. \n\nBelow, the model is trained on the first 400000 entries and validated on the next 100000 entries. The remaining 500000 is unused. You can do undersampling and upsampling to generate a well balanced data but the below also works.","541ff358":"Hi everyone!\n\nThis is my first Kaggle Competition and Kernel. I tried working with Support Vector Machines, and achieved very high F1 macro score with the same. I am sharing my results below.\nDataset used : https:\/\/www.kaggle.com\/cdeotte\/data-without-drift\n\nI have used 5 different SVM models. For more details and detailed plots, go here: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\/output\n\nThe above link explains how 5 different models were used to create synthetic data.\n\nI do not have much experience with Machine Learning, so I have naturally explained in a very simpler manner. Enjoy!","df33d884":"The following writes the testing predictions into csv file for submission:","460217f6":"The below cell will input data and store them in numpy arrays. There are 5 models: Model 0, Model 1...Model 4. They are our estimations of the original respective models used to generate respective batches:\n\n1. Model 0: \n    * **Training Batches** 0,1\n    * **Testing Batches** 0,3,8,10,11,12,13,14,15,16,17,18,19\n    * **Maximum Open Channels**: 1\n2. Model 1: \n    * **Training Batches** 2,6\n    * **Testing Batches** 4\n    * **Maximum Open Channels**: 1\n3. Model 2: \n    * **Training Batches** 3,7\n    * **Testing Batches** 1,9\n    * **Maximum Open Channels**: 3\n4. Model 3: \n    * **Training Batches** 4,9\n    * **Testing Batches** 5,7\n    * **Maximum Open Channels**: 10\n5. Model 4: \n    * **Training Batches** 5,8\n    * **Testing Batches** 2,6\n    * **Maximum Open Channels**: 5\n"}}