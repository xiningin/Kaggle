{"cell_type":{"4fdf1c3f":"code","95a5e9a1":"code","16d333dc":"code","496a2656":"code","0f117598":"code","b11eb64c":"code","dbc94675":"code","816465cd":"code","64587bef":"code","95e21610":"code","43911889":"code","5c8476a5":"code","00d509e0":"markdown","2740da3a":"markdown","e7960d36":"markdown","47978226":"markdown","039bf282":"markdown"},"source":{"4fdf1c3f":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","95a5e9a1":"class LogisticRegression :\n    def __init__(self, iterations , lr):\n        self.iterations = iterations\n        self.lr = lr\n        self.b = None\n        \n    def fit(self , x , y):\n        self.x = x\n        self.y = y\n        self.m , self.n = x.shape\n        self.weights = np.zeros(self.n)\n        self.b = 0\n        \n        #Implementing Gradient Descent\n        for i in range(self.iterations):\n            linReg = np.dot(self.x, self.weights) + self.b\n            y_pred = self.sigmoid(linReg)\n            \n            #Calculating derivatives w.r.t Parameters\n            D_w = (1\/self.m)*np.dot(self.x.T, (y_pred - self.y)) # we could use * because they are arrays # that was the pb why it was not working!\n            D_b = (1\/self.m)*np.sum(y_pred - self.y)\n            \n            #Updating Parameters\n            self.weights = self.weights - self.lr * D_w\n            self.b = self.b - self.lr * D_b\n            \n    def sigmoid(self, X) :\n        return 1 \/ (1 + np.exp(-X))\n            \n    def predict(self , x):\n        linReg = np.dot(x, self.weights) + self.b\n        y_preds = self.sigmoid(linReg)\n        \n        final_preds = [1 if pred>0.5 else 0 for pred in y_preds]\n        return final_preds\n        ","16d333dc":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain.drop('PassengerId', axis = 1, inplace = True)\ndf = train.append(test).reset_index(drop=True)\nfor i in range(len(df)):\n    if not(pd.isnull(df['Cabin'].iloc[i])):\n        df['Cabin'].iloc[i]=df['Cabin'].iloc[i][0] \n    else :\n        df['Cabin'].iloc[i]='No'\n    \n\nn = len(train)\n\ndf['Fsize'] = df['Parch'] + df['SibSp'] + 1\ndf['travelled_alone'] = 'No'\ndf.loc[df.Fsize == 1, 'travelled_alone'] = 'Yes'\ndf['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Capt', 'Col', 'Rev', 'Don', 'Countess', 'Jonkheer', 'Dona', 'Sir', 'Dr', 'Major', 'Dr'], 'Rare')\ndf['Title'] = df['Title'].replace(['Mlle', 'Mme', 'Ms'], 'Miss')\ndf['Title'] = df['Title'].replace(['Lady'], 'Mrs')\ndf.Embarked.fillna(train.Embarked.mode()[0], inplace = True)\nmean = train[\"Age\"].mean()\nstd = train[\"Age\"].std()\n\nis_null = df[\"Age\"].isnull().sum()\n# compute random numbers between the mean, std and is_null\nrand_age = np.random.randint(mean - std, mean + std, size = is_null)\n# fill NaN values in Age column with random values generated\nage_slice = df[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\ndf[\"Age\"] = age_slice\ndf[\"Age\"] = df[\"Age\"].astype(int)\ndf['Fare'].fillna(train['Fare'].mean(), inplace = True)\n\nfeatures = [\"Sex\", \"Pclass\",\"travelled_alone\", \"Cabin\", \"Embarked\", \"Title\"]\n\ndf=pd.get_dummies(df,columns=features,drop_first=True)\ndf.drop(['Name', 'Ticket'], axis = 1, inplace = True)\n\ntrain = df[:n ] # the three outliers\ntest = df[n:]\n\nX = train.drop(['Survived','PassengerId'], axis = 1)\ny = train.Survived\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","496a2656":"model = LogisticRegression(1000, 0.001)","0f117598":"model.fit(x_train,y_train)","b11eb64c":"pred = model.predict(x_test)","dbc94675":"pred[10:20]","816465cd":"y_test[10:20]","64587bef":"accuracy_score(y_test, pred)","95e21610":"from sklearn.linear_model import LogisticRegression","43911889":"lr = LogisticRegression(solver='liblinear')\nlr.fit(x_train,y_train)","5c8476a5":"pred2 = lr.predict(x_test)\naccuracy_score(y_test, pred2)","00d509e0":"Let's apply it on Titanic dataset.\n\nFor pre-processing part I just copied it from one of my previous kernels in this competition, you can take a look at it if you don't understand something.\n\nHere is the link :\n\nhttps:\/\/www.kaggle.com\/khkuggle\/simple-and-intermediate-eda-modeling-for-titanic","2740da3a":"## Coding Time :\n\nWe choose to work with OOP since it is appropriate with that kind of problems, and all models are implemented using OOP in Sickit learn.\n\nThe two functions we need to code are fit and predict, the first one is for model training, i.e it will find the parameters of our model based on the number of iterations and learning rate we are working with; and the second one to get the predictions for new samples.","e7960d36":"Logistic Regression is a supervised ML model analogues to Linear Regression, but tries to predict for categorical variables or discrete ones (yes\/no), to be more precise Logistic Regression is a variation of Linear Regression.\n\nWhen it is suitable?\n\n- if data is binary 0\/1, yes\/no, True\/False\n- if you need probabilistic results.\n\n### 1 . Hypothetical function :\n---\n\nThe output from the hypothesis is the estimated probability. This is used to infer how confident can predicted value be actual value when given an input X. \n\n$$\\hat{Y} = sigmoid(b + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n) = P(Y=1|X) $$\n\nWhere\n$$ sigmoid(t)=\\frac{1}{1+e^{-t}}$$\n\n<img src=\"https:\/\/miro.medium.com\/max\/700\/1*RqXFpiNGwdiKBWyLJc_E7g.png\n\" width=\"400\"><\/img>\n\n $b$ is the biais, and $ \\theta_i $ are the weights of the linear model.\n\n\n### 2 . Cost function :\n---\n\nWe need now to set a specific function to calculate the error of our model since it is not perfect, we call this function $cost function$ :\n\n<blockquote> Cost function is an error function which is used to find the error term of a model on a data.\n    With $\\hat{y}_i$ is the model\u2019s output label.  \n<\/blockquote>\n    \n\n$$J(\\theta) = \\frac{1}{m}\\sum \\limits _{i=1} ^{m} \\hat{y}_i log(\\hat{y}_i) + (1-\\hat{y}_i)log(1-\\hat{y}_i)$$\n\nFor just two variables with $\\theta_0$ is our biais b, we get the shape in the picture :\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Alp-Yilmaz\/publication\/342720152\/figure\/fig3\/AS:910374819864576@1594061679394\/Cost-function-for-Logistic-Regression.png\" \nwidth=\"350\"><\/img>\n\n##### Why cost function which has been used for linear can not be used for logistic?\n\nLinear regression uses mean squared error as its cost function. If this is used for logistic regression, then it will be a non-convex function of parameters (theta). Gradient descent will converge into global minimum only if the function is convex.\n\n\n### 3 . Gradient descent and updates :\n---\n\nCan you set directly the parameters of our Hypothetical function and get the precise predictions you are looking for?! Obviously No even with a little chance! \n\nIf we change randomly the values we may pick the best ones but in the same time our chance to be lost is too large!\n\nIndeed, the only efficient way is to get these parameters when the cost function is in its minimum.\n\nand that is the concept of the famous method Gradient Descent.\n\n<blockquote> Gradient Descent algorithm will change, at each iteration, the values of all $\\theta_i$ and the biais $b$ until the best possible combinaition of parameters is found. <\/blockquote>\n    \n   \nImagine that you are on a hill, and you want to go down it. With each new step (analogy to iteration), you look around to find the best slope to move down. Once you have found the slope, you take a step forward one magnitude $\\alpha$.\n\nHere is an example with one variable how it looks, with w is our parameter in that case :\n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*fU8XFt-NCMZGAWND.\" width=\"400\"><\/img>\n\n\n<br>\n\nHere are the formulas of Gradient Descent for the hypothesis function we are working on from the beginning :\n\n$$ \\theta_i = \\theta_i - \\alpha d\\theta_i$$\n\n$$ b = b - \\alpha db$$\n\nHere are the derivatives we need, you can replace them in the equations :\n\n$$d \\theta_i = - \\frac{2}{m} \\sum \\limits _{i=1} ^{m} x_i (\\hat{y_i} - y)$$\n\n$$d b = - \\frac{2}{m}\\sum \\limits _{i=1} ^{m} (\\hat{y_i} - y)$$\n","47978226":"## If you like that kernel, please don't forget to upvote it !","039bf282":"We got a good score.\n\nLet's try now with the real model from scikit learn:"}}