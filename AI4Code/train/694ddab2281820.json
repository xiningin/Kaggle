{"cell_type":{"b92984a7":"code","3d03bfcf":"code","96012f6e":"code","3e54f081":"code","db86040a":"code","910f7d22":"code","cbbb817e":"code","49e2c243":"code","4d989ab6":"code","41601dc2":"code","712af423":"code","230c55d5":"code","e6f2e77b":"code","a26a73b2":"code","de6a2bbf":"markdown","665fbee0":"markdown","ecce034b":"markdown","15c96928":"markdown","5eed1b04":"markdown"},"source":{"b92984a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d03bfcf":"#read data\ndata = pd. read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","96012f6e":"#show data\nprint(data.head())\nprint(data.columns)","3e54f081":"#see unique labels\nprint(data.Class.value_counts())","db86040a":"#Separate majority and minority (this sound teribble lol)\n#then take a sample of the majority\ndata1 = data[data.Class==1]\ndata0 = data[data.Class==0].sample(len(data1.Class)*2)\n\n#print for sanity\nprint(data0.shape)\nprint(data1.shape)\n\n#combine data\ndata = pd.concat([data1,data0])\nprint(data.shape)","910f7d22":"#prepare the data\n\nx = data.drop(columns=[\"Class\"])\ny = data.Class\nprint(x.shape, y.shape)","cbbb817e":"from sklearn.model_selection import train_test_split as tts\n\n#split data to train and test\n#use seed for shuffle for sanity sake\n#stratify y for proportional distribution\n\nx_train, x_test, y_train, y_test = tts(x, y ,test_size = 0.2, shuffle=True, random_state = 21, stratify=y)\nprint(x_train.shape,y_train.shape)\n\n#sanity check for y distribution\nprint(y_train.mean(), y_test.mean())","49e2c243":"from sklearn.preprocessing import StandardScaler\n\n#standarized the data\nx_train_std = StandardScaler().fit_transform(x_train)\nx_test_std = StandardScaler().fit_transform(x_test)\n\n#python array to numpy just for safe\ny_train=y_train.to_numpy()\ny_test=y_test.to_numpy()","4d989ab6":"import tensorflow as tf\n#importing tensorflow","41601dc2":"#callbacks for premature stop, just in case lol\n\nclass uShallStopNow(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.965):\n      print(\"\\n96.5% GOOD ENOUGH!!!!\")\n      self.model.stop_training = True","712af423":"\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(30,)),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel.summary()","230c55d5":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01), \n              loss='binary_crossentropy',\n              metrics=['accuracy'])","e6f2e77b":"model.fit(x_train_std, y_train, epochs=80,callbacks=[uShallStopNow()],validation_data=(x_test_std,y_test))\n","a26a73b2":"#preditc and plot cf matrix\nprediction = model.predict_classes(x_test_std, verbose=1)\nprint(prediction.transpose().reshape(-1,).shape)\nprint(y_test.shape)\nprint(y_test.sum())\ncf_matrix = tf.math.confusion_matrix(prediction.transpose().reshape(-1,), y_test)\n#plot plot plot\nimport seaborn as sns\nsns.heatmap(cf_matrix, annot=True)","de6a2bbf":"The data is highly skewed lets just sample some majority data twice as large of the minority data","665fbee0":"lets just use 1 neuron clasification **because an ANN is overkill lol*","ecce034b":"BAM! good enough for a single brain cell identifying a fraudulent tx","15c96928":"lets Standarize x and make y numpy array ","5eed1b04":"The data consist of time, features, amount, and class.\nThe class column is the clasification for fraudulent tx."}}