{"cell_type":{"89b92986":"code","245bbc36":"code","2a7be426":"code","894db773":"code","902cad87":"code","d21154e0":"code","c48b4ca0":"code","bb026edf":"code","26b39d32":"code","c804e74c":"code","b33a183e":"code","b2268d84":"code","517d612c":"code","952ce052":"code","d2bb5979":"code","bd067aae":"code","f27ac433":"code","b01fb83f":"code","65698cba":"code","5e497295":"code","1a3052f5":"code","3ff5f7c5":"code","92bf7330":"code","651c2ca4":"code","e844dc3b":"code","0615b0cc":"code","b524215a":"code","8723a958":"code","c78f5db1":"code","f68d1905":"code","a869fc94":"code","65299124":"code","8491b779":"code","8d5b1667":"code","4b6f4ad3":"code","9b750957":"code","9f0a6477":"code","1d97617d":"code","5b33a63b":"code","4ee20a36":"code","16db7190":"code","f5e2aeb4":"code","64228d33":"code","3a3186ea":"code","45aabb06":"code","4de559d3":"code","b8edb406":"code","01607c18":"code","32dc2c7e":"code","fbac196e":"code","75efc9c7":"code","d18ab709":"code","75dec4e7":"code","a869ed34":"code","44898aee":"code","65026e2e":"markdown","c84a80c6":"markdown","a70ba9d2":"markdown","b9073c17":"markdown","ad0d3721":"markdown","d6707c27":"markdown","743fd288":"markdown","6b842dde":"markdown","65c9c507":"markdown","a60d69db":"markdown","6db7bdf7":"markdown","9a42267f":"markdown","61803af3":"markdown","ab0c48b0":"markdown","658a1ced":"markdown","0cfd8c44":"markdown","2fa90230":"markdown","83540268":"markdown","9fa1582b":"markdown","96d22580":"markdown","4cc3423c":"markdown","0d227c9d":"markdown"},"source":{"89b92986":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","245bbc36":"from sklearn import feature_selection\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import RFE\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nimport collections\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.model_selection import ParameterGrid\nfrom imblearn.over_sampling import ADASYN","2a7be426":"train=pd.read_csv(\"..\/input\/santander-customer-satisfaction\/train.csv\",index_col=\"ID\")\ntest_X=pd.read_csv(\"..\/input\/santander-customer-satisfaction\/test.csv\",index_col=\"ID\")","894db773":"train.describe()","902cad87":"train.info()","d21154e0":"test_X.info()","c48b4ca0":"train.isnull().sum().sum()","bb026edf":"test_X.isnull().sum().sum()","26b39d32":"sns.set_style(\"whitegrid\")\nsns.countplot(x=\"TARGET\",data=train)","c804e74c":"df=pd.DataFrame(train.TARGET.value_counts())\ndf['percentage']=100*df['TARGET']\/train.shape[0]\ndf","b33a183e":"# -999999 in var 3 means unkown so we are replacing it with the most common value in var3\ntrain.var3 = train.var3.replace(-999999,2)\ntest_X.var3 = test_X.var3.replace(-999999,2)","b2268d84":"train.loc[train.var3==-999999].shape","517d612c":"test_X.loc[test_X.var3==-999999].shape","952ce052":"train_X, val_X, train_y, val_y = train_test_split(train.drop(labels=['TARGET'], axis=1),train['TARGET'],test_size=0.2,random_state=1)","d2bb5979":"#Use Variance Thereshold to remove both constant,quasi-constant features\nselector = VarianceThreshold(threshold=0.01)","bd067aae":"selector.fit(train_X)","f27ac433":"constArr=selector.get_support()","b01fb83f":"constCol=[col for col in train_X.columns if col not in train_X.columns[constArr]]","65698cba":"# constant features\nconstCol","5e497295":"#check\ntrain_X.ind_var2_0.unique()","1a3052f5":"#dropping constant features from train,test,val set\ntrain_X.drop(columns=constCol,axis=1,inplace=True)\ntest_X.drop(columns=constCol,axis=1,inplace=True)\nval_X.drop(columns=constCol,axis=1,inplace=True)","3ff5f7c5":"print(train_X.shape)\nprint(test_X.shape)\nprint(val_X.shape)","92bf7330":"def duplicateColumns(data):\n    dupliCols=[]\n    for i in range(0,len(data.columns)):\n        col1=data.columns[i]\n        for col2 in data.columns[i+1:]:\n            if data[col1].equals(data[col2]):\n                dupliCols.append(col1+','+col2)\n    return dupliCols","651c2ca4":"dupCol=duplicateColumns(train_X)\ndCols=[col.split(',')[1] for col in dupCol]\ndCols","e844dc3b":"dupCol","0615b0cc":"dCols=list(set(dCols))","b524215a":"train_X.drop(columns=dCols,axis=1,inplace=True)\nval_X.drop(columns=dCols,axis=1,inplace=True)\ntest_X.drop(columns=dCols,axis=1,inplace=True)","8723a958":"def correlation(dataset,threshold):\n    col_corr=set() # set will contains unique values.\n    corr_matrix=dataset.corr() #finding the correlation between columns.\n    for i in range(len(corr_matrix.columns)): #number of columns\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j])>threshold: #checking the correlation between columns.\n                colName=corr_matrix.columns[i] #getting the column name\n                col_corr.add(colName) #adding the correlated column name heigher than threshold value.\n    return col_corr #returning set of column names","c78f5db1":"corrCol=list(correlation(train_X,0.8))","f68d1905":"len(corrCol)","a869fc94":"train_X.drop(columns=corrCol,axis=1,inplace=True)\nval_X.drop(columns=corrCol,axis=1,inplace=True)\ntest_X.drop(columns=corrCol,axis=1,inplace=True)","65299124":"scaler=StandardScaler()","8491b779":"train_sca_X = scaler.fit_transform(train_X)\ntest_sca_X = scaler.transform(test_X)\nval_sca_X = scaler.transform(val_X)","8d5b1667":"pd.DataFrame(train_sca_X,columns=train_X.columns,index=train_X.index)","4b6f4ad3":"sm = SMOTE(random_state=42)\ntrain_res_X, train_res_y = sm.fit_resample(train_sca_X, train_y)","9b750957":"train_res_y.value_counts()","9f0a6477":"baseline_xgb_clf = XGBClassifier(random_state=20)","1d97617d":"baseline_xgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])","5b33a63b":"pred_y = baseline_xgb_clf.predict_proba(val_sca_X)[:,1]","4ee20a36":"roc_auc_score(val_y,pred_y)","16db7190":"# param_grid = {\"learning_rate\"    : [0.05, 0.10] ,\n#  \"max_depth\"        : [5, 6, 8, 10],\n#  \"min_child_weight\" : [ 1, 3, 5],\n#  \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4],\n#  \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }","f5e2aeb4":"xgb_clf=XGBClassifier()","64228d33":"# best_score=0\n# for g in ParameterGrid(param_grid):\n#      xgb_clf.set_params(**g)\n#      xgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])\n#      pred_y = xgb_clf.predict_proba(val_sca_X)[:,1]\n#      score=roc_auc_score(val_y,pred_y)\n#      if score > best_score:\n#          best_score = roc_auc_score(val_y,pred_y)\n#          best_grid = g","3a3186ea":"best_grid={'colsample_bytree': 0.7,\n 'gamma': 0.1,\n 'learning_rate': 0.1,\n 'max_depth': 6,\n 'min_child_weight': 1}","45aabb06":"xgb_clf.set_params(**best_grid)\nxgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])","4de559d3":"pred_y = xgb_clf.predict_proba(test_sca_X)[:,1]","b8edb406":"subs = pd.DataFrame(({'ID': test_X.index, 'TARGET': pred_y}))","01607c18":"param_grid = {\"learning_rate\"    : [0.03] ,\n  \"max_depth\"        : [6],\n \"min_child_weight\" : [ 0,1],\n \"gamma\"            : [ 0.1],\n\"n_estimators\": [150,200,250],\n\"colsample_bytree\" : [ 0.8,0.85] }","32dc2c7e":"len(list(ParameterGrid(param_grid)))","fbac196e":"best_score=0\nfor g in ParameterGrid(param_grid):\n    xgb_clf.set_params(**g)\n    xgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])\n    pred_y = xgb_clf.predict_proba(val_sca_X)[:,1]\n    score=roc_auc_score(val_y,pred_y)\n    if score > best_score:\n        best_score = roc_auc_score(val_y,pred_y)\n        best_grid = g","75efc9c7":"best_score","d18ab709":"best_grid","75dec4e7":"# best_grid2={'colsample_bytree': 0.8,\n# 'gamma': 0.0,\n#  'learning_rate': 0.03,\n#  'max_depth': 6,\n#  'min_child_weight': 1,\n#  'n_estimators': 350}","a869ed34":"#best_score2=0.8262192223595399","44898aee":"#subs.to_csv('submission.csv', index=False)","65026e2e":"All features are numeric","c84a80c6":"# DATA CLEANING","a70ba9d2":"For detailed EDA please see https:\/\/www.kaggle.com\/cast42\/exploring-features#Clusters","b9073c17":"*Remove Duplicate features*","ad0d3721":"The Above Model has performed relatively well. Let's see if we could improve results by hyperparameter tuning","d6707c27":"Huge Class Imbalance as shown above","743fd288":"# Oversample Data","6b842dde":"Another set of grid search to find best hyperparameter","65c9c507":"# Modelling with xgboost","a60d69db":"*Feature Variance Analysis*","6db7bdf7":"We have finished using filter method to select features","9a42267f":"*Removing Features that are highly correlated to each other*","61803af3":"Using Grid Search Cross Validation to find best hyperparameters","ab0c48b0":"Validation & Train set Split","658a1ced":"# DATASET ANALYSIS","0cfd8c44":"# Hyperparameter Tuning","2fa90230":"We arrive at 0.8265 accuracy it is not the best I might come back at this problem later. But it should be a fairly good notebook to show we tackle imbalanced class classification problem","83540268":"There is no missing data in both test, train set","9fa1582b":"*Scale the data*","96d22580":"*oversample data with smote*","4cc3423c":"We have finished hyperparameter tuning. Now we should make a prediction on the test set","0d227c9d":"delete the any oen of them will be fine but we decide to delete the first column"}}