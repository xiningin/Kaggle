{"cell_type":{"8eda623d":"code","6a159590":"code","ee071a30":"code","e4631ef6":"code","467275d3":"code","50f92e46":"code","442981c0":"code","ce922976":"code","e732470d":"code","0c6c6f07":"code","9ca31af1":"code","02239068":"code","25f9582a":"code","4ce28be5":"code","696c9cf0":"code","45a2b4ea":"code","4d816765":"code","127fdb6a":"code","6619a747":"code","184a9b00":"code","230b22d6":"code","a36cee57":"code","238c1fe9":"code","fdc4fcfd":"code","379dfc86":"code","97e53c6c":"code","827037f8":"code","23af7c60":"code","780b7468":"code","93165293":"code","0f0ba1dc":"code","b051b2fb":"code","56b20a5d":"markdown","7c36d6e1":"markdown","c7984ad0":"markdown","f4afcb3e":"markdown","5f1900d4":"markdown","86361c95":"markdown","0ec8babb":"markdown","aacbb05d":"markdown","85d19a1d":"markdown","f4b029ad":"markdown","0141fe70":"markdown","e0b895b9":"markdown","624a6e6a":"markdown","8b0c7ef3":"markdown","d8b82b1a":"markdown","67a94ad0":"markdown","ce392a49":"markdown","59bfdbbd":"markdown","65498223":"markdown","dd75641b":"markdown","cfb28cbd":"markdown","a8fa93b8":"markdown","15c81764":"markdown","4cab8728":"markdown","d24c0a48":"markdown","f81426a0":"markdown","b19495fa":"markdown","27c64a52":"markdown","c05fa282":"markdown","acd961ef":"markdown","ddd9f918":"markdown","312a3596":"markdown","f0cc11bf":"markdown","a508c7b2":"markdown","0b5b4eb8":"markdown","66f7a88f":"markdown"},"source":{"8eda623d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.utils import resample\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score\nfrom sklearn import tree as t\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nimport graphviz","6a159590":"!head '\/kaggle\/input\/loan-prediction-part-1\/test_clean.csv'","ee071a30":"%%time\ndata = pd.read_csv('\/kaggle\/input\/loan-prediction-part-1\/train_clean.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/loan-prediction-part-1\/test_clean.csv')","e4631ef6":"\"{:.2f} MB on disk\".format(data.memory_usage().sum().sum() \/ 1024**2)","467275d3":"data.info()","50f92e46":"# Credit: https:\/\/www.kaggle.com\/wkirgsn\/fail-safe-parallel-memory-reduction\nfrom fail_safe_parallel_memory_reduction import Reducer","442981c0":"%%time\ndata = Reducer().reduce(data)","ce922976":"data.info()","e732470d":"plt.figure(figsize=(12,4))\nplt.subplot(121)\nsns.barplot(data.Loan_Status.value_counts(),y=['Approved (1)','Rejected (0)'])\nplt.subplot(122)\ndata.Loan_Status.value_counts(normalize=True).plot.pie(autopct=\"%.2f%%\")\nplt.show()","0c6c6f07":"#drop ID columns\ndata.drop(columns=['Loan_ID'],inplace=True)","9ca31af1":"X,y = data.drop(columns=['Loan_Status']), data['Loan_Status']\nprint(X.shape,y.shape,sep='\\n') ","02239068":"# Use a stratified train-test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2)\n\nprint(\"Train distribution\",y_train.value_counts(normalize=True),sep=\"\\n\")\n\nprint(\"Test distribution\",y_test.value_counts(normalize=True),sep=\"\\n\")","25f9582a":"y_train.value_counts()","4ce28be5":"# Use Oversampling to upscale the train data\nsm = SMOTE()\nX_s,y_s =sm.fit_sample(X_train,y_train)","696c9cf0":"pd.Series(y_s).value_counts()","45a2b4ea":"print(\"In binary classification, as per sklearn confusion_matrix function\",\"the count of true negatives is C00\",\"false negatives is C10\",\n      \"true positives is C11\",\"and false positives is C01.\",sep=\"\\n\")\ndef classification_metrics(y_actual,y_predict):\n    # get performance metrics\n    print(classification_report(y_actual,y_predict))\n\n    # confusion matrix\n    sns.heatmap(confusion_matrix(y_test,y_predict),cmap='coolwarm',annot=True)\n    plt.show()\n    \n# https:\/\/www.kaggle.com\/paultimothymooney\/decision-trees-for-binary-classification-0-99 \ndef plot_decision_tree(a,b):\n    dot_data = t.export_graphviz(a, out_file=None, \n                             feature_names=b,  \n                            # 0,1\n                             class_names=['Ineligible','Eligible'],  \n                             filled=False, rounded=True,  \n                             special_characters=False)  \n    graph = graphviz.Source(dot_data)  \n    return graph ","4d816765":"# fit model\ndt = DecisionTreeClassifier(random_state=5)\ntree = dt.fit(X_s,y_s)\n\nclassification_metrics(y_test,tree.predict(X_test))","127fdb6a":"f1_score(y_test,tree.predict(X_test))","6619a747":"# Visualize the tree classifier\nplot_decision_tree(tree,X.columns)","184a9b00":"# fit model\ntree = DecisionTreeClassifier(class_weight='balanced',random_state=5).fit(X_train,y_train)\n\n# get performance metrics\nprint(classification_report(y_test,tree.predict(X_test)))\n\n# confusion matrix\nsns.heatmap(confusion_matrix(y_test,tree.predict(X_test)),cmap='coolwarm',annot=True)\nplt.show()","230b22d6":"# It shows the f1-score for Loan Approvals\nf1_score(y_test,tree.predict(X_test))","a36cee57":"#random forest with original features\nrfc = RandomForestClassifier(random_state=5,class_weight=\"balanced\").fit(X_train,y_train)\n\nclassification_metrics(y_test,rfc.predict(X_test))\npd.DataFrame(rfc.feature_importances_,index=X_test.columns,columns=['importance']).sort_values(by='importance').plot.barh()\nplt.show()","238c1fe9":"X_train1,X_test1 = X_train[::],X_test[::]\n\nX_train1['Total Income'] = X_train['ApplicantIncome'] + X_train['CoapplicantIncome']\n# X_train1['Credibility'] = X_train['LoanAmount'] \/ X_train1['Total Income']\nX_test1['Total Income'] = X_test['ApplicantIncome'] + X_test['CoapplicantIncome']\n# X_test1['Credibility'] = X_test['LoanAmount'] \/ X_test1['Total Income']\n\nX_train1.drop(columns=['ApplicantIncome','CoapplicantIncome'],inplace=True)\nX_test1.drop(columns=['ApplicantIncome','CoapplicantIncome'],inplace=True)\n\n#random forest with new feature\nrfc = RandomForestClassifier(random_state=5).fit(X_train1,y_train)\n\nclassification_metrics(y_test,rfc.predict(X_test1))\n\npd.DataFrame(rfc.feature_importances_,index=X_test1.columns,columns=['importance']).sort_values(by='importance').plot.barh()\nplt.show()","fdc4fcfd":"# Credit: https:\/\/www.kaggle.com\/mlwhiz\/how-to-use-hyperopt","379dfc86":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials","97e53c6c":"def objective(space):\n    \n    rfc = RandomForestClassifier(\n        random_state=5,\n        class_weight=\"balanced\",\n        max_depth = space['max_depth']\n    ).fit(X_train,y_train)\n\n    f1 = f1_score(y_test,rfc.predict(X_test))\n    \n    # return needs to be in this below format. \n    # We use negative of accuracy since we want to maximize it.\n    return {'loss': -f1, 'status': STATUS_OK }","827037f8":"# Using single parameter to show its use\nspace = {\n    'max_depth': hp.quniform(\"x_max_depth\", 4, 16, 1),\n    }","23af7c60":"%%timw\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\nprint(best)","780b7468":"rfc = RandomForestClassifier(\n    random_state=5,\n    class_weight=\"balanced\",\n    max_depth = 9\n).fit(X_train,y_train)\n\nprediction = rfc.predict(X_test)","93165293":"submission.drop(columns = ['Unnamed: 0','Loan_ID'], inplace=True)\npredictions = rfc.predict(submission)","0f0ba1dc":"submission['predictions'] = predictions","b051b2fb":"submission[[\"predictions\"]].sample(10).style.background_gradient(cmap='Accent_r')","56b20a5d":"**Datatypes of various features before reduction.**","7c36d6e1":"# Choosing evaluation metrics for classification","c7984ad0":"Comparing with the above table, we are having a **Mild** degree of imbalance.\n\nThere are many methods but we will discuss about three main techniques for treating imbalanced dataset, you can employ these methods whenever these is a high or extreme case \n\n1. Oversampling \/ Upsampling the lower class target variable\n2. Undersampling \/ Downsampling the higher class target variable\n3. Class weight\n\n> `sklearn.resample` provides  module to leverage both over and undersampling.<br\/>Another important library is `imblearn` which provides more sophisticated up and down sampling methods.","f4afcb3e":"Just before that, let's create few helper functions \ud83d\udd28","5f1900d4":"**We were able to reduce the memory footprint to almost 1\/3 in a very short time.**","86361c95":"**Experiment 1: TotalIncome**","0ec8babb":"Since, now we have similar number of target values we can train on our model without facing any bias.","aacbb05d":"The memory footprint of this dataframe is small. However, for the purpose of learning, we will be reducing the memory usage of this dataframe.","85d19a1d":"**Last commit on api link was on 29th Nov 2019**\n\n[Demo link](https:\/\/loan-status-api.herokuapp.com\/apidocs\/)","f4b029ad":"> fi-score: 0.76","0141fe70":"---","e0b895b9":"# Import necessary dependencies and load data","624a6e6a":"# Treating imbalanced data\n\nAs we know, in this dataset we do not have highly imbalanced dataset. So it is not necessary to use any imbalance treatment here.\n\n![image.png](attachment:image.png)\n\nCredits: developers.google.com","8b0c7ef3":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-necessary-dependencies-and-load-data\" data-toc-modified-id=\"Import-necessary-dependencies-and-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import necessary dependencies and load data<\/a><\/span><\/li><li><span><a href=\"#Reducing-memory-usage\" data-toc-modified-id=\"Reducing-memory-usage-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Reducing memory usage<\/a><\/span><\/li><li><span><a href=\"#Treating-imbalanced-data\" data-toc-modified-id=\"Treating-imbalanced-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Treating imbalanced data<\/a><\/span><\/li><li><span><a href=\"#Choosing-evaluation-metrics-for-classification\" data-toc-modified-id=\"Choosing-evaluation-metrics-for-classification-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Choosing evaluation metrics for classification<\/a><\/span><\/li><li><span><a href=\"#Baseline-model\" data-toc-modified-id=\"Baseline-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Baseline model<\/a><\/span><\/li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Feature engineering<\/a><\/span><\/li><li><span><a href=\"#Model-selection\" data-toc-modified-id=\"Model-selection-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Model selection<\/a><\/span><\/li><li><span><a href=\"#Hyperparameter-tuning\" data-toc-modified-id=\"Hyperparameter-tuning-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Hyperparameter tuning<\/a><\/span><\/li><li><span><a href=\"#Deployment-demo\" data-toc-modified-id=\"Deployment-demo-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Deployment demo<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#If-you-like-my-work,please-do-upvote-\ud83d\udc4d\" data-toc-modified-id=\"If-you-like-my-work,please-do-upvote-\ud83d\udc4d-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>If you like my work,please do upvote \ud83d\udc4d<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>\n\nIf you missed out the part 1 of this notebook, you can check it out from [this link here.](https:\/\/www.kaggle.com\/psvishnu\/loan-prediction-part-1\/)\n\n**Kernal Status:** IN PROGRESS \ud83d\udc77\u200d\n","d8b82b1a":"**Method 2: Using class weights**","67a94ad0":"# Baseline model\n\nSince the label encoding has been applied on categorical variables(for both nominal and ordinal) linear models will give invalid result. We will go with Decision Trees as baseline model.","ce392a49":"---","59bfdbbd":"---","65498223":"**As mentioned in the above kernel defining hyperopt is a 4 step procedure**\n1. Import required library\n2. Create objective function, in order to either minimize the loss or maximize the metric\n3. Create search space for the classifier\n4. Run hyperopt","dd75641b":"**Modified data types**","cfb28cbd":"# Deployment demo","a8fa93b8":"# Output","15c81764":"---","4cab8728":"# Reducing memory usage","d24c0a48":"In imbalanced datasets, accuracy will not be the right measure because the metric will be biased towards the class having maximum number of observations\n\nSo we will go with any of the following metrics \n\n<img src=\"https:\/\/i.stack.imgur.com\/U0hjG.png\"\/>","f81426a0":"**Method 1: Oversampling method**\n\nTarget variable count before sampling","b19495fa":"Target variable count after sampling","27c64a52":"We will be using an utility script for doing all the heavy lifting for us.","c05fa282":"<center><img src=\"https:\/\/cdn.pixabay.com\/photo\/2016\/04\/25\/23\/30\/home-1353389_1280.jpg\" width=\"600px\"\/><\/center>","acd961ef":"**Which one to choose in our case ?**\n\nAlready we are just having 614 data samples and using `undersampling` would further reduce the number. So, we can go ahead with `oversampling` or`class weight` parameter in model. \n\nDisadvantage of oversampling could be artificial data. But if you want to use undersampling then we can consider an ensemble of models being trained on multiple undersampled dataset.","ddd9f918":"# Model selection + Feature engineering","312a3596":"---","f0cc11bf":"**Precision \/ Sensitivity:** Out of all the `predicted positives`, how many of them are True ones. A low precision means there are many False positives. False positives in our case would mean, we approved loans for many individuals who are \n\n**Specificity:** Opposite of Precision, correctness of Negative predictions.\n\n**Recall:** Out of all the `actual positives`, how many could you correctly identify. A low recall implies the model has predicted many `false negative` values.\n\n**F1 score:** The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.","a508c7b2":"Image here\n<!-- <img src=\"https:\/\/i.ytimg.com\/vi\/AYZz_qYw_j4\/maxresdefault.jpg\" width=\"400px\"\/> -->\n\n<h2>If you like my work,please do upvote \ud83d\udc4d<\/h2>","0b5b4eb8":"# Hyperparameter tuning","66f7a88f":"### **What to choose for our case?**\n\nIt depends on the priority of the company,\n\n- *High risk- High return* -> `Recall`: Approving more loans but at the risk of some house loan delinquency. Loan repayment lapses when we predict people would pay back loans but they don't which means False positive cases are more. Usually False positives are on low priority when we are choosing recall as our metrics.\n- *Low risk- Low return* -> `Precision`: Or approving few loans to individuals with high chance of repayment, this case we might even reject people who had the capability to pay back the loan on time and thereby we are loosing on profits such cases can be classified as False Negatives. Precision optimizes\/decreases the number of False Positives so we can employ it here\n \nWe cannot optimize for all metrics. However, if the firm needs a right mix of both then we will choose f1-score for optimization.\n\nIn this case study, my priority is to go with he f1-score of Loan approval."}}