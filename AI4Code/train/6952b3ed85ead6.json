{"cell_type":{"349710db":"code","3260b600":"code","388cc990":"code","b3e2df96":"code","edad50fa":"code","f74e2801":"code","1abb7f6f":"code","a0ac9576":"code","fbdca76b":"code","7633affc":"code","ed884411":"code","565cd238":"code","1ebfda0c":"code","26258090":"code","195a3f85":"code","e54a6329":"code","0acf4e41":"code","a8ff3d85":"code","cc84a69f":"code","f2158555":"markdown","a70df4e6":"markdown","e85476a0":"markdown","560a13cd":"markdown","67bb96eb":"markdown","cb593340":"markdown","29d9bc11":"markdown"},"source":{"349710db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Here i  would like to import urllib and bs4 packages . Both are really helpfull to scrap out data direct from web with ease . urllib is a really good http client for\n# python providing many advantages of its own . Beautifulsoup is a html parser and makes our really easy and simple . \n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_rows = 999","3260b600":"# Following are the links from which i will scrape data . urlopen is used to create a http request and response is been saved in open_link variables  \nopen_link_cpu = urlopen('https:\/\/store.steampowered.com\/hwsurvey\/cpus\/').read()\nopen_link_gpu = urlopen('https:\/\/store.steampowered.com\/hwsurvey\/videocard\/').read()\nopen_link_cpu_frequency = urlopen('https:\/\/store.steampowered.com\/hwsurvey\/processormfg\/').read() ","388cc990":"# soup variable is used to store parsed content from the response of http request . \nsoup=BeautifulSoup(open_link_cpu, \"html.parser\")\n\n# find_all() method finds all the html element with specified tag as arguments .\n# for e.g here after inspecting steam website in browser we will store all the as bs4.element.ResultSet in the following variables\ncategory_col = soup.find_all('div' , class_ = 'substats_col_left col_header')\nsubstats_col_left = soup.find_all('div' , class_ = 'substats_col_left')\nsubstats_col_month = soup.find_all('div' , class_ = 'substats_col_month')\nsubstats_col_month_last_pct = soup.find_all('div' , class_ = 'substats_col_month_last_pct')\nsubstats_col_month_last_chg = soup.find_all('div' , class_ = 'substats_col_month_last_chg')","b3e2df96":"# Here using .text method we will extract text information safed in each of the div tags and safe them in individual lists using list comprehension .\n\nos_type = [ item.text for item in category_col]\njul = [item.text for item in substats_col_month_last_pct]\ntype_of_cpu = [ item.text for item in substats_col_left]\npct_chng = [ item.text for item in substats_col_month_last_chg]","edad50fa":"# Since our soup variable - substats_col_month contains values for each of the 4 months i.e march , april , may , june we will itearate a loop to \n# to save values their respective variables .\nmar=[]\napr=[]\nmay=[]\njun=[]\n\ni=0\nfor item in substats_col_month:\n    if i % 4 == 0:\n        mar.append(item.text)\n        i += 1\n    elif i % 4 == 1:\n        apr.append(item.text)\n        i += 1\n    elif i % 4 == 2:\n        may.append(item.text)\n        i += 1\n    elif i % 4 == 3:\n        jun.append(item.text)\n        i += 1","f74e2801":"# This variable is been personally added by me to somewhat a little preprocessing before utilizing this data set .   \n\nos = []\n\ni=0\nfor item in os_type :\n    if i == 0  :\n        for j in range(0,21):\n            os.append('WINDOWS')\n        i += 1\n    elif i == 1:\n        for j in range(21,32):\n            os.append('OSX')\n        i += 1\n    elif i == 2:\n        for j in range(32,52):\n            os.append('LINUX')\n        i += 1","1abb7f6f":"# Defining a dataset and then saving it to csv format in a output file which i will put up as a seprate dataset on kaggle .\n\npc_physical_details_cpu = pd.DataFrame({'CPU CORES':type_of_cpu,\n                                        'OS':os,\n                                        'MAR': mar,\n                                        'APR': apr,\n                                        'MAY': may,\n                                        'JUN': jun,\n                                        'JUL': jul,\n                                        '% CHANGE':pct_chng})\n\npc_physical_details_cpu.drop([0,24,35], axis = 0, inplace = True)\npc_physical_details_cpu.reset_index(drop=True , inplace = True)\npc_physical_details_cpu.to_csv('D:\\DataScience\\datasets\\cpu_cores.csv')","a0ac9576":"soup=BeautifulSoup(open_link_gpu, \"html.parser\")\ncategory_col = soup.find_all('div' , class_ = 'substats_col_left col_header')\nsubstats_col_left = soup.find_all('div' , class_ = 'substats_col_left')\nsubstats_col_month = soup.find_all('div' , class_ = 'substats_col_month')\nsubstats_col_month_last_pct = soup.find_all('div' , class_ = 'substats_col_month_last_pct')\nsubstats_col_month_last_chg = soup.find_all('div' , class_ = 'substats_col_month_last_chg')","fbdca76b":"category = [ item.text for item in category_col]\njul = [item.text for item in substats_col_month_last_pct]\ngpu_name = [ item.text for item in substats_col_left]\npct_chng = [ item.text for item in substats_col_month_last_chg]","7633affc":"mar=[]\napr=[]\nmay=[]\njun=[]\n\ni=0\nfor item in substats_col_month:\n    if i % 4 == 0:\n        mar.append(item.text)\n        i += 1\n    elif i % 4 == 1:\n        apr.append(item.text)\n        i += 1\n    elif i % 4 == 2:\n        may.append(item.text)\n        i += 1\n    elif i % 4 == 3:\n        jun.append(item.text)\n        i += 1\n        ","ed884411":"directx = []\n\ni=0\nfor item in category :\n    if i == 0  :\n        for j in range(0,64):\n            directx.append('DIRECTX 12 GPUS')\n        i += 1\n    elif i == 1:\n        for j in range(64,98):\n            directx.append('DIRECTX 11 GPUS')\n        i += 1\n    elif i == 2:\n        for j in range(98,137):\n            directx.append('DIRECTX 10 GPUS')\n        i += 1\n    elif i == 3:\n        for j in range(137,175):\n            directx.append('DIRECTX 9 SHADER MODEL 2B AND 3.0 GPUS')\n        i += 1\n    elif i == 4:\n        for j in range(175,189):\n            directx.append('DIRECTX 9 SHADER MODEL 2.0 GPUS')\n        i += 1\n        ","565cd238":"combined_data_gpu = pd.DataFrame({'GPU NAME':gpu_name,\n                                        'MAR': mar,\n                                        'APR': apr,\n                                        'MAY': may,\n                                        'JUN': jun,\n                                        'JUL': jul,\n                                        '% CHANGE':pct_chng})\n\ncombined_data_gpu\n","1ebfda0c":"pc_physical_details_gpu , directx_distribution = combined_data_gpu[8:106] , combined_data_gpu[107:]\npc_physical_details_gpu.reset_index(drop = True , inplace = True)\npc_physical_details_gpu.to_csv('D:\\DataScience\\datasets\\gpu.csv')","26258090":"col = ['GPU NAME','DIRECTX', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', '% CHANGE']\ndirectx_distribution['DIRECTX'] = directx\ndirectx_distribution=directx_distribution[col]\ndirectx_distribution.drop([171,205,244,282],axis = 0 , inplace = True)\ndirectx_distribution.reset_index(drop = True , inplace = True)\ndirectx_distribution.to_csv('D:\\DataScience\\datasets\\gpu_directx.csv')","195a3f85":"soup=BeautifulSoup(open_link_cpu_frequency, \"html.parser\")\ncategory_col = soup.find_all('div' , class_ = 'substats_col_left col_header')\nsubstats_col_left = soup.find_all('div' , class_ = 'substats_col_left')\nsubstats_col_month = soup.find_all('div' , class_ = 'substats_col_month')\nsubstats_col_month_last_pct = soup.find_all('div' , class_ = 'substats_col_month_last_pct')\nsubstats_col_month_last_chg = soup.find_all('div' , class_ = 'substats_col_month_last_chg')","e54a6329":"category = [ item.text for item in category_col]\njul = [item.text for item in substats_col_month_last_pct]\ncpu_frequency = [ item.text for item in substats_col_left]\npct_chng = [ item.text for item in substats_col_month_last_chg]","0acf4e41":"mar=[]\napr=[]\nmay=[]\njun=[]\n\ni=0\nfor item in substats_col_month:\n    if i % 4 == 0:\n        mar.append(item.text)\n        i += 1\n    elif i % 4 == 1:\n        apr.append(item.text)\n        i += 1\n    elif i % 4 == 2:\n        may.append(item.text)\n        i += 1\n    elif i % 4 == 3:\n        jun.append(item.text)\n        i += 1\n","a8ff3d85":"manufacturer =[]\nos=[]\ni=0\nfor item in category :\n    if i == 0  :\n        for j in range(0,14):\n            manufacturer.append('INTEL')\n        i += 1\n    elif i == 1:\n        for j in range(14,25):\n            manufacturer.append('AMD')\n        i += 1\n    elif i == 2:\n        for j in range(25,39):\n            manufacturer.append('INTEL')\n        i += 1\n    elif i == 3:\n        for j in range(39,50):\n            manufacturer.append('INTEL')\n        i += 1\n    elif i == 4:\n        for j in range(50,61):\n            manufacturer.append('AMD')\n        i += 1\ni=0     \nfor i in [1,2,3]:\n    if i == 1:\n        for j in range(0,25):\n            os.append('WINDOWS')\n    if i == 2:\n        for j in range(25,39):\n            os.append('OSX')\n    if i == 3:\n        for j in range(39,61):\n            os.append('LINUX')","cc84a69f":"cpu_frequency_ = pd.DataFrame({'CPU FREQUENCY':cpu_frequency,\n                               'MANUFACTURER': manufacturer,\n                                                    'OS':os,\n                                                 'MAR': mar,\n                                                 'APR': apr,\n                                                 'MAY': may,\n                                                 'JUN': jun,\n                                                 'JUL': jul,\n                                        '% CHANGE':pct_chng})\n\ncpu_frequency_.drop([0,1,2,3,14,25,36,37,38,39,50] , axis = 0 , inplace = True)\ncpu_frequency_.reset_index(drop = True , inplace = True)\ncpu_frequency_.to_csv('D:\\DataScience\\datasets\\cpu_frequency.csv')","f2158555":"# CPU","a70df4e6":"### Source Links ","e85476a0":"# GPU","560a13cd":"In this notebook i will be explaining how to scrape data from steam website . We can also use it to scrape data from any other website . I am posting this notebook so as to help others as i had some problems figuring out that - most efficienct way to scrape data online . I tried many plugins , extensions but eventually I found writing my own script to be super efficient , powerful with a greater control overall . \n\nPlease upvote my notebook if you found it helpful ! ","67bb96eb":"### CPU FREQUENCY","cb593340":"Similarly we will be creating other data sets for cpu frequency , gpu , gpu-directx classification .\n","29d9bc11":"![image.png](attachment:image.png)\n\nHere as we can see the data we need it present inside div tags and with specified class namely - \n- substats_col_left col_header    \n- substats_col_left\n- substats_col_month\n- substats_col_month_last_pct\n- substats_col_month_last_chg"}}