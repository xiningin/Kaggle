{"cell_type":{"b219d282":"code","84e2be66":"code","e6c5a794":"code","ec9e83c8":"code","e9efda43":"code","25210987":"code","1d53b780":"code","45446fd9":"code","116fd0e2":"code","32cf4aec":"code","76f98208":"code","61bb078e":"code","8d206c7c":"code","0de126f5":"code","ba6c881b":"code","19fa2c4e":"code","2acaee18":"code","d9c288d3":"code","ecf3112e":"code","6e118cfe":"code","6dfb4bb1":"code","42fbd4d2":"code","61597020":"code","ffcee371":"code","1b17b3eb":"code","c2e4c7ed":"code","d49bdf00":"code","ff058064":"code","a4ac9653":"code","a9407f15":"code","8b722831":"code","d6a5b891":"code","402982b6":"code","717419a4":"code","4c962199":"code","16bf760e":"code","c42b9b4e":"code","b658811b":"code","bc2e5656":"code","a7119fb2":"code","a8f1cbc1":"code","8324700b":"code","57dab83a":"code","031cb30d":"code","720db512":"markdown","ba809a93":"markdown","47cabecb":"markdown","9f27c6bc":"markdown","3f94cc87":"markdown","8daf87b5":"markdown","60cb27ad":"markdown","23a43f25":"markdown","0793cd7c":"markdown","e722027f":"markdown","10a06b0f":"markdown"},"source":{"b219d282":"run_type='load' #save\/load\n\nadd_tokens=0\n\nmethod = '2' #1\/2 - defines how to use manconcorpus\nmancon_data_to_use = 'all' #all\/equal - defines how many training pairs to use from mancon corpus\nmultinli_data_to_use = 100000 #0\/number- 0 implies all, defines how many trainings pairs to use from multinli corpus\nmultinli_iteration = 3 #defines current iteration of continued training\nmednli_data_to_use = 0 #0\/number- 0 implies all, defines how many trainings pairs per class to use from mednli corpus\nmodel_name = 'allenai\/biobert-roberta-base' #\"allenai\/biobert-roberta-base\"\/\"deepset\/covid_bert_base\"\n\nmodel_continue = 0 #0\/1 - whether load & continue fine-tuning of model\nmodel_continue_sigmoid_path = \"\/kaggle\/input\/biobertmultinlipart2\/sigmoid.pickle\"\nmodel_continue_transformer_path = \"\/kaggle\/input\/biobertmultinlipart2\/transformer\"\n\nfinetune_multinli = 0\nfinetune_mednli = 0\nfinetune_mancon = 0","84e2be66":"#!pip install transformers","e6c5a794":"import os\nimport shutil\nimport json\nimport numpy as np\nimport pandas as pd\nimport xml.etree.ElementTree as et \nfrom itertools import permutations\n\nfrom keras.utils import np_utils\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import AutoModel\nfrom transformers import TFAutoModel, AutoTokenizer, AutoModelWithLMHead\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport pickle","ec9e83c8":"multinli_data = pd.read_csv('\/kaggle\/input\/multinli\/multinli_1.0_train.txt', sep='\\t', error_bad_lines=False)\nmultinli_test_data = pd.read_csv('\/kaggle\/input\/multinli-dev\/multinli_1.0_dev_matched.txt', sep='\\t', error_bad_lines=False)\n\nmultinli_data['gold_label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in multinli_data.gold_label]\nmultinli_test_data['gold_label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in multinli_test_data.gold_label]\n\nif multinli_data_to_use!=0:\n        print('Using only a subset of multiNLi for training')\n        #temp = multinli_data[multinli_data.gold_label==2].head(multinli_data_to_use).append(multinli_data[multinli_data.gold_label==1].head(multinli_data_to_use)).reset_index(drop=True)\n        #multinli_data = temp.append(multinli_data[multinli_data.gold_label==0].head(multinli_data_to_use)).reset_index(drop=True)\n        multinli_data = multinli_data.head(multinli_data_to_use*multinli_iteration).tail(multinli_data_to_use)\n\nx_train = '[CLS]'+multinli_data.sentence1+'[SEP]'+multinli_data.sentence2\nx_test = '[CLS]'+multinli_test_data.sentence1+'[SEP]'+multinli_test_data.sentence2\ny_train = np_utils.to_categorical(multinli_data.gold_label)\ny_test = np_utils.to_categorical(multinli_test_data.gold_label)","e9efda43":"mednli_data1 = pd.DataFrame()\nmednli_data2 = pd.DataFrame()\nmednli_test_data = pd.DataFrame()\n\nwith open('\/kaggle\/input\/mednli\/mli_train_v1.jsonl', 'r', encoding='utf-8') as f:\n    for line in f:\n        mednli_data1 = mednli_data1.append(json.loads(line.rstrip('\\n|\\r')),ignore_index=True)\nwith open('\/kaggle\/input\/mednli\/mli_dev_v1.jsonl', 'r', encoding='utf-8') as f:\n    for line in f:\n        mednli_data2 = mednli_data2.append(json.loads(line.rstrip('\\n|\\r')),ignore_index=True)\nmednli_data = mednli_data1.append(mednli_data2, ignore_index=True).reset_index(drop=True)\n\nwith open('\/kaggle\/input\/mednli\/mli_test_v1.jsonl', 'r', encoding='utf-8') as f:\n    for line in f:\n        mednli_test_data = mednli_test_data.append(json.loads(line.rstrip('\\n|\\r')),ignore_index=True)\n\nmednli_data['gold_label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in mednli_data.gold_label]\nmednli_test_data['gold_label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in mednli_test_data.gold_label]\n\nif mednli_data_to_use!=0:\n        print('Using only a subset of multiNLi for training')\n        temp = mednli_data[mednli_data.gold_label==2].head(mednli_data_to_use).append(mednli_data[mednli_data.gold_label==1].head(mednli_data_to_use)).reset_index(drop=True)\n        mednli_data = temp.append(mednli_data[mednli_data.gold_label==0].head(mednli_data_to_use)).reset_index(drop=True)\n\nx_train_3 = '[CLS]'+mednli_data.sentence1+'[SEP]'+mednli_data.sentence2\nx_test_3 = '[CLS]'+mednli_test_data.sentence1+'[SEP]'+mednli_test_data.sentence2\ny_train_3 = np_utils.to_categorical(mednli_data.gold_label)\ny_test_3 = np_utils.to_categorical(mednli_test_data.gold_label)","25210987":"if method=='1':\n    xtree = et.parse('\/kaggle\/input\/manconcorpus\/ManConCorpus.xml')\n    xroot = xtree.getroot() \n\n    manconcorpus_data = pd.DataFrame(columns = ['claim','assertion','question'])\n\n    for node in xroot:\n        for claim in node.findall('CLAIM'):\n            manconcorpus_data = manconcorpus_data.append({'claim':claim.text,\\\n                                                        'assertion':claim.attrib.get('ASSERTION'),\\\n                                                        'question':claim.attrib.get('QUESTION')},\n                                                         ignore_index=True)\n    print(len(manconcorpus_data))","1d53b780":"if run_type=='save' and method=='1':\n    questions = list(set(manconcorpus_data.question))\n    con = pd.DataFrame(columns=['claim1','claim2','label'])\n    ent = pd.DataFrame(columns=['claim1','claim2','label'])\n\n    for q in questions:\n        claim_yes = pd.DataFrame(manconcorpus_data.loc[(manconcorpus_data.question==q) & (manconcorpus_data.assertion=='YS'),'claim'])\n        claim_no = pd.DataFrame(manconcorpus_data.loc[(manconcorpus_data.question==q) & (manconcorpus_data.assertion=='NO'),'claim'])\n        temp = claim_yes.assign(key=1).merge(claim_no.assign(key=1), on='key').drop('key', 1)\n        temp1 = temp.rename(columns={'claim_x':'claim1','claim_y':'claim2'})\n        con = con.append(temp1)\n        #Swap claim1 & claim2 to generate more examples. This will handle directionality during fine-tuning.\n        temp2 = temp.rename(columns={'claim_x':'claim2','claim_y':'claim1'})\n        con = con.append(temp2)\n        con['label'] = 1   \n        con.drop_duplicates(inplace=True)\n\n        for i,j in list(permutations(claim_yes.index, 2)):\n            ent = ent.append({'claim1':claim_yes.claim[i],\\\n                        'claim2':claim_yes.claim[j],\\\n                        'label':0},\\\n                       ignore_index=True)\n\n        for i,j in list(permutations(claim_no.index, 2)):\n            ent = ent.append({'claim1':claim_no.claim[i],\\\n                        'claim2':claim_no.claim[j],\\\n                        'label':0},\\\n                       ignore_index=True)\n\n    transfer_data = pd.concat([con,ent]).reset_index(drop=True)\n    transfer_data['label'] = transfer_data.label.astype('float')\n    print(len(con))\n    print(len(ent))","45446fd9":"if run_type=='save' and method=='1':\n    x_train_2,x_test_2,y_train_2,y_test_2=train_test_split('[CLS]'+transfer_data.claim1+'[SEP]'+transfer_data.claim2,transfer_data['label'],test_size=0.2)\n    print(y_train_2.sum())\n    print(y_test_2.sum())","116fd0e2":"# if run_type=='save' and method=='2':\ntransfer_data = pd.read_csv('\/kaggle\/input\/manconcorpus-sent-pairs\/manconcorpus_sent_pairs_200516.tsv', sep ='\\t')\ntransfer_data['label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in transfer_data.label]\ntransfer_data['label'] = transfer_data.label.astype('float')\nprint(len(transfer_data[transfer_data.label==2]))\nprint(len(transfer_data[transfer_data.label==1]))\nprint(len(transfer_data[transfer_data.label==0]))","32cf4aec":"if run_type=='save' and method=='2':\n    if mancon_data_to_use=='equal':\n        temp = transfer_data[transfer_data.label==2].append(transfer_data[transfer_data.label==1].head(1000)).reset_index(drop=True)\n        transfer_data = temp.append(transfer_data[transfer_data.label==0].head(1000)).reset_index(drop=True)\n    print(len(transfer_data[transfer_data.label==2]))\n    print(len(transfer_data[transfer_data.label==1]))\n    print(len(transfer_data[transfer_data.label==0]))","76f98208":"if run_type=='save' and method=='2':\n    x_train_2,x_test_2,y_train_2,y_test_2=train_test_split('[CLS]'+transfer_data.text_a+'[SEP]'+transfer_data.text_b,transfer_data['label'],test_size=0.2)\n    print(len(y_train_2[transfer_data.label==2]))\n    print(len(y_train_2[transfer_data.label==1]))\n    print(len(y_train_2[transfer_data.label==0]))\n    print(len(y_test_2[transfer_data.label==2]))\n    print(len(y_test_2[transfer_data.label==1]))\n    print(len(y_test_2[transfer_data.label==0]))\n    y_train_2 = np_utils.to_categorical(y_train_2)\n    y_test_2 = np_utils.to_categorical(y_test_2)","61bb078e":"if run_type=='save' and method=='3':\n    x_train_2,x_test_2,y_train_2,y_test_2=train_test_split('[CLS]'+manconcorpus_data.question+'[SEP]'+manconcorpus_data.claim1,manconcorpus_data['assertion'],test_size=0.2)","8d206c7c":"def regular_encode(texts, tokenizer, maxlen=512):\n    \"\"\" Function to encode many sentences\"\"\"\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        sep_token='[SEP]'\n    )\n    return np.array(enc_di['input_ids'])","0de126f5":"def build_model(transformer, max_len=512):\n    \"\"\"\n    Require a transformer of type TFAutoBert\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    if method=='1':\n        out = Dense(1, activation='sigmoid', name='sigmoid')(cls_token)\n    if method=='2':\n        out = Dense(3, activation='softmax', name='softmax')(cls_token)\n    model = Model(inputs=input_word_ids, outputs=out)\n    if method=='1':\n        model.compile(Adam(lr=1e-6), loss='binary_crossentropy', metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), 'accuracy'])\n    if method=='2':\n        model.compile(Adam(lr=1e-6), loss='categorical_crossentropy', metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), tf.keras.metrics.CategoricalAccuracy()])\n    return model","ba6c881b":"def save_model(model, transformer_dir='transformer'):\n    \"\"\"\n    Special function to save a keras model that uses a transformer layer\n    \"\"\"\n    transformer = model.layers[1]\n    !mkdir transformer\n    transformer.save_pretrained(transformer_dir)\n    sigmoid = model.get_layer(index=3).get_weights()\n    pickle.dump(sigmoid, open('sigmoid.pickle', 'wb'))\n\ndef load_model(pickle_path, transformer_dir='transformer', max_len=512):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    transformer = TFAutoModel.from_pretrained(transformer_dir)\n    model = build_model(transformer, max_len=max_len)\n    sigmoid = pickle.load(open(pickle_path, 'rb'))\n    if method=='1':\n        model.get_layer('sigmoid').set_weights(sigmoid)\n    if method=='2':\n        model.get_layer('softmax').set_weights(sigmoid)\n\n    return model","19fa2c4e":"if run_type=='save':\n    print(int(int(x_train.str.len().max())))\n    print(int(x_train.str.len().median()))\n    print(int(int(x_train_2.str.len().max())))\n    print(int(x_train_2.str.len().median()))\n    print(int(int(x_train_3.str.len().max())))\n    print(int(x_train_3.str.len().median()))","2acaee18":"# Configuration params\nEPOCHS = 3\nMAX_LEN = 512\nBATCH_SIZE = 32","d9c288d3":"drug_names = pd.read_csv('\/kaggle\/input\/drugnames\/DrugNames.txt',header=None)\ndrug_names = list(drug_names[0])\nprint('Full list of drugs:',len(drug_names))\nif method=='1':\n    text = ' '.join(list(set(transfer_data.claim1)))\nif method=='2':\n    text = ' '.join(list(set(transfer_data.text_a)))\ndrug_names = [drug for drug in drug_names if drug in text]\nprint('List of drugs in training & testing corpus:',len(drug_names))\nvirus_names = pd.read_csv('\/kaggle\/input\/virus-words\/virus_words.txt',header=None)\nvirus_names = list(virus_names[0])","ecf3112e":"if model_name == 'deepset\/covid_bert_base':\n    MODEL = \"deepset\/covid_bert_base\"\nelse:\n    MODEL = \"allenai\/biomed_roberta_base\"\n\n# First load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nif add_tokens==1:\n    tokenizer.add_tokens(drug_names+virus_names)","6e118cfe":"len(tokenizer)","6dfb4bb1":"%%time\nif run_type=='save':\n    x_train_str = []\n    for x in x_train:\n        x_train_str.append(str(x))\n\n    x_test_str = []\n    for x in x_test:\n        x_test_str.append(str(x))\n    \n    x_train = regular_encode(x_train_str, tokenizer, maxlen=MAX_LEN)\n    x_test = regular_encode(x_test_str, tokenizer, maxlen=MAX_LEN)\n    \n    x_train_3_str = []\n    for x in x_train_3:\n        x_train_3_str.append(str(x))\n\n    x_test_3_str = []\n    for x in x_test_3:\n        x_test_3_str.append(str(x))\n    \n    x_train_3 = regular_encode(x_train_3_str, tokenizer, maxlen=MAX_LEN)\n    x_test_3 = regular_encode(x_test_3_str, tokenizer, maxlen=MAX_LEN)\n    \n    x_train_2 = regular_encode(x_train_2.values, tokenizer, maxlen=MAX_LEN)\n    x_test_2 = regular_encode(x_test_2.values, tokenizer, maxlen=MAX_LEN)","42fbd4d2":"es = EarlyStopping(monitor='val_accuracy', \n                    min_delta=0.001, \n                    patience=3,\n                    verbose=1, \n                    mode='max', \n                    restore_best_weights=True)","61597020":"# !pip install wandb\n# !wandb login\n# import wandb\n# from wandb.keras import WandbCallback\n# wandb.init(project=\"vt-relation-extract\", sync_tensorboard=True)","ffcee371":"if run_type=='save' and model_continue==0:\n    strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n    if model_name == 'deepset\/covid_bert_base':\n        model = AutoModelWithLMHead.from_pretrained(\"deepset\/covid_bert_base\")\n        model.resize_token_embeddings(len(tokenizer))\n        !mkdir covid_bert_base\n        model.save_pretrained(\"covid_bert_base\")\n        with strategy.scope():\n          model = TFAutoModel.from_pretrained(\"covid_bert_base\", from_pt=True)\n          #model.resize_token_embeddings(len(tokenizer))\n          model = build_model(model)\n        !rm -r covid_bert_base\n    else:\n        model = AutoModel.from_pretrained(\"allenai\/biomed_roberta_base\")\n        model.resize_token_embeddings(len(tokenizer))\n        !mkdir biomed_roberta_base\n        model.save_pretrained(\"biomed_roberta_base\")\n        with strategy.scope():\n          model = TFAutoModel.from_pretrained(\"biomed_roberta_base\", from_pt=True)\n          #model.resize_token_embeddings(len(tokenizer))\n          model = build_model(model)\n        !rm -r biomed_roberta_base\n    BATCH_SIZE = 2 * strategy.num_replicas_in_sync","1b17b3eb":"if run_type=='save' and model_continue==1:\n    strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n    with strategy.scope():\n        model = load_model(model_continue_sigmoid_path, model_continue_transformer_path)\n    BATCH_SIZE = 2 * strategy.num_replicas_in_sync","c2e4c7ed":"if run_type=='save':\n    model.summary()","d49bdf00":"# Fine tune on MultiNLI\n\nif run_type=='save' and finetune_multinli==1:\n    train_history = model.fit(\n                        x_train, y_train,\n                        batch_size = BATCH_SIZE,\n                        validation_data=(x_test, y_test),\n                        #callbacks=[es, WandbCallback()],\n                        callbacks=[es],\n                        epochs=EPOCHS\n                        )","ff058064":"# Fine tune on MedNLI\n\nif run_type=='save' and finetune_mednli==1:\n    train_history = model.fit(\n                        x_train_3, y_train_3,\n                        batch_size = BATCH_SIZE,\n                        validation_data=(x_test_3, y_test_3),\n                        #callbacks=[es, WandbCallback()],\n                        callbacks=[es],\n                        epochs=EPOCHS\n                        )","a4ac9653":"# Fine tune on Manconcorpus\n\nif run_type=='save' and finetune_mancon==1:\n    train_history = model.fit(\n                        x_train_2, y_train_2,\n                        batch_size = BATCH_SIZE,\n                        validation_data=(x_test_2, y_test_2),\n                        #callbacks=[es, WandbCallback()],\n                        callbacks=[es],\n                        epochs=EPOCHS\n                        )","a9407f15":"# from google.colab import auth\n# from datetime import datetime\n# auth.authenticate_user()\n# !gsutil cp -r best_epoch_roberta gs:\/\/coronaviruspublicdata\/temp_data\/snapshots","8b722831":"#import pickle \n#save model, input: sentence, output: binary\n#pickle.dump(model, open( \"bioERT_model1.pickle\", \"wb\" ) )\n# !gsutil cp model.pickle gs:\/\/coronaviruspublicdata\/model.pickle","d6a5b891":"if run_type=='save':\n    save_model(model)\n    shutil.make_archive('biobert_output', 'zip', '\/kaggle\/working\/')","402982b6":"if run_type=='load':\n    model = load_model(\"\/kaggle\/input\/biobertfinalmodelv2\/sigmoid.pickle\", \"\/kaggle\/input\/biobertfinalmodelv2\/transformer\")","717419a4":"# model.summary()\n# model.get_layer(index=3)","4c962199":"# !gsutil cp -r transformer3 gs:\/\/coronaviruspublicdata\/re_final_best2\/s\n# !gsutil cp sigmoid3.pickle gs:\/\/coronaviruspublicdata\/re_final_best2\/s","16bf760e":"output_data = pd.read_excel('\/kaggle\/input\/pilotannotations\/Pilot_Contra_Claims_Annotations_06_30 - Copy.xlsx',sheet_name='All_phase2')\noutput_data = output_data.dropna().reset_index(drop=True)","c42b9b4e":"ls = []\nfor i in range(len(output_data)):\n    ls.append(str('[CLS]'+output_data.loc[i,'claim_1']+'[SEP]'+output_data.loc[i,'claim_2']))\n    \ntest_example = regular_encode(ls, tokenizer, maxlen=MAX_LEN)\npredictions = model.predict(test_example)\nif method=='1':\n    output_data['BioBERT_Prediction'] = [p[0] for p in predictions]\nif method=='2':\n    output_data['BioBERT_Prediction_con'] = [p[0] for p in predictions]\n    output_data['BioBERT_Prediction_ent'] = [p[1] for p in predictions]\n    output_data['BioBERT_Prediction_neu'] = [p[2] for p in predictions]","b658811b":"print(len(output_data))\nprint(len(output_data.loc[output_data.label=='Contradiction',:]))\nprint(len(output_data.loc[output_data.label=='Entailment',:]))\nprint(len(output_data.loc[output_data.label=='Neutral',:]))","bc2e5656":"if method=='1':\n    print(max(output_data.BioBERT_Prediction))\nif method=='2':\n    print(max(output_data.BioBERT_Prediction_con))\n    print(max(output_data.BioBERT_Prediction_ent))\n    print(max(output_data.BioBERT_Prediction_neu))","a7119fb2":"if method=='1':\n    output_data['label'] = [1 if a=='contradiction' else 0 for a in output_data.annotation]\n    output_data['BioBERT_Prediction_class'] = [1 if p>=0.375 else 0 for p in output_data.BioBERT_Prediction]\n    \n    print('Overall accuracy: '\\\n      + str(accuracy_score(output_data['label'], output_data['BioBERT_Prediction_class'] )))\n    print('Precision: '\\\n          + str(precision_score(output_data['label'], output_data['BioBERT_Prediction_class'] )))\n    print('Recall: '\\\n          + str(recall_score(output_data['label'], output_data['BioBERT_Prediction_class'] )))\n    print('F1 score: '\\\n          + str(f1_score(output_data['label'], output_data['BioBERT_Prediction_class'] )))\n\nif method=='2':\n    output_data['label'] = output_data.label\n    output_data['BioBERT_Prediction_class'] = output_data[['BioBERT_Prediction_con','BioBERT_Prediction_ent','BioBERT_Prediction_neu']].idxmax(axis=1)\n    output_data['BioBERT_Prediction_class'].replace(to_replace={'BioBERT_Prediction_con':'Contradiction','BioBERT_Prediction_ent':'Entailment','BioBERT_Prediction_neu':'Neutral'}\\\n                                                   ,inplace=True)\n    \n    print('Overall accuracy: '\\\n      + str(accuracy_score(output_data['label'], output_data['BioBERT_Prediction_class'] )))\n    print('Precision: '\\\n          + str(precision_score(output_data['label'], output_data['BioBERT_Prediction_class'], average = None)))\n    print('Recall: '\\\n          + str(recall_score(output_data['label'], output_data['BioBERT_Prediction_class'], average = None)))\n    print('F1 score: '\\\n          + str(f1_score(output_data['label'], output_data['BioBERT_Prediction_class'], average = None)))","a8f1cbc1":"output_data.to_csv('bioBERT_Output.csv',header=True)","8324700b":"# !ln -sf \/opt\/bin\/nvidia-smi \/usr\/bin\/nvidia-smi\n# !pip install gputil\n# !pip install psutil\n# !pip install humanize\n# import psutil\n# import humanize\n# import os\n# import GPUtil as GPU\n# GPUs = GPU.getGPUs()\n# # XXX: only one GPU on Colab and isn\u2019t guaranteed\n# gpu = GPUs[0]\n# def printm():\n#  process = psutil.Process(os.getpid())\n#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n# printm()","57dab83a":"# from google.colab import auth\n# from datetime import datetime\n# auth.authenticate_user()\n","031cb30d":"# !gsutil cp -r transformer gs:\/\/coronaviruspublicdata\/re_snapshot\/4_13_2020\n# !gsutil cp sigmoid.pickle gs:\/\/coronaviruspublicdata\/re_snapshot\/4_13_2020","720db512":"Method 3 - Use PICO format questions (i.e. original structure of ManConCorpus)","ba809a93":"# Load the MultiNLI train,dev data set for fine-tuning","47cabecb":"### Saving\/Exporting\nA model isn't useful if it cannot be used in a production pipeline.","9f27c6bc":"# Load the MedNLI dataset for fine-tuning","3f94cc87":"### Qualitative Evaluation\nWe will now qualitatively look at a few examples.","8daf87b5":"# Fine-Tuning BioBERT","60cb27ad":"Method 2 - Adding claims from different questions as neutral","23a43f25":"Method 1 - Extract yes\/no answer pairs to same question as contradiction","0793cd7c":"### Tests for RAM usage\nBasic check to determine how much RAM is available.","e722027f":"# Training Roberta for contradiction classification\n[Code based on this](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta).\nOur goal is to train a binary classification model to determine if a pair of drug-treatment sentences contain any contradiction.","10a06b0f":"# Load the ManConCorpus and split into train and test set for fine-tuning"}}