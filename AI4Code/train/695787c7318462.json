{"cell_type":{"f85d5ef6":"code","de896bc5":"code","5473f6c8":"code","31eefaa1":"code","8cf8e76c":"code","2075ba9a":"code","239998f4":"code","17552fe2":"code","ff69c594":"code","d4c0a8c4":"code","27b9fe88":"code","9ce24ee8":"code","bfee224e":"markdown","d0ce129c":"markdown"},"source":{"f85d5ef6":"import random, os, warnings, math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer","de896bc5":"PATH = '..\/input\/commonlitreadabilityprize\/'\n#BASE_MODEL = '..\/input\/huggingface-bert\/bert-base-uncased\/'\n","5473f6c8":"MAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = df_train['target']\ninput_categories =df_train['excerpt']\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","31eefaa1":"def fix_length(tokens, max_sequence_length=512):\n    length = len(tokens)\n    if length > max_sequence_length:\n        tokens = tokens[:max_sequence_length-1]\n    return tokens\n\n# function for tokenizing the input data for transformer.\ndef transformer_inputs(text,tokenizer,MAX_SEQUENCE_LENGTH = 512):\n\n    text_tokens = tokenizer.tokenize(str(text))\n    text_tokens = fix_length(text_tokens)\n    ids_q = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + text_tokens)\n    padded_ids = (ids_q + [tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(ids_q)))[:MAX_SEQUENCE_LENGTH]\n    #token_type_ids = ([0] * MAX_SEQUENCE_LENGTH)[:MAX_SEQUENCE_LENGTH]\n    attention_mask = ([1] * len(ids_q) + [0] * (MAX_SEQUENCE_LENGTH - len(ids_q)))[:MAX_SEQUENCE_LENGTH]\n\n    return padded_ids,attention_mask\n\n# function for creating the input_ids, masks and segments for the bert input\ndef input_for_model(df, tokenizer):\n    print(f'generating input for transformer...')\n    input_ids,input_attention_masks = [], []\n    for text in df['excerpt'].values:\n        ids, mask = transformer_inputs(text,tokenizer)\n        input_ids.append(ids)\n        input_attention_masks.append(mask)\n    \n    return (\n        np.asarray(input_ids, dtype=np.int32),\n        np.asarray(input_attention_masks, dtype=np.int32))\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","8cf8e76c":"def model_fn(encoder, seq_len):\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids, \n                       'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs)\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss=losses.MeanSquaredError(), \n                  metrics=[metrics.RootMeanSquaredError()])\n    \n    return model","2075ba9a":"BATCH_SIZE = 8 \nLEARNING_RATE = 1e-5 \nEPOCHS = 35\nES_PATIENCE = 7\nPATIENCE = 2\nN_FOLDS = 5\nSEQ_LEN = 512 #300\nBASE_MODEL = '\/kaggle\/input\/huggingface-roberta\/roberta-base\/'\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)","239998f4":"encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\nmodel = model_fn(encoder, 512)\nmodel.summary()","17552fe2":"#gkf = GroupKFold(n_splits=5).split(X=df_train.excerpt, groups=df_train.excerpt)\nimport tqdm\ninputs=input_for_model(df_train,tokenizer)\ntest_inputs=input_for_model(df_test,tokenizer)\noutputs=compute_output_arrays(df_train,'target')","ff69c594":"seed=42\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\nvalid_preds = [];\nvalid_labels = [];\nhistory_list = [];\ntest_preds = []\n\n\nfor fold,(train_idx,valid_idx) in enumerate(skf.split(df_train)):\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(train_idx)} VALID: {len(valid_idx)}')\n    # Model\n    K.clear_session()\n\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    \n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n                       patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path, monitor='val_root_mean_squared_error', mode='min', \n                                 save_best_only=True, save_weights_only=True)\n    train_inputs=[inputs[i][train_idx] for i in range(2)]\n    train_outputs=outputs[train_idx].reshape((-1,1)).flatten()\n\n    valid_inputs = [inputs[i][valid_idx] for i in range(2)]\n    valid_outputs = outputs[valid_idx].reshape((-1,1)).flatten()\n    valid_labels.append(valid_outputs)\n   \n    \n    history = model.fit(train_inputs, train_outputs, batch_size=8,\n                validation_data=(valid_inputs, valid_outputs), \n                    steps_per_epoch=50, \n                    callbacks=[es, checkpoint], \n                    epochs=35,  \n                    verbose=2).history\n    history_list.append(history)\n    # Save last model weights\n    model.load_weights(model_path)\n    \n    # Results\n    print(f\"#### FOLD {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n\n    \n    \n    valid_preds.append(model.predict(valid_inputs)['logits'])\n    test_preds.append(model.predict(test_inputs)['logits'])\n    ","d4c0a8c4":"def plot_metrics(history):\n    metric_list = list(history.keys())\n    size = len(metric_list)\/\/2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size * 5))\n    axes = axes.flatten()\n    \n    for index in range(len(metric_list)\/\/2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()\n\n\nfor fold, history in enumerate(history_list):\n    print(f'\\nFOLD: {fold+1}')\n    plot_metrics(history)","27b9fe88":"y_true = np.concatenate(valid_labels)\ny_preds = np.concatenate(valid_preds)\n\n\nfor fold, history in enumerate(history_list):\n    print(f\"FOLD {fold+1} RMSE: {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \nprint(f'OOF RMSE: {mean_squared_error(y_true, y_preds, squared=False):.4f}')","9ce24ee8":"submission = df_test[['id']]\ndf_sub['target'] = np.mean(test_preds, axis=0)\ndf_sub.to_csv('submission.csv', index=False)\ndisplay(df_sub.head(10))","bfee224e":"Implementation","d0ce129c":"Submission"}}