{"cell_type":{"e66d1f05":"code","5cb08d4b":"code","84d5b7d4":"code","f31e38b0":"code","e0f19953":"code","66c0085f":"code","b465ff98":"code","ec72d327":"code","1250ab64":"code","a5cda06f":"code","0a1f42dd":"code","4035d6a6":"code","52bd15b6":"code","7640a97c":"code","0e50b7d4":"code","2b9dde6e":"code","b57ff5c1":"code","deacf98e":"code","4bbb7766":"code","e315d7d7":"code","fb1e49a8":"code","e9afa8df":"code","297e82f9":"code","a679df16":"code","aab96640":"code","85d6666d":"code","44f4dcd0":"code","df15bb4b":"code","5b24315c":"code","939f5085":"code","e2a544ae":"code","aae102ab":"code","7a4fdba3":"code","a1c75731":"code","410eeb8f":"code","85f1534e":"code","03e02a41":"code","46f236e7":"code","69dd9627":"code","677f5cdb":"code","763f0c3e":"code","bef7a1a6":"code","7ab5e288":"markdown","deaf12ac":"markdown","21110911":"markdown","d981e102":"markdown","3f59c39a":"markdown","5c4ca598":"markdown","ba66ff06":"markdown","cea30b20":"markdown","bc26f7c0":"markdown","950ba558":"markdown","eb91a2c2":"markdown","7024e9cb":"markdown","8e407c5e":"markdown"},"source":{"e66d1f05":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport gc\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, BatchNormalization, RNN, GRU\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.regularizers import l2\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split","5cb08d4b":"dspath = '\/kaggle\/input\/darknet2020data\/Darknet.CSV'\ndf = pd.read_csv(dspath)\ndf.head()","84d5b7d4":"labels = df.pop('Label.1')","f31e38b0":"remove_cols = [\n    'Flow ID'\n]","e0f19953":"for each in remove_cols:\n    df.pop(each)\n    \ngc.collect()","66c0085f":"def convert_to_int(x):\n    new_list = []\n    for each in x:\n        new_list.append(int(each))\n    return new_list","b465ff98":"x = df[\"Src IP\"].str.split(\".\", n = 4, expand = True)\ndf['s_ip_0'] = convert_to_int(x.values[:, 0])\ndf['s_ip_1'] = convert_to_int(x.values[:, 1])\ndf['s_ip_2'] = convert_to_int(x.values[:, 2])\ndf['s_ip_3'] = convert_to_int(x.values[:, 3])\n\nx = df[\"Dst IP\"].str.split(\".\", n = 4, expand = True)\ndf['d_ip_0'] = convert_to_int(x.values[:, 0])\ndf['d_ip_1'] = convert_to_int(x.values[:, 1])\ndf['d_ip_2'] = convert_to_int(x.values[:, 2])\ndf['d_ip_3'] = convert_to_int(x.values[:, 3])\n\ndel x\ndf.pop('Dst IP')\ndf.pop('Src IP')\ngc.collect()","ec72d327":"factorized = pd.factorize(df['Label'])\ndf['Label'] = factorized[0]","1250ab64":"df['Timestamp'] =  pd.to_datetime(df['Timestamp'])","a5cda06f":"df.set_index('Timestamp', drop=True, append=False, inplace=True, verify_integrity=False)\ndf = df.sort_index()","0a1f42dd":"df = df.infer_objects()","4035d6a6":"df.head(20)","52bd15b6":"df['Flow Bytes\/s'] = df['Flow Bytes\/s'].replace(np.inf, 0, inplace=True)\ndf['Flow Packets\/s'] = df['Flow Packets\/s'].replace(np.inf, 0, inplace=True)","7640a97c":"factorized_labels = pd.factorize(labels)\nlabels = factorized_labels[0]\nprint (factorized_labels[1])","0e50b7d4":"X_train, X_test, train_y, test_y = train_test_split(df, labels, test_size=0.20, random_state=42, stratify=labels)","2b9dde6e":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train.values)\nX_test = scaler.transform(X_test.values)","b57ff5c1":"y_train = tf.keras.utils.to_categorical(train_y).astype('uint8')\ny_test = tf.keras.utils.to_categorical(test_y).astype('uint8')\ny_train.shape, y_test.shape","deacf98e":"threshold_feature_selection = 2\nx_train = X_train[:, 0:threshold_feature_selection]\nx_test = X_test[:, 0:threshold_feature_selection]","4bbb7766":"X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\nX_train.shape, X_test.shape","e315d7d7":"model = Sequential()\nmodel.add(LSTM(input_shape=(88, 1), units=512, activation='relu',return_sequences=False, return_state=False,))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=11, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', metrics=[keras.losses.mean_squared_error, keras.losses.mean_absolute_error], optimizer=Adam())\nprint(model.summary())","fb1e49a8":"%%time\nhistory = model.fit(x = X_train, y = y_train, batch_size=1024, validation_data=(X_test, y_test), verbose=1, epochs = 10)","e9afa8df":"plt.plot(history.history['mean_squared_error'])\nplt.plot(history.history['val_mean_squared_error'])\nplt.title('model mean_squared_error')\nplt.ylabel('mean_squared_error')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","297e82f9":"plt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('model mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","a679df16":"lstm = model","aab96640":"model = Sequential()\nmodel.add(GRU(input_shape=(88, 1), units=64, activation='relu',return_sequences=False, return_state=False,))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=1024, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=16, activation='relu'))\nmodel.add(Dense(units=11, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', metrics=[keras.losses.mean_squared_error, keras.losses.mean_absolute_error], optimizer=Adam())\nprint(model.summary())","85d6666d":"%%time\nhistory = model.fit(x = X_train, y = y_train, batch_size=64, validation_data=(X_test, y_test), verbose=1, epochs = 10)","44f4dcd0":"plt.plot(history.history['mean_squared_error'])\nplt.plot(history.history['val_mean_squared_error'])\nplt.title('model mean_squared_error')\nplt.ylabel('mean_squared_error')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","df15bb4b":"plt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('model mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","5b24315c":"rnn = model","939f5085":"model = Sequential()\nmodel.add(keras.layers.Input(shape=(88, )))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=11, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', metrics=[keras.losses.mean_squared_error, keras.losses.mean_absolute_error], optimizer=Adam())\nprint(model.summary())","e2a544ae":"%%time\nhistory = model.fit(x = X_train[:, :, 0], y = y_train, batch_size=16, validation_data=(X_test[:, :, 0], y_test), verbose=1, epochs = 10)","aae102ab":"plt.plot(history.history['mean_squared_error'])\nplt.plot(history.history['val_mean_squared_error'])\nplt.title('model mean_squared_error')\nplt.ylabel('mean_squared_error')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","7a4fdba3":"plt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('model mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","a1c75731":"!pip install -q scikit-fuzzy","410eeb8f":"from skfuzzy import gaussmf, gbellmf, sigmf\n\nclass MemFuncs:\n    'Common base class for all employees'\n    funcDict = {'gaussmf': gaussmf, 'gbellmf': gbellmf, 'sigmf': sigmf}\n\n\n    def __init__(self, MFList):\n        self.MFList = MFList\n\n    def evaluateMF(self, rowInput):\n        if len(rowInput) != len(self.MFList):\n            print(\"Number of variables does not match number of rule sets\")\n\n        return [[self.funcDict[self.MFList[i][k][0]](rowInput[i],**self.MFList[i][k][1]) for k in range(len(self.MFList[i]))] for i in range(len(rowInput))]","85f1534e":"def partial_dMF(x, mf_definition, partial_parameter):\n    \"\"\"Calculates the partial derivative of a membership function at a point x.\n\n\n\n    Parameters\n    ------\n\n\n    Returns\n    ------\n\n    \"\"\"\n    mf_name = mf_definition[0]\n\n    if mf_name == 'gaussmf':\n\n        sigma = mf_definition[1]['sigma']\n        mean = mf_definition[1]['mean']\n\n        if partial_parameter == 'sigma':\n            result = (2.\/sigma**3) * np.exp(-(((x-mean)**2)\/(sigma)**2))*(x-mean)**2\n        elif partial_parameter == 'mean':\n            result = (2.\/sigma**2) * np.exp(-(((x-mean)**2)\/(sigma)**2))*(x-mean)\n\n    elif mf_name == 'gbellmf':\n\n        a = mf_definition[1]['a']\n        b = mf_definition[1]['b']\n        c = mf_definition[1]['c']\n\n        if partial_parameter == 'a':\n            result = (2. * b * np.power((c-x),2) * np.power(np.absolute((c-x)\/a), ((2 * b) - 2))) \/ \\\n                (np.power(a, 3) * np.power((np.power(np.absolute((c-x)\/a),(2*b)) + 1), 2))\n        elif partial_parameter == 'b':\n            result = -1 * (2 * np.power(np.absolute((c-x)\/a), (2 * b)) * np.log(np.absolute((c-x)\/a))) \/ \\\n                (np.power((np.power(np.absolute((c-x)\/a), (2 * b)) + 1), 2))\n        elif partial_parameter == 'c':\n            result = (2. * b * (c-x) * np.power(np.absolute((c-x)\/a), ((2 * b) - 2))) \/ \\\n                (np.power(a, 2) * np.power((np.power(np.absolute((c-x)\/a),(2*b)) + 1), 2))\n\n    elif mf_name == 'sigmf':\n\n        b = mf_definition[1]['b']\n        c = mf_definition[1]['c']\n\n        if partial_parameter == 'b':\n            result = -1 * (c * np.exp(c * (b + x))) \/ \\\n                np.power((np.exp(b*c) + np.exp(c*x)), 2)\n        elif partial_parameter == 'c':\n            result = ((x - b) * np.exp(c * (x - b))) \/ \\\n                np.power((np.exp(c * (x - c))) + 1, 2)\n\n\n    return result","03e02a41":"# -*- coding: utf-8 -*-\nimport itertools\nimport numpy as np\nimport copy\n\nclass ANFIS:\n    \"\"\"Class to implement an Adaptive Network Fuzzy Inference System: ANFIS\"\n\n    Attributes:\n        X\n        Y\n        XLen\n        memClass\n        memFuncs\n        memFuncsByVariable\n        rules\n        consequents\n        errors\n        memFuncsHomo\n        trainingType\n\n\n    \"\"\"\n\n    def __init__(self, X, Y, memFunction):\n        self.X = np.array(copy.copy(X))\n        self.Y = np.array(copy.copy(Y))\n        self.XLen = len(self.X)\n        self.memClass = copy.deepcopy(memFunction)\n        self.memFuncs = self.memClass.MFList\n        self.memFuncsByVariable = [[x for x in range(len(self.memFuncs[z]))] for z in range(len(self.memFuncs))]\n        self.rules = np.array(list(itertools.product(*self.memFuncsByVariable)))\n        self.consequents = np.empty(self.Y.ndim * len(self.rules) * (self.X.shape[1] + 1))\n        self.consequents.fill(0)\n        self.errors = np.empty(0)\n        self.memFuncsHomo = all(len(i)==len(self.memFuncsByVariable[0]) for i in self.memFuncsByVariable)\n        self.trainingType = 'Not trained yet'\n\n    def LSE(self, A, B, initialGamma = 1000.):\n        coeffMat = A\n        rhsMat = B\n        S = np.eye(coeffMat.shape[1])*initialGamma\n        x = np.zeros((coeffMat.shape[1],1)) # need to correct for multi-dim B\n        for i in range(len(coeffMat[:,0])):\n            a = coeffMat[i,:]\n            b = np.array(rhsMat[i])\n            S = S - (np.array(np.dot(np.dot(np.dot(S,np.matrix(a).transpose()),np.matrix(a)),S)))\/(1+(np.dot(np.dot(S,a),a)))\n            x = x + (np.dot(S,np.dot(np.matrix(a).transpose(),(np.matrix(b)-np.dot(np.matrix(a),x)))))\n        return x\n\n    def trainHybridJangOffLine(self, epochs=5, tolerance=1e-5, initialGamma=1000, k=0.01):\n\n        self.trainingType = 'trainHybridJangOffLine'\n        convergence = False\n        epoch = 1\n\n        while (epoch < epochs) and (convergence is not True):\n\n            #layer four: forward pass\n            [layerFour, wSum, w] = forwardHalfPass(self, self.X)\n\n            #layer five: least squares estimate\n            layerFive = np.array(self.LSE(layerFour,self.Y,initialGamma))\n            self.consequents = layerFive\n            layerFive = np.dot(layerFour,layerFive)\n\n            #error\n            error = np.sum((self.Y-layerFive.T)**2)\n            print('current error: '+ str(error))\n            average_error = np.average(np.absolute(self.Y-layerFive.T))\n            self.errors = np.append(self.errors,error)\n\n            if len(self.errors) != 0:\n                if self.errors[len(self.errors)-1] < tolerance:\n                    convergence = True\n\n            # back propagation\n            if convergence is not True:\n                cols = range(len(self.X[0,:]))\n                dE_dAlpha = list(backprop(self, colX, cols, wSum, w, layerFive) for colX in range(self.X.shape[1]))\n\n\n            if len(self.errors) >= 4:\n                if (self.errors[-4] > self.errors[-3] > self.errors[-2] > self.errors[-1]):\n                    k = k * 1.1\n\n            if len(self.errors) >= 5:\n                if (self.errors[-1] < self.errors[-2]) and (self.errors[-3] < self.errors[-2]) and (self.errors[-3] < self.errors[-4]) and (self.errors[-5] > self.errors[-4]):\n                    k = k * 0.9\n\n            ## handling of variables with a different number of MFs\n            t = []\n            for x in range(len(dE_dAlpha)):\n                for y in range(len(dE_dAlpha[x])):\n                    for z in range(len(dE_dAlpha[x][y])):\n                        t.append(dE_dAlpha[x][y][z])\n\n            eta = k \/ np.abs(np.sum(t))\n\n            if(np.isinf(eta)):\n                eta = k\n\n            ## handling of variables with a different number of MFs\n            dAlpha = copy.deepcopy(dE_dAlpha)\n            if not(self.memFuncsHomo):\n                for x in range(len(dE_dAlpha)):\n                    for y in range(len(dE_dAlpha[x])):\n                        for z in range(len(dE_dAlpha[x][y])):\n                            dAlpha[x][y][z] = -eta * dE_dAlpha[x][y][z]\n            else:\n                dAlpha = -eta * np.array(dE_dAlpha)\n\n\n            for varsWithMemFuncs in range(len(self.memFuncs)):\n                for MFs in range(len(self.memFuncsByVariable[varsWithMemFuncs])):\n                    paramList = sorted(self.memFuncs[varsWithMemFuncs][MFs][1])\n                    for param in range(len(paramList)):\n                        self.memFuncs[varsWithMemFuncs][MFs][1][paramList[param]] = self.memFuncs[varsWithMemFuncs][MFs][1][paramList[param]] + dAlpha[varsWithMemFuncs][MFs][param]\n            epoch = epoch + 1\n\n\n        self.fittedValues = predict(self,self.X)\n        self.residuals = self.Y - self.fittedValues[:,0]\n\n        return self.fittedValues\n\n\n    def plotErrors(self):\n        if self.trainingType == 'Not trained yet':\n            print(self.trainingType)\n        else:\n            import matplotlib.pyplot as plt\n            plt.plot(range(len(self.errors)),self.errors,'ro', label='errors')\n            plt.ylabel('error')\n            plt.xlabel('epoch')\n            plt.show()\n\n    def plotMF(self, x, inputVar):\n        import matplotlib.pyplot as plt\n        from skfuzzy import gaussmf, gbellmf, sigmf\n\n        for mf in range(len(self.memFuncs[inputVar])):\n            if self.memFuncs[inputVar][mf][0] == 'gaussmf':\n                y = gaussmf(x,**self.memClass.MFList[inputVar][mf][1])\n            elif self.memFuncs[inputVar][mf][0] == 'gbellmf':\n                y = gbellmf(x,**self.memClass.MFList[inputVar][mf][1])\n            elif self.memFuncs[inputVar][mf][0] == 'sigmf':\n                y = sigmf(x,**self.memClass.MFList[inputVar][mf][1])\n\n            plt.plot(x,y,'r')\n\n        plt.show()\n\n    def plotResults(self):\n        if self.trainingType == 'Not trained yet':\n            print(self.trainingType)\n        else:\n            import matplotlib.pyplot as plt\n            plt.plot(range(len(self.fittedValues)),self.fittedValues,'r', label='trained')\n            plt.plot(range(len(self.Y)),self.Y,'b', label='original')\n            plt.legend(loc='upper left')\n            plt.show()\n\n\n\ndef forwardHalfPass(ANFISObj, Xs):\n    layerFour = np.empty(0,)\n    wSum = []\n\n    for pattern in range(len(Xs[:,0])):\n        #layer one\n        layerOne = ANFISObj.memClass.evaluateMF(Xs[pattern,:])\n\n        #layer two\n        miAlloc = [[layerOne[x][ANFISObj.rules[row][x]] for x in range(len(ANFISObj.rules[0]))] for row in range(len(ANFISObj.rules))]\n        layerTwo = np.array([np.product(x) for x in miAlloc]).T\n        if pattern == 0:\n            w = layerTwo\n        else:\n            w = np.vstack((w,layerTwo))\n\n        #layer three\n        wSum.append(np.sum(layerTwo))\n        if pattern == 0:\n            wNormalized = layerTwo\/wSum[pattern]\n        else:\n            wNormalized = np.vstack((wNormalized,layerTwo\/wSum[pattern]))\n\n        #prep for layer four (bit of a hack)\n        layerThree = layerTwo\/wSum[pattern]\n        rowHolder = np.concatenate([x*np.append(Xs[pattern,:],1) for x in layerThree])\n        layerFour = np.append(layerFour,rowHolder)\n\n    w = w.T\n    wNormalized = wNormalized.T\n\n    layerFour = np.array(np.array_split(layerFour,pattern + 1))\n\n    return layerFour, wSum, w\n\n\ndef backprop(ANFISObj, columnX, columns, theWSum, theW, theLayerFive):\n\n    paramGrp = [0]* len(ANFISObj.memFuncs[columnX])\n    for MF in range(len(ANFISObj.memFuncs[columnX])):\n\n        parameters = np.empty(len(ANFISObj.memFuncs[columnX][MF][1]))\n        timesThru = 0\n        for alpha in sorted(ANFISObj.memFuncs[columnX][MF][1].keys()):\n\n            bucket3 = np.empty(len(ANFISObj.X))\n            for rowX in range(len(ANFISObj.X)):\n                varToTest = ANFISObj.X[rowX,columnX]\n                tmpRow = np.empty(len(ANFISObj.memFuncs))\n                tmpRow.fill(varToTest)\n\n                bucket2 = np.empty(ANFISObj.Y.ndim)\n                for colY in range(ANFISObj.Y.ndim):\n\n                    rulesWithAlpha = np.array(np.where(ANFISObj.rules[:,columnX]==MF))[0]\n                    adjCols = np.delete(columns,columnX)\n\n                    senSit = partial_dMF(ANFISObj.X[rowX,columnX],ANFISObj.memFuncs[columnX][MF],alpha)\n                    # produces d_ruleOutput\/d_parameterWithinMF\n                    dW_dAplha = senSit * np.array([np.prod([ANFISObj.memClass.evaluateMF(tmpRow)[c][ANFISObj.rules[r][c]] for c in adjCols]) for r in rulesWithAlpha])\n\n                    bucket1 = np.empty(len(ANFISObj.rules[:,0]))\n                    for consequent in range(len(ANFISObj.rules[:,0])):\n                        fConsequent = np.dot(np.append(ANFISObj.X[rowX,:],1.),ANFISObj.consequents[((ANFISObj.X.shape[1] + 1) * consequent):(((ANFISObj.X.shape[1] + 1) * consequent) + (ANFISObj.X.shape[1] + 1)),colY])\n                        acum = 0\n                        if consequent in rulesWithAlpha:\n                            acum = dW_dAplha[np.where(rulesWithAlpha==consequent)] * theWSum[rowX]\n\n                        acum = acum - theW[consequent,rowX] * np.sum(dW_dAplha)\n                        acum = acum \/ theWSum[rowX]**2\n                        bucket1[consequent] = fConsequent * acum\n\n                    sum1 = np.sum(bucket1)\n\n                    if ANFISObj.Y.ndim == 1:\n                        bucket2[colY] = sum1 * (ANFISObj.Y[rowX]-theLayerFive[rowX,colY])*(-2)\n                    else:\n                        bucket2[colY] = sum1 * (ANFISObj.Y[rowX,colY]-theLayerFive[rowX,colY])*(-2)\n\n                sum2 = np.sum(bucket2)\n                bucket3[rowX] = sum2\n\n            sum3 = np.sum(bucket3)\n            parameters[timesThru] = sum3\n            timesThru = timesThru + 1\n\n        paramGrp[MF] = parameters\n\n    return paramGrp\n\n\ndef predict(ANFISObj, varsToTest):\n\n    [layerFour, wSum, w] = forwardHalfPass(ANFISObj, varsToTest)\n\n    #layer five\n    layerFive = np.dot(layerFour,ANFISObj.consequents)\n\n    return layerFive\n\n","46f236e7":"def return_rule():\n    return [['gaussmf',{'mean':0.,'sigma':1.}],['gaussmf',{'mean':-1.,'sigma':2.}],['gaussmf',{'mean':-4.,'sigma':10.}],['gaussmf',{'mean':-7.,'sigma':7.}]]","69dd9627":"mf = []\nfor x in range(0, threshold_feature_selection):\n    mf.append(return_rule())","677f5cdb":"%%time\nmfc = MemFuncs(mf)\nanf = ANFIS(x_train, train_y, mfc)\npredictions = anf.trainHybridJangOffLine(epochs=2)","763f0c3e":"print(round(anf.consequents[-1][0],6))\nprint(round(anf.consequents[-2][0],6))\nprint(round(anf.fittedValues[9][0],6))\nif round(anf.consequents[-1][0],6) == -5.275538 and round(anf.consequents[-2][0],6) == -1.990703 and round(anf.fittedValues[9][0],6) == 0.002249:\n    print('test is good')\n\nprint(\"Plotting errors\")\nanf.plotErrors()\nprint(\"Plotting results\")\nanf.plotResults()","bef7a1a6":"mean_absolute_error(train_y, predictions), mean_squared_error(train_y, predictions)","7ab5e288":"# Processing columns","deaf12ac":"Src IP, Dst IP, Label need to be pre-processed","21110911":"As we sorted data with respect to time, We can not split train and test with respect to time.","d981e102":"# HONN","3f59c39a":"# ANFIS","5c4ca598":"# Reading File","ba66ff06":"change labels to integers","cea30b20":"# RNN","bc26f7c0":"# Normalize Data","950ba558":"Train Test Split","eb91a2c2":"Threshold preprocessing required for ANFIS","7024e9cb":"# LSTM","8e407c5e":"# Separating(Y)"}}