{"cell_type":{"15693b12":"code","687e9294":"code","c91a0dd5":"code","a3d4bc17":"code","9281689d":"code","f1acbd75":"code","901e6080":"code","a7a3d932":"code","f1e563c8":"code","138b52f8":"code","b8726d68":"code","dc88d42a":"code","8f0f16a5":"code","283c0cbb":"code","fe164f2c":"code","76d71cda":"code","56fe57d1":"code","691f81c3":"code","a68bcf13":"code","e9a7f1b7":"code","fdbfd80f":"code","be941ccf":"code","a1570716":"code","66de2197":"code","be3ad854":"code","c77c0dab":"code","e70f9a73":"code","e33ca93e":"code","4a9aa1c3":"code","bbcbdde2":"code","a086f171":"code","43133c68":"code","615cac26":"code","50fa10c1":"code","c5216129":"code","2e960d2e":"code","74de1233":"code","01804fa5":"code","0bd75151":"code","87781a85":"code","ac9a2401":"code","8b9d0547":"code","402644aa":"code","5de00325":"code","e8e87589":"code","1e3687e9":"code","48b103c6":"code","c4da82ed":"code","e85931cb":"code","ad5aff76":"code","868f23d0":"code","5e5ef28d":"code","1884101a":"code","d4e80dda":"code","901e5607":"code","a0e9f06b":"code","9c9dc479":"code","591760be":"code","aa9bdf91":"code","6f9a88c9":"code","a0439a0b":"code","7fe165d1":"code","95000165":"code","7a013bcd":"code","492438d7":"code","c3d72cba":"code","bb4e045c":"code","c46bec6b":"code","b35d2ad3":"code","f7c3df5b":"code","d39cdd13":"code","4e4c293d":"code","0cb87a7c":"code","39319625":"code","89afdf19":"code","e3510ce7":"markdown","911ad81c":"markdown","602ebb0b":"markdown","d0956662":"markdown","806d62cc":"markdown","136a1d9e":"markdown","e48e710a":"markdown","bdabf8b0":"markdown","fb90540b":"markdown","3eef6d33":"markdown","4c1ab815":"markdown","a30110c8":"markdown","a8cc435d":"markdown","c5974278":"markdown","22607ae3":"markdown","6886d391":"markdown"},"source":{"15693b12":"import pandas as pd\n#load the data\nsample=pd.read_csv(\"..\/input\/samplereviews.csv\")","687e9294":"#check the loaded data\nprint(sample.shape)","c91a0dd5":"#look of the dataset\nsample.head()","a3d4bc17":"# Understand how customer ratings are distributed\nimport seaborn as sns\nsns.countplot(sample.Score)","9281689d":"#converting the Numerical reviws to categorical reviews on codition above 3 are\n#positive and below 3 are negative as reviews rating with 3 are not much useful\n#for analysis\n\n#function\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'\n\n#changing reviews with score less than 3 to be positive\nactualScore = sample['Score']\npositiveNegative = actualScore.map(partition) \nsample['Score'] = positiveNegative","f1acbd75":"sample.head()","901e6080":"# no of positive and negative reviews\nsample[\"Score\"].value_counts()\n#here we can say it is a unbalanced data set","a7a3d932":"#dropping  the duplicates column if any using drop duplicates from pandas\nsorted_data=sample.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape","f1e563c8":"# no duplicate columns found\n(final['Id'].size*1.0)\/(sample['Id'].size*1.0)*100","138b52f8":"final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n# Help..Num is always less than Denom.. as Denom is people who upvote and donwvote\n#Before understanding text preprocessing lets see the number of entries left\nprint(final.shape)\n\n#How many positive and negative reviews are present in our dataset?\nfinal['Score'].value_counts()\n\n# after removing duplicate rows we found, 8346 positive and 1457 negative","b8726d68":"# After Removing Duplicate rows\nimport seaborn as sns\nsns.countplot(final.Score)","dc88d42a":"# find sentences containing HTML tags\nimport re\ni=0;\nfor sent in final['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;","8f0f16a5":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nsno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer which is developed in recent years\nstop=set(stopwords.words('english'))\n\n\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned\nprint(stop)\nprint('************************************')\nprint(sno.stem('tasty'))","283c0cbb":"i=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'positive': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","fe164f2c":"final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \nfinal['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")","76d71cda":"final.shape # cleaned text column added","56fe57d1":"final.head(3) #below the processed review can be seen in the CleanedText Column ","691f81c3":"# however, this is not required for clustering, just segregaring positive,negative and storing seperetly\ndata_pos = final[final[\"Score\"] == \"positive\"]\ndata_neg = final[final[\"Score\"] == \"negative\"]\nfinal = pd.concat([data_pos, data_neg])\nscore =final[\"Score\"]\nfinal.head()\n","a68bcf13":"#Converting the time frame and sorting in increasing order for easyness\nfinal[\"Time\"] = pd.to_datetime(final[\"Time\"], unit = \"s\")\nfinal= final.sort_values(by = \"Time\")\nfinal.head()","e9a7f1b7":"# Generating bag of words features.\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nbow = count_vect.fit_transform(final['CleanedText'].values)\nbow.shape","fdbfd80f":"bow","be941ccf":"# to understand what kind of words generated as columns by BOW\nterms = count_vect.get_feature_names()","a1570716":"#first 10 columns generated by BOW\nterms[1:10]","66de2197":"#using all processes jobs=-1 and k means++ for starting initilization advantage\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters = 10,init='k-means++', n_jobs = -1,random_state=99)\nmodel.fit(bow)","be3ad854":"labels = model.labels_\ncluster_center=model.cluster_centers_","c77c0dab":"cluster_center","e70f9a73":"from sklearn import metrics\nsilhouette_score = metrics.silhouette_score(bow, labels, metric='euclidean')","e33ca93e":"# which tells us that clusters are far away from each other \nsilhouette_score","4a9aa1c3":"# Giving Labels\/assigning a cluster to each point\/text \ndf = final\ndf['Bow Clus Label'] = model.labels_ # the last column you can see the label numebers\ndf.head(2)","bbcbdde2":"# How many points belong to each cluster -> using group by in pandas\ndf.groupby(['Bow Clus Label'])['Text'].count()","a086f171":"#Refrence credit - to find the top 10 features of cluster centriod\n#https:\/\/stackoverflow.com\/questions\/47452119\/kmean-clustering-top-terms-in-cluster\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = count_vect.get_feature_names()\nfor i in range(10):\n    print(\"Cluster %d:\" % i, end='')\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind], end='')\n        print()","43133c68":"# visually how points or reviews are distributed across 10 clusters \nimport matplotlib.pyplot as plt\nplt.bar([x for x in range(10)], df.groupby(['Bow Clus Label'])['Text'].count(), alpha = 0.4)\nplt.title('KMeans cluster points')\nplt.xlabel(\"Cluster number\")\nplt.ylabel(\"Number of points\")\nplt.show()","615cac26":"# Reading a review which belong to each group.\nfor i in range(10):\n    print(\"A review of assigned to cluster \", i)\n    print(\"-\" * 70)\n    print(df.iloc[df.groupby(['Bow Clus Label']).groups[i][0]]['Text'])\n    print('\\n')\n    print(\"_\" * 70)","50fa10c1":"#considers sample of 3 random reviews for cluster 0\n\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[0][3]]['Text'])\nprint(\"_\" * 70)\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[0][15]]['Text'])\nprint(\"_\" * 70)\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[0][25]]['Text'])","c5216129":"#consider sample of 3 random reviews for cluster 4\n\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[3][3]]['Text'])\nprint(\"_\" * 70)\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[3][15]]['Text'])\nprint(\"_\" * 70)\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[3][25]]['Text'])","2e960d2e":"#consider sample of 3 random reviews for cluster 4\n\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[5][3]]['Text'])\nprint(\"_\" * 70)\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[5][15]]['Text'])\nprint(\"_\" * 70)\nprint(df.iloc[df.groupby(['Bow Clus Label']).groups[5][25]]['Text'])","74de1233":"#tfidf vector initililization\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect = TfidfVectorizer()\ntfidf = tfidf_vect.fit_transform(final['CleanedText'].values)\ntfidf.shape","01804fa5":"from sklearn.cluster import KMeans\nmodel_tf = KMeans(n_clusters = 10, n_jobs = -1,random_state=99)\nmodel_tf.fit(tfidf)","0bd75151":"labels_tf = model_tf.labels_\ncluster_center_tf=model_tf.cluster_centers_","87781a85":"cluster_center_tf","ac9a2401":"# to understand what kind of words generated as columns by BOW\nterms1 = tfidf_vect.get_feature_names()","8b9d0547":"terms1[1:10]","402644aa":"from sklearn import metrics\nsilhouette_score_tf = metrics.silhouette_score(tfidf, labels_tf, metric='euclidean')","5de00325":"silhouette_score_tf","e8e87589":"# Giving Labels\/assigning a cluster to each point\/text \ndf1 = df\ndf1['Tfidf Clus Label'] = model_tf.labels_\ndf1.head(5)","1e3687e9":"# How many points belong to each cluster ->\n\ndf1.groupby(['Tfidf Clus Label'])['Text'].count()","48b103c6":"#Refrence credit - to find the top 10 features of cluster centriod\n#https:\/\/stackoverflow.com\/questions\/47452119\/kmean-clustering-top-terms-in-cluster\nprint(\"Top terms per cluster:\")\norder_centroids = model_tf.cluster_centers_.argsort()[:, ::-1]\nfor i in range(10):\n    print(\"Cluster %d:\" % i, end='')\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms1[ind], end='')\n        print()","c4da82ed":"# visually how points or reviews are distributed across 10 clusters \n\nplt.bar([x for x in range(10)], df1.groupby(['Tfidf Clus Label'])['Text'].count(), alpha = 0.4)\nplt.title('KMeans cluster points')\nplt.xlabel(\"Cluster number\")\nplt.ylabel(\"Number of points\")\nplt.show()","e85931cb":"# Reading a review which belong to each group.\nfor i in range(10):\n    print(\"4 review of assigned to cluster \", i)\n    print(\"-\" * 70)\n    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][5]]['Text'])\n    print('\\n')\n    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][10]]['Text'])\n    print('\\n')\n    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][20]]['Text'])\n    print('\\n')\n    print(\"_\" * 70)","ad5aff76":"# Train your own Word2Vec model using your own text corpus\ni=0\nlist_of_sent=[]\nfor sent in final['CleanedText'].values:\n    list_of_sent.append(sent.split())","868f23d0":"print(final['CleanedText'].values[0])\nprint(\"*****************************************************************\")\nprint(list_of_sent[0])","5e5ef28d":"\n# removing html tags and apostrophes if present.\nimport re\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned","1884101a":"i=0\nlist_of_sent_train=[]\nfor sent in final['CleanedText'].values:\n    filtered_sentence=[]\n    sent=cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if(cleaned_words.isalpha()):    \n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue \n    list_of_sent_train.append(filtered_sentence)","d4e80dda":"import gensim\n# Training the wor2vec model using train dataset\nw2v_model=gensim.models.Word2Vec(list_of_sent_train,size=100, workers=4)","901e5607":"import numpy as np\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this train\nfor sent in list_of_sent_train: # for each review\/sentence\n    sent_vec = np.zeros(100) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\nsent_vectors = np.array(sent_vectors)\nsent_vectors = np.nan_to_num(sent_vectors)\nsent_vectors.shape\n","a0e9f06b":"# Number of clusters to check.\nnum_clus = [x for x in range(3,11)]\nnum_clus","9c9dc479":"# Choosing the best cluster using Elbow Method.\n# source credit,few parts of min squred loss info is taken from different parts of the stakoverflow answers.\n# this is used to understand to find the optimal clusters in differen way rather than used in BOW, TFIDF\nsquared_errors = []\nfor cluster in num_clus:\n    kmeans = KMeans(n_clusters = cluster).fit(sent_vectors) # Train Cluster\n    squared_errors.append(kmeans.inertia_) # Appending the squared loss obtained in the list\n    \noptimal_clusters = np.argmin(squared_errors) + 2 # As argmin return the index of minimum loss. \nplt.plot(num_clus, squared_errors)\nplt.title(\"Elbow Curve to find the no. of clusters.\")\nplt.xlabel(\"Number of clusters.\")\nplt.ylabel(\"Squared Loss.\")\nxy = (optimal_clusters, min(squared_errors))\nplt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\nplt.show()\n\nprint (\"The optimal number of clusters obtained is - \", optimal_clusters)\nprint (\"The loss for optimal cluster is - \", min(squared_errors))","591760be":"# Training the best model --\nfrom sklearn.cluster import KMeans\nmodel2 = KMeans(n_clusters = optimal_clusters)\nmodel2.fit(sent_vectors)","aa9bdf91":"word_cluster_pred=model2.predict(sent_vectors)\nword_cluster_pred_2=model2.labels_\nword_cluster_center=model2.cluster_centers_","6f9a88c9":"word_cluster_center[1:2]","a0439a0b":"# Giving Labels\/assigning a cluster to each point\/text \ndfa = df1\ndfa['AVG-W2V Clus Label'] = model2.labels_\ndfa.head(2)","7fe165d1":"# How many points belong to each cluster ->\ndfa.groupby(['AVG-W2V Clus Label'])['Text'].count()","95000165":"# Reading a review which belong to each group.\nfor i in range(optimal_clusters):\n    print(\"A review of assigned to cluster \", i)\n    print(\"-\" * 70)\n    print(dfa.iloc[dfa.groupby(['AVG-W2V Clus Label']).groups[i][0]]['Text'])\n    print('\\n')\n    print(dfa.iloc[dfa.groupby(['AVG-W2V Clus Label']).groups[i][1]]['Text'])\n    print('\\n')\n    print(\"_\" * 70)","7a013bcd":"from sklearn.cluster import DBSCAN","492438d7":"# Computing 200th Nearest neighbour distance\nminPts = 2 * 100\n# Lower bound function copied from -> https:\/\/gist.github.com\/m00nlight\/0f9306b4d4e61ba0195f\ndef lower_bound(nums, target): # This function return the number in the array just greater than or equal to itself.\n    l, r = 0, len(nums) - 1\n    while l <= r: # Binary searching.\n        mid = int(l + (r - l) \/ 2)\n        if nums[mid] >= target:\n            r = mid - 1\n        else:\n            l = mid + 1\n    return l\n\ndef compute200thnearestneighbour(x, data): # Returns the distance of 200th nearest neighbour.\n    dists = []\n    for val in data:\n        dist = np.sum((x - val) **2 ) # computing distances.\n        if(len(dists) == 200 and dists[199] > dist): # If distance is larger than current largest distance found.\n            l = int(lower_bound(dists, dist)) # Using the lower bound function to get the right position.\n            if l < 200 and l >= 0 and dists[l] > dist:\n                dists[l] = dist\n        else:\n            dists.append(dist)\n            dists.sort()\n    \n    return dists[199] # Dist 199 contains the distance of 200th nearest neighbour.","c3d72cba":"# Computing the 200th nearest neighbour distance of some point the dataset:\ntwohundrethneigh = []\nfor val in sent_vectors[:1500]:\n    twohundrethneigh.append( compute200thnearestneighbour(val, sent_vectors[:1500]) )\ntwohundrethneigh.sort()","bb4e045c":"\n# Plotting for the Elbow Method :\nplt.figure(figsize=(14,4))\nplt.title(\"Elbow Method for Finding the right Eps hyperparameter\")\nplt.plot([x for x in range(len(twohundrethneigh))], twohundrethneigh)\nplt.xlabel(\"Number of points\")\nplt.ylabel(\"Distance of 200th Nearest Neighbour\")\nplt.show()\n","c46bec6b":"# Training DBSCAN :\nmodel = DBSCAN(eps = 5, min_samples = minPts, n_jobs=-1)\nmodel.fit(sent_vectors)","b35d2ad3":"\ndfdb = dfa\ndfdb['AVG-W2V Clus Label'] = model.labels_\ndfdb.head(2)","f7c3df5b":"dfdb.groupby(['AVG-W2V Clus Label'])['Id'].count()","d39cdd13":"import scipy\nfrom scipy.cluster import hierarchy\ndendro=hierarchy.dendrogram(hierarchy.linkage(sent_vectors,method='ward'))\nplt.axhline(y=35)# cut at 30 to get 5 clusters","4e4c293d":"from sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  #took n=5 from dendrogram curve \nAgg=cluster.fit_predict(sent_vectors)","0cb87a7c":"# Giving Labels\/assigning a cluster to each point\/text \naggdfa = dfdb\naggdfa['AVG-W2V Clus Label'] = cluster.labels_\naggdfa.head(2)","39319625":"# How many points belong to each cluster ->\naggdfa.groupby(['AVG-W2V Clus Label'])['Text'].count()","89afdf19":"# Reading a review which belong to each group.\nfor i in range(5):\n    print(\"2 reviews of assigned to cluster \", i)\n    print(\"-\" * 70)\n    print(aggdfa.iloc[aggdfa.groupby(['AVG-W2V Clus Label']).groups[i][0]]['Text'])\n    print('\\n')\n    print(aggdfa.iloc[aggdfa.groupby(['AVG-W2V Clus Label']).groups[i][1]]['Text'])\n    print('\\n')\n    print(\"_\" * 70)","e3510ce7":"##  K means using TFIDF","911ad81c":"The dataset contains the following columns :\n\n1.Id->Review for each ID\n\n2.Product Id->Unique identifier for the product\n\n3.User Id->Unique identifier for the user\n\n4.Profile Name->A user who has given the review\n\n5.Helpful Numerator->No. of users who found the review helpful\n\n6.Helpful Denominator->No. of users who found the review helpful or not\n\n7.Score->Five being is the highest rating and 1 being the lowest rating\n\n8.Time->Date and time  when the review was given\n\n9.Summary->Summary of the review\n\n10.Text->Review text\n","602ebb0b":"#  Clustering\n\nFind Clustering models for both Bag of words, term frequcny\/ inverse document frequcny and avg word to vector","d0956662":"# Average Word to Vector","806d62cc":"__Analysis of K means for BOW:__\n\n__Of all the clusters, 0, 4 and 6 accounts to more % of reviews, undertsanding differences between these 3 clusters is key.\nAlso, the clusters 2 and 9 have only 1 review__ \n\nIf we observe the top terms per cluster, The cluster 4 which consists of LIKE AND LOVE, which are top centroid features and can say this cluster consists of all positive reviews, let us obersve few reviews of each cluster and try to understand the differences\n\nBy reading the cluster 2 and 9 which contains only one review, which is clearlt negative reviews and we can conluded customers\ndidnt liked the product at all and not word is used extensively\n\nBy reading random reviews of cluster 0, we can easily say that these reviews are extremly positive of the product usage and customers are very happy with the product\n\nBy reading random reviews of cluster 4, we can  say that the key word __BUT__ is repeating acorss the review which indicates some kind of peopel agree with most of the things related to the products but their is something which is slightyly disagree with product quality or delivery or  some thing less than their expectation\n\n","136a1d9e":"# Text Processing\n\nTo make the text clean by removing HTML tag reviews, stopwords to segregate and adding timestamp","e48e710a":"# Introduction","bdabf8b0":"__Analysis of K means for TF_IDF:__\n\n__Of all the cluster 4  accounts to more % of reviews i,e above 4000.\n\nIf we observe the top terms per cluster, The clusters based on the products and product wise like and dislikes.\nfor example, if we oberve cluster 8, the reviews talk more about chips, potatos, and other products which are like snacks\n\nIn these, its better to understand the cluster center top features rather than individual reviews.","fb90540b":" # Data Cleaning","3eef6d33":"## K Means CLustering for Avg word to vectors","4c1ab815":"# Clustering Hierarchical","a30110c8":"### K means using bag of words","a8cc435d":"Conclusion:\nKmeans for bag of words and TFIDF\n1. By using Elbow method, we generated optimal 10 clusters for both the bag of words and tfidf techniques\n2. In both the cases, one cluster accounts around 6000 reviews which is large chunk from 10k reviews and rest are distributed unevenly\n3. we can ignore 2 clusters or keep 2 clusters depending upon the business goal for bag of words generation as both contain only 1 review\n\nFinal Observations:\n\nFOR TFIDF K means is best for identification than K MEANS for BOW, all the clusters are clearly refelcting they were grouped based on the categories\/products. However, K means did best on the cluster centers top terms but however when we caopare reviews , few places it is not correalting.\n\nDBSCAN is very poorly performining on the 10k columns as it is grouping all reviews in one cluster\n\nHierarchical, for BOW and TFIDF, we cannot identify the clusters and not divded unevenly, but for avg word to vectors all are grouped and divided evenly. It is very difficult to identify the type of reveiws based on Hirarchial formation. (PLEASE NOTE  HIERARCHICAL  IS TAKING SO MUCH TIME TO PROCESS, Please if any one would like to see the hierarchical , please upvote and i will send the jupyter notebook) Thank you\n\n\n\n\n\n","c5974278":"https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\nThe dataset consists of 13 year\u2019s data which consists of 10 attributes for 568000 reviews. Due to the computational complexity, we will use a random sample of 10,000 reviews for our analysis.\n\nLets understand on how and what kind of  clusters are formed based on text reviews. Please note we are not performing DENSITY BASED for bag of words and TF-IDF as it is not feasible with lot of features or columns.However, with Avg word to vectors, the DBSCAN is shown in the notebook,","22607ae3":"## Clustering DBSCAN","6886d391":"Conclusions for Elbow Method\n\nThe Knee point seems to be 5. So Eps = 5"}}