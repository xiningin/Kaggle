{"cell_type":{"cd74a726":"code","bd2d1dd6":"code","784293ba":"code","4773ebb1":"code","de00cbfc":"code","b3b8f2f4":"code","6a518210":"code","3f4ba164":"code","6df6ccfa":"code","56c4e529":"code","b5017c98":"code","ebfdfe36":"code","0a157a20":"code","16e7e1c5":"code","696788c5":"code","5fd009b4":"code","17f9e0a6":"code","9662d71a":"code","8eb89d7e":"code","0bd869c9":"code","524fed6e":"code","2533e17e":"code","23c7e12f":"code","dd007e80":"code","2a9baa86":"code","d96a3ce0":"code","8a41525f":"code","e31f66db":"code","32e0dd3d":"code","81fd2cc9":"code","e3bbfe85":"code","b12818f1":"code","b6ff32d9":"code","918ae974":"code","a96c8631":"code","6f408dfc":"code","f86ecdbb":"code","f6169397":"code","91658e73":"code","70d207d1":"code","13c9de51":"code","c433bc97":"code","dda91a79":"code","3ba4efea":"code","ab4a6565":"code","7ddf7a97":"code","58849ce3":"code","aca024b4":"code","edd06f70":"code","fe85e924":"code","9d624bd8":"code","e8d087d5":"code","89d8cfca":"code","db177d68":"code","752305e9":"markdown","4b6053f0":"markdown","df2421bc":"markdown","1356f96a":"markdown","36540c68":"markdown","1c5d7c8a":"markdown","e7ff1369":"markdown","5306982b":"markdown","bd924c20":"markdown","c6697362":"markdown","0e5b8997":"markdown","9335a42c":"markdown","a3f18918":"markdown","7534443d":"markdown","8752ce76":"markdown","2302d5d9":"markdown","0da5ccca":"markdown","7e622a6e":"markdown","04ecd576":"markdown","09fc97d2":"markdown","66f41b4a":"markdown","5cfbb395":"markdown","b3ae0928":"markdown","677f9e95":"markdown","5302ef15":"markdown","24ed2db1":"markdown","a061eb91":"markdown","db1f2c9f":"markdown","831e523b":"markdown","94cb0ae9":"markdown","142453d3":"markdown","2461457e":"markdown","0ea4e228":"markdown","8f6101ef":"markdown","75aeb428":"markdown","0f65555b":"markdown","9853e533":"markdown","f6e54ab0":"markdown","a008f114":"markdown","30389f18":"markdown","cde828cb":"markdown","79c93b54":"markdown","ae376fb1":"markdown","c4a8d8d7":"markdown"},"source":{"cd74a726":"!pip install fastai2 torchviz","bd2d1dd6":"import multiprocessing as mp\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom skmultilearn.model_selection import IterativeStratification\nfrom fastai2.layers import *\nfrom fastai2.vision.all import *\nfrom fastai2.callback.core import *\nfrom fastai2.optimizer import *\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nimport scipy.optimize as opt\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport gc\nimport zipfile\nimport os\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n      \nROOT = Path('..\/input\/jovian-pytorch-z2g')\nDIR = ROOT \/ 'Human protein atlas'\nTRAIN = DIR \/ 'train'\nTEST = DIR \/ 'test'\nSEED = 2020\nFOCAL = False\nEPOCHS = 12\nBATCH_SIZE = 8\nIMG_SIZE = 512\neps = 1e-6\npct_start = 0.5\nwd=1e-4\n\n\nMODEL_PATH = Path('\/kaggle\/working\/models')\nMODEL_PATH.mkdir(exist_ok=True)\n\nsubmission = pd.read_csv(ROOT \/ 'submission.csv')","784293ba":"#all_files = list(TRAIN.iterdir()) + list(TEST.iterdir())\n#x_tot, x2_tot = [], []\n#for file in tqdm(files):\n#    img = cv2.imread(str(file), cv2.COLOR_RGB2BGR)\n#    img = img\/255.0\n#    x_tot.append(img.reshape(-1, 3).mean(0))\n#    x2_tot.append((img**2).reshape(-1, 3).mean(0))\n\n#image stats\n#img_avr =  np.array(x_tot).mean(0)\n#img_std =  np.sqrt(np.array(x2_tot).mean(0) - img_avr**2)\n#print('mean:',img_avr, ', std:', np.sqrt(img_std))\n# -","4773ebb1":"mean = torch.tensor([[0.05438065, 0.05291743, 0.07920227]])\nstd = torch.tensor([[0.39414383, 0.33547948, 0.38544176]])","de00cbfc":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False # This seems to speed up the training without losing reproduction\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n \n\ndef encode_label(label):\n    target = torch.zeros(10)\n    for l in str(label).split(' '):\n        target[int(l)] = 1.\n    return target\n\ndef decode_target(target, text_labels=False, threshold=0.5):\n    result = []\n    for i, x in enumerate(target):\n        if (x >= threshold):\n            if text_labels:\n                result.append(labels[i] + \"(\" + str(i) + \")\")\n            else:\n                result.append(str(i))\n    return ' '.join(result)\n\n\nlabels = {\n    0: 'Mitochondria',\n    1: 'Nuclear bodies',\n    2: 'Nucleoli',\n    3: 'Golgi apparatus',\n    4: 'Nucleoplasm',\n    5: 'Nucleoli fibrillar center',\n    6: 'Cytosol',\n    7: 'Plasma membrane',\n    8: 'Centrosome',\n    9: 'Nuclear speckles'\n}\n\nindexes = {str(v):k for k,v in labels.items()}\n\ndef get_test_images(img_ids):\n    return [f'{TEST}\/{img_id}.png' for img_id in img_ids]\n\n# Get the Image Files in the same order as the submission dataframe\ntest_images = get_test_images(submission.Image.tolist())\n\ndef create_split_df(nfolds=5, order=1):\n\n    df = pd.read_csv(DIR \/ 'train.csv').sort_values(\"Image\").reset_index(drop=True)\n\n    split_df = pd.get_dummies(df.Label.str.split(\" \").explode())\n\n    split_df = split_df.groupby(split_df.index).sum() \n\n    X, y = split_df.index.values, split_df.values\n\n    k_fold = IterativeStratification(n_splits=nfolds, order=order)\n\n    splits = list(k_fold.split(X, y))\n\n    fold_splits = np.zeros(df.shape[0]).astype(np.int)\n\n    for i in range(nfolds):\n        fold_splits[splits[i][1]] = i\n\n    df['Split'] = fold_splits\n\n    df_folds = []\n\n    for fold in range(nfolds):\n\n        df_fold = df.copy()\n            \n        df_fold['is_valid'] = False\n        \n        df_fold.loc[df_fold.Split == fold, 'is_valid'] = True\n        \n        df_folds.append(df_fold)\n\n    return df_folds\n\ndef get_x(r):\n    return f\"{TRAIN}\/{r['Image']}.png\"\n\ndef get_y(r):    \n    return [labels[int(l)] for l in r['Label'].split()]\n\ndef splitter(df, full_train=False):\n    # If you want to use the whole training set, \n    # just sample one image from the more numerous class and with a single label as a dummy validation set\n    if full_train:\n        valid = df[(df.Label.str.contains('4')) & (df.Label.str.len() == 1)].sample(n=1)\n        train = df.drop(valid.index, axis=0)        \n        return valid.index.tolist(), train.index.tolist()\n                \n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    \n    return train,valid\n\n\ndef get_dataloaders(df, bs, size, simple_aug=False, full_train=False):\n    \n    splt = partial(splitter, full_train=full_train)\n    \n    if simple_aug:\n        tfms = setup_aug_tfms([Rotate(draw=90, p=0.5),\n                               Brightness(max_lighting=0.4, p=0.75),\n                               Dihedral(p=0.5),\n                               Normalize.from_stats(mean, std)])\n    else:       \n        tfms = [*aug_transforms(size=size, min_scale=0.75, flip_vert=True),\n                               Normalize.from_stats(mean, std)]\n                \n    item_tfms = Resize(size) if size < 512 else None\n    \n    dblock = DataBlock(get_x=get_x, \n                       get_y=get_y, \n                       blocks=(ImageBlock, MultiCategoryBlock),\n                       splitter=splt,\n                       item_tfms=item_tfms, \n                       batch_tfms=tfms)\n    \n    return dblock.dataloaders(df, bs=bs, num_workers=mp.cpu_count()) #pin_memory=True returns an error\n\n\ndef data_iterator(nfolds=5, order_splits=2, bs=32, size=512, simple_aug=False, full_train=False, with_test=False):\n    \"\"\" Helper function to lazily return nfolds of fastai dataloaders and optionally a test one\n    \"\"\"\n    dfs = get_dfs(nfolds, order_splits)\n    for df in dfs:\n        dls = get_dataloaders(df, bs=bs, size=size, simple_aug=simple_aug, full_train=full_train)        \n        if with_test:\n            test_dl = dls.test_dl(test_images, shuffle=False, drop_last=False)\n            yield dls, test_dl\n        else:\n            yield dls, None","b3b8f2f4":"def sigmoid_np(x):\n    return 1.0\/(1.0 + np.exp(-x))\n\ndef F1_soft(preds,targs,th=0.0,d=25.0):\n    preds = sigmoid_np(d*(preds - th))\n    targs = targs.astype(np.float)\n    score = 2.0*(preds*targs).sum(axis=0)\/((preds+targs).sum(axis=0) + 1e-6)\n    return score\n\ndef fit_val(x,y):\n    \n    params = np.zeros(10)\n    wd = 1e-5\n    error = lambda p: np.concatenate((F1_soft(x,y,p) - 1.0,\n                                      wd*p), axis=None)\n    p, success = opt.leastsq(error, params)\n    return p\n\ndef get_threshold(pred, y):\n    \"\"\"For each prediction on the validation set, we perform a 10-fold cross-validation to fit the threshold which maximizes the F1_Score\n    The cross validation minimizes the over-fitting of the threshold to the specific fold\n    \"\"\"\n    pred = pred.numpy()\n    y = y.numpy()\n    \n    th, score, cv = 0,0,10\n    \n    for j in range(cv):\n        xt,xv,yt,yv = train_test_split(pred,y,test_size=0.2,random_state=j)\n        th_i = fit_val(xt,yt)\n        th += th_i\n        score += f1_score(yv, xv>th_i, average='macro')\n    th\/=cv\n    score\/=cv\n    return th, score","6a518210":"class FocalSigmoidLossFuncV2(torch.autograd.Function):\n    '''\n    compute backward directly for better numeric stability\n    '''\n    @staticmethod\n    def forward(ctx, logits, label, alpha, gamma):\n        logits = logits.float()\n        coeff = torch.empty_like(logits).fill_(1 - alpha)\n        coeff[label == 1] = alpha\n\n        probs = torch.sigmoid(logits)\n        log_probs = torch.where(logits >= 0,\n                F.softplus(logits, -1, 50),\n                logits - F.softplus(logits, 1, 50))\n        log_1_probs = torch.where(logits >= 0,\n                -logits + F.softplus(logits, -1, 50),\n                -F.softplus(logits, 1, 50))\n        probs_gamma = probs ** gamma\n        probs_1_gamma = (1. - probs) ** gamma\n\n        ctx.coeff = coeff\n        ctx.probs = probs\n        ctx.log_probs = log_probs\n        ctx.log_1_probs = log_1_probs\n        ctx.probs_gamma = probs_gamma\n        ctx.probs_1_gamma = probs_1_gamma\n        ctx.label = label\n        ctx.gamma = gamma\n\n        term1 = probs_1_gamma * log_probs\n        term2 = probs_gamma * log_1_probs\n        loss = torch.where(label == 1, term1, term2).mul_(coeff).neg_()\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        '''\n        compute gradient of focal loss\n        '''\n        coeff = ctx.coeff\n        probs = ctx.probs\n        log_probs = ctx.log_probs\n        log_1_probs = ctx.log_1_probs\n        probs_gamma = ctx.probs_gamma\n        probs_1_gamma = ctx.probs_1_gamma\n        label = ctx.label\n        gamma = ctx.gamma\n\n        term1 = (1. - probs - gamma * probs * log_probs).mul_(probs_1_gamma).neg_()\n        term2 = (probs - gamma * (1. - probs) * log_1_probs).mul_(probs_gamma)\n\n        grads = torch.where(label == 1, term1, term2).mul_(coeff).mul_(grad_output)\n        return grads, None, None, None\n\n\nclass FocalLossV2(nn.Module):\n    '''\n    This use better formula to compute the gradient, which has better numeric stability\n    '''\n    __name__ = 'focal_loss_v2'\n    reduce = 'none'\n    \n    def __init__(self,\n                 alpha=0.25,\n                 gamma=2,\n                 reduction='mean',\n                 thresh=0.5):\n        super(FocalLossV2, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.thresh = thresh\n\n    def forward(self, logits, label):\n        loss = FocalSigmoidLossFuncV2.apply(logits, label, self.alpha, self.gamma)\n        if self.reduction == 'mean':\n            loss = loss.mean()\n        if self.reduction == 'sum':\n            loss = loss.sum()\n        return loss\n    \n    def decodes(self, x):    return x>self.thresh\n    \n    def activation(self, x): return x","3f4ba164":"encoder = xresnet34","6df6ccfa":"dfs = create_split_df(10, order=2)","56c4e529":"metrics = [partial(accuracy_multi, thresh=0.5), F1ScoreMulti(thresh=0.3)]","b5017c98":"df = dfs[0] # Get just the first fold\ndls = get_dataloaders(df, bs=BATCH_SIZE*4, size=IMG_SIZE\/\/4, simple_aug=True)","ebfdfe36":"@delegates(torch.optim.AdamW.__init__)\ndef lookahead_adamw(param_groups, **kwargs):\n    optimizer = torch.optim.AdamW([{'params': ps, **kwargs} for ps in param_groups])\n    lookahead = Lookahead(optimizer)\n    return OptimWrapper(lookahead)","0a157a20":"# Uncomment this line and comment the other one if you want to use the FocalLoss\nlearn = cnn_learner(dls, encoder, metrics=metrics, loss_func=FocalLossV2(alpha=0.5, thresh=0.3))\n#learn = cnn_learner(dls, encoder, metrics=metrics)\nlearn.to_fp16() #Use float16 ( Mixed Precision Training ) to train a lot faster!\nlearn.opt_func = partial(lookahead_adamw, wd=wd, eps=eps),  # Change the optimizer","16e7e1c5":"from torchviz import make_dot\n\nx, y = dls.one_batch()\ny = learn.model(x)\n\nmake_dot(y.mean(), params=dict(learn.model.params))","696788c5":"lr, lr_step = learn.lr_find()","5fd009b4":"print(lr, lr_step)","17f9e0a6":"learn.model_dir = MODEL_PATH # This is necessary, otherwise the library will try to save in a read_only location raising an error","9662d71a":"learn.fine_tune(epochs=EPOCHS, base_lr=lr_step, freeze_epochs=1, pct_start=0.5)","8eb89d7e":"learn.save('model-0-128') # Save the model of fold 0 for the size 128\ngc.collect()","0bd869c9":"interp = ClassificationInterpretation.from_learner(learn)","524fed6e":"interp.most_confused() # actual, predicted, # of ocurrences","2533e17e":"interp.plot_top_losses(9, figsize=(15,10))","23c7e12f":"learn.load('model-0-128', with_opt=True)\nlearn.fine_tune(epochs=EPOCHS, base_lr=lr_step\/10, freeze_epochs=0, pct_start=0)","dd007e80":"# Define new dataloaders, bigger size and smaller batch size\ndls = get_dataloaders(df, bs=BATCH_SIZE * 2, size=IMG_SIZE\/\/2)\n\n# Substitute the learner dataloader\nlearn.dls = dls \n\n# Load the Previous Weights\nlearn.load('model-0-128', with_opt=True)\n\n\nlearn.fine_tune(epochs=EPOCHS, base_lr=lr_step, freeze_epochs=0, pct_start=0.5)\n\n\nlearn.save('model-0-256')","2a9baa86":"learn.load('model-0-256', with_opt=True)\nlearn.show_results()","d96a3ce0":"interp = ClassificationInterpretation.from_learner(learn)","8a41525f":"interp.most_confused() # actual, predicted, # of ocurrences","e31f66db":"interp.plot_top_losses(9, figsize=(15,10))","32e0dd3d":"interp.plot_top_losses(9, figsize=(15,10)","81fd2cc9":"learn.load('model-0-256', with_opt=True)\nlearn.fine_tune(epochs=EPOCHS, base_lr=lr_step, freeze_epochs=0, pct_start=0)\nlearn.save('model-0-256')","e3bbfe85":"#dls = get_dataloaders(df, bs=BATCH_SIZE, size=IMG_SIZE)\n\n# Substitute the learner dataloader\n#learn.dls = dls \n\n# Load the Previous Weights\n#learn.load('model-0-256') ","b12818f1":"#cbs = [EarlyStoppingCallback(monitor='valid_loss', patience=4)]\n\n#cbs += [SaveModelCallback(monitor='valid_loss', fname='model-0-512')]\n\n#learn.fine_tune(EPOCHS * 2, lr, freeze_epochs=4, cbs=cbs) ","b6ff32d9":"#gc.collect() #free up some memory","918ae974":"#learn.load('model-0-512')\n#learn.show_results(max_n=15)","a96c8631":"#interp = ClassificationInterpretation.from_learner(learn)","6f408dfc":"#interp.most_confused() # actual, predicted, # of ocurrences","f86ecdbb":"#interp.plot_top_losses(9, figsize=(15,10))","f6169397":"# Get the predictions on the validation set\npreds, targs = learn.get_preds()","91658e73":"from pprint import pprint as pp\n\n# Let's get the thresholds which maximizes the F1_score for this train,valid split.\nth, score = get_threshold(preds, targs)\n\nprint('The best threshold for this fold is: \\n')\npp(th)\nprint(f'\\nThe F1 Score using these threshold is: {score}')","70d207d1":"test_dl = learn.dls.test_dl(test_images, shuffle=False, drop_last=False)","13c9de51":"test_dl.show_batch(figsize=(15,10))","c433bc97":"test_dl.items[:5]","dda91a79":"submission.head()","3ba4efea":"# The reorder argument, just as the name implies, reorder the results to the original order, Since we didn't shuffle it, we don't reorder here\npreds_test, targs_test, decoded_preds = learn.get_preds(dl=test_dl, reorder=False, with_decoded=True) ","ab4a6565":"predictions = preds_test > torch.from_numpy(th)","7ddf7a97":"(predictions != decoded_preds).sum().item()","58849ce3":"submission[\"Label\"] = [decode_target(label) for label in predictions]\nsubmission.to_csv('submission.csv', index=False)","aca024b4":"# Get the predictions on the validation set\npreds, targs = learn.tta(n=8)","edd06f70":"from pprint import pprint as pp\n\n# Let's get the thresholds which maximizes the F1_score for this train,valid split.\nth, score = get_threshold(preds, targs)\n\nprint('The best threshold for this fold is: \\n')\npp(th)\nprint(f'\\nThe F1 Score using these threshold is: {score}')","fe85e924":"predictions = preds_test > torch.from_numpy(th)","9d624bd8":"preds_test, _ = learn.tta(dl=test_dl, n=8) ","e8d087d5":"predictions = preds_test > torch.from_numpy(th)","89d8cfca":"(predictions != decoded_preds).sum().item()","db177d68":"submission[\"Label\"] = [decode_target(label) for label in predictions]\nsubmission.to_csv('submission_tta.csv', index=False)","752305e9":"**The function `aug_transforms` returns a bunch of augmentations, usually this is indicated for any image dataset. Nevertheless I've just used in the competition simple Flip, Rotation and Normalization from the Statistic derived above.** I'll use the more strong augmentations here.","4b6053f0":"## Progressive Resizing\ntl;dr: **train, experiment and overfit first with small images**\n\nOne good technique to quickly iterate and test your models is to train first with smaller images, experiment and tweak the hyperparameters. When satisfied with your workflow you can overfit to these smaller images first. Save the weights from the training and use this weights to train with bigger images. \n\nCreate the dataloaders with the above helper function, train first with 128, then 256 and only saving and fine-tuning the model with image size 512","df2421bc":"It's good practice to train and experiment with just one fold. Latter when you are satisfied that your model is good enough you perform the whole training with all folds","1356f96a":"## Imports","36540c68":"This was just a training flow of 1 fold using very simple augmentations. It can be heavily improved if used the tips in the next steps","1c5d7c8a":"## The output of the executed notebook is here: https:\/\/jovian.ml\/ronaldokun\/zero2gans-fastai2-3rd-place-solution\n\nI was getting errors when commiting here on Kaggle but the notebook executed sucessfully","e7ff1369":"Create a list of splitted datasets","5306982b":"Let's check how our model is doing","bd924c20":"* Top: True Label\n* Bottom: Prediction","c6697362":"## Train with bigger images","0e5b8997":"## Metrics and Custom Loss Function\nFrom the father competition, it was discussed that Focal Loss is a lot more robust to Class Imbalance and is a good metric to use.\n\nThe following implementation of Focal Loss performs the gradient computation from scratch. It's a lot faster and stable that the one using the Autograd Module\n\nBorrowed from this repository: https:\/\/github.com\/CoinCheung\/pytorch-loss\/blob\/d895b58553cac268e82c6d22dff636ef8a866e8b\/focal_loss.py","9335a42c":"For experimentation purposes, here I changed the default optimizer, `Adam` to the Pytorch `AdamW` and wraps it in the `Lookahead` Optimizer. For more info on the Lookahead optimizer check this presentation [link](https:\/\/towardsdatascience.com\/lookahead-optimizer-k-steps-forward-1-step-back-b18ffbae1bdd). Those two are modern versions of optimizers that speed up the optimization. ( At least for me it converges the training a lot faster). The following snipet of code is necessary to make this work in fastai2, unfortunately I just made it work now that the competition is over. The model really seems to converge a lot faster than in my local runs. Don't worry if looks complicated if the next function looks complicated, it's a little convoluted but in pure Pytorch is a lot simpler","a3f18918":"It definitely got better, remember that we use very basic data augmentations here.","7534443d":"The great thing about fast.ai is that they have popular models with custom and optimized heads and other tricks ready to use. In my implementation I used the `densenet121`. Since it was showed that a simpler resnet18 could achieve great results, I will use the optimized fastai2 version of resnet18; `xresnet18` ","8752ce76":"\nIf this code seems to much don't worry. It takes a little time to get used to fastai code but it can do wonders to your training workflow and has lots of cool features. \nYou don't need the complicated function of the loss or optimizer, you can ignore those obscure parts.","2302d5d9":"Our model is still pretty bad. It has a hard time distinguising between Cytosol and Centrosome.\n\nLet's see the worst predictions","0da5ccca":"As you can see the thresholds are very different from each other. To use a single number as threshold limits a lot the capacity of our models. In the training workflow you should keep the different thresholds for the different fold and average them in the end to use in the validation set.","7e622a6e":"Even if the last model of the training loop is not the best in terms of F1 Score or validatio loss, the model is still adjusting the parameters, so we save the model at the end for image size 128, this is not the time to save the best model yet, we reserve this for the bigger images in the end.","04ecd576":"## Visualize the model with [torchviz](https:\/\/github.com\/szagoruyko\/pytorchviz)","09fc97d2":"As you can see, even when the Binary Cross Entropy in the Validation Set gets lower and the F1 Score gets better, the Focal Loss sometimes gets worse, so I used here just as metric and not loss.","66f41b4a":"Let's check how much of our predictions differ when using the fitted thresholds  against the default 0.5.","5cfbb395":"Now let's add some helpful callbacks. Callbacks are modifiers of the training loop in fastai, it basically everything is done with Callbacks under the hood. We pass them as a list to the argument `cbs`. We are going to save the model with the best validation loss and also stops the training if the model doesn't get better after 5 epochs.","b3ae0928":"It definitely took a lot longer in my local runs during the competition to get to this F1 Score. Tha `AdamW` combined with the `Lookahead` really seemed to make a difference","677f9e95":"Let's check how much of our predictions differ when using the fitted thresholds  against the default 0.5.","5302ef15":"## Finding the Thresholds which maximizes the F1 Score in the validation set ( Now using TTA )","24ed2db1":"* Update 2: \n - Changing `alpha=0.5` and `thresh=0.3` in Focal Loss \n - Doubling the `lr` with bigger images but only going to `image_size=256` \n - Just 1 freeze epoch per cycle ( default )\n - using xresnet34\n - Using Simpler Transforms ( The Whole bunch from `aug_transforms` from fastai were harder to generalize )","a061eb91":"The interpretation also shows the model misses a lot of predictions for Cytosol","db1f2c9f":"I've used Deep Learning, and particularly fastai, for some time now. Normally I was reluctant to share anything thinking is not going to be of much use, but motivated by this competition, I decided to try and share a notebook in pure Pytorch to show how to implement a Multilabel Stratification and Cross Validation strategy https:\/\/www.kaggle.com\/ronaldokun\/multilabel-stratification-cv-and-ensemble because I thought it was very important to have a competitive edge. I am glad I did that because the notebook proved useful for some, it motivated me to also try to write more pure Pytorch. \n\nI used this competition to learn the latest version of the fastai library, fastai2 (still in development but already ready to use). \n\nfastai is awesome, with a lot of advanced stuff and neat tricks already build in the library, but it can be somewhat of a \"black box\", hard to inspect and adapt the code because is vastly different than familiar python codebases. That's why I share my entire workflow here in fastai2 so it can be useful to others to get to know the library, use it and adapt to other competitions. ","831e523b":"## Test Time Augmentation\nIn TTA the model gets a validations \/ test image, makes a center crop in it and other `n` transforms in the whole dataset and return the average of the predictions. It's another way to improve the generalization of your model. ","94cb0ae9":"Now let's train with the full size image, but since our model is already trained a little we are going to divide the learning rate again by 5, internally the lr is further divided by 2 in the unfrozen fase, this will make the lr 10 times lower from the last cycle and a 100 times lower than the original one.","142453d3":"Find the optimal learning rate for the last layers, the model is frozen by default","2461457e":"The `fine_tune` uses the one_cycle policy and discriminative learning rate:\n* Freezes the model and trains for `freeze_epochs` in one cycle, from very low learning rate until `base_lr`. \n* It halfs the base_lr: `base_lr \/= 2`, unfreeze the model and runs other cycle for `epochs`. \n* The parameter `pct_start` is the percentage of the epochs for each cycle to train in the warm-up fase of the cosine annealing\n\nSlices the learning rate `slice(base_lr\/lr_mult, base_lr)` and distributes to the different layers of the model. The default value of `lr_mult=100`","0ea4e228":"## Finding the Thresholds which maximizes the F1 Score in the validation set ","8f6101ef":"## Helper Functions to fit the Thresholds\nAdapted from Notable Kaggle Master iafoss\nhttps:\/\/www.kaggle.com\/iafoss\/pretrained-resnet34-with-rgby-0-460-public-lb","75aeb428":"Now let's a little more","0f65555b":"The great thing about fast.ai is this. In the following function you just provide the dataloader, the encoder and any other metrics you want to use and you have a model ready to train. It uses **Adam** as a default optimizer and recognizes the dataset format and automatically uses a version of `nn.BCEWithLogits` if you don't provide a custom loss function. \n\n**The Focal Loss, even the highly optimized version above, proved to be very hard to train and converge. If you want to use the default loss, don't provide the `loss_func` in the following function and just use the Focal Loss inside the metric list.**\n\nSo the following is enough to define a model\n```python\nlearn = cnn_learner(dls, encoder)\n```\n\nIf you want to try to use it just provide the aditional parameter `loss_func=FocalLossV2()` to the `cnn_learner` function","9853e533":"Now we train a little more but without the warm-up part of the learning rate: `pct_start=0` and no freezing part.","f6e54ab0":"Now we create a new dataloader with bigger image size and lower batch_size and just replace it in the learner. It's normal for the model to get worse than the previous one in the beginning, since the image size duplicated it gets a little time for the model to adjust and then get better, now we just freeze again for the first 4 epochs. ","a008f114":"## Statistics of the Dataset.\nMany top competitors in Kaggle don't just use the Imagenet Statistics but calculate the Statistics of the current whole dataset ( train + test ) and use those instead. I lack expertise to know which is best but nevertheless I calculated the statistics of the current dataset. The values are already calculated, uncomment the following cell if you want to make the calculation yourself.","30389f18":"1. ## Helpers Functions to Create a Fastai V2 DataSet","cde828cb":"`preds_test` is the actual vector with the probabilities returned by the model with the activation function of the loss ( in this case `torch.sigmoid` ) applied to them. The targs_test is `null` since we don't have the truth label of the test set.\nThe `decoded_preds` is the boolean vector resulted from the comparison of `preds_test` using a fixed threshold (0.5 by default in the loss function but you can change it ). This is returned because of the argument `with_decoded=True`. If you don't want this remove the argument because the default is `False` \n\nSince we have a custom threshold, we'll use just the `preds_test` tensor and compare it with the threshold tensor. Then you can compare your results with the `decoded_test` tensor.","79c93b54":"Let's check now how much our model improved. ","ae376fb1":"Double check to see if the order was preserved","c4a8d8d7":"# Next Steps\n* Use the `Mixup` Callback: `cnn_learner(...cbs=Mixup())`. This is a ingenious trick that create images that are part one label and part the other. It makes for a really powerfull augmentation technique and can fight the class imbalance. The same observation from the other item applies. Don't load any saved weights because the data will be fundamentally different. Also Mixup requires more epochs to converge but can improve considerably the robustness of your models.\n* Run different folds of cross-validation, save the model weights, thresholds and predictions on the test set. In the end you can average the predictions and average of the thresholds and submit this as a final predictions.\n* Test other architectures\n\nFor more details, check my other notebook https:\/\/www.kaggle.com\/ronaldokun\/multilabel-stratification-cv-and-ensemble"}}