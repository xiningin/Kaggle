{"cell_type":{"a7df128b":"code","4f4fa07b":"code","9987ea2e":"code","084b92ae":"code","f6af5e80":"code","0189afed":"code","c24bbb64":"code","f7f18932":"code","fe28b45a":"code","357449c4":"code","8c066ce1":"markdown","b0780977":"markdown","816cc69c":"markdown","ca111e26":"markdown","33e0b029":"markdown","d2cdb7f2":"markdown","35f1ad33":"markdown","c4057df5":"markdown","e66dca23":"markdown"},"source":{"a7df128b":"!pip install -q -U trax","4f4fa07b":"import trax\nimport matplotlib.pyplot as plt\n# Use the tensorflow-numpy backend.\ntrax.fastmath.set_backend('tensorflow-numpy')\nprint(trax.fastmath.backend_name())\nfrom trax.supervised import training\nfrom trax import layers as tl\nimport os","9987ea2e":"class Config:\n    # Origin image size\n    image_size = 28\n    # Input shape of image\n    input_shape = [image_size, image_size, 1]\n    # Learning rate of the Model\n    learning_rate = 0.001\n    # Weight decay rate of the Model\n    batch_size = 100\n    # Number of classes\n    num_classes = 10\n    # Number of Traning Epochs\n    num_epochs = 3\n    # Train Steps Per Epochs\n    train_steps_per_epochs = 60000 \/\/ batch_size\n    # Number of Training Steps\n    num_steps = train_steps_per_epochs * num_epochs","084b92ae":"def dispaly_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()","f6af5e80":"train_stream = trax.data.TFDS(\"fashion_mnist\", keys={\"image\", \"label\"}, train=True)()\neval_stream = trax.data.TFDS(\"fashion_mnist\", keys={\"image\", \"label\"}, train=False)()","0189afed":"train_data_pipeline = trax.data.Serial(\n    trax.data.Shuffle(),\n    trax.data.Batch(Config.batch_size)\n)\ntrain_batches_stream = train_data_pipeline(train_stream)\neval_data_pipeline = trax.data.Serial(\n    trax.data.Batch(Config.batch_size)\n)\neval_batches_stream = eval_data_pipeline(eval_stream)","c24bbb64":"def get_model():\n    model = tl.Serial(\n        tl.ToFloat(),\n        \n        tl.Conv(32, (3, 3), (1, 1), \"SAME\"),\n        tl.LayerNorm(),\n        tl.Relu(),\n        tl.MaxPool(),\n        \n        tl.Conv(64, (3, 3), (1, 1), \"SAME\"),\n        tl.LayerNorm(),\n        tl.Relu(),\n        tl.MaxPool(),\n        \n        tl.Flatten(),\n        tl.Dense(Config.num_classes)\n    )\n    return model","f7f18932":"model = get_model()\nmodel","fe28b45a":"train_task = training.TrainTask(\n    labeled_data=train_batches_stream,\n    loss_layer=tl.CategoryCrossEntropy(),\n    optimizer=trax.optimizers.Adam(Config.learning_rate),\n    n_steps_per_checkpoint=Config.train_steps_per_epochs\n)\neval_task = training.EvalTask(\n    labeled_data=eval_batches_stream,\n    metrics=[tl.CategoryCrossEntropy(), tl.CategoryAccuracy()],\n    n_eval_batches=20\n)","357449c4":"model = get_model()\ntraining_loop = training.Loop(model, train_task, eval_tasks=[eval_task], output_dir=\".\/model\")\ntraining_loop.run(Config.num_steps)","8c066ce1":"## Helpers","b0780977":"## Model Development","816cc69c":"## Overview\nTrax is a high-level API of TensorFlow like Keras and It's also from Google Brain with cleaner code and more elegant architecture to some extend. One of the advantages is that it's very convenient to implement Transformer-Based applications. In this notebook I will implement a Fashion MNIST Classification Model to get started with this Library. You can find more details in https:\/\/github.com\/google\/trax.","ca111e26":"## Training","33e0b029":"# Fashion MNIST: Get Started with Trax\n\n## Table of Contents\n- Overview\n- Setup\n- Configuration\n- Helpers\n- Import and preprcess dataset\n- Model Development\n","d2cdb7f2":"## Import and preprocess dataset","35f1ad33":"## Setup","c4057df5":"### Display Images","e66dca23":"## Configuration"}}