{"cell_type":{"7f3f078d":"code","990b8a35":"code","9ef438de":"code","a236d918":"code","16c2b89b":"code","c44ddd35":"code","a7dfe03f":"code","4e1a6eb0":"code","8dd735c4":"code","91ef55af":"code","35fca1c8":"code","7e7392fb":"code","7fcad65e":"code","87db9cf9":"code","5ff4aac0":"code","71384937":"code","9e74b2e0":"code","86f029d4":"code","0b0de089":"code","19ac5bf5":"code","2f71ccd0":"code","ada520f5":"code","c5a64d86":"code","630c8b8f":"code","a6124e64":"code","ae1c33e1":"code","471e11bd":"code","ac600349":"code","42a74fb8":"code","83a365a0":"code","39536e1b":"code","92f0d276":"code","be66969f":"code","7a6274f2":"code","94ed1034":"code","e3dc4f52":"code","e65b8af6":"code","3f171b9c":"code","20643e52":"markdown","5402b042":"markdown","3f685159":"markdown","bf80ebc3":"markdown","fa7913f9":"markdown","5bc32b1b":"markdown","77d334c2":"markdown","56871202":"markdown","17df810e":"markdown","3e6806a1":"markdown","eb00ea38":"markdown","034932bd":"markdown","ca054735":"markdown","2a329f80":"markdown","2027544c":"markdown","15a0ef03":"markdown","5e300551":"markdown","ebf6ef31":"markdown","46c2587b":"markdown","96357795":"markdown"},"source":{"7f3f078d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","990b8a35":"import os\nimport gc ##garbage collection\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm #for seeing progress\n\nimport sklearn\nimport sklearn.preprocessing\nimport datetime\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport torch #pytorch\nimport torch.nn as nn ##neural net stuff\nimport torch.nn.functional as F #useful torch function (eg.: activations)\nfrom torch.utils.data import TensorDataset, DataLoader, Subset #for easy data batches\nimport torch.optim as optim #optimisation functions\n\nfrom sklearn.model_selection import train_test_split #for creating test and train data\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler #scaling the data to be in range [0,1]","9ef438de":"#load data\nbase_loc = \"\/kaggle\/input\/binance-full-history\/\"\ndf_ltc = pd.read_parquet(base_loc + \"LTC-USDT.parquet\") #load LTC-USDT price?\ndf_btc = pd.read_parquet(base_loc + \"BTC-USDT.parquet\") #load LTC-USDT price?\n#df_btc = pd.read_parquet(base_loc + \"BTC-USDT.parquet\") #load LTC-USDT price?\ndf_eth = pd.read_parquet(base_loc + \"ETH-USDT.parquet\") #load LTC-USDT price?\n\ndf_ltc.info()\ndf_ltc.head()","a236d918":"df_ltc.isna().sum()\ndf_btc.isna().sum()\ndf_eth.isna().sum()","16c2b89b":"print(df_ltc.iloc[-1]) #last known entry","c44ddd35":"plt.figure(figsize=(15, 5));\nplt.plot(df_ltc.index, df_ltc.open.values, color='red', label='ltc open')\nplt.plot(df_ltc.index, df_ltc.close.values, color='green', label='ltc close')\nplt.plot(df_btc.index, df_btc.open.values, label='btc open')\nplt.plot(df_btc.index, df_btc.close.values, label='btc close')\nplt.plot(df_eth.index, df_eth.open.values, label='eth open')\nplt.plot(df_eth.index, df_eth.close.values, label='eth close')\n#plt.plot(df.low.values, color='blue', label='low')\n#plt.plot(df.high.values, color='black', label='high')\nplt.title('LTC,BTC,ETH - USD, stock price')\nplt.xlabel('time')\nplt.ylabel('price (USD)')\nplt.legend(loc='best')\nplt.show()","a7dfe03f":"#the data is stored per minute values so 30 day window is\nwindow = 60*24*30 #minutes * hours * days\ndf_ltc[\"30_moving_avg\"] = df_ltc[\"open\"].rolling(window).mean()\ndf_ltc[\"30_moving_avg_exp\"] = df_ltc[\"open\"].ewm(alpha = 0.0001).mean()\n\ndf_ltc[[\"open\", \"30_moving_avg\", \"30_moving_avg_exp\"]].plot(title=\"moving averages of LTC\", color = [\"black\", \"red\", \"blue\"], figsize = (18, 9))","4e1a6eb0":"plt.figure(figsize=(18, 9));\ndelta_df_ltc = df_ltc[[\"open\", \"close\", \"high\", \"low\"]].pct_change()\ndelta_df_btc = df_btc[[\"open\", \"close\", \"high\", \"low\"]].pct_change()\ndelta_df_eth = df_eth[[\"open\", \"close\", \"high\", \"low\"]].pct_change()\n#delta_df_ltc[[\"open\"]].plot(title=\"delta\", figsize=(18,9))\n#delta_df_btc[[\"open\"]].plot(title=\"delta\", figsize=(18,9))\n#delta_df_eth[[\"open\"]].plot(title=\"delta\", figsize=(18,9))\nplt.plot(delta_df_ltc.index, delta_df_ltc.open.values, label='ltc open')\nplt.plot(delta_df_btc.index, delta_df_btc.open.values, label='btc open')\nplt.plot(delta_df_eth.index, delta_df_eth.open.values, label='eth open')\nplt.title(\"percentage change\")\nplt.legend()\nplt.show\ndelta_df_ltc.head()\n#delta_df_ltc.head()","8dd735c4":"def create_delta_df(delta_dataframe, dataframe):\n    return delta_dataframe.join(dataframe[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]].copy())","91ef55af":"new_df_ltc = create_delta_df(delta_df_ltc, df_ltc)\nnew_df_btc = create_delta_df(delta_df_btc, df_btc)\nnew_df_eth = create_delta_df(delta_df_eth, df_eth)\nnew_df_ltc.head()","35fca1c8":"#we add the 1 hour, 1 day, 3 days, 7 days, cumulative values\n#delta_df.drop([\"30_moving_avg\", \"30_moving_avg_exp\"], 1, inplace = True)\n#semi_df_ltc = df_ltc[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]].copy()\n#new_df = delta_df_ltc.join(semi_df_ltc)\n#new_df = new_df.fillna(0)\n#new_df.head()\n#delta_df_ltc.head()","7e7392fb":"def expected_return(dataframe, window_length):\n    return dataframe.rolling(window_length + 1).sum()","7fcad65e":"new_df_ltc[\"open_hour_exp\"] = expected_return(new_df_ltc[\"open\"], 60)\nnew_df_btc[\"open_hour_exp\"] = expected_return(new_df_btc[\"open\"], 60)\nnew_df_eth[\"open_hour_exp\"] = expected_return(new_df_eth[\"open\"], 60)","87db9cf9":"plt.figure(figsize=(18, 9));\nplt.plot(new_df_ltc.index, new_df_ltc.open_hour_exp.values, label='ltc open')\nplt.plot(new_df_btc.index, new_df_btc.open_hour_exp.values, label='btc open')\nplt.plot(new_df_eth.index, new_df_eth.open_hour_exp.values, label='eth open')\nplt.title(\"expected percentage return\")\nplt.legend()\nplt.show\nnew_df_ltc.head()\n#delta_df_ltc.head()","5ff4aac0":"#new_df[\"open_hour_exp\"] = new_df[\"open\"].rolling(60 + 1).sum()\n#new_df[\"open_day_exp\"] = new_df[\"open\"].rolling(60*24 + 1).sum()\n#new_df[\"open_week_exp\"] = new_df[\"open\"].rolling(60*24*5 + 1).sum()\n##do the same for close\n#new_df[\"close_hour_exp\"] = new_df[\"close\"].rolling(60 + 1).sum()\n#new_df[\"close_day_exp\"] = new_df[\"close\"].rolling(60*24 + 1).sum()\n#new_df[\"close_week_exp\"] = new_df[\"close\"].rolling(60*24*5 + 1).sum()\n\n##do the same for high\n#new_df[\"high_hour_exp\"] = new_df[\"high\"].rolling(60 + 1).sum()\n#new_df[\"high_day_exp\"] = new_df[\"high\"].rolling(60*24 + 1).sum()\n#new_df[\"high_week_exp\"] = new_df[\"high\"].rolling(60*24*5 + 1).sum()\n\n##do the same for low\n#new_df[\"low_hour_exp\"] = new_df[\"low\"].rolling(60 + 1).sum()\n#new_df[\"low_day_exp\"] = new_df[\"low\"].rolling(60*24 + 1).sum()\n#new_df[\"low_week_exp\"] = new_df[\"low\"].rolling(60*24*5 + 1).sum()\n\n\n#new_df[[\"open_hour_exp\", \"open_day_exp\", \"open_week_exp\", \"open\"]].plot(title=\"expected cumulative return over periods\", figsize=(18,9))\n#print(60*24*5)","71384937":"def access_all_exp(name = \"open\"):\n    strings = [name + \"_hour_exp\", name + \"_day_exp\", name + \"_week_exp\", name]\n    return strings","9e74b2e0":"#new_df[access_all_exp(\"open\")].hist(bins = 500, figsize = (18,9))\n#print(new_df[access_all_exp(\"open\")].mean())","86f029d4":"#new_df[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]].hist(bins = 500, figsize = (18,9))\n#print(new_df[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]].mean())","0b0de089":"def normalise_df(dataframe1, dataframe2, scaler1=None, scaler2=None):\n    \n    scaler1.fit(dataframe1)\n    df1 = scaler1.transform(dataframe1)\n        \n    scaler2.fit(dataframe2)\n    df2 = scaler2.transform(dataframe2)\n    \n    return df1, df2, scaler1, scaler2\n\ndef normalise_df(dataframe, scaler):\n    scaler.fit(dataframe)\n    df1 = scaler.transform(dataframe)\n    \n    return df1, scaler","19ac5bf5":"vol_string = [\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]\npred_string = [\"open_hour_exp\"]\n#new_df_ltc[pred_string], new_df_ltc[vol_string], scaler1_ltc, scaler2_ltc = normalise_df(new_df_ltc[pred_string],\n#                                                                                              new_df_ltc[vol_string],\n#                                                                                              RobustScaler(),\n#                                                                                              MinMaxScaler())\n\n#new_df_btc[pred_string], new_df_btc[vol_string], scaler1_btc, scaler2_btc = normalise_df(new_df_btc[pred_string],\n#                                                                                              new_df_btc[vol_string],\n#                                                                                              RobustScaler(),\n#                                                                                              MinMaxScaler())\n\n#new_df_eth[pred_string], new_df_eth[vol_string], scaler1_eth, scaler2_eth = normalise_df(new_df_eth[pred_string],\n#                                                                                              new_df_eth[vol_string],\n#                                                                                              RobustScaler(),\n#                                                                                              MinMaxScaler())\n\nnew_df_ltc[vol_string], scaler_vol_ltc = normalise_df(new_df_ltc[vol_string], MinMaxScaler())\nnew_df_btc[vol_string], scaler_vol_btc = normalise_df(new_df_btc[vol_string], MinMaxScaler())\nnew_df_eth[vol_string], scaler_vol_eth = normalise_df(new_df_eth[vol_string], MinMaxScaler())\n\nnew_df_ltc.head()\nnew_df_ltc[vol_string].plot()\nnew_df_ltc[pred_string].plot()\nnew_df_btc[pred_string].plot()\nnew_df_eth[pred_string].plot()","2f71ccd0":"#new_df_norm = new_df.copy()\n#scaler = RobustScaler()\n#predict_strings = [   \"open_hour_exp\", \"open_day_exp\", \"open_week_exp\", \"open\",\n#                      \"close_hour_exp\", \"close_day_exp\", \"close_week_exp\", \"close\",\n#                      \"high_hour_exp\", \"high_day_exp\", \"high_week_exp\", \"high\",\n#                      \"low_hour_exp\", \"low_day_exp\", \"low_week_exp\", \"low\"]#\n#\n#predict_strings = [\"open_hour_exp\"]#\n#\n#scaler.fit(new_df_ltc_norm[[\"open_hour_exp\", \"open_day_exp\", \"open_week_exp\", \"open\"]])\n#scaler_vol = MinMaxScaler()\n#scaler_vol.fit(new_df_norm[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]])\n#new_df_norm[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]] = scaler_vol.transform(new_df_norm[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]])\n#scaler.fit(new_df_norm)\n#new_df_norm = scaler.transform(new_df_norm)\n#new_df_norm[[\"open_hour_exp\", \"open_day_exp\", \"open_week_exp\", \"open\"]] = scaler.transform(new_df_norm[[\"open_hour_exp\", \"open_day_exp\", \"open_week_exp\", \"open\"]])\n#new_df_norm[[\"open_hour_exp\", \"open_day_exp\", \"open_week_exp\", \"open\"]].plot()\n#new_df_norm[[\"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]].plot()\n#new_df_norm.head()","ada520f5":"def extract_sliding_windows(df, seq_len = 60, start = 0, end=None):\n    \n    array = []\n    if end == None: end = len(df)\n    #else: end = len(df) - end\n        \n    for i in tqdm(range(start,end-seq_len)):\n        array.append(df[i:(i+seq_len)].values)\n    return np.array(array)","c5a64d86":"variables = [\"open\", \"close\", \"high\", \"low\", \"volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]\ngc.collect()\ntraindata_X_ltc = extract_sliding_windows(new_df_ltc[variables], 30, 0, len(new_df_ltc)-30)\ntraindata_X_btc = extract_sliding_windows(new_df_btc[variables], 30, 0, len(new_df_btc)-30)\n#traindata_X_eth = extract_sliding_windows(new_df_eth[variables], 30, 0, len(new_df_eth)-30)\n#traindata_X = np.array(traindata_X)\n##too much ram usage\ntraindata_Y_ltc = new_df_ltc[\"open_hour_exp\"].iloc[60:].values\ntraindata_Y_btc = new_df_btc[\"open_hour_exp\"].iloc[60:].values\n#traindata_Y_eth = new_df_eth[\"open_hour_exp\"].iloc[60:].values\n\nprint(f\"X data length [{len(train_data_X)}], Y data length [{len(train_data_Y)}]\")","630c8b8f":"#print(type(train_data_Y))\n#print(type(train_data_X))\n#train_data_X = train_data_X[:len(train_data_X)-30]\n#print(len(train_data_Y), \", \", len(train_data_X))\n\n#tdy = torch.from_numpy(train_data_Y)\n#print(tdy)\n#print(type(tdy))","a6124e64":"#create data loaders\ntraindata_X_ltc, testdata_X_ltc, traindata_Y_ltc, testdata_Y_ltc = train_test_split(traindata_X_ltc, traindata_Y_ltc, test_size = 0.2, random_state = 42)\ntrain_set = TensorDataset(torch.from_numpy(traindata_X_ltc).type(torch.FloatTensor),\n                            torch.from_numpy(traindata_Y_ltc).type(torch.FloatTensor))\n\ntest_set = TensorDataset(torch.from_numpy(testdata_X_ltc).type(torch.FloatTensor),\n                            torch.from_numpy(testdata_Y_ltc).type(torch.FloatTensor))\ngc.collect()","ae1c33e1":"def split_index(total, percentage = 20):\n    idx = total - percentage * (total\/100)\n    return int(idx)","471e11bd":"#print(len(train_dataset))\n#idx = split_index(len(train_dataset), 20)\n#print(idx)\n\n#train_set = Subset(train_dataset, range(idx))\n#test_set = Subset(train_dataset, range(idx,len(train_dataset)))\n#print(\"------------\")\nprint(\"train dataset length \", len(train_set))\nprint(\"test dataset length \", len(test_set))","ac600349":"batch_size_train = 512\nbatch_size_test = 512\n\ntrain_dataloader = DataLoader(train_set, batch_size = batch_size_train, shuffle = True, pin_memory = True)\ntest_dataloader = DataLoader(test_set, batch_size = batch_size_test, shuffle = False, pin_memory = True)","42a74fb8":"class Linear_Model(nn.Module):\n    \n    def __init__(self, input_dim, seq_len, out_dim):\n        super(Linear_Model, self).__init__()\n        self.input_dim = input_dim\n        self.seq_len = seq_len\n        self.lin1 = nn.Linear(input_dim * seq_len, out_dim)\n        \n    def forward(self, x):\n        #print(\"x shape \", x.size())\n        x = x.view(self.input_dim * self.seq_len)\n        #print(\"x shape \", x.size())\n        return self.lin1(x) #we assume x is a vector aka. it has been unrolled","83a365a0":"class Relu_Model(nn.Module):\n    \n    def __init__(self, input_dim, seq_len, layer_dim, out_dim):\n        super(Relu_Model, self).__init__()\n        self.input_dim = input_dim\n        self.seq_len = seq_len\n        self.lin1 = nn.Linear(input_dim * seq_len, layer_dim)\n        self.lin2 = nn.Linear(layer_dim, out_dim)\n        \n    def forward(self, x):\n        #print(\"x shape \", x.size())\n        x = x.view(x.size(0),-1)\n        x = F.relu(self.lin1(x))\n        #print(\"x shape \", x.size())\n        return self.lin2(x) #we assume x is a vector aka. it has been unrolled","39536e1b":"class MultiLayer_Model(nn.Module):\n    \n    def __init__(self, input_dim, seq_len, layer_dim, out_dim):\n        super(MultiLayer_Model, self).__init__()\n        self.input_dim = input_dim\n        self.seq_len = seq_len\n        self.lin1 = nn.Linear(input_dim * seq_len, 3*layer_dim)\n        self.lin2 = nn.Linear(3*layer_dim, 2*layer_dim)\n        self.lin3 = nn.Linear(2*layer_dim, layer_dim)\n        self.lin4 = nn.Linear(layer_dim, out_dim)\n        \n    def forward(self, x):\n        #print(\"x shape \", x.size())\n        x = x.view(x.size(0),-1)\n        x = F.relu(self.lin1(x))\n        x = F.relu(self.lin2(x))\n        x = F.relu(self.lin3(x))\n        x = self.lin4(x)\n        #print(\"x shape \", x.size())\n        return torch.tanh(x) #we assume x is a vector aka. it has been unrolled","92f0d276":"class Dropout_Model(nn.Module):\n    \n    def __init__(self, input_dim, seq_len, layer_dim, out_dim):\n        super(Dropout_Model, self).__init__()\n        self.input_dim = input_dim\n        self.seq_len = seq_len\n        self.lin1 = nn.Linear(input_dim * seq_len, 3*layer_dim)\n        self.lin2 = nn.Linear(3*layer_dim, layer_dim)\n        self.lin3 = nn.Linear(layer_dim, out_dim)\n        \n        self.drop1 = nn.Dropout(p=0.5)\n        self.drop2 = nn.Dropout(p=0.75)\n        \n    def forward(self, x):\n        #print(\"x shape \", x.size())\n        x = x.view(x.size(0),-1)\n        x = F.relu(self.lin1(x))\n        x = self.drop1(x)\n        x = F.relu(self.lin2(x))\n        x = self.drop2(x)\n        x = self.lin3(x)\n        \n        return torch.tanh(x) #we assume x is a vector aka. it has been unrolled","be66969f":"class SignLoss(nn.Module):\n    \n    def __init__(self):\n        super(SignLoss, self).__init__()\n    \n    def forward(self, outputs, targets):\n        outputs = outputs.view(outputs.size(0))\n        #print(\"________________\")\n        #print(targets.view(-1))\n        #print(outputs.view(-1))\n        val = torch.abs(torch.sign(targets-outputs))\n        #print(val.view(-1))\n        #print(targets.size())\n        #print(outputs.size())\n        #print(val.size())\n        #print(\"________________\")\n        return torch.sum(val)","7a6274f2":"def accuracy(model, dataloader, batch_size):\n    model = model.eval()\n    sgn_loss = SignLoss()\n    \n    n = len(dataloader)\n    loss_total = 0\n    for i, (x,y) in enumerate(dataloader):\n        \n        pred_y = model(x)\n        \n        y = torch.sign(y)\n        pred_y = torch.sign(pred_y)\n            \n        loss = sgn_loss(pred_y, y) \/ batch_size\n        \n        if i<(n-1):\n            print(f\"iter [{i} \/ {n}]\", end = \"\\r\")\n        else:\n            print(f\"iter [{i} \/ {n}]\")\n                     \n        loss_total += sgn_loss(pred_y, y) \/ batch_size\n        \n    return (1 - (loss_total\/len(dataloader))) * 100\n        ","94ed1034":"def fit(model, dataloader, test_loader, test_batch_size, epochs = 10, valid_epoch=10):\n    #train the model for epochs\n    losses = []\n    accuracies = []\n    n = len(dataloader)\n    #tqdm = tqdm(total = epochs * len(dataloader))\n    for epoch in range(epochs):\n        m=1\n        avg_loss = 0\n        for i, (x,y) in enumerate(dataloader):\n            optimizer.zero_grad()               \n            pred_y = model(x)\n            #print(\"pred y size \", pred_y.size())\n            #print(\"y size \", y.size())\n            #print(pred_y.size())\n            #pred_y = pred_y.view(-1)\n            #print(\"prediction size \", pred_y.size())\n            #print(\"real size \", y.size())\n            #print(\"prediction \", pred_y)\n            #print(\"real \", y)\n            y = torch.sign(y).view(y.size(0), 1)\n            #print(y.size())\n            #print(pred_y.size())\n            #print(\"###############\")\n            loss = error(pred_y, y)\n            #print(\"loss value = \", loss.data)\n            loss.backward()\n            #print(\"--------------------\")\n            \n            #print(f\"pre optimizer, {i} \/ {len(dataloader)}\")\n            optimizer.step()\n            #print(\"loss \", loss)            \n            #print(f\"past optimizer {i} \/ {len(dataloader)}\")\n            avg_loss += loss.cpu().data\n            m+= 1\n            if i<(n-1):\n                print(f\"iter [{i+1} \/ {n}]; avg_loss [{avg_loss\/m}]\", end = \"\\r\")\n            else:\n                print(f\"iter [{i+1} \/ {n}]\")\n            \n            \n            #print(f\"past append {i} \/ {len(dataloader)}\")\n        #print(\"pre losses\")\n        losses.append(avg_loss\/n)\n        #print(\"past losses\")\n        print(f\"epochs = [{epoch+1} \/ {epochs}], losses = [{losses[-1]}]\")\n        \n        if (epoch+1) % valid_epoch == 0:\n            accuracies.append(accuracy(model, test_dataloader, test_batch_size))\n            print(f\"Accuracy at epoch [{epoch+1}] based on sign [{accuracies[-1]}%]\")\n            model = model.train()\n    \n    return losses, accuracies\n    ","e3dc4f52":"#we will use the mean squared error for accuracy\n#length of inputs will be 8*60\nmodel_lin = Linear_Model(len(variables), 30, 1) #input everything and try to predict expected daily return\nmodel_2 = Relu_Model(len(variables), 30, 30, 1)\nmodel_3 = MultiLayer_Model(len(variables), 30, 30, 1)\nmodel_4 = Dropout_Model(len(variables), 30, 30, 1)\n\nprint(model_2)\nprint(model_3)\nprint(model_4)","e65b8af6":"lr = 0.0002\n\nerror = nn.MSELoss()\nmodel_3.train()\noptimizer = optim.Adam(model_3.parameters(), lr=lr, weight_decay = 1e-4, amsgrad = True)\n\nlosses, accuracies = fit(model_3, train_dataloader, test_dataloader, batch_size_test, 100, 2)","3f171b9c":"acc = accuracy(model_2, test_dataloader, batch_size_test)\nprint(f\"accuracy is {acc}%\")\n\n#print(losses)\nfig, ax = plt.subplots(1,1, figsize = (18,9))\nax.plot(losses, label = \"training loss\")\n#plt.ylim([4.4,4.6])\nplt.title(\"losses throughout training\")\nplt.legend()\nplt.show()","20643e52":"# Start","5402b042":"# Normalisation","3f685159":"## Simple Linear","bf80ebc3":"# Data loading","fa7913f9":"# Model","5bc32b1b":"create a deep neural network with a fixed window input\n\nwe will feed in the deltas into the network and the output will be to classify if the price increased or decreased after the last entry","77d334c2":"### validation functions","56871202":"30 day moving average and exponential moving average","17df810e":"## Create training data","3e6806a1":"## Nonlinear Relu","eb00ea38":"# Training","034932bd":"# Plotting","ca054735":"# Distributions","2a329f80":"first we need to create the training data and the classification","2027544c":"# Data formatting\/preprocessing","15a0ef03":"# Plotting","5e300551":"### training function","ebf6ef31":"We can see that we have over one million entries with no null values.","46c2587b":"## Accuracy check\n\nWe will check the accuracy with a binary check for positivity or negativity. If there is a sign mismatch that's a loss","96357795":"## Better Model"}}