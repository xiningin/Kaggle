{"cell_type":{"fe4d6887":"code","422dd1b1":"code","ead80221":"code","3eba7278":"code","de2f7e45":"code","95321b00":"code","dc5fb168":"code","451f727f":"code","469d9262":"code","acf3ba8d":"code","3114aec0":"code","e2c88dbb":"code","5ac5ed1f":"code","52e344f5":"code","89460fa0":"code","a58c1f1d":"code","b25be575":"code","385b5d3a":"code","aefa51f4":"code","2a522c6d":"code","daccace1":"markdown","013532a1":"markdown","b642325f":"markdown","7bcc4f9d":"markdown","14495d4a":"markdown"},"source":{"fe4d6887":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","422dd1b1":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ead80221":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nnRowsRead = None # \u010cita sve redove, mogu\u0107e je pro\u010ditati samo odre\u0111eni broj\n\ndf1 = pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\ndf2=pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\ndf3=pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\ndf4=pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Monday-WorkingHours.pcap_ISCX.csv\")\ndf5=pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\ndf6=pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\ndf7=pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Tuesday-WorkingHours.pcap_ISCX.csv\")\ndf8=pd.read_csv(\"\/kaggle\/input\/MachineLearningCSV\/MachineLearningCVE\/Wednesday-workingHours.pcap_ISCX.csv\")\n\n\ndf = pd.concat([df1,df2])\ndel df1,df2\ndf = pd.concat([df,df3])\ndel df3\ndf = pd.concat([df,df4])\ndel df4\ndf = pd.concat([df,df5])\ndel df5\ndf = pd.concat([df,df6])\ndel df6\ndf = pd.concat([df,df7])\ndel df7\ndf = pd.concat([df,df8])\ndel df8\n\nnRow, nCol = df.shape\nprint(f'U tabeli ima {nRow} redova i {nCol} kolona')","3eba7278":"df.head()","de2f7e45":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","95321b00":"plotPerColumnDistribution(df, 79, 5)","dc5fb168":"#Split dataset on train and test\nfrom sklearn.model_selection import train_test_split\ntrain, test=train_test_split(df,test_size=0.3, random_state=10)\n\n#Exploratory Analysis\n# Descriptive statistics\ntrain.describe()\ntest.describe()\n\n# Packet Attack Distribution\ntrain[' Label'].value_counts()\ntest[' Label'].value_counts()","451f727f":"#Scalling numerical attributes\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = train.select_dtypes(include=['float64','int64']).columns\nsc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))\nsc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))\n\n# turn the result back to a dataframe\nsc_traindf = pd.DataFrame(sc_train, columns = cols)\nsc_testdf = pd.DataFrame(sc_test, columns = cols)","469d9262":"# importing one hot encoder from sklearn \nfrom sklearn.preprocessing import OneHotEncoder \n\n# creating one hot encoder object \nonehotencoder = OneHotEncoder() \n\ntrainDep = train[' Label'].values.reshape(-1,1)\ntrainDep = onehotencoder.fit_transform(trainDep).toarray()\ntestDep = test[' Label'].values.reshape(-1,1)\ntestDep = onehotencoder.fit_transform(testDep).toarray()","acf3ba8d":"train_X=sc_traindf\ntrain_y=trainDep[:,0]\n\ntest_X=sc_testdf\ntest_y=testDep[:,0]","3114aec0":"#Feature Selection\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier();\n\n# fit random forest classifier on the training set\nrfc.fit(train_X, train_y);\n\n# extract important features\nscore = np.round(rfc.feature_importances_,3)\nimportances = pd.DataFrame({'feature':train_X.columns,'importance':score})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\n\n# plot importances\nplt.rcParams['figure.figsize'] = (11, 4)\nimportances.plot.bar();","e2c88dbb":"#Recursive feature elimination\nfrom sklearn.feature_selection import RFE\nimport itertools\n\nrfc = RandomForestClassifier()\n\n# create the RFE model and select 20 attributes\nrfe = RFE(rfc, n_features_to_select=20)\nrfe = rfe.fit(train_X, train_y)\n\n# summarize the selection of the attributes\nfeature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), train_X.columns)]\nselected_features = [v for i, v in feature_map if i==True]\n\nselected_features\n\na = [i[0] for i in feature_map]\ntrain_X = train_X.iloc[:,a]\ntest_X = test_X.iloc[:,a]","5ac5ed1f":"#Dataset Partition\nX_train,X_test,Y_train,Y_test = train_test_split(train_X,train_y,train_size=0.70, random_state=2)\n\n#Fitting Models\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Train KNeighborsClassifier Model\nKNN_Classifier = KNeighborsClassifier(n_jobs=-1)\nKNN_Classifier.fit(X_train, Y_train); \n\n# Train LogisticRegression Model\nLGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\nLGR_Classifier.fit(X_train, Y_train);\n\n# Train Gaussian Naive Baye Model\nBNB_Classifier = BernoulliNB()\nBNB_Classifier.fit(X_train, Y_train)\n\n# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTC_Classifier.fit(X_train, Y_train)","52e344f5":"#Evaluate Models\nfrom sklearn import metrics\n\nmodels = []\nmodels.append(('Naive Baye Classifier', BNB_Classifier))\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('KNeighborsClassifier', KNN_Classifier))\nmodels.append(('LogisticRegression', LGR_Classifier))\n\nfor i, v in models:\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, v.predict(X_train))\n    confusion_matrix = metrics.confusion_matrix(Y_train, v.predict(X_train))\n    classification = metrics.classification_report(Y_train, v.predict(X_train))\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","89460fa0":"#Validate Models\nfor i, v in models:\n    accuracy = metrics.accuracy_score(Y_test, v.predict(X_test))\n    confusion_matrix = metrics.confusion_matrix(Y_test, v.predict(X_test))\n    classification = metrics.classification_report(Y_test, v.predict(X_test))\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()        ","a58c1f1d":"# PREDICTING FOR TEST DATA\npred_knn = KNN_Classifier.predict(test_X)\npred_NB = BNB_Classifier.predict(test_X)\npred_log = LGR_Classifier.predict(test_X)\npred_dt = DTC_Classifier.predict(test_X)\n","b25be575":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom keras.layers import Input,Dropout,Dense\nfrom keras.models import Model\nfrom keras import regularizers\nfrom keras.utils.data_utils import get_file\n%matplotlib inline","385b5d3a":"#Buildling and training the model\n\ndef getModel():\n    inp = Input(shape=(X_train.shape[1],))\n    d1=Dropout(0.3)(inp)\n    encoded = Dense(8, activation='relu', activity_regularizer=regularizers.l1(10e-5))(d1)\n    decoded = Dense(X_train.shape[1], activation='relu')(encoded)\n    autoencoder = Model(inp, decoded)\n    autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n    return autoencoder\n\nautoencoder=getModel()\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\nhistory=autoencoder.fit(X_train, Y_train,\n               epochs=32,\n                batch_size=150,\n                shuffle=True,\n                validation_split=0.1, callbacks=[callback]\n                       )","aefa51f4":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training','validation'])\nplt.title('Training and validation loss')\nplt.xlabel('epoch')","2a522c6d":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training','validation'])\nplt.title('Training and validation accuracy')\nplt.xlabel('epoch')","daccace1":"# DataSet\n","013532a1":"# Classification","b642325f":"# Functions for grapfs","7bcc4f9d":"Data preprocess","14495d4a":"# Libraries\n"}}