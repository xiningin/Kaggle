{"cell_type":{"88fbfb49":"code","0ee8441a":"code","a1c04683":"code","60e99d3d":"code","3d81db42":"code","96dec161":"code","05ed893c":"code","a64063f0":"code","f6877406":"code","ce51ad5f":"code","47050723":"code","8228318f":"code","ac521661":"code","b4474446":"code","f3be07fd":"code","bca442f4":"code","8d2580f0":"code","792b78b4":"code","687073e5":"code","835217bd":"code","b6f54b8b":"code","c61dbfa1":"code","f44e22ba":"code","e201ac4a":"code","fd5cea2b":"code","04ce3d44":"code","872a14e2":"code","b2d7db16":"code","9bf41a7c":"code","c24dbd0d":"code","b3055a46":"code","70165e96":"code","69a8eec9":"code","a5548a47":"markdown","b46d52e1":"markdown","7a8fc908":"markdown","3fd2feb9":"markdown","65b46b41":"markdown","c228eae4":"markdown","cacfde2e":"markdown","46630114":"markdown","2400a0c1":"markdown"},"source":{"88fbfb49":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\n\nfrom sklearn.model_selection import GroupKFold\nfrom typing import Any\nfrom numba import jit\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom itertools import product\nimport copy\nimport time\n\nimport random\nseed = 1234\nrandom.seed(seed)\nnp.random.seed(seed)","0ee8441a":"train = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\ntrain_labels = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\ntest = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\nspecs = pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\nsample_submission = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","a1c04683":"train.head()","60e99d3d":"train.shape","3d81db42":"train.columns","96dec161":"# Select all Assessment from installation_id\nkeep_id = train[train[\"type\"] == \"Assessment\"][\"installation_id\"].drop_duplicates()\n\ntrain = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")","05ed893c":"train.shape","a64063f0":"keep_id.shape","f6877406":"train.type.value_counts()","ce51ad5f":"fig = plt.figure(figsize=(12, 10))\n\nax1 = fig.add_subplot(211)\nax1 = sns.countplot(y=\"type\", data=train, order= train.type.value_counts().index)\nplt.title(\"number of events by type\")\n\nax2 = fig.add_subplot(212)\nax2 = sns.countplot(y=\"world\", data=train, order = train.world.value_counts().index)\nplt.title(\"number of events by world\")\nplt.tight_layout(pad=0)","47050723":"fig = plt.figure(figsize=(12,10))\n\ntitle_count = train.title.value_counts().sort_values(ascending=True)\ntitle_count.plot.barh()\nplt.title(\"Event counts by title\")","8228318f":"train['timestamp'] = pd.to_datetime(train['timestamp'])\ntrain['date'] = train['timestamp'].dt.date\ntrain['month'] = train['timestamp'].dt.month\ntrain['hour'] = train['timestamp'].dt.hour\ntrain['dayofweek'] = train['timestamp'].dt.dayofweek","ac521661":"fig = plt.figure(figsize=(12,10))\ndate = train.groupby('date')['date'].count()\ndate.plot()\nplt.xticks(rotation=90)\nplt.title(\"Event counts by date\")","b4474446":"fig = plt.figure(figsize=(12,10))\nday_of_week = train.groupby('dayofweek')['dayofweek'].count()\n# convert num -> category\nday_of_week.index = list(calendar.day_abbr)\nday_of_week.plot.bar()\nplt.title(\"Event counts by day of week\")\nplt.xticks(rotation=0)","f3be07fd":"fig = plt.figure(figsize=(12,10))\nhour = train.groupby('hour')['hour'].count()\nhour.plot.bar()\nplt.title(\"Event counts by hour of day\")\nplt.xticks(rotation=0)","bca442f4":"test.head()","8d2580f0":"test.shape","792b78b4":"test.installation_id.nunique()","687073e5":"sample_submission.shape","835217bd":"set(list(train.installation_id.unique())).intersection(list(test.installation_id.unique()))","b6f54b8b":"test['timestamp'] = pd.to_datetime(test['timestamp'])\n\nprint(f'The range of date in train is: {train.date.min()} to {train.date.max()}')\nprint(f'The range of date in test is: {test.timestamp.dt.date.min()} to {test.timestamp.dt.date.max()}')","c61dbfa1":"train_labels.head()","f44e22ba":"train_labels.shape","e201ac4a":"pd.crosstab(train_labels.title, train_labels.accuracy_group)","fd5cea2b":"plt.figure(figsize=(12,6))\nsns.countplot(y=\"title\", data=train_labels, order = train_labels.title.value_counts().index)\nplt.title(\"Counts of titles\")","04ce3d44":"df = train_labels.groupby(['accuracy_group', 'title'])['accuracy_group']\ndf.count()","872a14e2":"se = train_labels.groupby(['title', 'accuracy_group'])['accuracy_group'].count().unstack('title')\nse.plot.bar(stacked=True, rot=0, figsize=(12,10))\nplt.title(\"Counts of accuracy group\")","b2d7db16":"train[~train.installation_id.isin(train_labels.installation_id.unique())].installation_id.nunique()","9bf41a7c":"train = train[train.installation_id.isin(train_labels.installation_id.unique())]\ntrain.shape","c24dbd0d":"print(f'No. of rows in train_labels: {train_labels.shape[0]}')\nprint(f'Number of unique game_sessions in train_labels: {train_labels.game_session.nunique()}')","b3055a46":"train = train.drop(['date', 'month', 'hour', 'dayofweek'], axis=1)","70165e96":"def encode_title(train, test, train_labels):\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    \n    all_title_event_code = list(set(train['title_event_code'].unique()).union(test['title_event_code'].unique()))\n    \n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    \n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    \n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    \n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    \n    activities_map = dict(zip(list_of_user_activities, np,arange(len(list_of_user_activities))))\n    \n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    \n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    \n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    \n    # replace title with its number from the dictionary\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    \n    train_labels['title'] = train_labels['title'].map(activities_map)\n    \n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    \n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    return train, test, train_labels, all_title_event_code, list_of_user_activities, list_of_event_code, list_of_event_id, activities_labels, assess_titles\n    ","69a8eec9":"train, test, train_labels, all_title_event_code, list_of_user_activities, list_of_event_code, list_of_event_id, activities_labels, assess_titles = encode_title(train, test, train_labels)","a5548a47":"accuracy_group =>\n* 0: the assessment was never solved\n* 1: the assessment was solved after 3 or more attempts\n* 2: the assessment was solved on the second attempt\n* 3: the assessment was solved on the first attempt","b46d52e1":"Feature engineering","7a8fc908":"train_labels dataset","3fd2feb9":"installation_id's who did assessments but without results in the train_labels (we have already taken out the ones who never took one)","65b46b41":"Check if there is any overlap with regards to installation_id's in the train and test set","c228eae4":"There are no installation_ids without assessment in the test set","cacfde2e":"Cannot train on installation_id's anyway, so taking them out of the train set. This reduces train set from 8.3 million rows to 7.7 million.","46630114":"'''Kaggle provided the following note: Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.'''\n\nDeleting 'installation_id' which are not \"Assessment\"","2400a0c1":"There are no installation_id's that appear in both train and test"}}