{"cell_type":{"e139e863":"code","981bfb28":"code","3994ad22":"code","b11c3f09":"code","71772bbc":"code","51429faf":"code","d4a8f6c5":"code","a6b4e9b3":"code","d93ea163":"code","08ab49fc":"code","06ca6814":"code","6c2c1992":"code","606e88fe":"code","78184358":"code","a5b08339":"code","d8180908":"code","46ccb902":"code","cbacb43e":"code","f324459b":"code","fa618c96":"code","337f701a":"code","cf684706":"code","7180f133":"code","6b2848fd":"code","9da73ab3":"code","6e15f20e":"code","9dd1e0b7":"code","bed38e7b":"code","73ada482":"code","de5b1dc7":"code","76ab4400":"code","90bb7f5b":"code","bc375229":"code","d2840092":"code","c39e7f15":"code","98e02545":"code","ce818d72":"code","673efcb0":"code","1f3a6365":"code","6e4d4c1b":"code","cf2b01b2":"code","d8cd341c":"code","4b70e5fd":"code","1424f41c":"code","f5903746":"code","971f0ed6":"code","6a6fbaec":"code","728d824f":"code","d3e63e90":"code","91b0674d":"code","28abac98":"code","8f93467c":"code","e5c30f22":"code","3d176aa2":"code","653c2c2b":"code","ba292d96":"code","6c305721":"code","4d776c2d":"code","af1b676a":"code","dff6ef24":"code","40f7ff62":"code","b9cde211":"code","5aa877d7":"code","0a54b615":"code","cc8adde0":"code","a43437e0":"code","1234b688":"code","bffc9245":"code","194eeebc":"code","d9c4d5a4":"code","183aa69e":"code","add2c6fc":"code","a9d79400":"code","478c07a1":"code","90bd3f31":"code","6701525e":"code","67616697":"code","18867626":"code","879ead89":"code","81a213cb":"code","d83c112a":"code","c9d878f7":"code","04d37f22":"code","3127adc1":"code","04c8ccae":"code","99adadd6":"code","cc211747":"markdown","f9523555":"markdown","f581de0f":"markdown","08efcba8":"markdown","ff7bf206":"markdown","3f1f5c10":"markdown","f9a0b8b7":"markdown","3f325314":"markdown","04b09064":"markdown","b3cf02b8":"markdown","8130445a":"markdown","ddb1dfbd":"markdown","9e77839d":"markdown","56fed4cc":"markdown","69bc7b86":"markdown","a14ea725":"markdown","e0612f9f":"markdown","e77505d2":"markdown","d9a64a7d":"markdown","ff5f853d":"markdown","e502079a":"markdown","52411d68":"markdown","b9530bfc":"markdown","dabf43fc":"markdown","be55c2f8":"markdown","4424a655":"markdown","3f439c32":"markdown","e1a341cb":"markdown","48ce014b":"markdown","5fe312e9":"markdown","ac21c1d0":"markdown","41226131":"markdown","71e3667d":"markdown","6c409735":"markdown","af48287a":"markdown","a05f7310":"markdown","0862e2f9":"markdown","6855cb82":"markdown","90fe6231":"markdown","8d4eab5f":"markdown","0c391d73":"markdown","3008e7bc":"markdown","5b564602":"markdown","65aff2a7":"markdown","e779bbe9":"markdown","47db750a":"markdown","1ac66855":"markdown","71166b20":"markdown","8e35ce19":"markdown","5ec67b7b":"markdown","6a544e27":"markdown","0a0eb3b4":"markdown","e5b113f1":"markdown","23f18982":"markdown","55c8a5d9":"markdown","4dae634f":"markdown","0910f4e7":"markdown","1654a4c0":"markdown","00854597":"markdown","20380c36":"markdown","db73a92c":"markdown"},"source":{"e139e863":"# Load the standard Python data science packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Load the LinearRegression class from scikit-learn's linear_model module\nfrom sklearn.linear_model import LinearRegression\n\n# Load the stats module from scipy so we can code the functions to compute model statistics\nfrom scipy import stats\n\n# Load StatsModels API\n# Note that if we wish to use R-style formulas, then we would load statsmodels.formula.api instead\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf","981bfb28":"# Load the corrected Boston housing data set\n# Create a multi-index on TOWN and TRACT columns\n# Exclude the TOWNNO, LON, LAT, and MEDV columns when loading the set\nboston_filepath = \"..\/input\/corrected-boston-housing\/boston_corrected.csv\"\nboston = pd.read_csv(boston_filepath, index_col = [\"TOWN\", \"TRACT\"], \n                     usecols = [\"TOWN\", \"TRACT\", \"CMEDV\", \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"])","3994ad22":"boston.head()","b11c3f09":"boston.isnull().any()","71772bbc":"# Create a LinearRegression object\nreg = LinearRegression()\n# Need to extract the two columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\nX = boston[\"LSTAT\"].values.reshape(-1, 1)\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\nreg.fit(X, y);","51429faf":"print(\"Model coefficients:\", reg.coef_)\nprint(\"Model intercept:\", reg.intercept_)","d4a8f6c5":"# In the score function, X is an array of the test samples\n# It needs to have shape = (num_samples, num_features)\n# y is an array of the true values for the given X\nreg.score(X, y)","a6b4e9b3":"def detailed_linear_regression(X, y):\n    \"\"\"\n    Assume X is array-like with shape (num_samples, num_features)\n    Assume y is array-like with shape (num_samples, num_targets)\n    Computes the least-squares regression model and returns a dictionary consisting of\n    the fitted linear regression object; a series with the residual standard error,\n    R^2 value, and the overall F-statistic with corresponding p-value; and a dataframe\n    with columns for the parameters, and their corresponding standard errors,\n    t-statistics, and p-values.\n    \"\"\"\n    # Create a linear regression object and fit it using x and y\n    reg = LinearRegression()\n    reg.fit(X, y)\n    \n    # Store the parameters (regression intercept and coefficients) and predictions\n    params = np.append(reg.intercept_, reg.coef_)\n    predictions = reg.predict(X)\n    \n    # Create matrix with shape (num_samples, num_features + 1)\n    # Where the first column is all 1s and then there is one column for the values\n    # of each feature\/predictor\n    X_mat = np.append(np.ones((X.shape[0], 1)), X, axis = 1)\n    \n    # Compute residual sum of squares\n    RSS = np.sum((y - predictions)**2)\n    \n    # Compute total sum of squares\n    TSS = np.sum((np.mean(y) - y)**2)\n    \n    # Estimate the variance of the y-values\n    obs_var = RSS\/(X_mat.shape[0] - X_mat.shape[1])\n    \n    # Residual standard error is square root of variance of y-values\n    RSE = obs_var**0.5\n    \n    # Variances of the parameter estimates are on the diagonal of the \n    # variance-covariance matrix of the parameter estimates\n    var_beta = obs_var*(np.linalg.inv(np.matmul(X_mat.T, X_mat)).diagonal())\n    \n    # Standard error is square root of variance\n    se_beta = np.sqrt(var_beta)\n    \n    # t-statistic for beta_i is beta_i\/se_i, \n    # where se_i is the standard error for beta_i\n    t_stats_beta = params\/se_beta\n    \n    # Compute p-values for each parameter using a t-distribution with\n    # (num_samples - 1) degrees of freedom\n    beta_p_values = [2 * (1 - stats.t.cdf(np.abs(t_i), X_mat.shape[0] - 1))\n                    for t_i in t_stats_beta]\n    \n    # Compute value of overall F-statistic, to measure how likely our\n    # coefficient estimate are, assuming there is no relationship between\n    # the predictors and the response\n    F_overall = ((TSS - RSS)\/(X_mat.shape[1] - 1))\/(RSS\/(X_mat.shape[0] - X_mat.shape[1]))\n    F_p_value = stats.f.sf(F_overall, X_mat.shape[1] - 1, X_mat.shape[0] - X_mat.shape[1])\n    \n    # Construct dataframe for the overall model statistics:\n    # RSE, R^2, F-statistic, p-value for F-statistic\n    oa_model_stats = pd.Series({\"Residual standard error\": RSE, \"R-squared\": reg.score(X, y),\n                                \"F-statistic\": F_overall, \"F-test p-value\": F_p_value})\n    \n    # Construct dataframe for parameter statistics:\n    # coefficients, standard errors, t-statistic, p-values for t-statistics\n    param_stats = pd.DataFrame({\"Coefficient\": params, \"Standard Error\": se_beta,\n                                \"t-value\": t_stats_beta, \"Prob(>|t|)\": beta_p_values})\n    return {\"model\": reg, \"param_stats\": param_stats, \"oa_stats\": oa_model_stats}","d93ea163":"detailed_reg = detailed_linear_regression(X, y)","08ab49fc":"np.round(detailed_reg[\"param_stats\"], 4)","06ca6814":"np.round(detailed_reg[\"oa_stats\"], 4)","6c2c1992":"# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(boston[\"LSTAT\"])\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod = sm.OLS(endog, exog)\n\n# Fit the model\nres = mod.fit()\n\n#Print out model summary\nprint(res.summary())","606e88fe":"def param_conf_int(X, y, level = 0.95):\n    \"\"\"\n    Assume X is array-like with shape (num_samples, num_features)\n    Assume y is array-like with shape (num_samples, num_targets)\n    Assume level, if given, is a float with 0 < level < 1\n    Computes confidence intervals at the given confidence level for each parameter\n    in the linear regression model relating the predictors X to the response y\n    Returns a dataframe with the endpoints of the confidence interval for each parameter\n    \"\"\"\n    # Store parameters and corresponding stats for easy access\n    detailed_reg = detailed_linear_regression(X, y)\n    param_stats = detailed_reg[\"param_stats\"]\n    conf_intervals = pd.DataFrame()\n    # Degrees of freedom = num_samples - (num_features + 1)\n    df = X.shape[0] - (X.shape[1] + 1)\n    a, b = str(round((1 - level)*100\/2, 2)) + \"%\", str(round((1 + level)*100\/2, 2)) + \"%\"\n    # Loop through each parameter\n    for index in param_stats.index:\n        coeff = param_stats.loc[index, \"Coefficient\"]\n        std_err = param_stats.loc[index, \"Standard Error\"]\n        # alpha = level of confidence\n        # df = degrees of freedom = num_samples - number of parameters\n        # loc = center of t-interval = estimated coefficient value\n        # scale = standard error in coefficient estimate\n        conf_intervals = conf_intervals.append(pd.DataFrame([stats.t.interval(level, df, loc = coeff, scale = std_err)],\n                                                            columns = [a, b]), ignore_index = True)\n    return conf_intervals","78184358":"param_conf_int(X, y)","a5b08339":"res.conf_int()","d8180908":"param_conf_int(X, y, level = 0.99)","46ccb902":"res.conf_int(alpha = 0.01)","cbacb43e":"def predict_intervals(X, y, X_star, level = 0.95, kind = \"confidence\"):\n    \"\"\"\n    Assume X is array-like with shape (num_samples, num_features)\n    Assume y is array-like with shape (num_samples, num_targets)\n    Assume X_star is array-like with shape (num_predictions, num_features) with x-values for which we want predictions\n    Assume level, if given, is a float with 0 < level < 1\n    Assume kind, if given is either the string \"confidence\" or \"prediction\" for the kind of interval\n    Computes confidence intervals at the given confidence level for each parameter\n    in the linear regression model relating the predictors X to the response y\n    Returns a dataframe with the endpoints of the confidence interval for each parameter\n    \"\"\"\n    # Store parameters and corresponding stats for easy access\n    detailed_reg = detailed_linear_regression(X, y)\n    predictions = detailed_reg[\"model\"].predict(X_star)\n    RSE = detailed_reg[\"oa_stats\"][\"Residual standard error\"]\n    intervals = pd.DataFrame()\n    # Degrees of freedom = num_samples - (num_features + 1)\n    df = X.shape[0] - (X.shape[1] + 1)\n    a, b = str(round((1 - level)*100\/2, 2)) + \"%\", str(round((1 + level)*100\/2, 2)) + \"%\"\n    x_bar = X.mean()\n    x_tss = np.sum((X - x_bar)**2)\n    # Loop through each x-value being used for prediction\n    for i in range(len(predictions)) :\n        prediction = predictions[i, 0]\n        x_star = X_star[i, 0]\n        conf_error = RSE * (1\/X.shape[0] + (x_star - x_bar)**2\/x_tss)**0.5\n        predict_error = (RSE**2 + conf_error**2)**0.5\n        # alpha = level of confidence\n        # df = degrees of freedom = num_samples - number of parameters\n        # loc = center of t-interval = predicted value from linear regression model\n        # scale = standard error in predicted value estimate\n        if (kind == \"confidence\"):\n            lower, upper = stats.t.interval(level, df, loc = prediction, scale = conf_error)\n            intervals = intervals.append(pd.Series({\"prediction\": prediction, a: lower, b: upper}),\n                                         ignore_index = True)\n        elif (kind == \"prediction\"):\n            lower, upper = stats.t.interval(level, df, loc = prediction, scale = predict_error)\n            intervals = intervals.append(pd.Series({\"prediction\": prediction, a: lower, b: upper}),\n                                         ignore_index = True)\n    return intervals","f324459b":"predict_intervals(X, y, np.array([5, 10, 15]).reshape((-1, 1)), level = 0.95, kind = \"confidence\")","fa618c96":"predict_intervals(X, y, np.array([5, 10, 15]).reshape((-1, 1)), level = 0.99, kind = \"confidence\")","337f701a":"predict_intervals(X, y, np.array([5, 10, 15]).reshape((-1, 1)), level = 0.95, kind = \"prediction\")","cf684706":"predict_intervals(X, y, np.array([5, 10, 15]).reshape((-1, 1)), level = 0.99, kind = \"prediction\")","7180f133":"reg_predictions = res.get_prediction(np.array([[1, 5], [1, 10], [1, 15]]))","6b2848fd":"pd.DataFrame(reg_predictions.conf_int(alpha = 0.05), columns = [\"2.5%\", \"97.5%\"])","9da73ab3":"# Produce 99% prediction intervals for the predicted values of CMEDV\npd.DataFrame(reg_predictions.conf_int(obs = True, alpha = 0.01), columns = [\"0.5%\", \"99.5%\"])","6e15f20e":"class ExtendedLinearRegression(LinearRegression):\n    \n    def detailed_linear_regression(self, X, y):\n        \"\"\"\n        Assume X is array-like with shape (num_samples, num_features)\n        Assume y is array-like with shape (num_samples, num_targets)\n        include_intercept is a boolean where True means X does not already have a column\n        for the intercept\n        Computes the least-squares regression model and returns a dictionary consisting of\n        the fitted linear regression object; a series with the residual standard error,\n        R^2 value, and the overall F-statistic with corresponding p-value; and a dataframe\n        with columns for the parameters, and their corresponding standard errors,\n        t-statistics, and p-values.\n        \"\"\"\n        # Create a linear regression object and fit it using x and y\n        self.training_X, self.training_y = X, y\n        self.fit(X, y)\n    \n        # Store the parameters (regression intercept and coefficients) and predictions\n        self.params = np.append(self.intercept_, self.coef_)\n        predictions = self.predict(X)\n    \n        # Create matrix with shape (num_samples, num_features + 1)\n        # Where the first column is all 1s and then there is one column for the values\n        # of each feature\/predictor\n        X_mat = np.append(np.ones((X.shape[0], 1)), X, axis = 1)\n    \n        # Compute residual sum of squares\n        self.RSS = np.sum((y - predictions)**2)\n    \n        # Compute total sum of squares\n        self.TSS = np.sum((np.mean(y) - y)**2)\n    \n        # Estimate the variance of the y-values\n        obs_var = self.RSS\/(X_mat.shape[0] - X_mat.shape[1])\n    \n        # Residual standard error is square root of variance of y-values\n        self.RSE = obs_var**0.5\n    \n        # Variances of the parameter estimates are on the diagonal of the \n        # variance-covariance matrix of the parameter estimates\n        self.var_beta_mat = obs_var*(np.linalg.inv(np.matmul(X_mat.T, X_mat)))\n        self.var_beta = self.var_beta_mat.diagonal()\n    \n        # Standard error is square root of variance\n        self.se_beta = np.sqrt(self.var_beta)\n    \n        # t-statistic for beta_i is beta_i\/se_i, \n        # where se_i is the standard error for beta_i\n        t_stats_beta = self.params\/self.se_beta\n    \n        # Compute p-values for each parameter using a t-distribution with\n        # (num_samples - 1) degrees of freedom\n        beta_p_values = [2 * (1 - stats.t.cdf(np.abs(t_i), X_mat.shape[0] - 1)) for t_i in t_stats_beta]\n    \n        # Compute value of overall F-statistic, to measure how likely our\n        # coefficient estimate are, assuming there is no relationship between\n        # the predictors and the response\n        self.F_overall = ((self.TSS - self.RSS)\/(X_mat.shape[1] - 1))\/(self.RSS\/(X_mat.shape[0] - X_mat.shape[1]))\n        self.F_p_value = stats.f.sf(self.F_overall, X_mat.shape[1] - 1, X_mat.shape[0] - X_mat.shape[1])\n    \n        # Construct dataframe for the overall model statistics:\n        # RSE, R^2, F-statistic, p-value for F-statistic\n        oa_model_stats = pd.Series({\"Residual standard error\": self.RSE, \"R-squared\": self.score(X, y), \"F-statistic\": self.F_overall, \"F-test p-value\": self.F_p_value})\n    \n        # Construct dataframe for parameter statistics:\n        # coefficients, standard errors, t-statistic, p-values for t-statistics\n        param_stats = pd.DataFrame({\"Coefficient\": self.params, \"Standard Error\": self.se_beta, \"t-value\": t_stats_beta, \"Prob(>|t|)\": beta_p_values})\n        return {\"model\": self, \"param_stats\": param_stats, \"oa_stats\": oa_model_stats}\n    \n    def param_conf_int(self, level = 0.95):\n        \"\"\"\n        Assume level, if given, is a float with 0 < level < 1\n        Computes confidence intervals at the given confidence level for each parameter\n        in the linear regression model relating the predictors X to the response y\n        Returns a dataframe with the endpoints of the confidence interval for each parameter\n        \"\"\"\n        conf_intervals = pd.DataFrame()\n        # Degrees of freedom = num_samples - (num_features + 1)\n        df = self.training_X.shape[0] - (self.training_X.shape[1] + 1)\n        a, b = str(round((1 - level)*100\/2, 2)) + \"%\", str(round((1 + level)*100\/2, 2)) + \"%\"\n        # Loop through each parameter\n        for i in range(len(self.params)):\n            coeff = self.params[i]\n            std_err = self.se_beta[i]\n            # alpha = level of confidence\n            # df = degrees of freedom = num_samples - number of parameters\n            # loc = center of t-interval = estimated coefficient value\n            # scale = standard error in coefficient estimate\n            conf_intervals = conf_intervals.append(pd.DataFrame([stats.t.interval(level, df, loc = coeff, scale = std_err)], columns = [a, b]), ignore_index = True)\n        return conf_intervals\n    \n    def predict_intervals(self, X_pred, level = 0.95, kind = \"confidence\"):\n        \"\"\"\n        Assume X_pred is array-like with shape (num_predictions, num_features) with x-values for which we want predictions\n        Assume level, if given, is a float with 0 < level < 1\n        Assume kind, if given is either the string \"confidence\" or \"prediction\" for the kind of interval\n        Computes confidence intervals at the given confidence level for each parameter\n        in the linear regression model relating the predictors X to the response y\n        Returns a dataframe with the endpoints of the confidence interval for each parameter\n        \"\"\"\n        # Store predictions for easy access\n        predictions = self.predict(X_pred)\n        intervals = pd.DataFrame()\n        # Degrees of freedom = num_samples - (num_features + 1)\n        df = self.training_X.shape[0] - (self.training_X.shape[1] + 1)\n        a, b = str(round((1 - level)*100\/2, 2)) + \"%\", str(round((1 + level)*100\/2, 2)) + \"%\"\n        # Loop through each x-value being used for prediction\n        for i in range(len(predictions)):\n            prediction = predictions[i]\n            # Need to append the leading 1 since our matrix of regression parameter\n            # Estimates has first row the estimate for the constant\n            x_star = np.append(np.ones(1), X_pred[i])\n            conf_error = np.matmul(np.matmul(x_star.T, self.var_beta_mat), x_star)**0.5\n            predict_error = (self.RSE**2 + conf_error**2)**0.5\n            # alpha = level of confidence\n            # df = degrees of freedom = num_samples - number of parameters\n            # loc = center of t-interval = predicted value from linear regression model\n            # scale = standard error in predicted value estimate\n            if (kind == \"confidence\"):\n                lower, upper = stats.t.interval(level, df, loc = prediction, scale = conf_error)\n                intervals = intervals.append(pd.Series({\"prediction\": prediction[0], a: lower[0], b: upper[0]}), ignore_index = True) \n            elif(kind == \"prediction\"):\n                lower, upper = stats.t.interval(level, df, loc = prediction, scale = predict_error)\n                intervals = intervals.append(pd.Series({\"prediction\": prediction[0], a: lower[0], b: upper[0]}), ignore_index = True)\n        return intervals","9dd1e0b7":"extended_reg = ExtendedLinearRegression()\ndetailed_regression_stats = extended_reg.detailed_linear_regression(X, y)\nnp.round(detailed_regression_stats[\"param_stats\"], 4)","bed38e7b":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","73ada482":"extended_reg.predict_intervals(np.array([5, 10, 15]).reshape((-1, 1)), level = 0.99, kind = \"prediction\")","de5b1dc7":"# Plot scatterplot with regression line and default 95% confidence interval for regression estimate\n# Set the marker transparancy to 0.25 in order to more clearly see the regression line\n# Make regression line orange so it is more visible\nsns.regplot(x = \"LSTAT\", y = \"CMEDV\", data = boston, scatter_kws = {\"alpha\":0.25}, line_kws = {\"color\":\"orange\"})","76ab4400":"# Plot scatterplot with regression line and 99% confidence interval for regression estimate\n# Set the marker transparancy to 0.25 in order to more clearly see the regression line\n# Make regression line orange so it is more visible\nsns.regplot(x = \"LSTAT\", y = \"CMEDV\", data = boston, ci = 99, scatter_kws = {\"alpha\":0.25}, line_kws = {\"color\":\"orange\"})","90bb7f5b":"# Set the marker transparency to 0.25 in order to improve visibility\nsns.residplot(x = \"LSTAT\", y = \"CMEDV\", data = boston, scatter_kws = {\"alpha\":0.25})","bc375229":"# Generate a range of x-values to feed into the regression model for producing the\n# line that gets passed to the plot function\nx = np.linspace(0, 40, num = 100).reshape(-1, 1)\npredictions = reg.predict(x)\nfig = plt.figure()\nax = plt.axes()\n# Plot the regression line in orange with a line width of 3 to increase visibility\nax.plot(x, predictions, color = \"orange\", linewidth = 3)\n# Plot the scatter plot using an alpha value of 0.25 for the markers to reduce clutter\nax.scatter(boston[\"LSTAT\"], boston[\"CMEDV\"], alpha = 0.25)\n# Give the scatterplot some labels that are more descriptive\nax.set(xlabel = \"LSTAT\", ylabel = \"CMEDV\", xlim = (0, 40))","d2840092":"# Generating residual plot by hand using scikit-learn\n# Compute predicted values\npredicted_cmedv = reg.predict(X)\n# Compute residuals\nresiduals = y - predicted_cmedv\nfig = plt.figure()\nax = plt.axes()\n# Plot residuals versus fitted values\nax.scatter(predicted_cmedv, residuals, alpha = 0.25)\n# Plot orange dashed horizontal line y = 0\nax.axhline(y = 0, color = \"orange\", linestyle = \"--\")\n# Give the plot some descriptive axis labels\nax.set(xlabel = \"Fitted value of CMEDV\", ylabel = \"Residual value\")","c39e7f15":"# Generating residual plot by hand using StatsModels\n# Compute predicted values\npredicted_cmedv = res.predict()\n# Compute residuals\nresiduals = res.resid\nfig = plt.figure()\nax = plt.axes()\n# Plot residuals versus fitted values\nax.scatter(predicted_cmedv, residuals, alpha = 0.25)\n# Plot orange dashed horizontal line y = 0\nax.axhline(y = 0, color = \"orange\", linestyle = \"--\")\n# Give the plot some descriptive axis labels\nax.set(xlabel = \"Fitted value of CMEDV\", ylabel = \"Residual value\")","98e02545":"# Appened leading column of ones to the matrix of predictors\ndesign_mat = np.append(np.ones((X.shape[0], 1)), X, axis = 1)\n# Compute hat matrix\nhat_mat = design_mat @ np.linalg.inv(design_mat.T @ design_mat) @ design_mat.T\n# Leverage values are the diagonal of the hat matrix\nleverage_vals = hat_mat.diagonal()\nresiduals = (y - reg.predict(X)).flatten()\nresidual_standard_error = (np.sum(residuals**2) \/ (design_mat.shape[0] - design_mat.shape[1]))**0.5\n# Compute studentized residuals\nstudentized_residuals = residuals\/(residual_standard_error*(1 - leverage_vals)**0.5)\nfig = plt.figure()\nax = plt.axes()\n# Plot studentized residuals versus fitted values\nax.scatter(predicted_cmedv, studentized_residuals, alpha = 0.25)\n# Plot orange dashed horizontal line y = 0\nax.axhline(y = 0, color = \"orange\", linestyle = \"--\")\n# Give the plot some descriptive axis labels\nax.set(xlabel = \"Fitted value of CMEDV\", ylabel = \"Studentized residual value\")","ce818d72":"fig = plt.figure()\nax = plt.axes()\n# Plot leverage values for each observation\nax.scatter(np.arange(design_mat.shape[0]), leverage_vals, alpha = 0.25)\n# Plot orange dashed horizontal line y = (p + 1)\/n, the average leverage for all observations\nax.axhline(y = design_mat.shape[1]\/design_mat.shape[0], color = \"orange\", linestyle = \"--\")\n# Give the plot some descriptive axis labels\nax.set(xlabel = \"Index\", ylabel = \"Leverage value\", ylim = (0, leverage_vals.max()*1.1))","673efcb0":"leverage_vals.argmax()","1f3a6365":"# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Need to extract the columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\nX = boston.loc[:, [\"LSTAT\", \"AGE\"]].values\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","6e4d4c1b":"np.round(detailed_regression_stats[\"param_stats\"], 4)","cf2b01b2":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","d8cd341c":"# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(boston.loc[:, [\"LSTAT\", \"AGE\"]])\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod = sm.OLS(endog, exog)\n\n# Fit the model\nres = mod.fit()\n\n#Print out model summary\nprint(res.summary())","4b70e5fd":"# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Need to extract the columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\nX = boston.drop(columns = [\"CMEDV\"]).values\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","1424f41c":"np.round(detailed_regression_stats[\"param_stats\"], 4)","f5903746":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","971f0ed6":"# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(boston.drop(columns = [\"CMEDV\"]))\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod = sm.OLS(endog, exog)\n\n# Fit the model\nres = mod.fit()\n\n#Print out model summary\nprint(res.summary())","6a6fbaec":"def vif(predictors):\n    \"\"\"\n    Assumes predictors is a Pandas dataframe with at least two columns\n    Returns a Pandas series containing the variance inflation factor for each column variable\n    \"\"\"\n    columns = predictors.columns\n    vif_series = pd.Series()\n    for col_name in columns:\n        X = predictors.drop(columns = [col_name]).values\n        y = predictors[col_name].values.reshape(-1, 1)\n        reg = LinearRegression().fit(X, y)\n        r_sq = reg.score(X, y)\n        vif_series[col_name] = 1\/(1 - r_sq)\n    return vif_series","728d824f":"vif(boston.drop(columns = [\"CMEDV\"]))","d3e63e90":"# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Need to extract the columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\nX = boston.drop(columns = [\"CMEDV\", \"AGE\"]).values\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","91b0674d":"np.round(detailed_regression_stats[\"param_stats\"], 4)","28abac98":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","8f93467c":"# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(boston.drop(columns = [\"CMEDV\", \"AGE\"]))\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod = sm.OLS(endog, exog)\n\n# Fit the model\nres = mod.fit()\n\n#Print out model summary\nprint(res.summary())","e5c30f22":"# Using patsy to include interaction terms via R-style formulas\n\n# Generate a linear regression model with LSTAT, AGE, and an interaction term between\n# them to predict CMEDV\nmod = smf.ols(formula = \"CMEDV ~ LSTAT*AGE\", data = boston)\nres = mod.fit()\nprint(res.summary())","3d176aa2":"# Creating a column forthe interaction terms by hand\n\n# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(boston.loc[:, [\"LSTAT\", \"AGE\"]].assign(LSTAT_AGE = boston[\"LSTAT\"] * boston[\"AGE\"]))\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod = sm.OLS(endog, exog)\n\n# Fit the model\nres = mod.fit()\n\n#Print out model summary\nprint(res.summary())","653c2c2b":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# Create a pipeline which first transforms the data to include up to second degree terms\n# Setting interaction_only to True indicates that we only want interaction terms\n# and excludes higher powers of the individual features\n# Since the transformed data includes the 0-degree (i.e. constant = 1) feature\n# an intercept is not necessary in the linear regression\nmodel = Pipeline([(\"poly\", PolynomialFeatures(degree = 2, interaction_only = True)),\n                 (\"linear\", LinearRegression(fit_intercept = False))])\nX = boston.loc[:, [\"LSTAT\", \"AGE\"]].values\ny = boston[\"CMEDV\"].values.reshape((-1, 1))\nmodel = model.fit(X, y)\nprint(model.named_steps[\"linear\"].coef_)","ba292d96":"from sklearn.preprocessing import PolynomialFeatures\n# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Create a 2nd degree Polynomial features transformer which only includes interaction terms\npoly = PolynomialFeatures(degree = 2, interaction_only = True)\n# Need to extract the columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\n# Transform X using the PolynomialFeatures transformer\n# Exclude the intercept column so it plays nicely with how I've written the ExtendedLinearRegression class\nX = poly.fit_transform(boston.loc[:,[\"LSTAT\", \"AGE\"]].values)[:, 1:]\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","6c305721":"np.round(detailed_regression_stats[\"param_stats\"], 4)","4d776c2d":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","af1b676a":"# Creating a column for LSTAT**2 by hand\n\n# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(boston.loc[:, [\"LSTAT\"]].assign(LSTAT_sq = np.square(boston[\"LSTAT\"])))\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod_square = sm.OLS(endog, exog)\n\n# Fit the model\nres_square = mod_square.fit()\n#Print out model summary\nprint(res_square.summary())","dff6ef24":"# Using patsy to include the term LSTAT**2 via R-style formulas\n\nmod_square = smf.ols(formula = \"CMEDV ~ LSTAT + np.square(LSTAT)\", data = boston)\nres_square = mod_square.fit()\nprint(res_square.summary())","40f7ff62":"# Using PolynomialFeatures transformer with scikit-learn to include the term LSTAT**2\n\nfrom sklearn.preprocessing import PolynomialFeatures\n# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Create a 2nd degree Polynomial features transformer\npoly = PolynomialFeatures(degree = 2)\n# Need to extract the columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\n# Transform X using the PolynomialFeatures transformer\n# Exclude the intercept column so it plays nicely with how I've written the ExtendedLinearRegression class\nX = poly.fit_transform(boston.loc[:,[\"LSTAT\"]].values)[:, 1:]\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","b9cde211":"np.round(detailed_regression_stats[\"param_stats\"], 4)","5aa877d7":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","0a54b615":"mod_square = smf.ols(formula = \"CMEDV ~ LSTAT + np.square(LSTAT)\", data = boston).fit()\nmod = smf.ols(formula = \"CMEDV ~ LSTAT\", data = boston).fit()\nanova_table = sm.stats.anova_lm(mod, mod_square)\nanova_table","cc8adde0":"# Plot scatterplot with regression line and default 95% confidence interval for regression estimate\n# Set the marker transparancy to 0.25 in order to more clearly see the regression line\n# Make regression line orange so it is more visible\nsns.regplot(x = \"LSTAT\", y = \"CMEDV\", data = boston, order = 2, scatter_kws = {\"alpha\":0.25}, line_kws = {\"color\":\"orange\"})","a43437e0":"# Set the marker transparency to 0.25 in order to improve visibility\nsns.residplot(x = \"LSTAT\", y = \"CMEDV\", data = boston, order = 2, scatter_kws = {\"alpha\":0.25})","1234b688":"poly = PolynomialFeatures(degree = 2)\n\n# Generate a range of x-values to feed into the regression model for producing the\n# line that gets passed to the plot function\nx = np.linspace(0, 40, num = 100).reshape(-1, 1)\n# Need to transform the x array to properly feed into the predict function\ntransformed = poly.fit_transform(x)[:, 1:]\npredictions = reg.predict(transformed)\nfig, axes = plt.subplots(nrows = 2, figsize = (10, 10), gridspec_kw = {})\n# Plot the regression line in orange with a line width of 3 to increase visibility\naxes[0].plot(x, predictions, color = \"orange\", linewidth = 3)\n# Plot the scatter plot using an alpha value of 0.25 for the markers to reduce clutter\naxes[0].scatter(boston[\"LSTAT\"], boston[\"CMEDV\"], alpha = 0.25)\n# Give the scatterplot some labels that are more descriptive\naxes[0].set(xlabel = \"LSTAT\", ylabel = \"CMEDV\", xlim = (0, 40))\n\n# Generating residual plot by hand using scikit-learn\n# Compute predicted values\npredicted_cmedv = reg.predict(X)\n# Compute residuals\nresiduals = y - predicted_cmedv\n# Plot residuals versus fitted values\naxes[1].scatter(predicted_cmedv, residuals, alpha = 0.25)\n# Plot orange dashed horizontal line y = 0\naxes[1].axhline(y = 0, color = \"orange\", linestyle = \"--\")\n# Give the plot some descriptive axis labels\naxes[1].set(xlabel = \"Fitted value of CMEDV\", ylabel = \"Residual value\")","bffc9245":"# Creating a fifth-order polynomial fit by hand\n\n# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(boston.loc[:, [\"LSTAT\"]].assign(LSTAT_2 = boston[\"LSTAT\"]**2,\n                                                      LSTAT_3 = boston[\"LSTAT\"]**3,\n                                                      LSTAT_4 = boston[\"LSTAT\"]**4,\n                                                      LSTAT_5 = boston[\"LSTAT\"]**5))\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod_quint = sm.OLS(endog, exog)\n\n# Fit the model\nres_quint = mod_quint.fit()\n\n#Print out model summary\nprint(res_quint.summary())","194eeebc":"# Using PolynomialFeatures transformer with scikit-learn to create fifth-order polynomial fit\n\nfrom sklearn.preprocessing import PolynomialFeatures\n# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Create a 2nd degree Polynomial features transformer\npoly = PolynomialFeatures(degree = 5)\n# Need to extract the columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\n# Transform X using the PolynomialFeatures transformer\n# Exclude the intercept column so it plays nicely with how I've written the ExtendedLinearRegression class\nX = poly.fit_transform(boston.loc[:,[\"LSTAT\"]].values)[:, 1:]\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","d9c4d5a4":"np.round(detailed_regression_stats[\"param_stats\"], 6)","183aa69e":"np.round(detailed_regression_stats[\"oa_stats\"], 6)","add2c6fc":"# Using PolynomialFeatures transformer to create fifth-order polynomial fit with StatsModels\n\npoly = PolynomialFeatures(degree = 5)\n# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# No need to include a column for intercept in this case, since it is\n# included when applying the fit_transform function\n# Make sure to reshape the LSTAT column values to play nicely with fit_transform\nexog = poly.fit_transform(boston[\"LSTAT\"].values.reshape((-1, 1)))\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod_quint = sm.OLS(endog, exog)\n\n# Fit the model\nres_quint = mod_quint.fit()\n\n#Print out model summary\nprint(res_quint.summary())","a9d79400":"# Using Python string manipulation alongside patsy to create fifth-order polynomial fit\n\n# Create string for the higher-order polynomial terms\npoly_terms = \"+\".join([\"I(LSTAT**{0})\".format(i) for i in range(2, 6)])\n# Join this string with the rest of the formula I wish to use\nmy_formula = \"CMEDV ~ LSTAT + \" + poly_terms\nmod_quint = smf.ols(formula = my_formula, data = boston)\nres_quint = mod_quint.fit()\nprint(res_quint.summary())","478c07a1":"# Creating a column for log(RM) by hand, using StatsModels\n# Here log refers to the natural logarithm\n\n# Use the terms exog (exogenous) and endog (endogenous) for X and y,\n# respectively to match the language used in the StatsModels documentation\n# Need to manually add a column for the intercept, as StatsModels does not\n# include it by default when performing ordinary least-squares regression\nexog = sm.add_constant(np.log(boston[\"RM\"].rename(\"log(RM)\")))\nendog = boston[\"CMEDV\"]\n\n# Generate the model\nmod_log = sm.OLS(endog, exog)\n\n# Fit the model\nres_log = mod_log.fit()\n#Print out model summary\nprint(res_log.summary())","90bd3f31":"# Using patsy to include the term log(RM) via R-style formulas\n\nmod_log = smf.ols(formula = \"CMEDV ~ np.log(RM)\", data = boston)\nres_log = mod_log.fit()\nprint(res_log.summary())","6701525e":"# Creating a column for log(RM) by hand, using scikit-learn\n\n# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Need to extract the columns we are using and then reshape them so that\n# X has the shape (n_samples, n_features),\n# Y has the shape (n_samples, n_targets)\n# When using -1 as one of the reshape arguments, NumPy will infer the value\n# from the length of the array and the values for the other dimensions\n# Transform the RM column using np.log\nX = np.log(boston[\"RM\"]).values.reshape(-1, 1)\ny = boston[\"CMEDV\"].values.reshape(-1, 1)\n# Fit linear regression model using X = LSTAT, y = CMEDV\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","67616697":"np.round(detailed_regression_stats[\"param_stats\"], 4)","18867626":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","879ead89":"# Load the Carseats data set\n# Use the unnamed zeroth column as the index\ncarseats_filepath = \"..\/input\/islr-carseats\/Carseats.csv\"\ncarseats = pd.read_csv(carseats_filepath, index_col = [\"Unnamed: 0\"])","81a213cb":"carseats.head()","d83c112a":"carseats.isnull().any()","c9d878f7":"# Using patsy to include the perform multiple regression using the Carseats data\n# Include interaction terms for Income:Advertising and Price:Age\n# Note that there are some qualitative predictors\n\n# Create string for the names of all of the columns\nall_columns = \"+\".join(carseats.columns.drop(\"Sales\"))\n# Join this string with the rest of the formula I wish to use\nmy_formula = \"Sales ~\" + all_columns + \"+ Income:Advertising + Price:Age\"\nmod = smf.ols(formula = my_formula, data = carseats)\nres = mod.fit()\nprint(res.summary())","04d37f22":"from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n# Create an ExtendedLinearRegression object\nreg = ExtendedLinearRegression()\n# Create a 2nd degree Polynomial features transformer which only includes interaction terms\npoly = PolynomialFeatures(degree = 2, interaction_only = True)\n# Create columns for interaction terms Income:Advertising and Price:Age\nincome_advert = pd.Series(poly.fit_transform(carseats.loc[:, [\"Income\", \"Advertising\"]])[:, -1], name = \"Income:Advertising\")\nprice_age = pd.Series(poly.fit_transform(carseats.loc[:, [\"Price\", \"Age\"]])[:, -1], name = \"Price:Age\")\n# Encode categorical predictors using OneHotEncoder\n# Set the categories and drop the first category when encoding to use reduced-rank coding\n# This then replicates the default behavior of how Patsy and R do categorical encoding\nenc = OneHotEncoder(categories = [[\"Bad\", \"Medium\", \"Good\"], [\"No\", \"Yes\"], [\"No\", \"Yes\"]], drop = \"first\")\ncat_pred = enc.fit_transform(carseats.loc[:, [\"ShelveLoc\", \"Urban\", \"US\"]]).toarray()\ncat_pred = pd.DataFrame(cat_pred, columns = [\"ShelveLocMedium\", \"ShelveLocGood\", \"UrbanYes\", \"USYes\"])\nquant_pred = carseats.loc[:, [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]].reset_index(drop = True)\n\n# Combine all of the columns into a single dataframe of predictors\n# Note that we needed to reset the index for quant_pred in order to have it align with the indices\n# for the other columns when joining\n# We could avoid this if we worked purely with the underlying NumPy arrays\nX = cat_pred.join([quant_pred, income_advert, price_age])\ny = carseats[\"Sales\"].values.reshape(-1, 1)\ndetailed_regression_stats = reg.detailed_linear_regression(X, y)","3127adc1":"np.round(detailed_regression_stats[\"param_stats\"], 4)","04c8ccae":"np.round(detailed_regression_stats[\"oa_stats\"], 4)","99adadd6":"# List of columns to remember which column corresponds to each coefficient\nX.columns","cc211747":"A quick way to peform a regression using all of the predictors in a data set is to use the `drop()` function in Pandas to drop the column containing the response variable.","f9523555":"One last thing to note is that we can use other non-linear transformations of the predictors. For example, we can do a logarithmic model where the predictor is $\\ln(X)$, and $X$ is `RM`, the average number of rooms.","f581de0f":"Variance inflation factors close to the minimum value of 1 indicate a small amount of collinearity, while values exceeding 5 or 10 are generally considered to indicate a problematic amount of collinearity. For this data, most of the variance inflation factors are low to moderate, though `RAD` and `TAX` stand out as having values on the high end.","08efcba8":"Some fancier statistics require coding additional functions by hand. For example, while StatsModels does have a function to [compute variance inflation factors](http:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.stats.outliers_influence.variance_inflation_factor.html), its current implementation can only do so for one predictor variable at a time. Thus, we will write our own function to compute the variance inflation factors for all of our predictors at once.","ff7bf206":"Alternatively, if for some reason we do not wish to use `R`-style formulas when using StatsModels, we can create the columns for the interaction terms by hand.","3f1f5c10":"# Lab: Linear Regression","f9a0b8b7":"If we wish to access individual components of a scikit-learn LinearRegression object or StatsModels OLSResults object, we can access they by name. The command `dir` shows what class variables and functions are available to access. For more details, we can also look at the documentation for [scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html) or [StatsModels](http:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.regression.linear_model.OLSResults.html).","3f325314":"This tells us that the regression line takes the form $y = -0.95x + 34.58$, where $y$ is the response `CMEDV` and $x$ is the predictor `LSTAT`. We can also access the value for $R^2$, the coefficient of determination, using the `score()` function.","04b09064":"Notice that we can alter some options for how the points of the scatter plot and the regression line are displayed by using the `scatter_kws` and `line_kws` arguments, respectively. For further information about the options that can be tweaked, refer to the Matplotlib documentation for [plot](https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.pyplot.plot.html) for the options for `line_kws` and for [scatter](https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.pyplot.scatter.html) for `scatter_kws`.","b3cf02b8":"In the multiple linear regression output, we see that `AGE` has a high p-value of 0.958, so we might want to exclude it. If we wish to perform regression with a small number of excluded variables the simplest thing to do is to re-run the regression after dropping the variable(s) we wish to exclude.","8130445a":"This lab involves the `Boston` and `Carseats` data sets, so before getting started we should make sure we have them available on our computer along with our usual Python data science packages. Recall that since we are analyzing these data sets in Python instead of `R`, we may should make sure to download the corresponding CSV files for each set from the book's [website](http:\/\/www.statlearning.com) under the \"Data Sets and Figures\" link. The corrected Boston housing data set which I am using can be downloaded from the [CMU StatLib archive](http:\/\/lib.stat.cmu.edu\/datasets\/boston_corrected.txt). The `Carseats` data set wasn't available directly as a CSV file from the book's website, so I needed to load it from the ISLR library in R and then export it.","ddb1dfbd":"Recall that the variance inflation factor for a predictor $X_j$ is the ratio of the variance of its coefficient $\\hat{\\beta}_j$ in the full model using divided by the variance of $\\hat{\\beta}_j$ in the model just using $X_j$. Another way to compute the variance inflation factor for each variable is to use the formula\n\n\\begin{equation}\n\\text{VIF}(\\hat{\\beta}_j) = \n\\frac{1}{1 - R^2_{X_j | X_{-j}}},\n\\end{equation}\n\nwhere $R^2_{X_j | X_{-j}}$ is the $R^2$ value from a regression using $X_j$ as the response and the remaining variables as the predictors.","9e77839d":"The `anova_lm()` function performs a hypothesis test comparing the two models. The null hypothesis is that both models fit the data equally well, while the alternative hypothesis is that the second model (in our case the model including the quadratic term) performs better. Here, we have an F-statistic of about 138 and an p-value that is essentially zero, which provides strong evidence that the model containing the predictors `LSTAT` and `LSTAT**2` is a better fit than the one containing only `LSTAT`. This further confirms our initial suspicions based on the non-linearity we saw in the scatter plot for `CMEDV` and `LSTAT`.","56fed4cc":"## Interaction Terms","69bc7b86":"This summary suggests that including additional polynomial terms further improves the model fit. Remember that we *do not* care about the coefficient values when evaluating model fit. We care about statistical indicators such as the $R^2$ value for each model. Further investigation of the data indicates that polynomial terms beyond fifth order do not have significant p-values in a regression fit. Before moving on to other non-linear transformations of the predictors, note that we can use Python's string manipulation capabilities as another way of more conveniently performing higher order polynomial fits when using `R`-style formulas in StatsModels.","a14ea725":"In addition, as we can see above, there isn't a discernible pattern in the residuals for the model which includes the `LSTAT**2` term.","e0612f9f":"FIrst we produce 95% confidence intervals for the predicted `CMEDV` value using `LSTAT` values of 5, 10, and 15.","e77505d2":"By default, the `conf_int()` method gives 95% confidence intervals for the predicted values, though by adjusting the `level` parameter we can change the confidence\/prediction level, and `obs` parameter allows us to change from confidence intervals to prediction intervals.","d9a64a7d":"If we wish to include higher-order predictors of the form $X^k$, the strategy of adding those columns by hand becomes less convenient. It is still doable by appropriately adjusting the how we call the `assign()` function to add columns to our dataframe. Let's demonstrate this by producing a fifth-order polynomial fit.","ff5f853d":"As discussed earlier, there are a few different ways of generating residual plots for us to have visual evidence that a model containing the predictors `LSTAT` and `LSTAT**2` is a better fit than the one containing only `LSTAT`. First, we can set the `order` parameter in Seaborn's `residplot()` function to generate residual plots for higher-order polynomial regression. For completeness, we first use Seaborn to plot the quadratic fit on a scatterplot of the data as well.","e502079a":"Using a pipeline like this doesn't play nicely with the ExtendedLinearRegression class I created, so if we want to use that for access to more of the detailed model statistics, we'll need to do things a little more by hand. However, in a production setting, pipeline usage is very useful for convenience and encapsulation, joint parameter selection, and safety.","52411d68":"We can also quickly plot the residuals with Seaborn's `residplot()` function.","b9530bfc":"By adjusting the `level` parameter in the `param_conf_int()` function I wrote, or by adjusting the `alpha` parameter in the `conf_int()` function from StatsModels, we can also produce other confidence intervals for the parameters. For example, we can produce 99% confidence intervals.","dabf43fc":"Next, we compute confidence and prediction intervals for the prediction of `CMEDV` for a given value of `LSTAT`. Once again, we'll start off by coding a function by hand to do this, referring to [these notes from Stanford](http:\/\/statweb.stanford.edu\/~susan\/courses\/s141\/horegconf.pdf) for the underlying mathematics of what values of the error to use when computing the intervals. Note that the Stanford notes only cover the situation of a single predictor and a single response.","be55c2f8":"## Loading the necessary packages and data sets","4424a655":"From the scatter plot, we can see that there is some evidence that the relationship between `lstat` and `medv` is non-linear. We'll take a look at this a little later in the lab.","3f439c32":"As we can see, these results match up with what we computed by hand. Once again, in this context using StatsModels proves to be a lot more convenient! The functions we created for this section are pretty handy, but since we did them one at a time in a somewhat piecemeal fashion, they aren't as cohesive and nicely implemented as they could be, so lets combine them all into a class that extends the scikit-learn LinearRegression class. In addition, I'll rewrite my `predict_intervals()` function to properly work for the situation of multiple linear regression. To do so, I will follow the math from [these notes from the University of Minnesota](http:\/\/users.stat.umn.edu\/~helwig\/notes\/mlr-Notes.pdf). ","e1a341cb":"Since we stored the leverage values in a NumPy array, we can use the `argmax()` function to find the index of the maximum leverage value. In other words, we can use it in this case to find out which observation has the largest leverage value.","48ce014b":"## Qualitative Predictors","5fe312e9":"Looking at the intervals, we can see that a 95% confidence interval for the predicted value of `CMEDV` using an `LSTAT` value of 10 is (24.481, 25.631), while the corresponding 95% prediction interval is (12.913, 37.199). Both intervals are centered around the same point, the predicted value for `CMEDV` of 25.053, but the prediction interval is much wider. Now, let's compare with the intervals we get when using StatsModels.","ac21c1d0":"## Multiple Linear Regression","41226131":"First, we'll use scikit-learn to fit a simple linear regression model using `CMEDV` as the response and `LSTAT` as the predictor. To do so, we create a `LinearRegression` object and then use the `LinearRegression.fit(X, y)` function to fit a linear model with `X` as the predictor and `y` as the response. For more details, we can look at the [official documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html) and a [basic example](https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_ols.html). Note that the `fit(X, y)` function requires the training `X` and `y` data to have the correct shape. In particular, when working with a single predictor, we cannot just pass the corresponding column of the dataframe as an argument, as `df[\"Col_name\"].shape` has the value `(num_rows, )`, meaning that the underlying values are stored in a one-dimensional array. Instead, we need a two-dimensional array with shape `(num_rows, 1)`, which means that we have `num_rows` observations, and a single feature value for each observation. As soon as we start using multiple predictors, `df.loc[:, [\"Pred_1\", ..., \"Pred_p\"]]` will have the correct shape of `(num_rows, p)`. The same is true as soon as we start using multiple targets.","71e3667d":"This was a lot of work! It took me a solid 1.5 days of working through the Stackoverflow post and section of *Elements of Statistical Learning* to feel like I had a decently confident grasp on the underlying mathematics and how to translate that math into Python code, but I think it was a really good process to go through. Now, let's see how we can generate the model and access the summary statistics using [StatsModels](http:\/\/www.statsmodels.org\/stable\/regression.html#references).","6c409735":"To perform this regression with scikit-learn, we'll use the [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) to code the categorical predictors and again use the PolynomialFeatures transformer to add the interaction terms. We can look at the preprocessing section of the user guide for more information on how to use scikit-learn for [encoding categorical data](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#encoding-categorical-features).","af48287a":"While plotting with Seaborn is pretty convenient, it is limited in the types of regression models it can produce and the transformations we can make to the predictor variable. In addition, Seaborn can only produce one type of residual plot. If we want to produce other plots, we'll need to do things a little more directly using Matplotlib.pyplot. First, let's demonstrate how to produce a scatter plot of `LSTAT` versus `CMEDV` with the least squares regression line on it. Here we'll be using the object-oriented syntax for using Matplotlib.pyplot.","a05f7310":"When using a model generated by StatsModels, we can use the `conf_int(alpha = 0.05)` method of the RegressionResults object. Note that in StatsModels, the argument `alpha` refers to the *significance level*, which is equivalent to $1 - l$, where $l$ is the *confidence level*.","0862e2f9":"For now we won't worry about producing the Q-Q plot or Scale-Location plot in Python, but for future reference one option is to use some of the [built-in plotting methods in StatsModels](https:\/\/www.statsmodels.org\/stable\/graphics.html) for this if we don't want to code things by hand. We will, however, still go over how to compute leverage statistics and (internally) studentized residuals and then plot them. We will refer to the [Wikipedia page on studentized residuals](https:\/\/en.wikipedia.org\/wiki\/Studentized_residual) for the underlying math.","6855cb82":"Recall that NumPy arrays are zero-indexed, so this tells us that the 375th observation has the largest leverage value.","90fe6231":"Having a class like this makes the act of accessing the various regression statistics more like the way things are done with StatsModels. For example, we can go back and redo some of the computations we did in a way that extends better to the further examples later on in this lab.","8d4eab5f":"## Non-linear transformations of the predictors","0c391d73":"Once we have fitted a linear regression model, the `LinearRegression` object stores some basic info that we can access.","3008e7bc":"If we wish to include interaction terms in a linear regression model [using StatsModels](http:\/\/www.statsmodels.org\/stable\/example_formulas.html), we can use the syntax `statsmodels.formula.api.ols(formula = \"y~x1:x2\", data = df)`, which uses [patsy](https:\/\/patsy.readthedocs.io\/en\/latest\/) to implement `R`-style formula syntax which tells StatsModels to include an interaction term between `x1` and `x2`. This would give us the regression function $y = \\beta_0 + \\beta_{12}x_1x_2$. If we want to include the individual variables themselves, as well as the interaction term between them, we can use `x1*x2`, which is a shorthand for `x1 + x2 + x1:x2`. Calling `ols(formula = \"y~x1*x2\", data = df)` would give us the regression function $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{12}x_1x_2$. ","5b564602":"Now we produce 95%, and then 99%, prediction intervals for the predicted `CMEDV` values, once again using `LSTAT` values of 5, 10, and 15.","65aff2a7":"If we want any more information beyond this, such as p-values and standard errors for the coefficients, or residual standard error and F-statistic values for the model, we'll need to write some additional code of our own using SciPy and some linear algebra knowledge. The underlying math which goes into the calculation of the variance for each coefficient in a least-squares model with p predictors can be found at the beginning of Chapter 3 in the book [*Elements of Satistical Learning*](http:\/\/www-stat.stanford.edu\/ElemStatLearn). This [Stackoverflow post](https:\/\/stackoverflow.com\/a\/42677750) serves as the basis for my implementation.","e779bbe9":"It is much more convenient to use the PolynomialFeatures transformer from scikit-learn.","47db750a":"When using scikit-learn, while creating the columns for interaction terms by hand is still an option, a better way is to use [PolynomialFeatures transformer](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions) as part of a model pipeline.","1ac66855":"Note that there are a lot of other attributes of the fitted model that we can analyze and use. See the documentation for the [OLSResults object](http:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults). As we can see, that was a lot simpler! This reflects one of the big differences between the philosophies behind scikit-learn and StatsModels -- scikit-learn is more focused on applying the results of machine learning strategies, while StatsModels has a larger emphasis on being able to easily analyze the detailed underlying statistics that go into generating a model.\n\nNext, let's compute some confidence and prediction intervals for various aspects of our least squares regression model. Once again we'll start off with doing things by hand using SciPy, though thankfully this is a bit easier to do as we can use the `interval(alpha, df, loc = 0, scale = 1)` function associated with SciPy's [Student t-distribution](https:\/\/docs.scipy.org\/doc\/scipy-1.3.0\/reference\/generated\/scipy.stats.t.html) object. First off, we'll produce 95% confidence intervals for the parameter estimates.","71166b20":"Next we'll examine the `Carseats` data set from the `ISLR` library and attempt to predict `Sales` (child car seat sales) in 400 locations based on a number of predictors. To start with, we'll do our usual overview inspection of the first few rows of the data set and then check for missing values.","8e35ce19":"There are a few different options for plotting a least squares regression line alongside the scatterplot of `CMEDV` versus `LSTAT`. The first, which would be best for exploring whether or not least squares regression would be appropriate, as well as explore some simple variable transforms beyond linear regression (e.g. using polynomial regression of some degree bigger than 1), would be to use Seaborn's `lmplot()` or `regplot()` functions. The `lmplot()` function can be especially handy if we want to quickly visualize regression models that reflect different facets of the data. One example given in the [tutorial for visualizing linear relationships in Seaborn](http:\/\/seaborn.pydata.org\/tutorial\/regression.html) is exploring the difference between the tipping habits smokers and non-smokers as it relates to the total bill. Note that by default, Seaborn will include translucent bands around the regression line to indicate a 95% confidence interval for the regression estimate. This behavior can be altered by setting the `ci` parameter with an integer between 0 and 100 for the confidence level, or be turned off by setting `ci = None`.","5ec67b7b":"In this lab, we will go over how to do linear regression using Python. There are a few ways we can go about this, depending on how much we are interested in investigating ancillary statistics for the coefficients in the model (p-values, residual standard error, etc.). If our main goal is to generate the model and focus on the basic statistics for evaluating the model, then [scikit-learn](https:\/\/scikit-learn.org\/) will work well. If we want to analyze detailed ancillary statistics associated with the model, then [StatsModels](http:\/\/www.statsmodels.org\/) will generate those statistics without requiring us to code functions to compute them from scratch. Also, StatsModels allows for the usage of [R-style formulas](http:\/\/www.statsmodels.org\/stable\/example_formulas.html) when fitting models for those who are already comfortable with that syntax. For completeness and to get practice using both packages, I will complete this lab both ways.","6a544e27":"Similarly, we can produce 99% confidence intervals for the predicted `CMEDV` values, again using `LSTAT` values of 5, 10, and 15.","0a0eb3b4":"If we want to use StatsModels to compute leverage values and studentized residuals, we use the `OLSResults.get_influence()` to generate an [OLSInfluence](http:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.stats.outliers_influence.OLSInfluence.html) object. From there, we can use the `OLSInfluence.hat_matrix_diagonal()` function to compute leverage values and `OLSInfluence.resid_studentized()` to compute (internally) studentized residuals.","e5b113f1":"## Simple linear regression","23f18982":"To perform multiple linear regression using least squares, we again use the LinearRegression class from scikit-learn (or our ExtendedLinearRegression class, which extends the scikit-learn LinearRegression class), or we use the OLS class from StatsModels. In either case, the only change in syntax is in selecting the columns we use for regression. This will give a linear regression function of the form $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n$. To start off with, we perform multiple linear regression using `LSTAT` and `AGE` to predict `CMEDV`.","55c8a5d9":"Next, we use scikit-learn and Matplotlib directly to produce the scatterplot with quadratic fit and residuals plot.","4dae634f":"By default, Patsy uses [treatment (dummy) coding](https:\/\/patsy.readthedocs.io\/en\/latest\/API-reference.html#patsy.Treatment), though there are lots of other contrast coding options we can explore in the linked documentation. In addition, by default the reference is the first level (using alphabetical order), though we have the option to specify this explicitly if we wish. Here we see that Patsy created a dummy variable `ShelveLoc[T.Good]` which is equal to 1 if the shelving location is good and 0 otherwise, along with a dummy variable `ShelveLoc[T.Medium]` which is equal to 1 if the shelving location is medium and 0 otherwise. With this encoding, a bad shelving location corresponds to both dummy variables having a value of 0. The positive coefficients for `ShelveLoc[T.Good]` and `ShelveLoc[T.Medium]` in the regression output indicates that good or medium shelving locations contribute to higher sales compared to a bad location. The higher value of the coefficient for `ShelveLoc[T.Good]` indicates that a good shelving location has leads to a bigger increase in sales (over a bad location) than a medium shelving location.","0910f4e7":"Since the quadratic term `LSTAT**2` has an extremely small p-value, we have evidence to believe that its inclusion leads to an improved model. To further quantify the extent to which the quadratic fit is superior to the linear fit, we can use the `anova_lm()` function from StatsModels. More details can be found in [the documentation](http:\/\/www.statsmodels.org\/stable\/anova.html).","1654a4c0":"While Seaborn makes it really convenient to produce the residuals plot, it isn't quite as convenient to generate the other diagnostic plots which R can generate when applyint the `plot()` function to directly to the output from `lm()`. This is especially the case if we are using just scikit-learn, as we would have to calculate quantities such as the residuals or leverage values a little more directly. Things are a little easier when using StatsModels, as the `OLSResults` class has a number of built-in functions to calculate these quantities. To start off, let's compare the process for making a residuals plot by hand using scikit-learn versus StatsModels.","00854597":"We can also use additional non-linear transformations of our variables when generating least-squares regression models. For example, if we wish to create a predictor $X^2$ from the predictor $X$, we have a few different options depending on which packages we are using. Some of the options include\n\n- Creating a column for $X^2$ by hand, which works for both scikit-learn and StatsModels\n- Using `np.square(X)` or `X**2` as part of an `R`-style formula in StatsModels\n- Using the PolynomialFeatures transformer with scikit-learn\n\nLet's use each of these three strategies to perform a regression of `CMEDV` onto `LSTAT` and `LSTAT**2`.","20380c36":"Looking at the data, we see that there are a number of qualitative predictors. For example, the `ShelveLoc` predictor is an indicator of the quality of the shelving location, or the space within a store in which the car seat is displayed, at each location. This predictor takes on three possible values: *Bad*, *Medium*, and *Good*. If we include a qualitative variable in `statsmodels.formula.api.ols()`, `Patsy` will automatically generate dummy variables for the possible values of that variable. For more details on how this is done, we can refer to a tutorial on [contrast coding systems for categorical variables with Patsy](http:\/\/www.statsmodels.org\/stable\/contrasts.html) on the StatsModels documentation site, as well as the official [Patsy documentation for coding categorical data](https:\/\/patsy.readthedocs.io\/en\/latest\/categorical-coding.html). Now we'll create a multiple regression model that also includes some interaction terms. Note that as far as I can tell (as of November 2019), Patsy currently does not have an equivalent to `.` to include all columns in the way `R` can do this. However, following [this StackExchange post](https:\/\/stackoverflow.com\/a\/22388673), we can use Python string manipulation for this purpose.","db73a92c":"As we saw in the third applied exercise from Chapter 2, one of the factors in the `Boston` data set is `cmedv`, which records the median house value for 506 tracts of land around Boston. To start with, we'll try to predict `cmedv` using the other factors, such as `rm` (average number of rooms per house), `age` (percent of owner-occupied homes built prior to 1940), and `lstat` (percent of low socioeconomic households). First, we'll take a look at the first few rows of data and then check again for missing values."}}