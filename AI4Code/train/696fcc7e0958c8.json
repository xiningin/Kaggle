{"cell_type":{"05e1b453":"code","8dcae529":"code","ae6208a6":"code","7fee9a95":"code","24d50dfe":"code","94f509e0":"code","9c9e9b40":"code","3e384091":"code","fdd16fa9":"code","d09bda25":"code","4bf66baa":"code","c399aae8":"code","1af52540":"code","4899d42d":"code","6c8e5f9f":"code","307663c8":"code","dfaa5d49":"code","02565c12":"code","e8632e4a":"code","d4a8a0ff":"code","923f76ed":"code","82363050":"code","0435167d":"code","fba39446":"code","88c83c14":"code","c59ebd4c":"code","986f9939":"code","1dbd7c9c":"code","18e58844":"code","3e360bf4":"code","12a72834":"code","41442aff":"code","448a61ea":"code","f25c1e5b":"code","f5b32dec":"code","2fa4e102":"code","55435c8b":"code","69e077bc":"code","3c0ddd0c":"code","8b2c26c2":"code","e8e9140b":"code","cb6b8d23":"code","b98aa516":"code","a037332c":"code","330d2cb6":"code","ae2249ef":"code","8eef590b":"code","e35bffd4":"code","4881611d":"code","698bc2a7":"code","c7c70ffd":"code","020d7a5c":"code","42061817":"code","17b224c2":"code","c7ccaf5b":"code","4972a0dc":"code","6ad2565a":"code","6e51b627":"code","1a794122":"code","94ef6b01":"code","092f8c42":"code","de5ca74d":"markdown"},"source":{"05e1b453":"train_path = '..\/input\/amazon-ml-challenge-2021-hackerearth\/train.csv'\ntest_path = '..\/input\/amazon-ml-challenge-2021-hackerearth\/test.csv'\nsample_sub_path = '..\/input\/amazon-ml-challenge-2021-hackerearth\/sample_submission.csv'","8dcae529":"import tensorflow as tf","ae6208a6":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","7fee9a95":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport csv","24d50dfe":"# Loading in the data \ntrain_data = pd.read_csv(train_path , escapechar = \"\\\\\" , quoting = csv.QUOTE_NONE, nrows = 1000000 \/ 2)\ntrain_data.head()","94f509e0":"store_ids = train_data['BROWSE_NODE_ID'].to_numpy()","9c9e9b40":"store_ids","3e384091":"d = dict()\ns = set()\ni = 0\nfor j in store_ids:\n    if j not in s:\n        s.add(j)\n        d[i] = j\n        i+=1\n        \n# d","fdd16fa9":"new_train_sample = train_data.sample(frac= 0.5 , random_state= 42)","d09bda25":"del train_data","4bf66baa":"new_train_sample.isnull().sum()","c399aae8":"new_train_sample['BROWSE_NODE_ID'].min()","1af52540":"new_train_sample.DESCRIPTION.fillna(new_train_sample.TITLE, inplace=True)","4899d42d":"new_train_sample.TITLE.fillna(new_train_sample.DESCRIPTION, inplace=True)","6c8e5f9f":"new_train_sample.isnull().sum()","307663c8":"new_train_sample.BULLET_POINTS.fillna(\"[\"+new_train_sample.DESCRIPTION+\"]\", inplace=True)","dfaa5d49":"new_train_sample.BRAND.fillna(new_train_sample.TITLE.str.split(n=1).str[0], inplace=True)","02565c12":"new_train_sample = new_train_sample.dropna()","e8632e4a":"new_train_sample = new_train_sample.drop(columns = ['BULLET_POINTS'] , axis = 1)","d4a8a0ff":"new_train_sample","923f76ed":"new_train_sample.isnull().sum()","82363050":"# new_train_sample.to_csv('mycsvfile.csv',index=False)","0435167d":"# train_path = '.\/mycsvfile.csv'","fba39446":"# train_data = pd.read_csv(train_path)","88c83c14":"new_train_sample.info()","c59ebd4c":"store_ids = new_train_sample['BROWSE_NODE_ID'].to_numpy()\nd = dict()\ns = set()\ns1 = set()\ni = 0\nfor j in store_ids:\n    if j not in s:\n        s.add(j)\n        d[i] = j\n        i+=1\n        s1.insert(i)","986f9939":"def preprocess_dataframe(df,shuffle = True,fill_na = True ):\n    if shuffle: \n        df = df.sample(frac= 1 , random_state= 42)\n    df = df.drop(df[df.BROWSE_NODE_ID > 9919].index,axis = 0 )\n    if fill_na : \n        df = df.fillna(value = 'NAN')\n  \n    print(f'numrs of none values {df.isna().sum()}')\n    return df ","1dbd7c9c":"new_train_sample = preprocess_dataframe(new_train_sample, shuffle = False, fill_na = False)","18e58844":"import numpy as np\nnp.save('data.npy', d)","3e360bf4":"import tensorflow_hub as hub\nimport tensorflow as tf","12a72834":"base_layer_1 = hub.KerasLayer('https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4',trainable = False)","41442aff":"inputs = tf.keras.layers.Input(shape = [], dtype = tf.string)\nbase_model_layer = base_layer_1(inputs)\nlam_layer = tf.keras.layers.Lambda(lambda x : tf.expand_dims(x, axis =1))(base_model_layer)\nx = tf.keras.layers.LSTM(564)(lam_layer)\nx = tf.keras.layers.Dense(1080, activation= 'relu')(x)\nx = tf.keras.layers.Dense(1080, activation= 'relu')(x)\nx = tf.keras.layers.LSTM(564)(lam_layer)\nx = tf.keras.layers.Dense(1080, activation= 'relu')(x)\noutputs = tf.keras.layers.Dense(9919,activation='softmax')(x)\n\nconv_model = tf.keras.Model(inputs, outputs)","448a61ea":"conv_model.summary()","f25c1e5b":"conv_model.compile(loss =tf.keras.losses.sparse_categorical_crossentropy, \n              optimizer = tf.keras.optimizers.Adam(), \n              metrics = 'accuracy')","f5b32dec":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re","2fa4e102":"def cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\n\n\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#|*]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/|*]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\n\n\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent","55435c8b":"new_train_sample['TITLE'] = new_train_sample['TITLE'].str.lower()\nnew_train_sample['TITLE'] = new_train_sample['TITLE'].apply(cleanHtml)\nnew_train_sample['TITLE'] = new_train_sample['TITLE'].apply(cleanPunc)\nnew_train_sample['TITLE'] = new_train_sample['TITLE'].apply(keepAlpha)\nnew_train_sample.head()","69e077bc":"from gensim.parsing.preprocessing import remove_stopwords\ndef remstpwords(text):\n    new_text = remove_stopwords(text)\n    return new_text\n\nnew_train_sample['TITLE'] = new_train_sample['TITLE'].apply(remstpwords)","3c0ddd0c":"stemmer = SnowballStemmer('english')\nnew_train_sample['TITLE'] = new_train_sample['TITLE'].apply(stemmer.stem)","8b2c26c2":"title_sent = new_train_sample['TITLE'].to_numpy().astype(str)\n\ntarget = new_train_sample['BROWSE_NODE_ID'].to_numpy()\n\ntrain_dataset= tf.data.Dataset.from_tensor_slices((title_sent, target))","e8e9140b":"max(new_train_sample['BROWSE_NODE_ID'])","cb6b8d23":"train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","b98aa516":"import os\nos.listdir()\n\nimport keras","a037332c":"filepath = \"zweights-{loss:.4f}.h5\"\ncheckpoint = keras.callbacks.ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, save_weights_only = True, mode = 'auto', period = 1)\n#CSVLogger makes CSV file of epoch , acc , loss , val_acc , val_loss and it will use to plot the graph of accuracy\nlog_csv =   keras.callbacks.CSVLogger('my_log_csv',separator = \",\",append = False)\n    \ncallbacks_list = [checkpoint]","330d2cb6":"first_model = conv_model.fit(train_dataset, epochs = 1, callbacks = callbacks_list) ","ae2249ef":"import time","8eef590b":"del train_dataset\ndel new_train_sample","e35bffd4":"time.sleep(60)","4881611d":"test_data = pd.read_csv(test_path , escapechar = \"\\\\\" , quoting = csv.QUOTE_NONE)\ntest_data.TITLE.fillna(test_data.DESCRIPTION, inplace=True)\ntest_data.TITLE.fillna(test_data.BULLET_POINTS, inplace=True)\ntest_data.TITLE.fillna(test_data.BRAND, inplace=True)\n# test_data = test_data.dropna()","698bc2a7":"test_data['TITLE'] = test_data['TITLE'].str.lower()\ntest_data['TITLE'] = test_data['TITLE'].apply(cleanHtml)\ntest_data['TITLE'] = test_data['TITLE'].apply(cleanPunc)\ntest_data['TITLE'] = test_data['TITLE'].apply(keepAlpha)\ntest_data['TITLE'].head()","c7c70ffd":"test_data['TITLE'] = test_data['TITLE'].apply(remstpwords)\ntest_data['TITLE'] = test_data['TITLE'].apply(stemmer.stem)","020d7a5c":"time.sleep(60)","42061817":"time.sleep(60)","17b224c2":"# conv_model.load_weights('.\/zweights-1.3521.h5')","c7ccaf5b":"LSTM_predictions  = conv_model.predict(test_data.TITLE.to_numpy().astype(np.str))\ntime.sleep(60)","4972a0dc":"LSTM_prediction_values = np.argmax(LSTM_predictions,axis = 1)\n# time.sleep(60)\nsubmission1 = pd.DataFrame()\nsubmission1.insert(0,'PRODUCT_ID',value = test_data.PRODUCT_ID)\nsubmission1.insert(1,'BROWSE_NODE_ID',LSTM_prediction_values)\nsubmission1.to_csv('sumbission.csv',index = False)","6ad2565a":"store_ids = submission1['BROWSE_NODE_ID']\n\nfor i in store_ids:\n    if(store_ids[i] in d):\n        store_ids[i] = store_ids[d[i]]\n    \nLSTM_prediction_values","6e51b627":"submission1.insert(1,'BROWSE_NODE_ID',LSTM_prediction_values)\nsubmission1.to_csv('sumbission6.csv',index = False)","1a794122":"submission1.shape","94ef6b01":"LSTM_predictions = 0","092f8c42":"sumission1 = 0","de5ca74d":"# MODEL"}}