{"cell_type":{"effcb33d":"code","74aa47a8":"code","b15bce9c":"code","42f45eee":"code","016f03c0":"code","7fefb27f":"code","633b34e3":"code","3972534f":"code","bb89f747":"code","c9d0bf97":"code","a26b6f3e":"code","57c5167d":"code","9a872401":"code","844d4f6c":"code","794bc299":"code","a79ad5dd":"code","d9f8d2d8":"code","7b328ee7":"code","9bfba535":"code","87d353b2":"code","0bbdd350":"code","001032c5":"code","fac19fc3":"code","37c35e8d":"code","938ffa4c":"code","72b52ef8":"code","b83fa416":"code","c2c4e57a":"code","27bcd63d":"code","6379c919":"code","1a64e492":"code","67bd51fb":"code","22b9c7cf":"code","c2460a88":"code","fe6e7815":"code","2a509b74":"code","1c242d67":"code","e191b094":"code","53d77de2":"code","50fa4227":"code","bb72ddce":"code","213f4703":"code","d8e44ca0":"code","b43ebdb9":"code","37f4dbce":"code","3bc3fcfd":"code","8f99941a":"code","bcd7c726":"code","9e1366af":"code","7d6407a3":"code","fc36f3d6":"code","79c0f694":"code","b501b9c2":"code","cbf17b11":"code","15b56217":"code","b37c3976":"code","8333c0b1":"code","8eb6f505":"code","a69b8879":"code","202f6d9a":"code","3394e818":"code","d89f8dd3":"code","a8365ff8":"code","f292fba7":"code","493b1922":"code","e06df854":"code","b75864c2":"code","6a160a5d":"code","79d59caf":"code","734ab896":"code","86547813":"code","057fc579":"code","deb0cc2a":"code","a22fef99":"code","fa358b15":"code","96116da8":"code","6b304e6c":"code","282c77bb":"code","d977fb12":"code","d8270cfa":"code","73dc02cc":"code","35e81833":"code","2d6915ce":"code","c8bb4769":"code","5a58ee53":"code","284217b5":"code","f05973f5":"code","f786b0ba":"code","849eff49":"code","eefa710c":"code","4304cc38":"code","6a160fed":"code","69d04fe2":"code","5f6dbeef":"code","3136ace8":"code","d4fe5076":"code","02df0aec":"code","f4d2bb3f":"code","36499cc8":"code","33b3c9dc":"code","78866460":"code","16e27f6b":"code","b539891c":"code","dab8e327":"code","8c537ad2":"code","86d97792":"code","855152b9":"code","9d9d770d":"code","43feaa03":"code","5d157c3a":"code","02140621":"code","d09ae994":"code","a5c762a1":"code","1e9bb2f5":"markdown","08367e7a":"markdown","c731eddd":"markdown","5b715983":"markdown","0bb5d698":"markdown","c67ca8e5":"markdown","bb425a25":"markdown","2147c125":"markdown","006e8540":"markdown","2709f345":"markdown","d3a533f5":"markdown","304c6787":"markdown","85eaa7f3":"markdown","7cc0f677":"markdown","77e73764":"markdown","d70f8174":"markdown","a937442b":"markdown","bf1888a6":"markdown","cf010f2a":"markdown","03050ced":"markdown","83e2947f":"markdown","f4c5b754":"markdown","1226bcec":"markdown","fa1b2783":"markdown"},"source":{"effcb33d":"# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Import packages\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","74aa47a8":"#import data\ntraining = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","b15bce9c":"#Look at training data - 12 populated columns\ntraining","42f45eee":"#Look at test data - 11 populated columns since there is no 'Survived' column\ntest","016f03c0":"#add new column to training named train_test - it will be populated with 1s\ntraining['train_test']= 1","7fefb27f":"#notice new column next to embarked\ntraining","633b34e3":"#add new column to test named train_test - it will be populated with 0s\ntest['train_test']=0","3972534f":"#notice new column next to embarked\ntest","bb89f747":"#add new column to test named survived - it will be populated with NaNs since that is what we are trying to determine\ntest['Survived']=np.NaN","c9d0bf97":"#new column next to train_test\ntest","a26b6f3e":"#concatenate the two data sets into one set\nall_data = pd.concat([training,test])","57c5167d":"#now look at the concatenated set \n#train_test now is a binary identifier where 1= from training set and 0= from test set\n#Survived has 0s or 1s if the row is from training and NaN if it is from test\n\nall_data","9a872401":"# %matplotlib inline embeds a static image of what we are trying to show in our notebook\n#.columns shows the columns we are using\n%matplotlib inline\nall_data.columns","844d4f6c":"# Understand nature of data with .info() and .describe()\n# Histograms and boxplots (For numericals)\n# Value counts (For categoricals)\n# Missing data (How are we going to remove or impute (substitute in for) these missing values)\n# Correlation between the metrics \n# Explore interesting themes, such as \n    # Did the wealthy survive?\n    # Was location a factor in determining survival?\n    # Age scatterplot with ticket price\n    # Young and wealthy variable?\n    # Total spent?\n# Feature engineerings\n# Preprocess data together or use a transformer?\n    # Use label for train and test\n# Scaling?\n\n# Model baseline\n# Model comparison with CV","794bc299":"# For Numeric data, explore by:\n    # Making histograms to understand distributions\n    # Corrplot\n    # Pivot table comparing surival rate across numeric variables\n    \n# For Categorical data\n    # Make bar charts to understand balance of classes\n    # Make pivot tables to understand relationship with survival","a79ad5dd":"# .info() prints a summary of our data types and the non-null counts\n# training has 891 rows. So any row that has less than 891 non-null values contains nulls\n#Age and cabin have a lot of nulls\n    #Think early about how to manage that\ntraining.info()","d9f8d2d8":"test.info()","7b328ee7":"# .describe() shows us the measures of central tendency for our data\n#Help us think about the data differntly and help us make associations\ntraining.describe()","9bfba535":"# look at numeric and categorical values separately \n# We will use 10 of the 12 rows from the original training table\n    # excluding name and passenger ID since we cannot learn from those\n\n#Numeric dataframe\n    # Age = Age in years\n    # Sibsp = numbers of siblings\/spouses aboard\n    # Parch = number of parents\/childrens aboard the Titantic\n    # Fare = Passenger fare\ndf_num = training [['Age', 'SibSp', 'Parch', 'Fare']]\n\n#Categorical datframe\n    # Survived = did they survive (binary)\n    # Pclass = ticket class 1=first 2=second 3=third\n    # Sex = Sex (male\/female)\n    # Ticket = ticket number\n    # Cabin = cabin number\n    # Embarked = port of embarkation (C = Cherbourg) (Q = Queenstown) (S = Southhampton)\ndf_cat = training [['Survived', 'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']]","87d353b2":"# for loop runs through all of the numeric variables and displays histograms for them all\n\nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","0bbdd350":"# Only Age is fairly normally distributed. Perhaps consider normalizing Fare.","001032c5":"# Prints a correlation matrix that shows how variables correlate with each other.\nprint(df_num.corr())","fac19fc3":"# Print a heatmap representation of the above matrix\nsns.heatmap(df_num.corr())","37c35e8d":"# When you are using regressions, these charts can be very important\n# They can help you avoid multicolinearity, when variables are too highly correlated\n# This has an overwhelming effect on the model","938ffa4c":"#Get the average numerical attributes of the groups that survived\/didn't survive\npd.pivot_table(training, index = 'Survived', values = ['Age', 'SibSp', 'Parch', 'Fare'])","72b52ef8":"#Create a table made of only people who did not survive\ndeadtable = training.copy()\ndeadrows = training[~(training['Survived'] < 1)].index \ndeadtable.drop(deadrows , inplace=True)","b83fa416":"#Describe the table, look the the means. They are the same as the means next to 0 in the pivot\ndeadtable.describe()","c2c4e57a":"# So based on the above metrics:\n    # Younger people may have a better chance of surviving?\n    # People who paid more may have a better chance of surviving?\n    # If you have parents maybe you have a better chance?\n    # If you have siblings you might have a worse chance?","27bcd63d":"# Visualize value counts for the categoricals\n    \nfor i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index, df_cat[i].value_counts()).set_title(i)\n    plt.show()","6379c919":"# More people died than survived\n# 3rd class most tickets, 1st class second most, 2nd class third most\n# About 2x men vs women\n# Most people are from Southmapton","1a64e492":"# Comparing each of these categorical variables to our dependent variable\n\n#Class breakout of survived\/didn't survive\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket', aggfunc = 'count'))\nprint()\n\n#Sex breakout of survived\/didn't survive\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket', aggfunc = 'count'))\nprint()\n\n#Origin point breakout of survived\/didn't survive\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket', aggfunc = 'count'))\nprint()","67bd51fb":"# Survival rate decreases as class gets worse\n# Women survived over men\n# Point of origin unclear, Cherbourg seems best","22b9c7cf":"#Look at cabin to figure out how to feature engineer\ntraining.Ticket.head(20)","c2460a88":"# Cabin is currently a letter than a number following it, like C85\n\n#This splits on spaces, which allows us to see if people have multiple cabins \n    #because their current value would be \"C85 C18 C29\" or something like that\n    #So this creates a new column called cabin_multiple\n    #Based on the cabin column of the training data \n    #Sets the value in this new column to =0 if there is nothing in the cabin category for this passenger\n    #If there is something, it splits it when it sees a ' ' (a space)\n    \n# .apply() arguments call the  calls the lambda function \n    # applies lambda function it to every row or column of the dataframe and returns a modified copy of the dataframe\n    # in this case it is applying to the Cabin column of the training dataframe\n\n#  lambda\n    # take lambda x.x\n        # lambda is the keyword\n        # first x is the bound variable\n        # second x is the body\n#pd.isna(x)   \n    # return 0 if there is an NaN value for that person \n    # Because this means they have no cabin\n\n# else len(x.split(' '))\n    # if the value is NOT NaN, split it on spaces\n    # then, get the length of the resulting list\n    # So if there was two spaces, it would be split into 3 values\n    # This means the person had 3 cabins and their value in this column would be 3\n    \ndf_cat.Cabin\ntraining['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\n# So now we have a column that counts number of cabins owned per passengers\n# .value_counts groups the values into bins\n# As we can see, 24 people had multiple cabins, 180 people had 1, 687 had 0\n\ntraining['cabin_multiple'].value_counts()","fe6e7815":"# 238 cabins in total\ntraining['cabin_multiple'].sum()","2a509b74":"# Make a pivot table from training \n\n\n# index = the rows\n    # Survived is binary, so rows are 0 and 1\n    \n# columns = the columns\n    # cabin_multiple has values 0-4 so these are the column headers\n\n# values = how to populate the cells\n    # It counts the number of people who survived\/died by summing the counts of tickets\n    # Then categorizes based on their number of cabins\n    \n#aggfunc = 'count' \n    # makes it so the cells are populated by counting the ticket values who match the row\/column criteria\n\npd.pivot_table(training, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket', aggfunc ='count')","1c242d67":"# Create a new column called cabin_adv that categorizes passengers based on what the first letter of their cabin is\n\n# .apply() arguments call the  calls the lambda function \n    # applies lambda function it to every row or column of the dataframe and returns a modified copy of the dataframe\n    # in this case it is applying to the Cabin column of the training dataframe\n\n#  lambda\n    # take lambda x.x\n        # lambda is the keyword\n        # first x is the bound variable\n        # second x is the body\n        \n# str(x)[0]\n    #str(x) converts the value in cabin to a string\n    # [0] means that you take the first element in the string, in this case the letter\n\n\ntraining['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])","e191b094":"# Look at the new column\ntraining['cabin_adv'].head(20)","53d77de2":"# Break new column into bins by letter\n# mostly n which stands for null\ntraining['cabin_adv'].value_counts()","50fa4227":"#get survival rate by cabin letter\n\npd.pivot_table(training, index = 'Survived', columns = 'cabin_adv', values = 'Name', aggfunc ='count')","bb72ddce":"# Survival rate if you had a cabin was much better than if you did not have a cabin","213f4703":"# Understand ticket values better\n# Numeric vs non numeric\n# isnumeric evaluates if a string contains ONLY numeric characters\n\ntraining['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntraining['numeric_ticket'].value_counts()","d8e44ca0":"#So 661 tickets contrain only numeric characters","b43ebdb9":"# find out if there is a difference in survival rate for numeric vs non-numeric tickets\n\npd.pivot_table(training, index = 'Survived', columns = 'numeric_ticket', values = 'Ticket', aggfunc ='count')","37f4dbce":"#Look at ticket column to figure out how to feature engineer\ntraining.Ticket.head(20)","3bc3fcfd":"# new columns ticket_letters\n#returns a lowercase version of just the letter codes (which may have numbers at the end) in the ticket\n\n\n# .apply() arguments call the  calls the lambda function \n    #applies lambda function it to every row or column of the dataframe and returns a modified copy of the dataframe:\n\n#  lambda\n    # take lambda x.x\n        # lambda is the keyword\n        # first x is the bound variable\n        # second x is the body\n        \n# .join\n    #string_name.join(iterable) \n        # string_name \n            #It is the name of string in which joined elements of iterable will be stored.\n        \n        \n# .split() arguments\n    # .split(separator, maxsplit)\n        #separator = where splits occur\n        #maxsplit = maximum # of splits\n            #[] = which part of the value you keep. \n                #So if you put [0], you keep everything before the split\n                # if you put [1], you keep everything after the split\n        \n# .replace(arguments)\n    # can take a maximum of 3 arguments\n    #old - old substring you want to replace\n    #new - new substring which will replace the old substring\n    #count (optional) - the number of times you want to replace the old substring with the new substring\n\n#EXPLANATION OF FUNCTION BELOW\n    \n# So this applies the lambda function to everything in the Ticket columns\n\n# Lambda function is is made up of a join function\n\n# The join function joins the result of a split function and 2 replace function\n\n# The split function splits on ' ' \n    # because it contains [:-1], it returns all elements of the sequence except for the last once \n\n#Now the replace function replaces what the split function outputted\n    #it replaces '.' with '' and '\/' with ''\n    \n#this lambda is only applied if the output of the first x.split function is greater than 0\n    # ie if there is a space in the ticket number. \n    # If there isn't a space, there aren't letters preceding numbers\n    \ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace\n('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)","8f99941a":"#look at our new column\ntraining.ticket_letters.head(20)","bcd7c726":"# none of these are common enough\n# could maybe aggregate them if you wanted to go further\npd.set_option (\"max_rows\", None)\ntraining['ticket_letters'].value_counts()","9e1366af":"#survival rate across different ticket types \n# not conclusive\npd.pivot_table(training,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count')","7d6407a3":"# feature engineering based on person's title\n\n#Look at first 20\n#notice that we could split out their title from this\ntraining.Name.head(20)","fc36f3d6":"# .apply() arguments call the  calls the lambda function \n    #applies lambda function it to every row or column of the dataframe and returns a modified copy of the dataframe:\n\n#  lambda\n    # take lambda x.x\n        # lambda is the keyword\n        # first x is the bound variable\n        # second x is the body\n        \n# .split() arguments\n    # .split(separator, maxsplit)\n        #separator = where splits occur\n        #maxsplit = maximum # of splits\n        #[] = which part of the value you keep. So if you put [0], you keep everything before the split\n            # if you put [1], you keep everything after the split\n            \n#.strip()\n    #removes characters from the right and left of a string\n    \n#So this applies the lambda function to everything in the columns\n#lambda function is made up of a .split function that splits on ',' and keeps the value at the [1] index\n    #So that would be everything after the ,\n    #then it splits on the period and keeps everything befor the period\n#.strip() removes everything before and after the string, in this case the spaces are removed\n    \n\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","79c0f694":"# Look at new column\ntraining.name_title.head(20)","b501b9c2":"# Get value counts from new column\n\ntraining['name_title'].value_counts()","cbf17b11":"# Survival rates across different titles\npd.pivot_table(training,index='Survived',columns='name_title', values = 'Ticket', aggfunc='count')","15b56217":"# Women survived at a far higher rate\n# All reverends died\n# Captain went down with the ship","b37c3976":"# count number of unique titles\nn=len(training['name_title'].unique())\nn","8333c0b1":"#create all categorical variables that we did above for both training and test sets\n\nall_data['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace\n('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","8eb6f505":"# Check for nulls\nall_data.info()","a69b8879":"# impute nulls for continuous data\n    # Continuous data is data that can take any value\n\n# fill NaN values with the median of the column\nall_data.Age = all_data.Age.fillna(training.Age.median())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())","202f6d9a":"# Drop null 'embarked' rows\nall_data.dropna(subset = ['Embarked'], inplace = True)","3394e818":"# Make sure that they are gone\nall_data.info()","d89f8dd3":"# Do the same for test\ntest.dropna(subset = ['Embarked'], inplace = True)\ntest.Fare = test.Fare.fillna(training.Fare.median())","a8365ff8":"test.info()","f292fba7":"# Hist of SibSp for comparison\nall_data['SibSp'].hist()","493b1922":"# log norm of sibsp\n\n# np.log()\n    # is a mathematical function that helps user to calculate Natural logarithm of x \n        #x is the column SibSp in this case\n    \nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\n# Visualize norm_sibsp - outliers are gone\nall_data['norm_sibsp'].hist()","e06df854":"# Hist of Fare for comparison\nall_data['Fare'].hist()","b75864c2":"# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()","6a160a5d":"# Convert class to categorical for pd.get_dummies\nall_data.Pclass = all_data.Pclass.astype(str)","79d59caf":"# Create dummy variables for categories\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked',\n'cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])","734ab896":"all_dummies.head(20)","86547813":"# set up X_train \n    #First have it only keep rows where the value in train_test is equal to 1\n    # then drop the train_test column\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\n\n# set up X_test\n    #First have it only keep rows where the value in train_test is equal to 0\n    # then drop the train_test column\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)","057fc579":"# set up y_train\n    # have it only keep rows where the value in train_test is equal to 1\ny_train = all_data[all_data.train_test ==1].Survived\ny_train.shape","deb0cc2a":"# Scale data\n    # Scaling data helps to normalize data within a particular range\n    \n# import packages\nfrom sklearn.preprocessing import StandardScaler\n\n# set scale as = to StandardScaler\nscale = StandardScaler()\n\n#Standard scaler makes it so that the mean value of the data is 0 and standard deviation = 1 \n\n#make a new variable for scaled data\nall_dummies_scaled = all_dummies.copy()\n\n#scale is the variable we declared that calls standard scaler\n# then fit to the data, then transform it\n\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]=scale.fit_transform(all_dummies_scaled[['Age','SibSp',\n                                                                                                'Parch','norm_fare']])\n","a22fef99":"# set up X_train_scaled\n    #First have it only keep rows where the value in train_test is equal to 1\n    # then drop the train_test column\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\n\n# set up X_test_scaled\n    #First have it only keep rows where the value in train_test is equal to 0\n    # then drop the train_test column\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test ==1].Survived","fa358b15":"# Import Packages \n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","96116da8":"# Naive Bayes \n\ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","6b304e6c":"# logistic regession - #1 (unscaled)\n\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","282c77bb":"# logistic regession - #2 (scaled)\n\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d977fb12":"# decision tree - #1 (unscaled)\n\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d8270cfa":"# decision tree - #2 (scaled)\n\n\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","73dc02cc":"# K Nearest Neighbor - #1 (unscaled)\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","35e81833":"# K Nearest Neighbor  - #2 (scaled)\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","2d6915ce":"# Random Forecast Classifier - #1 (unscaled)\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","c8bb4769":"# Random Forecast Classifier - #2 (scaled)\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","5a58ee53":"# Support Vector Classification \n\nsvc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","284217b5":"# Xtreme Gradient Boosting \n\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1, use_label_encoder=False)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","f05973f5":"#Voting classifier takes all of the inputs and averages the results.\n\n#For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote.\n    #For this, you generally want odd numbers\n    \n#A \"soft\" classifier averages the confidence of each of the models. \n    # If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),\n                                            ('svc',svc),('xgb',xgb)], voting = 'soft') ","f786b0ba":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","849eff49":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)","eefa710c":"basic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}","4304cc38":"base_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","6a160fed":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","69d04fe2":"# Simple performance reporting function\n\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","5f6dbeef":"# No tuning for Naive Bays","3136ace8":"# logistic regression tuned\n\n# Set up the logistic regression\nlr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\n# Grid search allows you to put in a number of parameters, and it will output the best score and the best parameters\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","d4fe5076":"# No tuning for decision tree","02df0aec":"# K nearest Neighbor tuned\n\n# Set up the K nearest neighbor\nknn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\n\n# Grid search allows you to put in a number of parameters, and it will output the best score and the best parameters\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","f4d2bb3f":"# Support Vector Classifier \n\n# Set up the Vector classifier\nsvc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\n\n# Grid search allows you to put in a number of parameters, and it will output the best score and the best parameters\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","36499cc8":"\"\"\"\n#Because the total feature space is so large, use a randomized search to narrow down the parameters for the RF model. \n\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n\n# Random Search\nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')\n\"\"\"","33b3c9dc":"# So now we have best parameters from the random search. Use them as a basis below. Our goal is to improve on 83.24%\n# Best Parameters: {'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', \n                    #'max_depth': 75, 'bootstrap': True}","78866460":"# Implement best parameters- model improves slightly \n\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [999, 1000, 1001],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [74, 75, 76],\n                                  'max_features': ['auto'],\n                                  'min_samples_leaf': [1,2,3],\n                                  'min_samples_split': [4, 5, 6]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","16e27f6b":"# Rank the features by importance - how much did they influence the model?\n\nbest_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","b539891c":"\"\"\"\n# Use random search to derive best parameters and implement below\n\nxgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n    'subsample': np.arange(0.05, 1.01, 0.05),\n    'min_child_weight': [0,.01,0.1,1,10,100],\n    'n_jobs': [1], \n    'verbosity': [0]\n}\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')\n\"\"\"","dab8e327":"# So now we have best parameters from the random search. Use them as a basis below. Our goal is to improve on 84.59%\n# Best Parameters: {'verbosity': 0, 'subsample': 0.2, 'n_jobs': 1, 'n_estimators': 100, 'min_child_weight': 0, 'max_depth': 10, 'learning_rate': 0.1}","8c537ad2":"# Implement optimized parameters - Model Improves\n\nxgb = XGBClassifier(random_state = 1,use_label_encoder =False)\n\nparam_grid = {\n    'n_estimators': [99,100,101],\n    'max_depth': [9, 10, 11],\n    'learning_rate': [0.05, 0.1, 0.15],\n    'subsample': np.arange(0.05, 1.01, 0.05),\n    'min_child_weight': [0,.01,0.1],\n    'n_jobs': [1], \n    'verbosity': [0]\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb,'XGB')","86d97792":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\nxgb_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('xgb_submission3.csv', index=False)","855152b9":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())","9d9d770d":"#in a soft voting classifier you can weight some models more than others. I used a grid search to explore different weightings\n# No new results\nparams = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_weight = vote_weight.fit(X_train_scaled,y_train)\nclf_performance(best_clf_weight,'VC Weights')\nvoting_clf_sub = best_clf_weight.best_estimator_.predict(X_test_scaled)","43feaa03":"#Fit our classifiers to the data\nvoting_clf_hard.fit(X_train_scaled, y_train)\nvoting_clf_soft.fit(X_train_scaled, y_train)\nvoting_clf_all.fit(X_train_scaled, y_train)\nvoting_clf_xgb.fit(X_train_scaled, y_train)\n\n#Once fitted, predict\nbest_rf.fit(X_train_scaled, y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\ny_hat_rf = best_rf.predict(X_test_scaled).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test_scaled).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test_scaled).astype(int)","5d157c3a":"#convert output to dataframe \nfinal_data = {'PassengerId': test.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': test.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data=final_data_comp)","02140621":"#track differences between outputs \ncomparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)","d09ae994":"comparison.difference_hard_all.value_counts()","a5c762a1":"#prepare submission files \nsubmission.to_csv('submission_rf.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft.csv', index=False)\nsubmission_4.to_csv('submission_vc_all.csv', index=False)\nsubmission_5.to_csv('submission_vc_xgb2.csv', index=False)","1e9bb2f5":"### Number of cabins owned","08367e7a":"# Model Baseline Performance vs Tuned Performance","c731eddd":"|Model|Baseline|Tuned Performance|\n|-----|--------|-----------------|\n|Naive Bayes| 72.22%| \n|Logistic Regression| 82.23%  | 82.79%\n|Decision Tree| 78.75%  | \n|K Nearest Neighbor| 81.44%  | 82.79%\n|Random Forest| 80.43% | 83.36%\n|Support Vector Classifier| 83.36%  | 83.36% \n|Xtreme Gradient Boosting| 81.89% | 85.04%\n","5b715983":"## Model Building (Baseline Validation Performance)","0bb5d698":"1. Drop null values from Embarked (there are 2) and fare (there is 1)  \n\n\n2. Include only relevant variables  \n    a. We can exclude things like name and passenger ID so our model doesn't have to deal with unnecessary features  \n    b. So we are left with  \n        1. Pclass (passenger class)  \n        2. Sex   \n        3. Age  \n        4. SibSp (Count of siblings and\/or spouses aboard)  \n        5. Parch (Count of parents and\/or childrens aboard)  \n        6. Fare (cost in dollar)  \n        7. Embarked (Point of origin)  \n        8. Cabin_adv (first letter of cabin or n)  \n        9. Cabin_multiple (Count of cabins owned)  \n        10. numeric_ticket (Binary column evaluating whether ticket ID is strictly numerical)  \n        11. name_title (Title of passenger ie 'Mr.' 'Mrs.' 'Dr.' etc)   \n        \n3. Do categorical transforms on all data.  \n    a. Usually we would use a transformer. But with this approach we can ensure training and test sets have the same columns.  \n    b. We may also be able to infer something about the shape of the data through this method  \n    c. A categorical transform is necessary because in the case of categorical variables, the model cannot take them all in in one column  \n        1. For example, in the case of name_title there are 17 unique values, as we calculated above   \n        2. There needs to be one column for each category, with a 0 if the passenger wasn't that category and a 1 if they were that category  \n    d. In this case, we are doing this for all of the data.   \n        1. But in real world data science, you would only want to train on the training data not the whole set\n         \n4. Impute data with mean for fare and age  \n\n5. Normalized fare using logarithm to give more semblance of a normal distribution  \n\n6. Scaled data 0-1 with standard scalar\n    ","c67ca8e5":"1) Experimented with a hard voting classifier of three estimators (KNN, SVC, RF) (81.1%)\n\n2) Experimented with a soft voting classifier of three estimators (KNN, SVC, RF) (81.3%) \n\n3) Experimented with soft voting on all estimators performing better than 80% except xgb (KNN, RF, LR, SVC) (82.8%)\n\n4) Experimented with soft voting on all estimators including XGB (KNN, SVC, RF, LR, XGB) (83.1%)","bb425a25":"# Model Baseline Performance \n\n\n|Model|Baseline|\n|-----|--------|\n|Naive Bayes| 72.22%|\n|Logistic Regression| 82.23%  | \n|Decision Tree| 78.75%  | \n|K Nearest Neighbor| 81.44%  |\n|Random Forest| 80.43% |\n|Support Vector Classifier| 83.36%  | \n|Xtreme Gradient Boosting| 81.89% |\n|Soft Voting Classifier - All Models|82.34%|\n","2147c125":"# Titanic Kaggle Project\n\n**The purpose of this project is to follow and learn from Ken Jee's Titantic Project Example Lecture**","006e8540":"### Ticket Lettering Conventions","2709f345":"## Feature engineering","d3a533f5":"### Based on Person's title","304c6787":"## Setup","85eaa7f3":"**The above pivot table shows the mean value for each category broken into whether they survived or not. Look below for proof**","7cc0f677":"### Is ticket numeric","77e73764":"## Project Planning","d70f8174":"## Process","a937442b":"1. Understand the Shape of the data through histograms, box plots, etc.  \n2. Clean the Data  \n3. Explore the data  \n4. Feature engineering\n5. Data preprocessing for model  \n6. Basic model building  \n7. Model tuning  \n8. Ensemble model building  \n9. Results","bf1888a6":"### Letter Before Cabin Identifier","cf010f2a":"## Model Additional Ensemble Approaches","03050ced":"Before going further, evaluate how different models perform with default parameters. Here, we are going to try the following models using 5 fold cross validation to get a baseline. By getting a baseline, we can determine how much tuning improves each of the models. The fact that a model has a high baseline on this validation does NOT mean that it will do better on the eventual test set.","83e2947f":"## Light Data Exploration","f4c5b754":"## Model Tuned Performance","1226bcec":"**Interpretation of degrees of correlation:**\n\nPerfect: If the value is near \u00b1 1, then it said to be a perfect correlation: as one variable increases, the other variable tends to also increase (if positive) or decrease (if negative).\n\nHigh degree: If the coefficient value lies between \u00b1 0.50 and \u00b1 1, then it is said to be a strong correlation.\n\nModerate degree: If the value lies between \u00b1 0.30 and \u00b1 0.49, then it is said to be a medium correlation.\n\nLow degree: When the value lies below + .29, then it is said to be a small correlation.\n\nNo correlation: When the value is zero.","fa1b2783":"## Data Preprocessing for Model"}}