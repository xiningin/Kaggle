{"cell_type":{"8d4cda21":"code","391e64fc":"code","c28fbcca":"markdown","d5547b8f":"markdown","ea7801f4":"markdown","0b35c095":"markdown","074234ac":"markdown","bec71af8":"markdown","9fc09a44":"markdown","3e8db70c":"markdown","0765d20d":"markdown","c78c8eeb":"markdown","161aadb4":"markdown","07d798b5":"markdown","e519545d":"markdown","f53f3556":"markdown","11b83f41":"markdown","b346f215":"markdown","dbeb034f":"markdown","4840ea8e":"markdown"},"source":{"8d4cda21":"from IPython.display import YouTubeVideo\nYouTubeVideo('iVsteYEF0ko', width=800, height=450)","391e64fc":"YouTubeVideo('dGcsHMXbSOA', width=800, height=450)","c28fbcca":"To bring more interactivity to the search experience, I decided to embed the tSNE visualization into the search results (this is available on Desktop view only).\n\nOn each result page, the points on the plot (on the left) represent the same search results (on the right): this gives an idea on how results relate to each other in a semantic space.\n\n<div align=\"center\">\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*v1OY0kIQJdzyDx00ydfyMg.png\">\n<\/div>","d5547b8f":"- I decided, finally, and for fun mainly, to represent the articles in an interactive 2D map to provide a visual interpretation of the clusters and their separability. To do this, I applied a tSNE dimensionality reduction on the PCA components. Nothing fancy.\n\n<div align=\"center\">\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*RFSYt1RaT31y1mOab1g0WA.png\">\n<\/div>","ea7801f4":"## Spread the word! Share Corona Papers with your community\n\nIf you made it this far, I\u2019d really want to thank you for reading!\n\nIf you find Corona Papers useful, please share this link: https:\/\/covid19.ai2prod.com with your community.\n\nIf you have a feature request for improvement or if you want to report a bug, don\u2019t hesitate to contact me.\n\n\nI\u2019m looking forward to hearing from you!","0b35c095":"> ### Front-end\n\nThe front-end interface is built using Material-UI, a great React UI [library](http:\/\/material-ui.com\/) with a variety of well-designed and robust components.\n\nIt has been used to design the different pages, and more specifically the search page with its collapsable panel of search filters :\n\n- publication date\n- publishing company (i.e. the source)\n- journal name\n- peer-reviewed articles\n- h-index of the journal\n- the topics\n\n<div align=\"center\">\n   <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*IBsgPuNG5Eyw4uBK5EMH2w.png\">\n<\/div>\n","074234ac":"> ### Back-end\n \nAt its core, it uses Elasticsearch to perform full-text queries, complex aggregation and sorting. When you type in a list of keywords, for example, Elasticsearch matches them first with the titles, abstract, and eventually the author names.\n\n\nHere\u2019s an example of a query:\n\n\n<div align=\"center\">\n   <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*7WQDsOIgnvBMqksgQv4nMw.png\">\n<\/div>\n<br>\n<br>\n\nAnd here\u2019s a second one that matches an author\u2019s name:\n\n<br>\n<br>\n<div align=\"center\">\n   <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*KkclSa0G2lEfoaaqOA11WA.png\">\n<\/div>\n\n","bec71af8":"In this post, I\u2019ll:\n- **Go through the main functionalities of Corona Papers and emphasize what makes it different from other search engines**\n- **Share the code of the data preprocessing and topic detection pipelines so that it can be applied in similar projects**","9fc09a44":"If you\u2019re interested in the data processing and topic extraction pipelines, you can look at the code in my Github [repository](https:\/\/github.com\/ahmedbesbes\/covidbert-topic-mining).\nYou\u2019ll find two notebooks:\n- **1-data-consolidation.ipynb:**\n    - consolidates the CORD database with external metadata from Altmetric, Scimago Journal, and CrossRef\n    - generates CovidBERT embeddings from the titles and excerpts\n\n\n- **2-topic-mining.ipynb:**\n    - generates topics using CovidBERT embeddings\n    - select relevant keywords for each cluster","3e8db70c":"## How to use CovidBERT in practice\n\n\nUsing the **sentence_transformers** package to load and generate embedding from CovidBERT is as easy writing these few lines\n\n<div align=\"center\">\n   <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*Rl-eEcWPx5Z7Hw-W6h7T5w.png\">\n<\/div>","0765d20d":"Because accessibility matters, I aimed at making Corona Papers a responsive tool that researchers can use on different devices. Using Material-UI helped us design a clean and simple interface.\n\n<div align=\"center\">\n   <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*qujEaCIgwTy37JT6ppZA9w.png\">\n<\/div>\n","c78c8eeb":"*original post [here](https:\/\/medium.com\/@ahmedbesbes\/releasing-corona-papers-an-ai-powered-search-engine-to-explore-covid-19-research-4b18d1259491)*\n\nAfter participating in this great challenge with @marwandebbiche and @pmlee2017, we took the liberty to bring our software engineering expertise and build a tool. \n\nWithout further ado, meet [Corona Papers](https:\/\/covid19.ai2prod.com)  \ud83c\udf89\n","161aadb4":"## What is Corona Papers?\n\nCorona Papers is a search engine that indexes the latest research papers about COVID-19.\n\nIf you\u2019ve just watched the video, the following sections will dive into more details. If you haven't watched it yet, all you need to know is here.","07d798b5":"### **2 \u2014 Automatic topic extraction using a language model \ud83e\udd16**\n\nCorona Papers **automatically tags each article with a relevant topic using a machine learning pipeline**.\n\nThis is done using [CovidBERT](https:\/\/huggingface.co\/gsarti\/covidbert-nli): a state-of-the-art language model fine-tuned on medical data. With the great power of the Hugging Face library, using this model is pretty easy.\n\n<div align=\"center\">\n<img src=\"https:\/\/miro.medium.com\/max\/350\/1*rr1CnqBV4_xxDH8oOZigeg.png\">\n<\/div>\n\n\nLet\u2019s break down the topic detection pipeline for more clarity:\n\n- Given that abstracts represent the main content of each article, they\u2019ll be used to discover the topics instead of the full content.<br>\n  They are first embedded using CovidBERT. This produces vectors of **768 dimensions**. <br>\n  \u2014 Note that CovidBERT **embeds each abstract as a whole** so that the resulting vector encapsulates the semantics of the full document.\n  \n\n- Principal Component Analysis (PCA) is performed on these vectors to reduce their dimension in order to remove redundancy and speed-up later computations. **250 components** are retained to ensure 95% of the explained variance.\n\n\n- KMeans clustering is applied on top of these PCA components in order to discover topics. After many iterations on the number of clusters, **8 seemed to be the right choice**. <br>\n  \u2014 There are many ways to select the number of clusters. I personally looked at the silhouette plot of each cluster (figure below). <br>\n  \u26a0\ufe0f An assumption has been made in this step: each article is assigned a unique topic, i.e. the dominant one. If you're looking at generating a mixture of topics per paper, the right way would be to use Latent Dirichlet Allocation. **The downside of this approach, however, is that it doesn\u2019t integrate COVIDBert embeddings**.\n  \n\n- After generating the clusters, I looked into each one of them to understand the underlying sub-topics. I first tried a word-count and [TF-IDF](https:\/\/fr.wikipedia.org\/wiki\/TF-IDF) scoring to select the most important keywords per cluster. **But what worked best here was extracting those keywords by performing an LDA on the documents of each cluster.** This makes sense because each cluster is itself a collection of sub-topics. <br>\n  Different coherent clusters were discovered. Here are some examples, with the corresponding keywords (the cluster names have been manually attributed on the basis of the keywords)\n  \n<div align=\"center\">\n<img src=\"https:\/\/miro.medium.com\/max\/552\/1*pRIQIr6AApNsxy3o-R2iXw.png\">\n<img src=\"https:\/\/miro.medium.com\/max\/552\/1*HU9NDoexQ5ixGRFNio0LDQ.png\">\n<img src=\"https:\/\/miro.medium.com\/max\/552\/1*pLO7_8YUW03HVJC__7WnZg.png\">\n<\/div>\n<br>","e519545d":"### **1 \u2014 A curated list of papers and rich metadata \ud83d\udcc4**\n\nCorona Papers indexes the COVID-19 Open Research Dataset (CORD-19) provided by [Kaggle](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge). This dataset is a regularly updated resource of over 138,000 scholarly articles, including over 69,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.\n\n\nCorona Papers also integrates additional metadata from [Altmetric](https:\/\/www.altmetric.com\/) and Scimgo [Journal](https:\/\/www.scimagojr.com\/journalrank.php) to account for the online presence and academic popularity of each article. More specifically, it fetches information such as the number of shares on Facebook walls, the number of posts on Wikipedia, the number of retweets, the [H-Index](https:\/\/en.wikipedia.org\/wiki\/H-index) of the publishing journal, etc.\n\n\nThe goal of integrating this metadata is to consider each paper\u2019s impact and virality among the community, both the academic and non-academic ones.\n\n<div align=\"center\">\n<img width=\"100%\" src=\"https:\/\/miro.medium.com\/max\/1400\/1*jnnO6sc6O8f9zAT-bPhIAw.png\">\n<\/div>","f53f3556":"### **3 \u2014 Recommendation of similar papers **\n\nOnce you click on a given paper, Corona Papers with show you detailed information about it such as the title, the abstract, the full content, the URL to the original document, etc.\n\nBesides, it proposes a selection of similar articles that the user can read and bookmark.\n\nThese articles are based on a similarity measure computed on CovidBert embeddings.\n\nHere are two examples:\n\n<div align=\"center\">\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*lJQnvJOSV2WZ-8JJyD6wuQ.png\">\n<\/div>\n\n<div align=\"center\">\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*Wyi57QvG2NPUvmtu0Vpp5w.png\">\n<\/div>\n\n\n","11b83f41":"## What key lessons can be learned from this project?\n\n\nBuilding Corona Papers has been a fun journey. It was an opportunity to mix up NLP, search technologies, and web design. This was also a playground for a lot of experiments.\n\nHere are some technical and non-technical notes I first kept to myself but am now sharing with you:\n\n\n- Don\u2019t underestimate the power of Elasticsearch. This tool offers great customizable search capabilities. Mastering it requires a great deal of effort but it\u2019s a highly valuable skill. <br>\n  Visit the official [website](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/getting-started.html) to learn more.\n\n\n- Using language models such as CovidBERT provides efficient representations for text similarity tasks.\n  If you\u2019re working on a text similarity task, look for a language model that is pretrained on a corpus that resembles yours. Otherwise, train your own language model.\n  There are lots of available models [here](https:\/\/huggingface.co\/models).\n\n\n- Docker is the go-to solution for deployment. Pretty neat, clean, and efficient to orchestrate the multiple services of your app.\n  Learn more about Docker [here](https:\/\/docker-curriculum.com\/).\n\n\n- Composing a UI in React is really fun and not particularly difficult, especially when you play around with libraries such as Material-UI.\n  The key is to first start by sketching your app, then design individual components separately, and finally assemble the whole thing.\n  This took me a while to grasp because I was new to React, but here are some tutorials I used:\n  \n    - React official [website](https:\/\/reactjs.org\/tutorial\/tutorial.html)\n    - Material UI official [website](https:\/\/material-ui.com\/) where you can find a bunch of components\n    - I also recommend this guy\u2019s channel. It\u2019s awesome, fun, and quickly gets you to start with React fundamentals.\n","b346f215":"> ### Cloud and DevOps\n\nI deployed Corona Papers on AWS using docker-compose.","dbeb034f":"- Text clustering is not a fully automatic process. You\u2019ll have to fine-tune the number of clusters almost manually to find the right value. This requires monitoring some metrics and qualitatively evaluating the results.\n\nOf course, there are things I wish I had time to try like setting up CI-CD workflow with Github actions and building unit tests. If you have experience with those tools, I\u2019d really appreciate your feedback.","4840ea8e":"### **4 \u2014 A stack of modern web technologies \ud83d\udcf2 **\n\nCorona Papers is built using modern web technologies\n\n<div align=\"center\">\n   <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*yf_yfEYRxpoP8j8W8t5iMg.png\">\n<\/div>\n"}}