{"cell_type":{"88b2e883":"code","77b0de62":"code","a6d0c39f":"code","c0d89761":"code","44874048":"code","55e13a81":"code","0a37e08a":"code","2d312a59":"code","e16567db":"code","d853e2c1":"code","2f186c76":"code","2364c4f1":"code","5692c484":"code","24df3ea0":"code","1c784efa":"code","1f2ab00b":"code","971aeafd":"code","52859f2d":"code","9247442e":"code","67e2bfde":"code","d0d0928c":"code","aa174f8f":"code","569d20cf":"code","a09e3438":"code","788564de":"code","f6e4b504":"code","6df0b5ca":"code","fca8c51b":"code","5dcf77e3":"code","87986e01":"code","50adc911":"code","8e206648":"code","7a74f96d":"code","3f4fc2cb":"code","8c3b1854":"code","745503c6":"code","40a88a48":"code","5d22e8a0":"code","d6c4b7a9":"code","93387156":"code","2d3d97a1":"code","c0e50456":"code","cfb43365":"markdown","0aa444c0":"markdown","e100517f":"markdown","f6258fff":"markdown","2677129d":"markdown","a77ebc84":"markdown","728d4af2":"markdown","8a80acaf":"markdown","1758dd2a":"markdown","6bed62da":"markdown","3cff609a":"markdown","4a1bfb37":"markdown","7608f890":"markdown","c76e91f1":"markdown","a4f7a992":"markdown"},"source":{"88b2e883":"import warnings\nwarnings.filterwarnings('ignore', 'SettingWithCopyWarning')","77b0de62":"import random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport datatable as dt\nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn import metrics, model_selection\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.ensemble import StackingClassifier\n\nfrom eli5.sklearn import PermutationImportance\nfrom IPython.display import display, Markdown, Latex","a6d0c39f":"# matplotlib\nplt.rc('font', size=15)\nplt.rc('axes', titlesize=18)  \nplt.rc('xtick', labelsize=10)  \nplt.rc('ytick', labelsize=10)\n\n# seaborn\nsns.set(font_scale = 1.2)\nsns.set_style(\"whitegrid\")","c0d89761":"class Config:\n    RANDOM_STATE = 2021\n    TRAIN_DATA = '..\/input\/tabular-playground-series-sep-2021\/train.csv'\n    TEST_DATA = '..\/input\/tabular-playground-series-sep-2021\/test.csv'\n    SUBMISSION = '..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv'    \n    SUBMISSION_FILE = 'submission.csv'\n    TEST_SIZE = 0.3\n    SAMPLE_FRAC = 0.02\n    \n    INDEX = 'id'\n    TARGET = 'claim'\n    FEATURES = ['f{}'.format(i) for i in range(1, 119)]\n    COLUMNS = FEATURES + [TARGET]\n    \n    @staticmethod\n    def set_seed():\n        random.seed(Config.RANDOM_STATE)\n        np.random.seed(Config.RANDOM_STATE)\n\nConfig.set_seed()","44874048":"%%time\n\ntrain_data = pd.read_csv(Config.TRAIN_DATA).set_index(Config.INDEX)\ntrain_data","55e13a81":"%%time\n\ntest_data = pd.read_csv(Config.TEST_DATA).set_index(Config.INDEX)\ntest_data","0a37e08a":"memory_usage = train_data.memory_usage(deep=True) \/ 1024 ** 2\nprint('Memory (train): {:.2f} MB'.format(memory_usage.sum()))\n\nmemory_usage = test_data.memory_usage(deep=True) \/ 1024 ** 2\nprint('Memory (test) : {:.2f} MB'.format(memory_usage.sum()))","2d312a59":"# reduce memory usage\ntrain_data[Config.FEATURES] = train_data[Config.FEATURES].astype(np.float32) \ntest_data[Config.FEATURES] = test_data[Config.FEATURES].astype(np.float32)","e16567db":"memory_usage = train_data.memory_usage(deep=True) \/ 1024 ** 2\nprint('Memory (train): {:.2f} MB'.format(memory_usage.sum()))\n\nmemory_usage = test_data.memory_usage(deep=True) \/ 1024 ** 2\nprint('Memory (test) : {:.2f} MB'.format(memory_usage.sum()))","d853e2c1":"dtypes = pd .DataFrame({\n    'feature': train_data.columns,\n    'dtype': train_data.dtypes\n}).set_index('feature')\n\ndtypes","2f186c76":"train_data[Config.COLUMNS].describe().T.style.bar(\n    subset=['mean'], color='Bules'\n).background_gradient(subset=['50%'], cmap='Blues')","2364c4f1":"fig, ax = plt.subplots(1, 1, figsize=(7, 5))\nax = sns.countplot(\n    x='claim', \n    palette='Blues_r',\n    data=train_data\n)\n\nfig.show()","5692c484":"def plot_pdf(\n    data:pd.DataFrame, \n    feature:str, \n    title='Histplot',\n    bins=50,\n    ax=None):\n    \"\"\" Plots the estimated pdf. \n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    # plot pdf\n    sns.histplot(\n        data=data[feature], \n        bins=bins,\n        palette='Blues_r',\n        shrink=.8,\n        legend=True,\n        ax=ax\n    )\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('Count')\n    \n    return ax","24df3ea0":"def plot_boxplot(\n    data:pd.DataFrame, \n    feature:str, \n    title='Boxplot',\n    ax=None):\n    \n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    ax = sns.boxplot(\n        x=Config.TARGET, \n        y=feature,\n        palette='Blues_r',\n        data=data\n    )\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Target {}'.format(Config.TARGET))\n    ax.set_ylabel('Feature {}'.format(feature))\n    \n    return ax","1c784efa":"data = train_data.sample(frac=0.01)\n\nfor feature in Config.FEATURES:\n    display(Markdown('#### Plot feature `{}`'.format(feature)))\n            \n    fig, ax = plt.subplots(1, 2, figsize=(18, 5))\n\n    plot_pdf(train_data, feature, ax=ax[0])\n    plot_boxplot(train_data, feature, ax=ax[1])\n    \n    plt.show()","1f2ab00b":"pd.DataFrame({\n    'data_set': ['train', 'test'],\n    'missing_values': [\n        train_data.isna().sum().sum(), \n        test_data.isna().sum().sum()\n    ]\n}).set_index('data_set')","971aeafd":"idx = data[Config.FEATURES].isna().index\ntrain_data.iloc[idx]","52859f2d":"def add_na_count(data):\n    \"\"\"Adds the number of NaNs in a row as feature `nan_count`.\n    \"\"\"\n    df = data.copy()\n    \n    df['nan_count'] = df[Config.FEATURES].isna().sum(axis=1) \n    df['std_dev'] =  df[Config.FEATURES].isna().std(axis=1) \n    \n    return df\n\ntrain_data = FunctionTransformer(add_na_count).fit_transform(train_data)","9247442e":"train_data.loc[:, ('nan_count', 'std_dev')]","67e2bfde":"idx0 = train_data[train_data['claim'] == 0].index\nidx1 = train_data[train_data['claim'] == 1].index\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\nplot_pdf(train_data.loc[idx0], 'nan_count', ax=ax[0])\nplot_pdf(train_data.loc[idx1], 'nan_count', ax=ax[1])\n\nplot_boxplot(train_data, 'nan_count', ax=ax[1])\n\nplt.show()","d0d0928c":"corr_matrix = train_data[Config.COLUMNS].corr()","aa174f8f":"plt.figure(figsize = (20, 15))\n\nsns.heatmap(\n    corr_matrix, \n    annot = False, \n    cmap = 'Blues', \n    mask = np.triu(corr_matrix), \n    linewidths = 0.1, \n    linecolor = 'white', \n    cbar = True\n)\n\nplt.show()","569d20cf":"def add_features(data):\n    \"\"\"\n    \"\"\"\n    df = data.copy()\n    \n    df['med'] = df[Config.FEATURES].median(axis=1).astype(np.float)\n    df['mean'] = df[Config.FEATURES].mean(axis=1).astype(np.float)\n    \n    df['max'] = df[Config.FEATURES].max(axis=1).astype(np.float)\n    df['min'] = df[Config.FEATURES].min(axis=1).astype(np.float)\n    \n    df['max2'] = df[Config.FEATURES].abs().max(axis=1).astype(np.float)\n    df['min2'] = df[Config.FEATURES].abs().min(axis=1).astype(np.float)\n    \n    df['skew'] = df[Config.FEATURES].skew(axis=1).astype(np.float)\n    \n    return df\n\ntrain_data = FunctionTransformer(add_features).fit_transform(train_data)","a09e3438":"train_data.loc[:, ('med', 'mean', 'max', 'min', 'max2', 'min2', 'skew')]","788564de":"data = train_data.sample(frac=0.2, random_state=Config.RANDOM_STATE)\nn_components = 35\n\npca = make_pipeline(\n    FunctionTransformer(add_na_count),\n    FunctionTransformer(add_features),\n    SimpleImputer(strategy='mean'),\n    QuantileTransformer(output_distribution='normal'),\n    RobustScaler(),\n    \n    PCA(n_components=n_components, \n        random_state=Config.RANDOM_STATE)\n)\n\npca_cols = ['pc{}'.format(i) for i in range(1, n_components + 1)]\ncomponents = pca.fit_transform(data)","f6e4b504":"pca_data = pd.DataFrame({Config.TARGET: data[Config.TARGET]})\n\nfor i in range(1, n_components + 1):\n    pca_data[pca_cols[i-1]] = components[:, i-1]","6df0b5ca":"variance = pca['pca'].explained_variance_ratio_\nvar=np.cumsum(np.round(variance, decimals=3)*100)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].plot(variance)\nax[0].set_xlabel('# of Components')\nax[0].set_ylabel('Explained variance')\nax[0].set_title(\"PCA Analysis\")\n\nax[1].plot(var)\nax[1].set_ylabel('% Variance Explained')\nax[1].set_xlabel('# of Components')\n\nfig.tight_layout()\nfig.show()","fca8c51b":"def plot_pca(data, x, y, ax=None):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n        \n    sns.scatterplot(\n        data=data,\n        x=x, \n        y=y,\n        hue=Config.TARGET,\n        palette=sns.color_palette(['red', 'blue']),\n        alpha=0.3, \n        ax=ax)","5dcf77e3":"fig, ax = plt.subplots(1, 4, figsize=(25, 5))\n\nplot_pca(pca_data, 'pc1', 'pc2', ax=ax[0])\nplot_pca(pca_data, 'pc2', 'pc3', ax=ax[1])\nplot_pca(pca_data, 'pc3', 'pc4', ax=ax[2])\nplot_pca(pca_data, 'pc4', 'pc5', ax=ax[3])\n\nplt.show()","87986e01":"idx = train_data.sample(frac=1, random_state=Config.RANDOM_STATE).index\n\nX_data = train_data.iloc[idx][Config.FEATURES]\ny_data = train_data.iloc[idx][Config.TARGET]","50adc911":"# spit data into train and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(\n    X_data,\n    y_data,\n    test_size=Config.TEST_SIZE, \n    random_state=Config.RANDOM_STATE\n)","8e206648":"print(f'train size: {X_train.shape[0]} rows')\nprint(f'val size  : {X_val.shape[0]} rows')","7a74f96d":"def create_baseline_model():\n    \"\"\"\n    \"\"\"\n    estimators = [\n        ('lgbm', LGBMClassifier(\n            max_depth = 3,\n            num_leaves = 7,\n            n_estimators = 2000,\n            colsample_bytree = 0.3,\n            subsample = 0.5,\n            random_state = 42,\n            reg_alpha=18,\n            reg_lambda=17,\n            learning_rate = 0.095,\n            objective= 'binary')\n        ),\n        ('sgd', SGDClassifier()),\n        ('lr', LogisticRegression()),\n        ('ridge', RidgeClassifier())\n    ]\n        \n    model = make_pipeline(\n        FunctionTransformer(add_na_count),\n        FunctionTransformer(add_features),\n        SimpleImputer(strategy='mean'),\n        QuantileTransformer(output_distribution='normal'),\n        RobustScaler(),\n        StackingClassifier(\n            estimators=estimators, \n            final_estimator=LogisticRegression()\n        )\n    )\n    return model\n\nmodel = create_baseline_model()","3f4fc2cb":"%%time\n\ny_pred = model.fit(X_train, y_train).predict(X_val)\nprint('ROC: {}'.format(roc_auc_score(y_val, y_pred)))","8c3b1854":"fig, ax = plt.subplots(figsize=(8, 8))\nmetrics.plot_roc_curve(model, X_val, y_val, ax=ax)  \n\nplt.show()","745503c6":"plot_confusion_matrix(\n    model, \n    X_val, \n    y_val, \n    cmap=plt.cm.Blues,\n    normalize='true') \n\nplt.show()","40a88a48":"def plot_feature_importances(feature_imp, feature_names, num=20, ax=None):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    df = pd.DataFrame({\n        'feature': feature_names,\n        'value': feature_imp\n    }).sort_values('value', ascending=False).head(num)\n    \n    sns.barplot(\n        x='value', \n        y='feature', \n        palette='Blues_r',\n        data=df,\n        ax=ax\n    ) \n\n    ax.set_title(\"Permutation Importance of each feature\")\n    ax.set_ylabel(\"Features\")\n\n    return ax","5d22e8a0":"feature_imp = model['stackingclassifier'].estimators_[0].feature_importances_\nfeature_names = Config.FEATURES + ['nan_count', 'std_dev', 'med', 'mean', 'max', 'min', 'max2', 'min2', 'skew']","d6c4b7a9":"fig, ax = plt.subplots(figsize=(12, 8))\nplot_feature_importances(feature_imp, feature_names, num=30, ax=ax)\n\nfig.tight_layout()\nplt.show()","93387156":"y_pred_submission = model.predict_proba(test_data[Config.FEATURES])[:, 1]","2d3d97a1":"submission_data = pd.DataFrame({\n    Config.INDEX: test_data.index,\n    Config.TARGET: y_pred_submission,\n}).set_index(Config.INDEX)\n\nsubmission_data","c0e50456":"# save submission file\nsubmission_data.to_csv(Config.SUBMISSION_FILE)","cfb43365":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/28009\/logos\/header.png?)","0aa444c0":"## Exploratory data analysis (EDA)","e100517f":"## Baseline model","f6258fff":"## Target `claim`","2677129d":"## Feature Engineering","a77ebc84":"## Submission","728d4af2":"## Imports","8a80acaf":"# Tabular Playground Series - Sep 2021","1758dd2a":"## Configuration","6bed62da":"## Confusion Matrix","3cff609a":"## Missing values","4a1bfb37":"## Permutation Importance","7608f890":"## Import Data","c76e91f1":"## Correlation","a4f7a992":"## Principal component analysis (PCA)"}}