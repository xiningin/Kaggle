{"cell_type":{"4c04844d":"code","f9faa891":"code","7b687687":"code","b7e69611":"code","f7a0aac3":"code","8407c789":"code","3de84846":"code","27065908":"code","ceba4e7a":"code","ab5bd981":"code","c86b91f3":"code","6169ddb2":"code","0fbc4700":"code","c5935adb":"code","ecbce15a":"code","c5754891":"code","e621c589":"code","24d29a7d":"code","a596a590":"code","fcd82d0c":"code","919ea58b":"code","2e395da9":"code","c62d13b1":"code","113293f8":"code","24143333":"code","1d36ac85":"code","95ba37ee":"code","e825ffab":"code","2912ddbd":"code","c78e3f7e":"code","f5f2b7a9":"code","8112cdfc":"markdown","8bb53934":"markdown","770d2db7":"markdown","9549412b":"markdown","6e62e834":"markdown","5798400c":"markdown","0a9303b7":"markdown","cb898dfa":"markdown","1a1a4063":"markdown","af7fb454":"markdown","2b710f24":"markdown","b2596fcc":"markdown","fec685f4":"markdown","61fb91f8":"markdown","b028b557":"markdown","3b0d4265":"markdown","55365e48":"markdown","c813f548":"markdown"},"source":{"4c04844d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","f9faa891":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","7b687687":"train.head()","b7e69611":"#drop id columns\ntrain = train.drop('Id', axis = 1)\ntest = test.drop('Id', axis = 1)\n\n#treatind year fields as objects to classify them as categorical data\ntrain['YearBuilt'] =  train['YearBuilt'].astype(str)\ntrain['YearRemodAdd'] =  train['YearRemodAdd'].astype(str)\ntrain['YrSold'] =  train['YrSold'].astype(str)\ntest['YearBuilt'] =  test['YearBuilt'].astype(str)\ntest['YearRemodAdd'] =  test['YearRemodAdd'].astype(str)\ntest['YrSold'] =  test['YrSold'].astype(str)\n\n#dependent and independent variables\ntrain_independent = train.iloc[:, :-1]\ntrain_dependent = train.iloc[:, -1]","f7a0aac3":"#categorical and numerical features\nnumerical_features = []\ncategorical_features = []\nfor column in train_independent.columns:\n    if train_independent[column].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n        numerical_features.append(column)\n    elif train_independent[column].dtype == object:\n        categorical_features.append(column)\n\ncolumns = categorical_features + numerical_features\ntrain_independent = train[columns]\ntest = test[columns]","8407c789":"#merging test and train independent variables\nfeatures = pd.concat([train_independent, test], axis = 0)","3de84846":"print('train features :', train_independent.shape)\nprint('train target :', train_dependent.shape)\nprint('test features :', test.shape)\nprint('train dataset :', train.shape)\nprint('all features :', features.shape)","27065908":"#Numerical features with null values\nnull = features[numerical_features].isna().sum().sort_values(ascending = False)\nnull_values = pd.DataFrame(null)\nnull_values","ceba4e7a":"features['GarageYrBlt'] = features['GarageYrBlt'].fillna(0)\nfeatures['MasVnrArea'] = features['MasVnrArea'].fillna(0)\nfeatures['BsmtFullBath'] = features['BsmtFullBath'].fillna(0)\nfeatures['BsmtHalfBath'] = features['BsmtHalfBath'].fillna(0)\nfeatures['TotalBsmtSF'] = features['TotalBsmtSF'].fillna(0)\nfeatures['BsmtUnfSF'] = features['BsmtUnfSF'].fillna(0)\nfeatures['BsmtFinSF2'] = features['BsmtFinSF2'].fillna(0)\nfeatures['GarageCars'] = features['GarageCars'].fillna(0)\nfeatures['GarageArea'] = features['GarageArea'].fillna(0)\nfeatures['BsmtFinSF1'] = features['BsmtFinSF1'].fillna(0)\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\n\n#checking for anymore missing values\nfeatures[numerical_features].isna().any()","ab5bd981":"#Categorical features with null values\nnull = features[categorical_features].isna().sum().sort_values(ascending = False)\nnull_values = pd.DataFrame(null)\nnull_values","c86b91f3":"features['PoolQC'] = features['PoolQC'].fillna('Null')\nfeatures['MiscFeature'] = features['MiscFeature'].fillna('Null')\nfeatures['Alley'] = features['Alley'].fillna('Null')\nfeatures['Fence'] = features['Fence'].fillna('Null')\nfeatures['FireplaceQu'] = features['FireplaceQu'].fillna('Null')\nfeatures['GarageCond'] = features['GarageCond'].fillna('Null')\nfeatures['GarageQual'] = features['GarageQual'].fillna('Null')\nfeatures['GarageFinish'] = features['GarageFinish'].fillna('Null')\nfeatures['GarageType'] = features['GarageType'].fillna('Null')\nfeatures['BsmtExposure'] = features['BsmtExposure'].fillna('Null')\nfeatures['BsmtCond'] = features['BsmtCond'].fillna('Null')\nfeatures['BsmtQual'] = features['BsmtQual'].fillna('Null')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].fillna('Null')\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].fillna('Null')\nfeatures['Utilities'] = features['Utilities'].fillna('Null')\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode()[0])\nfeatures['Functional'] = features['Functional'].fillna(features['Functional'].mode()[0])\nfeatures['MasVnrType'] = features['MasVnrType'].fillna(features['MasVnrType'].mode()[0])\nfeatures['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\nfeatures['MSZoning'] = features['MSZoning'].fillna(features['MSZoning'].mode()[0])\n\n#checking for remaining null values\nfeatures[categorical_features].isna().any()","6169ddb2":"features['age'] = features['YrSold'].astype(int) - features['YearBuilt'].astype(int)\nfeatures['remod_age'] = features['YrSold'].astype(int) - features['YearRemodAdd'].astype(int)\nfeatures['extra_rooms'] = features['TotRmsAbvGrd'] - features['BedroomAbvGr'] - features['KitchenAbvGr']\nfeatures['floors_area'] = features['1stFlrSF'] + features['1stFlrSF']\nfeatures['total_bathrooms'] = features['FullBath'] + (0.5 * features['HalfBath']) + features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath'])\nfeatures['porch_area'] = features['WoodDeckSF'] + features['OpenPorchSF'] + features['EnclosedPorch'] + features['3SsnPorch'] + features['ScreenPorch'] + features['PoolArea']\nfeatures['walled_area'] = features['TotalBsmtSF'] +features['GrLivArea']\nfeatures['TotalOccupiedArea'] = features['walled_area'] + features['porch_area']","0fbc4700":"#dependent variable\nf, ax = plt.subplots(figsize=(9, 8))\nsns.distplot(train_dependent, bins = 20, color = 'Magenta')\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")","c5935adb":"# log transformation\ntrain_dependent = np.log1p(train_dependent)\n\n#after transformation\nf, ax = plt.subplots(figsize=(9, 8))\nsns.distplot(train_dependent, bins = 20, color = 'Magenta')\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")","ecbce15a":"# Other numerical variables\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nskew_features = features[numerical_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features","c5754891":"# Normalize skewed features with boxcox transformation\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","e621c589":"#defining numerical features again to include the added features for the correlation plot to be plotted.\nnumerical_features = []\nfor column in train_independent.columns:\n    if train_independent[column].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n        numerical_features.append(column)\n\nnew_train_set = pd.concat([features.iloc[:len(train_dependent), :], train_dependent], axis=1)","24d29a7d":"def correlation_map(f_data, f_feature, f_number):\n    f_most_correlated = f_data.corr().nlargest(f_number,f_feature)[f_feature].index\n    f_correlation = f_data[f_most_correlated].corr()\n    \n    f_mask = np.zeros_like(f_correlation)\n    f_mask[np.triu_indices_from(f_mask)] = True\n    with sns.axes_style(\"white\"):\n        f_fig, f_ax = plt.subplots(figsize=(12, 10))\n        f_ax = sns.heatmap(f_correlation, mask=f_mask, vmin=0, vmax=1, square=True,\n                           annot=True, annot_kws={\"size\": 10}, cmap=\"BuPu\")\n\n    plt.show()\n\ncorrelation_map(new_train_set, 'SalePrice', 20)","a596a590":"#dropping features with high correlation with other independent variables to avoid chances of multicolinearity.\nfeatures = features.drop(['GarageCars','1stFlrSF', 'walled_area'], axis = 1)","fcd82d0c":"features = pd.get_dummies(features).reset_index(drop=True)\nfeatures.shape","919ea58b":"features_to_be_dropped = []\nfor feature in features.columns:\n    all_value_counts = features[feature].value_counts()\n    zero_value_counts = all_value_counts.iloc[0]\n    if zero_value_counts \/ len(features) > 0.995:\n        features_to_be_dropped.append(feature)\nprint('\\nFeatures with predominant zeroes:\\n')\nprint(features_to_be_dropped)\n\nfeatures = features.drop(features_to_be_dropped, axis=1).copy()\nfeatures.shape","2e395da9":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal', 'floors_area']\n\nfeatures = logs(features, log_features)\n","c62d13b1":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['OverallQual', 'LotFrontage_log', \n              'TotalBsmtSF_log', 'GrLivArea_log',\n              'GarageArea_log', 'floors_area_log']\nfeatures = squares(features, squared_features)","113293f8":"x_train = features.iloc[:len(train_dependent), :]\nx_test = features.iloc[len(train_dependent):, :]\ny_train = train_dependent\ntrain_set = pd.concat([x_train, y_train], axis=1)","24143333":"print('train features:', x_train.shape)\nprint('train target:', y_train.shape)\nprint('test features:', x_test.shape)\nprint('train set:', train_set.shape)","1d36ac85":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom mlxtend.regressor import StackingCVRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n#randomforest\nrf = RandomForestRegressor(n_estimators=300, random_state=0)\n\n#adaboost\nada = AdaBoostRegressor(learning_rate = 0.05, loss =  'linear', n_estimators = 100 , random_state = 0)\n\n#xgb\nxgb = XGBRegressor(learning_rate=0.01, n_estimators=6000, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, colsample_bytree=0.7,\n                       objective='reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27, reg_alpha=0.00006, random_state=0)\n\n#ridgecv\nkfolds = KFold(n_splits=10, shuffle=True, random_state=0)\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=[13.5, 14, 14.5, 15, 15.5], cv=kfolds))\n\n#gradient\ngrad = GradientBoostingRegressor(n_estimators=4000, learning_rate=0.01, max_depth=4, max_features='sqrt', min_samples_leaf=15, \n                                 min_samples_split=10, loss='huber', random_state=0)\n\n#stackcv \nstackcv = StackingCVRegressor(regressors=(rf, ada, xgb, \n                                          ridge, grad),\n                              meta_regressor=xgb,\n                              use_features_in_secondary=True)","95ba37ee":"#individual performance\nscores_rf = -1 * cross_val_score(rf, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_ada = -1 * cross_val_score(ada, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_xgb = -1 * cross_val_score(xgb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_ridge = -1 * cross_val_score(ridge, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_grad = -1 * cross_val_score(grad, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nprint('random forest mae:', scores_rf.mean())\nprint('Ada boost:', scores_ada.mean())\nprint('xgboost:', scores_xgb.mean())\nprint('ridgecv:', scores_ridge.mean())\nprint('Gradient Boosting:', scores_grad.mean())","e825ffab":"#fitting\nrf_fit = rf.fit(x_train, y_train)\nada_fit = ada.fit(x_train, y_train)\nxgb_fit = xgb.fit(x_train, y_train)\nridge_fit = ridge.fit(x_train, y_train)\ngrad_fit = grad.fit(x_train, y_train)\n\nstackcv_fit = stackcv.fit(np.array(x_train), np.array(y_train))","2912ddbd":"blend = [0.1994, 0.0000, 0.2031, 0.2017, 0.2032, 0.2043]","c78e3f7e":"#blending                \ny_pred = np.expm1((blend[0] * rf_fit.predict(x_test)) +\n                  (blend[1] * ada_fit.predict(x_test)) +\n                  (blend[2] * xgb_fit.predict(x_test)) +\n                  (blend[3] * ridge_fit.predict(x_test)) +\n                  (blend[4] * grad_fit.predict(x_test)) +\n                  (blend[5] * stackcv_fit.predict(np.array(x_test))))","f5f2b7a9":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission.iloc[:, 1] = np.round_(y_pred)\nsubmission.to_csv(\"submission_z.csv\", index=False)","8112cdfc":"method from https:\/\/www.kaggle.com\/pcbreviglieri\/enhanced-house-price-predictions\/notebook#Enhanced-House-Price-Predictions","8bb53934":"## Model Selection, Stacking, Fitting and Preicting.","770d2db7":"#### dropping columns with predominant 0 values","9549412b":"## Feature Generation","6e62e834":"## Submission","5798400c":"***gradientboostingregressor, xgbregressor, randomforestregressor, adaboostregressor, ridgecv, stackngcvregressor***","0a9303b7":"The extra number of columns can be seen after encoding.","cb898dfa":"* **lotfrantage**:\n    We could check feature with highest correlation w\/lotfrontage and fill the missing values with mean of lot frontage of houses grouped by the feature with the highest correlation but this leaves out the categorical variables which have not been encoded yet.\n    So, neighborhood makes the most sense.\n* **GarageYrBlt**, **GarageCars**, **GarageArea**, **GarageQual**: going to assume no garages.\n* **Bsmts**: assuming no basements.\n* **MasonVeneer Area**: No Type, no area.","1a1a4063":"Adaboost kinda sucks","af7fb454":"### Reconstructing train and test sets","2b710f24":"## Encoding categorical variables","b2596fcc":"* Original Age\n* Age since Remodelling\n* Extra Rooms\n* Floors_Area\n* Bathrooms\n* Walled Area\n* Porch Area\n* Occupied area\n* Basement Size\n","fec685f4":"## Null Values","61fb91f8":"## Feature Transformations","b028b557":"Right tailed.","3b0d4265":"## Multicolinearity","55365e48":"## Skewness","c813f548":"method from https:\/\/www.kaggle.com\/pcbreviglieri\/enhanced-house-price-predictions\/notebook#Enhanced-House-Price-Predictions again."}}