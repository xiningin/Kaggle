{"cell_type":{"fd6f6975":"code","ab8a2719":"code","eeb6f6a6":"code","6a0d6b6c":"code","8a240f41":"code","2910cd36":"code","ac0e436b":"code","7129f1fa":"code","7f049434":"code","db140816":"code","59303037":"code","4a39f135":"code","95c1b8e8":"code","2b8266cf":"code","2ab33b77":"code","6b0fdc43":"code","b94848d5":"code","f1b7b641":"code","64264783":"code","121696b3":"code","728beb34":"code","964cdd0d":"code","1dfa88e5":"code","c4c0d397":"code","a101a95d":"code","8cda3162":"markdown","44640f12":"markdown","d007e1e3":"markdown","5662ff11":"markdown","91e85939":"markdown","e8ee337d":"markdown","e7b34801":"markdown","e8796d18":"markdown","154559b4":"markdown","218fd0c4":"markdown","f4a7e23c":"markdown","f808df77":"markdown","5e3d75ea":"markdown"},"source":{"fd6f6975":"# import necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# use seaborn to costomize matplotlib plot theme\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport scipy.stats as stats\n# use ridge regession library provided by sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error","ab8a2719":"# filter unnecessary warnings\n# thanks to Ahmad Javed for teaching me this method!\nimport warnings \nwarnings.filterwarnings('ignore')","eeb6f6a6":"# load data\ndataTrain=pd.read_csv('..\/input\/train.csv')\ndataTest=pd.read_csv('..\/input\/test.csv')","6a0d6b6c":"print(dataTrain.head(), '\\n', dataTrain.shape)\nprint(100 * '*')\nprint(dataTest.head(), '\\n', dataTest.shape)","8a240f41":"# check possible outliers in the dependent variable, SalePrice\nplt.scatter(x='GrLivArea', y='SalePrice', data=dataTrain, color='b', marker='*')\nplt.show()","2910cd36":"print('before removal: ', dataTrain.shape)\n# remove two outliers\ndataTrain.drop(dataTrain[(dataTrain['GrLivArea']>4000) & (dataTrain['SalePrice']<200000)].index, inplace=True)\nprint('after removal: ', dataTrain.shape)","ac0e436b":"# check NaN values in the training and test set\nprint(dataTrain.isnull().sum().sort_values(ascending=False).head(25))\nprint(100*'*')\nprint(dataTest.isnull().sum().sort_values(ascending=False).head(35))","7129f1fa":"# record all train columns\ndicColumn = dataTrain.columns\nfor nameColumn in dicColumn:\n    # object features\n    if dataTrain[nameColumn].dtype == 'object':\n        # value that should be 'NA', meaning 'no'\n        if dataTrain[nameColumn].isnull().sum()>30:\n            dataTrain[nameColumn].fillna(value='NA', inplace=True)\n        # value that is unavailable\n        else:\n            dataTrain[nameColumn].fillna(value=dataTrain[nameColumn].mode()[0], inplace=True)\n    # numerical features\n    else:\n        if dataTrain[nameColumn].isnull().any():\n            # fill with median\n            dataTrain[nameColumn].fillna(value=dataTrain[nameColumn].median(), inplace=True)\n            \n# record all test columns\ndicColumnT = dataTest.columns\nfor nameColumn in dicColumnT:\n    # object features\n    if dataTest[nameColumn].dtype == 'object':\n        # value that should be 'NA', meaning 'no'\n        if dataTest[nameColumn].isnull().sum()>30:\n            dataTest[nameColumn].fillna(value='NA', inplace=True)\n        # value that is unavailable\n        else:\n            dataTest[nameColumn].fillna(value=dataTest[nameColumn].mode()[0], inplace=True)\n    # numerical features\n    else:\n        if dataTest[nameColumn].isnull().any():\n            # fill with median\n            dataTest[nameColumn].fillna(value=dataTest[nameColumn].median(), inplace=True)\n            \n# check NaN values in the training and test set\nprint(dataTrain.isnull().sum().sort_values(ascending=False).head(10))\nprint(100*'*')\nprint(dataTest.isnull().sum().sort_values(ascending=False).head(10))","7f049434":"plt.figure(figsize=(15,12))\nsns.heatmap(dataTrain.corr(), vmax=0.9)","db140816":"# correlation between features\nprint(dataTrain.corr()['YearBuilt']['GarageYrBlt'])\n# correlation between features and saleprice\nprint(dataTrain.corr()['SalePrice']['YearBuilt'])\nprint(dataTrain.corr()['SalePrice']['GarageYrBlt'])","59303037":"# correlation between features\nprint(dataTrain.corr()['GarageArea']['GarageCars'])\n# correlation between features and saleprice\nprint(dataTrain.corr()['SalePrice']['GarageArea'])\nprint(dataTrain.corr()['SalePrice']['GarageCars'])","4a39f135":"# correlation between features\nprint(dataTrain.corr()['TotalBsmtSF']['1stFlrSF'])\n# correlation between features and saleprice\nprint(dataTrain.corr()['SalePrice']['TotalBsmtSF'])\nprint(dataTrain.corr()['SalePrice']['1stFlrSF'])","95c1b8e8":"# correlation between features\nprint(dataTrain.corr()['GrLivArea']['TotRmsAbvGrd'])\n# correlation between features and saleprice\nprint(dataTrain.corr()['SalePrice']['GrLivArea'])\nprint(dataTrain.corr()['SalePrice']['TotRmsAbvGrd'])","2b8266cf":"# correlation between features\nprint(dataTrain.corr()['GrLivArea']['TotRmsAbvGrd'])\n# correlation between features and saleprice\nprint(dataTrain.corr()['SalePrice']['GrLivArea'])\nprint(dataTrain.corr()['SalePrice']['TotRmsAbvGrd'])","2ab33b77":"print(dataTrain.shape, dataTest.shape)\ndataTrain.drop(['GarageYrBlt','GarageArea','1stFlrSF','TotRmsAbvGrd'], axis=1, inplace=True)\ndataTest.drop(['GarageYrBlt','GarageArea','1stFlrSF','TotRmsAbvGrd'], axis=1, inplace=True)\nprint(dataTrain.shape, dataTest.shape)","6b0fdc43":"# correlation between SalePrice and features in ascending order\nprint(dataTrain.corr()['SalePrice'].abs().sort_values(ascending=True).head(25))","b94848d5":"print(dataTrain.shape, dataTest.shape)\n# delete features w=with correlation factors less than 0.3\nirrelatedCol = dataTrain.corr()['SalePrice'].abs().sort_values(ascending=True).head(19).index\ndataTrain.drop(irrelatedCol, axis=1, inplace=True)\ndataTest.drop(irrelatedCol, axis=1, inplace=True)\nprint(dataTrain.shape, dataTest.shape)","f1b7b641":"# check skewness in dependent variable\nsns.distplot(dataTrain['SalePrice'])","64264783":"# log transformation\ndataTrain['SalePrice']=np.log1p(dataTrain['SalePrice'])\nsns.distplot(dataTrain['SalePrice'])","121696b3":"xTrainData = dataTrain.drop(['SalePrice'], axis=1)\ninputData = xTrainData.append(dataTest)\n# convert using hot encoding\ninputData = pd.get_dummies(inputData)\nxTrainLenth = xTrainData.shape[0]\nxTrainData = inputData[0:xTrainLenth]\nxTestData = inputData[xTrainLenth:]\nprint(xTrainData.shape, xTestData.shape)\n# split the training set into set for model building and set for model optimizing\nxTrain, xOpt, yTrain, yOpt = train_test_split(xTrainData, dataTrain['SalePrice'], test_size=0.3, random_state=100)","728beb34":"# iterate throw the parameter list for the best value of parameter alpha\nlistPara=[0.0001, 0.001, 0.01, 1, 10, 100, 1000]\n# record the error for each parameter chosen\nerror = []\nfor i in range(len(listPara)):\n    # apply ridge regression\n    ridgeReg = Ridge(alpha=listPara[i], copy_X=True, fit_intercept=True)\n    ridgeReg.fit(xTrain, yTrain)\n    optPredict = ridgeReg.predict(xOpt)\n    # calculate the error\n    error.append(np.sqrt(mean_squared_error(optPredict, yOpt)))\n# plot the error\nplt.scatter(x=listPara, y=error, color='b', marker='*')\n# calculate the best parameter in list\nprint(pd.Series(data=error, index=listPara).idxmin())","964cdd0d":"# iterate throw the parameter list for the best value of parameter alpha\nlistPara = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n# record the error for each parameter chosen\nerror = []\nfor i in range(len(listPara)):\n    # apply ridge regression\n    ridgeReg = Ridge(alpha=listPara[i], copy_X=True, fit_intercept=True)\n    ridgeReg.fit(xTrain, yTrain)\n    optPredict = ridgeReg.predict(xOpt)\n    # calculate the error\n    error.append(np.sqrt(mean_squared_error(optPredict, yOpt)))\n# plot the error\nplt.scatter(x=listPara, y=error, color='b', marker='*')\n# calculate the best parameter in list\nprint(pd.Series(data=error, index=listPara).idxmin())","1dfa88e5":"# iterate throw the parameter list for the best value of parameter alpha\nlistPara = np.arange(1, 21, 1)\n# record the error for each parameter chosen\nerror = []\nfor i in range(len(listPara)):\n    # apply ridge regression\n    ridgeReg = Ridge(alpha=listPara[i], copy_X=True, fit_intercept=True)\n    ridgeReg.fit(xTrain, yTrain)\n    optPredict = ridgeReg.predict(xOpt)\n    # calculate the error\n    error.append(np.sqrt(mean_squared_error(optPredict, yOpt)))\n# plot the error\nplt.scatter(x=listPara, y=error, color='b', marker='*')\n# calculate the best parameter in list\nprint(pd.Series(data=error, index=listPara).idxmin())","c4c0d397":"# iterate throw the parameter list for the best value of parameter alpha\nlistPara = np.arange(6, 8, 0.1)\n# record the error for each parameter chosen\nerror = []\nfor i in range(len(listPara)):\n    # apply ridge regression\n    ridgeReg = Ridge(alpha=listPara[i], copy_X=True, fit_intercept=True)\n    ridgeReg.fit(xTrain, yTrain)\n    optPredict = ridgeReg.predict(xOpt)\n    # calculate the error\n    error.append(np.sqrt(mean_squared_error(optPredict, yOpt)))\n# plot the error\nplt.scatter(x=listPara, y=error, color='b', marker='*')\n# calculate the best parameter in list\nprint(pd.Series(data=error, index=listPara).idxmin())","a101a95d":"# apply ridge regression\nridge = Ridge(alpha=6.7)\n# use all training data available\nridge.fit(xTrainData, dataTrain['SalePrice'])\ndataPredicted = ridge.predict(xTestData)\n# perform the counter-transformation for log-transformation\ndataPredicted = np.exp(list(dataPredicted))-1\n\nidDf = pd.DataFrame(pd.read_csv('..\/input\/test.csv')['Id'])\ndataPreDf = pd.DataFrame(dataPredicted, columns=['SalePrice'])\noutput = pd.concat([idDf, dataPreDf], axis=1)\noutputResult = pd.DataFrame(output)\noutputResult.to_csv('submission.csv', index=False)","8cda3162":"**Prefix**\n\nHi, everyone! As a high school student interested in computer science and economics, this is my first taste of applying computational tools and machine learning algorithms, after learning Python programming and some machine learning.\n\nI want to thank kagglers Junying(Emma) Zhang, Vijay Gupta, and meikegw whose posts provide valuable guidance to me, since this is my first time to work on this platform. \n\nSharing my approach here, I hope it could, in turn, help other beginners to start out, and I am also excited to learn and improve, so please comment anything you think I should work more on.\n\nI would also continue to improve my work, so check back for any update!\n\nThank you!","44640f12":"***Ridge regression***\n\nTo apply ridge regression, I first need to transform categorial features so that they could be calculaed. Here, I use a function in the panda library to hot encode.\n\nThen, I split the training set into two parts: one for regression training, one for testing result. Because ridge regression's accuracy depends on parameter alpha, I will test a range of alpha and then using this small testing set to test the result. In this way, I could choose the optimized model for ridge regression. This technique is called cross validation.","d007e1e3":"***Importing packages***\n\nThe first step is to import the packages that I will use next, such as matplotlib for ploting, numpy for computing, and seaborn for customizing plot style.","5662ff11":"***Apply ridge regression and produce output***\n\nIn the previous processes, I have found that ridge regression works best when alpha is 6.7. Therefore, I perform ridge regression again, now using the complete training set provided.\n\nUsing the regression model, I predict sale prices for the test set and output the results.","91e85939":"***Import data for this competition***","e8ee337d":"***Log transformation***\n\nPlotting the dependent variable, SalePrice, shows that it is actually skewed. Therefore, a log transformation is needed to yield more accurate result. Due to log transformation, the predicted results need a step more to transform back to the original scale.","e7b34801":"***Filling NaN values***\n\nData set containing NaN values couldn't be directly modeled. So I first check the number of NaN values contained by column. I notice that a lot of categorial features contain high number of NaN values. Checking the description of the data set, I find that it is because some values called 'NA', for example meaning that the house doesn't have a fireplace, are incorrectly regarded as missing values by panda library during data importing. Therefore, I need to fill those values with their orininal true values. \n\nHowever, there are some data points that are missing, so for categorial values that are missing, I fill the mode, and for numerical values, I fill the median. The use of median instead of the mean is to avoid possible influence of skewness. ","e8796d18":"**Detailed Approach**","154559b4":"***Correlation analysis***\n\nI first plot the correlation matrix in heat map format. Looking at the map, I find that there are four pairs of  features that are highly correlated. Because in regression, features should be as independent from each other as possible and correlation among features could bring undesirable results, in case of a pair of correlated features, I will only keep the feature that has higher correlation to the dependent variable, SalePrice.\n\nAfter that, I calculate the correlation factors between features and SalePrice and sort them in ascending order. Some correlation factors are too small that they may be caused purely by chance, therefore, I delete features that have correlation factors less than 0.3.","218fd0c4":"***Familiarizing ourselves with the data***\n\nBy printing out a portion of the data set and the dimensions (shape) of it, I could know that the training set is fairly large with 1460 entries, and there are 80 features provided.","f4a7e23c":"**Keywords**\n\n- NaN Value Filling\n- Deleting Outliers\n- Log Transformation\n- Correlation Analysis\n- Encoding Categorial Variables\n- Ridge Regression\n- Cross Validation","f808df77":"**Score**\n\nThe score of the submission is 0.12956.\n\n\n**Thank you for reading!**\n\n**I hope that it helps. Please feel free to post comments about anything that I need to clarify, where I did wrongly, and where I need to improve.**","5e3d75ea":"***Deleting outliers***\n\nOutliers could sometimes have great influence on statistical results, therefore, when building models, we should delete outliers. Here, I plot the value of the response variable, SalePrice, against value of one feature, GrLivArea. From the scatter plot, I notice that the two points at the lower right corner are clearly outliers, so I drop these two data entries from the training set."}}