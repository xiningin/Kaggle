{"cell_type":{"4f7ca96a":"code","ec9bed57":"code","1ae86367":"code","344749d9":"code","f327ce06":"code","4e8e9434":"code","7b8820c5":"code","71d872b1":"code","69a6550b":"code","002d7888":"code","152d8f70":"code","60ad5739":"code","35301001":"code","4a77bf3d":"code","1c53e0af":"markdown","b86e27b2":"markdown","f5c4bbb2":"markdown","e4327a05":"markdown","eebfc953":"markdown","62499555":"markdown","f3ffcbc7":"markdown","47cbbd81":"markdown","5fa64613":"markdown"},"source":{"4f7ca96a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport random\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import load_img\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec9bed57":"os.listdir(\"..\/input\/10-monkey-species\")","1ae86367":"train = os.listdir(\"..\/input\/10-monkey-species\/training\/training\")\nvalidation = os.listdir(\"..\/input\/10-monkey-species\/validation\/validation\")","344749d9":"os.listdir('..\/input\/10-monkey-species\/training\/training')","f327ce06":"#filenames  = os.listdir('..\/input\/10-monkey-species\/training\/training\/n1')\n#sample = random.choice(filenames)\n\nlist1 = [\"..\/input\/10-monkey-species\/training\/training\/n0\/\"\n, \"..\/input\/10-monkey-species\/training\/training\/n1\/\"\n,\"..\/input\/10-monkey-species\/training\/training\/n2\/\"\n, \"..\/input\/10-monkey-species\/training\/training\/n3\/\"\n, \"..\/input\/10-monkey-species\/training\/training\/n4\/\"\n,\"..\/input\/10-monkey-species\/training\/training\/n5\/\"\n,\"..\/input\/10-monkey-species\/training\/training\/n6\/\"\n,\"..\/input\/10-monkey-species\/training\/training\/n7\/\"\n,\"..\/input\/10-monkey-species\/training\/training\/n8\/\",\n\"..\/input\/10-monkey-species\/training\/training\/n9\/\"]\n\nfig = plt.figure(figsize=(12, 15))\nfig.set_size_inches(13,13)\nplt.style.use(\"ggplot\")\nj=1\nfor i in list1:   \n    filenames  = os.listdir(i)\n    sample = random.choice(filenames)\n    image = load_img(i+sample) #'..\/input\/10-monkey-species\/training\/training\/n1'\n    plt.subplot(2,5,j)\n    plt.imshow(image)\n    plt.xlabel(\"Monkey Species: n{}\".format(j))\n    j+=1\nplt.tight_layout()","4e8e9434":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntr_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\ntrain_datagen = tr_datagen.flow_from_directory(directory = \"..\/input\/10-monkey-species\/training\/training\",target_size=(150,150),\n                                            class_mode='categorical',batch_size=60)","7b8820c5":"test_datagen = ImageDataGenerator(rescale = 1\/255.0)\nvalid_datagen = test_datagen.flow_from_directory(directory = \"..\/input\/10-monkey-species\/validation\/validation\",target_size=(150,150),\n                                            class_mode='categorical',batch_size=60)","71d872b1":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten,Dense,Conv2D,MaxPool2D,Dropout,BatchNormalization","69a6550b":"model = Sequential()\nmodel.add(Conv2D(filters = 32,kernel_size = (3,3),activation = 'relu',input_shape=(150,150,3)))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(2,2))\n\nmodel.add(Conv2D(filters = 32,kernel_size = (3,3),activation = 'relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(2,2))\n\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation = 'relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10,activation='softmax'))","002d7888":"model.summary()","152d8f70":"from tensorflow.keras.optimizers import SGD\nmodel.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n\ncheckpoint = ModelCheckpoint(\"model_1.h5\",monitor = 'val_accuracy',verbose=1,save_best_only = True,save_weights_only = False,\n                             mode='auto',period=1)\nearlystop = EarlyStopping(monitor ='val_acc',patience=20,min_delta = 0,verbose=1,mode='auto')\n","60ad5739":"batch_size = 60\nhistory = model.fit_generator(generator = train_datagen,steps_per_epoch = len(train_datagen),epochs=150,\n                              validation_data = valid_datagen,validation_steps =len(valid_datagen) ,\n                              callbacks=[checkpoint,earlystop],verbose=1)#train_number\/\/batch_size,valid_number\/\/batch_size","35301001":"def plot_learning_curve(history,epochs):\n    epochs = np.arange(1,epochs+1)\n    plt.figure(figsize=(10,6))\n    plt.plot(epochs,history.history['accuracy'])\n    plt.plot(epochs,history.history['val_accuracy'])\n    plt.title(\"Accuracy\")\n    plt.xlabel('Epochs')\n    plt.ylabel(\"Accuracy\")\n    plt.legend(['Train','Val'],loc='upper left')\n    plt.show()\n    \n    plt.figure(figsize=(10,6)) \n    plt.plot(epochs,history.history['loss'])\n    plt.plot(epochs,history.history['val_loss'])\n    plt.title(\"Loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel(\"Loss\")\n    plt.legend(['Train','Val'],loc='upper left')\n    plt.show()\n    ","4a77bf3d":"plot_learning_curve(history,150)","1c53e0af":"><h3>Plotting some of the images of different Species<\/h3>","b86e27b2":"><h3>Learning Curve<\/h3>","f5c4bbb2":"><h3>Model Creating<\/h3>","e4327a05":"---\n\n<h1 style=\"text-align: center;font-size: 20px;color: blue\">Thanks for Reading<\/h1>\n\n---","eebfc953":"><h3>Defining Train & Validation directory<\/h3>","62499555":"\n\n---\n\n<h1 style=\"text-align: center;font-size: 40px;\">Monkey Species Classification<\/h1>\n\n---\n\n<center><img src=\"https:\/\/advances.sciencemag.org\/content\/advances\/3\/1\/e1600946\/F7.large.jpg?download=true\n\"width=\"800\" height=\"100\"><\/center>\n\n---","f3ffcbc7":">Image Augmentation & creating train & validation DataGen","47cbbd81":"><h3>Name of Monkey Species had in this dataset<\/h3>","5fa64613":"><h3>Importing Necessary libraries<\/h3>"}}