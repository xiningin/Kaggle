{"cell_type":{"1ea9c2ae":"code","87391783":"code","27d7d307":"code","a05fe5ae":"code","1e038144":"code","a65092a1":"code","b5b4eba9":"code","b8eab29b":"code","3fbf9301":"code","fa347143":"code","a9e634e8":"code","0612b1a6":"code","ab2b320c":"code","5ed1a5ca":"code","68e0caa9":"code","3f4d0514":"code","3e4390ac":"code","a17aaff3":"code","f361a2ef":"code","436fd99d":"code","7c03d831":"code","ca13ac74":"code","857672c9":"code","01fc3180":"code","acc33837":"code","974088f6":"code","05b96991":"code","e4785a38":"code","290f64f7":"code","63a6366b":"code","fc66fe81":"code","e2b55742":"code","77dc71c9":"code","2e0d128c":"code","123b1401":"code","09b2e931":"code","12d045a0":"code","591078ed":"code","83865ad2":"code","eecc7d99":"code","d387c7e8":"code","ce72bbe6":"code","134c0786":"code","aff5edb5":"markdown","cfc77f62":"markdown","0af79532":"markdown","1cee92f8":"markdown","a3c637e9":"markdown","58e6615c":"markdown","450044ea":"markdown","8f1a708a":"markdown","b277aeb2":"markdown","360e1189":"markdown","c0a94e5a":"markdown","2b9a93d0":"markdown","19ad9ce8":"markdown","0353213b":"markdown","4b9a7cd3":"markdown","b989787c":"markdown","4ed1983a":"markdown","86cdc067":"markdown","dcd37fbd":"markdown","b7d9e517":"markdown","04a6ced6":"markdown"},"source":{"1ea9c2ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","87391783":"# read train\ntrain = pd.read_csv(\"..\/input\/train.csv\")\nprint(train.shape)\ntrain.head()","27d7d307":"train.iloc[0].value_counts()","a05fe5ae":"# read test\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(test.shape)\ntest.head()","1e038144":"# put labels into y_train variable\nY_train = train[\"label\"]\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"], axis = 1)\nX_train.head()","a65092a1":"# visualize number of digits classes\nplt.figure(figsize=(15,7))\ng = sns.countplot(Y_train, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\nY_train.value_counts()","b5b4eba9":"# plot some samples\n#as_matrix = Converting to matrix\nimg = X_train.iloc[0].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap=\"gray\")\nplt.title(Y_train.iloc[0]) #or plt.title(train.iloc[0,0]) both of them are okay.\nplt.axis(\"off\")\nplt.show()","b8eab29b":"# plot some samples\nimg = X_train.iloc[7].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(Y_train.iloc[7]) #or plt.title(train.iloc[7,0]) both of them are okay.\nplt.axis(\"off\")\nplt.show()","3fbf9301":"# plot some samples\nimg = X_train.iloc[7223].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(Y_train.iloc[7223]) #or plt.title(train.iloc[7223,0]) both of them are okay.\nplt.axis(\"off\")\nplt.show()","fa347143":"# Normalize the data\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\nprint(\"X_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","a9e634e8":"# Reshape\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nprint(\"X_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","0612b1a6":"# Label Encoding \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding(one hot vectors)\nY_train = to_categorical(Y_train, num_classes = 10)","ab2b320c":"# Split the train and the validation set for the fitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 2)\nprint(\"x_train shape: \",x_train.shape)\nprint(\"x_val shape: \",x_val.shape)\nprint(\"y_train shape: \",y_train.shape)\nprint(\"y_val shape :\",y_val.shape)","5ed1a5ca":"# Some examples\nplt.imshow(x_train[4][:,:,0], cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()","68e0caa9":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential # to create a cnn model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same'))\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\n# fully connected\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10, activation = \"softmax\"))","3f4d0514":"model.summary()","3e4390ac":"# Define the optimizer\n#optimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n\nfrom keras.optimizers import RMSprop,Adam,SGD,Adagrad,Adadelta,Adamax,Nadam\n#optimizer=SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n#optimizer=Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n#optimizer=Adadelta(lr=0.001, rho=0.95, epsilon=0.1, decay=0.1)\n#optimizer=Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n#optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n\n#optimizer=Adagrad(lr=0.01, epsilon=None, decay=0.0)\n","a17aaff3":"# # Define the optimizer\n# optimizer = RMSprop(lr = 0.001, rho=0.9, epsilon=1e-08, decay=0.0)","f361a2ef":"# Compile the model\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","436fd99d":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","7c03d831":"epochs = 25  # for better result increase the epochs\nbatch_size = 250","ca13ac74":"# data augmentation\ndatagen = ImageDataGenerator(   \n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=10,  # randomly rotate images in the range 10 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False  # randomly flip images\n        )\ndatagen.fit(x_train)","857672c9":"# # Fit the model\n# history = model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n#                    epochs = epochs, validation_data = (x_val,y_val), steps_per_epoch = x_train.shape[0] \/\/ batch_size)","01fc3180":"history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n                              epochs=epochs, validation_data = (x_val, y_val),\n                              verbose = 2, steps_per_epoch=x_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction]) ","acc33837":"# Plot the loss curve for training\nplt.plot(history.history['loss'], color='r', label=\"Train Loss\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","974088f6":"# Plot the accuracy curve for training\nplt.plot(history.history['acc'], color='g', label=\"Train Accuracy\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","05b96991":"# Plot the loss curve for validation \nplt.plot(history.history['val_loss'], color='r', label=\"Validation Loss\")\nplt.title(\"Validation Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","e4785a38":"# Plot the accuracy curve for validation \nplt.plot(history.history['val_acc'], color='g', label=\"Validation Accuracy\")\nplt.title(\"Validation Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","290f64f7":"print('Train accuracy of the model: ',history.history['acc'][-1])","63a6366b":"print('Train loss of the model: ',history.history['loss'][-1])","fc66fe81":"print('Validation accuracy of the model: ',history.history['val_acc'][-1])","e2b55742":"print('Validation loss of the model: ',history.history['val_loss'][-1])","77dc71c9":"print(x_val.shape)\nplt.imshow(x_val[100].reshape(28,28),cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()","2e0d128c":"trueY = y_val[100]\nimg = x_val[100]\ntest_img = img.reshape(1,28,28,1)\n\npreds = model.predict_classes(test_img)\nprob = model.predict_proba(test_img)\n\nprint(\"trueY: \",np.argmax(trueY))#show the max value in trueY values(trueY is one hot vector!)\nprint(\"Preds: \",preds)\nprint(\"Prob: \",prob)","123b1401":"d = {'pred': model.predict_classes(x_val), 'true': np.argmax(y_val,axis=1)} #axis=1!important!\ndf = pd.DataFrame(data=d)\n\n#looking at wrong predicted values(For 1! you can change it.)\narray1 = np.array(df[(df.pred != df.true) & (df.true==1)].index)\nprint(array1)\n\n# shows total mistakes\ndf2 = df[(df.pred != df.true)]\ndf2","09b2e931":"plt.figure(figsize = (12,12))\n\nfor i in range(len(df2)):\n    plt.subplot(6, 4, i+1)\n    img = x_val[df2.index[i]]\n    img = img.reshape((28,28))\n    plt.imshow(img, cmap='gray')\n    plt.title(\"True Class: \" + str(df2[\"true\"].iloc[i])+\"    Pred Class: \" + str(df2[\"pred\"].iloc[i]))\n    plt.axis('off')\n    \nplt.show()","12d045a0":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(x_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap = \"gist_yarg_r\", linecolor=\"black\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","591078ed":"for i in range(len(confusion_mtx)):\n    print(\"Class:\",str(i))\n    print(\"Number of Wrong Prediction:\", str(sum(confusion_mtx[i])-confusion_mtx[i][i]), \"out of \"+str(sum(confusion_mtx[i])))\n    print(\"Percentage of True Prediction: {:.2f}%\".format(confusion_mtx[i][i] \/ (sum(confusion_mtx[i])\/100) ))\n    print(\"***********************************************************\")","83865ad2":"test = pd.read_csv('..\/input\/test.csv')\ntest = test.values.reshape(-1,28,28,1)\ntest.shape","eecc7d99":"# predict results\nresults = model.predict(test)","d387c7e8":"# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","ce72bbe6":"submission = pd.concat([pd.Series(range(1,len(test)+1),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"Digit_Recognizer_CNN_Result_2.csv\",index=False)","134c0786":"submission.head(10)","aff5edb5":"<a id=\"1\"><\/a> <br>\n# INTRODUCTION\n* In this kernel, we will be working on Digit Recognizer Dataset (Implementing with Keras).","cfc77f62":"<a id=\"15\"><\/a>\n# Conclusion\n* If you like it, please upvote.\n* If you have any question, I will be appreciate to hear it.","0af79532":"# Convolutional Neural Networks (CNNs \/ ConvNets)","1cee92f8":"<a id=\"9\"><\/a>\n### Data Augmentation","a3c637e9":"* For example,let's look at first sample pixel values","58e6615c":"Content:\n* [Introduction](#1):\n* [Loading the Data Set](#2):\n* [Normalization, Reshape and Label Encoding](#3):\n* [Train-Test Split](#4):\n* [Convolutional Neural Network(Implementing with Keras)](#5):\n* [Define Optimizer](#6):\n* [Compile Model](#7):\n* [Epochs and Batch Size](#8):\n* [Data Augmentation](#9):\n* [Fit the Model](#10):\n* [Evaluate the model](#11):\n* [Predict For Random Sample](#12):\n* [Wrong Predicted Digit Values](#13):\n* [Predict Test Data](#14):\n* [Conclusion](#15):","450044ea":"<a id=\"11\"><\/a>\n## Evaluate the model\n* Validation and Loss visualization\n* Confusion matrix","8f1a708a":"<a id=\"13\"><\/a>\n### Let's show the wrong predicted digit values","b277aeb2":"<a id=\"10\"><\/a>\n### Fit the Model","360e1189":"<a id=\"6\"><\/a>\n### Define Optimizer   \n* Adam optimizer: Change the learning rate","c0a94e5a":"<a id=\"8\"><\/a>\n### Epochs and Batch Size","2b9a93d0":"* **As you can see, some numbers are hard to understand!**","19ad9ce8":"## Implementing with Keras","0353213b":"<a id=\"3\"><\/a> <br>\n## Normalization, Reshape and Label Encoding \n* Normalization\n    * We perform a grayscale normalization to reduce the effect of illumination's differences.\n    * If we perform normalization, CNN works faster.\n* Reshape\n    * Train and test images (28 x 28) \n    * We reshape all data to 28x28x1 3D matrices.\n    * Keras needs an extra dimension in the end which correspond to channels. Our images are gray scaled so it use only one channel. \n* Label Encoding  \n    * Encode labels to one hot vectors \n        * 2 => [0,0,1,0,0,0,0,0,0,0]\n        * 4 => [0,0,0,0,1,0,0,0,0,0]","4b9a7cd3":"<a id=\"14\"><\/a>\n## PREDICT TEST DATA","b989787c":"# Orhan SERTKAYA","4ed1983a":"<a id=\"5\"><\/a>\n## Convolutional Neural Network ","86cdc067":"<a id=\"2\"><\/a> <br>\n## Loading the Data Set\n* In this part we load and visualize the data.","dcd37fbd":"<a id=\"7\"><\/a>\n### Compile Model\n* categorical crossentropy\n* We make binary cross entropy at previous parts and in machine learning tutorial\n* At this time we use categorical crossentropy. That means that we have multi class.\n* <a href=\"https:\/\/ibb.co\/jm1bpp\"><img src=\"https:\/\/preview.ibb.co\/nN3ZaU\/cce.jpg\" alt=\"cce\" border=\"0\"><\/a>","b7d9e517":"<a id=\"12\"><\/a>\n### Predict For Random Sample","04a6ced6":"<a id=\"4\"><\/a>\n## Train-Test Split\n* We split the data into train and test sets.\n* test size is 10%.\n* train size is 90%."}}