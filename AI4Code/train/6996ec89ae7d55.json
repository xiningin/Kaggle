{"cell_type":{"15c720f7":"code","24be151f":"code","4a388aff":"code","32936cc5":"code","01598d23":"code","ee9bd48e":"code","2c02e64d":"code","e1ecb564":"code","ac679d46":"code","2c18687c":"code","184ce18c":"code","5c086136":"code","96fa9321":"code","bc424599":"code","8088082b":"code","5596092e":"markdown","fd501c24":"markdown","6d80157f":"markdown","d2595f90":"markdown"},"source":{"15c720f7":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler, EarlyStopping\nfrom keras import backend as K\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport datetime\n\n# To stop potential randomness\nseed = 42\nnp.random.seed(seed)\n\nimport os\nprint(os.listdir(\"..\/input\"))","24be151f":"sample = pd.read_csv(\"..\/input\/sample_submission.csv\") #28 * 28 pixel\ntrain =  pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","4a388aff":"img_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)\nnum_classes = 10\n\nY_ = train['label'] # target data\nX = train.drop(['label'],axis = 1) #train data\n\nX = X.values.reshape(X.shape[0], img_rows, img_cols, 1).astype('float32')\/255\nY = keras.utils.to_categorical(Y_, num_classes)\n\nX.shape, Y.shape","32936cc5":"def model_initiation(load_model=False, model_name=''):\n    # Based on Model from https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size = 3, padding='same', activation='relu', input_shape = (28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 3, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, kernel_size = 3, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, kernel_size = 4, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.25))\n    model.add(Dense(10, activation='softmax'))\n    \n    if load_model:\n        model.load_weights(model_name)\n\n    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return(model)","01598d23":"# Model\ndef cnn_fit(X_train,y_train, X_test,y_test, batch_size=128, epochs = 100, verbose = 2, cnn=0):\n    model = model_initiation(load_model=False,)\n    # CREATE MORE IMAGES VIA DATA AUGMENTATION - by randomly rotating, scaling, and shifting images.\n    datagen = ImageDataGenerator(rotation_range=10,\n                               zoom_range=0.1,\n                               width_shift_range=0.1,\n                               height_shift_range=0.1)\n\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                              patience=4, \n                                              verbose=verbose, \n                                              factor=0.75, \n                                              min_lr=0.00001)\n    \n    #annealer = LearningRateScheduler(lambda x: 1e-2 * 0.75 ** x)\n    \n    # IMORTANT PART\n    # we save only the best epoch on accuracy\n    file_name_model = (\"model_\"+str(cnn)+\".hdf5\")\n    checkpointer = ModelCheckpoint(monitor='val_acc',\n                                   filepath=(\".\/\"+file_name_model),\n                                   verbose=0, save_best_only=True, mode='max')\n    \n    #earlystopper = EarlyStopping(monitor='val_loss', min_delta=0,\n    #                             patience=17, verbose=1, mode='auto')\n\n    history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                                epochs = epochs, \n                                validation_data = (X_test,y_test),\n                                callbacks=[learning_rate_reduction, checkpointer],\n                                steps_per_epoch=(len(X_train)\/\/batch_size),\n                                verbose = verbose,)\n\n    #score = model.evaluate(X_test,y_test, verbose=0)\n    #print('Test loss:', score[0], ' accuracy:', score[1])\n    return(history, model)","ee9bd48e":"# split data into training set and testing set\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.15, random_state=42)","2c02e64d":"%%time\nhistory, model = cnn_fit(X_train, y_train, X_val, y_val, batch_size=64, epochs = 75, verbose = 2)","e1ecb564":"# plot the accuracy and loss in each process: training and validation\ndef plot_(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    f, [ax1, ax2] = plt.subplots(1,2, figsize=(15, 5))\n    ax1.plot(range(len(acc)), acc, label=\"acc\")\n    ax1.plot(range(len(acc)), val_acc, label=\"val_acc\")\n    ax1.set_title(\"Training Accuracy vs Validation Accuracy\")\n    ax1.legend()\n\n    ax2.plot(range(len(loss)), loss, label=\"loss\")\n    ax2.plot(range(len(loss)), val_loss, label=\"val_loss\")\n    ax2.set_title(\"Training Loss vs Validation Loss\")\n    ax2.legend()","ac679d46":"plot_(history)","2c18687c":"score = model.evaluate(X_val,y_val, verbose=0)\nprint('loss:', round(score[0], 5))\nprint('accuracy:', round(score[1], 5))","184ce18c":"print('Load best Model')\nK.clear_session()\ndel model\ngc.collect()\nmodel = model_initiation(load_model=True, model_name='model_0.hdf5')\nscore = model.evaluate(X_val,y_val, verbose=0)\nprint('loss:', round(score[0], 5))\nprint('accuracy:', round(score[1], 5))","5c086136":"print(test.shape)\nX_sub = test.values.reshape(test.shape[0], img_rows, img_cols, 1).astype('float32')\/255\ntotal_add_models = 0\n\n# add or not add predict from model 0\nif (round(score[1], 5) > 0.996):\n    results = model.predict(X_sub)\n    print('add predict from model 0')\n    total_add_models+=1\nelse: \n    results = np.zeros((test.shape[0], 10))\n    print('skip predict from model 0')\n\nK.clear_session()\ndel model\ndel history\ngc.collect()","96fa9321":"%%time\ncnn=0\nselection_threshold_acc = 0.996\nn_splits = 8\n\nfor batch_size in [64,128]:\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=batch_size)\n    print('START with batch_size:', batch_size)\n    for train_index, test_index in skf.split(X, Y_):\n        cnn+=1\n        time_start = datetime.datetime.now()\n        X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n        y_train, y_test = np.array(Y_)[train_index], np.array(Y_)[test_index]\n        # \u0434\u043b\u044f StratifiedKFold \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u043e\u0441\u0442\u044b\u043b\u044c \u0432 Y\n        y_train = keras.utils.to_categorical(y_train, num_classes)\n        y_test = keras.utils.to_categorical(y_test, num_classes)\n        #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape, )\n\n        history, model = cnn_fit(X_train, y_train, X_test, y_test, \n                                 batch_size=batch_size, epochs=100, verbose=0, cnn=cnn)\n        \n        print(cnn, 'CNN accuracy', \\\n                '| Train last =', round(history.history['acc'][-1],5), \\\n                '| Test max =', round(max(history.history['val_acc']),5), \\\n                '| Took Time: ', (datetime.datetime.now() - time_start),)\n        \n        # Models selection\n        if round(max(history.history['val_acc']), 5) >= selection_threshold_acc:\n            K.clear_session()\n            del model\n            model = model_initiation(load_model=True, model_name='model_'+str(cnn)+'.hdf5')\n            results = results + model.predict(X_sub)\n            total_add_models+=1\n            print('+ ADD', cnn, 'CNN with accuracy:', round(max(history.history['val_acc']), 5), ' Total add models:', total_add_models)\n        else:\n            print('- Skip', cnn, 'CNN not enough accuracy:', round(max(history.history['val_acc']),5))\n\n        K.clear_session()\n        del model\n        del history\n        #gc.collect()\n    print('='*40)\nprint('Total add models:', total_add_models)","bc424599":"print(results.shape)\n\n# Sum predictions\ntest_labels = np.argmax(results, axis=1)\nprint(test_labels[:10])\n\n# submission dataframe\nsub_df = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), pd.Series(test_labels, name=\"Label\")], axis=1)\n\nprint('save submission')\nsub_df.to_csv('MNIST_submission_v29.csv', index=False)","8088082b":"# All models are saved - the submission is reproducible\nprint(os.listdir(\".\/\"))","5596092e":"> # Build (10 CNN models on 10 KFold) * N batch_size with Models selection","fd501c24":"## Test CNN model","6d80157f":"# SUBMIT","d2595f90":"### Validation model"}}