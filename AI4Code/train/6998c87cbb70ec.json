{"cell_type":{"873ab7dc":"code","22663eb0":"code","3f3219c6":"code","9f5ecada":"code","bc07cb24":"code","89e721fd":"code","c3b905d6":"code","8dc892a0":"code","7fbe125a":"code","f946667f":"code","176900db":"code","ebf0f199":"code","56f1ae79":"code","09c25b4b":"code","0cbbb028":"code","adb947c6":"code","94efb4cc":"code","e59dfb9c":"code","b86478de":"code","c5132bd0":"code","90b928a6":"code","0128bd10":"code","f0134095":"code","bdb45e47":"code","696ed5ab":"code","c15a2df0":"code","2d215431":"code","99ca1bc8":"code","652b6273":"code","3204c76d":"code","572aac43":"markdown","391fa47c":"markdown","08dd6404":"markdown","cea7f4b7":"markdown","45431ab5":"markdown","4a3b891b":"markdown","471b95d1":"markdown","6907cf10":"markdown","142a3ee3":"markdown","10e671d1":"markdown","b3cfafbb":"markdown","eb00decf":"markdown","40c34268":"markdown","8bef3b72":"markdown","1dbfb984":"markdown"},"source":{"873ab7dc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical","22663eb0":"(X_train, y_train), (X_test, y_test)= tf.keras.datasets.fashion_mnist.load_data()","3f3219c6":"labels = ['T-shirt\/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneakers','Bag','Ankle boot']","9f5ecada":"X_train.shape","bc07cb24":"y_train[0]","89e721fd":"X_test.shape","c3b905d6":"plt.imshow(X_train[0])","8dc892a0":"plt.imshow(X_train[0], cmap=\"gray_r\")\nplt.axis('off')\nplt.title(y_train[0])","7fbe125a":"X_train1  = np.expand_dims(X_train,-1)\nX_test1  = np.expand_dims(X_test,-1)","f946667f":"X_train1 = X_train1\/255\nX_test1 = X_test1\/255","176900db":"y_train1 = to_categorical(y_train)\ny_test1 = to_categorical(y_test)","ebf0f199":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(Conv2D(64, (2,2), input_shape = (28,28, 1), activation='relu'))\nmodel.add(Conv2D(64, (2,2), activation='relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, (2,2), activation = 'relu'))\nmodel.add(Conv2D(128, (2,2), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64 , (2,2), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dense(10, activation = 'softmax'))\n\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])","56f1ae79":"model.summary()","09c25b4b":"model.fit(X_train1,y_train1,epochs = 100,batch_size = 200,verbose = 1, validation_data=(X_test1,y_test1))","0cbbb028":"X_train2 = X_train.astype('float32')\/255\nX_test2 = X_test.astype('float32')\/255","adb947c6":"from tensorflow.keras.layers import UpSampling2D, Input\n\ninput_img = Input(shape=(28,28,1))\nencoded = Conv2D(32, (2, 2), name='e1', activation='relu', padding='same')(input_img)\nencoded = MaxPooling2D(pool_size=(2, 2), name='e2')(encoded)\nencoded = Conv2D(16, (2, 2), name='e3', activation='relu', padding='same')(encoded)\nencoded = MaxPooling2D(pool_size=(2, 2), name='e4')(encoded)\nencoded = Conv2D(8, (2, 2), name='e5', activation='relu', padding='same')(encoded)\n\ndecoded = UpSampling2D(size=(2, 2), name='d1')(encoded)\ndecoded = Conv2D(16, (2, 2), name='d2', activation='relu', padding='same')(decoded)\ndecoded = UpSampling2D(size=(2, 2), name='d3')(decoded)\ndecoded = Conv2D(32, (2, 2), name='d4', activation='relu', padding='same')(decoded)\ndecoded = Conv2D(1, (2, 2), name='d5', activation='sigmoid', padding='same')(decoded)\n\nautoencoder = Model(input_img, decoded)\nautoencoder.summary()","94efb4cc":"encoder = Model(input_img, encoded)\nencoder.summary()","e59dfb9c":"encoded_input = Input(shape=(7,7,8))\ndecoder_layer = autoencoder.get_layer('d1')(encoded_input)\ndecoder_layer = autoencoder.get_layer('d2')(decoder_layer)\ndecoder_layer = autoencoder.get_layer('d3')(decoder_layer)\ndecoder_layer = autoencoder.get_layer('d4')(decoder_layer)\ndecoder_layer = autoencoder.get_layer('d5')(decoder_layer)\ndecoder = Model(encoded_input, decoder_layer)\ndecoder.summary()","b86478de":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')","c5132bd0":"autoencoder.fit(X_train2, X_train2, epochs=50, batch_size=256, shuffle=True, validation_data=(X_test2, X_test2))\nencoded_imgs = encoder.predict(X_test)\ndecoded_imgs = decoder.predict(encoded_imgs)","90b928a6":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(nrows=2, ncols=10,figsize=(20, 4))\nplt.gray()\nfor indice, row in enumerate(ax):\n    for indice2, col in enumerate(row):\n        if indice == 0:\n            col.imshow(X_test[indice2].reshape(28,28,1))\n        else:\n            col.imshow(decoded_imgs[indice2].reshape(28,28,1))\n\nplt.show()","0128bd10":"plt.imshow(X_test[547])","f0134095":"img_test = encoded_imgs[547]","bdb45e47":"def custom_cosine_sim(a,b):\n    return np.dot(a, b) \/ ( np.linalg.norm(a) * np.linalg.norm(b))","696ed5ab":"from scipy import spatial\ncosine_list = []\nfor index_image,xt in enumerate(encoded_imgs):\n    result = 1 - spatial.distance.cosine(img_test.reshape(-1), xt.reshape(-1))\n    cosine_list.append(dict({'res':result, 'i':index_image}))","c15a2df0":"from operator import itemgetter\ncosine_list.sort(key=itemgetter('res'), reverse=True)","2d215431":"cosine_list","99ca1bc8":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(nrows=1, ncols=10,figsize=(20, 4))\nplt.gray()\nfor indice, row in enumerate(ax):\n    print (cosine_list[indice]['i'])\n    row.imshow(X_test[cosine_list[indice]['i']].reshape(28, 28, 1))\n\nplt.show()","652b6273":"cosine_list.sort(key=itemgetter('res'), reverse=False)","3204c76d":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(nrows=1, ncols=10,figsize=(20, 4))\nplt.gray()\nfor indice, row in enumerate(ax):\n    print (cosine_list[indice]['i'])\n    row.imshow(X_test[cosine_list[indice]['i']].reshape(28,28,1))\n\nplt.show()","572aac43":"On aura \u00e9galement 10000 images pour tester notre mod\u00e8le","391fa47c":"On obtient des r\u00e9sultats assez satisfaisants avec ce mod\u00e8le de classification","08dd6404":"### Cr\u00e9ation et entrainement du mod\u00e8le pour la classification","cea7f4b7":"Quelques exemple ci-dessus d'images associ\u00e9es entre elles (l'image du haut avec celle juste en dessous)\n\nOn va faire l'exp\u00e9rience avec une image prise au hasard dans les images de test","45431ab5":"#### Moins Similaire","4a3b891b":"## Dataset Fashion MNIST (classification, similarit\u00e9, moins similaire)","471b95d1":"On peut voir que la premi\u00e8re image est une superbe chaussure montante de la marque Nike","6907cf10":"### Chargement des donn\u00e9es","142a3ee3":"On a donc ici la liste des images les plus similaires \u00e0 notre image encod\u00e9e","10e671d1":"Ici on va tenter d'associer les images de d\u00e9part avec des images encod\u00e9es en fonction de leur similarit\u00e9","b3cfafbb":"Les t-shirt et les sandales sont, d'apr\u00e8s notre mod\u00e8le, les v\u00eatements qui se ressemblent le moins, \u00e7a peut toujours servir","eb00decf":"### Similarit\u00e9","40c34268":"On a donc 60000 images d'entrainement de format 28x28","8bef3b72":"### Librairies","1dbfb984":"On va afficher ici la liste des images les moins ressemblantes \u00e0 notre exemple"}}