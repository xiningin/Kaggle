{"cell_type":{"125be71d":"code","97a9d3f8":"code","403bc3e3":"code","85dc7b7d":"code","fb398dab":"code","bd603204":"code","33c5ec46":"code","4ea08b22":"code","026a5bd9":"code","f97b78eb":"code","a61e9ea9":"code","7ba5e329":"code","1fb667ae":"code","5ff4a7ef":"code","e00246ce":"code","8497181a":"code","83592118":"code","52568fe5":"code","065a9492":"code","e3fd301b":"code","46c41afd":"code","fa9c60d2":"code","29cdd8b5":"code","0e323f81":"markdown","a355f88d":"markdown","af4d1a4b":"markdown","df87ccb3":"markdown","1e569131":"markdown","f017e9e2":"markdown","0168fcea":"markdown","dd67a985":"markdown","df5cb62f":"markdown","50be1a0b":"markdown","9c2a153c":"markdown","4dbd9345":"markdown","068ba052":"markdown","8b9ec90e":"markdown","129d4676":"markdown","8baae517":"markdown","3dd99da6":"markdown","260042bb":"markdown","eb0e64a5":"markdown","ae3c5752":"markdown","5630b855":"markdown","a73daf73":"markdown","b274776d":"markdown"},"source":{"125be71d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","97a9d3f8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.metrics import mean_squared_error,explained_variance_score,mean_absolute_error,mean_squared_log_error,r2_score,roc_auc_score","403bc3e3":"data_mat = pd.read_csv('..\/input\/student-mat.csv')\ndata_por = pd.read_csv('..\/input\/student-por.csv')\n","85dc7b7d":"data_mat['Dalc'] = data_mat['Dalc'] + data_mat['Walc']     #Alcohol consumption for all days\ndata_por['Dalc'] = data_por['Dalc'] + data_por['Walc']\n","fb398dab":"data_mat=data_mat.drop(['Walc'],axis=1)             #We dont we Walc now\ndata_por=data_por.drop(['Walc'],axis=1)","bd603204":"y_train=data_por['G3'].values      #training on student-por \ny_test=data_mat['G3'].values       #testing on student-mat","33c5ec46":"x_train=data_por.drop(['G3'],axis=1)   #training on student-por \nx_test=data_mat.drop(['G3'],axis=1)    #testing on student-mat ","4ea08b22":"#Only 85 entries with different G1, G2 and paid are there \n'''df=pd.merge(data_mat,data_por, how='inner',on=[\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\",'guardian',\n 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'health', 'absences'])'''","026a5bd9":"x_train.columns","f97b78eb":"le=LabelEncoder()\n#x_train = x_train.apply(le.fit_transform)\ncol=['school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic']\nfor i in col:\n    #print(i)\n    x_train[i]=le.fit_transform(x_train[i])\n    x_test[i]=le.fit_transform(x_test[i])","a61e9ea9":"#Columns with more than 2 values after LabelEncoder need to be OneHotEncoded\ncol=['Mjob','Fjob','reason','guardian']      \nonehot=OneHotEncoder(categorical_features=[8,9,10,11])\nx_train=onehot.fit_transform(x_train).toarray()\nx_test=onehot.fit_transform(x_test).toarray()\n","7ba5e329":"'''#Scaling values\n#col=['age','Medu','Fedu','traveltime','studytime','famrel','freetime','goout','Dalc','health','absences','G1','G2']\ncol=[2,6,7,12,13,23,24,25,26,27,28,29,30]\nfor i in col:\n    x_train[i]=x_train[i]\/x_train[i].max()\n    x_test[i]=x_test[i]\/x_test[i].max()'''","1fb667ae":"#Here it is not required\nfrom sklearn.preprocessing import MinMaxScaler\nsc=MinMaxScaler()\nx_train=sc.fit_transform(x_train)\nx_test=sc.transform(x_test)\nscy=MinMaxScaler()\n\ny_train=scy.fit_transform(y_train.reshape(-1,1))\ny_test=scy.transform(y_test.reshape(-1,1))","5ff4a7ef":"'''y_train=y_train\/y_train.max()\ny_test=y_test\/y_test.max()  ''' \n\n","e00246ce":"from sklearn import svm\nsvm_reg=svm.SVR(kernel='linear')\nsvm_reg.fit(x_train,y_train)\ny_pred=svm_reg.predict(x_test)\n\n\nmse2=mean_squared_error(y_test,y_pred)\nvar2=explained_variance_score(y_test,y_pred)\nmae2=mean_absolute_error(y_test,y_pred)\nr22=r2_score(y_test,y_pred)\nprint(mse2,var2,mae2,r22)\nplt.scatter(np.arange(1,len(y_test)+1).tolist(), y_test, color = 'blue')\nplt.scatter(np.arange(1,len(y_pred)+1).tolist(), y_pred, color = 'red')\nplt.title(\"SVM\")\nplt.xlabel(\"Entry\")\nplt.ylabel(\"Values\")\nplt.show()","8497181a":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(x_train,y_train)\n\ny_pred=lr.predict(x_test)\n\n\nmse3=mean_squared_error(y_test,y_pred)\nvar3=explained_variance_score(y_test,y_pred)\nmae3=mean_absolute_error(y_test,y_pred)\nr23=r2_score(y_test,y_pred)\nprint(mse3,var3,mae3,r23)\nplt.scatter(np.arange(1,len(y_test)+1).tolist(), y_test, color = 'blue')\nplt.scatter(np.arange(1,len(y_pred)+1).tolist(), y_pred, color = 'red')\nplt.title(\"Linear Regression\")\nplt.xlabel(\"Entry\")\nplt.ylabel(\"Values\")\nplt.show()","83592118":"from sklearn.tree import DecisionTreeRegressor\nclf=DecisionTreeRegressor()\nclf=clf.fit(x_train,y_train)\n\ny_pred=clf.predict(x_test)\n\n\nmse4=mean_squared_error(y_test,y_pred)\nvar4=explained_variance_score(y_test,y_pred)\nmae4=mean_absolute_error(y_test,y_pred)\nr24=r2_score(y_test,y_pred)\nprint(mse4,var4,mae4,r24)\nplt.scatter(np.arange(1,len(y_test)+1).tolist(), y_test, color = 'blue')\nplt.scatter(np.arange(1,len(y_pred)+1).tolist(), y_pred, color = 'red')\nplt.title(\"Decision tree\")\nplt.xlabel(\"Entry\")\nplt.ylabel(\"Values\")\nplt.show()","52568fe5":"from sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor(n_estimators=500)\nrf.fit(x_train,y_train)\n\ny_pred=rf.predict(x_test)\n\n\nmse5=mean_squared_error(y_test,y_pred)\nvar5=explained_variance_score(y_test,y_pred)\nmae5=mean_absolute_error(y_test,y_pred)\nr25=r2_score(y_test,y_pred)\nprint(mse5,var5,mae5,r25)\nplt.scatter(np.arange(1,len(y_test)+1).tolist(), y_test, color = 'blue')\nplt.scatter(np.arange(1,len(y_pred)+1).tolist(), y_pred, color = 'red')\nplt.title(\"Random Forest\")\nplt.xlabel(\"Entry\")\nplt.ylabel(\"Values\")\nplt.show()","065a9492":"from sklearn.ensemble import AdaBoostRegressor\nad=AdaBoostRegressor()\nad.fit(x_train,y_train)\n\ny_pred=ad.predict(x_test)\n\nmse6=mean_squared_error(y_test,y_pred)\nvar6=explained_variance_score(y_test,y_pred)\nmae6=mean_absolute_error(y_test,y_pred)\nr26=r2_score(y_test,y_pred)\nprint(mse6,var6,mae6,r26)\nplt.scatter(np.arange(1,len(y_test)+1).tolist(), y_test, color = 'blue')\nplt.scatter(np.arange(1,len(y_pred)+1).tolist(), y_pred, color = 'red')\nplt.title(\"Adaboost\")\nplt.xlabel(\"Entry\")\nplt.ylabel(\"Values\")\nplt.show()","e3fd301b":"x=['SVM','Linear Reg','DecsTree','Randforest','Adaboost']\ny=[mse2,mse3,mse4,mse5,mse6]\nfig, ax = plt.subplots()\nplt.bar(x,y)\nax.set_ylabel('Mean sq error')\nax.set_xlabel('Model type')\n","46c41afd":"y=[var2,var3,var4,var5,var6]\nfig, ax = plt.subplots()\nplt.bar(x,y)\nax.set_ylabel('Variance')\nax.set_xlabel('Model type')\n","fa9c60d2":"y=[mae2,mae3,mae4,mae5,mae6]\nfig, ax = plt.subplots()\nplt.bar(x,y)\nax.set_ylabel('Mean abs error')\nax.set_xlabel('Model type')\n","29cdd8b5":"y=[r22,r23,r24,r25,r26]\nfig, ax = plt.subplots()\nplt.bar(x,y)\nax.set_ylabel('r2 score')\nax.set_xlabel('Model type')\n","0e323f81":"**Mean Squared Error**","a355f88d":"**Decision Tree Regressor**","af4d1a4b":"# **Visualizations and Evaluations**","df87ccb3":"**AdaBoost Regressor**","1e569131":"# **Reading Data**","f017e9e2":"**Random Forest Regressor**","0168fcea":"# **Reasoning**","dd67a985":"# **Label Encoder**","df5cb62f":"# **Dependent Variables**","50be1a0b":"# **Preprocessing data**","9c2a153c":"# **Inferences**","4dbd9345":"# **Model Creation**","068ba052":"**SVM Regressor**","8b9ec90e":"**Linear Regressor**","129d4676":"**R2 score**","8baae517":"# **Importing Libraries**","3dd99da6":"As we see the scatter plot between actual values and predicted values Linear regressor and Random Forest Regressor are most accurate among all other models and Adaboost and Decision Tree are least accurate.","260042bb":"# **Scaling **","eb0e64a5":"**Mean Absolute Error**","ae3c5752":"* Linear Regression and Random Forest Regressor are best models for this data with mean squared error of 0.0123 and 0.011 respectively .\n* Decision Tree performs worst with mean squared error of 0.019.\n","5630b855":"**Variance**","a73daf73":"# **Define dependent variables**","b274776d":"# **One Hot Encoder**"}}