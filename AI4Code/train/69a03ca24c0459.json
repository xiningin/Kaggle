{"cell_type":{"a59e6bc3":"code","0dd1b197":"code","08f85260":"code","7f2e7c01":"code","fc251aa4":"code","60c62f9f":"code","5e6336f0":"code","8969b1fc":"code","deb64e8c":"code","25e31cce":"code","acd0ffd8":"code","a252fa06":"code","3e6cb6fd":"code","f42383e8":"code","a7811c6c":"code","e4e48df9":"code","a41640cf":"code","3759abd1":"code","e8df7c41":"code","333e9122":"code","00397c1c":"code","15d48f30":"code","37668fd2":"code","c595e37a":"code","d77c7d45":"code","a36f5cd2":"code","4dd4937c":"code","5cfacb4c":"code","8de86264":"code","4efbb80b":"code","54f49cb1":"code","2f2115e5":"code","6363e540":"code","5299f9cb":"code","4da5d035":"code","0b908060":"code","2250c1f1":"code","4befe134":"code","4c8acd60":"code","810f24ee":"code","500b5b79":"code","bc15df3a":"code","b39f7efb":"code","d2ed93e1":"code","0a356194":"code","6fd2ef10":"code","2c9c8f0d":"code","19ae660e":"code","274fa5b6":"code","230ab900":"code","4f2ddb7e":"code","bd5d813a":"code","023e4819":"code","4a63d6b8":"code","5fd2f848":"code","a3ae8789":"code","bf0d8420":"code","815574ac":"code","94a4047d":"code","87e62748":"code","b8375003":"code","0bd19013":"code","ee3c498b":"code","513891d9":"code","6a0dd23c":"code","68de0c6f":"code","f92cf561":"code","cb61da3f":"code","073114d3":"code","88a6ebf8":"code","bb473cb7":"code","1826bd52":"code","15e7134b":"code","77d9ed59":"code","5873d9a7":"code","b1b928b8":"code","beabb94c":"code","df2a6cd9":"code","6040d298":"code","ac2a55dd":"code","949051f8":"code","8af8d8b8":"code","efcdb610":"code","8c304cb0":"code","f2afe6bf":"code","ae1926a5":"code","07b822fe":"code","e087bce5":"code","82c94ed1":"code","e6678110":"code","f84db66c":"code","638f0432":"code","bbe56b56":"code","95e7b4b4":"code","247c1b13":"code","70975265":"code","49b87502":"markdown","920b54e5":"markdown","9ed49d96":"markdown","d266e4b7":"markdown","b962aed9":"markdown","7a6e42dd":"markdown","5e5b3cc0":"markdown","6b825379":"markdown","83baa6a4":"markdown","067e8893":"markdown","e61085b6":"markdown","3bcacbf2":"markdown","2d50ba25":"markdown","8ea9006f":"markdown","3b2f237a":"markdown","8bcd23d8":"markdown","7aaab094":"markdown","ffe7cd5b":"markdown","0c0eb365":"markdown","090a09f0":"markdown","bcdffbc1":"markdown"},"source":{"a59e6bc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0dd1b197":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","08f85260":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.head()","7f2e7c01":"test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.head()","fc251aa4":"passengerId = test_data.PassengerId","60c62f9f":"!pip install fancyimpute","5e6336f0":"data.info()","8969b1fc":"data.shape","deb64e8c":"round(100*(data.isnull().sum()\/len(data.index)),2)","25e31cce":"round(100*(test_data.isnull().sum()\/len(test_data.index)),2)","acd0ffd8":"#checking Cabin\ndata[['Cabin']].head(20)","a252fa06":"#Lets split the column into Cabin Letter and Cabin number. As there are large number of missing values, instead of imputing we will replace the missing values with letter 'M'\ndata[['cabin_letter', 'cabin_number']] = data['Cabin'].str.extract('([A-Z])(\\d*)', expand = True)\ntest_data[['cabin_letter', 'cabin_number']] = test_data['Cabin'].str.extract('([A-Z])(\\d*)', expand = True)\n","3e6cb6fd":"data.drop(['Cabin','cabin_number'],axis=1,inplace=True)\ntest_data.drop(['Cabin','cabin_number'],axis=1,inplace=True)","f42383e8":"data['cabin_letter'].value_counts()","a7811c6c":"pd.crosstab(data.Survived,data.cabin_letter,normalize=True)","e4e48df9":"data['cabin_letter'] = data['cabin_letter'].fillna('M')\ntest_data['cabin_letter'] = data['cabin_letter'].fillna('M')\n\nround(100*(data.isnull().sum()\/len(data.index)),2)","a41640cf":"round(100*(test_data.isnull().sum()\/len(test_data.index)),2)","3759abd1":"#checking Embarked\ndata['Embarked'].value_counts(dropna=False) ","e8df7c41":"test_data['Embarked'].value_counts(dropna=False) ","333e9122":"data.loc[data['Embarked'].isnull()]","00397c1c":"data.groupby(by=['Embarked','Pclass','cabin_letter'])['Fare'].describe()","15d48f30":"data['Embarked'] = data['Embarked'].fillna('C')","37668fd2":"round(100*(data.isnull().sum()\/len(data.index)),2)","c595e37a":"#data.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)","d77c7d45":"round(100*(test_data.isnull().sum()\/len(test_data.index)),2)","a36f5cd2":"#Function to check total counts, skewness and importance of a categorical column based on conversion rate. \ndef check_count_conversion_rate(data,X,target):\n    #checking counts of col\n    col_counts = pd.DataFrame(data[X].value_counts()).reset_index()\n    col_counts.columns = [X,'Counts']\n    col_counts['Total%'] = col_counts['Counts']\/len(data.index)\n    #checking conversion rate by col\n    groupby_col = pd.DataFrame(data.groupby(X)[target].mean()).reset_index()\n\n    col_counts_percentage = col_counts.merge(groupby_col,how='inner',on=X)\n    return col_counts_percentage\n","4dd4937c":"# Checking Sex column \npd.crosstab(data.Survived,data.Sex,normalize = True)","5cfacb4c":"check_count_conversion_rate(data,'Sex','Survived')","8de86264":"#Checking distribution of PClass \nsns.countplot(\"Survived\",data=data,hue='Pclass')","4efbb80b":"check_count_conversion_rate(data,'Pclass','Survived')","54f49cb1":"#Checking Age\ndata.loc[data['Survived']==0]['Age'].hist(color='green')\ndata.loc[data['Survived']==1]['Age'].hist(color='blue')\n","2f2115e5":"sns.boxplot(y='Age',x='Survived',data=data)","6363e540":"#Checking Age\ndata.loc[data['Survived']==0]['Fare'].hist(color='green',bins=50)\ndata.loc[data['Survived']==1]['Fare'].hist(color='blue',bins=50)\n","5299f9cb":"sns.boxplot(x='Survived',y='Fare',data=data)","4da5d035":"sns.boxplot(x='Pclass',y='Fare',hue='Survived',data=data)","0b908060":"data['Parch'].value_counts(dropna=False)","2250c1f1":"check_count_conversion_rate(data,'Parch','Survived')","4befe134":"test_data['Parch'].value_counts(dropna=False)","4c8acd60":"sns.heatmap(data.corr(),annot=True)","810f24ee":"data.head()","500b5b79":"#Extracting titles from name\ndata['Title'] = [i.split('.')[0] for i in data['Name']]\ndata['Title'] = [i.split(',')[1] for i in data['Title']]\n\ntest_data['Title'] = [i.split('.')[0] for i in test_data['Name']]\ntest_data['Title'] = [i.split(',')[1] for i in test_data['Title']]\n\ndata['Title'].value_counts(dropna = False)","bc15df3a":"data['Title'].unique()","b39f7efb":"data.Title = data.Title.apply(lambda x:x.strip())\ntest_data.Title = test_data.Title.apply(lambda x:x.strip())","d2ed93e1":"data.Title.replace(('Ms','Mlle','Mme'),'Miss',inplace=True)\ndata.Title.replace(('Rev','Don','Dr','Major','Lady','Sir','Col','Capt','the Countess','Jonkheer'),'Royalty',inplace=True)\n\ntest_data.Title.replace(('Ms','Mlle','Mme'),'Miss',inplace=True)\ntest_data.Title.replace(('Rev','Don','Dr','Major','Lady','Sir','Col','Capt','the Countess','Jonkheer','Dona'),'Royalty',inplace=True)\n\ndata.Title.value_counts(dropna=False)","0a356194":"test_data.Title.value_counts(dropna=False)","6fd2ef10":"check_count_conversion_rate(data,'Title','Survived')","2c9c8f0d":"data.drop(['Name','PassengerId','Ticket'],axis=1,inplace=True)\ntest_data.drop(['Name','PassengerId','Ticket'],axis=1,inplace=True)","19ae660e":"#Imputing age and Fare","274fa5b6":"data.head()","230ab900":"test_data.head()","4f2ddb7e":"#Checking Age - We will impute Age by using Iterative Imputer","bd5d813a":"#Creating dummies\ndata.Sex.replace(['female','male'],[0,1],inplace=True)\ntest_data.Sex.replace(['female','male'],[0,1],inplace=True)","023e4819":"data.head()","4a63d6b8":"# For pd.get_dummies: the data should be object data type\n# Create a subset of categorical data\ndata_cat = data[['Pclass','SibSp','Embarked','cabin_letter','Title']]\ntest_data_cat = test_data[['Pclass','SibSp','Embarked','cabin_letter','Title']]\ndata_cat.head()","5fd2f848":"data_cat.info()","a3ae8789":"#There are still categorical columns which are of type int. Convert them to object data datatype\nfor i in ['Pclass','SibSp','Embarked','cabin_letter','Title']:\n  data_cat[i] = data_cat[i].astype(str)\n  test_data_cat[i] = test_data_cat[i].astype(str)","bf0d8420":"test_data_cat.info()","815574ac":"#Now we can create dummies\ndata_cat_dummy1 = pd.get_dummies(data_cat[['Pclass','SibSp','Embarked','Title']],drop_first=True)\ndata_cat_dummy2 = pd.get_dummies(data_cat['cabin_letter'])\n\ntest_data_cat_dummy1 = pd.get_dummies(test_data_cat[['Pclass','SibSp','Embarked','Title']],drop_first=True)\ntest_data_cat_dummy2 = pd.get_dummies(test_data_cat['cabin_letter'])","94a4047d":"#Dropping value 'M' (Missing) as this was the value which we replace the NaN \ndata_cat_dummy2.drop(['M'],axis=1,inplace=True)\ntest_data_cat_dummy2.drop(['M'],axis=1,inplace=True)","87e62748":"data.drop(['Pclass','SibSp','Embarked','cabin_letter','Title'],axis=1,inplace=True)\ntest_data.drop(['Pclass','SibSp','Embarked','cabin_letter','Title'],axis=1,inplace=True)","b8375003":"data.head()","0bd19013":"data = pd.concat([data,data_cat_dummy1,data_cat_dummy2],axis=1)\ntest_data = pd.concat([test_data,test_data_cat_dummy1,test_data_cat_dummy2],axis=1)\n\nprint(data.shape)","ee3c498b":"print(test_data.shape)","513891d9":"from fancyimpute import IterativeImputer","6a0dd23c":"#Using Iterative Imputer to impute missing value of Age\n\n#First let us store the column names\ndata_ii  = data.drop(['Survived'],axis=1)\ndata_cols = data_ii.columns\n\nii = IterativeImputer()\ndata_clean = pd.DataFrame(ii.fit_transform(data.drop(['Survived'],axis=1)))\ntest_data_clean = pd.DataFrame(ii.transform(test_data))","68de0c6f":"# the output is the numpy array.\n# ii looks for all the columns which have missing values and then imputes them\n# need to cross check the age imputation as ii can assign values from -inf to +inf\ndata_clean.columns=data_cols\ntest_data_clean.columns = data_cols\ndata_clean.head()","f92cf561":"data.shape","cb61da3f":"#round(100*(data_clean.isnull().sum()\/len(data_clean)),2)\nround(100*(test_data_clean.isnull().sum()\/len(test_data_clean)),2)","073114d3":"#cross checking age\nsns.boxplot(y=test_data_clean['Age'])","88a6ebf8":"sns.boxplot(y=test_data['Age'])","bb473cb7":"#to convert continuous variables to categorical using pd.cut function\n#data_clean['ageGroup'] = pd.cut(data_clean['Age'],bins = [0,16,32,48,64,300],labels=[0,1,2,3,4])\n#test_data_clean['ageGroup'] = pd.cut(test_data_clean['Age'],bins = [0,16,32,48,64,300],labels=[0,1,2,3,4])\n\n","1826bd52":"#data_clean.loc[data_clean['ageGroup'].isnull()]","15e7134b":"#data_clean.drop(['Age'],axis=1,inplace=True)\n#test_data_clean.drop(['Age'],axis=1,inplace=True)","77d9ed59":"data_clean.columns","5873d9a7":"#checking for outlier\nsns.boxplot(y=data['Fare'])","b1b928b8":"# There is only one data point that lies far apart. It is better to delete that one data point and keep the others as it is.\n# the other outliers are actually depicting pattern in the data. It is better not to cap it. Also the outliers are more in number.\n# Any change in the outlier will change the pattern and meaning of the data.\ndata_clean.drop(data_clean.index[data_clean['Fare']>300],inplace=True)\ndata.drop(data.index[data['Fare']>300],inplace=True)","beabb94c":"data_clean.shape\ndata.shape","df2a6cd9":"#creating dummies for AgeGroup now\n#data_cat_dummy = pd.get_dummies(data_clean['ageGroup'],drop_first=True)\n#test_data_clean_cat_dummy = pd.get_dummies(test_data_clean['ageGroup'],drop_first=True)\n\n#data_clean = pd.concat([data_clean,data_cat_dummy],axis=1)\n#test_data_clean = pd.concat([test_data_clean,test_data_clean_cat_dummy],axis=1)","6040d298":"data_clean.isnull().sum()","ac2a55dd":"#diving data in train and test\nX = data_clean\ny = data['Survived']\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100,stratify=y)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","949051f8":"#checking data imbalance\n100 * y_train.value_counts()\/len(y_train)","8af8d8b8":"# Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\n\ny_pred = logreg.predict(X_test)\nprint(\"Accuracy: {}\".format(metrics.accuracy_score(y_test,y_pred)))\nprint(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred)))\n","efcdb610":"# Logistic Regression model with class_weight\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlogreg = LogisticRegression(class_weight='balanced')\n#logreg = LogisticRegression(class_weight={0:38.164251,1:61.835749})\nlogreg.fit(X_train,y_train)\n\ny_pred = logreg.predict(X_test)\nprint(\"Accuracy: {}\".format(metrics.accuracy_score(y_test,y_pred)))\nprint(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred)))\n","8c304cb0":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train,y_train)\n\ny_pred = rf_model.predict(X_test)\nprint(\"Accuracy: {}\".format(metrics.accuracy_score(y_test,y_pred)))\nprint(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred)))\n","f2afe6bf":"#hyper parameter tuning\n# Create the parameter grid based on the results of random search \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\ncv = StratifiedShuffleSplit(n_splits=3, random_state=2)\nparam_grid = {\n    'max_depth': [4,5,10],\n    'min_samples_leaf': [5, 6,7],\n    'min_samples_split': [5, 10, 16],\n    'n_estimators': [100,500,700], \n    'max_features': [5,10,15]\n}\n# Create a based model\nrf = RandomForestClassifier()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = cv, n_jobs = -1,verbose = 1)\n","ae1926a5":"grid_search.fit(X_train, y_train)","07b822fe":"print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","e087bce5":"#using best parameters after tuning\nrf_model = RandomForestClassifier(class_weight='balanced',\n                                  criterion='gini',\n                                  min_samples_leaf =5,\n                                  min_samples_split= 5,\n                                  n_estimators=100,\n                                  max_features=10,\n                                  max_depth=4)\nrf_model.fit(X_train,y_train)\ny_pred = rf_model.predict(X_test)\nprint(\"Accuracy: {}\".format(metrics.accuracy_score(y_test,y_pred)))\nprint(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred)))","82c94ed1":"#Cheking feature importance\npd.concat([pd.DataFrame(X.columns,columns = ['variables']),\n           pd.DataFrame(rf_model.feature_importances_,columns = ['importance'])],\n          axis=1).sort_values(by='importance',ascending=False)","e6678110":"test_data_clean.isnull().sum()","f84db66c":"#scaling test data\ntest_data_scaled = scaler.transform(test_data_clean)","638f0432":"test_pred = rf_model.predict(test_data_scaled)","bbe56b56":"submission = pd.DataFrame({\n        \"PassengerId\": passengerId,\n        \"Survived\": test_pred\n    })\nsubmission['Survived'] = submission['Survived'].astype('int')\n#submission.to_csv('gender_submission.csv', index=False)","95e7b4b4":"submission.head()","247c1b13":"submission.to_csv('gender_submission_results1.csv', index=False)","70975265":"#Score in Kaggle: 0.7846","49b87502":"#### Feature Engineering","920b54e5":"- Mean value did not change after imputation. Therefore, all the imputations are correct","9ed49d96":"#### Scaling Data","d266e4b7":"Lets bucket the 'Age' into Groups as follows\n- 'Age' < 16 :0\n- 16 & <=32 :1\n- 32 & <=48 :2\n- 48 & <=64 :3\n- 'Age' >64 :4 ","b962aed9":"Look for variables that looks most important in deciding whether a person survived or not.\n- PClass\n- Sex\n- Age","7a6e42dd":"Combining together the titles with similar meanings\n","5e5b3cc0":"- out of all the passengers 26% of them were female who survived.\n- Out of all the females, 74% of them survived","6b825379":"- We notice that after applying the class_weight option, the recall is increased to 78% from 73%","83baa6a4":"Lets see if we can derive some columns","067e8893":"#### Exploratory data analysis","e61085b6":"- Though there are less number of passengers in Pclass 1 but they have the maximum survival rate.\n- 63% of first class passengers survived and 48% of second class passengers survived. ","3bcacbf2":"- The null values are in Cabin B28, Pclass 1 and have a Ticket fare of 80.\n- Lets see how PClass Fare and Embarked are related.","2d50ba25":"#### Predicting on Provided Test data","8ea9006f":"- Only Age and Fare are missing. We will impute these missing values by using IterativeImputer. Will work on these in feature engineering section.","3b2f237a":"Let us use text mining to extract details from Ticket and Nane.\n- From Name we can extract title like Mr, Ms\n- From Ticket we can extract the letters which might be usefull for us.","8bcd23d8":"#### Missing Value Treatment","7aaab094":"- Younger Passengers survuved more","ffe7cd5b":"- Miss and Mrs Survived the most with 70% and ~80% survival rate\n- Among the males, passengers with Title Master have the highest conversion rate ~57% \n","0c0eb365":"- Of all the passengers, passengers in cabin B and C survived the most.\n- As the cabin letters range from A-G,T, lets replace the missing cabin letter with letter 'M","090a09f0":"- Higher fare passegers survived more.","bcdffbc1":"#### Modelling"}}