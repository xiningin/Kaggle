{"cell_type":{"e05b679f":"code","ba3e88f9":"code","c4897fb1":"code","8e19d44d":"code","242dd489":"code","f0de122e":"code","8cd0c5c1":"code","41faf918":"code","3837aa65":"code","688c4f37":"code","391493dd":"code","eafff75a":"code","4edf6246":"code","5c520ab8":"code","2b382fc0":"code","e69edec1":"code","31556a33":"code","8754839b":"code","ca28bc80":"code","6025cbee":"code","9f2ddc85":"code","fe867a7d":"code","5904d15a":"code","93cba681":"code","ccc5d260":"code","de2fc529":"code","2fce89f9":"code","2400d114":"code","796b534c":"code","06c6de6b":"code","15da3f10":"code","19871856":"code","b01e6fb2":"code","637b3099":"code","96404525":"code","522e8cfa":"code","7f584b62":"code","dafdd885":"code","6ea86714":"code","20486d3b":"code","1179ede1":"code","9433b0d8":"code","41a0205f":"code","53675ecc":"code","463b660f":"code","9bf2df4e":"code","c23ee891":"code","3d410695":"code","e81e96ce":"code","c175d82d":"code","14d9eec4":"code","d0f64fac":"markdown","81aa4089":"markdown","917741a5":"markdown","d08c880c":"markdown","f3a26d4a":"markdown","63e28c1d":"markdown","0107705b":"markdown","2ab31933":"markdown","a66106de":"markdown","79aa3858":"markdown","167bfaeb":"markdown","75e674e5":"markdown","86512d4b":"markdown","f6085434":"markdown","4ada4b56":"markdown","b41c0075":"markdown","ee810dfe":"markdown","a2bbe2c7":"markdown","6f675995":"markdown","5756f640":"markdown","a4e076b1":"markdown","0b1ffe2e":"markdown"},"source":{"e05b679f":"!pip install WordCloud\nfrom wordcloud import WordCloud","ba3e88f9":"import nltk # Natural Language tool kit \nimport matplotlib.pyplot  as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport numpy as np \nimport pandas as pd\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\n\nimport re \nfrom collections import Counter\nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \n\nfrom sklearn.metrics import fbeta_score\n","c4897fb1":"data = pd.read_csv('..\/input\/eurecom-aml-2021-challenge-3\/train.csv')\n","8e19d44d":"data.head()","242dd489":"data.info()","f0de122e":"train_df = data","8cd0c5c1":"target_conversion = {\n    'neutral': 0,\n    'positive': 1,\n    'negative': -1\n}\ntrain_df['target'] = train_df['sentiment'].map(target_conversion)\nval_df['target'] = val_df['sentiment'].map(target_conversion)","41faf918":"plt.rcParams['figure.figsize'] = [10,5] \ntrain_df['target'].hist(bins=30)\nplt.xticks([-1, 0, 1],labels=['Negative', 'Neutral', \"Positive\"])\nplt.title('Target  Label Distribution in train data')\nplt.show()","3837aa65":"\ntrain_df['length'] = train_df.text.apply(len)\n","688c4f37":"print('The average character count is:',  train_df.length.mean())\nprint('The minimum character count is:',  train_df.length.min())\nprint('The maximujm character count is:',  train_df.length.max())","391493dd":"plt.hist(train_df.length,bins=100)\nplt.xlabel('WORD_COUNTS')\nplt.title('WORD COUNT DISTRIBUTION')\nplt.show()","eafff75a":"neg_df = train_df[train_df.target == -1]\npos_df =  train_df[train_df.target == 1]\nneu_df = train_df[train_df.target == 0]","4edf6246":"sentences_neg = neg_df.text.to_list()\nsentences_pos = pos_df.text.to_list()\nsentences_neu = neu_df.text.to_list()","5c520ab8":"sentences_neg = \" \".join(sentences_neg)\nsentences_pos = \" \".join(sentences_pos)\nsentences_neu = \" \".join(sentences_neu)","2b382fc0":"plt.figure(figsize=(15,15))\nplt.imshow(WordCloud().generate(sentences_neg))","e69edec1":"plt.figure(figsize=(15,15))\nplt.imshow(WordCloud().generate(sentences_pos))","31556a33":"plt.figure(figsize=(15,15))\nplt.imshow(WordCloud().generate(sentences_neu))","8754839b":"#removing stop word\ndef clean_text(text):\n \n    text = re.sub('[^a-zA-Z]', ' ', text)  \n\n    text = text.lower()  \n\n    # split to array(default delimiter is \" \") \n    text = text.split()  \n    \n    text = [w for w in text if not w in set(stopwords.words('english'))] \n\n    text = ' '.join(text)    \n            \n    return text","ca28bc80":"train_df['text'] = train_df['text'].apply(lambda x : clean_text(x))","6025cbee":"def counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","9f2ddc85":"text_values = train_df[\"text\"]\n\ncounter = counter_word(text_values)","fe867a7d":"print(f\"The len of unique words is: {len(counter)}\")\nlist(counter.items())[:10]","5904d15a":"vocab_size = len(counter)\nembedding_dim = 64\n\n# Max number of words in each complaint.\nmax_length = 30\ntrunc_type='post'\npadding_type='post'\n\n# oov_took its set for words out our word index\noov_tok = \"<XXX>\"\nseq_len = 12","93cba681":"train, val = train_test_split(train_df, test_size=0.1, random_state=1)\n\ntraining_sentences = train['text'].values\ntraining_labels = train['target'].values\ntraining_labels = tf.keras.utils.to_categorical(training_labels, num_classes=3)\n\nval_sentences = val['text'].values\nval_labels = val['target'].values\nval_labels = tf.keras.utils.to_categorical(val_labels, num_classes=3)","ccc5d260":"print('The Shape of training ',training_sentences.shape)\nprint('The Shape of validation',val_sentences.shape)","de2fc529":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)","2fce89f9":"word_index = tokenizer.word_index","2400d114":"print(\"THe first word Index are: \")\nfor x in list(word_index)[0:15]:\n    print (\" {},  {} \".format(x,  word_index[x]))","796b534c":"training_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","06c6de6b":"print(train.text[1])\nprint(training_sequences[1])","15da3f10":"val_sequences = tokenizer.texts_to_sequences(val_sentences)\nval_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","19871856":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(14, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(3, activation='softmax') \n])\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","b01e6fb2":"num_epochs = 2\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(val_padded, val_labels))","637b3099":"val_predictions = model.predict_classes(val_padded)\nfbeta = fbeta_score(val['target'].values, val_predictions, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)","96404525":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","522e8cfa":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","7f584b62":"vocab_size = len(counter)  # Only consider the top 20k words\nmaxlen = 120  # Only consider the first 200 words of each movie review\nx_train = training_sequences\ny_train = training_labels\nx_val = val_sequences\ny_val = val_labels\nprint(len(x_train), \"Training sequences\")\nprint(len(x_val), \"Validation sequences\")\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)","dafdd885":"embed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(20, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(3, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)","6ea86714":"model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n)","20486d3b":"val_predictions = model.predict(x_val)\ny_classes = val_predictions.argmax(axis=-1)\nfbeta = fbeta_score(val['target'].values, y_classes, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)","1179ede1":"count_vect = CountVectorizer()","9433b0d8":"X_train_counts = count_vect.fit_transform(list(train['text'].values))\nX_val_counts = count_vect.transform(list(val['text'].values))","41a0205f":"clf = RandomForestClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train_counts, train['target'])\nscores = cross_val_score(clf, X_train_counts, train['target'], cv=5)","53675ecc":"print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\np = clf.predict(X_val_counts)\nfbeta = fbeta_score(val['target'].values, p, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)\naccuracy = (p == val['target'].values).mean()\nprint('The accuracy of our Random Forest classifier with 100 estimators is: {:.2f}%'.format(accuracy*100))","463b660f":"clf = RandomForestClassifier(n_estimators=200, random_state=0)\nclf.fit(X_train_counts, train['target'])\nscores = cross_val_score(clf, X_train_counts, train['target'], cv=5)","9bf2df4e":"print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\np = clf.predict(X_val_counts)\nfbeta = fbeta_score(val['target'].values, p, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)\naccuracy = (p == val['target'].values).mean()\nprint('The accuracy of our Random Forest classifier with 200 estimators is: {:.2f}%'.format(accuracy*100))","c23ee891":"clf1 = AdaBoostClassifier(n_estimators=300, random_state=0)\nclf1.fit(X_train_counts, train['target'])\nscores = cross_val_score(clf, X_train_counts, train['target'], cv=5)","3d410695":"print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\np = clf1.predict(X_val_counts)\nfbeta = fbeta_score(val['target'].values, p, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)\naccuracy = (p == val['target'].values).mean()\nprint('The accuracy of our Ada boost classifier with 300 estimators is: {:.2f}%'.format(accuracy*100))","e81e96ce":"clf = RandomForestClassifier(n_estimators=300, random_state=0)\nclf.fit(X_train_counts, train['target'])\nscores = cross_val_score(clf, X_train_counts, train['target'], cv=5)","c175d82d":"print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\np = clf.predict(X_val_counts)\nfbeta = fbeta_score(val['target'].values, p, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)\naccuracy = (p == val['target'].values).mean()\nprint('The accuracy of our Random Forest classifier with 300 estimators is: {:.2f}%'.format(accuracy*100))","14d9eec4":"test_predictions_nb = clf.predict(X_test_counts)\n\nsubmission_df = pd.DataFrame()\nsubmission_df['textID'] = test['textID']\nsubmission_df['sentiment'] = test_predictions_nb\nsubmission_df.to_csv('baseline.csv', index=False)","d0f64fac":"## Data Preprocessing","81aa4089":"We have experminted with various models starting from a LSTM network, transformer network and some bagging methods like Random forest and adaboost classifier.","917741a5":"The clean_text, is a pipeline to remove punctuation and stop words that do are not necessary for a model.","d08c880c":"It can be seen that the majority of the samples belong to class-0, but both the Negative and Positive class also have significant samples, thereby we can confirm theres not much class imbalance in the dataset.Hence, any model that we use may not be biased to the label with majority representation.","f3a26d4a":"we have to convert the text experssion of target labels positive, negative and neutral into , a number encdoing of 1 , -1 and 0 respectively, so that we can fit the target labels into the machine learning model.","63e28c1d":"## Random Forest","0107705b":"**Exploring the length characters in each sentence of the text column**\n","2ab31933":"### DATA PREPROCESSING AND CLEANING.","a66106de":"Above, wordcloud we can see that the words that were most repeated in negative class. It is evident from, words like sad, sorry , hurt etc.","79aa3858":"Above, wordcloud we can see that the words that were most repeated in neutral class. It is strange that some of the common words that do have sentiment either positive and negative feeling attached to them are observed even in a neutral class.","167bfaeb":"# Adaboost Classifier","75e674e5":"## LSTM NETWORK","86512d4b":"**Visualising the word cloud for negative, positive and neutral sentiments to say which are the most frequent words in each labels.**","f6085434":"!kaggle competitions download -c eurecom-aml-2021-challenge-3","4ada4b56":"## Transformer Network","b41c0075":"The below histogram shows, the number of characters in each sentence, of the text column, it can be seen that, the maximum  of the samples have on average a characeter count of 68.","ee810dfe":"Removing punctucations and stop words( Most common words, that do not affect the sentiment of the sentence).some of the data pre-processing that is target conversion has already been done.","a2bbe2c7":"#Model","6f675995":"From the info we can say that there are close to, 24732 rows and 4 columns, of which the test and sentiment columns are the most important for us. we can also observe that none of the columns have null values.All of the data types are object , but we have to convertthe sentiment columns into numeric, so that the each type of text sentiment is converted into a encoded number.","5756f640":"Above, wordcloud we can see that the words that were most repeated in postive class. It is evident from, words like good, love , great etc. But somehow of the common words like now, today etc also repeatdly appear in the negetive class.","a4e076b1":"# Best Results","0b1ffe2e":"It can also be observed from the histogram, that most of the samples on average have a character length in between, 40 to 60."}}