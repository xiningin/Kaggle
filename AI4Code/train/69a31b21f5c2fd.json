{"cell_type":{"c5065b38":"code","544542be":"code","d78a422e":"code","5dd34a88":"code","07834add":"code","53bd3f59":"code","7fc644ee":"code","e18cee06":"code","32b70ba5":"code","8df210d2":"code","0d94913e":"code","1d1fa01c":"code","6f6f07fd":"code","324605ad":"code","1804a812":"code","4a907070":"code","8b206a4b":"code","3e22a553":"code","973782cc":"code","8277933d":"code","80664302":"code","1511c917":"code","19c31e95":"code","290828e9":"code","41ebf34f":"code","87f4e3b3":"code","bec77168":"code","62cb3cb9":"code","ee7aebf3":"code","0e9da325":"code","8481079d":"code","0cad7727":"code","5a0f5f13":"code","72df7b8b":"code","74d35c16":"code","909385cb":"code","ef274379":"code","e8bd11fc":"code","1e7f6d7d":"code","adf575c4":"code","f5f4cb9a":"code","cc42f1d9":"code","044f7028":"code","d9ae0272":"code","d983f394":"code","ecf992b7":"markdown","a62bb20b":"markdown","fd184e95":"markdown","6b37edee":"markdown","29d43653":"markdown","726d0877":"markdown","d1f0a1cf":"markdown","7849e8f8":"markdown","44cdd6db":"markdown","7969e706":"markdown","3998119e":"markdown","f6d4a493":"markdown","4da14d78":"markdown","27a574ee":"markdown","3f150a8b":"markdown","8035ffe2":"markdown"},"source":{"c5065b38":"# Basics of regularization","544542be":"# How does Regularization Work?","d78a422e":"# Ridge regression:","5dd34a88":"# Lasso regression","07834add":"### Implementation of Lasso Regression","53bd3f59":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","7fc644ee":"from sklearn.datasets import load_boston\nboston=load_boston()","e18cee06":"# Getting attributes of boston\ndir(boston)","32b70ba5":"# printing description\nboston.DESCR","8df210d2":"# Printing \"data\" attributes of the dataset, its our input \nboston.data","0d94913e":"# Getting features names of the dataset\nboston.feature_names","1d1fa01c":"# Printing first 10 values of target \nboston.target[0:10]","6f6f07fd":"# Describing dataframe from the data\ndf=pd.DataFrame(boston.data,columns=boston.feature_names)","324605ad":"# Printing first 2 rows of the dataframe 'df'\ndf.head(2)","1804a812":"# adding a new column 'target' from boston.target\ndf['target']=boston.target","4a907070":"df.head(2)","8b206a4b":"# Printing consized summary about the dataset\ndf.info()","3e22a553":"X=df.iloc[:,:-1].values\ny=df.iloc[:,-1].values","973782cc":"from sklearn.model_selection import train_test_split","8277933d":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)","80664302":"print(X_train.shape,y_train.shape)","1511c917":"print(X_test.shape,y_test.shape)","19c31e95":"# now we will start training of the model on multiple regression\nfrom sklearn.linear_model import LinearRegression\nlr=LinearRegression()","290828e9":"lr.fit(X_train, y_train)","41ebf34f":"lr_pred=lr.predict(X_test)","87f4e3b3":"# calculation mean squared error\nmse=np.mean((lr_pred-y_test)**2)\nmse","bec77168":"# Putting together the coefficient and their columns\n\nlr_coeff=pd.DataFrame()\nlr_coeff['Columns']=df.columns\nlr_coeff['Coefficient Values']=pd.Series(lr.coef_)\n\nprint(lr_coeff)","62cb3cb9":"# Regularizing using ridge regression\nfrom sklearn.linear_model import Ridge","ee7aebf3":"ridge_reg=Ridge(alpha=1)\n# here alpha parameter indicates Regularization strength; it must be a positive floating number","0e9da325":"ridge_reg.fit(X_train,y_train)","8481079d":"y_pred=ridge_reg.predict(X_test)","0cad7727":"ridge_coeff=pd.DataFrame()\nridge_coeff['columns']=df.columns\nridge_coeff['Coefficient estimates']=pd.Series(ridge_reg.coef_)\nprint(ridge_coeff)","5a0f5f13":"from sklearn.linear_model import Lasso\nlasso=Lasso(alpha=1)","72df7b8b":"lasso.fit(X_train,y_train)\ny_pred1=lasso.predict(X_test)","74d35c16":"lasso_mse=np.mean((y_pred1-y_test)**2)","909385cb":"print(lasso_mse)","ef274379":"lasso_coef=pd.DataFrame()\nlasso_coef['columns']=df.columns\nlasso_coef['coeffienct values']=pd.Series(lasso.coef_)","e8bd11fc":"lasso_coef","1e7f6d7d":"type(lasso_coef)","adf575c4":"from sklearn.linear_model import ElasticNet\nelastic=ElasticNet(alpha=1)","f5f4cb9a":"elastic.fit(X_train,y_train)","cc42f1d9":"y_pred2=elastic.predict(X_test)","044f7028":"elastic_mse=np.mean((y_pred2-y_test)**2)\n# Here for reminding, mean squared error is the mean of sqaure of diffrence in y_predicted and y_test\n\nprint(elastic_mse)","d9ae0272":"# making dataframe of column wise coefficient of elasticnet\n\nelastic_coeff=pd.DataFrame()\nelastic_coeff['columns']=df.columns\nelastic_coeff['coeff values']=pd.Series(elastic.coef_)\n\nprint(elastic_coeff)","d983f394":"type(elastic_coeff)","ecf992b7":"# what is regularization in ML\n\n- a technique to prevent the model from overfitting by adding extra information to it.\n- it maintain all variables or features in the model by reducing the magnitude of the variables. \n- Hence, it maintains accuracy as well as a generalization of the model.\n- In simple words, \"In regularization technique, we reduce the magnitude of the features by keeping the same number of features.\"\n- mainly regularizes or reduces the coefficient of features toward zero","a62bb20b":"- As we can observe from the above plots that alpha helps in regularizing the coefficient and make them converge faster. \n- it shows some of the coefficients become zero. In Ridge Regularization, the coefficients can never be 0, they are just too small to observe in above plots. ","fd184e95":"### Python implementation of Elastic Net ","6b37edee":"- a small amount of bias is added\n- reduces the complexity of the model, \n- also called L2 regularization\n- cost function is altered by adding the penalty term to it\n- amount of bias added to the model is called Ridge Regression penalty..","29d43653":"### Implementation of lasso regression using sklearn","726d0877":"Lasso Regression:\n- stands for Least Absolute Shrinkage and Selection Operator\n- also called L1 regularization\n- reduces the complexity of the model\n- similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights\n- Since it takes absolute values, hence, it can shrink the slope to zero\n- whereas Ridge Regression can only shrink it near to 0.\n- Some of the features are completely neglected for model evaluation\n- hence Lasso helps in reducing overfitting and also feature selection","d1f0a1cf":"We are going to use the Boston house prediction dataset, that is an inbuilt dataset in sklearn","7849e8f8":"- we have 13 independent variable and one dependent (House price) variable","44cdd6db":"- a technique to prevent the model from overfitting by adding extra information to it.\n-  maintains accuracy as well as a generalization of the mode\n-  reduces the magnitude of the variables, hence maintain all variables or features\n-  In simple words, \"In regularization technique, we reduce the magnitude of the features by keeping the same number of features\"\n- by adding a penalty or complexity term to the complex model","7969e706":"From the cost function of Ridge Regression we can see that if the values of \u03bb tends to zero, the equation becomes the cost function of the linear regression model..\n\nA general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.","3998119e":"- we add Mean Absolute value of coefficients in place of mean square value\n- Unlike Ridge Regression, Lasso regression can completely eliminate the variable by reducing its coefficient value to 0.","f6d4a493":"- Elastic Net is a combination of both of the above regularization. It contains both the L1 and L2 as its penalty term. \n- It performs better than Ridge and Lasso Regression for most of the test cases","4da14d78":"Let's consider the simple linear regression equation:\ny= \u03b20+\u03b21x1+\u03b22x2+\u03b23x3+\u22ef+\u03b2nxn +b\n\nY represents the value to be predicted\nX1, X2, \u2026Xn are the features for Y.\n\n\u03b20,\u03b21,\u2026..\u03b2n are the weights or magnitude\nb represents the intercept.\n\nThe loss function for the linear regression is called as RSS or Residual sum of squares.\n\nTechniques of Regularization:\n\u2022 Ridge Regression\n\u2022 Lasso Regression","27a574ee":"Lasso Regression adds \u201cabsolute value of magnitude\u201d of coefficient as penalty term to the loss function(L). \nRidge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function(L).","3f150a8b":"### Regularization in Machine Learning","8035ffe2":"- We can see that most of the columns do not significant coefficients and hence they do not contribute much in model performance,\n- we need to regularize the model"}}