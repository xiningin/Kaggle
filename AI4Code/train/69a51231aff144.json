{"cell_type":{"c885b889":"code","33d13184":"code","b7749bea":"code","c76c181d":"code","b6bdce82":"code","2a48d20b":"code","69007614":"code","208f9a54":"code","d96ea68f":"code","d9621160":"code","ed6241c6":"code","57db0186":"code","6d1c5a76":"code","8bc7f501":"code","6311a4c5":"code","9d939b34":"code","fcf00ed5":"code","de568526":"code","041ffda2":"code","17c194a1":"code","790bdb46":"code","4f24eda7":"code","2eceb8af":"code","3853e0b5":"code","63de24e7":"code","0940a685":"code","b3a47381":"code","20e3f200":"code","64eaa3e6":"code","8204480c":"code","5f0ecff7":"code","9b600827":"code","e170d72f":"code","5cc07a8c":"code","d244c609":"code","272b5619":"code","2b631d9a":"code","87e87275":"code","4d5df8cb":"markdown","b283e6ca":"markdown","7a2bcc77":"markdown","8212eb49":"markdown","bee61f64":"markdown","99660141":"markdown","46f11a72":"markdown","9e311f34":"markdown","ae629578":"markdown","2d3da153":"markdown","f635d3c6":"markdown","9a794494":"markdown","6d8d8c4f":"markdown","aa01fca1":"markdown","275e3a32":"markdown","81d2a126":"markdown","96ad9575":"markdown","7e12085b":"markdown","9c4fbccf":"markdown","f164e7ca":"markdown","4857893b":"markdown","668d8a0f":"markdown","6d2f87b6":"markdown"},"source":{"c885b889":"!pip install pycaret==2.0","33d13184":"!pip install mplcyberpunk","b7749bea":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mplcyberpunk\nfrom sklearn.datasets import load_wine\nplt.style.use(\"cyberpunk\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c76c181d":"wine=load_wine()\ndf = pd.DataFrame(wine['data'],columns = wine['feature_names'])\ndf.head()","b6bdce82":"df = df.rename(columns={'od280\/od315_of_diluted_wines': '% of diluted_wines'})","2a48d20b":"wine.keys()","69007614":"y=wine['target']\ny","208f9a54":"df['label']=y\ndata_pycaret=df.copy()\ndf.head()","d96ea68f":"df.describe()","d9621160":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","ed6241c6":"df.isnull().sum() # checking for null values","57db0186":"## plotting the target values\nsns.countplot(df['label'])\nmplcyberpunk.add_glow_effects()\nplt.show()","6d1c5a76":"#Plot a boxplot to check for Outliers\n#Target variable is Label. So will plot a boxplot each column against target variable\nsns.boxplot('label', 'alcohol', data = df)","8bc7f501":"sns.boxplot('label', 'malic_acid', data = df)","6311a4c5":"sns.boxplot('label', 'ash', data = df)","9d939b34":"sns.boxplot('label', 'alcalinity_of_ash', data = df)","fcf00ed5":"sns.boxplot('label', 'magnesium', data = df)","de568526":"sns.boxplot('label', 'color_intensity', data = df)","041ffda2":"sns.boxplot('label', 'hue', data = df)","17c194a1":"le = LabelEncoder()\ndf['label']=le.fit_transform(df['label'])\nsc = StandardScaler()\nx = sc.fit_transform(df.iloc[:,:-1])","790bdb46":"X_train, X_test, y_train,y_test = train_test_split(df.iloc[:,:-1], y, test_size = 0.2,random_state=2)","4f24eda7":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)","2eceb8af":"print(classification_report(y_test,lr_predict))","3853e0b5":"print(confusion_matrix(y_test,lr_predict))","63de24e7":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = lr.predict(X_test)","0940a685":"print(classification_report(y_test,y_pred))","b3a47381":"print(confusion_matrix(y_test,y_pred))","20e3f200":"svc = SVC()\nsvc.fit(X_train, y_train)\ny_pred=svc.predict(X_test)","64eaa3e6":"print(classification_report(y_test,y_pred))","8204480c":"print(confusion_matrix(y_test,y_pred))","5f0ecff7":"mlp=MLPClassifier()\nmlp.fit(X_train,y_train)\ny_pred=mlp.predict(X_test)","9b600827":"print(classification_report(y_test,y_pred))","e170d72f":"print(confusion_matrix(y_test,y_pred))","5cc07a8c":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred=rf.predict(X_test)","d244c609":"print(classification_report(y_test,y_pred))","272b5619":"print(confusion_matrix(y_test,y_pred))","2b631d9a":"from pycaret.classification import *\nwine= setup(data = data_pycaret, target = 'label',\n            remove_outliers=True,\n            session_id=1)","87e87275":"compare_models()","4d5df8cb":"**Logistic Regression gives 92% accuracy.**","b283e6ca":"## Creating The Dataset.","7a2bcc77":"**Random Forest Turned Out to be the best Classifier giving 97% accuracy.**","8212eb49":"# Splitting The Data into Train and Test sets.","bee61f64":"Fortunately the dataset has no null values.","99660141":"**2. K Nearest Neighbour**","46f11a72":"**SVC gives 69% accuracy.**","9e311f34":"#### The above part required a lot of effort and coding to create.Now lets go to the easiest and most fun part","ae629578":"**3. Support Vector Machine- SVC** ","2d3da153":"We observe that just by removing outliers the accuracy of all the models increase considerably.","f635d3c6":"# Data Visualization","9a794494":"![image.png](attachment:image.png)","6d8d8c4f":"**1. Logistic Regression**","aa01fca1":"Just one more step and we are done.It saves a lot of time and efforts which can help us in prioritizing model,so that we can mainly focus on feature engineering.","275e3a32":"<font size=\"+2\" color=\"chocolate\"><b>My Other Kernels<\/b><\/font><br>\n\n#### Click on the button to view kernels...\n\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/explaining-resnet-model-fine-tuning-pca-t-sne\" class=\"btn btn-primary\" style=\"color:white;\">Explaining ResNet architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/explaining-alexnet-model-tutorial-fine-tuning-pca\" class=\"btn btn-primary\" style=\"color:white;\">Explaining AlexNet architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/multilayer-perceptron-fine-tuning-pca-t-sne-mnist\" class=\"btn btn-primary\" style=\"color:white;\">Explaining Multi layer Perceptron architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/explaining-vgg-model-fine-tuning-pca-t-sne\" class=\"btn btn-primary\" style=\"color:white;\">Explaining VGG architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/predicting-wine-quality-using-svm-knn-with-eda\" class=\"btn btn-primary\" style=\"color:white;\">Wine Quality using SVM,KNN<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/glove-lstm-sentiment-analysis-for-beginners\" class=\"btn btn-primary\" style=\"color:white;\">NLP tutorial on Glove and LSTM<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/horse-vs-human-classification-by-resnet50-beginner\" class=\"btn btn-primary\" style=\"color:white;\">Horse Vs Humans<\/a>\n\n<br>\n\n###  If these kernels impress you,give them an <font size=\"+2\" color=\"red\"><b>Upvote<\/b><\/font>.<br>\n","81d2a126":"# Pycaret Implementation","96ad9575":"# Thank You for opening this notebook!!!\n## This notebook tutorial is for those who are beginners to machine learning.\n\n*In this notebook, First I have done some exploration on the data using matplotlib and seaborn with mplcyberpunk for the glowing effects. Then, I use different classifier models to predict the quality of the wine.*\n\n__1. Logistic Regression__\n\n__2. K Nearest Neighbour Classifier__\n\n__3. Support Vector Classifier(SVC)__\n\n__4. Multi Level Perceptron Classifier__\n\n__5. Random Forest Classifier__\n\n## Intermediate and advanced level people can skip to the PyCaret section.\n\n*In the end,I use a low code ML library called as __PyCaret__,this library automatically fits different classifier models to the data and gives output within seconds.I urge all the intermediate and advanced level ML enthusiasts to have a look at __PyCaret__ implementation as it is really useful for deciding which models to focus on and also helps in its fine tuning*\n\n### If you find this notebook useful then please upvote.\u00b6","7e12085b":"**KNN gives 92% accuracy.**","9c4fbccf":"**4. Multi Level Perceptron Classifier**","f164e7ca":"**5.Random Forest Classifier**","4857893b":"But First we will label encode the target variable and  Standard scaling to feature values to get optimized result.","668d8a0f":"**MLP only gives  94% accuracy**","6d2f87b6":"The column name % of diluted wines is not named properly so we rename the column."}}