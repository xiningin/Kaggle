{"cell_type":{"292b6f83":"code","b31980c0":"code","f5e27efc":"code","f84a8feb":"code","be4de868":"code","28deb33d":"code","40c6b4e0":"code","d65f832c":"code","abbe1f90":"code","6fb6daae":"code","a9dc5283":"code","02ca2f70":"code","a4e18fc0":"code","ded85691":"code","2eaafa97":"code","75d74e5f":"code","70e17134":"markdown","dfb71a11":"markdown","1718d89c":"markdown","c1914577":"markdown","a2b48586":"markdown","ce5d0014":"markdown","1e11aad0":"markdown","b923986a":"markdown","0b189542":"markdown","2d13c634":"markdown","a150c48f":"markdown","d8a32bc4":"markdown","f8bdc66c":"markdown","a9cf6cee":"markdown","43ea3721":"markdown","28bcb4ac":"markdown","fb485525":"markdown","3e5ca7fb":"markdown","f4f68046":"markdown","0468697b":"markdown"},"source":{"292b6f83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b31980c0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f5e27efc":"df=pd.read_csv(\"..\/input\/kyphosis.csv\")\ndf.head()\n","f84a8feb":"df.info()\n#There are 81 rows and 4 columns in the dataset","be4de868":"sns.pairplot(df,hue=\"Kyphosis\")","28deb33d":"from sklearn.model_selection import train_test_split\nX=df.drop(\"Kyphosis\",axis=1) # All of the columns except from the target column\ny=df[\"Kyphosis\"]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)","40c6b4e0":"from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier() # Here we create an instant of ht ealgorithm\ndtree.fit(X_train,y_train) # here we make algorithm fit the training dataset","d65f832c":"predictions=dtree.predict(X_test)\npredictions","abbe1f90":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,predictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test,predictions))","6fb6daae":"from sklearn import metrics","a9dc5283":"# Create Decision Tree classifer object\nclassifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n# Train Decision Tree Classifer\nclassifier = classifier.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = classifier.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","02ca2f70":"from sklearn.ensemble import RandomForestClassifier\n","a4e18fc0":"rfc=RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)","ded85691":"rfc_predictions=rfc.predict(X_test)","2eaafa97":"print(confusion_matrix(y_test,rfc_predictions))\nprint(\"\\n\")\nprint(classification_report(y_test,rfc_predictions))","75d74e5f":"print(classification_report(y_test,predictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test,predictions))\nprint(3*\"\\n\")\nprint(confusion_matrix(y_test,rfc_predictions))\nprint(\"\\n\")\nprint(classification_report(y_test,rfc_predictions))","70e17134":"<font color=\"blue\" >\n*Decision trees classify the examples by sorting them down the tree from the root to some leaf\/terminal node, with the leaf\/terminal node providing the classification of the example.\n\n*Each node in the tree acts as a test case for some attribute, and each edge descending from the node corresponds to the possible answers to the test case. This process is recursive in nature and is repeated for every subtree rooted at the new node.\n\n*Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that the purity of the node increases with respect to the target variable. The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.","dfb71a11":"<font color=\"blue\" >\n*Decision Tree algorithm belongs to the family of supervised learning algorithms. Unlike other supervised learning algorithms, the decision tree algorithm can be used for solving regression and classification problems too.\n\n*The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).\n\n*In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the record\u2019s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.\n\n*In decision trees we have nodes, they represent certain feature of the data.\n    The idea is to choose features that best split our data\n    -The root node performs the first split\n    -There are also interim nodes between the root node and leaves\n    -Leaves are the terminal nodes that predicit the outcome","1718d89c":"<font color=\"blue\" >\nAfter splitting data, we begin with training single decision tree","c1914577":"However entropy performs worse than gini for this dataset","a2b48586":"<font color=\"blue\" >\nThe results are not good, so we will change the criterion from gini to entropy","ce5d0014":"<font color=\"blue\" >\n*This data represents patients who have kyphosis which is a spinal disorder in which an excessive outward curve of the spine results in an abnormal rounding of the upper back.\n*The data shows whether kyphosis was absent or present after the surgery:\n    The age columns represents the age in months\n    The number column is a number of vertebra involved in the surgery\n    The start column is the first vertebra that was operated before the others","1e11aad0":"<font color=\"blue\" >\n*Random Forests is a way to improve the performance of single decision trees because they may not have best predictive accuracy\n\n*To improve the performance, we can use many trees with a random sample of features chosen as the split.\n\n*What we do with random forest is to create an ensemble of decision trees sampling from the training set with replacement\n\n*We build each tree such: when a split is considered,a random sample of m features is choosen as a split candidate from full features\n\n*The split is only allowed to use one of those m features,and a new random sample features is chosen for every single tree and split\n\n*For classification this m random sample of m features is chosen to be square root of p, which is the full set of features\n\n","b923986a":"<font color=\"blue\" >\n*It is obvious that random Forest Classifier predicts better than the decision tree algorithm\n\n*It is better to use both of these algorithms and compare the prediction, and after comparison decide to use one of them\n\n*If we have larger dataset, Random Forest Classifier Algorithm performs usually better than the Decision Tree Algorithm\n\n*Random Forest is very powerful algorithm and the first choice of data scientist for creating very fast classification model\nif we want to see quickly accuracy and precision we can get from a model before implementing other ML models.We can get a quick idea of what is possible with the data we have","0b189542":"# 4. Splitting Data And Training the Algorithm:","2d13c634":"<font color=\"blue\" >\n*Now we will compare the results of both of the algorithms","a150c48f":"DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n |  \n |  A decision tree classifier.","d8a32bc4":"              *Which is better Linear or tree-based models?\nWell, it depends on the kind of problem you are solving:\n\nIf the relationship between dependent & independent variables is well approximated by a linear model, linear regression will outperform the tree-based model.\n\nIf there is a high non-linearity & complex relationship between dependent & independent variables, a tree model will outperform a classical regression method.\n\nIf you need to build a model that is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression!","f8bdc66c":"# 1. Definition and Theoretical Background:","a9cf6cee":"# 3. Explanatory Data Analysis:","43ea3721":"<font color=\"blue\" >\nThe next step is to compare these results with random forest model","28bcb4ac":"<font color=\"blue\" >\n*We will also evaluate the predictions of the Random Forest Classifier","fb485525":"# 2. Importing Libraries and Getting the Data Set:","3e5ca7fb":"<font color=\"blue\" >\nNow we will evaluate whether our model predict good or not not","f4f68046":"RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n |  \n |  A random forest classifier.\n |  \n |  A random forest is a meta estimator that fits a number of decision tree\n |  classifiers on various sub-samples of the dataset and uses averaging to\n |  improve the predictive accuracy and control over-fitting.\n |  The sub-sample size is controlled with the `max_samples` parameter if\n |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n |  each tree.","0468697b":"<font color=\"blue\" >\n*Random Forest is an example of ensemble learning, in which we combine multiple machine learning algorithms to obtain better predictive performance.\n\n*Random Forest is one of the most common ensemble methods, which consists of a collection of Decision Trees.\n\n*The idea behind a Random Forest is actually pretty simple: We repeatedly select data from the data set (with replacement) and build a Decision Tree with each new sample.\n\n*In the case of classification with Random Forests, we use each tree in our forest to get a prediction, then the label with the most votes becomes the predicted class for that data point. The usual parameters when building a forest (standard defaults used in the SciKit-Learn library) are 10 trees and only considering the square root of \u2018n\u2019 features, where n is the total number of features."}}