{"cell_type":{"8ed78cef":"code","e9d4299a":"code","42b1e190":"code","fcb98360":"code","011e7c79":"code","a96aa49a":"code","5353df08":"code","25580772":"code","4b73f024":"code","6fa79239":"code","7b8d57c8":"code","aae56fda":"code","84ee253f":"code","58e10d38":"code","79690a6e":"code","cf294537":"code","b0d462e1":"markdown","304152be":"markdown","02775a9e":"markdown","355f01a2":"markdown","dbd7b6e5":"markdown","17ea9b82":"markdown","640640b1":"markdown","7f9058b1":"markdown"},"source":{"8ed78cef":"import numpy as np\nimport pandas as pd","e9d4299a":"# read data from csv file\ndef get_data(file):\n    data = pd.read_csv(file)\n    data = data.to_numpy()\n    labels = []\n    images = []\n    \n    for row_ind in range(data.shape[0]):\n        labels.append(data[row_ind, 0])\n        images.append(data[row_ind, 1:])\n        \n    images = np.reshape(images, newshape=(-1, 28, 28))\n    labels = np.array(labels)\n    return images, labels\n\nX_train, y_train = get_data('..\/input\/Kannada-MNIST\/train.csv')\nX_test, test_Id = get_data('..\/input\/Kannada-MNIST\/test.csv')\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","42b1e190":"pd.Series(y_train).value_counts()","fcb98360":"from sklearn.model_selection import train_test_split\n# make sure train and val are from same distribution whether is class counts or image-label quality\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, shuffle=True)","011e7c79":"# visualization of one random data sample\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimgplot = plt.imshow(X_train[24], cmap='gray')\nprint(y_train[24])","a96aa49a":"# use one-hot encoder for multi-class classification\nfrom keras.utils import to_categorical\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)","5353df08":"# add another dimension to the data since they are gray scale images\nX_train = np.expand_dims(X_train, axis=3)\nX_val = np.expand_dims(X_val, axis=3)\nX_test = np.expand_dims(X_test, axis=3)","25580772":"# do image augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n        rotation_range=10, \n        zoom_range = 0.1,  \n        width_shift_range=0.1, \n        height_shift_range=0.1) \n\nprint(X_train.shape)\nprint(X_val.shape)","4b73f024":"import tensorflow as tf\ntf.keras.backend.clear_session()","6fa79239":"from keras.layers import Input, Conv2D, BatchNormalization, ReLU, MaxPooling2D, AveragePooling2D, Flatten, Dense\nfrom keras.models import Model\n\ninputs = Input(shape=(28, 28, 1))\n\nx = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\nx = BatchNormalization(momentum=0.1)(x)\nx = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization(momentum=0.1)(x)\nx = AveragePooling2D((2, 2))(x)\n\nx = Conv2D(16, (5, 5), padding='same', activation='relu')(x)\nx = BatchNormalization(momentum=0.1)(x)\nx = Conv2D(16, (5, 5), padding='same', activation='relu')(x)\nx = BatchNormalization(momentum=0.1)(x)\nx = AveragePooling2D((2, 2))(x)\n\nx = Flatten()(x)\n# from feature representation to categorical prob\nx = Dense(100, activation='sigmoid')(x)\nx = BatchNormalization(momentum=0.1)(x)\npred = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=pred)\nmodel.summary()","7b8d57c8":"from keras.optimizers import Adam\nadam = Adam(learning_rate=0.00005)\n\n# Compile Model\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n# setup callbacks for auto earlystopping\nes = EarlyStopping(monitor='val_accuracy', min_delta=0.00001, patience=5, verbose=1, mode='auto', restore_best_weights=True)\n\n# Train the Model\nbatch_size = 64\nepochs = 50\nhistory = model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=batch_size), \n                              epochs=epochs,\n                              steps_per_epoch=X_train.shape[0]\/\/batch_size,\n                              validation_data=(X_val, y_val),\n                              callbacks=[es],\n                              verbose=1)","aae56fda":"# Plot the chart for accuracy and loss on both training and validation\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\n# creates a new figure\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","84ee253f":"# confusion matrix for validation data\nfrom sklearn.metrics import confusion_matrix\nval_prediction = model.predict(X_val)\nval_prediction = np.argmax(val_prediction, axis=1)\n\nval_truth = np.argmax(y_val,axis=1)\n# conf[i, j] is no. of true i are predicted to be j\nconf = confusion_matrix(val_truth, val_prediction)\nconf = pd.DataFrame(conf, index=range(0,10), columns=range(0,10))\nfrom IPython.display import display\ndisplay(conf)","58e10d38":"# analyse how the majority wrong classification is caused\ndef case_analysis():\n    for i in range(len(val_prediction)):\n        if (val_prediction[i] == 1 and val_truth[i] == 0) or (val_prediction[i] == 7 and val_truth[i] == 3):\n            yield i\n\ncases = list(case_analysis())  \nprint('no. of cases: {}'.format(len(cases)))","79690a6e":"fig=plt.figure(figsize=(20, 20))\ncolumns = 5\nrows = 3\nfor i in range(1, columns*rows +1):\n    if i == 14:\n        break\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(np.squeeze(X_val[cases[i-1]]))\n    plt.title('pred: {}, truth: {}'.format(val_prediction[cases[i-1]], val_truth[cases[i-1]]))\nplt.show()","cf294537":"# generate prediction for test set\nprediction = model.predict(X_test)\nprediction = np.argmax(prediction, axis=1)\nsubmission_df = pd.DataFrame({'id':test_Id , 'label':prediction.astype(np.int32)})\n# for kernel based competition\nsubmission_df.to_csv('.\/submission.csv', index=False)","b0d462e1":"# Step 1: Prepare (images, label) split for train, test, validation sets","304152be":"# Step 3: Define simple CNN baseline model","02775a9e":"Kannada numbers official representation\n![](https:\/\/t3.ftcdn.net\/jpg\/01\/62\/22\/58\/500_F_162225807_ntQwmEkc76NoItsdipA1eNHQZKqdBoio.jpg)","355f01a2":"# Step 2: One-hot encoder and ImageDataGenerator for image augmentation","dbd7b6e5":"# Step 4: Error analysis","17ea9b82":"Most of the wrong prediction comes some of the true 0s are predicted to be 1s, some of true 3s are predicted to be 7s, lets see why this is the case below.","640640b1":"As one can see from the above, almost all of the 'wrongly classified case' are due to wrong labels, for example, for the 'pred 1, truth 0' cases, human can tell it is obviously a 1 rather than 0 as the ground truth, the similar goes for 7 and 3. This is rather unfortunate due to human labelling during data preparation. Therefore, as far as this validation performance of our model, it is as perfect and as simple as a baseline model can get.","7f9058b1":"Evenly distributed, non-skewed classes"}}