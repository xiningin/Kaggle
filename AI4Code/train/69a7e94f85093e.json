{"cell_type":{"735a0e65":"code","af06e9c4":"code","d8643e5c":"code","5671aecb":"code","c45db6db":"code","25eb15da":"code","22c251e3":"code","0f0b177a":"code","25b1cd2e":"code","f5ed96d9":"code","9434e1d3":"code","807cb9d1":"code","40a9f982":"code","abf4afd9":"code","4de57149":"code","b1a64b21":"code","45ac51b7":"code","b38c2978":"code","47fc94ff":"code","2002060c":"code","060e27c3":"code","9f5e19d9":"code","aba82e23":"code","2bcda31a":"code","ab6070f3":"code","5eb075ad":"code","854a3e51":"code","f7bd2e4f":"code","b0b6eb6a":"code","9f1dc6fc":"code","8c5cecea":"code","df38581f":"code","fa20b2f4":"code","b0c98205":"code","23edb48d":"code","902dfa16":"code","1a7469f0":"code","8e345aca":"code","4f6dca83":"code","8de31e5d":"code","8643d2d6":"code","d6779d64":"code","2d0f9985":"code","dd57207f":"code","93abacb4":"code","c2c1bcdc":"code","6156d741":"code","cf2ba23b":"code","30b75180":"code","707616dd":"code","d472b0bf":"code","06454da9":"code","19f71f98":"code","ccee1c0c":"code","5a031de6":"markdown","94719b67":"markdown","ab87e8ea":"markdown","08a528d2":"markdown","97458f5d":"markdown","79ac9015":"markdown","39cf9e74":"markdown","635b38a8":"markdown","de43ac6b":"markdown","417ef4c2":"markdown","c9470daf":"markdown","049a21a4":"markdown"},"source":{"735a0e65":"## load all libraries\n# load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom collections import Counter\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder as le ,MinMaxScaler, RobustScaler, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nimport math\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor\nfrom xgboost import XGBRegressor \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV, cross_val_score, KFold, learning_curve\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n# preprocessing libraries","af06e9c4":"os.chdir(r'..\/input\/upvotes-dataset')\n\ntrain_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\nsample_submission = pd.read_csv('sample.csv')\n\ntrain_data.head(5)","d8643e5c":"# now exploring data\nprint('shape of training data is : ',train_data.shape)\nprint('no.of examples in train data is : {} and in test_data is :{}'.format(train_data.shape[0],test_data.shape[0]))","5671aecb":"# finding the no. of null values in both train and test\nprint(train_data.isnull().sum())\nprint(train_data.isnull().sum())","c45db6db":"# since there are no null values we will check is there any outlier present in data \n# outliers greatly affect the performance of model\n\n# removing outlier from the data using inter - quaartile range method\ndef outlier_removal(df,attributes,n):\n    # take loop through the attributes\n    outliers_indices = []\n    for col in attributes:\n        # calculating inter-quartile range\n        Q1 = df[col].quantile(0.25)\n        # 3rd quartile (75%)\n        Q3 = df[col].quantile(0.75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        index = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        outliers_indices.extend(index)\n    # finding index which have more than 2\n    outliers_indices = Counter(outliers_indices)\n    multiple_outliers = list(k for k,v in outliers_indices.items() if v>n) # n tells the no. of outliers present in an example for removal\n    return multiple_outliers   ","25eb15da":"index_list_count = []\nfor i in range(len(train_data.columns[2:6])):\n    multioutlier_indices = outlier_removal(train_data,train_data.columns[2:6],i)\n    print('There are {} no. of examples which have outliers in {} columns '.format(len(multioutlier_indices), i+1))\n    index_list_count.append(len(multioutlier_indices))      \n    print('the examples for this are',train_data.loc[multioutlier_indices] )      ","22c251e3":"sns.barplot(x = [1,2,3,4],y = index_list_count)\nplt.ylabel('no. of example')\nplt.xlabel('no. of columns containing outlier')\nplt.title('plot for examples vs outlier columns ')","0f0b177a":"# concatenate the test data an train data to apply same operations\ndataset = pd.concat(objs = [train_data,test_data],axis=0,sort=True).reset_index(drop=True)\ndataset.drop('ID',axis=1,inplace=True)  # dropping id to be not used for prediction in any way\nprint(len(dataset))\ndataset","25b1cd2e":"# now checking the scales of train data\ntrain_data.drop('ID',axis=1,inplace=True)\ntrain_data.describe()\n","f5ed96d9":"# now checking the info \ntrain_data.info()\n","9434e1d3":"## firstly let's plot the correlation of each individual on upvotes \n# note here no preprocessing is done so the correlation might be different\n\n# making heatmap of correlation\nsns.heatmap(train_data.iloc[:,1:].corr(),annot = True, fmt = '.2f')","807cb9d1":"#  Analysis of Reputation\nprint('max no. of Reputation on entire dataset : ',dataset['Reputation'].max())\nprint('min no. of Reputation on entire dataset : ',dataset['Reputation'].min())\nprint('max no. of Reputation on train dataset : ',train_data['Reputation'].max())\nprint('min no. of Reputation on train dataset : ',train_data['Reputation'].min())\nprint('max no. of Reputation on test dataset : ',test_data['Reputation'].max())\nprint('min no. of Reputation on test dataset : ',test_data['Reputation'].min())","40a9f982":"# starting with Reputation\n\n# attribute 1 analysis\ng = sns.distplot(dataset[\"Reputation\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Reputation\"].skew()))\ng.legend(loc='best')","abf4afd9":"g = sns.boxplot(dataset['Reputation'])","4de57149":"### huge no. of outliers now trying to convert it to logarathmic scale to see, skweness reduce or not\ndataset['Reputation'] = dataset['Reputation'].map(lambda i:np.log(i) if i>0 else 0)\ng = sns.distplot(dataset[\"Reputation\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Reputation\"].skew()))\ng.legend(loc='best')\n","b1a64b21":"# checking in box plot\ng = sns.boxplot(dataset['Reputation'])","45ac51b7":"#now checking the relation of \ntrain_data['Reputation'] = train_data['Reputation'].map(lambda i:np.log(i) if i>0 else 0)\n\n# on train data only for 1st feature\ng = sns.scatterplot(x = 'Reputation' , y = 'Upvotes' , data = train_data)\n","b38c2978":"#  Analysis of Answers\nprint('max no. of Answers on entire dataset : ',dataset['Answers'].max())\nprint('min no. of Answers on entire dataset : ',dataset['Answers'].min())\nprint('max no. of Answers on train dataset : ',train_data['Answers'].max())\nprint('min no. of Answers on train dataset : ',train_data['Answers'].min())\nprint('max no. of Answers on test dataset : ',test_data['Answers'].max())\nprint('min no. of Answers on test dataset : ',test_data['Answers'].min())","47fc94ff":"# Attribute no. 2 (Answers)\ng = sns.distplot(dataset[\"Answers\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Answers\"].skew()))\ng.legend(loc='best')","2002060c":"g = sns.boxplot(dataset['Answers'])","060e27c3":"# Again treating outliers for this column\n### huge no. of outliers\ndataset['Answers'] = dataset['Answers'].map(lambda i:np.log(i) if i>0 else 0)\ng = sns.distplot(dataset[\"Answers\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Answers\"].skew()))\ng.legend(loc='best')\n\n","9f5e19d9":"# checking in box plot\ng = sns.boxplot(dataset['Answers'])","aba82e23":"#plotting the variation of Upvotes vs Answers\n#now checking the relation \ntrain_data['Answers'] = train_data['Answers'].map(lambda i:np.log(i) if i>0 else 0)\n\n# on train data only for 1st feature\ng = sns.scatterplot(x = 'Answers' , y = 'Upvotes' , data = train_data)","2bcda31a":"#  Analysis of views\nprint('max no. of views on entire dataset : ',dataset['Views'].max())\nprint('min no. of views on entire dataset : ',dataset['Views'].min())\nprint('max no. of views on train dataset : ',train_data['Views'].max())\nprint('min no. of views on train dataset : ',train_data['Views'].min())\nprint('max no. of views on test dataset : ',test_data['Views'].max())\nprint('min no. of views on test dataset : ',test_data['Views'].min())","ab6070f3":"# Attribute no. 2 (Views)\ng = sns.distplot(dataset[\"Views\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Views\"].skew()))\ng.legend(loc='best')","5eb075ad":"### huge no. of outliers now trying to convert it to logarathmic scale to see, skweness reduce or not\ndataset['Views'] = dataset['Views'].map(lambda i:np.log(i) if i>0 else 0)\ng = sns.distplot(dataset[\"Views\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Views\"].skew()))\ng.legend(loc='best')\n","854a3e51":"# checking in box plot\ng = sns.boxplot(dataset['Views'])","f7bd2e4f":"# now username it is assumed that it doesn't have any significant effect on upvote but let' see\ng = sns.distplot(dataset[\"Username\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Username\"].skew()))\ng.legend(loc='best')\n","b0b6eb6a":"# checking in box plot\ng = sns.boxplot(dataset['Username'])","9f1dc6fc":"# checking the relation of username with upvotes\nusername_div = train_data.groupby('Username')\nusername_div.Upvotes.apply(np.mean)","8c5cecea":"# on train data \ng = sns.scatterplot(x = 'Username' , y = 'Upvotes' , data = train_data)","df38581f":"# Analysis of attribute tags\ng = sns.countplot(\"Tag\", data = dataset)","fa20b2f4":"# checking in box plot\ng = sns.boxplot(train_data['Tag'],dataset['Upvotes'])","b0c98205":"sns.barplot(train_data['Tag'],dataset['Upvotes'])","23edb48d":"# may be it seems right to label encode the dependencies in Tag\ndataset['Tag'] = le().fit_transform(dataset['Tag'])\ndataset.head(5)","902dfa16":"# firstly applying features scaling\nsd = MinMaxScaler()\nnorm1 = sd.fit(dataset[['Answers','Reputation','Username','Views']])\nx = norm1.transform(dataset[['Answers','Reputation','Username','Views']])\nx.shape","1a7469f0":"x","8e345aca":"dataset[['Answers','Reputation','Username','Views']] = x[:,:]\ndataset.describe()","4f6dca83":"# slicing back to train and test set\n\ny_train = np.array(dataset['Upvotes'],dtype = np.int64)[0:len(train_data)]\nx_train = dataset[0:len(train_data)][['Answers','Reputation','Tag','Username','Views']]\nx_test = dataset[len(train_data):][['Answers','Reputation','Tag','Username','Views']]\nprint(x_train.shape,x_test.shape,y_train.shape)","8de31e5d":"y_train","8643d2d6":"# again applying PCA with 8 features\n# identifying which features adds how much variance to data\n\n\ncovar_matrix = PCA(n_components = 5) #we have 8 numerical features\ncovar_matrix.fit(dataset[['Answers','Reputation','Tag','Username','Views']])\nvariance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n\nvar=np.cumsum(np.round(variance, decimals=3)*100)\ncomponents = covar_matrix.fit_transform(dataset[['Answers','Reputation','Tag','Username','Views']])\nvar #cumulative sum of variance explained with [n] features","d6779d64":"# checking importance using pvalue \n# Applying p-value to check feature dependence\nimport statsmodels.api as sm\nregressor_OLS = sm.OLS(endog = y_train, exog = x_train).fit()\nregressor_OLS.summary()","2d0f9985":"# making heatmap of correlation\nsns.heatmap(pd.concat([x_train,pd.DataFrame(y_train, columns= ['Upvotes'])],axis=1).corr(),annot = True, fmt = '.2f')","dd57207f":"index_list_count1 = []\nfor i in range(len(x_train.columns)):\n    multioutlier_indices = outlier_removal(x_train,x_train.columns,i)\n    print('There are {} no. of examples which have outliers in {} columns '.format(len(multioutlier_indices), i+1))\n    index_list_count1.append(multioutlier_indices)      \n    print('the examples for this are',train_data.loc[multioutlier_indices] )      ","93abacb4":"#x_val1 = x_train.loc[index_list_count1[0]]\n#y_val1 = pd.DataFrame(y_train).loc[index_list_count1[0]]\nx_train = x_train.drop(index_list_count1[0])\nlen(x_train)","c2c1bcdc":"index_list_count1","6156d741":"y_train = pd.DataFrame(y_train).drop(index_list_count1[0])\nlen(y_train)","cf2ba23b":"# Applying train - val split\n\nx_train,x_val,y_train,y_val  = train_test_split(x_train,y_train,test_size = 0.2, random_state=42)\nx_val","30b75180":"# dropping two columns in all\n\nx_train.drop(['Username','Tag'],axis=1,inplace = True)\nx_val.drop(['Username','Tag'],axis=1,inplace = True)\nx_test.drop(['Tag','Username'],axis=1,inplace = True)","707616dd":"# using k folds cross validation with 5 splits\nkfold = KFold( n_splits = 5)\n","d472b0bf":"# taking extra trees as model\n\nExtC = ExtraTreesRegressor(random_state =2)\n\nExtC.fit(x_train,y_train)\ny_val_predict = ExtC.predict(x_val) \nmath.sqrt(mean_squared_error(y_val,y_val_predict))\n","06454da9":"pred = ExtC.predict(x_test)\npred","19f71f98":"y_val_predict","ccee1c0c":"sample_submission['Upvotes'] = pred","5a031de6":"### Now merging the train and test data\nWe merge both, in order to apply the same transformation to both data","94719b67":"*Nice! it looks like a pure normal distribution*","ab87e8ea":"## MODEL BUILDING","08a528d2":"### Checking PCA variance of each feature","97458f5d":"No. of oulier examples looks huge but at this time it is not clear that these are beneficial or not so it is not good to remove them directly, let's apply some scaling and preprocessing then come back to same steps. ","79ac9015":" *This plot clearly defines as the reputation of user increases the upvotes on their question increases* #positive correlation","39cf9e74":"### Data Analysis and Visualisation","635b38a8":"## Loading data","de43ac6b":"Heatmap defines that username doesn't have any significant effect on upvotes which seems to be true compared to others","417ef4c2":" the uneven distribution still persist even after scale conversion as some questions were mostly answered","c9470daf":"*As expected it doesn't need any transformation*","049a21a4":"### Now again checking 2 things \n1. heatmap of features\n2. ouliers present in data"}}