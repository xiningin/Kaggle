{"cell_type":{"73034853":"code","495e6b7e":"code","670e8fb1":"code","aa3aaa2f":"code","83aab454":"code","aae0914c":"code","f6c37e57":"code","6732264c":"code","e0033d32":"code","4cc44592":"code","001027da":"code","16c374b9":"code","026097b3":"code","6f12b608":"code","e57e1124":"code","24bd9db1":"code","cdaed380":"code","d8fda7bc":"code","b67d2b8a":"code","a08cd4ec":"code","f1f9ac9f":"code","d65a746f":"code","af79ef97":"code","590bf9b1":"code","c7fe95d6":"code","a9a09ee4":"code","c11c6871":"code","10c82c8f":"code","ff53e590":"code","1c48f0f6":"code","fcd35458":"code","2533123d":"code","15c765c4":"code","d1efcea5":"code","b017e8f0":"code","fed03eab":"code","3dde650f":"code","e0527983":"code","46f985dd":"code","2b57c724":"code","27422e94":"code","9055517a":"code","2fd6b380":"code","c0f939ff":"code","aee64780":"markdown","7f11bdfd":"markdown","863d85df":"markdown","e3cb59c1":"markdown","4307c883":"markdown","22b6fb26":"markdown","44c11618":"markdown","d90e1f99":"markdown","d1f6fee8":"markdown","d59a91a2":"markdown","45b3655f":"markdown","ba64beb3":"markdown","acc3e168":"markdown","cf8c5f81":"markdown","c530844b":"markdown","e9cdcb40":"markdown","a6f1390b":"markdown","2a1537ed":"markdown","828ec83c":"markdown","c388441f":"markdown","f167636b":"markdown","c1212c69":"markdown","7fece20e":"markdown","596e2d9d":"markdown","e27f8433":"markdown","1985336c":"markdown","293a78bd":"markdown","313ee5c2":"markdown","27ba5a7a":"markdown","b9462cef":"markdown","b2325457":"markdown","3842a57f":"markdown","4f16f0fb":"markdown","f40c9000":"markdown"},"source":{"73034853":"%%html\n\n<div style=\"background: linear-gradient(320deg, rgb(62, 0, 42), rgb(2, 2, 106), rgb(36, 0, 181)); \ncolor: #fff; border-radius: 10px;\">\n<div style=\"color: #fff; padding-top: 20px; padding-left: 10px; padding-bottom: 10px; font-size: 24px; line-height: 25px\">\nWorking with the Chinese Calligraphy Styles dataset*<\/div>\n<div style=\"padding: 10px; color: #fff;\">* By<strong> Kauvin Lucas<\/strong>, submitted in Kaggle [<a href=\"#references\">1<\/a>] and Jovian [<a href=\"#references\">2<\/a>].\nMore details in the references section.<\/div><\/div>","495e6b7e":"import os\nimport torch\nimport torchvision\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchvision.utils import make_grid\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\n#import jovian","670e8fb1":"data_dir = '..\/input\/chinese-calligraphy-styles-by-calligraphers\/data\/data'","aa3aaa2f":"dataset = ImageFolder(data_dir+'\/train', transform=ToTensor())","83aab454":"classes = os.listdir(data_dir + \"\/train\")\nprint(os.listdir(data_dir))\nprint(classes)","aae0914c":"for class_ in classes:\n    print(\"Class \"+ class_ + \": \" + str(len(os.listdir(data_dir + \"\/train\/\" + class_))))","f6c37e57":"def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","6732264c":"# Example from class 1\nimg1, label1 = dataset[0]\nprint(img1.shape, label1)\nprint(img1)\nshow_example(*dataset[0])","e0033d32":"# Example from class 9\nimg2, label2 = dataset[40000]\nprint(img2.shape, label2)\nprint(img2)\nshow_example(*dataset[40000])","4cc44592":"# Example from class 12\nimg2, label2 = dataset[55000]\nprint(img2.shape, label2)\nprint(img2)\nshow_example(*dataset[55000])","001027da":"random_seed = 1234\ntorch.manual_seed(random_seed);\n\nval_size = 8202 # 10% of the total size\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","16c374b9":"batch_size=128\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)","026097b3":"def show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break","6f12b608":"show_batch(train_dl)","e57e1124":"simple_model = nn.Sequential(\n    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n    nn.MaxPool2d(2, 2)\n)","24bd9db1":"for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = simple_model(images)\n    print('out.shape:', out.shape)\n    break","cdaed380":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n        \ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","d8fda7bc":"class Cifar10CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Flatten(), \n            nn.Linear(16384, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 20))\n        \n    def forward(self, xb):\n        return self.network(xb)","b67d2b8a":"model = Cifar10CnnModel()\nmodel","a08cd4ec":"for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","f1f9ac9f":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","d65a746f":"device = get_default_device()\ndevice","af79ef97":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device);","590bf9b1":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","c7fe95d6":"model = to_device(Cifar10CnnModel(), device)","a9a09ee4":"evaluate(model, val_dl)","c11c6871":"# Define hyperparameters\nnum_epochs = 6 # Number of epochs, enough to prevent overfitting\nopt_func = torch.optim.Adam # Implements Adam algorithm for optimization\nlr = 0.001 # Learning rate","10c82c8f":"# Log hyperparamenters to Jovian\n\n# jovian.reset()\n# jovian.log_hyperparams({\n#     'num_epochs': num_epochs,\n#     'opt_func': opt_func.__name__,\n#     'batch_size': batch_size,\n#     'lr': lr,\n# })","ff53e590":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","1c48f0f6":"# log fitted model metrics to Jovian\n\n# jovian.log_metrics(train_loss=history[-1]['train_loss'], \n#                    val_loss=history[-1]['val_loss'], \n#                    val_acc=history[-1]['val_acc'])","fcd35458":"# Plot accuracy history\ndef plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\nplot_accuracies(history)","2533123d":"# Plot loss history\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","15c765c4":"plot_losses(history)","d1efcea5":"test_dataset = ImageFolder(data_dir+'\/test', transform=ToTensor())","b017e8f0":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","fed03eab":"img, label = test_dataset[0]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","3dde650f":"img, label = test_dataset[1002]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","e0527983":"img, label = test_dataset[6153]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","46f985dd":"img, label = test_dataset[12000]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","2b57c724":"img, label = test_dataset[14560]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","27422e94":"test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\nresult = evaluate(model, test_loader)\nresult","9055517a":"torch.save(model.state_dict(), 'cifar10-cnn.pth')","2fd6b380":"model2 = to_device(Cifar10CnnModel(), device)","c0f939ff":"model2.load_state_dict(torch.load('cifar10-cnn.pth'))","aee64780":"### 6 - Modelling","7f11bdfd":"[1] \"Calligraphy Style Classification\", by Kauvin Lucas in Kaggle: https:\/\/www.kaggle.com\/kauvinlucas\/calligraphy-style-classification.\n\n[2] \"Calligraphy Style Classification\", by Kauvin Lucas in Jovian: https:\/\/jovian.ai\/kauvinlucas\/calligraphy-style-classification.\n\n[3] Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis, by Pengyuan Lyu, Xiang Bai, Cong Yao, Zhen Zhu, Tengteng Huang, Wenyu Liu1 in 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR): https:\/\/doi.org\/10.1109\/ICDAR.2017.181\n\n[4] Recognizing Chinese Calligraphy Styles: A Cage Fight, by Chen Yu-Sheng, Li Haihong, Su Guangjun in Stanford University:http:\/\/cs229.stanford.edu\/proj2016\/poster\/ChenSuLi-Machine%20Learning%20for%20Different%20Calligraphy%20Style%20Recognition-poster.pdf","863d85df":"<a id='references'><\/a>\n### 11 - References","e3cb59c1":"#### 6.2 - Define helper functions for training and validation\nWe will use the Cross Entropy loss function for calculating loss in the training and validation steps","4307c883":"#### 7.5 - Fit model and log metrics","22b6fb26":"### 1 - Install and import the required libraries","44c11618":"#### 8.1 - Transform test data","d90e1f99":"#### 7.2 - Move model to device","d1f6fee8":"#### 8.4 - Look for overall loss and accuracy on test data","d59a91a2":"All of the classes contain a total of 82022 images","45b3655f":"We've defined our model with 9 layers, and after trying with different hyperparamenters, we arrived at a model that performed quite well and generated an accuracy of around **95%** on the test data. No batch normalization was employed since this overfitted the model.\n\nThis notebook was intented to explore simple CNN models to predict the caligraphers initials behind each calligraphy font. Of course, a few things could be done to improve the model like applying augmentation schemes on the training data.","ba64beb3":"If our simple model has been called without problems, we'll proceed to define our main CNN model","acc3e168":"#### 7.3 - Evaluate the model by its initial parameters","cf8c5f81":"#### 7.1 - Define functions to fit and evaluate the model","c530844b":"#### 7.4 - Define and log hyperparameters","e9cdcb40":"There are a total of 20 classes in the dataset","a6f1390b":"### 3 - Explore the dataset","2a1537ed":"#### 3.1 - How many classes are in the dataset?","828ec83c":"### 2 - Load and transform the the dataset","c388441f":"### 10 - Conclusion and final remarks","f167636b":"#### 7.6 - Plot accuracy and loss history","c1212c69":"![header](https:\/\/i.imgur.com\/XzJ2RvS.png)","7fece20e":"#### 6.3 - Chain layers into a single network architecture","596e2d9d":"#### 8.2 - Predict and compare labels (5 examples)","e27f8433":"### 8 - Testing","1985336c":"Chinese calligraphy is a very unique visual art and an important manifestation of Chinese ancient culture which is popular with many people in the world [3]. As with any other artwork, Chinese calligraphy can take several years to master and have a high economic value for the holders. \n\nAlthough it's a form of art, there are traditional, strict rules that must be followed to make a legitimate calligraphy. Still, the style of each calligraphy is unique and anyone looking at it can tell its caligrapher.\n\nIn this notebook, we'll build and test a simple Convolutional Neural Network (CNN) model with 9 layers without batch normalization to identify the author's initials of each available font. Compared to other classification models, CNN performs well without feature design [4].","293a78bd":"### 7 - Training","313ee5c2":"#### 6.4 - Build helper functions to move model and data to a CUDA device","27ba5a7a":"#### 8.5 - Save the model once it's done","b9462cef":"#### 3.3 - Visualize 3 examples","b2325457":"### 5 - Load data in batches","3842a57f":"### 4 - Split the dataset","4f16f0fb":"#### 3.2 - How many examples are in each class?","f40c9000":"#### 6.1 - Test a simple model"}}