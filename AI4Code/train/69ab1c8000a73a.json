{"cell_type":{"5c3bd0b4":"code","15f182c5":"code","584080f2":"code","65e7ea0a":"code","c2a6da6e":"code","a7b2916f":"code","4af7bc49":"code","622c9557":"code","a58a9e3c":"code","4f0a6583":"code","2b581f04":"code","0572fa6d":"code","97db7caf":"code","c8ffa750":"code","49dac130":"code","220cb853":"code","23f60984":"code","58bc8149":"code","13bb0775":"code","1803d97e":"code","017cd60e":"code","43b208ce":"code","ee6b44fa":"code","b6bfd4a7":"code","6b0d5f66":"code","498c210f":"code","f867eb4b":"code","055e7004":"code","f2697137":"code","79821063":"code","870d3b97":"code","41d2d847":"code","e8d1e7c1":"code","e52e68eb":"code","5ec19fd0":"code","d0b1a22c":"code","0fc4561b":"code","efd3b75a":"code","00198fb0":"code","bf603fd6":"code","ac18504e":"code","2f3d8f90":"code","b7936379":"code","a767233a":"code","1893675f":"code","0a4c5b68":"code","8e71bda1":"markdown","dc0163fc":"markdown","f8cb4637":"markdown","e72545cc":"markdown","b85962eb":"markdown","c2bdf9f9":"markdown","e0d1e400":"markdown","7df50236":"markdown","644d50b1":"markdown","ae6ef6a6":"markdown","8fe1c6ab":"markdown","4f4874da":"markdown","0f57ea4d":"markdown","a9c3f6fd":"markdown","26423b87":"markdown","fac605fb":"markdown","cc28a0b6":"markdown","ec8181b3":"markdown","185f11cb":"markdown","27b30f0c":"markdown","59aac18d":"markdown","f07ed6a6":"markdown","ef64a1f5":"markdown","1e892a22":"markdown","7e791be9":"markdown","dcf7d259":"markdown","ffd6b972":"markdown","0a1e9e6e":"markdown","2522ad2a":"markdown","7cf21057":"markdown","1f3f06c9":"markdown","b1c3cf02":"markdown","3b6645b0":"markdown","7a261db4":"markdown","03916375":"markdown","e989c146":"markdown","7845abc9":"markdown"},"source":{"5c3bd0b4":"# import necessary libraries first\n\n# pandas to open data files & processing it.\nimport pandas as pd\n\n# numpy for numeric data processing\nimport numpy as np\n\n# sklearn to do preprocessing & ML models\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Matplotlob & seaborn to plot graphs & visulisation\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# to fix random seeds\nimport random, os\n\n# ignore warnings\nimport warnings\nwarnings.simplefilter(action='ignore')","15f182c5":"titanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_data.shape","584080f2":"titanic_data.head()","65e7ea0a":"titanic_data.describe()","c2a6da6e":"# Survival\ntitanic_data['Survived'].value_counts()","a7b2916f":"# Ticket class\ntitanic_data['Pclass'].value_counts()","4af7bc49":"# Gender\ntitanic_data['Sex'].value_counts()","622c9557":"# Siblings\ntitanic_data['SibSp'].value_counts()","a58a9e3c":"# Parent or Childs\ntitanic_data['Parch'].value_counts()","4f0a6583":"# Embarked station\ntitanic_data['Embarked'].value_counts()","2b581f04":"sns.countplot(titanic_data['Sex']);","0572fa6d":"sns.barplot(titanic_data['Survived'], titanic_data['Sex']);","97db7caf":"sns.barplot(titanic_data['Survived'], titanic_data['Fare'], titanic_data['Pclass']);","c8ffa750":"sns.boxplot(x=titanic_data[\"Fare\"])\nplt.show()","49dac130":"# Only take rows which have \"Fare\" value less than 250.\ntitanic_data = titanic_data[titanic_data['Fare'] < 250]\ntitanic_data.shape","220cb853":"sns.boxplot(x=titanic_data[\"Age\"])\nplt.show()","23f60984":"titanic_data.isna().sum()","58bc8149":"titanic_data.drop(\"Cabin\", axis=1, inplace=True)\ntitanic_data.shape","13bb0775":"titanic_data.columns","1803d97e":"age_mean = titanic_data['Age'].mean()\nprint(age_mean)","017cd60e":"titanic_data['Age'].fillna(age_mean, inplace=True)","43b208ce":"titanic_data.isna().sum()","ee6b44fa":"titanic_data['Embarked'].value_counts()","b6bfd4a7":"titanic_data['Embarked'].fillna(\"S\", inplace=True)","6b0d5f66":"titanic_data.isna().sum()","498c210f":"titanic_data.head(10)","f867eb4b":"titanic_data['total_family_members'] = titanic_data['Parch'] + titanic_data['SibSp'] + 1\n\n# if total family size is 1, person is alone.\ntitanic_data['is_alone'] = titanic_data['total_family_members'].apply(lambda x: 0 if x > 1 else 1)\n\ntitanic_data.head(10)","055e7004":"sns.barplot(titanic_data['total_family_members'], titanic_data['Survived'])","f2697137":"sns.barplot(titanic_data['is_alone'], titanic_data['Survived'])","79821063":"def age_to_group(age):\n    if 0 < age < 12:\n        # children\n        return 0\n    elif 12 <= age < 50:\n        # adult\n        return 1\n    elif age >= 50:\n        # elderly people\n        return 2\n    \ntitanic_data['age_group'] = titanic_data['Age'].apply(age_to_group)\ntitanic_data.head(10)","870d3b97":"sns.barplot(titanic_data['age_group'], titanic_data['Survived']);","41d2d847":"titanic_data['name_title'] = titanic_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ntitanic_data.head()","e8d1e7c1":"titanic_data['name_title'].value_counts()","e52e68eb":"def clean_name_title(val):\n    if val in ['Rev', 'Col', 'Mlle', 'Mme', 'Ms', 'Sir', 'Lady', 'Don', 'Jonkheer', 'Countess', 'Capt']:\n        return 'RARE'\n    else:\n        return val\n\ntitanic_data['name_title'] = titanic_data['name_title'].apply(clean_name_title)\ntitanic_data['name_title'].value_counts()","5ec19fd0":"sns.barplot(titanic_data['name_title'], titanic_data['Survived']);","d0b1a22c":"titanic_data.head(10)","0fc4561b":"# save the target column \ntarget = titanic_data['Survived'].tolist()\n\ntitanic_data.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis=1, inplace=True)","efd3b75a":"titanic_data.head()","00198fb0":"le = preprocessing.LabelEncoder()\ntitanic_data['Sex'] = le.fit_transform(titanic_data['Sex'])\ntitanic_data['Embarked'] = le.fit_transform(titanic_data['Embarked'])\ntitanic_data['name_title'] = le.fit_transform(titanic_data['name_title'])\ntitanic_data.head()","bf603fd6":"train_data, val_data, train_target, val_target = train_test_split(titanic_data, target, test_size=0.2)\ntrain_data.shape, val_data.shape, len(train_target), len(val_target)","ac18504e":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n# We fix all the random seed so that, we can reproduce the results.\nseed_everything(2020)","2f3d8f90":"# Train the LogisticRegression model.\n\nmodel = LogisticRegression()\nmodel.fit(train_data, train_target)","b7936379":"# Predict labels on Validation data which model have never seen before.\n\nval_predictions = model.predict(val_data)\nlen(val_predictions)","a767233a":"# first 10 values of validation_predictions\nval_predictions[:10]","1893675f":"# Calculate the accuracy score on validation data.\n# We already have correct target information for them.\n\naccuracy = accuracy_score(val_target, val_predictions)\naccuracy","0a4c5b68":"print(\"We got %.3f percent accuracy on our validation unseen data !!\"%(accuracy*100))\nprint(\"We are %.3f correct in predicting whether a person will survice in Titanic crash !!\"%(accuracy*100))","8e71bda1":"So, we have total of 891 rows & 12 columns","dc0163fc":"People with `Mrs` & `Miss` titles i.e. females have high chances of survival.<br>\nBut in males, with `Master` title, you have higher chances of survival !<br><br>","f8cb4637":"# 6. If it performs good in validation, use model to predict future real world data.\n\n### Now, we can use this model to other people & predict if they were on Titanic ship in 1912 !! ","e72545cc":"There are 177 NaN values in Age & 686 NaN values in Cabin column.<br>\nIn Cabin more than 75% values are empty.<br>\nSo, we will just remove that column.","b85962eb":"## How cool is that..!!\n\nThere's a lot can be done to improve performance.<br>\nBut we will not do that as of now to keep things simple as of now.<br>","c2bdf9f9":"## 3.4 Convert all the data into numeric form\n\nWe can see, `Sex`, `Embarked` & `name_title` are not in numeric form.<br>\nLet's convert them via LabelEncoder from sci-kit learn.","e0d1e400":"Now, we have everything in numbers !!","7df50236":"Let's drop columns which are not useful to us as of now.<br>","644d50b1":"# 3. Preprocess data\n\nIn preprocessing step, we detect outliers & remove them from our data.\n\n## 3.1\n\n## What is an outlier in data? Why does it occur?\nOutliers are as the name suggests, very different from general \/ normal trend.<br>\nThey occur in data because of some faults in data collection pipeline.<br><br>\n\n## Why we generally remove outliers?\nBecause one big outlier can mess up whole model's performance.<br>\nEven if all other contributions might be of a low value, one high outlier value already shifts the entire gradient towards higher values as well.<br>\n\nMost of the time, we remove outliers so that, we can train our model only from general trends.<br>\nLet's see some examples using our titanic data.<br>\n\n### One common practice followed to detect outliers is BoxPlot.","ae6ef6a6":"## Let's check unique values for each column","8fe1c6ab":"### Why this age_group feature is useful ?\nLet's see..","4f4874da":"We can see there are some outliers in `Age`, but they are not much far. So, we will keep as of now.","0f57ea4d":"There are just 2 NaN values in `Embarked` column.<br>\nWe handle NaN values in `Embarked` column by filling most occuring value in that column.","a9c3f6fd":"# 1. Open the data files.","26423b87":"So, total 335 people have survived & 547 people have died in the Titanic.","fac605fb":"We can fill all the NaN values using `fillna` ","cc28a0b6":"We have our training data & validation data.<br>\nWe have randomly choosen 20% of the all the rows on which we will check our model's performance.","ec8181b3":"Training is done.<br>\nWe have trainied our Logistic Regression model.<br><br>\n\n# 5. Validate the trained model i.e. checking it's performance on unseen data.\n\nIt's called \"unseen\" because our ML model have never seen this data.<br>\nIt's kind of a test for it.<br>\nWhere it's performance will be checked on data which it have never seen or train.","185f11cb":"# Summary\n\nSo, in this lesson, we saw, what a typical pipeline looks like in solving a machine learning(ML) problem.\n\n1. Open the data files.\n2. Understand the data. What each column in the table means.\n3. Preprocess data\n    * Remove the outliers.\n    * Fill `NaN` or `null` values. Sometimes, we also remove all the rows with `NaN` values.\n    * Feature engineering - Create new columns out of existing columns using our understanding.\n    * Converting data into numeric form if it's not.\n4. Train a machine learning model.\n5. Validate the trained model i.e. checking it's performance on unseen data.\n6. If it performs good in validation, use model to predict future real world data.\n\n\n## Upvote this kernel if you have learned something from it.\n## Tell me if you have any kind of doubts \/ questions in comment section below.\n\n## In next lesson we will solve this same problem with deep learning.\n## See you in the [next lesson](https:\/\/www.kaggle.com\/prashantkikani\/solving-the-titanic-problem-deep-learning-way) \ud83d\udc4b","27b30f0c":"## Voila !!<br>\n","59aac18d":"We can see, for majority of passengers, `Fare` price is less than 250.<br>\nSo, let's only keep the rows with `Fare` < 250.","f07ed6a6":"# 4. Train a machine learning model.\n\nIn this step, we choose a ML model & train it one the data we have.<br>\nFor this lesson, we will use basic `LogisticRegression` model.<br>\n\nBut first of all, let's split our data into training & validation part.<br>\nThere's `train_test_split` from sci-kit learn.","ef64a1f5":"We can see `Cabin` column is removed from our data.<br><br>\nNow, `Age` is a numeric column.<br>\nSo, let's fill NaN values by mean of all the other non-NaN values.","1e892a22":"Wow !<br>\n~75% of females have survived.<br>\nEven if total number of females are less than males.<br><br>\n\nMay be because, females were given more priority in lifeboats than males. May be.","7e791be9":"People with higher class have higher chances of survival !","dcf7d259":"So, we have removed 9 rows.<br>Originally, there were 891 rows.","ffd6b972":"## 3.2 Fill NaN or null values in data\n\n### Why NaN (not a number) values occur in data?\nSometimes, while collecting data, if some information is missing for some rows, it's filled as NaN.<br>\nIt means nothing is there.<br>\nIt's empty.<br>\n\n### How NaN values can be handled?\nThere are several methods.\n* Fill a specified value like \"EMPTY\" or -1 for all the NaN values.\n    * This option is good for categorical type columns \/ features.\n* If column is numeric in nature, fill with mean or median of that specific column.\n    * This option is good for numerical type columns \/ features.\n* Remove all the raws who have atleast 1 NaN value in any column.\n    * If total number of raws with NaN values is less, we can just remove those rows from our data.\n\nLet's look if there are any missing values in our data.","0a1e9e6e":"# 2. Understand the data. What each column in the table means.\n\n**Each row in above table contains data of a passenger.<br>**\nThose details include following columns.<br>\n\nHere is all the columns of above table mean.<br>\n\n`PassengerId` : Unique ID for each passenger.<br><br>\n`Survived` : Whether that passenger survived or not. (0 = No, 1 = Yes)<br><br>\n`Pclass` : Ticket class of passenger. (1 = Upper, 2 = Middle, 3 = Lower)<br><br>\n`Name` : Name of passenger<br><br>\n`Sex` : Gender of passenger<br><br>\n`Age` : Age of passenger<br><br>\n`SibSp` : # of siblings \/ spouses aboard the Titanic of passenger<br><br>\n`Parch` : # of parents \/ children aboard the Titanic of passenger<br><br>\n`Ticket` : Ticket number of passenger<br><br>\n`Fare` : Ticket amount \/ passenger fare.<br><br>\n`Cabin` : Cabin number of passenger<br><br>\n`Embarked` : Port of Embarkation of passenger. (C = Cherbourg, Q = Queenstown, S = Southampton)<br><br>","2522ad2a":"Let's once again look at what we have at hand.<br><br><br>\n`PassengerId` : Unique ID for each passenger.<br><br>\n`Survived` : Whether that passenger survived or not. (0 = No, 1 = Yes)<br><br>\n`Pclass` : Ticket class of passenger. (1 = Upper, 2 = Middle, 3 = Lower)<br><br>\n`Name` : Name of passenger<br><br>\n`Sex` : Gender of passenger<br><br>\n`Age` : Age of passenger<br><br>\n`SibSp` : # of siblings \/ spouses aboard the Titanic of passenger<br><br>\n`Parch` : # of parents \/ children aboard the Titanic of passenger<br><br>\n`Ticket` : Ticket number of passenger<br><br>\n`Fare` : Ticket amount \/ passenger fare.<br><br>\n`Embarked` : Port of Embarkation of passenger. (C = Cherbourg, Q = Queenstown, S = Southampton)<br><br>\n\n### How can we use these columns to create more relevant information?\n\nLet's use `SibSp` & `Parch` to create a `total_family_members` feature.","7cf21057":"`0` i.e. children have higher survival rate compared to adults & elderly people.<br>\nThis data may become useful to our model.<br>\n\n### Can you think of any way we can use `name` column ?\nWe can capture name title like Mr. Ms. Miss. etc.","1f3f06c9":"People with family have 20% higher chance of survival than people travelling alone !!\n\n`Age` column also can be used to create partitions.<br>\nWe can use `apply` function to `Age` column to create new column `age_group`<br>\nLike..","b1c3cf02":"Next step is **Feature Engineering**\n\n## 3.3\n\n### What is Feature Engineering?\n> Feature Engineering is creating more meaningful data out of existing data using our domain knowledge & comman sense.<br>\n\nIn other words, we try to create more relevant information for our ML models. <br>\nSo, that our model can capture patterns in faster & better ways.\n\n### Now, this is a creative step. We need to use brain to create relevant features in the data.\n\nLet's think.","3b6645b0":"Now, we can see, no NaN values are there in our whole data.","7a261db4":"Interesting.<br>\nPeople with total_family_members = 4 have more than 70% chances of survival !<br><br>","03916375":"This tells `3` value occurs 491 times, `1` value occurs 207 times etc.","e989c146":"Most of the passengers have embarked from \"Cherbourg\" & \"Southampton\"","7845abc9":"In the [last lesson](https:\/\/www.kaggle.com\/prashantkikani\/introduction-to-basic-python-libraries-for-ml), we saw basics of Python libraris which we use in Machine Learning.<br>\nToday in this lesson, we are going to solve one of the most famous & interesting problem. **Titanic survival problem** \ud83d\udef3\ufe0f\n\n![titanic](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg)\n\n<br>\nIn this problem, we have some data about each passenger that were into that ship.<br>\n**Our problem is to predict or forecast, whether this person will survive the ship sinking or not**.<br><br>\n\nWe will try to solve this problem in a standard way.<br>\nWe can solve any machine learning problem this way.<br>\n\n## Goal of this lesson is to learn a standard way to solve \/ approch any machine learning problem.\n\nWe will keep this lesson as simple as possible so that everyone can grasp the idea & learn to solve any basic problem in ML.\n\nHere are the steps we tentatively follow.<br>\n1. Open the data files.\n2. Understand the data. What each column in the table means.\n3. Preprocess data\n    * Remove the outliers.\n    * Fill `NaN` or `null` values. Sometimes, we also remove all the rows with `NaN` values.\n    * Feature engineering - Create new columns out of existing columns using our understanding.\n    * Converting data into numeric form if it's not.\n4. Train a machine learning model.\n5. Validate the trained model i.e. checking it's performance on unseen data.\n6. If it performs good in validation, use model to predict future real world data.\n\n<br>\nAbove steps are generally followed to solve a ML problem.<br>\nSo, let's start.."}}