{"cell_type":{"e664fbf9":"code","404dd805":"code","44a38e03":"code","d033dd5e":"code","26f215e9":"code","f52c151b":"code","65f20693":"code","cc11a625":"code","18a3b0ef":"code","f400dbe9":"code","5527e604":"code","73767ebd":"code","8f455bbc":"code","390370a1":"code","5de19211":"code","ffb00f6e":"code","3b455a2f":"code","ebf9982a":"code","8ea114a0":"code","7fc38465":"code","e406fc33":"code","38acd8f7":"code","4b46ec0d":"code","f216ca3e":"code","c396974d":"code","526bca03":"code","98c186d4":"code","b58c48c5":"code","c652b4f1":"markdown","a48509ea":"markdown","e3a263a0":"markdown"},"source":{"e664fbf9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.metrics import cohen_kappa_score\nimport xgboost as xgb\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport plotly.graph_objects as go\nimport warnings\nwarnings.filterwarnings('ignore')","404dd805":"%%time\ntest = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/test.csv\")\ntrain_labels = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train.csv\")\nspecs = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/specs.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv\")\nprint(\"Data Loaded!\")","44a38e03":"train.head()","d033dd5e":"train_labels.head(4)","26f215e9":"specs.head()","f52c151b":"specs['info'][1]","65f20693":"test.sample(5)","cc11a625":"train['title'].nunique()","18a3b0ef":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\nsns.set()\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=train['title'], data=train)","f400dbe9":"countsT = train[\"type\"].value_counts()\nvalues = list(range(4))\nlabels = 'Game' ,'Activity', 'Assessment', 'Clip'\nsizes = countsT.values\nexplode = (0.1, 0.1, 0.1, 0.9)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')  \nplt.show()","5527e604":"countsT = train_labels[\"accuracy_group\"].value_counts()\nvalues = list(range(4))\nlabels = '3' ,'2', '1', '0'\nsizes = countsT.values\nexplode = (0.1, 0.1, 0.1, 0.9)  \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')  \nplt.show()","73767ebd":"countsT = train[\"world\"].value_counts()\nvalues = list(range(4))\nlabels = 'MAGMAPEAK' ,'CRYSTALCAVES', 'TREETOPCITY', 'NONE'\nsizes = countsT.values\nexplode = (0.1, 0.1, 0.1, 0.9)  \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')  \nax1.set_title('Title to which Game\/Video belongs to')","8f455bbc":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain['date'] = train['timestamp'].dt.date\ngroup2 = train.groupby(['date'])['event_id'].agg('count')\nfig = go.Figure([go.Scatter(x=group2.index, y=group2.values, line_color= \"#B22222\", )])\nfig.update_layout(title_text='Time Series for all Events')\nfig.show()","390370a1":"train_labels.drop(['num_correct','num_incorrect','accuracy','title'],axis=1,inplace=True)","5de19211":"train.drop(['event_data','date'],axis=1,inplace=True)","ffb00f6e":"not_req=(set(train.installation_id.unique()) - set(train_labels.installation_id.unique()))\ntrain = train[~train['installation_id'].isin(not_req)]\nprint(train.shape)","3b455a2f":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    return df","ebf9982a":"def prepare_data(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['hour_of_day']=df['timestamp'].map(lambda x : int(x.hour))\n    #one hot encoding on event code\n    join_one=pd.get_dummies(df[['event_code','installation_id','game_session']],\n                            columns=['event_code']).groupby(['installation_id','game_session'],as_index=False,sort=False).agg(sum)\n    \n    #dictionary to perform some aggregate functions after grouping\n    agg={'event_count':sum,'hour_of_day':'mean','game_time':['sum','mean'],'event_id':'count'}\n    \n    join_two=df.drop(['timestamp'],axis=1).groupby(['installation_id','game_session'],as_index=False,sort=False).agg(agg)\n    \n    join_two.columns= [' '.join(col).strip() for col in join_two.columns.values]\n\n    join_three=df[['installation_id','game_session','type','world','title']].groupby(['installation_id','game_session'],as_index=False,sort=False).first()\n    \n    join_four=(join_one.join(join_two.drop(['installation_id','game_session'],axis=1))). \\\n                        join(join_three.drop(['installation_id','game_session'],axis=1))\n    return join_four","8ea114a0":"join_train=prepare_data(train)\ncols=join_train.columns.to_list()[2:-3]\njoin_train[cols]=join_train[cols].astype('int16')","7fc38465":"join_test=prepare_data(test)\ncols=join_test.columns.to_list()[2:-3]\njoin_test[cols]=join_test[cols].astype('int16')","e406fc33":"cols=join_test.columns[2:-8].to_list()\ncols.append('event_id count')\ncols.append('installation_id')","38acd8f7":"df=join_test[['hour_of_day mean','event_count sum','game_time mean','game_time sum',\n    'installation_id']].groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=join_test[cols].groupby('installation_id',as_index=False,\n                               sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=join_test[['title','type','world','installation_id']].groupby('installation_id',\n         as_index=False,sort=False).last().drop('installation_id',axis=1)\n        ","4b46ec0d":"final_train=pd.merge(train_labels,join_train,on=['installation_id','game_session'],\n                                         how='left').drop(['game_session'],axis=1)\n\n#final_test=join_test.groupby('installation_id',as_index=False,sort=False).last().drop(['game_session','installation_id'],axis=1)\nfinal_test=(df.join(df_two)).join(df_three).drop('installation_id',axis=1)","f216ca3e":"df=final_train[['hour_of_day mean','event_count sum','game_time mean','game_time sum','installation_id']]. \\\n    groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=final_train[cols].groupby('installation_id',as_index=False,\n                                 sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=final_train[['accuracy_group','title','type','world','installation_id']]. \\\n        groupby('installation_id',as_index=False,sort=False). \\\n        last().drop('installation_id',axis=1)\n\nfinal_train=(df.join(df_two)).join(df_three).drop('installation_id',axis=1)","c396974d":"#concat train and test and Label Encode Categorical Columns\n\nfinal=pd.concat([final_train,final_test])\nencoding=['type','world','title']\nfor col in encoding:\n    lb=LabelEncoder()\n    lb.fit(final[col])\n    final[col]=lb.transform(final[col])\n    \nfinal_train=final[:len(final_train)]\nfinal_test=final[len(final_train):]","526bca03":"X_train=final_train.drop('accuracy_group',axis=1)\ny_train=final_train['accuracy_group']","98c186d4":"%%time\n\npars = {\n    'colsample_bytree': 0.5,                 \n    'learning_rate': 0.01,\n    'max_depth': 10,\n    'subsample': 0.5,\n    'objective':'multi:softprob',\n    'num_class':4\n}\n\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\ny_pre=np.zeros((len(final_test),4),dtype=float)\nfinal_test=xgb.DMatrix(final_test.drop('accuracy_group',axis=1))\n\n\nfor train_index, val_index in kf.split(X_train):\n    train_X = X_train.iloc[train_index]\n    val_X = X_train.iloc[val_index]\n    train_y = y_train[train_index]\n    val_y = y_train[val_index]\n    xgb_train = xgb.DMatrix(train_X, train_y)\n    xgb_eval = xgb.DMatrix(val_X, val_y)\n    \n    xgb_model = xgb.train(pars,\n                  xgb_train,\n                  num_boost_round=10000,\n                  evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n                  verbose_eval=False,\n                  early_stopping_rounds=100\n                 )\n    \n    val_X=xgb.DMatrix(val_X)\n    pred_val=[np.argmax(x) for x in xgb_model.predict(val_X)]\n    \n    print('choen_kappa_score :',cohen_kappa_score(pred_val,val_y,weights='quadratic'))\n    \n    pred=xgb_model.predict(final_test)\n    y_pre+=pred\n    \npred = np.asarray([np.argmax(line) for line in y_pre])","b58c48c5":"sub=pd.DataFrame({'installation_id':submission.installation_id,'accuracy_group':pred})\nsub.to_csv('submission.csv',index=False)","c652b4f1":"**Model**","a48509ea":"**This part is taken from this kernel,check it out and upvote it if you like.**\n\nhttps:\/\/www.kaggle.com\/shahules\/xgboost-starter-dsbowl","e3a263a0":"**DATA PREPARATION**"}}