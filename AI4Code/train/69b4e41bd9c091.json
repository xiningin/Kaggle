{"cell_type":{"6f432446":"code","6d609913":"code","d9d89bf4":"code","911ef294":"code","1c89f1cb":"code","42fd1f51":"code","8e9b79ea":"code","9fb0a644":"code","b35dd46f":"code","281cb379":"code","45fba693":"code","23a221f7":"code","ee6cfd6f":"code","913a9ea0":"code","feb5e112":"code","d4177d05":"code","899c276d":"code","02de7b74":"code","582ae92e":"markdown","9ccea89a":"markdown","f3eef289":"markdown","3eb64a9d":"markdown","efcc1ddd":"markdown","ef3d518a":"markdown","2661da7e":"markdown","b23dc252":"markdown","ac8712dd":"markdown","6d654a94":"markdown","8e5489f2":"markdown","13db68be":"markdown"},"source":{"6f432446":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nimport glob","6d609913":"# For quickly switching between training and test data\ndef train_test(mode):\n    # mode = \"train\"\/\"test\"\n    file_name = '..\/input\/optiver-realized-volatility-prediction\/' + mode + '.csv'\n    return pd.read_csv(file_name)\n\n","d9d89bf4":"train = train_test(\"train\")\ntrain.head()","911ef294":"order_book_training = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')","1c89f1cb":"# custom aggregate function\ndef wap2vol(df):\n    # wap2vol stands for WAP to Realized Volatility\n    temp = np.log(df).diff() # calculating tik to tik returns\n    # returning realized volatility\n    return np.sqrt(np.sum(temp**2)) ","42fd1f51":"# function for calculating realized volatility per time id for a given stock\ndef rel_vol_time_id(path):\n    # book: book is an order book\n    book = pd.read_parquet(path) # order book for a stock id loaded\n    # calculating WAP\n    p1 = book[\"bid_price1\"]\n    p2 = book[\"ask_price1\"]\n    s1 = book[\"bid_size1\"]\n    s2 = book[\"ask_size1\"]\n    \n    book[\"WAP\"] = (p1*s2 + p2*s1) \/ (s1 + s2)\n    # calculating realized volatility for each time_id\n    transbook = book.groupby(\"time_id\")[\"WAP\"].agg(wap2vol)\n    return transbook","8e9b79ea":"%%time \nstock_id = []\ntime_id = []\nrelvol = []\nfor i in order_book_training:\n    # finding the stock_id\n    temp_stock = int(i.split(\"=\")[1])\n    # find the realized volatility for all time_id of temp_stock\n    temp_relvol = rel_vol_time_id(i)\n    stock_id += [temp_stock]*temp_relvol.shape[0]\n    time_id += list(temp_relvol.index)\n    relvol += list(temp_relvol)","9fb0a644":"past_volatility = pd.DataFrame({\"stock_id\": stock_id, \"time_id\": time_id, \"volatility\": relvol})","b35dd46f":"joined = train.merge(past_volatility, on = [\"stock_id\",\"time_id\"], how = \"left\")\nR2 = round(r2_score(y_true = joined['target'], y_pred = joined['volatility']),3)\nprint(f'The R2 score of the naive prediction for training set is {R2}')","281cb379":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\nrmspe = rmspe(joined[\"target\"], joined[\"volatility\"])\nprint(f'The RMSPE score of the native prediciton for the training set is {rmspe}')","45fba693":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# for training\ndef linear_training(X,y,degree):\n    # instantiating polynomial features\n    polyfeat = PolynomialFeatures(degree = degree)\n    linreg = LinearRegression()\n    # preprocessing the training data\n    x = np.array(X).reshape(-1,1)\n    # creating the polynomial features\n    X_ = polyfeat.fit_transform(x)\n    # training the model\n    weights = 1\/np.square(y)\n    return linreg.fit(X_, np.array(y).reshape(-1,1), sample_weight = weights)\n\n\nstock_id_train = train.stock_id.unique() # all stock_id for the train set\nmodels = {} # dictionary for holding trained models for each stock_id\ndegree = 2\nfor i in stock_id_train:\n    temp = joined[joined[\"stock_id\"]==i]\n    X = temp[\"volatility\"]\n    y = temp[\"target\"]\n    models[i] = linear_training(X,y,degree)\n    ","23a221f7":"models","ee6cfd6f":"# listing all test order books\norder_book_test = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')","913a9ea0":"%%time \nstock_id = []\ntime_id = []\nrelvol = []\nfor i in order_book_test:\n    # finding the stock_id\n    temp_stock = int(i.split(\"=\")[1])\n    # find the realized volatility for all time_id of temp_stock\n    temp_relvol = rel_vol_time_id(i)\n    stock_id += [temp_stock]*temp_relvol.shape[0]\n    time_id += list(temp_relvol.index)\n    relvol += list(temp_relvol)\n    \npast_test_volatility = pd.DataFrame({\"stock_id\": stock_id, \"time_id\": time_id, \"volatility\": relvol})","feb5e112":"# for inference\ndef linear_inference(models, stock_id, past_volatility, degree):\n    model = models[stock_id]\n    polyfeat = PolynomialFeatures(degree = degree)\n    return model.predict(polyfeat.fit_transform([[past_volatility]]))[0][0]\n    ","d4177d05":"# creating the header for the submission file\nsubmission = pd.DataFrame({\"row_id\" : [], \"target\" : []})  \nsubmission[\"row_id\"] = past_test_volatility.apply(lambda x: str(int(x.stock_id)) + '-' + str(int(x.time_id)), axis=1)\n# prediction for test data\nsubmission[\"target\"] = past_test_volatility.apply(lambda x: linear_inference(models,\\\n                                                                            x.stock_id,\\\n                                                                            x.volatility,\\\n                                                                            degree),\\\n                                                 axis = 1)","899c276d":"submission.to_csv('submission.csv',index = False)","02de7b74":"submission","582ae92e":"### Importing all necessary libraries","9ccea89a":"We will be using *linear_inference* for prediction","f3eef289":"Each member in *order_book_training* corresponds to a single stock. Each stock contains several *time_id*. The goal is to predict volatility for each (*stock_id, time_id*) tuples.\n\nHere we utilize the fact that the Panda's groupby operation retains the order of the rows. We write a custom aggregate function to calculate WAP -> tik to tik returns -> realized volatility.\n\n**Warning**: Custom aggregate functions are not Cythonized and they are slow. Basically cutom aggregate functions are syntactic sugars with a sprinkle of enhanced readability.","3eb64a9d":"Now we iterate over all order books and compute realized volatility of each (*stock_id, time_id*) tuples.","efcc1ddd":"The *order_book* data are partitioned on the basis of *stock_id*. The following command lists all the parquet file names, it will help us to iterate over all stocks in later section.","ef3d518a":"We start by calculating the past volatility of the test set.","2661da7e":"Let's make prediction on the test set and submit a sample submission","b23dc252":"Let's train a simple OLS model for each stock_id. Use *degree* to specify the degree of the linear model.","ac8712dd":"Now we create the dataframe containing realized volatilities for all _(stock_id, time_id)_ tuples.","6d654a94":"A simplified reproduction of the [tutorial notebook](http:\/\/)https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data provided by the content organizers. By \"simplified\" I am meaning that the usage of ultra familiar tools is maximized. Hope it will ease the on-boarding of new joiners in this competition. Heavily inspired by this [notebook](http:\/\/)https:\/\/www.kaggle.com\/lucasmorin\/realised-vol-weighted-regression-baseline?select=sample_submission.csv. ","8e5489f2":"Now we join *past_volatility* with *training* to calculate the error metrics, mainly for a sanity check to confirm that it is a correct reproduction. Here we are naively assuming that **past level of volatility = future level of volatility**.","13db68be":"Instead of concatenating dataframes corresponding to each stock, I am rather taking an unified approach by listing all *stock_id, time_id* and their realized volatility. Later these lists will be converted to a dataframe. This approach is recommended\/reiterated in this [stack exchange discussion](http:\/\/)https:\/\/stackoverflow.com\/questions\/13784192\/creating-an-empty-pandas-dataframe-then-filling-it "}}