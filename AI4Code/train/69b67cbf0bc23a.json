{"cell_type":{"80eebefd":"code","c9e3cdec":"code","2676ec82":"code","2dd0608c":"code","7cc39759":"code","9dd93adf":"code","f88a736d":"code","7dfa5d3d":"code","8929a4e6":"code","674a80d0":"code","6c02a417":"code","60e91eb5":"code","6087eb11":"code","a0bbb5c2":"code","f4f0898c":"code","5ad7d40f":"code","89fa7f4f":"code","28a8b353":"code","af6964f5":"code","e34680b8":"code","027617ea":"code","abe0e8f6":"code","08d06d5d":"code","0e24b633":"code","49de905d":"code","c055e772":"code","01105274":"code","bf8813e2":"code","ebe089db":"code","0a1079c5":"code","4c04fd41":"code","17c825fb":"code","95dcd5dd":"markdown","87cf3cc0":"markdown","fc5de1aa":"markdown","abf5d506":"markdown","93eac588":"markdown","289c281b":"markdown","cb632dd2":"markdown","c878e771":"markdown","9eded6a1":"markdown","32f076ee":"markdown","a4281d59":"markdown","0a5a9d2a":"markdown","f4285351":"markdown","ce0abffe":"markdown","9d56f63b":"markdown"},"source":{"80eebefd":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","c9e3cdec":"# Load the data\ndata_dict = pd.read_excel('..\/input\/Data_Dictionary.xlsx')\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv')\nmerchants = pd.read_csv('..\/input\/merchants.csv')\nnew_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv')\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","2676ec82":"print(data_dict)\n\n# The Excel documents actually has several sheets, which we load in turn and append\nexcel_doc = pd.ExcelFile('..\/input\/Data_Dictionary.xlsx')\ndata_dict_train  = pd.read_excel(excel_doc, 'train', skiprows=2)\ndata_dict_historical = pd.read_excel(excel_doc, 'history', skiprows=2)\ndata_dict_new_merchants = pd.read_excel(excel_doc, 'new_merchant_period', skiprows=2)\ndata_dict_merchant = pd.read_excel(excel_doc, 'merchant', skiprows=2)\n\ndata_dict = data_dict_train.append(data_dict_historical) \\\n                            .append(data_dict_new_merchants).append(data_dict_merchant)\n\nprint(data_dict)","2dd0608c":"historical_transactions.head()","7cc39759":"merchants.head()","9dd93adf":"new_transactions.head()","f88a736d":"train_data.head()","7dfa5d3d":"# Check for null values\ndata_dict.isnull().sum()","8929a4e6":"# Check for duplicates\ndata_dict.duplicated().sum()\n\n# Remove duplicates\ndata_dict.drop_duplicates(inplace=True)","674a80d0":"# Check for null values\nhistorical_transactions.isnull().sum()","6c02a417":"# Let's examine what categories 2 and 3 represent, from the data dictionary\ndata_dict.loc[(data_dict.Columns == 'category_3') | (data_dict.Columns == 'category_2'),]","60e91eb5":"# Let's examine a few rows with missing merchant ID's\nhistorical_transactions.loc[historical_transactions.merchant_id.isnull(),].head()","6087eb11":"# Let's drop rows with missing merchant ID's, but keep those with missing categories 2 and 3\nhistorical_transactions.dropna(subset=['merchant_id'], axis=0, inplace=True)","a0bbb5c2":"# Check for duplicates\nhistorical_transactions.duplicated().sum()","f4f0898c":"# Visualize possible values for categories 1, 2 and 3 to check for invalid data\nprint(historical_transactions.category_1.unique(),\n      historical_transactions.category_2.unique(),\n      historical_transactions.category_3.unique())","5ad7d40f":"# Definition of purchase amount\nprint(data_dict.loc[data_dict.Columns == 'purchase_amount',])\n\n# Boxplot\nplt.boxplot(historical_transactions.purchase_amount)\nplt.show();","89fa7f4f":"# Remove 1st outlier at 600,000 (probably was not normalized), and visualize again\nhistorical_transactions = historical_transactions.loc[historical_transactions.purchase_amount < 500000,]\nplt.boxplot(historical_transactions.purchase_amount)\nplt.show();","28a8b353":"# Remove more outliers, greater than 2\nhistorical_transactions = historical_transactions.loc[historical_transactions.purchase_amount < 2,]\nplt.boxplot(historical_transactions.purchase_amount)\nplt.show();","af6964f5":"# Boxplot for month lag\nplt.boxplot(historical_transactions.month_lag)\nplt.show();","e34680b8":"# Boxplot for installments\nplt.boxplot(historical_transactions.installments)\nplt.show();\n\n# Remove the outlier at 1000 installments\nhistorical_transactions = historical_transactions.loc[historical_transactions.installments < 900,]","027617ea":"# Examine null values\nmerchants.isnull().sum()","abe0e8f6":"# Define the lagged sales\nprint(data_dict.loc[data_dict.Columns == 'avg_sales_lag3','Description'])\n\n# Drop rows with missing values of lagged sales\nmerchants.dropna(subset=['avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12'], axis=0, inplace=True)","08d06d5d":"# Create boxplots for numerical columns\nmerchants.boxplot()\nplt.xticks(rotation='vertical')\nplt.show();","0e24b633":"# Check for missing values\nnew_transactions.isnull().sum()","49de905d":"# Similar to the historical transactions, we drop missing merchant ID's\nnew_transactions.dropna(subset = ['merchant_id'], axis=0, inplace=True)","c055e772":"# Check for duplicates\nnew_transactions.duplicated().sum()","01105274":"# Check for outliers\nnew_transactions.boxplot()\nplt.xticks(rotation='vertical')\nplt.show();","bf8813e2":"# Similarly to historical transactions, we remove outliers for the purchase amount and installments\nnew_transactions = new_transactions.loc[(new_transactions.purchase_amount < 2) & (new_transactions.installments < 900),]","ebe089db":"# Check for null values\ntrain_data.isnull().sum()","0a1079c5":"# Check for duplicates\ntrain_data.isnull().sum()","4c04fd41":"# Visualize the range of values for the target\nplt.boxplot(train_data.target)\nplt.show();","17c825fb":"historical_transactions.to_csv('historical_transactions_clean.csv')","95dcd5dd":"### Train","87cf3cc0":"All of the individual datasets have been loaded in their respective dataframes. Let's clean each in turn, before engineering features a way that makes sense for analysis.","fc5de1aa":"We remove duplicates which arose from the appending of individual data dictionaries.\n\n### Historical Transactions","abf5d506":"We'll go through the dataframes in turn and check for classical examples of dirty data, such as missing values or duplicate rows.","93eac588":"### Merchants","289c281b":"We keep the rows with missing values in the category_2 column (for a similar reason as the previous dataset) but drop missing values of lagged sales (only 13 rows so it will not make a huge difference). Looking at boxplots for numerical columns we can spot a lot of outliers, but they seem more likely to be valid data than for the previous dataset. Hence we keep these rows for future analysis.","cb632dd2":"## Data Wrangling\n\nThe first step is to get all our data in one place. We use Pandas' functionalities to load our data.","c878e771":"Then we can examine the first few rows of each dataset in turn to check we have everything in the right format.","9eded6a1":"## Data Cleaning","32f076ee":"### Data Dictionary","a4281d59":"Let's save the data we have cleaned.","0a5a9d2a":"Looking at the missing values, we decide to drop the rows with missing merchant ID's, because it will be impossible for us to match this back to individual merchant information, and hence to compute a loyalty score in later steps. There are missing values in the category_2 and category_3, but we leave them as is, and will create a dummy variable for this later: it is indeed possible for a merchant to only be in one category.\n\nThere are no duplicates, but several rows with outlier values, in particular in the normalized purchase amount columns. My interpretation for the latter is that the amount was not normalized, but lacking the original mean and standard deviation we cannot make the transformation ourselves and hence choose to simply drop these rows.","f4285351":"We conduct a very similar data cleaning to historical transactions, removing outliers and missing merchant ID's.","ce0abffe":"# ELO Merchant Category Recommendation\n\nThe goal of this kernel is to get the data in a suitable form for analysis. This is the first kernel in a series which will follow the typical steps in any data science project:\n- get the data (data wrangling)\n- clean the data (data cleaning)\n- explore the data and engineer features (exploratory data analysis)\n- model the data (data modeling)\n- interpret the results and recommend actions\n\nThis is a flexible framework I recommend everyone to follow. In this kernel, we will focus on the first two steps of this analysis.","9d56f63b":"### New Transactions"}}