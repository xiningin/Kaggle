{"cell_type":{"180eff06":"code","7b1793ad":"code","7f136b5f":"code","8a905295":"code","6c094b9e":"code","bd931694":"code","ee0ce832":"code","7fe017a1":"code","31c9343f":"code","cdf11ed9":"code","0b3956e3":"code","24c55cde":"code","96998f62":"code","29211443":"code","66005203":"code","e5304209":"code","124d3c28":"code","e7160221":"code","c7af2250":"code","b1bf7ca6":"code","ce6a12d1":"code","71ea1eef":"code","7c7f3249":"code","343d8594":"code","bf679f8e":"code","9490f0af":"code","6f12d9f3":"code","0cc0c79d":"code","4c1985c1":"code","61918f15":"code","8ae4df29":"code","0d3d7cac":"code","edf78a3b":"code","935908ee":"code","a4e58cf6":"code","f95c5cf3":"code","caf37d04":"code","d547c58c":"code","75fe87b3":"code","7d5b0602":"code","8ada3082":"code","e3597966":"code","e5993f0b":"code","5ac7acd9":"code","6cc0ee53":"code","4fcb9cf6":"code","c931b7ee":"code","e3efeffa":"code","842034a3":"code","aabbb76e":"code","2acffb62":"code","72228615":"code","8c759540":"code","1a9c775c":"code","8e029e67":"code","e7c49102":"code","ca162fe4":"code","7019ad2f":"code","451657a5":"code","2f06c9c7":"code","d46e0279":"markdown","24fbadaf":"markdown","2b526b45":"markdown","4f7e1009":"markdown","2d3fc746":"markdown","9fb062b4":"markdown","826cd1eb":"markdown","d391ace7":"markdown","cc7521d4":"markdown","62a20a17":"markdown","65acb67e":"markdown","ba04d06c":"markdown","59c70687":"markdown","667291bf":"markdown","479466f1":"markdown","6a67717b":"markdown","6f563eff":"markdown","470361f1":"markdown","d853e039":"markdown","ad541f49":"markdown","c2a639cc":"markdown","e615cbd2":"markdown","dcb8ce9e":"markdown","89482fab":"markdown","bae4cfa2":"markdown","c84c524e":"markdown","ff2ea89d":"markdown","ee73a7fa":"markdown","e1e311b3":"markdown","2d7aacc3":"markdown","970e2532":"markdown","1e7d6adb":"markdown","7db3af81":"markdown","d1d964d8":"markdown","8ac335b2":"markdown","0c82c42d":"markdown","3c35d032":"markdown","4fc07120":"markdown"},"source":{"180eff06":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\n#Set basic configuration for pandas, matplotlib and seaborn\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 150)\nsns.set_style(\"darkgrid\")\n\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","7b1793ad":"raw_df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\nraw_df","7f136b5f":"raw_df.info()","8a905295":"#drop all null values from dataframe[RainTomorrow]\nraw_df.dropna(subset = [\"RainTomorrow\"], inplace = True)","6c094b9e":"raw_df.describe().T","bd931694":"plt.figure(figsize = (20,10))\nsns.heatmap(raw_df.corr(), annot = True)","ee0ce832":"sns.boxplot(x = \"MinTemp\", y = \"RainTomorrow\", data = raw_df, dodge = True);","7fe017a1":"sns.boxplot(x = \"MaxTemp\", y = \"RainTomorrow\", data = raw_df, dodge = True);","31c9343f":"sns.boxplot(x = \"Cloud9am\", y = \"RainTomorrow\", data = raw_df, dodge = True);","cdf11ed9":"raw_df.isna().sum().sort_values(ascending = False)","0b3956e3":"#Fetching numeric columns from the dataset\nnumeric_cols = raw_df.select_dtypes(include = np.number).columns.to_list()\nprint(numeric_cols)","24c55cde":"#Fetching categorical or string columns from the dataset\ncategorical_cols = raw_df.select_dtypes('object').columns.to_list()\nprint(categorical_cols)","96998f62":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = \"mean\").fit(raw_df[numeric_cols])","29211443":"#apply the mean imputing strategy on numerical columns of raw_df\nraw_df[numeric_cols] = imputer.transform(raw_df[numeric_cols])","66005203":"raw_df[numeric_cols].isna().sum()","e5304209":"raw_df[categorical_cols].isna().sum()","124d3c28":"raw_df[categorical_cols] = raw_df[categorical_cols].fillna(\"unknown\")","e7160221":"raw_df[categorical_cols].isna().sum()","c7af2250":"raw_df[\"year\"] = pd.to_datetime(raw_df.Date).dt.year;\nraw_df[\"month\"] = pd.to_datetime(raw_df.Date).dt.month;\nraw_df[\"day\"] = pd.to_datetime(raw_df.Date).dt.day;","b1bf7ca6":"raw_df.drop(columns = [\"Date\"], inplace = True)","ce6a12d1":"raw_df.info()","71ea1eef":"numeric_cols = raw_df.select_dtypes(include = np.number).columns.to_list()\nprint(numeric_cols)","7c7f3249":"#Fetching categorical or string columns from the dataset\ncategorical_cols = raw_df.select_dtypes('object').columns.to_list()\nprint(categorical_cols)","343d8594":"train_df = raw_df[raw_df[\"year\"] < 2015]\nvalidate_df = raw_df[raw_df[\"year\"] == 2015]\ntest_df = raw_df[raw_df[\"year\"] > 2015]","bf679f8e":"print(\"Shape of train dataset\", train_df.shape)\nprint(\"Shape of validation dataset\", validate_df.shape)\nprint(\"Shape of test dataset\", test_df.shape)","9490f0af":"# Let us seperate input and target columns \n#Seperating target columns and dropping it from train, validate and test dataframes\ntrain_target = train_df[\"RainTomorrow\"]\ntrain_df.drop(columns = [\"RainTomorrow\"], inplace = True)\n\nvalidate_target = validate_df[\"RainTomorrow\"]\nvalidate_df.drop(columns = [\"RainTomorrow\"], inplace = True)\n\n\ntest_target = test_df[\"RainTomorrow\"]\ntest_df.drop(columns = [\"RainTomorrow\"], inplace = True)","6f12d9f3":"#Separating input columns\ntrain_inputs = train_df.copy()\nvalidate_inputs = validate_df.copy()\ntest_inputs = test_df.copy()","0cc0c79d":"print(\"Shape of Train input\", train_inputs.shape)\nprint(\"Shape of Validate input\", validate_inputs.shape)\nprint(\"Shape of Test input\", test_inputs.shape)","4c1985c1":"print(\"Shape of Train targer\", train_target.shape)\nprint(\"Shape of Validate target\", validate_target.shape)\nprint(\"Shape of Test target\", test_target.shape)","61918f15":"#Fetch numerical columns\nnumeric_cols = train_df.select_dtypes(include = np.number).columns.to_list()\nprint(\"Numerical columns\", numeric_cols)\n\n#Fetching categorical or string columns from the dataset\ncategorical_cols = train_df.select_dtypes('object').columns.to_list()\nprint(\"Categorical columns\", categorical_cols)","8ae4df29":"from sklearn.preprocessing import OneHotEncoder \nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(raw_df[categorical_cols])","0d3d7cac":"#Fetch new columns which will be created as a part of OneHotEncoding operation\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\nprint(encoded_cols)","edf78a3b":"#Transform categorical cols for train, test and validation dataset\ntrain_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\nvalidate_inputs[encoded_cols] = encoder.transform(validate_inputs[categorical_cols])\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])","935908ee":"print(\"Shape of training dataset\", train_inputs.shape)\nprint(\"Shape of validation dataset\", validate_inputs.shape)\nprint(\"Shape of test dataset\", test_inputs.shape)","a4e58cf6":"X_train = train_inputs[numeric_cols + encoded_cols]\nX_validate = validate_inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols]","f95c5cf3":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 42)","caf37d04":"%%time\ndt.fit(X_train, train_target)","d547c58c":"from sklearn.metrics import accuracy_score, confusion_matrix","75fe87b3":"train_preds = dt.predict(X_train)\nprint(pd.value_counts(train_preds))","7d5b0602":"accuracy_score(train_target, train_preds)","8ada3082":"dt.score(X_validate, validate_target)","e3597966":"dt.score(X_test, test_target)","e5993f0b":"from sklearn.tree import plot_tree, export_text\nplt.figure(figsize=(80,20))\nplot_tree(dt, feature_names=X_train.columns, max_depth=2, filled=True);","5ac7acd9":"#Max depth of decision tree\nprint(\"Depth of the decision tree\", dt.tree_.max_depth)","6cc0ee53":"print(dt.feature_importances_)","4fcb9cf6":"\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': dt.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(importance_df.head())","c931b7ee":"plt.title('Feature Importance')\nsns.barplot(data = importance_df.head(10), x='importance', y='feature');","e3efeffa":"dt = DecisionTreeClassifier(max_depth = 4, random_state = 42)\ndt.fit(X_train, train_target)","842034a3":"#Scoring against training dataset\ndt.score(X_train, train_target)","aabbb76e":"#Scoring against validation dataset\ndt.score(X_validate, validate_target)","2acffb62":"#scoring against test dataset\ndt.score(X_test, test_target)","72228615":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","8c759540":"#Confusion matrix for training data\ntrain_pred = dt.predict(X_train)\nmatrix_train = confusion_matrix(train_target, train_pred)\nprint(matrix_train)\n\nmatrix_train = classification_report(train_target, train_pred)\nprint(matrix_train)","1a9c775c":"#Confusion matrix for validation data\nvalidation_pred = dt.predict(X_validate)\nmatrix_validate = confusion_matrix(validate_target, validation_pred)\nprint(matrix_validate)\n\nmatrix_validate = classification_report(validate_target, validation_pred)\nprint(matrix_validate)","8e029e67":"test_pred = dt.predict(X_test)\nmatrix_test = confusion_matrix(test_target, test_pred)\nprint(matrix_test)\n\nmatrix_test = classification_report(test_target, test_pred)\nprint(matrix_test)","e7c49102":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_jobs = 1, random_state = 42)","ca162fe4":"%%time\nrfc.fit(X_train, train_target)","7019ad2f":"print(\"Training accuracy = \", rfc.score(X_train, train_target) * 100, \"%\")","451657a5":"print(\"Validation accuracy = \", rfc.score(X_validate, validate_target) * 100, \"%\")","2f06c9c7":"print(\"Test accuracy = \", rfc.score(X_test, test_target) * 100, \"%\")","d46e0279":"Let us seperate input and target column ","24fbadaf":"Let us score the model on training, validation and test dataset again","2b526b45":"# Import required libraries","4f7e1009":"## Overview\nPredicting weather for rain, sunshine, cold, etc. has been the most usual application of forcasting and prediction in Machine Learning. We will use Australia's weather and rain dataset with data of almost 10 years to predict if it will rain tomorrow. \n\n## Objective \nTo apply Data science, data analysis and machine learning on a sample problem for classification (Yes\/No)\n\n## Dataset \nData is openly available on Kaggle on this link - https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package","2d3fc746":"Let us now get the score of model for train, test and validation dataset","9fb062b4":"Get numerical and categorical columns","826cd1eb":"# Hyperparamter tuning \nOur model had 100 % training accuracy which means that model is memorising the inputs. Comparing it with validation and test accuracy of approx. 77 % we clearly see a case of overfitting. We need to try and make some changes in the paramters of model training to avoid overfitting. One possible way of doing it is to reduce the max depth of the tree. Let us train the model again","d391ace7":"We can also get the feature importance which will give us a clue on which feature is the most important feature impacting the decision","cc7521d4":"## Steps\nWe will follow steps given below - \n* Install and import required libraries \n* Download dataset and import it in pandas \n* Perform basic descriptive analytics on the data \n* Visualize data for patterns and identify critical features impacting the decision\n* Prepare data for training - data cleaning, data normalization, data encoding, train\/test split, etc. \n* Train data using Decision tree Classifier\n* Evaluate and improve model performance \n* Packaging model ","62a20a17":"# Exploratory data analysis","65acb67e":"# Downloading\/Import data ","ba04d06c":"From above output we can see that there are many missing values in the dataset. We need to handle missing data in various steps to ensure that our ML model training does not fail. \nOn priority, let us drop all null values from RainTomorrow column. We need to do this as RainTomorrow is the target column and must have a value for model to get trained.","59c70687":"## Predict and evaluate model performance on training data itself","667291bf":"As we can see above accuracy score is 1.0 which is possibly correct as our model was trained on the same data which was used for prediction. Let us try to get the score of the model for validation dataet ","479466f1":"We now have a significantly better performance on training and test dataset Let us get the confusion matrix ","6a67717b":"We need to add these 3 columns in the numeric_cols list","6f563eff":"# Train Random Forest algorithm\nRamdom Forest is an ensemble technique where\n* multiple DecisionTrees will be trained with different hyperparatmers\n* outcome of each DecisionTree will be voted \/ averaged \n* the one with most count in terms of Classifier will be the winner prediction","470361f1":"## Dealing with missing values\nMissing values are to be fixed before being fed to a model. Also, they must be dealt properly to ensure that we dont tamper the data with nonsense entries. \nLet us get the count of missing values in the dataset for each column","d853e039":"As we can see the training accuracy is just 83% which means the model is not memorising and overfitting the values. Let us try the same for validation and test dataset","ad541f49":"# Drawing the decision tree ","c2a639cc":"# Feature Engineering \nday and month might have a significant impact on the prediction. For example in India, it is highly likely to rain in monsoon than in winters. Let break date into day, month and year and then drop date column from the dataset ","e615cbd2":"### Encoding categorical columns \nWe have categorical columns which must be convereted to integer as ML model cannot deal with string values. We will use OneHotEncoding as we have more than 2 classes or categories in our columns ","dcb8ce9e":"Let us get the max depth of the tree","89482fab":"Let us check the accuracy score -","bae4cfa2":"### Dealing with missing values in numercial columns\nOne can use median, mode and mean to replace missing numerical values. For simplicity, we will use mean to replace missing values","c84c524e":"# Training model using DecisionTreeClassifier","ff2ea89d":"### Dealing with missing values in Categorical columns","ee73a7fa":"# Model evaluation and metrics for train, test and validation ","e1e311b3":"# Title\nTo predict if it will rain tomorrow in Australia using Decision Tree Classifier ","2d7aacc3":"Let us try heatmap of correlation between all features in the dataset","970e2532":"### Splitting train, test and validation data\nWe have chronological data for years say from 2008 to 2017. We are expecting our model to predict values in the future and it is a good idea to seperate training data, validation data and test data chronologically. We will have data till 2015 as training data, validation data as data of 2015 and test data for year > 2015 ","1e7d6adb":"Let us print the shape of train inputs and train targets ","7db3af81":"As we can see there are many missing values. For example Sunshine has 67,816 entries missing which is almost half of the total records we have. imiliarly Evaporation, Cloud3pm and Cloud9am have over 50,000 missing values. \nWe cannnot remove this many number of records as we will loose almost half of our dataset and this will impact our model training as we will not have enough data to train on. We might have to deal seperately with different category of columns and so let us seperate numeric and categorical\/string columns ","d1d964d8":"Let us now check if we have missing values","8ac335b2":"Let us replace all null\/missing values with \"Unknown\" which will become another category ","0c82c42d":"From above heatmap of correlation, we can see that there are a few features which are impacting other and can be termed as positively correlated","3c35d032":"# Preparing data for training","4fc07120":"So we have new columns of year, month and day in the dataset. We can now drop Date column from the dataset"}}