{"cell_type":{"06629c13":"code","b3fa084a":"code","12859166":"code","62e54684":"code","a1bff736":"code","39eda60a":"code","5d331e38":"code","6a537352":"code","5cda0c75":"code","1c679a02":"code","33374755":"code","1aa08df4":"code","9c071e89":"code","b564545b":"code","5b04fe33":"code","d6025bbb":"code","adab2027":"code","ddc23552":"code","46fa528a":"code","e296ba58":"code","7a2bfd64":"code","129b926a":"code","a897c02d":"code","395f1607":"code","3180ae9a":"code","f635c76a":"code","6b19aaa4":"code","c602adc4":"code","d9f9ca1b":"code","dea16a14":"code","11edbe93":"markdown","09fb0c7c":"markdown","31906005":"markdown","e691863c":"markdown","b1c45994":"markdown","10dcffd9":"markdown"},"source":{"06629c13":"import pandas as pd\n\nmain_file_path = '..\/input\/test-data\/train.csv' # this is the path to the Iowa data that you will use\ndata = pd.read_csv(main_file_path)\n\n# Run this code block with the control-enter keys on your keyboard. Or click the blue botton on the left\nprint('Some output from running this cell')","b3fa084a":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndata.columns\n\n","12859166":"from sklearn.model_selection import train_test_split\ny = data.SalePrice\ninteresting = [\"LotArea\",'YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\nx =data[interesting]\n#applying the split funtion to the dataset\ntrain_x,test_x, train_y,test_y = train_test_split(x,y,random_state =0)\n#defining the model\nmodel = DecisionTreeRegressor()\n#fitting the model\nmodel.fit(train_x,train_y)\n#predicting the prices on test data\nmodel.predict(test_x)\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n","62e54684":"def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)\n\nfor max_leaf_nodes in [5,40,45, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_x, test_x, train_y, test_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","a1bff736":"from sklearn.ensemble import RandomForestRegressor\n\nforest_model =RandomForestRegressor()\nforest_model.fit(train_x, train_y)\nforest_model.predict(test_x)\nprint(mean_absolute_error(test_y,forest_model.predict(test_x)))","39eda60a":"#applying the max_leaf_nodes function to random forest to bring down the mean absolute error\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)\n\nfor max_leaf_nodes in [5,45, 50,55, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_x, test_x, train_y, test_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","5d331e38":"#submitting my first dataset on Kaggle!!!\n\ntest = pd.read_csv('..\/input\/test-data\/test.csv')\ntest_x = test[interesting]\npredicted_prices = forest_model.predict(test_x)\nprint(predicted_prices)\n\nmy_submission = pd.DataFrame({'Id': test.Id,'SalePrice':predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","6a537352":"#getting columns with missing data\nprint(data.isnull().sum())","5cda0c75":"target = data.SalePrice\npredictors = data.drop(['SalePrice'], axis=1)\n\n# For the sake of keeping the example simple, we'll use only numeric predictors. \nnumeric_predictors = predictors.select_dtypes(exclude=['object'])","1c679a02":"numeric_predictors.head()","33374755":"X_train, X_test, y_train, y_test = train_test_split(numeric_predictors, \n                                                    target,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=0)","1aa08df4":"def score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","9c071e89":"#checking if predictions get better when we drop columns\ncols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","b564545b":"from sklearn.preprocessing import Imputer\n\nimputer= Imputer()\nimputed_X_train = imputer.fit_transform(X_train)\nimputed_X_test = imputer.transform(X_test)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))","5b04fe33":"imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns if X_train[col].isnull().any())\n\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n    \n#imputation\nimputed_X_train_plus = imputer.fit_transform(imputed_X_train_plus )\nimputed_X_test_plus = imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","d6025bbb":"one_hot_encoded_training_predictors = pd.get_dummies(X_train)","adab2027":"from sklearn.model_selection import cross_val_score\n\ndef get_mae(X_train,y_train):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50),x,y, scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = X_train.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n","ddc23552":"print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","46fa528a":"one_hot_encoded_training_predictors = pd.get_dummies(X_train)\none_hot_encoded_test_predictors = pd.get_dummies(X_test)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\nfinal_train.drop(['GarageYrBlt','LotFrontage'],axis=1,inplace=True)\n\nimputer.fit_transform(final_train)\nfinal_train.drop('MasVnrArea',axis=1,inplace= True)\n\nmodel.fit(final_train,y_train)\nfinal_test.drop(['GarageYrBlt','MasVnrArea','LotFrontage'],axis=1,inplace=True)\nmodel.predict(final_test)\n","e296ba58":"print(get_mae(final_train,y_train))","7a2bfd64":"my_submission1 = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission1.to_csv('submission1.csv', index=False)\n\ndata=pd.read_csv('..\/input\/test-data\/train.csv')\ntestdata = pd.read_csv('..\/input\/test-data\/test.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop('SalePrice',axis = 1).select_dtypes(exclude = ['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n\ntestdata = testdata.select_dtypes(exclude = ['object'])\ntestdata = my_imputer.fit_transform(testdata)","129b926a":"\ny = data.SalePrice\nX = data.drop('SalePrice',axis = 1).select_dtypes(exclude = ['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n\n","a897c02d":"from xgboost import XGBRegressor\n\nxg = XGBRegressor()\n","395f1607":"xg.fit(train_X, train_y, verbose=False)","3180ae9a":"preds = xg.predict(test_X)\n","f635c76a":"print(\"Mean absolute error : \"+ str(mean_absolute_error(preds,test_y)))","6b19aaa4":"xg1 =XGBRegressor(n_estimators=1000)\nxg1.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X,test_y)],verbose=False)","c602adc4":"pred = xg1.predict(test_X)\nprint(\"Mean absolute error : \"+ str(mean_absolute_error(pred,test_y)))","d9f9ca1b":"xg = XGBRegressor(n_estimators=5, learning_rate=0.05)\nxg.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X,test_y)],verbose=False)","dea16a14":"preds = xg.predict(test_X)\nprint(\"Mean absolute error : \"+ str(mean_absolute_error(preds,test_y)))","11edbe93":"MAE goes quite down. Lastly, let's check the same for name the columns that were imputed","09fb0c7c":"Improving the predictions by applying XGBoost model","31906005":"Yes it does gets better!!\n Now let's check the same after imputing","e691863c":"# Introduction\n**This will be your workspace for the [Machine Learning course](https:\/\/www.kaggle.com\/learn\/machine-learning).**\n\nYou will need to translate the concepts to work with the data in this notebook, the Iowa data. Each page in the Machine Learning course includes instructions for what code to write at that step in the course.\n\n# Write Your Code Below","b1c45994":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","10dcffd9":"Now, let's perform one hot encoding "}}