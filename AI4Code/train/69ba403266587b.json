{"cell_type":{"5dfa7e92":"code","8e18d094":"code","c5a9bc02":"code","18601a86":"code","a2e4f5ac":"code","700074fc":"code","e5a0b8a7":"code","9252e63c":"code","789e0171":"code","3010ee30":"code","ca679518":"code","52b02207":"code","bfb78954":"code","7306bdb0":"code","9a6e7773":"code","c93f368e":"code","ff401c72":"code","23c9a18c":"code","6c47f1ae":"code","e2ee508d":"code","7a4e34e8":"code","1ca3f11e":"code","38bdd128":"code","50c6ab65":"code","ea5099a3":"code","998ece5f":"code","9f097fb9":"code","305a3c94":"code","aa04baa3":"markdown","4af25ebf":"markdown","a1f93dd3":"markdown","52f72eb6":"markdown","f1a6a1ab":"markdown","cf25a4ea":"markdown","835e4be9":"markdown","bccc9f9a":"markdown","8f8b7c4b":"markdown","aaa6d6ff":"markdown","d8140bdb":"markdown","0e50f10f":"markdown","7c4f7807":"markdown","64aa4e2d":"markdown","4b4a0fa7":"markdown"},"source":{"5dfa7e92":"import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import make_regression \nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","8e18d094":"X, y = make_regression(n_samples=10000, n_features=1,\n                                      n_informative=1, shuffle=True,\n                                      random_state=42)","c5a9bc02":"plt.plot(X, y, 'o')","18601a86":"np.random.seed(42)\nX[:1000] = 3 + 0.5 * np.random.normal(size=(1000, 1))\ny[:1000] = -3 + 10 * np.random.normal(size=1000)","a2e4f5ac":"plt.plot(X, y, 'o')","700074fc":"reg = LinearRegression(fit_intercept = False)","e5a0b8a7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","9252e63c":"fit_model = reg.fit(X_train, y_train)","789e0171":"fit_model.coef_","3010ee30":"fit_model.intercept_","ca679518":"predictions = fit_model.predict(X_test)","52b02207":"def error_calculator(prediction_values, true_values):\n    value_matrix = pd.DataFrame([true_values, prediction_values, true_values-prediction_values])\n    value_matrix = value_matrix.T\n    value_matrix = value_matrix.rename(columns = {0 : 'Real Values', 1 : 'Predictions', 2 : 'Errors'})\n    return value_matrix","bfb78954":"evaluation_matrix = error_calculator(predictions, y_test)","7306bdb0":"evaluation_matrix","9a6e7773":"print('Overall error : {0}'.format(str(evaluation_matrix['Errors'].sum())))","c93f368e":"evaluation_matrix['SquareofErrors'] = evaluation_matrix['Errors'] ** 2\nevaluation_matrix['RootofSquares'] = evaluation_matrix['SquareofErrors'] ** (0.5)\nevaluation_matrix","ff401c72":"print('Overall error : {0}'.format(str(evaluation_matrix['RootofSquares'].sum())))","23c9a18c":"print('Mean : {0}'.format(str(evaluation_matrix['Real Values'].mean())))","6c47f1ae":"evaluation_matrix['TotalVariance'] = (evaluation_matrix['Real Values'] - evaluation_matrix['Real Values'].mean())**2","e2ee508d":"evaluation_matrix","7a4e34e8":"UnexplainedVariance = evaluation_matrix['SquareofErrors'].sum()\nTotalVariance = evaluation_matrix['TotalVariance'].sum()\nprint(\"Unexplained Variance = {0}\".format(UnexplainedVariance))\nprint(\"Total Variance = {0}\".format(TotalVariance))\nprint(\"R Square Score = {0}\".format(1 - (UnexplainedVariance \/ TotalVariance)))","1ca3f11e":"reg.set_params(fit_intercept = True)","38bdd128":"fit_model = reg.fit(X_train, y_train)","50c6ab65":"predictions = fit_model.predict(X_test)","ea5099a3":"fit_model.coef_","998ece5f":"fit_model.intercept_","9f097fb9":"fit_model.score(X_test, y_test)","305a3c94":"plt.plot(X_test, y_test, 'o')\nplt.plot(X_test, predictions);","aa04baa3":"* Lineer regresyon modelimizi e\u011fitmeye haz\u0131r\u0131z. E\u011fitim verilerimizi en iyi temsil eden do\u011fruyu olu\u015fturmak i\u00e7in regresyon modelimizin \"fit\" y\u00f6ntemini \u00e7a\u011f\u0131ral\u0131m.\n* We are ready to train our linear regression model. Let's call the \"fit\" method of our regression model to create a line which is representing our train data as the best one. ","4af25ebf":"* Ger\u00e7ek de\u011ferlerin ortalamas\u0131 -0,67'ye e\u015fittir. Her bir ger\u00e7ek de\u011ferden ger\u00e7ek de\u011ferlerin ortalamas\u0131n\u0131 \u00e7\u0131kar\u0131rsak, sonu\u00e7lar\u0131n karesini al\u0131r ve toplarsak, verilerimizin toplam varyans\u0131n\u0131 buluruz. Ayr\u0131ca hatalar\u0131n kareleri de var ve bu de\u011ferleri toplarsak verilerimizin a\u00e7\u0131klanamayan (unexplained) varyans\u0131n\u0131 bulabiliriz. A\u00e7\u0131klanamayan varyans ile toplam varyans aras\u0131ndaki oran\u0131 1'den \u00e7\u0131kar\u0131rsak, regresyon modelimiz i\u00e7in \"R kare de\u011feri\" hesaplayabiliriz. \u00d6nce bu de\u011feri hesaplamaya \u00e7al\u0131\u015fal\u0131m sonra R karenin de\u011ferine g\u00f6re de\u011ferlendirmelerimizi yap\u0131p ger\u00e7ekte ne oldu\u011funu ve bu de\u011ferin bizim i\u00e7in ne anlama geldi\u011fini anlamaya \u00e7al\u0131\u015fal\u0131m.\n* The mean of real values is equal to -0,67. If we subtract the average real values from each of the real values, square the results, and sum them we find the total variance for our data. We also have the squares of errors and if we sum these values we can find the unexplained variance of our data. If we subtract the ratio between unexplained variance and total variance from 1, we can calculate \"the R square value\" for our regression model. Firstly let's try to calculate this value after then we will make our evaluations according to the value of R square and try to understand what really it is and what is the meaning of this value for us.","a1f93dd3":"* Tahminleri ve ger\u00e7ek de\u011ferleri girdi olarak alan bir metot olu\u015fturduk. Bu metot, ger\u00e7ek de\u011ferleri, tahmin de\u011ferlerini ve ger\u00e7ek de\u011fer - tahmin de\u011ferine e\u015fit olan hatalar\u0131 saklayan bir data frame olu\u015fturuyor. \"Errors\" s\u00fctunundaki t\u00fcm de\u011ferleri toplad\u0131\u011f\u0131m\u0131zda -18059 ile kar\u015f\u0131la\u015f\u0131yoruz. Ama bu ger\u00e7ekten do\u011fru mu? 5. (index 4) sat\u0131r ve 3300. (index 3299) sat\u0131rdaki hata de\u011ferlerini kontrol edelim. Bu de\u011ferler yakla\u015f\u0131k -12,7 ve 11,9'dur. Bu iki de\u011feri toplarsak, bu iki veri noktas\u0131n\u0131n toplam hatas\u0131n\u0131 -0,8 olarak buluruz ancak hakl\u0131s\u0131n\u0131z bu hata de\u011ferlerini do\u011frudan toplayamay\u0131z. Bu toplam, pozitif ve negatif de\u011ferler nedeniyle hatalar\u0131n birbirini yok etmesine neden olabilir. Hata de\u011ferlerinin karesini alarak negatiflik etkisini ortadan kald\u0131rmaya \u00e7al\u0131\u015fal\u0131m.\n* We have created a method that takes predictions and real values as input. This method is creating a data frame that stores real values, prediction values, and errors which is equal to real value - prediction value. Once we sum all of the values in the \"Errors\" column we come up with -18059. But is it really true? Let's check the error values at the 5th (index 4) row and the 3300th (index 3299) row. These values are approximately -12,7 and 11,9. If we sum these two values we find the overall error of these two data points as -0,8 but you are right we can not directly sum these error values. This summation can cause errors to destroy each other because of positive and negative values. Let's try to eliminate the effect of negativity by taking the square of error values.  ","52f72eb6":"* G\u00f6rd\u00fc\u011f\u00fcn\u00fcz gibi, verilerimiz lineer regresyon i\u00e7in zaten m\u00fckemmel durumda. Lineer regresyon i\u00e7in i\u015fleri zorla\u015ft\u0131rmak i\u00e7in verilerimize biraz g\u00fcr\u00fclt\u00fc ekleyelim. X (input) ve y (output) verilerimizin ilk 1000 sat\u0131r\u0131 i\u00e7in numpy k\u00fct\u00fcphanesini kullanarak yeni rastgele de\u011ferler olu\u015ftural\u0131m ve verilerimizi yeniden \u00e7izdirelim.\n* As you can see our data is already in perfect shape for linear regression. To make things hard for linear regression let's add a little bit of noise to our data. For the first 1000 rows of our X (input) and y (output) data let's create new random values by using numpy library and re-plot our data.","f1a6a1ab":"* Bu notebook'ta sizlere lineer regresyonu t\u00fcm detaylar\u0131yla a\u00e7\u0131klamaya \u00e7al\u0131\u015ft\u0131m. Umar\u0131m her \u015fey sizin i\u00e7in bilgilendiricidir. Bu al\u0131\u015ft\u0131rmalar\u0131 en az bir kez daha denemenizi veya modelimize yeni \u015feyler eklemeyi deneyerek R kare skorundaki de\u011fi\u015fimi g\u00f6rmenizi \u00f6neririm. \u00d6n\u00fcm\u00fczdeki hafta ML101 serisinin ikinci algoritmas\u0131 olan Logistic Regression ile ilgili yeni bir notebook olu\u015fturup payla\u015faca\u011f\u0131m. \u00d6n\u00fcm\u00fczdeki Cumartesi tekrar g\u00f6r\u00fc\u015fene kadar sa\u011fl\u0131kl\u0131 bir hafta ge\u00e7irmenizi dilerim.\n* In this notebook, I have tried to explain linear regression to you with all of the details. I hope that everything is informative for you. I suggest you try these exercises at least one more time or you can try to add new things to our model to see the change at the R squared score. Next week I will create and share a new notebook about the second algorithm of the ML101 series that is Logistic Regression. I wish you a healthy week until we see each other the next Saturday. ","cf25a4ea":"* \u015eimdi verilerimizi e\u011fitim ve test k\u0131s\u0131mlar\u0131 olmak \u00fczere 2 par\u00e7aya ay\u0131ral\u0131m. Bu verileri b\u00f6lmek i\u00e7in sklearn'in train_test_split y\u00f6ntemini kullanaca\u011f\u0131z. Verilerimizin 0.67'sini e\u011fitim verisi ve geri kalan\u0131n\u0131 test verisi olarak b\u00f6lelim.\n* Now let's split our data into 2 parts which are train and test parts. To split this data, we will use sklearn's train_test_split method. Let's split 0.67 of our data as train data and the rest as test data. ","835e4be9":"* \u015eimdiye kadar do\u011frusal bir regresyon modeline uydurabilmek i\u00e7in veriler olu\u015fturduk. E\u011fitim verilerini kullanarak bir lineer regresyon modeli e\u011fittik. \u015eimdi Python'da bir yerde bir do\u011frumuz var. Bu do\u011frunun ne oldu\u011funu merak etmiyor musunuz? Lineer regresyon do\u011frusunun katsay\u0131lar\u0131n\u0131 ve sabit de\u011ferini g\u00f6rmek i\u00e7in lineer regresyonun \"coef_\" ve \"intercept_\" \u00f6zniteliklerini \u00e7a\u011f\u0131ral\u0131m.\n* So far we have created data to fit a linear regression model. We have trained a linear regression model by using train data. Now we have a line somewhere in Python. Are not you wondering what is this line? Let's call \"coef_\"  and \"intercept_\" attributes of linear regression to see the coefficients and intercept value of the linear regression line. ","bccc9f9a":"* \u015eimdi her \u015fey biraz daha karma\u015f\u0131k g\u00f6r\u00fcn\u00fcyor. Tahmin yapmak hala \u00e7ok daha kolay g\u00f6r\u00fcn\u00fcyor ama art\u0131k lineer regresyon algoritmas\u0131 uygulayacak bir veri setimiz var. Bir intercept(do\u011fru denklemindeki sabit de\u011fer) de\u011feri olmayan bir lineer regresyon do\u011frusu olu\u015fturmak i\u00e7in fit_intercept parametre de\u011ferini \"False\" olarak ayarlayarak Do\u011frusal Regresyon y\u00f6ntemimizden bir \u00f6rnek olu\u015fturarak ba\u015flayal\u0131m.\n* Now everything looks a bit more complex. It still looks too much easier to make predictions but in the end, we now have a dataset to apply linear regression algorithm. Let's start by creating an instance from our Linear Regression method by setting fit_intercept parameter value to \"False\" to create a linear regression line without an intercept value(constant value in the line equation).","8f8b7c4b":"* R kare de\u011ferini 0,43 olarak buluyoruz. R kare skoru, bize verilerin regresyon do\u011frusuna ne kadar yak\u0131n oldu\u011funu s\u00f6yler. Ba\u015fka bir deyi\u015fle, verilerimize bir regresyon do\u011frusuna \u00e7izme konusunda ne kadar ba\u015far\u0131l\u0131 oldu\u011fumuzu g\u00f6sterir. Do\u011frusal bir model taraf\u0131ndan a\u00e7\u0131klanan hedef de\u011fi\u015fken varyasyonunun y\u00fczdesidir. R kare skoru (belirleme katsay\u0131s\u0131 olarak da bilinir) hesaplamam\u0131z\u0131n pay\u0131nda, hatalar\u0131n toplam\u0131n\u0131n karesine bulunmakta. Modelimiz verilerimize tam olarak uyuyorsa, yani 0 hata varsa R kare skoru tam olarak 1 olacakt\u0131r. R kare skoru 0 ise bu, pay ve paydan\u0131n birbirine e\u015fit oldu\u011fu anlam\u0131na gelir. Bu durumda model, y de\u011ferlerinin ortalamas\u0131na e\u015fit bir sabit de\u011fer tahminler. Bu t\u00fcr bir modele \"sabit model\" ad\u0131 verilir. \u015eimdi y = 15.8285946 * x olan tek bir regresyon modelimiz var. Intercept de\u011feri olan modeli denemedik bile. Dolay\u0131s\u0131yla bu modelin bu verileri en iyi temsil etti\u011fini s\u00f6yleyemeyiz. \u015eimdi regresyon modelimize intercept de\u011ferini ekleyelim ve R kare de\u011ferini tekrar hesaplayal\u0131m.\n* We come up with a 0.43 R squared value. R squared score simply tells us how close the data are to the fitted regression line. In other words, shows us how successful we are to fit a regression line to our data. It is the percentage of the target variable variation that is explained by a linear model. At the numerator of our R squared score (also known as the coefficient of determination) calculation, we have the square of the summation of errors. If our model fits our data perfectly which means 0 error score R squared score will be exactly 1. If the R squared score is 0 that means the numerator and the denominator are equal to each other. In that case, the model is predicting a constant value that is equal to the mean of y values. That kind of model is named as\"constant model\". Now we have only one regression model which is y = 15.8285946 * x. We did not even try the model with an intercept value. Hence we can not say that this model is the best one to represent this data. Now let's add the intercept value to our regression model and calculate the r squared value again.","aaa6d6ff":"* \u015eimdi verilen X_test verilerine g\u00f6re tahminlerimiz var. Herhangi bir model, verilen girdiyi kullanarak herhangi bir de\u011feri tahmin edebilir. Fakat tahminlerin do\u011fru oldu\u011funu nas\u0131l bilebiliriz? Bir s\u0131n\u0131fland\u0131rma algoritmas\u0131 i\u00e7in, tahmin edilen s\u0131n\u0131f ger\u00e7ek s\u0131n\u0131fla ayn\u0131ysa, bu tahminin do\u011fru oldu\u011fu anlam\u0131na gelir. Ancak bir regresyon algoritmas\u0131 i\u00e7in bu tam olarak do\u011fru de\u011fildir. Bir yak\u0131t t\u00fcketimi probleminiz oldu\u011funu d\u00fc\u015f\u00fcn\u00fcn. Bir araban\u0131n 100 mil boyunca ne kadar yak\u0131t t\u00fcketti\u011fini tahmin etmek istiyorsunuz. Ger\u00e7ek de\u011fer 100 mil ba\u015f\u0131na 18,3 galon iken, regresyon modeliniz bu de\u011feri 100 mil ba\u015f\u0131na 22,4 galon olarak tahmin ediyor. Modelin tamamen yanl\u0131\u015f oldu\u011funu s\u00f6yleyemeyiz. Hala yanl\u0131\u015f ama 100 mil ba\u015f\u0131na 3,9 galona e\u015fit bir \"hata\" ile de do\u011fru. Bu hesaplamay\u0131 verilerimizdeki t\u00fcm veri noktalar\u0131 i\u00e7in yaparsak ve bu de\u011ferleri toplarsak, regresyon modelimizin genel hatas\u0131n\u0131 bulabiliriz. Bulabilir miyiz? Bu de\u011feri hesaplamak i\u00e7in kendi metodumuzu olu\u015ftural\u0131m.\n* Now we have our predictions according to the given X_test data. Any model can predict any value by using the given input. But how can we know that the predictions are true? For a classification algorithm, we can easily say that if the predicted class is the same as the real class that means the prediction is true. But for a regression algorithm, it is not exactly true. Imagine that you have a fuel consumption problem. You want to predict how much fuel a car consumes for 100 miles. While the real value is 18.3 gallons per 100 miles, your regression model is predicting this value as 22.4 gallons per 100 miles. We can not say the model is totally wrong. It is still wrong but it is also right with an \"error\" which is equal to 3.9 gallons per 100 miles. If we make this calculation for all of the data points in our data and we sum these values, we can find the overall error of our regression model. Can we find it? Let's create our own method to calculate this value.","d8140bdb":"* Her hafta yeni bir algoritma \u00f6\u011frenece\u011fimiz maceram\u0131z\u0131n ilk ad\u0131m\u0131ndan yani Lineer Regresyon algoritmas\u0131ndan hepinize merhabalar. Bu notebook'ta, Lineer Regresyon algoritmam\u0131z\u0131 kendi olu\u015fturmu\u015f oldu\u011fumuz bir veri setini kullanarak uygulayacak, \u00e7\u0131kt\u0131lar\u0131n\u0131 de\u011ferlendirip parametreleri de\u011fi\u015ftirip tekrar deneyip g\u00fcn\u00fcn sonunda en ba\u015far\u0131l\u0131 Lineer Regresyon do\u011frusunu bulmaya \u00e7al\u0131\u015faca\u011f\u0131z. Hatta do\u011frumuzun bir de grafi\u011fini \u00e7izdirip nas\u0131l bir tahmin yapt\u0131\u011f\u0131n\u0131 da daha rahat g\u00f6rebilece\u011fiz. K\u00fct\u00fcphaneleri i\u00e7e aktararak ba\u015flayal\u0131m.\n* Hello to all of you from the first step of our adventure, where we will learn a new algorithm every week, namely the Linear Regression algorithm. In this notebook, we will apply our Linear Regression algorithm using a data set we have created, evaluate the outputs, change the parameters and try again, and try to find the most successful Linear Regression line at the end of the day. In fact, we will be able to draw a graph of our line and see what kind of prediction it makes more easily. Let's start by importing the libraries.","0e50f10f":"* \u00d6ncelikle t\u00fcm hata de\u011ferlerinin karesini hesaplad\u0131k. Daha sonra bu hesaplaman\u0131n karek\u00f6k\u00fcn\u00fc al\u0131yoruz. Bunu yaparak i\u015faretin etkisini ortadan kald\u0131rd\u0131k. Daha sonra \"RootofSquares\" i\u00e7indeki t\u00fcm de\u011ferleri toplayarak yeni hata de\u011ferini kontrol ettik. Orijinal (\"Errors\" s\u00fctunu) hatalar\u0131n hepsini toplayarak buldu\u011fumuz ilk hata de\u011feri 18059'du ama \u015fimdi \"RootofSquares\" s\u00fctun de\u011ferlerini toplayarak buldu\u011fumuz hata de\u011feri 55420. \u0130lkinin neredeyse 3 kat\u0131 ama size mant\u0131kl\u0131 geliyor mu? 55420 gibi bir hata de\u011ferinin neye g\u00f6re anlam\u0131 nedir? Y\u00fcksek mi d\u00fc\u015f\u00fck m\u00fc yoksa normal mi? De\u011ferine g\u00f6re kesin bir \u015fey s\u00f6yleyemeyiz. Genel bir metrik bulmaya \u00e7al\u0131\u015fal\u0131m. B\u00f6yle bir metrik olu\u015fturmak i\u00e7in \u00f6nce ger\u00e7ek de\u011ferlerin ortalamas\u0131n\u0131 hesaplayal\u0131m.\n* Firstly we calculated the square of all of the error values. After then we get the square root of this calculation. By doing this we eliminated the effect of the sign. After then we have checked the new error value by summing all of the values in the \"RootofSquares\". The first error value which we found by summing all of the original (\"Errors\" column) errors was 18059 but now the error value which we found by summing \"RootofSquares\" column values is 55420. It is almost 3 times the first one but does it make sense to you? What is the meaning of an error value such as 55420 according to what? Is it high, low, or normal? We are not able to say exact things according to the value. Let's try to come up with a general metric. To create such a metric first, let's calculate the mean of real values.","7c4f7807":"* \u00d6ncelikle Lineer Regresyon uygulamak i\u00e7in bir veri seti olu\u015ftural\u0131m. Sevgili k\u00fct\u00fcphanemiz sklearn bize make_regression ad\u0131nda bir y\u00f6ntem sunuyor. Bu y\u00f6ntem, ko\u015fullar\u0131n\u0131z\u0131 kullanarak sizin i\u00e7in bir veri k\u00fcmesi olu\u015fturur. Bu notebook'ta make_regression y\u00f6ntemini kullanarak sadece bir input s\u00fctunu ve buna kar\u015f\u0131l\u0131k gelen bir output s\u00fctunu i\u00e7eren bir veri seti olu\u015fturaca\u011f\u0131z. Bu veri setinde 10.000 sat\u0131r\u0131m\u0131z olacak. Bir veri k\u00fcmesi olu\u015fturmak i\u00e7in y\u00f6ntemimizi \u00e7a\u011f\u0131ral\u0131m ve ard\u0131ndan veri noktalar\u0131m\u0131z\u0131 net bir \u015fekilde g\u00f6rmek i\u00e7in bu verileri g\u00f6rselle\u015ftirelim.\n* Firstly, let's create a dataset to apply Linear Regression. Our lovely library is sklearn provides us a method which name is make_regression. This method creates a dataset for you by using your conditions. In this notebook, we will create a dataset with only one input column and one corresponding output column by using make_regression method. We will have 10.000 rows in this dataset. Let's call our method to create a dataset and after then let's visualize this data to see our data points clearly.","64aa4e2d":"* Biz ne yapt\u0131k? \u0130lk olarak sklearn'in en sihirli y\u00f6ntemlerinden biri olan set_params'\u0131 kulland\u0131k. Bu y\u00f6ntem, model parametrelerini g\u00fcncellememize yard\u0131mc\u0131 olur. Fit_intercept parametresinin de\u011ferini \"False\"tan \"True\"ya g\u00fcncellemek i\u00e7in ilk modelimizde bu y\u00f6ntemi kulland\u0131k. Daha sonra modelimizi tekrar fit ettik ve ayn\u0131 veri setini kullanarak yeni de\u011ferler tahmin ettik. Modelimizin coef_ ve intercept_ \u00f6zniteliklerini \u00e7a\u011f\u0131rd\u0131\u011f\u0131m\u0131zda, intercept de\u011feri -5'e e\u015fit olan farkl\u0131 bir modelimiz oldu\u011funu g\u00f6rebiliriz. Ve son olarak, yeni R kare de\u011ferimiz var. Ama \u015fimdi g\u00f6rd\u00fc\u011f\u00fcn\u00fcz gibi katsay\u0131 matrisimizi kullanmad\u0131k veya yeni bir tane olu\u015fturmad\u0131k. Yepyeni bir y\u00f6ntem olan \"score\" y\u00f6ntemini kulland\u0131k. Bu y\u00f6ntem, girdi olarak X ve y de\u011ferlerini al\u0131r ve arka planda bu verileri bir modeli e\u011fitmek i\u00e7in kullan\u0131r. Y\u00f6ntem bir regresyon do\u011frusunu \u00e7izdi\u011finde, yeni de\u011ferleri tahmin etmek i\u00e7in de kullan\u0131r. Tahminler yap\u0131ld\u0131\u011f\u0131nda y\u00f6ntem, R kare skorunu hesaplamak i\u00e7in katsay\u0131 matrisimizle yapt\u0131\u011f\u0131m\u0131z\u0131n ayn\u0131s\u0131n\u0131 yapmak i\u00e7in bu tahminleri kullan\u0131r ve \u00e7\u0131kt\u0131 olarak R kare skorunu d\u00f6nd\u00fcr\u00fcr. Bu iki modelin R kare sonu\u00e7lar\u0131n\u0131 kar\u015f\u0131la\u015ft\u0131r\u0131rsak, intercept de\u011feri olan\u0131n intercept de\u011feri olmayandan daha iyi oldu\u011funu g\u00f6rebiliriz. Bu veriler i\u00e7in en iyi modelimiz ikinci lineer regresyon do\u011frusudur. Ancak bu do\u011fru, verilerimizi bir koordinat d\u00fczleminde nas\u0131l temsil ediyor? Hadi grafi\u011fini \u00e7izelim!\n* What did we do? Firstly we used one of the most magical methods of sklearn that is set_params. This method helps us to update model parameters. We have used this method with our initial model to update fit_intercept parameter's value from \"False\" to \"True\". After then we have fitted our model again and predicted new values by using the same dataset. Once we call the coef_ and intercept_ attributes of our model, we can see that now we have a different model now with an intercept value that is equal to -5. And finally, we have our new R square value. But as you can see now we did not use our coefficient matrix or we did not create a new one. We have used a brand new method that is \"score\". This method gets X and y values as input and in the background uses these data to fit a model. Once the method fits a regression line it is also used to predict new values. When the predictions are done method uses these predictions to do the same thing that we did with our coefficient matrix to calculate R squared score and returns the R squared score as an output. If we compare the R squared results of these two models, we can see that the one with an intercept value is better than the one without an intercept value. For this data, our best model linear regression line is the second one. But how does this line represents our data in a coordinate plane? Let's graph it!","4b4a0fa7":"* \"coef_\" \u00f6zniteli\u011finin \u00e7\u0131kt\u0131s\u0131 bize e\u011fitilmi\u015f bir lineer regresyon do\u011frusunun katsay\u0131lar\u0131n\u0131 g\u00f6sterir. Modelimizde sadece bir girdi s\u00fctunu vard\u0131, dolay\u0131s\u0131yla sadece bir katsay\u0131m\u0131z var. Hat\u0131rlayaca\u011f\u0131n\u0131z gibi fit_intercept parametre de\u011ferini \"False\" yaparak lineer regresyon modelimizin do\u011fru denklemimizde sabit bir de\u011fer olu\u015fturmas\u0131n\u0131 engelliyoruz. Bu nedenle model, sabit de\u011feri 0 olarak ayarlad\u0131 ve (y = 15.8285946 * x + 0) gibi bir do\u011fru form\u00fcl\u00fc olu\u015fturdu. Art\u0131k modelimiz herhangi bir X de\u011ferini tahmin etmeye haz\u0131r. Do\u011frusal regresyonun \"predict\" y\u00f6ntemini kullanarak X_test verilerinin sonu\u00e7lar\u0131n\u0131 tahmin etmeye \u00e7al\u0131\u015fal\u0131m.\n* The output of the \"coef_\" attribute shows us the coefficients of a trained linear regression line. In our model there was only one input column hence we only have one coefficient. As you remember, we prevent our linear regression model to create a constant value in our line equation by setting fit_intercept parameter value to \"False\". Hence model set the constant value to 0 and created a line formula such as (y = 15.8285946 * x + 0). Now our model is ready to predict any X value. Let's try to predict the results of X_test data by using \"predict\" method of linear regression."}}