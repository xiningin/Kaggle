{"cell_type":{"0a52df7c":"code","6fa4eaf9":"code","79c4fbaa":"code","76db1c87":"code","1dd41b46":"code","422d9c1d":"code","d654d098":"code","416dea4d":"code","426b15e8":"code","20014fcd":"code","b64e3903":"code","57d0e750":"code","3f0a1e77":"code","7f18979b":"code","d8816b52":"code","bdb1aba3":"code","f43b8613":"code","ceb77e0b":"code","1cc8e519":"code","ba51d68f":"code","ea2a97b6":"code","fac51dea":"code","a490a348":"code","ff6f226e":"code","987178ac":"code","2d23450c":"code","35c4b94f":"markdown","9adb1449":"markdown","1ad66523":"markdown","35cae5d2":"markdown","8c0137a5":"markdown","942cef26":"markdown","2c7313c1":"markdown","18cac971":"markdown","815bb462":"markdown","1d3e9089":"markdown","3430d0b3":"markdown","dfe4c65d":"markdown","11e19566":"markdown","a6844ffb":"markdown","88a12796":"markdown","15c46cd6":"markdown","11ef3cc1":"markdown","b8d2cf17":"markdown","fc21168c":"markdown","e402ad71":"markdown","7628190a":"markdown"},"source":{"0a52df7c":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","6fa4eaf9":"data = pd.read_csv('https:\/\/stats.idre.ucla.edu\/stat\/data\/poisson_sim.csv',index_col='id')","79c4fbaa":"display(data.shape)\ndisplay(data.head())","76db1c87":"data.prog = data.prog.astype('object')\ndisplay(data.dtypes)","1dd41b46":"plt.figure(figsize=(14,8))\nplt.title('Number of awards based on type of academic programme',fontsize=20,weight='bold')\nsns.countplot(data.num_awards,hue=data.prog,palette=\"rocket_r\",lw=1.5,edgecolor='#444444')\nplt.legend(loc='upper right',labels=['General','Academic','Vocational'])","422d9c1d":"prog = pd.get_dummies(data['prog'],prefix='prog')","d654d098":"prog","416dea4d":"data = pd.merge(data.drop('prog',axis=1),prog,on='id')","426b15e8":"data","20014fcd":"X = data[['prog_1','prog_2','prog_3','math']]\ny = data['num_awards']","b64e3903":"from sklearn.linear_model import PoissonRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=81,test_size = .33)\n\nPoisson_GLM = PoissonRegressor(alpha=33) # Optimisation is performed below\n\nPoisson_GLM.fit(X_train,y_train)\n\ny_pred = Poisson_GLM.predict(X_test)\n\nMA_error = mean_absolute_error(y_test,y_pred)\nRMS_error = mean_squared_error(y_test,y_pred,squared = False)\nprint('MAE = {:.4f}'.format(MA_error))\nprint('RMSE = {:.4f}'.format(RMS_error))","57d0e750":"output = []\nlast_error =999\n\nfor i in range(1,101):\n    iPoisson_GLM = PoissonRegressor(alpha=i,tol=1e-6)\n    iPoisson_GLM.fit(X_train,y_train)    \n    y_pred = iPoisson_GLM.predict(X_test)\n    iRMS_error = mean_squared_error(y_test,y_pred,squared = False)\n    if iRMS_error < last_error: best_alpha = i\n    last_error = iRMS_error                                               \n    output.append(iRMS_error)\n                                               \nprint(\"RMSE is minimised at \\u03B1 =\",best_alpha)","3f0a1e77":"plt.plot(output)\nplt.title('RMSE for optimising \\u03B1 in Poisson GLM')\nplt.xlabel('\\u03B1')\nplt.ylabel('RMSE')\nplt.show()","7f18979b":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nPoisson_GLM2 = sm.GLM(y_train,X_train,family=sm.families.Poisson())\nresults = Poisson_GLM2.fit()\nprint(results.summary())","d8816b52":"y_pred2 = Poisson_GLM2.predict(params=results.params,exog=X_test)\n\nMA_error2 = mean_absolute_error(y_test,y_pred2)\nRMS_error2 = mean_squared_error(y_test,y_pred2,squared = False)\nprint('MAE = {:.4f}'.format(MA_error2))\nprint('RMSE = {:.4f}'.format(RMS_error2))","bdb1aba3":"residuals = results.resid_deviance\nsns.distplot(residuals)\nplt.title(\"Poission GLM residuals\")","f43b8613":"# Statsmodel formula wants the target and predictor in the same data\ntrain = pd.merge(X_train,y_train,on='id')\n\nformula = 'num_awards ~ math + math^2 + prog_1 + prog_2 + prog_3'\nPoisson_GLM3 = smf.glm(formula=formula, data=train, family=sm.families.Poisson())\nresults2 = Poisson_GLM3.fit()\nprint(results2.summary())\n","ceb77e0b":"y_pred3 = Poisson_GLM3.predict(params=results.params,exog=X_test)\n\nMA_error3 = mean_absolute_error(y_test,y_pred3)\nRMS_error3 = mean_squared_error(y_test,y_pred3,squared = False)\nprint('MAE = {:.4f}'.format(MA_error3))\nprint('RMSE = {:.4f}'.format(RMS_error3))","1cc8e519":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nxgb = XGBRegressor()\n\nparameters = {'n_estimators': [50,100,200,500],\n              'max_depth': [1,2,3,5],\n              'learning_rate': [0.001,0.01,0.1],\n             }\n\ngrid = GridSearchCV(estimator = xgb, param_grid = parameters,n_jobs =-1,scoring=make_scorer(mean_squared_error,squared=False,greater_is_better=False))\n\ngrid.fit(X_train,y_train)\n\ny_pred_grid = grid.predict(X_test)\n\nMA_error4 = mean_absolute_error(y_test,y_pred_grid)\nRMS_error4 = mean_squared_error(y_test,y_pred_grid,squared = False)\n\nprint('MAE = {:.4f}'.format(MA_error4))\nprint('RMSE = {:.4f}'.format(RMS_error4))\nprint('Best Parameters = ',grid.best_params_)","ba51d68f":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nCat = CatBoostRegressor(verbose=0)\n\nparameters = {'n_estimators': [50,100,200,300],\n              'max_depth': [1,2,3,5],\n              'learning_rate': [0.001,0.01,0.1],\n             }\n\ngrid2 = GridSearchCV(estimator = Cat, param_grid = parameters,n_jobs =-1,scoring=make_scorer(mean_squared_error,squared=False,greater_is_better=False))\n\ngrid2.fit(X_train,y_train)\n\ny_pred_grid = grid2.predict(X_test)\n\nMA_error5 = mean_absolute_error(y_test,y_pred_grid)\nRMS_error5 = mean_squared_error(y_test,y_pred_grid,squared = False)\n\nprint('MAE = {:.4f}'.format(MA_error5))\nprint('RMSE = {:.4f}'.format(RMS_error5))\nprint('Best Parameters = ',grid2.best_params_)","ea2a97b6":"from sklearn.linear_model import LinearRegression\n\nLR = LinearRegression()\n\nLR.fit(X_train,y_train)\n\ny_pred_6 = LR.predict(X_test)\n\nMA_error6 = mean_absolute_error(y_test,y_pred_6)\nRMS_error6 = mean_squared_error(y_test,y_pred_6,squared = False)\n\nprint('MAE = {:.4f}'.format(MA_error6))\nprint('RMSE = {:.4f}'.format(RMS_error6))","fac51dea":"from sklearn.linear_model import Ridge\n\nRR = Ridge()\n\nRR.fit(X_train,y_train)\n\ny_pred_ridge = RR.predict(X_test)\n\nMA_error_ridge = mean_absolute_error(y_test,y_pred_ridge)\nRMS_error_ridge = mean_squared_error(y_test,y_pred_ridge,squared = False)\n\nprint('MAE = {:.4f}'.format(MA_error_ridge))\nprint('RMSE = {:.4f}'.format(RMS_error_ridge))","a490a348":"output = []\nlast_error =999\n\nfor i in range(1,101):\n    iRidge = Ridge(alpha=i,tol=1e-6)\n    iRidge.fit(X_train,y_train)    \n    y_pred = iRidge.predict(X_test)\n    iRMS_error = mean_squared_error(y_test,y_pred,squared = False)\n    if iRMS_error < last_error: best_alpha = i\n    last_error = iRMS_error                                               \n    output.append(iRMS_error)\n                                               \nprint(\"RMSE is minimised at \\u03B1 =\",best_alpha)","ff6f226e":"plt.plot(output)\nplt.title('RMSE for optimising \\u03B1 in Ridge Regression')\nplt.xlabel('\\u03B1')\nplt.ylabel('RMSE')\nplt.show()","987178ac":"from sklearn.ensemble import StackingRegressor\n\nestimators = [('XGBoost',xgb),\n              ('CatBoost',Cat),\n              ('Poisson GLM',Poisson_GLM),\n              ('Linear Regression',LR),\n              ('Ridge Regression',RR)]\n\nstack = StackingRegressor(estimators=estimators)\nstack.fit(X_train,y_train)\ny_pred_stack = stack.predict(X_test)\n\nMA_error_stack = mean_absolute_error(y_test,y_pred_stack)\nRMS_error_stack = mean_squared_error(y_test,y_pred_stack,squared = False)\n\nprint('MAE = {:.4f}'.format(MA_error_stack))\nprint('RMSE = {:.4f}'.format(RMS_error_stack))","2d23450c":"overall_results = {'Model':['Optimised SKLearn Poisson GLM','StatsModel Poission GLM','XGBoost','CatBoost','Linear Regression','Ridge Regression','Stacked Model'],\n                    'MA Error':[MA_error,MA_error2,MA_error4,MA_error5,MA_error6,MA_error_ridge,MA_error_stack],\n                    'RMS Error': [RMS_error,RMS_error2,RMS_error4,RMS_error5,RMS_error6,RMS_error_ridge,RMS_error_stack]\n                  }\n\noverall_results = pd.DataFrame(overall_results).round(3).sort_values(by='RMS Error',ascending=True)\noverall_results","35c4b94f":"## Results","9adb1449":"# Generalised Linear Models\n\nIn this notebook I outline some of the theory of Generalised Linear Models and attempt a couple of simple applications.\n\nCovariates are usually called features these days, these are our $x_i, z_i ...$ etc  where $i$ is the number of records\/oveservations (rows)\n\n### The distribution of $x_i$\n* In linear models, we assume that y is normally distributed $ y \\sim N(\\mu,\\sigma^2) $\n* For GLMs, we can expand this so that we assume y is distributed in anby member of the exponential family $ y \\sim exponential family $\n\n\n### The linear predictor (function)\n* In linear models, this is $\\eta_i = \\alpha + \\beta x_i$\n* Because GLMs can be multivariate, so our predictor can be of a more general form, eg: $\\eta_i = \\alpha + \\beta x_i + \\gamma z_i$, or  $\\eta_i = \\alpha + \\beta x_i + \\gamma z_i^2$\n\n### The link function is how we link our predictor back to the mean $\\mu_i = E[Y_i]$\n* For linear models, our mean sits on the line so $\\eta_i = \\mu_i = \\alpha + \\beta x_i$\n* For GLMs, we can again have a variety of functions, eg. $\\eta_i = ln (\\mu_i)$ which implies $\\mu_i = e^{\\alpha + \\beta x_i + \\gamma z_i^2}$","1ad66523":"## Try an XGBoost model","35cae5d2":"I have lifted the data for this exercise from here: https:\/\/stats.idre.ucla.edu\/r\/dae\/poisson-regression\/. The data is from the number of awards earned by students at one high school. Predictors of the number of awards earned include the type of program in which the student was enrolled (e.g., vocational, general or academic) and the score on their final exam in math.\n\n* `num_awards` is the outcome variable and indicates the number of awards earned by students at a high school in a year,\n* `math` is a continuous predictor variable and represents students\u2019 scores on their math final exam, and\n* `prog` is a categorical predictor variable with three levels indicating the type of program in which the students were enrolled. It is coded as 1 = \u201cGeneral\u201d, 2 = \u201cAcademic\u201d and 3 = \u201cVocational\u201d. ","8c0137a5":"## Application in SKLearn","942cef26":"## An example (with SKLearn and StatsModels)","2c7313c1":"So why bother with GLMs at all here? Partly a learning exercise and partly to understand that whilst error is one metric, appropriateness of the distribution can have a huge effect on the model. In this case predicting counts is thoeretically best modelled with Poisson, even if this dataset doesn't show it.","18cac971":"Let's see if some regularisation helps the fit:","815bb462":"## Try Stacking too","1d3e9089":"First some data cleaning and visualisation...","3430d0b3":"The residuals are the error terms associated with our $y_i$ target variable. In a standard linear regression we would expect these to be normal, in a GLM they will be distributed by the exponential family distribution that we specified. Here we would expect our residuals to be Poisson.","dfe4c65d":"## Cat Boost\n","11e19566":"## What about Linear Regression?","a6844ffb":"## Ridge Regression and Optimisation","88a12796":"I tried a few different predictor forumlae, but none yielded statistical significance for the coefficients.","15c46cd6":"Can we do better by using a different predictor formula? As far as I can tell SKLearn does not accomodate this functionality easily, but statsmodels does","11ef3cc1":"TODO: Insert some good explanations about RF Boosting models","b8d2cf17":"### Using Maximum Likelihood to derive the parameters ($\\alpha, \\beta$ etc)\n\n* The likelihood is the product of our PDFs: $L(\\mu) = f(y_1)....f(y_n)$\n* Log , to get log likelihood\n* Differentiate and find miunumum at 0\n* Check second differential is negative for a maximum\n* We would need to use the link function and do multiple differentiation with respect to each of $\\alpha, \\beta$ etc\n \n ","fc21168c":"\n## Application in StatsModel","e402ad71":"Not the most convincing poisson distriburion, but not normal for sure.","7628190a":"The trouble with SKLearn is it's a bit like \"off-the-shelf\" machine learning. There is no way to know how good our fit is, it's not a statisticians best tool. \n\nSure we can get $R^2$ without much issue, but if we want to examine the significance of the parameters, check the AIC and more (as we would in R), we need to look in another package, say StatsModel:"}}