{"cell_type":{"e9af9222":"code","61f8dac4":"code","dd12a9fd":"code","1097f3d4":"code","e4dd8f2e":"code","5479f05a":"code","ff06bcd7":"code","7086c75e":"code","8920db8a":"code","69d234a4":"code","27feb2a1":"code","65805918":"code","e5b111ca":"code","f51fc47a":"code","5760a95c":"code","5d4efb45":"code","4afc2494":"code","b4b2fa1c":"code","cba8576f":"code","7ee6968b":"code","5d4f2b6f":"code","eb818ce4":"markdown","759bfce4":"markdown","19ff5cc6":"markdown","f3ad4dd2":"markdown","83aeb87f":"markdown","b1fe1219":"markdown","f6283ba2":"markdown","3108809b":"markdown","897d238c":"markdown","a5c46194":"markdown","33f3e3d4":"markdown"},"source":{"e9af9222":"from __future__ import absolute_import, division, print_function\n\nimport codecs\nimport glob\nimport logging\nimport multiprocessing\nimport os\nimport pprint\nimport re\nimport string\n\nimport nltk\nimport gensim.models.word2vec as w2v\nfrom sklearn.manifold import TSNE\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport gensim\nfrom nltk.corpus import stopwords\n#nltk.download(\"stopwords\")","61f8dac4":"stopwords = set(stopwords.words('spanish'))","dd12a9fd":"hhgroups = pd.read_csv('..\/input\/hhgroups_merge_28_05.csv')\nhhgroups.head()","1097f3d4":"letras = list(hhgroups['letra'])\nletras_limpio = []\n# Eliminamos canciones sin letra\nfor letra in list(letras):\n    if \"\u00bfTienes ya la letra para este tema? Ay\u00fadanos y \u00a1Env\u00edanosla!\" in letra:\n        letras.remove(letra)\n\n#Eliminamos l\u00edneas vac\u00edas o con [Artista] o [Estribillo]\nfor i in range(len(letras)):\n    cancion_limpia = []\n    for linea in letras[i].split(\"\\n\"):\n        if (\"[\" not in linea and \"(\" not in linea and linea != \"\"):\n            #Pasamos l\u00ednea a min\u00fasculas y eliminamos puntuaci\u00f3n\n            linea = bytes(linea, 'utf-8').decode('utf-8', 'ignore')\n            linea = \"\".join(c for c in linea if (c not in string.punctuation and c not in ['','\u00a1','\u00bf'])).lower()\n            linea = linea.split(\" \")\n            #Eliminamos stopwords\n            for palabra in list(linea):\n                #palabra = palabra.replace(u'\\xa0', u'') #Estp les pasa por usar latin en vez de UTF-8\n                if palabra in stopwords or palabra in string.punctuation:\n                    linea.remove(palabra)\n            cancion_limpia += linea\n    letras_limpio += [cancion_limpia]","e4dd8f2e":"letras_limpio[0][:10]","5479f05a":"len(letras_limpio)","ff06bcd7":"hhgroups2vec = w2v.Word2Vec(\n    letras_limpio,\n    sg=1,\n    seed=1,\n    workers=multiprocessing.cpu_count(),\n    size=256,\n    min_count=50,\n    window=12\n)","7086c75e":"hhgroups2vec.wv.most_similar(\"familia\")","8920db8a":"hhgroups2vec.wv.most_similar(\"espa\u00f1a\")","69d234a4":"hhgroups2vec.wv.most_similar(\"1\")","27feb2a1":"hhgroups2vec.wv.most_similar(\"arte\")","65805918":"hhgroups2vec.wv.most_similar(\"m\u00fasica\")","e5b111ca":"hhgroups2vec.wv.most_similar(\"joder\")","f51fc47a":"hhgroups2vec.wv.most_similar(\"bien\")","5760a95c":"def nearest_similarity_cosmul(start1, end1, end2):\n    similarities = hhgroups2vec.wv.most_similar_cosmul(\n        positive=[end2, start1],\n        negative=[end1]\n    )\n    start2 = similarities[0][0]\n    print(\"{0} es a {1}, lo que {2} es a {3}\".format(start1, end1, start2, end2))","5d4efb45":"nearest_similarity_cosmul(\"ser\", \"siendo\", \"haciendo\")","4afc2494":"nearest_similarity_cosmul(\"amar\", \"amor\", \"odio\")","b4b2fa1c":"nearest_similarity_cosmul(\"europa\", \"espa\u00f1a\", \"argentina\")","cba8576f":"hhgroups2vec.wv.doesnt_match(\"felicidad amor alegr\u00eda envidia\".split(\" \"))","7ee6968b":"hhgroups2vec.most_similar(positive=['mujer', 'rey'], negative=['hombre'])[2:]","5d4f2b6f":"def tsne_plot(model):\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(64, 64)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\n    \n# call the function on our dataset\ntsne_plot(hhgroups2vec)","eb818ce4":"# Entrenar el modelo Word2Vec\nA la hora de entrenar Word2Vec, hay dos arquitecturas:\n* **Skip-gram**: funciona bien con datasets peque\u00f1os, representa bien incluso palabras y frases poco frecuentes. **El objetivo al entrenar la red neuronal es predecir el contexto dada una palabra.**\n* **Continuous Bag of Words**: mucho m\u00e1s r\u00e1pido de entrenar, m\u00e1s precisi\u00f3n con palabras muy frecuentes. **El objetivo es predecir una palabra dado un contexto.**\n\n**Usaremos Skip-Gram** Porque nos interesa saber, de una palabra determinada, las palabras de la misma sem\u00e1ntica. \n\nPar\u00e1metros:\n* **letras**, el corpus de texto\n* **sg**, 1 para usar skip-gram, 0 para usar CBOW\n* **seed**, 1 para tener resultados reproducibles y debugear\n* **size**, el tama\u00f1o de los vectores de palabras, o neuronas. Suele estabilizarse en 300 incluso con datasets de 300MB pero depende m\u00e1s de la complejidad del vocabulario. Usa m\u00faltiples de 2 para rendimiento.\n* **min_count**, m\u00ednimo de veces que ha de aparecer cada palabra, para ser considerada. Si no se repite lo suficiente, no podremos sacar intuici\u00f3n sem\u00e1ntica significativa, por falta de ejemplos.\n* **window**, ventana contextual de cada palabra. En caso de 4, por cada palabra le afectar\u00e1n m\u00e1ximo las 4 anteriores y 4 siguientes.\n\nM\u00e1s info en: http:\/\/methodmatters.blogspot.com\/2017\/11\/using-word2vec-to-analyze-word.html","759bfce4":"## Comprobar qu\u00e9 ha aprendido el modelo\n### Predecir contexto seg\u00fan palabra\nComo s\u00f3lo miramos las palabras que se repiten al menos 50 veces en un contexto de 10 palabras alrededor m\u00e1ximo, deber\u00edamos ver conceptos bastante generales que no tengan muchas palabras irrelevantes.\n\nSe han colado algunas rimas, pero tiene bastante precisi\u00f3n.","19ff5cc6":"### Sumar y restar palabras","f3ad4dd2":"# Limpiar el dataset y crear corpus de texto\nEliminamos las stopwords que son palabras que no aportan contenido. Faltan otras tareas de NLP como eliminar morfemas, etc.\n\nTambi\u00e9n eliminamos las l\u00edneas que contienen [Artista], y las canciones vac\u00edas.\nUna vez limpio lo convertimos en un \"corpus\", que ser\u00eda una lista de palabras de todas las canciones.\nEl c\u00f3digo es un poco \"hacky\" y nada pythonico, pero funciona.\nRecomiendo preferiblemente usar Pandas para limpiar el dataset.","83aeb87f":"## Imports y cargar dataset","b1fe1219":"### Descartar palabras fuera de sem\u00e1ntica","f6283ba2":"Ejemplo: https:\/\/www.tensorflow.org\/images\/linear-relationships.png","3108809b":"# Red neuronal Word2Vec aprende la sem\u00e1ntica del rap espa\u00f1ol\nWord2Vec es un modelo que convierte palabras en vectores que guardan la relaci\u00f3n sem\u00e1ntica respecto al resto de palabras. Permite predecir el contexto dada una palabra (palabras que le \"rodean\") con skip-gram, o predecir una palabra dado un contexto con CBOW.\n\nEste modelo permite crear detectores de spam y ortograf\u00eda, predecir g\u00e9nero musical seg\u00fan letra, predecir canciones seg\u00fan g\u00e9nero musical, encontrar sin\u00f3nimos, visualizar sem\u00e1ntica, comparar y clasificar reviews (como ya hace Airbnb), sugerir m\u00fasica y g\u00e9neros musicales en Spotify, recomendar anuncios...\n\n# Dataset: 9322 letras de canciones de HHGroups.com\nDescargu\u00e9 las letras a lo largo del mes de Mayo de 2019, ~24MB de texto, suficiente para mis 3 objetivos:\n1. **Modelo Word2Vec de Gensim aprende la sem\u00e1ntica del rap**\n    * Podemos visualizarla en 2D\/3D y observar como hay cl\u00fasteres de tem\u00e1ticas.\n    * Dada una palabra podemos predecir su contexto, las palabras que le rodean.\n    * Sumar y restar palabras entre ellas en un espacio de vectores que representa la sem\u00e1ntica\n2. Generar letras de rap con cadenas de Markov\n    * Respuestas al momento detectando rimas\n    * M\u00e1s adelante probar redes neuronales a nivel de car\u00e1cter y skip-gram.\n3. (pendiente) Clasificar raperos seg\u00fan complejidad de vocabulario","897d238c":"Quedan 9317 canciones limpias.","a5c46194":"## Visualizando el modelo en 2D con T-SNE","33f3e3d4":"### Analog\u00edas\nUn aforismo requiere bastante pensamiento abstracto, pero como las palabras son vectores, podemos mirar la distancia entre dos palabras, para que, dada una tercera palabra nos adivine la cuarta.\n\nM\u00e1s que un aforismo es conservar la dist\u00e1ncia sem\u00e1ntica entre palabras."}}