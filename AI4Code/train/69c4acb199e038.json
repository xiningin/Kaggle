{"cell_type":{"a87e8499":"code","3dca238f":"code","bbbdfb8c":"code","6c120e21":"code","a1340f3f":"code","aae099cd":"code","2ff53ff7":"code","983306ff":"code","39c5612a":"code","4a80d879":"code","c1e5fb44":"code","71b388cd":"code","86f9677d":"code","dc0477b8":"code","5476f5f6":"code","a7f546dc":"code","b1f56845":"code","1c77d6d3":"code","207cad72":"code","52e99149":"code","cf5100c3":"code","9a52d9dc":"code","ea7ad4f7":"code","b039cb41":"code","0faef590":"code","1f8f4c96":"code","52442b80":"code","fc1b22fb":"code","3096a999":"code","5c3c4968":"code","ced81b53":"code","99afd6c6":"code","672872e5":"code","82b65a3d":"code","c329b1fc":"code","b90ae8e9":"code","7e4b19bd":"code","ce897020":"code","a7327fc5":"code","aa92261c":"markdown","a69b0826":"markdown","f00792cf":"markdown","126a2261":"markdown","de03da37":"markdown","faa4326f":"markdown","78716a2b":"markdown","c39d4528":"markdown","d3957d2f":"markdown","c83699b7":"markdown","db538d60":"markdown","bc58ccc9":"markdown","e617e47d":"markdown"},"source":{"a87e8499":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3dca238f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"\/kaggle\/input\/ecommerce-data\/data.csv\")\ndf.head(10)","bbbdfb8c":"df.info()","6c120e21":"import re\ndf['InvoiceNo'].str.contains('C', flags = re.IGNORECASE, regex = True)","a1340f3f":"df[df['InvoiceNo'].str.contains('C', flags = re.IGNORECASE, regex = True)]","aae099cd":"idx_cancelled_invoices = df[df['InvoiceNo'].str.contains('C', flags = re.IGNORECASE, regex = True)].index\nidx_cancelled_invoices","2ff53ff7":"df = df[~(df['InvoiceNo'].str.contains('C', flags = re.IGNORECASE, regex = True))]\ndf","983306ff":"df.isnull().sum()","39c5612a":"print(f\"Before Dropping CustomerID Null Rows:\\nNumber of rows = {df.shape[0]}\\nNumber of cols = {df.shape[1]}\")\ndf.dropna(inplace = True)\nprint(f\"\\nAfter Dropping CustomerID Null Rows:\\nNumber of rows = {df.shape[0]}\\nNumber of cols = {df.shape[1]}\")","4a80d879":"df.isnull().sum()","c1e5fb44":"print(f\"Before Dropping Duplicates:\\nNumber of rows = {df.shape[0]}\\nNumber of cols = {df.shape[1]}\")\ndf.drop_duplicates(inplace = True)\nprint(f\"\\nAfter Dropping Duplicates:\\nNumber of rows = {df.shape[0]}\\nNumber of cols = {df.shape[1]}\")","71b388cd":"df.head(10)","86f9677d":"df['CustomerID'] = df['CustomerID'].astype('int64').astype('category')\ndf['CustomerID'].dtype","dc0477b8":"df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\ndf['InvoiceDate'].dtype","5476f5f6":"df.describe()","a7f546dc":"df['TotalPurchaseValue'] = df['Quantity'] * df['UnitPrice']\ndf.head(10)","b1f56845":"customer_wise_total_purchase_value_df = df[['CustomerID', 'TotalPurchaseValue']].groupby('CustomerID', as_index = False).sum()\ncustomer_wise_total_purchase_value_df.rename(columns = {'TotalPurchaseValue' : 'Monetary'}, inplace = True)\ncustomer_wise_total_purchase_value_df","1c77d6d3":"customer_wise_frequent_purchases_df =  df[['CustomerID', 'InvoiceNo']].groupby('CustomerID', as_index = False).count()\ncustomer_wise_frequent_purchases_df.rename(columns = {'InvoiceNo': 'Frequency'}, inplace = True)\ncustomer_wise_frequent_purchases_df","207cad72":"merged_df = customer_wise_total_purchase_value_df.merge(customer_wise_frequent_purchases_df, on = \"CustomerID\", how = \"inner\")\nmerged_df","52e99149":"last_purchase_df = df[['CustomerID', 'InvoiceDate']].groupby('CustomerID', as_index = False).max()\nlast_purchase_df.rename(columns = {'InvoiceDate': 'LastPurchaseDate'}, inplace = True)\nlast_purchase_df","cf5100c3":"days_since_last_purchase = df['InvoiceDate'].max() - last_purchase_df['LastPurchaseDate'] \ndays_since_last_purchase = days_since_last_purchase + pd.Timedelta(\"1 days\")\ndays_since_last_purchase","9a52d9dc":"time_diff_in_days = pd.Series(data = [d.days for d in days_since_last_purchase], index = merged_df.index)\ntime_diff_in_days","ea7ad4f7":"merged_df['Recency'] = time_diff_in_days\nmerged_df","b039cb41":"merged_df.isnull().sum()","0faef590":"merged_df.dropna(inplace = True)\nmerged_df.drop(columns = 'CustomerID', inplace = True)","1f8f4c96":"merged_df.isna().sum()","52442b80":"fig, axis = plt.subplots(nrows = 2, ncols = 3, \n                         figsize = (15, 4), dpi = 100,\n                         sharex = False, sharey = False,\n                         gridspec_kw = {'height_ratios': [3, 1]}\n                         )\n\n# Monetary\naxis[0, 0].hist(merged_df['Monetary'], bins = 'sturges', facecolor = 'red', edgecolor = 'black')\nsns.boxplot(x = 'Monetary', data = merged_df,  color = 'red', ax = axis[1, 0])\naxis[0, 0].set_title(\"Histogram & Boxplot for Monetary\")\n\n# Frequency\naxis[0, 1].hist(merged_df['Frequency'], bins = 'sturges', facecolor = 'green', edgecolor = 'black')\nsns.boxplot(x = 'Frequency', data = merged_df,  color = 'green', ax = axis[1, 1])\naxis[0, 1].set_title(\"Histogram & Boxplot for Frequency\")\n\n# Recency\naxis[0, 2].hist(merged_df['Recency'], bins = 'sturges', facecolor = 'purple', edgecolor = 'black')\nsns.boxplot(x = 'Recency', data = merged_df,  color = 'purple', ax = axis[1, 2])\naxis[0, 2].set_title(\"Histogram & Boxplot for Recency\")\n\nplt.show()","fc1b22fb":"def treating_outliers(df, col):\n  col_q1 = df[col].quantile(0.25)\n  col_q3 = df[col].quantile(0.75)\n  col_iqr = col_q3 - col_q1\n  new_df = df[(df[col] >= col_q1 - 1.5 * col_iqr) & (df[col] <= col_q3 + 1.5 * col_iqr)]\n  return new_df\n\nnew_df = treating_outliers(merged_df, 'Monetary')\nnew_df = new_df.reset_index(drop = True)\nnew_df","3096a999":"fig, axis_mon = plt.subplots(nrows = 2, ncols = 1, \n                         figsize = (10, 4), dpi = 100,\n                         sharex = False, sharey = False,\n                         gridspec_kw = {'height_ratios': [3, 1]}\n                         )\n\n# Monetary\naxis_mon[0].hist(new_df['Monetary'], bins = 'sturges', facecolor = 'red', edgecolor = 'black')\nsns.boxplot(x = 'Monetary', data = new_df,  color = 'red', ax = axis_mon[1])\naxis_mon[0].set_title(\"Histogram & Boxplot for Monetary\")\n\nplt.show()","5c3c4968":"# standardise all parameters\nfrom sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler()\nnorm_new = standard_scaler.fit_transform(new_df)\nnorm_new_df = pd.DataFrame(norm_new)\nnorm_new_df.columns = new_df.columns\nnorm_new_df","ced81b53":"norm_new_df.describe().loc[['mean', 'std'], :]","99afd6c6":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H\n\nhopkins(norm_new_df)","672872e5":"# K Means with K = 5: chosen randomly.\nfrom sklearn.cluster import KMeans\n\nkmeans_model = KMeans(n_clusters = 5, init = 'k-means++', random_state = 14)\nkmeans_model.fit(norm_new_df)\n\ncluster_labels = pd.Series(data = kmeans_model.labels_, index = norm_new_df.index)\ncluster_labels.value_counts()","82b65a3d":"# analysis of clusters formed\nkm_df = pd.concat([new_df, cluster_labels], axis = 1)\nkm_df.columns = list(new_df.columns) + ['ClusterLabel']\nkm_df","c329b1fc":"import plotly.express as px\n# df = px.data.iris()\nplotly_fig = px.scatter_3d(km_df, x = 'Monetary', y = 'Frequency', z = 'Recency', color = 'ClusterLabel')\nplotly_fig.show()","b90ae8e9":"from sklearn.metrics import silhouette_score\nsse_ = []\nfor k in range(2, 15):\n    kmeans = KMeans(n_clusters = k).fit(norm_new_df)\n    sse_.append([k, silhouette_score(norm_new_df, kmeans.labels_)])\n    \nplt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1])","7e4b19bd":"# Rebuilding KMeans model with 4 clusters.\nkmeans_model2 = KMeans(n_clusters = 4, init = 'k-means++', random_state = 14)\nkmeans_model2.fit(norm_new_df)\n\ncluster_labels2 = pd.Series(data = kmeans_model2.labels_, index = norm_new_df.index)\ncluster_labels2.value_counts()","ce897020":"# analysis of clusters formed\nkm_df2 = pd.concat([new_df, cluster_labels2], axis = 1)\nkm_df2.columns = list(new_df.columns) + ['ClusterLabel']\nkm_df2","a7327fc5":"import plotly.express as px\n# df = px.data.iris()\nplotly_fig2 = px.scatter_3d(km_df2, x = 'Monetary', y = 'Frequency', z = 'Recency', color = 'ClusterLabel')\nplotly_fig2.show()","aa92261c":"### Building KMeans Model\n\n- Randomly choosing 5 clusters\n- Initialising centriods with `k-means++` approach","a69b0826":"### Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","f00792cf":"### Data Preprocessing\n\n- Remove the row entries containing the cancelling orders or invoices\n- Remove the rows\/columns containing the null or missing values\n- Remove duplicates\n- Convert the `CustomerID` values to integer (from float) followed by categorical values.","126a2261":"Mean and standard deviation of values in the RFM features after scaling are 0 and 1 respectively.","de03da37":"### Rebuilding KMeans Model","faa4326f":"### Extract Recency, Frequency, Monetary (RFM) Metrics\n\nGroup the data frame by customer ID separately and aggregate the `InvoiceDate`, `Quantity` and `UnitPrice` columns.","78716a2b":"### Feature Scaling\n\nScaling the RFM features using the standard scaling technique to converge to global centroids faster using the KMeans clustering algorithm.","c39d4528":"Data Set Information:\n\nThis is a transactional data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\n\nAttribute Information:\n\n- `InvoiceNo`: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n\n- `StockCode`: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n\n- `Description`: Product (item) name. Nominal.\n\n- `Quantity`: The quantities of each product (item) per transaction. Numeric.\n\n- `InvoiceDate`: Invice Date and time. Numeric, the day and time when each transaction was generated.\n\n- `UnitPrice`: Unit price. Numeric, Product price per unit in sterling.\n\n- `CustomerID`: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n\n- `Country`: Country name. Nominal, the name of the country where each customer resides.\n\n\nLink to the dataset: https:\/\/archive.ics.uci.edu\/ml\/datasets\/online+retail\n\n","d3957d2f":"`Monetary` and `Frequency` columns contain highly skewed values which indicates possibility of outliers.\n\nTreating likely outliers in the `Monetary` column with the inter-quartile range (IQR) metric.","c83699b7":"As the number of clusters increase, the similarity of points within a cluster decrease. Hence, Ideally we should chose 4 clusters to be formed.","db538d60":"The Hopkins Statistics value is between 0.7 and 0.99. Hence, there is a higher tendency of RFM features towards clustering.","bc58ccc9":"### Data Visualisation\n\nCreate histograms and box plots for RFM features to evaluate the data distribution within each features.","e617e47d":"### Cluster Tendency\n\nMeasuring the cluster tendency of the RFM features using the Hopkins Statistics method.\n\n- If the value is between ${0.01, \\dots, 0.3}$, the data is regularly spaced.\n\n- If the value is around $0.5$, it is random.\n\n- If the value is between ${0.7, \\dots, 0.99}$, it has a high tendency to cluster."}}