{"cell_type":{"ea8a78d2":"code","9e9f86b8":"code","3915c3ab":"code","d76865fb":"code","7e7c7e5b":"code","96a457db":"code","866f1bdf":"code","7bb105df":"code","416047e4":"code","6a1cc59f":"code","8fcea045":"code","52ab5208":"code","9c56b3ed":"code","f17cd7a5":"code","e24641ee":"code","1adc0628":"code","91ff4b46":"code","8c88c154":"code","1d9481f2":"code","885d61ed":"code","a0ce3b77":"code","a7b9be76":"code","26cd3d7d":"code","fbecade8":"code","ed4908be":"markdown","1354be30":"markdown","91da2898":"markdown","12bba0f0":"markdown","f6da2379":"markdown","ef6f33a1":"markdown","d8ba55c6":"markdown","f7308b75":"markdown","01d33b1f":"markdown","ea68a9c0":"markdown","ac428ad3":"markdown","b1a8bd78":"markdown","e590c430":"markdown","79cbcc4b":"markdown","b2ad907b":"markdown","99633baa":"markdown"},"source":{"ea8a78d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e9f86b8":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport os\n\ntrain_data_dir='..\/input\/creditcardfraud\/creditcard.csv'\n\ndef get_data():\n    \n    # Import the csv data as pands's dataframe\n    data=pd.read_csv(train_data_dir)\n    min_max_scaler=MinMaxScaler()\n    #Create an instance of sklearn's MinMaxScalar and use it to map the feature values in the range [0,1]\n    df_cred=data.drop(\"Time\",axis=1)\n    df_cred_scaled = min_max_scaler.fit_transform(df_cred.iloc[:,:-1])\n    df_cred_normalized = pd.DataFrame(df_cred_scaled)\n    \n    df_cred_normalized[\"Class\"]=df_cred[\"Class\"]\n    \n    print(\"class count : \",df_cred_normalized[\"Class\"].value_counts())\n    \n    # train and test splits process initiated \n    df_cred_normalized_train=df_cred_normalized[df_cred_normalized[\"Class\"]==0]\n    df_cred_normalized_test=df_cred_normalized[df_cred_normalized[\"Class\"]==1]\n    \n    # taking out 10 percent sample out of train set and to feed in test and validation set  \n    df_cred_normalized_test_part_1=df_cred_normalized_train.sample(frac=0.05)\n    df_cred_normalized_train=df_cred_normalized_train.drop(df_cred_normalized_test_part_1.index)\n    df_cred_normalized_test_part_2=df_cred_normalized_train.sample(frac=0.05)\n    df_cred_normalized_train=df_cred_normalized_train.drop(df_cred_normalized_test_part_2.index)\n    \n    # frauds cases in test and validation set as dicsussed earlier \n    df_cred_normalized_test_class_1=df_cred_normalized_test.sample(frac=0.5)\n    df_cred_normalized_validation_class_1=df_cred_normalized_test.drop(df_cred_normalized_test_class_1.index)\n    \n    print(\"fraud cases shape : \",df_cred_normalized_test_class_1.shape)\n    \n    df_cred_normalized_test_set=df_cred_normalized_test_part_1.append(df_cred_normalized_test_class_1)\n    df_cred_normalized_validation_set=df_cred_normalized_test_part_2.append(df_cred_normalized_validation_class_1)\n    \n    print(\"train set dimensions :\",df_cred_normalized_train.shape)\n    print(\"test set dimensions :\",df_cred_normalized_test_set.shape)\n    print(\"validate set dimensions :\",df_cred_normalized_validation_set.shape)\n    \n    print(\"class counts on validation set\")\n    print(df_cred_normalized_validation_set[\"Class\"].value_counts())\n    \n    x_train, x_test = train_test_split(df_cred_normalized_train, test_size=0.2, random_state=2020)\n    x_train = x_train[x_train.Class == 0]\n    y_train = x_train[\"Class\"]\n    x_train = x_train.drop(['Class'], axis=1)\n    y_test = x_test['Class']\n    x_test = x_test.drop(['Class'], axis=1)\n    x_train = x_train.values\n    x_test = x_test.values\n    print(\"train data set shape\")\n    print(x_train.shape)\n    print(\"test data set shape\")\n    print(x_test.shape)\n    y_train = y_train.values\n    y_test = y_test.values\n    \n    x_val_set_1=df_cred_normalized_test_set.iloc[:,:-1]\n    y_val_set_1=df_cred_normalized_test_set[\"Class\"]\n    x_val_set_1=x_val_set_1.values\n    y_val_set_1=y_val_set_1.values\n    \n    x_val_set_2=df_cred_normalized_validation_set.iloc[:,:-1]\n    y_val_set_2=df_cred_normalized_validation_set[\"Class\"]\n    x_val_set_2=x_val_set_2.values\n    y_val_set_2=y_val_set_2.values\n        \n    return [x_train, y_train], [x_test, y_test] ,[x_val_set_1,y_val_set_1],[x_val_set_2,y_val_set_2]","3915c3ab":"import os\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport sys\nfrom random import shuffle\nimport pandas as pd","d76865fb":"rm -rf \"\/kaggle\/working\/tf_records_root_dir\/\"","7e7c7e5b":"os.mkdir(\"\/kaggle\/working\/tf_records_root_dir\")\nos.mkdir(\"\/kaggle\/working\/tf_records_root_dir\/tf_records_train\")\nos.mkdir(\"\/kaggle\/working\/tf_records_root_dir\/tf_records_test\")\nos.mkdir(\"\/kaggle\/working\/tf_records_root_dir\/tf_records_validation_set_1\")\nos.mkdir(\"\/kaggle\/working\/tf_records_root_dir\/tf_records_validation_set_2\")","96a457db":"def float_feature(value):\n    ''' Helper function that wraps float features into the tf.train.Feature class \n    \n    @param value: the feature or label of type float, that we want to convert \n    '''\n    \n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef int64_feature(value):\n    ''' Helper function that wraps integer features into the tf.train.Feature class \n    \n    @param value: the feature or label of type integer, that we want to convert \n    '''\n    \n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef write_tf_records_file(x, y, tf_writer,data_type):\n    '''This function writes a feature-label pair to a TF-Records file\n    \n    @param x: the feature\n    @param y: the label\n    @ tf_writer: the TensorFlow Records writer instance that writes the files\n    '''\n    \n    # Convert numpy array to list, because tf.train.Feature accepts only lists\n    x=x.tolist()\n    #Features we want to convert to the binary format\n    feature_dict={ 'features': float_feature(x),\n                   'labels': float_feature(y),                         \n                 }\n\n    #Another wrapper class\n    features_wrapper=tf.train.Features(feature=feature_dict) \n    # Aaaand another wrapper class lol\n    example = tf.train.Example(features=features_wrapper)\n\n    # Finally we make the files binary and write them to TF-Records file\n    tf_writer.write(example.SerializeToString())\n\n\n\ndef run(tf_records_dir, data, data_type=None):\n    '''Main function for the writing process\n    \n    @param tf_records_dir: path where the files should be written into\n    @param data: the dataset that contains the features and labels\n    '''\n    \n    # If the directory is not present, create one\n    \n      \n    #Get the features and labels from the dataset\n    features=data[0]\n    labels=data[1]\n    # Number of instances in the dataset\n    n_data_instances=features.shape[0]\n    # Initialize a counter for the data instances\n    data_instance_counter=0\n    \n    # Specify the number of samples that will be saved in one TF-Records file\n    samples_per_file=500\n    \n    # Number of all TF-Records files in the end\n    n_tf_records_files=round(n_data_instances\/samples_per_file)\n    # Counter for the TF-Records files\n    tf_records_counter=0\n    \n   \n    #Iterate over the number of total TF-Records files\n    while tf_records_counter < n_tf_records_files:\n\n        # Give each file an unique name(full-path)\n        tfrecords_file_name='%s\/%s_%i.tfrecord' % (tf_records_dir, data_type, tf_records_counter)\n\n        #Initialize a writer for the files\n        with tf.python_io.TFRecordWriter(tfrecords_file_name) as tf_writer:\n            \n            sample_counter=0\n\n            #Iterate over all data samples and number of samples per TF-Records file\n            while data_instance_counter<n_data_instances and sample_counter<samples_per_file:\n                \n                sys.stdout.write('\\r>> Converting data instance %d\/%d' % (data_instance_counter+1, n_data_instances))\n                sys.stdout.flush()\n \n                # Extract a feature instance\n                x=features[data_instance_counter]\n                # Extract a label instance\n                y=labels[data_instance_counter]\n                \n                # Write feature and label to a TF-Records file\n                write_tf_records_file(x,y,tf_writer,data_type)\n\n                # Increase the counters\n                data_instance_counter+=1\n                sample_counter+=1\n                \n            tf_records_counter+=1\n            \n    print('\\nFinished converting the dataset!')\n    \n    \nif __name__ == \"__main__\":\n    \n    # Build the paths for the training, test, and validation TF-Records files\n    tf_records_root_dir='\/kaggle\/working\/tf_records_root_dir\/'\n    train_dir = os.path.join(tf_records_root_dir, 'tf_records_train')\n    test_dir = os.path.join(tf_records_root_dir, 'tf_records_test')\n    validation_dir_set_1 = os.path.join(tf_records_root_dir, 'tf_records_validation_set_1')\n    validation_dir_set_2 = os.path.join(tf_records_root_dir, 'tf_records_validation_set_2')\n    \n    # Get the preprocessed data \n    train_data, test_data ,validation_set_1,validation_set_2=get_data()    \n    #Write for each dataset TF-Records file\n    run(train_dir, train_data, data_type='training')\n    run(test_dir, test_data, data_type='test')\n    run(validation_dir_set_1, validation_set_1, data_type='validation_1')\n    run(validation_dir_set_2, validation_set_2, data_type='validation_2')","866f1bdf":"train_sample=199364\nvalidate_sample=28453\ntest_sample=56990","7bb105df":"def get_training_data(filepaths):\n    '''Prepares training dataset with the tf-data API for the input pipeline.\n    Reads TensorFlow Records files from the harddrive and applies several\n    transformations to the files, like mini-batching, shuffling etc.\n    \n    @return dataset: the training dataset\n    '''\n    \n    filenames=['\/kaggle\/working\/tf_records_root_dir\/tf_records_train\/'+f for f in os.listdir(filepaths)]\n    \n    dataset = tf.data.TFRecordDataset(filenames,num_parallel_reads=32)\n    dataset = dataset.map(parse,num_parallel_calls=4)\n    dataset = dataset.shuffle(buffer_size=205275)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size=64)\n    dataset = dataset.prefetch(buffer_size=16)\n    \n    return dataset\n \n\n\ndef get_test_data(filepaths):\n    '''Prepares validation dataset with the tf-data API for the input pipeline.\n    Reads TensorFlow Records files from the harddrive and applies several\n    transformations to the files, like mini-batching, shuffling etc.\n    \n    @return dataset: the validation dataset\n    '''\n    \n    filenames=['\/kaggle\/working\/tf_records_root_dir\/tf_records_test\/'+f for f in os.listdir(filepaths)]\n    \n    \n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(parse)\n    dataset = dataset.shuffle(buffer_size=1)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size=51319)\n    dataset = dataset.prefetch(buffer_size=1)\n    \n    return dataset\n\ndef get_validation_set_1(filepaths):\n    '''Prepares test dataset with the tf-data API for the input pipeline.\n    Reads TensorFlow Records files from the harddrive and applies several\n    transformations to the files, like mini-batching, shuffling etc.\n    \n    @return dataset: the test dataset\n    '''\n    \n    filenames=['\/kaggle\/working\/tf_records_root_dir\/tf_records_validation_set_1\/'+f for f in os.listdir(filepaths)]\n\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(parse)\n    dataset = dataset.shuffle(buffer_size=10000)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size=14462)\n    dataset = dataset.prefetch(buffer_size=1)\n    \n    return dataset\n\n\ndef get_validation_set_2(filepaths):\n    '''Prepares test dataset with the tf-data API for the input pipeline.\n    Reads TensorFlow Records files from the harddrive and applies several\n    transformations to the files, like mini-batching, shuffling etc.\n    \n    @return dataset: the test dataset\n    '''\n    \n    filenames=['\/kaggle\/working\/tf_records_root_dir\/tf_records_validation_set_2\/'+f for f in os.listdir(filepaths)]\n\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(parse)\n    dataset = dataset.shuffle(buffer_size=10000)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size=13751)\n    dataset = dataset.prefetch(buffer_size=1)\n    \n    return dataset\n\n\ndef parse(serialized):\n\n    features={'features':tf.FixedLenFeature([29], tf.float32),\n              'labels':tf.FixedLenFeature([1], tf.float32),\n              }\n    \n    \n    parsed_example=tf.parse_single_example(serialized,\n                                           features=features,\n                                           )\n \n    features=parsed_example['features']\n    label = tf.cast(parsed_example['labels'], tf.int32)\n    \n    return features, label","416047e4":"import time","6a1cc59f":"def feed_forward_propagations(data):\n    a1=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(data, W1), b1))\n    a2=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(a1, W2), b2))\n    a3=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(a2, W3), b3))\n    predictions=tf.matmul(a3, W4)\n    \n    return predictions\n\n\n### Define some hyperparameters ###\n\n# Number of training samples\nn_training_samples=250000\n# Batch size\nbatch_size=64\n# Learning rate \nlearning_rate=0.001\n# Number of test data samples\nn_test_samples=56990\n# Number of validate samples\nn_validate_samples=28453\n# Number of epochs\nnum_epoch=30\n# number of batches\nn_batches=64\n# Evaluate model after number of steps\neval_after=1000\n# Path of the TF Records datasets for the training\ntrain_path=\"\/kaggle\/working\/tf_records_root_dir\/tf_records_train\"\n# Path of the TF Records datasets for the testing\ntest_path=\"\/kaggle\/working\/tf_records_root_dir\/tf_records_test\"\n# Path of the TF Records datasets for the validation\n# Path of the TF Records datasets for the training\nvalidation_path_set_1=\"\/kaggle\/working\/tf_records_root_dir\/tf_records_validation_set_1\"\n# Path of the TF Records datasets for the testing\nvalidation_path_set_2=\"\/kaggle\/working\/tf_records_root_dir\/tf_records_validation_set_2\"\n# Path of the TF Records datasets for the validation\n\nweight_initializer=tf.random_normal_initializer(mean=0, stddev=0.02)\nbias_initializer=tf.zeros_initializer()\n\nn_batches=int(n_training_samples\/batch_size)\n\n# Define the dataflow graph for the training\ntraining_graph=tf.Graph()\n\nwith training_graph.as_default():\n\n    # Access the tf.dataset instance of the tf.data API for the training, \n    # testing and validation of the model\n    training_dataset=get_training_data(train_path)\n    test_dataset=get_test_data(test_path)\n    validation_dataset_1=get_validation_set_1(validation_path_set_1)\n    validation_dataset_2=get_validation_set_2(validation_path_set_2)\n\n    # build an interator for each dataset to access the elements of the dataset\n    iterator_train = training_dataset.make_initializable_iterator()\n    iterator_test = test_dataset.make_initializable_iterator()\n    iterator_val_1 = validation_dataset_1.make_initializable_iterator()\n    iterator_val_2 = validation_dataset_2.make_initializable_iterator()\n\n    # get the features (x) and labels (y) from the dataset\n    x_train, y_train = iterator_train.get_next()\n    x_test, y_test = iterator_test.get_next()\n    x_val_1, y_val_1 = iterator_val_1.get_next()\n    x_val_2, y_val_2 = iterator_val_2.get_next()\n\n\n    x_train_copy=tf.identity(x_train,name=None) \n    y_train_copy=tf.identity(y_train,name=None)\n\n\n    W1=tf.get_variable('weight_1',shape=[29,20],initializer=weight_initializer)\n    b1=tf.get_variable('bias_1',shape=[20],initializer=bias_initializer)\n\n    W2=tf.get_variable('weight_2',shape=[20,8],initializer=weight_initializer)\n    b2=tf.get_variable('bias_2',shape=[8],initializer=bias_initializer)\n\n    W3=tf.get_variable('weight_3',shape=[8,20],initializer=weight_initializer)\n    b3=tf.get_variable('bias_3',shape=[20],initializer=bias_initializer)\n\n    W4=tf.get_variable('weight_4',shape=[20,29],initializer=weight_initializer)\n\n    predictions=feed_forward_propagations(x_train_copy)\n\n    loss=tf.reduce_mean(tf.square(predictions - x_train_copy))\n\n    optimizer= tf.train.AdamOptimizer(learning_rate=0.001, name='adam_optimizer').minimize(loss)\n\n    val_loss_op=tf.reduce_mean(tf.square(feed_forward_propagations(x_test)-x_test))\n\n    predictions_val_1=feed_forward_propagations(x_val_1)\n    predictions_val_2=feed_forward_propagations(x_val_2)\n\n        ### session started up ###\n    with tf.Session(graph=training_graph) as sess:    \n\n        sess.run(tf.global_variables_initializer())\n        t = time.process_time()\n        print('\\n\\n\\nBegin training...')\n        for epoch in range(0, num_epoch):  \n\n            sess.run(iterator_train.initializer)\n            sess.run(iterator_test.initializer)\n\n            temp_loss=0\n\n            for iter_nr in range(n_batches-1): \n\n                _, l=sess.run((optimizer, loss))\n                temp_loss+=l\n\n                sys.stdout.write('\\r>> Training on batch_nr: %i\/%i' % (iter_nr+1, n_batches))\n                sys.stdout.flush()\n\n                if iter_nr>1 and iter_nr%eval_after==0:\n                    val_loss=sess.run(val_loss_op)\n                    print('\\n>> epoch_nr: %i, training_loss: %.5f , validation_loss: %.5f' %(epoch+1, \n                            (temp_loss\/eval_after),val_loss))\n\n                    temp_loss=0\n\n\n        print('\\n\\nResult of the evaluation on the test set: \\n')\n\n        # Test the model after the training is complete with the test dataset\n        sess.run(iterator_val_1.initializer)\n        sess.run(iterator_val_2.initializer)\n\n        predictions_val_1,x_val_1,y_val_1=sess.run((predictions_val_1,x_val_1,y_val_1))\n        predictions_val_2,x_val_2,y_val_2=sess.run((predictions_val_2,x_val_2,y_val_2))\n        ","8fcea045":"print(\"\\n\\n Time elapsed in process \",(time.process_time() - t)\/60)","52ab5208":"import matplotlib.pyplot as plt\nimport seaborn as sns","9c56b3ed":"mse = np.mean(np.power(x_val_1 - predictions_val_1, 2), axis=1)\nerror_df_test = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_val_1.ravel()})\nerror_df_test.describe()","f17cd7a5":"fig = plt.figure(figsize=(12,5))\nax = plt.subplot(1,2,1)\nnormal_error_df = error_df_test[(error_df_test['true_class']== 0) & (error_df_test['reconstruction_error'] < 10)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=200)\nplt.title(\"Mse spread on normal transaction data\")\nplt.xlabel(\"mse error\")\nplt.ylabel(\"Total number of transactions\")\n\nax = plt.subplot(1,2,2)\nfraud_error_df = error_df_test[error_df_test['true_class'] == 1]\n_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=100)\nplt.title(\"Mse spread on fraud transaction data\")\nplt.xlabel(\"mse error\")\nplt.ylabel(\"Total number of transactions\")\n","e24641ee":"error_df_test[\"predicted_class\"]=[1 if x > 0.001 else 0 for x in error_df_test[\"reconstruction_error\"]]","1adc0628":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)","91ff4b46":"fpr, tpr, thresholds = roc_curve(error_df_test.true_class, error_df_test.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();\n","8c88c154":"print(classification_report(error_df_test[\"true_class\"],error_df_test[\"predicted_class\"]))","1d9481f2":"LABELS = [\"Normal\", \"Fraud\"]\ny_pred = [1 if e > 0.001 else 0 for e in error_df_test.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df_test.true_class,error_df_test.predicted_class)\nplt.figure(figsize=(8, 8))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","885d61ed":"mse = np.mean(np.power(x_val_2 - predictions_val_2, 2), axis=1)\nerror_df_test = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_val_2.ravel()})\nerror_df_test.describe()","a0ce3b77":"fig = plt.figure(figsize=(12,5))\nax = plt.subplot(1,2,1)\nnormal_error_df = error_df_test[(error_df_test['true_class']== 0) & (error_df_test['reconstruction_error'] < 10)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=200)\nplt.title(\"Mse spread on normal transaction data\")\nplt.xlabel(\"mse error\")\nplt.ylabel(\"Total number of transactions\")\n\nax = plt.subplot(1,2,2)\nfraud_error_df = error_df_test[error_df_test['true_class'] == 1]\n_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=100)\nplt.title(\"Mse spread on fraud transaction data\")\nplt.xlabel(\"mse error\")\nplt.ylabel(\"Total number of transactions\")\n","a7b9be76":"error_df_test[\"predicted_class\"]=[1 if x > 0.001 else 0 for x in error_df_test[\"reconstruction_error\"]]","26cd3d7d":"print(classification_report(error_df_test[\"true_class\"],error_df_test[\"predicted_class\"]))","fbecade8":"LABELS = [\"Normal\", \"Fraud\"]\ny_pred = [1 if e > 0.001 else 0 for e in error_df_test.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df_test.true_class,error_df_test.predicted_class)\nplt.figure(figsize=(8, 8))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","ed4908be":"# Analysis on validation set 1","1354be30":"# Spliting","91da2898":"A TFRecord file stores your data as a sequence of binary strings. This means you need to specify the structure of your data before you write it to the file. Tensorflow provides two components for this purpose: tf.train.Example and tf.train.SequenceExample. You have to store each sample of your data in one of these structures, then serialize it and use a tf.python_io.TFRecordWriter to write it to disk.","12bba0f0":"# TF Record writer","f6da2379":"# Analysis on validation set 2","ef6f33a1":"So guys my above kernel explains why I have chosen auto-encoder as for classifying fraud cases but as you can see that training of this model requires lot of computation time atleast 10 minutes on kaggle's gpu.\nwe have to move towards some faster solution not to save our training time but to save company resources as well sometimes even in kaggle competition time contsraint issues can be solved if training can be done much more faster ways\n\n* but is just not about winining kaggle competition it is about real time working (model deployment should be done in Tensorflow)\n\n* because in the end it's all about scalability ,  request response in real-time and model's reliablity over it's metrics","d8ba55c6":"### Splitting Dataset as per requirement for training this auto-encoder (reason of split already explained in my earlier notebooks link given above)","f7308b75":"To start with, it makes easy to combine multiple datasets and integrates seamlessly with the data import and preprocessing functionality provided by the library. Especially for datasets that are too large to be stored fully in memory this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed.","01d33b1f":"### let's start with explaining what advance methods we will be using and it's code implication part \n### so that you won't miss any of it \n\n1. #### Step 1 : Script to split data in train test and validation data \n1. #### Step 2 : Conversion of data dirs to Tf records (data engg)\n1. #### Step 3 : Using tf.data api for construction of data input pipeline (data engg)\n1. #### Step 4 : tf.flags and namescopes are indeed for naming of layers hyper-parameter initialization \n1. #### Step 5 : Some more methods how we can speed up cpu gpu scheduling (feeding Steriods)","ea68a9c0":"If you are working with large datasets, using a binary file format for storage of your data can have a significant impact on the performance of your import pipeline and as a consequence on the training time of your model. Binary data takes up less space on disk, takes less time to copy and can be read much more efficiently from disk. This is especially true if your data is stored on spinning disks, due to the much lower read\/write performance in comparison with SSDs.","ac428ad3":"![](http:\/\/digital-thinking.de\/wp-content\/uploads\/2019\/07\/tensorflow.gif)\n### with tensorflow records gpu utilization (apart from that we use other methods)to control cpu and gpu to make this parallelization process to achieve next level","b1a8bd78":"### we will be first writing script to split our dataset in 3 parts train , test , validation after that it will be converted in Tf records","e590c430":"![](http:\/\/digital-thinking.de\/wp-content\/uploads\/2019\/07\/KerasGPU.gif)\n#### with keras gpu utlization it shows gpu is waiting for something but what it is actually is the data which need to get preprocessed first","79cbcc4b":"![](https:\/\/pbs.twimg.com\/media\/D_6npzuUIAA3WZG.jpg)","b2ad907b":"# Work Extension https:\/\/www.kaggle.com\/rohandx1996\/v-auto-encoder-vs-frauds-on-imbalance-data-wins\n\n### so here in this kernel i would like demonstrate some advance methods of Tensorflow framework to fasten up training process with parallelization of cpu gpu schedulling with primary focus on data engg (tf records) and tf.data api (data input pipelines)","99633baa":"# What are tf records ? how it help us to optimize computaiton power ?"}}