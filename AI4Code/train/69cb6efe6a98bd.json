{"cell_type":{"895a1a3a":"code","c3838755":"code","72f6695e":"code","1ccd8936":"code","b24e14d1":"code","3048c96d":"code","bef9aa29":"code","68e9de5d":"code","44a43ee3":"code","eb48c8a6":"code","c7c152ce":"markdown","2aef583b":"markdown","e8228d65":"markdown","d258fb9d":"markdown","b3e93755":"markdown","7e860b3c":"markdown","f1780ce6":"markdown","2dd2d61d":"markdown","55e813f7":"markdown"},"source":{"895a1a3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3838755":"import gym  \nimport numpy as np\nfrom collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nimport random\nimport matplotlib.pyplot as plt","72f6695e":"env = gym.make(\"Taxi-v3\").env \nenv.render() ","1ccd8936":"print('State Space:',env.observation_space)","b24e14d1":"print('Action Space:',env.action_space) #Aksiyon say\u0131s\u0131","3048c96d":"State = env.encode(3,1,2,3) # Taxi row, taxi column, passenger index, destination  \nprint(State)","bef9aa29":"env.s = State # to show the state 331 \nenv.render()","68e9de5d":"env.reset()\n\ntime_step = 0 #Passed time in while loop \ntotal_reward = 0 \nlist_visualize = []\nwhile True: #To recognize the environment with a while loop\n  time_step +=1\n  #Choose Action\n  action = env.action_space.sample() \n  #Perform Action & Get reward\n  state, reward, done, _ = env.step(action) \n\n  #Total Reward\n  total_reward +=reward\n  list_visualize.append({\"frame\":env.render(),\n                         \"state\":state,\"action\":action,\n                         \"reward\":reward,\"total_reward\":total_reward\n  \n  \n  })\n\n\n  env.render()\n  #Visualization\n\n  if done:\n     break","44a43ee3":"# Q table\nq_table = np.zeros([env.observation_space.n,env.action_space.n])\n\n#Hyper Parametrelerinin olu\u015fturulmas\u0131\nalpha = 0.1\ngamma = 0.9\nepsilon = 0.1\n\n#Plotting Metrix\n\nreward_list = []\ndroputs_list = []\n\n#Episode \n\nepisode_number = 20000\nfor i in range(1,episode_number):\n\n  #In\u0131tilaze Enviroment\n  state = env.reset() \n    \n  reward_count = 0 \n  dropouts = 0\n\n\n  while True:\n\n    #Exploit & Explore to find action\n    #epsilon=0.1 => % 10 explore %90 exploit\n    if random.uniform(0,1) < epsilon:  \n      action = env.action_space.sample()  \n    else:\n      action = np.argmax(q_table[state]) \n      \n\n    #Action process and take reward \/ observation\n    next_state, reward, done, _ = env.step(action) #step metodu action\u0131 ger\u00e7ekle\u015ftiren metottur. Bu 4 de\u011fi\u015fken d\u00f6nd\u00fcr\u00fcr. _ infodur ve kullan\u0131lmayaca\u011f\u0131 i\u00e7in bu \u015fekilde tan\u0131mlanm\u0131\u015ft\u0131r. \n    \n\n    #Q learning function\n    old_value = q_table[state,action] \n    next_max = np.max(q_table[next_state]) \n    next_value = (1-alpha)*old_value + alpha*(reward + gamma*next_max) \n\n    #Q table update\n    q_table[state,action] = next_value \n \n\n\n    #Update State\n    state = next_state # State'i next state e\u015fitledik \u00e7\u00fcnk\u00fc bir sonraki ad\u0131mda next state state olacak.\n\n\n\n    #find wrong drouputs\n    if reward == -10:\n      dropouts += 1\n\n    reward_count += reward \n\n    if done:\n      break\n\n    if i%10 == 0:\n\n      droputs_list.append(dropouts)\n      reward_list.append(reward_count)\n      print(\"Episode: {}, reward {}, wrong dropout {}\".format(i,reward_count,dropouts))\n\n    ","eb48c8a6":"fig ,axs = plt.subplots(1,2)\naxs[0].plot(reward_list)\naxs[0].set_xlabel(\"episode\")\naxs[0].set_ylabel(\"reward\")\n\naxs[1].plot(droputs_list)\naxs[1].set_xlabel(\"episode\")\naxs[1].set_ylabel(\"dropouts\")\n\naxs[0].grid(True)\naxs[1].grid(True)\n\nplt.show()","c7c152ce":"**Source:** https:\/\/github.com\/openai\/gym\/blob\/master\/gym\/envs\/toy_text\/taxi.py\n\n<img src=\"https:\/\/storage.googleapis.com\/lds-media\/images\/Reinforcement_Learning_Taxi_Env.width-1200.png\" width=\"500px\">\n\n\n\n**Description:**\n* There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). \n* When the episode starts, the taxi starts off at a random square and the passenger is at a random location. \n* The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n\n**Observations:** \n* There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n\n**Passenger locations:**\n* 0: R(ed)\n* 1: G(reen)\n* 2: Y(ellow)\n* 3: B(lue)\n* 4: in taxi\n\n**Destinations:**\n* 0: R(ed)\n* 1: G(reen)\n* 2: Y(ellow)\n* 3: B(lue)\n        \n**Actions:**\nThere are 6 discrete deterministic actions:\n* 0: move south\n* 1: move north\n* 2: move east \n* 3: move west \n* 4: pickup passenger\n* 5: dropoff passenger\n\n**Rewards:** \n*There is a default per-step reward of -1,except for delivering the passenger, which is +20,\nor executing \"pickup\" and \"drop-off\" actions illegally, which is -10.*\n    \n**Rendering:**\n* blue: passenger\n* magenta: destination\n* yellow: empty taxi\n* green: full taxi\n* other letters (R, G, Y and B): locations for passengers and destinations","2aef583b":"### If you like this kernel, Please Upvote :) Thanks\n\n<img src=\"https:\/\/media1.giphy.com\/media\/l0ExvuzJGJNZJZ47S\/giphy.gif?cid=790b76115cc05331372f4d64593e8962\" width=\"500px\">\n\n","e8228d65":"## Training Process","d258fb9d":"### Action & State Numbers and visualization of certain states ","b3e93755":"# Visualization Rewards & Dropouts","7e860b3c":"### Environment Properties","f1780ce6":"### Create & Visualization Environment ","2dd2d61d":"### \u0130mport Libraries","55e813f7":"### Recognize to Environment with Agent"}}