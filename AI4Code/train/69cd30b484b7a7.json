{"cell_type":{"0c6a42ab":"code","6475fa62":"code","6880b55e":"code","ad9d8102":"code","eef8c9ab":"code","9b7b787c":"code","fb289190":"code","7ad98e82":"code","73ebaa48":"code","d14c0a5e":"code","b3b899a2":"code","448e84f3":"code","dc31961a":"code","2a757e43":"code","c2acc6cc":"code","6a7dbe06":"code","2f148c5c":"code","59f1c31f":"code","226a6bc0":"code","5d3b6577":"code","765393ab":"code","b886b324":"code","bd8c697c":"code","9f256335":"code","6158cd9f":"code","c5b4d7fc":"code","e0efada8":"code","0c318b1e":"markdown","1d1f4311":"markdown","fcd9facf":"markdown","261bb1ff":"markdown","f8b6c447":"markdown","ee4e2256":"markdown","7a24a524":"markdown","54c35691":"markdown","5087cc8c":"markdown","3b4a4a4e":"markdown","e82bda81":"markdown"},"source":{"0c6a42ab":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgb_optuna\n#import dask_xgboost as xgb\n#import dask.dataframe as dd6\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm_notebook as tqdm\nfrom scipy.sparse import csr_matrix\nimport pickle\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6475fa62":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns: #columns\u6bce\u306b\u51e6\u7406\n        col_type = df[col].dtypes\n        if col_type in numerics: #numerics\u306e\u30c7\u30fc\u30bf\u578b\u306e\u7bc4\u56f2\u5185\u306e\u3068\u304d\u306b\u51e6\u7406\u3092\u5b9f\u884c. \u30c7\u30fc\u30bf\u306e\u6700\u5927\u6700\u5c0f\u5024\u3092\u5143\u306b\u30c7\u30fc\u30bf\u578b\u3092\u52b9\u7387\u7684\u306a\u3082\u306e\u306b\u5909\u66f4\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","6880b55e":"PATHS = {}\nfor store_id in ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']:\n    PATHS[store_id] = '\/kaggle\/input\/m5-all-data\/df_' + store_id + '.pkl'\n    \nPATHS2 = {}\nPATHS2['CA_1'] = '\/kaggle\/input\/binary-challenge-evaluation-ca-1-2\/binary_pred_CA_1.pkl'\nPATHS2['CA_2'] = '\/kaggle\/input\/binary-challenge-evaluation-ca-1-2\/binary_pred_CA_2.pkl'\nPATHS2['CA_3'] = '\/kaggle\/input\/binary-challenge-evaluation-ca-3-4\/binary_pred_CA_3.pkl'\nPATHS2['CA_4'] = '\/kaggle\/input\/binary-challenge-evaluation-ca-3-4\/binary_pred_CA_4.pkl'\nPATHS2['TX_1'] = '\/kaggle\/input\/binary-challenge-evaluation-tx-1-2-3\/binary_pred_TX_1.pkl'\nPATHS2['TX_2'] = '\/kaggle\/input\/binary-challenge-evaluation-tx-1-2-3\/binary_pred_TX_2.pkl'\nPATHS2['TX_3'] = '\/kaggle\/input\/binary-challenge-evaluation-tx-1-2-3\/binary_pred_TX_3.pkl'\nPATHS2['WI_1'] = '\/kaggle\/input\/binary-challenge-evaluation-wi-1-2-3\/binary_pred_WI_1.pkl'\nPATHS2['WI_2'] = '\/kaggle\/input\/binary-challenge-evaluation-wi-1-2-3\/binary_pred_WI_2.pkl'\nPATHS2['WI_3'] = '\/kaggle\/input\/binary-challenge-evaluation-wi-1-2-3\/binary_pred_WI_3.pkl'\n\nPATHS3 = {}\nPATHS3['CA_1'] = '\/kaggle\/input\/m5-lags-features-1to35-ca\/lags_df_1to35_CA_1.pkl'\nPATHS3['CA_2'] = '\/kaggle\/input\/m5-lags-features-1to35-ca\/lags_df_1to35_CA_2.pkl'\nPATHS3['CA_3'] = '\/kaggle\/input\/m5-lags-features-1to35-ca\/lags_df_1to35_CA_3.pkl'\nPATHS3['CA_4'] = '\/kaggle\/input\/m5-lags-features-1to35-ca\/lags_df_1to35_CA_4.pkl'\nPATHS3['TX_1'] = '\/kaggle\/input\/fork-of-m5-lags-features-1to35-tx-and-wi\/lags_df_1to35_TX_1.pkl'\nPATHS3['TX_2'] = '\/kaggle\/input\/fork-of-m5-lags-features-1to35-tx-and-wi\/lags_df_1to35_TX_2.pkl'\nPATHS3['TX_3'] = '\/kaggle\/input\/fork-of-m5-lags-features-1to35-tx-and-wi\/lags_df_1to35_TX_3.pkl'\nPATHS3['WI_1'] = '\/kaggle\/input\/fork-of-m5-lags-features-1to35-tx-and-wi\/lags_df_1to35_WI_1.pkl'\nPATHS3['WI_2'] = '\/kaggle\/input\/fork-of-m5-lags-features-1to35-tx-and-wi\/lags_df_1to35_WI_2.pkl'\nPATHS3['WI_3'] = '\/kaggle\/input\/fork-of-m5-lags-features-1to35-tx-and-wi\/lags_df_1to35_WI_3.pkl'","ad9d8102":"te2 = pd.read_pickle('..\/input\/m5-target-encoding2\/te_60.pkl')\nevent_lag = pd.read_pickle('..\/input\/kernel1f1484cf46\/event_lag_df.pkl').drop(columns=['sales', 'event_name_1','event_lag_0'])\n\ndef load_data(store_id):\n    df1 = pd.read_pickle(PATHS[store_id])\n    df2 = pd.read_pickle(PATHS2[store_id])\n    df3 = pd.read_pickle(PATHS3[store_id]).drop(columns=['store_id', 'sales']).iloc[:,:29]\n    df1 = df1.merge(df2,on=['id', 'd'],how='left')\n    df1 = df1.merge(df3,on=['id', 'd'],how='left')\n    df1 = df1.merge(te2,on=['id', 'd'],how='left')\n    df1 = df1.merge(event_lag,on=['id', 'd'],how='left')\n    del df2,df3\n    df1.id = df1.id.astype('category')\n    gc.collect()\n\n    return df1","eef8c9ab":"gc.collect()","9b7b787c":"TARGET = 'sales'\n\nbasic_features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n        'release', 'sell_price', 'price_max', 'price_min', 'price_std',\n       'price_mean', 'price_norm', 'price_nunique', 'item_nunique',\n       'price_momentum', 'price_momentum_m', 'price_momentum_y',\n       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y',\n       'tm_wm', 'tm_dw', 'tm_w_end']\n\nencoding_features = ['te_id_28', 'te_item_id_28', 'te_dept_id_28', 'te_cat_id_28',\n       'te_store_id_28', 'te_state_id_28', 'te_id_tm_dw_28',\n       'te_item_id_tm_dw_28', 'te_dept_id_tm_dw_28', 'te_cat_id_tm_dw_28',\n       'te_store_id_tm_dw_28', 'te_state_id_tm_dw_28'] + [\n       'te_id_60', 'te_item_id_60',\n       'te_dept_id_60', 'te_cat_id_60', 'te_store_id_60', 'te_state_id_60',\n       'te_id_tm_dw_60', 'te_item_id_tm_dw_60', 'te_dept_id_tm_dw_60',\n       'te_cat_id_tm_dw_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60']\n\nlag_features = [\n        'sales_lag_28','sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32',\n        'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36','sales_lag_37',\n        'sales_lag_38', 'sales_lag_39', 'sales_lag_40','sales_lag_41', 'sales_lag_42', \n        'rolling_mean_7', 'rolling_std_7','rolling_mean_14', 'rolling_std_14',\n        'rolling_mean_30','rolling_std_30', 'rolling_mean_60', 'rolling_std_60',\n        'rolling_mean_180', 'rolling_std_180',]\n\nday_by_day = ['sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_4',\n       'sales_lag_5', 'sales_lag_6', 'sales_lag_7', 'sales_lag_8',\n       'sales_lag_9', 'sales_lag_10', 'sales_lag_11', 'sales_lag_12',\n       'sales_lag_13', 'sales_lag_14', 'sales_lag_15', 'sales_lag_16',\n       'sales_lag_17', 'sales_lag_18', 'sales_lag_19', 'sales_lag_20',\n       'sales_lag_21', 'sales_lag_22', 'sales_lag_23', 'sales_lag_24',\n       'sales_lag_25', 'sales_lag_26', 'sales_lag_27']\n\nrecursive_features =['rolling_mean_tmp_1_7','rolling_mean_tmp_1_14',\n    'rolling_mean_tmp_1_30','rolling_mean_tmp_1_60',\n    'rolling_mean_tmp_7_7','rolling_mean_tmp_7_14',\n    'rolling_mean_tmp_7_30','rolling_mean_tmp_7_60',\n    'rolling_mean_tmp_14_7','rolling_mean_tmp_14_14',\n    'rolling_mean_tmp_14_30','rolling_mean_tmp_14_60']\n\n\t\n\nadditional_features = ['binary_pred'] + ['event_lag_1','event_lag_2','event_lag_3','event_lag_4','event_lag_5','event_lag_6',\n     'event_lag_-7','event_lag_-6','event_lag_-5','event_lag_-4','event_lag_-3','event_lag_-2','event_lag_-1'] + day_by_day\n\nremove_features = ['store_id', 'state_id', \n                   'te_store_id_28', 'te_state_id_28', 'te_store_id_tm_dw_28', 'te_state_id_tm_dw_28',\n                   'te_store_id_60', 'te_state_id_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60',\n                   'snap_CA', 'snap_TX', 'snap_WI']\n\nuse_enc_feat = True\nuse_lag_feat = True\nuse_rec_feat = False\nuse_add_feat = True\n\nfeature = basic_features\nif use_enc_feat:\n    feature += encoding_features\nif use_lag_feat:\n    feature += lag_features\nif use_rec_feat:\n    feature += recursive_features\nif use_add_feat:\n    feature += additional_features\n    \nfeature = [i for i in feature if i not in remove_features]\n    \nlen(feature)","fb289190":"feature","7ad98e82":"# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb(train, val,test, features, custom_loss, custom_eval, optuna_params={}, use_custom=True):\n\n    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n    del train\n    gc.collect()\n    \n    val_set = lgb.Dataset(val[features], val[TARGET].values, free_raw_data=False, params={'data_type':'validation'})\n    del val\n    gc.collect()\n    \n    test_set = lgb.Dataset(test[features], test[TARGET].values, free_raw_data=False, params={'data_type':'test'})\n    del test\n    gc.collect()\n\n    if use_custom:\n        params = {\n            'boosting_type': 'gbdt',\n            'first_metric_only': True,\n            'objective': 'custom',\n            'metric': 'custom',\n#             'n_jobs': -1,\n#             'seed': 42,\n#             'lambda_l1': 0.011947955379673579,\n#             'lambda_l2': 0.0002267728823544454,\n#             'num_leaves': 31,\n#             'feature_fraction': 0.45199999999999996,\n#              'bagging_fraction': 1.0,\n#              'bagging_freq': 0,\n#              'min_child_samples': 50,\n#             'learning_rate': 0.1,\n#             'n_estimators': 20,\n#             'bagging_fraction': 0.75,\n#             'bagging_freq': 10, \n#             'colsample_bytree': 0.75\n        }\n        params.update(optuna_params)\n        \n        model = lgb.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'], \n                          verbose_eval = 10, fobj = custom_loss, feval = custom_eval)\n    else:\n        params = {\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'first_metric_only': True,\n        'tweedie_variance_power': 1.1,\n        'metric': 'custom',\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n        params.update(optuna_params)\n\n        model = lgb.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n                          verbose_eval = 10,feval = custom_eval)\n    return model\n\n# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb_no_early_stopping(train, features, custom_loss, custom_eval, optuna_params={}, use_custom=True, num_boost_round = 200):\n\n    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n    del train\n    gc.collect()\n\n    if use_custom:\n        params = {\n            'boosting_type': 'gbdt',\n            'first_metric_only': True,\n            'objective': 'custom',\n            'metric': 'rmse',\n#             'n_jobs': -1,\n#             'seed': 42,\n#             'learning_rate': 0.1,\n# #             'n_estimators': 20,\n#             'bagging_fraction': 0.75,\n#             'bagging_freq': 10, \n#             'colsample_bytree': 0.75\n        }\n        params.update(optuna_params)\n        \n        model = lgb.train(params, train_set, valid_sets = [train_set], num_boost_round = num_boost_round, verbose_eval = 10, fobj = custom_loss)\n    else:\n        params = {\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'first_metric_only': True,\n        'tweedie_variance_power': 1.1,\n        'metric': 'custom',\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n        params.update(optuna_params)\n\n        model = lgb.train(params, train_set, num_boost_round = num_boost_round, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n                          verbose_eval = 10,feval = custom_eval)\n    return model\n\n# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb_optuna(train, val,test, features, custom_loss, custom_eval, use_custom=True):\n\n    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n    del train\n    gc.collect()\n    \n    val_set = lgb.Dataset(val[features], val[TARGET].values, free_raw_data=False, params={'data_type':'validation'})\n    del val\n    gc.collect()\n    \n    test_set = lgb.Dataset(test[features], test[TARGET].values, free_raw_data=False, params={'data_type':'test'})\n    del test\n    gc.collect()\n    \n    best_params, history = {}, []\n\n    if use_custom:\n        params = {\n            'boosting_type': 'gbdt',\n            'first_metric_only': True,\n            'objective': 'custom',\n            'metric': 'wrmsse',\n            }\n        model = lgb_optuna.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'], \n                          verbose_eval = 10, fobj = custom_loss, feval = custom_eval,best_params=best_params,tuning_history=history)\n    else:\n        params = {\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'first_metric_only': True,\n        'tweedie_variance_power': 1.1,\n        'metric': 'custom'\n        }\n        \n        model = lgb_optuna.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n                          verbose_eval = 10,feval = custom_eval,best_params=best_params,tuning_history=history)\n    return model, best_params, history","73ebaa48":"# define cost and eval functions\ndef custom_asymmetric_train(y_pred, y_true):\n    y_true = y_true.get_label()\n    a = 1.15\n    b = 1\n    residual = (y_true - y_pred).astype(\"float\")\n    grad = np.where(residual < 0, -2 * residual * b, -2 * residual * a)\n    hess = np.where(residual < 0, 2 * b, 2 * a)\n    return grad, hess\n\ndef tweedie(y_pred, y_true):\n    p = 1.1\n    y_true = y_true.get_label()\n    grad = -y_true*y_pred**(-p) + y_pred**(1-p)\n    hess = p*y_true*y_pred**(-p-1)-(1-p)*y_pred**(-p)\n    return grad, hess\n\ndef tweedie2(y_pred, y_true):\n    p = 1.09\n    y_true = y_true.get_label()\n    grad = -y_true*np.exp(y_pred*(1-p))+np.exp(y_pred*(2-p))\n    hess = -(1-p)*y_true*np.exp(y_pred*(1-p))+(2-p)*np.exp(y_pred*(2-p))\n    return grad, hess\n\ndef tweedie3(y_pred, y_true):\n    p = y_true.get_data()['p'].values\n    print(p)\n    y_true = y_true.get_label()\n    print(y_true)\n    print(y_pred)\n    grad = np.where(p<1,-y_true + np.exp(y_pred),-y_true*np.exp(y_pred*(1-p))+np.exp(y_pred*(2-p)))\n    grad = np.where(p>1.5,-y_true*np.exp(y_pred*(1-1.5))+np.exp(y_pred*(2-1.5)),grad)\n    hess = np.where(p<1,np.exp(y_pred),-(1-p)*y_true*np.exp(y_pred*(1-p))+(2-p)*np.exp(y_pred*(2-p)))\n    hess = np.where(p>1.5,-y_true*np.exp(y_pred*(1-1.5))+np.exp(y_pred*(2-1.5)),hess)\n    print(grad,hess)\n    return grad, hess\n\ndef tweedie_sum(y_pred, y_true):\n    p1 = 1.1\n    p2 = 1.5\n    y_true = y_true.get_label()\n    grad = -y_true*np.exp(y_pred*(1-p1))+np.exp(y_pred*(2-p1))\n    grad += -y_true*np.exp(y_pred*(1-p2))+np.exp(y_pred*(2-p2))\n    hess = -(1-p1)*y_true*np.exp(y_pred*(1-p1))+(2-p1)*np.exp(y_pred*(2-p1))\n    hess += -(1-p2)*y_true*np.exp(y_pred*(1-p2))+(2-p2)*np.exp(y_pred*(2-p2))\n#     print(grad,hess)\n    return grad, hess\n\n# define cost and eval functions\ndef custom_asymmetric_train_2(y_pred, y_true):\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n    grad = np.where(y_true == 0, (1-0.5)*np.exp(y_pred), grad)\n    hess = np.where(residual < 0, 2, 2 * 1.15)\n    hess = np.where(y_true == 0, (1-0.5)*np.exp(y_pred), hess)\n\n    return grad, hess\n\ndef zero_inflated_poisson_loss(y_pred, y_true):\n    p = 0.3\n    y_true = y_true.get_label()\n    y_pred = np.exp(y_pred)\n    grad = np.where(y_true == 0, (1-p)*y_pred,-y_true + y_pred)\n    hess = np.where(y_true == 0, (1-p)*y_pred, y_pred)\n    return grad, hess\n\ndef poisson(y_pred, y_true):\n    y_true = y_true.get_label()\n    y_pred = np.exp(y_pred)\n    grad = -y_true + y_pred\n    hess = y_pred\n    return grad, hess\n\ndef custom_a(y_pred, y_true):\n    y_true = y_true.get_label()\n    d = y_pred - y_true \n    grad = np.tanh(d)\/y_true\n    hess = (1.0 - grad*grad)\/y_true\n    return grad, hess\n\ndef my_objective(y_pred, y_true):\n    y_true = y_true.get_label()\n    d = y_pred - y_true \n    grad = 2*d\n    hess = 2*d\/d\n    return grad, hess\n\ndef loglikelood(preds, train_data):\n    labels = train_data.get_label()\n    preds = 1. \/ (1. + np.exp(-preds))\n    grad = preds - labels\n    hess = preds * (1. - preds)\n    return grad, hess\n\n\ndef asymmetric_plus_tweedie(y_pred, y_true):\n    p = 1.06\n    a = 1.2\n    b = 1\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    y_pred = np.exp(y_pred)\n    grad = -y_true*y_pred**(1-p)+y_pred**(2-p)\n    grad += np.where(residual < 0, -2 * (y_true-y_pred)*y_pred * b, -2 * (y_true-y_pred)*y_pred * a)\n    hess = -(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p)\n    hess += np.where(residual < 0, -2 * (y_true-2*y_pred)*y_pred * b, -2 * (y_true-2*y_pred)*y_pred * a)\n    return grad, hess\n\n\ndef asymmetric_tweedie(y_pred, y_true):\n    p = 1.0\n    a = 1.2\n    b = 1\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    y_pred = np.exp(y_pred)\n    grad = np.where(residual < 0,(-y_true*y_pred**(1-p)+y_pred**(2-p)) * b,(-y_true*y_pred**(1-p)+y_pred**(2-p)) * a)\n    hess = np.where(residual < 0,(-(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p))*b,(-(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p))*a)\n    return grad, hess","d14c0a5e":"def link_exp(pred):\n    return np.exp(pred)\ndef link_normal(pred):\n    return pred\nlink_func_dict = {'tweedie2':link_exp,'custom_asymmetric_train':link_normal,'asymmetric_plus_tweedie':link_exp,'asymmetric_tweedie':link_exp}\nloss_func_dict = {'tweedie2':tweedie2,'custom_asymmetric_train':custom_asymmetric_train,'asymmetric_plus_tweedie':asymmetric_plus_tweedie,'asymmetric_tweedie':asymmetric_tweedie}\nLOSS_NAME = 'tweedie2'\n","b3b899a2":"def get_weight_mat(product):\n    NUM_ITEMS = len(product['id'].unique()) \n    weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n                       pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n                       np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n                       ].T\n\n    weight_mat_csr = csr_matrix(weight_mat)\n    del weight_mat; gc.collect()\n    return weight_mat_csr\n\ndef weight1_calc(product,weight_mat_csr):\n    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n    sales_train_val = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\n    sales_train_val = sales_train_val.set_index('id')\n    sales_train_val = sales_train_val.loc[product.id]\n    d_name = ['d_' + str(i+1) for i in range(1941)]\n    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n    # calculate the start position(first non-zero demand observed date) for each item \/ \u5546\u54c1\u306e\u6700\u521d\u306e\u58f2\u4e0a\u65e5\n    # 1-1914\u306eday\u306e\u6570\u5217\u306e\u3046\u3061, \u58f2\u4e0a\u304c\u5b58\u5728\u3057\u306a\u3044\u65e5\u3092\u4e00\u65e60\u306b\u3057\u30010\u30929999\u306b\u7f6e\u63db\u3002\u305d\u306e\u3046\u3048\u3067minimum number\u3092\u8a08\u7b97\n    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))\n    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n    flag = np.dot(np.diag(1\/(start_no+1)) , np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))<1\n    sales_train_val = np.where(flag,np.nan,sales_train_val)\n    # denominator of RMSSE \/ RMSSE\u306e\u5206\u6bcd\n    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)\/(1941-start_no)\n\n    del sales_train_val\n    gc.collect()\n    \n    return weight1\n\ndef weight2_calc(data_v,product,weight_mat_csr):\n    # calculate the sales amount for each item\/level\n    df_tmp = data_v[['id','sales','sell_price']]\n    df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n    df_tmp = df_tmp[product.id].values\n    weight2 = weight_mat_csr * df_tmp \n    weight2 = weight2\/np.sum(weight2)\n    \n    return  weight2\n\ndef weight2_calc_for_train(data_v,product,weight_mat_csr):\n    # calculate the sales amount for each item\/level\n    df_tmp = data_v[['id','sales','sell_price']]\n    df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n    df_tmp.amount = df_tmp.amount.astype(int)\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n    df_tmp = df_tmp[product.id].values\n    weight2 = weight_mat_csr * df_tmp \n    weight2 = weight2\/np.sum(weight2)\n    \n    return  weight2\n\ndef wrmsse(preds, data):\n    \n    # this function is calculate for last 28 days to consider the non-zero demand period\n    \n    y_true = data.get_label()\n    if use_custom_loss:\n        preds = link_func_dict[LOSS_NAME](preds)\n    \n    if data.params['data_type']=='train':\n        return 'wrmsse', 1371, False\n        weight2 = weight2_tr\n        MASK = np.isin(np.arange(train_width_date*NUM_ITEMS),lost_days)\n\n        pred_tmp = np.arange(train_width_date*NUM_ITEMS)\n        pred_tmp[MASK] = 0\n        pred_tmp[~MASK] = preds\n        preds = pred_tmp\n        \n        y_true_tmp = np.arange(train_width_date*NUM_ITEMS)\n        y_true_tmp[MASK] = 0\n        y_true_tmp[~MASK] = y_true\n        y_true = y_true_tmp\n        \n    elif data.params['data_type']=='validation':\n        weight2 = weight2_val\n    elif data.params['data_type']=='test':\n        weight2 = weight2_te\n\n    reshaped_preds = preds.reshape([-1, NUM_ITEMS]).T\n    reshaped_true = y_true.reshape([-1, NUM_ITEMS]).T    \n          \n    train = weight_mat_csr*(reshaped_preds - reshaped_true)\n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(train)\n                        ,axis=1) \/ weight1) * weight2)\n#     print(np.mean(np.sqrt(np.mean(np.square(reshaped_preds-reshaped_true),axis=1))))\n    return 'wrmsse', score, False\n\ndef wrmsse_sub(preds, y_true):\n              \n    reshaped_preds = preds.reshape([-1, NUM_ITEMS]).T\n    reshaped_true = y_true.reshape([-1, NUM_ITEMS]).T    \n    if use_custom_loss:\n        preds = link_func_dict[LOSS_NAME](preds)\n        \n    train = weight_mat_csr*(reshaped_preds - reshaped_true)\n\n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(train)\n                        ,axis=1) \/ weight1) * weight2)\n\n    return 'wrmsse', score, False","448e84f3":"def rmse(preds, data):\n    y_true = data.get_label()\n    if use_custom_loss:\n        preds = link_func_dict[LOSS_NAME](preds)\n                  \n    train = preds - y_true\n    \n    score = np.mean(np.sqrt(np.mean(np.square(train))))\n    \n    return 'rmse', score, False","dc31961a":"def metrics(preds, data):\n    \"\"\"\u8907\u6570\u306e\u8a55\u4fa1\u6307\u6a19\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\"\"\"\n    return [\n        wrmsse(preds, data),\n        rmse(preds, data)\n    ]","2a757e43":"use_custom_loss = True\n\ntest_start_date = 1941 - 28\ntest_end_date = 1941\n\ntrain_width_date = 365 * 5\nval_width_date = 28\nshift_width_date = 28\nmin_train_date = 0\n\nslide_list = []\nfor i in range(test_start_date-1,1,-shift_width_date):\n    end_date = i\n    split_date = end_date - val_width_date\n    start_date = split_date - train_width_date\n    if start_date < min_train_date:\n        break\n    slide_list.append([start_date,split_date,end_date])","c2acc6cc":"# 5 times validation for fast notebook\nslide_list = slide_list[:5]","6a7dbe06":"slide_list","2f148c5c":"%%time\n\nproduct = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nproduct = product[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()","59f1c31f":"STORE_IDS = list(product.store_id.unique())","226a6bc0":"# store_id = 'CA_1'\n# start_date, split_date, end_date = slide_list[0] \n# print('start_date, split_date, end_date:',start_date, split_date, end_date)\n# print('store_id:',store_id)\n# print('load dataset')\n# df = load_data(store_id)\n# day_mask = (df.d>=start_date)&(df.d<split_date)\n# train = df[day_mask]\n# train_ids = train.id.unique()\n# NUM_ITEMS = len(train_ids)\n\n# day_mask = (df.d>=split_date)&(df.d<=end_date)\n# val = df[day_mask]\n# val = val[val.id.isin(train_ids)]\n# day_mask = (df.d>=test_start_date)&(df.d<=test_end_date)\n# test = df[day_mask]\n# del df\n# gc.collect()\n\n# test_tmp = test[test.id.isin(train_ids)]\n# product_tmp = product[product.id.isin(train_ids)]\n\n# print('calc weight')\n# weight_mat_csr = get_weight_mat(product_tmp)\n# weight1 = weight1_calc(product_tmp,weight_mat_csr)\n# weight2_te = weight2_calc(test_tmp,product_tmp,weight_mat_csr)\n# weight2_val = weight2_calc(val,product_tmp,weight_mat_csr)\n","5d3b6577":"params = {\n\"lambda_l1\":0.0000013771438547444856,\n\"lambda_l2\":1.8704380943506742,\n\"num_leaves\":27,\n\"feature_fraction\":0.8,\n\"bagging_fraction\":0.8154703815794264,\n\"bagging_freq\":1,\n\"min_child_samples\":20\n}","765393ab":"# for PREDICT_DAY in range(1,29): \n# #        tmp_lag = day_by_day_lag_features[PREDICT_DAY-1:]\n#     remove_lag = []\n#     for i in range(1, PREDICT_DAY):\n#         remove_lag.append('sales_lag_{}'.format(i))\n#     new_features = list(set(feature) - set(remove_lag))\n#     print('train model; day', PREDICT_DAY)\n#     print(remove_lag)\n#     model = run_lgb(train,val,test_tmp, new_features + ['snap_' + store_id.split('_')[0]], loss_func_dict[LOSS_NAME], wrmsse, optuna_params=params)\n#    # model = run_lgb_no_early_stopping(train, new_features, loss_func_dict[LOSS_NAME], wrmsse, num_boost_round = num_boost_rounds[store_id])\n#     model_name = 'lgb_model_'+store_id+'_'+str(PREDICT_DAY)+'.bin'\n#     pickle.dump(model, open(model_name, 'wb'))\n#     del model\n","b886b324":"# num_boost_rounds = {\n#     'CA_1': 250,\n#     'CA_2': 50,\n#     'CA_3': 50,\n#     'CA_4': 100,\n#     'TX_1': 250,\n#     'TX_2': 50,    \n#     'TX_3': 50,\n#     'WI_1': 75,\n#     'WI_2': 75,\n#     'WI_3': 100,\n# }","bd8c697c":"store_id = 'CA_1'\nsub_start_date = 1942\nsub_end_date = 1969\nprint('store_id:',store_id)\nprint('load dataset')\ndf = load_data(store_id)\nday_mask = (df.d<sub_start_date)\ntrain = df[day_mask]\ntrain_ids = train.id.unique()\nNUM_ITEMS = len(train_ids)\ngc.collect()\nday_mask = (te2.d>=sub_start_date)&(te2.d<=sub_end_date)\nall_pred = te2[day_mask][['id','d']]\nall_pred['sales'] = 0\n\nfor PREDICT_DAY in range(1,29): \n#        tmp_lag = day_by_day_lag_features[PREDICT_DAY-1:]\n    remove_lag = []\n    for i in range(1, PREDICT_DAY):\n        remove_lag.append('sales_lag_{}'.format(i))\n    new_features = list(set(feature) - set(remove_lag))\n    print('train model; day', PREDICT_DAY)\n#     model = run_lgb(train,val,test_tmp, new_features, loss_func_dict[LOSS_NAME], wrmsse)\n    model = run_lgb_no_early_stopping(train, new_features + ['snap_' + store_id.split('_')[0]], loss_func_dict[LOSS_NAME], wrmsse, optuna_params=params, num_boost_round = 300)\n    day_mask = df.d == sub_start_date + PREDICT_DAY - 1\n    pred1 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=100)\n    pred2 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=200)\n    pred3 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=300)\n    pred = (pred1 + pred2 + pred3)\/3.0\n    pred = link_func_dict[LOSS_NAME](pred)\n    day_mask2 = all_pred.d == sub_start_date + PREDICT_DAY - 1\n    all_pred.loc[(all_pred.id.isin(train_ids))&(day_mask2),'sales'] += pred\n\n    model_name = 'lgb_model_'+store_id+'_'+str(PREDICT_DAY)+'.bin'\n    pickle.dump(model, open(model_name, 'wb'))\n    del model","9f256335":"all_pred","6158cd9f":"sub = all_pred[['d','sales','id']].pivot(index='id', columns='d', values='sales').reset_index()\nsub.columns = ['id'] + ['F'+str(i) for i in range(1,29)]","c5b4d7fc":"submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')[['id']]\nsubmission = submission.merge(sub, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission.csv', index=False)","e0efada8":"submission[submission.F1!=0]","0c318b1e":"## Submission","1d1f4311":"## Make Parameter Grid","fcd9facf":"## Training","261bb1ff":"### Training Parameters","f8b6c447":"## Preparation","ee4e2256":"## Use Files","7a24a524":"## Define Features","54c35691":"## Loss Function","5087cc8c":"## Evaluation Function","3b4a4a4e":"## Model","e82bda81":"## import"}}