{"cell_type":{"738c1a54":"code","675649b3":"code","4ae42fd8":"code","d2b9ea15":"code","e92a30ee":"code","0936aa32":"code","74068254":"code","3b1130c4":"code","1a5b756a":"code","cfcdf984":"code","263bcb6f":"code","3a55ae03":"code","d1f92507":"code","9374541a":"code","1ea25c87":"code","1f3760aa":"code","23d6ee61":"code","585b1ec6":"code","b5b0f57a":"code","52f2fe2d":"code","716db27c":"code","9a0966c0":"code","8f24e1dc":"code","515a156c":"code","6d3a06d9":"code","59e46772":"code","cc8c8583":"code","bb330fed":"code","55a78ebc":"code","f58961b1":"code","f24ad73d":"code","6ff18d70":"code","847cb1da":"code","5076fa95":"code","a599525a":"code","f9d463ac":"code","b60f039e":"code","8ba064db":"code","92429106":"code","9288703b":"code","671781bd":"code","241efb67":"code","176d9338":"code","02b9e18a":"code","f05abf68":"code","c1613667":"code","0de290c3":"code","77dae7db":"code","7e98f881":"code","8f124fd1":"code","3083f593":"code","a0e5f163":"code","683b51af":"code","82ee18be":"code","ec4973f8":"code","2d5db7bc":"code","42b0f454":"code","1b1915f0":"markdown","26a0c7fa":"markdown","69bfb468":"markdown","15c2234d":"markdown","c750bc1e":"markdown","6acbd429":"markdown","8961b0ac":"markdown","7cc16ad4":"markdown","f3a7e6a5":"markdown","ebcb6d51":"markdown","269a086f":"markdown"},"source":{"738c1a54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","675649b3":"df=pd.read_csv('\/kaggle\/input\/news-article\/news_articles.csv')\ndf_clicks=pd.read_csv('\/kaggle\/input\/click-data\/clicks_new - clicks.csv')","4ae42fd8":"df.head(5)","d2b9ea15":"df.shape","e92a30ee":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nimport os\nimport codecs\nfrom sklearn import feature_extraction\nimport mpld3","0936aa32":"titles=list(df['Title'])\ncontent=list(df['Content'])\ntitles[:10]","74068254":"stopwords = nltk.corpus.stopwords.words('english')\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\n\ndef tokenize_and_stem(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems\n\n\ndef tokenize_only(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    return filtered_tokens\n\n","3b1130c4":"#not super pythonic, no, not at all.\n#use extend so it's a big flat list of vocab\ntotalvocab_stemmed = []\ntotalvocab_tokenized = []\nfor i in content:\n    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize\/stem\n    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n    \n    allwords_tokenized = tokenize_only(i)\n    totalvocab_tokenized.extend(allwords_tokenized)","1a5b756a":"vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\nprint('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')","cfcdf984":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=10, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(content) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)","263bcb6f":"from sklearn.cluster import KMeans\ndef find_optimal_clusters(data, max_k):\n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        sse.append(KMeans(n_clusters=k).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')\nfind_optimal_clusters(tfidf_matrix, 10)","3a55ae03":"\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 5\n\nkm = KMeans(n_clusters=num_clusters)\n\nclusters=km.fit_predict(tfidf_matrix)\n","d1f92507":"clusters","9374541a":"import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","1ea25c87":"def plot_tsne_pca(data, labels):\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n    \n    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))\n    \n    \n    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i\/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    ax[0].set_title('PCA Cluster Plot')\n    \n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster Plot')\n    \nplot_tsne_pca(tfidf_matrix, clusters)","1f3760aa":"def get_top_keywords(data, clusters, labels, n_terms):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    for i,r in df.iterrows():\n        print('\\nCluster {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n            \nget_top_keywords(tfidf_matrix, clusters, tfidf_vectorizer.get_feature_names(), 15)","23d6ee61":"from collections import Counter\nCounter(clusters)","585b1ec6":"new_article=['KARACHI: Seven Pakistani soldiers were killed in two separate terror attacks in the restive Balochistan province, an official statement said on Tuesday. Terrorists targeted a vehicle of the Frontier Corps using improvised explosive devices in the Pir Ghaib area on Monday night, killing six Pakistan Army soldiers, said Inter-Services Public Relations (ISPR), the media wing of the Pakistani military. In a separate incident in Balochistan Kech, another soldier was killed during an exchange of fire with the militants.Resource-rich Balochistan in southwestern Pakistan borders Afghanistan and Iran, But it is also Pakistan largest and poorest province, rife with ethnic, sectarian and separatist insurgencies.']\nunseen_tfidf = tfidf_vectorizer.transform(new_article)\nkm.predict(unseen_tfidf)","b5b0f57a":"cluster0_index= []\ncluster1_index= []\ncluster2_index= []\ncluster3_index= []\ncluster4_index= []\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==0:\n        cluster0_index.append(i)\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==1:\n        cluster1_index.append(i)\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==2:\n        cluster2_index.append(i)\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==3:\n        cluster3_index.append(i)\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==4:\n        cluster4_index.append(i)\nlen(cluster2_index)","52f2fe2d":"#randomly\nimport random\nsampling0 = random.choices(cluster0_index, k=2)\nsampling1 = random.choices(cluster1_index, k=2)\nsampling2 = random.choices(cluster2_index, k=2)\nsampling3 = random.choices(cluster3_index, k=2)\nsampling4 = random.choices(cluster4_index, k=2)\nshow_list=[sampling0,sampling1,sampling2,sampling3,sampling4]\nrecommend_index=[]\nfor x in show_list:\n    for ele in x:\n        recommend_index.append(ele)\nn=0\nprint('Articles Recommended for you :) ')\nfor i in recommend_index:\n    n=n+1\n    print('Article ',n,': ',df['Title'][i])","716db27c":"#Breaking news\ndf1=df.copy()\ndf1.isna(). sum()","9a0966c0":"df1=df1.dropna(subset=['Date'])\ndf1.shape","8f24e1dc":"df1[df1['Date'] == ' Saranya Ponvannan']\ndf1=df1.drop(544)","515a156c":"import datetime\ndf1['Date'] = df1['Date'].astype('datetime64[ns]') ","6d3a06d9":"df1=df1.sort_values(by=\"Date\")\ndf1.head(5)","59e46772":"list_article=list(df1['Article_Id'])\ncluster0_date_index=[]\ncluster1_date_index=[]\ncluster2_date_index=[]\ncluster3_date_index=[]\ncluster4_date_index=[]\nfor i in cluster0_index:\n    if i in list_article:\n        cluster0_date_index.append(i)\nfor i in cluster1_index:\n    if i in list_article:\n        cluster1_date_index.append(i)\nfor i in cluster2_index:\n    if i in list_article:\n        cluster2_date_index.append(i)\nfor i in cluster3_index:\n    if i in list_article:\n        cluster3_date_index.append(i)\nfor i in cluster4_index:\n    if i in list_article:\n        cluster4_date_index.append(i)","cc8c8583":"import operator\ndict0={}\ndict1={}\ndict2={}\ndict3={}\ndict4={}\nfor i in cluster0_date_index:\n    dict0[i]=df1['Date'][i]\ndict0 = sorted(dict0.items(), key=operator.itemgetter(1))\nfor i in cluster1_date_index:\n    dict1[i]=df1['Date'][i]\ndict1 = sorted(dict1.items(), key=operator.itemgetter(1))\nfor i in cluster2_date_index:\n    dict2[i]=df1['Date'][i]\ndict2 = sorted(dict2.items(), key=operator.itemgetter(1))\nfor i in cluster3_date_index:\n    dict3[i]=df1['Date'][i]\ndict3 = sorted(dict3.items(), key=operator.itemgetter(1))\nfor i in cluster4_date_index:\n    dict4[i]=df1['Date'][i]\ndict4 = sorted(dict4.items(), key=operator.itemgetter(1))","bb330fed":"#breaking news recommendation from each cluster\nx1=dict0[-2:][0][0]\nx2=dict0[-2:][1][0]\nx3=dict1[-2:][0][0]\nx4=dict1[-2:][1][0]\nx5=dict2[-2:][0][0]\nx6=dict2[-2:][1][0]\nx7=dict3[-2:][0][0]\nx8=dict3[-2:][1][0]\nx9=dict4[-2:][0][0]\nx10=dict4[-2:][1][0]\nlist_breaking=[x1,x2,x3,x4,x5,x6,x7,x8,x9,x10]\nn=0\nprint(\"Breaking News Recommendation:):\")\nfor i in list_breaking:\n    n=n+1\n    print('Article ',n,': ',df['Title'][i])\n","55a78ebc":"df['click_count']=list(df_clicks['Total_clicks'])\ndf.head(5)","f58961b1":"import operator\ndict_click0={}\ndict_click1={}\ndict_click2={}\ndict_click3={}\ndict_click4={}\nfor i in cluster0_index:\n    dict_click0[i]=df['click_count'][i]\ndict_click0 = sorted(dict_click0.items(), key=operator.itemgetter(1))\nfor i in cluster1_index:\n    dict_click1[i]=df['click_count'][i]\ndict_click1 = sorted(dict_click1.items(), key=operator.itemgetter(1))\nfor i in cluster2_index:\n    dict_click2[i]=df['click_count'][i]\ndict_click2 = sorted(dict_click2.items(), key=operator.itemgetter(1))\nfor i in cluster3_index:\n    dict_click3[i]=df['click_count'][i]\ndict_click3 = sorted(dict_click3.items(), key=operator.itemgetter(1))\nfor i in cluster4_index:\n    dict_click4[i]=df['click_count'][i]\ndict_click4 = sorted(dict_click4.items(), key=operator.itemgetter(1))\ndict_click4","f24ad73d":"#breaking news recommendation from each cluster\ny1=dict_click0[-2:][0][0]\ny2=dict_click0[-2:][1][0]\ny3=dict_click1[-2:][0][0]\ny4=dict_click1[-2:][1][0]\ny5=dict_click2[-2:][0][0]\ny6=dict_click2[-2:][1][0]\ny7=dict_click3[-2:][0][0]\ny8=dict_click3[-2:][1][0]\ny9=dict_click4[-2:][0][0]\ny10=dict_click4[-2:][1][0]\nlist_famous=[y1,y2,y3,y4,y5,y6,y7,y8,y9,y10]\nn=0\nprint(\"Frequently viewed News Articles:\")\nfor i in list_famous:\n    n=n+1\n    print('Article ',n,': ',df['Title'][i])","6ff18d70":"import pickle\nobjects = []\nwith (open(\"\/kaggle\/input\/embed-mean\/emeddings_bert_mean.txt\", \"rb\")) as openfile:\n    while True:\n        try:\n            objects.append(pickle.load(openfile))\n        except EOFError:\n            break","847cb1da":"embed_list=[]\nfor x in objects[0]:\n    emb_np=x.cpu().detach().numpy()\n    embed_list.append(emb_np)","5076fa95":"len(embed_list)","a599525a":"from sklearn.cluster import KMeans\ndef find_optimal_clusters(data, max_k):\n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        sse.append(KMeans(n_clusters=k).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')\nfind_optimal_clusters(embed_list, 10)","f9d463ac":"from sklearn.cluster import KMeans\n\nnum_clusters = 5\n\nkm = KMeans(n_clusters=num_clusters)\n\nclusters=km.fit_predict(embed_list)","b60f039e":"import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","8ba064db":"emd_arr=np.array(embed_list)","92429106":"def plot_tsne_pca(data, labels):\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n    \n    pca = PCA(n_components=2).fit_transform(data[max_items,:])\n    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:]))\n    \n    \n    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i\/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    ax[0].set_title('PCA Cluster Plot')\n    \n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster Plot')\n    \nplot_tsne_pca(emd_arr, clusters)","9288703b":"from collections import Counter\nCounter(clusters)","671781bd":"cluster0_index= []\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==0:\n        cluster0_index.append(i)\n","241efb67":"cluster0_articles=[]\nfor i in cluster0_index:\n    cluster0_articles.append(df['Content'][i])\nlen(cluster0_articles)","176d9338":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=10, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(cluster0_articles) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)","02b9e18a":"def display_scores(vectorizer, tfidf_result):\n    scores = zip(vectorizer.get_feature_names(),\n                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    for item in sorted_scores:\n        print(\"{0:50} Score: {1}\".format(item[0], item[1]))","f05abf68":"display_scores(tfidf_vectorizer, tfidf_matrix)","c1613667":"cluster1_index= []\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==1:\n        cluster1_index.append(i)\ncluster1_articles=[]\nfor i in cluster1_index:\n    cluster1_articles.append(df['Content'][i])\nlen(cluster1_articles)","0de290c3":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=10, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(cluster1_articles) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)","77dae7db":"display_scores(tfidf_vectorizer, tfidf_matrix)","7e98f881":"cluster2_index= []\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==2:\n        cluster2_index.append(i)\ncluster2_articles=[]\nfor i in cluster2_index:\n    cluster2_articles.append(df['Content'][i])\nlen(cluster2_articles)","8f124fd1":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=5, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(cluster2_articles) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)","3083f593":"display_scores(tfidf_vectorizer, tfidf_matrix)","a0e5f163":"cluster3_index= []\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==3:\n        cluster3_index.append(i)\ncluster3_articles=[]\nfor i in cluster3_index:\n    cluster3_articles.append(df['Content'][i])\nlen(cluster3_articles)","683b51af":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=5, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(cluster3_articles) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)","82ee18be":"display_scores(tfidf_vectorizer, tfidf_matrix)","ec4973f8":"cluster4_index= []\nfor i in range(0,len(clusters)-1):\n    if clusters[i]==4:\n        cluster4_index.append(i)\ncluster4_articles=[]\nfor i in cluster4_index:\n    cluster4_articles.append(df['Content'][i])\nlen(cluster4_articles)","2d5db7bc":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=5, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(cluster4_articles) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)","42b0f454":"display_scores(tfidf_vectorizer, tfidf_matrix)","1b1915f0":"BERT embeddings","26a0c7fa":"stopwords , stemming , tokenizing","69bfb468":"recommend famous news articles","15c2234d":"# without user prefrences (recommend either randomly or breaking news from each cluster )","c750bc1e":"# cluster 2","6acbd429":"# cluster 1","8961b0ac":"# Cluster 3","7cc16ad4":"# cluster 4","f3a7e6a5":"# Cluster0","ebcb6d51":"**article id's for each clusters**\n","269a086f":"* **** cluster 0:\n* **** cluster 1:\n* **** cluster 2:\n* **** cluster 3:\n* **** cluster 4:\n# Prediction"}}