{"cell_type":{"f1ee1efb":"code","7032744c":"code","09216e62":"code","7bc6fdc7":"code","9fe4eb10":"code","fe8e9bfa":"code","2d346432":"code","4f01160f":"code","51f38d40":"code","e658a758":"code","1f384221":"code","6f75f995":"code","8a54da0b":"code","226857ec":"code","3f75372d":"code","d8ea3d1d":"code","7fce0fb1":"code","9c66f514":"code","58431a28":"code","bfe014ca":"code","fb8f2f58":"code","106e1446":"code","4e988c08":"code","671dff14":"markdown","5b9d25b2":"markdown","1b973421":"markdown","5973ec98":"markdown","389b93bc":"markdown"},"source":{"f1ee1efb":"import pandas as pd","7032744c":"train = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\");\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\");\nsubmit = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv\");\n\n# Specify the maximum number of columns to display\npd.set_option('display.max_columns', 32)","09216e62":"# Check the number of data\nprint(len(train), len(test),len(submit))","7bc6fdc7":"train.head()","9fe4eb10":"test.head()","fe8e9bfa":"### Check for missing data\ny_train = train['target']\ntrain_work = train.drop('target',axis=1)\ntrain_work.head()","2d346432":"marge_data = pd.concat([train_work,test],join='inner')\nprint(len(marge_data))","4f01160f":"marge_data.isnull().sum()","51f38d40":"!pip install --quiet git+https:\/\/github.com\/pfnet-research\/xfeat.git","e658a758":"from xfeat import SelectCategorical, LabelEncoder, Pipeline, SelectNumerical, GBDTFeatureSelector, GBDTFeatureExplorer\n# Extract only categorical data\ncategorical_df = SelectCategorical().fit_transform(marge_data)\ncategorical_df.head()","1f384221":"# Extract only numerical data\nnumerical_df = SelectNumerical().fit_transform(marge_data)\nnumerical_df.head()","6f75f995":"# Label Encoding\nencoder = Pipeline([\n    SelectCategorical(),\n    LabelEncoder(output_suffix=''),\n])\n\nencoded_df = encoder.fit_transform(marge_data)\nencoded_df.head()","8a54da0b":"marge_data_encoded = pd.concat([numerical_df,encoded_df], axis=1)\nmarge_data_encoded.head()","226857ec":"X_train = marge_data_encoded[:len(train)]\nX_test = marge_data_encoded[len(train):]","3f75372d":"from functools import partial\nimport optuna\nimport lightgbm as lgb\n\nLGBM_PARAMS = {\n        'objective': 'binary',\n        'metric': 'binary_error',\n        'verbosity': -1,\n}\n\n\ndef objective(df, selector, trial):\n    selector.set_trial(trial)\n    selector.fit(df)\n    input_cols = selector.get_selected_cols()\n\n    # Set the parameters and range for Hyper Parameter Tuning.\n    lgbm_params = {\n        'num_leaves': trial.suggest_int('num_leaves', 3, 10),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n    }\n    lgbm_params.update(LGBM_PARAMS)\n\n    # Evaluate with selected columns\n    train_set = lgb.Dataset(df[input_cols], label=df['target'])\n    scores = lgb.cv(lgbm_params, train_set, num_boost_round=100, stratified=False, seed=1)\n    \n    binary_error_score = scores['binary_error-mean'][-1]\n    return 1 - binary_error_score\n\n\n# Create a feature searcher.\nselector = GBDTFeatureExplorer(\n    input_cols=X_train.columns.tolist(),\n    target_col='target',\n    fit_once=True,\n    threshold_range=(0.8, 1.0),\n    lgbm_params=LGBM_PARAMS,\n)\n\n# Hyper Parameter Tuning\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(partial(objective, pd.concat([X_train,y_train], axis=1), selector), n_trials=100)\n\n# Check the selected features.\nselector.from_trial(study.best_trial)","d8ea3d1d":"print('Selected columns:', selector.get_selected_cols())","7fce0fb1":"print(study.best_params)","9c66f514":"print(study.best_value)","58431a28":"params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_error',\n    'num_leaves': study.best_params['num_leaves'],\n    'max_depth': study.best_params['max_depth'],\n    'verbose': 0,\n}","bfe014ca":"lgb_train_data = lgb.Dataset(\ndata=X_train[selector.get_selected_cols()], \nlabel=y_train, \nfeature_name='auto'\n)\n\nlgb_cv = lgb.cv(\n    params = params,\n    train_set = lgb_train_data,\n    num_boost_round=2000,\n    stratified=True,\n    nfold = 5,\n    verbose_eval=50,\n    seed = 23)","fb8f2f58":"best_cv_score = min(lgb_cv['binary_error-mean'])\nprint(best_cv_score)","106e1446":"model = lgb.train(\n    params=params, \n    train_set = lgb_train_data\n)","4e988c08":"import numpy as np\ny_pred = model.predict(X_test[selector.get_selected_cols()])\nsub = submit\nsub['target'] =  np.where(y_pred > 0.49, 1, 0)\nsub.to_csv('.\/submission.csv', index=False)","671dff14":"### Check for missing data","5b9d25b2":"We were able to confirm that there was no missing data.","1b973421":"### Feature Engineering with Xfeat","5973ec98":"### Load Data","389b93bc":"### Feature search using Xfeat and optuna"}}