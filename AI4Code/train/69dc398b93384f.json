{"cell_type":{"6e8e2c0b":"code","df70c81f":"code","e1cad3fb":"code","87b931d6":"code","6e78e85e":"code","42a50629":"code","13ec66f6":"code","e28f0c01":"code","793ece8a":"code","e8ebc615":"code","f7fc18a7":"code","78f8952c":"code","8912dd2a":"code","5687c8e6":"code","f6864a63":"code","4ddd44ab":"code","629f5472":"code","e4226d7f":"code","e87fe123":"code","e7d7d603":"code","03ee7b5d":"code","85e91289":"markdown","335dc5c4":"markdown","ab2b5afb":"markdown","09c4401c":"markdown","13420454":"markdown","f053661f":"markdown","529c6c4b":"markdown","d8f367ac":"markdown","87b4e875":"markdown","f220277f":"markdown"},"source":{"6e8e2c0b":"# Importing Tensorflow and keras\n#Keras is built into TF 2.0\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n#Setting the Theme of the data visualizer Seaborn\nsns.set(style=\"white\",context=\"notebook\",palette=\"deep\")\n\n#Tensorflow Version\nprint(tf.version.VERSION)\nprint(tf.keras.__version__)","df70c81f":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n","e1cad3fb":"Y_train = train[\"label\"]\n\n#Dropping Label Column\nX_train = train.drop(labels=[\"label\"],axis=1)\n\n#free up some space\ndel train\n\ngraph = sns.countplot(Y_train)\n \nY_train.value_counts()","87b931d6":"#Checking for any null or missing values\nX_train.isnull().any().describe()\n\ntest.isnull().any().describe()\n","6e78e85e":"X_train = X_train\/255\ntest = test\/255","42a50629":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","13ec66f6":"Y_train = tf.keras.utils.to_categorical(Y_train, num_classes=10)\n#To enable label into hot vector. For Eg.7 -> [0,0,0,0,0,0,0,1,0,0]","e28f0c01":"X_train,X_val,Y_train,Y_val = train_test_split(X_train,Y_train,test_size=0.1,\n                                                random_state = 2)","793ece8a":"#Show some example \n\ng = plt.imshow(X_train[0][:,:,0])","e8ebc615":"#CNN Architecture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> \n                           #Flatten -> Dense -> Dropout -> Out\nmodel = tf.keras.Sequential()\n\nmodel.add(layers.Conv2D(filters=32, kernel_size=(5,5), padding='Same', \n                       activation=tf.nn.relu, input_shape = (28,28,1)))\nmodel.add(layers.Conv2D(filters=32, kernel_size=(5,5), padding='Same', \n                       activation=tf.nn.relu))\nmodel.add(layers.MaxPool2D(pool_size=(2,2)))\nmodel.add(layers.Dropout(0.25))\n\n\nmodel.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding='Same', \n                       activation=tf.nn.relu, input_shape = (28,28,1)))\nmodel.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding='Same', \n                       activation=tf.nn.relu))\nmodel.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(layers.Dropout(0.25))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256,activation=tf.nn.relu))\nmodel.add(layers.Dropout(0.25))\nmodel.add(layers.Dense(10,activation=tf.nn.softmax))","f7fc18a7":"#Defining Optimizer\n\noptimizer = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n","78f8952c":"#Compiling Model\n\nmodel.compile(optimizer = optimizer, loss='categorical_crossentropy', \n             metrics=[\"accuracy\"])","8912dd2a":"#Setting Learning rate annealer\n\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc',\n                                           patience=3,\n                                           verbose=1,\n                                           factor=0.5,\n                                           min_lr=0.00001)","5687c8e6":"epochs=20\nbatch_size = 86","f6864a63":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","4ddd44ab":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","629f5472":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","e4226d7f":"# Look at confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","e87fe123":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted :{} True :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","e7d7d603":"# predict results\nresults = model.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","03ee7b5d":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","85e91289":"###Confusion Matrix Plotting\nIt is used for grphical representation of performance of the model. It shows the performance of Model in predicting every class.","335dc5c4":"##3.Convolutional Neural Network Model","ab2b5afb":"###Normalisation","09c4401c":"#Spliting Train and test set","13420454":"### Label Encoding","f053661f":"###Data Augmentation","529c6c4b":"## 1. Importing Library","d8f367ac":"###Reshape","87b4e875":"##2.Data Preparation","f220277f":"###Model Fitting"}}