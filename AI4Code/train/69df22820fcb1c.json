{"cell_type":{"b93ba23d":"code","808b42ec":"code","bc88da29":"code","29fa2ec8":"code","047112a2":"code","a23aeb45":"code","409bcc36":"code","98d20a8f":"code","45029424":"code","b056972a":"code","f2bb5c28":"code","8073eb9f":"code","5993f9cc":"code","ba485ce7":"code","98a00799":"code","bd039952":"markdown","e0f90990":"markdown","567f406d":"markdown","3b84edd0":"markdown","c338dc19":"markdown","ae9feffb":"markdown","78ab9d77":"markdown","e746828a":"markdown","3d12a02e":"markdown","d9b2de9d":"markdown","6eb0ecc1":"markdown"},"source":{"b93ba23d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","808b42ec":"from glob import glob\nimport pandas as pdlib\n\ndef produceCombinedCSV(list_of_files):\n   \n   # Consolidate all CSV files into one object\n   result_df = pdlib.concat([pdlib.read_csv(file).add_prefix(str(list_of_files.index(file)) + '_') for file in list_of_files], axis=1).T.drop_duplicates().T\n   return result_df\n\n# Move to the path that holds our CSV files\ncsv_file_path = '\/kaggle\/input\/'\n\n# List all CSV files in the working dir\nfile_pattern = \"csv\"\n#list_of_files = [file for file in glob('*.{}'.format(file_pattern))]\nlist_of_files = glob(csv_file_path + \"*.csv\")\nprint(list_of_files)\n\ndf_consolidated_columnwise = produceCombinedCSV(list_of_files)","bc88da29":"df_consolidated_columnwise.rename(columns={'0_Unnamed: 0': 'DateTime'}, inplace=True)\ndf_consolidated_columnwise.set_index('DateTime', inplace=True)\ndf_consolidated_columnwise.head()","29fa2ec8":"from glob import glob\nimport pandas as pdlib\nfrom os import chdir\ndef produceCombinedCSV(list_of_files):\n   \n   # Consolidate all CSV files into one object\n   result_df = pdlib.concat([pdlib.read_csv(file) for file in list_of_files], keys=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19'])\n   # Convert the above object into a csv file and export\n   result_df.to_csv('\/kaggle\/working\/ConsolidateOutput_rowwise.csv')\n\n# Move to the path that holds our CSV files\ncsv_file_path = '\/kaggle\/input\/'\nchdir(csv_file_path)\n\n# List all CSV files in the working dir\nfile_pattern = \"csv\"\nlist_of_files = [file for file in glob('*.{}'.format(file_pattern))]\nprint(list_of_files)\n\nproduceCombinedCSV(list_of_files)","047112a2":"import pandas as pd\ndf_consolidated_rowwise = pd.read_csv(\"\/kaggle\/working\/ConsolidateOutput_rowwise.csv\", index_col=[0]).drop('Unnamed: 1', axis=1)\ndf_consolidated_rowwise.rename(columns={'Unnamed: 0.1': 'DateTime'}, inplace=True)\ndf_consolidated_rowwise.head()","a23aeb45":"from matplotlib import pyplot\n# load dataset\nvalues = df_consolidated_columnwise.values\n# specify columns to plot\ngroups = [0, 1, 2, 3]\ni = 1\n# plot each column\npyplot.figure(figsize=(24,12))\nfor group in groups:\n    pyplot.subplot(len(groups), 1, i)\n    pyplot.plot(values[:, group])\n    pyplot.title(df_consolidated_columnwise.columns[group], y=0.5, loc='right')\n    i += 1\npyplot.show()","409bcc36":"sample = df_consolidated_rowwise.loc[0].copy()","98d20a8f":"sample_df = sample[(sample['0'] < 100) & (sample['0'] > -100) & (sample['1'] < 100) & (sample['1'] > -100) & (sample['2'] < 100) & (sample['2'] > -100) & (sample['3'] < 100) & (sample['3'] > -100)].copy()\nsample_df.head()","45029424":"from matplotlib import pyplot\n# load dataset\nvalues = sample_df.values\n# specify columns to plot\ngroups = [1,2,3,4]\ni = 1\n# plot each column\npyplot.figure(figsize=(24,12))\nfor group in groups:\n    pyplot.subplot(len(groups), 1, i)\n    pyplot.plot(values[:, group])\n    pyplot.xticks(np.arange(0, 3000, 100)) \n    pyplot.title(sample_df.columns[group], y=0.5, loc='right')\n    i += 1\npyplot.show()","b056972a":"sample_df['0_sqr'] = np.square(sample_df['0'])\nsample_df['1_sqr'] = np.square(sample_df['1'])\nsample_df['2_sqr'] = np.square(sample_df['2'])\nsample_df['3_sqr'] = np.square(sample_df['3'])\n\nsample_df['DateTime'] = pd.to_datetime(sample_df['DateTime'])\nsample_df.set_index('DateTime', inplace=True)\nsample_df.head()","f2bb5c28":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(sample_df['0_sqr'], lags=50, alpha=1)","8073eb9f":"import statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\n\ntrain, test = sample_df['0_sqr'].iloc[0:70], sample_df['0_sqr'].iloc[70:1000]\n#train_log, test_log = np.log10(train), np.log10(test)\nmy_order = (0,0,0)\nmy_seasonal_order = (1, 1, 1, 12)","5993f9cc":"history = [x for x in train]\npredictions = list()\npredict_log=list()\nfor t in range(len(test)):\n    model = sm.tsa.SARIMAX(history, order=my_order, seasonal_order=my_seasonal_order,enforce_stationarity=False,enforce_invertibility=False)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    predict_log.append(output[0])\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\nprint('predicted=%f, expected=%f' % (output[0], obs))\n#error = math.sqrt(mean_squared_error(test_log, predict_log))\n#print('Test rmse: %.3f' % error)\n# plot\nfigsize=(24, 12)\nplt.figure(figsize=figsize)\npyplot.plot(sample_df['0_sqr'].iloc[70:1000].index, test,label='Actuals')\npyplot.plot(sample_df['0_sqr'].iloc[70:1000].index, predictions, color='red',label='Predicted')\npyplot.legend(loc='upper right')\npyplot.show()","ba485ce7":"def finding_first_fault(df):\n    #Noise Removal\n    sample_df = df[(df['0'] < 100) & (df['0'] > -100) & (df['1'] < 100) & (df['1'] > -100) & (df['2'] < 100) & (df['2'] > -100) & (df['3'] < 100) & (df['3'] > -100)].copy()\n    \n    #Squaring the Waveforms\n    sample_df['0_sqr'] = np.square(sample_df['0'])\n    sample_df['1_sqr'] = np.square(sample_df['1'])\n    sample_df['2_sqr'] = np.square(sample_df['2'])\n    sample_df['3_sqr'] = np.square(sample_df['3'])\n\n    #Setting Index\n    sample_df['DateTime'] = pd.to_datetime(sample_df['DateTime'])\n    sample_df.set_index('DateTime', inplace=True)\n\n    #Windowed MAX\n    sample_df['0_max'] = sample_df['0_sqr'].rolling(72).max()\n    sample_df['1_max'] = sample_df['1_sqr'].rolling(72).max()\n    sample_df['2_max'] = sample_df['2_sqr'].rolling(72).max()\n    sample_df['3_max'] = sample_df['3_sqr'].rolling(72).max()\n    \n    #Removal of Blanks (Initial values of the window)\n    sample_df.dropna(inplace=True)\n    \n    #First order difference of Rolling MAX\n    sample_df['0_change'] = sample_df['0_max'].diff()\n    sample_df['1_change'] = sample_df['1_max'].diff()\n    sample_df['2_change'] = sample_df['2_max'].diff()\n    sample_df['3_change'] = sample_df['3_max'].diff()\n    \n    fault_date_0 = sample_df[(sample_df['0_change']>=sample_df['0_change'].nlargest(15).mean()) | (sample_df['0_change']<=sample_df['0_change'].nsmallest(15).mean())].index[0]\n    fault_date_1 = sample_df[(sample_df['1_change']>=sample_df['1_change'].nlargest(15).mean()) | (sample_df['1_change']<=sample_df['1_change'].nsmallest(15).mean())].index[0]\n    fault_date_2 = sample_df[(sample_df['2_change']>=sample_df['2_change'].nlargest(15).mean()) | (sample_df['2_change']<=sample_df['2_change'].nsmallest(15).mean())].index[0]\n    fault_date_3 = sample_df[(sample_df['3_change']>=sample_df['3_change'].nlargest(15).mean()) | (sample_df['3_change']<=sample_df['3_change'].nsmallest(15).mean())].index[0]\n    \n    date_list = [fault_date_0, fault_date_1, fault_date_2, fault_date_3]\n    \n    fault_induction_date = min(date_list)\n    \n    return date_list","98a00799":"machine_nos = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n\n\ndf_machine_and_fault = pd.DataFrame(columns=['Machine No.', 'Fault_0', 'Fault_1', 'Fault_2', 'Fault_3', 'Fault_Inception_Date'])\ndf_machine_and_fault['Machine No.'] = machine_nos\nfor machine_no in machine_nos:\n    df_machine_and_fault.iloc[machine_no,1:5] = finding_first_fault(df_consolidated_rowwise.loc[machine_no])\n    \n#We will now select the earliest date of the dates identified for all 4 signals, since we want to be able to identify the fault at the earliest.\n\n#Through deductive analysis, we will be excluding the Signal 2 from this because signal 2 has been found to contain \n#values that breach the set threshold, even before the fault has set in, i.e. in the normal mode of operation itself. \n#These can be considered as outliers, and since the other signals are contributing enough to the fault identification \n#this signal can be excluded.\n\ndf_machine_and_fault['Fault_Inception_Date'] = df_machine_and_fault[['Fault_0', 'Fault_1', 'Fault_3']].min(axis=1)\ndf_machine_and_fault","bd039952":"Now we will try using a couple of methods\/algorithms to identify the fault inception date\/time. We will try using - \n\n1. SARIMAX Model\n2. Custom Function\n3. LSTM","e0f90990":"From the above plot, we can see that the values in all fields, follow a certain pattern upto a certain point in time, after which a sudden disruption in the pattern is observed, post which, the signal goes 0.\n\nAlso, the entire signal reading is loaded with a lot of noisy measurements as indicated by the high-value, vertical readings. We'll remove those to better view the waveforms.","567f406d":"## 1. SARIMAX\n\nSince our data contains a time-dependent signal, it can be modelled using Autoregressive models. In AR models, present value is modeled as a function of past values and noise. We will be using the SARIMAX model here, because as shown by the ACF plot below, our signal is dependent on the past values and also shows some seasonality (a repeating pattern of 12 periods approximately). Also since the signal is non-stationary, we need to use a time-differenced model which explains use of the I (integrated) in SARIMAX.","3b84edd0":"## Windowed MAX and First order Differences","c338dc19":"Here, we will follow a set of steps to specifically identify incoherent patterns in the signal values.\n1. First we will square the signal values to transform negative values to positive\n\n2. Then we will take a rolling max of squared signal values. This will enable to get constant signal values for normal patterns, which will rise or fall if a different pattern emerges in the original signal.\n\n3. Then we will take the first order difference of the Windowed MAX values which should largely remain 0 (since the max for normal mode of operation would be the same due to the signal following a constant pattern), and should change only for incoherent signal patterns\/disturbances indicating the commencement of fault.\n\n4. Then we will filter the first order differences by taking a threshold for noise tolerance (this threshold is set to be the mean of 15 largest values of the differenced values) and get the datetime of the first such value which breaches the threshold.\n\n5. These steps will be computed for all the signals (0, 1, 2, 3) and the corresponding dates identified by filtering would be returned by our function\n\n","ae9feffb":"We'll now plot the features of one machine's reading, to check the waveforms of the 4 signals.","78ab9d77":"### Noise Removal","e746828a":"### Plot for clean values","3d12a02e":"Since the signal contains both negative and positive values, we'll square the signal values and use the squared value for our analysis.","d9b2de9d":"## Data Acquisition\/Loading\n\nFirst I will load the data of all the machines in a single dataframe by concatenating individual dataframes corresponding to each csv for each of the machines. I have concatenated the data both, column wise and row wise and stored the result in 2 different dataframes, so that any one can be used whenever required.","6eb0ecc1":"So, while SARIMAX works well at predicting signals so that discrepancies in signal values can be identified by subtracting from the predicted signal, it works for single variables\/signals only. Using this, we'll have to create a different SARIMAX model for each of the signals of each machine. This can be very expensive process-wise so we will assess other options first."}}