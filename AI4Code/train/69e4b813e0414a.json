{"cell_type":{"61d6f52f":"code","2ca81df2":"code","f2de9b02":"code","c8a643b6":"code","a4acc07b":"code","cdc1a333":"code","87e97ff6":"code","999fd534":"code","f1f460b0":"code","b31eecf8":"code","3a04ebf2":"code","fc7c8687":"code","48c3727f":"code","c33ae2b5":"code","8d526211":"code","d37db40d":"code","2465ecb2":"code","6dac50fa":"code","f29166af":"code","5065d450":"markdown","2a00e998":"markdown","fb7d34c7":"markdown","7a665248":"markdown","2c2b6264":"markdown","11672c29":"markdown","a0a6bcf6":"markdown","1412ca3a":"markdown","d62da364":"markdown","224f12ea":"markdown","ad5fdbc7":"markdown","cf7492c1":"markdown","f4707bba":"markdown","73d27232":"markdown","dc92d2f7":"markdown","f83c2167":"markdown","08b8d000":"markdown","b7977d5c":"markdown"},"source":{"61d6f52f":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nfrom datetime import datetime\n\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\nfrom textwrap import wrap","2ca81df2":"df = pd.read_csv('..\/input\/natural-disasters-in-india-1990-2021\/Natural_Disasters_in_India .csv').dropna()\ndf.drop('Unnamed: 0',axis='columns', inplace=True)\ndf.head()","f2de9b02":"\ndef get_df(key_words):\n    DF = pd.DataFrame(columns = df.columns)\n\n    for index, row in df.iterrows():\n        for key_word in key_words:\n            words = [w.lower() for w in row['Title'].split(' ') ]\n            if key_word in words:\n                DF.loc[len(DF.index)] = row\n            \n    return DF.dropna()","c8a643b6":"\nmonth_to_num  = {'January':'01', 'February':'02', 'March':'03', 'April':'04', 'May':'05', 'June':'06', 'July':'07','August':'08','September' : '09','October' : '10','November' : '11','December' : '12'}\n\ndef plot_time_data(names,dates,Title):\n    # Choose some nice levels\n    levels = np.tile([-5, 5,-4,4,-3, 3, -2, 2,-1,1],\n                     int(np.ceil(len(dates)\/8)))[:len(dates)]\n\n    # Create figure and plot a stem plot with the date\n    fig, ax = plt.subplots(figsize=(15, 6), constrained_layout=True)\n    ax.set(title = Title )\n\n    markerline, stemline, baseline = ax.stem(dates, levels,\n                                             linefmt=\"C0-\", basefmt=\"k-\",\n                                             use_line_collection=True)\n\n    plt.setp(markerline, mec=\"c\", mfc=\"w\", zorder=3)\n\n    # Shift the markers to the baseline by replacing the y-data by zeros.\n    markerline.set_ydata(np.zeros(len(dates)))\n\n    # annotate lines\n    vert = np.array(['top', 'bottom'])[(levels > 0).astype(int)]\n    for d, l, r, va in zip(dates, levels, names, vert):\n        ax.annotate(r, xy=(d, l), xytext=(-4, np.sign(l)*3),\n                    textcoords=\"offset points\", va=va, ha=\"right\")\n\n    # format xaxis with 4 month intervals\n    ax.get_xaxis().set_major_locator(mdates.MonthLocator(interval=30))\n    ax.get_xaxis().set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n    plt.setp(ax.get_xticklabels(), rotation=40, ha=\"right\")\n\n    # remove y axis and spines\n    ax.get_yaxis().set_visible(False)\n    for spine in [\"left\", \"top\", \"right\"]:\n        ax.spines[spine].set_visible(False)\n\n    ax.margins(y=0.1)\n    plt.show()\n","a4acc07b":"def get_names_dates(DF):\n    dates = []\n    names = []\n\n    for name,date in zip(DF['Title'],DF['Date']):\n        # one mis-valued data i.e : 2020-08\u201325 , change this to 2020-08-25\n        if date == '2020-08\u201325 ': \n            date = '2020-08-25'\n        dates.append(datetime.strptime(date, \"%Y-%m-%d\"))\n        names.append(name)\n    \n    return names,dates","cdc1a333":"def generate_wordcloud(data,title):\n    wc = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\").generate_from_frequencies(data)\n    plt.figure(figsize=(10,8))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title('\\n'.join(wrap(title,60)),fontsize=13)\n    plt.show()","87e97ff6":"df_count = df.groupby('Year').count()\n\nYears = list(df_count.index)\nCounts = list(df_count['Title'])\n  \nfig = plt.figure(figsize = (15, 5))\n\nplt.bar(Years, Counts, color ='blue',\n        width = 0.3)\n \nplt.xlabel(\"Year\")\nplt.ylabel(\"No. of Disasters\")\nplt.title(\"Disasters per year\")\nplt.show()","999fd534":"#here","f1f460b0":"Earthquake_df = get_df(['earthquake','earthquakes'])\n\nEarthquake_df.head()","b31eecf8":"names,date = get_names_dates(Earthquake_df)\n\ntitle = \"Major Earthquakes in 31 years\"\n\nplot_time_data(names,date,title)","3a04ebf2":"Flood_df = get_df(['flood','floods'])\n\nFlood_df.head()","fc7c8687":"names,date = get_names_dates(Flood_df)\n\ntitle = \"Major Floods in 30 years\"\n\nplot_time_data(names,date,title)","48c3727f":"df['Disaster_Info'][0]","c33ae2b5":"nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n\n# Lemmatization with stopwords removal\ndf['lemmatized']=df['Disaster_Info'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))","8d526211":"df_G=df[['Year','lemmatized']].groupby(by='Year').agg(lambda x:' '.join(x))\ndf_G.head()","d37db40d":"\nCountVectorizer_ = CountVectorizer(analyzer='word')\n\ndata = CountVectorizer_.fit_transform(df_G['lemmatized'])\n\nDocument_Term_Matrix = pd.DataFrame(data.toarray(), columns = CountVectorizer_.get_feature_names())\n\nDocument_Term_Matrix.index=df_G.index\nDocument_Term_Matrix.head()","2465ecb2":"Document_Term_Matrix = Document_Term_Matrix.transpose()\nfor index,disaster_year in enumerate(Document_Term_Matrix.columns):\n    generate_wordcloud(Document_Term_Matrix[disaster_year].sort_values(ascending=False),str(disaster_year))","6dac50fa":"#here","f29166af":"#end","5065d450":"### Floods","2a00e998":"## Thank you, happy to hear your suggestions\/thoughts  ","fb7d34c7":"### Earthquakes ","7a665248":"### Document Term Matrix","2c2b6264":"# Exploratory data analysis: looking into the data ","11672c29":"### Let's Start the text analysis ","a0a6bcf6":"## Let's have a look at some specific disasters that happen over the years ","1412ca3a":"## Word cloud for the Disaster Information pr year","d62da364":"### Remove Stopwords and Lemmatization","224f12ea":"#### From the above word cloud, we can derive a lot of information about what disasters happened that year. ","ad5fdbc7":"### Let's start","cf7492c1":"## Helper functions","f4707bba":"### load the data","73d27232":"#### Looks like disasters are increasing almost constantly. \n#### Mainly due to climate change and increasing human population \n\n","dc92d2f7":"## Disasters per year \n\n#### Let\u2019s see the disasters that happed pr year \n","f83c2167":"#### Hellow reders \n\n#### In this notebook we will explore the dataset and analysis the data its different features and try to understand the underlying pattern in the dataset.\n\nnotebook is inspired from [this](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/beginners-guide-exploratory-data-analysis-text-data\/) article ","08b8d000":"## Analisis ","b7977d5c":"## Let\u2019s have a look at the text data "}}