{"cell_type":{"ae459245":"code","8d7f18cd":"code","0bfdeccc":"code","d70a57f4":"code","69e7c051":"code","59100659":"code","564479d6":"code","3472ad9f":"code","599a3b9e":"code","60bab868":"code","e41b6693":"code","32a2770b":"code","865c13b2":"code","ba91433a":"code","8b60ec66":"code","ac01e8a4":"code","0522b2c7":"code","9a73e2a7":"code","05731d96":"code","e2bf39cb":"code","acbb7891":"code","127ec29c":"code","59cd094c":"code","d4993de3":"code","593d0395":"code","7ad1652a":"code","3b69600e":"code","81130b97":"code","b5046c5e":"code","ba7914fa":"code","ec3668af":"code","b21b9eff":"code","191edefe":"code","b6381960":"code","dd2dfd58":"code","5458a514":"code","0c7bac2a":"code","784ea340":"code","61af293a":"code","8f507d8c":"code","a4ee2a69":"code","f7a76785":"code","4beb7fc2":"code","636b1e33":"code","b2f8cf11":"code","3d3f92f8":"code","eecc891d":"code","2183ad46":"code","203e1bed":"code","598f13c3":"code","c6d6cddd":"code","c5a796f6":"code","8c08a0c5":"code","d32adc23":"code","a3da26f8":"code","e48cdc44":"code","e034088d":"code","34626737":"code","be7aa91a":"markdown","99687d90":"markdown","741874dd":"markdown","d786f1d9":"markdown","898e4d4b":"markdown","4752fae5":"markdown","5ca7f757":"markdown","8e09ecc4":"markdown","2c024f85":"markdown","f77a1f87":"markdown","357590f1":"markdown","3d2e21d0":"markdown","96beb853":"markdown","5043c2b8":"markdown","99801086":"markdown","cfa886b0":"markdown","65a805e0":"markdown","405e989c":"markdown","93798218":"markdown","aff0edb3":"markdown","bcfac38c":"markdown"},"source":{"ae459245":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d7f18cd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0bfdeccc":"# Load training data\ntrain_df = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')","d70a57f4":"train_df.head()","69e7c051":"train_df.shape","59100659":"train_df.info()","564479d6":"# Rename the *question_text* column for convenience\ntrain_df = train_df.rename({'question_text': 'question'}, axis=1)","3472ad9f":"train_df['question'].isnull().sum()","599a3b9e":"train_df['question'].isna().sum()","60bab868":"train_df.columns","e41b6693":"train_df.target.value_counts()","32a2770b":"import matplotlib.ticker as ticker\n\nncount = train_df.shape[0]\n\nplt.figure(figsize=(7, 5))\n\nax = sns.countplot(data=train_df, x='target')\nplt.title('Portion of Questions')\nplt.xlabel('Number of Axles')\n\n# Make twin axis\nax2=ax.twinx()\n\n# Switch so count axis is on right, frequency on left\nax2.yaxis.tick_left()\nax.yaxis.tick_right()\n\n# Also switch the labels over\nax.yaxis.set_label_position('right')\nax2.yaxis.set_label_position('left')\n\nax2.set_ylabel('Frequency [%]')\n\nfor p in ax.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax.annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n            ha='center', va='bottom') # set the alignment of the text\n\n# Use a LinearLocator to ensure the correct number of ticks\nax.yaxis.set_major_locator(ticker.LinearLocator(11))\n\n# Fix the frequency range to 0-100\nax2.set_ylim(0,100)\nax.set_ylim(0,ncount)\n\n# And use a MultipleLocator to ensure a tick spacing of 10\nax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n\n# Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\nax2.grid(None)","865c13b2":"pd.set_option('display.max_columns', None)\ninsincere_qes = train_df[train_df['target'] == 1]\nprint(insincere_qes[-5:].question.to_string())","ba91433a":"pd.set_option('display.max_columns', None)\nsincere_qes = train_df[train_df['target'] == 0]\nprint(sincere_qes[-5:].question)","8b60ec66":"from wordcloud import WordCloud","ac01e8a4":"print('Word cloud image generated from insincere questions')\ninsincere_wordcloud = WordCloud(width=800, height=400, background_color ='black', min_font_size = 10).generate(str(train_df[train_df[\"target\"] == 1][\"question\"]))\n#Positive Word cloud\nplt.figure(figsize=(15,6), facecolor=None)\nplt.imshow(insincere_wordcloud)\nplt.axis(\"off\")\n# plt.tight_layout(pad=0)\nplt.show();","0522b2c7":"print('Word cloud image generated from sincere questions')\nsincere_wordcloud = WordCloud(width=600, height=400, background_color ='black', min_font_size = 10).generate(str(train_df[train_df[\"target\"] == 0][\"question\"]))\n#Positive Word cloud\nplt.figure(figsize=(15,6), facecolor=None)\nplt.imshow(sincere_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show();","9a73e2a7":"train_df.head()","05731d96":"import string\nfrom nltk.corpus import stopwords\n\n# Set up contraction dictionary\ncontraction_dict = {\"dont\": \"do not\", \"aint\": \"is not\", \"isnt\": \"is not\", \"doesnt\": \"does not\", \"cant\": \"cannot\", \"mustnt\": \"must not\", \"hasnt\": \"has not\", \"havent\": \"have not\", \"arent\": \"are not\", \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"\u2018cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"Iam\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\n# Set up stop words list\nstop_words = stopwords.words('english')\nstop_words.remove('not')\n\n# Set up puntuation list\npunctuation = string.punctuation","e2bf39cb":"#Feature Engineering on train_df data\n\ndef create_features(df):\n    \"\"\"Retrieve from the text column the number of: characters, words, unique words, stopwords,\n    punctuations, upper\/lower case chars\"\"\"\n    df[\"lenght\"] = df[\"question\"].apply(lambda x: len(str(x)))\n    df[\"no_words\"] = df[\"question\"].apply(lambda x: len(x.split()))\n    df[\"no_unique_words\"] = df[\"question\"].apply(lambda x: len(set(str(x).split())))\n    df[\"no_stopwords\"] = df[\"question\"].apply(lambda x : len([nw for nw in str(x).split() if nw.lower() in stop_words]))\n    df[\"no_punctuation\"] = df[\"question\"].apply(lambda x : len([np for np in str(x) if np in punctuation]))\n    df[\"no_uppercase\"] = df[\"question\"].apply(lambda x : len([nu for nu in str(x).split() if nu.isupper()]))\n    df[\"no_lowercase\"] = df[\"question\"].apply(lambda x : len([nl for nl in str(x).split() if nl.islower()]))\n    return df","acbb7891":"train_df = create_features(train_df)","127ec29c":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\ntrain_df[train_df['target'] == 0].describe()","59cd094c":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\ntrain_df[train_df['target'] == 1].describe()","d4993de3":"# num_feat = ['lenght', 'no_unique_words', 'no_stopwords', \n#             'no_punctuation', 'no_uppercase', 'no_lowercase', 'target'] \n# # side note : remove target if needed later\n\n# dfsample = train_df[num_feat].sample(n=round(train_df.shape[0]\/6), random_state=42)\n\n# plt.figure(figsize=(15,15))\n# sns.set_context(\"paper\", rc={\"axes.labelsize\":16})\n# sns.pairplot(data=dfsample, hue='target')\n# plt.show()","593d0395":"plt.figure(figsize=(10, 10))\nsns.displot(train_df, x='no_lowercase', hue='target', kind='hist', bins=50)\nplt.title(\"Distribution of the question's number of lowercases\")","7ad1652a":"target_correlation = train_df.corr()['target'][1:]","3b69600e":"# mask = np.zeros_like(train_df[num_feat].corr(), dtype=np.bool) \n# mask[np.triu_indices_from(mask)] = True \n\n# f, ax = plt.subplots(figsize=(10, 10))\n# plt.title('Question features Correlation Matrix',fontsize=20)\n\n# sns.heatmap(train_df[num_feat].corr(),square=True, linewidths=0.25,vmax=0.7,cmap=\"YlGnBu\",\n#             linecolor='w',annot=True,annot_kws={\"size\":10},mask=mask,cbar_kws={\"shrink\": .9});","81130b97":"plt.figure(figsize=(15, 5))\n\ntarget_correlation = train_df.corr()['target'][1:]\nplt.plot(target_correlation)","b5046c5e":"import re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import SnowballStemmer\n\nlemmatizer = WordNetLemmatizer()\nstemmer = SnowballStemmer('english')","ba7914fa":"train_df['question']","ec3668af":"def qes_preprocessing(qes):\n    # Data cleaning:\n    qes = re.sub(re.compile('<.*?>'), '', qes)\n    qes = re.sub('[^A-Za-z0-9]+', ' ', qes)\n\n    # Lowercase:\n    qes = qes.lower()\n\n    # Tokenization:\n    tokens = word_tokenize(qes)\n\n    # Contractions replacement:\n    tokens = [contraction_dict.get(token) if (contraction_dict.get(token) != None) else token for token in tokens]\n\n    # Stop words removal:\n    tokens = [w for w in tokens if w not in stop_words]\n\n    # Stemming:\n    tokens = [stemmer.stem(token) for token in tokens]\n    \n    # Lemmatization:\n    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n\n    # Join words after preprocessed:\n    qes = ' '.join(tokens) \n    \n#     aug = naw.ContextualWordEmbsAug(\n#     model_path='bert-base-uncased', action=\"insert\")\n#     augmented_text = aug.augment(text)\n\n    return qes","b21b9eff":"train_df['preprocessed_questions'] = train_df['question'].apply(qes_preprocessing)\ntrain_df['preprocessed_questions']","191edefe":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\n\nX = train_df['preprocessed_questions']\ny = train_df.target","b6381960":"vectorizer = TfidfVectorizer(lowercase=False, analyzer=lambda x: x, min_df=0.01, max_df=0.999)\n# min_df & max_df param added for less memory usage\n\ntf_idf = vectorizer.fit_transform(X).toarray()\npd.DataFrame(tf_idf, columns=vectorizer.get_feature_names()).head()","dd2dfd58":"X_train, X_test, y_train, y_test = train_test_split(tf_idf, y, test_size=0.2, random_state=42)","5458a514":"# # BOW\n# nb_bow_pipeline = Pipeline([(\"cv\", CountVectorizer(analyzer=\"word\", ngram_range=(2,4), max_df=0.85)),\n#                      (\"model\", MultinomialNB())])\n\n# # TF-IDF\n# nb_tdf_pipelione = Pipeline([(\"tfid\", TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)),\n#                      (\"model\", MultinomialNB())])","0c7bac2a":"# # BOW\n# lr_bow_pipeline = Pipeline([(\"cv\", CountVectorizer(analyzer=\"word\", ngram_range=(1,4), max_df=0.9)),\n#                      (\"model\", LogisticRegression(solver=\"saga\", class_weight=\"balanced\", C=0.45, max_iter=250, verbose=1, n_jobs=-1))\n#                            ])\n\n# # TF-IDF\n# lr_tdf_pipeline = Pipeline([(\"tfid\", TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)),\n#                      (\"model\", LogisticRegression(solver=\"saga\", class_weight=\"balanced\", C=0.45, max_iter=250, verbose=1, n_jobs=-1))\n#                            ])","784ea340":"def get_fscore_matrix(fitted_clf, model_name):\n    print(model_name, ' :')\n    \n    # get classes predictions for the classification report \n    y_train_pred, y_pred = fitted_clf.predict(X_train), fitted_clf.predict(X_test)\n    print(classification_report(y_test, y_pred), '\\n') # target_names=y\n    \n    # computes probabilities keep the ones for the positive outcome only      \n    print(f'F1-score = {f1_score(y_test, y_pred):.2f}')","61af293a":"from xgboost import XGBClassifier\n\nXGBmodel = XGBClassifier(objective=\"binary:logistic\")\nXGBmodel.fit(X_train, y_train)\nget_fscore_matrix(XGBmodel, 'XGB Clf withOUT weights')","8f507d8c":"ratio = ((len(y_train) - y_train.sum()) - y_train.sum()) \/ y_train.sum()\nratio","a4ee2a69":"WXGBmodel = XGBClassifier(objective=\"binary:logistic\", scale_pos_weight=ratio)\nWXGBmodel.fit(X_train, y_train)\nget_fscore_matrix(WXGBmodel, 'XGB Clf WITH weights')","f7a76785":"import lightgbm as lgb\n\nLGBMmodel = lgb.LGBMClassifier(n_jobs = -1, class_weight={0:y_train.sum(), 1:len(y_train) - y_train.sum()})\nLGBMmodel.fit(X_train, y_train)\nget_fscore_matrix(LGBMmodel, 'LGBM weighted')","4beb7fc2":"# models = [nb_bow_pipeline, nb_tdf_pipelione, lr_bow_pipeline, lr_tdf_pipeline]","636b1e33":"# import matplotlib.pyplot as plt  \n# from sklearn.datasets import make_classification\n# from sklearn.metrics import plot_confusion_matrix\n# from sklearn.model_selection import train_test_split\n# from sklearn.svm import SVC\n\n# # X, y = make_classification(random_state=0)\n# # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# # clf = SVC(random_state=0)\n# # clf.fit(X_train, y_train)\n# # SVC(random_state=0)\n# # plot_confusion_matrix(clf, X_test, y_test, normalize=None, cmap=plt.cm.Blues)\n# # plt.show()\n\n# vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n# vectorized = vectorizer.fit_transform(X)\n# pd.DataFrame(vectorized.toarray(), \n#             index=['sentence '+str(i) \n#                    for i in range(1, 1+len(X))],\n#             columns=vectorizer.get_feature_names())\n\n# # vectorizer = TfidfVectorizer()\n# # X = vectorizer.fit_transform(X)\n# # print(vectorizer.get_feature_names())\n# # print(X.shape)","b2f8cf11":"# kfold = KFold(n_splits=2, shuffle=False)\n\n# fold = 1\n\n# for train_index, test_index in kfold.split(X, y):\n#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index] \n#     print(X_train)\n#     print(f'Fold {fold}:')\n#     for model_pipeline in models:\n#         print(model_pipeline)\n#         model_pipeline.fit(X_train, y_train)\n# #         predictions = model_pipeline.predict_log_proba(X_test)\n\n#         print(classification_report(y_test, predictions), '\\n') \n        \n#         print(f'F1-score = {f1_score(y_test, predictions):.2f}')\n\n#     fold += 1","3d3f92f8":"test_df = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')","eecc891d":"test_df.head()","2183ad46":"test_df.info()","203e1bed":"test_df.shape","598f13c3":"test_df['preprocessed'] = test_df['question_text'].apply(qes_preprocessing)","c6d6cddd":"vectorizer = TfidfVectorizer(lowercase=False, analyzer=lambda x: x, min_df=0.01, max_df=0.999)\n# min_df & max_df param added for less memory usage\n\ntf_idf = vectorizer.fit_transform(test_df['preprocessed']).toarray()\npd.DataFrame(tf_idf, columns=vectorizer.get_feature_names()).head()","c5a796f6":"# X_test, y_test = train_test_split(tf_idf, y, test_size=0.2, random_state=42)","8c08a0c5":"# predictions = models[2].predict(test_df['preprocessed'])","d32adc23":"prediction1 = XGBmodel.predict(tf_idf)","a3da26f8":"prediction2 = WXGBmodel.predict(tf_idf)","e48cdc44":"prediction3 = LGBMmodel.predict(tf_idf)","e034088d":"print(len(prediction1))\nprint(prediction1[-10:])\nprint(test_df['question_text'][-10:])","34626737":"test_df['prediction'] = prediction1\nresults = test_df[['qid', 'prediction']]\nresults.to_csv('submission.csv', index=False)\nresults.shape","be7aa91a":"**Model 5: LGBM with weights**","99687d90":"**Add model to model list**","741874dd":"**Model 1: MultinomialNB**","d786f1d9":"The statistics of sincere questions","898e4d4b":"# **2. Load test.csv, make predictions and output submission**","4752fae5":"**2.4 Make submission**","5ca7f757":"# **Train model**","8e09ecc4":"# **Data Preprocessing**","2c024f85":"****The distribution of the questions's number of lowercases****","f77a1f87":"**Plot Vectorize matrix**","357590f1":"**Model 4: XGBoost Classifier with weigths**","3d2e21d0":"**2.1 Load dataset**","96beb853":"**2.3 Make predictions**","5043c2b8":"## **Words cloud**","99801086":"****Feature engineering****","cfa886b0":"**Vectorizing**","65a805e0":"The statistics of insincere questions","405e989c":"**Model 3: XGBoost Classifier without weigths**","93798218":"## **Text statistics** ","aff0edb3":"**Model 2: Logistic Regression**","bcfac38c":"**2.2 Preprocessing data**"}}