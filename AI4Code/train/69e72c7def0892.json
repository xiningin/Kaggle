{"cell_type":{"78b72639":"code","adfaf282":"code","e5da3e45":"code","1b2689f7":"code","e72490df":"code","40f714ef":"code","55c2acbd":"code","37ef0f6c":"code","94516c0e":"code","96a5ca0b":"code","38ca733b":"code","c511226d":"code","377194e2":"code","2557224d":"code","e7f49aac":"code","ebc17c5c":"code","ac47387b":"code","e653aa5b":"code","16c0d1eb":"code","23f361ad":"code","d7be2f0b":"code","edcf7b68":"code","47d54857":"code","c44f8c51":"code","eb1f778c":"code","74e125ad":"code","26799096":"code","97be6867":"code","ba69c86b":"code","be49439e":"code","9a35396d":"code","c2c25ade":"code","a2e4636d":"code","43bc9828":"code","196bb79f":"code","8c2907ee":"code","59e1b7ca":"code","8da6f201":"code","b06067ea":"code","85a936c5":"code","e9806e30":"code","195d1265":"code","36fcb218":"code","79e05792":"code","49821fba":"code","7106729f":"code","1ec20379":"code","b1e21438":"code","f9c81346":"code","17a33e09":"code","a4b3e46a":"markdown","d6ba1bdc":"markdown","e8f5d054":"markdown","e5f0f602":"markdown","29ae2f44":"markdown","5285bd58":"markdown","37eb602a":"markdown","f8b71a2d":"markdown","a5598bf0":"markdown","a090415a":"markdown","b9e33ce0":"markdown","cc6ab5d0":"markdown","d17861e7":"markdown","c3e65ca3":"markdown","e911663c":"markdown","b1a452be":"markdown","452d8b4a":"markdown","876a9899":"markdown","f711297f":"markdown","c831c84f":"markdown","0948eb6b":"markdown","f2a3225c":"markdown","c52f3f5e":"markdown","21b0e67b":"markdown","07dcacd1":"markdown","3f7c950d":"markdown","9235aa38":"markdown","81a412b6":"markdown","bf0ad8df":"markdown","3789665c":"markdown","b9136471":"markdown","10c37760":"markdown","7b19398c":"markdown","b30f12db":"markdown","e4a9038d":"markdown","6a229194":"markdown","9c835432":"markdown","8bfe7316":"markdown","a29882f7":"markdown","d2651972":"markdown"},"source":{"78b72639":"import numpy as np\nimport pandas as pd\nimport os\n\n\ndata = pd.read_csv('..\/input\/online-retail-ii-uci\/online_retail_II.csv')\n\ndata","adfaf282":"data.isnull().sum()","e5da3e45":"data.info()","1b2689f7":"pd.set_option(\"display.max_rows\", 101)\npd.set_option(\"display.max_columns\", 10)\n","e72490df":"#items whose price is negative\nnegprice = data[data['Price'] < 0].sum() \n\n#items whose quantity is negative\nnegquantity = data[data['Quantity'] < 0].count()\n","40f714ef":"np.set_printoptions(threshold=np.inf)\n#df = pd.DataFrame(data)\ndata['Description'].unique()","55c2acbd":"df_new = data['Description']\ndf_new = df_new.dropna()\ndf_new.dropna()\ndf_new = pd.DataFrame(df_new)\n\n\nmy_list = [y for x in df_new['Description'] for y in x.split() if y.islower()]\nmylist = list(set(my_list))\nmylist","37ef0f6c":"data_bad = data[data['Description'].isin([ mylist ])]\n\ndata = data[~data.apply(tuple,1).isin(data_bad.apply(tuple,1))] ","94516c0e":"#replace null customer values with 99999\ndata[['Customer ID']] =data[['Customer ID']].fillna(99999)\n#replace null description values with 'Unknown'\ndata[['Description']] =data[['Description']].fillna('Unknown')\n\n#leave only positive quantities and known customer IDs\ndata = data[data['Quantity'] > 0]\ndata = data[data['Customer ID'] != 99999]\n\n# Removing returned products (Invoice numbers starting with C) from the data set\ndata = data[~data[\"Invoice\"].str.contains(\"C\", na = False)]\n\ndata.info()","96a5ca0b":"data.shape","38ca733b":"data = data.drop_duplicates()","c511226d":"data.shape","377194e2":"amount = pd.DataFrame(data.Quantity * data.Price, columns = ['Amount'])\n\namount","2557224d":"data_cust = np.array(data['Customer ID'], dtype=np.object)\n\ndata_cust = pd.DataFrame(data_cust, columns = [\"Customer ID\"])\n\ndata_cust = pd.concat(objs = [data_cust, amount], axis = 1, ignore_index = False)\n\nmonetary = data_cust.groupby(by = [\"Customer ID\"]).Amount.sum()\n\nmonetary = monetary.reset_index()\nmonetary = monetary[monetary['Customer ID'] != 99999]\nmonetary","e7f49aac":"frequency = data[['Customer ID', 'Invoice']]\n\nfrequency_df = frequency.groupby(\"Customer ID\").Invoice.count()\nfrequency_df = pd.DataFrame(frequency_df)\nfrequency_df = frequency_df.reset_index()\nfrequency_df.columns = [\"Customer ID\", \"Frequency\"]\nfrequency_df.head()","ebc17c5c":"import datetime as dt\n\ndata[\"InvoiceDate\"].max() # Last invoice date","ac47387b":"today_date = dt.datetime(2011,12,9) # last invoice date is assigned to today_date variable","e653aa5b":"# The type of Customer ID variable needs to be turned into an integer for following commands.\ndata[\"Customer ID\"] = data[\"Customer ID\"].astype(int) ","16c0d1eb":"# The type of InvoiceDate variable needs to be turned into datetime for following commands.\ndata[\"InvoiceDate\"] = pd.to_datetime(data[\"InvoiceDate\"])","23f361ad":"# Grouping the last invoice dates according to the Customer ID variable, subtracting them from today_date, and assigning them as recency\nrecency = (today_date - data.groupby(\"Customer ID\").agg({\"InvoiceDate\":\"max\"}))\n# Rename column name as Recency\nrecency.rename(columns = {\"InvoiceDate\":\"Recency\"}, inplace = True)\n# Change the values to day format\nrecency_df = recency[\"Recency\"].apply(lambda x: x.days)\nrecency_df.head()","d7be2f0b":"RFM = frequency_df.merge(monetary, on = \"Customer ID\")\nRFM = RFM.merge(recency_df, on = \"Customer ID\")\nRFM","edcf7b68":"import plotly.express as px\n\nfig = px.box(RFM, y = [ 'Frequency', 'Amount'] )\nfig.show()","47d54857":"#outlier treatment: We will delete everything outside the IQR\n\nQ1 = RFM.Amount.quantile(0.25)\nQ3 = RFM.Amount.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Amount >= (Q1 - 1.5*IQR)) & (RFM.Amount <= (Q1 + 1.5 * IQR))]","c44f8c51":"#outlier treatment : We will delete everything outside the IQR\n\nQ1 = RFM.Frequency.quantile(0.25)\nQ3 = RFM.Frequency.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Frequency >= (Q1 - 1.5*IQR)) & (RFM.Frequency <= (Q1 + 1.5 * IQR))]","eb1f778c":"fig = px.box(RFM, y = [ 'Recency'] )\nfig.show()","74e125ad":"from sklearn.preprocessing import normalize, StandardScaler","26799096":"RFM = RFM.drop(['Customer ID'], axis=1)","97be6867":"standard_scaler = StandardScaler()\nRFM = standard_scaler.fit_transform(RFM)\n\n#RFM = pd.DataFrame(RFM)\nRFM = pd.DataFrame(data = RFM, columns = ['Frequency', 'Amount', 'Recency'])\n\nRFM","ba69c86b":"!pip3 install pyclustertend\nfrom pyclustertend import hopkins \n\nhopkins(RFM, RFM.shape[0])","be49439e":"from scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster import hierarchy\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import pdist\nimport plotly.graph_objects as go\n\nZ = hierarchy.linkage(RFM, 'ward')\n\nc, coph_dists = hierarchy.cophenet(Z, pdist(RFM, 'hamming'))\n\nward = c\n\n\nA = hierarchy.linkage(RFM,'average')\n\nc, coph_dists = hierarchy.cophenet(A, pdist(RFM, 'hamming'))\n\naverage = c\n\n\nB = hierarchy.linkage(RFM,'single')\n\nc, coph_dists = hierarchy.cophenet(B, pdist(RFM, 'hamming'))\n\nsingle = c\n\n\nC = hierarchy.linkage(RFM,'complete')\n\nc, coph_dists = hierarchy.cophenet(C, pdist(RFM, 'hamming'))\n\ncomplete = c\n\n\nD = hierarchy.linkage(RFM,'weighted')\n\nc, coph_dists = hierarchy.cophenet(D, pdist(RFM, 'hamming'))\n\nweighted = c\n\n\nE = hierarchy.linkage(RFM,'centroid')\n\nc, coph_dists = hierarchy.cophenet(E, pdist(RFM, 'hamming'))\n\ncentroid = c\n\nF = hierarchy.linkage(RFM,'median')\n\nc, coph_dists = hierarchy.cophenet(F, pdist(RFM, 'hamming'))\n\nmedian = c\n\n\nmetrics=['ward', 'average', 'single', 'complete', 'weighted', 'centroid', 'median']\n\n\nfig = go.Figure([go.Bar(x=metrics, y=[ward, average, single, complete, weighted, centroid, median])])\nfig.show()\n\n","9a35396d":"from sklearn.metrics import silhouette_score\nfor n_clusters in range(2,15):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(RFM)\n    \n\n    score = silhouette_score (RFM, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","c2c25ade":"from sklearn.metrics import davies_bouldin_score\n\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(RFM)\n    \n    score = davies_bouldin_score (RFM , preds)\n    print (\"For n_clusters = {}, the Davies-Bouldin score is {})\".format(n_clusters, score))","a2e4636d":"from sklearn.metrics import calinski_harabasz_score\n\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(RFM)\n    \n    score = calinski_harabasz_score(RFM, preds)\n    print (\"For n_clusters = {}, the Calinski-Harabasz score is {})\".format(n_clusters, score))","43bc9828":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters = None)\nmodel = model.fit(RFM)\n\nZ = hierarchy.linkage(model.children_, 'complete')\n\nplt.figure(figsize=(20,10))\n\ndn = hierarchy.dendrogram(Z)","196bb79f":"import seaborn as sns\n\nmodel = AgglomerativeClustering(distance_threshold=None, n_clusters = 3)\nmodel = model.fit(RFM)\ny_agg=model.fit_predict(RFM)\n\n","8c2907ee":"RFM_agg = RFM.copy()\n\nRFM_agg['Agg_Labels'] = y_agg\n\nfig = px.scatter_3d(RFM_agg, x='Frequency', y='Amount', z='Recency',\n              color= 'Agg_Labels')\nfig.show()","59e1b7ca":"from sklearn.neighbors import NearestNeighbors\n\nnearest_neighbors = NearestNeighbors(n_neighbors=63) #sqrt(samples = 4082)\nnearest_neighbors.fit(RFM)\ndistances, indices = nearest_neighbors.kneighbors(RFM)\ndistances = np.sort(distances, axis=0)[:, 1]\n#print(distances)\nplt.figure(figsize=(20,10))\nplt.plot(distances)\nplt.show()","8da6f201":"from sklearn.cluster import DBSCAN\n\nmodel2 = DBSCAN(eps = 0.3, min_samples = 13)\n\nmodel2 = model2.fit(RFM)\n\ny_db=model2.labels_\n\nRFM_DB = RFM.copy()\nRFM_DB[\"DBLabels\"] = y_db\n\nRFM_DB[\"DBLabels\"].value_counts(0)","b06067ea":"silhouette_score (RFM_DB, y_db) ","85a936c5":"fig = px.scatter_3d(RFM_DB, x='Frequency', y='Amount', z='Recency',\n              color= 'DBLabels')\nfig.show()","e9806e30":"RFM_DB_filtered = RFM_DB.copy()\n\n# Get names of indexes for which column DB_Labels has value -1\nindexNames = RFM_DB_filtered[ RFM_DB_filtered['DBLabels'] == -1 ].index\n# Delete these row indexes from dataFrame\nRFM_DB_filtered.drop(indexNames , inplace=True)\n\nRFM_DB_filtered['DBLabels'].unique()","195d1265":"#plot clustered data without noise\n\nfig = px.scatter_3d(RFM_DB_filtered, x='Frequency', y='Amount', z='Recency',\n              color= 'DBLabels')\nfig.show()","36fcb218":"from sklearn.mixture import GaussianMixture as GMM\n\nfor n_components in range(2,15):\n    clusterer = GMM(n_components=n_components)\n    preds = clusterer.fit_predict(RFM)\n    \n\n    score = silhouette_score (RFM, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_components, score))","79e05792":"import numpy as np\nimport itertools\n\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n\n\nprint(__doc__)\n\n\n\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 9)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a Gaussian mixture with EM\n        gmm = GMM(n_components=n_components,\n                                      covariance_type=cv_type)\n        gmm.fit(RFM)\n        bic.append(gmm.bic(RFM))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n\nbic = np.array(bic)\ncolor_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n                              'darkorange'])\nclf = best_gmm\nbars = []\n\n# Plot the BIC scores\nplt.figure(figsize=(20, 10))\nspl = plt.subplot(2, 1, 1)\nfor i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n    xpos = np.array(n_components_range) + .2 * (i - 2)\n    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n                                  (i + 1) * len(n_components_range)],\n                        width=.2, color=color))\nplt.xticks(n_components_range)\nplt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\nplt.title('BIC score per model')\nxpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n    .2 * np.floor(bic.argmin() \/ len(n_components_range))\nplt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\nspl.set_xlabel('Number of components')\nspl.legend([b[0] for b in bars], cv_types)","49821fba":"n_components = np.arange(1, 21)\nmodels = [GMM(n, covariance_type='full', random_state=0).fit(RFM)\n          for n in n_components]\n\nplt.plot(n_components, [m.bic(RFM) for m in models], label='BIC')\nplt.plot(n_components, [m.aic(RFM) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components');","7106729f":"RFM_gmm = RFM.copy()\n\nRFM_gmm['gmm'] = GMM(n_components=6, random_state=42).fit_predict(RFM)","1ec20379":"fig = px.scatter_3d(RFM_gmm, x='Frequency', y='Amount', z='Recency',\n              color= 'gmm')\nfig.show()","b1e21438":"from sklearn.cluster import KMeans\n\nfrom sklearn.metrics import silhouette_score\nfor n_clusters in range(2,15):\n    clusterer = KMeans (n_clusters=n_clusters)\n    preds = clusterer.fit_predict(RFM)\n    \n\n    score = silhouette_score (RFM, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","f9c81346":"from scipy.spatial.distance import cdist\n\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,10) \n  \nfor k in K: \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(RFM) \n    kmeanModel.fit(RFM)     \n      \n    distortions.append(sum(np.min(cdist(RFM, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) \/ RFM.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n    mapping1[k] = sum(np.min(cdist(RFM, kmeanModel.cluster_centers_, \n                 'euclidean'),axis=1)) \/ RFM.shape[0] \n    mapping2[k] = kmeanModel.inertia_ \n    \nfor key,val in mapping2.items(): \n    print(str(key)+' : '+str(val))\n    \nplt.plot(K, inertias, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Inertia') \nplt.title('The Elbow Method using Inertia') \nplt.show()     ","17a33e09":"model4 = KMeans(n_clusters = 3)\n\nmodel4 = model4.fit(RFM)\n\nnp.unique(model4.labels_)\n\nRFM_kmeans = RFM.copy()\n\nRFM_kmeans['kmeans'] = KMeans(n_clusters=3).fit_predict(RFM)\n\n\nfig = px.scatter_3d(RFM_kmeans, x='Frequency', y='Amount', z='Recency',\n              color= 'kmeans')\nfig.show()","a4b3e46a":"Based on the gradient descent of the criterion, it seems that 6 clusters with a full covariance type is the optimal option.","d6ba1bdc":"The choice of number of components measures how well GMM works as a density estimator, not how well it works as a clustering algorithm. From the above plot it shows that optimal number of components is 11 (where the gradient stops decreasing). We will choose nonetheless 6 clusters since they can be interpreted more easily than 11.","e8f5d054":"3 clusters have the lowest score from a reasonable range of clusters.","e5f0f602":"## **In this notebook I will try to do an exploratory data analysis and clustering for the Online Retail II dataset, found in the UCI machine learning repository.**\n\n","29ae2f44":"We begin the clustering process with the agglomerative clustering algorithm for a simple reason: it is a hierarchical clustering algorithm, so we simplify the problem of having to choose beforehand the number of clusters in our model.However, hierarchical clustering does not avoid the problem with choosing the number of clusters. It simply  constructs the tree spaning over all samples, which shows which samples (later on clusters) merge together to create a bigger cluster. This happens recursively till you have just two clusters (this is why the default number of clusters is 2) which are merged to the whole dataset.","5285bd58":"## Scaling (Standardization)","37eb602a":"<h1 id=\"DBSCAN\">\n2.2 DBSCAN\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#DBSCAN\">\u00b6<\/a>\n<\/h1>","f8b71a2d":"Optimal value for eps where a 'knee' is formed is 0.3.","a5598bf0":"Let's apply the Knee method using KNN to find the optimal eps value for our model. Since KNN is a supervised learning algorithm and our data is not labeled, we will apply a general rule of thumb popularized by the \"Pattern Classification\" book by Duda et al., saying that the optimal K value usually found is the square root of N, where N is the total number of samples.","a090415a":"Let's check the Davies-Bouldin score.We want a values as close to 0 as possible","b9e33ce0":"At its simplest, GMM is also a type of clustering algorithm. As its name implies, each cluster is modelled according to a different Gaussian distribution. This flexible and probabilistic approach to modelling the data means that rather than having hard assignments into clusters like k-means, we have soft assignments. This means that each data point could have been generated by any of the distributions with a corresponding probability.","cc6ab5d0":"<h1 id=\"EM\">\n2.3 EM using GMM\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#EM\">\u00b6<\/a>\n<\/h1>","d17861e7":"A general rule of thumb is ==> Min points \u2265dimensionality +1\nAfter some trial and error, min_samples = 7 produces the lowest amount of noise possible\n","c3e65ca3":"## Outlier Detection","e911663c":"Kmeans algorithm is an iterative algorithm that tries to partition the dataset into K pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group\nIt tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster\u2019s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.","b1a452be":"We will opt for 3 clusters","452d8b4a":"We clearly see that the complete linkage method is the preferred one.","876a9899":"We observe that all the possible descriptions that indicate faulty items are in lowercase letters.Let's see only the lowercase descriptions.","f711297f":"Let's calculate some useful metrics that will help us decide the number of clusters. Since the ground truth labels are not known we will use such metrics as the silhouette coefficient, the Davies-Bouldin score and the Calinski-Harabasz Index.","c831c84f":"We will cut the dendrogram at 7000, so we will have 3 clusters\n\nNow, let's plot our data using the labels that the algorithm generated.We are going to make a scatterplot.","0948eb6b":"### RFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in retail and professional services industries.\n\n### RFM stands for the three dimensions:\n\n### * Recency \u2013 How recently did the customer purchase?\n### * Frequency \u2013 How often do they purchase?\n### * Monetary Value \u2013 How much do they spend?\n### Customer purchases may be represented by a table with columns for the customer name, date of purchase and purchase value.\n### One approach to RFM is to assign a score for each dimension on a scale from 1 to 10 but in this notebook we are just going to standardize the values of each column prior to clustering.\n\nLet it be noted that because the nature of clustering is experimental, there is no objectively correct algorithm","f2a3225c":"From the various silhouette score we can see that 2 clusters is the optimal number.","c52f3f5e":"Let's calculate the BIC score.This criterion gives us an estimation on how good the GMM in terms of predicting the data we actually have. The lower is the BIC, the better is the model to actually predict the data we have, and by extension, the true, unknown, distribution. In order to avoid overfitting, this technique penalizes models with big number of clusters.","21b0e67b":"Let's check the shilhouette score","07dcacd1":"We will remove the items whose description implies they are faulty","3f7c950d":"<h1 id=\"kmeans\">\n2.4 KMeans\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#kmeans\">\u00b6<\/a>\n<\/h1>","9235aa38":"<h1 id=\"Clustering\">\n2. Clustering\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Clustering\">\u00b6<\/a>\n<\/h1>","81a412b6":"Let's assess the clusterability of the dataset using the hopkins statistic. According to the pyclustertend library, on a scale from 0 to 1, the lower the score, the better the clusterability of the dataset","bf0ad8df":"# RFM Implementation","3789665c":"This score looks quite promising","b9136471":"We will perform Standardization scaling to our data to make the job of the clustering algorithms we will use next easier","10c37760":"<h1 id=\"agglomerative\">\n2.1 Agglomerative Clustering\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#agglomerative\">\u00b6<\/a>\n<\/h1>","7b19398c":"<h1 id=\"Exploratory_Data_Analysis\">\n1. Exploratory Data Analysis\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Exploratory_Data_Analysis\">\u00b6<\/a>\n<\/h1>","b30f12db":"Calinski-Harabasz index.We want as high a score as possible","e4a9038d":"Following, we apply to our dataset the DBSCAN algorithm.DBSCAN works by running a connected components algorithm across the different core points. If two core points share border points, or a core point is a border point in another core point\u2019s neighborhood, then they\u2019re part of the same connected component, which forms a cluster.\n\nA low min_samples parameter means it will build more clusters from noise, so we shouldn't choose it too small.\nThe DBSCAN paper suggests to choose minPts based on the dimensionality, and eps based on the elbow in the k-distance graph.\nFor eps, we can try to do a knn distance histogram and choose a \"knee\" there, but there might be no visible one, or multiple.\n\nIn the more recent publication\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19.\n\n\nThe authors suggest to use a larger minpts for large and noisy datasets, and to adjust epsilon depending on whether you get too large clusters (decrease epsilon) or too much noise (increase epsilon). Clustering requires iterations.\n","6a229194":"Let's calculate the silhouette score for various number of n_components","9c835432":"2 clusters seem like the most preferable option. Let's also check using the elbow method","8bfe7316":"Firstly, we are going to determine which linkage method to use. In order to do that we will calculate the cophenet index. Cophenet index is a measure of the correlation between the distance of points in feature space and distance on the dendrogram. If the distance between these points increases as the dendrogram distance between the clusters does then the Cophenet index is closer to 1. So, values closer to 1 mean a better linkage method.","a29882f7":"2 clusters","d2651972":"We will make 3 different dataframes: one containing the Customer's ID and amount he\/she spent, one containing the Customer's ID and the frequency of purchases and one containing the Customer's ID and the recency of purchases. In the end we will merge those together."}}