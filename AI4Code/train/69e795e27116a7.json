{"cell_type":{"a5ee046d":"code","44b2bd67":"code","a3935563":"code","0236589c":"code","106fddad":"code","7241237c":"code","db81f39c":"code","1d12ccc3":"code","b7c67cac":"code","5cae4df9":"code","eb3d514a":"code","820ba09a":"code","66da8da3":"code","3ddf9c9c":"code","d46ba6d2":"code","56fb3bab":"code","a4f9294e":"code","314df426":"code","edd6c7ed":"code","3dd3831f":"code","ae204268":"code","49078649":"code","f7da6fb2":"code","aa5618c0":"code","8fd4d8ab":"code","dee91996":"code","fc922a67":"code","10048549":"code","d9283a5b":"code","b1f7b84e":"code","33ea0437":"code","8b3afc52":"code","5ec4ea4f":"code","c72aa1d1":"markdown","2622deb9":"markdown","bb6e6c74":"markdown","217e8b9b":"markdown","ceaa8033":"markdown","455a7701":"markdown","5fcb9d72":"markdown","f237f98b":"markdown","8e902032":"markdown","858c7122":"markdown","a41563ab":"markdown","43b59429":"markdown","99b6984c":"markdown","6c95e1b6":"markdown","6947310f":"markdown","d48db6fd":"markdown","92002e2c":"markdown","41f5f1e5":"markdown","2bc02366":"markdown","eca6ba83":"markdown","84480a6f":"markdown","0890cc3a":"markdown","ce9d9b8e":"markdown","44646300":"markdown","c8921a9f":"markdown","a20e63f8":"markdown","f41c0b99":"markdown","8d9fabc8":"markdown","c41edd07":"markdown","94adb60d":"markdown","405345cc":"markdown","2b9bf3ee":"markdown","e7108d0d":"markdown","c3085af6":"markdown","19b50f58":"markdown","e148022d":"markdown","fcf1b5d8":"markdown","375012bf":"markdown","95dcad56":"markdown","dc4a08fc":"markdown","e73989f2":"markdown","2c48cc58":"markdown","341d6f59":"markdown","f02c115e":"markdown","de4f3170":"markdown","ab2d0a95":"markdown","69e05503":"markdown"},"source":{"a5ee046d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score","44b2bd67":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","a3935563":"(market_train_df, news_train_df) = env.get_training_data()","0236589c":"print(f'{market_train_df.shape[0]} samples and {market_train_df.shape[1]} features in the training market dataset.')","106fddad":"market_train_df.head()","7241237c":"data = []\nfor asset in np.random.choice(market_train_df['assetName'].unique(), 10):\n    asset_df = market_train_df[(market_train_df['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","db81f39c":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01 22:00:00+0000',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01 22:00:00+0000',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Another crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","1d12ccc3":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()","b7c67cac":"print(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")","5cae4df9":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","eb3d514a":"market_train_df.sort_values('price_diff')[:10]","820ba09a":"market_train_df['close_to_open'] =  np.abs(market_train_df['close'] \/ market_train_df['open'])","66da8da3":"print(f\"In {(market_train_df['close_to_open'] >= 1.2).sum()} lines price increased by 20% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.8).sum()} lines price decreased by 20% or more.\")","3ddf9c9c":"print(f\"In {(market_train_df['close_to_open'] >= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.5).sum()} lines price decreased by 100% or more.\")","d46ba6d2":"market_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']","56fb3bab":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby(['time']).agg({'price_diff': ['std', 'min']}).reset_index()\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * np.round(g['price_diff']['min'], 2)).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values * 5,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","a4f9294e":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['returnsOpenNextMktres10'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of returnsOpenNextMktres10 by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","314df426":"data = []\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\n\nprice_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].mean().reset_index()\n\ndata.append(go.Scatter(\n    x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = price_df['returnsOpenNextMktres10'].values,\n    name = f'{i} quantile'\n))\nlayout = go.Layout(dict(title = \"Treand of returnsOpenNextMktres10 mean\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","edd6c7ed":"data = []\nfor col in ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10']:\n    df = market_train_df.groupby('time')[col].mean().reset_index()\n    data.append(go.Scatter(\n        x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = df[col].values,\n        name = col\n    ))\n    \nlayout = go.Layout(dict(title = \"Treand of mean values\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","3dd3831f":"news_train_df.head()","ae204268":"print(f'{news_train_df.shape[0]} samples and {news_train_df.shape[1]} features in the training news dataset.')","49078649":"text = ' '.join(news_train_df['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline')\nplt.axis(\"off\")\nplt.show()","f7da6fb2":"# Let's also limit the time period\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2010-01-01 22:00:00+0000']","aa5618c0":"(news_train_df['urgency'].value_counts() \/ 1000000).plot('bar');\nplt.xticks(rotation=30);\nplt.title('Urgency counts (mln)');","8fd4d8ab":"news_train_df['sentence_word_count'] =  news_train_df['wordCount'] \/ news_train_df['sentenceCount']\nplt.boxplot(news_train_df['sentence_word_count'][news_train_df['sentence_word_count'] < 40]);","dee91996":"news_train_df['provider'].value_counts().head(10)","fc922a67":"(news_train_df['headlineTag'].value_counts() \/ 1000)[:10].plot('barh');\nplt.title('headlineTag counts (thousands)');","10048549":"for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","d9283a5b":"#%%time\n# code mostly takes from this kernel: https:\/\/www.kaggle.com\/ashishpatel26\/bird-eye-view-of-two-sigma-xgb\n\ndef data_prep(market_df,news_df):\n    market_df['time'] = market_df.time.dt.date\n    market_df['returnsOpenPrevRaw1_to_volume'] = market_df['returnsOpenPrevRaw1'] \/ market_df['volume']\n    market_df['close_to_open'] = market_df['close'] \/ market_df['open']\n    market_df['volume_to_mean'] = market_df['volume'] \/ market_df['volume'].mean()\n    \n    news_df['time'] = news_df.time.dt.hour\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['headlineLen'] = news_df['headline'].apply(lambda x: len(x))\n    news_df['assetCodesLen'] = news_df['assetCodes'].apply(lambda x: len(x))\n    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    lbl = {k: v for v, k in enumerate(news_df['headlineTag'].unique())}\n    news_df['headlineTagT'] = news_df['headlineTag'].map(lbl)\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n\n    lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\n    \n    market_df = market_df.dropna(axis=0)\n    \n    return market_df\n\nmarket_train_df.drop(['price_diff', 'assetName_mean_open', 'assetName_mean_close'], axis=1, inplace=True)\nmarket_train = data_prep(market_train_df, news_train_df)\nprint(market_train.shape)\nup = market_train.returnsOpenNextMktres10 >= 0\n\nfcol = [c for c in market_train.columns if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'assetCodeT', 'volume_to_mean', 'sentence_word_count',\n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', 'returnsOpenPrevRaw1_to_volume',\n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\nX = market_train[fcol].values\nup = up.values\nr = market_train.returnsOpenNextMktres10.values\n\n# Scaling of X values\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) \/ rng)","b1f7b84e":"X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.1, random_state=99)\n\n# xgb_up = XGBClassifier(n_jobs=4,\n#                        n_estimators=300,\n#                        max_depth=3,\n#                        eta=0.15,\n#                        random_state=42)","33ea0437":"params = {'learning_rate': 0.05, 'max_depth': 5, 'boosting': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'is_training_metric': True, 'seed': 42}\nmodel = lgb.train(params, train_set=lgb.Dataset(X_train, label=up_train), num_boost_round=2000,\n                  valid_sets=[lgb.Dataset(X_train, label=up_train), lgb.Dataset(X_test, label=up_test)],\n                  verbose_eval=50, early_stopping_rounds=30)","8b3afc52":"def generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: np.random.randint(0, 255), range(3)))\n    return color\n\ndf = pd.DataFrame({'imp': model.feature_importance(), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\ndata = [df]\nfor dd in data:  \n    colors = []\n    for i in range(len(dd)):\n         colors.append(generate_color())\n\n    data = [\n        go.Bar(\n        orientation = 'h',\n        x=dd.imp,\n        y=dd.col,\n        name='Features',\n        textfont=dict(size=20),\n            marker=dict(\n            color= colors,\n            line=dict(\n                color='#000000',\n                width=0.5\n            ),\n            opacity = 0.87\n        )\n    )\n    ]\n    layout= go.Layout(\n        title= 'Feature Importance of LGB',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis=dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n    )\n\n    py.iplot(dict(data=data,layout=layout), filename='horizontal-bar')","5ec4ea4f":"days = env.get_prediction_days()\nimport time\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n    \n    t = time.time()\n    market_obs_df = data_prep(market_obs_df, news_obs_df)\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X_live = market_obs_df[fcol].values\n    X_live = 1 - ((maxs - X_live) \/ rng)\n    prep_time += time.time() - t\n    \n    t = time.time()\n    lp = model.predict(X_live)\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    confidence = 2 * lp -1\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n    \nenv.write_submission_file()","c72aa1d1":"It is cool to be able to see how markets fall and rise again.\nI have shown 4 events when there were serious stock price drops on the market.\nYou could also notice that higher quantile prices have increased with time and lower quantile prices decreased.\nMaybe the gap between poor and rich increases... on the other hand maybe more \"little\" companies are ready to go to market and prices of their shares isn't very high.","2622deb9":"The file is too huge to work with text directly, so let's see a wordcloud of the last 100000 headlines.","bb6e6c74":"### Getting data and importing libraries","217e8b9b":"Now, let's look at these price drops in details.","ceaa8033":"We have two datasets, let's explore them separately.","455a7701":"![](http:\/\/fintechnews.ch\/wp-content\/uploads\/2016\/11\/Deutsche-Bank-Survey-87-of-Financial-Market-Participants-Say-Blockchain-Will-Disrupt-The-Industry-1440x564_c.jpg)","5fcb9d72":"We can see huge price fluctiations when market crashed. Just think about it... **But this is wrong!** There was no huge crash on January 2010... Let's dive into the data!","f237f98b":"\u043f\u043e\u043b\u0443\u0447\u0438\u0432 \u0442\u0430\u043a\u0438\u0435 \u0442\u0440\u0435\u0432\u043e\u0436\u043d\u044b\u0435 \u0441\u0438\u0433\u043d\u0430\u043b\u044b \u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u0430\u0432\u0442\u043e\u0440 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u0432\u044b\u0434\u0435\u043b\u044f\u044f \u0437\u0430\u0448\u043a\u0430\u043b\u0438\u0432\u0430\u044e\u0449\u0443\u044e \u0432\u043e\u043b\u0430\u0442\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432 20%. \u043d\u0443 \u0438 \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u0442 \u0442\u0430\u043a\u0438\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u043d\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u0430 \u043f\u0435\u0440\u0438\u043e\u0434. \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u0448\u0430\u0433 \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043a\u0430\u043a \u0438\u0437\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0442 \u0432\u044b\u0441\u043a\u043e\u0447\u0435\u043a","8e902032":"We can see that quantiles have a high deviation, but mean value doesn't change much.\n\nNow I think it is time to throw an old part of dataset. Let's leave only data since 2010 year, this way we will get rid of the data of the biggest crisis.","858c7122":"At first let's take 10 random assets and plot them.","a41563ab":"Fluctuations seem to be high, but in fact they are lower that 8 percent. In fact it looks like a random noise...","43b59429":"There were no spikes.","99b6984c":"Well, these were some random companies. But it would be more interesting to see general trends of prices.","6c95e1b6":"\u0442\u0443\u0442 \u0430\u0432\u0442\u043e\u0440 \u043e\u0446\u0435\u043d\u0438\u043b \u0432\u043e\u0437\u043d\u0438\u043a\u043d\u043e\u0432\u0435\u043d\u0438\u0435 \u0441\u043a\u0430\u0447\u043a\u043e\u0432 \u0432\u043e\u043b\u0430\u0442\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438 \u043a\u0440\u0438\u0437\u0438\u0441\u043e\u0432 \u043d\u0430 \u0440\u044b\u043d\u043a\u0435 . \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u043b \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043d\u0438\u043c\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438. \u0438 \u0441\u0434\u0435\u043b\u0430\u043b \u0432\u044b\u0432\u043e\u0434, \u0447\u0442\u043e \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0435\u0432\u0430\u043b\u0438\u0434\u043d\u044b\u0435, \u043f\u043e\u043f\u0440\u043e\u0441\u0442\u0443 \u0432\u0440\u0443\u0442","6947310f":"## Modelling\n\nIt's time to build a model!\nI think that in this case we should build a binary classifier - we will simply predict whether the target goes up or down.","d48db6fd":"Well, it seems that in fact urgency \"2\" is almost never used.","92002e2c":"![](https:\/\/i.imgur.com\/C3COWfe.png)","41f5f1e5":"### News data","2bc02366":"\u0438 \u0442\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0435\u043d\u044b \u043a \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438. \u043e\u043d \u043f\u0438\u0448\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435. \u044f \u0431\u044b \u0432\u043e\u043e\u0431\u0449\u0435 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0435\u043b \u043e\u0431\u0445\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u0432 ML \u0431\u0435\u0437 \u0438\u043d\u0442\u0443\u0438\u0446\u0438\u0438, \u043d\u043e \u0432\u0438\u0434\u0438\u043c\u043e \u043f\u043e\u043a\u0430 \u043d\u0435\u043b\u044c\u0437\u044f \u0431\u0435\u0437 \u043d\u0435\u0435 \u0434\u043b\u044f \u0443\u0441\u043f\u0435\u0448\u043d\u043e\u0433\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u044f","eca6ba83":"\u0422\u0443\u0442 \u0430\u0432\u0442\u043e\u0440 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u043a\u043e\u0442\u0438\u0440\u043e\u0432\u043a\u0438 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u0438\u0437\u043c\u0443 \u043a\u0432\u0430\u043d\u0442\u0438\u043b\u0435\u0439, \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u044f \u0432\u043e\u043b\u0430\u0442\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c (\u0442.\u0435. \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u043c\u043e\u0441\u0442\u044c) \u0430\u043a\u0446\u0438\u0439. \u0412\u043e\u043b\u0430\u0442\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043d\u0443\u0436\u043d\u0430 \u0434\u043b\u044f \u0441\u043f\u0435\u043a\u0443\u043b\u044f\u0442\u0438\u0432\u043d\u043e\u0439 \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u0438. \u0415\u0441\u043b\u0438 \u0430\u043a\u0446\u0438\u044f \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e \u0440\u0430\u0441\u0442\u0435\u0442, \u0435\u0435 \u0436\u0435 \u043d\u0438\u043a\u0442\u043e \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u043e\u0434\u0430\u0432\u0430\u0442\u044c, \u0432\u0435\u0440\u043d\u043e? \u0422.\u0435. \u0442.\u043d. \"\u0433\u043e\u043b\u0443\u0431\u044b\u0435 \u0444\u0438\u0448\u043a\u0438\"  \u0438 \u043f\u0435\u0440\u0438\u043e\u0434 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043c\u0435\u043d\u0435\u0435 \u0433\u043e\u0434\u043d\u044b \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u043f\u0435\u043a\u0443\u043b\u044f\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0434\u043e\u0445\u043e\u0434\u0430","84480a6f":"It isn't surprising that Reuters is the most common provider :)","0890cc3a":"\u041f\u0435\u0440\u0435\u0445\u043e\u0434\u0438\u043c \u043a \u0430\u043d\u0430\u043b\u0438\u0437\u0443 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439. 9.3\u041c \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u044d\u0442\u043e \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0441\u043e\u043a\u0440\u0430\u0449\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043e 100000 \u0441\u0442\u0440\u043e\u043a \u0441 \u043a\u043e\u043d\u0446\u0430. \u0421\u0442\u0440\u043e\u0438\u043c \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0445\u0441\u044f \u0441\u043b\u043e\u0432. \u0414\u0430\u043b\u0435\u0435 \u0430\u0432\u0442\u043e\u0440 \u043a\u0440\u0443\u0442\u0438\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u0432\u044b\u0434\u0435\u043b\u044f\u044f \u0432\u044b\u0441\u043a\u043e\u0447\u0435\u043a \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044f \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439","ce9d9b8e":"I plot data for all periods because I'd like to show long-term trends.\nAssets are sampled randomly, but you should see that some companies' stocks started trading later, some dissappeared. Disappearence could be due to bankruptcy, acquisition or other reasons.","44646300":"Now the graph is much more reasonable.","c8921a9f":"Let's look at the target variable now.","a20e63f8":"\u0410\u0432\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u0435\u0442 \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u044c \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u0432\u043e\u043b\u0430\u0442\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442 \u0437\u0430\u0431\u0438\u0442\u044c \u043d\u0430 \u043d\u0438\u0445.","f41c0b99":"At first I was sad that we don't have access to the texts of the news, but I have realized that we won't be able to use them anyway due to kernel memory limitations.","8d9fabc8":"Now let's remember the description:\n```\nThe marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n\n    Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another).\n    Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n    Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n    Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.\n```\n\nLet's have a look at means of these variables.","c41edd07":"Well, this isn't much considering we have more than 4 million lines and a lot of these cases are due to price falls during market crash. Well just need to deal with outliers.","94adb60d":"Well, most news are tagless.","405345cc":"## Market data\n\nWe have a really interesting dataset which contains stock prices for many companies over a decade!\n\nFor now let's have a look at the data itself and not think about the competition. We can see long-term trends, appearing and declining companies and many other things.","2b9bf3ee":"\u041f\u0440\u0438\u0432\u0435\u0442 \u0432\u0441\u0435. \u044f \u0440\u0443\u0441\u0441\u043a\u043e\u044f\u0437\u044b\u0447\u043d\u044b\u0439 \u043d\u043e\u0432\u0438\u0447\u043e\u043a \u0432 ML \u0438 \u0432\u043f\u0435\u0440\u0432\u044b\u0435 \u043e\u043a\u0443\u043d\u0443\u043b\u0441\u044f \u0432 \u043f\u043b\u0430\u0442\u043d\u043e\u0435 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0435. \u041c\u043e\u0438 \u0437\u0430\u043c\u0435\u0442\u043a\u0438 \u0441\u043a\u043e\u0440\u0435\u0435 \u0434\u043b\u044f \u0441\u0435\u0431\u044f, \u043d\u043e \u0432\u0434\u0440\u0443\u0433 \u043a\u043e\u043c\u0443-\u043d\u0438\u0431\u0438\u0434\u044c \u043e\u043d\u0438 \u0431\u0443\u0434\u0443\u0442 \u043f\u043e\u043b\u0435\u0437\u043d\u044b.\n\u042f \u0443\u0436\u0435 \u043f\u043e\u043d\u044f\u043b, \u0447\u0442\u043e \u043b\u0443\u0447\u0448\u0438\u0439 \u0441\u043f\u043e\u0441\u043e\u0431 \u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 - \u043f\u043e\u0447\u0438\u0442\u0430\u0442\u044c \u0447\u0443\u0436\u043e\u0439 \u043a\u043e\u0434 \u0438\u0437 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0445. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u0444\u043e\u0440\u043a\u043d\u0443\u043b \u0434\u0430\u043d\u043d\u044b\u0439 \u043a\u0435\u0440\u043d\u0435\u043b \u0438 \u0447\u0438\u0442\u0430\u044e \u0435\u0433\u043e. \u0412\u0441\u0435\u0433\u0434\u0430 \u0432\u044b\u0431\u0438\u0440\u0430\u044e \u043a\u0435\u0440\u043d\u0435\u043b \u0441 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c Exploratory Data Analysis (EDA) ","e7108d0d":"Now let's try to build that graph again.","c3085af6":"\u0422\u0443\u0442 \u0431\u044b\u043b\u0430 \u0444\u0438\u0448\u043a\u0430 \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u0440\u044b\u043d\u043a\u0430 \u043e \u0443\u043e\u0442\u0438\u0440\u043e\u0432\u043a\u0430\u0445 \u0431\u0443\u043c\u0430\u0433 \u043d\u0435 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u0440\u043e\u0441\u0442\u043e \u0442\u0430\u043a. \u041e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0442\u043e\u0440 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0410\u041f\u0418 \u043f\u043e \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u044f\u043c\u043e \u0432 \u043a\u0435\u0440\u043d\u0435\u043b, \u043c\u0438\u043d\u0443\u044f \u0432\u0430\u0448 \u041f\u041a.","19b50f58":"For a quick fix I'll replace outliers in these lines with mean open or close price of this company.","e148022d":"Now let's take a look at out target variable.","fcf1b5d8":"I think it is quite funny that Apple is a company with most both negative and positive sentiments.","375012bf":"\u0434\u0430\u043b\u0435\u0435 \u0430\u0432\u0442\u043e\u0440 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442 \u0441\u0443\u0431\u044c\u0435\u043a\u0442\u0438\u0432\u043d\u043e \u043e\u0434\u043d\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u044f \u0435\u0433\u043e \u043f\u043e\u0432\u0442\u043e\u0440\u044e.","95dcad56":"There are some big outliers, but sentences mostly have 15-25 words in them.","dc4a08fc":"## General information\n\nTwo Sigma Financial News Competition is a unique competitions: not only it is a Kernel-only competition, but we aren't supposed to download data and during stage two our solutions will be used to predict future real data.\n\nI'll try to do an extensive EDA for this competition and try to find some interesting things about the data.\n\nP. S. I'l learning to use plotly, so there will be interactive charts at last!\n\n*The work is in progress.*","e73989f2":"So, let's try to find strange cases.","2c48cc58":"\u0442.\u043a. \u0441\u0442\u0430\u0442\u044c\u0438 \u0443\u0436\u0435 \u043f\u0440\u043e\u0442\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043a\u0430\u043a \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0438 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435, \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0430\u043c \u0442\u0435\u043a\u0441\u0442 \u0438 \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u044d\u043c\u043e\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u043e\u043a\u0440\u0430\u0441\u043a\u0443. \u0414\u0430 \u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u0430 \u043a \u0442\u0435\u043a\u0441\u0442\u0430\u043c \u043d\u0435\u0442.","341d6f59":"So price of \"Towers Watson & Co\" shares was almost 10k... I think this is simply an error in data.\n\nBut what about Bank of New York Mellon Corp?\n\nLet's see data by Yahoo:","f02c115e":"\u0442\u0435\u043f\u0435\u0440\u044c \u0433\u0440\u0430\u0444\u0438\u043a \u043a\u0432\u0430\u043d\u0442\u0438\u043b\u0435\u0439 \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u0435\u043d. \u0447\u0435\u043c \u0431\u043e\u043b\u0435\u0435 \u0434\u0430\u043b\u0435\u043a\u0438\u0439 \u043a\u0432\u0430\u043d\u0442\u0438\u043b\u044c, \u0442\u0435\u043c \u043c\u0435\u043d\u0435\u0435 \u043e\u043d \u0432\u0435\u0440\u043e\u044f\u0442\u0435\u043d.\n\u0421\u043d\u043e\u0432\u0430 \u043f\u0440\u043e\u0431\u0435\u0436\u0438\u043c\u0441\u044f \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u0432 \u043f\u043e\u0438\u0441\u043a\u0430\u0445 \u0441\u0443\u043f\u0435\u0440\u0432\u043e\u043b\u0430\u0442\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u0438. \u0418 \u0442\u0430\u043a\u043e\u0439 \u043d\u0435\u0442, \u0444\u043b\u0443\u043a\u0442\u0443\u0430\u0446\u0438\u044f \u0443\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0432 8%. \u0416\u0438\u0437\u043d\u0435\u043d\u043d\u043e.","de4f3170":"Well, for me it is difficult to interpret this, but it seems that returns for previous 10 days fluctuate the most.","ab2d0a95":"Another case is with cost equal to 999, such numbers are usually suspicious. Let's look at Archrock Inc - no spikes there as well.\n\n![](https:\/\/i.imgur.com\/KYZKkSd.png)","69e05503":"### Possible data errors\n\nAt first let's simply sort data by the difference between open and close prices."}}