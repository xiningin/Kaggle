{"cell_type":{"c9f6df94":"code","c69568b9":"code","0bc5fd80":"code","47d815f5":"code","69af83cf":"code","b4201b1f":"code","fc4f37c0":"code","437fbfce":"code","c92c23fb":"code","592f2393":"code","9f4f3f8e":"code","3862b2a8":"code","b4769f21":"code","0c458f60":"code","0c50bb16":"code","94ed4205":"code","3515718a":"code","32752a1c":"code","f54936f7":"code","06ee1b67":"code","1004e497":"code","a8c399ec":"code","5eaebf5c":"code","8fe03c33":"code","0af85ab4":"code","4460c71e":"code","cdf0a3b9":"code","4ec03738":"code","4d92198c":"code","b7b8c5c7":"code","21cdcc78":"code","177ed492":"code","a3ed916e":"code","9bd33e77":"code","7ed4e574":"code","e5144834":"code","55cb0f49":"code","841ac3fe":"code","6b8efd4e":"code","58f7a540":"code","0acd101b":"code","02fea557":"code","fe424a57":"code","aa9b6a44":"code","c1c7f8df":"code","925d0054":"code","f32716bf":"code","428417b9":"code","76083825":"markdown","7ae6952d":"markdown","7d67c1d2":"markdown","95778e93":"markdown","9afc80c1":"markdown","d150756f":"markdown","fbd100c9":"markdown","b9b78d1f":"markdown","596000c0":"markdown","af2e6077":"markdown","23ed09cb":"markdown","86aa5883":"markdown","eef8f5c7":"markdown","99f4c30d":"markdown","2b53dcdb":"markdown","a81c9373":"markdown","2e397930":"markdown","d2cff7c4":"markdown","3d87a5e2":"markdown","93663475":"markdown","1329e312":"markdown","2d7e56a9":"markdown","5d45d1fb":"markdown","6b4e42c6":"markdown","b82f777a":"markdown","0aa2f2b9":"markdown","00c96203":"markdown","b4359963":"markdown","7eed65e7":"markdown","9f398e75":"markdown","f47c3af7":"markdown","fd2dc6cc":"markdown","e5a36ab6":"markdown","a9aba7b2":"markdown","60446c57":"markdown","bad82ad6":"markdown","62b91080":"markdown","ee5d9910":"markdown","74a60042":"markdown"},"source":{"c9f6df94":"!pip install lifetimes\nimport pandas as pd\nimport lifetimes\nfrom datetime import datetime\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","c69568b9":"orders = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\ncustomers = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_customers_dataset.csv')\npayments = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv')","0bc5fd80":"orders.head(5)","47d815f5":"customers.head(5)","69af83cf":"payments.head(5)","b4201b1f":"orders['order_status'].value_counts()","fc4f37c0":"orders = orders[orders['order_status'] == 'delivered']\norders.shape","437fbfce":"# We are left with only the columns we need\n\norders = orders[['customer_id', 'order_id', 'order_purchase_timestamp']]\ncustomers = customers[['customer_id', 'customer_unique_id']]\npayments = payments[['order_id', 'payment_value']]","c92c23fb":"# We perform the union of the datasets by their respective primary keys\n\ndf = pd.merge(orders, customers, how='inner', on='customer_id')\ndf = pd.merge(df, payments, how='inner', on='order_id')\ndf","592f2393":"df = df.drop(['customer_id', 'order_id'], axis=1)","9f4f3f8e":"df.isnull().sum(axis=0)","3862b2a8":"df.drop_duplicates(inplace=True)\ndf.shape","b4769f21":"df['customer_unique_id'].nunique()","0c458f60":"df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\ndf['order_purchase_timestamp'] = df.order_purchase_timestamp.dt.date\ndf['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\ndf['order_purchase_timestamp'].describe()","0c50bb16":"# The last purchase made in the dataset was on August 29, 2018, so we will use that date as our current \n# date to simulate an immediate study of the company's transactions.\n\ntoday = '2018-08-29'\ndate_today = datetime.strptime(today, '%Y-%m-%d')","94ed4205":"r = df.groupby('customer_unique_id').agg(['min', 'max'])['order_purchase_timestamp']\nr['recency'] = r['max'] - r['min']","3515718a":"r['T'] = date_today - r['min']\nr = r[['recency', 'T']]\n\n# Let's take a look at our new variables\nr","32752a1c":"aggregations = {\n    'order_purchase_timestamp':'count',\n    'payment_value': 'sum'}\nf = df.groupby('customer_unique_id').agg(aggregations)\nf['frequency'] = f['order_purchase_timestamp'] - 1\nf = f[['frequency']]","f54936f7":"# Let's create our final table joining both\n\nrf = pd.merge(r,f, left_index=True, right_index=True)\nrf","06ee1b67":"from lifetimes.utils import summary_data_from_transaction_data\n\nrfm = summary_data_from_transaction_data(df, customer_id_col='customer_unique_id', datetime_col='order_purchase_timestamp', \n                                    monetary_value_col ='payment_value', observation_period_end='2018-08-29', \n                                    datetime_format='%Y-%m-%d', freq='W')\nrfm","1004e497":"px.histogram(rfm, x=rfm['frequency'],title='Frequency of purchase',\n                   labels={'frequency':'Frequency'}, \n                   opacity=0.8, marginal='violin',\n                   color_discrete_sequence=['indianred'])","a8c399ec":"px.histogram(rfm, x=rfm['recency'],title='Recency of purchase',\n                   labels={'recency':'Recency'}, nbins=50,\n                   opacity=0.8, marginal='violin',\n                   color_discrete_sequence=['indianred'])","5eaebf5c":"px.histogram(rfm, x=rfm['T'],title='Time from first purchase',\n                   labels={'T':'Weeks'}, \n                   opacity=0.8, marginal='violin',\n                   color_discrete_sequence=['indianred'])","8fe03c33":"from lifetimes import BetaGeoFitter\n\n# penalizer_coef es un par\u00e1metro que penaliza al likelihood, usualmente se utilizan valores como 0.001 o \n# 0.01 con muestras peque\u00f1as para evitar que los par\u00e1metros se vuelvan demasiado grandes\n\nbgf = BetaGeoFitter(penalizer_coef=0.001)\nbgf.fit(rfm['frequency'], rfm['recency'], rfm['T'], verbose=True)\nprint(bgf)","0af85ab4":"bgf.summary","4460c71e":"from lifetimes.plotting import plot_frequency_recency_matrix\n\n# T = Unit times(weeks)\n\nfig = plt.figure(figsize=(12,8))\nplot_frequency_recency_matrix(bgf, T=4)","cdf0a3b9":"from lifetimes.plotting import plot_probability_alive_matrix\n\nfig = plt.figure(figsize=(12,8))\nplot_probability_alive_matrix(bgf)","4ec03738":"t = 4 # Weeks for a future transaction \nrfm['expected_4week'] = round(bgf.conditional_expected_number_of_purchases_up_to_time(t, rfm['frequency'], rfm['recency'], rfm['T']), 2)\nrfm.sort_values(by='expected_4week', ascending=False)","4d92198c":"rfm['expected_8week'] = round(bgf.predict(8, rfm['frequency'], rfm['recency'], rfm['T']), 2)\nrfm['expected_12week'] = round(bgf.predict(12, rfm['frequency'], rfm['recency'], rfm['T']), 2)","b7b8c5c7":"t=12\nrandom_person = rfm.iloc[65432]\nprediction = bgf.predict(t, random_person['frequency'], random_person['recency'], random_person['T'])\n\nprint(\"Customer '{}' has \".format(random_person.name), round(prediction, 2), \"% of probability to buy an item for the next {} weeks\".format(t))","21cdcc78":"from lifetimes.utils import calibration_and_holdout_data\n\nrfm_val = calibration_and_holdout_data(df, customer_id_col='customer_unique_id', datetime_col='order_purchase_timestamp', \n                                    monetary_value_col ='payment_value', calibration_period_end='2018-05-29',\n                                    observation_period_end='2018-08-29', datetime_format='%Y-%m-%d', freq='W')\n\n\nrfm_val.head(5)","177ed492":"bgf_val = BetaGeoFitter(penalizer_coef=0.001)\nbgf_val.fit(rfm_val['frequency_cal'], rfm_val['recency_cal'], rfm_val['T_cal'], verbose=True)\nprint(bgf_val)","a3ed916e":"from lifetimes.plotting import plot_period_transactions\n\nfig = plt.figure(figsize=(12,8))\nax = plot_period_transactions(bgf_val)\nax.set_yscale('log')","9bd33e77":"from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases\n\nfig = plt.figure(figsize=(12,8))\nplot_calibration_purchases_vs_holdout_purchases(model=bgf_val, calibration_holdout_matrix=rfm_val)","7ed4e574":"rfm_gg = rfm[rfm['frequency'] > 0]\nlen(rfm_gg)","e5144834":"rfm_gg[['monetary_value', 'frequency']].corr()","55cb0f49":"from lifetimes import GammaGammaFitter\n\nggf = GammaGammaFitter(penalizer_coef = 0.0)\nggf.fit(rfm_gg['frequency'], rfm_gg['monetary_value'])\nprint(ggf)","841ac3fe":"rfm['avg_transaction'] = round(ggf.conditional_expected_average_profit(rfm_gg['frequency'],\n                                                     rfm_gg['monetary_value']), 2)\n\nrfm['avg_transaction'] = rfm['avg_transaction'].fillna(0)\nrfm.sort_values(by='avg_transaction', ascending=False)","6b8efd4e":"rfm['CLV'] = round(ggf.customer_lifetime_value(bgf, rfm['frequency'],\n                    rfm['recency'], rfm['T'], rfm['monetary_value'],\n                    time=26, discount_rate=0.01, freq='W'))\n\nrfm.sort_values(by='CLV', ascending=False)","58f7a540":"clusters = rfm.drop(rfm.iloc[:, 0:4], axis=1)","0acd101b":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled = scaler.fit_transform(clusters)\nscaled","02fea557":"from sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 6):\n    kmeans = KMeans(n_clusters=i, max_iter=1000, random_state=0)\n    kmeans.fit(scaled)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 6), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","fe424a57":"model = KMeans(n_clusters = 3, max_iter = 1000)\nmodel.fit(scaled)\nlabels = model.labels_","aa9b6a44":"clusters['cluster'] = labels\nclusters['cluster'].value_counts()","c1c7f8df":"from sklearn.decomposition import PCA\n\nfeatures = [\"expected_4week\", \"expected_8week\", \"expected_12week\", \"avg_transaction\", \"CLV\"]\n\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(clusters[features])\nnpc = np.array(components)\ndfc = pd.DataFrame(npc, columns=['PC1','PC2'])\ndfc = dfc.set_index(clusters.index)\ndfc['label'] = clusters['cluster']\ndfc","925d0054":"px.scatter(dfc, x=\"PC1\", y=\"PC2\", color=\"label\")","f32716bf":"clusters.groupby('cluster').agg(['max','min'])['CLV']","428417b9":"clusters['cluster'].replace(to_replace=[0,1,2], value = ['Non-Profitable', 'Very Profitable', 'Profitable'], inplace=True)\nclusters.sort_values(by='CLV', ascending=False)","76083825":"### Why should we care it?\n\n- It allows us to segment our clients according to their contributions.\n- Identify patterns of behavior in our target audience.\n- It enables better management of commercial investment for each consumer.\n- Make an approximate prediction about future income to the company.\n- Improve more personalized loyalty programs.\n\nTherefore we can understand that improving the CLTV means increasing the amount of time in which this consumer is spending money in our company.","7ae6952d":"Now that we have our final variable, we will move on to the last step of this case, the clustering of our consumers based on the profitability provided to the company. For this we can already eliminate our initial variables that we use to build the models.","7d67c1d2":"<center><img src=\"https:\/\/www.wearemarketing.com\/uploads\/media\/default\/0001\/21\/033ed59c7fa7a026804f9507fa7fc3a390390667.jpeg\" width=\"800\"><\/center>","95778e93":"### Creating T\nThis same table helps us to create our variable T.","9afc80c1":"## Validating our model\n\nWell we have our predictions, but we cannot blindly use the results of a model, we need to validate them with reality. When we talk about making predictions for the future, instead of doing the classic partition of the sample in Train and Test, we do it with respect to time. We will use the last three months of results whose numbers we already know and compare them with what we obtain with our BG\/NBD model, for this task the *Lifetimes* library has a very useful function that divides our data set according to the indicated date.\n","d150756f":"## Preprocess\n\n### Importing libraries","fbd100c9":"Our metrics are divided into *cal* and *holdout*, the first being those used in training and the second being the result in the two months that we established, while duration refers to the time in weeks that was taken for the test. Now it is time to train this new table with the same model that we configured previously.","b9b78d1f":"We are left with a sample of 96,478 completed orders.\n\n### Merging our data\n\nLet's start with merging the three datasets into one from which we can extract the variables we need.","596000c0":"One of the faculties of this model is to calculate the conditional expectation for the average profit per transaction for a group of one or more customers.","af2e6077":"Let's continue doing basic preprocessing, checking for null or duplicate data.","23ed09cb":"We can see that in the Orders dataset we have a column called *order_status* which includes the current status of each order. We are only interested in working with the orders that have been concluded so far, so we must filter them.\n","86aa5883":"We have already trained our model, we can make greater use of the library with the following visualizations.\n\n**Frequency recency matrix:** This matrix shows us the probability of future customer purchases in a given time, using recency and frequency as estimators.\n\n**Probability alive matrix:** Like the previous one, using recency and frequency as indicators, this matrix indicates the probability that a client is \"alive\" at the current moment.","eef8f5c7":"At first glance, we see that our cluster \"0\" makes a clear reference to the entire group of sporadic shopping users that represented the vast majority of our e-commerce. We can get a better visualization of these new segments by using PCA to reduce our variables to two and be able to translate them into a scatter plot.","99f4c30d":"### What is its formula?\n\nThere are several ways to calculate this indicator, which vary by the periods of time with which one works or the expert who is dealing with the subject. But to facilitate its understanding we will use the most intuitive of them:\n\n> CLTV = Average spending x  Frequency of purchase x Lifetime of client\n\n","2b53dcdb":"As we can see, both probabilities increase the greater the number of purchases and the time the customer had on his last purchase. It should be noted that the probabilities of future purchases are quite low in the next four weeks and are restricted to a few customer quadrants, this can be easily explained by the trend in sporadic purchases in our dataset.\n\nBased on our model we can create predictions for the period of time we choose, in our case we will do it for the first 4, 8 and 12 weeks, these being a strong short-term indicator to increase our advertising efforts in users with the greatest possibility of invest in our company.","a81c9373":"## Visualizing our customers","2e397930":"Here we have the final merge, but it is necessary to make a differentiation between the variables that we have: **order_id** clearly refers to the code of the order carried out, on the other hand we have **customer_id** which is the code assigned to the user by purchase (that is, the code varies from session to session, even if the person is the same) and finally **customer_unique_id** which actually reflects the user as a person, which is what we need to calculate their respective CLTV. Due to that we will proceed to eliminate the other two columns that do not serve us for the case.","d2cff7c4":"In the case of the customer's lifetime in his last purchase, we can observe a distribution more similar to a Gaussian distribution, with 40 weeks as the mode. We can also observe a greater concentration in the left area, corroborating that most of them are sporadic purchases. Seeing this, let's move on to our first model.\n\n## BetaGeoFitter\n\nAlso known as BG \/ NBD, this model is intended to estimate transactions in a future period of time for each user, in addition to the probability that this is \"alive\". This is an alternative to the well-known Pareto \/ NBD model which uses the Bayesian probability in a hierarchical way to make its estimates. Its main difference with Pareto is that BG assumes that customer abandonment occurs immediately after the purchase of the product, while its alternate assumes that abandonment can occur at any time. Without delving further into theory, let's move on to its application in our data set.\n\nhttp:\/\/brucehardie.com\/papers\/018\/fader_et_al_mksc_05.pdf","3d87a5e2":"## Briefing\n\nNow that we have seen a little theory about this powerful metric and how its implementation can bring great benefits in the short and long term for our company, it is time to see the case that we will discuss today.\n\n### Objective\n\nOur main objective is to calculate the Lifetime Value per customer so that, based on them, segments can be created based on the profitability they bring to the company.\n\n### Data information\n\nThe dataset we will work with was provided by Olist Store, the largest department store in Brazilian marketplaces. Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Therefore, our dataset contains a large number of purchases made between the 2016-2018 periods in different stores that used the Olist service.\n\n- **Reliability:** High, the data is completely real and has been anonymized for the safety of the users.\n\n- **Bias:** In advance, understand that the transactions belonging to different stores, with such varied items and segments, will not follow a standard pattern and the most probable is that they will not have a normal distribution. But to explore the subject and verify the scope of the Lifetimes library it will be quite useful.\n\n- **License:** CC BY-NC-SA 4.0.\n\n- **Repository:** Kaggle.\n\n- **Link:** https:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce\n\nNow that we have the data, our next step is to prepare it for analysis.","93663475":"We can see that our unique users are 93357, it is to each of them that we must calculate new variables. This is where an important point about the *Lifetimes* library comes in, and that is that to make use of its models it is necessary to present 4 specific variables called recency, frequency, T and monetary value. Do not worry, I will proceed to explain what each one of them is about:\n\n- **Frequency:** Represents the number of repeat purchases the customer has made. This means that it\u2019s one less than the total number of purchases.\n- **Recency:** Represents the age of the customer when they made their most recent purchases. This is equal to the duration between a customer\u2019s first purchase and their latest purchase. (Thus if they have made only 1 purchase, the recency is 0.)\n- **T:** Represents the age of the customer in whatever time units chosen. This is equal to the duration between a customer\u2019s first purchase and the end of the period under study.\n- **Monetary value:** Represents the average value of a given customer\u2019s purchases. This is equal to the sum of all a customer\u2019s purchases divided by the total number of purchases. Note that the denominator here is different than the frequency described above.\n\nNow that we know which variables we are going to use, if we see our created dataset again we have everything to calculate them, that will be our next step.\n\n## Feature Engineering\n\nLet's start by formatting our date variable and exploring its minimum (first purchase) and maximum (last purchase) values.","1329e312":"We saw that the K-Means model did a good job clustering our users, but my intuition together with the result of the graph and the number of users per cluster tell me that the order of the clusters does not really represent the importance of the client with respect to the cost effectiveness. Let's corroborate this mini theory by looking at the maximums and minimums of each cluster.","2d7e56a9":"As with the previous variable, most users have zero recency, having made only a single purchase in their time as a customer.","5d45d1fb":"The column that we have just created gives us the approximate amount that the customer will make in a future purchase, but if we review well we will find that this metric does not really tell us anything by itself, because customers with the highest number of transaction are unlikely to buy in the following periods. In summary, this new indicator is conditioned on the assumption that a purchase is actually made.\n\nTo find the value we are looking for, and which is found in the title of this article, we will use another module of this model which uses the model we created previously to estimate the CLV of each client in the period we choose. For our reality we will use an estimate of the next 26 weeks (6 months) to calculate the Lifetime Value.\n- *discount_rate* refers to external factors that may influence the company's income, such as currency inflation, new company taxes, among others.","6b4e42c6":"### Creating Frequency\nAs before we will group by unique user to create this variable.","b82f777a":"In turn, we can have more molecular analyzes, by seeing the individual probabilities for each client:","0aa2f2b9":"I consider that the number three is the most appropriate for our segmentation, due to being a symbolically very powerful number in time and of course this is the result of our graph. With this number of clusters we can now train our model with the scaled database and attach the created labels to our initial table.","00c96203":"## Clustering our forecasts\n\nWe have four variables to be able to carry out our segmentation, for this we will use an unsupervised Machine Learning method already known to many, K-Means. To make use of it, it is necessary to normalize our variables, since as this is a model that works with the distances of the data, the dimensions and magnitudes play a very important role in its implementation.","b4359963":"For our convenience, the *Lifetimes* library includes a module that takes care of this cumbersome process automatically. We will work with this table created by the library, since it includes the variable **monetary_value** and allows us to define the period of time with which we want to work (in our case we will choose weekly since our dataset belongs to different industries, this is a regular period of time).","7eed65e7":"### First look to our datasets\n\nLet's start with a first look at the data we have, in this case we will only use the Orders, Customer and Payments datasets. Since these three contain the variables that we will need to make use of the Lifetimes library and its models.","9f398e75":"In this last graph we can see how the model starts making predictions quite close to the margin of error, said distance increases as the number of purchases increases until it shoots up violently, this is most likely due to some anomalous data that the model did not reach to take into account. In any case, our model maintains a coherent exponential growth and is similar in some points to the reality of the business. Extending the time that is taken for the test could improve the validation of our model.\n\n## Gamma-Gamma model\n\nWe have predictions for our next few months but we do not know exactly what will be the remuneration that we will obtain from our clients, a client who buys 13 times an average of 7 dollars is not the same as another who buys 2 times 300 dollars. This is when the second model in this case will help us to give an estimate of the profitability of each client, using the *monetary_value* column which we had left until now.\n\n\nWe begin first of all with the fact that one of the requirements of this model is to work only with observations with a purchase frequency greater than 0, we see that our sample is considerably reduced to 1918 subjects who are still well to be able to train a model of this type.","f47c3af7":"Our first validation visualization shows the comparison between the frequency of real transactions and those established by the new model. We can see that as frequencies increase, the model performs a worse job of predicting, this may be due to the great heterogeneity of e-commerce as mentioned above or to the lack of a sufficient sample to establish reliable estimates.\n- It should be noted that a logarithmic scaling is carried out to be able to visualize the high frequencies, since compared to frequency 0 their heights are very small.","fd2dc6cc":"Our second visualization will give us a more accurate understanding of the effectiveness of our model estimates, when comparing actual purchases versus predictions.","e5a36ab6":"When choosing the number of clusters to create, it is advisable to discuss it with the department involved and set a number that responds to the nature of the business and the needs of the problem. But in our case we will use the famous \"Elbow Method\" that will indicate the number of clusters that reduces the inertia (closeness of the points to their centroid) to a relevant point.","a9aba7b2":"The purpose of this case study is to explore the Lifetimes library, created by Cameron Davidson-Pilon, in its application for the prediction and calculation of the Lifetime value for each consumer of a Brazilian E-commerce, for its subsequent segmentation in order to optimize of resources and commercial efforts for the company.\n\n## What is the CLTV\/CLV?\n\nBetter known as Customer Lifetime Value, this is a metric from the marketing world which, in short, reflects the profit that a certain customer will provide us in a given period of time. This allows us to get out of the subjectivism of how much a client is worth, and land that value in a more workable field related to the profitability that it brings to the company.","60446c57":"Indeed, group 1 represents the group with the highest profitability, while group 2 represents the group with moderate profitability. Let's finish our final table by assigning the corresponding names to each segment.","bad82ad6":"We begin our case study with a typical relational database of various E-Commerce, ending with a segmentation of each of its users regarding their probability of future purchase and the profitability of the company. The *Lifetimes* package is a very powerful package, although at the beginning it feels like a black box, the study and understanding of each of its functions together with the validation of its models, makes it an ally for all that company with the intention to know your consumers better and optimize your investment in each area of contact with the customer. I hope you reader have enjoyed this case and it has been \"profitable\" for a better understanding of both the library and the application capacity it brings to the business.\n\n## Recomendations\n\n- A better performance of each model should be given with the application in more homogeneous databases referring to each company.\n- The use of more specific variables at the time of clustering can give more precise and enriching results for the area to which the study is intended.\n- *Lifetimes* has other models based on Bayesian probability, each with its own advantages and particular applications. Studying them and seeing if they fit the case at hand could yield better results.\n","62b91080":"Here we find an important point which can influence our model, the vast majority of buyers only made one transaction in their entire lifetime in the company (remember that frequency is calculated by subtracting the number of purchases minus 1).","ee5d9910":"### Creating Recency\n\nWe will group by user in a new table.","74a60042":"On the other hand, we need to make sure that there is no correlation between the frequency of purchase and the monetary value of each consumer. This is due to the fact that the Gamma-Gamma model assumes that there is no such correlation. In practice, check if the Pearson correlation between the two vectors is close to 0 in order to use this model."}}