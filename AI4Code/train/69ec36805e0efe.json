{"cell_type":{"8259f8e9":"code","255dae1a":"code","d04723ce":"code","be352004":"code","fe440d5d":"code","899d0314":"code","285ff65a":"code","af842354":"code","03491f6f":"code","f6725760":"code","64440282":"code","a8bd0b18":"code","462912d2":"code","ada2709c":"code","a263027b":"code","abb505c9":"code","bbf225ac":"code","af6fe22f":"code","e4845592":"code","e3f9643b":"code","f85ee4c6":"code","7a852c6c":"code","13c0b350":"code","836aaf03":"code","2c5c4564":"code","bd5928fd":"code","f63d3f23":"code","dc8d80db":"code","12887364":"code","2af66d7a":"code","b413ca28":"code","7b579404":"code","9111e724":"code","9dd3756f":"code","21b595cf":"code","f5abc01a":"code","db1784cf":"code","44b31fc3":"code","7b6e5922":"code","72db37c2":"code","4135caa9":"code","e4f26542":"code","e42a98f1":"code","000dd18f":"code","8d37c6f0":"code","eb132725":"code","11e929d5":"code","0d012240":"code","e9927224":"code","63ccb76d":"markdown","23bb39bd":"markdown","6d4879fa":"markdown","e9c89d7e":"markdown","1a6153c4":"markdown","78d4a3ba":"markdown","760b9044":"markdown","de35bbcf":"markdown","0065e775":"markdown","2e1f0837":"markdown","93ff2a7b":"markdown","8d26935e":"markdown","2e0183c0":"markdown","0aa071a2":"markdown","3ed0aa88":"markdown","c42d9fe0":"markdown","22e3a911":"markdown","cbd1a969":"markdown","426a70ec":"markdown","00d8fa52":"markdown","b60991de":"markdown","83e4521f":"markdown","d7abfc67":"markdown","1ac75af0":"markdown","9667a186":"markdown","322b5761":"markdown","826bc58f":"markdown","ec37f007":"markdown","f34cea8c":"markdown","6e4dd3e1":"markdown","8ddb77e4":"markdown","06fbff0a":"markdown","10c0b23f":"markdown","ee4cf376":"markdown","4fcc8fc7":"markdown","4ed464f5":"markdown","375848f4":"markdown","0250dc6f":"markdown","0d54bbb7":"markdown","0c0d536b":"markdown"},"source":{"8259f8e9":"import numpy as np # linear algebra - arrays & matrices\nimport pandas as pd # for data structures & tools, data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp #for integrals, solving differential equations, optimization\nimport matplotlib.pyplot as plt #for plots, graphs - data visualization\n%matplotlib inline \nimport seaborn as sns #plots - heat maps, time series & violin plots\nimport sklearn as sklearn #machine learning models\nimport statsmodels as stmodels #explore data, estimate statistical models, & perform statistical test\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","255dae1a":"#load train dataset\ndata = '..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv'\ndataset = pd.read_csv(data) #for those datasets without headers can use dataset = pd.read_csv(data, header = None)\ndataset_withoutheaders = pd.read_csv(data, header = None)\ndataset.shape","d04723ce":"dataset.columns #finding name of columns of the dataset","be352004":"dataset.head()","fe440d5d":"dataset_withoutheaders.head()","899d0314":"#checking datatypes of features\n\ndataset.dtypes","285ff65a":"dataset.describe() #returns a statistical summary - but this method does not include object datatype columns, basically skip rows & columns that do not contain numbers","af842354":"#for full summary with every column & row\ndataset.describe(include = \"all\")","03491f6f":"dataset[['sl_no', 'salary', 'gender']].describe(include = \"all\") #describing only sl_no & salary columns in the dataset, notice I can arrange the way columns appear sl_no --> salary --> gender\n\n#include = \"all\" is added to display gender column since it is an object type, else not needed","f6725760":"dataset.info()","64440282":"#DB-API\nfrom dbmodule import connect\n\n#Create a connection object\nconnection = connect('databasename', 'username', 'password')\n\n#Create a cursor object\ncursor = connection.cursor()\n\n#Run queries\ncursor.execute('select * from tablename')\nresults = cursor.fetchall()\n\n#Free resources\nCursor.close()\nconnection.close()","a8bd0b18":"#output missing data\nmissing_data = dataset.isnull()\nmissing_data.head(10)","462912d2":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","ada2709c":"dataset.head(10)","a263027b":"dataset.dropna(subset=[\"salary\"], axis = 0, inplace = True) #axis = 0 will drop the entire row with NaN value & axis = 1 will drop the entire column with NaN value, inplace = True will write the result back to the original dataset\n\n#dataset.dropna(subset=[\"salary\"], axis = 0, inplace = True) is the same as dataset = dataset.dropna(subset=[\"salary\"], axis = 0)\n#if want to drop all rows with NaN value regardless of column\/variable just use dataset.dropna(axis = 0, inplace = True)","abb505c9":"dataset.head(10)","bbf225ac":"#reset the dataset to original\ndataset = pd.read_csv(data)\ndataset.head(10)","af6fe22f":"#dataframe.replace(missing_value, new_value)\n#replacing with mean value\nmean = dataset[\"salary\"].mean()\ndataset[\"salary\"].replace(np.nan, mean, inplace = True)\ndataset.head(10)","e4845592":"dataset[['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary']].head(10)","e3f9643b":"#Simple Feature Scaling\ndataset[\"salary\"] = dataset[\"salary\"]\/data[\"salary\"].max()\n\n#Min-Max Scaling\ndataset[\"salary\"] = (dataset[\"salary\"] - dataset[\"salary\"].min())\/(dataset[\"salary\"].max()-dataset[\"salary\"].min())\n\n#Z-Score Sacling\ndataset[\"salary\"] = (dataset[\"salary\"]-dataset[\"salary\"].mean())\/dataset[\"salary\"].std()","f85ee4c6":"bins = np.linspace(min(dataset[\"salary\"]), max(dataset[\"salary\"]),4) #to have 3 bins, need 4 equally spaced numbers hence 4 in code\ngroup_names = [\"Low\", \"Medium\", \"High\"]\ndataset[\"salary-binned\"] = pd.cut(dataset[\"salary\"], bins, labels = group_names, include_lowest = True)","7a852c6c":"dataset.head()","13c0b350":"%matplotlib inline\nimport matplotlib as plt\nfrom matplotlib import pyplot\nplt.pyplot.hist(dataset[\"salary\"])\n\n# set x\/y labels and plot title\nplt.pyplot.xlabel(\"Salary\")\nplt.pyplot.ylabel(\"Count\")\nplt.pyplot.title(\"Salary Bins\")","836aaf03":"pyplot.bar(group_names, dataset[\"salary-binned\"].value_counts())\n\n# set x\/y labels and plot title\nplt.pyplot.xlabel(\"Salary\")\nplt.pyplot.ylabel(\"Count\")\nplt.pyplot.title(\"Salary Bins\")","2c5c4564":"dataset.gender.unique()","bd5928fd":"#To see which values are present in a particular column, we can use the \".value_counts()\" method:\ndataset['gender'].value_counts()","f63d3f23":"#We can see that males are the most common type. We can also use the \".idxmax()\" method to calculate for us the most common type automatically:\ndataset['gender'].value_counts().idxmax()","dc8d80db":"dummy_variable_1 =  pd.get_dummies(dataset[\"gender\"])\ndummy_variable_1.head()","12887364":"dataset.head(10)","2af66d7a":"# merge data frame \"dataset\" and \"dummy_variable_1\" \ndataset = pd.concat([dataset, dummy_variable_1], axis=1)\n\n# drop original column \"gender\" from \"dataset\"\ndataset.drop(\"gender\", axis = 1, inplace=True)","b413ca28":"dataset.head(10)","7b579404":"#reset the dataset to original\ndataset = pd.read_csv(data)\ndataset.head(10)","9111e724":"#basic descriptive statistics function\ndataset.describe(include = \"all\")","9dd3756f":"#summarizing categorical data\nprint(dataset[\"gender\"].value_counts())\nprint()\nprint(dataset[\"ssc_b\"].value_counts())\nprint()\nprint(dataset[\"hsc_b\"].value_counts())\nprint()\nprint(dataset[\"hsc_s\"].value_counts())\nprint()\nprint(dataset[\"degree_t\"].value_counts())\nprint()\nprint(dataset[\"workex\"].value_counts())\nprint()\nprint(dataset[\"specialisation\"].value_counts())\nprint()\nprint(dataset[\"status\"].value_counts())","21b595cf":"#Box Plots\nsns.boxplot(x=\"gender\", y=\"salary\", data=dataset)","f5abc01a":"#Scatter Plots\nx = dataset[\"etest_p\"]\ny = dataset[\"salary\"]\nplt.scatter(x,y)\n\nplt.title(\"Employment Test Percentage vs Salary offered by corporate to candidates\")\nplt.xlabel(\"Employment Test Percentage\")\nplt.ylabel(\"Salary Offered\")","db1784cf":"#Groupby Function\ndataset_test = dataset[['gender', 'degree_t', 'salary']]\ndataset_grp = dataset_test.groupby(['gender', 'degree_t'], as_index = False).mean() #.mean() is used to see how average salary differ across different groups\ndataset_grp","44b31fc3":"#Pivot Table\ndataset_pivot = dataset_grp.pivot(index = 'gender', columns = 'degree_t')\ndataset_pivot","7b6e5922":"#Heatmap Plot\nfig, ax = plt.subplots()\nim = ax.pcolor(dataset_pivot, cmap='RdBu')\n\n#label names\nrow_labels = dataset_pivot.columns.levels[1]\ncol_labels = dataset_pivot.index\n\n#move ticks and labels to the center\nax.set_xticks(np.arange(dataset_pivot.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(dataset_pivot.shape[0]) + 0.5, minor=False)\n\n#insert labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(col_labels, minor=False)\n\n#rotate label if too long\nplt.xticks(rotation=90)\n\nfig.colorbar(im)\nplt.show()","72db37c2":"#Correlation\nsns.regplot (x=\"etest_p\", y=\"salary\", data=dataset)\nplt.ylim(0,)","4135caa9":"from scipy import stats\npearson_coef, p_value = stats.pearsonr(dataset['etest_p'], dataset['salary'])","e4f26542":"#replacing with mean value\netest_mean = dataset[\"etest_p\"].mean()\ndataset[\"etest_p\"].replace(np.nan, etest_mean, inplace = True)","e42a98f1":"salary_mean = dataset[\"salary\"].mean()\ndataset[\"salary\"].replace(np.nan, salary_mean, inplace = True)","000dd18f":"from scipy import stats\npearson_coef, p_value = stats.pearsonr(dataset['etest_p'], dataset['salary'])","8d37c6f0":"print(\"Correlation Coefficient is \" + str(pearson_coef))\nprint(\"p_value is \" + str(p_value))","eb132725":"#finding correlation value between multiple features\ndataset[['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary']].corr()","11e929d5":"#Correlation Heatmap\ncorr_mat = dataset.corr()\n\nplt.figure(figsize = (13,5))\nsns_plot = sns.heatmap(data = corr_mat, annot = True, cmap='GnBu')\nplt.show()","0d012240":"dataset.specialisation.unique()","e9927224":"dataset_anova = dataset[[\"specialisation\", \"salary\"]]\ngrouped_anova = dataset_anova.groupby([\"specialisation\"])\n\nf_val, p_val = stats.f_oneway(grouped_anova.get_group(\"Mkt&HR\")[\"salary\"],grouped_anova.get_group(\"Mkt&Fin\")[\"salary\"])\nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val)   ","63ccb76d":"\nMissing values appear in the dataset as \"?\", \"NaN\", \"0\" or a blank cell.\n","23bb39bd":"Descriptive analysis provide understanding on basic features of the data & give short summary on the sample and measure of the data.","6d4879fa":"## Loading a dataset","e9c89d7e":"Data normalization is uniforming the features values with different ranges to have a fair comparison between different features.","1a6153c4":"## Data Normalization (Centering\/Scaling)","78d4a3ba":"To export the modified dataset to file <br> <br>\nexport_path = \"C:\\Windows\\....\\exportedfile.csv\" <br>\ndf.to_csv(export_path) <br> <br>\n\n**Different formats** <br>\n* For csv format, reading: pd.read_csv(), exporting: df.to_csv()\n* For json format, reading: pd.read_json(), exporting: df.to_json()\n* For Excel format, reading: pd.read_excel(), exporting: df.to_excel()\n* For SQL format, reading: pd.read_sql(), exporting: df.to_sql()","760b9044":"This is my personal note for common functions in data analysis so that I can refer to in times of need.","de35bbcf":"EDA is used to -:\n* Summarize main characteristics of the data\n* Gain better understanding of the data set\n* Uncover relationships between variables\n* Uncover important variables <br> <br>\n\nIn this dataset, using EDA, we can uncover what are the characteristics that have the most impact on the salary? But as mentioned before, my focus on this notebook is not performing a full analysis on the dataset, rather testing out and noting down the Data Analysis functions for my future reference.","0065e775":"### Correlation Strength\n#### Pearson Correlation Method\n\n* Measure the strength of the correlation between two features (Correlation coefficient & P-value)\n* Correlation coefficient (Close to +1: Strong positive relationship, Close to -1: Strong negative relationship, Close to 0: No relationship)\n* P-value (P-value<0.001 Highly confident in the result\/relationship, P-value<0.05 Moderately confident, P-value<0.1 Slightly confident, P-value>0.1 Not confident)","2e1f0837":"Since this dataset consists of headers already, when I use header = None, it treated the headers as rows or data points. <br> <br>\nLets say the dataset doesn't contain headers and so we used header = None to import data, we can assign column names\/headers with a panda method as below: <br> <br>\nheaders = [\"sl_no\",\"gender\",\"ssc_p\",\"hsc_p\",\"hsc_s\" etc...] <br>\ndataset.columns = headers","93ff2a7b":"**Description on columns**\n* sl_no          :           Serial Number\n* gender         :           Gender - Male='M, Female='F'\n* ssc_p          :           Secondary Education percentage - 10th Grade\n* ssc_b          :           Board of Education - Central\/ Others\n* hsc_p          :           Higher Secondary Education percentage- 12th Grade\n* hsc_b          :           Board of Education - Central\/ Others\n* hsc-s          :           Specialization in Higher Secondary Education\n* degree_p       :           Degree Percentage\n* degree_t       :           Under Graduation(Degree type) - Field of degree education\n* workex         :           Work Experience\n* etest_p        :           Employability test percentage (conducted by college)\n* specialisation :           Post Graduation(MBA) - Specialization\n* mba_p          :           MBA percentage\n* status         :           Status of placement- Placed\/Not placed\n* salary         :           Salary offered by corporate to candidates\n\n<br> \n\n**What is in it?** <br>\nThis dataset consists of Placement data of students at Jain University Bangalore. It includes secondary and higher secondary school percentage and specialization. It also includes degree specialization, type and Work experience and salary offers to the placed students.\n\n<br>\n\n**Questions we can ask about this dataset** <br>\nWhich factor influenced a candidate in getting placed? <br>\nDoes percentage matters for one to get placed? <br>\nWhich degree specialization is much demanded by corporate? <br>\nPlay with the data conducting all statistical tests. <br>","8d26935e":"Here, we can see salary feature has ridiculously high values compared to other features. This will produce a data bias in an analysis. <br> <br>\n\n**3 methods of normalization** <br>\n* Simple Feature Sacling - dividing each value with the maximum value of that feature, will give a new value range between 0 & 1.\n* Min-Max Scaling - ((current value - minimun value)\/(maximun value - minimum value)), will give a new value range between 0 & 1.\n* Z-Score Scaling - (current value - average value of feature)\/standard deviation (sigma) - will give a value around 0 (typically between -3 & +3).","2e0183c0":"There seems to be a positive linear relationship between 2 variables albeit rather weak. <br>\nBut note that correlation does not mean causation","0aa071a2":"The result is not good. A small F test score showing a weak correlation and a not so small P value implying that it is not certain about statistical significance.","3ed0aa88":"* unique - number of distinct objects in the column\n* top - most frequently occuring object\n* freq - number of times the top object appears in the column\n* NaN - Not A Number","c42d9fe0":"# Basics","22e3a911":"Scatter plots are suitable to show  the relationship between 2 continuous data. <br>\nScatter plot has 2 variables - independent variable & dependent variable. Independent variable is used to predict the dependent variable. <br>\nNormally, an independent variable is placed on the x-axis & a dependent variable is placed on the y-axis.","cbd1a969":"## Data Binning","426a70ec":"**How to read a box plot**\n* the horizontal line at the top is upper extreme value\n* the horizontal line at the bottom is lower extreme value\n* the horizontal line at the center of the box is median vlaue (middle datapoint)\n* the upper quartile is the 75th percentile of data\n* the lower quartile is the 25th percentile of data\n* the dots outside the upper & lower extreme values are outliers\n<br> <br>\nThe data between the upper and lower quartile represents the interquartile range. <br>\nThe upper & lower extreme values are calculated as 1.5 times the interquartile range above the 75th percentile and as 1.5 times the interquartile range below the 25th percentile.","00d8fa52":"# Data Pre-processing","b60991de":"### Replacing missing values with new values","83e4521f":"**One-hot encoding method** <br>\nSo we have 2 categorical values in gender feature. <br>\nWe can encode these categorical values of gender into numeric values by adding dummy variables for each unique category and assign 0\/1 in each category.","d7abfc67":"## Identifying & handling missing values","1ac75af0":"### Analysis of Variance (ANOVA)\n#### ANOVA is finding a correlation between different groups of a categorical variable\n\nANOVA returns 2 values:\n* F-test score: variation between sample group means divided by variation within sample group\n* p-value : confidence degree\n\nSmall F-test score means a weak correlation between variable categories & a target variable.","9667a186":"**Dealing with missing values**\n\n* Check with the data collection source\n* Drop the missing values - either by dropping the variable (whole column) or just the data entry with the missing value (just the row)\n* Replacing the missing values - replacing with an mean\/ average value (of similar datapoints) -> this will not work on categorical data, replacing it by mode\/ most frequent value, replacing it based on other functions (based on domain knowledge or experience)\n* Leaving it as a missing data - least preferable","322b5761":"## Data formatting","826bc58f":"## Descriptive Statistics","ec37f007":"Correlation coefficient is not much. So it is a weak positive correlation. <br>\nAnd p_value is less than 0.05, so we are moderately certain that it is a weak positive correlation.","f34cea8c":"Note that the correlation heat map left out features with categorical values. To include them, we need to encode them to numerical labels such as 1 for Male, 0 for Female in gender etc.","6e4dd3e1":"Data entries can be in different formats for one expression\/ value. <br>\nFor example, Singapore can be in S.G., SG, S.G, S.G.P, singapore etc. <br>\nStandardizing different formats of same expression is necessary in data analysis. <br>\nSometimes values can also be in a unit that is not preferred in the calculation\/analysis such as entries are in miles instead of kilometers etc. <br> <br>\n\n**Changing format of data entry values** <br>\ndf[\"city-mpg\"] = 235\/df[\"city-mpg\"] <br>\ndf.rename(columns={\"city-mpg\" : \"city-L\/100km\"}, inplace = True) <br> \n-------------------------------------------------------------------------------------------------------------------<br>\nAnother scenario could be numerical values being in object datatype and not included in the calculation. <br> <br>\n**Converting datatypes** <br>\ndf[\"salary\"] = df[\"salary\"].astype(\"float\")","8ddb77e4":"### Dropping rows with NaN values","06fbff0a":"## Loading libraries","10c0b23f":"## Turning Categorical values to numeric variables","ee4cf376":"Error encountered due to NaN values involved.","4fcc8fc7":"Binning is grouping of values into bins to have a better understanding of data distribution or to improve accuracy of the model.","4ed464f5":"# Exploratory Data Analysis (EDA)","375848f4":"## Accessing databases with Python","0250dc6f":"General rule of thumb, prices & measurement values should be float64 - otherwise, it will be a problem in an analysis.","0d54bbb7":"## Exploring a dataset","0c0d536b":"**Different Datatypes**\n* Pandas Type: object, Native Python Type: string, Description: numbers and strings\n* Pandas Type: int64, Native Python Type: int, Description: Numeric characters\n* Pandas Type: float64, Native Python Type: float, Description: Numberic characters with decimals\n* Pandas Type: datetime64,timedelta[ns], Native Python Type: refer to datetime module in Python's standard library, Description: time data"}}