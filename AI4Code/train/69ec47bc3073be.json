{"cell_type":{"39dda308":"code","29070942":"code","d2b860db":"code","c526f208":"code","e8fa3728":"code","6e7a374f":"code","2a4cd6e0":"code","e3f395f0":"code","e594fac6":"code","0a761c4c":"code","717a6bd5":"code","a0eeb7cb":"code","02d844df":"code","d384e1cb":"code","6eb01f18":"code","7beec1e8":"code","680c0cec":"code","15ddc529":"code","2955eafd":"code","c4c7b145":"code","af32b6a8":"code","dff7ff67":"code","e71ed372":"code","620e4301":"code","11377e47":"code","9b4d9425":"code","c3f7d1d6":"code","5559604f":"code","3901f322":"code","6c95520e":"code","a29e1e60":"code","c2ae0fed":"code","09bd82da":"code","7e42b022":"code","a9ea1cc3":"code","3c944331":"code","859ab0ad":"code","166cf597":"code","44d8c585":"markdown","d6db6448":"markdown","d55a78e9":"markdown","b3329c87":"markdown","3a15bcd0":"markdown","6605fbf3":"markdown","01928361":"markdown","52ae5c94":"markdown","33a22e12":"markdown","0f04aaaa":"markdown","5ac0cb96":"markdown","ade8adac":"markdown","fa94558a":"markdown","8d9cc71c":"markdown","03135aaf":"markdown","ebb15c30":"markdown","7c6e7976":"markdown","637bb77d":"markdown","46744a29":"markdown","02b95ae1":"markdown","a06cf7bf":"markdown","417c41b2":"markdown","882eda55":"markdown"},"source":{"39dda308":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport string\nimport collections\n \nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import MDS\n\nimport sklearn\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.metrics import classification_report\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nimport scipy as sc\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.cluster.hierarchy import ward, dendrogram\n\nimport gensim\nfrom gensim import corpora\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.models import KeyedVectors\n\n# libraries for visualization\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n\nfrom itertools import chain\n\nimport json\n\n%matplotlib inline","29070942":"df = pd.read_csv('..\/input\/songdata.csv')\n# remove new lines from lyrics\ndf['text'] = df.text.apply(lambda x: x.replace('\\n',''))","d2b860db":"artists = ['Death', 'Xzibit', 'Britney Spears']\nnum_artists = len(artists)\ndf_artists = df.loc[df.artist.isin(artists)]\nfor artist in artists:\n    print ('Number of {} songs: {}'.format(artist, len(df_artists[df_artists.artist == artist])))","c526f208":"df_artists.head()","e8fa3728":"stopwords = stopwords.words('english')\npunctuation = string.punctuation\ntrantab = str.maketrans(punctuation, ' '*len(punctuation))\n\ndef process_text(text):\n    \"\"\"Remove punctuation, lower text, tokenize text, remove stop words and stem tokens (words).\n    Args:\n      text: a string.\n    Returns:\n      Tokenized text i.e. list of stemmed words. \n    \"\"\"\n    \n    # replace all punctuation by blanc spaces\n    text = text.translate(trantab)\n    \n    # lower text\n    text = text.lower()\n    \n    # tokenize text\n    word_tokens = word_tokenize(text) \n  \n    # remove stop words\n    filtered_text = [w for w in word_tokens if not w in stopwords] \n        \n    # stemm text\n    stemmer = PorterStemmer()\n    tokens = [stemmer.stem(t) for t in filtered_text]\n\n    return tokens","6e7a374f":"def define_vectorizer(additional_stopwords = [], max_df=0.5, min_df=0.1, ngram_range=(1, 1)):\n    \"\"\"Transform texts to Tf-Idf coordinates.\n    Args:\n      additional_stop_words: addititional stop_words, list of strings.\n      ngram_range: tuple (min_n, max_n) (default=(1, 1))\n        The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n        All values of n such that min_n <= n <= max_n will be used.\n      max_df: float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency strictly higher than\n        the given threshold (corpus-specific stop words).\n        If float, the parameter represents a proportion of documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n      min_df: float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency strictly lower than\n        the given threshold. This value is also called cut-off in the literature.\n        If float, the parameter represents a proportion of documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n    Returns:\n      Vectorizer. \n    \"\"\"\n    vectorizer = TfidfVectorizer(tokenizer=process_text,\n                             stop_words=additional_stopwords,\n                             max_df=max_df,\n                             min_df=min_df,\n                             ngram_range=ngram_range)\n    return vectorizer\n\ndef compute_tfidf_matrix(corpus, vectorizer):\n    \"\"\"Transform texts to Tf-Idf coordinates.\n    Args:\n      corpus: list of strings.\n      vectorizer: sklearn TfidfVectorizer.\n    Returns:\n      A sparse matrix in Compressed Sparse Row format with tf-idf scores.\n    Raises:\n      ValueError: If `corpus` generates empty vocabulary or after pruning no terms remain.\n    \"\"\"\n    tfidf_matrix = vectorizer.fit_transform(corpus) # raises ValueError if bad corpus\n    return tfidf_matrix\n\ndef corpus_features(df, field):\n    \"\"\"\n    Returns vectorizer, tfidf_matrix, dist computed on df[field].\n    \"\"\"\n    corpus = df[field].values\n    vectorizer = define_vectorizer()\n    tfidf_matrix = compute_tfidf_matrix(corpus = corpus, vectorizer = vectorizer)\n    dist = 1 - cosine_similarity(tfidf_matrix)\n    return vectorizer, tfidf_matrix, dist","2a4cd6e0":"field = 'text'","e3f395f0":"vectorizer, tfidf_matrix, dist = corpus_features(df_artists, field)","e594fac6":"num_clusters = num_artists","0a761c4c":"def annotate_labels(labels, vectorizer, tfidf_matrix, num_clusters, num_words = 3):\n    \"\"\" Label clusters with num_words most common words \"\"\"\n    feature_array = vectorizer.get_feature_names()\n    # we create a numpy array here in order to use list indexing list_of_clusters[cluster_texts_index]\n    # we choose dtype=object because str type will actually be char (string of length 1)\n    list_of_clusters = np.empty(len(labels), dtype=object)\n    for i in range(num_clusters):\n        cluster_texts_index = np.where(labels==i)[0]\n        r = tfidf_matrix[cluster_texts_index]\n        tfidf_sorting = np.argsort(r.toarray()).flatten()[::-1]\n        top_n = [feature_array[k] for k in tfidf_sorting][:num_words]\n        word_label = ' '.join(top_n)\n        list_of_clusters[cluster_texts_index] = str(i)+' '+word_label # total_label #str(i)+'\n    return list_of_clusters\n    \ndef dict_labels(labels): \n    \"\"\" Returns a dictionary with names of clusters as keys and lists of indices of corresponding elements as values \"\"\"\n    clustering = collections.defaultdict(list)\n    for i, label in enumerate(labels):\n        clustering[label].append(i) \n    return dict(clustering)\n\ndef conf_matrix_clusters(df, clusters):\n    \"\"\" Prints the confusion matrix \"\"\"\n    results = []\n    cluster_names = clusters.keys()\n    assert df['artist'].nunique() == len(cluster_names), \"conf_matrix_clusters() only works for num_clusters == num_artists\"\n    for k in cluster_names:\n        row = df.iloc[clusters[k]].groupby('artist')['song'].nunique().to_dict()\n        for artist in artists:\n            if artist not in row:\n                row[artist] = 0\n        results.append(row)\n    \n    results_df = pd.DataFrame(results, cluster_names).transpose()\n    \n    f, ax = plt.subplots(figsize=(11, 9))\n    sns.heatmap(results_df, annot=True, fmt=\"d\", cmap=\"YlGnBu\", ax=ax)","717a6bd5":"def kmeans_clustering(vectorizer, tfidf_matrix, num_clusters = 0):\n    \"\"\"Perform KMeans clustering.\n    Args:\n      vectorizer: vectorizer.\n      tfidf_matrix: tfidf_matrix.\n      num_clusters: non-negative integer. If 0, automatically chooses the optimal number of clusters using\n        silouhettes, otherwise performs clustering with `num_clusters` clusters.\n    Returns:\n      List `labels` where `labels[i]` is the cluster of `i`-th string in the `corpus`.\n    \"\"\"\n    \n    feature_array = vectorizer.get_feature_names()\n    labels = []\n    \n    # custom number of clusters\n    if num_clusters != 0:  \n        # fix random_state for reproducibility\n        km_model = KMeans(n_clusters=num_clusters, random_state = 42).fit(tfidf_matrix)\n        labels = km_model.labels_\n        #clusters = cluster_texts(km_model) \n    # auto - using the maximal silhouette score\n    if num_clusters == 0:\n        range_n_clusters = range(2,21)\n        silhouettes = []\n        for n_clusters in range_n_clusters:\n            km_model = KMeans(n_clusters=n_clusters).fit(tfidf_matrix)\n            labels = km_model.labels_\n            silhouettes.append(silhouette_score(tfidf_matrix, labels))\n        num_clusters = np.argmax(silhouettes) + range_n_clusters[0]\n        km_model = KMeans(n_clusters=num_clusters).fit(tfidf_matrix)\n        labels = km_model.labels_\n        \n    return labels","a0eeb7cb":"def hierarchical_clustering(vectorizer, tfidf_matrix, num_clusters = 0):\n    \"\"\"Perform hierarchical clustering.\n    Args:\n      vectorizer: vectorizer.\n      tfidf_matrix: tfidf_matrix.\n      num_clusters: non-negative integer. If 0, automatically chooses the optimal number of clusters using\n        silouhettes, otherwise performs clustering with `num_clusters` clusters.\n    Returns:\n      List `labels` where `labels[i]` is the cluster of `i`-th string in the `corpus`.\n    \"\"\"\n    feature_array = vectorizer.get_feature_names()\n    labels = []\n    \n    # for hierarchical clustering\n    dist = 1 - cosine_similarity(tfidf_matrix)\n    linkage_matrix = ward(dist)\n        \n    # custom number of clusters\n    if num_clusters != 0:  \n        labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust') - 1 # substract 1 to get numeration from 0\n    # auto - using the maximal silhouette score\n    if num_clusters == 0:\n        range_n_clusters = range(2,21)\n        silhouettes = []\n        for n_clusters in range_n_clusters:\n            labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust') - 1 # substract 1 to get numeration from 0\n            silhouettes.append(silhouette_score(tfidf_matrix, labels))\n        num_clusters = np.argmax(silhouettes) + range_n_clusters[0]\n        labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust') - 1 # substract 1 to get numeration from 0\n    \n    return labels","02d844df":"def cluster(clustering_function, vectorizer, tfidf_matrix, num_clusters = 3):\n    labels = clustering_function( vectorizer, tfidf_matrix, num_clusters)\n    annotated_labels = annotate_labels(labels, vectorizer, tfidf_matrix, num_clusters, num_words = 3)\n    dict_cluster_indices = dict_labels(annotated_labels)\n    conf_matrix_clusters(df_artists, dict_cluster_indices)\n    return annotated_labels","d384e1cb":"kmeans_labels = cluster(kmeans_clustering, vectorizer, tfidf_matrix, num_clusters = 3)","6eb01f18":"hierarchical_labels = cluster(hierarchical_clustering, vectorizer, tfidf_matrix, num_clusters = 3)","7beec1e8":"def viz_clusters(dist, labels):\n    \n    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n    pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n    xs, ys = pos[:, 0], pos[:, 1]   \n    \n    #create data frame that has the result of the MDS plus the cluster numbers and titles\n    df_viz = pd.DataFrame(dict(x=xs, y=ys, label=labels, title=df_artists['song'].values)) \n    #group by cluster\n    groups = df_viz.groupby('label')\n\n\n    # set up plot\n    fig, ax = plt.subplots(figsize=(21, 15)) # set size\n    ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n\n    # note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color\/label\n    for name, group in groups:\n        label = group.iloc[0]['label']\n        ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=label, mec='none')\n        ax.set_aspect('auto')\n        ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n        ax.tick_params(axis='y', which='both', left=False, top=False, labelleft=False)\n\n    ax.legend(numpoints=1)  # show legend with only 1 point\n\n    # add label in x,y position with the label as the film title\n    for i in range(len(df_viz)):\n        ax.text(df_viz.iloc[i]['x'], df_viz.iloc[i]['y'], df_viz.iloc[i]['title'], size=8)  \n\n    plt.show() #show the plot\n    \ndef plot_dendrogram(vectorizer, dist):\n    \"\"\"Plots a dendogram which shows how hierarchical clustering works.\n    Args:\n      df: pandas data frame with the column `cluster`.\n      field: a column in `df` on which we will perform clustering.\n    Returns:\n      Nothing. Plots the dendogram.\n    Raises:\n      AssertionError: If a parameter is wrong.\n    \"\"\"\n    \n    linkage_matrix = ward(dist)\n\n    fig, ax = plt.subplots(figsize=(15, 30)) # set size\n    ax = dendrogram(linkage_matrix, orientation=\"right\");\n\n    plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n\n    plt.tight_layout()","680c0cec":"viz_clusters(dist, kmeans_labels)","15ddc529":"viz_clusters(dist, hierarchical_labels)","2955eafd":"plot_dendrogram(vectorizer, dist)","c4c7b145":"from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_short, strip_punctuation, stem_text","af32b6a8":"strip_short2 = lambda x: strip_short(x, minsize=2)\n# Pipeline for preprocessing in Genism is different. For simplicity we don't use stemming here.\nCUSTOM_FILTERS = [lambda x: x, remove_stopwords, strip_punctuation, strip_short2, stem_text]\ntokens = df_artists[field].apply(lambda s: preprocess_string(s, CUSTOM_FILTERS))","dff7ff67":"dictionary = corpora.Dictionary(tokens)\ndoc_term_matrix = [dictionary.doc2bow(text) for text in tokens]","e71ed372":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Creating the object for LDA model using gensim library\nLDA = gensim.models.ldamodel.LdaModel\n\n# number of topics is the same as for K-Means \nnum_topics = num_clusters\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, \n                num_topics=num_topics, \n                alpha=[0.0001] * num_topics, \n                eta=[0.0001] * len(dictionary),\n                chunksize=2000,\n                passes=4,\n                random_state=100,\n               )","620e4301":"lda_model.print_topics(num_words=8)","11377e47":"# Visualize the topics\npd.options.display.max_colwidth = 2000\nviz = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary, mds='tsne')","9b4d9425":"pyLDAvis.enable_notebook()\nviz","c3f7d1d6":"def LDA_clustering(num_words=3):\n    \n    all_topics = lda_model.get_document_topics(doc_term_matrix, per_word_topics=True)\n    \n    topics = lda_model.print_topics(num_words=num_words)\n    clusters_lda = {}\n    cluster_names = {}\n\n    # get named clusters \n    for x in topics:\n        cluster_words = ''\n        for proba_with_word in x[1].split('+'):\n            cluster_words += ' ' + proba_with_word.split('\"')[1]\n        clusters_lda[str(x[0])+cluster_words] = []\n        cluster_names[x[0]] = str(x[0])+cluster_words\n\n    counter = 0\n    for doc_topics, word_topics, phi_values in all_topics:\n        cluster_with_highest_proba = max(doc_topics, key = lambda item: item[1])[0]\n        cluster_name = cluster_names[cluster_with_highest_proba]\n        clusters_lda[cluster_name].append(counter)\n        counter += 1\n        \n    return clusters_lda","5559604f":"clusters_lda = LDA_clustering()","3901f322":"conf_matrix_clusters(df_artists, clusters = clusters_lda)","6c95520e":"def structured_field_clustering(df, field):\n    \"\"\"\n    Fill `cluster` column in dataframe `df` based on `field` column. \n    \"\"\"\n    assert field in df.columns, field+' is not in df columns'\n    assert sum(df[field].isna()) == 0, 'There are some undefined values in df.'+field\n    df_ri = df.reset_index(drop=True)\n    df_ri['cluster'] = 'cluster ' + df_ri[field]\n    return df_ri","a29e1e60":"df_artists_SSL = structured_field_clustering(df_artists, 'artist')\nfor i in range(len(df_artists_SSL)):\n    # leave approximately 30% for training\n    if np.random.random() > 0.3:\n        df_artists_SSL.loc[i, 'cluster'] = None","c2ae0fed":"def auto_fill_in(df, field):\n    \"\"\"\n    Automatically propagate labels in `df`.\n    \"\"\"\n    assert field in df.columns, field+' is not in df columns'\n    df_ri = df.reset_index(drop=True)\n    train_ind = df_ri.loc[~df_ri.cluster.isna()].index\n    target_ind = df_ri.loc[df_ri.cluster.isna()].index\n    assert len(train_ind) > 0, 'There is no labeled data in df'\n    assert len(target_ind) > 0, 'There is no unlabeled data in df'\n    train = df_ri.iloc[train_ind]\n    target = df_ri.iloc[target_ind]\n    \n    texts = df_ri[field].apply(lambda x: x.lower().replace('\\n', ''))\n    vectorizer = define_vectorizer()\n    X = compute_tfidf_matrix(texts.values, vectorizer)\n    \n    X_train = X[train_ind]\n    X_target = X[target_ind]\n    \n    le = LabelEncoder()\n    y_train = le.fit_transform(train['cluster'].values)\n        \n    KNN = KNeighborsClassifier(n_neighbors=1) # any n_neighbors is fine\n    KNN.fit(X_train, y_train) \n    \n    y_target = KNN.predict(X_target)\n    y_cluster = le.inverse_transform(y_target)\n    \n    df_ri.loc[target_ind, 'cluster'] = y_cluster\n    \n    return df_ri","09bd82da":"df_artists_SSL_filled = auto_fill_in(df_artists_SSL, 'text')","7e42b022":"conf_matrix_clusters(df_artists_SSL_filled, dict_labels(df_artists_SSL_filled['cluster']))","a9ea1cc3":"df_artists_SSL = structured_field_clustering(df_artists, 'artist')\nfor i in range(len(df_artists_SSL)):\n    # leave approximately 50% for training\n    if np.random.random() > 0.5:\n        df_artists_SSL.loc[i, 'cluster'] = None","3c944331":"df_artists_SSL_filled = auto_fill_in(df_artists_SSL, 'text')","859ab0ad":"conf_matrix_clusters(df_artists_SSL_filled, dict_labels(df_artists_SSL_filled['cluster']))","166cf597":"from pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\nidf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) # minDocFreq: remove sparse terms\n\nlabel_stringIdx = StringIndexer(inputCol = \"cluster\", outputCol = \"label\").setHandleInvalid(\"keep\") # keep NaN\n\npipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n\ndf = spark.read.load(\"..\/input\/df_artists.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\npipelineFit = pipeline.fit(df)\ntrain_df = pipelineFit.transform(df)\ntrain_tfidf_only = train_df.select('features')\n\nfor k in range(2,10):\n    # Trains a k-means model.\n    kmeans = KMeans().setK(k).setSeed(1)\n    model = kmeans.fit(train_tfidf_only)\n    # Make predictions\n    predictions = model.transform(train_tfidf_only)\n    # Evaluate clustering by computing Silhouette score\n    evaluator = ClusteringEvaluator()\n    silhouette = evaluator.evaluate(predictions)\n    print(str(k)+\" Silhouette with squared euclidean distance = \" + str(silhouette))\n\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)","44d8c585":"## Clustering","d6db6448":"## LDA\nHere we try a new approach, Latent Dirichlet Allocation, which is destined to model topics. We can expect that result will be different.","d55a78e9":"## Acknowledgments\nLibraries' documentations, many solutions that I found on Stackoverflow.","b3329c87":"Note that the cluster names are different - this is so because the cluster names are defined by the songs in the clusters and the produced sets of songs in the clusters are different for KMeans and Hierarchical clusterings.","3a15bcd0":"Choose the field on which we want to cluster. For instance it could be *song* (name of the songs) or *text* (lyrics of the songs).","6605fbf3":"### Vizualization","01928361":"## Imports","52ae5c94":"### Reading the data","33a22e12":"## Transforming text to numerical features (td-idf)\nWe separate the computation of *tf-idf* matrix into 2 steps (*define_vectorizer()* and *compute_tfidf_matrix()*) because later we will need the *vectorizer* itself (for auto-labelling of clusters).","0f04aaaa":"### K-Means clustering","5ac0cb96":"### Define a compact function for clustering","ade8adac":"### Choosing the artists whose songs we cluster\nFor the sake of simplicity we choose artists from different music genres on purpose.","fa94558a":"And now we need to define the number of clusters that we want to have.\nAt first we set *num_clusters = num_artists* which is cheating because we know that we want to have *num_artists* clusters, but later we will try some methods in order to automatically find the optimal number of clusters.","8d9cc71c":"### Util functions","03135aaf":"So as we can see LDA doesn't cluster the songs by artists, instead it found some different topics.","ebb15c30":"### Hierarchical clustering","7c6e7976":"## Preprocessing \n\nWe need to define how we preprocess the lyrics of a song.  \nWe use the following pipeline:\n\n1. Remove punctuation.\n2. Lower text.\n3. Tokenize.\n4. Remove stop words.\n5. Stem.\n\n*NLTK stopwords.words('english')* contains only lowercase letters and we don't want any punctuation in the tokens so we remove punctuation and lower text before the tokenization. After we can remove stop words and proceed with stemming or lemmatization if we want to.","637bb77d":"## Data","46744a29":"# Clustering songs\nIn this notebook we compare performance of different clustering algorithms - Kmeans clustering, hierarchical clustering, LDA clustering and semi-supervised learning with label propagation - on a subset of original *55000+ Song Lyrics* dataset.  \nIn addition we show how to use silhouette score to automatically choose the number of clusters and we label clusters with words.","02b95ae1":"If we want to get a better result, we can label some more data and iterate the process until we are satisfied with the quality.","a06cf7bf":"Once we have chosen the field on which we want to cluster, we define *corpus, vectorizer, tfidf_matrix* here\nand will pass them to clustering functions in order to save computational time.","417c41b2":"## Semi-supervised learning\nWhat if we label some songs by hand and propagate the labels on the unlabeled data?  \nTo save time let's cluster the songs automatically (because we know the right answer) and later delete a few values in *cluster* column.","882eda55":"## Pyspark\nWhat if we want to work with big data? In this case we might want to use Spark.  \nUnfortuntely I was not able to install pyspark in Kaggle notebooks so I just provide the Pyspark code of KMeans. "}}