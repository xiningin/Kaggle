{"cell_type":{"8a0feaa3":"code","6472f7f1":"code","88d2aa39":"code","63748d47":"code","69d12880":"code","4677ef8d":"code","64e9cc59":"code","17ae9999":"code","2081c0e3":"code","1c086def":"code","f7bfa535":"code","7a90b0c5":"code","84794733":"code","46829752":"code","d7dcffe2":"code","189c962f":"code","c9910eed":"code","950d24ba":"code","f61f278d":"code","7e88bb45":"code","8fb70d49":"code","6e543eb9":"code","8b8bc894":"code","41cf4a20":"code","0ad00101":"code","748baeec":"code","a76b6245":"code","7048193a":"code","b9e4731e":"code","d306904b":"code","e99e043a":"code","3d9e0983":"code","8b40cf2e":"code","16db1828":"code","91aa6b5d":"code","af2b93b1":"code","5865c91f":"code","ffda082b":"code","1ccb8fe1":"code","862c21bd":"markdown","341118c9":"markdown","ce2fb1e1":"markdown","727a8fb9":"markdown","56a7f898":"markdown","c837c6da":"markdown","eca98834":"markdown","532a01b9":"markdown","999c4771":"markdown","f2c19e64":"markdown","7e69a436":"markdown","b6d7561f":"markdown","ced764d5":"markdown","a7c7c3a2":"markdown","ab4b68a8":"markdown","b0952e9a":"markdown","4c92cd05":"markdown","5b608b12":"markdown","3e08c1f7":"markdown","d809fff3":"markdown","3b5fd907":"markdown","51489fb7":"markdown","523f9ff3":"markdown","636012d3":"markdown","d4975555":"markdown","a146bdf9":"markdown","28ee6567":"markdown","8b792959":"markdown","3d5d516a":"markdown","4af60220":"markdown","d9d1780e":"markdown"},"source":{"8a0feaa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfifa = pd.read_csv('..\/input\/data.csv', index_col='ID')\nfifa.head()\n\n# Any results you write to the current directory are saved as output.","6472f7f1":"fifa = fifa.drop('Unnamed: 0,Name,Photo,Nationality,Flag,Club,Club Logo,Jersey Number,Joined,Real Face,Loaned From,Release Clause'.split(','), axis=1)\nfifa.head()","88d2aa39":"def parse_money(s):\n    if s.startswith('\u20ac'):\n        s = s[1:]\n    multiplier = None\n    if s.endswith('M'):\n        s = s[:-1]\n        multiplier = 1e6\n    elif s.endswith('B'):\n        s = s[:-1]\n        multiplier = 1e9\n    elif s.endswith('K'):\n        s = s[:-1]\n        multiplier = 1e3\n    f = float(s)\n    if multiplier:\n        f = f * multiplier\n    return f","63748d47":"fifa['Value'] = fifa['Value'].apply(parse_money)\nfifa['Wage'] = fifa['Wage'].apply(parse_money)\nfifa[['Value', 'Wage']].head()","69d12880":"fifa[['Work Rate1', 'Work Rate2']] = fifa['Work Rate'].str.split('\/', expand=True)\nfifa = fifa.drop('Work Rate', axis=1)\nfifa[['Work Rate1', 'Work Rate2']].head()","4677ef8d":"def parse_date(s):\n    if isinstance(s, str) and ',' in s:\n        return float(s.split()[2])\n    else:\n        return float(s)\nfifa['Contract Valid Until'] = fifa['Contract Valid Until'].apply(parse_date)\nfifa['Contract Valid Until'].head()","64e9cc59":"def parse_height(s):\n    if isinstance(s, float):\n        return s\n    f, i = s.split(\"'\")\n    return int(f)*12 + int(i)\nfifa['Height'] = fifa['Height'].apply(parse_height)\nfifa['Height'].head()","17ae9999":"def parse_weight(s):\n    if isinstance(s, str) and s.endswith('lbs'):\n        return float(s[:-3])\n    return float(s)\nfifa['Weight'] = fifa['Weight'].apply(parse_weight)\nfifa['Weight'].head()","2081c0e3":"rating_cols = \"LS, ST, RS, LW, LF, CF, RF, RW, LAM, CAM, RAM, LM, LCM, CM, RCM, RM, LWB, LDM, CDM, RDM, RWB, LB, LCB, CB, RCB, RB\".split(', ')\nfor col in rating_cols:\n    fifa[[f\"{col}1\", f\"{col}2\"]] = fifa[col].str.split('+', expand=True)\n    fifa[f\"{col}1\"] = fifa[f\"{col}1\"].astype('float')\n    fifa[f\"{col}2\"] = fifa[f\"{col}2\"].astype('float')\n    fifa = fifa.drop(col, axis=1)\nfifa.head()","1c086def":"def null_counts(df):\n    for col in df.columns:\n        print(f\"{col} : {df[col].isnull().sum()}\")\nnull_counts(fifa)","f7bfa535":"fifa = fifa[~fifa['ShortPassing'].isnull()]\nnull_counts(fifa)","7a90b0c5":"fifa.loc[fifa['Position'].isnull(), 'Position'] = 'Unknown'\nnull_counts(fifa)","84794733":"cat_columns = fifa.select_dtypes('object', 'category').columns\nnull_counts(fifa[cat_columns])","46829752":"fifa['Body Type'].value_counts()","d7dcffe2":"fifa.loc[~fifa['Body Type'].isin(['Normal', 'Stocky', 'Lean']), 'Body Type'] = fifa['Body Type'].value_counts().index[0]\nfifa['Body Type'].value_counts()","189c962f":"categoricals = fifa.select_dtypes('object')\nfor col in categoricals.columns:\n    print(col)\n    print(\"----------------------------\")\n    print(fifa[col].value_counts())","c9910eed":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(fifa, test_size=0.2, random_state=42)","950d24ba":"labels = ['Value', 'Wage']\nX_train, X_test = (X.drop(labels=labels, axis=1, inplace=False) for X in (train, test))\ny_train, y_test = (X[labels].copy() for X in (train, test))\ny_test.columns","f61f278d":"cat_columns = X_train.select_dtypes('object', 'category').columns\nnum_columns = X_train.select_dtypes(exclude=['object', 'category']).columns","7e88bb45":"from sklearn.base import BaseEstimator, TransformerMixin\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_names):\n        self.feature_names = feature_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.feature_names].values\n    def get_feature_names(self):\n        return self.feature_names","8fb70d49":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\ncat_selector = DataFrameSelector(cat_columns)\nohe = OneHotEncoder(sparse=False)\ncat_pipeline = Pipeline([\n    ('selector', cat_selector),\n    ('ohe', ohe)\n])","6e543eb9":"from sklearn.preprocessing import Imputer, StandardScaler\nnum_selector = DataFrameSelector(num_columns)\nnum_pipeline = Pipeline([\n    ('selector', num_selector),\n    ('imputer', Imputer(strategy='median')),\n    ('scaler', StandardScaler())\n])","8b8bc894":"from sklearn.pipeline import FeatureUnion\npipeline = FeatureUnion([\n    ('cat_pipeline', cat_pipeline),\n    ('num_pipeline', num_pipeline)\n])","41cf4a20":"X_train_arr = pipeline.fit_transform(X_train)\nX_train_arr","0ad00101":"feature_names = list(ohe.get_feature_names()) + list(num_selector.get_feature_names())","748baeec":"X_test_arr = pipeline.transform(X_test)\nX_test_arr","a76b6245":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train_arr, y_train)","7048193a":"def print_metrics(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    print(\"Mean Squared error : %s\" % mean_squared_error(y_test, y_pred))\n    print(\"Mean Absolute error : %s\" % mean_absolute_error(y_test, y_pred))\n    print(\"R2 Score: %s\" % r2_score(y_test, y_pred))","b9e4731e":"from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nprint_metrics(lr, X_test_arr, y_test)","d306904b":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\nmodels = [SGDRegressor(), DecisionTreeRegressor(), LinearSVR(), AdaBoostRegressor(), \n          GradientBoostingRegressor(), RandomForestRegressor(), MLPRegressor()]\n\nlabel = 'Wage'\nfor model in models:\n    print(f\"Running model {type(model)}\")\n    model.fit(X_train_arr, y_train[label])\n    print_metrics(model, X_test_arr, y_test[label])\n    print(\"------------------------\")","e99e043a":"def sort_rsearch_results(rsearch):\n    cvres = rsearch.cv_results_\n    rsearch_results = sorted([(np.sqrt(-x[0]), x[1]) for x in zip(cvres['mean_test_score'], cvres['params'])])\n    return rsearch_results\n\ndef feature_importance(model):\n    return sorted(zip(feature_names, model.feature_importances_), key=lambda x : x[1], reverse=True)","3d9e0983":"from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, RandomizedSearchCV\nrfr = RandomForestRegressor()\nrfr_params = {'n_estimators' : [50, 100, 200], 'bootstrap' : [True, False], \n              'min_samples_split' : [0.1, 0.01, 0.001], 'max_depth' : [3, 5, 8],\n              'max_features' : ['sqrt', 0.1, 0.01], 'warm_start' : [True, False]\n          }\nrsearch = RandomizedSearchCV(rfr, rfr_params, cv=8, n_iter=10, n_jobs=5, scoring='neg_mean_squared_error', verbose=True)\nrsearch.fit(X_train_arr, y_train[label])","8b40cf2e":"sort_rsearch_results(rsearch)","16db1828":"rfr_final = rsearch.best_estimator_\nprint_metrics(rfr_final, X_test_arr, y_test[label])","91aa6b5d":"feature_importance(rfr_final)","af2b93b1":"from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, RandomizedSearchCV\ngbm = GradientBoostingRegressor(n_iter_no_change=10)\nparams = {'learning_rate' : [0.1, 0.01, 0.001], 'n_estimators' : [50, 100, 200],\n           'subsample' : [0.8, 0.9, 1.0], 'min_samples_split' : [0.01, 0.001],\n           'max_depth' : [3, 5, 8], 'max_features' : ['auto', 'sqrt', 'log2'],\n          }\nrsearch = RandomizedSearchCV(gbm, params, cv=8, n_iter=10, n_jobs=10, scoring='neg_mean_squared_error', verbose=True)\nrsearch.fit(X_train_arr, y_train[label])","5865c91f":"sort_rsearch_results(rsearch)","ffda082b":"gbm_final = rsearch.best_estimator_\nprint_metrics(gbm_final, X_test_arr, y_test[label])","1ccb8fe1":"feature_importance(gbm_final)","862c21bd":"## Estimate Wage\/Value from the player attributes\n**We are going to create some models to estimate the wage or value attributes from the player profile data (especially his\/her ratings & skills)**","341118c9":"All categoricals look fine. They dont have bad values that need fixing.","ce2fb1e1":"Let's run similar checks to remove bad data for other categorical variables","727a8fb9":"Overall seems to be contributing a lot to the prediction for this model but other ratings contribute too. It's interesting to see that categorical attributes like being Left or Right or their Position don't contribute too much (or are overshadowed by the Ratings features).","56a7f898":"That's not a bad MAE but the R2 isn't too great. Let's see if we can do better with GBMs","c837c6da":"# Data Cleaning","eca98834":"Wow! That's doing much better than the Random Forest model.","532a01b9":"There are 48 entries that have many features nulls; we'll just remove them as that's a lot of features missing","999c4771":"This concludes our data-preparation steps to get the data in a much more usable state for the regression models ahead. We haven't done \"Feature engineering\" much by looking at correlations or combining features together or PCA, but we'll start without that for now & come back to it.","f2c19e64":"Now, we'll create a pipeline to process Categorical features (using one-hot encoding) & Numeric features (using Imputation & feature scaling). Here, I am borrowing something from the \"Hands-On Machine Learning with Scikit-Learn & TensorFlow\" book by Aurelien Geron (great book, btw).\n\nThe other option for Categorical features is to use category_encoders library that returns back data-frames that we would concat manually. I didn't choose this option since most scikit-learn models accept arrays rather than data-frames.","7e69a436":"Let's remove all the attributes that will not be used for the modeling; I dont know what Special is, so will keep it for now.\nAlso, Nationality & Club might have some predictive powers but they have many unique values which will increase ","b6d7561f":"# Data Preparation","ced764d5":"**Now we can apply more data-science related data preparation steps such as handling nulls, fixing skewed features, etc.**","a7c7c3a2":"We'll use one-hot encoding for the categorical features since they are mostly not ordinal","ab4b68a8":"Body Type is just bad for 7 players; there are various ways to deal with it; I'll use a simple one of using the most frequent class which will still allow the model to use the remaining attributes for prediction. In a Production system, this should be handled by a data-validation step to avoid bad data.","b0952e9a":"So, we have parsed the various columns to machine readable format.","4c92cd05":"There are 103 features here which is not terribly huge, but not trivially small either. We should consider trying out PCA to reduce the feature dimentionality here. We'll skip this for now & come back to it.","5b608b12":"# Data Preparation for models","3e08c1f7":"We'll just try a Linear Regression as a Base model","d809fff3":"That's not a good R2 score. Let's see if we can do better. We'll try a few other models just using default parameters to see which ones seem promising. We'll use Wage as the target label since some regressors cannot deal with multi-output regression.","3b5fd907":"# Base Model","51489fb7":"Also, Contract Valid Until is a year for some & a date for others. I am going to postulate that the wage\/value of a player isn't dependent on the month or day of when the contract ends, but just the year. Hence, we'll convert the date to just the year for every player","523f9ff3":"**Let's look at categorical values & see if any need fixing or removal**\nAgain, this can be handled as a Preprocessing step in Production for model inference","636012d3":"First up, let's handle nulls so our models dont have to worry about them","d4975555":"Before we fill the null values for numerical columns with imputations, let's split the train & test data set so that we dont use the test data to compute our imputation values!\n\nNow, we can use stratified sampling to perform the split based on a feature, but with so many ratings & features, it's not so easy. We'll just try out randomized sampling to start with.","a146bdf9":"We'll take the 2 best models (GradientBoostingRegressor & RandomForestRegressor) & train them properly - use cross-validation, regularization & hyper-parameter tuning.\n\nWe'll try a RandomSearch with some wide parameter space & then, we can use GridSearch to fine-tune it further. Also, we should try more iterations, but I didn't just to save on execution time for now.","28ee6567":"So, we fixed all the null issues in String\/Categorical columns!","8b792959":"Let's preprocess the data so that it's in a machine-readable form","3d5d516a":"# Model Selection","4af60220":"We can do more fine-grained tuning with GridSearchCV to get the absolute best parameters, but I'll skip that for now (but I would do it if I was developing a Production model).\n\nFor now, I'll just get the best model & evaluate it against the test data-set","d9d1780e":"We'll also set null categorical variable values to 'Unknown' since nulls aren't processed correctly. This can be used as a preprocessing step in Production for model inference."}}