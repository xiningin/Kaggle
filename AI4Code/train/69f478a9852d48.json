{"cell_type":{"99bde97a":"code","ff2d857b":"code","fa32b5a9":"code","a281bdf2":"code","d55f17a3":"code","3fe13f5f":"code","893c3482":"code","ff0d6a86":"code","0d777e2f":"code","7bc2ecf4":"code","b513ed34":"code","bf6971da":"code","25b0892d":"code","4f90b5bc":"code","437decd6":"code","5fdbfc79":"code","c81b4f72":"code","e22c479d":"code","e5dc7489":"code","82d7afb5":"code","14b70598":"code","a7182358":"code","87e08dd9":"code","a473ed50":"code","e797e507":"code","97bbee0f":"code","fba6ec41":"code","c547846b":"code","b0050f8d":"code","6d8d547f":"code","768b0118":"code","c1d7ee34":"markdown","839f7ef9":"markdown","6c160104":"markdown","abd17755":"markdown","4db903f6":"markdown","c220a08c":"markdown","98c0695d":"markdown","4f0d3ded":"markdown","010d58a8":"markdown","e8c897d3":"markdown","d9f4ef5b":"markdown","6edb891c":"markdown","4c47c984":"markdown","e6872f01":"markdown","792f0383":"markdown","bc547b61":"markdown","99b4d7ff":"markdown","a10c2498":"markdown","e902a117":"markdown","3c70b6a5":"markdown","5ce68391":"markdown"},"source":{"99bde97a":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score, confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold, GridSearchCV, KFold\n\nimport xgboost\nimport lightgbm\nimport tqdm\nimport os,gc\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')","ff2d857b":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if(col != 'time'):\n            col_type = df[col].dtype\n\n            if col_type != object:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)\n            else:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","fa32b5a9":"train = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')\ntest = pd.read_csv('..\/input\/liverpool-ion-switching\/test.csv')\nprint(f' Shape of train : {train.shape}')\nprint(f' Shape of test : {test.shape}')","a281bdf2":"train.head(10)","d55f17a3":"train.tail()","3fe13f5f":"test.head(10)","893c3482":"train.describe()","ff0d6a86":"fig, ax = plt.subplots(1,2, figsize=(20,6))\nsns.countplot(train.open_channels, ax=ax[0])\nsns.distplot(train.open_channels, ax=ax[1])","0d777e2f":"train['open_channels'].value_counts()","7bc2ecf4":"fig, ax = plt.subplots(6,2,figsize=(20,36))\nax = ax.flatten()\nfor i in range(11):\n    sns.distplot(train[train['open_channels'] == i].signal, ax=ax[i]).set_title(f'Distribution of signal for {i} open channels')","b513ed34":"describe_df = train.groupby(['open_channels']).signal.describe()\ndescribe_df","bf6971da":"_, ax = plt.subplots(4, 2, figsize=(20,24))\nax = ax.flatten()\nfor k,i in enumerate(describe_df.columns):\n      sns.lineplot(describe_df.index, describe_df[i], ax=ax[k], lw=2).set_title(f'{i} signal vs open_channels')","25b0892d":"train['signal_diff'] = train['signal'].diff()\ntrain['open_channels_diff'] = train['open_channels'].diff()\ntrain = train.fillna(0)\ntrain","4f90b5bc":"_, ax = plt.subplots(5,1,figsize=(20,30))\nax = ax.flatten()\nfor i in range(5):\n    sns.scatterplot('signal_diff', 'open_channels_diff', data=train[int(1e6)*i:int(1e6)*i + int(1e5)], ax=ax[i]).set_title(f'Change in signal vs Change in open_channels (signal range {int(1e6)*i}:{int(1e6)*i + int(1e5)})')","437decd6":"_, ax = plt.subplots(10,2, figsize=(20, 60))\nax = ax.flatten()\nk=0\nfor i in range(10):\n    x = 500000\n    sample = train.iloc[x*i:x*(1+i)]\n    sns.distplot(sample.signal, ax=ax[k], color='g').set_title(f'Distribution of signal for batch {i+1}')\n    sns.countplot(sample.open_channels, ax=ax[k+1]).set_title(f'Count of open_channels for batch {i+1}')\n    k = k+2","5fdbfc79":"plt.figure(figsize=(25,8))\nsns.lineplot(train.time[:10000], train.signal[:10000])\nsns.lineplot(train.time[:10000], train.open_channels[:10000])","c81b4f72":"plt.figure(figsize=(25,8))\nsns.lineplot(train.time[100000:110000], train.signal[100000:110000])\nsns.lineplot(train.time[100000:110000], train.open_channels[100000:110000])","e22c479d":"plt.figure(figsize=(25,8))\nsns.lineplot(train.time[200000:210000], train.signal[200000:210000])\nsns.lineplot(train.time[200000:210000], train.open_channels[200000:210000])","e5dc7489":"plt.figure(figsize=(25,8))\nsns.lineplot(train.time[1000000:1010000], train.signal[1000000:1010000])\nsns.lineplot(train.time[1000000:1010000], train.open_channels[1000000:1010000])","82d7afb5":"plt.figure(figsize=(25,8))\nsns.lineplot(train.time[2000000:2010000], train.signal[2000000:2010000])\nsns.lineplot(train.time[2000000:2010000], train.open_channels[2000000:2010000])","14b70598":"plt.figure(figsize=(25,8))\nsns.lineplot(train.time[3000000:3010000], train.signal[3000000:3010000])\nsns.lineplot(train.time[3000000:3010000], train.open_channels[3000000:3010000])","a7182358":"plt.figure(figsize=(20,6))\nsns.distplot(test.signal, bins=500, color='red')","87e08dd9":"test.signal.describe([0, .25, .5, .75, .98])","a473ed50":"def add_group(data,size):\n    rows_per_group=size\n    groups =[]\n    group_no=0\n    \n    for i in range(0,len(data),rows_per_group):\n        groups.extend([group_no]*rows_per_group)\n        group_no+=1\n    print('Total Groups for size {}:'.format(size),len(set(groups)))\n    \n    groups=groups[:len(data)]\n    return groups","e797e507":"data_df = pd.concat([train, test], sort=False).reset_index(drop=True)\n\n\n# Create Groups (1k,2k,5k)\ndata_df['batch_1k'] = data_df[['time']].apply(lambda x:add_group(x,1000) )\ndata_df['batch_2k'] = data_df[['time']].apply(lambda x:add_group(x,2000)) \ndata_df['batch_5k'] = data_df[['time']].apply(lambda x:add_group(x,5000) )","97bbee0f":"data_df = reduce_mem_usage(data_df)\n\n\nbatch_cols = [i for i in data_df.columns if 'batch' in i]\nfor i in batch_cols:\n\n    data_df[f'signal_{i}_mean'] = data_df.groupby(i)['signal'].transform('mean')\n    data_df[f'signal_{i}_median'] = data_df.groupby(i)['signal'].transform('median')\n    data_df[f'signal_{i}_min'] = data_df.groupby(i)['signal'].transform('min')\n    data_df[f'signal_{i}_max'] = data_df.groupby(i)['signal'].transform('max')\n    data_df[f'signal_{i}_std'] = data_df.groupby(i)['signal'].transform('std')\n    data_df[f'signal_{i}_skew'] = data_df.groupby(i)['signal'].transform('skew')\n\n    data_df[f'signal_{i}_diff_max_min'] = data_df[f'signal_{i}_max'] - data_df[f'signal_{i}_min']\n    data_df[f'signal_{i}_ratio_max_min'] = data_df[f'signal_{i}_max'] \/ data_df[f'signal_{i}_min']\n    \n    data_df[f'signal_{i}_shift_1'] = data_df.groupby(i).shift(1)['signal']\n    data_df[f'signal_{i}_shift_-1'] = data_df.groupby(i).shift(-1)['signal']\n    data_df[f'signal_{i}_shift_2'] = data_df.groupby(i).shift(2)['signal']\n    data_df[f'signal_{i}_shift_-2'] = data_df.groupby(i).shift(-2)['signal']\n    data_df[f'signal_{i}_shift_3'] = data_df.groupby(i).shift(3)['signal']\n    data_df[f'signal_{i}_shift_-3'] = data_df.groupby(i).shift(-3)['signal']\n    \n    data_df[f'signal_{i}_rolling_W2_mean'] = data_df.groupby(i)['signal'].rolling(2).mean().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W10_mean'] = data_df.groupby(i)['signal'].rolling(10).mean().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W100_mean'] = data_df.groupby(i)['signal'].rolling(100).mean().reset_index(drop=True)\n\n\n    data_df[f'signal_{i}_rolling_W2_median'] = data_df.groupby(i)['signal'].rolling(2).median().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W10_median'] = data_df.groupby(i)['signal'].rolling(10).median().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W100_median'] = data_df.groupby(i)['signal'].rolling(100).median().reset_index(drop=True)\n\n\n    data_df[f'signal_{i}_rolling_W2_min'] = data_df.groupby(i)['signal'].rolling(2).min().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W10_min'] = data_df.groupby(i)['signal'].rolling(10).min().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W100_min'] = data_df.groupby(i)['signal'].rolling(100).min().reset_index(drop=True)\n\n\n    data_df[f'signal_{i}_rolling_W2_max'] = data_df.groupby(i)['signal'].rolling(2).max().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W10_max'] = data_df.groupby(i)['signal'].rolling(10).max().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W100_max'] = data_df.groupby(i)['signal'].rolling(100).max().reset_index(drop=True)\n\n\n    data_df[f'signal_{i}_rolling_W2_std'] = data_df.groupby(i)['signal'].rolling(2).std().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W10_std'] = data_df.groupby(i)['signal'].rolling(10).std().reset_index(drop=True)\n    data_df[f'signal_{i}_rolling_W100_std'] = data_df.groupby(i)['signal'].rolling(100).std().reset_index(drop=True)\n\n    data_df = reduce_mem_usage(data_df)","fba6ec41":"data_df.head()","c547846b":"train = data_df[data_df['train']==1]\ntest = data_df[data_df['train']==0]\ntrain['open_channels'] = train['open_channels'].astype(int)","b0050f8d":"del data_df\ngc.collect()","6d8d547f":"FEATURES = train.drop(['time', 'signal', 'open_channels', 'train', 'batch_1k',\n       'batch_2k', 'batch_5k'],1).columns\ny = train['open_channels']\n\nsubmission = pd.DataFrame()\nsubmission['time'] = test['time']\nsubmission\n\nKFOLD_SPLITS = 10\nSHUFFLE = True\nNUM_BOOST_ROUNDS = 2500\nEARLY_STOPPING_ROUNDS = 50\nVERBOSE_EVAL = 500","768b0118":"cv = KFold(n_splits=KFOLD_SPLITS, shuffle=SHUFFLE, random_state=21)\ncv\n\nparams = {'learning_rate': 0.05,\n          'max_depth': -1,\n          'num_leaves': 2**8+1,\n          'feature_fraction': 0.8,\n          'bagging_fraction': 0.8,\n          'objective':'regression',\n          'metric':'rmse'\n         }\n\noof_df = train[['signal', 'open_channels']].copy()\nfeature_importance_df = pd.DataFrame()\n\nfold_ = 1\nfor train_idx, val_idx in cv.split(train[FEATURES], y):\n    X_train, X_val = train[FEATURES].iloc[train_idx], train[FEATURES].iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    train_set = lightgbm.Dataset(X_train, y_train)\n    val_set = lightgbm.Dataset(X_val, y_val)\n    \n    model = lightgbm.train(params,\n                          train_set,\n                          num_boost_round=NUM_BOOST_ROUNDS,\n                          early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                          verbose_eval=VERBOSE_EVAL,\n                          valid_sets=[train_set, val_set]\n                          )\n    \n    val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n    val_preds = np.round(np.clip(val_preds, 0, 10)).astype(int)\n    \n    test_preds = model.predict(test[FEATURES], num_iteration=model.best_iteration)\n    test_preds = np.round(np.clip(test_preds, 0, 10)).astype(int)\n\n    oof_df.loc[oof_df.iloc[val_idx].index, 'oof'] = val_preds\n    submission[f'open_channels_fold{fold_}'] = test_preds\n    \n    f1 = f1_score(oof_df.loc[oof_df.iloc[val_idx].index]['open_channels'],\n                  oof_df.loc[oof_df.iloc[val_idx].index]['oof'],\n                            average = 'macro')\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = FEATURES\n    fold_importance_df[\"importance\"] = model.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f'Fold {fold_} - validation f1: {f1:0.5f}')\n    \n    fold_ += 1\n\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:100].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nbest_features.sort_values(by=\"importance\",ascending=False).to_csv('.\/FeatureIMP.csv', index=False)\n\nprint(f1_score(oof_df['open_channels'],\n                    oof_df['oof'],\n                    average = 'macro'))\n\nsubmission['open_channels'] = submission.drop(['time'],1).median(axis=1).astype(int)\nsubmission[['time','open_channels']].to_csv('submission.csv', index=False, float_format='%.4f')","c1d7ee34":"Let's take a look at the train set.","839f7ef9":"## <a>Introduction<\/a> \n\nThe University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.\n\nMany diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.","6c160104":"Let's plot the diff in signal vs diff in open_channels","abd17755":"Here,\n1. open_channels has 11 classes (0-10)\n2. The train set is imbalanced, as we've over 1.2M samples for class 0, and approx. 30K for class 10.","4db903f6":"Both signal and no. of open_channels fluctuate simultaneously.","c220a08c":"# Feature Engineering","98c0695d":"Here,\n1. We can say that for higher number of open_channels, the magnitude of signal is also high.\n2. Also, we can see overlapping signal range for different no. of open_channel. For both open_channels 4 and 5, signal density is highest in the region 2-4.","4f0d3ded":"It is mentioned that *** the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001***\nLet's plot signal distribution and countplot of open_channels for these discrete batches.","010d58a8":"Signal density is:\n1. Highest in range -3 to -2.\n2. Moderate in range -2 to 3.\n3. Less further on.\n","e8c897d3":"## <a>MODEL<\/a>","d9f4ef5b":"Insights:\n1. Mean of signal is increasing linearly with no. of open channels.\n2. However, max signal vs open_channels is a zig-zag curve.\n3. We've less samples for higher number of open_channels","6edb891c":"Does higher value of signal opens more number of channels ? Plotting the signal distribution for each channels.","4c47c984":"1. All classes are present in just two batches 5th and 10th.\n2. Only 0 and 1 classes are present in first two batches.","e6872f01":"## <a>Loading Packages and Data<\/a>","792f0383":"## <a>References<\/a>\n\n1. https:\/\/www.kaggle.com\/gpreda\/ion-switching-advanced-eda-and-prediction","bc547b61":"From these plots, we can say that a positive\/negative diff in signal leads to a positive\/negative diff in no of open channels respectively.  ","99b4d7ff":"Let's first check the imbalance of target classes. ","a10c2498":"Now, for the test set.","e902a117":"open_channels is our target column.","3c70b6a5":"Let's visualize the change in signal with time.","5ce68391":"# EDA\n"}}