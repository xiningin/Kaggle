{"cell_type":{"bc521852":"code","914f0168":"code","d06f9dfb":"code","e3937f25":"code","9b525a51":"code","f364bad3":"code","8223c84e":"code","dfb15caa":"code","2170fb61":"code","d2f379e8":"code","87a28321":"code","8f39d333":"code","312cdf8a":"code","0944e706":"code","f573393a":"code","0fda5e3b":"code","aaf7eb1e":"code","be8f2bc7":"code","384a2728":"code","d48d477a":"code","f82b7a14":"code","e582bda4":"code","e1aa5685":"code","8e092057":"code","0109be45":"code","70a9fb92":"code","167d95db":"code","a2550491":"code","cada4b67":"code","a316c65c":"code","316feb2f":"code","044b0ba9":"code","f071720f":"code","0b9b399e":"code","072669b2":"code","0efae7a1":"code","3fabb2b4":"code","bae184be":"code","42d5cd69":"code","45a4a0fc":"code","7c4d2557":"code","fe40c17b":"code","71e5371f":"code","09966796":"code","10e75c76":"code","b1a51d84":"markdown","521bba95":"markdown","01383946":"markdown","f82e274f":"markdown","f4fbbbe3":"markdown","00d14620":"markdown","a17fbcfa":"markdown","e90c64ba":"markdown","0dc2a154":"markdown","ad77fdec":"markdown","dedc534d":"markdown","66e6859c":"markdown","685b56fc":"markdown","eafe9104":"markdown","9dd8ef02":"markdown","431bee64":"markdown","1a257599":"markdown","35ad7312":"markdown","ce6152ee":"markdown","4dccf157":"markdown","1be76ba0":"markdown","35d0016a":"markdown","1b42235c":"markdown","0d77b883":"markdown","2d109547":"markdown","165bc5ed":"markdown","c3cd5bb5":"markdown","e2a41e3a":"markdown","976a3f0c":"markdown","2b28025e":"markdown","cd541eb8":"markdown","e2a02ac6":"markdown","6a063fa8":"markdown","6b2a9696":"markdown","3fe2f2b8":"markdown","aa9a4521":"markdown","b6f198a9":"markdown","b94e64a9":"markdown","215f7baa":"markdown","461ab0e7":"markdown","fba9d736":"markdown","6e08e47d":"markdown","3cbf052c":"markdown","281e3d11":"markdown","32f353b3":"markdown","120cf8d7":"markdown","2275eb53":"markdown","47c06654":"markdown","dedda144":"markdown","1d358b26":"markdown"},"source":{"bc521852":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","914f0168":"df = pd.read_csv('..\/input\/german-credit\/german_credit_data.csv', index_col=0)","d06f9dfb":"df.head()","e3937f25":"df.shape","9b525a51":"df.isnull().sum()","f364bad3":"df['Saving accounts'] = df['Saving accounts'].fillna('None')\ndf['Saving accounts'].value_counts(normalize=True)","8223c84e":"df['Checking account'] = df['Checking account'].fillna('None')\ndf['Checking account'].value_counts(normalize=True)","dfb15caa":"df.head()","2170fb61":"df.shape","d2f379e8":"df.Sex.hist();","87a28321":"df['Sex'] = df['Sex'].apply(lambda x: 1 if x=='male' else 0)","8f39d333":"df.Housing.value_counts(normalize=True)","312cdf8a":"df.Purpose.value_counts(normalize=True)","0944e706":"df['Purpose'].replace(['repairs', 'domestic appliances', 'vacation\/others'], 'others', inplace=True)","f573393a":"df.Purpose.value_counts(normalize=True)","0fda5e3b":"df['Saving accounts'].value_counts(normalize=True)","aaf7eb1e":"df['Saving accounts'].replace(['None', 'little', 'moderate', 'rich', 'quite rich'], [0,1,2,3,4], inplace=True)","be8f2bc7":"df['Checking account'].value_counts(normalize=True)","384a2728":"df['Checking account'].replace(['None', 'little', 'moderate', 'rich'], [0,1,2,3], inplace=True)","d48d477a":"df.head()","f82b7a14":"df.hist(figsize=(12,12));","e582bda4":"df['Duration'] = np.log(df['Duration'])\ndf['Age'] = np.log(df['Age'])\ndf['Credit amount'] = np.log(df['Credit amount'])","e1aa5685":"sns.pairplot(df);","8e092057":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True);","0109be45":"df = pd.get_dummies(df, drop_first=True)","70a9fb92":"df.head()","167d95db":"df.shape","a2550491":"from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE","cada4b67":"inertia = []\nk = range(1, 11)\nfor k_i in k:\n    km = KMeans(n_clusters=k_i).fit(df)\n    inertia.append(km.inertia_)\n    \nplt.plot(k, inertia)\nplt.xlabel('k')\nplt.ylabel('inertia')\nplt.title('The Elbow Method showing the optimal k');","a316c65c":"distance_mat = pdist(df)\n\nZ = hierarchy.linkage(distance_mat, 'ward')","316feb2f":"plt.figure(figsize=(20, 10))\n\nplt.title('Hierarchical Clustering Dendrogram (truncated)')\nplt.xlabel('cluster size')\nplt.ylabel('distance')\nhierarchy.dendrogram(\n    Z,\n    truncate_mode='lastp',\n    p=12,  \n    leaf_font_size=12.,\n    show_contracted=True, \n)\nplt.show()","044b0ba9":"silhouette_scores = [] \nk = range(2,8)\n\nfor n_cluster in k:\n    silhouette_scores.append( \n        silhouette_score(df, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(df))) \n    \n    \n# Plotting a bar graph to compare the results \n\nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() ","f071720f":"db = DBSCAN(eps=1.61, min_samples=4).fit(df)","0b9b399e":"# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)\nn_noise_ = list(db.labels_).count(-1)\n\nprint('Estimated number of clusters: {}'.format(n_clusters_))\nprint('Estimated percentage of noise points: {:.2f}%'.format(100*n_noise_\/df.shape[0]))","072669b2":"from sklearn.manifold import TSNE","0efae7a1":"def draw_tsne(df):\n    _, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8), sharey=True)\n\n    tsne=TSNE(perplexity=5).fit_transform(df)\n    axes[0, 0].title.set_text('Perplexity 5')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[0, 0]);\n\n    tsne=TSNE(perplexity=10).fit_transform(df)\n    axes[0, 1].title.set_text('Perplexity 10')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[0, 1]);\n\n    tsne=TSNE(perplexity=20).fit_transform(df)\n    axes[0, 2].title.set_text('Perplexity 20')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[0, 2]);\n\n    tsne=TSNE(perplexity=30).fit_transform(df)\n    axes[1, 0].title.set_text('Perplexity 30')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[1, 0]);\n\n    tsne=TSNE(perplexity=40).fit_transform(df)\n    axes[1, 1].title.set_text('Perplexity 40')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[1, 1]);\n\n    tsne=TSNE(perplexity=50).fit_transform(df)\n    axes[1, 2].title.set_text('Perplexity 50')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[1, 2]);","3fabb2b4":"draw_tsne(df)","bae184be":"tsne=TSNE(perplexity=30).fit_transform(df)","42d5cd69":"plt.figure(figsize=(12,12))\nplt.title('Perplexity 30')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1]);","45a4a0fc":"plt.figure(figsize=(10, 10))\nplt.title('DBSCAN, 2 clusters')\nplt.scatter(tsne[:, 0], tsne[:, 1], c=db.labels_);","7c4d2557":"km = KMeans(n_clusters=2).fit(df)\nagg_cluster = AgglomerativeClustering(n_clusters = 2).fit(df)\n\n_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n\naxes[0].title.set_text('K-MEANS, 2 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=km.labels_, ax=axes[0]);\n\n\nplt.title('Hierarchical clustering, 2 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=agg_cluster.labels_, ax=axes[1]);","fe40c17b":"km = KMeans(n_clusters=3).fit(df)\nagg_cluster = AgglomerativeClustering(n_clusters = 3).fit(df)\n\n_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n\naxes[0].title.set_text('K-MEANS, 3 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=km.labels_, ax=axes[0], palette=['green','orange','brown']);\n\n\nplt.title('Hierarchical clustering, 3 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=agg_cluster.labels_, ax=axes[1], palette=['green','orange','brown']);","71e5371f":"km = KMeans(n_clusters=4).fit(df)\nagg_cluster = AgglomerativeClustering(n_clusters = 4).fit(df)\n\n_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n\naxes[0].title.set_text('K-MEANS, 4 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=km.labels_, ax=axes[0], palette=['green','orange','brown', 'yellow']);\n\n\nplt.title('Hierarchical clustering, 4 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=agg_cluster.labels_, ax=axes[1], palette=['green','orange','brown', 'yellow']);","09966796":"agg_cluster = AgglomerativeClustering(n_clusters = 3).fit(df)","10e75c76":"fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(40, 20))\n\ni_col = 0\ni_row = 0\n\nfor column in df.columns:\n    sns.boxplot(y=column, x=agg_cluster.labels_, \n                     data=df, \n                     palette=\"colorblind\", ax=ax[i_row, i_col])\n    if i_row < 4:\n        i_row += 1\n    else:\n        i_col += 1\n        i_row = 0\n","b1a51d84":"### K Means","521bba95":"It's easy to count %, in this case. So, we have 18.3% of nulls in 'Saving account' feature and 39.4% of nulls in 'Checking account' feature. There aren't any solid rule for what to do with missing values. Usually, I get rid of observations, if it have less than 5% of nulls. In this case, let's take a closer look.","01383946":"Later we will encode it with numbers. We could use one-hot encoders, but here we can use just numbers (as ranking feature). Because we previously suggest, that nulls is a distinct category.","f82e274f":"It doesn't give any additional information.","f4fbbbe3":"After performing log transformation, we don't have to use scaling.","00d14620":"# Interpretation","a17fbcfa":"Now we using sklearn impolemention of metric, to better understand how much clusters we have with hierarchical clustering.","e90c64ba":"Let's check if we have any nulls.","0dc2a154":"Well, it's seem like we have 3 clusters. So let's try to find out who are they.","ad77fdec":"## Creating models","dedc534d":"### DBSCAN","66e6859c":"## One Hot Encoding","685b56fc":"### TSNE","eafe9104":"Age, credit amount and duration - numerical features with long tail. So we should try log them to get more normalized distrubution.\n\nAs we don't have too much observations, making pairplot won't take much time.","9dd8ef02":"I was trying to get epsilon with minimal amount of noise for 2 clusters.","431bee64":"## Categorical features","1a257599":"Well, there is some pattern. But not too good.\nLet's compare Kmeans and hierarchical clustering.","35ad7312":"So, there are three groups:\n- Men, with a moderate jobs and now savings\n- Women, with a highly skilled jobs and some savings. Also this group take higher amount of money for longer periods, and not for TV\/radio.\n- Men, with no job or not a resident and with a lot of savings","ce6152ee":"And again we can see three groups.","4dccf157":"### Hierarchical clustering","1be76ba0":"Just checking, that's everything is ok.","35d0016a":"First, let's substitute Nulls with 'None', to see how distribution of observations will look like.","1b42235c":"We can decrease the amount of categories for this feature. I take the last three categories and sum them up into category 'others'(each of them is presented in less than 5%).","0d77b883":"We have only 'Housing' and 'Purpose', that cannot be encoded like ranking, so we will use one-hot-encoding instead.","2d109547":"From silhouette score we have 3 clusters here. Probably 2 or 4, but not 5.","165bc5ed":"## Overwiew of the features","c3cd5bb5":"There two times more males than females. Let's encode sex. Male - 1, female - 0.","e2a41e3a":"Well, from the dendrogram 2, 3 and 4 - are the best fit of clusters. However, it's not clear.","976a3f0c":"# 4 clusters","2b28025e":"# 2 clusters","cd541eb8":"In this notebook I was trying to divide bank's clients into groups, based on their behaviour. I used a few methods for this:\n- KMeans clustering\n- Hierarchical clustering (Agglomerative)\n- Silhouette score\n- DBSCAN\n- TSNE ","e2a02ac6":"Just checking if everything is looks right. ","6a063fa8":"From KMeans it seems to be 2, 4 or maybe 5 clusters.","6b2a9696":"We don't have too correlated features. However we can see, that the most correlated features are: 'Job', 'Credit amount' and 'Duration'. Also it seems like that among clients of the banks men are older than women.","3fe2f2b8":"# 3 clusters","aa9a4521":"### Silhouettte score","b6f198a9":"## Missing values","b94e64a9":"Let's stick to our hypothesis. And do the same thing with 'Checking account' as with 'Saving account'.","215f7baa":"To use K-means method we should find the amount of clusters.","461ab0e7":"Let's try to interpret, what are these three groups.","fba9d736":"Let's check, if we've done everything right.","6e08e47d":"Let's see how good we are able to group clients in clusters. This is a function for choosing perplexity (it's not the best way to do it).","3cbf052c":"It's always feels like a game to guess, what is the best parameters for DBSCAN. However, this model can give you the percantage of noise (that still can be another cluster).","281e3d11":"It seems like hierachical clustering is better for two clusters.","32f353b3":"![](http:\/\/)Everything is working. Nice. So we are ready to go in clustering and stuff.","120cf8d7":"## Scaling","2275eb53":"It's a relatively small dataframe, so we can apply any method or function. It's still will take not too much time to perform on my laptop.","47c06654":"This feature should be encoded with one hot encoding.","dedda144":"It's not clear why we don't have information about Saving accounts in some observations. And we can't just drop 18.3 % of data. So I will leave it as it is, with additional option 'None'. Basically, we iterpret this nulls as one more category of the feature.","1d358b26":"It seems to have quiet a lot of categorical features."}}