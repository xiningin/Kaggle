{"cell_type":{"47e63179":"code","3bced343":"code","652d5d38":"code","4e227602":"code","cf587a1a":"code","94084e0c":"code","10d5ee22":"code","23448613":"code","4916f597":"code","50a3a6ba":"code","71510131":"code","bd59ab3c":"code","a05c1fa1":"code","b9b1b492":"code","4eeb89a3":"code","885987b2":"code","f1095982":"code","da04c8df":"code","e949f658":"code","5babe96a":"code","34b0ae97":"code","0f37d86e":"code","52360ac9":"code","9063aec3":"code","7bf5c0a6":"code","5d27cd5f":"code","542bb9be":"markdown","d8ba74a2":"markdown","4686c418":"markdown","914602f1":"markdown","64ae4400":"markdown","ded16713":"markdown","74f3e96d":"markdown","ed74e348":"markdown","75ca1f47":"markdown","f6e8207c":"markdown","e5068ddb":"markdown","1253ee47":"markdown","b2c83c5b":"markdown","99beb02f":"markdown","ab0b26fc":"markdown","0a6bed54":"markdown","b4d5e6da":"markdown","00e2b119":"markdown","aa667043":"markdown","eb9dc192":"markdown","e45c985f":"markdown","c7cf157c":"markdown","de071e81":"markdown","0292c0ca":"markdown","34a318a1":"markdown","59ba449b":"markdown","fc4e90aa":"markdown","f6b3fc5a":"markdown","b0e26f24":"markdown","c4cff426":"markdown","40a4bee6":"markdown","2800a953":"markdown","6fbab4e9":"markdown","40e89dcf":"markdown","05785a8b":"markdown","2dd4b123":"markdown","f8406609":"markdown","10ad147c":"markdown","ff7b75ce":"markdown","fbf173a2":"markdown","a68206b0":"markdown","0d1e08c4":"markdown","c23dc22e":"markdown","321525f0":"markdown","de9065bd":"markdown"},"source":{"47e63179":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3bced343":"data = pd.read_csv(\"\/kaggle\/input\/mushroom-classification\/mushrooms.csv\")","652d5d38":"data.head()","4e227602":"data.columns","cf587a1a":"data.info()","94084e0c":"data.shape","10d5ee22":"for i in data.columns:\n    print(i, data[i].unique())","23448613":"# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in data.columns:\n    data[i] = le.fit_transform(data[i])","4916f597":"data","50a3a6ba":"sns.countplot(data[\"class\"])\nplt.title(\"Countplot for class\")\nplt.show()","71510131":"data.corr()","bd59ab3c":"fig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, ax=ax)\nplt.show()","a05c1fa1":"y = data[\"class\"].values\nx = data.drop([\"class\"],axis=1)","b9b1b492":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=11)","4eeb89a3":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","885987b2":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # empty neural network\nfrom keras.layers import Dense # layer constitution\n\ndef build_classifier():\n    classifier = Sequential() # initialize neural network architecture\n    classifier.add(Dense(units = 8, kernel_initializer=\"uniform\", activation=\"relu\", input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 8, kernel_initializer=\"uniform\", activation=\"relu\")) #kernel_initializer: to initialize weights\n    classifier.add(Dense(units = 1, kernel_initializer=\"uniform\", activation=\"sigmoid\")) #output layer\n    classifier.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs=70, batch_size=10)\n# epoch = number of iteration, batch size : efers to the number of training examples utilized in one iteration.\naccurisies = cross_val_score(estimator=classifier, X=x_train, y = y_train, cv = 2)\nmean = accurisies.mean()\nvariance = accurisies.std()\nprint(\"Accuracy mean : \", str(mean))\nprint(\"Accuracy variance : \", str(variance))","f1095982":"history = classifier.fit(x_test, y_test, validation_split=0.20, epochs=70, batch_size=10, verbose=1)\n# Accurasy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy vs Epoch')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n# Loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss vs Epoch')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","da04c8df":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.reshape(-1,1).T\ny_test = y_test.reshape(-1,1).T","e949f658":"def initialize_parameters_and_layer_sizes_ANN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(5,x_train.shape[0]) * 0.1, #5 represents for 5 nodes\n                  \"bias1\": np.zeros((5,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],5) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","5babe96a":"def sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","34b0ae97":"def forward_propagation_ANN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"] # z = wx+b\n    y_head1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],y_head1) + parameters[\"bias2\"]\n    y_head2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"y_head1\": y_head1,\n             \"Z2\": Z2,\n             \"y_head2\": y_head2}\n    \n    return y_head2, cache","0f37d86e":"def compute_cost_ANN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y) # Y real y values\n    cost = -np.sum(logprobs)\/Y.shape[1] # for normalization\n    return cost","52360ac9":"def backward_propagation_ANN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"y_head2\"]-Y # derivative with respect to y_head2\n    dW2 = np.dot(dZ2,cache[\"y_head1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1] #keepdims=True writes results in to array\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"y_head1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","9063aec3":"def update_parameters_ANN(parameters, grads, learning_rate = 0.1):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate * grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate * grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate * grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate * grads[\"dbias2\"]}\n    \n    return parameters","7bf5c0a6":"def predict_ANN(parameters,x_test):\n    # x_test is a input for forward propagation\n    y_head2, cache = forward_propagation_ANN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(y_head2.shape[1]):\n        if y_head2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","5d27cd5f":"def two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_ANN(x_train, y_train)\n\n    for i in range(0, num_iterations): #it will make forward backward prop. , compute cost, update parameters by the amount of number of iterations\n         # Forward Propagation \n        y_head2, cache = forward_propagation_ANN(x_train,parameters)\n        \n        # Compute Cost \n        cost = compute_cost_ANN(y_head2, y_train, parameters)\n        \n         # Backward Propagation \n        grads = backward_propagation_ANN(parameters, cache, x_train, y_train)\n        \n         # Update Parameters\n        parameters = update_parameters_ANN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    plt.figure(figsize=(15,12))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # Prediction\n    y_prediction_test = predict_ANN(parameters,x_test)\n    y_prediction_train = predict_ANN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"Train Accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"Test Accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=7000)","542bb9be":"<a id = '14'><\/a><br>\n## Backward Propagation","d8ba74a2":"#### Cross Entropy Loss: \n![c_e_l.png](attachment:453a194d-89b8-4cef-abf8-bac1871789d9.png)","4686c418":"#### Weights and bias will be chosen randomly. The simplest way to initialize weights and biases is to set those to small uniform random values which works well for neural networks with a single hidden layer.","914602f1":"<font color = 'purple'>\nContent:\n    \n1. [Introduction](#1)\n1. [Read and Encode Data](#2)\n1. [Column Description](#3)\n1. [Label Encoding](#4)\n1. [Train-Test Split](#5)\n1. [ANN with Keras](#6)\n    * [Hyperparameters in ANN](#7)\n    * [Plot](#8)\n1. [ANN without Keras](#9)\n    * [2-Layer Neural Network](#10)\n        * [Size of layers and initializing parameters weights and bias](#11)\n        * [Forward propagation](#12)\n        * [Loss function and Cost function](#13)\n        * [Backward propagation](#14)\n        * [Update Parameters](#15)\n        * [Prediction with learnt parameters weight and bias](#16)\n        * [Create Model](#17)","64ae4400":"<a id = '16'><\/a><br>\n## Prediction with Learnt Parameters","ded16713":"<a id = '12'><\/a><br>\n## Forward Propagation","74f3e96d":"#### First of all, If you do not informed about logistic regression I highly recomend you to look at logistic regression first. There is a kernel about it written by me. In this notebook there are information about forward-backward propagation, creating models etc. You can find it [here](https:\/\/www.kaggle.com\/feritebrargrler\/heart-disease-classification-logistic-regression).","ed74e348":"#### Forward propagation is almost same with logistic regression. The only difference is we use tanh function and we make all process twice. For whom asking what is the difference between sigmoid and tanh function : Sigmoid returns values 0 to 1. Tanh returns values -1 to 1","75ca1f47":"<a id = '7'><\/a><br>\n### Hyperparameters in ANN","f6e8207c":"<a id = '9'><\/a><br>\n# ANN without Keras","e5068ddb":"![m3.jpg](attachment:dfaddaee-56b8-45c9-9fad-91dabe8d0727.jpg)","1253ee47":"<a id = '1'><\/a><br>\n# Introduction","b2c83c5b":"<a id = '3'><\/a><br>\n## Column Description","99beb02f":"<a id = '2'><\/a><br>\n# Read and Encode Data","ab0b26fc":"#### Some activation functions : \n![act.png](attachment:ff6f81d3-6217-40fb-a402-516a25648d79.png)","0a6bed54":"#### This is updating equations. By taking derivative of cost function according to weights and biases. Then multiply it with \u03b1 learning rate. Step by step model will be updated by this method. Learning rate can be say like learning speed. In other words, Incremental step is stepping by the helping of learning rate.","b4d5e6da":"![le.png](attachment:86c5010f-a816-4ec5-92eb-a0a6bad30e48.png)","00e2b119":"<a id = '8'><\/a><br>\n## Plot","aa667043":"#### Here, i see bunch of unknown words and meaningless letters :) Lets start with know them and make them meanningfull. ","eb9dc192":"<a id = '6'><\/a><br>\n# ANN with Keras","e45c985f":"#### Lets try to classify mushrooms edible or poisonous. First we will make it with keras. ","c7cf157c":"* #### Cost function helps us reach the optimal solution. The cost function is the technique of evaluating \u201cthe performance of our algorithm\/model\u201d. It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction. \n* #### Cost function is sumation of each inputs loss.\n* #### After forward propagation, our model would be contrast the predicted values and true values. After each forward propagation we will get new cost values.\n* #### In this model we will use cross entropy loss.","de071e81":"![layers.png](attachment:603bc4b9-6b9b-4f27-bf40-c6021a0d2eff.png)","0292c0ca":"#### The arranements on the numbers are like that becasue of the matrix multiplication.","34a318a1":"#### Backward propagation is all about derivative. Basically in neural networks, you forward propagate to get the output and compare it with the real value to get the error. Now, to minimize the error, you propagate backwards by finding the derivative of error with respect to each weight and then subtracting this value from the weight value. The purpose of this calculations is optimize the algorithm.","59ba449b":"#### Whole data is not null.","fc4e90aa":"#### For this stage, as it was in logistic regression z1 is calculated with fimiliar expression z = wx+b, after the it putted into an activation function tanh. For other node it replied. The only difference sigmoid is used. Lastly, probabilistic number from sigmoid is our output. ","f6b3fc5a":"<a id = '13'><\/a><br>\n## Loss and Cost Function","b0e26f24":"#### On this notebook aim is understand how artificial neural network is working, which methods are basically using background ? With this mushroom classification data we will classify mushrooms according to using artificial neural networks without using libraries. After that we will make it with keras.","c4cff426":"#### Path will be like that :\n* Size of layers and initializing parameters weights and bias\n* Forward propagation\n* Loss function and Cost function\n* Backward propagation\n* Update Parameters\n* Prediction with learnt parameters weight and bias\n* Create Model\n","40a4bee6":"# Mushroom Classification with ANN","2800a953":"#### For whom wanted to more know :\n#### * class: edible=e, poisonous=p\n#### * cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n#### * cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n#### * cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n#### * bruises: bruises=t,no=f\n#### * odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n#### * gill-attachment: attached=a,descending=d,free=f,notched=n\n#### * gill-spacing: close=c,crowded=w,distant=d\n#### * gill-size: broad=b,narrow=n\n#### * gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n#### * stalk-shape: enlarging=e,tapering=t\n#### * stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n#### * stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n#### * stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n#### * stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n#### * stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n#### * veil-type: partial=p,universal=u\n#### * veil-color: brown=n,orange=o,white=w,yellow=y\n#### * ring-number: none=n,one=o,two=t\n#### * ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n#### * spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n#### * population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n#### * habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n#### * Cap: The cap is the top of the mushroom (and often looks sort of like a small umbrella). Mushroom caps can come in a variety of colors but most often are brown, white, or yellow.\n#### * Gills, Pores, or Teeth: These structures appear under the mushroom's cap. They look similar to a fish's gills.\n#### * Ring: The ring (sometimes called the annulus) is the remaining structure of the partial veil after the gills have pushed through.\n#### * Stem or Stipe: The stem is the tall structure that holds the cap high above the ground.\n#### * Volva: The volva is the protective veil that remains after the mushroom sprouted up from the ground. As the fungus grows, it breaks through the volva.\n#### * Spores: Microscopic seeds acting as reproductive agents; they are usually released into the air and fall on a substrate to produce a new mushroom.","6fbab4e9":"#### Label encoder converts categorical datas to numeric datas which we need to work on.","40e89dcf":"<a id = '11'><\/a><br>\n## Size of Layers and Initializing Parameters Weights and Bias","05785a8b":"<a id = '15'><\/a><br>\n## Update Parameters","2dd4b123":"#### *  Learning rate can be say like learning speed. In other words, Incremental step is stepping by the helping of learning rate. There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc\n#### * Number iterations is the number of batches of data the algorithm has seen (or simply the number of passes the algorithm has done on the dataset). Epochs is the number of times a learning algorithm sees the complete dataset. Also it can be say like that : how many times the model will make forward-backward propagation, compute cost, predict values (You will see in without keras part)\n#### * Number of hidden units changeble from kind of problem. If data is less complex and is having fewer dimensions or features then neural networks with 1 to 2 hidden layers would work. If data is having large dimensions or features then to get an optimum solution, 3 to 5 hidden layers can be used. But regarding to problem this amount can be very big.\n#### * Number of hidden units are like number of hidden layers. If you have too few hidden units, you will get high training error and high generalization error due to underfitting and high statistical bias. If you have too many hidden units, you may get low training error but still have high generalization error due to overfitting and high variance.\n#### * An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network. The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function. Most common activation functions are sigmoid, tanh (hiperbolic tangent), relu, swish. These are changeble according to the problem. So we call them hyperparameters. But for general i can say that sigmoid is not using on hidden layers. If it would be used, it used in output layer. For hidden units most common one is relu.\n#### * Optimizers are like activation functions. Some of them are stochastic gradient desctent, RMSprop,Adagrad, momentum, adam. Each of them have advantages and disadvantages. It would be nice if you know what are these advantages and disadvantages. It would make sense to use the one that gives the optimum rate or accuracy at the optimum time. I can say that most common one is adam. Some advantages of Adam is that relatively low memory requirements and usually works well even with little tuning of hyperparameters","f8406609":"#### *  ANN can be thought as multiple recurring logistic regression.\n#### * In logistic regression there were input and output layers. But in ANN there are input, hidden(at least 1) and output layers.\n#### * Hidden layers(nodes) do not see inputs. For this reason they called as hidden layers.\n#### * Increasing on the number of layers, increases nonlinearity. This increases exploration. However, increasing number of layers is not increase accuracy all the time.\n#### * Hidden units are the number of neurons or each node in a layer.\n#### * In Deep Learning there are concept as feature engineering. This changed with model architecture engineering. Because, deep learning models decide which feature important or non important. (Weights)\n#### * In ANN input layers are not counted as layers. If someone says 2 layers network, you must understand that there are input layer, 1 hidden  and output layer.\n#### * First part (without keras), we will constitute 2 layers network.\n#### * Do not forget the equation : z = b + w1.x1 + w2.x2 + .... + wn.xn\n#### * Activation(z) will be our y_pred","10ad147c":"![svst.jfif](attachment:00fae41a-881c-420c-ad5b-a2b82c2e56e3.jfif)","ff7b75ce":"<a id = '17'><\/a><br>\n## Create Model","fbf173a2":"#### There are some hyperparameters that must be chosen intuitively.\n* Learning Rate\n* Number of Iteration\n* Number of Hidden Layers\n* Number of Hidden Units\n* Activation Functions\n* Optimizer","a68206b0":"<a id = '10'><\/a><br>\n## 2-Layer Neural Network","0d1e08c4":"<a id = '4'><\/a><br>\n### Label Encoding","c23dc22e":"#### I leave a [gif link](https:\/\/www.google.com\/search?q=woptimization+deep+learning+gif&tbm=isch&ved=2ahUKEwi88o2AidzxAhXIwKQKHRnZCbQQ2-cCegQIABAA&oq=woptimization+deep+learning+gif&gs_lcp=CgNpbWcQA1D8lAFYxZsBYLmcAWgAcAB4AIABbIgBvgKSAQMwLjOYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ&sclient=img&ei=MnHrYLyRKciBkwWZsqegCw&bih=610&biw=1366#imgrc=nkJHfKXTjfXwGM&imgdii=ZGctRMduE6Oc7M) like this so that aim of the optimization methods can be visualized in your mind.","321525f0":"![image.png](attachment:f594fe50-07d4-4f19-91cf-55d09dd27199.png)","de9065bd":"<a id = '5'><\/a><br>\n## Train Test Split"}}