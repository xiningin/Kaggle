{"cell_type":{"89a280a4":"code","1d218c11":"code","0b2e870d":"code","3da26006":"code","1c1a14c5":"code","443dec30":"code","715cdbc7":"code","79d013df":"code","ef18234f":"code","8ac5de32":"code","ae5cdc4f":"code","4b1af962":"code","81ca3c3f":"code","f4abde19":"code","c1377b4f":"code","89c5cb9f":"code","c5bd9fda":"code","0e02f455":"code","03a8a076":"markdown","8c6ff07b":"markdown","ee942e12":"markdown","6892f9e1":"markdown","41c362b6":"markdown","57ff03fe":"markdown","220717ef":"markdown","aed48b4c":"markdown","bb3e3da8":"markdown","5937e7fb":"markdown","e7692b93":"markdown","b65cea1b":"markdown","ec9d295f":"markdown","dfe5d13f":"markdown","15247fc8":"markdown","e836fbc8":"markdown","fdf6d2e5":"markdown","ac0faa42":"markdown"},"source":{"89a280a4":"from sklearn.datasets import fetch_olivetti_faces\nfaces, labels = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=42)","1d218c11":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef _plot_face(face):\n    if face.shape != (64, 64):\n        face = face.reshape(64, 64)\n    plt.imshow(face, cmap='gray')\n    plt.axis('off')\n\n    \ndef plot_faces(faces, cols=4):\n    faces = np.array(faces)\n    if len(faces.shape) == 1:\n        faces = faces[None, :]\n    m = faces.shape[0]\n    \n    rows = m \/\/ cols\n    if m % cols != 0:\n        rows += 1\n    fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))\n    if len(axes.shape) == 1:\n        axes = axes[None, :]\n    \n    for i in range(rows):\n        for j in range(cols):\n            try:\n                plt.sca(axes[i, j])  # set current axes\n                face = faces[i * cols + j]  # get face\n            except IndexError:\n                plt.axis('off')\n                continue\n            _plot_face(face)\n    return fig, axes\n\n# plot all distinct persons\n_, idx = np.unique(labels, return_index=True)\nplot_faces(faces[idx], cols=5);","0b2e870d":"from sklearn.decomposition import PCA\n\nn_components = 20\ncomp = n_components\npca = PCA(comp)\nX2D = pca.fit_transform(faces)\n\neokm = pca.components_.reshape((comp, 64,64))\nplot_faces(eokm, cols = 4)\nprint(\"Lambda Plot of the first 20 axes:\")\nplt.show()","3da26006":"print(\"The numeric Lambda values(Variances) for the Eigenfaces are:\\n\")\ni = 0\nfor k in pca.explained_variance_:\n    print(\"Komponente {} : Varianz {}\".format(i,k))\n    i = i+1","1c1a14c5":"evr =  pca.explained_variance_ratio_\ngesevr = round(sum(list(pca.explained_variance_ratio_))*100,2)\nprint('Varianz der einzelnen Komponenten\\n', evr,\"\\n\")\nprint('Gesamtvarianz:', gesevr,\"\\n\")\nprint(\"Plot for the portion of explained variance of the main components:\")\nplt.plot(evr,'-x')\n#plt.plot(gesevr)\n","443dec30":"from sklearn.decomposition import KernelPCA\n\nX = faces\nn = [5,10,20,50,100,200,300,400]\n\ndef _plot_faces(face):\n    if face.shape != (64,64):\n        face = face.reshape(64,64)\n        plt.imshow(face,cmap = 'bone')\n        plt.axis('off')\n        \nX_plot = []\n\nfor jj in n:\n    pca_n = PCA(n_components = jj)\n    X2D = pca_n.fit_transform(X)\n    X_recovered = pca_n.inverse_transform(X2D)\n    for kk in range(5):\n        X_plot.append(X_recovered[kk])\n        \nfig, ax = plt.subplots(8,5, figsize = (10,10))\nfor jj in range (0,8):\n    for kk in range(0,5):\n        plt.subplot(8,5,1+5*jj+kk)\n        _plot_faces(X_plot[5*jj+kk])\n\nprint(\"Reconstruction plot:\\n\")\nplt.show()","715cdbc7":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, X_test, y_train, y_test = train_test_split(faces, labels, test_size=0.2, stratify=labels, random_state=42)","79d013df":"rnd_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)\nrnd_clf.fit(X_train,y_train)\n\n#Class Affiliation propability - cap\n#Class Forecast - cf\ncap= rnd_clf.predict_proba(X_train)\ncf = rnd_clf.predict(X_train)\nprint(\"Class Affiliation propability:\\n\")\nprint(cap)\n\nprint(\"\\n Class Forecast:\\n\")\nprint(cf)\n\ngini = rnd_clf.feature_importances_.reshape(64,64)\nprint(\"\\n Plot Gini:\")\nplt.imshow(gini)\nplt.show()\nprint(\"Plot class affiliation probability:\")\nplt.imshow(cap)\nplt.show()","ef18234f":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR,SVC\nimport timeit\nfrom sklearn.ensemble import VotingClassifier","8ac5de32":"pipe_svc = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"SVC\", SVC(probability = True))\n])\n\nparams = [{\n    \"SVC__C\" : np.linspace(1e-6,0.0006, 5),\n    \"SVC__gamma\" : np.linspace(0.14,0.19,10),\n    \"SVC__kernel\" : [\"poly\",\"rbf\",\"sigmoid\"]\n}]\n\nbest_params = [{\n    \"SVC__C\" : [1e-6],\n    \"SVC__gamma\" : [0.14],\n    \"SVC__kernel\" : [\"rbf\"]\n    \n}]\n\nSVC_mod = GridSearchCV(pipe_svc, best_params, n_jobs =-1,cv = 4)\n\ngo = timeit.default_timer()\nSVC_mod.fit(X_train,y_train)\nfin = timeit.default_timer()\n\nM1_score = SVC_mod.best_score_\nM1_params = SVC_mod.best_params_\nprint(M1_score,M1_params)\nprint(\"Model 1 took {:.2f}seconds to fit\".format(fin-go))","ae5cdc4f":"M1y_pred_train = SVC_mod.predict(X_train)\nM1y_pred_test = SVC_mod.predict(X_test)\n\nM1a_train = accuracy_score(y_train,M1y_pred_train)\nM1a_test = accuracy_score(y_test,M1y_pred_test)\n\nprint(\"The Accuracy score on the training data of Model 1 is : {}\".format(M1a_train))\nprint(\"The Accuracy score on the test data of Model 1 is : {}\".format(M1a_test))","4b1af962":"pipe_rndfor = Pipeline([\n    (\"rand_forrest\", RandomForestClassifier(random_state=42))\n])\n\nrand_params = [{\n    \"rand_forrest__n_estimators\": [1500]\n}]\n\nrand_best_params = [{\n    \"rand_forrest__n_estimators\" : [1500]\n}]\n\nrand_mod = GridSearchCV(pipe_rndfor,rand_params, n_jobs=-1, cv=4)\n\ngo = timeit.default_timer()\nrand_mod.fit(X_train,y_train)\nfin = timeit.default_timer()\n\nM2_score = rand_mod.best_score_\nM2_params = rand_mod.best_params_\n\nprint(M2_score,M2_params)\nprint(\"Model 2 took {:.2f}seconds to fit\".format(fin-go))","81ca3c3f":"M2y_pred_train = rand_mod.predict(X_train)\nM2y_pred_test = rand_mod.predict(X_test)\n\nM2a_train = accuracy_score(y_train,M2y_pred_train)\nM2a_test = accuracy_score(y_test,M2y_pred_test)\n\nprint(\"The Accuracy score on the training data in Model 2 is : {}\".format(M2a_train))\nprint(\"The Accuracy score on the test data in Model 2 is : {}\".format(M2a_test))\n","f4abde19":"votes = VotingClassifier(estimators=[(\"SVC\",SVC_mod),(\"RFC\",rand_mod)], voting=\"soft\")\ngo = timeit.default_timer()\nvotes.fit(X_train,y_train)\nfin = timeit.default_timer()\nprint(\"Model 3 took{: 2f}seconds to fit\".format(fin-go))","c1377b4f":"votesy_pred_train = votes.predict(X_train)\nvotesy_pred_test = votes.predict(X_test)\n\nM3a_train = accuracy_score(y_train,votesy_pred_train)\nM3a_test = accuracy_score(y_test,votesy_pred_test)\n\nprint(\"Voting Classifier:\")\nprint(\"The Accuracy score on the test data in Model 2 is : {}\".format(M3a_train))\nprint((\"The Accuracy score on the test data in Model 2 is : {}\".format(M3a_test)))","89c5cb9f":"id_f = y_test != votesy_pred_test\nf_pred = X_test[id_f]\nf = f_pred.shape[0]\n\nprint(\"There are {} faces that are classified with (false)!\".format(f),\"\\n\")\nprint(\"Right classification: {}:\".format(y_test[id_f]),\"\\n\")\n\nplot_faces(f_pred,cols = 5)\nplt.show()\n\nf_mis = []\nfor a in votesy_pred_test[id_f]:\n    id = 0\n    for b in y_train:\n        if(a==b):\n            f_mis.append(id)\n            break\n        id +=1\n\nprint(\"Here are the wrong labels : {}:\".format(votesy_pred_test[id_f]))\n\nplot_faces(X_train[f_mis],cols = 5)\nplt.show()","c5bd9fda":"prepca = PCA(20,whiten=True)\nfaces_projected = prepca.fit_transform(faces)\n\nX_train, X_test, y_train, y_test = train_test_split(faces_projected, labels, test_size = 0.2, stratify=labels, random_state=42)","0e02f455":"plot_faces(prepca.inverse_transform(X_train[f_mis]),cols=5)\nplt.show()","03a8a076":"Um ein Bild zu plotten k\u00f6nnen wir `imshow` verwenden. Der folgende Code plottet jeweils das erste Gesicht der 40 verschiedenen Personen.","8c6ff07b":"## a) Eigengesichter\n- Plotte die ersten 20 Hauptachsen (Eigenvektoren der Kovarianzmatrix). ","ee942e12":"## d) Gesichtserkennung\n\nErstelle ein Modell zur Gesichtserkennung. Du kannst dazu Methoden deiner Wahl verwenden. Experimentiere mit verschiedenen Modellen (`SVC`, `RandomForestClassifier`, ...). \n- Erstelle auch einen `VotingClassifier` basierend auf verschiedenen Modellen. \n- Probiere auch eine `PCA` als Preprocessingschritt. Was macht der Parameter `whiten` in der PCA?\n- Wie hoch ist der Accuracy Score auf dem Trainings- und Testset? Kannst du einen Score von 1 auf dem Testset erreichen? Falls nicht, welche Personen werden verwechselt? Plotte die Geichter dieser Personen.\n","6892f9e1":"Evaluation of the Gini Feature Importance with imshow","41c362b6":"## b) Inverse Transformation\n\nBerechne eine PCA und plotte die Rekonstruktion von 5 Gesichter basierend auf $5, 10, 20, 50, 100, 200, 300$ und $ 400$ Hauptkomponenten. Dazu kannst du die Methode `inverse_transform` benutzen.","57ff03fe":"Code for PCA evaluation with a list of different components; and the reconstruction Plot:","220717ef":"## c) Feature Importance\n- Berechne die *Gini Feature Importance*. Du kannst `imshow` f\u00fcr die Visualisierung verwenden.","aed48b4c":"2)Model RandomForrestClassifier","bb3e3da8":"- Wie gro\u00df sind die zugeh\u00f6rigen Eigenwerte (Varianzen) dieser Eigengesichter?","5937e7fb":"Code for the numeric Lambda Values:","e7692b93":"3)Model Voting Classifier","b65cea1b":"Model1: Support Vector Classifier","ec9d295f":"- Plotte den Anteil der erkl\u00e4rten Varianz in Abh\u00e4ngigkeit der verwendeten Hauptkomponenten. Dazu kannst du das Attribut `explained_variance_ratio_` verwenden.","dfe5d13f":"Code for the lambda Plot: ","15247fc8":"Finale Version.","e836fbc8":"Code for the portion of explained variance of the main components ","fdf6d2e5":"Preprocessing\n","ac0faa42":"# Dimensionsreduktion und Ensemble-Methoden\n\nIn dieser \u00dcbung werden wir uns der wichtigen Hauptkomponentenanalyse (*Principal Component Analysis* PCA) widmen und verschiedene Ensemble-Methoden verwenden, um ein Modell zur Gesichtserkennung zu entwickeln.\nDazu verwenden wir das *Olivetti-Faces* Datenset, welches aus 400 verschiedenen Bildern von 40 verschiedenen Personen besteht. Jedes Bild hat $64 \\times 64$ Pixel."}}