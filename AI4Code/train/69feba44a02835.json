{"cell_type":{"1179bb07":"code","ab538f15":"code","e5bd2ba7":"code","208fcac3":"code","9a67a80b":"code","cad56a93":"code","508cdd27":"code","8fe5c520":"code","3d1e1464":"code","86e3ce60":"code","2fa554f3":"code","a54b849c":"code","235517e9":"code","29460723":"code","341b7899":"code","e88214f2":"code","fa7641fa":"code","4f5bbd9b":"code","cf27c0d1":"code","b6890361":"code","19676e3e":"code","7cd08551":"code","d07eae1a":"code","93f6bea6":"code","a44f3e49":"code","18f848e0":"code","9f7ed824":"code","3c98db47":"code","e35f0b0c":"code","4bca807a":"code","f42dac9e":"code","fb7c627d":"code","5414b01f":"code","d4e47f42":"markdown","f54393fe":"markdown","b0eafad0":"markdown","a61c699e":"markdown","16a382fe":"markdown","24bd6838":"markdown","70cd75d2":"markdown","a938d067":"markdown","688a8c35":"markdown","75da7e1a":"markdown","8e119cc2":"markdown","6befb8fc":"markdown","4af1323a":"markdown"},"source":{"1179bb07":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\nimport warnings","ab538f15":"warnings.filterwarnings('ignore')","e5bd2ba7":"df_genre = pd.read_csv(\"..\/input\/dataset-of-songs-in-spotify\/genres_v2.csv\")\ndf_playlist = pd.read_csv(\"..\/input\/dataset-of-songs-in-spotify\/playlists.csv\")","208fcac3":"df_genre.columns","9a67a80b":"df_genre.shape","cad56a93":"px.imshow(img=df_genre.isna(), title='Missing values(yellow: missing, blue: not missing)')","508cdd27":"df_genre['song_name'].head(10), df_genre['Unnamed: 0'].tail(10), df_genre['title'].tail(10)","8fe5c520":"df_pcmiss = df_genre.isna().sum(axis=0) \/ df_genre.shape[0] * 100\ndf_pcmiss = df_pcmiss.reset_index().rename(columns={'index': 'feature', 0: '% missing'})\npx.bar(df_pcmiss, x='feature', y='% missing', title='% of missing values for each feature')","3d1e1464":"dist_columns = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'type', 'duration_ms', 'time_signature', 'genre']","86e3ce60":"len(dist_columns)","2fa554f3":"rows = 3\ncols = 5\n\nfig = make_subplots(rows=rows, cols=cols, subplot_titles=dist_columns)\n\nx, y = np.meshgrid(np.arange(rows) + 1, np.arange(cols) + 1)\n\ncount = 0\nfor row, col in zip(x.T.reshape(-1), y.T.reshape(-1)):\n    fig.add_trace(\n            go.Histogram(x=df_genre[dist_columns[count]].values),\n            row=row, col=col\n        )\n    count += 1\n\nfig.update_layout(height=900, width=900, title_text=\"Feature distribution\", showlegend=False)\nfig.show()","a54b849c":"box_columns = ['danceability', 'energy', 'key', 'loudness', \n               'speechiness', 'acousticness', 'instrumentalness',\n               'liveness', 'valence', 'tempo', 'duration_ms']","235517e9":"len(box_columns)","29460723":"rows = 3\ncols = 4\n\nfig = make_subplots(rows=rows, cols=cols, subplot_titles=box_columns)\n\nx, y = np.meshgrid(np.arange(rows) + 1, np.arange(cols) + 1)\n\ncount = 0\nfor row, col in zip(x.T.reshape(-1), y.T.reshape(-1)):\n    try:\n        fig.add_trace(\n            go.Box(x=df_genre[box_columns[count]].values, name=''),\n            row=row, col=col\n        )\n        count += 1\n    #if we run out of features, stop plotting\n    except:\n        break\n\nfig.update_layout(height=900, width=900, title_text=\"Boxplots\", showlegend=False)\nfig.show()","341b7899":"features = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n        'duration_ms', 'time_signature']\nlabel    = 'genre'","e88214f2":"X = df_genre[features]\ny = df_genre[label]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n# X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.1)","fa7641fa":"print(X_train.shape)\nprint(X_test.shape)\n# print(X_val.shape)","4f5bbd9b":"loudness_scaler = MinMaxScaler(feature_range=(0, 1))\nX_train.loc[:, 'loudness'] = loudness_scaler.fit_transform(X_train['loudness'].values.reshape(-1, 1))\nX_test.loc[:, 'loudness'] = loudness_scaler.transform(X_test['loudness'].values.reshape(-1, 1))\n# X_val.loc[:, 'loudness'] = loudness_scaler.transform(X_val['loudness'].values.reshape(-1, 1))","cf27c0d1":"tempo_scaler = MinMaxScaler(feature_range=(0, 1))\nX_train.loc[:, 'tempo'] = tempo_scaler.fit_transform(X_train['tempo'].values.reshape(-1, 1))\nX_test.loc[:, 'tempo'] = tempo_scaler.transform(X_test['tempo'].values.reshape(-1, 1))\n# X_val.loc[:, 'tempo'] = tempo_scaler.transform(X_val['tempo'].values.reshape(-1, 1))","b6890361":"duration_ms_scaler = MinMaxScaler(feature_range=(0, 1))\nX_train.loc[:, 'duration_ms'] = duration_ms_scaler.fit_transform(X_train['duration_ms'].values.reshape(-1, 1))\nX_test.loc[:, 'duration_ms'] = duration_ms_scaler.transform(X_test['duration_ms'].values.reshape(-1, 1))\n# X_val.loc[:, 'duration_ms'] = duration_ms_scaler.transform(X_val['duration_ms'].values.reshape(-1, 1))","19676e3e":"X_train = pd.concat([X_train, pd.get_dummies(X_train['key'], prefix='key', drop_first=True)], axis=1)\n\nX_test = pd.concat([X_test, pd.get_dummies(X_test['key'], prefix='key', drop_first=True)], axis=1)\n\n# X_val = pd.concat([X_val, pd.get_dummies(X_val['key'], prefix='key', drop_first=True)], axis=1)","7cd08551":"X_train = pd.concat([X_train, pd.get_dummies(X_train['key'], prefix='key', drop_first=True)], axis=1)\n\nX_test = pd.concat([X_test, pd.get_dummies(X_test['key'], prefix='key', drop_first=True)], axis=1)\n\n# X_val = pd.concat([X_val, pd.get_dummies(X_val['key'], prefix='key', drop_first=True)], axis=1)","d07eae1a":"X_train = pd.concat([X_train, pd.get_dummies(X_train['time_signature'], prefix='time_signature', drop_first=True)], axis=1)\n\nX_test = pd.concat([X_test, pd.get_dummies(X_test['time_signature'], prefix='time_signature', drop_first=True)], axis=1)\n\n# X_val = pd.concat([X_val, pd.get_dummies(X_val['time_signature'], prefix='time_signature', drop_first=True)], axis=1)","93f6bea6":"X_train.drop(['key', 'time_signature'], axis=1, inplace=True)\nX_test.drop(['key', 'time_signature'], axis=1, inplace=True)\n# X_val.drop(['key', 'time_signature'], axis=1, inplace=True)","a44f3e49":"ohe_label = OneHotEncoder()\ny_train = ohe_label.fit_transform(y_train.values.reshape(-1, 1)).toarray()\ny_test = ohe_label.transform(y_test.values.reshape(-1, 1)).toarray()\n# y_val = ohe_label.transform(y_val.values.reshape(-1, 1)).toarray()","18f848e0":"params_dict = {\n    'n_estimators': [50, 75, 100, 125, 150],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': np.arange(8, 40, 4)\n}","9f7ed824":"gs_cv = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1),\n                     param_grid=params_dict,\n                     cv=5,\n                     verbose=10,\n                     n_jobs=-1,\n                    )","3c98db47":"gs_cv.fit(X_train, y_train)","e35f0b0c":"import pickle","4bca807a":"with open('grid_search_result.pkl', 'wb') as f:\n    pickle.dump(gs_cv, f)","f42dac9e":"test_preds = gs_cv.best_estimator_.predict(X_test)","fb7c627d":"meaningfull_preds_test = ohe_label.inverse_transform(test_preds).reshape(-1)\nmeaningfull_true_test  = ohe_label.inverse_transform(y_test).reshape(-1)","5414b01f":"plt.rcParams['figure.figsize'] = 14, 14\nsns.heatmap(confusion_matrix(meaningfull_true_test, meaningfull_preds_test), \n                            annot=True,\n                            xticklabels=ohe_label.categories_[0],\n                            yticklabels=ohe_label.categories_[0],\n                            fmt='d'\n           );","d4e47f42":"## Observation:\n\nApart from instrumentalness, valence and key, all other features have a lot of outliers.","f54393fe":"Now, drop `key` and `time_signature`","b0eafad0":"Lets look at the data distribution for these features.","a61c699e":"## Scaling\n\nA lot of features have values between 0 and 1(e.g. instrumentalness) while others have values in 100 thousands(duration_ms). We need to scale these features in the range of 0-1. \n\nBut first, we need to divide the dataset into train, test and validation sets.","16a382fe":"Interesting. The song_name column has missing values when `Unnamed: 0` and title columns are both null and vice versa. There's a little line for song_name at 20k but if you zoom in, it's gone. But the last 3 columns are a mystery. Why is there a missing pattern like that?  Let's see their content real quick.","24bd6838":"# EDA","70cd75d2":"# FUTURE WORK\n\n* Optimize the RF model on validation set\n* Try some more ML models\n* Try Deep Learning","a938d067":"## Feature Engineering","688a8c35":"## What % values are missing for each feature?","75da7e1a":"So, `title` looks like it contains the name of playlist, `Unnamed: 0` is just increasing numbers and `song_name` is, well, song's name. We're just not going to take these features into consideration anymore. We'll also drop `id`, `uri`, `track_href` and `analysis_url`.","8e119cc2":"# Model Building: Predicting Genre","6befb8fc":"# Observations:\n\n1. danceability - (almost) has a normal distribution\n2. energy - most of the songs are highly energetic\n3. key - many songs are in the key of 1, for others, key is distributed equally\n4. loudness - also distributed normally\n5. mode - not much interesting\n6. speechiness - follows a chi-square-esque distribution\n7. acousticness - also follows chi-square-esque distribution\n8. instrumentalness - most of the songs are not insrumental, as expected. Very few instrumental songs make it to the top. Most songs need to have vocals to be popular.\n9. liveness - distribution is weird, there's a peak at 0.11.\n10. valence - valence in music descibes the musical positiveness conveyed by the song. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). (Source: https:\/\/towardsdatascience.com\/what-makes-a-song-likeable-dbfdb7abe404#:~:text=Valence%3A%20Describes%20the%20musical%20positiveness,measure%20of%20intensity%20and%20activity). The distribution is linear with downward slope.\n11. tempo - (almost) follows a normal distribution\n12. type - there's just one value to this feature, redundant\n13. duration_ms - most songs are 2:30 min to 4:10. There's also a list of longer songs\n14. time_signature - no song has time_signature = 2. Most common time signature is 4.\n15. genre - most popular genres are Dark Trap and Underground Rap","4af1323a":"## One Hot Encoding"}}