{"cell_type":{"9f0f612a":"code","9966f88e":"code","0b8d2774":"code","2a24e0e1":"code","815711d6":"code","20a6c453":"code","f8bafcdf":"code","135647c4":"code","ecf6dbcd":"code","343ec535":"code","e2859983":"code","3e9386b6":"code","e396179a":"code","b69f30f1":"code","8f4de62d":"markdown","0457ddf5":"markdown","8271906e":"markdown","5ff842cc":"markdown","6e60e5ae":"markdown","c6dd6ca4":"markdown","1b7a7405":"markdown","4be4f932":"markdown","24a0b288":"markdown","d6b4ee03":"markdown","01f805c8":"markdown","dae69a35":"markdown"},"source":{"9f0f612a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Data imports\n# Visualization imports\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = [8, 4]\n# ML Imports\n## for data splitting and get validation\nfrom sklearn.model_selection import KFold, cross_val_score\n## for ensemble Learning Tqs.\nfrom sklearn.ensemble import BaggingClassifier ,VotingClassifier ,GradientBoostingClassifier \nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier,StackingClassifier\n##for Classification\nfrom sklearn.neighbors import KNeighborsClassifier #simple Classifier\nfrom sklearn.naive_bayes import GaussianNB #probability classifier \nfrom sklearn.tree import DecisionTreeClassifier #Graph Based Classifier\nfrom sklearn.linear_model import LogisticRegression #for Stacking part\n## for Pipelines and Feature Engineering\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9966f88e":"def running_models(models,preprocessor,nSplits,X,y):\n    split = KFold(n_splits=nSplits, shuffle=True, random_state=1234)\n    for name,model in models:\n        steps=Pipeline(steps=[('preprocessor', preprocessor),('model', model)])\n        cv_results=cross_val_score(steps,X,y,cv=split,scoring='accuracy',n_jobs=-1)\n        min_score=round(np.min(cv_results),4)\n        max_score=round(np.max(cv_results),4)\n        mean_score=round(np.mean(cv_results),4)\n        std_dev=round(np.std(cv_results),4)\n        print(f\"[{name}] Cross Validation Accuarcy Score: {round(mean_score*100,4)} % +\/- {round(std_dev*100,4)} % (std) min: {round(min_score*100,4)} %,max:{round(max_score*100,4)} %\")\n","0b8d2774":"hotel_bookings=pd.read_csv('\/kaggle\/input\/hotel-booking\/hotel_booking.csv')\nhotel_bookings.head(5)\n","2a24e0e1":"bookings_by_month = hotel_bookings.groupby('arrival_date_month', as_index=False)[['hotel']].count().rename(columns={\"hotel\": \"nb_bookings\"})\nmonths = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'] \nfig = px.bar(\n    bookings_by_month, \n    x='arrival_date_month', \n    y='nb_bookings', \n    title=f'Hotel Bookings by Month', \n    category_orders={\"arrival_date_month\": months}\n)\nfig.show(config={\"displayModeBar\": False})","815711d6":"from IPython.display import Image\nImage(\"http:\/\/scikit-learn.org\/dev\/_static\/ml_map.png\", width=750)","20a6c453":"models = [\n  (\"Decision Tree\", DecisionTreeClassifier(random_state=1234)),\n  (\"Random Forest\", RandomForestClassifier(random_state=1234,n_jobs=-1)),]","f8bafcdf":"# Preprocess numerical features:\nfeatures_num = [\n    \"lead_time\", \"arrival_date_week_number\", \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\", \"is_repeated_guest\" ,\n    \"previous_cancellations\", \"previous_bookings_not_canceled\", \"agent\", \"company\", \n    \"required_car_parking_spaces\", \"total_of_special_requests\", \"adr\"\n]\ntransformer_num = SimpleImputer(strategy=\"constant\")\n\n# Preprocess categorical features:\nfeatures_cat = [\n    \"hotel\", \"arrival_date_month\", \"meal\", \"market_segment\", \"distribution_channel\", \n    \"reserved_room_type\", \"deposit_type\", \"customer_type\"\n]\ntransformer_cat = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n    (\"onehot\", OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Create a preprocessing pipeline\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", transformer_num, features_num),\n    (\"cat\", transformer_cat, features_cat)\n])","135647c4":"features = features_num + features_cat\nX = hotel_bookings[features]\ny = hotel_bookings[\"is_canceled\"]","ecf6dbcd":"running_models(models,preprocessor,3,X,y)","343ec535":"clf_knn= KNeighborsClassifier(n_neighbors=5,algorithm='ball_tree')\nclf_dt= DecisionTreeClassifier(random_state=42)\nclf_nv= GaussianNB()\n\npre_models=[('knn',clf_knn),('DT',clf_dt),('NV',clf_nv),]\nrunning_models(pre_models,preprocessor,3,X,y)","e2859983":"voting_hard = VotingClassifier(estimators=pre_models,voting='hard')\nvoting_soft = VotingClassifier(estimators=pre_models, voting='soft')\nvotingModels=[('hard_voting',voting_hard),('soft_voting',voting_soft),]\nrunning_models(votingModels,preprocessor,3,X,y)","3e9386b6":"knn_bag=BaggingClassifier(clf_knn,max_samples=0.5, max_features=0.5)\ndt_bag =BaggingClassifier(clf_dt,max_samples=0.5,max_features=0.5)\nnv_bag =BaggingClassifier(clf_nv,max_samples=0.5,max_features=0.5)\nbaggingModels=[('KNN bagging',knn_bag),('DT Bagging',dt_bag),('NV Bagging',nv_bag),]\nrunning_models(baggingModels,preprocessor,3,X,y)\n","e396179a":"smallAdaBoost=AdaBoostClassifier(n_estimators=100)\nbigAdaBoost=AdaBoostClassifier(n_estimators=1000)\nsmallGB=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\nbigGB=GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1,max_depth=1, random_state=0)\nboostingModels=[('100 estimators Adaboost',smallAdaBoost),\n                ('1000 estimators Adaboost',bigAdaBoost),\n                ('100 estimators XGB',smallGB),\n                ('1000 estimators XGB',bigGB),]\n\nrunning_models(boostingModels,preprocessor,3,X,y)\n","b69f30f1":"final_estimator= LogisticRegression(solver='liblinear', random_state=0)\npre_model_stacking=StackingClassifier(estimators=pre_models,final_estimator=final_estimator)\nbagging_model_stacking=StackingClassifier(estimators=baggingModels,final_estimator=final_estimator)\nboosting_model_stacking=StackingClassifier(estimators=boostingModels,final_estimator=final_estimator)\nstackingModels=[('pre model stacking',pre_model_stacking),\n               ('bagging model stacking',bagging_model_stacking),\n               ('boosting model stacking',boosting_model_stacking),]\nrunning_models(stackingModels,preprocessor,3,X,y)","8f4de62d":"<h3>Voting Classifier<\/h3>","0457ddf5":"<H1>Ensemple Learning<\/H1>","8271906e":"<H3>Preparing For Ensemple Learning <\/H3>","5ff842cc":"# Predicting Cancellation Rates\n\nIn this notebook, you will build a machine learning model to predict whether or not a customer cancelled a hotel booking. You will be introduced to the `scikit-learn` framework to do machine learning in Python. \n\nWe will use a dataset on hotel bookings from the article [\"Hotel booking demand datasets\"](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340918315191), published in the Elsevier journal, [Data in Brief](https:\/\/www.sciencedirect.com\/journal\/data-in-brief). The abstract of the article states \n\n> This data article describes two datasets with hotel demand data. One of the hotels (H1) is a resort hotel and the other is a city hotel (H2). Both datasets share the same structure, with 31 variables describing the 40,060 observations of H1 and 79,330 observations of H2. Each observation represents a hotel booking. Both datasets comprehend bookings due to arrive between the 1st of July of 2015 and the 31st of August 2017, including bookings that effectively arrived and bookings that were canceled. \n\nFor convenience, the two datasets have been combined into a single csv file `data\/hotel_bookings.csv`. Let us start by importing all the functions needed to import, visualize and model the data.","6e60e5ae":"## 3. Preprocess the data\n\nThe next step is to set up a pipeline to preprocess the features. We will impute all missing values with a constant, and one-hot encode all categorical features.","c6dd6ca4":"<h3>Stacking<\/h3>","1b7a7405":"## 0. Get the data\n\nThe first step in any machine learning workflow is to get the data and explore it.","4be4f932":"## 2. Choose a class of models, and hyperparameters.\n\nThe next step is to choose a class of models and specify hyperparameters. This is just for starters and we will see later how we can specify a range of values for hyperparameters and tune the model for optimal performance! We will pick the simple, yet very effective Decision Tree and Random Forest models.\nWe will use `scikit-learn` to fit the models and evaluate their performance.","24a0b288":"<h3>Boosting<\/h3>","d6b4ee03":"## 4. Fit the models and evaluate performance\n\nFinally, we will fit the Decision Tree and Random Forest models on the training data and use 4-fold cross-validation to evaluate their performance.","01f805c8":"Let us look at the number of bookings by month.","dae69a35":"<h3>Bagging Classifier<\/h3>"}}