{"cell_type":{"c404e940":"code","597fb3ea":"code","2cfa7651":"code","9d80d60e":"code","8e1b9bc8":"code","e61b8343":"code","8fea8b6f":"code","f9bae91d":"code","b1b280a3":"code","3d0ecff7":"code","1679ec25":"code","edd5eb24":"code","b4093137":"code","4114b69b":"code","f1bb5102":"code","d6c342f3":"code","33e7b8bd":"code","481e1113":"code","a6ac0219":"code","b9fb37fc":"code","1b243aca":"code","d4b2e5cf":"code","9e19d62d":"code","8ba0cb03":"code","f51a121b":"code","cf230751":"code","0dfd2f15":"code","25493676":"code","492f9e2e":"code","e17fccf4":"code","44504a00":"code","375d47c8":"code","d3c5aa53":"code","59eeaea8":"code","432159b6":"code","52b91fb4":"code","d1d47071":"code","a0600f99":"code","70f69025":"code","e5cbd958":"code","5733c30a":"code","939147b3":"code","9eda28aa":"code","63add975":"code","0ecb3230":"code","7250dd21":"code","85450719":"code","d1ef8d03":"code","48c9dbc4":"code","9cd61cb5":"code","16a8ef67":"code","c2a8780a":"code","88b6b668":"code","4d3dbeb9":"code","521cd75b":"code","d3f6c49b":"code","2d60a4f3":"code","923e1f04":"markdown","3e810a57":"markdown","708ddae9":"markdown","be575d35":"markdown","09de405a":"markdown","d9f1b4bb":"markdown","3095b3d5":"markdown","0e8327db":"markdown","9b15b94b":"markdown","4e7145e2":"markdown","1d4826cb":"markdown","d99c068d":"markdown","01b07658":"markdown","e83470b7":"markdown","38994be0":"markdown","adb4aeed":"markdown","507ec5cb":"markdown","5391281b":"markdown","905cb1ef":"markdown","f24739ce":"markdown","1fc38abd":"markdown","344101f3":"markdown","3aa30a32":"markdown","9261af94":"markdown"},"source":{"c404e940":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","597fb3ea":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","2cfa7651":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","9d80d60e":"data.describe()      #description of dataset ","8e1b9bc8":"data.info()","e61b8343":"data.shape       #569 rows and 33 columns","8fea8b6f":"data.columns     #displaying the columns of dataset","f9bae91d":"data.value_counts","b1b280a3":"data.dtypes","3d0ecff7":"data.isnull().sum()","1679ec25":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","edd5eb24":"data","b4093137":"data.corr()","4114b69b":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","f1bb5102":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","d6c342f3":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","33e7b8bd":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","481e1113":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","a6ac0219":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","b9fb37fc":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","1b243aca":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","d4b2e5cf":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","9e19d62d":"print(len(x_train))\n","8ba0cb03":"print(len(x_test))","f51a121b":"print(len(y_train))","cf230751":"print(len(y_test))","0dfd2f15":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","25493676":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","492f9e2e":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","e17fccf4":"print(accuracy_score(y_test,y_pred)*100)","44504a00":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","375d47c8":"print(\"Best CV score\", cv.best_score_*100)","d3c5aa53":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","59eeaea8":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","432159b6":"print(accuracy_score(y_test,y_pred)*100)","52b91fb4":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","d1d47071":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","a0600f99":"print(accuracy_score(y_test,y_pred)*100)","70f69025":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","e5cbd958":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","5733c30a":"print(accuracy_score(y_test,y_pred)*100)\n","939147b3":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","9eda28aa":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","63add975":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","0ecb3230":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","7250dd21":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","85450719":"print(accuracy_score(y_test,y_pred)*100)","d1ef8d03":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","48c9dbc4":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","9cd61cb5":"print(accuracy_score(y_test,y_pred)*100)","16a8ef67":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","c2a8780a":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","88b6b668":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","4d3dbeb9":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","521cd75b":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","d3f6c49b":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","2d60a4f3":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","923e1f04":"# 3. Random Forest Classifier","3e810a57":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","708ddae9":"# LOADING THE DATASET","be575d35":"# 4. KNeighborsClassifier\n\n","09de405a":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","d9f1b4bb":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**","3095b3d5":"#  7. Gradient Boosting Classifier","0e8327db":"**So we get a accuracy score of 58.7 % using logistic regression**","9b15b94b":"# 9. Naive Bayes","4e7145e2":"# IMPORTING THE LIBRARIES","1d4826cb":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","d99c068d":"# VISUALIZING THE DATA","01b07658":"**So we get a accuracy score of 63.7 % using SVC**","e83470b7":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","38994be0":"# 8. XGBClassifier","adb4aeed":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","507ec5cb":"# TRAINING AND TESTING DATA","5391281b":"# 6. AdaBoostClassifier","905cb1ef":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","f24739ce":"# 1. Logistic Regression","1fc38abd":"# 2. DECISION TREE CLASSIFIER","344101f3":"# 5. SVC","3aa30a32":"# MODELS","9261af94":"**So we get a accuracy score of 63.29 % using Naive Bayes**"}}