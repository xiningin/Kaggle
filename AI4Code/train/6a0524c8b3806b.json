{"cell_type":{"86474ec3":"code","dec46575":"code","6883ea60":"code","fb3fb69b":"code","727a1d9d":"code","b46c797a":"code","788d6853":"code","8eb6da61":"code","b3d79de7":"code","016acf3d":"code","90586a68":"code","03902f3b":"code","1bb051fb":"code","17b3b91d":"code","fdbc2d3b":"code","961b9a85":"code","a936f35e":"code","304db10c":"code","59f857a0":"code","31d7edea":"code","c2fc7488":"code","acfcf456":"code","f3c91bcb":"code","7725d89f":"code","bab4d41c":"code","0a91047f":"code","9f8df443":"code","7e96d4bc":"markdown","61f93c85":"markdown","1f4fea9c":"markdown","923dbca2":"markdown","d653b8bf":"markdown","b4c070ef":"markdown","74f85803":"markdown","6c1cab14":"markdown","b436dad1":"markdown","d8e62afd":"markdown","440b1003":"markdown","ce9c3122":"markdown","5c737dfe":"markdown","6872fa12":"markdown","db6fc8f6":"markdown","32233df5":"markdown","56ee9a08":"markdown"},"source":{"86474ec3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/A_DeviceMotion_data\/A_DeviceMotion_data\"))\n\n# Any results you write to the current directory are saved as output.","dec46575":"# Data Folders:\nFolders = glob('..\/input\/A_DeviceMotion_data\/A_DeviceMotion_data\/*_*')\nFolders = [s for s in Folders if \"csv\" not in s]\n\nDf_all_list = []\nExp = 0\n# Segment the data to 400 sampels frames , each one will be a different Expirament\nSegment_Size = 400\n\n# Activety types dict:\nactivity_codes = {'dws':1,'jog':2,'sit':3,'std':4,'ups':5,'wlk':6}        \nactivity_types = list(activity_codes.keys())\n\n# Load All data:\nfor j  in Folders:\n    Csv = glob(j + '\/*' )\n\n\n    for i in Csv:\n        df = pd.read_csv(i)\n        # Add Activety label, Subject name and Experiment number\n        df['Activity'] = activity_codes[j[49:52]]\n        df['Sub_Num'] = i[len(j)+5:-4]\n        df['Exp_num'] = 1\n        ExpNum = np.zeros((df.shape[0])) \n        for i in range(0,df.shape[0]-Segment_Size,Segment_Size):\n            ExpNum[range(i,i+Segment_Size)] = i\/Segment_Size +Exp*100 \n        df['Exp_num'] = ExpNum\n        #Df_all = pd.concat([Df_all,df])\n        Df_all_list.append(df)\n        Exp += 1        \n\nDf_all = pd.concat(Df_all_list,axis=0)  \nprint(Df_all.shape)\nprint(Df_all.columns)","6883ea60":"np.unique(Df_all['Sub_Num'])","fb3fb69b":"### Missing values\nchecks = pd.isna(Df_all).sum()\nprint(checks)\n### class balance\nclass_counts = list()\n\nfor act in activity_types[:1]:\n    class_counts.append(Df_all[Df_all['Activity']==activity_codes[act]].count())\nplt.figure(1)\nplt.title('Size of each class')\nplt.xlabel('activity type')\nplt.hist(Df_all['Activity'],bins=range(1,8),rwidth=0.5,align='left')\n\n### Length of time series\nseries_length = list()\nfor act in activity_types:\n    for sub in range(1,25):\n        sub = str(sub)\n        series_length.append(Df_all[(Df_all['Sub_Num']==sub) & (Df_all['Activity']==activity_codes[act])].shape[0])\nplt.figure(2)\nplt.title('Histogram of length of raw time series')\nplt.hist(series_length,rwidth=0.5,align='left')\n\n\n### show the first few records of motion type\nplt.figure(3)\ncolors = ['r','g','b','c','m','y','k']\nfor act in activity_types:\n    plt.subplot('61'+str(activity_codes[act]))\n    plt.subplots_adjust(hspace=1.0)\n    df = Df_all[(Df_all['Sub_Num']=='1') & (Df_all['Activity']==activity_codes[act])]\n    plt.title(act)\n    plt.plot(df['userAcceleration.z'][:400])\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels\n\n\n","727a1d9d":"#  Calculate features\ndf_sum = Df_all.groupby('Exp_num', axis=0).mean().reset_index()\ndf_sum.columns = df_sum.columns.str.replace('.','_sum_')\n\ndf_sum_SS = np.power(Df_all.astype(float),2).groupby('Exp_num', axis=0).median().reset_index() \ndf_sum_SS.columns = df_sum_SS.columns.str.replace('.','_sumSS_')\n\ndf_max = Df_all.groupby('Exp_num', axis=0).max().reset_index()\ndf_max.columns = df_max.columns.str.replace('.','_max_')\n\ndf_min = Df_all.groupby('Exp_num', axis=0).min().reset_index()\ndf_min.columns = df_min.columns.str.replace('.','_min_')\n\ndf_skew = Df_all.groupby('Exp_num', axis=0).skew().reset_index()\ndf_skew.columns = df_skew.columns.str.replace('.','_skew_')\n\ndf_std = Df_all.groupby('Exp_num', axis=0).std().reset_index()\ndf_std.columns = df_std.columns.str.replace('.','_std_')\n\nDf_Features = pd.concat([ df_max , df_sum[df_sum.columns[2:-2]], \n                         df_min[df_min.columns[2:-2]], df_sum_SS[df_sum_SS.columns[2:-2]], \n                         df_std[df_std.columns[2:-2]], df_skew[df_skew.columns[2:-2]]], axis=1)\n\nX = Df_Features.drop(['Exp_num','Unnamed: 0','Activity','Sub_Num'],axis=1)\nY = Df_Features['Activity']\n\nprint('Shape of X:', X.shape)\nprint('Shape of Y:', Y.shape)","b46c797a":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\n#### dimension reduction\n### use pca to reduce the dimension to 2D directly.\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\nplt.figure(1)\ncolors = ['r','g','b','c','m','y','k']\nlw = 2\n\nfor color, i, target_name in zip(colors, range(6), activity_types):\n    plt.scatter(X_r[Y == i, 0], X_r[Y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('Use PCA directly')\n\n### sklearn tsne\n# sites.google.com\/s\/1HAV-HEiBhPHLgdh5Ejmu31TrVIQqw9HU\/p\/1bPpOCDlxW7i5nOpy3bvpnmkqa8Y-SDVa\/edit\n# Scale data\nscl = StandardScaler()\nscaled_X = scl.fit_transform(X)\n\n# Reduce dimensions before feeding into tsne\npca = PCA(n_components=0.9, random_state=3)\npca_transformed = pca.fit_transform(scaled_X)\n\n# Transform data\ntsne = TSNE(random_state=3)\ntsne_transformed = tsne.fit_transform(pca_transformed)\n\nplt.figure(2)\nfor color, i, target_name in zip(colors, range(6), activity_types):\n    plt.scatter(X_r[Y == i, 0], X_r[Y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('TSNE')\n","788d6853":"series_list = list()\nlabels_list = list()\nfor act in activity_types:\n\n    df = Df_all[(Df_all['Sub_Num']=='1') & (Df_all['Activity']==activity_codes[act])]\n    series_list.append(df['userAcceleration.z'][:400])\n    labels_list.extend([act])\n\n#print(labels_list)","8eb6da61":"from dtaidistance import dtw\nimport numpy as np\nfrom dtaidistance import clustering\n\nseries = np.array(series_list)\nds = dtw.distance_matrix_fast(series)\n\nmodel = clustering.LinkageTree(dtw.distance_matrix_fast, {})\nmodel.fit(series)\n\nmodel.plot(show_ts_label=labels_list,\n           show_tr_label=True)\n","b3d79de7":"import itertools\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.datasets import mnist\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.regularizers import l2\nfrom keras.utils import to_categorical\nimport keras\n","016acf3d":"#### Construct neural Architeture for baseline model\ninput_dim = X.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(7, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])","90586a68":"# One-hot encoding\nY = to_categorical(Y)\nprint(Y.shape)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])","03902f3b":"#### Check model performance\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])\n\n","1bb051fb":"# One-hot decoding\ny_pred = np.argmax(model.predict(X_test),axis=1)\ny_test = np.argmax(Y_test,axis=1)\n#print(y_pred,y_test)\n\ncorrect = np.nonzero(y_pred==y_test)[0]\nincorrect = np.nonzero(y_pred!=y_test)[0]\n#print(correct)\n\n### Check the correctly-predicted samples\nplt.figure(1)\nfor i, cor in enumerate(correct[:9]):\n    plt.subplot(3,3,i+1)\n    plt.subplots_adjust(hspace=1.0,wspace=1.0)\n    plt.plot(X_test.iloc[cor,:])\n    plt.title(\"Predicted:{}\\nTrue:{}\".format(activity_types[y_pred[cor]-1], \n                                              activity_types[y_test[cor]-1]))\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels\n    #plt.tight_layout()\nplt.show()\n### Check the incorrectly-predicted samples\nplt.figure(2)\nfor i, cor in enumerate(incorrect[:9]):\n    plt.subplot(3,3,i+1)\n    plt.subplots_adjust(hspace=1.0,wspace=1.0)\n    plt.plot(X_test.iloc[cor,:])\n    plt.title(\"Predicted:{}\\nTrue:{}\".format(activity_types[y_pred[cor]-1], \n                                              activity_types[y_test[cor]-1]))\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels\n    #plt.tight_layout()\nplt.show()","17b3b91d":"### Confusion matrix (predictive performance on different classes)\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    function provided by sklearn example\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cnf_matrix, classes=activity_types,\n                      title='Confusion matrix, without normalization')","fdbc2d3b":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf1 = RandomForestClassifier(n_estimators=100, max_depth=None,\n     min_samples_split=2, random_state=0)\nclf1.fit(X_train, Y_train)\nfeatureImportance = clf1.feature_importances_\n\n# normalize by max importance\nfeatureImportance = featureImportance \/ featureImportance.max()\nfeature_names = X.columns\nidxSorted = np.argsort(featureImportance)[-10:]\nbarPos = np.arange(idxSorted.shape[0]) + .5\nplt.barh(barPos, featureImportance[idxSorted], align='center')\nplt.yticks(barPos, feature_names[idxSorted])\nplt.xlabel('Variable Importance')\nplt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)\nplt.show()","961b9a85":"# Need to one-hot decode before feding into GBM\ny_train = np.argmax(Y_train,axis=1)\nclf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=1, random_state=0)\nclf2.fit(X_train, y_train)\nfeatureImportance = clf2.feature_importances_\n\n\n# normalize by max importance\nfeatureImportance = featureImportance \/ featureImportance.max()\nfeature_names = X.columns\nidxSorted = np.argsort(featureImportance)[-10:]\nbarPos = np.arange(idxSorted.shape[0]) + .5\nplt.barh(barPos, featureImportance[idxSorted], align='center')\nplt.yticks(barPos, feature_names[idxSorted])\nplt.xlabel('Variable Importance')\nplt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)\nplt.show()\n","a936f35e":"### One-hot encoding for tree classifier\nprint(Y_train.shape)","304db10c":"# Let target label become subjects\nY1 = Df_Features['Sub_Num'].iloc[:,0]","59f857a0":"#### Construct neural Architeture for baseline model\ninput_dim = X.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(25, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])","31d7edea":"# One-hot encoding\n\ny1 = to_categorical(Y1)\nprint(Y1.shape)\nX_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.3, random_state=0)\n\nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])\n\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])\n","c2fc7488":"for i in range(1,7):\n    df = Df_Features[Df_Features['Activity']==i]\n    x = df.drop(['Exp_num','Unnamed: 0','Activity','Sub_Num'],axis=1)\n    y = df['Sub_Num'].iloc[:,0]\n    y = to_categorical(y)\n    \n    #### Construct neural Architeture\n    input_dim = x.shape[1]\n    input_img = Input(shape=(input_dim,))\n    d = Dense(50, activation='relu')(input_img)\n    d = Dense(20, activation='relu')(d)\n    output = Dense(25, activation='softmax', kernel_regularizer=l2(0.01))(d)\n    model = Model(input_img,output)\n    model.compile(optimizer='adam', loss='binary_crossentropy',\n                     metrics=['categorical_accuracy'])\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n\n    history = model.fit(X_train, Y_train,\n                    epochs=1000,\n                    batch_size=100,\n                    shuffle=True,\n                    verbose=0,\n                    validation_data=[X_test,Y_test])\n\n    ### Overall test accuracy\n    score = model.evaluate(X_test, Y_test)\n    print('Activity type:', activity_types[i-1])\n    print('keras test accuracy score:', score[1])","acfcf456":"mapping = {\n    '1': 1,\n    '2': 1,\n    '3': 0,\n    '4': 1,\n    '5': 0,\n    '6':1,\n    '7':0,\n    '8':0,\n    '9':1,\n    '10':0,\n    '11':1,\n    '12':1,\n    '13':1,\n    '14':1,\n    '15':1,\n    '16':0,\n    '17':1,\n    '18':0,\n    '19':0,\n    '20':1,\n    '21':1,\n    '22':1,\n    '23':0,\n    '24':0\n\n}","f3c91bcb":"# Create gender labels\nY2 = [mapping[i] for i in Df_Features['Sub_Num'].iloc[:,0]]","7725d89f":"y2 = to_categorical(Y2)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y2, test_size=0.3, random_state=0)\nprint('The number of training samples:',X_train.shape[0])\nprint('The number of test samples:',X_test.shape[0])\n\n\n#### Construct neural Architeture for baseline model\ninput_dim = x.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(2, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])\n    \nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])\n\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])","bab4d41c":"mapping = {\n    '1': 1,\n    '2': 0,\n    '3': 0,\n    '4': 1,\n    '5': 0,\n    '6':1,\n    '7':0,\n    '8':0,\n    '9':1,\n    '10':0,\n    '11':0,\n    '12':0,\n    '13':0,\n    '14':0,\n    '15':0,\n    '16':1,\n    '17':1,\n    '18':0,\n    '19':1,\n    '20':1,\n    '21':0,\n    '22':1,\n    '23':0,\n    '24':1\n\n}","0a91047f":"# Create gender labels\nY3 = [mapping[i] for i in Df_Features['Sub_Num'].iloc[:,0]]","9f8df443":"y3 = to_categorical(Y3)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y3, test_size=0.3, random_state=0)\nprint('The number of training samples:',X_train.shape[0])\nprint('The number of test samples:',X_test.shape[0])\n\n\n#### Construct neural Architeture for baseline model\ninput_dim = x.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(2, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])\n    \nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])\n\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])","7e96d4bc":"**Load the dataset**\n<br> I follow the [RoyT's kernel](https:\/\/www.kaggle.com\/talmanr\/a-simple-features-dnn-using-tensorflow) to segment 400 datapoints into one experiment, and construct features for each time series. ","61f93c85":"on gender:\n<br> 1 for male or female??","1f4fea9c":"Prediction on subjects","923dbca2":"**Build hierachical clustering w\/ dynamic time warpping**\n<br> I tried to adopted the methods provided by library \"dtaidistance.\"  400 datapoints of each motion type are segmented and similiarity between each segment is calculated by dynamic time warpping.","d653b8bf":"Overfitting??","b4c070ef":"on weight:\n<br> The subjects above the average weight 72.125 are labeled as 1; 0 for below average.","74f85803":"**Data Characterization**\n    * Missing values\n    * class balance \n    * show the first few records of each motion type","6c1cab14":"Check the feature importance","b436dad1":"Train multi-layer perceptron classifier","d8e62afd":"**Check the similarity between motion types**","440b1003":"**Feature Construction**:\n<Br> I follow [RoyT's work](https:\/\/www.kaggle.com\/talmanr\/a-simple-features-dnn-using-tensorflow) to calculate mean, squared_median, max, min, skewness and std for each segment","ce9c3122":"Based on hierachical clustering, 'dws', 'ups', 'wlk' are similar to each other, 'sit' and 'std' are similair, and 'jog' is distinctive from the other five.","5c737dfe":"From above analysis,\n* This dataset raised concern on class imbalance.\n* The distribution of time-series length is broad.","6872fa12":"Table of Content:\n* Data characterization\n    * shape of dataset\n    * Missing values\n    * class balance \n    * show the first few records of each motion type\n* Feature construction\n* Check similiarity between time series of each motion type\n    * 2D visualization with the help dimension reduction: pca, tsne\n    \n    * hierachical clustering with dynamic time warpping\n* Predictve model\n    * on motion type\n        * on overall dataset\n        <br> model performance\n        <br> feature importance\n        <br> check correctly-predicted samples\n        <br> check incorrectly-predicted samples\n        * on dataset of individual motion\n    * on gender\n    * on weight\n\n    reference:\n    * https:\/\/www.kaggle.com\/morrisb\/what-does-your-smartphone-know-about-you","db6fc8f6":"* Predictve model\n    * on motion type\n        * train neural network\n            *  check model performance\n            * check correctly-predicted samples\n            * check incorrectly-predicted samples\n        * train tree-based classifier\n            * feature importance\n\n   * on subject\n       * On overall dataset\n       * On dataset of individual dataset\n   * on gender\n   * on weight","32233df5":"'ups' has the highest misclassification rate, and is clasified as 'dws' and 'wlk'","56ee9a08":"on individual dataset"}}