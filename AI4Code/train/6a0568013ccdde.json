{"cell_type":{"a84cbafd":"code","51792157":"code","4543e98b":"code","8460015a":"code","f3cdaeb9":"code","10658ac8":"code","c0150ce1":"code","20f21927":"code","e9a20b44":"code","826f2ecd":"code","e9aa3474":"code","a111d21e":"code","99f4a688":"code","ffb48fb7":"code","81b2f236":"code","3e79f9f2":"code","a9a2ef9d":"code","7330489e":"code","750161d6":"code","da8ad06c":"markdown","05ac2a20":"markdown","79957af3":"markdown","145f78b4":"markdown","ecb9a945":"markdown","e5260145":"markdown","64236d66":"markdown","bbd3a024":"markdown","ee23fcc5":"markdown","e798236a":"markdown","c79d964a":"markdown","8872d10f":"markdown","f0753be0":"markdown","0b5bed3e":"markdown","314b9195":"markdown","75f971bc":"markdown","b53b94fe":"markdown","269e487e":"markdown","41999873":"markdown","3f639c31":"markdown","c33356a4":"markdown","03be81ed":"markdown","108bd03a":"markdown","41a6faa9":"markdown","6b36fd0c":"markdown","d00d193a":"markdown","9e846428":"markdown","919e9262":"markdown","4a1b04d4":"markdown","6dd2a4ee":"markdown","9551c5cd":"markdown"},"source":{"a84cbafd":"#Importing Requierd Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#plt.style.use('seaborn-pastel')\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nsns.set_palette(\"Greens_d\")\n# for Interactive Shells\nfrom IPython.display import display\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom mpl_toolkits.mplot3d import Axes3D\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom yellowbrick.cluster import KElbowVisualizer\n\n#removing warnings\nimport sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    \n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import KMeans \nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nimport sklearn.cluster as cluster\nimport time\n\n#secrets\nfrom kaggle_secrets import UserSecretsClient\nsecret_label = \"notebook_secret\"\nsecret_value = UserSecretsClient().get_secret(secret_label)","51792157":"#loading data\ntry:\n    df = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv', sep='\\t')\nexcept:\n    df = pd.read_csv('marketing_campaign.csv', sep='\\t')\n    \n#making a function for examining data\ndef data_research(data, data_name='data', un=False):\n    #basic\n    print(f'Examining \"{data_name}\"')\n    display(data.head(2))\n    #display(data.info())\n    display(data.describe( include='all'))\n    #display(data.columns)\n    \n    #duplicates\n    duplicates = data.duplicated().sum()\n    if duplicates > 0:\n        print('There are no duplicated entries.')\n    else:\n        print(f'There are {duplicates} duplicates.')\n        \n    #missing\n    data_missing = pd.DataFrame(round(data.isnull().sum()))\n    if data_missing[0].sum() > 0:\n        data_missing.plot(kind='bar')\n        plt.grid()\n        plt.title('Missing values');\n    else:\n        print(f'There are no missing values in \"{data_name}\".')\n    \n    #unique values\n    if un == True:\n        for i in data.columns:\n            if data[i].dtype == 'object' or data[i].dtype == 'str':\n                print(data[i].unique())\n    \ndata_research(df, data_name='Marketing data')","4543e98b":"#adding one new column \ndf[\"mspent\"] = df[\"MntWines\"] + df[\"MntFruits\"] + df[\"MntMeatProducts\"] + df[\"MntFishProducts\"] + df[\"MntSweetProducts\"] + df[\"MntGoldProds\"]\n\n# Renaming columns\ndf.columns = df.columns.str.lower()\n\nnumeric_columns_all = ['income', 'kidhome',\n                   'teenhome', 'recency', 'mntwines', 'mntfruits',\n                   'mntmeatproducts', 'mntfishproducts', 'mntsweetproducts',\n                   'mntgoldprods', 'numdealspurchases', 'numwebpurchases',\n                   'numcatalogpurchases', 'numstorepurchases', 'numwebvisitsmonth',\n                   'acceptedcmp3', 'acceptedcmp4', 'acceptedcmp5', 'acceptedcmp1',\n                   'acceptedcmp2', 'complain', 'z_costcontact', 'z_revenue', 'response']\n\nnumeric_columns = ['income', 'kidhome',\n                   'teenhome', 'recency', 'mntwines', 'mntfruits',\n                   'mntmeatproducts', 'mntfishproducts', 'mntsweetproducts',\n                   'mntgoldprods', 'numdealspurchases', 'numwebpurchases',\n                   'numcatalogpurchases', 'numstorepurchases', 'numwebvisitsmonth']\n\nbool_columns = ['acceptedcmp3', 'acceptedcmp4', 'acceptedcmp5', 'acceptedcmp1',\n                   'acceptedcmp2', 'complain', 'response']\n\ncategorical_columns = ['education', 'marital_status']\ndata_columns = ['year_birth','dt_customer']\n\n# changing data types and dropping columns\ndf['dt_customer'] = df['dt_customer'].astype(np.datetime64)\ndf['education'],df['marital_status'] = df['education'].astype('category'), df['marital_status'].astype('category')\ndf['year_month'] = df['dt_customer'].dt.strftime('%Y-%m')\ndf['mnt_products'] = df['mntwines'] + df['mntfruits'] + df['mntmeatproducts'] + df['mntfishproducts'] +\\\n                        df['mntsweetproducts'] + df['mntgoldprods']\ndf.drop(['z_costcontact', 'z_revenue'], axis=1, inplace=True)\n\ndisplay(df.info())","8460015a":"# correlation between columns\nfig = plt.figure(figsize=(8,6))\nplt.title('Correlation between values')\nsns.heatmap(df.iloc[:, :9].corr(), annot=True, cmap=\"Greens\");","f3cdaeb9":"#df.groupby('Education')['income'].mean()     # income by education group\n#df.groupby('Kidhome')['income'].mean()     # income by Kidhome group\nprint('Skewness before filling nans', round(df['income'].skew(),3))\n\ndf[\"income\"] = df[\"income\"].fillna(df.groupby(['education','kidhome'])[\"income\"].transform('mean')) #filling nans\n\ndf['income'].plot(kind='hist', bins=30)\nplt.title('Skewness after filling nans')\nplt.grid();\nprint('Skewness after filling nans', round(df['income'].skew(),3))","10658ac8":"# outliers\ndef boxplots_custom(dataset, columns_list, rows, cols, suptitle,size=(20,16)):\n    fig, axs = plt.subplots(rows, cols,  figsize=size)\n    fig.suptitle(suptitle,y=0.93, size=16)\n    axs = axs.flatten()\n    for i, data in enumerate(columns_list):\n        if i % cols == 0:\n            axs[i].set_ylabel('Values')\n        sns.boxplot( data=dataset[data], orient='v', ax=axs[i])\n        axs[i].set_title(data)\n        \nboxplots_custom(dataset=df, columns_list=numeric_columns, rows=3, cols=5, suptitle='Boxplots before deleting outliers')","c0150ce1":"# deleting outliers\nnumeric_columns_to_filter = ['income', 'mntwines', 'mntfruits',\n                   'mntmeatproducts', 'mntfishproducts', 'mntsweetproducts',\n                   'mntgoldprods', 'numdealspurchases', 'numwebpurchases',\n                   'numcatalogpurchases', 'numwebvisitsmonth']\nQ1 = df[numeric_columns_to_filter].quantile(0.0)\nQ3 = df[numeric_columns_to_filter].quantile(0.90)\nIQR = Q3 - Q1\nprint('Here we will get IQR for each column\\n',IQR)\n\ndf_filtered = df[~((df[numeric_columns_to_filter] < (Q1 - 1.5 * IQR)) |(df[numeric_columns_to_filter] > (Q3 + 1.5 * IQR))).any(axis=1)]\ndisplay(df.shape)\ndisplay(df_filtered.shape)","20f21927":"boxplots_custom(dataset=df_filtered, columns_list=numeric_columns_to_filter[:-1], rows=2,\n                cols=5, suptitle='Boxplots after deleting outliers',size=(20,8))","e9a20b44":"# distributions\ndef dist_custom(dataset, columns_list, rows, cols, suptitle, size=(16,8), y=0.92):\n    fig, axs = plt.subplots(rows, cols,figsize=size)\n    fig.suptitle(suptitle,y=y, size=16)\n    axs = axs.flatten() \n    for i, data in enumerate(columns_list):\n        mean, median = dataset[data].mean(), dataset[data].median()\n        graph = sns.histplot(dataset[data], ax=axs[i])\n        graph.axvline(mean, c='red',label='mean')\n        graph.axvline(median, c='green',label='median')\n        plt.legend()\n        axs[i].set_title(data + ', skew: '+str(round(dataset[data].skew(axis = 0, skipna = True),2)))\n        \ndist_custom(dataset=df_filtered, columns_list=numeric_columns_to_filter[:3], \n            rows=1, cols=3, suptitle='Distibution for each variable', size=(16,5), y=1.0)","826f2ecd":"dist_custom(dataset=df_filtered, columns_list=numeric_columns_to_filter[3:], \n            rows=2, cols=4, suptitle='Distibution for each variable', size=(16,10), y=0.95)","e9aa3474":"def dist_custom(dataset, columns_list, rows, cols, suptitle, size=(16,8), y=0.92, inc=False):\n    fig, axs = plt.subplots(rows, cols,figsize=size)\n    fig.suptitle(suptitle,y=y, size=16)\n    fig.tight_layout(pad=6.0)\n    axs = axs.flatten() \n    \n    for i, data in enumerate(columns_list):\n        df_c = dataset.groupby(data).count().reset_index()\n        pal = sns.color_palette(\"Greens_d\", len(df_c))\n        rank = df_c['id'].argsort().argsort() \n        \n        g=sns.barplot(df_c[data], df_c['id'], ax=axs[i], palette=np.array(pal[::-1])[rank])\n        plt.legend()\n        for index, row in df_c.iterrows():\n            g.text(row.name,row.id, round(row.id,2), color='black', ha=\"center\")\n            \n        if inc == True:\n            axs[i].tick_params(axis='y')\n            df_c = dataset.groupby(data)['income'].mean().reset_index()\n            axs[i] = axs[i].twinx()\n            sns.lineplot(df_c[data], df_c['income'], ax=axs[i], color='red', label='mean income')\n            axs[i].tick_params(axis='y')\n        \n        \ndist_custom(dataset=df_filtered, columns_list=categorical_columns, \n            rows=1, cols=2, suptitle='Barchar for education types and marital status',\n            size=(16,8), y=0.83, inc=True)","a111d21e":"# the data in acceptedcmp5 is equal to acceptedcmp3\ndist_custom(dataset=df_filtered, columns_list=bool_columns[1:], \n            rows=2, cols=3, suptitle='Barchar for education types and marital status',\n            size=(14,11), y=0.87, inc=False)","99f4a688":"fig = plt.figure(figsize=(14, 8))\nsns.lineplot(data=df_filtered.groupby(df_filtered.year_month)['id'].nunique(),\n             label='The customers enrollment', palette=\"r\",markers=True, dashes=False)\nsns.lineplot(data=df_filtered.groupby(df_filtered.year_month)['numdealspurchases'].sum(),\n             label='The number of purchases', palette=\"flare\")\nsns.lineplot(data=df_filtered.groupby(df_filtered.year_month)['mnt_products'].sum() \/ 100,\n             label='Money spent 10^2', palette=\"flare\")\nplt.title(label='The dynamic of customer enrollment')\nplt.xlabel('Data'), plt.ylabel('The number of customers')\nplt.legend()\nplt.xticks(rotation=45)\nplt.grid(b=True)\nplt.show()","ffb48fb7":"#1. Encoding categorical columns \nle = preprocessing.LabelEncoder()\ndf_c = df_filtered.copy() #keep a copy of data\ncategorical_columns.append('dt_customer')\n\nfor i in categorical_columns:\n    df_c[i]=df_c[[i]].apply(le.fit_transform)\ndf_c.drop('id', axis=1, inplace=True)\n\n#2. Scaling\ncols_del = ['acceptedcmp3', 'acceptedcmp4', 'acceptedcmp5', 'acceptedcmp1','acceptedcmp2','year_month', 'complain', 'response']\ndf_c = df_c.drop(cols_del, axis=1)\nscaler = StandardScaler()\nscaler.fit(df_c)\ndf_s = pd.DataFrame(scaler.transform(df_c),columns= df_c.columns )\n\n#3. Reducing dimentions to 3 with the PCA usage\npca = PCA(n_components=3)\npca.fit(df_s)\ndf_pca = pd.DataFrame(pca.transform(df_s), columns=([\"f1\",\"f2\", \"f3\"]))\ndf_pca.describe().T\n#print('Result table after perfoming encoding, scaling and PCA methods')\n#df_pca.head()\n\nprint('The result after permorming steps from 1 to 3')\n\n#Ploting result data with the use of scatterplot. plotly\nx =df_pca[\"f1\"]\ny =df_pca[\"f2\"]\nz =df_pca[\"f3\"]\n\nfig = go.Figure(data=[go.Scatter3d(\n    x=x,y=y,z=z,mode='markers',\n    marker=dict(size=6,color=x,opacity=0.8))])\n\n# tight layout\nfig.update_layout( title={'text': \"3D scatterplot of size-reduced data\",'y':0.9,\n        'x':0.5,'xanchor': 'center','yanchor': 'top'},\n                  margin=dict(l=200, r=220, b=0, t=0))\nfig.show()","81b2f236":"# 4. Performing Elbow Method\nplt_elb = KElbowVisualizer(KMeans(), k=10)\nplt_elb.fit(df_pca)\nplt_elb.show();","3e79f9f2":"g = sns.pairplot(df_pca, diag_kind=\"kde\",\n                 plot_kws = {'alpha': 0.6, 's': 10, 'edgecolor': 'k'})\ng.fig.suptitle('Parplot for reduced data', \n             size = 20, y=1.07);","a9a2ef9d":"plot_kwds = {'alpha' : 0.25, 's' : 50, 'linewidths':0}\nclusters_series = []\n\ndef plot_clusters(data, algorithms, args, kwds, axs):\n    for j, i in enumerate(axs):\n        algorithm = algorithms[j]\n        start_time = time.time()\n        labels = algorithm(*args[j], **kwds[j]).fit_predict(data)\n        end_time = time.time()\n        clusters_series.append(labels)\n        #plotting\n        palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n        colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]        \n        i.scatter(data.iloc[:,0], data.iloc[:,1],c=colors,  **plot_kwds)\n        i.set_title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=14)\n        i.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=10)\n    \n    \nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(16, 10), sharex=True, sharey=True)\naxs = axs.flatten()\ndata_alg = [cluster.MeanShift,cluster.AffinityPropagation , cluster.KMeans, \n           cluster.SpectralClustering, cluster.AgglomerativeClustering,cluster.DBSCAN]\ndata_arg = [(0.475,), (), (), \n           (), (), ()]\ndata_kwd = [{'cluster_all':False}, {'preference':-1.0, 'damping':0.95}, {'n_clusters':4},\n           {'n_clusters':4}, {'n_clusters':4, 'linkage':'ward'}, {'eps':0.605}]\n\nplot_clusters(data=df_pca, algorithms=data_alg, \n              args=data_arg, \n              kwds=data_kwd, axs=axs)\n\ndf_pca[\"clusters_affinity\"], df_pca[\"clusters_spectral\"] =  clusters_series[1],clusters_series[3]\ndf_pca[\"clusters_kmeans\"], df_pca[\"clusters_agglom\"] =   clusters_series[2],clusters_series[4]\n\ndf_filtered[\"clusters_affinity\"], df_filtered[\"clusters_spectral\"] =  clusters_series[1],clusters_series[3]\ndf_filtered[\"clusters_kmeans\"], df_filtered[\"clusters_agglom\"] =   clusters_series[2],clusters_series[4]","7330489e":"fig = go.Figure(data=[go.Scatter3d(\n    x=x,y=y,z=z,mode='markers',\n    marker=dict(size=6,color=df_pca[\"clusters_spectral\"],colorscale='Viridis',opacity=0.8))])\n\n# tight layout\nfig.update_layout( title={'text': \"3D scatterplot of spectral model clusters\",'y':0.9,\n        'x':0.5,'xanchor': 'center','yanchor': 'top'},\n                  margin=dict(l=200, r=220, b=0, t=0))\nfig.show()","750161d6":"fig, ax = plt.subplots(2,2, figsize=(26,16))\nax = ax.flatten()\ndata, titles = [\"clusters_agglom\", \"clusters_kmeans\", \"clusters_affinity\",\"clusters_spectral\"],[\"Agglomerative clustering\", \"Kmeans\", \"Affinity propagation\",\"Spectral clustering\"]\n\nfor j, i in enumerate(ax):\n    sns.scatterplot(data = df_filtered,x=df_filtered[\"mspent\"], y=df_filtered[\"income\"],hue=df_filtered[data[j]], ax=i, palette=\"Greens_d\")\n    i.set_title(titles[j])\n    i.legend()\n    plt.suptitle(\"Cluster's Profile Based On Income And Spending\", y=0.95);","da8ad06c":">Okay, now we will introduce EDA according to obtained results,\n><br>Firstly, let's plot scatter for each clusters found corresponding to 'mspent' and 'income' features.\n><br>Thanks to Karnika Kapoor and [her great work](https:\/\/www.kaggle.com\/karnikakapoor\/customer-segmentation-clustering) for the idea above","05ac2a20":"<a id=\"section-end\"><\/a>\n<div style=\"font-size:40px\" align=center>Work in Progress\n<img src=\"https:\/\/i.pinimg.com\/564x\/9a\/42\/79\/9a4279006e123929b83ad139c42c5da6.jpg\" width=50>\n\n<\/div>\n\n><div style=\"background-color: #F6FFF6;\">\n><ul style=\"font-size:20px;\">\n <b>Thank you<\/b> so much for reading my project! \n <br>Please, UPvote, if you like it or find smth usefull, your votes help me to continue and don't give up via researching\ud83e\uddbe\n\n><\/ul>\n><\/div>","79957af3":">\n>","145f78b4":"<a id=\"sub-33\"><\/a>\n><div style=\"font-weight: bold;font-size:20px\">2.3 Ideal Customer<\/div>\n>in the process\n","ecb9a945":"><div style=\"background-color: #F6FFF6;\"> \n><div style=\"font-weight: bold;font-size:18px\">Observations<\/div>\n><ul style=\"font-size:16px;\">\n>\n>At the first glance at the data: \n<br>- There are 2240 entries and 29 columns, one column is an customer's ID;\n<br>- The columns 'z_costcontact','z_revenue' have zero std. All values are equal, we will drop this columns from the df.\n<br>- No duplicated values and errors (at the first glance), only 'Income' column has missing values;\n<br>- There are some columns, which datatypes are should be changed, for example 'object' to 'category'.<br>\nAt the next step we'll study and preprocess outliers, rename column names and change datatypes.\n><\/ul>\n><\/div>","e5260145":"><div style=\"background-color: #F6FFF6;\">\n><div style=\"font-weight: bold;font-size:18px\">Observations<\/div>\n><ul style=\"font-size:16px;\">\n>\n>in the process\n> \n><\/ul>\n><\/div>","64236d66":"><div style=\"background-color: #F6FFF6;\"> \n><div style=\"font-weight: bold;font-size:18px\">Observations:<\/div>\n><ul style=\"font-size:16px;\">\n<br>-  Most columns contain Right-skewed distribution except the last one; \n<br>-  We can very well tell from this graph that, the customers are more often buying on web than through a catalog and making deals; \n<br>-  The number of web visits is about 5 in average;\n><br>\n><br><b>Some questions for an additional investigation:<\/b>\n<br>- Is there a correlation between the amount of money spent and number of visits?\n<br>-  What is a dynamic of purchases and the amount of money spent?\n<br>-  What is a relation between anount of customer's enrollment and purchases?   \n<br>-  Who has the highest\\lowest income?\n    \n><\/ul>\n><\/div>","bbd3a024":">There is almost no data distortion after filling nans, so lets go on!\n>\n>The next step is to check outliers in data and make filtered DataFreame if it's needed","ee23fcc5":"<a id=\"sub-33\"><\/a>\n><div style=\"font-weight: bold;font-size:20px\">3.2 Clustering  <\/div>\n>\n>\n>Clustering techniques apply when there is no class to be predicted but rather when the instances are to be divided into natural groups.\n><br>\u2014 Page 141, [Data Mining: Practical Machine Learning Tools and Techniques, 2016.](https:\/\/amzn.to\/2R0G3uG)\n>____________________________________\n><b><br>Mean shift<\/b> is centroid based, like K-Means and affinity propagation, but can return clusters instead of a partition. The underlying idea of the Mean Shift algorithm is that there exists some probability density function from which the data is drawn, and tries to place centroids of clusters at the maxima of that density function. It approximates this via kernel density estimation techniques, and the key parameter is then the bandwidth of the kernel used. This is easier to guess than the number of clusters, but may require some staring at, say, the distributions of pairwise distances between data points to choose successfully. The other issue (at least with the sklearn implementation) is that it is fairly slow depsite potentially having good scaling!\n>\n><br><b>Affinity Propagation<\/b> is a newer clustering algorithm that uses a graph based approach to let points \u2018vote\u2019 on their preferred \u2018exemplar\u2019. The end result is a set of cluster \u2018exemplars\u2019 from which we derive clusters by essentially doing what K-Means does and assigning each point to the cluster of it\u2019s nearest exemplar. Affinity Propagation has some advantages over K-Means. \n>\n><b><br>K-Means<\/b> is fast, easy to understand, and available everywhere. K-Means has a few problems however. The first is that it isn\u2019t a clustering algorithm, it is a partitioning algorithm. That is to say K-means doesn\u2019t \u2018find clusters\u2019 it partitions your dataset into as many (assumed to be globular) chunks as you ask for by attempting to minimize intra-partition distances. That leads to the second problem: you need to specify exactly how many clusters you expect. The third problem is K-Means dependents upon initialization; give it multiple different random starts and you can get multiple different clusterings. This does not engender much confidence in any individual clustering that may result.\n>\n><b><br>Spectral Clustering<\/b> is fast, easy to understand, and available everywhere. K-Means has a few problems however. The first is that it isn\u2019t a clustering algorithm, it is a partitioning algorithm. That is to say K-means doesn\u2019t \u2018find clusters\u2019 it partitions your dataset into as many (assumed to be globular) chunks as you ask for by attempting to minimize intra-partition distances. That leads to the second problem: you need to specify exactly how many clusters you expect. The third problem is K-Means dependents upon initialization; give it multiple different random starts and you can get multiple different clusterings. This does not engender much confidence in any individual clustering that may result.\n>\n><b><br>Agglomerative Clustering<\/b> is really a suite of algorithms all based on the same idea. The fundamental idea is that you start with each point in it\u2019s own cluster and then, for each cluster, use some criterion to choose another cluster to merge with. Do this repeatedly until you have only one cluster and you get get a hierarchy, or binary tree, of clusters branching down to the last layer which has a leaf for each point in the dataset. \n>\n><br><b>DBSCAN<\/b> is a newer clustering algorithm that uses a graph based approach to let points \u2018vote\u2019 on their preferred \u2018exemplar\u2019. The end result is a set of cluster \u2018exemplars\u2019 from which we derive clusters by essentially doing what K-Means does and assigning each point to the cluster of it\u2019s nearest exemplar. Affinity Propagation has some advantages over K-Means. ","e798236a":"<a id=\"section-three\"><\/a>\n<div style=\"font-weight: bold;font-size:30px\">Step 2: EDA<\/div>","c79d964a":"<a id=\"section-five\"><\/a>\n<div style=\"font-weight: bold;font-size:30px\"> Overall conclusion<\/div>","8872d10f":"<a id=\"sub-22\"><\/a>\n><div style=\"font-weight: bold;font-size:20px\">1.2 Data preproccessing<\/div>\n> Let's preprocces our data","f0753be0":">It\u2019s messy, but there are may occur some clusters that you cann't pick out by eye right away; determining the exact boundaries of those clusters is harder of course, but we can hope that our clustering algorithms will find at least some of those clusters\ud83d\udc4c\n><br>So, on to testing \ud83d\udc40","0b5bed3e":"<a id=\"sub-41\"><\/a>\n><div style=\"font-weight: bold;font-size:20px\">3.1 Preprocessing and PCA<\/div>\n> Beforehand we have to prepare our data (again, yes!) to perform clustering.\n><br><b>Step-by-Step (open the code cell to see it in detail):<\/b>\n><br> <u>1. LabelEncoder()<\/u> to encode categorical columns with value between 0 and n_classes-1,\n><br> <u>2. StandardScaler()<\/u> to standardize features by removing the mean and scaling to unit variance.\n><br> <u>3. PCA()<\/u> to reduce feature dimentions to 3-5,\n><br> <u>4. Elbow Method<\/u> to determine the number of clusters in a data set. ","314b9195":">It is interesting: we can see negative relation between Income and Kidhome, it can help us to fill nans in the first column. ","75f971bc":">So let\u2019s have a closer look at the data and see what we have.","b53b94fe":"<a id=\"sub-32\"><\/a>\n><div style=\"font-weight: bold;font-size:20px\">2.2 Customer Personality <\/div>\n><div style=\"font-size:16px\">\nAt this step we investigate the Income groups, answer the question 'What is a dynamic of purchases and the amount of money spent?' and find who spend the most.\n<\/div>","269e487e":"><br>Firstly, we've added the new column named 'mspent' (all expenses were summed),\n><br>Secondly,all columns were changed to lower case,\n><br>And then, we've changed some qualitative column's dtypes from 'object' to 'category'. \n>\n>Also we have nan values in the 'income' column, the plan is:\n>\n>- Check correlation between values, define column correlated with 'income'\n>- Fill nans with the use .groupby method and 'Education', column from the 1st step\n>- Compare distributions before and after altering data","41999873":">From the plot above we can see a high rate of new clients enrollment with the company, that's why a great deal of money has been spent on products from August 2018 to May 2014. Most purchases were made in March 2013.\n>\n>Who those people who spent the most? Let's answer this question!","3f639c31":"><b>Using the \"elbow\" or \"knee of a curve\"<\/b> as a cutoff point is a common heuristic in mathematical optimization to choose a point where diminishing returns are no longer worth the additional cost. In clustering, this means one should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data.\n>\n><b>The KElbowVisualizer<\/b> implements the \u201celbow\u201d method to help selecting the optimal number of clusters by fitting the model with a range of values for K. \n><br>If the line chart resembles an arm, then the \u201celbow\u201d (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. \n><br>In the visualizer \u201celbow\u201d will be annotated with a dashed line.\n>\n> [See the documentation](https:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/elbow.html) to learn more about Elbow Method!","c33356a4":"><b>We see some interesting results:<\/b>\n><br> It is hard to determine some clusters due to high density, not all algorithms show reasonable results.\n><br>Kmeans and Agglomerative Clustering show simular clusters, groupes found by Spectral Clustering differs (mostly, the 2d and 3d cluster). \n><br>To examine the clusters let's plot the 3D distribution of the clusters for spectral model as an example.","03be81ed":">There are not so many people who've complained - only 21, and a high amount of responsed offer customers.","108bd03a":"<div style=\"font-weight: bold;font-size:40px\">Introduction<\/div>\n\n><div style=\"background-color: #F6FFF6;\">\n><ul style=\"font-size:16px;\">\n>    \n><b>Problem Statement<\/b>\n>\n>Customer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company\u2019s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment.\n>\n> <b>Target is:<\/b>\n>to perform clustering to summarize customer segments.\n>  \n>Many thanks to the \u0441reator of this dataset,\n><br>If you like this project, please, support me - UPvote!\ud83d\ude03\n\n<center><img src=\"https:\/\/i.pinimg.com\/564x\/91\/60\/af\/9160af85a18cab9f7b5e90a6b7d3a535.jpg\" width=300><\/center>\n\n    \n<div style=\"font-weight: bold;font-size:30px\">Data columns description<\/div>\n\n><div style=\"background-color: #F6FFF6;\">\n><ul style=\"font-size:14px;\">\n>    \n><br><b>People<\/b>\n><br>- ID: Customer's unique identifier\n><br>- Year_Birth: Customer's birth year\n><br>- Education: Customer's education level\n><br>- Marital_Status: Customer's marital status\n><br>- Income: Customer's yearly household income\n><br>- Kidhome, Teenhome: Number of children\\teenagers in customer's household\n><br>- Dt_Customer: Date of customer's enrollment with the company\n><br>- Recency: Number of days since customer's last purchase\n><br>- Complain: 1 if customer complained in the last 2 years, 0 otherwise\n><br><b>Products<\/b>\n><br>- MntWines, MntFruits : Amount spent on wine\\fruits in last 2 years\n><br>- MntMeatProducts, MntFishProducts: Amount spent on meat\\fish in last 2 years\n><br>- MntSweetProducts, MntGoldProds: Amount spent on sweets\\gold in last 2 years\n><br><b>Promotion<\/b>\n><br>- NumDealsPurchases: Number of purchases made with a discount\n><br>- AcceptedCmp1-5: 1 if customer accepted the offer in the 1st-5th campaign, 0 otherwise\n><br>- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n><br><b>Place<\/b>\n><br>- NumWebPurchases: Number of purchases made through the company\u2019s web site\n><br>- NumCatalogPurchases: Number of purchases made using a catalogue\n><br>- NumStorePurchases: Number of purchases made directly in stores\n><br>- NumWebVisitsMonth: Number of visits to company\u2019s web site in the last month\n<\/ul>\n<\/div>\n\n<br><div style=\"font-weight: bold;font-size:30px\">Table of Contents<\/div>\n\n>[Step 1: Examining Data](#section-two)\n>    - [Basic information](#sub-21)\n>    - [Data preproccessing](#sub-22)\n>    - [Making additional columns](#sub-23)\n>\n>[Step 2: EDA](#section-three)\n>    - [2.1 Research of  distributions](#sub-31)\n>    - [2.2 Customer Personality](#sub-32)\n>    - [2.3 Ideal Customer](#sub-33)  <b>(in process..)<\/b>\n>    \n>[Step 3: Clustering](#section-four)  \n>    - [3.1 Preprocessing and PCA](#sub-41)\n>    - [3.2 Clustering](#sub-42)\n>    - [3.3 Comparing the results](#sub-43)<b>(in process..)<\/b>\n>\n>[Overall Conclusion](#section-end)<b>(in process..)<\/b>","41a6faa9":"<a id=\"section-four\"><\/a>\n<div style=\"font-weight: bold;font-size:30px\">Step 3: Clustering<\/div>\n\n>\n>Clustering is an unsupervised learning problem.\n>In our data analysis project it is used as a technique for discovering groups of customers based on their behavior.\n>\n>There are many clustering algorithms to choose from and no single best clustering algorithm for all cases. Instead, it is a good idea to explore a range of >clustering algorithms and different configurations for each algorithm. \n><br>First of all, it is needed to prepare our data, let's do it ->","6b36fd0c":">We can see that the 'income' column has mean and almost simular median values unlike to 'mntwines' and 'mntfruits' values. This distribution has many peaks close together, the top of the distribution resembles a plateau. In average, income value is about 50 000, but some persons earn more.\nHist with 'mntwines' and 'mntfruits' data are Right-skewed.\n>\n>It seems like fruits are popular among customers, they spend a lot of money to it, as the hist is positive skewed and its value > 2, average value is higher than median (a lot of expensive purchases).\n>\n>Let's check another features ->","d00d193a":">From the plot above we can see that customers with Master and PhD degrees have the highest income in comparison to others education types. Clients with 'Basic' education don't earn a lot in average.\n>\n>Marital status barplot: customers with marital status \"Absurd\" has the highest income. Maybe it is erutliers in data, it should be investigated carefully. The lowest income reffers to alone people: it seems the reason is that such clients can be too young or too old and don't earn lots money.  \n>\n>YOLO is a popular online acronym that stands for: You Only Live Once. It was a surprise to meet it here! The additional link about this [acronym.](https:\/\/www.liveabout.com\/definition-of-yolo-3486193)","9e846428":"<a id=\"sub-44\"><\/a>\n><div style=\"font-weight: bold;font-size:20px\">3.3 Comparing the results  <\/div>\n>\n>\n>","919e9262":">We have plotted boxplots for all columns, that are quantative, the columns 'kidhome', 'teenhome' are discret that is why it isn't reasonable to filter them.\n>\n>There are some outliers in 'income', columns about product purchases spend and kinds of purchases ('numdealspurchases' and etc.), it can be usefull to filter df.","4a1b04d4":"><div style=\"background-color: #F6FFF6;\">\n><div style=\"font-weight: bold;font-size:18px\">Observations<\/div>\n><ul style=\"font-size:16px;\">\n> To sum up:\n<br>- We've plot the heatmap with correlation between all columns in order to fill nans in the 'Income' column. This didn't cause any change in data;\n<br>- Outliers in data were investigated, there are lots of it in the 'Income', columns about money spent on products and purchases made. But nevertheless we see some outliers even in filtered data. We'll use filtered data further.\n>\n> At the next step we'll investigate some features closer.\n><\/ul>\n><\/div>","6dd2a4ee":"<a id=\"section-two\"><\/a>\n<div style=\"font-weight: bold;font-size:30px\">Step 1: Examining Data<\/div>\n<a id=\"sub-21\"><\/a>\n\n\n>\n><div style=\"font-weight: bold;font-size:20px\">1.1 Basic information<\/div>\n>Importing libraries and reading data","9551c5cd":"<a id=\"sub-31\"><\/a>\n><div style=\"font-weight: bold;font-size:20px\">2.1 Research of  distributions<\/div>\n><div style=\"font-size:16px\">\n>A frequency distribution shows how often each different value in a set of data occurs. <b>It looks very much like a bar chart<\/b>, but they are not the same! <b>The major difference<\/b> is that a histogram is only used to plot the frequency of score occurrences in a continuous data set that has been divided into classes, called bins. Bar charts, on the other hand, can be used for a great deal of other types of variables including ordinal and nominal data sets.\n>    \n>Let's visualize hists for the each quantative column in our dataset ->\n<\/div>"}}