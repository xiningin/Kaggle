{"cell_type":{"95a75610":"code","fcaf0b42":"code","b654de87":"code","492493ce":"code","ba00acbc":"code","decf288a":"code","17466bec":"code","d5298050":"code","d443a913":"code","19576df9":"code","fa0726af":"code","adec0bdd":"code","469bab51":"code","3325722d":"code","c72433db":"code","742a8416":"code","f575b86a":"code","af7b136f":"code","cbbb7fc8":"code","15fac5e7":"markdown","deaa157a":"markdown","aaf8a36f":"markdown","4b250456":"markdown","4d902f4a":"markdown","610a3156":"markdown","4c15ecfa":"markdown"},"source":{"95a75610":"import os\nimport gc\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import BertTokenizer,BertConfig,TFBertModel\nfrom tqdm import tqdm\n\ntqdm.pandas()\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fcaf0b42":"DATA_PATH = '\/kaggle\/input\/tweet-sentiment-extraction\/'\ntrain_df = pd.read_csv(DATA_PATH+'train.csv')\ntest_df = pd.read_csv(DATA_PATH+'test.csv')\nsubmission_df = pd.read_csv(DATA_PATH+'sample_submission.csv')","b654de87":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 32\n    EPOCHS = 5\n    BERT_CONFIG = \"\/kaggle\/input\/bertconfig\/bert-base-uncased-config.json\" \n    BERT_PATH = \"\/kaggle\/input\/bert-base-uncased-huggingface-transformer\/\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        f\"{BERT_PATH}\/bert-base-uncased-vocab.txt\", \n        lowercase=True)\n    SAVEMODEL_PATH = '\/kaggle\/input\/tftweetfinetuned\/finetuned_bert.h5'\n    THRESHOLD = 0.4","492493ce":"def process_data(tweet, selected_text, tokenizer):\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1):\n            char_targets[ct] = 1\n            \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n            \n    targets = [0] * len(input_ids_orig)\n    for idx in target_idx:\n        targets[idx] = 1\n    return targets","ba00acbc":"train_df['targets'] = train_df.progress_apply(lambda row: process_data(\n                                                                    str(row['text']), \n                                                                    str(row['selected_text']),\n                                                                    config.TOKENIZER),\n                                                                    axis=1)","decf288a":"## pad all the targets\ntrain_df['targets'] = train_df['targets'].apply(lambda x :x + [0] * (config.MAX_LEN-len(x)))","17466bec":"def _convert_to_transformer_inputs(text, tokenizer, max_sequence_length):\n    inputs = tokenizer.encode(text)\n    input_ids =  inputs.ids\n    input_masks = inputs.attention_mask\n    input_segments = inputs.type_ids\n    padding_length = max_sequence_length - len(input_ids)\n    padding_id = 0\n    input_ids = input_ids + ([padding_id] * padding_length)\n    input_masks = input_masks + ([0] * padding_length)\n    input_segments = input_segments + ([0] * padding_length)\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arrays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df.iterrows()):\n        ids, masks, segments= _convert_to_transformer_inputs(str(instance.text),tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns].values.tolist())","d5298050":"outputs = compute_output_arrays(train_df,'targets')\ninputs = compute_input_arrays(train_df, config.TOKENIZER, config.MAX_LEN)\ntest_inputs = compute_input_arrays(test_df, config.TOKENIZER, config.MAX_LEN)","d443a913":"def create_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    mask = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    attn = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    bert_conf = BertConfig() \n    #bert_conf.output_hidden_states = True\n    bert_model = TFBertModel.from_pretrained(config.BERT_PATH+'\/bert-base-uncased-tf_model.h5', config=bert_conf)\n    \n    output = bert_model(ids, attention_mask=mask, token_type_ids=attn)\n    \n    out = tf.keras.layers.Dropout(0.1)(output[0]) \n    out = tf.keras.layers.Conv1D(1,1)(out)\n    out = tf.keras.layers.Flatten()(out)\n    out = tf.keras.layers.Activation('sigmoid')(out)\n    model = tf.keras.models.Model(inputs=[ids, mask, attn], outputs=out)\n    return model","19576df9":"K.clear_session()\nmodel = create_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)","fa0726af":"if not os.path.exists(config.SAVEMODEL_PATH):\n    model.fit(inputs,outputs, epochs=config.EPOCHS, batch_size=config.TRAIN_BATCH_SIZE)\n    model.save_weights(f'finetuned_bert.h5')\nelse:\n    model.load_weights(config.SAVEMODEL_PATH)","adec0bdd":"predictions = model.predict(test_inputs, batch_size=32, verbose=1)","469bab51":"## Will change to higher threshold in upcoming versions\nthreshold = config.THRESHOLD\npred = np.where(predictions>threshold,1,0)","3325722d":"def decode_tweet(original_tweet,idx_start,idx_end,offsets):\n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n    return filtered_output","c72433db":"outputs = []\nfor test_idx in range(test_df.shape[0]):\n    indexes = list(np.where(pred[test_idx]==1)[0])\n    text = str(test_df.loc[test_idx,'text'])\n    encoded_text = config.TOKENIZER.encode(text)\n    if len(indexes)>0:\n        start = indexes[0]\n        end =  indexes[-1]\n    else:  ### if we found nothing above threshold\n        start = 0\n        end = len(encoded_text.ids) - 1\n    if end >= len(encoded_text.ids): ## -1 for SEP token at last\n        end = len(encoded_text.ids) - 1\n    if start>end: \n        selected_text = test_df.loc[test_idx,'text']\n    else:\n        selected_text = decode_tweet(text,start,end,encoded_text.offsets)\n    outputs.append(selected_text)","742a8416":"test_df['selected_text'] = outputs","f575b86a":"def replacer(row):\n    if row['sentiment'] == 'neutral' or len(row['text'].split())<2:\n        return row['text']\n    else:\n        return row['selected_text']\ntest_df['selected_text'] = test_df.apply(replacer,axis=1)","af7b136f":"test_df.head(100)","cbbb7fc8":"submission_df['selected_text'] = test_df['selected_text']\nsubmission_df.to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 80)\nsubmission_df.sample(20)","15fac5e7":"# Create Targets","deaa157a":"# Prediction","aaf8a36f":"# Convert into Bert Format","4b250456":"# Create the model","4d902f4a":"# Import the necessary libraries","610a3156":"# About this kernel\n### This kernel is Just exploring the possibilities of using BERT in this competition as NER approach.\n### This is just a starter kernel, alot can be improved from here.\n\n### Inspired from this @akensert's kernel [bert-base-tf2-0-now-huggingface-transformer](https:\/\/www.kaggle.com\/akensert\/bert-base-tf2-0-now-huggingface-transformer)\n### These kernels were very helpful [tensorflow-roberta-0-705](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705) and [bert-base-uncased-using-pytorch](https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch)\n### <font color='red'>If you find this kernel helpful please upvote \ud83d\ude0a. (Don't Just Fork Only)<\/font>","4c15ecfa":"# Training"}}