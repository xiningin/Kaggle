{"cell_type":{"c1a91b44":"code","c918d7b3":"code","9fe5eec7":"code","30fa9786":"code","e282931a":"code","6c848d18":"code","da5ece5c":"code","fd867845":"code","d54f8f48":"code","c483c2d4":"code","123ce555":"code","1544464e":"code","46dd66f0":"code","6a33659a":"code","8941e3a4":"code","04ab1c75":"code","4092c50a":"code","c572e25f":"code","cc44e611":"code","95d55ac7":"code","e8e79830":"code","4b8ca4e3":"code","125a3d88":"code","2bc159cc":"code","c5368769":"code","8e33ee77":"code","2c666553":"code","284b19ba":"code","58e6cabd":"code","2d55aa59":"code","a41dc426":"code","8e2df6e7":"code","f1dce743":"code","9f999917":"code","dabcc2b3":"code","c0ff83ac":"code","e90079e6":"code","ffbebedb":"code","f439b957":"code","235da2bf":"code","2aafb2f2":"code","35c35e11":"code","20e97e20":"code","db42c0cc":"code","f4adf835":"code","6242dc3e":"code","c80914df":"code","0cc661c5":"code","554b0816":"code","8507943b":"code","891154c0":"code","f85dd785":"code","63f9e587":"code","e001aa59":"markdown","b53e6ad9":"markdown","c9d3d1ef":"markdown","e9956c9b":"markdown","c85bcb2b":"markdown","6cfbc1ca":"markdown","317ef27d":"markdown","dd980f1e":"markdown","898daefa":"markdown","9852f8fa":"markdown","1a5bf1a1":"markdown","113fad4f":"markdown","5c76f5d2":"markdown","03e45af0":"markdown","dace2126":"markdown","0f3da2fb":"markdown","e1ee4348":"markdown","8bfe519c":"markdown","24326070":"markdown","d5fb484b":"markdown","6ef24a02":"markdown","ef7565b6":"markdown","51e8ec07":"markdown","b3579dee":"markdown","49841e45":"markdown","0d90b5e9":"markdown","be1cef5b":"markdown","b2dcba70":"markdown","dbbbcb09":"markdown","174f2ca4":"markdown","884d4e76":"markdown","3d1690b3":"markdown","df4fe860":"markdown","61e18c90":"markdown","90404961":"markdown","76f64498":"markdown","01e46298":"markdown","02104f8e":"markdown","40fe3af3":"markdown","c29f007d":"markdown","21b06a85":"markdown","4d245702":"markdown","5f5e1179":"markdown","7a766282":"markdown","396dafdd":"markdown","f8939916":"markdown"},"source":{"c1a91b44":"#main libraries\nimport os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\n\n#machine learning libraries:\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing  import StandardScaler, LabelEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import (BaggingRegressor, AdaBoostRegressor,GradientBoostingRegressor, \n                              RandomForestRegressor,  GradientBoostingRegressor)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\nfrom mlxtend.regressor import StackingCVRegressor\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# You can go offline on demand by using\ncf.go_offline() \n# initiate notebook for offline plot\ninit_notebook_mode(connected=False)         \n\n# set some display options:\nplt.rcParams['figure.dpi'] = 100\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"\n\n# see our files:\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c918d7b3":"#gather our data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = test.reset_index().drop('index',axis=1)['Id']\n\n\n#make a copy of the original data\ntrain_df_orig = train.copy()\ntest_df_orig = test.copy()\n\n#join all the data together\nfull_df = pd.concat([train.set_index('Id'), test.set_index('Id')]).reset_index(drop=True)\ndel full_df['SalePrice']","9fe5eec7":"train.head()","30fa9786":"test.head()","e282931a":"#The shape of our data\nrows, columns = full_df.shape\nprint('This data contains {} rows and {} columns splited into test\/train datasets with ratio {}'.\\\n      format(rows, columns, round((test.shape[0]\/train.shape[0])*100,2)))","6c848d18":"print('We have {} columns : \\n{}'.format(len(train.columns),train.columns.values))","da5ece5c":"#see information about the data\ntrain.info()\nprint('_ '*40)\ntest.info()","fd867845":"#finding the unique values in each column (type object)\nfor col in full_df.select_dtypes('O').columns:\n    print('We have {} unique values in {} column : {}'.format(len(train[col].unique()),col,train[col].unique()))\n    print('__'*30)","d54f8f48":"# lets see the correlation between columns and target column\ncorr = train.corr()\ncorr['SalePrice'].sort_values(ascending=False)[1:15].to_frame()\\\n.style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","c483c2d4":"#correlation heatmap\ncorr = train.corr()\n\n#apply a mask\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\n\n#sett a palette\ncmap = sns.diverging_palette(180, 30, as_cmap=True)\n\nwith sns.axes_style('white'):\n    fig, ax = plt.subplots(figsize=(25, 25))\n    sns.heatmap(corr,  mask=mask, cmap=cmap, annot=True, center=0, vmin=-1, vmax=0.8,\n                square=True, cbar_kws={'shrink':.5, 'orientation': 'vertical'}, linewidth=.02);","123ce555":"#Visualize columns have corr with SalePrice\n\nhigh_corr = corr['SalePrice'].sort_values(ascending=False)[1:][:13].index.tolist()\n\nfig, axes = plt.subplots(4,3, figsize=(20, 10), sharey=True);\nplt.subplots_adjust(hspace = 0.7, wspace=0.1)\nfig.suptitle('Highest Correlation with sale price', fontsize=20)\n\nfor i,col in zip(range(12),high_corr):\n    sns.scatterplot(y=train['SalePrice'], x=train[col],ax=axes[i\/\/3][i%3])\n    axes[i\/\/3][i%3].set_title('SalesPrice with '+col)","1544464e":"#selectting the indices of outliers from the scatter plots above\n\ndrop_index = train[((train['GarageArea']>1200) & (train['SalePrice']<300000))|\n                  ((train['GrLivArea']>5000) & (train['SalePrice']<300000))|\n                  ((train['1stFlrSF']>4000) & (train['SalePrice']<300000))|\n                  ((train['TotalBsmtSF']>5000) & (train['SalePrice']<300000))|\n                  ((train['MasVnrArea']>1200) & (train['SalePrice']<700000))|\n                  ((train['SalePrice']>600000))].index","46dd66f0":"#first lets see the distribution of our target (SalePrice)\n\ntemp = pd.DataFrame()\ntemp['Sale Price'] = train['SalePrice']\ntemp['Log Sale Price'] = np.log1p(train['SalePrice'])\n\ntemp.iplot(kind='hist',\n           subplots=True,\n           fill=True,\n           subplot_titles=True,\n           title='Sales Distribution')","6a33659a":"Y_train = train['SalePrice']\ndel train['SalePrice']\n\n#Converting the saleprice with Logarithms to overcome the high skewness and the outliers\nY_train = np.log1p(Y_train) \n\nfull_df = pd.concat([train.iloc[:,1:], test.iloc[:,1:]])\nntrain = len(train)\nntest = len(test)","8941e3a4":"#null percentage for each column\n\nnull_df = round(100*(full_df.isnull().sum().sort_values(ascending=False)\/len(full_df.index)),2)\\\n                    .to_frame().rename(columns={0:'Null values percentage'})[:15]\nnull_df","04ab1c75":"#Pie plot for the percentage values\n\nnull_df.reset_index().iplot(kind='pie',\n                            labels='index',\n                            title='Pie plot for null values percentage',\n                            textinfo='label+text+percent',\n                            values='Null values percentage')","4092c50a":"#Heatmap for the null values\n\nplt.figure(figsize=(20,6))\nplt.title('Heatmap for the null values in each column')\nsns.heatmap(full_df.isnull(),cmap='viridis');","c572e25f":"num_cols = ['LotFrontage', 'LotArea', 'YearBuilt','YearRemodAdd', 'MasVnrArea', \n            'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n            'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n            'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n            '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\ncat_cols = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour','LotConfig', 'LandSlope', 'Neighborhood',\n            'Condition1', 'Condition2','BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st','Exterior2nd', \n            'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n            'BsmtFinType2','Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual','TotRmsAbvGrd', 'Functional', \n            'FireplaceQu', 'GarageType','GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC','Fence', \n            'MiscFeature', 'MoSold', 'SaleType', 'SaleCondition','OverallCond', 'YrSold']","cc44e611":"null_num = full_df[num_cols].isnull().sum()[full_df[num_cols].isnull().sum()>0].index\nprint('All categorical coulmns that have null values that are : \\n\\n{}'.format(null_num.values))","95d55ac7":"full_df['Utilities'].value_counts()","e8e79830":"del full_df['Utilities']","4b8ca4e3":"# Basement columns\nBS_col = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n          'BsmtFullBath','MasVnrArea','BsmtHalfBath','BsmtFinSF1', \n          'BsmtFinSF2', 'BsmtUnfSF']\n\n# Meaning that No Basement\nfor col in BS_col:\n    full_df[col].fillna(0, inplace=True) \n    \nfull_df['BsmtCond'].fillna('NA', inplace=True)         # Meaning that No Basement(general condition)\nfull_df['BsmtExposure'].fillna('NA', inplace=True)     # Meaning that No Basement(walkout or garden level walls)\nfull_df['BsmtQual'].fillna('NA', inplace=True)         # Meaning that No Basement(height of the basement)\nfull_df['BsmtFinType1'].fillna('NA', inplace=True)     # Meaning that No Basement(basement finished area)\nfull_df['BsmtFinType2'].fillna('NA', inplace=True)\n\n#Garage Columns\ngr_col = ['GarageType','GarageYrBlt','GarageFinish','GarageCars',\n          'GarageArea','GarageQual','GarageCond']\n\n# Meaning that NO Garage\n# Numeric\nfull_df['GarageYrBlt'].fillna(0, inplace=True)\nfull_df['GarageCars'].fillna(0, inplace=True)          \nfull_df['GarageArea'].fillna(0, inplace=True)\n\n# Categorical\nfull_df['GarageCond'].fillna('NA', inplace=True)       \nfull_df['GarageQual'].fillna('NA', inplace=True)       \nfull_df['GarageType'].fillna('NA', inplace=True)       \nfull_df['GarageFinish'].fillna('NA', inplace=True)   \n\n\nfull_df['MasVnrType'].fillna('NA', inplace=True)     # Meaning that NO Masonry veneer\nfull_df['PoolQC'].fillna('NA', inplace=True)         # Meaning that NO Pool \nfull_df['Alley'].fillna('NA', inplace=True)          # Meaning that NO Alley ccess\nfull_df['Fence'].fillna('NA', inplace=True)          # Meaning that NO Fence\nfull_df['FireplaceQu'].fillna('NA', inplace=True)    # Meaning that NO Fireplace\nfull_df['MiscFeature'].fillna('NA', inplace=True)    # Meaning that NO Miscellaneous feature not covered in \n                                                     # other categories  \n    \n\nfull_df['Functional'].fillna('Typ' ,inplace=True)    # Typical Functionality\nfull_df['Electrical'].fillna('SBrkr' ,inplace=True)  # Standard Circuit Breakers & Romex\nfull_df['SaleType'].fillna('Oth' ,inplace=True)      # Other\nfull_df['KitchenQual'].fillna('TA' ,inplace=True)    # Typical\/Average\nfull_df['SaleType'].fillna('Oth' ,inplace=True)      # Other\nfull_df['Exterior1st'].fillna('Other' ,inplace=True) # Other\nfull_df['Exterior2nd'].fillna('Other' ,inplace=True) # Other\n \n# split the data before imputing the numeric data to avoid (Data lackage)\ntrain = full_df[:ntrain].reset_index().drop('index',axis=1)\ntest = full_df[ntrain:].reset_index().drop('index',axis=1)\n\nfor data in [train,test]:\n    # LotFrontage: Linear feet of street connected to property\n    data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n    \n    # Meaning that NO Masonry veneer\n    data['MSZoning'] = data['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n#combine all the data together again\nfull_df=pd.concat([train,test])","125a3d88":"#Check if still there are null values\nfull_df.isnull().sum().max()","2bc159cc":"#print the count of unique values in each categorical columns\nprint('Categorical columns      Unique values count\\n')\nfor col in cat_cols:\n    print(col,'-'*(30-len(col)),'>',len(full_df[col].unique()))","c5368769":"rows, cols = full_df.shape\nprint('Our new data has {} rows and {} columns'.format(rows, cols))","8e33ee77":"# Total Area\nfull_df['TotalSF'] = full_df['TotalBsmtSF'] + full_df['1stFlrSF'] + full_df['2ndFlrSF']\n\n# add the column to numeric columns to normalize them\nnum_cols.append('TotalSF')","2c666553":"# Convert YrSold, MoSold, MSSubClass, and OverallCond columns from numeric to string\n# We have to encode the data as categorical values, to benefit from increasing or decreasing to build the model\n\nfull_df['YrSold'] = full_df['YrSold'].astype(str)\nfull_df['MoSold'] = full_df['MoSold'].astype(str)\nfull_df['MSSubClass'] = full_df['MSSubClass'].apply(str)\nfull_df['OverallCond'] = full_df['OverallCond'].astype(str)","284b19ba":"# Convert all categorical collumns to numeric values\nfor c in cat_cols:\n    lbl = LabelEncoder() \n    full_df[c] = lbl.fit_transform(full_df[c])","58e6cabd":"#Now we don't have any non-numeric columns\n\nfull_df.select_dtypes('O').columns","2d55aa59":"#lets create a dataframe for the numeric columns with high skewness\n\nskewness = pd.DataFrame()\n\nskewness[['Positive Columns','Skewness(+v)']] = full_df[num_cols].skew().sort_values(ascending=False)[:10].reset_index()\nskewness[['Negative Columns','Skewness(-v)']] = full_df[num_cols].skew().sort_values(ascending=True)[:10].reset_index()\n\nskewness.columns = pd.MultiIndex.from_tuples([('Positive Skewness', 'Columns'), ('Positive Skewness', 'Skewness'),\n                                              ('Negative Skewness', 'Columns'), ('Negative Skewness', 'Skewness')])\nskewness","a41dc426":"#Visualize columns with highest Skewness\ncols = ['MiscVal','PoolArea','LotArea','LowQualFinSF','3SsnPorch','BsmtFinSF2','KitchenAbvGr',\n        'BsmtFinSF2','EnclosedPorch','ScreenPorch','GarageYrBlt','BsmtHalfBath']\n\nsns.set_style('whitegrid')\nfig, axes = plt.subplots(3,4, figsize=(18, 8));\nplt.subplots_adjust(hspace = 0.7, wspace=0.2)\nfig.suptitle('Highest Skewness', fontsize=20)\n\nfor i,col in zip(range(12),cols):\n    sns.kdeplot(full_df[col], ax=axes[i\/\/4][i%4], fill=True);\n    axes[i\/\/4][i%4].set_title(col+' Distribution')","8e2df6e7":"df = full_df.copy()\n\n#convert all categorical columns to integer values\nfor col in cat_cols:\n    df[col] = df[col].astype(int)\n\n# convert our categorical columns to dummies instead of LabelEncoding\nfor col in cat_cols:\n    dumm = pd.get_dummies(df[col], prefix = col, dtype=int)\n    df = pd.concat([df,dumm], axis=1)\n    \ndf.drop(cat_cols, axis=1, inplace=True)\n\n#Normalize our numeric data\ndf[num_cols] = df[num_cols].apply(lambda x:np.log1p(x)) #Normalize the data with Logarithms\n\ntrain_set = df[:ntrain].reset_index().drop('index',axis=1)\ntest_set = df[ntrain:].reset_index().drop('index',axis=1)\n\n#dropping the outliers\ntrain_set = train_set.drop(drop_index)\nY_train = Y_train.drop(drop_index)","f1dce743":"# define models to test:\n\nbase_models = {\"Elastic Net\":make_pipeline(RobustScaler(),                    #Elastic Net model(Regularized model)\n                                            ElasticNet(alpha=0.0005,\n                                                       l1_ratio=0.9)),\n               \"Kernel Ridge\" :KernelRidge(),                                 #Kernel Ridge model(Regularized model)\n               \"Lasso\" : make_pipeline(RobustScaler(), Lasso(alpha =0.0005,   #Lasso model(Regularized model)\n                                                             random_state=1)),\n               \"Random Forest\": RandomForestRegressor(n_estimators=300),      #Random Forest model\n               \"SVM\": SVR(),                                                  #Support Vector Machines\n               \"XGBoost\": XGBRegressor(),                                     #XGBoost model\n               \"LGBM\" : LGBMRegressor(objective='regression',num_leaves=5,\n                                      learning_rate=0.05, n_estimators=720,\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction = 0.2319,\n                                      feature_fraction_seed=9, bagging_seed=9,\n                                      min_data_in_leaf =6, min_sum_hessian_in_leaf = 11),                                              \n               \"Gradient Boosting\":make_pipeline(StandardScaler(),\n                                                 GradientBoostingRegressor(n_estimators=3000, #GradientBoosting model\n                                                                           learning_rate=0.005,     \n                                                                           max_depth=4, max_features='sqrt',\n                                                                           min_samples_leaf=15, min_samples_split=10, \n                                                                           loss='huber', random_state =5))}","9f999917":"# Preprocessing, fitting, making predictions and scoring for every model:\nmodels_data = {'R^2':{'Training':{},'Testing':{}},\n               'Adjusted R^2':{'Training':{},'Testing':{}},\n               'MAE':{'Training':{},'Testing':{}},\n               'MSE':{'Training':{},'Testing':{}},\n               'RMSE':{'Training':{},'Testing':{}}}\n\nX_train, X_test, y_train, y_test = train_test_split(train_set, Y_train, test_size=0.2, random_state=42)\np = train_set.shape[1]\ntrain_n = X_train.shape[0]\ntest_n = X_test.shape[0]\n\nfor name in base_models:\n    #fitting the model\n    model = base_models[name].fit(X_train, y_train)\n    #make predictions with train and test datasets\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    #calculate the R-Squared for training and testing\n    r2_train,r2_test = model.score(X_train, y_train), model.score(X_test, y_test)\n    models_data['R^2']['Training'][name], models_data['R^2']['Testing'][name] = r2_train, r2_test\n            \n    #calculate the Adjusted R-Squared for training and testing\n    adj_train, adj_test = (1-(1-r2_train)*(train_n-1)\/(train_n-p-1)) ,(1-(1-r2_test)*(train_n-1)\/(train_n-p-1))\n    models_data['Adjusted R^2']['Training'][name], models_data['Adjusted R^2']['Testing'][name] = adj_train, adj_test\n               \n    #calculate the Mean absolute error for training and testing\n    mae_train, mae_test = mean_absolute_error(y_train, y_pred_train), mean_squared_error(y_test, y_pred_test)         \n    models_data['MAE']['Training'][name], models_data['MAE']['Testing'][name] = mae_train, mae_test\n               \n    #calculate Mean square error for training and testing\n    mse_train, mse_test = mean_squared_error(y_train, y_pred_train), mean_squared_error(y_test, y_pred_test)\n    models_data['MSE']['Training'][name], models_data['MSE']['Testing'][name] = mse_train, mse_test\n\n    #calculate Root mean error for training and testing    \n    rmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\n    models_data['RMSE']['Training'][name], models_data['RMSE']['Testing'][name] = rmse_train, rmse_test\n    \n    print('\\n========================={}========================='.format(name))\n    print('**********Training**********************Testing********')\n    print('R^2    : ',r2_train,' '*(25-len(str(r2_train))),r2_test) \n    print('Adj R^2: ',adj_train,' '*(25-len(str(adj_train))),adj_test) \n    print('MAE    : ',mae_train,' '*(25-len(str(mae_train))),mae_test) \n    print('MSE    : ',mse_train,' '*(25-len(str(mse_train))),mse_test) \n    print('RMSE   : ',rmse_train,' '*(25-len(str(rmse_train))),rmse_test)","dabcc2b3":"R_2 = pd.DataFrame(models_data['R^2']).sort_values(by='Testing',ascending=False)\nAdjusted_R_2 = pd.DataFrame(models_data['Adjusted R^2']).sort_values(by='Testing',ascending=False)\nMAE = pd.DataFrame(models_data['MAE']).sort_values(by='Testing',ascending=True)\nMSE = pd.DataFrame(models_data['MSE']).sort_values(by='Testing',ascending=True)\nRMSE = pd.DataFrame(models_data['RMSE']).sort_values(by='Testing',ascending=True)","c0ff83ac":"#order the results by testing values\n\npx.line(data_frame=R_2.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='R-Squared for training and testing')","e90079e6":"#order the results by testing values\n\npx.line(data_frame=Adjusted_R_2.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='Adjusted R-Squared for training and testing')","ffbebedb":"#order the results by testing values\n\npx.line(data_frame=MAE.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='Mean absolute error for training and testing')","f439b957":"#order the results by testing values\n\npx.line(data_frame=MSE.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='Mean square error for training and testing')","235da2bf":"#order the results by testing values\n\npx.line(data_frame=RMSE.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='Root mean square error for training and testing')","2aafb2f2":"# prepare configuration for cross validation test\n\n#Create two dictionaries to store the results of R-Squared and RMSE \nr_2_results = {'R-Squared':{},'Mean':{},'std':{}}   \nrmse_results = {'RMSE':{},'Mean':{},'std':{}}\n\nn_folds = 5\nkfold = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_set)\n\nfor name in base_models:\n    model = base_models[name]\n    r_2 = cross_val_score(model, train_set, Y_train, scoring='r2', cv=kfold)   #R-Squared \n    rms = np.sqrt(-cross_val_score(model, train_set, Y_train, cv=kfold,        #RMSE\n                                   scoring='neg_mean_squared_error'))\n    \n    #save the R-Squared reults\n    r_2_results['R-Squared'][name] = r_2\n    r_2_results['Mean'][name] = r_2.mean()\n    r_2_results['std'][name] = r_2.std()\n    \n    #save the RMSE reults\n    rmse_results['RMSE'][name] = rms\n    rmse_results['Mean'][name] = rms.mean()\n    rmse_results['std'][name] = rms.std()","35c35e11":"#visualizing the results of R-Squared for each model\n\nr_2_cv_results = pd.DataFrame(index=r_2_results['R-Squared'].keys())\n\n#append the max R-Squared for each model to the dataframe\nr_2_cv_results['Max'] = [r_2_results['R-Squared'][m].max() for m in r_2_results['R-Squared'].keys()]\n#append the mean of all R-Squared for each model to the dataframe\nr_2_cv_results['Mean'] = [r_2_results['Mean'][m] for m in r_2_results['Mean'].keys()]\n#append the min R-Squared for each model to the dataframe\nr_2_cv_results['Min'] = [r_2_results['R-Squared'][m].min() for m in r_2_results['R-Squared'].keys()]\n#append the std of all R-Squared for each model to the dataframe\nr_2_cv_results['std'] = [r_2_results['std'][m] for m in r_2_results['std'].keys()]\n\nr_2_cv_results = r_2_cv_results.sort_values(by='Mean',ascending=False)\nr_2_cv_results.iplot(kind='bar',\n                 title='Max, Min, Mean, and standard deviation <br>For R-Squared values for each model')","20e97e20":"#visualizing the variance of R-Squared for each model\n\nscores = pd.DataFrame(r_2_results['R-Squared'])\nscores.iplot(kind='box',\n             title='Box plot for the variation of R-Squared for each model')","db42c0cc":"#visualize the results of RMSE for each model\n\nrmse_cv_results = pd.DataFrame(index=rmse_results['RMSE'].keys())\n\n#append the max R-Squared for each model to the dataframe\nrmse_cv_results['Max'] = [rmse_results['RMSE'][m].max() for m in rmse_results['RMSE'].keys()]\n#append the mean of all R-Squared for each model to the dataframe\nrmse_cv_results['Mean'] = [rmse_results['Mean'][m] for m in rmse_results['Mean'].keys()]\n#append the min R-Squared for each model to the dataframe\nrmse_cv_results['Min'] = [rmse_results['RMSE'][m].min() for m in rmse_results['RMSE'].keys()]\n#append the std of all R-Squared for each model to the dataframe\nrmse_cv_results['std'] = [rmse_results['std'][m] for m in rmse_results['std'].keys()]\n\nrmse_cv_results = rmse_cv_results.sort_values(by='Mean',ascending=True)\nrmse_cv_results.iplot(kind='bar',\n                 title='Maximum, Minimun, Mean values and standard deviation <br>For RMSE values for each model')","f4adf835":"#visualize the variance of RMSE for each model\n\nscores = pd.DataFrame(rmse_results['RMSE'])\nscores.iplot(kind='box',\n             title='Box plot for the variation of RMSE values for each model')","6242dc3e":"parametersGrid = {\"max_iter\": [1, 5, 10, 100],\n                  \"alpha\": [0.0005, 0.005, 0.001, 0.01, 0.1, 1, 10, 100],\n                  \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\nkfold = KFold(n_splits=10)\n\neNet  = ElasticNet()\neNet_grid = GridSearchCV(eNet, parametersGrid, scoring='r2', cv=kfold)\neNet_grid.fit(train_set, Y_train)","c80914df":"ElasticNet_model = base_models['Elastic Net']\nr2_1 = round(cross_val_score(ElasticNet_model ,train_set, Y_train, scoring='r2',cv=kfold).mean()*100, 3)\nr2_2 = round(eNet_grid.score(train_set, Y_train)*100, 3)\nprint('Lasso Model : ')\nprint(f'Before tuning the hyperparmeters ---> {r2_1} \\nAfter tuning the hyperparmeters ---> {r2_2}')","0cc661c5":"#Define our base and meta models for stacking\n\nbase_models = (KernelRidge(),\n               make_pipeline(RobustScaler(),Lasso(alpha=0.0005, random_state=1)),\n               make_pipeline(RobustScaler(),ElasticNet(alpha=0.0005, l1_ratio=0.9)),             \n               make_pipeline(StandardScaler(),GradientBoostingRegressor(learning_rate=0.005, \n                                                                        loss='huber',\n                                                                        max_depth=4, \n                                                                        max_features='sqrt',\n                                                                        min_samples_leaf=15,\n                                                                        min_samples_split=10,\n                                                                        n_estimators=3000,\n                                                                        random_state=1)))\nmeta_model = LGBMRegressor(bagging_fraction=0.8, bagging_freq=5, \n                           feature_fraction=0.2319, feature_fraction_seed=9,\n                           learning_rate=0.05, max_bin=55, min_data_in_leaf=6,\n                           min_sum_hessian_in_leaf=11, n_estimators=720, num_leaves=5,\n                           bagging_seed=9,objective='regression')","554b0816":"#Building the stacking model\n\nstack = StackingCVRegressor(regressors=base_models,\n                            meta_regressor=meta_model, \n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,cv=kfold,\n                            random_state=1)","8507943b":"#fitting the model to our data\nstack.fit(train_set,Y_train)","891154c0":"#see the results of the model for training\n\nstack_score = round(stack.score(train_set, Y_train)*100, 3)\npredictions = stack.predict(train_set)\nstack_rmse = round(np.sqrt(mean_squared_error(Y_train, predictions).mean())*100, 3)\nprint(' _'*15)\nprint('\\nStacking Results for trining test : \\n')\nprint(f'Score : {stack_score}%')\nprint(f'RMSE  : {stack_rmse}%')\nprint(' _'*15)","f85dd785":"#lets make the predictions for the submission \n\ny_stacking = np.expm1(stack.predict(test_set)) #using expm1 (The inverse of log1p)","63f9e587":"submission = pd.DataFrame({\n        \"Id\": test_id,\n        \"SalePrice\": y_stacking })\nsubmission.to_csv('submission.csv', index=False)","e001aa59":"### Based on the description of the data i will fill the null values as following","b53e6ad9":"<h3> I found that Elastic Net estimator is the best choice, it has:<\/h3><ul>\n<li><b>Highest<\/b> score with testing data <b>(low variance)<\/b> and <b>reasonable<\/b> score with training data<b> (low bias)<\/b>.\n<li><b>Lowest<\/b> error with training data and also a <b>not bad<\/b> error with training data.","c9d3d1ef":"<a id=\"Phase I\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 100px;\">Phase I<\/p>","e9956c9b":"### Normalizing and encoding the data","c85bcb2b":"<a id=\"head-1-1\"><\/a>\n### Getting the Data","6cfbc1ca":"> ### To overcome the high skewness issue, i will transform the data into logarithm values.","317ef27d":"<a id=\"head-1-3-1\"><\/a>\n### Outliers Detiction","dd980f1e":"<a id=\"train\/test\"><\/a>\n<h4><b>Iterate the models and compute train\/test results","898daefa":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\">Importing necessary modules and libraries\ud83d\udcda<\/p>","9852f8fa":"<a id=\"head-1-5\"><\/a>\n### Skewness","1a5bf1a1":"><b>R-Squared","113fad4f":"## Imputing missing values","5c76f5d2":"<h3>I think Stacking is the best choice because:<\/h3>\n<ul> \n    <li>Unlike bagging, in stacking, the models are typically different and fit on the same dataset.\n    <li>Unlike boosting, in stacking, a single model is used to learn how to best combine the predictions from the contributing models.\n<\/ul>\n<h3>This's how stacking work:<\/h3>\n<ul>\n    <li>Split the training data into K-folds using<b> K-fold cross-validation.<\/b>\n    <li>The <b>Level-0 models (Base models)<\/b> are fitted on the K-Fold parts and predictions are made.\n    <li>Then, Predictions from the train set are used as features for the <b>level-1 (Meta-model)<\/b> model.\n    <li>This model <b>(Meta-Model)<\/b> is used to make final predictions on the test set using the prediction values as features.\n<\/ul>\n<img src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20190515104518\/stacking.png\"><\/img>","03e45af0":"<a id=\"head-1-3\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Data Cleaning\ud83d\udd27<\/p>","dace2126":"><b> In this column all rows only one value are 'AllPub', so it will add nothing to the model","0f3da2fb":"<a class=\"anchor\" id=\"head-1\"><\/a>\n### Data Pre-processing ","e1ee4348":"### ElasticNet model","8bfe519c":"### <li><b>Evaluate the Regression models based on different metrics.\n<p><b>Note: <\/b>It is important to scale the data before performing regularized models, as they are sensitive to the scale of the input features, and i already have scaled the data using <b>Logarithmic Scale<\/b>.","24326070":"**Cross validation (CV)** is one of the technique used to test the effectiveness of a machine learning models, it is also a re-sampling procedure used to evaluate a model if we have a limited data.<br>To perform CV we need to keep aside a sample\/portion of the data on which is not used to train the model, later use this sample for testing\/validating.\nSo, k-fold cross validation is used for two main purposes:\n<li>To tune hyper parameters.\n<li>To better evaluate the performance of a model.","d5fb484b":"><h3>It's better we still can achieve a higher score !! ","6ef24a02":"### Visulizing the null values in each columns","ef7565b6":"<a href=\"#head-1-3\"><\/a>\n### Correlation with SalesPrice","51e8ec07":"<a id=\"cross-val\"><\/a>\n<b><h3>Lets try to evaluate our models using cross validation<\/h3><br>\n<p>I will calculate the follwing but with cross validation:\n    <ul>\n        <li>R-Squared\n        <li>Root mean squared error\n    <\/ul>\n","b3579dee":"------------------------------------------\n------------------------------------------\n# <p style=\"background-color:gray; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 10px 100px; color:black; hight:max\"> Upvote my work if you found it useful.\ud83c\udfaf <\/p>\n\n# <p style=\"background-color:#CCE3F2; font-family:newtimeroman; font-size:175%; text-align:center; border-radius: 15px 50px;\">Predicting the price at which a house was sold \ud83c\udfe1<\/p>\n\n------------------------------------------\n------------------------------------------\n\n<img src=\"https:\/\/www.interest.co.nz\/sites\/default\/files\/feature_images\/house-price-growth.jpg\" alt=\"Titanic\" hight=50 width=800><\/img>\n\n------------------------------------------\n------------------------------------------\n","49841e45":"### Visualizing the results","0d90b5e9":"<a id=\"head-1-2\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Data Exploration and Analysis \ud83d\udd0d<\/p>","be1cef5b":"><b>RMSE","b2dcba70":"<h4> According the train\/test and cross validation of the models i think that <b>ElasticNet<\/b> is the best choice because:<\/h4><br>\n<h4>In train\/test:<\/h4>\n    <ul>\n        <li><b>ElasticNet<\/b> preforms good in train\/test with score <b>(94.0779%)<\/b> for training and score <b>(89.5601%)<\/b> for testing.\n        <li><b>ElasticNet<\/b> hase also low <b>Root mean square error<\/b> for training and teting.\n    <\/ul>\n<h4>In cross validation :<\/h4>\n    <ul>\n        <li><b>ElasticNet<\/b> reached high mean score with cross validation also <b>(91.1784%)<\/b> with Standard deviation <b>(1.1132%)<\/b>\n        <li><b>ElasticNet<\/b> hase also a low mean value of Root mean square error values <b>(11.6398%)<\/b> for cross validation with Standard deviation <b>(1.1601%)<\/b>","dbbbcb09":"<a id=\"head-1-3-2\"><\/a>\n### Null values","174f2ca4":"<a id=\"modeld_init\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Initializing our models\ud83d\udcdd<\/p>","884d4e76":"### Visualizing our results","3d1690b3":"## Stacked Generalization (Stacking)","df4fe860":"<img src=\"https:\/\/media1.tenor.com\/images\/047e6fd4e7169886e992a8899e62b90b\/tenor.gif?itemid=12547153\" height=\"200\" style=\"margin: 0 ; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"House price prediction\"><\/img>","61e18c90":"<h2 align=\"center\" style='color:red' > If you liked the notebook or learned something please <b>Upvote<\/b>! <\/h2>\n<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">You can also see:<\/p>\n<ul>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/predicting-the-survival-of-titanic-top-6'>Predicting the Survival of Titanic (Top 6%)<\/a>    \n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/prediction-of-heart-disease-machine-learning'>Prediction of Heart Disease (Machine Learning)<\/a>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/data-exploration-and-visualization-uber-data'>Data exploration and visualization(Uber Data)<\/a><br>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/hotel-booking-eda-cufflinks-and-plotly'>Hotel booking EDA (Cufflinks and plotly)\n<\/a><br>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/suicide-rates-visualization-and-geographic-maps\/edit\/run\/53135916'>Suicide Rates visualization and Geographic maps<\/a>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/superstore-data-analysis-with-plotly-clustering'>Superstore Data Analysis With Plotly(Clustering)<\/a>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/superstore-analysis-with-cufflinks-and-pandas'>Superstore Analysis With Cufflinks and pandas<\/a>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/learn-data-analysis-using-sql-and-pandas'>Learn Data Analysis using SQL and Pandas<\/a>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/european-soccer-database-with-sqlite3'>European soccer database with sqlite3<\/a>\n<li><b><a href='https:\/\/www.kaggle.com\/alaasedeeq\/chinook-questions-with-sqlite'>Chinook data questions with sqlite3<\/a>","90404961":"><b><li>Using Logarithms helps us to have a normal distribution which could help us in a number of different ways such as outlier detection.<br>\n><b><li>In this data We have a right skewed distribution in which most Sales are between  0 and 340K.","76f64498":"<a id=\"stack\"><\/a>  \n#### Lets try Stacking these Classifiers to build more powerfull model","01e46298":"### Make a submission","02104f8e":"### <h1 align=\"center\" style=\"color:red \">Thanks for reading<\/h1>","40fe3af3":"<a id=\"head-1-4\"><\/a>\n### Prepare the data for ML","c29f007d":"<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Introduction<\/b><\/p>\n<b>The objective of this project is to build a Machine Learning model for the prediction of housing prices based on pattern extracted from analysing 79 descriptive features like their Area, Street, Alley YearBuilt etc.<br>\nThis project consists of two phases:\n<ul>\n    <li>Phase I: Focuses on data cleaning, exploration and preprocessing.\n    <li>Phase II : Machine Learning model building, validation and prediction.\n<\/ul>\n<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">We have 79 descriptive features:<\/p>\n<ul>\n    <li>SalePrice : The property's sale price in dollars. This is the target variable that you're trying to predict.<\/li>\n    <li>MSSubClass: The building class<\/li>\n    <li>MSZoning: The general zoning classification<\/li>\n    <li>LotFrontage: Linear feet of street connected to property<\/li>\n    <li>LotArea: Lot size in square feet<\/li>\n    <li>Street: Type of road access<\/li>\n    <li>Alley: Type of alley access<\/li>\n    <li>LotShape: General shape of property<\/li>\n    <li>LandContour: Flatness of the property<\/li>\n    <li>Utilities: Type of utilities available<\/li>\n    <li>LotConfig: Lot configuration<\/li>\n    <li>LandSlope: Slope of property<\/li>\n    <li>Neighborhood: Physical locations within Ames city limits<\/li>\n    <li>Condition1: Proximity to main road or railroad<\/li>\n    <li>Condition2: Proximity to main road or railroad (if a second is present)<\/li>\n    <li>BldgType: Type of dwelling<\/li>\n    <li>HouseStyle: Style of dwelling<\/li>\n    <li>OverallQual: Overall material and finish quality<\/li>\n    <li>OverallCond: Overall condition rating<\/li>\n    <li>YearBuilt: Original construction date<\/li>\n    <li>YearRemodAdd: Remodel date<\/li>\n    <li>RoofStyle: Type of roof<\/li>\n    <li>RoofMatl: Roof material<\/li>\n    <li>Exterior1st: Exterior covering on house<\/li>\n    <li>Exterior2nd: Exterior covering on house (if more than one material)<\/li>\n    <li>MasVnrType: Masonry veneer type<\/li>\n    <li>MasVnrArea: Masonry veneer area in square feet<\/li>\n    <li>ExterQual: Exterior material quality<\/li>\n    <li>ExterCond: Present condition of the material on the exterior<\/li>\n    <li>Foundation: Type of foundation<\/li>\n    <li>BsmtQual: Height of the basement<\/li>\n    <li>BsmtCond: General condition of the basement<\/li>\n    <li>BsmtExposure: Walkout or garden level basement walls<\/li>\n    <li>BsmtFinType1: Quality of basement finished area<\/li>\n    <li>BsmtFinSF1: Type 1 finished square feet<\/li>\n    <li>BsmtFinType2: Quality of second finished area (if present)<\/li>\n    <li>BsmtFinSF2: Type 2 finished square feet<\/li>\n    <li>BsmtUnfSF: Unfinished square feet of basement area<\/li>\n    <li>TotalBsmtSF: Total square feet of basement area<\/li>\n    <li>Heating: Type of heating<\/li>\n    <li>HeatingQC: Heating quality and condition<\/li>\n    <li>CentralAir: Central air conditioning<\/li>\n    <li>Electrical: Electrical system<\/li>\n    <li>1stFlrSF: First Floor square feet<\/li>\n    <li>2ndFlrSF: Second floor square feet<\/li>\n    <li>LowQualFinSF: Low quality finished square feet (all floors)<\/li>\n    <li>GrLivArea: Above grade (ground) living area square feet<\/li>\n    <li>BsmtFullBath: Basement full bathrooms<\/li>\n    <li>BsmtHalfBath: Basement half bathrooms<\/li>\n    <li>FullBath: Full bathrooms above grade<\/li>\n    <li>HalfBath: Half baths above grade<\/li>\n    <li>Bedroom: Number of bedrooms above basement level<\/li>\n    <li>Kitchen: Number of kitchens<\/li>\n    <li>KitchenQual: Kitchen quality<\/li>\n    <li>TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)<\/li>\n    <li>Functional: Home functionality rating<\/li>\n    <li>Fireplaces: Number of fireplaces<\/li>\n    <li>FireplaceQu: Fireplace quality<\/li>\n    <li>GarageType: Garage location<\/li>\n    <li>GarageYrBlt: Year garage was built<\/li>\n    <li>GarageFinish: Interior finish of the garage<\/li>\n    <li>GarageCars: Size of garage in car capacity<\/li>\n    <li>GarageArea: Size of garage in square feet<\/li>\n    <li>GarageQual: Garage quality<\/li>\n    <li>GarageCond: Garage condition<\/li>\n    <li>PavedDrive: Paved driveway<\/li>\n    <li>WoodDeckSF: Wood deck area in square feet<\/li>\n    <li>OpenPorchSF: Open porch area in square feet<\/li>\n    <li>EnclosedPorch: Enclosed porch area in square feet<\/li>\n    <li>3SsnPorch: Three season porch area in square feet<\/li>\n    <li>ScreenPorch: Screen porch area in square feet<\/li>\n    <li>PoolArea: Pool area in square feet<\/li>\n    <li>PoolQC: Pool quality<\/li>\n    <li>Fence: Fence quality<\/li>\n    <li>MiscFeature: Miscellaneous feature not covered in other categories<\/li>\n    <li>MiscVal: Value of miscellaneous feature<\/li>\n    <li>MoSold: Month Sold<\/li>\n    <li>YrSold: Year Sold<\/li>\n    <li>SaleType: Type of sale<\/li>\n    <li>SaleCondition: Condition of sale<\/li>    \n<\/ul>\n<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">Outline:<\/p>\n<ul>\n    <li><a href=\"#Phase I\"><b>Phase I<\/b><\/a>\n        <ul>\n            <li><a href=\"#head-1\">Data Pre-processing<\/a>\n                <ul>\n                    <li><a href=\"#head-1-1\">Gathering the Data<\/a>\n                    <li><a href=\"#head-1-2\">Data Exploration and Analysis<\/a>\n                    <li><a href=\"#head-1-3\">Data Cleaning<\/a>\n                        <ul>\n                            <li><a href=\"#head-1-3-1\">Outliers Detection<\/a>\n                            <li><a href=\"#head-1-3-2\">Dealing with Null values<\/a>\n                            <li><a href=\"#head-1-3-3\">Adding new features<\/a>\n                        <\/ul>\n                    <li><a href=\"#head-1-4\">Prepare the data for ML<\/a>\n                    <li><a href=\"#head-1-5\">Dealing with the high skewness of the data<\/a>\n                <\/ul>\n        <\/ul>\n    <li><a href=\"#Phase II\"><b>Phase II:<\/b><\/a>\n    <\/li>\n    <ul>\n        <li><a href=\"#modeld_init\">Initialize the models.<\/a>\n        <li><a href=\"#ml_models\">Comparing different Machine learning models.<\/a>\n            <ul>\n                <li><a href=\"#train\/test\">Compute train\/test results.<\/a><\/li>\n                <li><a href=\"#cross-val\">Evaluate our models using cross validation.<\/a><\/li>\n            <\/ul>\n        <li><a href=\"#grid\">Improving the top models<\/a><\/li>\n        <li><a href=\"#stack\"> Stacking the best model to get the best score<\/a><\/li>","21b06a85":"<a id=\"grid\"><\/a>\n## Lets try to improve the result of our top models","4d245702":"### That's Better !!","5f5e1179":"To evaluate the performance of any machine learning model we need to test it on some unseen data, based on the models performance on unseen data we can say weather our model is :\n<ul>\n<li>Under-fitting.\n<li>Over-fitting.\n<li>Well generalized.\n<\/ul>","7a766282":"<b>First, I will extract the features (categorical and numerical)","396dafdd":"<a id=\"Phase II\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Phase II<\/p>","f8939916":"<a id=\"head-1-3-3\"><\/a>\n### Adding a new feature"}}