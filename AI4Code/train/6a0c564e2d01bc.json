{"cell_type":{"c3dea674":"code","11c4877e":"code","bc694245":"code","3b35b44a":"code","f261ed83":"code","78525940":"code","c6239aa3":"code","f2ff051c":"code","60330796":"code","94604801":"code","688774cf":"code","8a132101":"code","9a6c6861":"code","fa54fe4b":"code","d6f8efa0":"code","85869890":"code","9b3b551d":"code","ed81c9e7":"code","089fd1af":"code","13707798":"code","dcffe98e":"code","849daf76":"code","f17203a3":"code","b5fda76a":"code","34572214":"code","3c92168e":"code","f86bdeff":"code","28fd582b":"code","fb82ba71":"code","e0e6e348":"code","46862d03":"code","48ea8b60":"code","baefe864":"code","43599e38":"code","ff73a609":"code","b39b0906":"code","1b77fb1a":"code","824c2429":"code","b9ceb993":"code","72859920":"code","ce2d99c3":"code","e54e2242":"code","97049678":"markdown","57560bee":"markdown","c367246d":"markdown","dd03496b":"markdown","68fbc3fc":"markdown","de041689":"markdown"},"source":{"c3dea674":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\nimport tensorflow_datasets as tfds\nfrom matplotlib.pyplot import figure\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport seaborn as sns\nfrom keras.datasets import mnist\nimport os.path\nimport gzip\nimport lzma\nimport torch\nimport codecs","11c4877e":"# Random seeds\ndef set_seed(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_seed()","bc694245":"for dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3b35b44a":"## kaggle\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u9700\u8981\u4eceQMNIST120000\u6570\u636e\u4e2d\u5254\u9664\ntest_data=pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(test_data.shape)\n## kaggle\u539f\u59cb\u6570\u636e\uff0c\u8bad\u7ec3\u6570\u636e6000\u6761\uff0c\u6d4b\u8bd5\u6570\u636e10000\u6761\n(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\ntrain_images=train_images.reshape(-1,28*28)\ntest_images=test_images.reshape(-1,28*28)\nprint(train_images.shape,test_images.shape)\n","f261ed83":"##\u539f\u59cb\u6570\u636e\u603b\u548c\nall_images=pd.DataFrame(train_images).append(pd.DataFrame(test_images))\nprint(all_images.head)","78525940":"all_labels=np.append(train_labels,test_labels)\n","c6239aa3":"sns.countplot(x=np.append(train_labels,test_labels))","f2ff051c":"## \u4e3a\u63d0\u53d6QMNIST\u6570\u636e\u51c6\u5907\u51fd\u6570\ndef open_maybe_compressed_file(path):\n    if path.endswith('.gz'):\n        return gzip.open(path, 'rb')\n    elif path.endswith('.xz'):\n        return lzma.open(path, 'rb')\n    else:\n        return open(path,'rb')\n    \ndef get_int(b):\n    return int(codecs.encode(b, 'hex'), 16)\n\ndef read_idx2_int(path):\n    with open_maybe_compressed_file(path) as f:\n        data = f.read()\n        assert get_int(data[:4]) == 12*256 + 2\n        length = get_int(data[4:8])\n        width = get_int(data[8:12])\n        parsed = np.frombuffer(data, dtype=np.dtype('>i4'), offset=12)\n        return torch.from_numpy(parsed.astype('i4')).view(length,width).long()\n\ndef read_idx3_ubyte(path):\n    with open_maybe_compressed_file(path) as f:\n        data = f.read()\n        assert get_int(data[:4]) == 8 * 256 + 3\n        length = get_int(data[4:8])\n        num_rows = get_int(data[8:12])\n        num_cols = get_int(data[12:16])\n        parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n        #print(parsed.shape)\n        return torch.from_numpy(parsed).view(length, num_rows, num_cols)\n","60330796":"## \u63d0\u53d6QMNIST\u6570\u636e\uff0c\u5305\u62ec\u8bad\u7ec3\u6570\u636e60000\u6761\uff0c\u6d4b\u8bd5\u6570\u636e1000\u6761\uff0c\u5e76\u4e14\u5c06\u6570\u636e\u548c\u6807\u7b7e\u5404\u81ea\u5408\u5e76\nqmnist_train_data = read_idx3_ubyte('\/kaggle\/input\/qmnist\/qmnist-train-images-idx3-ubyte')\nqmnist_X_data=qmnist_train_data.numpy()\nqmnist_test_data = read_idx3_ubyte('\/kaggle\/input\/qmnist\/qmnist-test-images-idx3-ubyte')\nqmnist_t_data=qmnist_test_data.numpy()\nqmnist_train_label=read_idx2_int('\/kaggle\/input\/qmnist\/qmnist-train-labels-idx2-int')\nqmnist_y_train=qmnist_train_label.numpy()\nqmnist_y_train=pd.DataFrame(qmnist_y_train)\n#qmnist_y_train=qmnist_y_train.iloc[:,0]\n\nqtd=pd.DataFrame(qmnist_t_data.reshape(-1,28*28))\nqxd=pd.DataFrame(qmnist_X_data.reshape(-1,28*28))\nqxd=qxd.append(qtd)\n\n\n\nqmnist_test_label=read_idx2_int('\/kaggle\/input\/qmnist\/qmnist-test-labels-idx2-int')\nqmnist_y_test=qmnist_test_label.numpy()\nqmnist_y_test=pd.DataFrame(qmnist_y_test)\n#qmnist_y_test=qmnist_y_test.iloc[:,0]\nqmnist_y=qmnist_y_train.append(qmnist_y_test)\n#qmnist_y=qmnist_y.iloc[:,0]\n\n","94604801":"qmnist_y","688774cf":"sns.countplot(x=qmnist_y.iloc[:,0].values)","8a132101":"##\u67e5\u770b\u4e00\u4e0bQMNIST\u548cMNIST\u6570\u636e\u662f\u5426\u4e00\u81f4\nimport matplotlib.pyplot as plt\nfrom random import randint\n\nrandom_num = randint(0, 70000)\n\nfig=plt.figure(figsize=(2,2),facecolor='blue')\nf = fig.add_subplot(2,2,1)\nimg = np.asarray(all_images.iloc[random_num ,0:].values.reshape((28,28))\/255);\nplt.imshow(img, cmap='gray')\nf = fig.add_subplot(2,2,2)\nimg = np.asarray(qxd.iloc[random_num ,0:].values.reshape((28,28))\/255);\nplt.imshow(img, cmap='gray')\n\nplt.show()","9a6c6861":"merage_images=pd.DataFrame(test_data.values).append(all_images)\ncheck=merage_images.duplicated()","fa54fe4b":"check.sum()","d6f8efa0":"## \u63d0\u53d6QMNIST\u6d4b\u8bd5\u96c6\u6570\u636e\uff0c\u548cMNIST\u6d4b\u8bd5\u96c6\u5408\u6570\u636e\u96c6\ncheck=check[28000:98000]\nqmnist_test=qxd[0:70000].values[check]\n","85869890":"y_qmnist_test=qmnist_y[0:70000].values[check]","9b3b551d":"print(qmnist_test.shape)\nprint(y_qmnist_test.shape)","ed81c9e7":"## \u53cd\u8f6c\u63d0\u53d6\u6d4b\u8bd5\u6570\u636e\u610f\u5916\u7684\u8bad\u7ec3\u6570\u636e\ncheck=check==False\nprint(check.sum())\nX_qmnist=pd.DataFrame(qxd[0:70000].values[check])\nX_qmnist=X_qmnist.append(qxd[70000:])\nprint(X_qmnist)","089fd1af":"y_qmnist=pd.DataFrame(qmnist_y[0:70000].values[check])\ny_qmnist=y_qmnist.append(qmnist_y[70000:])\nprint(y_qmnist)","13707798":"print(y_qmnist)","dcffe98e":"train=pd.concat([y_qmnist.iloc[:,0],X_qmnist],axis=1)","849daf76":"train","f17203a3":"pd.DataFrame(train).to_csv('train.csv', index=False)","b5fda76a":"y_qmnist_test","34572214":"pd.DataFrame(y_qmnist_test).iloc[:,0]","3c92168e":"val=pd.concat([pd.DataFrame(y_qmnist_test).iloc[:,0],pd.DataFrame(qmnist_test)],axis=1)","f86bdeff":"pd.DataFrame(val).to_csv('val.csv', index=False)","28fd582b":"X = np.array(X_qmnist, dtype=\"float32\") \/ 255\nprint(X.shape)","fb82ba71":"X = X.reshape(-1, 28, 28, 1)\nprint(X.shape)\n","e0e6e348":"pd.read_csv(\"\/kaggle\/input\/qmnist\/val.csv\")","46862d03":"y=y_qmnist.iloc[:,0]\nprint(y)","48ea8b60":"from keras.utils import np_utils\ny2=np_utils.to_categorical(y)","baefe864":"from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,f1_score\nfrom keras.utils import np_utils\n(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\nX_val=qmnist_test.reshape(-1,28,28,1)\ny_val=y_qmnist_test[:,0]\ny_val2=np_utils.to_categorical(y_val)","43599e38":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=30, verbose=1, restore_best_weights=True)\n\n\n","ff73a609":"\nmodels=[]\nfor i in range(10):\n    model = tf.keras.models.Sequential([\n        preprocessing.RandomTranslation(height_factor=0.05, width_factor=0.05, fill_mode='constant'),\n        preprocessing.RandomRotation(factor=0.05, fill_mode='constant'),\n        preprocessing.RandomZoom(height_factor=(-0.05,0.05), width_factor=(-0.05,0.05), fill_mode='constant'),   \n      tf.keras.layers.Conv2D(48, (3,3), activation='relu', input_shape=(28, 28, 1)),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),  \n        tf.keras.layers.Dropout(0.4),\n    #   tf.keras.layers.MaxPooling2D(2,2),  \n      tf.keras.layers.Conv2D(96, (3,3), activation='relu'),  \n          tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n      tf.keras.layers.MaxPooling2D(2,2),  \n          tf.keras.layers.Dropout(0.4),\n      tf.keras.layers.Conv2D(192, (3,3), activation='relu'),\n          tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n       tf.keras.layers.MaxPooling2D(2,2),  \n              tf.keras.layers.Dropout(0.4),\n      tf.keras.layers.Conv2D(384, (3,3), activation='relu'),  \n          tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n               tf.keras.layers.Dropout(0.4),\n    #  tf.keras.layers.MaxPooling2D(2,2),  \n      tf.keras.layers.Flatten(),\n        \n     # tf.keras.layers.Dense(512), \n    #   tf.keras.layers.Dense(256),   \n       tf.keras.layers.Dense(32),\n         tf.keras.layers.Dropout(0.4),\n      tf.keras.layers.Dense(10, activation='softmax')\n    ])\n    models.append(model)","b39b0906":"historys=[]\ny=np.array(y)\n\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n\ni=0\n\nfor train, validation in kfold.split(X, y):\n\n    models[i].compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n                  #metrics=['sparse_categorical_accuracy'])\n\n\n    history = models[i].fit(X[train], \n                        y[train],\n                        epochs=80,\n                        batch_size=128,\n                        validation_data=(X[validation],y[validation]),\n                        verbose=1,\n                        callbacks=[callback])\n    historys.append(history)\n    model_json = models[i].to_json()\n    with open('.\/model'+str(i)+'.json', 'w') as file:\n        file.write(model_json)\n    # \u4fdd\u5b58\u8bad\u7ec3\u7684\u6743\u91cd\n    models[i].save_weights('.\/model'+str(i)+'.h5')\n\n    #test_loss, test_acc = model.evaluate(test_images, test_labels)","1b77fb1a":"from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score,  precision_score\nstyles=[':','-.','--','-',':','-.','--','-',':','-.','--','-',':','-.','--','-']\nnames = ['model'+str(i) for i in range(15)]\n\n# PLOT ACCURACIES\nplt.figure(figsize=(15,5))\nfor i in range(len(models)):\n    plt.plot(historys[i].history['val_accuracy'],linestyle=styles[i])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(names, loc='upper left')\naxes = plt.gca()\naxes.set_ylim([0.98,1])\nplt.show()\n","824c2429":"results = np.zeros( (X_val.shape[0],10) ) \nfor i in range(len(models)):\n    y_pred=models[i].predict(X_val)\n    results = results + y_pred\n    y_pred=y_pred.argmax(1)\n    #n=numbers[i]\n#     print(i,'\u6b63\u786e\u7387',1-len(n)\/len(y_val),'\u9519\u8bef\u6b21\u6570',len(n))\n    print('|',names[i],'|',np.sum(y_pred!=y_val),'|',models[i].count_params(),'|',round(accuracy_score(y_val, y_pred),6),'|',round(recall_score(y_val, y_pred,average='macro'),6),'|',round(precision_score(y_val, y_pred,average='macro'),6),'|',round(f1_score(y_val, y_pred, average='macro'),6),'|',)\nresults = np.argmax(results,axis = 1)\ny_pred=results\nprint('|','summary','|',np.sum(y_pred!=y_val),'|',models[i].count_params(),'|',round(accuracy_score(y_val, y_pred),6),'|',round(recall_score(y_val, y_pred,average='macro'),6),'|',round(precision_score(y_val, y_pred,average='macro'),6),'|',round(f1_score(y_val, y_pred, average='macro'),6),'|',)\n","b9ceb993":"test_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntest_data = np.array(test_data, dtype=np.float32)\/255\ntest_data = test_data.reshape(-1,28,28,1)\n\n\n","72859920":"# ENSEMBLE PREDICTIONS AND SUBMIT\nresults = np.zeros( (test_data.shape[0],10) ) \nfor j in range(15):\n    results = results + models[j].predict(test_data)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submission.csv\",index=False)","ce2d99c3":"pd.read_csv('submission.csv')","e54e2242":"#os.remove(\"\/kaggle\/working\/submission.csv\")","97049678":"# \ud83d\udcda **Import Libraries**","57560bee":"# **This is to remove a submission from the output directory**","c367246d":"# **Prepare and send the submission to the output directory**","dd03496b":"# ***Defining the model and adding callback.***","68fbc3fc":"The callback will stop the training when there is no improvement in the\nloss for 30 consecutive epochs.\n\nThe restore_best_weights=True will take the model back to its best fit. ","de041689":"QMNIST\u662fMNIST\u7684\u6269\u5c55\uff0c\u989d\u5916\u63d0\u4f9b\u4e8650000\u4e2a\u6d4b\u6570\u6570\u636e\uff0c\u5305\u62ec6\u4e07\u4e2a\u8bad\u7ec3\u6570\u636e\uff0c6\u4e07\u4e2a\u6d4b\u8bd5\u6570\u636e\uff0c\u603b\u5171\u6709120000\u4e2a\u6570\u636e\u3002QMNIST\u7684\u8bad\u7ec3\u6570\u636e\u548cMNIST\u7684\u8bad\u7ec3\u6570\u636e\u662f\u4e00\u81f4\u7684\uff0c\u6d4b\u8bd5\u6570\u636e\u524d1000\u6761\u548cMNIST\u7684\u6d4b\u8bd5\u6570\u636e\u662f\u4e00\u81f4\u7684\u3002\n\u4e0d\u8fc7\uff0ckaggle\u4e0a\u63d0\u4f9b\u7684[QMNIST\u6570\u636e](https:\/\/www.kaggle.com\/fedesoriano\/qmnist-the-extended-mnist-dataset-120k-images)\u662f\u6574\u4f53120000\u4e2a\u6570\u636e\uff0c\u5e76\u6ca1\u6709\u533a\u5206\u4e3a\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u6570\u6570\u636e\uff0c\u800c\u4e14\uff0cDigit Recongnizer\u63d0\u4f9b\u7684MNIST\u8bad\u7ec3\u6570\u636e\u4e3a48000\u6761\uff0c\u6d4b\u8bd5\u6570\u636e\u4e3a12000\u6761\u3002\u4f7f\u7528QMNIST\u989d\u5916\u6570\u636e\u6765\u63d0\u5347\u8bad\u7ec3\u7ed3\u679c\uff0c\u9700\u8981\u5c06MNIST\u7684\u6d4b\u6570\u6570\u636e\u4eceQMNIST\u4e2d\u5254\u9664\u3002 \u4f46\u662fkaggle QMNIST\u7684\u6570\u636e\u987a\u5e8f\u548cMNIST\u7684\u6570\u636e\u987a\u5e8f\u4e0d\u4e00\u81f4\uff0c\u6240\u4ee5\u9700\u8981\u4f7f\u7528\u539f\u59cbQMNIST\u6570\u636e\u3002\n\u9996\u5148\u662f\u6bd4\u8f83\u5206\u6790kaggle Digit Recongnizer mnist \u6d4b\u8bd5\u6570\u636e\u5bf9\u5e94\u539f\u59cbMNIST\u4f4d\u7f6e\u5e76\u6807\u8bb0\uff0c\u7136\u540e\u4eceQMNIST\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u5220\u9664\u3002\n\u6700\u7ec8\u8bad\u7ec3\u6570\u636e\u96c6\u4ece42000\u6269\u5145\u523092000\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8bad\u7ec3\u7ed3\u679c\u3002\n\nQMNIST is an extension of MNIST, providing 50000 additional test data, including 60000 training data and 60000 test data, with a total of 120000 data. QMNIST's training data is consistent with MNIST's training data, and the first 1000 test data are consistent with MNIST's test data.\nHowever, the [qmnist data](https:\/\/www.kaggle.com\/fedesoriano\/qmnist-the-extended-mnist-dataset-120k-images) provided on kaggleIt is 120000 data as a whole, which is not divided into training data and test data. In addition, digital Recongnizer provides 48000 MNIST training data and 12000 test data. To use additional data of qmnist to improve training results, MNIST test data need to be removed from QMNIST."}}