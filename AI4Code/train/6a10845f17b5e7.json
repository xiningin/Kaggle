{"cell_type":{"fec7baad":"code","4e7e58c3":"code","f3f985a3":"code","e442fbab":"code","8d19fc67":"code","7e7e55ed":"code","9d66bbbd":"code","d7d51b40":"code","b978b99a":"code","99bc7110":"code","179e4af8":"code","54291943":"code","c5eeb909":"code","2f4f888d":"code","0ed52f6f":"code","c04de8a9":"code","2cd3f088":"code","76615c13":"code","e89a1b3f":"markdown","5a73db68":"markdown","dbcae327":"markdown","fecc0cfb":"markdown","ed08e48e":"markdown","7163feb1":"markdown","39d0d0b7":"markdown","8f640113":"markdown","c386e6fd":"markdown","f1e45a9a":"markdown","d0cd8432":"markdown","fb412e95":"markdown","887fb285":"markdown","bb71a672":"markdown","2b81998d":"markdown","1a44acf1":"markdown","488365a1":"markdown","04c34258":"markdown","11cc09ed":"markdown","8e5b7fd8":"markdown"},"source":{"fec7baad":"# Packages\n\n# For Kaggle Environment --> Upgrade the seaborn version!\n!pip install seaborn --upgrade\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")","4e7e58c3":"# Import Data within Kaggle Notebooks\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data  = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nall_data = pd.concat([train_data,test_data])\n","f3f985a3":"# Dataframe Head\nprint(all_data.head())\n\n# Dataframe Shape\nprint(all_data.shape)\nprint()\n\n# Dataframe Overview\nprint(all_data.info())\nprint()\n      \n# Data Description (Numerical Data)      \nprint(all_data.describe().T)\nprint()\n      \n# Data Description (Categorical Data)\nprint(all_data.describe(include=['O']).T)\nprint()","e442fbab":"# Dataframe Shape\nprint(train_data.shape)\nprint()\n\n# Dataframe Overview\nprint(train_data.info())\nprint()\n      \n# Data Description (Numerical Data)      \nprint(train_data.describe().T)\nprint()\n      \n# Data Description (Categorial Data)\nprint(train_data.describe(include=['O']).T)\nprint()","8d19fc67":"# Dataframe Shape\nprint(test_data.shape)\nprint()\n\n# Dataframe Overview\nprint(test_data.info())\nprint()\n      \n# Data Description (Numerical Data)      \nprint(test_data.describe().T)\nprint()\n      \n# Data Description (Categorial Data)\nprint(test_data.describe(include=['O']).T)\nprint()","7e7e55ed":"# Distribution Age\nfig_dims = (12,6)\nfig, axes = plt.subplots(1,3, constrained_layout=True, figsize=fig_dims)\n\nfig.suptitle('Age Distribution')\naxes[0].set_title('Complete Dataset')\naxes[1].set_title('Training Dataset')\naxes[2].set_title('Test Dataset')\n\nsns.histplot(data=all_data, x=\"Age\", ax = axes[0], stat=\"count\", kde=True)\nsns.histplot(data=train_data, x=\"Age\", ax = axes[1], stat=\"count\", kde=True)\nsns.histplot(data=test_data, x=\"Age\", ax = axes[2], stat=\"count\", kde=True)\n\n\n# Distribution Sex\nfig, axes = plt.subplots(1,3, constrained_layout=True, figsize=fig_dims)\n\nfig.suptitle('Sex Distribution')\naxes[0].set_title('Complete Dataset')\naxes[1].set_title('Training Dataset')\naxes[2].set_title('Test Dataset')\n\nsns.histplot(data=all_data, x=\"Sex\", ax = axes[0], stat=\"count\")\nsns.histplot(data=train_data, x=\"Sex\", ax = axes[1], stat=\"count\")\nsns.histplot(data=test_data, x=\"Sex\", ax = axes[2], stat=\"count\")\n\n\n# Distribution Class\nfig, axes = plt.subplots(1,3, constrained_layout=True, figsize=fig_dims)\n\nfig.suptitle('Class Distribution')\naxes[0].set_title('Complete Dataset')\naxes[1].set_title('Training Dataset')\naxes[2].set_title('Test Dataset')\n\nsns.histplot(data=all_data, x=\"Pclass\", ax = axes[0], stat=\"count\")\nsns.histplot(data=train_data, x=\"Pclass\", ax = axes[1], stat=\"count\")\nsns.histplot(data=test_data, x=\"Pclass\", ax = axes[2], stat=\"count\")\n\n\n# Distribution Fare\nfig, axes = plt.subplots(1,3, constrained_layout=True, figsize=fig_dims)\n\nfig.suptitle('Fare Distribution')\naxes[0].set_title('Complete Dataset')\naxes[1].set_title('Training Dataset')\naxes[2].set_title('Test Dataset')\n\nsns.histplot(data=all_data, x=\"Fare\", ax = axes[0], stat=\"count\", kde=True)\nsns.histplot(data=train_data, x=\"Fare\", ax = axes[1], stat=\"count\", kde=True)\nsns.histplot(data=test_data, x=\"Fare\", ax = axes[2], stat=\"count\", kde=True)\n\n\n# Distribution Siblings\/Spouses\nfig, axes = plt.subplots(1,3, constrained_layout=True, figsize=fig_dims)\n\nfig.suptitle('Siblings\/Spouses Distribution')\naxes[0].set_title('Complete Dataset')\naxes[1].set_title('Training Dataset')\naxes[2].set_title('Test Dataset')\n\nsns.histplot(data=all_data, x=\"SibSp\", ax = axes[0], stat=\"count\", kde=True)\nsns.histplot(data=train_data, x=\"SibSp\", ax = axes[1], stat=\"count\", kde=True)\nsns.histplot(data=test_data, x=\"SibSp\", ax = axes[2], stat=\"count\", kde=True)\n\n\n# Distribution Parents\/Children\nfig, axes = plt.subplots(1,3, constrained_layout=True, figsize=fig_dims)\n\nfig.suptitle('Parents\/Children Distribution')\naxes[0].set_title('Complete Dataset')\naxes[1].set_title('Training Dataset')\naxes[2].set_title('Test Dataset')\n\nsns.histplot(data=all_data, x=\"Parch\", ax = axes[0], stat=\"count\", kde=True)\nsns.histplot(data=train_data, x=\"Parch\", ax = axes[1], stat=\"count\", kde=True)\nsns.histplot(data=test_data, x=\"Parch\", ax = axes[2], stat=\"count\", kde=True)\n\nplt.show()","9d66bbbd":"# Survived Passengers\naxes = sns.countplot(x=\"Survived\", data=train_data)\naxes.set_title('Number of Passengers vs. Survived')\naxes.set_xlabel('Survived')\naxes.set_ylabel('Count')\nplt.show()\n\n\n# Survival Rate\naxes = sns.barplot(x=\"Survived\", y=\"Survived\", data=train_data, estimator=lambda x: len(x) \/ len(train_data) * 100)\naxes.set_title('Survival Rate')\naxes.set_xlabel('Survived')\naxes.set_ylabel('Percentage')\nplt.show()\n\n\nmean_surv = round(train_data['Survived'].mean(),3)*100\nprint(\"The survival rate in the training dataset is: \" + str(mean_surv) + \" %\")","d7d51b40":"# Survival Rate vs. Age\ntrain_date_age = train_data\nage_bins = [0,1,10,18,30,60,100]\nage_cat_train = pd.cut(train_data['Age'],age_bins,labels=['Baby','Child','Youth','YoungAdult','MiddleAged','Senior'])\ntrain_date_age['Age_Cat'] = age_cat_train\n\naxes = sns.barplot(x=\"Age_Cat\", y=\"Survived\", data=train_data)\naxes.set_title('Survival Rate vs. Age')\naxes.set_xlabel('Age Group')\naxes.set_ylabel('Survival Rate')\nplt.xticks(rotation=45)\nplt.show()\n\nprint(train_data.groupby([\"Age_Cat\"])[\"Survived\"].mean().round(3)*100)\nprint(\"\\n \\n \\n\")\n\n\n# Survival Rate vs. Sex\naxes = sns.barplot(x=\"Sex\", y=\"Survived\", data=train_data)\naxes.set_title('Survival Rate vs. Sex')\naxes.set_xlabel('Sex')\naxes.set_ylabel('Survival Rate')\nplt.show()\n\nprint(train_data.groupby([\"Sex\"])[\"Survived\"].mean().round(3)*100)\nprint(\"\\n \\n \\n\")\n\n\n# Survival Rate vs. Class\naxes = sns.barplot(x=\"Pclass\", y=\"Survived\", data=train_data)\naxes.set_title('Survival Rate vs. Class')\naxes.set_xlabel('Class')\naxes.set_ylabel('Survival Rate')\nplt.show()\n\nprint(train_data.groupby([\"Pclass\"])[\"Survived\"].mean().round(3)*100)\nprint(\"\\n \\n \\n\")\n\n\n# Survival Rate vs. Sex vs. Class\naxes = sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train_data)\naxes.set_title('Survival Rate vs. Sex vs. Class')\naxes.set_xlabel('Class')\naxes.set_ylabel('Survival Rate')\nplt.show()\n\nprint(train_data.groupby([\"Sex\",\"Pclass\"])[\"Survived\"].mean().round(3)*100)\nprint(\"\\n \\n \\n\")\n\n\n# Survival Rate vs. SibSp\naxes = sns.barplot(x=\"SibSp\", y=\"Survived\", data=train_data)\naxes.set_title('Survival Rate vs. Number of Siblings\/Spouses')\naxes.set_xlabel('Number of Siblings\/Spouses')\naxes.set_ylabel('Survival Rate')\nplt.show()\n\nprint(train_data.groupby([\"SibSp\"])[\"Survived\"].mean().round(3)*100)\nprint(\"\\n \\n \\n\")\n\n# Survival Rate vs. Parch\naxes = sns.barplot(x=\"Parch\", y=\"Survived\", data=train_data)\naxes.set_title('Survival Rate vs. Number of Parents\/Children')\naxes.set_xlabel('Number of Parents\/Children')\naxes.set_ylabel('Survival Rate')\nplt.show()\n\nprint(train_data.groupby([\"Parch\"])[\"Survived\"].mean().round(3)*100)\nprint(\"\\n \\n \\n\")\n\n\n# Survival Rate vs. Embarked\naxes = sns.barplot(x=\"Embarked\", y=\"Survived\", data=train_data)\naxes.set_title('Survival Rate vs. Embarked')\naxes.set_xlabel('Embarked')\naxes.set_ylabel('Survival Rate')\nplt.show()\n\nprint(train_data.groupby([\"Embarked\"])[\"Survived\"].mean().round(3)*100)","b978b99a":"# Prepare Data\ntrain_data_new = train_data[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\",\"Fare\",\"Embarked\", \"Survived\"]]\ntrain_data_new['Sex'].replace(['male','female'],[0,1],inplace=True)\ntrain_data_new['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n\n# Create Matrix\nmatrix = np.triu(train_data_new.corr())\nplt.figure(figsize=(12,8))\nplt.title('Correlation between Features')\nsns.heatmap(train_data_new.corr(), annot=True, mask=matrix)\nplt.show()","99bc7110":"# NULL Values Analysis --> Number of NULL values per feature\nprint('Number of NULL Values before filling: ')\nprint(all_data.isnull().sum())\nprint(\"\\n \\n \\n\")\n\n\n\n# Replace NULL Values in feature \"Age\" --> median \"Age\" value of complete dataset - 0.5 (format for estimated ages)\nmedian_age = round(all_data['Age'].median(),2) - 0.5\nall_data['Age'].fillna(median_age, inplace=True)\ntrain_data['Age'].fillna(median_age, inplace=True)\ntest_data['Age'].fillna(median_age, inplace=True)\n\n# Replace NULL Values in feature \"Age\" --> median \"Age\" value per \"Pclass\" - 0.5 (format for estimated ages)\n'''\nmedian_age_class_1 = all_data['Age'][all_data.Pclass == 1].median() - 0.5\nmedian_age_class_2 = all_data['Age'][all_data.Pclass == 2].median() - 0.5\nmedian_age_class_3 = all_data['Age'][all_data.Pclass == 3].median() - 0.5\nall_data['Age'][(all_data.Pclass == 1) & ((all_data).Age.isnull())] = median_age_class_1\nall_data['Age'][(all_data.Pclass == 2) & ((all_data).Age.isnull())] = median_age_class_2\nall_data['Age'][(all_data.Pclass == 3) & ((all_data).Age.isnull())] = median_age_class_3\ntrain_data['Age'][(train_data.Pclass == 1) & ((train_data).Age.isnull())] = median_age_class_1\ntrain_data['Age'][(train_data.Pclass == 2) & ((train_data).Age.isnull())] = median_age_class_2\ntrain_data['Age'][(train_data.Pclass == 3) & ((train_data).Age.isnull())] = median_age_class_3\ntest_data['Age'][(test_data.Pclass == 1) & ((test_data).Age.isnull())] = median_age_class_1\ntest_data['Age'][(test_data.Pclass == 2) & ((test_data).Age.isnull())] = median_age_class_2\ntest_data['Age'][(test_data.Pclass == 3) & ((test_data).Age.isnull())] = median_age_class_3\n'''\n\n# Replace NULL Values in feature \"Fare\" --> median \"Fare\" value of complete dataset\nmedian_fare = round(all_data['Fare'].median())\nall_data['Fare'].fillna(median_fare, inplace=True)\ntrain_data['Fare'].fillna(median_fare, inplace=True)\ntest_data['Fare'].fillna(median_fare, inplace=True)\n\n# Replace NULL Values in feature \"Fare\" --> median \"Fare\" value per \"Pclass\"\n'''\nmedian_fare_class_1 = all_data['Fare'][all_data.Pclass == 1].median()\nmedian_fare_class_2 = all_data['Fare'][all_data.Pclass == 2].median()\nmedian_fare_class_3 = all_data['Fare'][all_data.Pclass == 3].median()\nall_data['Fare'][(all_data.Pclass == 1) & ((all_data).Fare.isnull())] = median_fare_class_1\nall_data['Fare'][(all_data.Pclass == 2) & ((all_data).Fare.isnull())] = median_fare_class_2\nall_data['Fare'][(all_data.Pclass == 3) & ((all_data).Fare.isnull())] = median_fare_class_3\ntrain_data['Fare'][(train_data.Pclass == 1) & ((train_data).Fare.isnull())] = median_fare_class_1\ntrain_data['Fare'][(train_data.Pclass == 2) & ((train_data).Fare.isnull())] = median_fare_class_2\ntrain_data['Fare'][(train_data.Pclass == 3) & ((train_data).Fare.isnull())] = median_fare_class_3\ntest_data['Fare'][(test_data.Pclass == 1) & ((test_data).Fare.isnull())] = median_fare_class_1\ntest_data['Fare'][(test_data.Pclass == 2) & ((test_data).Fare.isnull())] = median_fare_class_2\ntest_data['Fare'][(test_data.Pclass == 3) & ((test_data).Fare.isnull())] = median_fare_class_3\n'''\n\n\n# Replace NULL Values in feature \"Embarked\" --> S (Southampton) as the most common embarkment place\nall_data.groupby(['Embarked'])['Embarked'].count()\nall_data['Embarked'].fillna(\"S\", inplace=True)\ntrain_data['Embarked'].fillna(\"S\", inplace=True)\ntest_data['Embarked'].fillna(\"S\", inplace=True)\n\n\n\n# Check datasets for NULL values --> All NULL Values were replaced (besides \"Cabin\" feature) \nprint('Number of NULL Values after filling:')\nprint(all_data.isnull().sum())","179e4af8":"# Analyse Cabin Feature --> Percentage of NULL values\nnull = round((all_data['Cabin'].isnull().sum()) \/ (all_data['PassengerId'].count()) * 100,2)\nprint('Percentage of NULL value in feature \"Cabin\": ' + str(null) + ' %')\nprint(\"\\n\")\n\n# Analyse Cabin Feature --> Cabin types --> There are multiple Cabin entries with multiple cabins --> Difficult to work with\nprint(all_data['Cabin'].unique())","54291943":"# Check duplicates based on all features --> Sum = 0 --> No duplicates found\nduplicates = all_data.duplicated()\nprint('Number of duplicates based on all features: ' + str(duplicates.sum()))\nprint()\n\n# Check duplicates based on the feature \"Name\" --> Sum = 2 --> found 2x2 Passengers with identical names \nduplicates = all_data.duplicated(['Name'])\nprint('Number of duplicates based on the feature \"Name\": ' + str(duplicates.sum()))\nprint()\n\nprint('Duplicate records with all related records in the dataset: ')\nprint()\nprint (all_data[duplicates])\nprint(\"\\n\")\nprint(all_data[all_data['Name'] == 'Kelly, Mr. James'])\nprint(\"\\n\")\nprint(all_data[all_data['Name'] == 'Connolly, Miss. Kate'])","c5eeb909":"# Create Age Class --> Create classes with encoded values for 'Child','Youth','YoungAdult','MiddleAged','Senior'\n#age_bins = [0,10,18,30,60,100]\n#age_cat_train = pd.cut(train_data['Age'], age_bins, labels=[1,2,3,4,5])\n#age_cat_test = pd.cut(test_data['Age'], age_bins, labels=[1,2,3,4,5])\n#train_data['Age_Cat'] = age_cat_train\n#test_data['Age_Cat'] = age_cat_test\n\n# Create Age Class --> Create classes with encoded values for 'Baby',Child','Youth','YoungAdult','MiddleAged','Senior'\nage_bins = [0,1,10,18,30,60,100]\nage_cat_train = pd.cut(train_data['Age'], age_bins, labels=[1,2,3,4,5,6])\nage_cat_test = pd.cut(test_data['Age'], age_bins, labels=[1,2,3,4,5,6])\ntrain_data['Age_Cat'] = age_cat_train\ntest_data['Age_Cat'] = age_cat_test\n\n\n\n# Create Fare Class --> Create 4 classes with same number of samples \n# (Note: I suppose this needs a rework, since bins size are different between train and test dataset)\nfare_cat_train = pd.qcut(train_data['Fare'], 3, labels=[1,2,3])\nfare_cat_test = pd.qcut(test_data['Fare'], 3, labels=[1,2,3])\ntrain_data['Fare_Cat'] = fare_cat_train\ntest_data['Fare_Cat'] = fare_cat_test\n\n\n\n# Create Familiy Size\ntrain_data['FamSize'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['FamSize'] = test_data['SibSp'] + test_data['Parch'] + 1\n\n\n\n# Create Family Size Class\ntrain_data['FamSize_Cat'] = np.nan\ntrain_data['FamSize_Cat'][train_data['FamSize'] == 1] = 1\ntrain_data['FamSize_Cat'][(train_data['FamSize'] > 1) & (train_data['FamSize'] < 5)] = 2\ntrain_data['FamSize_Cat'][train_data['FamSize'] > 4] = 3 \ntrain_data['FamSize_Cat'] = train_data['FamSize_Cat'].astype('int')\ntest_data['FamSize_Cat'] = np.nan\ntest_data['FamSize_Cat'][test_data['FamSize'] == 1] = 1\ntest_data['FamSize_Cat'][(test_data['FamSize'] > 1) & (test_data['FamSize'] < 5)] = 2\ntest_data['FamSize_Cat'][test_data['FamSize'] > 4] = 3 \ntest_data['FamSize_Cat'] = test_data['FamSize_Cat'].astype('int')\n\n\n\n# Create Titel\ntrain_data['Title'] = train_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain_data['Title'] = train_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev','Sir', 'Jonkheer', 'Dona'], 'Other')\ntrain_data['Title'] = train_data['Title'].replace('Mlle', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Ms', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Mme', 'Mrs')\ntest_data['Title'] = test_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_data['Title'] = test_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev','Sir', 'Jonkheer', 'Dona'], 'Other')\ntest_data['Title'] = test_data['Title'].replace('Mlle', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Ms', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')\n\n\n# Create CabinExist\ntrain_data['CabinExist'] = np.nan\ntrain_data['CabinExist'][train_data['Cabin'].isnull()] = 0\ntrain_data['CabinExist'][train_data['Cabin'].notnull()] = 1\ntest_data['CabinExist'] = np.nan\ntest_data['CabinExist'][test_data['Cabin'].isnull()] = 0\ntest_data['CabinExist'][test_data['Cabin'].notnull()] = 1\n","2f4f888d":"# Scaling of numerical features by max-min scaling\ntrain_data_age_max = train_data['Age'].max()\ntrain_data_age_min = train_data['Age'].min()\ntrain_data_fare_max = train_data['Fare'].max()\ntrain_data_fare_min = train_data['Fare'].min()\n\ntrain_data['Age'] = (train_data['Age'] - train_data_age_min) \/ (train_data_age_max - train_data_age_min)\ntrain_data['Fare'] = (train_data['Fare'] - train_data_fare_min) \/ (train_data_fare_max - train_data_fare_min)\ntest_data['Age'] = (test_data['Age'] - train_data_age_min) \/ (train_data_age_max - train_data_age_min)\ntest_data['Fare'] = (test_data['Fare'] - train_data_fare_min) \/ (train_data_fare_max - train_data_fare_min)\n\n\n\n# Encoding feature \"Embarked\" (Label Encoding)\n#train_data['Embarked'].replace(['S','C','Q'],[1,2,3],inplace=True)\n#test_data['Embarked'].replace(['S','C','Q'],[1,2,3],inplace=True)\n\n# Encoding feature \"Embarked\" (One Hot Encoding)\ndummies = pd.get_dummies(train_data['Embarked'], prefix='Embarked')\ntrain_data = train_data.join(dummies)\ndummies = pd.get_dummies(test_data['Embarked'], prefix='Embarked')\ntest_data = test_data.join(dummies)\n\n\n\n# Encoding feature \"Sex\" (Label Encoding)\n#train_data['Sex'].replace(['male','female'],[0,1],inplace=True)\n#test_data['Sex'].replace(['male','female'],[0,1],inplace=True)\n\n# Encoding feature \"Sex\" (One Hot Encoding)\ndummies = pd.get_dummies(train_data['Sex'], prefix='Sex')\ntrain_data = train_data.join(dummies)\ndummies = pd.get_dummies(test_data['Sex'], prefix='Sex')\ntest_data = test_data.join(dummies)\n\n\n\n# Encoding feature \"Title\" (One Hot Encoding\ndummies = pd.get_dummies(train_data['Title'], prefix='Title')\ntrain_data = train_data.join(dummies)\ndummies = pd.get_dummies(test_data['Title'], prefix='Title')\ntest_data = test_data.join(dummies)\n\n\n# Adapting data types\ntrain_data['Age_Cat'] = pd.to_numeric(train_data['Age_Cat'], downcast='integer')\ntrain_data['Fare_Cat'] = pd.to_numeric(train_data['Fare_Cat'], downcast='integer')\ntrain_data[\"CabinExist\"] = train_data[\"CabinExist\"].astype(\"int\")  \ntest_data['Age_Cat'] = pd.to_numeric(test_data['Age_Cat'], downcast='integer')\ntest_data['Fare_Cat'] = pd.to_numeric(test_data['Fare_Cat'], downcast='integer')\ntest_data[\"CabinExist\"] = test_data[\"CabinExist\"].astype(\"int\") ","0ed52f6f":"# Create Training \/ Test Dataset\n#CLF_train_data = train_data.drop(columns = ['PassengerId', 'Name', 'Cabin','Ticket','Embarked', 'Sex', 'Title','CabinExist'])\n#CLF_test_data = test_data.drop(columns = ['PassengerId', 'Name', 'Cabin','Ticket','Embarked', 'Sex', 'Title','CabinExist'])\n\n# Create Training \/ Test Dataset --> Best set of features with 82.1 % accuracy (KNN wit default parameters settings)\nCLF_train_data = train_data[['Survived','Age','Pclass','Sex_female','FamSize','Fare_Cat','Title_Master']]\nCLF_test_data = test_data[['Age','Pclass','Sex_female','FamSize','Fare_Cat','Title_Master']]\n\n\n\n\n# Print datasets for final check\nprint(CLF_train_data.head(10))\nprint(\"\\n \\n \\n\")\nprint(CLF_train_data.info())\nprint(\"\\n \\n \\n\")\nprint(CLF_test_data.head(10))\nprint(\"\\n \\n \\n\")\nprint(CLF_test_data.info())\n","c04de8a9":"# Dataset Preparation\nx_train = CLF_train_data.drop(columns=['Survived'])\ny_train = CLF_train_data['Survived']\n\n\n# Train-Test Split (75% Train, 25% Test)\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25,random_state=1)\n\n\n# Create & Train Model with default parameter (k=5, weight = uniform)\nCLF = KNeighborsClassifier()\nCLF.fit(x_train,y_train)\n\n\n# Create prediction for test data split\ny_train_pred = CLF.predict(x_train)\n\n\n# Create prediction for test data split\ny_test_pred = CLF.predict(x_test)\n\n\n# Accuracy on train data split\nmodel_val_accuracy = ((accuracy_score(y_train, y_train_pred).round(3))*100).round(1)\nprint('Accuracy Train Split: ' + str(model_val_accuracy))\n\n\n# Accuracy on test data split\nmodel_val_accuracy = ((accuracy_score(y_test, y_test_pred).round(3))*100).round(1)\nprint('Accuracy Test Split: ' + str(model_val_accuracy))\n","2cd3f088":"# Dataset Preparation\nx_train = CLF_train_data.drop(columns=['Survived'])\ny_train = CLF_train_data['Survived']\n\n\n# Train-Test Split (75% Train, 25% Test)\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25,random_state=1)\n\n\n# Define parameter values\nknn_n_neighbors = list(range(1,20))\nknn_weights = ['uniform','distance']\nknn_algorithm = ['ball_tree','kd_tree','brute'] \nknn_metric = ['euclidean','manhattan','chebyshev','minkowski']\nknn_grid = dict(n_neighbors = knn_n_neighbors, weights = knn_weights, algorithm = knn_algorithm, metric = knn_metric)\n\n\n# Create and Train Model with hyperparameter optimization with standard 10-fold cross validation. Score = Accuracy\nCLF = KNeighborsClassifier()\nCLF_Grid = GridSearchCV(CLF, knn_grid, scoring = 'accuracy', cv=10)\nCLF_Grid.fit(x_train, y_train)\n\n\n# Results of hyperparameter optimization\n#print(CLF_Grid.cv_results_)\nprint('\\n')\nprint ('Best Score: ' + str(CLF_Grid.best_score_))\nprint('\\n')\nprint ('Best Parameters: ' + str(CLF_Grid.best_params_))\nprint('\\n')\nprint ('Best Estimators: ' + str(CLF_Grid.best_estimator_))\nprint('\\n')\n\n\n# Create prediction for test data split\ny_train_pred = CLF_Grid.predict(x_train)\n\n\n# Create prediction for test data split\ny_test_pred = CLF_Grid.predict(x_test)\n\n\n# Accuracy on train data split\nmodel_val_accuracy = ((accuracy_score(y_train, y_train_pred).round(3))*100).round(1)\nprint('Accuracy Train Split: ' + str(model_val_accuracy))\n\n\n# Accuracy on test data split\nmodel_val_accuracy = ((accuracy_score(y_test, y_test_pred).round(3))*100).round(1)\nprint('Accuracy Test Split: ' + str(model_val_accuracy))\n\n\n# Model CLF = CLF_Grid\nCLF = CLF_Grid","76615c13":"# Create prediction for titanic test dataset\ny_test_pred = CLF.predict(CLF_test_data)\n\n\n# Create submission file\ncol_survived = pd.Series(y_test_pred, name = 'Survived')\nresult = pd.DataFrame()\nresult['PassengerId'] = test_data['PassengerId']\nresult['Survived'] = col_survived\n\nresult.to_csv('submission.csv', index=False)\n\n# Show results\nprint(result.head(10))\nprint('\\n')\n\n# Show how many passengers are classified as \"Survived = 0\" or \"Survived = 1\" \nprint('Show how many passengers are classified as \"Survived = 0\" or \"Survived = 1\"')\nprint((result.groupby([\"Survived\"])[\"Survived\"].count()\/418).round(2))","e89a1b3f":"## 4.3 Relationship between Features\n\nThe correlation matrix shows the pairwise correlations for passenger feature vs. passenger feature and for passenger feature vs. outcome (survived n\/y)\n\nThe noticable correlations are:\n\n\n- Outcome vs. Sex --> Female passengers tend to have a higher surival chance than male passengers. \n- Outcome vs. Passenger Class --> Passengers of a higher class tend to have a higher survival chance\n- Passenger Class vs. Age --> Older passengers tend to belong to a higher passenger class\n- Passenger Class vs. Ticket Fare --> Higher class tickets are more expensive (Makes sense :-D)\n\n","5a73db68":"# 2. Data Description\n\n## 2.1 Complete Dataset\n\nThe complete dataset contains 1309 passengers (891 training dataset, 418 test dataset) with 12 features inclusive the outcome feature. \n\n- The dataset includes the outcome \"Survived\"\n- The dataset includes 5 catgeorical features (Name, Sex, Ticket, Cabin, Embarked)\n- The dataset includes 6 numerical features (PassengerId, Pclass, Age, SibSp, Parch, Fare)\n- The following features have NULL values:\n    - Age\n    - Cabin\n    - Fare\n    - Embarked\n    \n- The features Name, Cabin and PassengerId have a high variability","dbcae327":"## 2.3 Test  Dataset\n\nThe test dataset contains 418 passengers with 11 features \n\nThe following features have NULL values:\n- Age\n- Cabin\n- Fare","fecc0cfb":"## 5.3 Outlier Data Management\n\nThe outlier data management is not part of this notebook's version. I will add this asap.","ed08e48e":"# 5. Data Cleansing\n\n\n## 5.1 Missing Data Management\n\nThe data set (1309 passengeers) has missing data in the following features:\n\n- Age (263 NULL Values)\n- Fare (1 NULL Values)\n- Cabin (1014 NULL Values)\n- Embarked (2 NULL Values)\n\nThe missing values for \"Age\", \"Fare\" and \"Embarked\" have been replaced by standard values (by most common or median value). \n\n- Age by median age of all passengers of the same passenger class (best estimator for age based on correlation matrix)\n- Fare by median fare of all passengers of the same passenger class (best estimator for age based on correlation matrix)\n\nFurthrmore, I have analysed the feature \"Cabin\" in detail to evaluate, whether it makes sense to refill the NULL values or not.\n\n- 77.46 % of NULL values\n- there are multiple entries with multiple cabins\n\nSo I decided to not fill the NULL values due to the few true known values.","7163feb1":"## 2.2 Training  Dataset\n\nThe training dataset contains 891 passengers with 12 features inclusive the outcome feature \n\nThe following features have NULL values:\n- Age\n- Cabin\n- Embarked\n","39d0d0b7":"# 3. Data Distribution\n\nThe following plots compare the distribution of the features\n\n- Age\n- Sex\n- Class\n- Fare\n- Number of Siblings\/Spouses\n- Number of Parents\/Children\n\nfor the datasets\n\n- Complete Dataset\n- Training Dataset\n- Test Dataset\n\nThe plots show, that the distributions in the training and test dataset are similar. There are no noticeable differences.","8f640113":"# 8. Summary\n\n\n## General notes\n\nFor the following comparison, I have used the accuracy score for the KNN model based on the test data splitt of the training dataset (train_test_split(x_train,y_train,test_size=0.25,random_state=1).\n\n\n## 8.1 Notes to  Version 1\n\n\n**Within this version, I have adapted the model step by step as follows:**\n\n\n| Step                                                                        | Accuracy Score |\n|-----------------------------------------------------------------------------|----------------|\n| No Scaling for numerical features; label encoding for categorical features  | **<70 %**      |\n| + with scaling for numerical features                                       | **78.0 %**     |\n| + improved fill of NULL values for \"Age\" and \"Fare\" (median instead mean)   | **78.9 %**     |\n| + with one hot endcoding for nominal scaled features                        | **79.4 %**     |\n| + all feature data types to numerical data types                            | **79.4 %**     |\n\n<br>\n\nThe best k-Nearest Neighbor classification model has an accuracy of **79.4 %** based on the training dataset.\n\nThe same model achieved an accuracy of **74.4 %** for the test dataset.\n\n<br>\n\n**Next adaptions in Version 2 could be:**\n\n- KNN k & weights hyperparameter optimization\n- Oversampling for the class \"Survived = 1\"\n- Add one hot endcoding for ordinal scaled features\n- Add Outlier Data Management\n- Improve filling of NULL values for the features \"Age\" and \"Fare\"\n- Create a feature \"Titel\" based on the \"Names\" feature; fill NULL values\n- Create a feature \"Deck\" based on the \"Cabin\" feature; fill NULL values\n- Create a better set of features (based on the importance of the features on the outcome)\n\n<br>\n\n\n## 8.2 Notes to Version 2 \n\n**Within this version, I have adapted the model step by step as follows:**\n\n| Step                                                                        | Accuracy Score |\n|-----------------------------------------------------------------------------|----------------|\n| Version 1                                                                   | **79.4 %**     |\n| + improved filling of the feature \"Age_Cat\" (new category: Baby)            | **79.4 %**     |\n| + improved filling of the feature \"Age\" (group by Pclass) ROLLBACK!         | **76.2 %**     |\n| + improved filling of the feature \"Fare\" (group by Pclass) ROLLBACK!        | **76.2 %**     |\n| + add new feature \"Title\"                                                   | **79.4 %**     |\n| + add new feature \"CabinExist\"                                              | **78.5 %**     |\n\n<br>\n\nAs you can see, the further data prepartions were not really ssuccessfull. I couldn't improve the model.\n\nSo I deciced to go ahead with the following steps:\n\n- Feature Selection Optimization. Find the best set of features by manual testing of all created features (see chapter 6.2).\n- KNN Hyperparameter Optimization. Find the best parameter settings. \n\n| Step                                                                        | Accuracy Score |\n|-----------------------------------------------------------------------------|----------------|\n| + improved feature selection (by manual process)                            | **82.1 %**     |\n| + KNN hyperparameter optimization *                                         | **80.7 %**     |\n\n<br>\n\n\n*This is probably the best model (although score above is better) because the model parameters were optimized and the model accuracy was validated by a 10-fold cross validation which has a high reliability. \nSo I choose this model for the next survival prediction of the test dataset. \n\nThe best k-Nearest Neighbor classification model has an accuracy of **80.7 %** based on the training dataset (test split).\n\nThe same model achieved an accuracy of **75.6 %** for the test dataset.\n\n<br>\n\n**Next adaptions in Version 3 could be:**\n\n- Oversampling for the class \"Survived = 1\"\n- Add one hot endcoding for ordinal scaled features\n- Add Outlier Data Management\n\n<br>","c386e6fd":"# Titanic - my first dive into ML classification (Version 2)\n","f1e45a9a":"# 7. Classification\n\n\n## 7.1 k-Nearest Neighbor - with default parameters","d0cd8432":"## 7.2 k-Nearest Neighbor - with hyperparameter optimization","fb412e95":"## 4.2 Survival Rate vs. Features\n\nThe following plots show the survival rate for specific groups of passenger based on the features  \n\n- Age\n- Sex\n- Class\n- Number of Siblings\/Spouses\n- Number of Parents\/Children\n- Embarked\n\nfor the training dataset.\n\nYou can see that survival rate is higher for\n\n- female passengers vs. male passengers\n- passengers in higher classes vs. lower classes\n- babies and children vs. older people\n\nThe lowest survival rate is conntected to male passengers in the 3rd class.","887fb285":"## 6.2 Feature Transformation\n\n- Scaling of numerical features\n- Encoding of categorical features\n- Adapting data types","bb71a672":"## 6.2 Feature Selection\n\nCreate the final training and test datasets with a specifc set of features. \n\n**Training and Test Dataset includes the following features:**\n\n- Age (scaled)\n- Age_Cat\n- Sex (one hot encoded)\n- Pclass\n- Embarked (one hot encoded)\n- Fare (scaled)\n- Fare_Cat\n- SibSp\n- Parch\n- FamSize\n- FamSize_Cat\n- Title (one hot encoded)\n- CabinExist\n\n\n**The following features wont be used for the models:**\n\n- PassengerId (very high variability)\n- Name (very high variability)\n- Cabin (too many NULL values)\n- Ticket (very high variability)","2b81998d":"## 5.2 Dublicate Data Management\n\nThe complete dataset is checked for possible duplicates: \n\n- Duplicates based on all features\n- Duplicates based on the \"Name\" feature\n\nThe duplicate check shows that there are 2x2 Passengers with identical names. \nFurther investigation shows that only the persons names are identical, the person themselves are different.","1a44acf1":"# 9. Submission\n\nLast but not least, we create the prediction for the Titanic test dataset and save the prediction in the submission csv file.","488365a1":"**Some initial words**\n\nI'm pretty new to Machine Learning and this is my first Machine Learning project ever.\nSo I really appreciate any suggestions to improve my knowledge regarding data handling, machine learning methods and Python programming.\n\nI will start with the k-Nearest Neighbour classificator and try to adapt this notebook continuously (add more classification methods, improve data preparation) to see which steps influence and improve the prediction score.\n\n\n**Objective**\n\nThe objective of this analysis is to predict the survival of passengers of the Titanic disaster based on a trained machine learning classification model.\n\n\n**Datasets**\n\nThe data has been split into two groups:\n\n- training set (train.csv)\n- test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n**Features**\n\nThe datsets include the following features:\n\n- PassengerId : unique id number to each passenger\n- Survived : passenger survive(1) and died(0)\n- Pclass : passenger class\n- Name : name of passenger\n- Sex : gender of passenger\n- Age : age of passenger\n- SibSp : number of siblings\/spouse\n- Parch : number of parent\/children\n- Ticket : ticket number\n- Fare : amount of money spent on ticket\n- Cabin : cabin category\n- Embarked : port where passenger embarked (C= Cherbourg, Q=Quenntown, S = Southampton)\n\nThe test dataaset did not include the \"Survived\" feature.\n\n**Feature Notes**\n\n- Pclass: A proxy for socio-economic status \n    - 1st = Upper \n    - 2nd = Middle\n    - 3rd = Lower\n- Age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n- SibSp: The dataset defines family relations in this way...\n    - Sibling = brother, sister, stepbrother, stepsister\n    - Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n- Parch: The dataset defines family relations in this way...\n    - Parent = mother, father\n    - Child = daughter, son, stepdaughter, stepson\n    - Some children travelled only with a nanny, therefore parch=0 for them.","04c34258":"# 4. Data Exploration\n\n\n## 4.1 Survival Rate\n\nThe following plots show the survival rate based on the training dataset.\n\nThe survival rate in the training dataset is 38.4 %. \n\nIn addition, we see that there are more cases with the outcome \"Survived = 0\" than with the outcome \"Survived = 1\" within the training dataset. This means that our model may train the two outcomes with different levels of accuracy. Maybe an data oversampling for the outcome \"Survived = 1\" would be helpful to improve the models score.","11cc09ed":"# 6. Feature Engineering\n\n## 6.1 Feature Extraction\n\n- Create \"Age Class\" Feature\n- Create \"Fare Class\" Feature\n- Create \"Family Size\" Feature\n- Create \"Family Size Class\" Feature\n- Create \"Passenger Titel\" Feature\n- Create \"CabinExist\" Feature*\n\n*Of course It would be interesting to derive the cabin deck per passenger from the \"Cabin\" feature. However I suppose, it makes no sense due to the complexity and the lack of information due to the high number of NULL values. ","8e5b7fd8":"# 1. Data Import"}}