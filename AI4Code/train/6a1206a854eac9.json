{"cell_type":{"57e45f28":"code","e259198b":"code","35029693":"code","b5c3295b":"code","bdc4201c":"code","f0de723d":"code","9493655e":"code","cf19655d":"code","b763e6b4":"code","cd2a97b8":"code","ec2aa7fc":"code","5298f986":"code","66cea521":"code","3c4089e0":"code","8b7ecc62":"code","7dae5a37":"code","bfd17ed6":"code","5490f6ab":"code","2d7003fa":"code","5fed32ee":"code","ddb29a41":"code","85036d62":"code","5610371a":"code","ac8dc1dc":"code","f15189bc":"code","a6328ab4":"code","68b78516":"code","b4024e31":"code","9c5bfabf":"code","bcc38ecc":"code","99cd2f1e":"code","e6f2ea55":"code","a668a7ec":"code","9d2b60a4":"code","bb88ebb7":"code","b79fc8e0":"code","b855bdf2":"code","21e210ae":"code","29c9ccc8":"code","c7898de8":"code","6d8b8fc0":"markdown","a89e8c5a":"markdown","af72606a":"markdown","b94dabbc":"markdown","e7c972cc":"markdown","0aa51749":"markdown","83857962":"markdown","3a8ca5de":"markdown","6576d65c":"markdown","d7417c8e":"markdown","837f82a9":"markdown","07583e9c":"markdown","240ef438":"markdown","ac56fcb6":"markdown","f6f4a073":"markdown","15b2c6ba":"markdown","ca94f2a1":"markdown","e38ce8db":"markdown","b1ff551b":"markdown","327ed6f2":"markdown","8b325f76":"markdown"},"source":{"57e45f28":"!pip install d6tflow","e259198b":"import d6tflow\nimport luigi\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\n\nimport regex as re\nimport string\nimport spacy\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_validate","35029693":"class TaskGetData(d6tflow.tasks.TaskPqPandas): #save to parquet, load as pandas\n    persist=['df_train','df_test'] # declare what you will save\n    \n    def run(self):\n        df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n        df_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n        self.save({'df_train':df_train, 'df_test':df_test})","b5c3295b":"d6tflow.run(TaskGetData())","bdc4201c":"df_train = TaskGetData().output()['df_train'].load()\ndf_test = TaskGetData().output()['df_test'].load()","f0de723d":"df_train.head()","9493655e":"df_test.head()","cf19655d":"df_train.info()","b763e6b4":"print('% missing values')\nprint(df_train.apply(lambda x: x.isna().sum()\/len(x)))","cd2a97b8":"y=df_train.target.value_counts()\nsns.barplot([f'non-disaster({y[0]})',f'disaster:({y[1]})'],y,hue = y.index)\nplt.gca().set_ylabel('count')","ec2aa7fc":"df_train.groupby(['keyword','target']).size().unstack(1).fillna(0).sort_values([1]).plot(kind = 'barh',figsize=(20,80))","5298f986":"@d6tflow.requires(TaskGetData) # define upstream dependency\nclass TaskGetNGrams(d6tflow.tasks.TaskPqPandas):\n    \n    persist = ['df_disaster_ngrams','df_nondisaster_ngrams'] # must declare what to save if a single task has multiple outputs\n    n_gram = luigi.IntParameter(default=1) \n\n    def generate_ngrams(self,text, n_gram):\n        stop_words = set(stopwords.words('english'))\n        token = [token for token in text.lower().split(' ') if token != '' if token not in stop_words]\n        ngrams = zip(*[token[i:] for i in range(n_gram)])\n        return [' '.join(ngram) for ngram in ngrams]\n    \n    def run(self):\n        df_train = self.input()['df_train'].load() #load inputs from upstream dependency's outputs\n        df_test = self.input()['df_test'].load()\n        \n        DISASTER_TWEETS = df_train['target']==1\n        disaster_ngrams = defaultdict(int)\n        nondisaster_ngrams = defaultdict(int)\n        \n        for tweet in df_train[DISASTER_TWEETS]['text']:\n            for word in self.generate_ngrams(tweet,self.n_gram):\n                disaster_ngrams[word] += 1\n        \n        for tweet in df_train[~DISASTER_TWEETS]['text']:\n            for word in self.generate_ngrams(tweet,self.n_gram):\n                nondisaster_ngrams[word] += 1\n        \n        df_disaster_ngrams = pd.DataFrame(sorted(disaster_ngrams.items(), key=lambda x: x[1])[::-1],columns = ['term','count'])\n        df_nondisaster_ngrams = pd.DataFrame(sorted(nondisaster_ngrams.items(), key=lambda x: x[1])[::-1],columns = ['term','count'])\n        self.save({'df_disaster_ngrams':df_disaster_ngrams, 'df_nondisaster_ngrams':df_nondisaster_ngrams}) #save outputs,should match self.persist\n   ","66cea521":"@d6tflow.requires(TaskGetNGrams) #define upstream dependency\nclass TaskVisualize(d6tflow.tasks.TaskCache): #save to memory\n \n    def run(self):\n        df_disaster_ngrams = self.input()['df_disaster_ngrams'].load()\n        df_nondisaster_ngrams = self.input()['df_nondisaster_ngrams'].load()\n        fig, axs = plt.subplots(1, 2, figsize=(40,30),squeeze=False)\n        a = sns.barplot(df_disaster_ngrams['count'][:50],df_disaster_ngrams['term'][:50], ax = axs[0][0],color = 'b').set_title(f'Top 50 most common {self.n_gram}-gram in disaster tweets',fontsize=30)\n        b = sns.barplot(df_nondisaster_ngrams['count'][:50],df_nondisaster_ngrams['term'][:50], ax = axs[0][1],color = 'y').set_title(f'Top 50 most common {self.n_gram} in non-disaster tweets',fontsize=30)\n        plt.show()\n        \n    ","3c4089e0":"d6tflow.preview(TaskVisualize())","8b7ecc62":"d6tflow.run(TaskVisualize())","7dae5a37":"d6tflow.preview(TaskVisualize(n_gram = 2)) # change parameter","bfd17ed6":"d6tflow.run(TaskVisualize(n_gram = 2))","5490f6ab":"stemmer = SnowballStemmer(language='english')\nnlp = spacy.load(\"en_core_web_sm\")","2d7003fa":"@d6tflow.requires(TaskGetData) #define upstream dependency\nclass TaskPreprocess(d6tflow.tasks.TaskPqPandas):\n    persist=['df_train','df_test']\n    do_preprocess = luigi.BoolParameter(default = False)  # We will see if preprocessing really improves model performance\n    preprocess_method = luigi.Parameter(default = 'typical') # We will then add preprocessing especially for word embeddings\n    \n    def typical_preprocess(self,text):\n        # Make text lowercase, remove links, stem tokens,remove stop words, remove word of length <=1\n        \n        text = text.lower()\n        text = re.sub(r'http:\/\/\\S+|https:\/\/\\S+', '', text)\n        text = re.sub(r'www.\\S+\\.com','',text)\n        text = text.replace('...', '')\n        text = text.replace('..','')\n        text = text.replace(\"'s\",\"\")\n        text = [stemmer.stem(token.text) for token in nlp(text) if not (token.is_stop | token.is_punct | len(token.text)<=1)]\n        text = ' '.join(text)\n        return text\n    \n    # We can then add preprocessing for word embedding with :\n#     def word_embedding_preprocess(self,text):\n#         pass\n    \n    def run(self):\n        df_train = self.input()['df_train'].load()\n        df_test = self.input()['df_test'].load()\n        if self.do_preprocess:\n            if self.preprocess_method == 'typical':\n                df_train['text']=df_train['text'].apply(lambda x: self.typical_preprocess(x))\n                df_test['text']=df_test['text'].apply(lambda x: self.typical_preprocess(x))\n            elif self.preprocess_mehod == 'embedding':\n                pass\n        self.save({'df_train':df_train, 'df_test':df_test})  \n    ","5fed32ee":"d6tflow.invalidate_downstream(TaskGetNGrams(), TaskVisualize())\nd6tflow.invalidate_downstream(TaskGetNGrams(n_gram = 2), TaskVisualize(n_gram = 2))","ddb29a41":"d6tflow.preview(TaskVisualize())","85036d62":"d6tflow.preview(TaskVisualize(n_gram = 2))","5610371a":"# You don't need to rewrite all the codes, just change the upstream dependency.\n# I copy and paste the codes to ensure kaggle kernel run smoothly\n\n@d6tflow.requires(TaskPreprocess) # change upstream dependency \nclass TaskGetNGrams(d6tflow.tasks.TaskPqPandas):\n    \n    persist = ['df_disaster_ngrams','df_nondisaster_ngrams']\n    n_gram = luigi.IntParameter(default=1)\n\n    def generate_ngrams(self,text, n_gram):\n        stop_words = set(stopwords.words('english'))\n        token = [token for token in text.lower().split(' ') if token != '' if token not in stop_words]\n        ngrams = zip(*[token[i:] for i in range(n_gram)])\n        return [' '.join(ngram) for ngram in ngrams]\n    \n    def run(self):\n        df_train = self.input()['df_train'].load()\n        df_test = self.input()['df_test'].load()\n        \n        DISASTER_TWEETS = df_train['target']==1\n        disaster_ngrams = defaultdict(int)\n        nondisaster_ngrams = defaultdict(int)\n        \n        for tweet in df_train[DISASTER_TWEETS]['text']:\n            for word in self.generate_ngrams(tweet,self.n_gram):\n                disaster_ngrams[word] += 1\n        \n        for tweet in df_train[~DISASTER_TWEETS]['text']:\n            for word in self.generate_ngrams(tweet,self.n_gram):\n                nondisaster_ngrams[word] += 1\n        \n        df_disaster_ngrams = pd.DataFrame(sorted(disaster_ngrams.items(), key=lambda x: x[1])[::-1],columns = ['term','count'])\n        df_nondisaster_ngrams = pd.DataFrame(sorted(nondisaster_ngrams.items(), key=lambda x: x[1])[::-1],columns = ['term','count'])\n        self.save({'df_disaster_ngrams':df_disaster_ngrams, 'df_nondisaster_ngrams':df_nondisaster_ngrams})\n        \n@d6tflow.requires(TaskGetNGrams)\nclass TaskVisualize(d6tflow.tasks.TaskCache):\n    from matplotlib import pyplot as plt\n    import seaborn as sns\n \n    def run(self):\n        df_disaster_ngrams = self.input()['df_disaster_ngrams'].load()\n        df_nondisaster_ngrams = self.input()['df_nondisaster_ngrams'].load()\n        fig, axs = plt.subplots(1, 2, figsize=(40,30),squeeze=False)\n        a = sns.barplot(df_disaster_ngrams['count'][:50],df_disaster_ngrams['term'][:50], ax = axs[0][0],color = 'b').set_title(f'Top 50 most common {self.n_gram}-gram in disaster tweets',fontsize=30)\n        b = sns.barplot(df_nondisaster_ngrams['count'][:50],df_nondisaster_ngrams['term'][:50], ax = axs[0][1],color = 'y').set_title(f'Top 50 most common {self.n_gram} in non-disaster tweets',fontsize=30)\n        plt.show()","ac8dc1dc":"d6tflow.preview(TaskPreprocess())","f15189bc":"d6tflow.preview(TaskVisualize())","a6328ab4":"d6tflow.preview(TaskVisualize(do_preprocess = True))","68b78516":"d6tflow.run(TaskVisualize(do_preprocess = True))","b4024e31":"@d6tflow.requires(TaskPreprocess)\nclass TaskGetFeatures(d6tflow.tasks.TaskPickle):\n    persist = ['X_train','Y_train','X_test']\n    feature = luigi.Parameter(default = 'tf-idf')\n    n_gram_range = luigi.Parameter(default = (1,1)) \n    \n    def run(self):\n        df_train = self.input()['df_train'].load()\n        df_test = self.input()['df_test'].load()\n        X_train, X_test = df_train['text'],df_test['text']\n        Y_train = df_train['target']\n        \n        if self.feature == 'tf-idf':\n            tfidf = TfidfVectorizer(ngram_range = self.n_gram_range)\n            X_train = tfidf.fit_transform(X_train)\n            X_test = tfidf.transform(X_test)\n        \n        self.save({'X_train': X_train, 'Y_train': Y_train, 'X_test':X_test})\n  ","9c5bfabf":"d6tflow.preview(TaskGetFeatures(do_preprocess = True))","bcc38ecc":"d6tflow.run(TaskGetFeatures(do_preprocess = True))","99cd2f1e":"d6tflow.preview(TaskGetFeatures(do_preprocess = True,n_gram_range = (2,2))) #only use bigrams","e6f2ea55":"d6tflow.run(TaskGetFeatures(do_preprocess = True,n_gram_range = (2,2))) # only use bigrams","a668a7ec":"@d6tflow.requires(TaskGetFeatures)\nclass TaskTrain(d6tflow.tasks.TaskPickle):\n    model = luigi.Parameter(default = 'logistic')\n    \n    def run(self):\n        X_train = self.input()['X_train'].load()\n        Y_train = self.input()['Y_train'].load()\n        \n        if self.model == 'logistic':\n            model = LogisticRegression()\n        \n        elif self.model == 'svm':\n            model = SVC()\n        \n        elif self.model == 'KNN':\n            model = KNeighborsClassifier(2)\n        else:\n            raise ValueError('invalid model selection')\n        \n        model.fit(X_train,Y_train)\n        self.save(model) \n","9d2b60a4":"logistic_model = TaskTrain(do_preprocess=True, model='logistic' , n_gram_range = (1,1))\nd6tflow.preview(logistic_model)","bb88ebb7":"X_train = TaskGetFeatures(do_preprocess = True).output()['X_train'].load() # Load features from default feature engineering process\nY_train = TaskGetFeatures(do_preprocess = True).output()['Y_train'].load()\nX_test = TaskGetFeatures(do_preprocess = True).output()['X_test'].load()","b79fc8e0":"#unigram\n#logistic model\nlogistic_model = TaskTrain(do_preprocess=True, model='logistic')\nd6tflow.run(logistic_model)\nlogistic_model = logistic_model.output().load()\n\n#svm model\nsvm_model = TaskTrain(do_preprocess = True, model='svm')\nd6tflow.run(svm_model)\nsvm_model = svm_model.output().load()\n\n#KNN model\nKNN_model = TaskTrain(do_preprocess = True, model='KNN')\nd6tflow.run(KNN_model)\nKNN_model = KNN_model.output().load()\n\nprint('Insample accuracy ')\nprint('logistic_model: ', sklearn.metrics.accuracy_score(Y_train,logistic_model.predict(X_train))) \nprint('svm_model: ', sklearn.metrics.accuracy_score(Y_train,svm_model.predict(X_train)))\nprint('KNN_model: ', sklearn.metrics.accuracy_score(Y_train,KNN_model.predict(X_train)))","b855bdf2":"models = {'logistics' : logistic_model,\n         'svm' : svm_model,\n         'KNN' : KNN_model\n         }\n\nprint ('model  ','mean_accuracy   ', 'mean_f1   ')\nfor key, model in models.items():\n    scores = cross_validate(model, X_train, Y_train, scoring=('accuracy', 'f1'), cv=5)\n    print(f'{key}', scores['test_accuracy'].mean(),scores['test_f1'].mean())","21e210ae":"X_train = TaskGetFeatures(do_preprocess = True,n_gram_range = (2,2)).output()['X_train'].load() # Load features\nY_train = TaskGetFeatures(do_preprocess = True,n_gram_range = (2,2)).output()['Y_train'].load()\nX_test = TaskGetFeatures(do_preprocess = True,n_gram_range = (2,2)).output()['X_test'].load()","29c9ccc8":"#bigram\n#logistic model\nlogistic_model2 = TaskTrain(do_preprocess=True, model='logistic',n_gram_range = (2,2))\nd6tflow.run(logistic_model2)\nlogistic_model2 = logistic_model2.output().load()\n\n#svm model\nsvm_model2 = TaskTrain(do_preprocess = True, model='svm',n_gram_range = (2,2))\nd6tflow.run(svm_model2)\nsvm_model2 = svm_model2.output().load()\n\n#KNN model\nKNN_model2 = TaskTrain(do_preprocess = True, model='KNN',n_gram_range = (2,2))\nd6tflow.run(KNN_model2)\nKNN_model2 = KNN_model2.output().load()\n\nprint('Insample accuracy ')\nprint('logistic_model2: ', sklearn.metrics.accuracy_score(Y_train,logistic_model2.predict(X_train))) \nprint('svm_model2: ', sklearn.metrics.accuracy_score(Y_train,svm_model2.predict(X_train)))\nprint('KNN_model2: ', sklearn.metrics.accuracy_score(Y_train,KNN_model2.predict(X_train)))","c7898de8":"models = {'logistics2' : logistic_model2,\n         'svm2' : svm_model2,\n         'KNN2' : KNN_model2\n         }\n\nprint ('model  ','mean_accuracy   ', 'mean_f1   ')\nfor key, model in models.items():\n    scores = cross_validate(model, X_train, Y_train, scoring=('accuracy', 'f1'), cv=5)\n    print(f'{key}', scores['test_accuracy'].mean(),scores['test_f1'].mean())","6d8b8fc0":"Run the task and get execution summary","a89e8c5a":"First I will use tf-idf vectorization as features. We will see it is easy to change n-gram range and compare results with d6tflow.\nLater I will come back to this task, add feature conversion possibilities (count vectorization, word embbedings)","af72606a":"## N-grams Distribution\nN-grams can then form the bag-of-words features but I have no idea now whether I should use uni-grams or bi-grams. The process of getting n-grams can be long-reruning and therefore I decide to write it as a d6tflow task to easily reload outputs from this task later.\n","b94dabbc":"### models use only unigrams","e7c972cc":"# **2. Keyword and Target**","0aa51749":"# **3. N-grams Visualization**","83857962":"# **5. Feature Engineering**","3a8ca5de":"# **1. Install d6tflow and Import packages**","6576d65c":"Preview a task. See if upstream dependencies is pending or complete. If a task is labeled as complete, d6tflow will not rerun it. Otherwise, all the non-complete upstream tasks will be run.","d7417c8e":"# **6. Easy Model Comparison with d6tflow**","837f82a9":"# **0. Introduction of d6tflow**","07583e9c":"Load task outputs","240ef438":"## Target Class Distribution","ac56fcb6":"Cross Validation: easy model comparison","f6f4a073":"### models use only bigrams","15b2c6ba":"## Keywords Distribution","ca94f2a1":"Write the fist task for loading original data","e38ce8db":"Notice we should change TaskGetNGrams()'s upstream dependency to TaskPreprocess(). Remember to invalidate and rerun it after change or otherwise d6tflow will not rerun it as it is marked as complete.","b1ff551b":"[d6tflow](https:\/\/github.com\/d6t\/d6tflow) is a data science workflow manager built on top of luigi but unlike luigi it is optimized especially for data science workflows. \n\nThis competition project is a good example for why you should use d6tflow and how you can use d6tflow to optimize your data science work. In a text classification NLP project like this, you will experment a lot with different feature engineering ideas. Before you try, you don't know what kind of features (bag-of-words or word embeddings, uni-gram or n-gram, preprocessed or not), paired with which models, will result in the best performance. However, with all the experiments and going back and forth, your codes quickly get messy and difficult to audit. This is when d6tflow comes to rescue! \n\nYou will find this notebook helpful if you are looking for a way to make your data science workflow clear and intuitive. You can check out d6tflow library here: https:\/\/github.com\/d6t\/d6tflow.\n\nThis kernel includes EDA ideas from [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert) by [@Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan). \n","327ed6f2":"## %Missing Values","8b325f76":"# **4. Text Preprocessing**"}}