{"cell_type":{"84a3ab73":"code","0579d780":"code","6b9df994":"code","c8f92e7c":"code","fe51eb00":"code","dc9b7aa0":"code","569f721f":"code","b822ca52":"code","3892e6a4":"code","4d8a0cf7":"code","d229a551":"code","220f2bb3":"code","d4b98d6f":"code","bbcdd8c6":"code","d2e0b8b6":"code","bb7395be":"code","0c33e792":"code","de147286":"code","409b34c5":"code","072a386c":"markdown","6fde27d2":"markdown","071f4d02":"markdown","739824fb":"markdown","e6ba10a7":"markdown","39a33bba":"markdown"},"source":{"84a3ab73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0579d780":"df_sale = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ndf_calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\ndf_price = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')","6b9df994":"from fbprophet import Prophet\ncolumns = df_sale.columns\ndate_columns = columns[columns.str.contains(\"d_\")]\ndates_s = [pd.to_datetime(df_calendar.loc[df_calendar['d'] == str_date,'date'].values[0]) for str_date in date_columns]\n\ntmp = df_sale[date_columns].sum()\nignore_date = df_calendar[df_calendar['d'].isin(tmp[tmp < 10000].index.values)]['date'].values\n\ndf_ev_1 = pd.DataFrame({'holiday': 'Event 1', 'ds': df_calendar[~df_calendar['event_name_1'].isna()]['date']})\ndf_ev_2 = pd.DataFrame({'holiday': 'Event 2', 'ds': df_calendar[~df_calendar['event_name_2'].isna()]['date']})\ndf_ev_3 = pd.DataFrame({'holiday': 'snap_CA', 'ds': df_calendar[df_calendar['snap_CA'] == 1]['date']})\ndf_ev_4 = pd.DataFrame({'holiday': 'snap_TX', 'ds': df_calendar[df_calendar['snap_TX'] == 1]['date']})\ndf_ev_5 = pd.DataFrame({'holiday': 'snap_WI', 'ds': df_calendar[df_calendar['snap_WI'] == 1]['date']})\nholidays = pd.concat((df_ev_1, df_ev_2, df_ev_3, df_ev_4, df_ev_5))\n\ndef CreateTimeSeries(id):\n    item_series =  df_sale[df_sale['id'] == id]\n    dates = pd.DataFrame({'ds': dates_s}, index=range(len(dates_s)))\n    dates['y'] = item_series[date_columns].values.transpose()\n    # Remove chirstmas date\n    #dates = dates[~dates['ds'].isin(ignore_date)]\n    # Remove zero day\n    #dates = dates[dates['y'] > 0]        \n    return dates","c8f92e7c":"#%%time\n\n#df_price_ext = df_price.merge(df_calendar[['date','wm_yr_wk','d']], on='wm_yr_wk', how='left')\n#df_price_ext['id'] = df_price_ext['item_id'] + '_' + df_price_ext['store_id'] + '_validation'\n#df_price_ext = df_price_ext.pivot(index='id', columns='d', values='sell_price').reset_index()\n#df_price_ext = df_price_ext.fillna(0)","fe51eb00":"#last_month = list(date_columns[-28:])\n#df_sale_lastmonth = df_sale[['id'] + last_month].set_index('id')\n#df_price_lastmonth = df_price_ext[['id'] + last_month].set_index('id')\n#for col in last_month:\n#    df_sale_lastmonth[col] = df_sale_lastmonth[col].mul(df_price_lastmonth[col], axis=0)","dc9b7aa0":"#forecast with prophet without holiday\nids1 = np.array(['FOODS_3_090_WI_3_validation','FOODS_3_785_CA_1_validation'])\n#forecast with prophet with holiday\nids2 = np.array(['FOODS_3_362_CA_3_validation'])\n#forecast with prophet top down\nids3 = df_sale['id'].values\n#forecast with ma\nids4 = np.array(['FOODS_3_090_CA_3_validation'])\n\n#other predict\nids = np.concatenate((ids1, ids2, ids4), axis=0)\n#Default prophet top down\nids3 = ids3[~np.isin(ids3,ids)]\n\n\nprint(f'Prophet without: {ids1.shape[0]}, Prophet bu: {ids2.shape[0]}, Prophet td: {ids3.shape[0]}, MA : {ids4.shape[0]}')\nprint(f'Total ids: {ids1.shape[0] + ids2.shape[0] + ids3.shape[0] + ids4.shape[0]} , Total sample: {df_sale.shape[0]}')\n","569f721f":"# Check duplicate\nnp.setdiff1d(ids1,np.setdiff1d(ids1,ids2))","b822ca52":"%%time\n\n# Without holiday\n\ndef run_prophet(id):\n    timeserie = CreateTimeSeries(id)\n    model = Prophet(uncertainty_samples=False)\n    model.fit(timeserie)\n    forecast = model.make_future_dataframe(periods=28, include_history=False)\n    forecast = model.predict(forecast)\n    return np.append(np.array([id]),forecast['yhat'].values.transpose())\n\n\nfrom multiprocessing import Pool, cpu_count\nprint(f'Parallelism on {cpu_count()} CPU')\nwith Pool(cpu_count()) as p:\n    predictions1  = list(p.map(run_prophet, ids1))","3892e6a4":"%%time\n\n# With holiday\n\ndef run_prophet2(id):\n    timeserie = CreateTimeSeries(id)\n    model = Prophet(holidays = holidays, uncertainty_samples = False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.7)\n    # changepoint_prior_scale default is 0.5. Increasing it will make the trend more flexible    \n    model.fit(timeserie)\n    forecast = model.make_future_dataframe(periods=28, include_history=False)\n    forecast = model.predict(forecast)\n    return np.append(np.array([id]),forecast['yhat'].values.transpose())\n\n\nfrom multiprocessing import Pool, cpu_count\nprint(f'Parallelism on {cpu_count()} CPU')\nwith Pool(cpu_count()) as p:\n    predictions2  = list(p.map(run_prophet2, ids2))","4d8a0cf7":"%%time\n\ndf_sale_group_item = df_sale[np.hstack([['dept_id','store_id'],date_columns])].groupby(['dept_id','store_id']).sum()\ndf_sale_group_item = df_sale_group_item.reset_index()\n\ndef CreateTimeSeries3(dept_id, store_id):\n    item_series =  df_sale_group_item[(df_sale_group_item.dept_id == dept_id) & (df_sale_group_item.store_id == store_id)]\n    dates = pd.DataFrame({'ds': dates_s}, index=range(len(dates_s)))\n    dates['y'] = item_series[date_columns].values[0].transpose()     \n    return dates\n\ndef run_prophet3(dept_id, store_id):\n    timeserie = CreateTimeSeries3(dept_id, store_id)\n    # Tunned by one id\n    #model = Prophet(holidays = holidays, uncertainty_samples = False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.7)\n    # Tunned by level 9    \n    model = Prophet(holidays = holidays, uncertainty_samples = False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.7, seasonality_mode = 'multiplicative')\n    model.fit(timeserie)\n    forecast = model.make_future_dataframe(periods=28, include_history=False)\n    forecast = model.predict(forecast)\n    return np.append(np.array([dept_id,store_id]),forecast['yhat'].values.transpose())\n\n# create list param\nids = []\nfor i in range(0,df_sale_group_item.shape[0]):\n    ids = ids + [(df_sale_group_item[i:i+1]['dept_id'].values[0],df_sale_group_item[i:i+1]['store_id'].values[0])]\n\nprint(f'Parallelism on {cpu_count()} CPU')\nwith Pool(cpu_count()) as p:\n    predictions3  = list(p.starmap(run_prophet3, ids))","d229a551":"df_prophet_forecast_3 = pd.DataFrame()\nfor k in range(0, len(predictions3)):\n    dept_id = predictions3[k][0]\n    store_id = predictions3[k][1]\n\n    df_item = df_sale.loc[(df_sale.dept_id == dept_id) & (df_sale.store_id == store_id)][['id']]\n    df_item['val'] = df_sale[(df_sale.dept_id == dept_id) & (df_sale.store_id == store_id)].iloc[:, np.r_[0,-28:0]].sum(axis = 1)\n    for i in range(1,29):\n        df_item[f'F{i}'] = (df_item['val'] * float(predictions3[k][i+1]) \/ df_item['val'].sum())\n    df_prophet_forecast_3 = pd.concat([df_prophet_forecast_3, df_item])\n\ndf_prophet_forecast_3 = df_prophet_forecast_3.drop('val',axis=1)","220f2bb3":"df_sample = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","d4b98d6f":"# Forecast with MA\ndf_last_sale = df_sale.iloc[:, np.r_[0,-28:0]].melt('id', var_name='d', value_name='sale')\ndf_last_sale = df_last_sale.merge(df_calendar.loc[:,['d','date','wday']])\nlast_date = int(df_last_sale.d.max()[2:])\ndf_last_sale = df_last_sale.groupby(['id','wday'])['sale'].mean()\n\n\ndf_ma_forecast = df_sample.copy()\ndf_ma_forecast.columns = ['id'] + ['d_' + str(last_date + x) for x in range(1, 29)]\ndf_ma_forecast = df_ma_forecast.loc[df_ma_forecast.id.str.contains('validation')]\n\ndf_ma_forecast = df_ma_forecast.melt('id', var_name='d', value_name='sale')\ndf_ma_forecast = df_ma_forecast.drop('sale',axis = 1)\ndf_ma_forecast = df_ma_forecast.merge(df_calendar.loc[:,['d','date','wday']])\ndf_ma_forecast = df_ma_forecast.join(df_last_sale, on=['id', 'wday'])\n\ndf_ma_forecast = df_ma_forecast.pivot(index='id', columns='d', values='sale')\ndf_ma_forecast = df_ma_forecast.reset_index()","bbcdd8c6":"if len(ids4) > 0:\n    df_ma_forecast = df_ma_forecast[df_ma_forecast.id.isin(ids4)]\n    df_ma_forecast.columns = df_sample.columns\nelse:\n    df_ma_forecast = pd.DataFrame()\n\nif len(ids1) > 0:\n    df_prophet_forecast_1 = pd.DataFrame(predictions1)\n    df_prophet_forecast_1.columns = df_sample.columns\nelse:\n    df_prophet_forecast_1 = pd.DataFrame()\n\nif len(ids2) > 0:\n    df_prophet_forecast_2 = pd.DataFrame(predictions2)\n    df_prophet_forecast_2.columns = df_sample.columns\nelse:\n    df_prophet_forecast_2 = pd.DataFrame()\n\n\ndf_prophet_forecast_3.columns = df_sample.columns\ndf_prophet_forecast_3 = df_prophet_forecast_3[df_prophet_forecast_3.id.isin(ids3)]\n\n\ndf_sub_val = pd.concat([df_ma_forecast,df_prophet_forecast_1, df_prophet_forecast_2,df_prophet_forecast_3], sort=False)\n\ndf_sub_eval = df_sub_val.copy()\ndf_sub_eval['id'] = df_sub_eval['id'].str.replace(\"validation\", \"evaluation\")\n\ndf_sub = pd.concat([df_sub_val,df_sub_eval], sort=False)\ndf_sub = df_sub.sort_values('id')\n\n# Fix negative forecast\nnum = df_sub._get_numeric_data()\nnum[num < 0] = 0\n\ndf_sub.to_csv('submission.csv', index=False)\n\nprint(f'Submission shape: {df_sub.shape}')\nprint(f'Prophet without holiday: {df_prophet_forecast_1.shape[0]}, Prophet bu: {df_prophet_forecast_2.shape[0]}, Prophet td: {df_prophet_forecast_3.shape[0]}, MA: {df_ma_forecast.shape[0]}')","d2e0b8b6":"import matplotlib.pyplot as plt\nimport random\n%matplotlib inline\n\ndef plotForecast(item_id):\n    his_step = 100\n    plt.plot(dates_s[-his_step:] + [dates_s[-1:][0] + pd.DateOffset(days=x) for x in range(28)], np.append(df_sale[df_sale['id'] == item_id][date_columns].values[0][-his_step:],df_sub[df_sub['id'] == item_id].values[0][1:]))\n    plt.plot(dates_s[-his_step:], df_sale[df_sale['id'] == item_id][date_columns].values[0][-his_step:])\n    if item_id in ids1:\n        plt.title(f' Prophet without holiday forecast: {item_id}')    \n    elif item_id in ids2:\n        plt.title(f' Prophet bu forecast: {item_id}')\n    elif item_id in ids3:\n        plt.title(f' Prophet td forecast: {item_id}')\n    else:\n        plt.title(f' MA forecast: {item_id}')\n    plt.gcf().autofmt_xdate()","bb7395be":"if len(ids1)>0:\n    item_id = ids1[random.randint(0, len(ids1) - 1)]\n    plotForecast(item_id)","0c33e792":"if len(ids2)>0:\n    item_id = ids2[random.randint(0, len(ids2) - 1)]\n    plotForecast(item_id)","de147286":"if len(ids3)>0:\n    item_id = ids3[random.randint(0, len(ids3) - 1)]\n    plotForecast(item_id)","409b34c5":"if len(ids4)0:\n    item_id = ids4[random.randint(0, len(ids4) - 1)]\n    plotForecast(item_id)","072a386c":"# Prophet forecast","6fde27d2":"# Random check","071f4d02":"# Submission","739824fb":"# Introduction","e6ba10a7":"I have just joined this compete for a week. In my observation, almost of kernel which has better score using Light GBM with lag and aggregate feature. Some kernel used NN but not effective.\n\nLight GBM is good approach. There are many kernels forking, optimizing. In my opinion, it has not enough room to improve because of time and memory to train model limitation. I submit a new but very simple, clear and all in one approach. It is quick and easy to optimize because we can optimize per each series. Currently, it takes more than 3 hours on CPU to complete.\n\n* Why Prophet: It can detect abnormal, trend and seasonality very well. It support exogenous feature. \n* Why simple MA: There are many id, which have zero or low sale item in last day. So do not need complex algorithm. We need save time for other id forecast.\n* Why not ARIMA: I cannot choose suite orders (p, d, q) for all series. Cannot use Auto ARIMA due to runtime limitation\n\nBecause of WRMSSE evaluation, I tried algorithm selection by amount of sale but it is no better. I will try again later.\n\nI hope there are many kernel will fork, optimize this approach. So, I can learn more than from community. Please comment your opinion about this approach. It help me turning better. Thanks.\n\nEDA\n* https:\/\/www.kaggle.com\/binhlc\/m5-forecasting-accuracy-eda\n* https:\/\/www.kaggle.com\/binhlc\/prophet-hyperparameter-tuning\n\nMy deep learning approach (working only on GPU and not good score)\n* https:\/\/www.kaggle.com\/binhlc\/high-dimensional-time-series-forecasting-with-tf2\n\nReference\n* Time series with exogenous features https:\/\/mc.ai\/time-series-analysis-arima-or-prophet\/\n* https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html\n\nTo do:\n* Using exogenous feature (sale, event, store information)\n* Data cleaning\n* Turning both Prophet and MA (base on comparing result to Light GBM)\n* Optimize algorithm by series characteristic (algorithm selection)\n\nChange log:\n* Version 31: Reset algorithm\n* Version 30: Apply 4 algorithm\n* Version 28: Change MA by Prophet Top - Down with hyper parameter\n* Version 26: Rollback to MA, apply hyper parameter on prophet with holiday, reset manual tunning\n* Version 25: Change MA by Prophet Top - Down\n* Version 24: Turning by series\n* Version 23: Add manual select MA algorithm\n* Version 22: Change MA forecast by week day\n* Version 21: Fixed error on static ids list\n* Version 20: Turning by series\n* Version 18: Turning by series\n* Version 14: Add plot forecast for random id\n* Version 13: Apply changepoint_range to adjust trend. Increase Prophet forecast (est 5 hours to compleate)\n* Version 12: Reduce prophet forecast, apply uncertainty_samples\n* Version 11: Apply holiday and changepoint_prior_scale\n* Version 10: Keep noise on Prophet (for score)\n* Version 09: Rollback yearly_seasonality = True. Select algorithm by last 14 day sale\n* Version 08: Select algorithm by last 28 day sale\n* Version 07: Change yearly_seasonality = false and reduce prophet forecast, fix negative forecast\n* Version 06: Fixed MA forecast\n* Version 05: Remove noise (try with z score)\n* Version 04: Decrease prophet forecast\n* Version 03: Remove zero sale\n* Version 02: Turning model selection\n* Version 01: Apply simple MA \n* Version 00: Prophet predict\n","39a33bba":"# Simple MA forecast"}}