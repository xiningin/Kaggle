{"cell_type":{"8f0f08e2":"code","92cbe577":"code","f792c08f":"code","48a524b4":"code","b0f05b9f":"code","98602f14":"code","f193cc77":"code","eb81e88f":"code","2b4e13f9":"code","50d46457":"code","257898f7":"code","2b33a9d9":"code","3d577542":"code","feb52380":"code","23b47938":"code","fede6b89":"code","b1cf3e5d":"code","b9c6ab4c":"code","dcd6acef":"code","fe281871":"code","8f9134d7":"code","75634f5e":"code","55a01955":"code","d14025ee":"code","68976448":"markdown","5f362096":"markdown","7e229261":"markdown","87b6a77a":"markdown","fa1360a8":"markdown","5277b0d9":"markdown","2678d37b":"markdown","3dcf8f5c":"markdown","f4022c20":"markdown","3ab5fa05":"markdown","bb749e1f":"markdown","1b24c544":"markdown","f4d3dd01":"markdown","886ff4e7":"markdown","7dbd51ee":"markdown","5f9b0968":"markdown","0cc6f166":"markdown","45460d12":"markdown","c182bee9":"markdown"},"source":{"8f0f08e2":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob\nimport matplotlib.pyplot as plt\n%matplotlib inline","92cbe577":"potential_antivirals = ['abacavir', 'abacavir \/ dolutegravir \/ Lamivudine', 'abacavir \/ Lamivudine', 'abacavir \/ Lamivudine \/ Zidovudine', 'Acyclovir', 'adefovir', 'Amprenavir', 'asunaprevir', 'Atazanavir', 'Atazanavir \/ cobicistat', 'Baloxavir marboxil', 'bictegravir \/ emtricitabine \/ tenofovir alafenamide', 'boceprevir', 'brivudine', 'Cidofovir', 'cobicistat \/ darunavir', 'cobicistat \/ darunavir \/ emtricitabine \/ tenofovir alafenamide', 'cobicistat \/ elvitegravir \/ emtricitabine \/ tenofovir alafenamide', 'cobicistat \/ elvitegravir \/ emtricitabine \/ tenofovir disoproxil','daclatasvir', 'darunavir', 'dasabuvir', 'dasabuvir \/ ombitasvir \/ paritaprevir \/ Ritonavir', 'Delavirdine', 'Didanosine', 'dolutegravir', 'dolutegravir \/ Lamivudine', 'dolutegravir \/ Rilpivirine', 'DORAVIRINE', 'DORAVIRINE \/ Lamivudine \/ tenofovir disoproxil', 'efavirenz', 'efavirenz \/ emtricitabine \/ tenofovir disoproxil', 'efavirenz \/ Lamivudine \/ tenofovir disoproxil', 'elbasvir', 'elbasvir \/ grazoprevir', 'elvitegravir', 'emtricitabine', 'emtricitabine \/ Rilpivirine \/ tenofovir alafenamide', 'emtricitabine \/ Rilpivirine \/ tenofovir disoproxil', 'emtricitabine \/ tenofovir alafenamide', 'emtricitabine \/ tenofovir disoproxil','enfuvirtide', 'entecavir', 'etravirine', 'famciclovir', 'fosamprenavir', 'Foscarnet', 'Ganciclovir', 'glecaprevir \/ pibrentasvir', 'grazoprevir', 'ibalizumab', 'Idoxuridine', 'Indinavir', 'Inosine Pranobex', 'Lamivudine', 'Lamivudine \/ Nevirapine \/ Stavudine', 'Lamivudine \/ Nevirapine \/ Zidovudine', 'Lamivudine \/ tenofovir disoproxil', 'Lamivudine \/ Zidovudine', 'ledipasvir \/ sofosbuvir', 'letermovir', 'lopinavir \/ Ritonavir', 'lysozyme', 'maraviroc', 'moroxydine', 'Nelfinavir', 'Nevirapine', 'ombitasvir \/ paritaprevir \/ Ritonavir', 'Oseltamivir', 'penciclovir', 'peramivir', 'raltegravir', 'Ribavirin', 'Rilpivirine', 'Rimantadine', 'Ritonavir', 'Saquinavir', 'simeprevir', 'sofosbuvir', 'sofosbuvir \/ velpatasvir', 'sofosbuvir \/ velpatasvir \/ voxilaprevir', 'Stavudine', 'Tecovirimat', 'telaprevir', 'telbivudine', 'tenofovir alafenamide', 'tenofovir disoproxil', 'tipranavir', 'tromantadine', 'valacyclovir', 'valganciclovir', 'Vidarabine', 'Zalcitabine', 'Zanamivir', 'Zidovudine'] + ['Amodiaquine', 'artemether', 'artemether \/ lumefantrine', 'artesunate', 'Chloroquine', 'halofantrine', 'Hydroxychloroquine', 'Mefloquine', 'Primaquine', 'Proguanil', 'Pyrimethamine', 'Quinine', 'tafenoquine'] + ['remdesivir', 'galidesivir', 'favipiravir']","f792c08f":"antiviral_search_terms = []\nfor drug in potential_antivirals:\n    if ' \/ ' in drug:\n        antiviral_search_terms.extend([component.lower() for component in drug.split(' \/ ')])\n    else:\n        antiviral_search_terms.append(drug.lower())\nantiviral_search_terms = list(set(antiviral_search_terms))","48a524b4":"paper_lengths = []\nfor paper_path in glob.glob('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/*\/*\/*.json'):\n    with open(paper_path, 'r') as f:\n        paper = json.loads(f.read())\n    paper_text = '\\n'.join(item['text'] for item in paper['body_text'])\n    paper_lengths.append(len(paper_text))\n_ = plt.hist(paper_lengths, bins=[i*1000 for i in range(100)])\nplt.show()","b0f05b9f":"papers_referring_to_antivirals = []\npapers_searched = 0\npapers_matching = 0\nfor paper_path in glob.glob('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/*\/*\/*.json'):\n    with open(paper_path, 'r') as f:\n        paper = json.loads(f.read())\n    paper_text = '\\n'.join(item['text'] for item in paper['body_text']).lower()\n    if any([term in paper_text for term in antiviral_search_terms]):\n        papers_referring_to_antivirals.append(paper_path)\n        papers_matching += 1\n    papers_searched += 1\n    if papers_searched % 1000 == 0:\n        print(papers_searched, papers_matching)","98602f14":"for paper_path in papers_referring_to_antivirals:\n    filename = paper_path.split('\/')[-1].split('.')[0] + '.txt'\n    with open(filename, 'w') as f:\n        with open(paper_path, 'r') as g:\n            paper = json.loads(g.read())\n        f.write('\\n'.join(item['text'] for item in paper['body_text']).lower())","f193cc77":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(input='filename', lowercase=True, vocabulary=antiviral_search_terms)","eb81e88f":"output = cv.fit_transform(glob.glob('*.txt'))\npresence_of_term = np.where(output.toarray() > 0, True, False)","2b4e13f9":"antiviral_df = pd.DataFrame(presence_of_term, columns=antiviral_search_terms)\nantiviral_df['sha'] = [fname.split('.')[0] for fname in glob.glob('*.txt')]","50d46457":"index_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv')\noutput_df = index_df.merge(antiviral_df, on='sha', how='inner')\noutput_df.index = output_df['sha']","257898f7":"sha_repeats_dict = {}\ntimes_sha_repeats = []\nfor sha, row in output_df.iterrows():\n    if sha in sha_repeats_dict:\n        sha_repeats_dict[sha] += 1\n    else:\n        sha_repeats_dict[sha] = 0\n    times_sha_repeats.append(sha_repeats_dict[sha])","2b33a9d9":"output_df['times_sha_repeated'] = times_sha_repeats\noutput_df = output_df[output_df['times_sha_repeated'] == 0]","3d577542":"output_df[antiviral_search_terms].sum().sort_values(ascending=False)[:20]","feb52380":"output_df['drugs_mentioned'] = output_df[antiviral_search_terms].sum(axis=1)","23b47938":"output_df.sort_values('drugs_mentioned', ascending=False)[:20][['title', 'publish_time', 'doi', 'drugs_mentioned']]","fede6b89":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.max_length = 2000000 # there's at least one paper that exceeds the default max_length of 1000000","b1cf3e5d":"def get_article_text(sha):\n    with open(sha + '.txt', 'r') as f:\n        return f.read()","b9c6ab4c":"relevant_sentences = []\ni = 0\nfor sha, row in output_df.iterrows():\n    if i % 100 == 0:\n        print(i)\n    i += 1\n    relevant_sentences_in_paper = []\n    drugs_in_paper = [drug for drug, drug_in_paper in row[antiviral_search_terms].iteritems() if drug_in_paper]\n    doc = nlp(get_article_text(sha))\n    for token in doc:\n        if token.text in drugs_in_paper:\n            relevant_sentences_in_paper.append(token.sent.text)\n    relevant_sentences.append('\\n'.join(list(set(relevant_sentences_in_paper))))\noutput_df['relevant_sentences'] = relevant_sentences","dcd6acef":"output_df.to_csv('antiviral_paper_table.csv', index=False)","fe281871":"recent_favipiravir_df = output_df[(output_df['favipiravir']) & (output_df['publish_time'].notnull()) & ((output_df['publish_time'].str.contains('2019')) | (output_df['publish_time'].str.contains('2020')))]","8f9134d7":"print('\\n\\n'.join(recent_favipiravir_df['relevant_sentences'].tolist()))","75634f5e":"def search_article_subset(subset_df, search_terms):\n    search_results = ''\n    for sha, row in subset_df.iterrows():\n        sentences_matching_search = []\n        # sorry for the egregious list comprehension - each article's relevant sentences are newline separated sentences. we want to check each sentence against each search term.\n        matching_sentences = [sentence for sentence in row['relevant_sentences'].split('\\n') if any([st in sentence for st in search_terms])]\n        if matching_sentences != []:\n            sentences_matching_search.extend(matching_sentences)\n        if sentences_matching_search != []:\n            search_results += '** article sha: {}\\n'.format(sha)\n            search_results += '** article title: {}\\n'.format(row['title'])\n            search_results += '\\n\\n'.join(sentences_matching_search)\n            search_results += '\\n\\n'\n    return search_results","55a01955":"key_terms = ['effic', 'activ', 'effect', 'model']","d14025ee":"print(search_article_subset(recent_favipiravir_df, key_terms))","68976448":"### Simple examples","5f362096":"#### Which papers refer to large numbers of the drugs?\nLet's create a new column in the dataframe looking at the number of distinct drugs mentioned in the paper.","7e229261":"### A more complicated example: in what context are the drugs being mentioned?\nFor each article, I'll grab the sentences referring to antivirals and concatenate them to make a pseudo-summary of the paper. I'll use `spaCy` to handle the identification of sentences.","87b6a77a":"OK, there are some that are quite short, but it looks like it's working.","fa1360a8":"### Load papers and search\nI'm opting to work with the `body_text` only (approximately 13.2K documents of 29.5K); it's a list of dictionaries containing a `text` key. I join these together to construct the full text. First, I'll do a simple verificaiton that this method is reliably retrieving documents by checking the character counts.","5277b0d9":"I'll load the summary table and merge it with the dataframe of papers with mentions of antivirals.","2678d37b":"Turns out there are duplicate `sha`s in the dataset (you can check via `output_df['sha'].value_counts()`); I'll de-dupe the output dataframe by iterating through the dataframe and keeping track how many prior times the `sha` has been seen so far. Afterwards I'll add this as a new column and restrict `output_df` to places where the number of repeats of the `sha` are 0.","3dcf8f5c":"The following block of code steps through each row (article) of the dataframe. I use the `sha` value to point to the fulltext for the paper, retrieve it, and tokenize it. Then, I identify each token that is one of the antivirals, and store the sentence it was in. The sentences in each paper are concatenated (separated by newline characters) and stored as a new column in the dataframe. This part is a little slow to execute (tens of minutes).","f4022c20":"Next, I split compound medicines into their constituent agents, normalize capitalization, and reduce to the unique set of agents to create a set of search terms.","3ab5fa05":"Now I'll create a dataframe with the information about which drugs appear in each paper. The `sha` values will be used as the merge key.","bb749e1f":"# **Identifying and searching papers that reference antiviral medicines**\nIn this kernel, I use lists of drugs generated from [RxNorm](https:\/\/mor.nlm.nih.gov\/RxNav\/)\/[RxNav](https:\/\/mor.nlm.nih.gov\/RxNav\/)\/[RxClass](https:\/\/mor.nlm.nih.gov\/RxClass\/) to identify a subset of papers that may be of further interest for addressing the questions posed in the vaccines\/therapeutics task. From this subset of papers, I create a dataframe with useful features for querying. As an example for what can be done with this dataframe, I present the specific use case of searching mentions of these drugs for words related to animal models and drug efficacy.","1b24c544":"### Output the dataframe as a CSV\nThis may serve as a useful starting point for those looking to do a deep-dive into text analysis of papers referring to antiviral treatments.","f4d3dd01":"## Use case: finding useful sections of articles mentioning drugs\n**[Favipiravir](https:\/\/en.wikipedia.org\/wiki\/Favipiravir)** has been in the news as a drug with potential to treat COVID-19. What has been said in recent (2019-2020) papers? We can query the dataframe to identify this in the following way:\n* Take all papers that mention favipiravir (`output_df['favipiravir'] == True`)\n* Combine this with a query that looks at `publish_time` (note that quite a few papers are without; could consider trying to extract publication dates from `doi` where available) - we want it to contain either 2019 or 2020\n\nFrom the remaining articles, we can output the sentences found in `relevant_sentences` by putting them in a list, newline-joining them, and printing:","886ff4e7":"### Find out which agents are mentioned in each paper\nHere I use a count vectorizer with the antiviral search terms as the dictionary. I don't particularly care about the frequency that the agents show up in any one paper, so I'll \"flatten\" the vectorization to True (agent was found in paper) \/ False (agent was not found in paper).","7dbd51ee":"### Use RxNav to find classes of drugs that are of interest\nI've chosen to start with antivirals and antimalarials (primarily due to the present interest in chloroquine) shown in RxNav, but also recent experimental antivirals being discussed.","5f9b0968":"#### What drugs appear in the most papers?\nLet's look at the top 20 drugs.","0cc6f166":"Encouraging - over 10% of the papers reference one or more of the medicines! I'll write out the full text from this subset of papers to my working directory.","45460d12":"OK, that's quite a few... what if I only look at sentences where there are terms like `effic`, `activ`, `effect` (the stem will pick up the words I'm interested in for these first three), or `model` (i.e. animal model)? I'll write a function that searches for sentences containing terms of interest and return those in a formatted \"report\".","c182bee9":"### OK, let's try it out - what do our recent articles say about favipiravir in regards to animal models and efficacy?"}}