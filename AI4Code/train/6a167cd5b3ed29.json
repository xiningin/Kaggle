{"cell_type":{"3ed4458a":"code","2ab9d7a0":"code","6e63153e":"code","f8cfc47a":"code","485e7c03":"code","679574ca":"code","92cf5248":"code","2ca09286":"code","ef49887a":"code","6356d97a":"code","004246bc":"code","b6765439":"code","7573c973":"code","fe561ec1":"code","da7805d5":"markdown","5df7524d":"markdown","a847215d":"markdown","609b50fe":"markdown","e16c07f0":"markdown","b6e12e80":"markdown"},"source":{"3ed4458a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nfrom torch import nn\n\nimport warnings\nimport gc\nimport os\nimport shutil\nimport time\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\n\nfrom collections import defaultdict\n\nsns.set(font_scale=1.2)\nwarnings.filterwarnings('ignore')","2ab9d7a0":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nnp.random.seed(12)\ntorch.manual_seed(12)\n\nDATA_DIR = '..\/input\/fruit-and-vegetable-image-recognition'\nWORKING_DIR = '.\/'\nTRAIN_DIR = os.path.join(WORKING_DIR, 'train')\nVAL_DIR = os.path.join(WORKING_DIR, 'val')\nREAD_TRAIN_DIR = os.path.join(DATA_DIR, 'train')\nTEST_DIR = os.path.join(DATA_DIR, 'test')\n\nIS_TRAIN = True\nWITH_VALIDATION = False\n\n# creating directories for train and validation\nos.makedirs(TRAIN_DIR, exist_ok=True)\nos.makedirs(VAL_DIR, exist_ok=True)","6e63153e":"CATEGORIES = {i: name for i, name in enumerate(os.listdir(READ_TRAIN_DIR))}\nVAL_PART = 0.2 if WITH_VALIDATION else 0.0\n\n# dividing the training sample into training and validation samples\nimages_nums = {}\nfor category in CATEGORIES.values():\n    # preparing training and validation directories in working directory\n    train_category_dir = os.path.join(TRAIN_DIR, category)\n    val_category_dir = os.path.join(VAL_DIR, category)\n    \n    shutil.rmtree(val_category_dir, ignore_errors=True)\n    shutil.rmtree(train_category_dir, ignore_errors=True)\n    os.makedirs(val_category_dir, exist_ok=True)\n    shutil.copytree(os.path.join(READ_TRAIN_DIR, category), train_category_dir)\n    \n    # generating numbers of images for validation\n    images_names = sorted(os.listdir(train_category_dir))\n    images_num = len(images_names)\n    val_images_num = int(images_num * VAL_PART)\n    val_images_names = np.take(images_names, np.random.choice(images_num, val_images_num, replace=False))\n    images_nums[category] = images_num\n    \n    # copy needed images to validation directory and remove them from training directiry\n    for image_name in val_images_names:\n        cur_image = os.path.join(train_category_dir, image_name)\n        shutil.copy(cur_image, os.path.join(os.path.join(VAL_DIR, category), image_name))\n        os.remove(cur_image)\n    \n    print(f'{category}: train_images = {images_num - val_images_num}, val_images = {val_images_num}')","f8cfc47a":"weights = [100.0 \/ images_nums[category] for category in CATEGORIES.values()]","485e7c03":"IMG_SIZE = (224, 224)\n\n# augumentations\ntrain_transforms = T.Compose([\n    T.Resize(IMG_SIZE),\n    T.RandomAffine(20),\n    T.RandomHorizontalFlip(),\n    T.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.2)),\n    T.ToTensor(),\n    T.Lambda(lambda x: x[:3])\n])\n\nval_transforms = T.Compose([\n    T.Resize(IMG_SIZE),\n    T.ToTensor(),\n    T.Lambda(lambda x: x[:3])\n])","679574ca":"batch_size = 64\n\ntrain_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\ntrain_batch_gen = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nif WITH_VALIDATION:\n    val_dataset = torchvision.datasets.ImageFolder(VAL_DIR, transform=val_transforms)\n    val_batch_gen = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\nelse:\n    val_batch_gen = None","92cf5248":"class VGGBlock(nn.Module):\n    def __init__(self, layers_number, in_channels, out_channels):\n        super().__init__()\n        self.layers = nn.Sequential()\n        self.layers.add_module(module=nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), name='conv1')\n        self.layers.add_module(module=nn.BatchNorm2d(out_channels), name='bn1')\n        self.layers.add_module(module=nn.ReLU(), name='relu1')\n        for i in range(layers_number - 1):\n            self.layers.add_module(module=nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                                   name=f'conv{i + 2}')\n            self.layers.add_module(module=nn.BatchNorm2d(out_channels), name=f'bn{i + 2}')\n            self.layers.add_module(module=nn.ReLU(), name=f'relu{i + 2}')\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        return self.pool(self.layers(x))","2ca09286":"class VGGModel(nn.Module):\n    def __init__(self, classes_number):\n        super().__init__()\n        self.classes_number = classes_number\n        layers_structure = ((2, 3, 64), (2, 64, 128), (4, 128, 256), (4, 256, 512), (4, 512, 512))\n        \n        self.vgg_blocks = nn.Sequential()\n        for i, layer_structure in enumerate(layers_structure):\n            self.vgg_blocks.add_module(module=VGGBlock(*layer_structure), name=f'vgg{i + 1}')\n            \n        self.fltn = nn.Flatten()\n        \n        self.fc1 = nn.Linear(7 * 7 * 512, 4096)\n        self.bn1 = nn.BatchNorm1d(4096)\n        self.act1 = nn.ReLU()\n        self.do1 = nn.Dropout()\n        \n        self.fc2 = nn.Linear(4096, 4096)\n        self.bn2 = nn.BatchNorm1d(4096)\n        self.act2 = nn.ReLU()\n        self.do2 = nn.Dropout()\n        \n        self.fc3 = nn.Linear(4096, classes_number)\n        \n    def forward(self, x):\n        x = self.fltn(self.vgg_blocks(x))\n        \n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.do1(x)\n        \n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n        x = self.do2(x)\n        \n        return self.fc3(x)","ef49887a":"classes_number = len(CATEGORIES)\nmodel = VGGModel(classes_number).to(device)\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\nif WITH_VALIDATION:\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.01, factor=0.31, patience=7)\nelse:\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,35], gamma=0.31)","6356d97a":"def plot_learning_curves(history):\n    fig = plt.figure(figsize=(20, 7))\n\n    plt.subplot(1,2,1)\n    plt.title('Loss', fontsize=15)\n    plt.plot(history['loss']['train'], label='train')\n    \n    if 'val' in history['loss'].keys():\n        plt.plot(history['loss']['val'], label='val')\n        \n    plt.ylabel('loss', fontsize=15)\n    plt.xlabel('epoch', fontsize=15)\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.title('Accuracy', fontsize=15)\n    plt.plot(history['accuracy']['train'], label='train')\n    \n    if 'val' in history['accuracy'].keys():\n        plt.plot(history['accuracy']['val'], label='val')\n        \n    plt.ylabel('accuracy', fontsize=15)\n    plt.xlabel('epoch', fontsize=15)\n    plt.legend()\n    plt.show()","004246bc":"def train(\n    model, \n    criterion,\n    optimizer, \n    train_batch_gen,\n    val_batch_gen=None,\n    num_epochs=50,\n    scheduler=None,\n    history=None,\n    checkpoint_path='state.pt',\n):\n    if history is None:\n        history = defaultdict(lambda: defaultdict(list))\n\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_accuracy = 0\n        val_loss = 0\n        val_accuracy = 0\n        \n        start_time = time.time()\n\n        model.train(True) \n\n        for X_batch, y_batch in tqdm(train_batch_gen, leave=False):\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            logits = model(X_batch)\n            \n            loss = criterion(logits, y_batch.long().to(device))\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            train_loss += np.sum(loss.detach().cpu().numpy())\n            y_pred = logits.max(1)[1].detach().cpu().numpy()\n            train_accuracy += np.mean(y_batch.cpu().numpy() == y_pred)\n\n        train_loss \/= len(train_batch_gen)\n        train_accuracy \/= len(train_batch_gen) \n        history['loss']['train'].append(train_loss)\n        history['accuracy']['train'].append(train_accuracy)\n        \n        if val_batch_gen is not None:\n            model.train(False)\n\n            with torch.no_grad():\n                for X_batch, y_batch in tqdm(val_batch_gen, leave=False):\n                    X_batch = X_batch.to(device)\n                    y_batch = y_batch.to(device)\n                \n                    logits = model(X_batch)\n                    loss = criterion(logits, y_batch.long().to(device))\n                    val_loss += np.sum(loss.detach().cpu().numpy())\n                    y_pred = logits.max(1)[1].detach().cpu().numpy()\n                    val_accuracy += np.mean(y_batch.cpu().numpy() == y_pred)\n\n            val_loss \/= len(val_batch_gen)\n            val_accuracy \/= len(val_batch_gen) \n            history['loss']['val'].append(val_loss)\n            history['accuracy']['val'].append(val_accuracy)\n        \n        clear_output()\n\n        print(\"Epoch {} of {} took {:.3f}s\".format(\n            epoch + 1, num_epochs, time.time() - start_time))\n        print(\"  training loss: \\t\\t\\t{:.6f}\".format(train_loss))\n        \n        if val_batch_gen is not None:\n            print(\"  validation loss: \\t\\t\\t{:.6f}\".format(val_loss))\n        \n        print(\"  training accuracy: \\t\\t\\t{:.2f} %\".format(train_accuracy * 100))\n        \n        if val_batch_gen is not None:\n            print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_accuracy * 100))\n        \n        state_dict = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'history': dict(history),\n        }\n        \n        if scheduler is not None:\n            if val_batch_gen is None:\n                scheduler.step()\n            else:\n                scheduler.step(val_loss)\n            state_dict['scheduler_state_dict'] = scheduler.state_dict()\n\n        plot_learning_curves(history)\n        torch.save(state_dict, checkpoint_path)\n        gc.collect()\n        \n    return model, history","b6765439":"checkpoint_path = os.path.join(WORKING_DIR, 'vgg-adam-epoch50.pt')\nif IS_TRAIN:\n    model, history = train(model, criterion, optimizer, train_batch_gen, val_batch_gen,\n                           scheduler=scheduler, checkpoint_path=checkpoint_path)","7573c973":"test_dataset = torchvision.datasets.ImageFolder(TEST_DIR, transform=val_transforms)\ntest_batch_gen = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","fe561ec1":"state = torch.load(checkpoint_path)\nmodel.load_state_dict(state['model_state_dict'])\n\nmodel.eval()\n\ny_pred = []\ny_true = []\nwith torch.no_grad():\n    for X_batch, y_batch in test_batch_gen:\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n                \n        logits = model(X_batch)\n        y_pred += logits.max(1)[1].detach().cpu().numpy().tolist()\n        y_true += y_batch.cpu().numpy().tolist()\n        \nfinal_accuracy = accuracy_score(y_pred, y_true)\nprint('Final test accuracy: {:.2f} %'.format(final_accuracy * 100))","da7805d5":"Let's make the augmentations.","5df7524d":"## Main Model","a847215d":"We can see a small imbalance in the classes. We must take this into account when calculating the loss.","609b50fe":"## Data preparation and augumentations","e16c07f0":"## Testing","b6e12e80":"## Fitting"}}