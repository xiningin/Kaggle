{"cell_type":{"8872aef4":"code","3a3bfe95":"code","d66d8606":"code","f5389f51":"code","a8e6f3b4":"code","f4b1fddb":"code","afc2e05e":"code","86b48500":"code","a76e9436":"code","8bda1c0f":"code","4f94a79d":"code","16ce821f":"markdown","5e65cc98":"markdown","50b9cace":"markdown"},"source":{"8872aef4":"# import library\nimport numpy as np # numpy\n\nfrom sklearn import datasets # load dataset\nfrom sklearn.model_selection import train_test_split # split dataset\nfrom sklearn.preprocessing import StandardScaler # standard scaler\nfrom sklearn.linear_model import Perceptron # import model\nfrom sklearn.linear_model import SGDClassifier # import SGDClassifier\nfrom sklearn.metrics import accuracy_score # check accuracy\n\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt","3a3bfe95":"# load dataset\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class label :', np.unique(y))","d66d8606":"# split training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)","f5389f51":"# check unique value's count\nprint('y label count :', np.bincount(y))\nprint('y_train label count :', np.bincount(y_train))\nprint('y_test label count :', np.bincount(y_test))","a8e6f3b4":"# standardize\nsc = StandardScaler()\nsc.fit(X_train) # calculate mu and sigma\nX_train_std = sc.transform(X_train) # standardize\nX_test_std = sc.transform(X_test)","f4b1fddb":"# modeling\nppn = Perceptron(max_iter = 40, eta0 = 0.1, tol = 1e-3, random_state = 1)\nppn.fit(X_train_std, y_train)\ny_pred = ppn.predict(X_test_std)\nprint('The number of wrong classified sample :', (y_test != y_pred).sum())\nprint('The accuracy of Perceptron : %.2f' %((y_test == y_pred).sum() \/ len(y_test)))","afc2e05e":"# check accuracy by using sklearn library\nprint('The accuracy of Perceptron : %.2f' %(accuracy_score(y_test, y_pred)))\n\n# sklearn's classifier has score method by working to predict and to accuracy_score at the same time\n# print('The accuracy of Perceptron by using score method : %.2f' %(ppn.score(X_test_std, y_test)))","86b48500":"# define function about visualizing\ndef plot_decision_regions(X, y, classifier, test_idx = None, resolution = 0.02):\n    \n    # set marker and colormap\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    \n    # draws a decision boundary.\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                          np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha = 0.3, cmap = cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(X[y == cl, 0], X[y == cl, 1],\n                   alpha = 0.8, c = colors[idx],    # alpha : size of marker\n                   marker = markers[idx], label = cl,\n                   edgecolor = 'black')\n        \n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        \n        plt.scatter(X_test[:, 0], X_test[:, 1],\n                   c = '', edgecolor = 'black', alpha = 1,\n                   s = 100, label = 'test set')","a76e9436":"X_combined_std = np.vstack((X_train_std, X_test_std)) # vlookup combine\ny_combined_std = np.hstack((y_train, y_test))  # hlookup combine\nplot_decision_regions(X_combined_std, y_combined_std,\n                    classifier = ppn, test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","8bda1c0f":"ppn = SGDClassifier(loss = 'perceptron', random_state = 1)\nppn.fit(X_train_std, y_train)\n\ny_pred = ppn.predict(X_test_std)\nprint('The number of wrong classified sample :', (y_test != y_pred).sum())\nprint('The accuracy of Perceptron : %.2f' %((y_test == y_pred).sum() \/ len(y_test)))","4f94a79d":"plot_decision_regions(X_combined_std, y_combined_std,\n                    classifier = ppn, test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","16ce821f":"### Almost Sklearn's algorithm support multicalss classification model using OvR(One-versus-Rest) method.\n\nSo we can classify iris dataset.\n\n\n##### I write the notebook explaining perceptron. So if you can refer it.\n- Perceptron : [Titanic Survival prediction with Perceptron](https:\/\/www.kaggle.com\/choihanbin\/titanic-survival-prediction-with-perceptron)\n- Adaptive linear neuron : [Predict titanic survival by Adaptive linear neuron](https:\/\/www.kaggle.com\/choihanbin\/predict-titanic-survival-by-adaptive-linear-neuron)","5e65cc98":"If the data set is too large and slow, SGDClassifier can be used. \n\nThis is similar to the stochastic gradient descent method previously implemented in the [Adaline model](https:\/\/www.kaggle.com\/choihanbin\/predict-titanic-survival-by-adaptive-linear-neuron). Therefore, it has the advantage of being fast in terms of speed.","50b9cace":"### Linear regression model is weak for scale. So we have to adjust it by using sklearn's StandardScaler library"}}