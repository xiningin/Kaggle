{"cell_type":{"8f549c80":"code","5a3176f0":"code","a3ac8f63":"code","bad8593d":"code","67548041":"code","575ac186":"code","1552aba4":"code","ade69d62":"code","aec0332e":"code","5e6f0954":"code","5387f93a":"code","dda5bb8b":"code","c38ed446":"code","aaee808a":"code","7eb8da00":"code","37008d98":"code","64780d9f":"code","4a119b96":"code","f640b1e4":"code","9f06302a":"code","2ec2b404":"code","26a6bf14":"code","d5789d2d":"code","45a4dc17":"code","066f7d8a":"code","352e436a":"code","d0569f5b":"code","32b67d6d":"code","b97553ca":"code","241edd14":"code","88c64c17":"code","ddf616b8":"code","b12f8cc3":"code","2e058ac4":"code","c68e6c52":"code","a353b1d3":"code","886d554c":"code","32f6b406":"code","05082ce3":"code","1e4c99f8":"code","41618732":"code","225f8bb2":"code","3ca9a96d":"code","d8d4d0bd":"code","d1307cae":"code","ea949e31":"code","6c24bd2b":"code","241e38af":"code","db4db225":"code","e725ec51":"code","95620931":"code","a73aba67":"code","008987db":"code","ddf07733":"markdown","16b0e57a":"markdown","5e124b83":"markdown","bfcec953":"markdown","733ad313":"markdown","d09d7125":"markdown","519c3944":"markdown","a79043fd":"markdown","3eeeb2bf":"markdown","13793bbf":"markdown","fba8ce2b":"markdown","cbd04304":"markdown","e8e5ed6b":"markdown","9b5c94ed":"markdown","398d7af3":"markdown","7e666c20":"markdown","476ac0d2":"markdown","87a01c03":"markdown","d21a3eee":"markdown","868bdd67":"markdown","07749c98":"markdown","ac8d4f24":"markdown","c24b9dcc":"markdown","0d499ced":"markdown","ef901b76":"markdown","c845072b":"markdown"},"source":{"8f549c80":"!pip install dataprep","5a3176f0":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\nfrom dataprep.eda import create_report\nfrom dataprep.eda import plot_missing\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","a3ac8f63":"data = pd.read_csv('..\/input\/breastcancerdataset\/BRCA.csv')\ndf = data.copy()\npd.set_option('display.max_row',df.shape[0])\npd.set_option('display.max_column',df.shape[1]) \ndf.head()","bad8593d":"(df.isna().sum()\/df.shape[0]*100).sort_values(ascending=False)","67548041":"plot_missing(df)","575ac186":"plt.figure(figsize=(10,8))\nsns.heatmap(df.isna(),cbar=False)\nplt.show()","1552aba4":"print('There is' , df.shape[0] , 'rows')\nprint('There is' , df.shape[1] , 'columns')","ade69d62":"df.duplicated().sum()","aec0332e":"df.loc[df.duplicated(keep=False),:]","5e6f0954":"df.drop_duplicates(keep='first',inplace=True)\ndf.shape","5387f93a":"df = data.copy()\ndf = df.drop(['Patient_ID','Date_of_Surgery','Date_of_Last_Visit'],axis=1)\ndf['Patient_Status'].value_counts(normalize=True) #Classes d\u00e9s\u00e9quilibr\u00e9es","dda5bb8b":"target_dist = df['Patient_Status'].value_counts()\n\nfig, ax = plt.subplots(1, 1, figsize=(8,5))\n\nbarplot = plt.bar(target_dist.index, target_dist, color = 'lightgreen', alpha = 0.8)\nbarplot[1].set_color('darkred')\n\nax.set_title('Target Distribution')\npercentage = df['Patient_Status'].value_counts(normalize=True)[0]*100\nax.annotate(\"percentage of Alive Patients : {}%\".format(percentage),\n              xy=(0, 0),xycoords='axes fraction', \n              xytext=(0,-50), textcoords='offset points',\n              va=\"top\", ha=\"left\", color='grey',\n              bbox=dict(boxstyle='round', fc=\"w\", ec='w'))\n\nplt.xlabel('Target', fontsize = 12, weight = 'bold')\nplt.show()","c38ed446":"# Class count\ncount_class_0, count_class_1 = df['Patient_Status'].value_counts()\n\n# Divide by class\ndf_class_0 = df[df['Patient_Status'] == 'Alive']\ndf_class_1 = df[df['Patient_Status'] == 'Dead']\n\nprint(count_class_0)\nprint(count_class_1)","aaee808a":"df_class_0_under = df_class_0.sample(count_class_1,random_state=42)\ndf_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_under['Patient_Status'].value_counts())\n\ndf_under['Patient_Status'].value_counts().plot(kind='bar', title='Count (target)');","7eb8da00":"for col in df.select_dtypes(\"object\"):\n    print(f'{col :-<50} {df[col].unique()}')","37008d98":"fig, ax = plt.subplots(4,2, figsize=(30, 30))\ni=0\nsns.set(font_scale = 1.5)\nfor col in df.select_dtypes('object'): \n    sns.countplot(df_under[col], hue=df_under['Patient_Status'], ax=ax[i\/\/2][i%2])\n    i=i+1\nplt.show()","64780d9f":"Alive_df = df[df['Patient_Status']==\"Alive\"]\nDead_df = df[df['Patient_Status']==\"Dead\"]\nsns.set(font_scale = 1.5)\nfig, ax = plt.subplots(2,3, figsize=(30, 15))\ni=0\nfor col in df.select_dtypes(include=['float64','int64']):\n    sns.distplot(Alive_df[col],label='Alive',ax=ax[i\/\/3][i%3])\n    sns.distplot(Dead_df[col],label='Dead',ax=ax[i\/\/3][i%3])\n    i=i+1\nfig.legend(labels=['Alive','Dead'],fontsize='22')\nfig.show()","4a119b96":"def encoding(df):\n    code = {'FEMALE':0,\n            'MALE':1,\n            'III':3,\n            'II':2,\n            'I':1,\n            'Infiltrating Ductal Carcinoma':0,\n            'Mucinous Carcinoma':1,\n            'Infiltrating Lobular Carcinoma':2,\n            'Negative':0,\n            'Positive':1,\n            'Modified Radical Mastectomy':0,\n            'Lumpectomy':1,\n            'Simple Mastectomy':2,\n            'Other':3,\n            'Alive':1,\n            'Dead':0\n           }\n    for col in df.select_dtypes('object'):\n        df.loc[:,col]=df[col].map(code)        \n    return df\n\ndef imputation(df):\n    df = df.fillna(df.median())\n    df = df.dropna()\n    return df\n\ndef feature_engineering(df):\n    useless_columns = ['Patient_ID','Date_of_Surgery','Date_of_Last_Visit','ER status','PR status']\n    df = df.drop(useless_columns,axis=1)\n    return df\n\ndef preprocessing(df):\n    df = encoding(df)\n    df = feature_engineering(df)\n    df = imputation(df)\n    \n    X = df.drop('Patient_Status',axis=1)\n    y = df['Patient_Status']    \n\n    return df,X,y","f640b1e4":"df = data.copy()\ndf,X,y=preprocessing(df)","9f06302a":"# Class count\ncount_class_0, count_class_1 = df['Patient_Status'].value_counts()\n\n# Divide by class\ndf_class_0 = df[df['Patient_Status'] == 1]\ndf_class_1 = df[df['Patient_Status'] == 0]\n\ndf_class_0_under = df_class_0.sample(count_class_1,random_state=42)\ndf_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_under['Patient_Status'].value_counts())\n\n# Resampling\ndf_under['Patient_Status'].value_counts().plot(kind='bar', title='Count (target)');","2ec2b404":"sns.heatmap(df_under.corr())","26a6bf14":"sns.pairplot(df, height=2)","d5789d2d":"trainset, testset = train_test_split(df_under, test_size=0.2, random_state=0)\nfig, ax = plt.subplots(1,2, figsize=(10, 5))\nsns.countplot(x = trainset['Patient_Status'] , data = trainset['Patient_Status'],ax=ax[0],palette=\"Set3\").set_title('TrainSet')\nsns.countplot(x = testset['Patient_Status'] , data = testset['Patient_Status'],ax=ax[1],palette=\"Set2\").set_title('TestSet')","45a4dc17":"X_train = trainset.drop(['Patient_Status'],axis=1)\ny_train = trainset['Patient_Status']\nX_test = testset.drop(['Patient_Status'],axis=1)\ny_test = testset['Patient_Status']","066f7d8a":"preprocessor = make_pipeline(RobustScaler())\n\nPCAPipeline = make_pipeline(preprocessor, PCA(n_components=3,random_state=42))\n\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=42))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=42))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=42,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression(solver='sag',random_state=42))","352e436a":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X_train))\ny_train = y_train.astype(int)\ny_train.reset_index(drop=True, inplace=True)\nPCA_df = pd.concat([PCA_df, y_train], axis=1, ignore_index=True )\nPCA_df.head()","d0569f5b":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df[3],palette=sns.color_palette(\"tab10\", 2))\nplt.show()","32b67d6d":"import plotly.express as px\nfigure1 = px.scatter_3d(PCA_df,\n        x=0, \n        y=1, \n        z=2, \n        color = 3,\n                       width=600, height=800)\nfigure1.update_traces(marker=dict(size=5,\n                              line=dict(width=0.2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\n\nfigure1.show()","b97553ca":"dict_of_models = {'RandomForest': RandomPipeline,\n'AdaBoost': AdaPipeline,\n'SVM': SVMPipeline,\n'KNN': KNNPipeline,\n'LR': LRPipeline}","241edd14":"def evaluation(model):\n    # calculating the probabilities\n    y_pred_proba = model.predict_proba(X_test)\n\n    # finding the predicted valued\n    y_pred = np.argmax(y_pred_proba,axis=1)\n    print('Accuracy = ', accuracy_score(y_test, y_pred))\n    print('-')\n    print(confusion_matrix(y_test,y_pred))\n    print('-')\n    print(classification_report(y_test,y_pred))\n    print('-')\n    \n    N, train_score, test_score = learning_curve(model, X_train, y_train, \n                                               cv=4, scoring='f1', \n                                               train_sizes=np.linspace(0.1,1,10))\n    plt.figure(figsize=(5,5))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, test_score.mean(axis=1), label='validation score')\n    plt.legend()\n    plt.show()","88c64c17":"sns.set(font_scale = 1)\nfor name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    model.fit(X_train,y_train)\n    evaluation(model)","ddf616b8":"RandomPipeline.fit(X_train, y_train)\nevaluation(RandomPipeline)","b12f8cc3":"y_pred_prob = RandomPipeline.predict_proba(X_test)[:,1]\n\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\nplt.plot(fpr,tpr,label='RandomForest ROC Curve')\nplt.xlabel(\"False Survivor Rate\")\nplt.ylabel(\"True SurvivorR Rate\")\nplt.title(\"andomForest ROC Curve\")\nplt.show()","2e058ac4":"from sklearn.model_selection import RandomizedSearchCV\nRandomPipeline.get_params().keys()","c68e6c52":"hyper_params = {\n    'randomforestclassifier__n_estimators':[10,100,150,250,400,600],\n    'randomforestclassifier__criterion':['gini','entropy'],\n    'randomforestclassifier__min_samples_split':[2,6,12],\n    'randomforestclassifier__min_samples_leaf':[1,4,6,10],\n    'randomforestclassifier__max_features':['auto','srqt','log2',int,float],\n    'randomforestclassifier__verbose':[0,1,2],\n    'randomforestclassifier__class_weight':['balanced','balanced_subsample'],\n    'randomforestclassifier__n_jobs':[-1],\n}","a353b1d3":"RF_grid = RandomizedSearchCV(RandomPipeline,hyper_params,scoring='accuracy',n_iter=40)\nRF_grid.fit(X_train,y_train)","886d554c":"print(RF_grid.best_params_)","32f6b406":"best_forest = (RF_grid.best_estimator_)\nbest_forest.fit(X_train,y_train)\n# calculating the probabilities\ny_pred_proba = best_forest.predict_proba(X_test)\n#Finding the predicted valued\ny_pred = np.argmax(y_pred_proba,axis=1)\n\nN, train_score, test_score = learning_curve(best_forest, X_train, y_train, \n                                           cv=4, scoring='f1', \n                                           train_sizes=np.linspace(0.1,1,10))","05082ce3":"print('Accuracy = ', accuracy_score(y_test, y_pred))\nprint('-')\nprint(confusion_matrix(y_test,y_pred))\nprint('-')\nprint(classification_report(y_test,y_pred))\nprint('-')\n    \nplt.figure(figsize=(5,5))\nplt.plot(N, train_score.mean(axis=1), label='train score')\nplt.plot(N, test_score.mean(axis=1), label='validation score')\nplt.legend()\nplt.title('f1 score')\nplt.show()","1e4c99f8":"err = []\n  \nfor i in range(1, 40):\n    \n    model = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = i))\n    model.fit(X_train, y_train)\n    pred_i = model.predict(X_test)\n    err.append(np.mean(pred_i != y_test))\n  \nplt.figure(figsize =(10, 8))\nplt.plot(range(1, 40), err, color ='blue',\n                linestyle ='dashed', marker ='o',\n         markerfacecolor ='blue', markersize = 8)\n  \nplt.title('Mean Err = f(K)')\nplt.xlabel('K')\nplt.ylabel('Mean Err')","41618732":"KNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = 5))\nKNNPipeline.fit(X_train, y_train)","225f8bb2":"evaluation(KNNPipeline)","3ca9a96d":"import xgboost as xgb\ngbm = xgb.XGBClassifier(\n     learning_rate = 0.15,\n     n_estimators= 3000,\n     max_depth= 16,\n     min_child_weight= 2,\n     #gamma=1,\n     gamma=0.9,                        \n     subsample=0.8,\n     colsample_bytree=0.8,\n     objective= 'binary:logistic',\n     eval_metric = 'logloss',\n     nthread= -1,\n     scale_pos_weight=1).fit(X_train, y_train)\nevaluation (gbm)","d8d4d0bd":"SVMPipeline.fit(X_train, y_train)\nevaluation(SVMPipeline)","d1307cae":"y_pred_prob = SVMPipeline.predict_proba(X_test)[:,1]\n\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\nplt.plot(fpr,tpr,label='SVM ROC Curve')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"SVM ROC Curve\")\nplt.show()","ea949e31":"best_classifier = KNNPipeline\n\nthresholds = [0.3,0.4,0.5,0.6,0.7,0.8]\nbest_t = 0.3\nbest_acc = 0\nfor t in thresholds:\n    y_pred = (best_classifier.predict_proba(X_test)[:,1] >= t).astype(int)\n    acc = accuracy_score(y_test, y_pred)\n    if acc > best_acc:\n        best_acc=acc\n        best_t=t","6c24bd2b":"print('Accuracy on test set :',round(best_acc*100),\"%\")\nprint('Best threshold :',best_t)","241e38af":"# Importing the Keras libraries and packages\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout","db4db225":"X_train.shape","e725ec51":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))\nclassifier.add(Dropout(0.2))\n# Adding the second hidden layer\nclassifier.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\n# Adding the third hidden layer\nclassifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\nclassifier.add(Dropout(0.2))\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\ncallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=80)\nhistory =classifier.fit(X_train, y_train, batch_size = 10, epochs = 100, callbacks=callback)","95620931":"classifier.save('1rst-model.h5')","a73aba67":"accuracy = history.history['accuracy']\nloss = history.history['loss']","008987db":"plt.figure(figsize=(15,10))\n\nplt.subplot(2, 2, 1)\nplt.plot(accuracy, label = \"Training accuracy\")\nplt.legend()\nplt.title(\"Training vs validation accuracy\")\n\n\nplt.subplot(2,2,2)\nplt.plot(loss, label = \"Training loss\")\nplt.legend()\nplt.title(\"Training vs validation loss\")\n\nplt.show()","ddf07733":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)","16b0e57a":"### Comments\nWe can now analyze categorical features as quantitative features","5e124b83":"<h1><center><font size=\"30\">Target Distribution<\/font><\/center><\/h1>","bfcec953":"# A bit of data engineering ...","733ad313":"<h1><center><font size=\"30\">Categorical Features<\/font><\/center><\/h1>","d09d7125":"### Remark :\n\n#### The classes are really hard to classify looking at the graph above...","519c3944":"## PCA Analysis","a79043fd":"### Comments\nConsidering the correlations shown above, it seems really difficult to find a difference between Dead and Alive patients...\n\n#### Let's find out !","3eeeb2bf":"<h1><center>\ud83c\udf97\ufe0fBreast Cancer Data Analysis\ud83d\udd0e<\/center><\/h1>\n<h3><center>\ud83e\ude7a(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/media.slidesgo.com\/storage\/4701966\/breast-cancer-case1617872724.jpg\" alt =\"Titanic\" style='width: 600px;'><\/center>\n\n<h3>Overview<\/h3>\n<p>\nBreast cancer is cancer that forms in the cells of the breasts.\n\nAfter skin cancer, breast cancer is the most common cancer diagnosed in women in the United States. Breast cancer can occur in both men and women, but it's far more common in women.\n\nSubstantial support for breast cancer awareness and research funding has helped created advances in the diagnosis and treatment of breast cancer.\n    \nBreast cancer survival rates have increased, and the number of deaths associated with this disease is steadily declining, largely due to factors such as earlier detection, a new personalized approach to treatment and a better understanding of the disease.\n<\/p>\n\n<h3>What are the symptoms of breast cancer?<\/h3>\n<p>\nSigns and symptoms of breast cancer may include:\n\n- A breast lump or thickening that feels different from the surrounding tissue\n\n- Change in the size, shape or appearance of a breast\n\n- Changes to the skin over the breast, such as dimpling\n    \n- A newly inverted nipple\n    \n- Peeling, scaling, crusting or flaking of the pigmented area of skin surrounding the nipple (areola) or breast skin\n    \n- Redness or pitting of the skin over your breast, like the skin of an orange\n<\/p>","13793bbf":"### Optimization","fba8ce2b":"## Dataset Analysis","cbd04304":"<h1><center><font size=\"30\">Continuous Features<\/font><\/center><\/h1>","e8e5ed6b":"### Checking for duplicates","9b5c94ed":"# Modelling","398d7af3":"# Tuning Threshold","7e666c20":"# If you like please upvote !\n## Also check my other notebooks :\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83d\udc01Mice Trisomy (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83c\udf97\ufe0fBreast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### \ud83c\udf26\ud83c\udf21 Weather Forecasting \ud83d\udcc8 (98% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/weather-forecasting-98-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Heart Attack \ud83e\ude7a\ud83d\udc93 (90% Acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Mobile price (95.5% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83e\udde0 Stroke (74% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-stroke-74-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Holiday Package (89% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-holiday-package-89-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\udda0\ud83c\udf6c Diabetes Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-diabetes-detection\n#### \u26a1\ud83d\udc32 Pokemon Stats \ud83e\udd4a\u2728 : https:\/\/www.kaggle.com\/dorianvoydie\/pokemon-stats\n#### \ud83d\udc1fFish Classification - Using CNN\ud83d\udd2e (97% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/fish-classification-using-cnn-97-acc\n#### \ud83d\udc89\ud83d\udc69\u200d\u2695\ufe0f Vaccine & COVID-19 Indicators\ud83d\udcc8 : https:\/\/www.kaggle.com\/dorianvoydie\/vaccine-covid-19-indicators","476ac0d2":"# Resampling\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).","87a01c03":"# Exploratory Data Analysis\n\n## Aim :\n- Understand the data (\"A small step forward is better than a big one backwards\")\n- Begin to develop a modelling strategy\n\n## Features\n\n- Patient_ID: unique identifier id of a patient\n- Age: age at diagnosis (Years)\n- Gender: Male\/Female\n- Protein1, Protein2, Protein3, Protein4: expression levels (undefined units)\n- Tumour_Stage: I, II, III\n- Histology: Infiltrating Ductal Carcinoma, Infiltrating Lobular Carcinoma, Mucinous Carcinoma\n- ER status: Positive\/Negative\n- PR status: Positive\/Negative\n- HER2 status: Positive\/Negative\n- Surgery_type: Lumpectomy, Simple Mastectomy, Modified Radical Mastectomy, Other\n- DateofSurgery: Date on which surgery was performed (in DD-MON-YY) DateofLast_Visit: Date of last visit (in DD-MON-YY) [can be null, in case the patient didn\u2019t visited again after the surgery]\n\nPatient_Status: Alive\/Dead [can be null, in case the patient didn\u2019t visited again after the surgery and there is no information available whether the patient is alive or dead].\n\n## Base Checklist\n#### Shape Analysis :\n- **target feature** : Patient_Status\n- **rows and columns** : 341 , 16\n- **features types** : qualitatives : 11 , quantitatives : 5\n- **NaN analysis** :\n    - NaN (1 feature > 5 % of NaN)\n\n#### Features Analysis :\n- **Target Analysis** :\n    - Balanced (Oui\/Non) : Non\n    - Percentages : 79% Alive","d21a3eee":"# Training Artificial Neural Network","868bdd67":"Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet's implement a basic example, which uses the <code>DataFrame.sample<\/code> method to get random samples each class:","07749c98":"# Training models\n## Models overview","ac8d4f24":"## Using KNN","c24b9dcc":"# Conclusion\n\n#### According to the results shown above, these models (RF, AdaBoost, KNN, SVM, XGBoost, LR, ANN) can't make the classification between Dead and Alive patients.\n#### Best we can do is getting a 1\/2 chance of guessing right...\n\n## Hypothesis\n\n- The features have no impact on the target\n- There isn't enough rows in the dataset (need more people)\n- The dataset isn't representative of the population\n- As we undersampled the dataset, we only have 66*2 rows in the end. I could have tried to oversample instead","0d499ced":"## Using RandomForest","ef901b76":"## Using SVM","c845072b":"## Using XGBoost"}}