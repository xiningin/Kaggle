{"cell_type":{"0255b02d":"code","721a6160":"code","7040d441":"code","3bc9ab9d":"code","ea3af5cd":"code","4cf6d519":"code","3616740d":"code","95e4e39d":"code","e381a53c":"code","117225bd":"code","befb0a2e":"code","271b76e7":"code","b3dd5a79":"code","bc327e32":"code","08141b71":"code","56fc9e46":"code","906800b8":"code","12791ee7":"code","b57d63d9":"code","8fb84357":"code","e4b3ce0e":"code","b985c811":"code","345a5358":"code","aa8e07b5":"code","15812e20":"code","cf58aab0":"code","00852c0c":"code","ea878ceb":"code","4d9f4d16":"code","b159d5b6":"code","2f03c1cb":"code","e300a0a1":"markdown","ea533672":"markdown","432ddd33":"markdown","d94a59fb":"markdown","6551c655":"markdown","ae67554d":"markdown","5f185e6a":"markdown","dd194f26":"markdown","40ae3433":"markdown","825fbed1":"markdown","a35f379f":"markdown"},"source":{"0255b02d":"import pandas as pd\nimport io\nimport requests\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport gc\nimport random\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nimport lightgbm","721a6160":"dataset = pd.read_csv(\"\/kaggle\/input\/brasilian-houses-to-rent\/houses_to_rent_v2.csv\")\ndataset.shape","7040d441":"dataset.head()","3bc9ab9d":"dataset.columns","ea3af5cd":"dataset.describe()","4cf6d519":"dataset['city'].unique()","3616740d":"dataset['animal'].unique()","95e4e39d":"dataset['furniture'].unique()","e381a53c":"dataset['floor'].unique()","117225bd":"dataset['city'] = dataset['city'].replace(['S\u00e3o Paulo', 'Porto Alegre', 'Rio de Janeiro', 'Campinas','Belo Horizonte'],['Sao_Paulo','Porto_Alegre','Rio_Janeiro','Campinas','Belo_Horizonte'])\ndataset['city'].unique()","befb0a2e":"dataset['animal'] = dataset['animal'].replace(['acept', 'not acept'],['acept', 'not_acept'])\ndataset['animal'].unique()","271b76e7":"dataset['animal'] = dataset['animal'].replace(['furnished', 'not furnished'],['furnished', 'not_furnished'])\ndataset['animal'].unique()","b3dd5a79":"dataset['floor'] = dataset['floor'].replace(['7', '20', '6', '2', '1', '-', '4', '3', '10', '11', '24', '9',\n                                             '8', '17', '18', '5', '13', '15', '16', '14', '26', '12', '21',\n                                             '19', '22', '27', '23', '35', '25', '46', '28', '29', '301', '51','32'],\n                                            ['7', '20', '6', '2', '1', '0', '4', '3', '10', '11', '24', '9',\n                                             '8', '17', '18', '5', '13', '15', '16', '14', '26', '12', '21',\n                                             '19', '22', '27', '23', '35', '25', '46', '28', '29', '301', '51','32'])\ndataset['floor'] = dataset['floor'].astype(np.float64)\ndataset['floor'].unique()","bc327e32":"dataset['id'] = dataset.index*1\ndataset['id']","08141b71":"dataset['target'] = dataset['total (R$)'].astype(np.float64)\ndataset = dataset.drop(columns=[\"total (R$)\"])","56fc9e46":"dataset.columns","906800b8":"dataset.columns = ['city', 'area', 'rooms', 'bathroom', 'parking_spaces', 'floor',\n                   'animal', 'furniture', 'hoa', 'rent_amount',\n                   'property_tax', 'fire_insurance', 'id', 'target']\ndataset.head()","12791ee7":"#### Generate Metadata Function\n\ndef GenerateMetadata(train,var_id,targetname): \n    print('Running metadata...')\n    \n    for ids in var_id:\n        print('Renaming ---> ', ids,'to ---> ', 'ID_'+ids)\n        train = train.rename(columns={ids: 'ID_'+ids})\n   \n    train = train.rename(columns={targetname: 'target'})\n    # Verifying type of columns\n    t = []\n    for i in train.columns:\n            t.append(train[i].dtype)\n\n    n = []\n    for i in train.columns:\n            n.append(i)\n\n    aux_t = pd.DataFrame(data=t,columns=[\"Type\"])\n    aux_n = pd.DataFrame(data=n,columns=[\"Features\"])\n    df_tipovars = pd.merge(aux_n, aux_t, left_index=True, right_index=True) \n\n    data = []\n    for f in train.columns:\n        # Defining variable roles:\n        if f == 'target':\n            role = 'target'\n        elif f[0:3] == 'ID_':\n            role = 'id'\n        else:\n            role = 'input'\n\n        # Defining variable types: nominal, ordinal, binary ou interval\n        if f == 'target':\n            level = 'binary'\n        if train[f].dtype == 'object' or f == 'id': \n            level = 'nominal'\n        elif train[f].dtype in ['float','float64'] :\n            level = 'interval'\n        elif train[f].dtype in ['int','int64','int32'] :\n            level = 'ordinal'\n        else:\n            level = 'NA'\n\n        # Remove IDs\n        keep = True\n        if f[0:3] == 'ID_':\n            keep = False\n\n        #  Defining the type of input table variables\n        dtype = train[f].dtype\n\n        # Metadata list\n        f_dict = {\n            'Features': f,\n            'Role': role,\n            'Level': level,\n            'Keep': keep,\n            'Type': dtype\n        }\n        data.append(f_dict)\n\n    meta = pd.DataFrame(data, columns=['Features', 'Role', 'Level', 'Keep', 'Type'])\n\n    # Cardinality of columns\n    card = []\n\n    v = train.columns\n    for f in v:\n        dist_values = train[f].value_counts().shape[0]\n        f_dict = {\n                'Features': f,\n                'Cardinality': dist_values\n            }\n        card.append(f_dict)\n\n    card = pd.DataFrame(card, columns=['Features', 'Cardinality'])\n\n    metadata = pd.merge(meta, card, on='Features')\n    print('Metadada successfully completed')\n    return metadata, train ","b57d63d9":"id_list = ['id']\ntargetname = 'target'\nmetadata, dataset_01 = GenerateMetadata(dataset,id_list,targetname)","8fb84357":"metadata","e4b3ce0e":"### Convert numbers to \"float64\" and categorical to \"str\"\n\nnumeric_list = metadata[((metadata.Level  == 'ordinal')|(metadata.Level == 'interval')) & (metadata.Role == 'input')]\ncategory_list = metadata[(metadata.Level  == 'nominal') & (metadata.Role == 'input')]\n\nnumeric_list = list(numeric_list['Features'].values)\ncategory_list = list(category_list['Features'].values)","b985c811":"dataset_02 = dataset_01[numeric_list].astype(np.float64)\ndataset_03 = pd.merge(dataset_02, dataset_01[category_list].astype(np.str), left_index=True, right_index=True)\ndataset_03.shape","345a5358":"dataset_03['ID_id'] = dataset_01['ID_id'].values\ndataset_03['target'] = dataset_01['target'].values\ndataset_03.shape","aa8e07b5":"def DataPrep(metadados,input_df,var_id,targetname):\n    \n    print('Starting data preparation ...')\n    \n    #-------------- Handling missing of numeric columns -----------------\n    input_df.rename(columns={var_id: 'id', targetname: 'target'}, inplace=True)\n    df_00 = input_df\n    targetname = 'target'\n    print('Executing')\n    \n    #--------- Numeric Features --------------------\n    vars_numericas_df = metadados[((metadados.Level  == 'ordinal')|(metadados.Level == 'interval')) & (metadados.Role == 'input')]\n    lista_vars_numericas = list(vars_numericas_df['Features'])\n    df01 = df_00[lista_vars_numericas]\n    df01 = df01.fillna(df01[lista_vars_numericas].mean())\n    df01 = df01.round(4)\n    \n    print('Missings done')\n    \n    #-------------- Numeric Features - Standart Scaler -----------------\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    df01 = df01.astype(float)\n    df01[lista_vars_numericas] = scaler.fit_transform(df01[lista_vars_numericas])\n    \n    print('Normalization done')\n\n    #--------- Nominal Features - Low Cardinality --------------------\n    vars_char_baix_cardin_df = metadados[(metadados.Level  == 'nominal') & (metadados.Role == 'input') & (metadados.Cardinality <= 50)]\n    lista_char_baix_cardin_df = list(vars_char_baix_cardin_df['Features'])\n    \n    df_00[lista_char_baix_cardin_df].apply(lambda x: x.fillna(x.mode, inplace=True))\n    df02 = df_00[lista_char_baix_cardin_df]\n    \n    df03 = pd.get_dummies(df02,columns=lista_char_baix_cardin_df,drop_first=True,\n                          prefix=lista_char_baix_cardin_df,prefix_sep='_')\n    print('Dummifications done')    \n    \n    #--------- Nominal Features - High Cardinality --------------------\n    vars_char_alta_cardin_df = metadados[(metadados.Level  == 'nominal') & (metadados.Role == 'input') & (metadados.Cardinality > 50)]\n    lista_char_alta_cardin_df = list(vars_char_alta_cardin_df['Features'])\n    \n    df_00[lista_char_alta_cardin_df].apply(lambda x: x.fillna(x.mode, inplace=True)) \n    df04 = df_00[lista_char_alta_cardin_df]\n\n    def MultiLabelEncoder(columnlist,dataframe):\n        for i in columnlist:\n            labelencoder_X=LabelEncoder()\n            dataframe[i]=labelencoder_X.fit_transform(dataframe[i])\n\n    MultiLabelEncoder(lista_char_alta_cardin_df,df04)\n    print('Label Encodings done')\n    \n    #---------- Checking IDs -----------------------\n    vars_ids_df = metadados[(metadados.Role  == 'id')]\n    lista_ids = list(vars_ids_df['Features'])\n\n    df1_3 = pd.merge(df01, df03, left_index=True, right_index=True)\n    df1_3_4 = pd.merge(df1_3, df04, left_index=True, right_index=True)\n    \n    lista_vars_keep = lista_ids + [targetname]\n    \n    df_out = pd.merge(input_df[lista_vars_keep], df1_3_4, left_index=True, right_index=True)    \n    \n    print('Data Preparation Sucess')\n    \n    return df_out","15812e20":"dataset_04 = DataPrep(metadata, dataset_03,'id','target')\ndataset_04.shape","cf58aab0":"dataset_04.head()","00852c0c":"## Train\/Test split\n\nX = dataset_04.drop(['target','ID_id'], axis=1)\ny = dataset_04[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n\nX_train.shape, X_test.shape","ea878ceb":"model_lgbm = lightgbm.LGBMRegressor(n_estimators = 300,\n                                    learning_rate = 0.05,\n                                    max_depth = 6,\n                                    num_leaves = 40,\n                                    random_state = 42)\n\nmodel_lgbm.fit(X_train, y_train)\n\ny_pred_train = model_lgbm.predict(X_train)\ny_pred_test = model_lgbm.predict(X_test)\n\nresidual_train = (y_train - y_pred_train).astype(\"float\")\nresidual_test = (y_test - y_pred_test).astype(\"float\")","4d9f4d16":"print('Train Set Performance \\n')\nprint('R-Squared: ', np.round(metrics.r2_score(y_train, y_pred_train, multioutput='variance_weighted'),2))\nprint('Mean Absolute Error: ', np.round(metrics.mean_absolute_error(y_train, y_pred_train),2))  \nprint('Mean Squared Error: ', np.round(metrics.mean_squared_error(y_train, y_pred_train),2))  \nprint('Root Mean Squared Error: ', np.round(np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)),2))","b159d5b6":"print('Test Set Performance \\n')\nprint('R-Squared: ', np.round(metrics.r2_score(y_test, y_pred_test, multioutput='variance_weighted'),2))\nprint('Mean Absolute Error: ', np.round(metrics.mean_absolute_error(y_test, y_pred_test),2))  \nprint('Mean Squared Error: ', np.round(metrics.mean_squared_error(y_test, y_pred_test),2))  \nprint('Root Mean Squared Error: ', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)),2))","2f03c1cb":"ax = lightgbm.plot_importance(model_lgbm, max_num_features=15)\nplt.show()","e300a0a1":"## Contents\n\n\n### 1) Import Data\n\n\n### 2) Editing Data\n\n\n### 3) Generate Metadata\n\n\n### 4) Fast DataPrep - Missing Treatment, Dummification and Label Encoding\n\n\n### 5) Training Models","ea533672":"### Get Variable Importance","432ddd33":"### 1) Import Data","d94a59fb":"### Get Model Performance","6551c655":"### 5) Training Models","ae67554d":"### Train\/Test Split","5f185e6a":"# Brazilian Houses to Rent\n\n\n\n### Dataset\n\n- Este dataset possui 6079 casas para alugar com 13 classes diferentes. This dataset contains 6079 houses to rent with 13 diferent features\n- Webcrawler de informa\u00e7\u00f5es abertas de um site imobili\u00e1rio. Webcrawler from open information from real state website. (https:\/\/www.quintoandar.com.br\/)\n","dd194f26":"## 3) Fast DataPrep - Missing Treatment, Dummification and Label Encoding","40ae3433":"## Import Libraries","825fbed1":"### 2) Editing Data","a35f379f":"### 3) Generate Metadata\n\nI use a function to generate metadata of dataset. The goal here is make DataPrep easier."}}