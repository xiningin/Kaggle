{"cell_type":{"65071997":"code","93921942":"code","6ab68366":"code","11c49a8b":"code","884e7e94":"code","527df317":"code","9ceb68d6":"code","0720f617":"code","76285847":"code","b2a56dd5":"code","d2f19fbc":"code","95d7d3d8":"code","82920ef7":"code","f90efc1e":"code","55cbd6fa":"code","0e17e6e9":"code","bcb6f8a2":"code","f8f811f6":"code","9a937372":"code","a222f9fb":"code","75f8c56f":"code","e9d037cc":"code","161a7604":"code","05478075":"code","ea073db8":"code","3702dd44":"code","1014ec77":"code","8fb4ff86":"code","5013ae9b":"code","b119061d":"code","d3c2a237":"code","b5d7c526":"markdown","63a87235":"markdown","8eb76e6d":"markdown","65e8128f":"markdown","412065a1":"markdown","074b3b6d":"markdown","61a111c0":"markdown","8d2833e9":"markdown","7452b248":"markdown","0c966578":"markdown","bf53727a":"markdown","57d588c2":"markdown","bf0a33d2":"markdown","96234c7a":"markdown","2ba1fa67":"markdown","da4cdb50":"markdown","46817c40":"markdown","fbe8b6a5":"markdown","fb63493d":"markdown","cbdfaab7":"markdown"},"source":{"65071997":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93921942":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import ensemble\nfrom xgboost import XGBClassifier\nimport time","6ab68366":"import warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)","11c49a8b":"train = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","884e7e94":"df_train = train.copy()\ndf_test = test.copy()","527df317":"df_train.isnull().any().sum()","9ceb68d6":"df_test.isnull().any().sum()","0720f617":"X_train= df_train.drop(['label'],axis = 1)\nX_train","76285847":"X_train.shape","b2a56dd5":"X_test = df_train['label']\nX_test","d2f19fbc":"X_test.shape","95d7d3d8":"y_test = df_test.drop(['label'],axis = 1)\ny_test.shape","82920ef7":"X_train = X_train.astype('float32')\ny_test = y_test.astype('float32')\nX_train \/= 255.0\ny_test \/=255.0","f90efc1e":"seed = 99\nnp.random.seed(seed)\nX_train, X_val, y_train, y_val = train_test_split(X_train, X_test, test_size=0.1, random_state = seed)","55cbd6fa":"pca = PCA(n_components=100, random_state=42)\nX_train_pca =pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_val)\ny_test_pca =pca.transform(y_test)","0e17e6e9":"X_train_pca.shape","bcb6f8a2":"X_train_PCA1 = pd.DataFrame(X_train_pca)\nX_test_PCA1 = pd.DataFrame(X_test_pca)","f8f811f6":"\n# 1. LR Model\nstart1 = time.time()\n\nlogistic = LogisticRegression(max_iter=200, solver='liblinear')\nlogistic.fit(X_train_PCA1, y_train)\n\nend1 = time.time()\nlr_time = end1-start1\n\n# 2. SVC Model\nstart2 = time.time()\n\nsvc = SVC(C=13,kernel='rbf',gamma=\"auto\",probability = True)\nsvc.fit(X_train_PCA1, y_train)\n\nend2 = time.time()\nsvm_time = end2-start2\n\n# 3. Random Forest\nstart3 = time.time()\n\nrandom_forest = RandomForestClassifier(criterion='entropy', max_depth=70, n_estimators=100)\nrandom_forest.fit(X_train_PCA1, y_train)\n\nend3 = time.time()\nforest_time = end3-start3\n\n# 4. Gradient Boosting Method \nstart4 = time.time()\n\nGradient = ensemble.GradientBoostingClassifier(n_estimators=100)\nGradient.fit(X_train_PCA1, y_train)\n\nend4 = time.time()\ngradient_time = end4-start4\n\n# 5. XGBoost Method\nstart5 = time.time()\n\nxgb = XGBClassifier(use_label_encoder=False,objective=\"multi:softmax\",eval_metric=\"merror\")\nxgb.fit(X_train_PCA1, y_train.ravel())\n\nend5 = time.time()\nxgb_time = end5-start5\n\n\nprint(\"LR Time: {:0.2f} minute\".format(lr_time\/60.0))\nprint(\"SVC Time: {:0.2f} minute\".format(svm_time\/60.0))\nprint(\"Random Forest Time: {:0.2f} minute\".format(forest_time\/60.0))\nprint(\"Gradient Boosting Time: {:0.2f} minute\".format(gradient_time\/60.0))\nprint(\"XGBoost Time: {:0.2f} minute\".format(xgb_time\/60.0))","9a937372":"y_train_lr = logistic.predict(X_train_PCA1)\ny_pred_lr = logistic.predict(X_test_pca)\nlogistic_train = metrics.accuracy_score(y_train,y_train_lr )\nlogistic_accuracy = metrics.accuracy_score(y_val, y_pred_lr)\n\nprint(\"Train Accuracy score: {}\".format(logistic_train))\nprint(\"Test Accuracy score: {}\".format(logistic_accuracy))\nprint(metrics.classification_report(y_val, y_pred_lr))","a222f9fb":"con_matrix = pd.crosstab(pd.Series(y_val.values.flatten(), name='Actual' ),pd.Series(y_pred_lr, name='Predicted'))\nplt.figure(figsize = (9,6))\nplt.title(\"Confusion Matrix on Logistic Regression\")\nsns.heatmap(con_matrix, cmap=\"Blues\", annot=True, fmt='g')\nplt.show()","75f8c56f":"y_train_svc = svc.predict(X_train_PCA1)\ny_pred_svc = svc.predict(X_test_pca)\nsvc_train = metrics.accuracy_score(y_train,y_train_svc)\nsvc_accuracy = metrics.accuracy_score(y_val, y_pred_svc)\n\nprint(\"Train Accuracy score: {}\".format(svc_train))\nprint(\"Test Accuracy score: {}\".format(svc_accuracy))\nprint(metrics.classification_report(y_val, y_pred_svc))","e9d037cc":"con_matrix = pd.crosstab(pd.Series(y_val.values.flatten(), name='Actual' ),pd.Series(y_pred_svc, name='Predicted'))\nplt.figure(figsize = (9,6))\nplt.title(\"Confusion Matrix on SVC\")\nsns.heatmap(con_matrix, cmap=\"Blues\", annot=True, fmt='g')\nplt.show()","161a7604":"y_train_forest = random_forest.predict(X_train_PCA1)\ny_pred_forest = random_forest.predict(X_test_pca)\nrandom_forest_train = metrics.accuracy_score(y_train,y_train_forest)\nrandom_forest_accuracy = metrics.accuracy_score(y_val, y_pred_forest)\n\nprint(\"Train Accuracy score: {}\".format(random_forest_train))\nprint(\"Test Accuracy score: {}\".format(random_forest_accuracy))\nprint(metrics.classification_report(y_val, y_pred_forest))","05478075":"con_matrix = pd.crosstab(pd.Series(y_val.values.flatten(), name='Actual' ),pd.Series(y_pred_forest, name='Predicted'))\nplt.figure(figsize = (9,6))\nplt.title(\"Confusion Matrix on Logistic Regression\")\nsns.heatmap(con_matrix, cmap=\"Blues\", annot=True, fmt='g')\nplt.show()","ea073db8":"y_train_gradient = Gradient.predict(X_train_PCA1)\ny_pred_gradient = Gradient.predict(X_test_pca)\ngradient_train = metrics.accuracy_score(y_train,y_train_gradient)\ngradient_accuracy = metrics.accuracy_score(y_val, y_pred_gradient)\n\nprint(\"Train Accuracy score: {}\".format(gradient_train))\nprint(\"Test Accuracy score: {}\".format(gradient_accuracy))\nprint(metrics.classification_report(y_val, y_pred_gradient))","3702dd44":"con_matrix = pd.crosstab(pd.Series(y_val.values.flatten(), name='Actual' ),pd.Series(y_pred_gradient, name='Predicted'))\nplt.figure(figsize = (9,6))\nplt.title(\"Confusion Matrix on Logistic Regression\")\nsns.heatmap(con_matrix, cmap=\"Blues\", annot=True, fmt='g')\nplt.show()","1014ec77":"y_train_xgboost = xgb.predict(X_train_PCA1)\ny_pred_xgboost = xgb.predict(X_test_pca)\nxgb_train = metrics.accuracy_score(y_train,y_train_xgboost)\nxgb_accuracy = metrics.accuracy_score(y_val, y_pred_xgboost)\n\nprint(\"Train Accuracy score: {}\".format(xgb_train))\nprint(\"Test Accuracy score: {}\".format(xgb_accuracy))\nprint(metrics.classification_report(y_val, y_pred_xgboost))","8fb4ff86":"con_matrix = pd.crosstab(pd.Series(y_val.values.flatten(), name='Actual' ),pd.Series(y_pred_xgboost, name='Predicted'))\nplt.figure(figsize = (9,6))\nplt.title(\"Confusion Matrix on Logistic Regression\")\nsns.heatmap(con_matrix, cmap=\"Blues\", annot=True, fmt='g')\nplt.show()","5013ae9b":"Train_Accuracy = [logistic_train,svc_train,random_forest_train,gradient_train,xgb_train]\nTest_Accuracy = [logistic_accuracy,svc_accuracy,random_forest_accuracy,gradient_accuracy,xgb_accuracy]\ndata1 = {\n    'Algorithm': ['Logistic Regression','SVC','Random Forest Classifier','Gradient Boosting','XGBoost'],\n    'Train Accuracy':Train_Accuracy,\n    'Test Accuracy':Test_Accuracy\n}\n\ndf1 = pd.DataFrame(data1)","b119061d":"df1","d3c2a237":"fig = go.Figure(data=[\n    go.Bar(name='train set', x=data1['Algorithm'], y=data1['Train Accuracy'],text=np.round(data1['Train Accuracy'],2),textposition='outside'),\n    go.Bar(name='test set', x=data1['Algorithm'], y=data1['Test Accuracy'],text=np.round(data1['Test Accuracy'],2),textposition='outside')\n])\n\nfig.update_layout(barmode='group',title_text='Accuracy Comparison On Different Models',yaxis=dict(\n        title='Accuracy'))\nfig.show()","b5d7c526":"# Separating data and label","63a87235":"***Logistic Regression Report and Analysis***","8eb76e6d":"# Load dataset","65e8128f":"**3. Test Accuracy**\n\n\n# SVC>XGBoost>Random Forest>Gradient Boost>Logistic Regression","412065a1":"# **Conclusion**","074b3b6d":"**1. **Computation Time****\n\n\n# Gradient Boost>XGBoost>SVC>Random Forest>Logistic Regeression","61a111c0":"# Predicting the models","8d2833e9":"***Random Forest Report and Analysis***","7452b248":"# Split training and test sets ","0c966578":"# Evaluate the model","bf53727a":"***XGBoost Report and Analysis***","57d588c2":"***SVM Report and Analysis***","bf0a33d2":"# Normalization\nThe Pixel Values are often stored as Integer Numbers in the range 0 to 255, the range that a single 8-bit byte can offer. They need to be scaled down to [0,1] in order for Optimization Algorithms to work much faster. Here, we acheive Zero Mean and Unit Variance.","96234c7a":"# Model Comparison","2ba1fa67":"***Gradient Boosting Report and Analysis***","da4cdb50":"# Dimensionality Reduction using PCA\nThe advantage of PCA (and dimensionality reduction in general) is that it compresses the data down to something that is more effectively modeled. This means that it will, for example, compress away highly correlated and colinear variables, a useful thing to do when trying to run models that would otherwise be sensitive to these sort of data problems.","46817c40":"The algorithms which we are using for classification and analysis of the data are: \n\n1) Logistic Regression\n\n2) SVM\n\n3) Random Forest\n\n4) Gradient Boosting\n\n5) XGBoost","fbe8b6a5":"# Examine NaN values","fb63493d":"# Import packages","cbdfaab7":"**2. Train Accuracy**\n\n\n# Random Forest>XGBoost>SVC>Gradient Boost>Logistic Regression"}}