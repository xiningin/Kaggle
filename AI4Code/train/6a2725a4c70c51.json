{"cell_type":{"de9ffdf6":"code","06b36650":"code","8f4fc612":"code","8792d26a":"code","03ead841":"code","162e11cd":"code","b9d64ef8":"code","3867331b":"code","f3b41e2e":"code","bd14bc23":"code","5b85fd6d":"code","50d421d8":"code","5675e1da":"code","85ffa5a6":"code","cb4fb6e0":"code","03cc7c9d":"code","29d52933":"code","101fc61f":"code","57c08f65":"code","87aab58b":"code","a3216b5d":"code","fc0f2955":"code","3df7a748":"code","7a6c9c53":"code","68946c3a":"code","26ce9988":"code","dca177cd":"code","afe732d8":"code","84b68761":"code","fad09600":"code","299a0954":"code","78c8d183":"code","becedf09":"code","64b4db4b":"code","6464be65":"code","b53d0be4":"markdown","75ed5490":"markdown","e6f1b3a8":"markdown","3361aaee":"markdown","13c907e0":"markdown","846ed359":"markdown","abf0c8f8":"markdown","004b4093":"markdown","7fc3af3b":"markdown"},"source":{"de9ffdf6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","06b36650":"os.getcwd()","8f4fc612":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ncombine = [train_df, test_df]\n\ntrain_df.columns.values","8792d26a":"train_df.head()","03ead841":"train_df.info()\nprint('-'*40)\ntest_df.info()","162e11cd":"# Balanced? Yes\n\nprint(train_df[\"Survived\"].value_counts())","b9d64ef8":"print(train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('=='*40)\nprint(train_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('=='*40)\nprint(train_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('=='*40)\nprint(train_df[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('=='*40)\nprint(train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False))","3867331b":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","f3b41e2e":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","bd14bc23":"grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","5b85fd6d":"# By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nprint(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","50d421d8":"# feature creation\n\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","5675e1da":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","85ffa5a6":"# convert the categorical titles to ordinal.\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","cb4fb6e0":"# safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.\n\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","03cc7c9d":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","29d52933":"# fill nan for embarked\n\nfreq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","101fc61f":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","57c08f65":"# fill nans with zero for dataset in combine:\n\nfor dataset in combine:\n    for col in dataset.columns.values:\n        dataset[col] = dataset[col].fillna(0)\n    ","87aab58b":"train_df.head()","a3216b5d":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","fc0f2955":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","3df7a748":"# Support Vector Machines\nfrom sklearn.svm import SVC, LinearSVC\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","7a6c9c53":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","68946c3a":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","26ce9988":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","dca177cd":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","afe732d8":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","84b68761":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","fad09600":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree\n","299a0954":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","78c8d183":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": decision_tree.predict(X_test)\n    })","becedf09":"submission.head()","64b4db4b":"os.listdir('\/kaggle\/')","6464be65":"submission.to_csv('DECISION_TREE_submission.csv', index=False)","b53d0be4":"- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine","75ed5490":"### Train df\n\n- Cabin, Age, Embarked have null values. \n- Total 891 datapoints and 11 feaures\n\n### Test df\n\n- Age, fare, cabin have null values\n- 418x12","e6f1b3a8":"**Observations**\n\n- `pclass = 1` passengers had hig survival rate comparedto 2 > 3 (HIGHT CORRELATION\n- females survived in greater proportion \n- Make `SibSp` and 'Parch' into one feature (BECAUSE WE DONT OBSERVE ANY CORRELATION HERE)\n- Who embarked at C had higher survival chances\n","3361aaee":"# Titanic Dataset","13c907e0":"- `PassengerId` is useless\n- `Pclass` should be one hot encoded\n- Name contains both nicknames, full names and alt. names\n- `sex` should be one-hot encoded\n- `Ticket` must be discarded unless meaningful\n- Cabin is having empty fields\n- Embarked?","846ed359":"# Data Wrangling","abf0c8f8":"# ACCURACY WITHOUT MUCH DATA WRANGLING \/ FEATURE EXTRACTION","004b4093":"# Variable Notes\n\n`pclass:` \nA proxy for socio-economic status (SES)\n\n- 1st = Upper\n- 2nd = Middle\n- 3rd = Lower\n\n`age`: \nAge is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n`sibsp`: \nThe dataset defines family relations in this way...\n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n`parch`: \nThe dataset defines family relations in this way...\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","7fc3af3b":"**OBSERVATIONS:**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n"}}