{"cell_type":{"54d5a5e0":"code","c6e4c9d2":"code","37fd97c7":"code","b76b5937":"code","c3d571a9":"code","f8a276db":"code","13a7351d":"code","640a6a5e":"code","a9445a13":"code","2b5e924b":"code","b4188fae":"code","2f8ad54c":"code","4e773005":"code","50a6f0ea":"code","34c391d2":"code","b140e36e":"code","bd2eb84c":"code","3b4b6298":"code","8ab95807":"code","60bfbfa6":"code","3334a4be":"code","62afb4e8":"code","1cafad60":"code","02b25578":"code","16365005":"code","77268df8":"code","e071fcd4":"code","e7ad5276":"code","69fe530e":"code","ef06f84c":"code","8f0e1621":"code","83b94462":"code","8355f8ac":"code","054eb466":"code","3c466fad":"code","9628457d":"code","439c38a7":"code","2d4a9900":"code","b3c912ca":"code","806eaed5":"code","c2a1342d":"code","6129ff72":"code","e7f6c94d":"code","f598102c":"code","01f51d41":"code","5aaf1512":"code","5ec7eb1d":"code","3959a068":"code","6d375279":"code","8738cdaf":"code","cd48d909":"code","89c1fb29":"code","680e2f58":"code","17542e84":"code","a45f864b":"code","97bb5d68":"code","d36bea2b":"code","32f5feaf":"code","f8d755ee":"code","41b90375":"code","01362793":"code","4f87a514":"markdown","cfe0ca05":"markdown","ee1bf46a":"markdown","99f071f4":"markdown","1a69bbae":"markdown","a8228108":"markdown","54dde38d":"markdown","2a37e786":"markdown","7f6ad551":"markdown","e6d3d866":"markdown","938221f3":"markdown","bb4d9da1":"markdown","c7deee96":"markdown","c19c1047":"markdown","7e0bd4a5":"markdown","5d4bf4be":"markdown","35c654b7":"markdown","7cf522fb":"markdown","4a080ff7":"markdown","9384379a":"markdown","a3dc26a7":"markdown","5e8694ea":"markdown","a00ad608":"markdown","d30173d8":"markdown","992f9277":"markdown","3c0c071f":"markdown","daa891d1":"markdown","8b2a499d":"markdown","1ed4959a":"markdown"},"source":{"54d5a5e0":"# Author: Caleb Woy\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sb # plotting\nimport os # Reading data\nimport matplotlib.pylab as plt # plotting hyperparamter cost curves\nfrom sklearn import preprocessing # scaling features\nimport random # random centroid generation","c6e4c9d2":"path_to_data = \"\/kaggle\/input\/\"\n\n# Loading the training and test data sets into pandas\nsmall_xy = pd.read_csv(path_to_data + \"\/small_Xydf.csv\", header=0)\nsmall_xy = small_xy.drop(columns=[\"Unnamed: 0\"])\n\nlarge_xy = pd.read_csv(path_to_data + \"\/large_Xydf.csv\", header=0)\nlarge_xy = large_xy.drop(columns=[\"Unnamed: 0\"])\n\nred_wine = pd.read_csv(path_to_data + \"\/winequality-red.csv\", sep=';', \n                       header=0)\nred_wine['y'] = red_wine.apply(lambda x: int(x['quality'] - 3), axis=1)","37fd97c7":"# A two dimensional data set of 100 observations.\n# y is the label representing the true clustering.\nsmall_xy.head()","b76b5937":"# A two dimensional data set of 1000 observations.\n# y is the label representing the true clustering.\nlarge_xy.head()","c3d571a9":"# An eleven dimensional data set of 1600 observations.\n# quality is the label representing the true clustering.\n# I've added the y label to make feeding the data into \n# my own algorithm easier.\nred_wine.head()","f8a276db":"\"\"\"\nImplementing K-means clustering algorithm from scratch.\n\ndata: pandas dataframe, X data to be used for clustering\nk: int, the number of centroids to assign\nverbose: Boolean, default is true, if false nothing is printed\n\"\"\"\ndef Kmeans(data, k, verbose = True):\n    random.seed(k)\n    if verbose:\n        print(f'TESTING FOR K = {k}')\n    # scale the x data uniformly\n    x = data.values # Get data as np array\n    standard_scaler = preprocessing.StandardScaler()\n    x_scaled = standard_scaler.fit_transform(x)\n    data = pd.DataFrame(x_scaled)\n    # calculating the max and min values for each column and storing them in \n    # an array\n    min_max = data.apply(lambda x:\n        pd.Series(index=['min','max'],data=[x.min(),x.max()]))\n    min_max_list = min_max.T.values.tolist()\n    # Randomly initialize centroids within max and min range of feature vectors\n    centroids = {}\n    for i in range(k):\n        centroids[i] = [ \n            random.uniform(x[0], x[1]) for x in min_max_list]\n    # Create arrays for assignments to check for convergence\n    curr = np.array([0 for i in range(data.shape[0])])\n    prev = np.array([1 for i in range(data.shape[0])])\n    # iterate until assignments don't change\n    while not (curr == prev).all():\n        # copy current into prev\n        prev = np.copy(curr)\n        if verbose:\n            print(f'.', end=' ')\n        # Assignment step\n        # iterate over each row in the data \n        for index, row in data.iterrows():\n            # set min container and flag\n            min_dist, min_centroid = float(\"inf\"), 0\n            # iterate over each centroid\n            for l in range(k):\n                # get the centroid vector\n                a = np.array(centroids[l])\n                # get the row vector\n                b = np.array(row.T.values.tolist())\n                # calculate euclidean distance\n                dist = np.linalg.norm(a - b)\n                # set min distance if min\n                if dist < min_dist:\n                    min_dist, min_centroid = dist, l\n            # assign current datum\n            curr[index] = min_centroid\n        # Getting the counts of each cluster assignment\n        curr_counter = np.array(curr)\n        unique, counts = np.unique(curr_counter, return_counts=True)\n        counts = dict(zip(unique, counts))\n        # init container for new centroids\n        new_centroids = {}\n        for i in range(k):\n            new_centroids[i] = [0 for x in min_max_list]\n        # Update step\n        # iterate over all data\n        for index, row in data.iterrows():\n            # retrieve row\n            row = row.T.values.tolist()\n            # retrieve centroid\n            cent = new_centroids[curr[index]]\n            # iterate over each dimension of the centroid and accumulate the\n            # avg distance\n            for j in range(len(cent)):\n                cent[j] += row[j] \/ counts[curr[index]]\n            new_centroids[curr[index]] = cent\n        # reassign centroid\n        centroids = new_centroids\n    if verbose:    \n        print('FINISHED')\n    return curr","13a7351d":"\"\"\"\nPrints the cluster SSE.\n\ndata: pandas dataframe, X data\nverbose: Boolean, default is true, if false nothing is printed\n\"\"\"\ndef cluster_sse(data, verbose = True):\n    # calculating cluster centroids\n    labels = data['y']\n    counts = labels.value_counts()\n    data = data.drop(columns=['y'])\n    centroids = {}\n    for i in range(len(counts)):\n        centroids[i] = [0 for j in range(data.shape[1])]\n    for index, row in data.iterrows():\n        point = np.array(row.T.values.tolist())\n        cent = centroids[labels[index]]\n        for j in range(len(cent)):\n            cent[j] += row[j] \/ counts[labels[index]]\n        centroids[labels[index]] = cent\n    # calculating cluster sse\n    sse = [0 for j in range(len(counts))]\n    for index, row in data.iterrows():\n        a = np.array(centroids[labels[index]])\n        b = np.array(row.T.values.tolist())\n        dist = np.linalg.norm(a - b)\n        sse[labels[index]] += dist\n    if verbose:\n        print(f'CLUSTER SSE IS: {sse}')\n    return sse","640a6a5e":"\"\"\"\nPrints the total SSE.\n\ndata: pandas dataframe, X data\nverbose: Boolean, default is true, if false nothing is printed\n\"\"\"\ndef total_sse(data, verbose = True):\n    # calculating cluster centroids\n    labels = data['y']\n    counts = labels.value_counts()\n    data = data.drop(columns=['y'])\n    centroids = {}\n    for i in range(len(counts)):\n        centroids[i] = [0 for j in range(data.shape[1])]\n    for index, row in data.iterrows():\n        point = np.array(row.T.values.tolist())\n        cent = centroids[labels[index]]\n        for j in range(len(cent)):\n            cent[j] += row[j] \/ counts[labels[index]]\n        centroids[labels[index]] = cent\n    # calculating cluster sse\n    sse = 0\n    for index, row in data.iterrows():\n        a = np.array(centroids[labels[index]])\n        b = np.array(row.T.values.tolist())\n        dist = np.linalg.norm(a - b)\n        sse += dist\n    if verbose:\n        print(f'TOTAL SSE IS: {sse}')\n    return sse","a9445a13":"\"\"\"\nPrints the cluster SSB.\n\ndata: pandas dataframe, X data\nverbose: Boolean, default is true, if false nothing is printed\n\"\"\"\ndef cluster_ssb(data, verbose = True):\n    # calculating cluster centroids\n    labels = data['y']\n    counts = labels.value_counts()\n    data = data.drop(columns=['y'])\n    centroids = {}\n    for i in counts.iteritems():\n        centroids[i[0]] = [0 for j in range(data.shape[1])]\n    for index, row in data.iterrows():\n        point = np.array(row.T.values.tolist())\n        cent = centroids[labels[index]]\n        for j in range(len(cent)):\n            cent[j] += row[j] \/ counts[labels[index]]\n        centroids[labels[index]] = cent\n     # calculating global centroid\n    glob_centroid = [0 for j in range(data.shape[1])]\n    for index, row in data.iterrows():\n        point = np.array(row.T.values.tolist())\n        for j in range(len(glob_centroid)):\n            glob_centroid[j] += row[j] \/ data.shape[0]\n    # calculating total ssb\n    ssb = 0\n    a = np.array(glob_centroid)\n    for key, b in centroids.items():\n        b = np.array(b)\n        dist = np.linalg.norm(a - b) * counts[key]\n        ssb += dist\n    if verbose:\n        print(f'TOTAL SSB IS: {ssb}')\n    return ssb","2b5e924b":"def test_Kmeans_small_large(data, k_collection, k_special = 0):\n    x = data.drop(columns=['y'])\n    for k in k_collection:\n        x['y'] = Kmeans(x, k)\n        # Calculating test SSE, SSB\n        cluster_sse(x)\n        sse = total_sse(x)\n        ssb = cluster_ssb(x)\n        print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n        if k == k_special:\n            plt.scatter(x['X0'], x['X1'], c=x['y'])\n            plt.show()\n        x = x.drop(columns=['y'])\n        print()","b4188fae":"def test_Kmeans_redwine(data, k, k_special = 0):\n    x = data.drop(columns=['quality', 'y'])\n    x['y'] = Kmeans(x, k)\n    # Calculating test SSE, SSB\n    cluster_sse(x)\n    sse = total_sse(x)\n    ssb = cluster_ssb(x)\n    print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n    if k == k_special:\n        sb.pairplot(x, \n                    vars=x.loc[:, x.columns != 'y'], \n                    hue ='y',\n                    diag_kind = 'hist')\n    x = x.drop(columns=['y'])\n    print()","2f8ad54c":"def Eval_Kmeans_scree(data, k_collection):\n    x = data.drop(columns=['y'])\n    scores = [0 for i in k_collection]\n    for index, k in enumerate(k_collection):\n        x['y'] = Kmeans(x, k, False)\n        # Calculating test SSE, SSB\n        sse = total_sse(x, False)\n        ssb = cluster_ssb(x, False)\n        scores[index] = (k, sse \/ (sse + ssb))\n        x = x.drop(columns=['y'])\n    plt.plot(*zip(*scores))\n    plt.title(\"Scree Plot\")\n    plt.ylabel(\"sse \/ (sse + ssb)\")\n    plt.xlabel(\"k\")\n    plt.show()","4e773005":"# Calculating true cluster SSE\nprint('TRUE ', end='')\nsse = cluster_sse(small_xy)","50a6f0ea":"# Calculating total SSE\nprint('TRUE ', end='')\nsse = total_sse(small_xy)","34c391d2":"# Calculating true cluster SSB\nprint('TRUE ', end='')\nssb = cluster_ssb(small_xy)","b140e36e":"# plotting true clustering\nplt.scatter(small_xy['X0'], small_xy['X1'], c=small_xy['y'])","bd2eb84c":"k_collection = [x for x in range(2, 10)]\nEval_Kmeans_scree(small_xy, k_collection)","3b4b6298":"k_collection = [4]\ntest_Kmeans_small_large(small_xy, k_collection, 4)","8ab95807":"# Calculating true cluster SSE\nprint('TRUE ', end='')\nsse = cluster_sse(large_xy)","60bfbfa6":"# Calculating total SSE\nprint('TRUE ', end='')\nsse = total_sse(large_xy)","3334a4be":"# Calculating true cluster SSB\nprint('TRUE ', end='')\nssb = cluster_ssb(large_xy)","62afb4e8":"# plotting true clustering\nplt.scatter(large_xy['X0'], large_xy['X1'], c=large_xy['y'])","1cafad60":"k_collection = [x for x in range(2, 10)]\nEval_Kmeans_scree(large_xy, k_collection)","02b25578":"k_collection = [5]\ntest_Kmeans_small_large(large_xy, k_collection, 5)","16365005":"# Calculating true cluster SSE\nprint('TRUE ', end='')\nred_wine_no_qual = red_wine.drop(columns=['quality'])\nsse = cluster_sse(red_wine_no_qual)","77268df8":"# Calculating total SSE\nprint('TRUE ', end='')\nsse = total_sse(red_wine_no_qual)","e071fcd4":"# Calculating true cluster SSB\nprint('TRUE ', end='')\nssb = cluster_ssb(red_wine_no_qual)","e7ad5276":"# plotting true clustering\nsb.pairplot(red_wine_no_qual, \n            hue='y', \n            vars=red_wine_no_qual.columns[:-1],\n            diag_kind = 'hist')","69fe530e":"# Testing on wine dataset, had to split them out of the loop to get\n# the seaborn plot to show in proper order. It takes a while to load.","ef06f84c":"k_collection = [x for x in range(2, 10)]\nEval_Kmeans_scree(red_wine.drop(columns=['quality']), k_collection)","8f0e1621":"test_Kmeans_redwine(red_wine, 5, 5)","83b94462":"from sklearn import cluster as cl # for testing K-means","8355f8ac":"def Eval_SKLearn_Kmeans_scree(data, k_collection):\n    x = data.drop(columns=['y'])\n    scores = [0 for i in k_collection]\n    for index, k in enumerate(k_collection):\n        kmeans = cl.KMeans(n_clusters = k)\n        x['y'] = kmeans.fit_predict(x)\n        # Calculating test SSE, SSB\n        sse = total_sse(x, False)\n        ssb = cluster_ssb(x, False)\n        scores[index] = (k, sse \/ (sse + ssb))\n        x = x.drop(columns=['y'])\n    plt.plot(*zip(*scores))\n    plt.title(\"Scree Plot\")\n    plt.ylabel(\"sse \/ (sse + ssb)\")\n    plt.xlabel(\"k\")\n    plt.show()","054eb466":"def test_SKLearn_Kmeans_small_large(data, k_collection, k_special = 0):\n    x = data.drop(columns=['y'])\n    for k in k_collection:\n        kmeans = cl.KMeans(n_clusters = k)\n        x['y'] = kmeans.fit_predict(x)\n        # Calculating test SSE, SSB\n        cluster_sse(x)\n        sse = total_sse(x)\n        ssb = cluster_ssb(x)\n        print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n        if k == k_special:\n            plt.scatter(x['X0'], x['X1'], c=x['y'])\n            plt.show()\n        x = x.drop(columns=['y'])\n        print()","3c466fad":"def test_SKLearn_Kmeans_redwine(data, k, k_special = 0):\n    x = data.drop(columns=['quality', 'y'])\n    kmeans = cl.KMeans(n_clusters = k)\n    x['y'] = kmeans.fit_predict(x)\n    # Calculating test SSE, SSB\n    cluster_sse(x)\n    sse = total_sse(x)\n    ssb = cluster_ssb(x)\n    print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n    if k == k_special:\n        sb.pairplot(x, \n                    vars=x.loc[:, x.columns != 'y'], \n                    hue ='y',\n                    diag_kind = 'hist')\n    x = x.drop(columns=['y'])\n    print()","9628457d":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_Kmeans_scree(small_xy, k_collection)","439c38a7":"test_SKLearn_Kmeans_small_large(small_xy, [4], k_special = 4)","2d4a9900":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_Kmeans_scree(large_xy, k_collection)","b3c912ca":"test_SKLearn_Kmeans_small_large(large_xy, [3], k_special = 3)","806eaed5":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_Kmeans_scree(red_wine.drop(columns=['quality']), k_collection)","c2a1342d":"test_SKLearn_Kmeans_redwine(red_wine, 5, k_special = 5)","6129ff72":"from sklearn.mixture import GaussianMixture as GMM","e7f6c94d":"def Eval_SKLearn_GMM_scree(data, k_collection):\n    x = data.drop(columns=['y'])\n    scores = [0 for i in k_collection]\n    for index, k in enumerate(k_collection):\n        gmm = GMM(n_components = k)\n        x['y'] = gmm.fit_predict(x)\n        # Calculating test SSE, SSB\n        sse = total_sse(x, False)\n        ssb = cluster_ssb(x, False)\n        scores[index] = (k, sse \/ (sse + ssb))\n        x = x.drop(columns=['y'])\n    plt.plot(*zip(*scores))\n    plt.title(\"Scree Plot\")\n    plt.ylabel(\"sse \/ (sse + ssb)\")\n    plt.xlabel(\"k\")\n    plt.show()","f598102c":"def test_SKLearn_GMM_small_large(data, k_collection, k_special = 0):\n    x = data.drop(columns=['y'])\n    for k in k_collection:\n        gmm = GMM(n_components = k)\n        x['y'] = gmm.fit_predict(x)\n        # Calculating test SSE, SSB\n        cluster_sse(x)\n        sse = total_sse(x)\n        ssb = cluster_ssb(x)\n        print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n        if k == k_special:\n            plt.scatter(x['X0'], x['X1'], c=x['y'])\n            plt.show()\n        x = x.drop(columns=['y'])\n        print()","01f51d41":"def test_SKLearn_GMM_redwine(data, k, k_special = 0):\n    x = data.drop(columns=['quality', 'y'])\n    gmm = GMM(n_components = k)\n    x['y'] = gmm.fit_predict(x)\n    # Calculating test SSE, SSB\n    cluster_sse(x)\n    sse = total_sse(x)\n    ssb = cluster_ssb(x)\n    print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n    if k == k_special:\n        sb.pairplot(x, \n                    vars=x.loc[:, x.columns != 'y'], \n                    hue ='y',\n                    diag_kind = 'hist')\n    x = x.drop(columns=['y'])\n    print()","5aaf1512":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_GMM_scree(small_xy, k_collection)","5ec7eb1d":"test_SKLearn_GMM_small_large(small_xy, [5], k_special = 5)","3959a068":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_GMM_scree(large_xy, k_collection)","6d375279":"test_SKLearn_GMM_small_large(large_xy, [3], k_special = 3)","8738cdaf":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_GMM_scree(red_wine.drop(columns=['quality']), k_collection)","cd48d909":"test_SKLearn_GMM_redwine(red_wine, 5, k_special = 5)","89c1fb29":"from sklearn.cluster import AgglomerativeClustering as AGG","680e2f58":"def Eval_SKLearn_agg_scree(data, k_collection):\n    x = data.drop(columns=['y'])\n    scores = [0 for i in k_collection]\n    for index, k in enumerate(k_collection):\n        agg = AGG(n_clusters = k)\n        x['y'] = agg.fit_predict(x)\n        # Calculating test SSE, SSB\n        sse = total_sse(x, False)\n        ssb = cluster_ssb(x, False)\n        scores[index] = (k, sse \/ (sse + ssb))\n        x = x.drop(columns=['y'])\n    plt.plot(*zip(*scores))\n    plt.title(\"Scree Plot\")\n    plt.ylabel(\"sse \/ (sse + ssb)\")\n    plt.xlabel(\"k\")\n    plt.show()","17542e84":"def test_SKLearn_agg_small_large(data, k_collection, k_special = 0):\n    x = data.drop(columns=['y'])\n    for k in k_collection:\n        agg = AGG(n_clusters = k)\n        x['y'] = agg.fit_predict(x)\n        # Calculating test SSE, SSB\n        cluster_sse(x)\n        sse = total_sse(x)\n        ssb = cluster_ssb(x)\n        print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n        if k == k_special:\n            plt.scatter(x['X0'], x['X1'], c=x['y'])\n            plt.show()\n        x = x.drop(columns=['y'])\n        print()","a45f864b":"def test_SKLearn_agg_redwine(data, k, k_special = 0):\n    x = data.drop(columns=['quality', 'y'])\n    agg = AGG(n_clusters = k)\n    x['y'] = agg.fit_predict(x)\n    # Calculating test SSE, SSB\n    cluster_sse(x)\n    sse = total_sse(x)\n    ssb = cluster_ssb(x)\n    print(f'SSE RATIO: {sse \/ (sse + ssb)}')\n    if k == k_special:\n        sb.pairplot(x, \n                    vars=x.loc[:, x.columns != 'y'], \n                    hue ='y',\n                    diag_kind = 'hist')\n    x = x.drop(columns=['y'])\n    print()","97bb5d68":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_agg_scree(small_xy, k_collection)","d36bea2b":"test_SKLearn_agg_small_large(small_xy, [6], k_special = 6)","32f5feaf":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_agg_scree(large_xy, k_collection)","f8d755ee":"test_SKLearn_agg_small_large(large_xy, [5], k_special = 5)","41b90375":"k_collection = [x for x in range(2, 10)]\nEval_SKLearn_agg_scree(red_wine.drop(columns=['quality']), k_collection)","01362793":"test_SKLearn_agg_redwine(red_wine, 4, k_special = 4)","4f87a514":"## Implement Evaluation Functions","cfe0ca05":"The Scree plot here is pretty radically different than the one I produced with my kmeans algorithm. The rate of change is decreasing from k = 2 to 4 but then starts increasing and levelling off after k = 4. I'll print the cluster for k = 4 below along with relevant statistics.","ee1bf46a":"## Evaluating on Large data set","99f071f4":"It appears the elbow of the Scree plot is at k = 5. That's likely the best clustering. I'll plot it below long with the relevant stats.","1a69bbae":"## Evaluating on large data set","a8228108":"The elbow here appears at 3. It's more apparent than on the small data set. K = 3 is again, the same as the k value I chose before. I'll plot the clustering below.","54dde38d":"# Gaussian Mixture Model Clustering\n\nFrom sklearn:\n\n    A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.","2a37e786":"# Implementing K-Means Algorithm","7f6ad551":"# Agglomerative Clustering\n\nFrom sklearn:\n\n    The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n\n    Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n\n    Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n\n    Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n\n    Single linkage minimizes the distance between the closest observations of pairs of clusters.\n    \nI'll be testing with the ward strategy","e6d3d866":"## Implement cluster SSE, total SSE, and SSB calculations ","938221f3":"## Evaluating on small data set","bb4d9da1":"## Implement Testing Functions","c7deee96":"# Comparing with Sklearn K-means","c19c1047":"## Evaluating on Small data set","7e0bd4a5":"# Loading Data","5d4bf4be":"# Summary of results\n\nThe best model for each data set is different and different values of K were chosen for many of the models. The K-means from scratch and sklearn K-means produced similar performing clusters on the small and large data set but varied wildly on the Red Wine data set. This disparity must come from something caused by higher dimensionality. On the Red Wine data set the Gaussian Mixture Model performed much wore than the other algorithms. This was likely due to the features of Red_wine being mostly non-normal. The features of the large and small data sets are closer to normal. On all data sets the Agglomerative clustering algorithm performed pretty well. No algorithm was able to produce the embedded clusters present in the true clusterings, however, any of these top choice clusterings could be deployed for further evaluation against true popultion data in the future. \n\n### Small data set\n\n1. Sklearn K-Means\n\n    CLUSTER SSE IS: [25.113650316705517, 27.786510770471924, 28.88352039998379, 29.171406839346304, 23.197524969970992, 16.210872383529896]<br\/>\n    TOTAL SSE IS: 150.36348568000844<br\/>\n    TOTAL SSB IS: 434.4472418710884<br\/>\n    SSE RATIO: 0.2571147870519709<br\/>\n\n2. Gaussian Mixture Model\n\n    CLUSTER SSE IS: [22.259240947405164, 19.92423302517606, 29.943740736444642, 26.036592357641275, 20.790905667554412, 34.93146095209374]<br\/>\n    TOTAL SSE IS: 153.88617368631526<br\/>\n    TOTAL SSB IS: 438.35478003475976<br\/>\n    SSE RATIO: 0.25983710298898094<br\/>\n\n3. Agglomerative Clustering\n\n    CLUSTER SSE IS: [32.24772795411403, 25.22911100567976, 31.20294475079321, 27.592873393793326, 19.92423302517606, 17.463462019390224]<br\/>\n    TOTAL SSE IS: 153.66035214894663<br\/>\n    TOTAL SSB IS: 432.4894887114212<br\/>\n    SSE RATIO: 0.26215199840948433<br\/>\n\n4. From scratch K-Means\n\n    CLUSTER SSE IS: [14.321048894655888, 23.197524969970992, 27.786510770471924, 19.443766260326196, 25.22911100567976, 47.97577893804139]<br\/>\n    TOTAL SSE IS: 157.95374083914618<br\/>\n    TOTAL SSB IS: 433.7257603360633<br\/>\n    SSE RATIO: 0.26695827813100553<br\/>\n\n### Large data set\n\n1. Agglomerative Clustering\n\n    CLUSTER SSE IS: [574.1139319868796, 615.2491341845708, 242.14940863136596, 456.39404810917864, 301.95288952523384]<br\/>\n    TOTAL SSE IS: 2189.859412437227<br\/>\n    TOTAL SSB IS: 4061.450681598995<br\/>\n    SSE RATIO: 0.3503040769848168<br\/>\n\n2. From scratch K-Means\n\n    CLUSTER SSE IS: [662.0238124444755, 549.0543932592994, 389.12173771755505, 656.2668969006731]<br\/>\n    TOTAL SSE IS: 2256.466840322003<br\/>\n    TOTAL SSB IS: 3980.4496480032612<br\/>\n    SSE RATIO: 0.36179205614598653<br\/>\n\n3. Gaussian Mixture Model\n\n    CLUSTER SSE IS: [376.6374562533373, 702.1533787373509, 544.9836781878048, 618.9660942181022]<br\/>\n    TOTAL SSE IS: 2242.7406073965976<br\/>\n    TOTAL SSB IS: 3938.3921164928233<br\/>\n    SSE RATIO: 0.3628365070901396<br\/>\n\n4. Sklearn K-Means\n\n    CLUSTER SSE IS: [537.5157636165864, 666.3510261776227, 390.04468936603956, 646.4919803291158]<br\/>\n    TOTAL SSE IS: 2240.403459489365<br\/>\n    TOTAL SSB IS: 3891.086218694983<br\/>\n    SSE RATIO: 0.36539300840065864<br\/>\n\n### Red Wine data set\n\n1. Sklearn K-Means\n\n    CLUSTER SSE IS: [2471.767864327411, 3994.8735714567297, 3783.6483217172804, 11.0, 1116.7179271251641, 3297.3475044500997]<br\/>\n    TOTAL SSE IS: 14675.35518907669<br\/>\n    TOTAL SSB IS: 41888.52501338517<br\/>\n    SSE RATIO: 0.25944746252464423<br\/>\n\n2. Agglomerative Clustering\n\n    CLUSTER SSE IS: [4806.092560012689, 4724.766607466435, 5114.886149354945, 2770.9914134637097]<br\/>\n    TOTAL SSE IS: 17416.736730297715<br\/>\n    TOTAL SSB IS: 41716.80859601336<br\/>\n    SSE RATIO: 0.29453225972142505<br\/>\n\n3. Gaussian Mixture Model\n\n    CLUSTER SSE IS: [10954.087313831726, 3429.649604372425, 2638.962615217396, 1071.0795756489704, 660.6866912068739, 8562.14100201374]<br\/>\n    TOTAL SSE IS: 27316.606802291084<br\/>\n    TOTAL SSB IS: 39418.009047600324<br\/>\n    SSE RATIO: 0.40933189551484517<br\/>\n\n4. From scratch K-Means\n\n    CLUSTER SSE IS: [10888.685381436475, 1496.5269048973569, 10536.206011510541, 34.022488281135466, 6280.87021648497, 749.2728542794449]<br\/>\n    TOTAL SSE IS: 29985.58385688987<br\/>\n    TOTAL SSB IS: 35692.36077359333<br\/>\n    SSE RATIO: 0.456554845398932<br\/>","35c654b7":"## Testing on large dataset","7cf522fb":"## Evaluating on Red Wine data set","4a080ff7":"## Evaluating on Red Wine data set","9384379a":"## Evaluating on small data set","a3dc26a7":"The best looking cluster here appears to be at k = 5. I'll plot it below.","5e8694ea":"There's a pretty clear elbow here at k = 4. That's probably the best clustering. I'll plot the clustering below along with relevant statistics.","a00ad608":"## Evaluating on Red Wine data set","d30173d8":"It appears the elbow of the Scree plot is at k = 5. That's probably the best clustering. Afterwards we see an increase at k = 6 and then further decrease.","992f9277":"To make observations and further evaluate the model I'll use a Scree plot. The Scree plot is the relationship between [cluster see \/ (cluster sse + cluster ssb)] and k. To pick the best k value, the technique is to look for an elbow in the graph. An elbow is a spot where the rate of decrease changes from a sharp drop to a more shallow one. The elbow's interpretation is the k value such that we stop seeing benefit from a true clustering and start seeing the benefit of overfitting.","3c0c071f":"## Testing on small dataset","daa891d1":"## Testing on wine dataset","8b2a499d":"The following is adapted from a Homework assignment for which my professor was kind enough to give me permission to post. I hope this notebook serves as a good starting point example for anyone looking to jump into basic unsupervised learning methods. \n\n### The hardest problem \nThe trouble with clustering algorithms is that we don't know great ways to evaluate our clusters. I'll attempt to evaluate mine with:\n* Graphical comparison to the true clusters\n* SSE within and between clusters\n* A Scree plot\n\nI'll explain how all of those work later on when we get there.","1ed4959a":"## Evaluating on large data set"}}