{"cell_type":{"3611d1ba":"code","31f1fd25":"code","d13f93b2":"code","e84463ae":"code","b089c254":"code","b242e775":"code","80ae1446":"code","76f277b5":"code","2c62f238":"code","a1ae65e7":"code","4c684eb4":"code","f5218c6f":"code","9ff9dc30":"code","ae8337a1":"code","b4abb1be":"code","f44ebcb3":"code","4087f2be":"code","c6f43c40":"code","732cf346":"code","96e2ebb2":"code","cfee00bd":"code","12f8098c":"code","32d43e00":"code","b6eae302":"code","0f61ccee":"code","297134cf":"code","46742f62":"code","ff321aad":"code","01e50b19":"code","f8101c61":"code","d2b6f7bc":"code","cadfd6e8":"code","cabc11f4":"code","d9fb05d5":"code","048a14c4":"code","6884b676":"code","20ec8144":"code","efd03416":"code","4bc1bae1":"code","c0719169":"code","a86c03af":"code","fcce945f":"code","a6b250f0":"code","e2d04d7e":"code","265312a6":"code","dc55b781":"code","9da34bda":"code","01043fba":"code","4ee45dcc":"code","3dc00162":"code","64cfa185":"code","8e64d71e":"markdown","3697aa2c":"markdown","ce46b16f":"markdown","364b9189":"markdown","a3d858e0":"markdown","a4a23ca3":"markdown","4de9b0a7":"markdown","a2d308d5":"markdown","2b2cd3c5":"markdown","10e72150":"markdown","797f8dde":"markdown","aea7545d":"markdown","961e90ba":"markdown","990e6950":"markdown","e8d9aa79":"markdown","736a5654":"markdown","331537be":"markdown","8af84863":"markdown","7db4dee5":"markdown","a5983cc0":"markdown","a700b556":"markdown","80ce7914":"markdown","942a6f4a":"markdown","e22dd729":"markdown","52b4d7f3":"markdown","d16d3933":"markdown","732573eb":"markdown","9efaa14c":"markdown","5b2c33e5":"markdown","3534b697":"markdown"},"source":{"3611d1ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport string # library used to deal with some text data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library\npd.set_option('display.max_columns', 100) # Setting pandas to display a N number of columns\npd.set_option('display.max_rows', 10) # Setting pandas to display a N number rows\npd.set_option('display.width', 1000) # Setting pandas dataframe display width to N\nfrom scipy import stats # statistical library\nfrom statsmodels.stats.weightstats import ztest # statistical library for hypothesis testing\nimport plotly.graph_objs as go # interactive plotting library\nimport plotly.express as px # interactive plotting library\nfrom itertools import cycle # used for cycling colors at plotly graphs\nimport matplotlib.pyplot as plt # plotting library\nimport pandas_profiling # library for automatic EDA\n%pip install autoviz # installing and importing autoviz, another library for automatic data visualization\nfrom autoviz.AutoViz_Class import AutoViz_Class\nfrom IPython.display import display # display from IPython.display\nfrom itertools import cycle # function used for cycling over values\n%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\nimport ppscore as pps # importing ppscore\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(\"\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31f1fd25":"# Importing the data and displaying some rows\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ndisplay(df.head(10))","d13f93b2":"# The pandas profiling library is really useful on helping us understand the data we're working on.\n# It saves us some precious time on the EDA process.\nreport = pandas_profiling.ProfileReport(df)","e84463ae":"# Let's now visualize the report generated by pandas_profiling.\ndisplay(report)\n\n# Also, there is an option to generate an .HTML file containing all the information generated by the report.\n# report.to_file(output_file='report.html')","b089c254":"# Another great library for automatic EDA is AutoViz.\n# With this library, several plots are generated with only 1 line of code.\n# When combined with pandas_profiling, we obtain lots of information in a\n# matter of seconds, using less than 5 lines of code.\nAV = AutoViz_Class()\n\n# Let's now visualize the plots generated by AutoViz.\nreport_2 = AV.AutoViz(\"\/kaggle\/input\/titanic\/train.csv\")","b242e775":"# Creating different datasets for survivors and non-survivors\ndf_survivors = df[df['Survived'] == 1]\ndf_nonsurvivors = df[df['Survived'] == 0]","80ae1446":"# Filling in the data inside the Violin Objects\nviolin_survivors = go.Violin(\n    y=df_survivors['Age'],\n    x=df_survivors['Survived'],\n    name='Survivors',\n    marker_color='forestgreen',\n    box_visible=True)\n\nviolin_nonsurvivors = go.Violin(\n    y=df_nonsurvivors['Age'],\n    x=df_nonsurvivors['Survived'],\n    name='Non-Survivors',\n    marker_color='darkred',\n    box_visible=True)\n\ndata = [violin_nonsurvivors, violin_survivors]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Age\" of survivors vs Ages of non-survivors',\n  xaxis=dict(\n        title='Survived or not'\n    ),\n    yaxis=dict(\n        title='Age'\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","76f277b5":"# First distribution for the hypothesis test: Ages of survivors\ndist_a = df_survivors['Age'].dropna()\n\n# Second distribution for the hypothesis test: Ages of non-survivors\ndist_b = df_nonsurvivors['Age'].dropna()","2c62f238":"# Z-test: Checking if the distribution means (ages of survivors vs ages of non-survivors) are statistically different\nt_stat, p_value = ztest(dist_a, dist_b)\nprint(\"----- Z Test Results -----\")\nprint(\"T stat. = \" + str(t_stat))\nprint(\"P value = \" + str(p_value)) # P-value is less than 0.05\n\nprint(\"\")\n\n# T-test: Checking if the distribution means (ages of survivors vs ages of non-survivors) are statistically different\nt_stat_2, p_value_2 = stats.ttest_ind(dist_a, dist_b)\nprint(\"----- T Test Results -----\")\nprint(\"T stat. = \" + str(t_stat_2))\nprint(\"P value = \" + str(p_value_2)) # P-value is less than 0.05","a1ae65e7":"# Taking the count of each Sex value inside the Survivors\ndf_survivors_sex = df_survivors['Sex'].value_counts()\ndf_survivors_sex = pd.DataFrame({'Sex':df_survivors_sex.index, 'count':df_survivors_sex.values})\n\n# Taking the count of each Sex value inside the Survivors\ndf_nonsurvivors_sex = df_nonsurvivors['Sex'].value_counts()\ndf_nonsurvivors_sex = pd.DataFrame({'Sex':df_nonsurvivors_sex.index, 'count':df_nonsurvivors_sex.values})\n\n\n# Creating the plotting objects\npie_survivors_sex = go.Pie(  \n   labels = df_survivors_sex['Sex'],\n   values = df_survivors_sex['count'],\n   domain=dict(x=[0, 0.5]),\n   name='Survivors',\n   hole = 0.5,\n   marker = dict(colors=['violet', 'cornflowerblue'], line=dict(color='#000000', width=2))\n)\n\npie_nonsurvivors_sex = go.Pie(  \n   labels = df_nonsurvivors_sex['Sex'],\n   values = df_nonsurvivors_sex['count'],\n   domain=dict(x=[0.5, 1.0]), \n   name='non-Survivors',\n   hole = 0.5,\n   marker = dict(colors=['cornflowerblue', 'violet'], line=dict(color='#000000', width=2))\n)\n\ndata = [pie_survivors_sex, pie_nonsurvivors_sex]\n\n\n# Plot's Layout (background color, title, annotations, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Sex\" percentage from Survivors vs non-Survivors',\n    annotations=[dict(text='Survivors', x=0.18, y=0.5, font_size=15, showarrow=False),\n                 dict(text='Non-Survivors', x=0.85, y=0.5, font_size=15, showarrow=False)]\n)\n\nfig = go.Figure(data=data, layout=layout)\n\nfig.show()","4c684eb4":"# Taking the count of each Pclass value inside the Survivors\ndf_survivors_pclass = df_survivors['Pclass'].value_counts()\ndf_survivors_pclass = pd.DataFrame({'Pclass':df_survivors_pclass.index, 'count':df_survivors_pclass.values})\n\n# Taking the count of each Pclass value inside the Survivors\ndf_nonsurvivors_pclass = df_nonsurvivors['Pclass'].value_counts()\ndf_nonsurvivors_pclass = pd.DataFrame({'Pclass':df_nonsurvivors_pclass.index, 'count':df_nonsurvivors_pclass.values})\n\n\n# Creating the plotting objects\npie_survivors_pclass = go.Pie(  \n   labels = df_survivors_pclass['Pclass'],\n   values = df_survivors_pclass['count'],\n   domain=dict(x=[0, 0.5]),\n   name='Survivors',\n   hole = 0.5,\n   marker = dict(colors=['#636EFA', '#EF553B', '#00CC96'], line=dict(color='#000000', width=2))\n)\n\npie_nonsurvivors_pclass = go.Pie(  \n   labels = df_nonsurvivors_pclass['Pclass'],\n   values = df_nonsurvivors_pclass['count'],\n   domain=dict(x=[0.5, 1.0]), \n   name='non-Survivors',\n   hole = 0.5,\n   marker = dict(colors=['#EF553B', '#00CC96', '#636EFA'], line=dict(color='#000000', width=2))\n)\n\ndata = [pie_survivors_pclass, pie_nonsurvivors_pclass]\n\n\n# Plot's Layout (background color, title, annotations, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Pclass\" percentage from Survivors vs non-Survivors',\n    annotations=[dict(text='Survivors', x=0.18, y=0.5, font_size=15, showarrow=False),\n                 dict(text='Non-Survivors', x=0.85, y=0.5, font_size=15, showarrow=False)]\n)\n\nfig = go.Figure(data=data, layout=layout)\n\nfig.show()","f5218c6f":"# Checking out the differences between Fare distribution for survivors and non-survivors\nfare_survivors_box = go.Box(  \n   x=df_survivors['Fare'],\n   name='Survivors',\n   marker=dict(color='navy')\n)\n\nfare_nonsurvivors_box = go.Box(  \n   x=df_nonsurvivors['Fare'],\n   name='Non-Survivors',\n   marker=dict(color='steelblue')\n)\n  \ndata = [fare_nonsurvivors_box, fare_survivors_box]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Fare\" value of survivors vs \"Fare\" value of non-survivors',\n    barmode='stack',\n    xaxis=dict(\n        title='Fare distribution'\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","9ff9dc30":"# Third distribution for the hypothesis test - Fares of survivors\ndist_c = df_survivors['Fare'].dropna()\n\n# Fourth distribution for the hypothesis test - Fares of non-survivors\ndist_d = df_nonsurvivors['Fare'].dropna()","ae8337a1":"# Z-test: Checking if the distribution means (fares of survivors vs fares of non-survivors) are statistically different\nt_stat_3, p_value_3 = ztest(dist_c, dist_d)\nprint(\"----- Z Test Results -----\")\nprint(\"T stat. = \" + str(t_stat_3))\nprint(\"P value = \" + str(p_value_3)) # P-value is less than 0.05\n\nprint(\"\")\n\n# T-test: Checking if the distribution means (fares of survivors vs fares of non-survivors) are statistically different\nt_stat_4, p_value_4 = stats.ttest_ind(dist_c, dist_d)\nprint(\"----- T Test Results -----\")\nprint(\"T stat. = \" + str(t_stat_4))\nprint(\"P value = \" + str(p_value_4)) # P-value is less than 0.05","b4abb1be":"matrix_df = pps.matrix(df)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\nmatrix_df = matrix_df.apply(lambda x: round(x, 2)) # Rounding matrix_df's values to 0,XX\n\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.75, annot=True)","f44ebcb3":"import collections\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom category_encoders import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, make_scorer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom xgboost import XGBClassifier, plot_importance as plot_importance_xgb\nfrom lightgbm import LGBMClassifier, plot_importance as plot_importance_lgbm","4087f2be":"# Creating a categorical variable for Ages\ndf['AgeCat'] = ''\ndf['AgeCat'].loc[(df['Age'] < 18)] = 'young'\ndf['AgeCat'].loc[(df['Age'] >= 18) & (df['Age'] < 56)] = 'mature'\ndf['AgeCat'].loc[(df['Age'] >= 56)] = 'senior'\n\n\n# Creating a categorical variable for Family Sizes\ndf['FamilySize'] = ''\ndf['FamilySize'].loc[(df['SibSp'] <= 2)] = 'small'\ndf['FamilySize'].loc[(df['SibSp'] > 2) & (df['SibSp'] <= 5 )] = 'medium'\ndf['FamilySize'].loc[(df['SibSp'] > 5)] = 'large'\n\n\n# Creating a categorical variable to tell if the passenger is alone\ndf['IsAlone'] = ''\ndf['IsAlone'].loc[((df['SibSp'] + df['Parch']) > 0)] = 'no'\ndf['IsAlone'].loc[((df['SibSp'] + df['Parch']) == 0)] = 'yes'\n\n\n# Creating a categorical variable to tell if the passenger is a Young\/Mature\/Senior male or a Young\/Mature\/Senior female\ndf['SexCat'] = ''\ndf['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] <= 21)] = 'youngmale'\ndf['SexCat'].loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturemale'\ndf['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] > 50)] = 'seniormale'\ndf['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] <= 21)] = 'youngfemale'\ndf['SexCat'].loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturefemale'\ndf['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] > 50)] = 'seniorfemale'\n\n\n# Creating a categorical variable for the passenger's title\n# Title is created by extracting the prefix before \"Name\" feature\n# This title needs to be a feature because all female titles are grouped with each other\n# Also, creating a column to tell if the passenger is married or not\n# \"Is_Married\" is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles\ndf['Title'] = df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf['Is_Married'] = 0\ndf['Is_Married'].loc[df['Title'] == 'Mrs'] = 1\ndf['Title'] = df['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf['Title'] = df['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\n\n# Creating \"Ticket Frequency\" Feature\n# There are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier\ndf['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')\n\ndf.head(10)","c6f43c40":"def get_feature_names(df):\n    # Splitting the target\n    target = df['Survived']\n\n    # Dropping unused columns from the feature set\n    df.drop(['PassengerId', 'Survived', 'Ticket', 'Name', 'Cabin'], axis=1, inplace=True)\n\n    # Splitting categorical and numerical column dataframes\n    categorical_df = df.select_dtypes(include=['object'])\n    numeric_df = df.select_dtypes(exclude=['object'])\n\n    # And then, storing the names of categorical and numerical columns.\n    categorical_columns = list(categorical_df.columns)\n    numeric_columns = list(numeric_df.columns)\n    \n    print(\"Categorical columns:\\n\", categorical_columns)\n    print(\"\\nNumeric columns:\\n\", numeric_columns)\n\n    return target, categorical_columns, numeric_columns\n\ntarget, categorical_columns, numeric_columns = get_feature_names(df)","732cf346":"# You can call any of the functions below, if you wish, inside the \"defineBestModelPipeline()\" function\n\ndef balancingClassesRus(x_train, y_train):\n    \n    # Using RandomUnderSampler to balance our training data points\n    rus = RandomUnderSampler(random_state=7)\n    features_balanced, target_balanced = rus.fit_resample(x_train, y_train)\n    \n    print(\"Count for each class value after RandomUnderSampler:\", collections.Counter(target_balanced))\n    \n    return features_balanced, target_balanced\n\n\ndef balancingClassesSmoteenn(x_train, y_train):\n    \n    # Using SMOTEEN to balance our training data points\n    smn = SMOTEENN(random_state=7)\n    features_balanced, target_balanced = smn.fit_resample(x_train, y_train)\n    \n    print(\"Count for each class value after SMOTEEN:\", collections.Counter(target_balanced))\n    \n    return features_balanced, target_balanced\n\n\ndef balancingClassesSmote(x_train, y_train):\n\n    # Using SMOTE to to balance our training data points\n    sm = SMOTE(random_state=7)\n    features_balanced, target_balanced = sm.fit_resample(x_train, y_train)\n\n    print(\"Count for each class value after SMOTE:\", collections.Counter(target_balanced))\n\n    return features_balanced, target_balanced","96e2ebb2":"# Function responsible for checking our model's performance on the test data\ndef testSetResultsClassifier(classifier, x_test, y_test):\n    predictions = classifier.predict(x_test)\n    \n    results = []\n    f1 = f1_score(y_test, predictions)\n    precision = precision_score(y_test, predictions)\n    recall = recall_score(y_test, predictions)\n    roc_auc = roc_auc_score(y_test, predictions)\n    accuracy = accuracy_score(y_test, predictions)\n    \n    results.append(f1)\n    results.append(precision)\n    results.append(recall)\n    results.append(roc_auc)\n    results.append(accuracy)\n    \n    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n    print(\"F1 score, Precision, Recall, ROC_AUC score, Accuracy:\")\n    print(results)\n    \n    return results","cfee00bd":"# Now, we are going to create our Pipeline, fitting several different data preprocessing, feature selection \n# and modeling techniques inside a RandomSearchCV, to check which group of techniques has better performance.\n\n# Building a Pipeline inside RandomSearchCV, responsible for finding the best model and it's parameters\ndef defineBestModelPipeline(df, target, categorical_columns, numeric_columns):\n    \n    # Splitting original data into Train and Test BEFORE applying transformations\n    # Later in RandomSearchCV, x_train will be splitted into train\/val sets\n    # The transformations are going to be fitted specifically on the train set,\n    # and then applied to both train\/test sets. This way, information leakage is avoided!\n    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.10, random_state=42)\n    y_train = y_train.to_numpy() # Transforming training targets into numpy arrays\n    y_test = y_test.to_numpy() # Transforming test targets into numpy arrays\n    \n    \n    # # If desired, we can balance training classes using one of the functions below\n    # # Obtaining balanced data for modeling using Random Under Sampling\n    #x_train, y_train = balancingClassesRus(x_train, y_train)\n\n    # # Obtaining balanced data for modeling using SMOTEENN\n    #x_train, y_train = balancingClassesSmoteenn(x_train, y_train)\n\n    # # Obtaining balanced data for modeling using SMOTE\n    #x_train, y_train = balancingClassesSmote(x_train, y_train)\n    \n    \n    # 1st -> Numeric Transformers\n    # Here, we are creating different several different data transformation pipelines \n    # to be applied in our numeric features\n    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),\n                                            ('scaler', StandardScaler())])\n    \n    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n    \n    \n    # 2nd -> Categorical Transformer\n    # Despite my option of not doing it, you can also choose to create different \n    # data transformation pipelines for your categorical features.\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n    \n    # 3rd -> Combining both numerical and categorical pipelines\n    # Here, we are creating different ColumnTransformers, each one with a different numerical transformation\n    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    \n    # And finally, we are going to apply these different data transformations to RandomSearchCV,\n    # trying to find the best imputing strategy, the best feature engineering strategy\n    # and the best model with it's respective parameters.\n    # Below, we just need to initialize a Pipeline object with any transformations we want, on each of the steps.\n    pipe = Pipeline(steps=[('data_transformations', data_transformations_1), # Initializing data transformation step by choosing any of the above\n                           ('feature_eng', PCA()), # Initializing feature engineering step by choosing any desired method\n                           ('clf', SVC())]) # Initializing modeling step of the pipeline with any model object\n                           #memory='cache_folder') -> Used to optimize memory when needed\n    \n    \n    # Now, we define the grid of parameters that RandomSearchCV will use. It will randomly chose\n    # options for each step inside the dictionaries ('data transformations', 'feature_eng', 'clf'\n    # and 'clf parameters'). In the end of it's iterations, RandomSearchCV will return the best options.\n    params_grid = [\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [KNeighborsClassifier()],\n                     'clf__n_neighbors': stats.randint(1, 50),\n                     'clf__metric': ['minkowski', 'euclidean']},\n\n        \n\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [LogisticRegression()],\n                     'clf__penalty': ['l1', 'l2'],\n                     'clf__C': stats.uniform(0.01, 10)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [SVC()],\n                     'clf__C': stats.uniform(0.01, 1),\n                     'clf__gamma': stats.uniform(0.01, 1)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [DecisionTreeClassifier()],\n                     'clf__criterion': ['gini', 'entropy'],\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 5)]},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [RandomForestClassifier()],\n                     'clf__n_estimators': stats.randint(10, 175),\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 5)],\n                     'clf__random_state': stats.randint(1, 49)},\n        \n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [ExtraTreesClassifier()],\n                     'clf__n_estimators': stats.randint(10, 150),\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 6)]},\n\n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [GradientBoostingClassifier()],\n                     'clf__n_estimators': stats.randint(10, 100),\n                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n                     'clf__max_depth': [None, stats.randint(1, 6)]},\n\n        \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [LGBMClassifier()],\n                     'clf__n_estimators': stats.randint(1, 100),\n                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n                     'clf__max_depth': [None, stats.randint(1, 6)]},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [XGBClassifier()],\n                     'clf__n_estimators': stats.randint(5, 125),\n                     'clf__eta': stats.uniform(0.01, 1),\n                     'clf__max_depth': [None, stats.randint(1, 6)],\n                     'clf__gamma': stats.uniform(0.01, 1)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [StackingClassifier(estimators=[('svc', SVC(C=1, gamma=1)),\n                                                            ('rf', RandomForestClassifier(max_depth=7, max_features=None, n_estimators=60, n_jobs=-1, random_state=28)),\n                                                            ('xgb', XGBClassifier(eta=0.6, gamma=0.7, max_depth=None, n_estimators=30))], \n                                                final_estimator=LogisticRegression(C=1))]},\n   \n   \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [VotingClassifier(estimators=[('gbt', GradientBoostingClassifier(learning_rate=0.8, max_depth=None, n_estimators=30)),\n                                                          ('lgbm', LGBMClassifier(n_estimators=30, learning_rate=0.6, max_depth=None)),\n                                                          ('xgb', XGBClassifier(eta=0.8, gamma=0.8, max_depth=None, n_estimators=40))],\n                                              voting='soft')]}\n                ]\n    \n    \n    # Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    \n    # Creating our cross validation object with StratifiedShuffleSplit, 10 folds\n    # Stratification assures that we split the data such that the proportions\n    # between classes are the same in each fold as they are in the whole dataset\n    cross_validator = StratifiedShuffleSplit(n_splits=10, train_size=0.8, test_size=0.2)\n    \n    # Creating the randomized search cv object and fitting it\n    best_model_pipeline = RandomizedSearchCV(pipe, params_grid, n_iter=100, \n                                             scoring=metrics, refit='accuracy', \n                                             n_jobs=-1, cv=cross_validator, random_state=21)\n\n    best_model_pipeline.fit(x_train, y_train)\n    \n    \n    # At last, we check the final results\n    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n    print(\"\\n\\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n    print(\"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n    \n    return x_train, x_test, y_train, y_test, best_model_pipeline","12f8098c":"# Calling the function above, returing train\/test data and best model's pipeline\nx_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(df, target, categorical_columns, numeric_columns)\n\n\n# Checking best model's performance on test data\ntest_set_results = testSetResultsClassifier(best_model_pipeline, x_test, y_test)","32d43e00":"# Visualizing all results and metrics, from all models, obtained by the RandomSearchCV steps\ndf_results = pd.DataFrame(best_model_pipeline.cv_results_)\n\ndisplay(df_results)","b6eae302":"# Now visualizing all results and metrics obtained only by the best classifier\ndisplay(df_results[df_results['rank_test_accuracy'] == 1])","0f61ccee":"# Here, we access the categorical feature names generated by OneHotEncoder, and then concatenate them\n# with the numerical feature names, in the same order our pipeline is applying data transformations.\ncategorical_features_after_onehot = best_model_pipeline.best_estimator_.named_steps['data_transformations']\\\n                                        .transformers_[1][1].named_steps['onehot'].get_feature_names()\n\nfeature_names_in_order = numeric_columns + categorical_features_after_onehot\n\nprint(feature_names_in_order)","297134cf":"# # Plotting feature importances of the best model, if sklearn tree-based (top 5 features)\n#print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].feature_importances_, index=feature_names_in_order)\n#feat_importances.nlargest(5).plot(kind='barh')\n\n\n# # Plotting feature importances of the best model, if linear regression-based (top 5 features)\n#print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].coef_, index=feature_names_in_order)\n#feat_importances.nlargest(5).plot(kind='barh')\n\n\n# # Plotting feature importances for XGB Model\n#plot_importance_xgb(best_model_pipeline.best_estimator_.named_steps['clf'], height=0.4, \n#title='Feature Importances for XGB Classifier', importance_type='gain')\n\n\n# # Plotting feature importances for LGBM Model\n#plot_importance_lgbm(best_model_pipeline.best_estimator_.named_steps['clf'], \n#                     figsize=(10, 4), title='Feature importances for LGBM Classifier',\n#                     importance_type='gain', max_num_features=10)","46742f62":"# Importing data and displaying some rows\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n\n# Creating a categorical variable for Ages\ndf_test['AgeCat'] = ''\ndf_test['AgeCat'].loc[(df_test['Age'] < 18)] = 'young'\ndf_test['AgeCat'].loc[(df_test['Age'] >= 18) & (df_test['Age'] < 56)] = 'mature'\ndf_test['AgeCat'].loc[(df_test['Age'] >= 56)] = 'senior'\n\n\n# Creating a categorical variable for Family Sizes\ndf_test['FamilySize'] = ''\ndf_test['FamilySize'].loc[(df_test['SibSp'] <= 2)] = 'small'\ndf_test['FamilySize'].loc[(df_test['SibSp'] > 2) & (df_test['SibSp'] <= 5 )] = 'medium'\ndf_test['FamilySize'].loc[(df_test['SibSp'] > 5)] = 'large'\n\n\n# Creating a categorical variable to tell if the passenger is alone\ndf_test['IsAlone'] = ''\ndf_test['IsAlone'].loc[((df_test['SibSp'] + df_test['Parch']) > 0)] = 'no'\ndf_test['IsAlone'].loc[((df_test['SibSp'] + df_test['Parch']) == 0)] = 'yes'\n\n\n# Creating a categorical variable to tell if the passenger is a Young\/Mature\/Senior male or a Young\/Mature\/Senior female\ndf_test['SexCat'] = ''\ndf_test['SexCat'].loc[(df_test['Sex'] == 'male') & (df_test['Age'] <= 21)] = 'youngmale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'male') & ((df_test['Age'] > 21) & (df_test['Age']) < 50)] = 'maturemale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'male') & (df_test['Age'] > 50)] = 'seniormale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'female') & (df_test['Age'] <= 21)] = 'youngfemale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'female') & ((df_test['Age'] > 21) & (df_test['Age']) < 50)] = 'maturefemale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'female') & (df_test['Age'] > 50)] = 'seniorfemale'\n\n\n# Creating a categorical variable for the passenger's title\n# Title is created by extracting the prefix before \"Name\" feature\n# This title needs to be a feature because all female titles are grouped with each other\n# Also, creating a column to tell if the passenger is married or not\n# \"Is_Married\" is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles\ndf_test['Title'] = df_test['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_test['Is_Married'] = 0\ndf_test['Is_Married'].loc[df['Title'] == 'Mrs'] = 1\ndf_test['Title'] = df_test['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_test['Title'] = df_test['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\n\n# Creating \"Ticket Frequency\" Feature\n# There are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier\ndf_test['Ticket_Frequency'] = df_test.groupby('Ticket')['Ticket'].transform('count')\n\n\n# Dropping unnecessary columns\ndf_test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","ff321aad":"# Applying best_model_pipeline\n# Step 1 -> Transforming data the same way we did in the training set;\n# Step 2 -> making predictions using the best model obtained by RandomSearchCV.\ntest_predictions = best_model_pipeline.predict(df_test)\nprint(test_predictions)","01e50b19":"# Generating predictions file that is going to be submitted to the competition\ndf_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ndf_submission['Survived'] = test_predictions # Adding a column with predicted values\n\ndf_submission.drop(df_submission.columns.difference(['PassengerId', 'Survived']), axis=1, inplace=True) # Selecting only needed columns\n\ndf_submission.head(10)","f8101c61":"# Checking if the number of rows is OK (the file is expected to have 418 rows)\ndf_submission.count()","d2b6f7bc":"# Writing submitions to CSV file\ndf_submission.to_csv('submission.csv', index=False)","cadfd6e8":"# Defining a PCA Pipeline\ndef definePCAPipeline(categorical_columns, numeric_columns):\n\n    # 1st -> Numeric Transformer\n    numeric_transformer = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n    \n    \n    # 2nd -> Categorical Transformer\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n    \n    # 3rd -> Combining both numerical and categorical pipelines\n    data_transformations = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n\n    # 4th -> Final PCA Pipeline\n    pca_pipeline = Pipeline(steps=[('data_transformations', data_transformations),\n                           ('feature_eng', PCA(n_components=2, whiten=True))])\n    \n    \n    return pca_pipeline","cabc11f4":"# Generating transformed data after PCA, only 2 principal components chosen\npca_pipeline = definePCAPipeline(categorical_columns, numeric_columns)\npca_arr = pca_pipeline.fit_transform(df)\n\n# How much variance does our PCA obtained components explains from the original variance in data?\ncomp1 = pca_pipeline[1].explained_variance_ratio_[0]\ncomp2 = pca_pipeline[1].explained_variance_ratio_[1]\nexp_variance_pca = comp1 + comp2\n\nprint(\"Compontent 1 explained variance ratio:\", comp1)\nprint(\"Compontent 2 explained variance ratio:\", comp2)\nprint(\"Total explained variance ratio obtained from both components:\", exp_variance_pca)","d9fb05d5":"# Visualizing the new 2-dimensional dataset and points' respective classes\npca_df = pd.DataFrame(pca_arr, columns=[\"PC1\", \"PC2\"])\npca_df['Survived'] = target\n\npca_df.head(10)","048a14c4":"# Creating different datasets for survivors and non-survivors\npca_df_survivors = pca_df[pca_df['Survived'] == 1]\npca_df_nonsurvivors = pca_df[pca_df['Survived'] == 0]\n\n\n# Visualizing the two-dimensional dataset\nscatter_obj_survs = go.Scatter(x=pca_df_survivors['PC1'],\n                               y=pca_df_survivors['PC2'],\n                               mode=\"markers\",\n                               name='Survivors',\n                               marker=dict(color='forestgreen'))\n\n\nscatter_obj_nonsurvs = go.Scatter(x=pca_df_nonsurvivors['PC1'],\n                                  y=pca_df_nonsurvivors['PC2'],\n                                  mode=\"markers\",\n                                  name='Non-survivors',\n                                  marker=dict(color='darkred'))\n\n\ndata = [scatter_obj_survs, scatter_obj_nonsurvs]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout = go.Layout(title='2-Dimensional visualization of survivors and non-survivors',\n                   xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","6884b676":"# Generating transformed data using PCA pipeline's data transformations (pca_pipeline[0])\ntransformed_df = pca_pipeline[0].transform(df)\n\n# Then transforming this data with TSNE object\ntsne = TSNE(n_components=2, random_state=1)\ntsne_arr = tsne.fit_transform(transformed_df)","20ec8144":"# Visualizing the new 2-dimensional dataset and points' respective classes\ntsne_df = pd.DataFrame(tsne_arr, columns=[\"tsne_dim1\", \"tsne_dim2\"])\ntsne_df['Survived'] = target\n\ntsne_df.head(10)","efd03416":"# Creating different datasets for survivors and non-survivors\ntsne_df_survivors = tsne_df[tsne_df['Survived'] == 1]\ntsne_df_nonsurvivors = tsne_df[tsne_df['Survived'] == 0]\n\n\n# Visualizing the two-dimensional dataset\nscatter_obj_survs_tsne = go.Scatter(x=tsne_df_survivors['tsne_dim1'],\n                                    y=tsne_df_survivors['tsne_dim2'],\n                                    mode=\"markers\",\n                                    name='Survivors',\n                                    marker=dict(color='forestgreen'))\n\n\nscatter_obj_nonsurvs_tsne = go.Scatter(x=tsne_df_nonsurvivors['tsne_dim1'],\n                                       y=tsne_df_nonsurvivors['tsne_dim2'],\n                                       mode=\"markers\",\n                                       name='Non-survivors',\n                                       marker=dict(color='darkred'))\n\n\ndata_tsne = [scatter_obj_survs_tsne, scatter_obj_nonsurvs_tsne]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout_tsne = go.Layout(title='2-Dimensional visualization of survivors and non-survivors (t-SNE algorithm)',\n                        xaxis=dict(title='tsne_dim1'), yaxis=dict(title='tsne_dim2'))\n\nfig_tsne = go.Figure(data=data_tsne, layout=layout_tsne)\nfig_tsne.show()","4bc1bae1":"# K-Means and silhouette_score libraries\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n\n# Defining a function to find the optimal number of K-Means' n_clusters parameter:\ndef findOptimalNClustersKMeans(transformed_df):\n    \n    # Number of clusters to search for and silhouette_scores list\n    range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n    silhouette_scores = []\n\n    # Testing n_clusters options\n    for n_clusters in range_n_clusters:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=7)\n        cluster_labels = kmeans.fit_predict(transformed_df)\n        \n        # Evaluating clusters created by KMeans\n        silhouette_avg = silhouette_score(transformed_df, cluster_labels)\n        print(\"K-Means: for n_clusters =\", n_clusters, \", the average silhouette_score is\", silhouette_avg)\n        \n        # Appending iteration's avg silhouette score to scores list\n        silhouette_scores.append(silhouette_avg)\n        \n    return range_n_clusters, silhouette_scores\n\nrange_n_clusters, silhouette_scores = findOptimalNClustersKMeans(transformed_df)","c0719169":"# Visualizing the \"Elbow\" graph\nelbow_kmeans = go.Scatter(x=range_n_clusters,\n                          y=silhouette_scores,\n                          mode='lines',\n                          name='kmeans elbow line',\n                          marker_color='orange')\n\n# Plot's Layout (background color, title, etc.)\nlayout_elbow_kmeans = go.Layout(title='Number of Clusters x Average Silhouette Score, K-Means Algorithm',\n                          xaxis=dict(title='n_clusters'), yaxis=dict(title='Average Silhouette Score'))\n\nelbow_kmeans = go.Figure(data=elbow_kmeans, layout=layout_elbow_kmeans)\nelbow_kmeans.show()","a86c03af":"# Since the best value for silhouette_score was found with 3 clusters,\n# let's create a K-Means Object with n_clusters = 3.\nkmeans = KMeans(n_clusters=3, random_state=7)\n\n# Fitting and clusterizing with KMeans\nkmeans_defined_clusters = kmeans.fit_predict(transformed_df)\n\n# Concatenating K-Means clusters into the 2-D PCA transformed df\npca_df['KMeans_Defined_Clusters'] = ''\npca_df['KMeans_Defined_Clusters'] = kmeans_defined_clusters\npca_df['KMeans_Defined_Clusters'] = pca_df['KMeans_Defined_Clusters'].astype(str)\n\npca_df.head(10)","fcce945f":"# KMeans object also stores centroids' positions for each cluster\npca_cluster_centers = pca_pipeline[1].transform(kmeans.cluster_centers_)\n\n# Creating a dataframe to store cluster centers (centroids)\n# The goal is to visualize them (in 2-D) inside each cluster\ncentroids_df = pd.DataFrame(pca_cluster_centers, columns=[\"X_coord\", \"Y_coord\"])\n\n# Creating columns to store which centroid belongs to each cluster\ncentroids_df['cluster_centroid'] = \"\"\ncentroids_df['cluster_centroid'][0] = 'centroid, cluster 0'\ncentroids_df['cluster_centroid'][1] = 'centroid, cluster 1'\ncentroids_df['cluster_centroid'][2] = 'centroid, cluster 2'\n\ncentroids_df.head(10)","a6b250f0":"# Colors\ncolors = ['orange', 'steelblue', 'violet']\n\ncyclecolors = cycle(colors)\ncolor = next(cyclecolors)\n\nkmeans_clusters = pca_df.KMeans_Defined_Clusters.unique()\nkmeans_centroids = centroids_df.cluster_centroid.unique()\n\n\n# Visualizing the two-dimensional dataset\ndata_kmeans = []\n\nfor cluster in kmeans_clusters:\n    scatter_obj_cluster_kmeans = go.Scatter(x=pca_df[(pca_df['KMeans_Defined_Clusters'] == cluster)]['PC1'],\n                                            y=pca_df[(pca_df['KMeans_Defined_Clusters'] == cluster)]['PC2'],\n                                            mode='markers',\n                                            name=cluster,\n                                            marker_color=color)\n    data_kmeans.append(scatter_obj_cluster_kmeans)\n    color = next(cyclecolors)\n    \n\n# Plotting the centroids of each cluster\nfor centroid in kmeans_centroids:\n    scatter_obj_centroid_kmeans = go.Scatter(x=centroids_df[(centroids_df['cluster_centroid'] == centroid)]['X_coord'],\n                                             y=centroids_df[(centroids_df['cluster_centroid'] == centroid)]['Y_coord'],\n                                             mode='markers',\n                                             name=centroid,\n                                             marker_size=12,\n                                             marker_symbol='x-dot',\n                                             marker_color='black')\n    data_kmeans.append(scatter_obj_centroid_kmeans)\n\n\n# Plot's Layout (background color, title, etc.)\nlayout_kmeans = go.Layout(title='PCA 2-Dimensional visualization of clusters created by K-Means Algorithm',\n                          xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))\n\nfig_kmeans = go.Figure(data=data_kmeans, layout=layout_kmeans)\nfig_kmeans.show()","e2d04d7e":"# Agglomerative Clustering and dendrogram plotting libraries\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, ward\n\n# Generating Linkage array (using ward linkage) and dendrogram object\nlinkage_array = ward(transformed_df)\ndendrogram(linkage_array)\n\n# Plotting dendrogram\nax = plt.gca()\nbounds = ax.get_xbound()\n\nax.plot(bounds, [52, 52], '--', c='k')\nax.plot(bounds, [41, 41], '--', c='k')\nax.plot(bounds, [34.9, 34.9], '--', c='k')\nax.plot(bounds, [31.75, 31.75], '--', c='k')\nax.text(bounds[1], 52, ' two clusters', va='center', fontdict={'size': 15})\nax.text(bounds[1], 41, ' three clusters', va='center', fontdict={'size': 15})\nax.text(bounds[1], 34.9, ' four clusters', va='center', fontdict={'size': 15})\nax.text(bounds[1], 31.75, ' five clusters', va='center', fontdict={'size': 15})\n\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Cluster Distance\")","265312a6":"# Defining a function to find the optimal number of A.C.'s n_clusters parameter:\ndef findOptimalNClustersAC(transformed_df):\n    \n    # Number of clusters to search for and silhouette_scores list\n    range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n    silhouette_scores = []\n\n    # Testing n_clusters options\n    for n_clusters in range_n_clusters:\n        ac = AgglomerativeClustering(n_clusters=n_clusters)\n        cluster_labels = ac.fit_predict(transformed_df)\n        \n        # Evaluating clusters created by Agglomerative Clustering\n        silhouette_avg = silhouette_score(transformed_df, cluster_labels)\n        print(\"Agglomerative Clustering: for n_clusters =\", n_clusters, \", the average silhouette_score is\", silhouette_avg)\n        \n        # Appending iteration's avg silhouette score to scores list\n        silhouette_scores.append(silhouette_avg)\n        \n    return range_n_clusters, silhouette_scores\n\nrange_n_clusters_ac, silhouette_scores_ac = findOptimalNClustersAC(transformed_df)","dc55b781":"# Visualizing the \"Elbow\" graph\nelbow_ac = go.Scatter(x=range_n_clusters_ac,\n                      y=silhouette_scores_ac,\n                      mode='lines',\n                      name='agglomerative clustering elbow line',\n                      marker_color='dodgerblue')\n\n# Plot's Layout (background color, title, etc.)\nlayout_elbow_ac = go.Layout(title='Number of Clusters x Average Silhouette Score, Agglomerative Clustering Algorithm',\n                            xaxis=dict(title='n_clusters'), yaxis=dict(title='Average Silhouette Score'))\n\nelbow_ac = go.Figure(data=elbow_ac, layout=layout_elbow_ac)\nelbow_ac.show()","9da34bda":"# Since the best value for silhouette_score was found with 3 clusters,\n# let's create an A.C. Object with n_clusters = 3.\nac = AgglomerativeClustering(n_clusters=3)\nac_defined_clusters = ac.fit_predict(transformed_df)\n\n# Concatenating A.C. clusters into the 2-D PCA transformed df\npca_df['AgglomerativeClustering_Defined_Clusters'] = ''\npca_df['AgglomerativeClustering_Defined_Clusters'] = ac_defined_clusters\npca_df['AgglomerativeClustering_Defined_Clusters'] = pca_df['AgglomerativeClustering_Defined_Clusters'].astype(str)\n\npca_df.head(10)","01043fba":"# Colors\ncolors = ['orange', 'steelblue', 'violet', 'tomato', 'lime']\n\ncyclecolors = cycle(colors)\ncolor = next(cyclecolors)\n\nac_clusters = pca_df.AgglomerativeClustering_Defined_Clusters.unique()\n\n\n# Visualizing the two-dimensional dataset\ndata_ac = []\n\nfor cluster in ac_clusters:\n    scatter_obj_cluster_ac = go.Scatter(x=pca_df[(pca_df['AgglomerativeClustering_Defined_Clusters'] == cluster)]['PC1'],\n                                            y=pca_df[(pca_df['AgglomerativeClustering_Defined_Clusters'] == cluster)]['PC2'],\n                                            mode='markers',\n                                            name=cluster,\n                                            marker_color=color)\n    data_ac.append(scatter_obj_cluster_ac)\n    color = next(cyclecolors)\n\n\n# Plot's Layout (background color, title, etc.)\nlayout_ac = go.Layout(title='PCA 2-Dimensional visualization of clusters created by Agglomerative Clustering algorithm',\n                          xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))\n\nfig_ac = go.Figure(data=data_ac, layout=layout_ac)\nfig_ac.show()","4ee45dcc":"# DBSCAN's sklearn library\nfrom sklearn.cluster import DBSCAN\n\n# Creating DBSCAN object and defining clusters for each data point\n# NOTE: Noise points (outliers) are assigned to \"cluster -1\"\ndbscan = DBSCAN(min_samples=20, eps=2.4)\ndbscan_defined_clusters = dbscan.fit_predict(transformed_df)\n\n# Evaluating clusters created by DBSCAN\nsilhouette_dbscan = silhouette_score(transformed_df, dbscan_defined_clusters)\nprint(\"DBSCAN: for eps = 2.4 and min_samples = 20, the average silhouette_score is\", silhouette_dbscan)","3dc00162":"# Concatenating DBSCAN clusters into the 2-D PCA transformed df\npca_df['DBSCAN_Defined_Clusters'] = ''\npca_df['DBSCAN_Defined_Clusters'] = dbscan_defined_clusters\npca_df['DBSCAN_Defined_Clusters'] = pca_df['DBSCAN_Defined_Clusters'].astype(str)\n\npca_df.head(10)","64cfa185":"# Colors\ncolors = ['orange', 'steelblue', 'violet', 'tomato']\n\ncyclecolors = cycle(colors)\ncolor = next(cyclecolors)\n\ndbscan_clusters = pca_df.DBSCAN_Defined_Clusters.unique()\n\n\n# Visualizing the two-dimensional dataset\ndata_dbscan = []\n\nfor cluster in dbscan_clusters:\n    scatter_obj_cluster_dbscan = go.Scatter(x=pca_df[(pca_df['DBSCAN_Defined_Clusters'] == cluster)]['PC1'],\n                                            y=pca_df[(pca_df['DBSCAN_Defined_Clusters'] == cluster)]['PC2'],\n                                            mode='markers',\n                                            name=cluster,\n                                            marker_color=color)\n    data_dbscan.append(scatter_obj_cluster_dbscan)\n    color = next(cyclecolors)\n\n\n# Plot's Layout (background color, title, etc.)\nlayout_dbscan = go.Layout(title='PCA 2-Dimensional visualization of clusters created by DBSCAN',\n                          xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))\n\nfig_dbscan = go.Figure(data=data_dbscan, layout=layout_dbscan)\nfig_dbscan.show()","8e64d71e":"# Titanic Dataset: Automatic EDA, different Data Preprocessing & Modeling Techniques compared with Pipelines + RandomSearchCV and much more!\n\n![Titanic.jpg](attachment:Titanic.jpg)\n\nThe main goal of this notebook is to try to present a complete approach to modeling problems, that goes from Exploratory Data Analysis to applying Supervised and Unsupervised learning techniques to our data. This notebook's content is mainly directed to data scientists, data science students or people interested in how these techniques can be applied into data.\n\nThanks to Andreas C. Muller, Sarah Guido & other co-authors, for writting the book **\"Introduction to Machine Learning with Python\"**. A great source of knowledge for Data Scientists of all levels.\n\nNotebook written by **Pedro de Matos Gon\u00e7alves**","3697aa2c":"From the pie chart showed above, we can notice a peculiar behavior: when looking at passengers that didn't survive, ~68% of them were at \"Pclass\" 3. When looking at passengers that survived, only ~35% of them were at \"Pclass\" 3.\n\nAt the same point of view, when looking at passengers that survived, ~40% of them were at \"Pclass\" 1. At the non-survivors, only 14.6% of them were at \"Pclass\" 1.\n\nIt seems that there is some kind of relation between \"P-class\" and the fact of a passenger surviving the accident or not. Let's get into more detail.","ce46b16f":"## Part 1: Dimensionality Reduction\n\nWhen working on a dataset with too many columns (a high-dimensional dataset), one might think if there is any way of visualizing data points on a 2-D image without having to create (X, Y) plots for each possible pair of columns. Dimensionality Reduction techniques aim to solve this problem by reducing the original high number of dimensions to a new dimensional representation of your choice.\n\n## 1.1 - PCA (Principal Component Analysis)\n\nPCA is a very common technique for dimensionality reduction. It's generally used to find a new representation of your original dataset, that cointains only a percentage of it's original variance (information is lost in the process of representing higher dimensions into lower dimensions).\n\nIn a brief description, PCA tries to identify the directions where the original data varies the most (horizontal, vertical, diagonal, etc.) and sets these directions as elements called Principal Components. It means that, PC1 (Principal Component 1) is the direction to where our data varies that can better explain it's total variance, PC2 (principal component 2) is the second best direction for explaining it's total variance, and so on. These principal components are the new features in our reduced dimensional space.\n\n![PCA.webp](attachment:PCA.webp)\n\nIf you want to have a better look at how PCA math really works, please have a look at this youtube video by Josh Stormer, on his \"StatQuest\" youtube channel: https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ","364b9189":"---\n\n# *Extra Section - Unsupervised Learning: Dimensionality Reduction and Clustering*\n\nAnother important field of Machine Learning is Unsupervised Learning. Because it may not be so intuitive for beginners to clearly understand where and when to apply Unsupervised Learning techniques, the goal of this section is to try to clarify the way, showing some practical applications of these techniques in the Titanic dataset.","a3d858e0":"---\n\n# *Section 2 - Supervised Learning: Classification*\n\nNow that we have some nice context about the data we are working with, let's dive into the modeling part.\n\nFirst of all, we import the libraries we're going to use.","a4a23ca3":"From both PCA and t-SNE 2-dimensional representations, we can see that there isn't a clear linear separation between survivors and non-survivors. This can be interpreted as a sign that linear separation lines (linear models) probably wouldn't work that well on a job to separate both classes. Because of that, non-linear models such as Kernelized SVMs and Decision-tree based methods should have better results, when modeling for this specific problem.","4de9b0a7":"## 2.2 - Agglomerative Hierarchical Clustering\n\nThe term \"Agglomerative\" in the name of this algorithm can give us some pretty good intuiton about how it works. This technique is based on \"merging\" individual data points that are close to each other to form bigger groups, until the given number of clusters (parameter n_clusters) is reached.\n\n![Agglomerative_Hierarchical_Clustering.PNG](attachment:Agglomerative_Hierarchical_Clustering.PNG)\n\nThe image above was taken from page 185 of the book \"Introduction to Machine Learning with Python\" by Andreas C. Muller & Sarah Guido. Let's use it to exemplify how this algorithm clusterizes data points. Initially, we have 12 data points representing our original dataset. Agglomerative Clustering starts by declaring each point it's own cluster. Then, in each step, the algorithm identifies two clusters that are closest to each other (\"most similar\" ones) and merges them, creating smaller clusters. This merging process is repeated until the number of clusters defined by the user is reached (in this case, n_clusters = 3).\n\nThe definition of what \"most similar\" clusters means is set by the algorithm's parameter \"linkage\". The most commonly used ones are (they're also known as **Measures of Dissimilarity**):\n\n-> **Single linkage:** merges the two clusters with the minimum distance between all of their points (uses the minimum of the distances between all observations of the two sets).\n\n-> **Average linkage:** merges the two clusters that have the smallest average distance between all of their points (uses the average of the distances of each observation of the two sets).\n\n-> **Complete (a.k.a Maximum linkage) linkage:** merges the two clusters that have the smallest maximum distance between all of their points (uses the maximum distances between all observations of the two sets).\n\n-> **Ward (a.k.a Minimum Variance) linkage:** picks the two clusters to merge such as the variance within all clusters increases the least (minimizes the variance of the clusters being merged). Generally works well on most datasets (the standard parameter of AgglomerativeClustering's implementation on scikit-learn).\n\nOne benefit of using this algorithm is that you can visualize mergings in a diagram we call **Dendrogram**.\n\n![DendrogramExample.PNG](attachment:DendrogramExample.PNG)\n\nDendrograms allow you to visualize the hierachical nature of the cluster's construction\/merging process. On the bottom, the dendrogram shows all data points (in this example, numbered from 0 to 11) and it's respective merging branches, while the Y axis represents the distances between clusters. The length of each branch shows how far apart merged clusters are from each other.\n\nNote how distant the cluster formed by points (1, 4, 3, 2, 8) is from the cluster formed by points (5, 0, 11, 10, 7, 6, 9). In practice, it means that if we choose n_clusters=2, the task of going from 3 to 2 clusters will merge some very far-apart points, which may not be a good choice.\n\nUnfortunately, this algorithm is still not that good at separating too complex shaped datasets, and it also does not scale well to very large ones. Still, the option of visualizing dendrograms might make this method suitable for some cases.\n\nLet's take a look at a Dendrogram example.","a2d308d5":"## Predictions\n\nNow that we have tried different preprocessing and modeling techniques, resulting in a final best pipeline, let's use it to predict the test data provided by kaggle.\n\n**Remember:** All transformations that were done in the training dataset must be done in the test set.","2b2cd3c5":"## 2.3 - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nThis is another useful algorithm for clustering tasks. As it's name says, it bases itself on the density of the points distributed in the feature space to create clusters. Basically, DBSCAN categorizes data points into 3 types: **Noise** points, **Border** points and **Core** points. It's main parameters are **\"n_samples\"** and **\"eps\"**.\n\n![DBSCAN.png](attachment:DBSCAN.png)\n\nLet's think about an example where we defined \"eps\" value as 1.5, and \"n_samples\" as 4. Imagine that you could draw circles (like the ones showed in the image above) around each data point represented in the feature space. The **radius** of these circles is defined by the parameter **\"eps\"** (1.5, in this example). \n\nNow, to simulate how the algorithm works: first, take an arbitrary point and look at it. If there are at least **\"n_samples\" (in this example, 4) points** inside it's \"1.5 eps\" radius drawn circle, this point is considered as **\"Core\"**, and DBSCAN assigns it to cluster 0. \n\nThen, for each point inside this Core point's drawn circle, do the same task: if there are at least \"n_samples\" points inside their own circles, they're also categorized as \"Core\" points, and also assigned to cluster 0.\n\n![dbscan_2.png](attachment:dbscan_2.png)\n\nIn a case where a data point has **less than \"n_samples\"** points inside it's circle (2, for example), the algorithm checks if at least one of these points are considered \"Core\" ones. If so, this data point is classified as **\"Border\"**. \n\nIf the condition above is false (meaning there are less than \"n_samples\" points inside this point's circle, **and none of them are core ones**), it is classified as **\"Noise\"** (it can be interpreted as an outlier, that doesn't belong to any cluster). This process continues untill all points are analyzed and clusterized. \n\nThe main benefits of using DBSCAN, when compared to the other algorithms we have seen so far, are: you don't need to previously set the number of clusters to be created; it will be automatically found, depending (mainly) on the values set for \"eps\" and \"n_samples\". Also, it can correctly detect more complex cluster shapes and points that doesn't belong to any clusters (outliers).\n\nBenefits aside, the algorithm also has it's own disadvantadges. It fails to clusterize points when the variance inside each cluster is too different. Aditionally, just like Agglomerative Clustering, it doesn't allow predictions on new data.","10e72150":"### Visualizing Agglomerative Clustering results using the PCA reduced Dataset\n\nTo check out the clusters that were created by the Agglomerative Clustering algorithm, we can use the PCA 2-dimensional dataset to visualize points and it's respective segmentations.","797f8dde":"## PPS (Predictive Power Score)\n\nYou may have heard about correlation matrices. Basically, correlation matrices are able to identify linear relationships between variables. Because relationships in our data may sometimes be non-linear (most of the times, actually), we can use a PPS (Predictive Power Score) matrix, to figure out **non-linear relations** between columns.\n\nIf you want to understand why PPS is important, I recommend you to read this medium article: https:\/\/towardsdatascience.com\/rip-correlation-introducing-the-predictive-power-score-3d90808b9598\n\nAlso, take a look at the Python PPS implementation used in this notebook: https:\/\/github.com\/8080labs\/ppscore","aea7545d":"## 1.2 - t-SNE (t-Distributed Stochastic Neighbour Embedding)\n\nAnother great algorithm generally used for visualizing X-dimensional representations of data is **t-SNE**. \n\nIn a brief description, t-SNE initially calculates distances between data points in the original feature space (by creating something called a **Similarity Matrix**: a matrix representing each data point and it's distance to the other data points in the feature space). Then, it represents all original points by randomly positioning them in a new dimensional-space (for exmaple: in a 2-D space), following a t-distribution. After that, it creates another Similarity Matrix, now for this new random representation.\n\nLastly, it moves the points that were randomly positioned on the new dimensional space, trying to make the second similarity matrix as close as possible to the first one. That is, it tries to represent points that are closer in the original feature space closer, and points that are far apart, farther apart.\n\n\n![tSNE.png](attachment:tSNE.png)\n\n\nIf you want to know more about t-SNE, please take a look at this article: https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding\n\nAlso, there is a really didatic explanation in \"StatQuest\" youtube channel, so I invite you to watch it too: https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM","961e90ba":"As we can see both from the plot and hypothesis tests showed above, there is actually a statistically significant difference between the means of both distributions (ages of survivors and non-survivors). Let's do some more exploring to see what further information we can gather from this data.","990e6950":"## 2.1 - K-Means Clustering\n\n![KMeans.png](attachment:KMeans.png)\n\nK-Means does it's clustering by calculating the distances of each data point in the feature space from something we call **centroids**. Centroids are (X, Y) coordinates that are defined as the center of each one of the clusters.\n\nInitially, K-Means initializes all centroids randomly. The number of centroids is defined by the user on the attribute n_clusters. Then, for each data point in the feature space, it calculates distances from these centroids. Each data point is assigned to the cluster defined by the closest centroid. After that, K-means updates the centroids' positions to be the mean (center) of each cluster. It then calculates the distances for each data point again, and assign them to new clusters. This process is repeated until there are no major updates in centroids' positions.\n\nYou can see an example of K-Means training process in the image below. Square (a) shows data represented in the 2-dimensional feature space. Square (b) represents the random initialization of centroids (defined by KMeans' parameter setting of n_clusters = 2). Then, in square (c), each data point is assigned to a cluster based on it's distance to centroids. In square (d), centroids are updated to be the mean (center) of both clusters. Afterwards, each data point is again assigned to a cluster (based on it's distance to the updated centroids), and this process is repeated until no major changes are applied to centroid's positions.\n\n![k-means-process.jpg](attachment:k-means-process.jpg)\n\nK-Means is a great baseline algorithm. It's easily interpretable and scales well on large datasets. One negative point on it's usage is that you need to define the specific number of clusters you want the algorithm to find. In real world scenarios, where data shapes are much more complex, it is really hard to tell what is the optimal number of clusters (still, there are a few techniques to help dealing with that problem, such as the \"Elbow Method\"). Aditionally, because of it's behaviour of updating centroids to be the center of each cluster, K-Means can only capture relatively simple cluster shapes (spherical\/convex ones).\n\nThere is also a video about it on \"StatQuest\" youtube channel, so I recommend you to take a look at it: https:\/\/www.youtube.com\/watch?v=4b5d3muPQmA","e8d9aa79":"## Balancing Data\n\nAs we saw earlier on EDA section, our data is pretty much balanced, but we have a small number of observations in our training set. To try tackling this problem, we can take different approaches.\n\nThree common ones are  **RandomUnderSampling**, **SMOTE** and **SMOTEENN**. We can try using one of them to balance our data. \n\nWe also have the option of not balancing data, going straight to the Pipeline part.","736a5654":"## Pipeline Construction\n\nWell, you might be wondering now: What is a Pipeline?\n\n![pipeline.jpg](attachment:pipeline.jpg)\n\nWe can understand a Pipelines as a sequence of actions applied in data. Just like the image above, you can see that a full pipeline is made of several different small pipes. Take this to Data Science: imagine that each small pipe is a step in a modeling process. For example:\n\n#### -> Step 1: fill null values from numerical columns. \n\n#### -> Step 2: normalize numerical features, so they will be in the same scale. \n\n#### -> Step 3: fill null values from categorical features. \n\n#### -> Step 4: OneHotEncode categorical features.\n\n#### -> Step 5: fit a Machine Learning model and evaluate it.\n\nInstead of doing each one of these steps separately, we can create a Pipeline object that unites all of them, and then fit this object into our training data.\n\nAnd why should we do that?\n\nWell, there are a lot of advantages we get on using pipelines. Below are the ones I find most relevant for this discussion:\n\n\n### 1 - Production code gets much easier to implement\nWhen deploying a Machine Learning model into production, the main goal is to use it on data it hasn't seen before. To do that, the new data needs to be transformed the same way training data was. Instead of having several different functions for each one of the preprocessing tasks, you can use a single pipeline object to apply all of them sequentially. It means that, in 1 line of code, you can apply all needed transformations. Check an example of this in the \"Predictions\" section of this notebook.\n\n### 2 - When combined with RandomSearchCV, it is possible to test several different pipeline options\nYou must have already asked yourself, when training your models: \"for this type of data, what works best? Filling missing values with the average or the median of a column? Should I use MinMaxScaler or StandardScaler? Apply dimensionality reduction? Create more features using, for example, PolynomialFeatures?\" Using Pipelines and hyperparameter search functions (like RandomSearchCV), you can search through entire sets of data pipelines, models and parameters automatically, saving up effort invested by you in the search for optimal feautre engineering methods and models\/hyperparameters.\n\nSuppose we have 4 different pipelines:\n\n#### -> Pipeline 1: fill missing values from numeric features by imputing the mean of each column - apply MinMaxScaler - apply OneHotEncoder to categorical features - fits the data into a KNN Classifier with n_neighbors = 15.\n\n#### -> Pipeline 2: fill missing values from numeric features by imputing the mean of each column - apply StandardScaler - apply OneHotEncoder to categorical features - fits the data into a KNN Classifier with n_neighbors = 30.\n\n#### -> Pipeline 3: fill missing values from numeric features by imputing the median of each column - apply MinMaxScaler - apply OneHotEncoder to categorical features - fits the data into a Random Forest Classifier with n_estimators = 100.\n\n#### -> Pipeline 4: fill missing values from numeric features by imputing the median of each column - apply StandardScaler - apply OneHotEncoder to categorical features - fits the data into a Random Forest Classifier with n_estimators = 150.\n\nInitially, you might think that, to check which pipeline is better, all you need to do is to create all of them manually, fit your data, and then evaluate the results. But what if we want to increase the range of this search, let's say, to over hundreds of different pipelines? It would be really hard to do that manually. And that's where RandomSearchCV comes into play. \n\n\n### 3 - No information leakage when Cross-Validating\n\nThis one is a bit trickier, specially for beginners. Basically, when cross-validating, data should be transformed **inside each CV step, not before**. When doing cross validation **after** transforming the training set (with a StandardScaler, for example), **information from it is leaked to the validation set**. This may lead to biased\/unoptimal results. \n\nThe right way to do that is to normalize data **inside** cross-validation. That means, for each CV step, a scaler is fitted **only on the training set**. Then, this scaler transforms the **validation set**, and the model is evaluated. This way, no information from the training set is leaked to the validation set. When using pipelines inside RandomSearchCV (or GridSearchCV), this problem is taken care of.\n\n![train_valid_test.png](attachment:train_valid_test.png)\n\nThis is a key concept in Machine Learning, so it's important to understand why. I recommend reading more in-depth articles about the subject. Furthermore, chapter 6 of the book \"Introduction to Machine Learning with Python\" by Andreas C. Muller & Sarah Guido (pages 306 and 307, mainly) gives a really good perspective on this problem.\n\n---\n\nFor further information about Pipelines and RandomSearchCV, check the docs out:\n\n**Pipelines** - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n\n**RandomizedSearchCV** - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html","331537be":"## Feature Engineering \n\nTo help us get a better performance, we can create new features based on the original features of our dataset.\n\nSome new features created here were based on the great ideas shown on this brilliant notebook from **Gunes Evitan**: https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.-Feature-Engineering","8af84863":"---\n\n## Part 2: Clustering\n\nThe main objective of clustering techniques is to assing each data point in the dataset to a cluster (the concept is similar to classification). The difference is that in classification, your data is formed by features (X) and a specific target (Y) that is used to model a decision function. In clustering, no specific target (Y) is used to train the algorithms. The goal is to receive the feature vector (X) as an input, and assign similar data points to the same \"group\" (cluster).\n\nClustering techniques are widely used to help data scientists generate insights from data. Here, I present 3 common and useful algorithms used for this task: K-Means, Agglomerative Hierarchical Clustering and DBSCAN (Density-Based Spatial Clustering of Applications with Noise). Each one has got it's own advantages and disadvantages. I'll try to describe how each algorithm clusterizes data points, and the main benefits of using each one.","7db4dee5":"Checking out the plots and hypothesis tests over fare distributions, comparing Survivors and non-Survivors, we can again observe that there is a statistically significant difference between the means of both groups. \n\nWhen checking out the boxplots, we can see that fare values of survivors are generally higher, when compared to fare values of non-survivors. This information is probably related to the \"Pclass\" percentages we have seen before on the pie plots.","a5983cc0":"After creating new features, we can drop useless columns that we won't use in the training process.","a700b556":"Using the power of both automatic EDA libraries listed above, we can observe each variable's behaviour individually, with plots that goes from Histograms to Boxplots, Correlation Matrix and much more. It speeds up time and minimizes the effort spent on the initial process of our work.\n\nWe can gather some really useful information from both reports. Let's now point some of them out:\n\n* Our classes are not that much disbalanced. We have ~38% of the passengers into class \"1\" (survived) and ~62% of the passengers into class \"0\" (didn't survive).\n\n\n* The \"Pclass\" column, that informs us about the passenger's ticket class, shows us that ~55% of them are on class 3, ~24% of them are on class 2 and ~21% on class 1.\n\n\n* Most of the passengers into this dataset are male: ~35% of the passengers are female, and ~65% are male.\n\n\n* Almost 20% of the values in the \"Age\" column are missing. We can fill out these nulls with various techniques, such as filling them with the distribution's mean. The ages distribution is a little bit skewed, with it's mean being around 30 years old, and it's standard deviation being close to 15. The oldest passenger we have in this dataset is 80 years old.\n\n\n* According to the \"SibSP\" column, most of the passengers (~68%) didn't have any spouses or siblings aboard the ship. That is also applied when we check out the \"Parch\" column.\n\n\n* The distribution of Fares is much more skewed. It's mean value is around 32, with it's standard deviation being close to 50. It's minimum value is 0, and it's maximum value is 512.3292. That means that we're going to have to deal with this column carefully if we plan to use models such as SVMs.\n\n\n* When ckecking the \"Embarked\" column, it shows us that 72.3% of the passengers embarked at Southampton port, 18.9% of the passengers at Cherbourg port and 8.6% of the passengers at Queenstown port.\n\n\n* \"Fare\" values are higher for passengers with \"Pclass\" = 1, lower for passengers with \"Pclass\" = 2 and even lower for passengers with \"Pclass\" = 3. Logically, it looks like the classification of \"Pclass\" is defined by the value of the passenger's fare.\n\n","80ce7914":"### Visualizing K-Means results using the PCA reduced Dataset\n\nTo check out how clusters were created, we can use the PCA 2-dimensional dataset to visualize points and it's respective segmentations.","942a6f4a":"## More Exploration\n\nBefore we go to the modeling part, let's take a look at a few more plots that gives us a different perspective from the ones generated above. That may give us further insights and help us understand the differences between the passengers that survived the catastrophe and the people that didn't. For these visualizations, we are going to use Plotly, a library that gives us beautiful plots and allows us to interact with them.\n\nFirst, let's take a look at the differences between the ages of both groups, using a Violin plot.","e22dd729":"To begin our analysis, lets take our first look at the dataset. To save some precious time on our Exploratory Data Analysis process, we are going to use 2 libraries: **\"pandas_profiling\"** and **\"autoviz\"**.","52b4d7f3":"## Plotting Feature Importances\n\nIf we want to, it's also possible to check the feature importances of the best model, in case they're easy to understand and explain.\n\nJust remember that, if the best pipeline found in RandomSearchCV applies dimensionality reduction or creates new features using PolynomialFeatures, it will be much harder to explain importances.\n\nIn a scenario that no transformations are applied to the features inside the pipeline, if the model is tree-based (RandomForestClassifier, for example), or linear regression-based (Logistic Regression, for example), then explaining most important features becomes much easier.","d16d3933":"After going through all steps in RandomSearchCV, we can check the results from it's steps using the \"cv_results_\" atrribute.","732573eb":"\n## Model training & Evaluation functions\n\nAfter all the preprocessing, we are now ready to build and evaluate different Machine Learning models.\n\nFirst, let's create a function responsible for evaluating our classifiers on a test set we will create later.","9efaa14c":"Looking at this PPS matrix, we can see that the best univariate predictor of the **Survived** variable is the column **Ticket**, with 0.19 pps, followed by **Sex**, with 0.13 pps. That makes sense because women were prioritized during the rescue, and ticket is closely related to **Pclass**. The best univariate predictor of the **Parch** variable is the column **Cabin**, with 0.37 pps, and so on.","5b2c33e5":"### Visualizing DBSCAN's results using the PCA reduced Dataset\n\nTo check out the clusters that were created by DBSCAN, we can use the PCA 2-dimensional dataset to visualize points and it's respective segmentations.","3534b697":"---\n\n# *Section 1 - Data Exploration*\n\nThe first step is to import needed libraries."}}