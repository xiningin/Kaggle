{"cell_type":{"a8d1189d":"code","ebe2e7f6":"code","d42f90be":"code","c130ebd2":"code","c3ce025c":"code","cc83f1ab":"code","82e7c162":"code","3f49d853":"code","6fcecd26":"code","3704c0d6":"code","215173f7":"code","96d252f7":"code","52484f84":"code","6056b5b7":"code","6c34bace":"code","782f727a":"code","0708a1db":"code","0cf4391b":"code","33f65f34":"code","b10fd7ee":"code","7be85f12":"code","3628f1b6":"code","d624089a":"code","170ee448":"code","f046176a":"code","0101b539":"code","5de0e706":"code","2a376da8":"code","38de04b7":"code","2fc359d3":"code","15c032ac":"code","7c6abaeb":"code","ebe1d360":"code","ba87192f":"code","3b926aa9":"markdown","16646b9c":"markdown","ad3972f6":"markdown","1af42f5e":"markdown","d35cdc70":"markdown","6ebf3678":"markdown","1aef1409":"markdown","1d2b4ea2":"markdown","fd0b14d8":"markdown","c93ffabd":"markdown","adcc6cad":"markdown","6b828fe1":"markdown","edf684cc":"markdown","9d79e032":"markdown","5c86de54":"markdown","9fe802aa":"markdown","870c02dc":"markdown","48185a9f":"markdown","0888b4c9":"markdown","9c12c1b5":"markdown","76ffdc2f":"markdown","d337fe37":"markdown","2c9b0d4d":"markdown","51343844":"markdown","b49bb904":"markdown","aec147d3":"markdown","e78d132b":"markdown","fe4d1e56":"markdown","42f3e2ad":"markdown","10c14040":"markdown","7e16d39a":"markdown","c73c036f":"markdown","2b98175b":"markdown","a1b3d6f0":"markdown","f0ddaa05":"markdown","ba931aca":"markdown","87d396c4":"markdown","104db545":"markdown","cf144032":"markdown"},"source":{"a8d1189d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Plotting Functions\nimport matplotlib.pyplot as plt\n\n#Aesthetics\nimport seaborn as sns\nsns.set_style('ticks')\n\n#Data Import\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ebe2e7f6":"lter=pd.read_csv('\/kaggle\/input\/palmer-archipelago-antarctica-penguin-data\/penguins_lter.csv')\nsize=pd.read_csv('\/kaggle\/input\/palmer-archipelago-antarctica-penguin-data\/penguins_size.csv')","d42f90be":"print('***lter file:')\nlter.info()\nprint('----'*10)\nprint('***size file:')\nsize.info()","c130ebd2":"print(lter.columns)\nprint(lter['Species'].unique())\nlter.head()","c3ce025c":"new_lter=lter.copy()\n\nnew_lter['Species']=new_lter['Species'].replace({'Adelie Penguin (Pygoscelis adeliae)':'P.adeliae',\n                                                 'Chinstrap penguin (Pygoscelis antarctica)':'P.antartica',\n                                               'Gentoo penguin (Pygoscelis papua)':'P.papua'}) # Abbreviating the names\n\nnew_lter['Sex'].fillna('.', inplace=True)#First replace the null value with a '.' as part of that column contains them already\nnew_lter['Sex']=new_lter['Sex'].replace({'MALE':'male','FEMALE':'female'})#replacing uppercases with lower cases\nnew_lter['Sex']=new_lter['Sex'].replace({'.':new_lter['Sex'].mode()[0]})#By running this a second time with the space, it will take the mode of sex\n\nnew_lter['Culmen Length (mm)'].fillna(new_lter.groupby('Species')['Culmen Length (mm)'].transform('mean'),inplace=True)\n\nnew_lter['Culmen Depth (mm)'].fillna(new_lter.groupby('Species')['Culmen Depth (mm)'].transform('mean'),inplace=True)\n\nnew_lter['Flipper Length (mm)'].fillna(new_lter.groupby('Species')['Flipper Length (mm)'].transform('mean'),inplace=True)\n\nnew_lter['Body Mass (g)'].fillna(new_lter.groupby('Species')['Body Mass (g)'].transform('mean'),inplace=True)\n\nnew_lter['Delta 15 N (o\/oo)'].fillna(new_lter.groupby('Species')['Delta 15 N (o\/oo)'].transform('mean'),inplace=True)\n\nnew_lter['Delta 13 C (o\/oo)'].fillna(new_lter.groupby('Species')['Delta 13 C (o\/oo)'].transform('mean'),inplace=True)\n\nnew_lter['Body Mass (kg)']=new_lter['Body Mass (g)']\/1000 #Conversion to kilograms\n\nnew_lter=new_lter.drop(['Body Mass (g)','studyName','Sample Number','Comments','Individual ID'], axis=1)\n#We will not need these columns as they will not contribute to classification.\n\nprint(new_lter.info())#Lets check to make sure all NaN is gone\nnew_lter.head()#Visualize the table to confirm the changes we wanted","cc83f1ab":"#Observing Categorical Values\nprint('Region', new_lter['Region'].unique())\nprint('Island', new_lter['Island'].unique())\nprint('Stage', new_lter['Stage'].unique())\nprint('Clutch', new_lter['Clutch Completion'].unique())","82e7c162":"new_lter=new_lter.drop(['Region', 'Stage', 'Date Egg'], axis=1)\nnew_lter.head()","3f49d853":"fig, ax=plt.subplots()\n\nsns.countplot(data=new_lter, x='Species', hue='Sex', palette=['darkblue','darkred'])\n\n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=20)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Species', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Count', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Species Count.png')","6fcecd26":"fig, ax=plt.subplots()\n\nsns.countplot(data=new_lter, x='Island', hue='Species', palette=['darkblue','darkred','darkgreen'])\n\n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=20)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Island', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Count', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Island.png')","3704c0d6":"fig, ax=plt.subplots()\n\nsns.countplot(data=new_lter, x='Clutch Completion', hue='Species', palette=['darkblue','darkred', 'darkgreen'])\n\n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=20)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Clutch Completion', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Count', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Clutch.png')","215173f7":"fig, ax=plt.subplots()\n\nsns.scatterplot(data=new_lter, x='Culmen Length (mm)',y='Culmen Depth (mm)', hue='Species', palette=['b','r','g'], s=30)\n\n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=0)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Culmen Length (mm)', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Culmen Depth (mm)', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Culmen.png')","96d252f7":"color=['darkblue','darkred','darkgreen']\nsns.FacetGrid(data=new_lter, hue=\"Species\", height=6, palette=color)\\\n    .map(sns.kdeplot, \"Flipper Length (mm)\", lw=2) \n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\n\nplt.xticks(fontsize=12, fontweight='bold', rotation=0)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Flipper Length (mm)', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Count', fontsize=12, fontweight='bold', color='k')\nplt.legend(fontsize='large',prop={'weight':'bold'})\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Flipper.png')\n\nplt.show()\n","52484f84":"fig, ax=plt.subplots()\n\nsns.boxplot(data=new_lter, x='Species', y='Body Mass (kg)', palette=color)\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=20)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Species', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Body Mass (kg)', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Species Mass.png')","6056b5b7":"fig, ax=plt.subplots()\nsns.boxplot(data=new_lter, x='Species', y='Body Mass (kg)',hue='Sex', palette=color)\n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=20)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Species', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Body Mass (kg)', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Gender Mass.png')","6c34bace":"fig, ax=plt.subplots()\n\nsns.scatterplot(data=new_lter, x='Delta 15 N (o\/oo)',y='Delta 13 C (o\/oo)', hue='Species', palette=color, s=30)\n\n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=0)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('N-15', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('C-13', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Isotope ratios.png')","782f727a":"sns.pairplot(new_lter, hue='Species', palette=color)\nplt.savefig('pairplot.png')","0708a1db":"from sklearn import preprocessing \nLE=preprocessing.LabelEncoder()","0cf4391b":"lter_encode=new_lter.copy()\nlter_encode['Island']=LE.fit_transform(lter_encode['Island'])\nlter_encode['Clutch Completion']=LE.fit_transform(lter_encode['Clutch Completion'])\nlter_encode['Sex']=LE.fit_transform(lter_encode['Sex'])\nlter_encode['Species_Code']=LE.fit_transform(lter_encode['Species']) #This will be used for a correlation matrix\nlter_encode.head()","33f65f34":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif","b10fd7ee":"#Feature Selection\nX=lter_encode.drop(['Species', 'Species_Code'], axis=1)\nY=lter_encode['Species']\nbestfeatures = SelectKBest(score_func=f_classif, k='all')\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(12,'Score'))  #print 10 best features","7be85f12":"###Sets for full aesthetics of pots\nfig, ax = plt.subplots(figsize=(7,7))\n\n\n#Setting seaborn plot\nbar=sns.barplot(data=featureScores, x='Score', y='Feature', palette='viridis',linewidth=0.5, saturation=2, orient='h')\n\n#Setting tick parameters \nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=0)\nplt.yticks(fontsize=12, fontweight='bold')\n\n#Labels\nplt.xlabel('Importance Score', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('Feature', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('feature selection.png')","3628f1b6":"fig, ax = plt.subplots(figsize=(7,7))\nsns.heatmap(lter_encode.corr(),annot=True, linewidths=0.1)\nplt.xticks(fontsize=12, fontweight='bold', rotation=90)\nplt.yticks(fontsize=12, fontweight='bold')\n\nplt.savefig('correlation matrix.png')\nplt.show()","d624089a":"lter_encode_drop=lter_encode.copy()\nlter_encode_drop=lter_encode_drop.drop(['Sex','Clutch Completion','Species_Code'], axis=1)\nlter_encode_drop.head()","170ee448":"OHE=preprocessing.OneHotEncoder()\nlter_ohe=lter_encode_drop.copy()\n\nlter_code=OHE.fit_transform(lter_ohe[['Island']]).toarray()\n\nlter_list=list(sorted(new_lter['Island'].unique()))  #Will name the column values to the respective Island\n\nlter_code=pd.DataFrame(lter_code, columns=lter_list)#Setting OHE dataframe for merge\nlter_ohe=pd.concat([lter_code,lter_ohe], axis=1)#Merging Data Frames\n\nlter_ohe.head()","f046176a":"lter_ohe=lter_ohe.drop(['Island'], axis=1)\nlter_ohe.head()","0101b539":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(lter_ohe.drop(['Species'], axis=1), lter_ohe['Species'],test_size=0.25, random_state=0)\n\nprint(\"X_train shape :\",X_train.shape)\nprint(\"Y_train shape :\",y_train.shape)\nprint(\"X_test shape :\",X_test.shape)\nprint(\"Y_test shape :\",y_test.shape)","5de0e706":"scaler=preprocessing.StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train) #Scaling and fitting the training set to a model\nX_test_scaled=scaler.transform(X_test) #Transformation of testing set based off of trained scaler model","2a376da8":"from sklearn.decomposition import PCA\npca = PCA().fit(X_train_scaled)\nprint('Ratios:',pca.explained_variance_ratio_*100)#Ratio per component.","38de04b7":"num_com=np.arange(1,(len(pca.explained_variance_ratio_)+1),1)\n# The number of components to plot, the +1 is because np.arange cuts the number off at 1 minus endpoint value.\n\nratio=np.cumsum(pca.explained_variance_ratio_*100) #Setting the raios into a variable\n\nfig, ax = plt.subplots(figsize=(5,5))\n\nsns.lineplot(x=num_com, y=ratio, color='k', linewidth=3)\n\n#Tick parameters\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\n\nax.axhline(90, linestyle='--', color='k') # horizontal lines\n\n#Labels\nplt.xlabel('number of components', fontsize=12, fontweight='bold', color='k')\nplt.ylabel('cumulative explained variance (%)', fontsize=12, fontweight='bold', color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('PCA.png')","2fc359d3":"n=5 #Number of components\npca=PCA(n_components=n)\n\nX_train_pca=pca.fit_transform(X_train_scaled)# Fitting and transforming the training data\nX_test_pca=pca.transform(X_test_scaled)# Transforming the test data by the fitted trained PCA()","15c032ac":"from sklearn.svm import SVC #Classifier\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV #Paramterizers\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix #Accuracy metrics\nimport itertools #Used for iterations","7c6abaeb":"def Searcher(estimator, param_grid, search, train_x, train_y):\n    \"\"\"\n    This is a helper function for tuning hyperparameters using the two search methods.\n    Methods must be GridSearchCV or RandomizedSearchCV.\n    Inputs:\n        estimator: Any Classifier\n        param_grid: Range of parameters to search\n        search: Grid search or Randomized search\n        train_x: input variable of your X_train variables \n        train_y: input variable of your y_train variables\n    Output:\n        Returns the estimator instance, clf\n        \n    Modified from: https:\/\/www.kaggle.com\/crawford\/hyperparameter-search-comparison-grid-vs-random#To-standardize-or-not-to-standardize\n    \n    \"\"\"   \n    try:\n        if search == \"grid\":\n            clf = GridSearchCV(\n                estimator=estimator, \n                param_grid=param_grid, \n                scoring=None,\n                n_jobs=-1, \n                cv=10, #Cross-validation at 10 replicates\n                verbose=0,\n                return_train_score=True\n            )\n        elif search == \"random\":           \n            clf = RandomizedSearchCV(\n                estimator=estimator,\n                param_distributions=param_grid,\n                n_iter=10,\n                n_jobs=-1,\n                cv=10,\n                verbose=0,\n                random_state=1,\n                return_train_score=True\n            )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0)\n        \n    # Fit the model\n    clf.fit(X=train_x, y=train_y)\n\n    return clf","ebe1d360":"def plot_confusion_matrix(cm, title,label):\n    \"\"\"\n    Plot for Confusion Matrix:\n    Inputs:\n        cm: sklearn confusion_matrix function for y_true and y_pred as seen in https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\n        title: title of confusion matrix as a 'string'\n        label: the unique label that represents classes for prediction can be done as sorted(dataframe['labels'].unique()).\n    \"\"\"\n    classes=sorted(label)\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Greens)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    thresh = cm.mean()\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j]), \n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] < thresh else \"black\")   ","ba87192f":"#Grid Search SVM Parameters\nsvm_param = {\n    \"C\": [.01, .1, 1, 5, 10, 100], #Specific parameters to be tested at all combinations\n    \"gamma\": [0, .01, .1, 1, 5, 10, 100],\n    \"kernel\": [\"rbf\",\"linear\"\"poly\"],\n    \"random_state\": [1]}\n\n#Randomized Grid Search SVM Parameters\nsvm_dist = {\n    \"C\": np.arange(0.01,2, 0.01),   #By using np.arange it will select from randomized values\n    \"gamma\": np.arange(0,1, 0.01),\n    \"kernel\": [\"rbf\",\"linear\"\"poly\"],\n    \"random_state\": [1]}\n\n\"\"\"\nFollowing the code above, we can set the parameters for both grid search and randomized search. The grid search will evaluate all specified \nparameters while the randomized search will look at the parameters labeled in random order at the best training accuracy. The np.arange function\nallows for a multitude of points to be looked at between the set start and end values of 0.01 to 2. \"\"\"\n\n#Grid Search\nsvm_grid = Searcher(SVC(), svm_param, \"grid\", X_train_pca, y_train) #Searcher(clf, param_grid, searcher,train_x, train_y)\nacc = accuracy_score(y_true=y_test, y_pred=svm_grid.predict(X_test_pca))\ncfmatrix_grid = confusion_matrix(y_true=y_test, y_pred=svm_grid.predict(X_test_pca))#Setting Grid Search Confusion Matrix Function\n\n#Defining prints for accuracy metrics of grid\nprint(\"**Grid search results**\")\nprint(\"The best parameters are:\",svm_grid.best_params_)\nprint(\"Best training accuracy:\\t\", svm_grid.best_score_)\nprint('Classification Report:')\nprint(classification_report(y_true=y_test, y_pred=svm_grid.predict(X_test_pca)))\n\nprint('----'*20)\n\n#Randomized Grid Search\nsvm_random = Searcher(SVC(), svm_dist, \"random\", X_train_pca, y_train) #Searcher(clf, param_grid, searcher,train_x, train_y)\nacc = accuracy_score(y_true=y_test, y_pred=svm_random.predict(X_test_pca))\ncfmatrix_rand = confusion_matrix(y_true=y_test, y_pred=svm_random.predict(X_test_pca))#Setting Grid Search Confusion Matrix Function\n\n#Defining prints for accuracy metrics of randomized\nprint(\"**Random search results**\")\nprint(\"The best parameters are:\",svm_random.best_params_)\nprint(\"Best training accuracy:\\t\", svm_random.best_score_)\nprint('Classification Report:')\nprint(classification_report(y_true=y_test, y_pred=svm_random.predict(X_test_pca)))\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=lter_ohe['Species'].unique()) #grid matrix\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=lter_ohe['Species'].unique()) #randomized matrix\n\nplt.savefig('confusion.png')","3b926aa9":"# Splitting the Data \nFirst, the data will be split so we can train a scaler model to apply to an unknwon (test) data set. ","16646b9c":"At around ~4-5 components there is an accumulation of 90% explained variance. We will select 5 components as four is just <90%. ","ad3972f6":"# Conclusion\nWe observed that:\n* Many features have some prominent separation alone for distincting the penguin species.\n* Using KBestSelection we were able to use the most important features from the data set\n* 5 dimensions from PCA provided ~90% of cumulative data variance which:\n* Contributed enough variability to the data set to score 100% prediction accuracy towards the testing data set as shown with SVM using Grid\/Randomized Searches.","1af42f5e":"Laslty, we can summarize our EDA with this pairplot showing some of the separations we observed above between species. Similarly, this shows basic separation possibilities in different comparisons of our numerical variables. Next we will begin preprocessing for ML.","d35cdc70":"## Grid Search Function","6ebf3678":"## Confusion Matrix Function \n(Read red indents below def function for understanding of code)","1aef1409":"As we can see both searches provided 100% accuracy. Similarly, using the metric of an f1-score there was 100% accuracy showing that the model is very well built. ","1d2b4ea2":"Printing the column names and categorical classifiers will help make things faster by simple copy and paste. Similarly, by printing the columns we can check to make sure there is no extra spaccings in the titles when writing out the code later. Displaying the first few rows of the data frame allows us to get a quick look of what we are looking at.","fd0b14d8":"# Label Encoding and Feature Selection","c93ffabd":"Now that the data is clean, lets take a look at what values exist in the non-predicting categorical columns.","adcc6cad":"# Meaning To Each Feature\n- species: penguin species (Chinstrap, Ad\u00e9lie, or Gentoo)\n- culmen_length_mm: culmen length (mm)\n- culmen_depth_mm: culmen depth (mm)\n- flipper_length_mm: flipper length (mm)\n- body_mass_g: body mass (g)\n- island: island name (Dream, Torgersen, or Biscoe) in the Palmer Archipelago (Antarctica)\n- sex: penguin gender\n- Delta 15 N (o\/oo)- ratio of isotope ($^{15}$N)\n- Delta 13 C (o\/oo)- ratio of isotope ($^{13}$C)","6b828fe1":"We can further observe that generally, that the trend between all three species is that males weigh more. How about the elemental isotopic ratios?","edf684cc":"Given a majority from each species has clutch completion, this may be a bad variable to use later on as it doesnt offer much distinction. Looking further, maybe the culmen will vary.","9d79e032":"# Dimensionality Reduction With PCA\nPCA is a linear dimensionality reduction technique that computationally will reduce the data and separate it in an unsupervised manner by maximal variance. We will apply it to our data and see how it fits.","5c86de54":"## EDA\nBefore building a model, it is good practice to first explore the data for to find meaningful relationships. By exploring first, we can draw conclusions for why certain features may or may not be important later. First, we will simply count the total number of species by sex.","9fe802aa":"# Machine Learning with SVM and Grid\/Randomization Search\nFor modeling we will use the support vector machines classifiers (SVC). The SVC's can handle higher dimensional data and genearte hyperplanes for separation and score on a yes (1) no (1) basis. The rulings are decided for where a data point lands within a decision boundary. We can evalute multiple parameters at one using Grid or Randomization Search functions. Grid Search evalutes several input parameters at all combinations input while randomized search looks for the best. Cross-validation is the models self assemessment when trying to find the best parameters on the training data and can be done in \"n\" amount of replicates.","870c02dc":"Again, we can observe that both Sex and Clutch Completion have almost no correlation to species. Therfore, they will be dropped along with species code as that will no longer be needed either.","48185a9f":"## Label Encoding\nUsing label encoding will allow the conversion of categorical values to be changed to ordinal per column which can be then used for feature selection by correlation matrix. However, note that none of these variables are not truly ordinal and if selected will be converted to a One Hot Encoding (OHE) scheme as that is more appropriate for categorical data.","0888b4c9":"We see that both Region and Stage have only a single value and will not be useful for ML just by obsersvation. Therefore, we will drop those two prior to EDA.","9c12c1b5":"# <u>Penguins!: An Exploration and Classification of Species <\/u>\n## <br> Author: Christopher W. Smith <\/br>\n<br>Unfinished, Updated:10\/05\/2020 <\/br>\n\n<br> <i>linkdin<\/i>: www.linkedin.com\/in\/christopher-w-smith022 <\/br>\n\n<br> The purpose of this notebook is to perform an exploratory data analysis (EDA) on the data set from: Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081.<doi:10.1371\/journal.pone.0090081>. After, data preprocessing was performed looking at feature-engineering of selected features by ANOVA F-ratio. Then dimensionality reduction was done by princicipal component analysis (PCA) to reduce the number of features in the data set. Lastly, support vector machines (SVM) was used with hyper parameterization by grid and randomized searchers for the most accurately built models. <\/br>\n\nKaggle link: https:\/\/www.kaggle.com\/parulpandey\/palmer-archipelago-antarctica-penguin-data","76ffdc2f":"Indeed, we do see that P.papua does weigh more than the other two species. ","d337fe37":"Before building models, it is good practice to remove features that may be unneccessary. We can do this using the SelectKBest function in the sklearn package. This will give us a score of importance by ANOVA F-ratios. The higher the ratio the more important the feature will be.","2c9b0d4d":"# One Hot Encoding\nNext we will expand our categorical feature, Island, in a One-Hot encoding format to expand the data by cateogrical values. Given Island is the only one that is categorical for prediction we will use OHE on that.","51343844":"<b> Clearly, we have a higher count of P.adeliae than P.antartica which means when we move to machine learning later we may run into bias of our learning algorithm. Next, lets look at islands in which the species reside.","b49bb904":"# Scaling Preprocessing\nNext, the data will be scaled by the standard scaler function in the sklearn package using the formula $z=\\frac{X_{o}-\\mu}{\\sigma}$. This can help reduce the effect of outliers when modeling later.","aec147d3":"Intrestingly, P.antartica and P.papua are each found only on a specific island. This will assist in developing a model later. Next, the categorical variables within clutch completion can be counted as well.","e78d132b":"# Initial Relevant Packages\nLibraries to be used primarily for loading the data, cleaning, and EDA.","fe4d1e56":"# Cleaning The Data\nSimply, the data will be cleaned and abbreviated to make it easier to view. The species names can be abbreviated which will look better for plotting. The features for prediction will have the null values relaced by either the mean or the mode by group. ","42f3e2ad":"# Loading the Data","10c14040":"The two least important features as shown on the bar plot above are Clutch Completion and Sex. We can further verify and compare this by looking at a correlation matrix which considers a pearson (linear) correlation coefficient.","7e16d39a":"We will specifically work with the lter file as there are more features to analyze and provides more features to select from. We see that there are some null values that will have to be filled in. The data cleaning will be performed below.","c73c036f":"## Feature Selection","2b98175b":"![image.png](attachment:image.png)","a1b3d6f0":"Again we see a general separation using the isotopic ratios for both N and C. Below we can summarize the general findings of our results by the pairplot.","f0ddaa05":"# Next Steps\n* So far a model was build to specifically classify penguins by species. \n* Now that model is established and shown to be highly accurate, I want to build submodels now to classify individuals within species by specific Sex. \n* Through EDA we observed that there is a minor difference between males and females when observed body mass (kg).\n* Through Feature Selection it was determined that Sex is not a good predictor for species, so maybe it can be turned backaround and be used as a classifier.\n* A limitation that I see to using this as classifier however, was that we had to clean the Sex column and input some categorical values by mode and it may not be as accurate or correct to model this way.\n\n<b>Suggestions?\n\nIf you stuck around to the end please leave a comment for feedback or upvote!<\/b>\n\nLike what I have done? Check out my other notebooks here: https:\/\/www.kaggle.com\/christopherwsmith","ba931aca":"As seen above, One Hot Encoding creates a dummy variable scheme that allows for a yes no approach in multiple columns. The data is expanded and can now be further preprocessed by scaling. First, Island will be dropped as it is no longer needed.","87d396c4":"## Modeling","104db545":"We have some separation of species when observing the culmen length and the culmen depth, where the size ranges are more reserved to specific species. Do we see this same kind of separation for flipper length as well?","cf144032":"By species, there is a positive trend towards average flipper length by species. Similarly, we would expect that a bigger size flipper may indicate that P.papua will weigh more than the other two species."}}