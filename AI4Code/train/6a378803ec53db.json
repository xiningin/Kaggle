{"cell_type":{"49fe9ef4":"code","f34ccded":"code","048620f4":"code","1b7da3a6":"code","de98e3d3":"code","c005d3b8":"code","fcf519aa":"code","2001cd49":"code","47ae9fb0":"code","c98b65be":"code","1565d2a3":"code","10e707fa":"code","3a7e4363":"code","26c0c655":"code","16723d38":"code","d7eb5151":"code","a2825469":"code","27ad8088":"code","47443bdf":"code","f0419c5d":"code","1735eee3":"code","16652236":"code","8740404c":"code","38b96bc0":"code","218a20e5":"code","32ba911b":"code","610b3bcd":"code","942b0dc3":"code","db19bab3":"code","135133ec":"code","afe52281":"code","6374bc95":"code","73161493":"code","be26aefc":"code","5897b49b":"code","beb2fb82":"code","8cb5b8b6":"code","7b725075":"code","7f69b641":"code","c93574b1":"code","622a4b17":"code","1d7131c6":"code","739e7b5a":"code","8aaf6e88":"code","efc1256e":"code","c43d355e":"code","7c61f99c":"code","3b64914f":"code","e2c1d636":"code","331221d6":"code","bc9a27d3":"code","259afc26":"code","3ce89bb9":"code","86621d80":"code","3a6519c3":"code","07f475a5":"code","47af72d5":"code","5168ba48":"code","4e2f5d5d":"code","ceef53b6":"markdown","69f17dcd":"markdown","1a606b1e":"markdown","efbddb50":"markdown","93076605":"markdown","51ae4695":"markdown","b985c003":"markdown","49068f78":"markdown","4b01be67":"markdown","be37d9da":"markdown","e0b1b325":"markdown","42be091d":"markdown","3162c9c1":"markdown","c996e931":"markdown","00448632":"markdown","4c4c63a6":"markdown","10114281":"markdown","4ab1663d":"markdown","9a042761":"markdown","1e24cea0":"markdown","52409103":"markdown","95349831":"markdown","c2dac995":"markdown","883f1e34":"markdown","0c053a04":"markdown","75ffc01b":"markdown","9689b6eb":"markdown"},"source":{"49fe9ef4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f34ccded":"test_df = pd.read_csv('\/kaggle\/input\/multilabel-test\/test.csv')","048620f4":"'''\n#Reference:- https:\/\/www.geeksforgeeks.org\/working-zip-files-python\/\nfrom zipfile import ZipFile\nwith ZipFile('\/kaggle\/input\/case-study-1\/Case_Study_1.zip', 'r') as zip:\n  zip.printdir()\n  zip.extractall()\n  print('Done!')\n'''","1b7da3a6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","de98e3d3":"df_train_og = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Ogling\/train.csv')\ndf_test_og = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Ogling\/test.csv')\ndf_valid_og = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Ogling\/dev.csv')","c005d3b8":"def append_len(data):#function to length of description field\n  s = []\n  for i in data['Description']:\n      s.append(len(i))\n  return s","fcf519aa":"df_train_og.head(5)","2001cd49":"print('\\n Length of \\n train dataset is {}, \\n test dataset is {}, \\n validation data is {}'.format(len(df_train_og),len(df_test_og),len(df_valid_og)))","47ae9fb0":"def calculate_len(data):#function to calculate average sentence length of description field\n  m = 0\n  for i in data['Description']:\n      m += len(i)\n  return (m\/len(data))    ","c98b65be":"print('\\n Average Length of Description \\n train dataset is {}, \\n test dataset is {}, \\n validation data is {}'.format(calculate_len(df_train_og),calculate_len(df_test_og),calculate_len(df_valid_og)))","1565d2a3":"from matplotlib.pyplot import figure\nfigure(figsize=(8, 6), dpi=80)\nplt.subplot(311)\nax1 = sns.countplot(x=\"Category\", data=df_train_og)\nax1.set_title('Train Dataset')\nplt.subplot(312)\nax2 = sns.countplot(x=\"Category\", data=df_test_og)\nax2.set_title('Test Dataset')\nplt.subplot(313)\nax3 = sns.countplot(x=\"Category\", data=df_valid_og)\nax3.set_title('Valid Dataset')","10e707fa":"def chk_duplicate(data):\n  df = data[data.duplicated()]\n  return df,len(df)\ntrain_duplicate,count = chk_duplicate(df_train_og)\nprint('Number of duplicate rows',count)\nprint(train_duplicate)","3a7e4363":"test_duplicate,count = chk_duplicate(df_test_og)\nprint('Number of duplicate rows',count)\nprint(test_duplicate)","26c0c655":"valid_duplicate,count = chk_duplicate(df_valid_og)\nprint('Number of duplicate rows',count)\nprint(valid_duplicate)","16723d38":"df_train_co = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Commenting\/train.csv')\ndf_test_co = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Commenting\/test.csv')\ndf_valid_co = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Commenting\/dev.csv')","d7eb5151":"print('\\n Average Length of Description \\n train dataset is {}, \\n test dataset is {}, \\n validation data is {}'.format(calculate_len(df_train_co),calculate_len(df_test_co),calculate_len(df_valid_co)))","a2825469":"figure(figsize=(8, 6), dpi=80)\nplt.subplot(311)\nax1 = sns.countplot(x=\"Category\", data=df_train_co)\nax1.set_title('Train Dataset')\nplt.subplot(312)\nax2 = sns.countplot(x=\"Category\", data=df_test_co)\nax2.set_title('Test Dataset')\nplt.subplot(313)\nax3 = sns.countplot(x=\"Category\", data=df_valid_co)\nax3.set_title('Valid Dataset')","27ad8088":"train_duplicate_co,count = chk_duplicate(df_train_co)\nprint('Number of duplicate rows',count)\nprint(train_duplicate_co)\ntest_duplicate_co,count = chk_duplicate(df_test_co)\nprint('Number of duplicate rows',count)\nprint(test_duplicate_co)\nvalid_duplicate_co,count = chk_duplicate(df_valid_co)\nprint('Number of duplicate rows',count)\nprint(valid_duplicate_co)","47443bdf":"df_train_go = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Groping\/train.csv')\ndf_test_go = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Groping\/test.csv')\ndf_valid_go = pd.read_csv('\/kaggle\/input\/case-study-1\/Case_Study_1\/Groping\/dev.csv')","f0419c5d":"figure(figsize=(8, 6), dpi=80)\nplt.subplot(311)\nax1 = sns.countplot(x=\"Category\", data=df_train_go)\nax1.set_title('Train Dataset')\nplt.subplot(312)\nax2 = sns.countplot(x=\"Category\", data=df_test_go)\nax2.set_title('Test Dataset')\nplt.subplot(313)\nax3 = sns.countplot(x=\"Category\", data=df_valid_go)\nax3.set_title('Valid Dataset')","1735eee3":"train_duplicate_go,count = chk_duplicate(df_train_go)\nprint('Number of duplicate rows',count)\nprint(train_duplicate_go)\ntest_duplicate_go,count = chk_duplicate(df_test_go)\nprint('Number of duplicate rows',count)\nprint(test_duplicate_go)\nvalid_duplicate_go,count = chk_duplicate(df_valid_go)\nprint('Number of duplicate rows',count)\nprint(valid_duplicate_go)","16652236":"'''\nfrom zipfile import ZipFile\nwith ZipFile('train.zip', 'r') as zip:\n  zip.printdir()\n  zip.extractall()\n  print('Done!')\n'''","8740404c":"train_df = pd.read_csv('\/kaggle\/input\/train-csv\/train.csv')","38b96bc0":"train_df.head(5)","218a20e5":"\ntrain_df = train_df.rename(columns={'Ogling\/Facial Expressions\/Staring':'Ogling'})\ntrain_df = train_df.rename(columns={'Touching \/Groping':'Groping'})\n","32ba911b":"train_df.head(5)","610b3bcd":"\nfigure(figsize=(8, 6), dpi=80)\nplt.subplot(311)\nax1 = sns.countplot(x=\"Commenting\", data=train_df)\nax1.set_title('Commenting')\nplt.subplot(312)\nax2 = sns.countplot(x=\"Ogling\", data=train_df)\nax2.set_title('Ogling')\nplt.subplot(313)\nax3 = sns.countplot(x=\"Groping\", data=train_df)\nax3.set_title('Groping')\n","942b0dc3":"df_train_og.drop_duplicates(subset=\"Description\",keep= False, inplace = True)\ndf_train_go.drop_duplicates(subset=\"Description\",keep= False, inplace = True)\ndf_train_co.drop_duplicates(subset=\"Description\",keep= False, inplace = True)\ntrain_df.drop_duplicates(subset=\"Description\",keep= False, inplace = True)","db19bab3":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(data):\n    stop = stopwords.words('english')\n    res = []\n    data['Description'] = data['Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    for x in data['Description']:\n        x = str(x)\n        for punct in puncts:\n            if punct in x:\n                    x = x.replace(punct,' ')\n        res.append(x)\n    return res","135133ec":"from nltk.corpus import stopwords\n\ndf_train_og['Description'] = clean_text(df_train_og)\ndf_test_og['Description'] = clean_text(df_test_og)\ndf_train_go['Description'] = clean_text(df_train_go)\ndf_test_go['Description'] = clean_text(df_test_go)\ndf_train_co['Description'] = clean_text(df_train_co)\ndf_test_co['Description'] = clean_text(df_test_co)\ntrain_df['Description'] = clean_text(train_df)\n","afe52281":"def preprocess(data):\n  res = []\n  for i in data['Description']:\n    k = i.lower()\n    res.append(k)\n  return res","6374bc95":"df_train_og['lower'] = preprocess(df_train_og)\ndf_test_og['lower'] = preprocess(df_test_og)\n","73161493":"df_train_og = df_train_og.drop(['Description'],axis=1)\ndf_test_og = df_test_og.drop(['Description'],axis=1)","be26aefc":"df_train_og","5897b49b":"df_test_og","beb2fb82":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom imblearn.over_sampling import SMOTE\nfrom scipy.sparse import hstack\nsm_model = SMOTE(sampling_strategy='minority')\n\ndef Logistic_pipeline(data,test):\n    X = data['lower']\n    y = data['Category']\n    X_test = test['lower']\n    y_test = test['Category']\n    #X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n    \n    vect = TfidfVectorizer(ngram_range=(1,3),min_df=15,max_features=500,stop_words='english')\n    X = vect.fit_transform(X.values)\n    X_test = vect.transform(X_test)\n    #X,y = sm_model.fit_resample(X, y)\n    clf = LogisticRegression(random_state=0).fit(X, y)\n    ypred = clf.predict(X_test)\n    score = roc_auc_score(ypred,y_test)\n    print(score)\n    accuracy = accuracy_score(y_test, ypred)\n    print(accuracy)","8cb5b8b6":"Logistic_pipeline(df_train_og,df_test_og)","7b725075":"#!pip install xgboost\nfrom xgboost import XGBClassifier\ndef XGBoost_pipeline(data,test):\n    X = data['lower']\n    y = data['Category']\n    X_test = test['lower']\n    y_test = test['Category']\n    #X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n    \n    vect = TfidfVectorizer(ngram_range=(1,3),min_df=15,max_features=500,stop_words='english')\n    X = vect.fit_transform(X)\n    X_test = vect.transform(X_test)\n    \n    model = XGBClassifier(random_state=42,enable_categorical=True)\n    model.fit(X, y)\n    ypred = model.predict(X_test)\n    score = roc_auc_score(ypred,y_test)\n    print(score)\n    accuracy = accuracy_score(y_test, ypred)\n    print(accuracy)","7f69b641":"XGBoost_pipeline(df_train_og,df_test_og)","c93574b1":"df_train_go['lower'] = preprocess(df_train_go)\ndf_train_go.drop(['Description'],axis=1)\ndf_test_go['lower'] = preprocess(df_test_go)\ndf_test_go.drop(['Description'],axis=1)","622a4b17":"Logistic_pipeline(df_train_go,df_test_go)","1d7131c6":"XGBoost_pipeline(df_train_go,df_test_go)","739e7b5a":"df_train_co['lower'] = preprocess(df_train_co)\ndf_train_co.drop(['Description'],axis=1)\ndf_test_co['lower'] = preprocess(df_test_co)\ndf_test_co.drop(['Description'],axis=1)","8aaf6e88":"print('Logistic regression ROC and accuracy scores are')\nLogistic_pipeline(df_train_co,df_test_co)\nprint('XGBoost ROC and accuracy scores are')\nXGBoost_pipeline(df_train_co,df_test_co)","efc1256e":"train_df","c43d355e":"train_df['lower'] = preprocess(train_df)\ntest_df['lower'] = preprocess(test_df)\ntrain_df.drop(['Description'],axis=1)\ntest_df.drop(['Description'],axis=1)\nX = train_df['lower']\ny = train_df[['Commenting','Ogling','Groping']]\nfrom sklearn.model_selection import train_test_split\n#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(ngram_range=(1,3),min_df=15,max_features=500,stop_words='english')\nX_train = vect.fit_transform(X)\nX_test = vect.transform(test_df['lower'])","7c61f99c":"#X_train = X_train[1:1000]\n#y_train = y_train[1:1000]","3b64914f":"from skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = BinaryRelevance(classifier=SVC())\nclassifier.fit(X_train,y)\npredictions = classifier.predict(X_test)\nclf = BinaryRelevance(classifier=RandomForestClassifier())\nclf.fit(X_train,y)\npredictions_rf = clf.predict(X_test)\nmodel = BinaryRelevance(classifier=XGBClassifier())\nmodel.fit(X_train,y)\npredictions_xgb = model.predict(X_test)","e2c1d636":"y_test = test_df[['Commenting','Ogling\/Facial Expressions\/Staring','Touching \/Groping']]\nscore = roc_auc_score(predictions.toarray(),y_test)\nprint(score)\naccuracy = accuracy_score(y_test, predictions.toarray())\nprint(accuracy)\nprint('---------------------RF-----------------')\nscore = roc_auc_score(predictions_rf.toarray(),y_test)\nprint(score)\naccuracy = accuracy_score(y_test, predictions_rf.toarray())\nprint(accuracy)\nprint('---------------------XGB-----------------')\nscore = roc_auc_score(predictions_xgb.toarray(),y_test)\nprint(score)\naccuracy = accuracy_score(y_test, predictions_xgb.toarray())\nprint(accuracy)\n","331221d6":"from sklearn.metrics import hamming_loss\nham = hamming_loss(y_test, predictions.toarray())\nprint('The hamming loss is:{}'.format(ham))\nprint('The hamming score is: ',1-(ham))\nprint('---------------------XGB-----------------')\nham = hamming_loss(y_test, predictions_xgb.toarray())\nprint('The hamming loss is:{}'.format(ham))\nprint('The hamming score is: ',1-(ham))","bc9a27d3":"import tensorflow_hub as hub\nimport lightgbm as lgb\n\ndef generate_embeddings(X_train,X_test,y_train,y_test):\n    def embed_document(data):\n        model = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")\n        embeddings = np.array([np.array(model([i])) for i in data])\n        return pd.DataFrame(np.vstack(embeddings))\n    # vectorize the data\n    X_train_vec = embed_document(X_train)\n    X_test_vec = embed_document(X_test)\n    # USE doesn't have feature names\n    model = XGBClassifier(n_estimators=1000,learning_rate=0.001,max_depth=5,n_jobs=8)\n    model.fit(X_train_vec, y_train)\n    model.score(X_test_vec, y_test)\n    ypred = model.predict(X_test_vec)\n    print('XGBoost scores')\n    score = roc_auc_score(ypred,y_test)\n    print(score)\n    accuracy = accuracy_score(y_test, ypred)\n    print(accuracy)\n    print(\"-------------------------------------------------------------------\")\n    print('LightGBM scores:')\n    clf = lgb.LGBMClassifier()\n    clf.fit(X_train_vec, y_train)\n    clf.score(X_test_vec, y_test)\n    ypred = clf.predict(X_test_vec)\n    score = roc_auc_score(ypred,y_test)\n    print(score)\n    accuracy = accuracy_score(y_test, ypred)\n    print(accuracy)\n    ","259afc26":"y_train = df_train_og['Category']\ny_test = df_test_og['Category']\nX_train = df_train_og['lower']\nX_test = df_test_og['lower']\ngenerate_embeddings(X_train,X_test,y_train,y_test)\ny_train = df_train_go['Category']\ny_test = df_test_go['Category']\nX_train = df_train_go['lower']\nX_test = df_test_go['lower']\ngenerate_embeddings(X_train,X_test,y_train,y_test)\ny_train = df_train_co['Category']\ny_test = df_test_co['Category']\nX_train = df_train_co['lower']\nX_test = df_test_co['lower']\ngenerate_embeddings(X_train,X_test,y_train,y_test)","3ce89bb9":"'''\nfrom optuna.integration import LightGBMPruningCallback\nfrom sklearn.metrics import log_loss\ndef objective(trial, X, y):\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"alpha\": trial.suggest_int(\"alpha\", 0, 100, step=5),\n        \"lambda\": trial.suggest_int(\"lambda\", 0, 100, step=5),\n        \n        \n    }\n    def embed_document(data):\n        model = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")\n        embeddings = np.array([np.array(model([i])) for i in data])\n        return pd.DataFrame(np.vstack(embeddings))\n    # vectorize the data\n    X = embed_document(X)\n    \n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = XGBClassifier(**param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"auc\",\n            early_stopping_rounds=100,\n        )\n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n\n    return np.mean(cv_scores)\n'''","86621d80":"'''\n!pip install optuna\nimport optuna\nimport lightgbm as lgbm\nfrom sklearn.model_selection import StratifiedKFold\nstudy = optuna.create_study(direction=\"minimize\", study_name=\"XGBClassifier\")\nfunc = lambda trial: objective(trial, df_train_og['lower'],df_train_og['Category'].to_numpy())\nstudy.optimize(func, n_trials=20)\n'''","3a6519c3":"'''\nprint(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")\n'''","07f475a5":"\n'''\n!pip install sparknlp\n!pip install pyspark\nimport sparknlp\n\nspark = sparknlp.start(gpu = True) # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom pyspark.ml import Pipeline\nimport pandas as pd\n\nprint(\"Spark NLP version\", sparknlp.version())\n\nprint(\"Apache Spark version:\", spark.version)\n\nspark\n'''","47af72d5":"'''\ndocument_assembler = DocumentAssembler() \\\n.setInputCol(\"lower\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n\nbert_embeddings = BertEmbeddings().pretrained(name='bert_base_cased', lang='en') \\\n.setInputCols([\"document\",'token'])\\\n.setOutputCol(\"embeddings\")\n\nembeddingsSentence = SentenceEmbeddings() \\\n.setInputCols([\"document\", \"embeddings\"]) \\\n.setOutputCol(\"sentence_embeddings\") \\\n.setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n.setInputCols([\"sentence_embeddings\"])\\\n.setOutputCol(\"class\")\\\n.setLabelColumn(\"Category\")\\\n.setMaxEpochs(5)\\\n.setLr(0.001)\\\n.setBatchSize(8) \\\n.setEnableOutputLogs(True)\n#.setOutputLogsPath('logs')\n\nbert_clf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner,\n            lemma,\n            bert_embeddings,\n            embeddingsSentence,\n            classsifierdl])\n'''","5168ba48":"'''\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\nfrom pyspark.sql.types import DoubleType\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext(sc)\n\ndf_train_og = sqlContext.createDataFrame(df_train_og)\n\ndf_train_og = df_train_og.withColumn(\"Category\", df_train_og.Category.cast(DoubleType()))\nbert_clf_pipelineModel = bert_clf_pipeline.fit(df_train_og)\ndf_test_og = sqlContext.createDataFrame(df_test_og)\npreds = bert_clf_pipelineModel.transform(df_test_og)\n'''","4e2f5d5d":"#preds.select('lower','Category',\"class.result\").show(20, truncate=80)","ceef53b6":"**Ogling Dataset**","69f17dcd":"**Removing punctuation symbols and  special characters**","1a606b1e":"**Logistic regression(without hyperparameter optimization) giving an ROC of 0.88 and accuracy of 81% for a sample of 1000 datapoints as training the entire dataset was causing the RAM in Google Colab to crash**","efbddb50":"**Logistic regression(without hyperparameter optimization) giving an ROC of 0.78 and accuracy of 76.35% for a sample of 1000 datapoints as training the entire dataset was causing the RAM in Google Colab to crash**","93076605":"In the multilabel dataset, the number of incidents of occurence are more commenting followed by groping followed by ogling ","51ae4695":"Imbalanced dataset for Groping data as well","b985c003":"Paper Reference:- @inproceedings{karlekar2018safecity,\n\tauthor = {Karlekar, Sweta and Bansal, Mohit},\n\ttitle = {SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories},\n\tbooktitle = {EMNLP},\n\tyear = {2018},\n}\n\nData Reference:- https:\/\/github.com\/swkarlekar\/safecity","49068f78":"We see similar instances of duplication with different categories assigned in the test and validation datasets as well.\n\nSome descriptions also are not very informative like:-\n\n1. it was really bad.\n\n2. This survey is carried out by safecity (Red Do...\n\n3. misbehaved\n\n4. harassment","4b01be67":"**The data is imbalanced in all three datasets we have descriptions but it is definitely better balanced than the ogling dataset.**","be37d9da":"**Groping Dataset**","e0b1b325":"**The data is highly imbalanced in all three datasets we have descriptions which are not actual incidents way more than the ones that are ogling harassment incident description.**","42be091d":"Convert all the characters into small letters\n\n","3162c9c1":"We not only have duplicate data but also the category for these datasets is different at different occurences.\n\nFor example:- at index 139:- harassment has category 0\n\nand at 288 :- harassment has category 1\n\nThis looks like erroneous data which would likely needed to be removed. ","c996e931":"We see similar instances of duplication with different categories assigned in the train, test and validation datasets of the commenting data as well.","00448632":"**Conclusion**\n\nAs part of the step of EDA in this case study we looked into description and category fields of the single and multilabel datasets and our findings are as below:-\n\n1) Data is highly imbalanced in most of the cases \n\n2) Duplicate data is present for description with difference in categories for different occurences in the single label datasets","4c4c63a6":"***Ogling***","10114281":"***Groping***","4ab1663d":"**EDA for SIngle label Case Study**","9a042761":"**Multi label classification**\n","1e24cea0":"**PreProcessing of description feature**","52409103":"**Baseline Logistic Regression model**","95349831":"**Dropping duplicate description rows**","c2dac995":"***Commenting***","883f1e34":"We see similar instances of duplication with different categories assigned in the train, test and validation datasets of the commenting data as well.\n\nSome descriptions also are not very informative like:-\n\nit was really bad.\n\nThis survey is carried out by safecity (Red Do...\n\nmisbehaved\n\nharassment","0c053a04":"**Logistic regression(without hyperparameter optimization) giving an ROC of 0.857 and accuracy of 81.58% for a sample of 1000 datapoints as training the entire dataset was causing the RAM in Google Colab to crash**","75ffc01b":"**Commenting Dataset**","9689b6eb":"**EDA for Multi label Case Study**"}}