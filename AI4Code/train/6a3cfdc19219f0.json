{"cell_type":{"f8bc6955":"code","9b1168de":"code","37b26a19":"code","52460c7a":"code","18863b20":"code","f89ddab8":"code","e4ace555":"code","f4b893d8":"code","b28445ea":"code","332e491e":"code","78ffa027":"code","e7962a3c":"code","4f149fa0":"code","09872dce":"code","b0f5a284":"code","f496028f":"code","7cd90e1c":"code","edcc5b4d":"code","70ad8706":"code","2577c863":"code","e1bc94b1":"code","b2738c75":"code","ab2a7d89":"code","fe70dded":"code","bdd5a927":"code","59658ca5":"markdown"},"source":{"f8bc6955":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b1168de":"import tensorflow as tf\nfrom tensorflow.python.framework import ops\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()","37b26a19":"import matplotlib.pyplot as plt\n%matplotlib inline","52460c7a":"df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv', index_col=0)\ndf.head()","18863b20":"df[['cat'+str(i) for i in range(19)]] = df[['cat'+str(i) for i in range(19)]].astype('category')\ndf.info()","f89ddab8":"df_1 = df.drop('cat10', axis=1)","e4ace555":"df_2 = pd.get_dummies(df_1, columns=['cat'+str(i) for i in range(19) if i != 10], drop_first=True)\ndf_2.head()","f4b893d8":"X = df_2.drop('target', axis=1)\ny = df_2['target']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","b28445ea":"X_train.shape","332e491e":"y_train.shape","78ffa027":"X_test.shape","e7962a3c":"X_train = np.array(X_train.T)\nX_test = np.array(X_test.T)\ny_train = np.array(y_train.T).reshape((1, y_train.shape[0]))\ny_test = np.array(y_test.T).reshape((1, y_test.shape[0]))","4f149fa0":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","09872dce":"y_train.shape[0] ","b0f5a284":"def random_mini_batches(X, Y, minibatch_size, seed):\n    np.random.seed(seed)\n    indices = np.arange(0, X.shape[1])\n    np.random.shuffle(indices)\n    minibatches = []\n    num_of_minibatches = int(X.shape[1] \/ minibatch_size)\n    for i in range(num_of_minibatches):\n        X_mini = X[:, indices[i*minibatch_size : minibatch_size*(i+1)]]\n        Y_mini = Y[:, indices[i*minibatch_size : minibatch_size*(i+1)]]\n        minibatches.append((X_mini, Y_mini))\n    return minibatches","f496028f":"minibatches = random_mini_batches(X_train, y_train, 1000, 1)\nprint(len(minibatches))\nprint(minibatches[0][0].shape, minibatches[0][1].shape)","7cd90e1c":"def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n          num_epochs = 15, minibatch_size = 1000, print_cost = True):\n    \"\"\"\n    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X_train -- training set, of shape (input size = 317, number of training examples = 210000)\n    Y_train -- test set, of shape (output size = 1, number of training examples = 210000)\n    X_test -- training set, of shape (input size = 317, number of training examples = 90000)\n    Y_test -- test set, of shape (output size = 1, number of test examples = 90000)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    cost_per_iteration = []                           # \u0424-\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438\n    \n    # Create Placeholders of shape (n_x, n_y)\n    ### START CODE HERE ### (1 line)\n    X = tf.placeholder(tf.float32, [n_x, None])\n    Y = tf.placeholder(tf.float32, [n_y, None])\n    ### END CODE HERE ###\n\n    # Initialize parameters\n    ### START CODE HERE ### (1 line)\n    W1 = tf.get_variable(\"W1\", [25,317], initializer = tf.glorot_uniform_initializer())\n    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.glorot_uniform_initializer())\n    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\", [1,12], initializer = tf.glorot_uniform_initializer())\n    b3 = tf.get_variable(\"b3\", [1,1], initializer = tf.zeros_initializer())\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    ### END CODE HERE ###\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    ### START CODE HERE ### (1 line)\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n    Z1 = tf.add(tf.matmul(W1, X), b1)                                # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                               # Z2 = np.dot(W2, A1) + b2\n    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)   \n    ### END CODE HERE ###\n    \n    # Cost function: Add cost function to tensorflow graph\n    ### START CODE HERE ### (1 line)\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    # print('logits:', logits, 'labels:', labels)\n    \n    #!!!!!!!!!!!!!!!!!!\u041f\u0435\u0440\u0435\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0430 one_hot\n    cost = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(labels, logits))\n    ### END CODE HERE ###\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n    ### START CODE HERE ### (1 line)\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n    ### END CODE HERE ###\n    \n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            num_minibatches = int(m \/ minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                ### END CODE HERE ###\n                \n                cost_per_iteration.append(minibatch_cost \/ minibatch_size)\n                \n                epoch_cost += minibatch_cost \/ minibatch_size\n\n            # Print the cost every epoch\n            if print_cost == True: #and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            #if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('epochs')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n        \n        # plot the cost per iteration\n        plt.plot(np.squeeze(cost_per_iteration)[:420])\n        plt.ylabel('cost_per_iteration')\n        plt.xlabel('iterations')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.math.round(Z3), Y)\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n        \n        # \u041f\u043e\u043f\u044b\u0442\u043a\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c ROC-AUC\n#         m = tf.keras.metrics.AUC()\n#         m.update_state(Y_test, Z3)\n#         print(\"ROC-AUC:\", m.result())\n        \n        return parameters","edcc5b4d":"parameters = model(X_train, y_train, X_test, y_test)","70ad8706":"from keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nimport pydot\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model","2577c863":"def model_keras(input_shape):\n    \"\"\"\n    input_shape: The height, width and channels as a tuple.  \n        Note that this does not include the 'batch' as a dimension.\n        If you have a batch like 'X_train', \n        then you can provide the input_shape using\n        X_train.shape[1:]\n    \"\"\"\n \n    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n    X_input = Input(input_shape)\n \n    # Hidden layer 1\n    X = Dense(64, activation='relu', name='layer1')(X_input)\n    \n    # Hidden layer 2\n    X = Dense(32, activation='relu', name='layer2')(X)\n \n    # Output layer\n    X = Dense(1, activation='sigmoid', name='output')(X)\n \n    # Create model. This creates your Keras model instance, you'll use this instance to train\/test the model.\n    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n \n    return model","e1bc94b1":"X_train.shape","b2738c75":"happyModel = model_keras(X_train.T.shape[1:])\nhappyModel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\nhappyModel.fit(x = X_train.T, y = y_train.T, epochs = 15, batch_size = 1000)","ab2a7d89":"preds = happyModel.evaluate(x = X_test.T, y = y_test.T)\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","fe70dded":"happyModel.summary()","bdd5a927":"plot_model(happyModel, to_file='HappyModel.png')\nSVG(model_to_dot(happyModel).create(prog='dot', format='svg'))","59658ca5":"# \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 Keras"}}