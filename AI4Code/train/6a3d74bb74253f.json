{"cell_type":{"2dfad3b1":"code","6193134b":"code","09653fd5":"code","a42d3f4e":"code","884ec3e4":"code","de5e95d7":"code","c913fcc8":"code","f877bc0f":"code","532f36d5":"code","b6b92bc3":"code","7b01dbc4":"code","82df3838":"code","9c391d4f":"code","9ee758e0":"code","0ba6cdcb":"markdown","9eae34d7":"markdown","caf1771f":"markdown","769d3ebc":"markdown"},"source":{"2dfad3b1":"import torch\nfrom torch import nn\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata_path = '\/kaggle\/input\/digit-recognizer\/'","6193134b":"# Let's start by reading the data from csv files\ntrain_data = pd.read_csv(os.path.join(data_path, 'train.csv'))\n\ntest_data = pd.read_csv(os.path.join(data_path, 'test.csv'))\ntrain_data.head()","09653fd5":"# I'll split train data to construct train and validation splits\n# convert data frames to numpy arrays\nX_train, Y_train = train_data.iloc[:,1:].values, train_data.iloc[:,0].values\n\n# split the dataset\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = .25)\nx_test = test_data.values # we don't have targets in the test data\n\n# plot few samples from the training data \nfig, axs = plt.subplots(1, 3)\nidxs = np.random.randint(0, x_train.shape[0], 3)\nfor i, idx in enumerate(idxs):\n    axs[i].set_title(y_train[idx])\n    axs[i].imshow(x_train[idx].reshape(28, 28), cmap = 'gray')\nplt.show()","a42d3f4e":"# we will use these constants later on\nBATCH_SIZE = 256\nLR = 0.001\nN_EPOCHS = 50\nN_CLASSES = len(set(y_train)) # 10","884ec3e4":"# standardizing the data\n\ntrain_mean = x_train.mean()\ntrain_std = x_train.std()\n\n# we will standardize the validation and test set using \n# the mean and standard deviation of training set\n# otherwise, we would leak information about train set to validation and test sets \n\nx_train = (x_train - train_mean) \/ train_std\nx_val = (x_val - train_mean) \/ train_std\nx_test = (x_test - train_mean) \/ train_std\n# convert everything to torch tensors\nx_train = torch.from_numpy(x_train).type(torch.float32)\nx_val = torch.from_numpy(x_val).type(torch.float32)\nx_test = torch.from_numpy(x_test).type(torch.float32)\ny_train = torch.from_numpy(y_train)\ny_val = torch.from_numpy(y_val)\n\n# build the datasets and dataloaders\ntrain_ds = TensorDataset(x_train, y_train)\nval_ds = TensorDataset(x_val, y_val)\n# \ntrain_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\nval_loader = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = False)","de5e95d7":"class LinearModel(nn.Module):\n    \"\"\" Basic linear model with hidden layers \"\"\"\n    def __init__(self, n_input: int = 784, hidden_nodes = [128], n_output: int = N_CLASSES):\n        super().__init__()\n        if type(hidden_nodes) == int:\n            hidden_nodes = [hidden_nodes]\n        # concatenate the number of inputs, outputs and all the hidden nodes list         \n        # to automatically build a linear model using ModuleList\n        num_nodes = [n_input] + hidden_nodes + [n_output]\n\n        \n        # let's start\n        # we can group sequential Linear->ReLU models together to build the model automatically\n        module_list = nn.ModuleList([\n            nn.Sequential(\n            nn.Linear(num_nodes[i], num_nodes[i+1]), nn.ReLU()\n            )\n            for i in range(len(num_nodes)-2)\n        ]) # the last layer will have num_nodes[-3] input nodes and num_nodes[-2] output nodes\n        \n        \n        \n        self.linear = nn.Sequential(*module_list) # pass the modules in the module list to Sequential model\n        # since the last layer in `linear` will have number of num_nodes[-2] output nodes,\n        # we will set the input nodes `num_nodes[-2]` in the classifier \n        # we also concatenated `n_output` with `num_nodes` and \n        # last element of `num_nodes` is `n_output`\n        # so we will set the number of output nodes as `n_output` by passing `num_nodes[-1]`\n        self.classifier = nn.Linear(num_nodes[-2], num_nodes[-1])\n        \n    def configure(self, optimizer, loss_fn):\n        \"\"\" Simple function to set the optimizer and loss function \"\"\"\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n        \n    def forward(self, x):\n        x = self.linear(x) # output shape: Nx(n_hidden)\n        outputs = self.classifier(x) # output shape: Nx(n_output)\n        return outputs\n        \n    def train_step(self, x_batch, y_batch):\n        # feed data to the network\n        out = self.forward(x_batch)\n        # calculate the losses\n        loss = self.loss_fn(out, y_batch)\n        # backward propagation\n        loss.backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad() # we zero out the gradients when we're done, they will accumulate if we don't.\n        # finally return the loss\n        return loss.item()\n    \n    def val_step(self, x_batch, y_batch):\n        with torch.no_grad():\n            out = self.forward(x_batch)\n            loss = self.loss_fn(out, y_batch)\n        return loss.item()\n\n    def predict(self, x):\n        \"\"\" Function to make predictions, just returns the index of the maximums in model outputs \"\"\"\n        model_outputs = self.forward(x)\n        preds = model_outputs.argmax(1)\n        return preds","c913fcc8":"model = LinearModel(hidden_nodes = [256,256])\nprint(model)\ndel model","f877bc0f":"def train(model, train_loader, validation_loader, n_epochs = 20):\n    train_losses = np.zeros(n_epochs)\n    val_losses = np.zeros(n_epochs)\n    for i in range(n_epochs):\n        # define the running losses for the epoch\n        train_batch_loss = 0\n        val_batch_loss = 0\n        \n        for x, y in train_loader:\n            loss = model.train_step(x, y)\n            # CrossEntropyLoss is reducing the calculated loss by mean\n            # So every single time we add up the average loss by batches\n            # we also need to divide it by number of batches because of this\n            train_batch_loss += loss\n        \n        model.eval()\n        for x, y in val_loader:\n            val_loss = model.val_step(x, y)\n            val_batch_loss += val_loss \n        model.train()\n        \n        # divide it by the number of the batches per dataset phase\n        # to get the average losses\n        train_losses[i] = train_batch_loss \/ len(train_loader)\n        val_losses[i] = val_batch_loss \/ len(validation_loader)\n        \n        if (i+1)%10==0: # we will log the average losses per epoch\n            print(f\"Epoch {i+1}\/{n_epochs} | Avg. training loss: {train_losses[i]:.4f} | Avg. validation loss: {val_losses[i]:.4f}\")\n    return train_losses, val_losses","532f36d5":"# we can finally define the model and train it\nmodel = LinearModel(hidden_nodes = [256, 256])\nprint(model)\n# define the loss function and optimizer, then pass them to `configure` method of the model\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = LR)\nmodel.configure(optimizer, loss_fn)\n\n\ntrain_losses, val_losses = train(model, train_loader, val_loader, n_epochs = N_EPOCHS)\n\nplt.title('Losses')\nplt.plot(train_losses, label = 'training loss')\nplt.plot(val_losses, label = 'validation loss')\nplt.legend()\nplt.show()","b6b92bc3":"# I'm using this utility function to calculate \n# the number of columns of activation matrices\n# in the `plot_activations`\ndef mid_multiplier(num):\n    \"\"\"\n    Constructs the list of multipliers of given number\n    Then returns the multiplier at the middle    \n    It looks like a bit complex but I just filter numbers \n    from starting 1 to given number, then return the number\n    at the middle\n    \"\"\"\n    f = list(filter(lambda x: num%x==0, range(1, num)))\n    L = len(f)\n    return f[L\/\/2]\n    \ndef plot_activations(model: nn.Module, data: np.ndarray) -> None:\n    \"\"\"\n    Plots the given model's activations layer by layer\n    \n    params:\n        model - linear torch model (with the same structure as defined above)\n        data - numpy array that contains the samples\n    \n    It simply processes the given data iteratively while\n    plotting the activations as heatmaps\n    \n    I've set plotting parameters according to my use case,\n    you may need to change it accordingly before using it.\n    \n    \n    \"\"\"\n    model.eval()\n    with torch.no_grad(): \n        # we will first process the sample like we did before training\n        # so the samples should come from the first data frame as a numpy array\n        \n        # process the data\n        x = (data - train_mean) \/ train_std\n        # we need to pass it to the model as a batch tensor\n        x = torch.from_numpy(x.astype('float32')) # x.shape: 10x784\n        \n        num_layers = len(model.linear)\n        # plus 1 more row for the samples\n        fig, axs = plt.subplots( num_layers + 1, 10, figsize = (40, (num_layers + 1) * 10))\n        # plot the samples at first row\n        for i in range(data.shape[0]):\n            axs[0][0].set_ylabel(\"Inputs\", fontsize = 48)\n            axs[0][i].imshow(data[i].reshape(28, 28), cmap = 'gray')\n\n        # we need to use the previous outputs from (n-1)th layer in nth layer\n        # so we should iteratively pass the processed data through layers\n        # we can plot the activations as heatmaps after we reshape them \n        # to more suitable shape \n        output = torch.clone(x)\n        for i in range(num_layers):\n            output = model.linear[i](output)\n            b, n_node = output.shape # output.shape: BATCHx(out_nodes)\n            axs[i+1][0].set_ylabel(\"Layer %d\"%i, fontsize = 48)\n            for j in range(data.shape[0]):\n                # plot the layer outputs (activations)\n                y = int(mid_multiplier(n_node))\n                x = int(n_node\/y)\n                axs[i+1][j].imshow(output[j].view(x, y).numpy())\n        \n        plt.show()\n        \n    model.train()","7b01dbc4":"# pick one sample from each class\nsample_indices = []\nfor data in train_data.groupby('label'):\n    # data will be a tuple that contains group name at index 0\n    # and the data inside of the group at the index 1\n    # since the grouped data will also be a dataframe, \n    # we can get sample indices and take a random index in them\n    idx = np.random.choice(data[1].index) \n    sample_indices.append(idx)\n    \n# we can finally plot the model activations as heatmaps\nsamples = train_data.iloc[sample_indices].values\nlabels = samples[0,:]\ninputs = samples[:, 1:]\nplot_activations(model, inputs)\n","82df3838":"# make predictions using validation data\nmodel.eval()\npreds = model.predict(x_val)\nmodel.train()\npreds = preds.tolist()\n\n# let's take a look of the model's performance:\nprint(classification_report(preds, y_val))","9c391d4f":"# make predictions using test data and prepare the submission dataframe\nmodel.eval()\npreds = model.predict(x_test)\npreds = preds.tolist()\n\nsubmission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))\nsubmission['Label'] = preds\nsubmission.to_csv('submission.csv', index = None)","9ee758e0":"pd.read_csv('submission.csv')","0ba6cdcb":"## Evaluation ","9eae34d7":"## Reading and processing the dataset","caf1771f":"## Defining the model\nI love to have some helper methods in my model classes. It generally makes everything more structured while it is also possible to use them as snippets later. ","769d3ebc":"## Can we visualize the model?\n\nWell,generally we think of neural nets as black boxes but we can try to interpret what model learned by looking at the activations. We would look at the learned filters if we used CNNs but we can still try to understand"}}