{"cell_type":{"b08441c4":"code","5378ce1f":"code","5c75ed07":"code","dc025bcb":"code","0ea561d5":"code","d450f2c9":"code","37776727":"code","f5d92615":"code","ac05d3cf":"code","fdb1e1fe":"code","c5a920bb":"code","2719187f":"code","9bd70910":"code","60bf5084":"code","8efc0dd4":"code","d7f16872":"code","82ff3dab":"code","6c331912":"code","9e6d7f12":"code","273dce99":"code","83f93c0d":"code","1a29f124":"code","7654617f":"code","5f002752":"code","3cb503b9":"code","f464b11f":"code","6108f0fe":"code","34d0141f":"code","88c34ae0":"code","7e1a6fce":"code","a3d968b3":"code","4451715a":"code","66916614":"code","36c6d719":"code","0cbfc7fb":"code","e0b5a646":"code","faf19a51":"code","1c681fd0":"code","ffb51349":"code","af255eea":"code","18f6fe46":"code","35b983ec":"code","6677fb40":"code","eee22b6b":"code","c8c4b928":"code","ac00fa76":"code","2a6184da":"code","8bc2dfbd":"code","2dc477bd":"code","adb97b44":"markdown","96e08095":"markdown","4eac2904":"markdown","389b93ab":"markdown","44f7ed9b":"markdown","87db15c0":"markdown","e16f9cab":"markdown","d78d62a9":"markdown","7e28cc2e":"markdown"},"source":{"b08441c4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n","5378ce1f":"#load train data\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\",index_col = 'PassengerId')\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\",index_col = 'PassengerId')\ntrain_data.head()","5c75ed07":"train_data.dtypes","dc025bcb":"#one passenger \ntrain_data[train_data['Name'].squeeze().str.contains(\"Hosono\")==True]","0ea561d5":"sns.color_palette('Set2')\n\nplt.figure(figsize=(8,8))\nax =sns.countplot(x='Sex',data=train_data, order = ['male','female'],hue='Survived')\nax.set_xticklabels(['Male','Female'],rotation=40,ha=\"right\")\nplt.title(\"By Gender Distribution\")\ntotal= len(train_data)\n\n\nfor p in ax.patches:\n    percentage = f'{100 * p.get_height() \/ total:.1f}%\\n'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height()\n    ax.annotate(percentage, (x, y), ha='center', va='center')","d450f2c9":"#rate of men survival rate\nmen = train_data.loc[train_data.Sex =='male']['Survived']\nrate_men = sum(men) \/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n","37776727":"train_data.Age.isna().sum()","f5d92615":"train_data.Age.values.sort()\n","ac05d3cf":"#binning with range \nbins = [0,3,10,18,25,35,45, 60,80]\ntrain_data[\"binned_age\"] = pd.cut(train_data['Age'], bins)","fdb1e1fe":"binned_age = train_data.binned_age.unique().to_list()","c5a920bb":"plt.figure(figsize=(8,8))\nax =sns.countplot(x='binned_age',data=train_data, order = binned_age,hue='Survived')\nax.set_xticklabels(binned_age,rotation=40,ha=\"right\")\nplt.title(\"Age Group Distribution\")\n\nfor container in ax.containers:\n    ax.bar_label(container)","2719187f":"\nfig = plt.figure(figsize=(12,6))\n\n\nax1=fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\n\n\nsns.countplot(x='binned_age',data=train_data[train_data.Sex=='female'], order = binned_age,hue='Survived',ax=ax1)\nax1.set_xticklabels(binned_age,rotation=40,ha=\"right\")\nax1.title.set_text(\"Female Age Group Distribution\")\nfor container in ax1.containers:\n    ax1.bar_label(container)\n    \nsns.countplot(x='binned_age',data=train_data[train_data.Sex=='male'], order = binned_age,hue='Survived',ax=ax2)\nax2.set_xticklabels(binned_age,rotation=40,ha=\"right\")\nax2.title.set_text(\"Female Age Group Distribution\")\nfor container in ax2.containers:\n    ax2.bar_label(container)\nax2.title.set_text('Maleale by Age Group')\nfig.show()\n","9bd70910":"train_data.dtypes","60bf5084":"# 177 age values are missing. Replace Nan with average age for the later prediction.\n\ntrain_data['Age']=train_data.Age.fillna(train_data.Age.mean())\ntest_data['Age'] = test_data.Age.fillna(test_data.Age.mean())","8efc0dd4":"train_data.isna().sum()","d7f16872":"from mpl_toolkits import mplot3d\nfig = plt.figure(figsize=(10,10))\nax = plt.axes(projection='3d')\nsc =ax.scatter(train_data.Age,train_data.Pclass,train_data.Fare,c=train_data.Survived,cmap = 'viridis', linewidth=0.5)\n# legend\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)","82ff3dab":"targets =  train_data.Survived\nfeatures = train_data.drop('Survived',axis=1)","6c331912":"#features engineering with name  from https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\nimport re\n\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfeatures[\"Title\"] = features.Name.apply(get_title)\ntest_data[\"Title\"] =test_data.Name.apply(get_title)\n\n\nfeatures['Title'] = features['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\nfeatures['Title'] = features['Title'].replace('Mlle', 'Miss')\nfeatures['Title'] = features['Title'].replace('Ms', 'Miss')\nfeatures['Title'] = features['Title'].replace('Mme', 'Mrs')\n\ntest_data['Title'] = test_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_data['Title'] = test_data['Title'].replace('Mlle', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Ms', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')\n\n\n\n","9e6d7f12":"#Onehot encoding\nfeatures = pd.concat([features,pd.get_dummies(features.Title)],axis=1)\ntest_data= pd.concat([test_data,pd.get_dummies(test_data.Title)],axis=1)\n\nfeatures = features.drop(['Title','Name'],axis=1)\ntest_data = test_data.drop(['Title','Name'],axis=1)","273dce99":"#make Sex column to boolean\nfeatures['Sex'] = [0 if a =='male' else 1 for a in train_data.Sex]\nfeatures['Sex'] = features.Sex.astype('bool')\n\ntest_data['Sex'] = [0 if a =='male' else 1 for a in test_data.Sex]\ntest_data['Sex'] = test_data.Sex.astype('bool')","83f93c0d":"#make Cabin colum to boolean if a person have cabin or not\nfeatures.Cabin = features.Cabin.isna()\ntest_data.Cabin = test_data.Cabin.isna()\n","1a29f124":"#Onehot encoding\nfeatures = pd.concat([features,pd.get_dummies(features.Embarked)],axis=1)\ntest_data= pd.concat([test_data,pd.get_dummies(test_data.Embarked)],axis=1)","7654617f":"features = features.drop('Embarked',axis=1)\ntest_data = test_data.drop('Embarked',axis=1)","5f002752":"#Onehot encoding\nfeatures = pd.concat([features,pd.get_dummies(features.Pclass)],axis=1)\nfeatures = features.drop('Pclass',axis=1)\ntest_data= pd.concat([test_data,pd.get_dummies(test_data.Pclass)],axis=1)\ntest_data = test_data.drop('Pclass',axis=1)","3cb503b9":"# remove Name and Ticket for this prediction\nfeatures = features.drop(['Ticket','binned_age'],axis=1)\ntest_data = test_data.drop(['Ticket'],axis=1)","f464b11f":"features.isna().sum()","6108f0fe":"# one none value in Fare column\ntest_data['Fare'] = test_data.Fare.fillna(test_data.Fare.mean())","34d0141f":"# extra features engineering  Got ideas from several notebooks.\n\nfeatures['Family'] = features['SibSp']+ features['Parch']\n\nfeatures['FareperPerson'] = features.Fare\/(features.Family +1)\n\ntest_data['Family'] = test_data['SibSp'] + test_data['Parch']\n\ntest_data['FareperPerson'] = test_data.Fare \/ (test_data.Family+1)","88c34ae0":"features['Alone'] = [1 if a>0 else 0 for a in features.Family]\ntest_data['Alone'] = [1 if a >0 else 0 for a in test_data.Family]","7e1a6fce":"features","a3d968b3":"#scaling the numerical columns \n\nfrom sklearn import preprocessing\n#scaler = preprocessing.MinMaxScaler()\nscaler = preprocessing.StandardScaler()\n#scaler = preprocessing.RobustScaler()\n\n\nnumeric_features = features.columns[1:5].to_list()\nnumeric_features.append('Family')\nnumeric_features.append('FareperPerson')\n\nfeatures[numeric_features] = scaler.fit_transform(features[numeric_features])\ntest_data[numeric_features] = scaler.transform(test_data[numeric_features])","4451715a":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(features, targets,random_state = 0)","66916614":"#Making a prediction with RandomForestClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 1)\nmodel.fit(train_X,train_y)\npredictions = model.predict(val_X)\naccuracy = accuracy_score(val_y,predictions)\nprint('Accuracy: ', accuracy)\n","36c6d719":"from sklearn.metrics import classification_report\nprint(classification_report(val_y,predictions))","0cbfc7fb":"from pprint import pprint\n\nprint('Parameters currently in us: \\n')\npprint(model.get_params())","e0b5a646":"np.random.seed(0)","faf19a51":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","1c681fd0":"rf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_X, train_y)","ffb51349":"base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\nbase_model.fit(train_X, train_y)\nbase_accuracy = accuracy_score(val_y,base_model.predict(val_X))\n\n\nbest_random = rf_random.best_estimator_\nrandom_accuracy = accuracy_score(val_y, best_random.predict(val_X))\n\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))\n","af255eea":"random_accuracy","18f6fe46":"pprint(best_random.get_params())","35b983ec":"best_random","6677fb40":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [50, 60, 70, 80],\n    'max_features': [2, 3],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [400, 500, 600, 700]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","eee22b6b":"# Fit the grid search to the data\ngrid_search.fit(train_X, train_y)\ngrid_search.best_params_\n\n","c8c4b928":"best_grid = grid_search.best_estimator_\ngrid_accuracy = accuracy_score(val_y, best_grid.predict(val_X))\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","ac00fa76":"grid_accuracy","2a6184da":"predictions= best_grid.predict(test_data)","8bc2dfbd":"output = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","2dc477bd":"output['Survived'] = predictions\noutput.to_csv('my_submission.csv', index = False)\nprint(\"Your submission wasa successfully saved!\")","adb97b44":"<font size = '4'> Male passenger's survival rate is low. I rememeber women and childrens were prioritized to escape from the sinking ship in the movie. Male survivors were critisized a lot according to the wikipedia articles.   <\/font>","96e08095":"****************************************************************************************************\n## Trivia","4eac2904":"<font size = '4'> Let's see the data in detail.<\/font>","389b93ab":"<font size = '4'>He was the only Japanse passenger on the Titinic and happened to be a grandfather of Hosono Haruomi, who is a Japanese famous musician.\nhttps:\/\/en.wikipedia.org\/wiki\/Masabumi_Hosono <\/font>\n\n**********************************************************************************************","44f7ed9b":"<font size = '4'> Even kids age groups (including 0-3 years old) of male have lower survival rate than the same age group of female. <br>\nThey care for gender of babies and kids? It might be interesting to reserch why this happended.<\/font>","87db15c0":"# Making Submission File","e16f9cab":"# Data Explanatory","d78d62a9":"# Preprocess Data","7e28cc2e":"# Hyperparameter Tuning \n\n<font size = '4'> Referred to this article: https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74<\/font>\n\n"}}