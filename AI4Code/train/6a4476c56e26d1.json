{"cell_type":{"4660b2e8":"code","ecd62489":"code","02eb7d96":"code","5de61c33":"code","3ddeb75d":"code","b5192aca":"code","5fa151f2":"code","ad2d18b6":"code","67c43f1b":"code","1d3c04e1":"code","21eb5418":"markdown","d3a5fdd0":"markdown","c51cff6e":"markdown","68a0d500":"markdown","c70c3240":"markdown","c7be88e8":"markdown","59fe7a67":"markdown","c04072af":"markdown","8e74e7e8":"markdown","2e0a7f3e":"markdown","8d6ab1e5":"markdown","4e89f23b":"markdown","9a4ff173":"markdown","8d4bc0ee":"markdown","b6d1e73a":"markdown","ae21421b":"markdown","b11b8721":"markdown","eb735d17":"markdown"},"source":{"4660b2e8":"import torch\nfrom torch import nn\n\nimport matplotlib.pyplot as plt\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\n\nimport numpy as np\nfrom matplotlib import rcParams\nrcParams[\"savefig.jpeg_quality\"] = 80\nimport imageio\nfrom pathlib import Path\nimport base64\nfrom IPython import display\n# we don't like warnings\n# you can comment the following 2 lines if you'd like to\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up a random generator seed for reproducibility\ntorch.manual_seed(111)\n\n# Create a device object that points to the CPU or GPU if available\ndevice = \"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n    \n\nclass Helper():\n    @staticmethod\n    def show_gif(file_path):\n        \"\"\"\n        To show gif or video in Colab, we need to load the data and encode with base64.\n        \"\"\"\n        with open(file_path, 'rb') as file:\n            b64 = base64.b64encode(file.read()).decode('ascii')\n        return display.HTML(f'<img src=\"data:image\/gif;base64,{b64}\" \/>')\n\n    @staticmethod\n    def make_gif(images_files, gif_name=\"results.gif\"):\n        \"\"\"\n        Make gif from list of images\n        \"\"\"\n        images = [imageio.imread(file) for file in images_files]\n        imageio.mimsave(gif_name, images, fps=5)\n        \n    @staticmethod\n    def tensor_to_image(image_tensor, title, output_path, file_name, show):\n        \"\"\"\n        Convert tensor to image to display or save to file\n        \"\"\"\n        # Plot the image\n        image = np.transpose(image_tensor,(1,2,0))\n        plt.figure(figsize=(8,8))\n        plt.axis(\"off\")\n        plt.title(title)\n        plt.imshow(image)\n        # Save to file  \n        file_path = \"\"       \n        if output_path:\n            Path(output_path).mkdir(parents=True, exist_ok=True)\n            file_path = f\"{output_path}\/{file_name}\"\n            plt.savefig(file_path)\n        if not show: # Close the plot to not display image\n            plt.close('all')\n        # Return path of the saved file\n        return file_path\n\n    @staticmethod\n    def show_losses(losses_generator, losses_discriminator):\n        plt.figure(figsize=(10,5))\n        plt.title(\"Generator and Discriminator Loss During Training\")\n        plt.plot(losses_generator,label=\"G\")\n        plt.plot(losses_discriminator,label=\"D\")\n        plt.xlabel(\"iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.show()","ecd62489":"# Batch size during training\nbatch_size = 128\n\n# Set image size for the transformer\nimage_size = 64\n\n# Number of channels in images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Define the transform to load images from CelebA dataset\ntransform = transforms.Compose([transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                              ])\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Initialize BCELoss function\nloss_function = nn.BCELoss()\n\n# Custom weights initialization for neural network model\ndef weights_init(m):\n    \"\"\"\n    Randomly initialize all weights to mean=0, stdev=0.2\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","02eb7d96":"# Load CelebA dataset from Kaggle input folder\ntrain_set = torchvision.datasets.ImageFolder(\n    root=\"\/kaggle\/input\/celeba-dataset\", transform=transform\n)\n\n# Create a data loader to shuffle and return data in batches for training\ntrain_loader = torch.utils.data.DataLoader(\n    train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True\n)","5de61c33":"# Get total number of batches. We print the losses after training the last batch of each epoch\nnum_batches = len(train_loader)\n\n# Set how many repetitions of training with the whole dataset\nnum_epochs = 3\n\n## Because the labels remain the same for every batch, we define them as constants to use for all training steps:\n# Create tensor of labels for real samples with value=1 and shape is batch_size x 1\nreal_samples_labels = torch.ones((batch_size, 1)).to(device)\n# Create tensor of labels for generated samples with value=0 and shape is batch_size x 1\ngenerated_samples_labels = torch.zeros((batch_size, 1)).to(device) \n# Create tensor of labels for combined data\nall_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n\n# Create batch of fixed latent vectors that we will use to visualize the progression of the generator\nfixed_latent_vectors = torch.randn((batch_size, nz, 1, 1)).to(device)\n\n# Set where to save the generated image files. We just save it temporarily\noutput_path = '\/kaggle\/temp\/'","3ddeb75d":"class Generator(nn.Module):\n    def __init__(self, batchnorm=True):\n        super().__init__()\n        if batchnorm:\n            self.model = nn.Sequential(\n                # input is Z, going into a convolution\n                nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n                nn.BatchNorm2d(ngf * 8),\n                nn.ReLU(True),\n                # state size. (ngf*8) x 4 x 4\n                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(ngf * 4),\n                nn.ReLU(True),\n                # state size. (ngf*4) x 8 x 8\n                nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(ngf * 2),\n                nn.ReLU(True),\n                # state size. (ngf*2) x 16 x 16\n                nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(ngf),\n                nn.ReLU(True),\n                # state size. (ngf) x 32 x 32\n                nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n                nn.Tanh()\n                # state size. (nc) x 64 x 64\n            )\n        else:\n            self.model = nn.Sequential(\n                # input is Z, going into a convolution\n                nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n                nn.ReLU(True),\n                # state size. (ngf*8) x 4 x 4\n                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n                nn.ReLU(True),\n                # state size. (ngf*4) x 8 x 8\n                nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n                nn.ReLU(True),\n                # state size. (ngf*2) x 16 x 16\n                nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n                nn.ReLU(True),\n                # state size. (ngf) x 32 x 32\n                nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n                nn.Tanh()\n                # state size. (nc) x 64 x 64\n            )\n\n    def forward(self, input):\n        return self.model(input)\n    \n    \nclass Discriminator(nn.Module):\n    def __init__(self, batchnorm=True):\n        super().__init__()\n        if batchnorm:\n            self.model = nn.Sequential(\n                # input is (nc) x 64 x 64\n                nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf) x 32 x 32\n                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(ndf * 2),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf*2) x 16 x 16\n                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(ndf * 4),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf*4) x 8 x 8\n                nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(ndf * 8),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf*8) x 4 x 4\n                nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n                nn.Sigmoid()\n            )\n        else:\n            self.model = nn.Sequential(\n                # input is (nc) x 64 x 64\n                nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf) x 32 x 32\n                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf*2) x 16 x 16\n                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf*4) x 8 x 8\n                nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                # state size. (ndf*8) x 4 x 4\n                nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n                nn.Sigmoid()\n            )\n\n    def forward(self, input):\n        return self.model(input)","b5192aca":"class GAN():\n    def __init__(self, batchnorm=True):\n        self.batchnorm = batchnorm\n        \n        # Create the generator and the discriminator\n        self.generator = Generator(self.batchnorm).to(device)\n        self.discriminator = Discriminator(self.batchnorm).to(device)\n\n        # Apply the weights_init function\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # Setup Adam optimizers\n        self.optimizer_discriminator = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n        self.optimizer_generator = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(beta1, 0.999))\n    \n    \n    def train_discriminator(self, real_samples, generated_samples, alternating=True):\n        \"\"\"\n        Train the discriminator model by minimizing its error.\n        Input: \n            real_samples: tensor of images with shape: batch_size x channel x width x height\n            generated_samples: tensor of generated images with shape: batch_size x channel x width x height\n            alternating: training strategy:\n                alternating=True: Train the model alternately with different batches of real data and generated data.\n                alternating=False: Combine real and generated data into one batch and train\n        Return:\n            loss_discriminator: for printing purpose\n        \"\"\"  \n        # Clear the gradients of the discriminator to avoid accumulating them\n        self.discriminator.zero_grad()\n        \n        if alternating: #  D(x)  ->  backward  ->  D(G(z)) -> backward\n            \n            # Train the discriminator with the real data\n            output_discriminator_real = self.discriminator(real_samples)\n            # Calculate the loss function for the discriminator to minimize its error\n            loss_discriminator_real = loss_function(output_discriminator_real, real_samples_labels)\n            # Calculate the gradients for the discriminator\n            loss_discriminator_real.backward()\n\n            # Train the discriminator with the generated data\n            output_discriminator_generated = self.discriminator(generated_samples)\n            # Calculate the loss function for the discriminator to minimize its error\n            loss_discriminator_generated = loss_function(output_discriminator_generated, generated_samples_labels)\n            # Calculate the gradients for the discriminator\n            loss_discriminator_generated.backward()\n\n            # Calculate the total loss of the discriminator to show later\n            loss_discriminator = loss_discriminator_real + loss_discriminator_generated\n            \n        else: #  t = [x, G(z)]  ->  D(t)  ->  backward\n            \n            # Combine the real and generated data into one batch\n            all_samples = torch.cat((real_samples, generated_samples))\n            # Train the discriminator with the combined data\n            output_discriminator = self.discriminator(all_samples)\n            # Calculate the loss function for the discriminator to minimize its error\n            loss_discriminator = loss_function(output_discriminator, all_samples_labels)\n            # Calculate the gradients for the discriminator\n            loss_discriminator.backward()\n        \n        # Update the weights of the discriminator\n        self.optimizer_discriminator.step()\n\n        return loss_discriminator\n    \n    \n    def train_generator(self, output_generator):\n        \"\"\"\n        Continue to train the generator model with its output by maximizing the discriminator error.\n        Input:\n            output_generator: output of the generator model when feeding the latent data\n        Return:\n            loss_generator: for printing purpose\n        \"\"\"  \n        # Clear the gradients of the generator to avoid accumulating them \n        self.generator.zero_grad()\n        # Get the discriminator prediction on the generator's output \n        output_discriminator_generated = self.discriminator(output_generator)\n        # Calculate the loss function for the generator to maximize the discriminator error\n        loss_generator = loss_function(output_discriminator_generated, real_samples_labels)\n        # Calculate the gradients for the generator \n        loss_generator.backward()\n        # Update the weights of the generator\n        self.optimizer_generator.step()\n\n        return loss_generator\n    \n\n    def train(self, alternating_training_discriminator=True, output_name='results'):\n        # Save the losses to visualize\n        self.losses_discriminator, self.losses_generator = [], []\n        self.output_files = []\n\n        # Repeat the training process based on the number of epochs\n        for epoch in range(num_epochs):\n            # Load training data by batches\n            for batch, (real_samples, _) in enumerate(train_loader):\n                real_samples = real_samples.to(device)   \n                \n                ## Prepare data for training\n                # Randomize tensor of latent vectors with shape (batch_size x nz x 1 x 1)\n                latent_vectors = torch.randn((batch_size, nz, 1, 1)).to(device)\n                # Feed the latent vectors to the generator (1 line)\n                output_generator = self.generator(latent_vectors)\n                # Get the generated data without its gradients (to use in training the discriminator) \n                generated_samples = output_generator.detach()\n\n                \n                ## Train the discriminator (1 line): \n                loss_discriminator = self.train_discriminator(real_samples, generated_samples, \n                                                              alternating_training_discriminator)\n\n                ## Continue to train the generator with its output and get the loss_generator \n                loss_generator = self.train_generator(output_generator)\n\n                self.losses_discriminator += [loss_discriminator]\n                self.losses_generator += [loss_generator]\n\n                # Print losses\n                if (batch % 500 == 0) or (batch == num_batches - 1):\n                    print(f\"Epoch {epoch} - Batch {batch}. Loss D.: {loss_discriminator}. Loss G.: {loss_generator}\")\n                    title = f\"After {batch} batches of {epoch} epoch(s)\"\n                    file_name = f\"{output_name}_e{epoch:0=4d}_b{batch:0=4d}.jpg\"\n                    file = self.generate_images(title=title, output_path=output_path, \n                                                  file_name=file_name, show=False)\n                    self.output_files.append(file)\n        \n        Helper.make_gif(self.output_files, output_name+'.gif')\n        Helper.show_losses(self.losses_generator, self.losses_discriminator)\n        return Helper.show_gif(output_name+'.gif')\n        \n    \n    def generate_images(self, title=False, output_path=False, file_name=False, show=True):\n        \"\"\"\n        Generate images from a random vector using the generator.\n        Input:\n            title: title of the image showing how many epochs that the generator is trained\n            output_path: if you want to save file, define the output folder \n            show: display the plot or not. Set to False if you just want to save the image\n        Output:\n            file_path: path of the generated image file\n        \"\"\"     \n        with torch.no_grad():\n            # Generate data from fixed_latent_vectors with the generator \n            generated_samples = self.generator(fixed_latent_vectors)\n            # Move the data back to the CPU and create a view of data (without gradients)\n            generated_samples = generated_samples.cpu().detach()\n            # Create grid of 64 generated images\n            img_grid = vutils.make_grid(generated_samples[:64], padding=2, normalize=True)\n            \n        file_path = Helper.tensor_to_image(img_grid, title, output_path, file_name, show)\n        return file_path","5fa151f2":"gan1 = GAN(batchnorm=True)\ngan1.train(alternating_training_discriminator=True, output_name='gan_batchnorm_alternating')","ad2d18b6":"gan2 = GAN(batchnorm=True)\ngan2.train(alternating_training_discriminator=False, output_name='gan_batchnorm')","67c43f1b":"gan3 = GAN(batchnorm=False)\ngan3.train(alternating_training_discriminator=True, output_name='gan_alternating')","1d3c04e1":"gan4 = GAN(batchnorm=False)\ngan4.train(alternating_training_discriminator=False, output_name='gan')","21eb5418":"### Load dataset","d3a5fdd0":"## GAN2: batchnorm but no alternating training\n\nWhat happen if we combine the real and fake data into one batch and train?","c51cff6e":"## Implement GAN with options for batchnorm and alternating training discriminator","68a0d500":"## GAN3: no batchnorm but alternating training\n\nSo, how about no batchnorm?","c70c3240":"The training process looks stable and the result is quite good after just 1 epoch. This is what normal convergence looks like and what to expect when training GANs.","c7be88e8":"> _You may ask \"what is going on with my GAN...\". Don't worry, you are not alone..._\n\n## Why training GANs is so challenging? \n\nTo answer this, let's look at the game theory approach: both the generator model and the discriminator model are trained simultaneously in a zero sum game. This means that improvements to one model come at the expense of the other model. The goal of training two models involves finding a point of equilibrium between the two competing concerns.\n\nDuring the training process, every time the parameters of one of the models are updated, the nature of the optimization problem that is being solved is changed. Therefore, training GANs is often unstable. The two models can fail to converge. Another common failure is mode collapse: the generator finds one sample that can fool the discriminator and it keeps generating only that sample.\n\nIn this tutorial, we will first learn to train a stable GAN model for image generation using DCGAN and CelebA dataset with PyTorch. We will then impair the GAN models in different ways and explore a range of failure modes that you may encounter when training GAN models.\n\n> _TL;DR: You may want to skip the implementation and [see the results](https:\/\/www.kaggle.com\/linhvn\/try-different-gans-tricks\/#Do-those-GANs-tricks-work?) first._","59fe7a67":"## Initialize\n\n### Import libraries & define some helper functions","c04072af":"## GAN4: no batchnorm, no alternating training\n\nThis is almost the same technique that we use in [task 1](https:\/\/colab.research.google.com\/drive\/1PctdHRuE4GNZjP57xj2IHhBwa0FUL3j-). Let's see if it works: ","8e74e7e8":"### Set up hyperparameters based on DCGAN paper\n\nThis is one of the best pratice for GANs training.","2e0a7f3e":"The training process becomes completely unstable! Batchnorm helps stablize the learning process and also preventing the generator from collapsing all samples to a single point.","8d6ab1e5":"The models fail to converge and the results look nonsense...\nYou may ask why it works in the task 1 when we [implement a simple GAN model to generate handwritten digits images](https:\/\/colab.research.google.com\/drive\/1PctdHRuE4GNZjP57xj2IHhBwa0FUL3j-), but does not work in the [task 2](https:\/\/www.kaggle.com\/linhvn\/lab-3-solution-gan-task-2\/).\n\nThe reason is that the two GAN models are different. In task 2, we use Batchnorm, which will try to normalize the input to make the training process stable. However, at the very beginning of GAN's training, real and fake samples in a mini-batch have very different distributions. If we don't construct different mini-batches for real and fake samples, batch normalization will not work as it supposed. The generator finds a noise that can fool the discriminator and keep generating it.","4e89f23b":"### Setup some constants for training process","9a4ff173":"We see the mode collapse again. Samples of images generated at each epoch are all very low quality and look exactly the same.","8d4bc0ee":"## Do those GANs tricks work?\n\nLet's test some common GANs hacks:\n\n* **Batchnorm**, or Batch Normalization: Batch normalization helps reduce internal covariance shift in activation maps by making all of the activations be distributed equally (with zero mean and std equal to 1). It is recommended to apply Batchnorm in both generator and discriminator to stablize learning process.\n* **Alternating training** (for the discriminator): In training the discriminator, it is recommended to construct different mini-batches for real and fake, i.e. each mini-batch needs to contain only all real images or all generated images. ","b6d1e73a":"## GAN1: batchnorm and alternating training\n\nThe implementation is based on the DCGAN paper.","ae21421b":"## References:\n\nhttps:\/\/arxiv.org\/pdf\/1511.06434.pdf\n\nhttps:\/\/developers.google.com\/machine-learning\/gan\/problems\n\nhttps:\/\/machinelearningmastery.com\/how-to-train-stable-generative-adversarial-networks\/\n\nhttps:\/\/github.com\/soumith\/ganhacks\/issues\/9\n\nhttps:\/\/github.com\/soumith\/ganhacks\n\nhttps:\/\/machinelearningmastery.com\/practical-guide-to-gan-failure-modes\/\n\nhttps:\/\/machinelearningmastery.com\/how-to-code-generative-adversarial-network-hacks\/\n\nhttps:\/\/towardsdatascience.com\/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628","b11b8721":"## Conclusion\n\nThank god, those tricks really work! Take home:\n![](https:\/\/raw.githubusercontent.com\/soumith\/ganhacks\/master\/images\/batchmix.png)\n\nIf you ever encounter the same problems above, hope this tutorial helps to explain that. Let's check out more ideas to improve your GANs training process in the references below, especially the last 4 links.\nKeep calm and train your GANs!","eb735d17":"## Implement Generator & Discriminator with batchnorm as an option"}}