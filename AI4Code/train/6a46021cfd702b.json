{"cell_type":{"29f6c45a":"code","85068a19":"code","acbebe03":"code","a439cb72":"code","0bb87f47":"code","371fed8c":"code","ebee8394":"code","559d4f08":"code","cd701321":"code","2005fb34":"code","7a40deb7":"code","2686351f":"code","ce33b90d":"code","1c15c3b4":"code","adef0665":"code","b41e301f":"code","52e05027":"code","e4e4b541":"code","b22ef70b":"code","76d968d9":"code","db9e96f5":"code","ad2eb48a":"code","ab951c36":"code","bb13946a":"code","875e7c8b":"code","ba363fe2":"code","32e254db":"code","0620b3ef":"markdown","31044d86":"markdown","b711b10f":"markdown","4143e90d":"markdown","79c99e20":"markdown","0c0588ca":"markdown","eae6f3ff":"markdown","cb4b4219":"markdown","1a1bef2a":"markdown","b83eb3d2":"markdown","2f9f9c87":"markdown","d59f8385":"markdown","21f0bde9":"markdown","ee1528f3":"markdown","92a2e875":"markdown"},"source":{"29f6c45a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","85068a19":"import torch.nn as nn\nimport torchvision\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport time","acbebe03":"transform= transforms.Compose(\n    [   transforms.Resize(256),\n     transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223 , 0.24348513, 0.26158784])\n    ])","a439cb72":"trainset= torchvision.datasets.CIFAR10(root='.\/data', train=True, transform=transform,\n                                      download=True)\ntestset= torchvision.datasets.CIFAR10(root='.\/data', train=False, transform=transform,\n                                     download=True)","0bb87f47":"trainset.data.shape","371fed8c":"train_means= trainset.data.mean(axis=(0,1,2))\/255\ntrain_means","ebee8394":"train_stds= trainset.data.std(axis=(0,1,2))\/255\ntrain_stds","559d4f08":"trainloader= torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True,\n                                        num_workers=8)\ntestloader= torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False,\n                                       num_workers=8)","cd701321":"classes = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","2005fb34":"from torch.utils.data import Subset\ndef train_valid_split(dl, val_split=0.25):\n    total_items= dl.dataset.data.shape[0]\n    idxs= np.random.permutation(total_items)\n    train_idxs, valid_idxs= idxs[round(total_items*val_split):], idxs[:round(total_items*val_split)]\n    \n    train= Subset(dl, train_idxs)\n    valid= Subset(dl, valid_idxs)\n    return train, valid","7a40deb7":"train_dl, valid_dl= train_valid_split(trainloader)","2686351f":"import matplotlib.pyplot as plt\ndef show_image(img):\n    img= img.numpy()\n    plt.imshow(np.transpose(img, (1, 2, 0)))\n    plt.show()\n    ","ce33b90d":"# get some random training images\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# show images\nshow_image(torchvision.utils.make_grid(images[:4]))","1c15c3b4":"[classes[each] for each in labels[:4]]","adef0665":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# trainloader.to(device);\nprint(device)","b41e301f":"class AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        #1\n        self.features= nn.Sequential(\n        nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2),\n        #2\n        nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2),\n        #3\n        nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(inplace=True),\n        #4\n        nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(inplace=True),\n        #5\n        nn.Conv2d(384, 256, kernel_size=5, stride=1, padding=2),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool= nn.AvgPool2d(6)\n        self.classifier= nn.Sequential(\n            nn.Dropout(), nn.Linear(256*6*6, 4096), #128*2*2, 1024\n        nn.ReLU(inplace=True), nn.Dropout(),\n        nn.Linear(4096, num_classes))\n        \n    def forward(self, x):\n        x= self.features(x)\n        x=x.view(x.size(0), 256*6*6)\n        x= self.classifier(x)\n        return x","52e05027":"model= AlexNet(num_classes=10).to(device)","e4e4b541":"model","b22ef70b":"#loss function and optimizer\ncriterion= nn.CrossEntropyLoss()\noptimizer= torch.optim.Adam(params= model.parameters(), lr=3e-4)","76d968d9":"import datetime\n\ndef convert_seconds_format(n):\n    return str(datetime.timedelta(seconds =n))\n","db9e96f5":"all_losses=[]\nall_valid_losses=[]\nprint('training starting...')\nstart_time= time.time()\nfor epoch in range(10):\n    epoch_start=time.time()\n    model.train()\n    running_loss= 0.0\n    running_valid_loss=0.0\n    predictions=[]\n    total=0\n    correct=0\n    \n    for i, data in enumerate(train_dl.dataset, 0):\n\n        inputs, labels= data[0].to(device), data[1].to(device)\n\n        #zero parameter gradients\n        optimizer.zero_grad()\n\n        #forward + back optimize\n        outputs= model(inputs)\n        loss= criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        #stats\n        running_loss += loss.item()\n    all_losses.append(running_loss\/i)\n    \n    #evaluation mode\n    model.eval()\n    with torch.no_grad():\n        for i, data in enumerate(valid_dl.dataset, 0):\n            inputs, labels= data[0].to(device), data[1].to(device)\n            outputs= model(inputs)\n            valid_loss= criterion(outputs, labels)\n            running_valid_loss+= valid_loss.item()\n            \n            #the class with the highest score\n            _, predicted= torch.max(outputs.data, 1)\n            predictions.append(outputs)\n            total+= labels.size(0)\n            correct+= (predicted==labels).sum().item()\n    epoch_end=time.time()\n    epoch_time= convert_seconds_format(epoch_end-epoch_start)\n    \n    all_valid_losses.append(valid_loss)\n    print(f\"epoch {epoch+1}, running loss: {all_losses[-1]}\")\n    print(f\"validation accuracy: {correct\/total}. validation loss: {all_valid_losses[-1]}\")\n    print(f\"epoch time: {epoch_time}\")\nend_time= time.time()\ntrain_time= convert_seconds_format(end_time- start_time)\nprint('training complete')\nprint(f\"total time to train: {train_time}\")","ad2eb48a":"\nx_axis=[i for i in range(1, 11)]\nx_axis","ab951c36":"valid_losses_list=[each.item() for each in all_valid_losses]\n    ","bb13946a":"plt.plot(x_axis, all_losses, label='train')\nplt.plot(x_axis, valid_losses_list, label='valid')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend();","875e7c8b":"correct, total=0, 0\npredictions=[]\n","ba363fe2":"model.eval();\nwith torch.no_grad():\n    for i, data in enumerate(testloader, 0):\n        inputs, labels= data[0].to(device), data[1].to(device)\n        #inputs= inputs.view(-1, 32*32*3)\n        outputs= model(inputs)\n        #the class with the highest score\n        _, predicted= torch.max(outputs.data, 1)\n        predictions.append(outputs)\n        total+= labels.size(0)\n        correct+= (predicted==labels).sum().item()","32e254db":"print(f' Accuracy score of: {correct\/total}')\n","0620b3ef":"Below I get the mean and standard deviations of the pictures for each channel. These are the stats we use to normalize the images.","31044d86":"### training loop\nLets go for 10 epochs and see how the model performs.","b711b10f":"# AlexNet on CIFAR10 \nThe goal of this notebook is to reimplement [AlexNet](https:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)  on a 32x32 pixel dataset called CIFAR10. AlexNet is a Convolutional Neural Network Architecture developed by Alex Krizhevsky, and published with Ilya Sutskever and Krizhevsky's doctoral advisor Geoffrey Hinton. It won the [ImagNet large scale Visual Recognition Challange](https:\/\/en.wikipedia.org\/wiki\/ImageNet#ImageNet_Challenge) in September 2012.","4143e90d":"## Model\nThe model I use is [AlexNet](https:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). It is the model which basically revolutionalized the space of Deep Learning and brought a lot of attention to this space of Image Classification.","79c99e20":"I create a function to convert seconds into a human friendly format of hours-minutes-seconds. This I will use to keep track of the training time.","0c0588ca":"On the testing data, the model is correct 82% of the time. This is really impressive of our model having built it from scratch. \n\n\n","eae6f3ff":"### Dataloaders","cb4b4219":"### Transformations\n* I resize the images to 256 since AlexNet expects inputs of that size.\n* Perform Horizontal flips 50% of the time. This allows the model to recognize objects when they are facing the other direction too.\n* Make the images Tensors\n* Normalize the images. Now the stats I have below are the mean and standard deviations across all channels. I'll show below how I got them.\n","1a1bef2a":"Put the model on the GPU","b83eb3d2":"### Train and Test set","2f9f9c87":"### Get a validation set from the training","d59f8385":"### Conclusion\nAfter having reimplemented the AlexNet paper, I am glad it gets a great perfomance at classifying images on CIFAR10 datset. At the time of writing, the best model gets 96.53%, [as seen here](https:\/\/rodrigob.github.io\/are_we_there_yet\/build\/classification_datasets_results.html#43494641522d3130), which has a better architecture and tricks to improve model perfomance.\n\nPlease reach out to me if you spot something I might have missed as I am also still learning.","21f0bde9":"Below I plot the losses from the train and validation set over 10 epochs.","ee1528f3":"## Show images","92a2e875":"After 37 minutes of training, the moodel gets an accuracy of about 90% on the training set. Thanks Alex."}}