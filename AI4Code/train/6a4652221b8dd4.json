{"cell_type":{"ca8448a5":"code","98327942":"code","35d022c7":"code","c91fbced":"code","efb8a835":"code","1508f0aa":"code","73e5609b":"code","2c733c96":"code","90bb41db":"code","21c8d7a8":"code","64252b90":"code","18b7f3b9":"code","d9a10011":"code","e252ea88":"code","501541f5":"code","3300983e":"code","8641ceb3":"code","95cf63fe":"code","e0544c35":"code","e162320f":"code","df1e4b94":"code","c37b6da4":"code","20052ea5":"code","0ac606cd":"code","e7b95b58":"code","fbcc89a6":"code","1655f08d":"code","5d866963":"code","e1ba5c13":"code","b361c7c9":"code","568e2c20":"code","7d19a644":"code","7322ab28":"code","55195539":"code","cc74fa8b":"code","48b1c061":"code","aa7c3973":"code","14b79de1":"code","22533470":"code","baa5c9b7":"code","d238882d":"code","c29d12e9":"code","a8f2c0ca":"code","3f082589":"code","d98e2211":"markdown","456b17fa":"markdown","cfd78e51":"markdown","2ce25e53":"markdown","2147e675":"markdown","e5202bc0":"markdown","efe14dd0":"markdown","4b7b849f":"markdown","a4fc9a73":"markdown","c7c4813e":"markdown"},"source":{"ca8448a5":"#didn't want to use notebook space for package installation logs so,\n#writing to a log file to see in case there is an error\n!apt install -y swig >> apt_logs.txt\n!pip install smac[all] >> pip_logs.txt","98327942":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","35d022c7":"import warnings\nwarnings.filterwarnings('ignore')\n#ignore because sometimes MLP doesn't converge and\n#it starts prompting about it, it gets annoying sometimes\n#especially in the last exercise.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom typing import  List, Union, Type, Callable\n\n%matplotlib inline","c91fbced":"BIG_SEED = 123456789 #random seed for later\n\nfrom sklearn.metrics import accuracy_score, make_scorer\ndef weighted_score(y_true, y_pred):\n    weight = [0.12 if y==0 else 0.88 for y in y_true]\n    return accuracy_score(y_true, y_pred, sample_weight=weight)\nscore_metric = make_scorer(weighted_score)\n#score_metric = 'accuracy' #to choose accuracy as the metric\nlabel_metric = 'Weighted Accuracy'","efb8a835":"df = pd.read_csv('\/kaggle\/input\/fertility-data-set\/fertility.csv', skiprows=[0] ,\n                 names=['Season','Age','Childish diseases','Accident or trauma',\n                        'Surgical intervention','High fevers',\n                        'Frequency of alcohol consumption','Smoking habit' ,\n                        'Hours spent sitting per day','Class'])\ndf.head()","1508f0aa":"df = pd.get_dummies(df, columns=['Season','Frequency of alcohol consumption'])\n\nencode_col =['Childish diseases','Accident or trauma',\n             'Surgical intervention','High fevers', 'Class',\n             'Smoking habit', 'Hours spent sitting per day']\n\nfor col in encode_col:\n    le = LabelEncoder().fit(df[col])\n    df[col] = le.transform(df[col])\n    \ndf.Class = 1 - df.Class","73e5609b":"print(len(df))\ndf.head()","2c733c96":"df.Class.value_counts()\n#class imbalance problem!!","90bb41db":"X = df.drop('Class', axis=1).values\ny = df.Class","21c8d7a8":"X.shape","64252b90":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.utils import shuffle\n\nfrom smac.configspace import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import UniformFloatHyperparameter,\\\n                                        UniformIntegerHyperparameter\n\nfrom smac.scenario.scenario import Scenario\nfrom smac.facade.smac_hpo_facade import SMAC4HPO #Random Forest\nfrom smac.facade.smac_bo_facade import SMAC4BO   #GP\nfrom smac.initial_design.random_configuration_design import RandomConfigurations\nfrom smac.optimizer import acquisition","18b7f3b9":"def optimizer(surrogate_model :str, \n              config_space :Type[ConfigurationSpace],\n              obj_function :Type[Callable],\n              acquisition_type :str,\n              n_iter :int = 10,\n              init_points :int = 0,\n              seed :int=None) -> Union[Type[SMAC4BO], Type[SMAC4HPO]]:\n    \"\"\"\n    It is just a small wrapper to simplify and make more intuitive \n    the SMAC optimization for ML algorithms for this assignment.\n    PARAMETERS\n    ----------\n    @surrogate_model  can be 'gp' for GaussianProcess or \n                      'rf' for RandomForest\n    @config_space     an object of type ConfigSpace containing \n                      information regarding the hyperparameters\n    @obj_function     the objective function that needs to be minimized \n    @acquisition_type can be 'LCB', 'EI' or 'PI' and indicated the type \n                      of acquisition function to be used, if the value \n                      is different from 'LCB', 'EI' or 'PI' it will use \n                      the default acquisition function of SMAC.\n    @n_iter           number of iterations to do, defaults to 10 iterations\n    @init_points      number of initial points, defaults to 0 initial points\n    @seed             fix the seed for most of the randomness to have \n                      reproducible results\n    \n    RETURN\n    ------\n    either an optimized SMAC4BO object if 'gp' model_type was chosen \n    or an optimized SMAC4HPO object if 'rf' model_type was chosen\n    and in any other case an exception (ValueError) is raised\n    \"\"\"\n    #create the scenario required by SMAC optimizers\n    #First we write down it's values in a dictionary and comment the results\n    if seed is None:\n        deterministic = \"false\"\n    else:\n        deterministic = \"true\"\n    scen_dict = {\"run_obj\": \"quality\",    # we optimize quality\n            #the only two choises are quality and runtime\n            \"runcount-limit\": n_iter,# number iterations\n            \"cs\": cs,      # configuration space\n            \"output_dir\": \"logs\",    # create a log folder \n            #named 'logs' to save the logs of SMAC\n            \"deterministic\": deterministic, #if false the model\n            #may need to revaluate the same points more time\n            #to be sure of the value at that specific point\n            \"abort_on_first_run_crash\": False, #continue in case of a crash instead\n            #of failing the model itself, this way the model doesn't crash\n            \"always_race_default\": False\n            }\n    scenario = Scenario(scenario=scen_dict)\n    \n    #initial random points can also be created from the \n    #configuration space itself and the seed can be fixed if necessary\n    #cs.seed(seed)\n    #init = cs.sample_configuration(init_points)\n    \n    #choose the acquisition function\n    #only the one explained at the lessons are present here\n    if acquisition_type == 'LCB':\n        print('Using LCB acquisition Function')\n        _acquistion = acquisition.LCB\n    elif acquisition_type == 'EI':\n        print('Using EI acquisition Function')\n        _acquistion = acquisition.EI\n    elif acquisition_type == 'PI':\n        print('Using PI acquisition Function')\n        _acquistion = acquisition.PI\n    else:\n        #use the defualt one chosen by SMAC based on the \n        #type of surrogate model chosen\n        print('Using SMAC Default acquisition Function')\n        _acquistion = None\n        \n    #choose and optimize the SMAC model\n    if surrogate_model == 'gp':\n        #use SMAC4BO\n        smac = SMAC4BO(scenario=scenario, tae_runner=obj_function, \n                       rng=np.random.RandomState(seed),\n                       initial_design=RandomConfigurations, \n                       initial_design_kwargs={'n_configs_x_params':init_points,\n                                              'max_config_fracs':init_points\/n_iter},\n                       acquisition_function=_acquistion)\n        _best = smac.optimize()\n        print('Best Configuration found:')\n        print(_best)\n        return smac\n    if surrogate_model == 'rf':\n        #use SMAC4HPO (Bayesian Optimization with Random Forest)\n        smac = SMAC4HPO(scenario=scenario, tae_runner=obj_function, \n                        rng=np.random.RandomState(seed),\n                        initial_design=RandomConfigurations, \n                        initial_design_kwargs={'n_configs_x_params':init_points,\n                                               'max_config_fracs':init_points\/n_iter}, \n                        acquisition_function=_acquistion)\n        _best = smac.optimize()\n        print('Best Configuration found:')\n        print(_best)\n        return smac\n    #in case model_type was neither gp nor rf\n    raise ValueError(\"Model type can only 'gp' or 'rf'\")","d9a10011":"#SMAC needs a Configuration Space where it will search for the hyperparameters\ncs = ConfigurationSpace()\n\n#define the domain of each hyperparameter\nlr = UniformFloatHyperparameter(\"learning_rate_init\", 0.01, 0.1, default_value=0.05)\nmomentum = UniformFloatHyperparameter(\"momentum\", 0.1, 0.9, default_value=0.5)\n\n#add the hyperparameters to the configuration space\ncs.add_hyperparameters([lr, momentum])","e252ea88":"def MLP_score_1(conf: Union[Type[ConfigurationSpace], dict]) -> float:\n    \"\"\"\n    Basically takes a configuration does a 10-CV and return the average\n    score, the average score is modified so it gets minimized, for example\n    accuracy is converted into error.\n    PARAMETERS\n    ----------\n    @conf   is a ConfigurationSpace Object, but it acts as a dictionary\n            so we can call a value given the key: conf['key']\n            \n    Returns\n    -------\n    @return a float value which needs to be minimized in this case cv error\n    \n    This function takes in the configuration and computes the error score\n    using CV with 10 folds\n    \"\"\"\n    \n    MLPclf = MLPClassifier(hidden_layer_sizes=(4,2,), momentum = conf['momentum'],\n                          learning_rate_init = conf['learning_rate_init'],\n                          random_state=BIG_SEED)\n    \n    skf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\n    X = df.drop('Class', axis=1).values\n    y = df.Class\n    score = cross_val_score(MLPclf, X, y, cv=skf, scoring=score_metric)\n    print(\"Trying lr:\", conf['learning_rate_init'], \"momentum:\", conf['momentum'], \", got score:\", np.mean(score))\n    return 1 - np.mean(score) #Error metric","501541f5":"#Use gp and LCB\nsmac_LCB = optimizer('gp', cs, obj_function=MLP_score_1, \n                     acquisition_type='LCB', n_iter=25, \n                     init_points=5, seed=BIG_SEED)\n#plot the value for each iteration done\nplt.plot(1 - smac_LCB.get_X_y()[1])\nplt.ylabel(label_metric)\nplt.xlabel('Iterations')\nplt.show()","3300983e":"#Use gp and EI\nsmac_EI = optimizer('gp', cs, obj_function=MLP_score_1, \n                    acquisition_type='EI', n_iter=25, \n                    init_points=5, seed=BIG_SEED)\n#plot the value for each iteration done\nplt.plot(1 - smac_EI.get_X_y()[1])\nplt.ylabel(label_metric)\nplt.xlabel('Iterations')\nplt.show()","8641ceb3":"#Use gp and PI\nsmac_PI = optimizer('gp', cs, obj_function=MLP_score_1, \n                    acquisition_type='PI', n_iter=25, \n                    init_points=5, seed=BIG_SEED)\n#plot the value for each iteration done\nplt.plot(1 - smac_PI.get_X_y()[1])\nplt.ylabel(label_metric)\nplt.xlabel('Iterations')\nplt.show()","95cf63fe":"#Compare the two models removing the initial 5 points keeping only the best of them.\nplt.figure(figsize=(15,5))\nplt.plot(1-np.minimum.accumulate(smac_LCB.get_X_y()[1])[4:], 'o-') #0,1,2,3,4 sono i punti iniziali\nplt.plot(1-np.minimum.accumulate(smac_EI.get_X_y()[1])[4:], 'o-')\nplt.plot(1-np.minimum.accumulate(smac_PI.get_X_y()[1])[4:], 'o-')\nplt.legend(['LCB','EI','PI'])\nplt.ylabel('Best Seen '+label_metric)\nplt.xlabel('Iterations')\nplt.show()","e0544c35":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom time import time","e162320f":"#create the configuration grid with 5 element for each hyperparameter\nconf_grid = {'learning_rate_init':np.linspace(0.01, 0.1, num=5),\n             'momentum':np.linspace(0.1, 0.9, num=5)}\n\nMLP_GS = GridSearchCV(MLPClassifier(hidden_layer_sizes=(4,2,),random_state=BIG_SEED),\n                      conf_grid, scoring=score_metric, cv=10, n_jobs=-1) #n_jobs=-1 parallelize the computation\n\nt_0 = time()\nskf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\nX = df.drop('Class', axis=1).values\ny = df.Class\nMLP_GS = MLP_GS.fit(X, y)\nprint(MLP_GS.best_params_, \"->\", MLP_GS.best_score_)\nprint(\"Completed in %0.3fs\" % (time() - t_0))","df1e4b94":"print(\"Best Score\", np.max(MLP_GS.cv_results_['mean_test_score']))\nplt.plot(MLP_GS.cv_results_['mean_test_score'])\nplt.show()","c37b6da4":"param_grid = {'learning_rate_init':np.linspace(0.01, 0.1),\n              'momentum':np.linspace(0.1, 0.9)}\n\nMLP_RS = RandomizedSearchCV(MLPClassifier(hidden_layer_sizes=(4,2,), \n                                          random_state=BIG_SEED),\n                            param_grid, n_iter=25, scoring=score_metric, \n                            cv=10, random_state=BIG_SEED, n_jobs=-1)\n\nt_0 = time()\nskf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\nX = df.drop('Class', axis=1).values\ny = df.Class\nMLP_RS = MLP_RS.fit(X, y)\nprint(MLP_RS.best_params_, \"->\", MLP_RS.best_score_)\nprint(\"Completed in %0.3fs\" % (time() - t_0))","20052ea5":"print(\"Best Score\", np.max(MLP_RS.cv_results_['mean_test_score']))\nplt.plot(MLP_RS.cv_results_['mean_test_score'])\nplt.show()","0ac606cd":"plt.plot(np.maximum.accumulate(MLP_RS.cv_results_['mean_test_score']), 'o-')\nplt.plot(np.maximum.accumulate(MLP_GS.cv_results_['mean_test_score']), 'o-')\nplt.legend(['Random','Grid'])\nplt.ylabel('Best Seen '+label_metric)\nplt.xlabel('Iterations')\nplt.show()","e7b95b58":"plt.figure(figsize=(15,5))\niter_num = list(range(1,26))\nlcb, = plt.plot(iter_num, 1-np.minimum.accumulate(smac_LCB.get_X_y()[1]), 'o-')\nei,  = plt.plot(iter_num, 1-np.minimum.accumulate(smac_EI.get_X_y()[1]), 'o-')\npi,  = plt.plot(iter_num, 1-np.minimum.accumulate(smac_PI.get_X_y()[1]), 'o-')\nrs,  = plt.plot(iter_num, np.maximum.accumulate(MLP_RS.cv_results_['mean_test_score']), 'o-')\ngs,  = plt.plot(iter_num, np.maximum.accumulate(MLP_GS.cv_results_['mean_test_score']), 'o-')\nplt.legend([lcb,rs,ei, pi, gs],['LCB','Random','EI','PI','Grid']) #need to change order based on the score\nplt.ylabel('Best Seen '+label_metric)\nplt.xlabel('Iterations')\nplt.xlim(xmin=0.5, xmax=25.5)\nplt.xticks(iter_num)\nplt.show()","fbcc89a6":"best_1 = smac_LCB.get_X_y()[0][np.argmin(smac_LCB.get_X_y()[1])]","1655f08d":"from sklearn.model_selection import cross_validate\n\nMLPclf = MLPClassifier(hidden_layer_sizes=(4, 2),\n                      learning_rate_init=best_1[0], momentum=best_1[1],\n                      random_state=BIG_SEED)\n\nskf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\nscores = cross_validate(MLPclf, df.drop('Class', axis=1).values, df.Class, cv=skf,\n                        scoring=['f1','precision','recall','f1_macro','accuracy'])","5d866963":"print(\"F1:\\t\\t %0.3f\" % (np.mean(scores['test_f1'])))\nprint(\"Precision:\\t %0.3f\" % (np.mean(scores['test_precision'])))\nprint(\"Recall:\\t\\t %0.3f\" % (np.mean(scores['test_recall'])))\nprint(\"F1 Macro:\\t %0.3f\" % (np.mean(scores['test_f1_macro'])))\nprint(\"Accuracy:\\t %0.3f\" % (np.mean(scores['test_accuracy'])))","e1ba5c13":"best_1","b361c7c9":"from scipy.interpolate import griddata\nplt.figure(figsize=(15,10))\nbest_1 = smac_LCB.get_X_y()[0][np.argmin(smac_LCB.get_X_y()[1])]\nx = list(map(lambda x: x[0],smac_LCB.get_X_y()[0]))\ny = list(map(lambda x: x[1],smac_LCB.get_X_y()[0]))\nz = 1 - smac_LCB.get_X_y()[1]\n# define grid.\nxi = np.linspace(0.01,0.1,50)\nyi = np.linspace(0.1,0.9,50)\n# grid the data.\nzi = griddata((x, y), z, (xi[None,:], yi[:,None]), method='linear')\n# contour the gridded data, plotting dots at the randomly spaced data points.\nCS1 = plt.contour(xi,yi,zi,15,linewidths=0.5,colors='k')\nCS = plt.contourf(xi,yi,zi,15,cmap=plt.cm.Blues)\n#plt.colorbar()\nplt.clabel(CS1, inline=1, fontsize=12)\n\nplt.scatter(x, y, color='black')\nplt.scatter(best_1[0], best_1[1], color='red', marker='s', linewidths=6)\nplt.xlabel('learning rate')\nplt.ylabel('momentum')\nplt.title('Counter Plot')\n#plt.savefig('.\/imgs\/counter_weighted.png')\nplt.show()","568e2c20":"hidden1 = UniformIntegerHyperparameter('h1',1,5)\nhidden2 = UniformIntegerHyperparameter('h2',1,5)\n\ncs.add_hyperparameters([hidden1, hidden2])","7d19a644":"def MLP_score_2(conf: Union[Type[ConfigurationSpace], dict]) -> float:\n    \"\"\"\n    Basically takes a configuration does a 10-CV and return the average\n    score, the average score is modified so it gets minimized, for example\n    accuracy is converted into error.\n    PARAMETERS\n    ----------\n    @conf   is a ConfigurationSpace Object, but it acts as a dictionary\n            so we can call a value given the key: conf['key']\n            \n    Returns\n    -------\n    @return a float value which needs to be minimized in this case cv error\n    \n    This function takes in the configuration and computes the error score\n    using CV with 10 folds\n    \"\"\"\n    \n    MLPclf = MLPClassifier(hidden_layer_sizes=(conf['h1'],conf['h2'],), \n                           momentum = conf['momentum'],\n                           learning_rate_init = conf['learning_rate_init'],\n                           random_state=BIG_SEED)\n    \n    skf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\n    X = df.drop('Class', axis=1).values\n    y = df.Class\n    score = cross_val_score(MLPclf, X, y, cv=skf, scoring=score_metric)\n    #print(conf['learning_rate_init'], conf['momentum'], np.mean(score))\n    return 1 - np.mean(score)  # it needs to Minimize it!","7322ab28":"#Use rf and EI\nsmac_rf_EI = optimizer('rf', cs, obj_function=MLP_score_2, \n                    acquisition_type='EI', n_iter=110, \n                    init_points=10, seed=BIG_SEED)\n#plot the value for each iteration done\nplt.plot(1 - smac_rf_EI.get_X_y()[1])\nplt.ylabel(label_metric)\nplt.xlabel('Iterations')\nplt.show()","55195539":"#Use rf and LCB\nsmac_rf_LCB = optimizer('rf', cs, obj_function=MLP_score_2, \n                    acquisition_type='LCB', n_iter=110, \n                    init_points=10, seed=BIG_SEED)\n#plot the value for each iteration done\nplt.plot(1 - smac_rf_LCB.get_X_y()[1])\nplt.ylabel(label_metric)\nplt.xlabel('Iterations')\nplt.show()","cc74fa8b":"#Use rf and PI\nsmac_rf_PI = optimizer('rf', cs, obj_function=MLP_score_2, \n                    acquisition_type='PI', n_iter=110, \n                    init_points=10, seed=BIG_SEED)\n#plot the value for each iteration done\nplt.plot(1 - smac_rf_PI.get_X_y()[1])\nplt.ylabel(label_metric)\nplt.xlabel('Iterations')\nplt.show()","48b1c061":"#Compare the two models removing the initial 5 points keeping only the best of them.\nplt.figure(figsize=(15,5))\nplt.plot(1-np.minimum.accumulate(smac_rf_LCB.get_X_y()[1])[9:], 'o-') #0,1,2,3,4 sono i punti iniziali\nplt.plot(1-np.minimum.accumulate(smac_rf_EI.get_X_y()[1])[9:], 'o-')\nplt.plot(1-np.minimum.accumulate(smac_rf_PI.get_X_y()[1])[9:], 'o-')\nplt.legend(['LCB','EI', 'PI'])\nplt.ylabel('Best Seen '+label_metric)\nplt.xlabel('Iterations')\nplt.show()","aa7c3973":"#create the configuration grid with 5 element for each hyperparameter\n#We will obtain 125 iteration but during the plot we will plot only \n# the last 110 just so each model have the same lenght in terms of iterations.\nconf_grid = {'learning_rate_init':np.linspace(0.01, 0.1, num=5),\n             'momentum':np.linspace(0.1, 0.9, num=5),\n             'hidden_layer_sizes':[(1,1), (1,5), (3,3), (5,1), (5,5)]}\n\nMLP_GS = GridSearchCV(MLPClassifier(hidden_layer_sizes=(4,2,),random_state=BIG_SEED),\n                      conf_grid, scoring=score_metric, cv=10, n_jobs=-1)\n\nt_0 = time()\nskf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\nX = df.drop('Class', axis=1).values\ny = df.Class\nMLP_GS = MLP_GS.fit(X, y)\nprint(MLP_GS.best_params_, \"-> score\", MLP_GS.best_score_)\nprint(\"Completed in %0.3fs\" % (time() - t_0))","14b79de1":"hidden =[(i, j) for i in range(1,6) for j in range(1,6)]        \n        \nparam_grid = {'learning_rate_init':np.linspace(0.01, 0.1),\n              'momentum':np.linspace(0.1, 0.9),\n              'hidden_layer_sizes':hidden}\n\nMLP_RS = RandomizedSearchCV(MLPClassifier(hidden_layer_sizes=(4,2,), \n                                          random_state=BIG_SEED),\n                            param_grid, n_iter=110, scoring=score_metric, \n                            cv=10, random_state=BIG_SEED, n_jobs=-1)\n\nt_0 = time()\nskf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\nX = df.drop('Class', axis=1).values\ny = df.Class\nMLP_RS = MLP_RS.fit(X, y)\nprint(MLP_RS.best_params_, \"-> score\", MLP_RS.best_score_)\nprint(\"Completed in %0.3fs\" % (time() - t_0))","22533470":"plt.figure(figsize=(15,5))\nplt.plot(np.maximum.accumulate(MLP_RS.cv_results_['mean_test_score']), 'o-')\nplt.plot(np.maximum.accumulate(MLP_GS.cv_results_['mean_test_score']), 'o-')\nplt.legend(['Random','Grid'])\nplt.ylabel('Best Seen '+label_metric)\nplt.xlabel('Iterations')\nplt.show()","baa5c9b7":"plt.figure(figsize=(15,5))\niter_num = list(range(1,111))\nlcb, = plt.plot(iter_num, 1-np.minimum.accumulate(smac_rf_LCB.get_X_y()[1]), linewidth=4)\nei, = plt.plot(iter_num, 1-np.minimum.accumulate(smac_rf_EI.get_X_y()[1]),'*-', linewidth=3)\nrs, = plt.plot(iter_num, np.maximum.accumulate(MLP_RS.cv_results_['mean_test_score']), linewidth=2)\ngs, = plt.plot(iter_num, np.maximum.accumulate(MLP_GS.cv_results_['mean_test_score'])[15:], linewidth=2)\npi, = plt.plot(iter_num, 1-np.minimum.accumulate(smac_rf_PI.get_X_y()[1]), linewidth=2)\n\nplt.legend([ei,rs,pi,lcb,gs], ['EI','Random','PI','LCB','Grid'])\nplt.ylabel('Best Seen '+label_metric)\nplt.xlabel('Iterations')\nplt.xlim(xmin=0, xmax=111.5)\nplt.show()","d238882d":"best_2 = smac_rf_LCB.get_X_y()[0][np.argmin(smac_rf_LCB.get_X_y()[1])]","c29d12e9":"MLPclf = MLPClassifier(hidden_layer_sizes=(int(best_2[0]), int(best_2[1])),\n                      learning_rate_init=best_2[2], momentum=best_2[3],\n                      random_state=BIG_SEED)\n\nskf = StratifiedKFold(n_splits=10, random_state=BIG_SEED)\nscores = cross_validate(MLPclf, df.drop('Class', axis=1).values, df.Class, cv=skf,\n                        scoring=['f1','precision','recall','f1_macro','accuracy'])","a8f2c0ca":"print(\"F1:\\t\\t %0.3f\" % (np.mean(scores['test_f1'])))\nprint(\"Precision:\\t %0.3f\" % (np.mean(scores['test_precision'])))\nprint(\"Recall:\\t\\t %0.3f\" % (np.mean(scores['test_recall'])))\nprint(\"F1 Macro:\\t %0.3f\" % (np.mean(scores['test_f1_macro'])))\nprint(\"Accuracy:\\t %0.3f\" % (np.mean(scores['test_accuracy'])))","3f082589":"best_2","d98e2211":"For the GridSearch evenly distributed points are taken from the Configuration Domain (5 for each parameter), and RadomSearch searches random\npoints by itself.","456b17fa":"## Random and Grid Search","cfd78e51":"Let's see the best model found by LCB\nThe grid search performs ok because this is a small dimensional hyperparameter space (2 dimensions), as we increase the dimension the more difficult it gets.","2ce25e53":"## Grid and Randomized Search","2147e675":"The \u2018GP\u2019 surrogate model (SMAC4BO) is chosen for this part, and the acquisition functions \u2018EI\u2019, \u2018LCB\u2019 and \u2018PI\u2019 are used with 5 initial points, and\nthe iterations are set to 25(20 + 5 initial).\n\nThe MultiLayer perceptron classifier has two hidden layers with 4 and 2 neurons respectively and all the other configuration are the default one, except\nfor the seed which is fixed to have consistent results","e5202bc0":"Also in this case the we see the best model found using LCB.","efe14dd0":"## GP with LCB and EI","4b7b849f":"First of all, it is important to notice that maximing weighted accuracy the model achieved good performance also on accuracy and that the model doesn't follow the zero rule, now we will plot the results in a contour plot to see how he searched the space, the in the next section we will use more iterations to see if we can improve the model even more.","a4fc9a73":"## Random Forest with LBC and EI","c7c4813e":"Even though the weighted accuracy imporves the overall performance on the rare class is decreased, so using RF as a surrogate models even with more iterations doesn't yield better results in this case."}}