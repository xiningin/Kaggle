{"cell_type":{"d1b544c3":"code","3723740d":"code","6b57aa76":"code","e5fa11b9":"code","ec600675":"code","15393293":"code","a71312aa":"code","a8579ff4":"code","257ecdbc":"code","0b5b3599":"markdown","31b85d69":"markdown","f5a54f74":"markdown","5e4a7643":"markdown","be2b9c0b":"markdown","b88a3399":"markdown","7070edb9":"markdown"},"source":{"d1b544c3":"!git clone https:\/\/github.com\/FrancescoSaverioZuppichini\/A-journey-into-Convolutional-Neural-Network-visualization-.git\n!pip install efficientnet_pytorch==0.2.0","3723740d":"cd A-journey-into-Convolutional-Neural-Network-visualization-\/","6b57aa76":"from torchvision.models import *\nfrom visualisation.core.utils import device\nfrom efficientnet_pytorch import EfficientNet\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nfrom utils import *\nimport PIL.Image\nimport cv2\n\nfrom visualisation.core.utils import device \nfrom visualisation.core.utils import image_net_postprocessing\n\nfrom torchvision.transforms import ToTensor, Resize, Compose, ToPILImage\nfrom visualisation.core import *\nfrom visualisation.core.utils import image_net_preprocessing\n\n# for animation\n%matplotlib inline\nfrom IPython.display import Image\nfrom matplotlib.animation import FuncAnimation\nfrom collections import OrderedDict","e5fa11b9":"max_img = 5\npath = '\/kaggle\/input\/caltech256\/256_objectcategories\/256_ObjectCategories\/'\ninteresting_categories = ['009.bear','038.chimp','251.airplanes-101','158.penguin',\n                          '190.snake\/','024.butterfly','151.ostrich']\n\nimages = [] \nfor category_name in interesting_categories:\n    image_paths = glob.glob(f'{path}\/{category_name}\/*')\n    category_images = list(map(lambda x: PIL.Image.open(x), image_paths[:max_img]))\n    images.extend(category_images)\n\ninputs  = [Compose([Resize((224,224)), ToTensor(), image_net_preprocessing])(x).unsqueeze(0) for x in images]  # add 1 dim for batch\ninputs = [i.to(device) for i in inputs]","ec600675":"model_outs = OrderedDict()\nmodel_instances = [lambda i,pretrained:efficientnet(i,pretrained)]\n    \nmodel_names = ['EB' + str(i) for i in range(6)]\n\nimages = list(map(lambda x: cv2.resize(np.array(x),(224,224)),images)) # resize i\/p img\n\nfor i,name in enumerate(model_names):\n    module = EfficientNet.from_pretrained('efficientnet-b'+str(i)).to(device)\n    module.eval()\n\n    vis = GradCam(module, device)\n\n    model_outs[name] = list(map(lambda x: tensor2img(vis(x, None,postprocessing=image_net_postprocessing)[0]), inputs))\n    del module\n    torch.cuda.empty_cache()","15393293":"%%capture\n\n# create a figure with two subplots\nfig, (ax1, ax2, ax3, ax4, ax5, ax6,ax7) = plt.subplots(1,7,figsize=(20,20))\naxes = [ax2, ax3, ax4, ax5, ax6, ax7]\n    \ndef update(frame):\n    all_ax = []\n    ax1.set_yticklabels([])\n    ax1.set_xticklabels([])\n    ax1.text(1, 1, 'Orig. Im', color=\"white\", ha=\"left\", va=\"top\",fontsize=30)\n    all_ax.append(ax1.imshow(images[frame]))\n    for i,(ax,name) in enumerate(zip(axes,model_outs.keys())):\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])        \n        ax.text(1, 1, name, color=\"white\", ha=\"left\", va=\"top\",fontsize=20)\n        all_ax.append(ax.imshow(model_outs[name][frame], animated=True))\n\n    return all_ax\n\nani = FuncAnimation(fig, update, frames=range(len(images)), interval=1000, blit=True)\nmodel_names = [m.__name__ for m in model_instances]\nmodel_names = ', '.join(model_names)\nfig.tight_layout()\nani.save('..\/compare_arch.gif', writer='imagemagick') ","a71312aa":"Image('..\/compare_arch.gif')","a8579ff4":"cd ..","257ecdbc":"!rm -rf A-journey-into-Convolutional-Neural-Network-visualization-\/","0b5b3599":"# Installing dependencies","31b85d69":"# CAM Visualization","f5a54f74":"# Animating CAM","5e4a7643":"Recently Google AI Research published a paper titled \u201cEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u201d. In this paper the authors propose a new architecture which achieves state of the art classification accuracy on ImageNet while being 8.4x smaller and 6.1x faster on inference than the best existing CNN.\nIt achieves high level of acacuracy on many other datasets like CIFAR-100, Flowers and Cars. Good results on multiple dataset shows that the architecture can be used for transfer learning. \n\nIn this notebook, I try to compare the proposed efficient models EB0 to EB5. The pretrained model weights on imagenet dataset of EB6 and EB7 haven't been released yet. I use [GradCam](https:\/\/arxiv.org\/abs\/1610.02391) to highlight what different models are looking at.\n\nYou can find a very nice implementation of GradCam [here](https:\/\/github.com\/FrancescoSaverioZuppichini\/A-journey-into-Convolutional-Neural-Network-visualization-). I use the pretrained model weights provided [here](https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch#loading-pretrained-models) for visualization.","be2b9c0b":"# Import Packages","b88a3399":"# About EfficientNet\n\n\nA naive way to increase the performance of neural networks is to increase make CNN deeper. A great example, would be resnet ehich has several variations ranging from 18 to 202. Making the CNN deeper or wider may increase the performance but it comes at great computational cost. So we need some way to balance our ever increasing quest for performance with compuatational cost. In the paper, the authors propose a new model scaling method that uses a simple compound coefficient to scale up CNNs in a more structured manner. This method helps them to decide when to increase the depth or width of the network.\n\nThe authors wanted to optimize for accuracy and efficieny. So, they performed a neural architecture search. This search yielded th Efficient-B0 archictecture which looks pretty simple and straightforward to implement.\n\n![EfficientNet-B0 Architecture](https:\/\/1.bp.blogspot.com\/-DjZT_TLYZok\/XO3BYqpxCJI\/AAAAAAAAEKM\/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs\/s640\/image2.png)\n\nAs you can see from the performance graph, EfficientNet uses fewer parameters and achieves very high accuracy. For more details please [refer](https:\/\/arxiv.org\/abs\/1905.11946)\n\n![Effiecient Performance](https:\/\/1.bp.blogspot.com\/-oNSfIOzO8ko\/XO3BtHnUx0I\/AAAAAAAAEKk\/rJ2tHovGkzsyZnCbwVad-Q3ZBnwQmCFsgCEwYBhgL\/s640\/image3.png)","7070edb9":"# Loading Test Images\nI will be using images from Caltech 256 Image dataset. It has images from 257 different categories. Each category has around 120 images. I will be using images from some interesting categories. you can easily choose other categories for visualization if you want."}}