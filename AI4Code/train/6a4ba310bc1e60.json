{"cell_type":{"f0826095":"code","27575d70":"code","f14b80a3":"code","45ec784a":"code","943531b8":"code","e8b8a361":"code","e0804235":"code","8d40328e":"code","59a35008":"code","93a96fc2":"code","a5112c23":"code","2519ef12":"markdown","99f3f0bd":"markdown","ef61dbd7":"markdown","63c022a9":"markdown","406ab313":"markdown","8d29dfea":"markdown","bb628f21":"markdown","5861a0fe":"markdown","b20e22f7":"markdown","2ea87f70":"markdown","2640b881":"markdown","4573e287":"markdown","911b5577":"markdown","95a16c5d":"markdown","8e33ff02":"markdown","c7ced5bc":"markdown"},"source":{"f0826095":"import spacy\nnlp = spacy.load('en_core_web_sm')    #we are loading english ('en') core with size small ('sm')\n\n# we have something called sent_tokenize also to tokenize sentences. Give it a try!\nfrom nltk import word_tokenize\n\n# stopwords are those words which are filtered out before NLP processing.\n# can words like 'the', 'is' only be treated as stopwords? NO! Ever wondered how parental lock on sites work? It can have stop words like 'Sex', 'Porn', etc.\nfrom nltk.corpus import stopwords     \nimport string\npunctuations = string.punctuation","27575d70":"# NLTK implementation\nsent = \"The main challenge, is to start!\"\nstop = stopwords.words('english') + list(punctuations)    #removing unwanted punctuations also along with stopwords\nprint(\"NLTK implementation Result: \", [i for i in word_tokenize(sent) if i not in stop])\n\n# Spacy implementation\ndoc = nlp(sent)    # Create a Doc object\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nprint(\"Spacy implementation Result: \",[token.text for token in doc if token.text not in spacy_stopwords and token.text not in punctuations])","f14b80a3":"# NLTK implementation\nfrom nltk.stem import PorterStemmer\nporter = PorterStemmer()\nprint(\"NLTK implementation result: \",{\"running\": porter.stem(\"running\"),\"saw\": porter.stem(\"saw\"),\"troubling\": porter.stem(\"troubling\")})\n\n# Spacy implementation\n# It might be surprising to you but spaCy doesn't contain any function for stemming (AFAIK) as it relies on lemmatization only! ","45ec784a":"#NLTK implementation\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nprint(\"NLTK implementation result: \",wordnet_lemmatizer.lemmatize('saw',pos='v'))\n\n#Spacy implementation\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\nlemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\nlemmas = lemmatizer(u'saw', u'VERB')\nprint(\"Spacy implementation result: \", lemmas[0])","943531b8":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = TfidfVectorizer(stop_words=stop) #stop was defined initially using stopwords from NLTK\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X)","e8b8a361":"import re                                  #regular expression\nfrom collections import Counter            #creating frequency count dict\nimport heapq                               #for selecting n largest\nimport os","e0804235":"# we have uploaded a reference file which will provide access to correct spellings.\n# you can have similar file based on the domain you are working in. \n# say for restaurant related domain, you can have cities name, dishes name, cuisines as part of this file.\n\nos.listdir(\"..\/input\/\")","8d40328e":"def words(text): return re.findall(r'\\w+', text.lower())\n\nWORDS = Counter(words(open('..\/input\/big.txt').read()))\nWORDS.most_common(10)","59a35008":"def P(word, N=sum(WORDS.values())): \n    \"Probability of `word`.\"\n    return WORDS[word] \/ N\n\t\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    listProb = {word: P(word) for word in candidates(word)}\n    return listProb, max(candidates(word), key=P)\n\t\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\t\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)","93a96fc2":"def edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n","a5112c23":"def get_correct_word(word):\n    corrected_word = next(iter(correction(word)[0]))\n    print(\"Word passed: \", word, \" Word corrected To: \", corrected_word)\n    return corrected_word\n\nprint(get_correct_word('speling'))","2519ef12":"### 2(a) Lexical Processing","99f3f0bd":"Good Read: https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python","ef61dbd7":"#### Stemming\n\nStemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.","63c022a9":"One of the core task for any NLP project is to get all tokens(important words and much more(we'll discuss it later)) out. The term cat and cats gives the same context. Words like 'the', 'of' occur very frequently (stopwords). Let's see how to deal with them.","406ab313":"#### You can even use POSTMAN\n\n![Postman](https:\/\/i.imgur.com\/UWX2svt.png)","8d29dfea":"\n### 1. What is NLP?\n\n### 2. What are the major tasks involved in NLP?\n\n* **Lexical Processing**\n        * Stemming\n        * Lemmatization\n        * TF-IDF representation\n        * Edit Distance (Let's build a spelling corrector)\n\n* **Syntactic Processing**  (Will be covered in Part B)\n        * POS (Part of Speech) Tagging\n        * NER (Named Entity Recognition)\n        * Let's build a POS tagger\n\n* **Semantic Processing**  (Will be covered in Part C)\n        * LSA (Latent Semantic Analysis)\n        * Word2Vec Models\n        * Let's build a Sentiment Analysis App\n\n* **Other important applications** (Will be covered in Part D)\n        * Text Classification App\n        * Topic Modeling App\n        * Text Summarization App\n","bb628f21":"### Let's see how can we use this concept in creating a spell checker!","5861a0fe":"# 1. What is NLP?\n\nNatural language processing is a field which deals with the ability of a machine to understand, analyze and manipulate human language (*natural*). Ever wondered how Google is able to classify a mail as Spam? How auto-correct in Microsoft word works? Recently [grammarly](https:\/\/app.grammarly.com\/) is making some noise, wondered how? Answer is Natural Language Processing!","b20e22f7":"# 2. What are the major tasks involved in NLP?","2ea87f70":"### Let's see how to make a REST service for spell checker","2640b881":"from flask import Flask, request\n\nfrom flasgger import Swagger    *<font color='blue'>Fantastic library to generate documentation for your REST endpoints<\/font>*\n\nfrom spell_checker import check_word         *<font color='blue'>make sure you save spell checker related code in separate file say spell_checker.py and include it here.<\/font>*\n\n\napp = Flask(__name__)\n\nswagger = Swagger(app)\n\n\n#!\/usr\/bin\/env python3\n#-*- coding: utf-8 -*-\n\"\"\"\n@author: vjcalling\n\"\"\"\n\nfrom flask import Flask, request\n\nfrom flasgger import Swagger\n\nfrom spell_checker import check_word\n\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\n@app.route('\/check')\ndef get_correct_word():\n    \"\"\"Example endpoint returning corrected word post spell check\n    ---\n    parameters:\n      - name: word\n        in: query\n        type: string\n        required: true\n      \n    \"\"\"\n    word = request.args.get(\"word\")\n    correct_word = check_word(word)\n    return str(correct_word)\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n    \n","4573e287":"## <font color='green'>Now we can use this REST service in our application for spelling correction!<\/font>","911b5577":"#### Lemmatization\n\nLemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma.","95a16c5d":"#### Edit Distance\n\n\nEdit Distance (a.k.a. Levenshtein Distance) is a measure of similarity between two strings referred to as the source string and the target string.\n\n\nThe distance between the source string and the target string is the minimum number of edit operations (deletions, insertions, or substitutions) required to transform the source into the target. The lower the distance, the more similar the two strings. \n\nAmong the common applications of the Edit Distance algorithm are: **spell checking**, plagiarism detection, and translation memory systems.\n\n![Edit Distance](https:\/\/i0.wp.com\/python.gotrained.com\/wp-content\/uploads\/2018\/07\/edit-distant-1.png?w=221&ssl=1)\n\n\nSource: https:\/\/python.gotrained.com\/nltk-edit-distance-jaccard-distance\/","8e33ff02":"#### Using Flasgger For REST API testing\n\n\n![Flasgger](https:\/\/i.imgur.com\/vIGOZif.png)","c7ced5bc":"#### TF-IDF\n\nIn information retrieval, **tf\u2013idf or TFIDF**, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf\u2013idf is one of the most popular term-weighting schemes today; **83% of text-based recommender systems in digital libraries use tf\u2013idf**.\n\n[Source](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf)"}}