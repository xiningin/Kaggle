{"cell_type":{"665efb6d":"code","315eba30":"code","56902f8c":"code","fc66008e":"code","4f073aca":"code","88e10e40":"code","6a44c631":"code","de5279bd":"code","4592cd51":"code","950a9ceb":"code","92faf40b":"code","161b41e2":"code","d8b86f63":"code","370d15ef":"code","f42173a7":"code","a63dfc46":"code","e898e11a":"code","40e39c79":"code","019d2dda":"code","955e0a53":"code","acbad7ad":"code","f9a36499":"code","8e942acf":"markdown","9ee4f8cf":"markdown","4599960a":"markdown","81b392da":"markdown","961b53c9":"markdown","098f8a6a":"markdown","2f74212f":"markdown","2fceb174":"markdown","e1d699da":"markdown","6d005902":"markdown","c7ea9f55":"markdown","994d5fa2":"markdown"},"source":{"665efb6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","315eba30":"import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder","56902f8c":"train = pd.read_csv(\"\/kaggle\/input\/airbnb-recruiting-new-user-bookings\/train_users_2.csv.zip\")\ntest = pd.read_csv(\"\/kaggle\/input\/airbnb-recruiting-new-user-bookings\/test_users.csv.zip\")\ntrain.shape, test.shape","fc66008e":"train.head()","4f073aca":"# Before doing anything, let's get all the dates to date type\ndataset = pd.concat([train, test])\ndataset['date_account_created'] = pd.to_datetime(dataset['date_account_created'])\ndataset['timestamp_first_active'] = pd.to_datetime(dataset['timestamp_first_active'].astype('object'), format='%Y%m%d%H%M%S')\ndataset['date_first_booking'] = pd.to_datetime(dataset['date_first_booking'])\ndataset.head()","88e10e40":"sns.distplot(dataset['age'])","6a44c631":"dataset['signup_flow'] = dataset['signup_flow'].astype('object')\nobj = []\nfor col in dataset.drop(columns='id').columns:\n    if dataset[col].dtype == 'object':\n        obj.append(col)\nlen(obj)","de5279bd":"plt.figure(figsize=(12, 18))\nfor i, c in enumerate(obj):\n    plt.subplot(4,3,i+1)\n    sns.countplot(dataset[c])\n    plt.xticks(rotation=60)\nplt.tight_layout()    \nplt.show()","4592cd51":"missing_data = dataset.isnull().sum().sort_values(ascending=False)\nmissing_percent = missing_data\/len(dataset)\npd.DataFrame({'Count': missing_data, 'Percent': missing_percent})","950a9ceb":"# Too many missing values in \"date_first_booking\"\ndataset = dataset.drop(columns= ['date_first_booking', 'id'])","92faf40b":"# Seperate year, month, day from date columns\n\ndataset['year_account_created'] = pd.DatetimeIndex(dataset['date_account_created']).year\ndataset['month_account_created'] = pd.DatetimeIndex(dataset['date_account_created']).month\ndataset['day_account_created'] = pd.DatetimeIndex(dataset['date_account_created']).day\ndataset['year_first_active'] = pd.DatetimeIndex(dataset['timestamp_first_active']).year\ndataset['month_first_active'] = pd.DatetimeIndex(dataset['timestamp_first_active']).month\ndataset['day_first_active'] = pd.DatetimeIndex(dataset['timestamp_first_active']).day\ndataset['hour_first_active'] = pd.DatetimeIndex(dataset['timestamp_first_active']).hour\ndataset['minute_first_active'] = pd.DatetimeIndex(dataset['timestamp_first_active']).minute\n# dataset['year_first_booking'] = pd.DatetimeIndex(dataset['date_first_booking']).year\n# dataset['month_first_booking'] = pd.DatetimeIndex(dataset['date_first_booking']).month\n# dataset['day_first_booking'] = pd.DatetimeIndex(dataset['date_first_booking']).day\ndataset = dataset.drop(columns=['date_account_created', 'timestamp_first_active'])","161b41e2":"# \"country destination\" is the target, the missing data is all from test \n# only age and first_affiliate_tracked need to be addressed(as well as the info seperated from \"first booking\")\n# before dealing with age, remember we found that there were some outliers so let's remove\/modify the outliers first","d8b86f63":"age_values = dataset['age'].values\ndataset['age'] = np.where(np.logical_or(age_values>14, age_values<90), age_values, -1)\ndataset['first_affiliate_tracked'] = dataset['first_affiliate_tracked'].fillna(dataset['first_affiliate_tracked'].mode()[0])\n# dataset.isnull().sum().sum()","370d15ef":"dataset.head()","f42173a7":"data_dummy = pd.get_dummies(dataset.drop(columns='country_destination'))","a63dfc46":"df_train = data_dummy[:len(train)]\ny = train['country_destination']\ndf_test = data_dummy[len(train):]\ndf_train.head()","e898e11a":"le = LabelEncoder()\ny = le.fit_transform(y)","40e39c79":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom xgboost import XGBClassifier, plot_importance\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import learning_curve\nimport xgboost as xgb","019d2dda":"dtrain = xgb.DMatrix(df_train, y)\nparams = {\n    'objective': 'multi:softprob',              # So beneficial to multi classifications\n    'max_depth': 6,\n    'eval_metric': 'merror',                    # Multiclass classification error rate. It is calculated as #(wrong cases)\/#(all cases)\n    'learning_rate': 0.3,                       # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html this link includes all the params and the options\n    'colsample_bytree': 0.3,                    # of each params\n    'subsample': 0.5,\n    'num_class': len(pd.Series(y).value_counts().index)\n}\nres = xgb.cv(params, dtrain, num_boost_round=50, nfold=5, early_stopping_rounds=1, verbose_eval=1, show_stdv=True)","955e0a53":"clf = xgb.train(dtrain=dtrain, params=params, num_boost_round=res['test-merror-mean'].idxmin())","acbad7ad":"plot_importance(clf, max_num_features=10)","f9a36499":"dtest = xgb.DMatrix(df_test)\ny_pred = clf.predict(dtest)\nids = []\npred = []\nfor i in range(len(test)):\n    ids += [test['id'][i]]*5\n    pred += list(le.inverse_transform(y_pred[i].argsort()[::-1][:5]))\nsub = pd.DataFrame({'id': ids, 'country': pred})\nsub.to_csv('sub.csv', index=False)","8e942acf":"**Label Encode y**","9ee4f8cf":"# Data Explotary","4599960a":"# Data Preparation","81b392da":"* Some of the charts have less than 6 categories, they can be transformed by get_dummies directly\n* Some of the charts have too many categories, further process is needed","961b53c9":"**Missing Values**","098f8a6a":"# Modeling ","2f74212f":"**One Hot Encoding**","2fceb174":"**Export Result**","e1d699da":"* we can see that there are many outliers which are alive longer than turtles, removing them is next step","6d005902":"# **1. Data Explotary**\n# **2. Data Preparation**\n            * Missing Values\n            * One Hot Encoding\n            * LabelEncode Goal Data\n# **3. Modeling**\n            * Optimize module\n            * Feature Importance\n            * ExPORT Result","c7ea9f55":"**Feature Importance**","994d5fa2":"* At the very beginning, I wanted to use different modules to get the best one or ones, but the goal has too many classifications which makes my intention hard to accomplish.\n* So I chose xgboost which is able to cross validate and train multi\u2014classification data."}}