{"cell_type":{"afec4a31":"code","43799977":"code","948a9d05":"code","5d1dadad":"code","80609ca8":"code","99ac66a6":"code","edfdc030":"code","48491df5":"code","52562172":"code","5e18ea0b":"code","54c023e2":"code","819ab6bd":"code","4180365e":"code","f898441d":"code","39117876":"code","3b5658a9":"code","a5d6f7ab":"code","ae8f2f40":"code","8850ac95":"code","fb17b319":"code","6a854177":"code","0ad116ea":"code","bbce8ee6":"code","99668562":"code","4132464e":"code","4c6947fb":"code","94723cbd":"code","4f0605f6":"code","00257014":"code","63c55bee":"code","d1b12754":"code","43ccffdc":"markdown","888e33a6":"markdown","c08c434d":"markdown","1e1d29a9":"markdown","e9800c0a":"markdown","d3e4c06c":"markdown","1e81d3fe":"markdown","7ed14100":"markdown","4c60eac6":"markdown"},"source":{"afec4a31":"!pip install torchsummary","43799977":"from torch import nn \nfrom torch import optim\nimport torchvision.transforms as transforms\nimport torch, torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchsummary import summary\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom matplotlib import pyplot as plt\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split","948a9d05":"df = pd.read_csv('\/kaggle\/input\/makeup-lips-segmentation-28k-samples\/set-lipstick-original\/list.csv')\ndf.head(5)","5d1dadad":"IMAGES_PATH = '\/kaggle\/input\/makeup-lips-segmentation-28k-samples\/set-lipstick-original\/720p\/'\nMASKS_PATH = '\/kaggle\/input\/makeup-lips-segmentation-28k-samples\/set-lipstick-original\/mask'","80609ca8":"imgs_set = set(os.listdir(IMAGES_PATH))\nmasks_set = set(os.listdir(MASKS_PATH))\n\nimgs_set = set(''.join(filter(lambda x: x.isdigit(), i)) for i in imgs_set)\nmasks_set = set(''.join(filter(lambda x: x.isdigit(), i)) for i in masks_set)","99ac66a6":"len(imgs_set), len(masks_set)","edfdc030":"len(imgs_set.difference(masks_set)), len(masks_set.difference(imgs_set))","48491df5":"not_mask = imgs_set.difference(masks_set)\n\nnot_mask = [f'image{i}.jpg' for i in not_mask]\nnot_mask","52562172":"df = df.loc[~df['filename'].isin(not_mask)]\ndf.reset_index(drop=True, inplace=True)","5e18ea0b":"df.head(5)","54c023e2":"idx = np.random.randint(len(df))\n\nsample = df.iloc[idx]\n\nimg_path = os.path.join(IMAGES_PATH, sample['filename'])\nmask_path = os.path.join(MASKS_PATH, sample['mask'])\n\nimg = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_RGB2BGR)\nmask = cv2.imread(mask_path)[:, :, 1]\n\nplt.imshow(img)\nplt.show()\n\nplt.imshow(mask, cmap='gray')\nplt.show()","819ab6bd":"class LipsSegmentationDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __getitem__(self, idx):\n        data = self.df.iloc[idx]\n        \n        img_path = os.path.join(IMAGES_PATH, data['filename'])\n        mask_path = os.path.join(MASKS_PATH, data['mask'])\n        \n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_RGB2BGR)\n        mask = cv2.imread(mask_path)[:, :, 1]\n        \n        if self.transform:\n            img, mask = self.transform(img, mask)\n            \n        return img, mask\n    \n    def __len__(self):\n        return len(self.df)","4180365e":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","f898441d":"train_df.head(5)","39117876":"test_df.head(5)","3b5658a9":"transform = A.Compose([\n    A.RandomBrightnessContrast(),\n    A.HorizontalFlip(),\n    A.Rotate(limit=90, border_mode=0, value=0),\n    A.GaussNoise(var_limit=(50.0, 100.0)),\n    A.ChannelShuffle(),\n    A.OpticalDistortion(), \n    A.GridDistortion(distort_limit=(-0.15, 0.15), border_mode=0, value=0), \n    A.Transpose()\n])\n\nto_tensor = transforms.ToTensor()\n\ndef tensor_img_mask(img, mask, shape=(256, 256)):\n    img, mask = cv2.resize(img, shape), cv2.resize(mask, shape)\n    return to_tensor(img), to_tensor(mask)\n\ndef transform_img_mask(img, mask, shape=(256, 256)):\n    img, mask = cv2.resize(img, shape), cv2.resize(mask, shape)\n    transformed = transform(image=img, mask=mask)\n    img, mask = transformed['image'], transformed['mask']\n    img, mask = to_tensor(img), to_tensor(mask)\n    return img, mask","a5d6f7ab":"train_dataset = LipsSegmentationDataset(train_df, transform=transform_img_mask)\ntest_dataset = LipsSegmentationDataset(test_df, transform=tensor_img_mask)","ae8f2f40":"idx = int(np.random.random() * len(train_dataset))\n\nimg, mask = train_dataset[idx]\n\nplt.imshow(img.permute(1, 2, 0))\nplt.show()\n\nplt.imshow(mask.permute(1, 2, 0), cmap='gray')\nplt.show()","8850ac95":"class EncoderBlock(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, batchnorm=True):\n        super().__init__()\n        \n        self.batchnorm=batchnorm\n        \n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n\n        self.activation = nn.LeakyReLU(0.1)\n        \n        if self.batchnorm:\n            self.batchnorm = nn.BatchNorm2d(\n                num_features=out_channels\n            )\n        \n        \n    def forward(self, x):\n        \n        x = self.conv(x)\n        x = self.activation(x)\n\n        if self.batchnorm:\n            x = self.batchnorm(x)\n        \n        return x\n    \n    \nclass DecoderBlock(nn.Module):\n    \n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.upconv = nn.ConvTranspose2d(\n            in_channels=in_channels, \n            out_channels=out_channels, \n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n        \n        \n        self.activation = nn.LeakyReLU(0.1)\n\n        self.batchnorm = nn.BatchNorm2d(\n            num_features=out_channels\n        )\n        \n        \n    def forward(self, x, skip_in):\n        \n        x = self.upconv(x)\n        x = x + skip_in\n        x = self.activation(x)\n        \n        x = self.batchnorm(x)\n        \n        return x","fb17b319":"class LinkNet(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        # (3, 256, 256) -> (32, 128, 128)\n        self.e1 = EncoderBlock(\n            in_channels=3, \n            out_channels=32\n        )\n        \n        # (32, 128, 128) -> (64, 64, 64)\n        self.e2 = EncoderBlock(\n            in_channels=32, \n            out_channels=64\n        )\n        \n        # (64, 64, 64) -> (128, 32, 32)\n        self.e3 = EncoderBlock(\n            in_channels=64, \n            out_channels=128\n        )\n        \n        # (128, 32, 32) -> (256, 16, 16)\n        self.e4 = EncoderBlock(\n            in_channels=128, \n            out_channels=256\n        )\n        \n        # (256, 16, 16) -> (512, 8, 8)\n        self.e5 = EncoderBlock(\n            in_channels=256, \n            out_channels=512\n        )\n        \n        # bottleneck (512, 8, 8) -> (512, 4, 4)\n        self.b = nn.Conv2d(\n            in_channels=512,\n            out_channels=512,\n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n        \n        # (512, 4, 4) -> (512, 8, 8)\n        self.d1 = DecoderBlock(\n            in_channels=512, \n            out_channels=512\n        )\n        \n        # (512, 8, 8) -> (256, 16, 16)\n        self.d2 = DecoderBlock(\n            in_channels=512, \n            out_channels=256\n        )\n        \n        # (256, 16, 16) -> (128, 32, 32)\n        self.d3 = DecoderBlock(\n            in_channels=256, \n            out_channels=128\n        )\n        \n        # (128, 32, 32) -> (64, 64, 64)\n        self.d4 = DecoderBlock(\n            in_channels=128, \n            out_channels=64\n        )\n        \n        # (64, 64, 64) -> (32, 128, 128)\n        self.d5 = DecoderBlock(\n            in_channels=64, \n            out_channels=32\n        )\n        \n        # output (32, 128, 128) -> (1, 256, 256)\n        self.o = nn.ConvTranspose2d(\n            in_channels=32, \n            out_channels=1, \n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n        \n    def forward(self, x):\n        \n        e1 = self.e1(x)\n        e2 = self.e2(e1)\n        e3 = self.e3(e2)\n        e4 = self.e4(e3)\n        e5 = self.e5(e4)\n        \n        b = self.b(e5)\n        b = F.relu(b)\n        \n        d1 = self.d1(b, e5)\n        d2 = self.d2(d1, e4)\n        d3 = self.d3(d2, e3)\n        d4 = self.d4(d3, e2)\n        d5 = self.d5(d4, e1)\n        \n        output = self.o(d5)\n        output = F.sigmoid(output)\n        \n        return output","6a854177":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","0ad116ea":"linknet = LinkNet().to(device)","bbce8ee6":"summary(linknet, input_size=(3, 256, 256), batch_size=8, device='cuda')","99668562":"class DiceLoss(nn.Module):\n    \n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, logits, targets):\n        smooth = 1\n        num = targets.size(0)\n        probs = logits\n        m1 = probs.reshape(num, -1)\n        m2 = targets.reshape(num, -1)\n        intersection = (m1 * m2)\n\n        score = (2. * intersection.sum(1) + smooth) \/ (m1.sum(1) + m2.sum(1) + smooth)\n        score = 1 - (score.sum() \/ num)\n        return score","4132464e":"train_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=8, \n    shuffle=True, \n    num_workers=8\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset=test_dataset,\n    batch_size=8, \n    shuffle=False, \n    num_workers=8\n)","4c6947fb":"criterion = DiceLoss()\noptimizer = torch.optim.Adam(linknet.parameters(), lr=0.001)","94723cbd":"import time\n\n\ndef draw_imgs(img, mask, pred_mask):\n    \n    fig, ax = plt.subplots(1, 3, figsize=(17, 5))\n\n    ax[0].imshow(img.cpu().permute(1, 2, 0))\n    ax[0].set_title('Image')\n\n    ax[1].imshow(mask.cpu().permute(1, 2, 0), cmap='gray')\n    ax[1].set_title('Mask')\n\n    ax[2].imshow(pred_mask.detach().cpu().permute(1, 2, 0), cmap='gray')\n    ax[2].set_title('Predicted Mask')\n\n    plt.show()\n\n\ndef evaluate_model(model, loader, device, criterion):\n    loss = 0.\n    model.eval()\n    with torch.no_grad():\n        for data, label in loader:\n            data, label = data.to(device), label.to(device)\n\n            output = model(data)\n            loss += criterion(output, label).item()\n    \n    val_loss = loss \/ len(loader)\n    \n    print(\n        f'- val_loss: {val_loss:.4f}', \n        end='\\n')\n    \n    idx = np.random.randint(len(data))\n    draw_imgs(data[idx], label[idx], output[idx])\n    \n    model.train()\n    return loss\/len(loader)\n\n\ndef epoch_training(model, criterion, optimizer, train_set, device):\n    \n    running_loss = 0.0\n    \n    n_steps = len(train_set)\n    steps_n_signs = len(str(n_steps))\n    \n    for i, data in enumerate(train_set):\n        start_time = time.time()\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        epoch_loss = running_loss \/ (i + 1)\n        \n        step_duration = time.time() - start_time\n        duration = step_duration * n_steps\n\n        print(\n            '\\r' \\\n            f'step [{str(i + 1).zfill(steps_n_signs)}\/{n_steps}] ' \\\n            f'- loss: {epoch_loss:.4f} ',\n            end='', flush=True)\n    \n    return epoch_loss\n\n\ndef model_training(model, criterion, optimizer, epochs, device, train_set, valid_set=None):\n    history = {\n        'loss': []\n    }\n    if valid_set:\n        history['val_loss'] = []\n    \n    for epoch in range(epochs):\n\n        model.train()\n        start_time = time.time()\n        \n        print(\n            f'Epoch {epoch + 1}\/{epochs}', \n            end='\\n')\n        \n        epoch_loss = epoch_training(model, criterion, optimizer, train_set, device)\n        \n        duration = round(time.time() - start_time)\n        \n        if valid_set:\n            val_loss = evaluate_model(model, valid_set, device, criterion)\n            history['val_loss'].append(val_loss)\n        \n        history['loss'].append(epoch_loss)\n        \n        print(\n            '\\n' \\\n            f'duration: {duration}s ({round((duration\/len(train_set)) * 1000)}ms\/step)\\n', \n            end='\\n')\n\n    print('Training is finished!')\n    \n    return history","4f0605f6":"history = model_training(linknet, \n                         criterion, \n                         optimizer, \n                         epochs=7, \n                         device=device, \n                         train_set=train_loader, \n                         valid_set=test_loader)","00257014":"optimizer.param_groups[0]['lr'] \/= 10","63c55bee":"history = model_training(linknet, \n                         criterion, \n                         optimizer, \n                         epochs=3, \n                         device=device, \n                         train_set=train_loader, \n                         valid_set=test_loader)","d1b12754":"indices = np.random.randint(len(test_dataset), size=10)\n\nlinknet.eval()\nwith torch.no_grad():\n    for idx in indices:\n        img, mask = test_dataset[idx]\n        pred_mask = linknet(img[np.newaxis, :, :, :].to(device))\n\n        draw_imgs(img, mask, pred_mask[0, ...])","43ccffdc":"\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438, \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0438 \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a.","888e33a6":"\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0441\u0435\u0442\u044c \u0441 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043e\u0439 **LinkNet**.","c08c434d":"\u0415\u0441\u0442\u044c 54 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435\u0442 \u043c\u0430\u0441\u043e\u043a. \u0423\u0431\u0435\u0440\u0435\u043c \u0438\u0445 \u0438\u0437 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430.","1e1d29a9":"\u0417\u0430\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441\u044b \u0431\u043b\u043e\u043a\u043e\u0432, \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0431\u0443\u0434\u0435\u043c \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0441\u0435\u0442\u044c.","e9800c0a":"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c **Dice**.","d3e4c06c":"**EncoderBlock:**\n\n* \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0439 \u0441\u043b\u043e\u0439 (\u044f\u0434\u0440\u043e (4, 4), \u0448\u0430\u0433 (2, 2), padding (1, 1));\n* \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438 LeakyReLU (\u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0437\u0430\u043d\u0443\u043b\u044f\u0442\u044c \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f);\n* \u0441\u043b\u043e\u0439 BatchNormalization.\n\n\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0431\u043b\u043e\u043a\u0430 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0442\u0435\u043d\u0437\u043e\u0440 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\u044e:\n\noutput_shape = (n_samples, n_channels, input_width\/2, input_height\/2)","1e81d3fe":"Augmentation","7ed14100":"**DecoderBlock:**\n\n* \u0441\u043b\u043e\u0439 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u0441\u0432\u0435\u0440\u0442\u043a\u0438 (\u044f\u0434\u0440\u043e (4, 4), \u0448\u0430\u0433 (2, 2), padding (1, 1));\n* \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438 LeakyReLU (\u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0437\u0430\u043d\u0443\u043b\u044f\u0442\u044c \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f);\n* \u0441\u043b\u043e\u0439 BatchNormalization.\n\n\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0431\u043b\u043e\u043a\u0430 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0442\u0435\u043d\u0437\u043e\u0440 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\u044e:\n\noutput_shape = (n_samples, n_channels, input_width * 2, input_height * 2)","4c60eac6":"\u0423\u043c\u0435\u043d\u044c\u0448\u0438\u043c learning rate."}}