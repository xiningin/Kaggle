{"cell_type":{"f4ba3e0e":"code","c30e85e8":"code","365c950d":"code","d18753de":"code","cb90cba0":"code","410cc588":"code","40173ca6":"code","6027ce2a":"code","7dd6375b":"code","ba5da8ef":"code","7c43fc5b":"code","a38acadd":"code","f969c3a3":"code","9e83e377":"code","4e5e9bec":"code","5be31a68":"code","000046d3":"code","6e06d5da":"code","dcbd3a24":"code","d0c60e01":"code","66fc539a":"markdown","7e3e7bad":"markdown","0a523596":"markdown","54e838a4":"markdown","10e48314":"markdown","69d93f32":"markdown","9e17b115":"markdown","618fa28c":"markdown"},"source":{"f4ba3e0e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom random import sample\nimport time\n\nimport os\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Concatenate\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers.merge import concatenate\n\nfrom sklearn.model_selection import train_test_split\nfrom textblob import TextBlob","c30e85e8":"PATH = '\/kaggle\/input\/jigsaw-toxic-severity-rating\/'\nvalid_data = pd.read_csv(PATH + 'validation_data.csv')\ncomment_data = pd.read_csv(PATH + 'comments_to_score.csv')\nsub = pd.read_csv(PATH + 'sample_submission.csv')","365c950d":"valid_data.sort_values('worker', inplace=True)\nvalid_data.head()","d18753de":"valid_data.values.shape","cb90cba0":"txteg = valid_data.values[0,2] # get text example from more_toxic\nvalid_data[valid_data['less_toxic']==txteg].head() # look for example in less_toxic","410cc588":"training_data = pd.read_csv(\"..\/input\/jigsaw-rate-severity-text-augmentation\/jigsaw_rate_severity_training_data.csv\")\ntoxic_text = training_data['text'].values\ntarget = training_data['target'].values","40173ca6":"print(\"Text list length: \", len(toxic_text))\nprint(\"Target list length: \", len(target))","6027ce2a":"plt.hist(target, label='training target distribution');\nplt.legend();","7dd6375b":"MAX_LENGTH = 512\n\n# tokenize the sentences\ntokenizer = Tokenizer(lower=True)\ntokenizer.fit_on_texts(toxic_text)\n\ntext_seq = tokenizer.texts_to_sequences(toxic_text)\n\n# pad the sequences\ntext_vec = pad_sequences(text_seq, maxlen=MAX_LENGTH)\n\ntext_vec.shape","ba5da8ef":"print('Number of Tokens:', len(tokenizer.word_index))","7c43fc5b":"x_input = Input(shape=(MAX_LENGTH,))\n\nx = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100,)(x_input)\n\n#x = LSTM(units=128, return_sequences=True)(x)\n#x = Dropout(0.2)(x)\n\nx = LSTM(units=64, return_sequences=False)(x)\nx = Dropout(0.2)(x)\n\nx = Dense(64, activation='relu')(x)\nx = Dropout(0.25)(x)\n\noutputs = Dense(1)(x)\n\nmodel = Model(inputs=x_input, outputs=outputs)\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025))","a38acadd":"tf.keras.utils.plot_model(\n    model,\n    to_file=\"model.png\",\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n)","f969c3a3":"# Model hyperparameters \nBATCH_SIZE = 256\nEPOCHS = 20\n\n# model drive\ncp_file = '.\/lstm_model.h5'\ncp = ModelCheckpoint(cp_file, \n                     monitor='loss', \n                     verbose=0, \n                     save_best_only=True, mode='min')\n\nes = EarlyStopping(patience=3, \n                   monitor='loss', \n                   #restore_best_weights=True, \n                   mode='min', \n                   verbose=1)\n\n# model train\nhistory = model.fit(text_vec, target,\n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS,\n                    validation_split=0.1,\n                    callbacks=[es, cp],\n                    shuffle=True,\n                    )","9e83e377":"pd.DataFrame(history.history).plot(figsize=(12, 6));","4e5e9bec":"test_ids = comment_data['comment_id']\ntest_text = comment_data['text']\n\ntest_text_seq = tokenizer.texts_to_sequences(test_text)\n\n# pad the sequences\ntest_text_vec = pad_sequences(test_text_seq, maxlen=MAX_LENGTH)","5be31a68":"test_length = len(test_text_vec)\n\npreds = model.predict(test_text_vec)","000046d3":"plt.hist(preds, label='test prediction distribution');\nplt.legend();","6e06d5da":"sub['score'] = preds\nsub['score'] = sub['score'].rank(method='first')","dcbd3a24":"sub.to_csv('submission.csv', index=False)","d0c60e01":"sub","66fc539a":"# Jigsaw Rate Severity - Simple LSTM\n\n**Work:**\n - Forked https:\/\/www.kaggle.com\/elcaiseri\/jigsaw-keras-embedding-lstm\n - Revised data prep and model architecture to run with single input (text) and get single score (relative severity of toxicity)\n     - Target is created by using the (less) and (more) information to assign a value that adheres to all (less) and (more) information\n - Revised optimizer and manually tuned learning rate for better performance\n - Added text augmentation\n\n**References and Acknowledgements:**\n - https:\/\/www.kaggle.com\/elcaiseri\/jigsaw-keras-embedding-lstm\n - https:\/\/www.kaggle.com\/elcaiseri\n - https:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating\/overview\n - https:\/\/github.com\/tensorflow\/tensorflow\/issues\/38613\n - https:\/\/www.kaggle.com\/yeayates21\/commonlit-text-augmentation-eng-to-fre-to-eng\/notebook","7e3e7bad":"### Text Preprocessing for Deep Learning","0a523596":"## Quick EDA\n\nCan text be found more than once in either column?  - Answer: Yes","54e838a4":"## Data Wrangling","10e48314":"## Get Training Data\n\n - Data created here: https:\/\/www.kaggle.com\/yeayates21\/jigsaw-rate-severity-text-augmentation\/notebook\n - Creating data outside notebook reduces runtime and allows for internet access for language augmentation","69d93f32":"## Model","9e17b115":"## Prediction","618fa28c":"## Imports"}}