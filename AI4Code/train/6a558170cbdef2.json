{"cell_type":{"b8cc8716":"code","4b25534f":"code","ce7ef946":"code","6c7dccd5":"code","cb843f6e":"code","0c446b9f":"code","8d97b746":"code","e0ac02f1":"code","c6d3de83":"code","7bde4733":"code","97a3d173":"code","035b4c30":"code","07f85128":"code","869a9871":"code","87594f30":"code","ac9f2a30":"code","4be212ad":"code","2b9a2cd2":"code","8dfda592":"code","1386e015":"code","05acb1bf":"code","446d93c7":"code","cb889eaf":"code","f0358886":"code","884c4923":"code","39af8132":"code","36384506":"code","b6fccc75":"code","888ee4e7":"code","0040ced8":"code","d09de997":"code","e3c930fa":"code","f516e1ab":"code","4c38f30b":"code","1612b329":"code","3b4d60a3":"markdown","97ab4906":"markdown","ea39af9d":"markdown","3550c43c":"markdown","bfd35d61":"markdown","8ca2e6f1":"markdown","a8ae27aa":"markdown","2384fc58":"markdown","836ffea7":"markdown","f9d46a97":"markdown","00ccbf36":"markdown","b2633156":"markdown"},"source":{"b8cc8716":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4b25534f":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\nplt.rcParams['figure.figsize'] = (8, 6)\nplt.rcParams['font.size'] = 14","ce7ef946":"bikes=pd.read_csv(\"..\/input\/bikeshare.csv\", index_col='datetime', parse_dates=True)\nbikes.head()","6c7dccd5":"# \"count\" is a function, so to avoid  confusion we change the column name to total\nbikes.rename(columns={'count':'total'}, inplace=True)","cb843f6e":"bikes_data=bikes.copy()","0c446b9f":"print(bikes_data.shape)","8d97b746":"bikes_data.describe()","e0ac02f1":"# To check Multicollinearity \n\nbikes_data.corr()","c6d3de83":"# scatter plot\na=sns.lmplot(x='temp', y='total', fit_reg=True, data=bikes_data, aspect=1.5, scatter_kws={'alpha':0.2})\n","7bde4733":"# exploring more features\nfeature_cols = ['temp', 'season', 'weather', 'humidity']","97a3d173":"# multiple scatter plots in Seaborn\nsns.pairplot(bikes_data, x_vars=feature_cols, y_vars='total', kind='reg')","035b4c30":"# box plot of rentals, grouped by season\nbikes.boxplot(column='total', by='season')","07f85128":"# create dummy variables\nseason_dummies = pd.get_dummies(bikes_data.season, prefix='season')\n\n# print 5 random rows\nseason_dummies.sample(n=5, random_state=12)","869a9871":"# drop the first column\nseason_dummies.drop(season_dummies.columns[0], axis=1, inplace=True)\n\n# print 5 random rows\nseason_dummies.sample(n=5, random_state=12)","87594f30":"# concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns)\nbikes_data = pd.concat([bikes_data, season_dummies], axis=1)\n\n# print 5 random rows\nbikes_data.sample(n=5, random_state=12)","ac9f2a30":"# include dummy variables for season in the model\nfeature_cols = ['temp', 'season_2', 'season_3', 'season_4', 'humidity']\nX = bikes_data[feature_cols]  # input or independent variable \ny = bikes_data.total          # Output or Dependent variable\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nlist(zip(feature_cols, linreg.coef_))","4be212ad":"# splitting the data into training and test data.\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=12)","2b9a2cd2":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8dfda592":"# Buliding the Linear model with the algorithm\nlin_reg=LinearRegression()\nmodel=lin_reg.fit(X_train,y_train)","1386e015":"# feature_cols = ['temp', 'season_2', 'season_3', 'season_4', 'humidity'] #Input or independent variable\nprint(model.intercept_)\nprint (model.coef_)","05acb1bf":"## Predicting the x_test with the model\npredicted=model.predict(X_test)","446d93c7":"print ('MAE:', metrics.mean_absolute_error(y_test, predicted))\nprint ('MSE:', metrics.mean_squared_error(y_test, predicted))\nprint ('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predicted)))","cb889eaf":"# ** To measure accuracy of model the model generated RMSE value has to be lower than null RMSE** \n\n#Compute null RMSE\n# split X and y into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=12)\n\n# create a NumPy array with the same shape as y_test\ny_null = np.zeros_like(y_test, dtype=float)\n\n# fill the array with the mean value of y_test\ny_null.fill(y_test.mean())\ny_null","f0358886":"print(y_test.shape)\nprint(y_null.shape)","884c4923":"# compute null RMSE\nnp.sqrt(metrics.mean_squared_error(y_test, y_null))","39af8132":"# define a function that accepts a list of features and returns testing RMSE\ndef train_test_rmse(feature_cols):\n    X = bikes_data[feature_cols]\n    y = bikes_data.total\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))","36384506":"# compare different sets of features\nprint (train_test_rmse(['temp', 'season', 'weather', 'humidity']))\nprint (train_test_rmse(['temp', 'season', 'weather']))\nprint (train_test_rmse(['temp', 'season', 'humidity']))\nprint (train_test_rmse(['temp', 'humidity']))\nprint (train_test_rmse(['temp', 'season_2', 'season_3', 'season_4','weather', 'humidity']))\nprint (train_test_rmse(['temp', 'season_2', 'season_3', 'season_4','weather']))\nprint (train_test_rmse(['temp', 'season_2', 'season_3', 'season_4', 'humidity']))","b6fccc75":"bikes_data['hour']=bikes_data.index.hour","888ee4e7":"bikes_data.head()","0040ced8":"# hour as a categorical feature\nhour_dummies = pd.get_dummies(bikes_data.hour, prefix='hour')\nhour_dummies.drop(hour_dummies.columns[0], axis=1, inplace=True)\nbikes_data = pd.concat([bikes_data, hour_dummies], axis=1)\n#hour_dummies\nbikes_data.head()","d09de997":"# with hour.\nsns.factorplot(x=\"hour\",y=\"total\",data=bikes_data,kind='bar',size=5,aspect=1.5)","e3c930fa":"# hour as a categorical feature\nhour_dummies = pd.get_dummies(bikes_data.hour, prefix='hour')\nhour_dummies.drop(hour_dummies.columns[0], axis=1, inplace=True)\nbikes_data = pd.concat([bikes_data, hour_dummies], axis=1)\n#hour_dummies\nbikes_data.head()","f516e1ab":"# daytime as a categorical feature\nbikes_data['daytime'] = ((bikes_data.hour > 6) & (bikes_data.hour < 21)).astype(int)\nbikes_data.tail()","4c38f30b":"print (train_test_rmse(['hour']))\nprint (train_test_rmse(bikes_data.columns[bikes_data.columns.str.startswith('hour_')]))\nprint (train_test_rmse(['daytime']))","1612b329":"print('END')","3b4d60a3":"Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n\nThe data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this notebook, we try to combine historical usage patterns with season data in order to forecast bike rental demand in the Bikeshare program in Washington, D.C.\n","97ab4906":"When we look at the RMSE value of # 1 & 3 model, we have a slight decrease or no impact in value of RMSE with a drop of variable weather. So, it indicate, weather is highly correlated with other features or variables. So, we can drop  any variable among them (temp, season, weather,)  to achieve better RMSE value. \nThe RMSE reduces further with the dummy variable. So the lowest RMSE value is for Model 7 and it is a best model among all and remaining can be ignored.  ","ea39af9d":"\nTo interpret the season coefficients? It is measured against the baseline (spring):\n\nHolding all other features fixed, summer is associated with a rental decrease of 3.39 bikes compared to the spring.\n\nHolding all other features fixed, fall is associated with a rental decrease of 41.7 bikes compared to the spring.\n\nHolding all other features fixed, winter is associated with a rental increase of 64.4 bikes compared to the spring.\n\nWould it matter if we changed which season was defined as the baseline?\n\nNo, it would simply change our interpretation of the coefficients.\n\n**Important: Dummy encoding is relevant for all machine learning models, not just linear regression models.**","3550c43c":"The max bikes rental are during season 2  & 3 (summer & fall) but having winter more bike rental than spring is strange. The reason for such ambiguity is transition period of season is not defined in the dataset. When winter or season is starting and ending. Therefore, the values which need to be calculated in spring ends up in winter. Hence, potrays there are more rentals in the winter than the spring, but only because the system is experiencing overall growth and the winter months happen to come before the spring months.","bfd35d61":"It is noticed from the graph that season is showing some unsual trend. The number of bikes rental are high during winter than spring, which is unusual to accept that to in washington where we have adverse weather in winter (heavy snow). So go further with boxplot to dig more insight into it.","8ca2e6f1":"From the graph, we can see that the number of rental bikes increases as the temp increases. From the correlation matrix, it is observed that temp and total having the positive correlation.","a8ae27aa":"\ntemp and atemp are highly correlated, So having both of them in regression model lead to multicollinearity issue. Therefore, we will drop one of the variable.\n\nCategorical variables (season, holiday, workingday, weather) need chi square test, so we will not describe them and read only numerical values. \n\nWhereas, total is nothing but the combination of  casual & registered. So, we can ignore casual & registered because our dependent variable or output is 'total'. \n\n\n","2384fc58":"\nThe number of bikes rented on a average is 191.5 bikes.But due to large variation in min and max values of bikes rental lead to a high standard deviation. ","836ffea7":"However, we actually only need **three dummy variables (not four)**, and thus we'll drop the first dummy variable.\n\nWhy? Because three dummies captures all of the \"information\" about the season feature, and implicitly defines spring (season 1) as the **baseline level:**","f9d46a97":" Looking at the rmse value, the lowest rmse value is for model#2 with hour_dummies. whereas the 'daytime' is the second best this could be due to our understanding of daytime from >6 & <21 is not same as the bikers thought prcess. Maybe bikers are considering different time zone classification. ","00ccbf36":"**Handling categorical features**\n\nscikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n\nOrdered categories: transform them to sensible numeric values (example: small=1, medium=2, large=3)\n\nUnordered categories: use dummy encoding (0\/1)\n\nWhat are the categorical features in our dataset?\n\nOrdered categories: weather (already encoded with sensible numeric values)\n\nUnordered categories: season (needs dummy encoding), holiday (already dummy encoded), workingday (already dummy encoded)\n\nFor season, we can't simply leave the encoding as 1 = spring, 2 = summer, 3 = fall, and 4 = winter, because that would imply an ordered relationship. Instead, we create multiple dummy variables:","b2633156":"We  can see the the bike rentals are high during the morning hours between 7 to 9 am and similary between 5 to 6 pm in the evening. The main reason for this will be the office hours, where professionals try to beat the traffic with affordable transportation. In addition the weather condition are normal during morning & evening hours compare to day or night time. "}}