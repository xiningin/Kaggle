{"cell_type":{"8b1fa93a":"code","bde0be2a":"code","48d2565b":"code","26d5616e":"code","6e295445":"code","eb7d046d":"code","38c8895b":"code","c57f8124":"code","4abfb401":"code","5d714361":"code","f882055a":"code","f2944c4a":"code","7c9d7a0e":"code","ddaa8115":"code","c587990d":"code","b149a89e":"code","9cb56bd2":"code","6490cdbc":"code","804b804c":"code","41166bbd":"code","b7d492e4":"code","ce6cd7b6":"code","2ae697d0":"code","f91fa785":"code","3a49fafd":"code","f3676020":"code","7ea78bc7":"code","655688fe":"code","c0ff628f":"code","3c0b13ad":"code","d53936fb":"code","d8360d03":"markdown","787f5b44":"markdown","6b3f8d63":"markdown","01b3c301":"markdown","9c37f821":"markdown","fccfa208":"markdown","b429b3ce":"markdown","40a99cfe":"markdown","97b6ef8d":"markdown","46a0e507":"markdown","fab049c9":"markdown","f3c5d73f":"markdown","e494947c":"markdown","512cdd5d":"markdown"},"source":{"8b1fa93a":"import pandas as pd\nimport os.path as osp","bde0be2a":"INPUT_PATH = '\/kaggle\/input\/'","48d2565b":"!unzip -n \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip -d .\/ \n!unzip -n \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip -d .\/\n!unzip -n \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip -d .\/","26d5616e":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ntest_labels = pd.read_csv('test_labels.csv')","6e295445":"train.head()","eb7d046d":"test.head()","38c8895b":"test_labels.head()","c57f8124":"test_labels.describe()","4abfb401":"test = test.merge(test_labels, on=\"id\")\ntest = test[test.toxic != -1]\ntest.head()","5d714361":"test.describe()","f882055a":"toxic_comment = pd.concat([train, test])","f2944c4a":"toxic_comment.describe()","7c9d7a0e":"!ls \/kaggle\/input\/jigsaw-unintended-bias-in-toxicity-classification\/","ddaa8115":"!cp \/kaggle\/input\/jigsaw-unintended-bias-in-toxicity-classification\/all_data.csv .\/unintended.csv","c587990d":"unintended = pd.read_csv('unintended.csv')","b149a89e":"unintended.columns","9cb56bd2":"unintended.head()","6490cdbc":"unintended.columns","804b804c":"target_columns = [\n    \"id\", \"comment_text\", \"toxicity\", \"severe_toxicity\", \n    \"obscene\", \"identity_attack\", \"insult\", \"threat\"\n]\nunintended = unintended[target_columns]","41166bbd":"unintended.describe()","b7d492e4":"base_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nintended_columns = [\"toxicity\", \"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\"]","ce6cd7b6":"unintended = unintended.rename(columns = {x: y for x, y in zip(intended_columns, base_columns)})","2ae697d0":"print('length of jigsaw-toxic-comment-classification-challenge \\'s dataset :', len(toxic_comment))","f91fa785":"print('length of jigsaw-unintended-bias-in-toxicity-classification \\'s dataset :', len(unintended))","3a49fafd":"toxic_comment['dataset'] = 'toxic_comment'\nunintended['dataset'] = 'unintended'","f3676020":"final = pd.concat([toxic_comment, unintended])\nfinal.head()","7ea78bc7":"import pandas as pd\nimport numpy as np\nimport copy\nimport re\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom nltk import WordNetLemmatizer\n\n\nclass BaseTokenizer(object):\n    def process_text(self, text):\n        raise NotImplemented\n\n    def process(self, texts):\n        for text in texts:\n            yield self.process_text(text)\n\n\nRE_PATTERNS = {\n    ' american ':\n        [\n            'amerikan'\n        ],\n    ' adolf ':\n        [\n            'adolf'\n        ],\n    ' hitler ':\n        [\n            'hitler'\n        ],\n    ' fuck':\n        [\n            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n            'feck ', ' fux ', 'f\\*\\*', \n            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n        ],\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n    ' ass hole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n        ],\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n        ],\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n    ' trans gender':\n        [\n            'transgender'\n        ],\n    ' gay ':\n        [\n            'gay'\n        ],\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k'\n        ],\n    ' suck ':\n        [\n            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n        ],\n    ' cunt ':\n        [\n            'cunt', 'c u n t'\n        ],\n    ' bull shit ':\n        [\n            'bullsh\\*t', 'bull\\$hit'\n        ],\n    ' homo sex ual':\n        [\n            'homosexual'\n        ],\n    ' jerk ':\n        [\n            'jerk'\n        ],\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n                                                                                      'i d i o t'\n        ],\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n    ' shit ':\n        [\n            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n        ],\n    ' shit hole ':\n        [\n            'shythole'\n        ],\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n    ' rape ':\n        [\n            ' raped'\n        ],\n    ' dumb ass':\n        [\n            'dumbass', 'dubass'\n        ],\n    ' ass head':\n        [\n            'butthead'\n        ],\n    ' sex ':\n        [\n            'sexy', 's3x', 'sexuality'\n        ],\n    ' nigger ':\n        [\n            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n        ],\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n        ],\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n    ' mother fucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker',\n        ],\n    ' whore ':\n        [\n            'wh\\*\\*\\*', 'w h o r e'\n        ],\n}\n\n\nclass PatternTokenizer(BaseTokenizer):\n    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", patterns=RE_PATTERNS,\n                 remove_repetitions=True):\n        self.lower = lower\n        self.patterns = patterns\n        self.initial_filters = initial_filters\n        self.remove_repetitions = remove_repetitions\n\n    def process_text(self, text):\n        x = self._preprocess(text)\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                x = re.sub(pat, target, x)\n        x = re.sub(r\"[^a-z' ]\", ' ', x)\n        return x.split()\n\n    def process_ds(self, ds):\n        ### ds = Data series\n\n        # lower\n        ds = copy.deepcopy(ds)\n        if self.lower:\n            ds = ds.str.lower()\n        # remove special chars\n        if self.initial_filters is not None:\n            ds = ds.str.replace(self.initial_filters, ' ')\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n            ds = ds.str.replace(pattern, r\"\\1\")\n\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                ds = ds.str.replace(pat, target)\n\n        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n\n        return ds.str.split()\n\n    def _preprocess(self, text):\n        # lower\n        if self.lower:\n            text = text.lower()\n\n        # remove special chars\n        if self.initial_filters is not None:\n            text = re.sub(self.initial_filters, ' ', text)\n\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n            text = pattern.sub(r\"\\1\", text)\n        return text","655688fe":"tokenizer = PatternTokenizer()\nfinal[\"comment_text_processed\"] = tokenizer.process_ds(final[\"comment_text\"]).str.join(sep=\" \")","c0ff628f":"!rm *.csv","3c0b13ad":"final.head()","d53936fb":"final.to_csv('all_in_one_jigsaw.csv')","d8360d03":"It's a final dataset. (toxic_comment + unintended)","787f5b44":"### [Reference : https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data)\n\nYou are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\n* `toxic`\n* `severe_toxic`\n* `obscene`\n* `threat`\n* `insult`\n* `identity_hate`\n\nYou must create a model which predicts a probability of each type of toxicity for each comment.\n\nFile descriptions  \n* **train.csv** - the training set, contains comments with their binary labels\n* **test.csv** - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n* **sample_submission.csv** - a sample submission file in the correct format\n* **test_labels.csv** - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n","6b3f8d63":"<p style = \"font-size:25px; \nfont-family: Helvetica; \nfont-weight : normal; \nbackground-color: #036EB7; \ncolor : #FFFFFF; \ntext-align: left; \npadding: 0px 15px; \nborder-radius:3px\">\n    Merge\n<\/p>","01b3c301":"<p style = \"font-size:25px; \nfont-family: Helvetica; \nfont-weight : normal; \nbackground-color: #036EB7; \ncolor : #FFFFFF; \ntext-align: right; \npadding: 0px 15px; \nborder-radius:3px\">\n    Final!!!\n<\/p>","9c37f821":"<p style = \"font-size:25px; \nfont-family: Helvetica; \nfont-weight : normal; \nbackground-color: #036EB7; \ncolor : #FFFFFF; \ntext-align: left; \npadding: 0px 15px; \nborder-radius:3px\">\n    [Toxic Comment Classification Challenge] Data\n<\/p>","fccfa208":"### Reference : [https:\/\/www.kaggle.com\/fizzbuzz\/toxic-data-preprocessing](https:\/\/www.kaggle.com\/fizzbuzz\/toxic-data-preprocessing)","b429b3ce":"Cool!","40a99cfe":"My goal is creating all-in-one dataset for jigsaw competition.  \nFirst, I concatenated **'toxic comment classification challenge'**s dataset, **'jigsaw unintended bias in toxicity classification'**s dataset.   \nIf I find more new effective dataset, I'll update this notebook.  ","97b6ef8d":"<p style = \"font-size:25px; \nfont-family: Helvetica; \nfont-weight : normal; \nbackground-color: #036EB7; \ncolor : #FFFFFF; \ntext-align: left; \npadding: 0px 15px; \nborder-radius:3px\">\n    Toxic Data Preprocessing\n<\/p>","46a0e507":"<p style = \"font-size:25px; \nfont-family: Helvetica; \nfont-weight : normal; \nbackground-color: #036EB7; \ncolor : #FFFFFF; \ntext-align: left; \npadding: 0px 15px; \nborder-radius:3px\">\n   [Jigsaw Unintended Bias in Toxicity Classification] Data\n<\/p>","fab049c9":"There are some -1 labels. Delete that.","f3c5d73f":"We need `all_data.csv`.","e494947c":"<p style = \"font-size:40px; \nfont-family: Helvetica; \nfont-weight : bold; \nbackground-color: #036EB7; \ncolor : #FFFFFF; \ntext-align: left; \npadding: 0px 15px; \nborder-radius:3px\">\n\tJigsaw All-in-One Dataset\n<\/p>","512cdd5d":"### [Reference : https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/data](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/data)\n\nAt the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n\nIn the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).\n\nThe data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are:\n\n* `severe_toxicity`\n* `obscene`\n* `threat`\n* `insult`\n* `identity_attack`\n* `sexual_explicit`"}}