{"cell_type":{"10291d62":"code","0f2cc809":"code","c133e24f":"code","03f99cd4":"code","56f0cb18":"code","b8a1b101":"code","3c07e21e":"code","f4ac6839":"code","54af11e3":"code","0f6dec8f":"code","8226421f":"code","b2ec9ed2":"code","6f279689":"code","1b2b6c65":"code","c6fa6259":"code","f76f1975":"code","ced91e9b":"code","9b281ec3":"code","4bb8742a":"code","0fbbfcfa":"code","59caf695":"code","95e4f62e":"code","23b9bcee":"code","1b7081f2":"code","40421201":"code","aee06ece":"code","2aed30eb":"code","e7dd024f":"code","bf81d5ff":"code","64f46f27":"code","a8a6dd67":"code","44965b1c":"code","3456035b":"code","9673ebd7":"code","08b6949d":"code","be96ac3a":"code","27e81b27":"code","398432fc":"code","e767c0d8":"code","53b3ed3c":"code","4a87e62b":"markdown","43d80758":"markdown","33568662":"markdown","89293d66":"markdown","7f654c3a":"markdown","63b7c84b":"markdown","75586ec1":"markdown","95dc00b0":"markdown","b5a1995a":"markdown","25b079ad":"markdown","3b121c8f":"markdown","b76e130b":"markdown","727d2f42":"markdown"},"source":{"10291d62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f2cc809":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","c133e24f":"df=pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","03f99cd4":"df","56f0cb18":"df.info()","b8a1b101":"df.describe().transpose()","3c07e21e":"plt.figure(figsize=(12,8))\nsns.countplot('Class',data=df)\nplt.xlabel('Class')\nplt.ylabel('frequency')","f4ac6839":"X=df.drop('Class',axis=1)\ny=df['Class']\nprint('shape of X',X.shape)\nprint('shape of y',y.shape)","54af11e3":"(df['Class'].value_counts())","0f6dec8f":"fraud_data=df[df['Class']==1]\nnormal_data=df[df['Class']==0]\nprint('shape of fraud data',fraud_data.shape)\nprint('shape of normal data',normal_data.shape)","8226421f":"from imblearn.under_sampling import NearMiss","b2ec9ed2":"sampling=NearMiss()\nX_new,y_new=sampling.fit_sample(X,y)\nprint('shape of X after undersampling',X_new.shape)\nprint('shape of y after undersampling',y_new.shape)","6f279689":"from collections import Counter\nprint('before undersampling: {}'.format(Counter(y)))\nprint('after undersampling: {}'.format(Counter(y_new)))\n","1b2b6c65":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n","c6fa6259":"# import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier","f76f1975":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n   ","ced91e9b":"model_rdc=RandomForestClassifier(n_jobs=-1)","9b281ec3":"parameters={'max_depth':[3,5,7,10,None],\n           'n_estimators':[100,200,300,400,500],\n           'max_features':randint(1,13),\n           'criterion':['gini','entropy'],\n           'bootstrap':[True,False],\n           'min_samples_leaf':randint(1,5)}\n           ","4bb8742a":"def hyperparameter_tuning(model_rdc,parameters,n_of_itern,X_train,y_train):\n    random_search=RandomizedSearchCV(estimator=model_rdc,\n                                    param_distributions=parameters,\n                                    n_jobs=-1,\n                                     n_iter=n_of_itern,\n                                     cv=9)\n    random_search.fit(X_train,y_train)\n    params=random_search.best_params_\n    score=random_search.best_score_\n    return params,score\nfinal_params,final_score=hyperparameter_tuning(model_rdc,parameters,40,X_train,y_train)\n#this is our final best parameters for random forest classifier\nprint(final_params)\n# final accuracy with tuned parameters\nprint(final_score)\n","0fbbfcfa":"from sklearn.metrics import classification_report,confusion_matrix","59caf695":"model_rdc=RandomForestClassifier(n_estimators=100,\n                             criterion='gini',\n                             max_depth=10,\n                             max_features=8,\n                             min_samples_leaf=1,\n                             bootstrap=True)\nmodel_rdc.fit(X_train,y_train)\npred=model_rdc.predict(X_test)\nprint(classification_report(pred,y_test))\nprint('\\n')\nprint(confusion_matrix(pred,y_test))\n","95e4f62e":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout","23b9bcee":"X_train.shape","1b7081f2":"model=Sequential()\n\nmodel.add(Dense(30,activation='relu'))\nmodel.add(Dropout(rate=0.2))\n\nmodel.add(Dense(30,activation='relu'))\nmodel.add(Dropout(rate=0.2))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","40421201":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop=EarlyStopping(monitor='Val_loss',mode='min',verbose=1,patience=5)","aee06ece":"model.fit(x=X_train,\n            y=y_train,\n            epochs=100,\n            callbacks=[early_stop],\n            validation_data=(X_test,y_test))","2aed30eb":"losses_df=pd.DataFrame(model.history.history)","e7dd024f":"losses_df","bf81d5ff":"losses_df[['loss','val_loss']].plot()","64f46f27":"losses_df[['accuracy','val_accuracy']].plot()\n#nrural network is not working good ","a8a6dd67":"from imblearn.combine import SMOTETomek","44965b1c":"smk=SMOTETomek(random_state=42)\nX_new_o,y_new_o=smk.fit_sample(X,y)\nprint('shape of X after oversampling',X_new_o.shape)\nprint('shape of y after oversampling',y_new_o.shape)","3456035b":"from collections import Counter\nprint('before undersampling: {}'.format(Counter(y)))\nprint('after undersampling: {}'.format(Counter(y_new_o)))","9673ebd7":" from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_new_o, y_new_o, test_size=0.3, random_state=42)","08b6949d":"X_train.shape","be96ac3a":"from sklearn.linear_model import LogisticRegression","27e81b27":"lr=LogisticRegression()","398432fc":"lr.fit(X_train,y_train)\npred=lr.predict(X_test)\nprint(classification_report(y_test,pred))","e767c0d8":"from sklearn.neighbors import KNeighborsClassifier","53b3ed3c":"\nk=4\nmodel=KNeighborsClassifier(n_neighbors=k)\nmodel.fit(X_train,y_train)\npred=model.predict(X_test)\nprint(classification_report(pred,y_test))","4a87e62b":"# **Randomized SearchCV**","43d80758":"# **Dealing the data with Undersampling technique**","33568662":"# **Dealing The data With Oversampling technique**","89293d66":"# **Neural Network**","7f654c3a":"# **Hyperparameter Tuning**","63b7c84b":"# **Logistic Regression**","75586ec1":"# **KNN Classifier**","95dc00b0":"# **splitting**","b5a1995a":"# **Random Forest Classifier**","25b079ad":"# **Dependent and Independent Variables**","3b121c8f":"# **So we have now balanced dataset**","b76e130b":"# **Splitting**","727d2f42":"# **We can see Data is highly Imbalanced**"}}