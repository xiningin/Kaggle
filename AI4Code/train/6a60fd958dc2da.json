{"cell_type":{"53332faf":"code","181a0589":"code","861a269a":"code","930c9029":"code","0a0ffd33":"code","09881370":"code","f9b6f106":"code","66fea8e4":"code","45890294":"code","15211148":"code","0a4c2f62":"code","507c8637":"code","48187548":"code","6e3e5345":"markdown","f804a2fd":"markdown"},"source":{"53332faf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","181a0589":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom keras.models import Sequential\nfrom keras.layers import Dense","861a269a":"df = pd.read_csv(\"..\/input\/reddit-machine-learning-and-data-science\/machine_learning_and_data_science_subreddit_data.csv\")","930c9029":"df.head()","0a0ffd33":"df.isna().sum() # Checking for 'Nan' values if any","09881370":"del df['id']\ndel df['link_flair_text']\ndel df['edited']","f9b6f106":"df.head()","66fea8e4":"df.title.fillna(\" \",inplace = True)\ndf.body.fillna(\" \",inplace = True)","45890294":"df.title.value_counts()","15211148":"df.over_18.replace(True,1,inplace = True)\ndf.over_18.replace(False,0,inplace = True)","0a4c2f62":"df.over_18.value_counts()","507c8637":"df.shape","48187548":"x = df[:32639]\ntrain_true = x[x.over_18 == 1.0].body\nplt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(train_true)))\nplt.imshow(wc,interpolation = 'bilinear')","6e3e5345":"## Importing The Necessary Libraries","f804a2fd":"## Loading the Dataset"}}