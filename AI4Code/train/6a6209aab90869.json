{"cell_type":{"761b0a7f":"code","986634ee":"code","dd1b5a12":"code","fdd46c0c":"code","c675ac42":"code","a55b6608":"code","a8488a77":"code","6429b7ba":"code","3049b80c":"code","f12ffda2":"code","1c1bff92":"code","7e456fb7":"code","a1ceb19d":"code","f3cd0ab8":"code","f8d37f84":"code","2b6f85d1":"code","e2bdc231":"code","76ea1f8c":"code","539d2379":"code","59d64b9c":"code","914aadf7":"code","75c3de11":"code","a3e669be":"code","0047ab5e":"code","7b1ff9cb":"code","dbb1c722":"code","3872dec1":"code","bbda2f7e":"code","3463cb8b":"code","ea383562":"code","b9ecf8a4":"code","f98c3a53":"code","f1bfba14":"code","9891e7ea":"code","ca444ace":"code","c126400d":"code","c41b5050":"code","4edfaf6b":"code","9f46c3d4":"code","3e294875":"code","87755566":"code","2e9b034e":"code","81e3e64d":"code","edd22904":"code","48a5425d":"code","4de7f11f":"code","3b16efa7":"code","2c94d4cc":"code","a0b64c7b":"code","6038405c":"code","73e5472c":"code","5a331d42":"code","5c49fb3c":"code","ee3b73f0":"code","428207c3":"code","67792d56":"code","dd4fa8cc":"code","c765bd25":"code","171887d4":"code","ccf21c5b":"code","048a7103":"code","58bbe663":"code","4b379099":"code","46a11ddd":"code","be7fa05f":"code","d17a4273":"code","209ecabb":"code","0ecd2039":"code","6d7f06cb":"code","d80231c4":"code","4c56d6f6":"code","0d0ab0ce":"code","62c28ac8":"code","06a608c6":"code","5eb69e3e":"code","3494c092":"code","62a6766a":"code","11660aa8":"code","76b23448":"code","88a09073":"code","8532eb5c":"code","db34f681":"markdown","efd3615c":"markdown","e5358aed":"markdown","7a7b7851":"markdown","a4f490ad":"markdown","25897d8b":"markdown","9a2ef77f":"markdown","3d254d29":"markdown","e5ab89a6":"markdown","3be7088e":"markdown","b61f18f5":"markdown","27406348":"markdown","115e7eab":"markdown","db80d796":"markdown","64685c89":"markdown","c7e347a3":"markdown","0373221b":"markdown","bbd2ba00":"markdown","fea85c83":"markdown","778b692b":"markdown","7927e28d":"markdown","197f0f1c":"markdown","530932b3":"markdown","8890c495":"markdown","2a4113c8":"markdown","3d4f8274":"markdown","c48f6f83":"markdown","8697ffce":"markdown","dcfef520":"markdown","ddaa7492":"markdown","029c1efb":"markdown","9b1d73ab":"markdown","b7842a43":"markdown","92cf7284":"markdown","99bea9e5":"markdown","66fc0587":"markdown","d9cf9529":"markdown","30a80af9":"markdown","b4d00c0c":"markdown","cea3fc8c":"markdown","5b722d86":"markdown","6ab902ae":"markdown","6e172261":"markdown","ee0a7765":"markdown","116bbc54":"markdown","99be78e2":"markdown","8dcb0384":"markdown","1d65c145":"markdown","ae130fd6":"markdown","0f24bf89":"markdown","f11f2513":"markdown","4042ae6a":"markdown","ca71c6b9":"markdown","1e3404c3":"markdown","89ad383c":"markdown","92371870":"markdown","ad5f1ce4":"markdown","2c5d72cc":"markdown","5ff498c5":"markdown"},"source":{"761b0a7f":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 105)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","986634ee":"# setting the number of cross validations used in the Model part \nnr_cv = 5\n\n# switch for using log values for SalePrice and features     \nuse_logvals = 1    \n# target used for correlation \ntarget = 'SalePrice_Log'\n    \n# only columns with correlation above this threshold value  \n# are used for the ML Regressors in Part 3\nmin_val_corr = 0.4    \n    \n# switch for dropping columns that are similar to others already used and show a high correlation to these     \ndrop_similar = 1\n","dd1b5a12":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","fdd46c0c":"def print_cols_large_corr(df, nr_c, targ) :\n    corr = df.corr()\n    corr_abs = corr.abs()\n    print (corr_abs.nlargest(nr_c, targ)[targ])","c675ac42":"def plot_corr_matrix(df, nr_c, targ) :\n    \n    corr = df.corr()\n    corr_abs = corr.abs()\n    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n    cm = np.corrcoef(df[cols].values.T)\n\n    plt.figure(figsize=(nr_c\/1.5, nr_c\/1.5))\n    sns.set(font_scale=1.25)\n    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values\n               )\n    plt.show()","a55b6608":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\n\n#df_train = pd.read_csv(\"..\/input\/train.csv\")\n#df_test = pd.read_csv(\"..\/input\/test.csv\")","a8488a77":"print(df_train.shape)\nprint(\"*\"*50)\nprint(df_test.shape)","6429b7ba":"print(df_train.info())\nprint(\"*\"*50)\nprint(df_test.info())","3049b80c":"df_train.head()","f12ffda2":"df_train.describe()","1c1bff92":"df_test.head()","7e456fb7":"df_test.describe()","a1ceb19d":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","f3cd0ab8":"df_train['SalePrice_Log'] = np.log1p(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log']);\n# skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())\n# dropping old column\ndf_train.drop('SalePrice', axis= 1, inplace=True)","f8d37f84":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","2b6f85d1":"print(df_train[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train[categorical_feats].columns)","e2bdc231":"df_train[numerical_feats].head()","76ea1f8c":"df_train[categorical_feats].head()","539d2379":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","59d64b9c":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('None',inplace=True)\n    df_test[col].fillna('None',inplace=True)","914aadf7":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","75c3de11":"# fillna with mean or mode for the remaining values\ndf_train.fillna(df_train.mean(), inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)\ndf_train.fillna(df_train.mode(), inplace=True)\ndf_test.fillna(df_test.mode(), inplace=True)","a3e669be":"df_train.isnull().sum().sum()","0047ab5e":"df_test.isnull().sum().sum()","7b1ff9cb":"for col in numerical_feats:\n    print(col)\n    print(\"Skewness: %f\" % df_train[col].skew())\n    print(\"Kurtosis: %f\" % df_train[col].kurt())\n    print(\"*\"*50)","dbb1c722":"sns.distplot(df_train['GrLivArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea'].kurt())","3872dec1":"sns.distplot(df_train['LotArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea'].kurt())","bbda2f7e":"for df in [df_train, df_test]:\n    df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n    df.drop('GrLivArea', inplace= True, axis = 1)\n    df['LotArea_Log'] = np.log(df['LotArea'])\n    df.drop('LotArea', inplace= True, axis = 1)\n    \n    \n    \nnumerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n   ","3463cb8b":"sns.distplot(df_train['GrLivArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea_Log'].kurt())","ea383562":"sns.distplot(df_train['LotArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea_Log'].kurt())","b9ecf8a4":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['Id', 'SalePrice', 'SalePrice_Log']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target])\n            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","f98c3a53":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, target)[target]\n\ncols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\ncols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)","f1bfba14":"print(ser_corr)\nprint(\"*\"*30)\nprint(\"List of numerical features with r above min_val_corr :\")\nprint(cols_abv_corr_limit)\nprint(\"*\"*30)\nprint(\"List of numerical features with r below min_val_corr :\")\nprint(cols_bel_corr_limit)\n","9891e7ea":"for catg in list(categorical_feats) :\n    print(df_train[catg].value_counts())\n    print('#'*50)","ca444ace":"li_cat_feats = list(categorical_feats)\nnr_rows = 15\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=df_train, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()   ","c126400d":"catg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', \n                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncatg_weak_corr = ['Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                  'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', \n                  'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', \n                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', \n                  'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                  'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', \n                  'SaleCondition' ]\n      ","c41b5050":"nr_feats = len(cols_abv_corr_limit)","4edfaf6b":"plot_corr_matrix(df_train, nr_feats, target)","9f46c3d4":"id_test = df_test['Id']\n\nto_drop_num  = cols_bel_corr_limit\nto_drop_catg = catg_weak_corr\n\ncols_to_drop = ['Id'] + to_drop_num + to_drop_catg \n\nfor df in [df_train, df_test]:\n    df.drop(cols_to_drop, inplace= True, axis = 1)\n","3e294875":"catg_list = catg_strong_corr.copy()\ncatg_list.remove('Neighborhood')\n\nfor catg in catg_list :\n    #sns.catplot(x=catg, y=target, data=df_train, kind='boxen')\n    sns.violinplot(x=catg, y=target, data=df_train)\n    plt.show()\n    #sns.boxenplot(x=catg, y=target, data=df_train)\n    #bp = df_train.boxplot(column=[target], by=catg)","87755566":"fig, ax = plt.subplots()\nfig.set_size_inches(16, 5)\nsns.violinplot(x='Neighborhood', y=target, data=df_train, ax=ax)\nplt.xticks(rotation=45)\nplt.show()","2e9b034e":"for catg in catg_list :\n    g = df_train.groupby(catg)[target].mean()\n    print(g)","81e3e64d":"# 'MSZoning'\nmsz_catg2 = ['RM', 'RH']\nmsz_catg3 = ['RL', 'FV'] \n\n\n# Neighborhood\nnbhd_catg2 = ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'NWAmes', 'Somerst', 'Timber', 'Veenker']\nnbhd_catg3 = ['NoRidge', 'NridgHt', 'StoneBr']\n\n# Condition2\ncond2_catg2 = ['Norm', 'RRAe']\ncond2_catg3 = ['PosA', 'PosN'] \n\n# SaleType\nSlTy_catg1 = ['Oth']\nSlTy_catg3 = ['CWD']\nSlTy_catg4 = ['New', 'Con']\n\n\n#[]","edd22904":"for df in [df_train, df_test]:\n    \n    df['MSZ_num'] = 1  \n    df.loc[(df['MSZoning'].isin(msz_catg2) ), 'MSZ_num'] = 2    \n    df.loc[(df['MSZoning'].isin(msz_catg3) ), 'MSZ_num'] = 3        \n    \n    df['NbHd_num'] = 1       \n    df.loc[(df['Neighborhood'].isin(nbhd_catg2) ), 'NbHd_num'] = 2    \n    df.loc[(df['Neighborhood'].isin(nbhd_catg3) ), 'NbHd_num'] = 3    \n\n    df['Cond2_num'] = 1       \n    df.loc[(df['Condition2'].isin(cond2_catg2) ), 'Cond2_num'] = 2    \n    df.loc[(df['Condition2'].isin(cond2_catg3) ), 'Cond2_num'] = 3    \n    \n    df['Mas_num'] = 1       \n    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2 \n    \n    df['ExtQ_num'] = 1       \n    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2     \n    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3     \n    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4     \n   \n    df['BsQ_num'] = 1          \n    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2     \n    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3     \n \n    df['CA_num'] = 0          \n    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1    \n\n    df['Elc_num'] = 1       \n    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2 \n\n\n    df['KiQ_num'] = 1       \n    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2     \n    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3     \n    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4      \n    \n    df['SlTy_num'] = 2       \n    df.loc[(df['SaleType'].isin(SlTy_catg1) ), 'SlTy_num'] = 1  \n    df.loc[(df['SaleType'].isin(SlTy_catg3) ), 'SlTy_num'] = 3  \n    df.loc[(df['SaleType'].isin(SlTy_catg4) ), 'SlTy_num'] = 4  \n  ","48a5425d":"new_col_num = ['MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']\n\nnr_rows = 4\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(new_col_num):\n            sns.regplot(df_train[new_col_num[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","4de7f11f":"catg_cols_to_drop = ['Neighborhood' , 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncorr1 = df_train.corr()\ncorr_abs_1 = corr1.abs()\n\nnr_all_cols = len(df_train)\nser_corr_1 = corr_abs_1.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_1)\ncols_bel_corr_limit_1 = list(ser_corr_1[ser_corr_1.values <= min_val_corr].index)\n\n\nfor df in [df_train, df_test] :\n    df.drop(catg_cols_to_drop, inplace= True, axis = 1)\n    df.drop(cols_bel_corr_limit_1, inplace= True, axis = 1)    ","3b16efa7":"corr2 = df_train.corr()\ncorr_abs_2 = corr2.abs()\n\nnr_all_cols = len(df_train)\nser_corr_2 = corr_abs_2.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_2)","2c94d4cc":"df_train.head()","a0b64c7b":"df_test.head()","6038405c":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_all_cols = len(df_train)\nprint (corr_abs.nlargest(nr_all_cols, target)[target])","73e5472c":"nr_feats=len(df_train.columns)\nplot_corr_matrix(df_train, nr_feats, target)","5a331d42":"cols = corr_abs.nlargest(nr_all_cols, target)[target].index\ncols = list(cols)\n\nif drop_similar == 1 :\n    for col in ['GarageArea','1stFlrSF','TotRmsAbvGrd','GarageYrBlt'] :\n        if col in cols: \n            cols.remove(col)","5c49fb3c":"cols = list(cols)\nprint(cols)","ee3b73f0":"feats = cols.copy()\nfeats.remove('SalePrice_Log')\n\nprint(feats)","428207c3":"df_train_ml = df_train[feats].copy()\ndf_test_ml  = df_test[feats].copy()\n\ny = df_train[target]","67792d56":"\"\"\"\nall_data = pd.concat((df_train[feats], df_test[feats]))\n\nli_get_dummies = ['OverallQual', 'NbHd_num', 'GarageCars','ExtQ_num', 'KiQ_num',\n                  'BsQ_num', 'FullBath', 'Fireplaces', 'MSZ_num']\nall_data = pd.get_dummies(all_data, columns=li_get_dummies, drop_first=True)\n\ndf_train_ml = all_data[:df_train.shape[0]]\ndf_test_ml  = all_data[df_train.shape[0]:]\n\"\"\"","dd4fa8cc":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ndf_train_ml_sc = sc.fit_transform(df_train_ml)\ndf_test_ml_sc = sc.transform(df_test_ml)","c765bd25":"df_train_ml_sc = pd.DataFrame(df_train_ml_sc)\ndf_train_ml_sc.head()","171887d4":"X = df_train_ml.copy()\ny = df_train[target]\nX_test = df_test_ml.copy()\n\nX_sc = df_train_ml_sc.copy()\ny_sc = df_train[target]\nX_test_sc = df_test_ml_sc.copy()\n\nX.info()\nX_test.info()","ccf21c5b":"X.head()","048a7103":"X_sc.head()","58bbe663":"X_test.head()","4b379099":"from sklearn.model_selection import GridSearchCV\nscore_calc = 'neg_mean_squared_error'","46a11ddd":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv=nr_cv, verbose=1 , scoring = score_calc)\ngrid_linear.fit(X, y)\n\nsc_linear = get_best_score(grid_linear)","be7fa05f":"linregr_all = LinearRegression()\n#linregr_all.fit(X_train_all, y_train_all)\nlinregr_all.fit(X, y)\npred_linreg_all = linregr_all.predict(X_test)\npred_linreg_all[pred_linreg_all < 0] = pred_linreg_all.mean()","d17a4273":"sub_linreg = pd.DataFrame()\nsub_linreg['Id'] = id_test\nsub_linreg['SalePrice'] = pred_linreg_all\n#sub_linreg.to_csv('linreg.csv',index=False)","209ecabb":"from sklearn.linear_model import Ridge\n\nridge = Ridge()\nparameters = {'alpha':[0.001,0.005,0.01,0.1,0.5,1], 'normalize':[True,False], 'tol':[1e-06,5e-06,1e-05,5e-05]}\ngrid_ridge = GridSearchCV(ridge, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_ridge.fit(X, y)\n\nsc_ridge = get_best_score(grid_ridge)","0ecd2039":"pred_ridge_all = grid_ridge.predict(X_test)","6d7f06cb":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparameters = {'alpha':[1e-03,0.01,0.1,0.5,0.8,1], 'normalize':[True,False], 'tol':[1e-06,1e-05,5e-05,1e-04,5e-04,1e-03]}\ngrid_lasso = GridSearchCV(lasso, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_lasso.fit(X, y)\n\nsc_lasso = get_best_score(grid_lasso)\n\npred_lasso = grid_lasso.predict(X_test)","d80231c4":"from sklearn.linear_model import ElasticNet\n\nenet = ElasticNet()\nparameters = {'alpha' :[0.1,1.0,10], 'max_iter' :[1000000], 'l1_ratio':[0.04,0.05], \n              'fit_intercept' : [False,True], 'normalize':[True,False], 'tol':[1e-02,1e-03,1e-04]}\ngrid_enet = GridSearchCV(enet, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_enet.fit(X_sc, y_sc)\n\nsc_enet = get_best_score(grid_enet)\n\npred_enet = grid_enet.predict(X_test_sc)","4c56d6f6":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor()\nparameters = {'max_iter' :[10000], 'alpha':[1e-05], 'epsilon':[1e-02], 'fit_intercept' : [True]  }\ngrid_sgd = GridSearchCV(sgd, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_sgd.fit(X_sc, y_sc)\n\nsc_sgd = get_best_score(grid_sgd)\n\npred_sgd = grid_sgd.predict(X_test_sc)","0d0ab0ce":"from sklearn.tree import DecisionTreeRegressor\n\nparam_grid = { 'max_depth' : [7,8,9,10] , 'max_features' : [11,12,13,14] ,\n               'max_leaf_nodes' : [None, 12,15,18,20] ,'min_samples_split' : [20,25,30],\n                'presort': [False,True] , 'random_state': [5] }\n            \ngrid_dtree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\ngrid_dtree.fit(X, y)\n\nsc_dtree = get_best_score(grid_dtree)\n\npred_dtree = grid_dtree.predict(X_test)","62c28ac8":"dtree_pred = grid_dtree.predict(X_test)\nsub_dtree = pd.DataFrame()\nsub_dtree['Id'] = id_test\nsub_dtree['SalePrice'] = dtree_pred\n#sub_dtree.to_csv('dtreeregr.csv',index=False)","06a608c6":"from sklearn.ensemble import RandomForestRegressor\n\nparam_grid = {'min_samples_split' : [3,4,6,10], 'n_estimators' : [70,100], 'random_state': [5] }\ngrid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\ngrid_rf.fit(X, y)\n\nsc_rf = get_best_score(grid_rf)","5eb69e3e":"pred_rf = grid_rf.predict(X_test)\n\nsub_rf = pd.DataFrame()\nsub_rf['Id'] = id_test\nsub_rf['SalePrice'] = pred_rf \n\nif use_logvals == 1:\n    sub_rf['SalePrice'] = np.exp(sub_rf['SalePrice']) \n\nsub_rf.to_csv('rf.csv',index=False)","3494c092":"sub_rf.head(10)","62a6766a":"if use_logvals == 0 :\n\n    from sklearn.svm import SVR\n    from sklearn.model_selection import GridSearchCV\n    \n    param_grid = {'C': [100000,300000,400000,500000,600000,800000], 'gamma': [0.1, 0.07, 0.05, 0.04, 0.03, 0.02, 0.01, 0.007,0.005], 'kernel': ['rbf']}\n\n    grid_svr = GridSearchCV(SVR(), param_grid, cv=nr_cv, refit=True, verbose=3, scoring = score_calc)\n    grid_svr.fit(X_sc, y_sc)\n\n    print(grid_svr.best_score_)\n    print(grid_svr.best_params_)\n    print(grid_svr.best_estimator_)\n    \n    svr_pred = grid_svr.predict(X_test_sc)\n\n    sub_svr = pd.DataFrame()\n    sub_svr['Id'] = id_test\n    sub_svr['SalePrice'] = svr_pred \n    if use_logvals == 1:\n        sub_svr['SalePrice'] = np.exp(sub_svr['SalePrice']) \n    \n    sub_svr.head()\n    sub_svr.to_csv('svr.csv',index=False)\n    ","11660aa8":"list_scores = [sc_linear, sc_ridge, sc_lasso, sc_enet, sc_sgd, sc_dtree, sc_rf]\nlist_regressors = ['Linear','Ridge','Lasso','ElaNet','SGD','DTr','RF']","76b23448":"fig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x=list_regressors, y=list_scores, ax=ax)\nplt.ylabel('RMSE')\nplt.show()","88a09073":"predictions = {'Linear': pred_linreg_all, 'Ridge': pred_ridge_all, 'Lasso': pred_lasso,\n               'ElaNet': pred_enet, 'SGD': pred_sgd, 'DTr': pred_dtree, 'RF': pred_rf\n              }\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","8532eb5c":"# Create final table\n\npred_pd = pd.DataFrame()\npred_pd['Id'] = id_test\npred_pd['SalePrice'] = np.exp((pred_rf+pred_sgd)\/2.000000)\n\npred_pd.head(5)\n#pred_pd.to_csv('submission_mrig_v3.csv',index=False)","db34f681":"### Lasso","efd3615c":"**Imports**","e5358aed":"**Find columns with strong correlation to target**  \nOnly those with r > min_val_corr are used in the ML Regressors in Part 3  \nThe value for min_val_corr can be chosen in global settings","7a7b7851":"**List of features used for the Regressors in Part 3**","a4f490ad":"**Model tuning and selection with GridSearchCV**","25897d8b":"### Elastic Net","9a2ef77f":"### SGDRegressor  \nLinear model fitted by minimizing a regularized empirical loss with SGD. SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). ","3d254d29":"### Correlation Matrix 2 : All features with strong correlation to SalePrice","e5ab89a6":"### DecisionTreeRegressor","3be7088e":"### Checking correlation to SalePrice for the new numerical columns","b61f18f5":"### Convert categorical columns to numerical  \nFor those categorcial features where the EDA with boxplots seem to show a strong dependence of the SalePrice on the category, we transform the columns to numerical.\nTo investigate the relation of the categories to SalePrice in more detail, we make violinplots for these features \nAlso, we look at the mean of SalePrice as function of category.","27406348":"**SVR**","115e7eab":"### List of features with missing values","db80d796":"**Filling missing values**  \nFor a few columns there is lots of NaN entries.  \nHowever, reading the data description we find this is not missing data:  \nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.  ","64685c89":"### Ridge","c7e347a3":"**Of those features with the largest correlation to SalePrice, some also are correlated strongly to each other.**\n\n\n**To avoid failures of the ML regression models due to multicollinearity, these are dropped in part 2.**\n\n\n**This is optional and controlled by the switch drop_similar (global settings)**","0373221b":"**columns and correlation before dropping**","bbd2ba00":"### RandomForestRegressor","fea85c83":"### Plots of relation to target for all numerical features","778b692b":"**Combine train and test data**  \nfor one hot encoding (use pandas get dummies) of all categorical features  \nuncommenting the following cell increases the number of features  \nup to now, all models in Part 3 are optimized for not applying one hot encoder  \nwhen applied, GridSearchCV needs to be rerun","7927e28d":"### List of categorical features and their unique values","197f0f1c":"**mean of best models**","530932b3":"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).  \nThese will probably be useful for optimal performance of the Regressors in part 3.","8890c495":"## 1.2 Relation of features to target (SalePrice_log)","2a4113c8":"**Conclusion from EDA on numerical columns:**\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.  \nFor other features like 'MSSubClass' the correlation is very weak.  \nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.  \nThis threshold value can be choosen in the global settings : min_val_corr  \n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:  \n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF',  'LowQualFinSF',  'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',   \n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.  \nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)","3d4f8274":"**Settings and switches**\n\n**Here one can choose settings for optimal performance and runtime.**  \n**For example, nr_cv sets the number of cross validations used in GridsearchCV, and**  \n**min_val_corr is the minimum value for the correlation coefficient to the target (only features with larger correlation will be used).** ","c48f6f83":"# Part 1: Exploratory Data Analysis","8697ffce":"**Check for Multicollinearity**\n\nStrong correlation of these features to other, similar features:\n\n'GrLivArea_Log' and 'TotRmsAbvGrd'\n\n'GarageCars' and 'GarageArea'\n\n'TotalBsmtSF' and '1stFlrSF'\n\n'YearBuilt' and 'GarageYrBlt'\n\n**Of those features we drop the one that has smaller correlation coeffiecient to Target.**","dcfef520":"### Correlation matrix 1\n**Features with largest correlation to SalePrice_Log**  \nall numerical features with correlation coefficient above threshold ","ddaa7492":"**Creating Datasets for ML algorithms**","029c1efb":"## Part 3: Scikit-learn basic regression models and comparison of results\n\n**Test simple sklearn models and compare by metrics**\n\n**We test the following Regressors from scikit-learn:**  \nLinearRegression  \nRidge  \nLasso  \nElastic Net  \nStochastic Gradient Descent  \nDecisionTreeRegressor  \nRandomForestRegressor  \nSVR ","9b1d73ab":"**Missing values in train data ?**","b7842a43":"# Part 0 : Imports, Settings, Functions","92cf7284":"**List of all features with strong correlation to SalePrice_Log**  \nafter dropping all coumns with weak correlation","99bea9e5":"**Load data**","66fc0587":"**correlation of model results**","d9cf9529":"df_train has 81 columns (79 features + id and target SalePrice) and 1460 entries (number of rows or house sales)  \ndf_test has 80 columns (79 features + id) and 1459 entries  \nThere is lots of info that is probably related to the SalePrice like the area, the neighborhood, the condition and quality. \nMaybe other features are not so important for predicting the target, also there might be a strong correlation for some of the features (like GarageCars and GarageArea).\nFor some columns many values are missing: only 7 values for Pool QC in df_train and 3 in df_test","30a80af9":"**Conclusion from EDA on categorical columns:**\n\nFor many of the categorical there is no strong relation to the target.  \nHowever, for some fetaures it is easy to find a strong relation.  \nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType'\nAlso for the categorical features, I use only those that show a strong relation to SalePrice. \nSo the other columns are dropped when creating the ML dataframes in Part 2 :  \n 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', \n'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition' \n ","b4d00c0c":"### Comparison plot: RMSE of all models","cea3fc8c":"Numerical columns : drop similar and low correlation\n\nCategorical columns : Transform  to numerical","5b722d86":"## 1.1 Overview of features and relation to target\n\nLet's get a first overview of the train and test dataset  \nHow many rows and columns are there?  \nWhat are the names of the features (columns)?  \nWhich features are numerical, which are categorical?  \nHow many values are missing?  \nThe **shape** and **info** methods answer these questions  \n**head** displays some rows of the dataset  \n**describe** gives a summary of the statistics (only for numerical columns)","6ab902ae":"**For some more advanced studies inluding Feature Engineering and methods like Stacking, Boosting and Voting have a look at my second kernel on this competition which I will link here soon**","6e172261":"### Relation to SalePrice for all categorical features","ee0a7765":"### The target variable : Distribution of SalePrice","116bbc54":"### Numerical and Categorical features","99be78e2":"**Dropping the converted categorical columns and the new numerical columns with weak correlation**","8dcb0384":"### shape, info, head and describe","1d65c145":"## Part 2: Data wrangling\n\n**Drop all columns with only small correlation to SalePrice**  \n**Transform Categorical to numerical **  \n**Handling columns with missing data**  \n**Log values**  \n**Drop all columns with strong correlation to similar features**  ","ae130fd6":"As we see, the target variable SalePrice is not normally distributed.  \nThis can reduce the performance of the ML models because they assume normal distribution, see [sklearn info on preprocessing](http:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html)\n\nTherfore we make a log transformation, the resulting distribution looks much better.  ","0f24bf89":"### Linear Regression","f11f2513":"### Dropping all columns with weak correlation to SalePrice","4042ae6a":"### StandardScaler","ca71c6b9":"**Some useful functions**","1e3404c3":"**Outliers**","89ad383c":"### List of numerical features and their correlation coefficient to target","92371870":"**new dataframes**","ad5f1ce4":"**columns and correlation after dropping**","2c5d72cc":"**Missing values in test data ?**","5ff498c5":"### log transform\nLike the target variable, also some of the feature values are not normally distributed and it is therefore better to use log values in df_train and df_test. Checking for skewness and kurtosis:"}}