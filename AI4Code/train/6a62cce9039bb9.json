{"cell_type":{"685a0e17":"code","35dc5b92":"code","699d88c2":"code","f8007a0a":"code","4577aba0":"code","4efd2618":"code","57cc1d04":"code","6ce14921":"code","145280bb":"code","542402fc":"code","dd60e6a1":"code","bd05d36c":"code","81f5a7d0":"code","8ea2f498":"code","acbf8439":"code","dd8c8632":"code","1c068e34":"code","c4f18c32":"code","c5811fcc":"code","beba7dc7":"code","9fa54d07":"code","ca784452":"code","166f31dd":"code","4a555886":"code","ed2c1235":"code","f2fb458b":"code","3a029170":"code","8ef5b7ba":"code","cb33a384":"code","dcba5600":"code","5124f3e1":"markdown","d36cb384":"markdown","14cb713c":"markdown","2c267861":"markdown","7b41fef8":"markdown","8f595bf6":"markdown","c0d6f1e0":"markdown","7c7135b8":"markdown","e368d04d":"markdown","d575e151":"markdown","d7c3cd16":"markdown","be668c5f":"markdown","cd6c5a67":"markdown","ae461331":"markdown","2a715619":"markdown","70ae06c9":"markdown","77394e6d":"markdown","d5af5a7f":"markdown","05a9d8bb":"markdown","e2cd8a0c":"markdown","70511b1a":"markdown","d64c7ae4":"markdown"},"source":{"685a0e17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\npath = \"..\/input\/diamonds\/diamonds.csv\"\ndf = pd.read_csv(path)\ndf.head()\n","35dc5b92":"df=df.drop(['Unnamed: 0'],axis=1)","699d88c2":"df = df.dropna()\ndf.head()","f8007a0a":"df.describe()","4577aba0":"is_x_0 = df['x']==0\nprint(df[is_x_0].head())","4efd2618":"df.drop(df[df['x']==0].index,inplace=True)\ndf.drop(df[df['y']==0].index,inplace=True)\ndf.drop(df[df['z']==0].index,inplace=True)","57cc1d04":"fig, axes = plt.subplots(2, 3)\nfig.tight_layout()\nsns.boxplot(ax=axes[0,0], y=\"carat\",data=df).set(xlabel='', ylabel='weight of the diamond', title='carat')\nsns.boxplot(ax=axes[0,1], y=\"depth\",data=df).set(xlabel='', ylabel='total depth %', title='depth')\nsns.boxplot(ax=axes[0,2], y=\"table\",data=df).set(xlabel='', ylabel='top table width', title='table')\nsns.boxplot(ax=axes[1,0], y=\"x\",data=df).set(xlabel='', ylabel='length mm', title='x')\nsns.boxplot(ax=axes[1,1], y=\"y\",data=df).set(xlabel='', ylabel='width mm', title='y')\nsns.boxplot(ax=axes[1,2], y=\"z\",data=df).set(xlabel='', ylabel='depth mm', title='z')\n\n  \nplt.show()","6ce14921":"fig = plt.figure()\nsns.boxplot(y=df['price'],x=df['cut'])\nplt.title(\"cut boxplot\")\nplt.show()","145280bb":"fig = plt.figure()\nsns.boxplot(y=df['price'],x=df['color'])\nplt.title(\"color boxplot\")\nplt.show()","542402fc":"fig = plt.figure()\nsns.boxplot(y=df['price'],x=df['clarity'])\nplt.title(\"clarity boxplot\")\nplt.show()","dd60e6a1":"y1 = df[\"y\"].quantile(0.01)\ny99 = df[\"y\"].quantile(0.99)\nz1 = df[\"z\"].quantile(0.01)\nz99 = df[\"z\"].quantile(0.99)\ndf = df[(df[\"z\"]<z99)& (df[\"z\"]>z1)]\ndf = df[(df[\"y\"]<y99)& (df[\"y\"]>y1)]\n\nfig = plt.figure()\n\nsns.boxplot(y=\"y\",data=df).set(xlabel='', ylabel='width mm', title='y')\nplt.show()","bd05d36c":"fig = plt.figure()\nsns.boxplot(y=\"z\",data=df).set(xlabel='', ylabel='depth mm', title='z')\nplt.show()","81f5a7d0":"from sklearn import preprocessing\n\ncat = df.select_dtypes(include=['object'])\ndf = df.apply(preprocessing.LabelEncoder().fit_transform)\ndf.head()","8ea2f498":"X = df.drop('price', axis=1)\ny = df['price']\nprint(\"X:\")\nprint(X.head())     \nprint(\"\\ny:\")\nprint(y.head())","acbf8439":"corr = X.corr()\nax =sns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n            vmin=-1, vmax=1, annot=True, cmap='BrBG')\nplt.title('Heatmap of Diamond Features', fontsize = 20) \nplt.show()","dd8c8632":"ax = sns.displot(y).set(title='Histogram of Target Vector (Price)')\nplt.show()","1c068e34":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)","c4f18c32":"pip install mglearn","c5811fcc":"import mglearn\nmglearn.plots.plot_grid_search_overview()","beba7dc7":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom math import sqrt\n\ndef get_regressor_neg_rms(model, X, y, cv=7):\n\n    scores = cross_validate(model, X_train, y_train, return_train_score=True, scoring='neg_root_mean_squared_error')\n    \n    return [scores['train_score'].mean(),scores['test_score'].mean()]","9fa54d07":"from sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nfor model in [LinearRegression(), BayesianRidge(), RandomForestRegressor(random_state=64),GradientBoostingRegressor(random_state=79)]:\n    \n    model.fit(X_train, y_train)\n    results = get_regressor_neg_rms(model, X_train, y_train)\n    print(f\"Scores for {model}\")\n    print(f\"Neg RMS =>      Train Score: {results[0]:.2f}    Validation Score: {results[1]:.2f}\")\n    print(f\"root mean squared error  =>    Training set:{sqrt(mean_squared_error(y_train, model.predict(X_train))):.2f}   Test set: {sqrt(mean_squared_error(y_test, model.predict(X_test))):.2f}\")\n    print(f\"r-squared score  =>    Training set:{r2_score(y_train, model.predict(X_train)):.3f}   Test set: {r2_score(y_test, model.predict(X_test)):.3f}\")\n    print(\"\\n\\n\")","ca784452":"from sklearn.model_selection import GridSearchCV\n\n#Forest and Gradient parameters respectively\nforestparam = {'max_depth': [ 11, 13, 15, 17], 'n_estimators': [100, 200, 500], 'max_features': ['auto', 'log2', None]}\ngradientparam = {'max_depth': [1, 3, 5, 7], 'n_estimators': [100, 200, 500], 'learning_rate': [0.01, 0.1, 1.0]}","166f31dd":"#performing grid search for RandomForestRegressor\nforestgrid = GridSearchCV(RandomForestRegressor(random_state=64), forestparam)\nforestgrid.fit(X_train, y_train)","4a555886":"#performing grid search for GradientBoostingRegressor\ngradientgrid = GridSearchCV(GradientBoostingRegressor(random_state=79), gradientparam)\ngradientgrid.fit(X_train, y_train)","ed2c1235":"print(forestgrid.best_params_)\nprint('Best score: ', forestgrid.best_score_)","f2fb458b":"print(gradientgrid.best_params_)\nprint('Best score: ', gradientgrid.best_score_)","3a029170":"forest_model = RandomForestRegressor(random_state=64, max_depth= 15, max_features= 'auto', n_estimators= 500)\nforest_model.fit(X_train, y_train)\ngradient_model = GradientBoostingRegressor(random_state=79, learning_rate= 0.1, max_depth= 7, n_estimators= 200)\ngradient_model.fit(X_train, y_train)","8ef5b7ba":"print(f'RandomForestRegressor:')\nprint(f\"root mean squared error=>    training set:{sqrt(mean_squared_error(y_train, forest_model.predict(X_train))):.2f}   test set: {sqrt(mean_squared_error(y_test, forest_model.predict(X_test))):.2f}\")\nprint(f\"r-squared score=>    training set:{r2_score(y_train, forest_model.predict(X_train)):.6f}   test set: {r2_score(y_test, forest_model.predict(X_test)):.6f}\")","cb33a384":"print(f'GradientBoostingRegressor:')\nprint(f\"root mean squared error=>    training set:{sqrt(mean_squared_error(y_train, gradient_model.predict(X_train))):.2f}   test set: {sqrt(mean_squared_error(y_test, gradient_model.predict(X_test))):.2f}\")\nprint(f\"r-squared score=>    training set:{r2_score(y_train, gradient_model.predict(X_train)):.6f}   test set: {r2_score(y_test, gradient_model.predict(X_test)):.6f}\")","dcba5600":"df = pd.DataFrame({'predicted Price(USD)':gradient_model.predict(X), 'actual Price(USD)':y})\nsns.scatterplot(data=df, x = 'predicted Price(USD)', y = 'actual Price(USD)')\nplt.title(\"Predicted vs Actual Price\", fontsize =20)\naxes = plt.gca()\nx_vals = np.array(axes.get_xlim())\ny_vals = 1 * x_vals\nplt.plot(x_vals, y_vals, '--')\nplt.show()","5124f3e1":"Next I want to remove any na values from the dataset","d36cb384":"So now let us retrain the models using the optimized parameters","14cb713c":"# Data Preparation and Visualization\nIn this section, I will prepare the data as well as visualizing the features. Let us start looking at the boxplots of the features from the dataset. Although the target vector is the column price and the rest are features, I will not define the X and y dataframes yet until I have done the visualizations to see if further cleanup is needed.","2c267861":"# Create training and test sets\nHere, I will set the random state to 42 and test size to be 20% or 0.2.","7b41fef8":"For the categorical features, I will plot the boxplots based on the target variable, or price.","8f595bf6":"The best models for the RandomForestRegressor is:","c0d6f1e0":"Below will be what the analysis will follow for this dataset:","7c7135b8":"From here there are two things that I want to do before creating the X and y dataframes. The first thing is that from the boxplots I see there are extreme outliers in y and z boxplots. This is something that I want to get rid of. With this, I want to filter out the y and z features from the dataframe using 1% and 99% quantile.","e368d04d":"# Compare Performance of Different Models\nIn this section, we will compare the performance of the following models:\n\nLinearRegression\n\nRandomForestRegressor\n\nGradientBoostingRegressor\n\nBayesianRidge\n\n","d575e151":"# Content (based from Kaggle information)\nprice price in US dollars ($326--$18,823)\n\ncarat weight of the diamond (0.2--5.01)\n\ncut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n\ncolor diamond colour, from J (worst) to D (best)\n\nclarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n\nx length in mm (0--10.74)\n\ny width in mm (0--58.9)\n\nz depth in mm (0--31.8)\n\ndepth total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) (43--79)\n\ntable width of top of diamond relative to widest point (43--95)\n\n# Cleaning Up Data\nFirst I would like to clean up the data before the analysis.\n\nThe first thing that I want to do is to remove the first column that is titled 'Unnamed: 0' as it appears to be just an index.","d7c3cd16":"The best models for the GradientBoostingRegressor is:","be668c5f":"Histogram of target variable","cd6c5a67":"Now that the outliers have been filtered out from y and z, I would like to fix the second part, which is to transform the categories from english to numerical so that it can be easily interpreted by the machine learning models. For this, I would use LabelEncoder.","ae461331":"Let us now split the dataframe up into X and y dataframes:","2a715619":"# Conclusion\nOverall, with a r2 score of close to 0.99, I believe that this model is of high performance and highly optimized for the Diamonds Dataset.\n\n# # Interpretation:\n\nWith the high accuracy of the model, I believe it fullfills my objective of the proposal in using it to explore and perhaps invest in this commodity type.\n\n# # Reflection\n\nIt does not seem there are any deviations from the proposal given that everything runs smoothly and the model is of good performance. I suspect the reason for such good performance is because there is probably a set of rules that governs the price of diamonds in place that allows the model to easily grasp the patterns.","70ae06c9":"Since the numbers are very close, I have to increase the decimal places to see which one is a better model. Overall, there isn't much improvement when optimizing the RandomForestRegressor but with GradientBoostingRegressor we do see better results, and its performance is actually better than RandomForestRegressor.\n\nPredicted VS Actual \nLastly, let us plot the predicted vs actual  plot of the best, most-optimized model, which is the GradientBoostingRegressor with its hyperparameters optimized:","77394e6d":"Defining function for cross validation scores\nThe following function was developed to return mean training and validation negative rms","d5af5a7f":"Correlation Matrix for the Features of the dataset:","05a9d8bb":"Upon looking at the describe of the dataframe, a problem is identified. The problem is that the min values of x, y,z seems to have values of 0, this cannot be true as these values are the dimensions of a specific diamond. Let's verify this information by filterring values where x==0:","e2cd8a0c":"# Diamond Price Analysis\n\nReference:\nS. Agrawal, \u201cDiamonds,\u201d Kaggle: Diamonds Dataset, 25-May-2017. [Online]. Available: https:\/\/www.kaggle.com\/shivam2503\/diamonds?select=diamonds.csv. [Accessed: 13-Oct-2021].","70511b1a":"From the scores, it shows that the ensemble models (RandomForestRegressor and GradientBoostingRegressor) have higher accuracy. I suspect that there is a slight bit of underfitting for the linear models as the test and training scores are close but the scores are lower than the ensemble models and there is a slight hint of overfitting for RandomForestRegressor as the training has a r2 score that is a bit lower than the test score.\n\nOverall, the models performed great as they all achieved an r2 score higher than 0.9.\n\n# Hyperparameter tuning\nNow that we have evaluate our models, it is time to tweak it to mximize its performance. For this, I will use grid search. Also, since here I am focusing on getting the most optimized models for the diamonds dataset, I will not tweak the parameters for the linear models as it is clearly shown in the last section that the ensemble models have better performance. Although the RandomForestRegressor achieved the highest score, there are signs of overfitting as discussed earlier and that GradientBoostingRegressor isn't too far behind. As such, I will tune both models to see which one gets the better result.","d64c7ae4":"From the filtered columns, there does seem to have diamonds with 0 dimensions. I suspect that this is missing information and if included will affect the outcome of the model results. As such, these data will be removed:"}}