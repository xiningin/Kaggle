{"cell_type":{"f843d289":"code","5edf27f9":"code","99e4e8b0":"code","4eca034c":"code","79bd84c9":"code","04220e9b":"code","c3697caf":"code","e42dc72b":"code","24d834e4":"code","4cf1e9cc":"code","8ccf710d":"code","5b8f7ae1":"code","b3fdad43":"code","13cec009":"code","2202f88b":"code","493367fd":"markdown","28401e47":"markdown","0c3938f4":"markdown","146c19ee":"markdown","9aadfbca":"markdown","0952aced":"markdown","a72500f9":"markdown","6288b036":"markdown"},"source":{"f843d289":"import pickle,os\nimport numpy as np\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5edf27f9":"np.random.seed(50)\ndf_app = pd.read_csv('\/kaggle\/input\/application_train.csv').sample(50000)\n#Prev application\ndf_prev = pd.merge(df_app[['SK_ID_CURR','TARGET']], pd.read_csv('\/kaggle\/input\/previous_application.csv'),\n                   on = 'SK_ID_CURR', how = 'inner')\n#Credit cards \ndf_cc = pd.merge(df_prev[['SK_ID_PREV','TARGET']], pd.read_csv('\/kaggle\/input\/credit_card_balance.csv'),\n                   on = 'SK_ID_PREV', how = 'inner')","99e4e8b0":"df_cc.head()","4eca034c":"filter_dict = {\n'p0_12m' : \"(DF['MONTHS_BALANCE'] > -12) & (DF['MONTHS_BALANCE'] <= 0)\",\n'nz_draw': \"(DF['AMT_DRAWINGS_CURRENT'].fillna(0) > 0)\",\n'nz_bal' : \"(DF['AMT_BALANCE'].fillna(0) > 0)\",\n'nz_pos': \"(DF['AMT_DRAWINGS_POS_CURRENT'].fillna(0) > 0)\",\n'nz_pay': \"(DF['AMT_PAYMENT_CURRENT'].fillna(0) > 0)\",\n'nz_atm': \"(DF['AMT_DRAWINGS_ATM_CURRENT'].fillna(0) > 0)\"   \n}\nDF = df_cc.copy()","79bd84c9":"def nvl(x,y):\n    if (not pd.isnull(x)):\n        res = x \n    else :\n        res = y \n    return res \n\ndf_res = DF[['SK_ID_PREV','TARGET']].drop_duplicates() \n\ndf_res = pd.merge(df_res, DF[eval(filter_dict['p0_12m'] + \"&\" + filter_dict['nz_pos'])].\\\ngroupby('SK_ID_PREV').aggregate({'MONTHS_BALANCE' : 'count'}).reset_index().\\\nrename(columns = {'MONTHS_BALANCE':'CNT_MON_NZ_POS'}), on = 'SK_ID_PREV', how = 'left')\n\ndf_res = pd.merge(df_res, DF[eval(filter_dict['p0_12m'] + \"&\" + filter_dict['nz_atm'])].\\\ngroupby('SK_ID_PREV').aggregate({'MONTHS_BALANCE' : 'count'}).reset_index().\\\nrename(columns = {'MONTHS_BALANCE':'CNT_MON_NZ_ATM'}), on = 'SK_ID_PREV', how = 'left')\n\ndf_res['CC_DRAW_HABIT'] = df_res[['CNT_MON_NZ_ATM','CNT_MON_NZ_POS']].\\\napply(lambda x : (nvl(x[0],0)-nvl(x[1],0))\/max(nvl(x[0],0),nvl(x[1],0)) if (not pd.isnull(x[0])) or (not pd.isnull(x[1])) else np.nan, axis = 1)","04220e9b":"df_res2 = df_res[[not pd.isnull(x) for x in df_res['CC_DRAW_HABIT'].values]]\ndf_res2 = df_res2.sort_values(by = 'CC_DRAW_HABIT')\ndf_res2['rank'] = df_res2['CC_DRAW_HABIT'].rank(method = 'first')","c3697caf":"print('The number of clients with spending on credit cards for last 12 month is about {x},\\\n\\nThat is about {y}% of total'.format(x = df_res2['CC_DRAW_HABIT'].count(),\n                                                     y = 100*round(df_res2['CC_DRAW_HABIT'].count()\/50000,2)))","e42dc72b":"sns.distplot(df_res2['CC_DRAW_HABIT'].values, bins = 10,kde = False)\nplt.title('CC_DRAW_HABIT Distribution')","24d834e4":"#######################################\ndef get_bins(x, bin_intervals  ):\n    pair_bin = []\n    pair_bin.append((-np.inf,bin_intervals[0]))\n    for i in range(len(bin_intervals)-1):\n        pair_bin.append((bin_intervals[i],bin_intervals[i+1]))\n    pair_bin.append((bin_intervals[-1],np.inf))\n    \n    bin_num = np.asarray([x > bin_[0] and x <= bin_[1] for bin_ in pair_bin]).argmax()\n    return bin_num\nbins_ = []\nalpha = 10\nwhile( alpha < 100):\n        bins_.append( np.percentile(df_res2['rank'],alpha))\n        alpha = alpha + 10","4cf1e9cc":"fig, ax1 = plt.subplots(1, 1, figsize=(8, 4))\nplt.title('Draw habbit distribution')\n#ax1.axvline(x= df_res['CC_DRAW_HABIT'].mean(), color='r', linestyle='dashed', linewidth=2)\ndf_res2['groups'] = df_res2['rank'].\\\napply(lambda x:  get_bins(x, bin_intervals = bins_ ))\nsns.countplot(x =df_res2['groups'].values,color = 'gray')\n#####\ndf_res3 = pd.merge( df_res2, \n                    df_res2.groupby('groups').aggregate({'CC_DRAW_HABIT':max}).reset_index().\\\nrename(columns = {'CC_DRAW_HABIT': 'max_CC_DRAW_HABIT'}),\n                   on = 'groups')\nax2 = ax1.twinx()\nagg_ = pd.merge(df_res3[['groups','max_CC_DRAW_HABIT']].drop_duplicates(),\n                df_res3.groupby('max_CC_DRAW_HABIT').aggregate({'TARGET':'mean'}).reset_index ().\\\nrename(columns = {'TARGET': 'Default rate'}),\n               on = 'max_CC_DRAW_HABIT')\nplt.plot(agg_['groups'].values,agg_['Default rate'].values,  'b--', marker = 'o',ms =4)\nlabels_ = df_res3[['groups','max_CC_DRAW_HABIT']].drop_duplicates()['max_CC_DRAW_HABIT'].tolist()\nax1.set_xticklabels([round(x,2) for x in labels_])\nax1.set_xlabel('maximum of CC_DRAW_HABIT in each percentile group(10% step)')\n#ax1.legend('Default rate',)\n\nimport matplotlib.lines as mlines\nblue_line = mlines.Line2D([], [],ls = '--', marker = 'o',ms =4, color='blue', label='Default Rate')\nplt.legend(handles=[blue_line])\nprint(\"For each group I make a label with it's maximum of CC_DRAW_HABIT\\\n\\nIt is easy to see the groups with same Default Rate\\\n\\nOf course, we can mention a fact, that client with only ATM months have a bit more chance\\\n to get a default on their credit ~12% comparing to  ~9% for 'POS clients'\\\n \\nI will not calcaulate importance of this variable here for purpose of this challenge, you can\\\n make it for yourself ;)\")","8ccf710d":"def max_(x):\n    if(x.dropna().shape[0]):\n        res = max(x.dropna())\n    else :\n        res = np.nan\n    return res \n\nDF['CC_LOAD_RATE'] = DF[['AMT_BALANCE','AMT_CREDIT_LIMIT_ACTUAL']].\\\napply(lambda x : x[0]\/(1+x[1]), axis = 1)\ndf_res = DF[['SK_ID_PREV','TARGET']].drop_duplicates() \n\ndf_res = pd.merge(df_res, DF[eval(filter_dict['p0_12m'])].\\\ngroupby('SK_ID_PREV').aggregate({'CC_LOAD_RATE' : lambda x : max_(x) }).reset_index().\\\nrename(columns = {'CC_LOAD_RATE':'MAX_CC_LOAD_RATE'}), on = 'SK_ID_PREV', how = 'left')","5b8f7ae1":"df_res2 = df_res[[not pd.isnull(x) for x in df_res['MAX_CC_LOAD_RATE'].values]]\ndf_res2 = df_res2.sort_values(by = 'MAX_CC_LOAD_RATE')\ndf_res2['rank'] = df_res2['MAX_CC_LOAD_RATE'].rank(method = 'first')","b3fdad43":"bins_ = []\nalpha = 10\nwhile( alpha < 100):\n        bins_.append( np.percentile(df_res2['rank'],alpha))\n        alpha = alpha + 10","13cec009":"print('The number of clients with debt exist on credit cards for last 12 month is about {x},\\\n\\nThat is about {y}% of total'.format(x = df_res2['MAX_CC_LOAD_RATE'].count(),\n                                                     y = 100*round(df_res2['MAX_CC_LOAD_RATE'].count()\/50000,2)))","2202f88b":"fig, ax1 = plt.subplots(1, 1, figsize=(8, 4))\nplt.title('Distribution of maximum debt to credit card limit rate')\n#ax1.axvline(x= df_res['CC_DRAW_HABIT'].mean(), color='r', linestyle='dashed', linewidth=2)\ndf_res2['groups'] = df_res2['rank'].\\\napply(lambda x:  get_bins(x, bin_intervals = bins_ ))\n#####\nsns.countplot(x =df_res2['groups'].values,color = 'gray')\n#####\ndf_res3 = pd.merge( df_res2, \n                    df_res2.groupby('groups').aggregate({'MAX_CC_LOAD_RATE':max}).reset_index().\\\nrename(columns = {'MAX_CC_LOAD_RATE': 'max_MAX_CC_LOAD_RATE'}),\n                   on = 'groups')\n\nax2 = ax1.twinx()\nagg_ = pd.merge(df_res3[['groups','max_MAX_CC_LOAD_RATE']].drop_duplicates(),\n                df_res3.groupby('max_MAX_CC_LOAD_RATE').aggregate({'TARGET':'mean'}).reset_index ().\\\nrename(columns = {'TARGET': 'Default rate'}),\n               on = 'max_MAX_CC_LOAD_RATE')\nplt.plot(agg_['groups'].values,agg_['Default rate'].values,  'b--', marker = 'o',ms =4)\nlabels_ = df_res3[['groups','max_MAX_CC_LOAD_RATE']].drop_duplicates()['max_MAX_CC_LOAD_RATE'].tolist()\nax1.set_xticklabels([round(x,2) for x in labels_])\nax1.set_xlabel('maximum of MAX_CC_LOAD_RATE in each percentile group(10% step)')\n#ax1.legend('Default rate',)\n\nimport matplotlib.lines as mlines\nblue_line = mlines.Line2D([], [],ls = '--', marker = 'o',ms =4, color='blue', label='Default Rate')\nplt.legend(handles=[blue_line])\nprint(\"There we have high chance for default for clients with high debt to limit rate.\\\n\\nOf course it is strange to have this rate more than 1, but I think it is  because\\\n AMT_BALANCE consider a principal debt and interest debt and even smth more, \\\nfor exmpl\u0443 penalties for delinquency;).\\nAnyway, this variable works really good for this competition.\\\nand helps to detach these 20% of 'clients' who has debt for last 12 month on credit card\")\n","493367fd":"Banking domain was not new for me and it was really interesting to explore this data.\nI'd like to share exactly two usefull tips, both of them about credit card information, which I found most exciting for me during the Home Credit challenge:\n    <b> First Tip <\/b> <br>\n    First tip is about the way, how we can look at the client behaviour according to his credit card history. We can take into account the rative difference between active months with drawings in atm and active months with drawings in pos. So we finally get a nice normalize feature, which tell us that client is more about pos transactions if this feature is going to 1( or -1 depends on formula we use ). The same techinque we can apply for client's payment behaviour and compare the active months of payment with active months when client has a debt( there, however, we will get feature in the [0,1] boundaries ). I think this techinque could be applied for other aspects of this dataset too, but here we will talk only about first one described.\n    Finally this feature about client behaviour in transactions don't give very strong predictor for modelling, we will see down inside the code, how it looks like. <br>\n    <b> Second Tip <\/b> <br>\n    Second tip is more about strong predictor, which I get from the dataset. For people from banking domain I think there is no secret, that relation debt to sum is very powerfull feature for make a prediction about future client's default. Well, it was true in this case too and maximum of relation of credit card debt to credit card limit gives really powerfull instrument for prediciton. We will see it soon. \n    ","28401e47":"It is interesting to look on Default Rate across these groups. We have a bit shifted distribution to boundaries 1 and -1, so when we will divide our sample according to rank, we will have observations which we can not distinguish( first percentiles and last percentiles). That is why we need define the same Default rate for these groups manualy.","0c3938f4":"Of course, we dramatically cut our previous sample of 50000, cause not each client has a credit card account.. There we can see the portion of such people is about ~10%. We don't take into account the cases with more than one card per SK_ID_CURR key to make our analysis a bit easy.","146c19ee":"<b> Define some filters, so we can easily apply them to dataset:  <\/b><br>\n1) \"p0_12m\"  - we're looking only on 12 last months <br>\n2) \"nz_draw\" - the months where client has any drawings <br>\netc..\n","9aadfbca":"Now we can look at our data and see the clear picture to divide our clients on POS\/ATM type of spendings.","0952aced":"<b> Let's create a sample of 50000 observations for research <\/b>","a72500f9":"We will use <b>rank<\/b> on \"CC_DRAW_HABIT\" variable to divide our sample by percentiles.","6288b036":"<b> The same analysis I made for variable, which I mentioned as a Tip number 2 <\/b>"}}