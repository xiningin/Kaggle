{"cell_type":{"64f56fbb":"code","d3ea7619":"code","f3a8f34d":"code","f90db26a":"code","e26f6018":"code","dd02643b":"code","bc058092":"code","cb61b57b":"code","d932a9ff":"code","cfd7099a":"code","777b80ad":"code","00885901":"code","c01ce806":"code","24249d69":"code","39ccb5bf":"code","bc357401":"code","4a892fc6":"code","3e70088f":"code","e4579608":"code","8b766c50":"code","0ec9f3e7":"code","e7d97be3":"code","8a5dd8d1":"code","a33391ec":"code","176e2ca9":"code","e0643c73":"code","d071a418":"markdown","ff3ac10a":"markdown","2ee03937":"markdown","a4b26213":"markdown","7f6a7ad7":"markdown","287389c8":"markdown","ed515bf0":"markdown","e026f66c":"markdown","c8c749b0":"markdown","fc7e6c82":"markdown","00651559":"markdown","8de9f456":"markdown","ed196b3a":"markdown","d87d9417":"markdown","b3a2bd91":"markdown","9fab17f2":"markdown","63365d3e":"markdown","4523298f":"markdown","b3e4b11e":"markdown","3e30c1ee":"markdown","925481c1":"markdown","5c6e865e":"markdown","5d8f7c90":"markdown","ad9fe6d9":"markdown","71e491bb":"markdown","8994fcf2":"markdown","5be35d54":"markdown","e3bf20cb":"markdown","ddcf9cc6":"markdown","82db7a3c":"markdown","ca5a6dfc":"markdown"},"source":{"64f56fbb":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier as cat\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, plot_roc_curve, confusion_matrix","d3ea7619":"# Loading the data set into a pandas data frame\n\nprint(os.listdir(\"..\/input\"))\ndf = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","f3a8f34d":"df.info()","f90db26a":"df.head()","e26f6018":"# Checking the unique values of the columns:\n\nfor col in df:\n    print('\\n',col,': ', df[col].unique())","dd02643b":"# Illustrating the values and count of attributes using plots:\n\nlabel_SenCit = df['SeniorCitizen'].unique()\nvalue_SenCit = df['SeniorCitizen'].value_counts() \/ len(df) * 100\n\nlabel_gen = df['gender'].unique()\nvalue_gen = df['gender'].value_counts() \/ len(df) * 100\n\nlabel_Part = df['Partner'].unique()\nvalue_Part = df['Partner'].value_counts() \/ len(df) * 100\n\nlabel_Dep = df['Dependents'].unique()\nvalue_Dep = df['Dependents'].value_counts() \/ len(df) * 100\n\nfig = plt.figure()\n\nax1 = fig.add_axes([0, 0, 0.5, 0.5], aspect=2)\nax1.pie(value_SenCit, textprops={'size': 14}, autopct='%1.1f%%', labels=label_SenCit, radius = 2.2)\n\nax2 = fig.add_axes([0.8, 0, 0.5, 0.5], aspect=2)\nax2.pie(value_gen, textprops={'size': 14}, autopct='%1.1f%%', labels=label_gen, radius = 2.2)\n\nax3 = fig.add_axes([1.6, 0, 0.5, 0.5], aspect=2)\nax3.pie(value_Part, textprops={'size': 14}, autopct='%1.1f%%', labels=label_Part, radius = 2.2)\n\nax4 = fig.add_axes([2.4, 0, 0.5, 0.5], aspect=2)\nax4.pie(value_Dep, textprops={'size': 14}, autopct='%1.1f%%', labels=label_Dep, radius = 2.2)\n\nax1.set_title('SeniorCitizen', loc='center', pad = 100, fontdict={'fontsize':18})\nax2.set_title('gender', loc='center', pad = 100, fontdict={'fontsize':18})\nax3.set_title('Partner', loc='center', pad = 100, fontdict={'fontsize':18})\nax4.set_title('Dependents', loc='center', pad = 100, fontdict={'fontsize':18})\n\nplt.show()","bc058092":"# Check the balance of class labels:\n\nlabel_Churn = df['Churn'].unique()\nvalue_Churn = df['Churn'].value_counts()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nax.bar(label_Churn, value_Churn, color= 'purple', width = 0.6)\nax.set_title('Churn (Class label)', loc='center', pad = 40, fontdict={'fontsize':18})\nplt.show()","cb61b57b":"# Remove leading and tailing characters such as whitespace, tab, etc.\ndf['TotalCharges'] = df['TotalCharges'].str.strip()\n\n# Convert the column from string to number\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'])","d932a9ff":"# Since, non-numeric\/Alphabetic strings can not be converted to numeric, it will cause to NaN values. So, it is the time to check the null values of 'TotalCharges' again:\nprint('Count of null values:', df['TotalCharges'].isnull().sum())\n\n# Index of NaN values of 'TotalCharges':\nindex_na = df[df['TotalCharges'].isnull()].index.tolist()\nprint('\\nindex:', index_na)","cfd7099a":"# Checking the rows with Nan value in 'TotalCharges' to handle:\ndf.iloc[index_na,:].head(11)","777b80ad":"# All the Nan values will be replaced by zero\ndf['TotalCharges'] = df['TotalCharges'].fillna(0)\nprint('Count of null values:', df['TotalCharges'].isnull().sum())","00885901":"# LabelEncoding categorical columns:\n\ncat_col = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n           'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling','Contract', 'PaymentMethod', \"Churn\"]\n\ndf[cat_col] = df[cat_col].apply(LabelEncoder().fit_transform)\ndf.head()","c01ce806":"# # One-hot encoding categorical columns (It is an alternative method for LabelEncoding)\n\n# df = pd.get_dummies(df, prefix=None, prefix_sep='_', dummy_na=False, columns=[\n#     'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n#     'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n#     'StreamingMovies', 'PaperlessBilling', 'Contract', 'PaymentMethod'], drop_first=True)","24249d69":"df.info()","39ccb5bf":"DTC = RandomForestClassifier(random_state=4)\n\nX = df.drop(['Churn', 'customerID'] , axis=1)\nY = df['Churn']\n\nfeature_imp = DTC.fit(X, Y).feature_importances_\nlabel_feature_imp = X.columns\n\n\nsort_imp = pd.Series(data=feature_imp, index=label_feature_imp).sort_values(ascending=False)\nplt.figure(figsize=(12,8))\nplt.title(\"Feature Importance\", fontsize=18)\nax = sns.barplot(y=sort_imp.index, x=sort_imp.values, palette=\"rocket\", orient='h')\nax.set_xlabel('feature Importance',fontsize=16);\nax.set_ylabel('Feature',fontsize=16);\nplt.show()","bc357401":"corrmat = df.corr()\nf, ax = plt.subplots(figsize =(18, 14))\nsns.heatmap(corrmat, ax = ax, cmap =\"Purples\", linewidths = 0.1, annot=True)\nplt.show()","4a892fc6":"# Plotting the histograms of the real-vale attributes\n\nvalue_ten = df['tenure']\nvalue_Mon = df['MonthlyCharges']\nvalue_Tot = df['TotalCharges']\n\nfig = plt.figure()\n\nax1 = fig.add_axes([0.5, 0, 0.7, 0.7])\nax1.hist(value_ten, color='red', bins = 20)\n\nax2 = fig.add_axes([1.4, 0, 0.7, 0.7])\nax2.hist(value_Mon, color='red', bins = 20)\n\nax3 = fig.add_axes([2.3, 0, 0.7, 0.7])\nax3.hist(value_Tot, color='red', bins = 20)\n\nax1.set_title('tenure histogram', loc='center', pad = 20, fontdict={'fontsize':18})\nax2.set_title('MonthlyCharges histogram', loc='center', pad = 20, fontdict={'fontsize':18})\nax3.set_title('TotalCharges histogram', loc='center', pad = 20, fontdict={'fontsize':18})\nplt.show()","3e70088f":"# Boxplots to check outliers\n\nfig, axs = plt.subplots(1, 3)\n\naxs[0].boxplot(df['tenure'], widths = 0.4, patch_artist=True)\naxs[0].set_title('tenure')\n\naxs[1].boxplot(df['MonthlyCharges'], widths = 0.4, patch_artist=True)\naxs[1].set_title('MonthlyCharges')\n\n# change outlier point symbols\naxs[2].boxplot(df['TotalCharges'], widths = 0.4, patch_artist=True)\naxs[2].set_title('TotalCharges')\n# fig, ax = plt.subplots()\n# ax.boxplot(data)\n\nfig.subplots_adjust(left=0.5, right=2, bottom=0.1,\n                    top=1, hspace=20, wspace=1)\n\nplt.show()","e4579608":"# Churn rate versus customer's TotalCharges\n\nx_value = np.sort(df['TotalCharges'].unique())\nc = [len(df[(df.TotalCharges<=i) & (df.Churn==1)])\/len(df[(df.TotalCharges<=i)])*100 for i in x_value]\nplt.figure(figsize=(12,7))\nplt.xlabel('Cumulative TotalCharges', fontsize=12)\nplt.ylabel('Percentage of Churned Customers', fontsize=12)\nplt.title('Percentage of Churned Customers Respect to Cumulative TotalCharges', fontsize=14)\nplt.grid(True)\nplt.plot(x_value, c, color='blue', linewidth=2)\nplt.show()","8b766c50":"# Churn rate versus tenure\n\nx_value = np.sort(df['tenure'].unique())\nc = [len(df[(df.tenure<=i) & (df.Churn==1)])\/len(df[(df.tenure<=i)])*100 for i in x_value]\nplt.figure(figsize=(12,7))\nplt.xlabel('Cumulative tenure', fontsize=12)\nplt.ylabel('Percentage of Churned Customers', fontsize=12)\nplt.title('Percentage of Churned Customers Respect to Cumulative tenure', fontsize=14)\nplt.grid(True)\nplt.plot(x_value, c, color='blue', linewidth=2)\nplt.show()","0ec9f3e7":"# Churn rate versus customer's MonthlyCharges\n\nx_value = np.sort(df['MonthlyCharges'].unique())\nc = [len(df[(df.MonthlyCharges<=i) & (df.Churn==1)])\/len(df[(df.MonthlyCharges<=i)])*100 for i in x_value]\nplt.figure(figsize=(12,7))\nplt.xlabel('Cumulative MonthlyCharges', fontsize=12)\nplt.ylabel('Percentage of Churned Customers', fontsize=12)\nplt.title('Percentage of Churned Customers Respect to Cumulative MonthlyCharges', fontsize=14)\nplt.grid(True)\nplt.plot(x_value, c, color='blue', linewidth=2)\nplt.show()","e7d97be3":"# Split data into input and output (No need to split into train and test. It is automatically handled by cross validation)\n\nX = df.drop(['Churn', 'customerID'] , axis=1)\nY = df['Churn']","8a5dd8d1":"# Try classifiers from various types such as linear, tree, ensemble, neighborhood, voting (ensemble), margin-based, etc.\n\n# LOG = LogisticRegression(n_jobs = -1)\n# DTC = DecisionTreeClassifier(min_samples_leaf=10, min_samples_split=10)\nADA = AdaBoostClassifier(random_state=1, n_estimators=90, learning_rate=0.9)\n# KNN = KNeighborsClassifier(n_neighbors=7, weights='distance')\n# BAG = BaggingClassifier(random_state=1)\n# VOTE = VotingClassifier(estimators=[('LOG', LOG), ('ADA', ADA)], n_jobs = -1, voting='soft', weights=[2, 5])\n# XGB = xgb.XGBClassifier()\n# CAT = cat(learning_rate=1, depth=10, loss_function='CrossEntropy')\n# SVM = SVC(class_weight='balanced')","a33391ec":"# Use GridSearchCV for Hyperparameter tuning in order to determine the optimal values for the given model.\n\npar = {'n_estimators':[80, 90, 100],\n       'learning_rate':[0.8, 0.9]}\nGS = GridSearchCV(ADA, param_grid=par, cv=5, scoring='accuracy', n_jobs=-1)\nGS.fit(X, Y)\nprint('Best score of grid search:',GS.best_score_)\nprint('Best Parameters:', GS.best_params_)","176e2ca9":"\nscores = cross_validate(ADA, X, Y, cv=5, scoring=['accuracy','precision','recall','f1', 'roc_auc'], n_jobs = -1)\nprint('Measures using k-fold cross validation:\\n')\n\nprint('accuracy of folds:', scores['test_accuracy'])\nprint('overall accuracy:',scores['test_accuracy'].mean())\n\nprint('\\nprecision of folds:', scores['test_precision'])\nprint('overall precision:',scores['test_precision'].mean())\n\nprint('\\nrecall of folds:', scores['test_recall'])\nprint('overall recall:',scores['test_recall'].mean())\n\nprint('\\nf1 of folds:', scores['test_f1'])\nprint('overall f1:',scores['test_f1'].mean())\n\nprint('\\nroc_auc of folds:', scores['test_roc_auc'])\nprint('overall roc_auc:',scores['test_roc_auc'].mean())","e0643c73":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2) # 80% train, 20% test\n\nADA.fit(X_train,Y_train)\npreds = ADA.predict(X_test)\nprint('Measures using Train\/Test split:')\nprint('\\naccuracy:', accuracy_score(Y_test, preds))\nprint('\\nprecision:', precision_score(Y_test, preds))\nprint('\\nrecall:', recall_score(Y_test, preds))\nprint('\\nf1:', f1_score(Y_test, preds))\nprint('\\nroc_auc_score:', roc_auc_score(Y_test, preds))\nprint('\\nconfusion matrix:\\n', confusion_matrix(Y_test, preds))\n\nplot_roc_curve(ADA, X_test, Y_test)\n\nplt.show()","d071a418":"## Hyperparameter Tuning","ff3ac10a":"The second inquiry investigates the relation between the percentage of churned customers and corresponding cumulative tenure. It states that around 63% of the customers who have tenure less 1 month are churned, that is, trying to retain the customer for longer makes it less probable to abandon the business.","2ee03937":"The strongest positive correlation is between tenure and TotalCharges which stands by 83%.\ntenure and Contract correlation get the next major share in our heatmap with 67%.\nLast but not least, correlation with share greater than 50% goes to TotalCharge and MonthlyCharges with 65%.","a4b26213":"We can also go through the typical train\/test split.","7f6a7ad7":"## Feature Importance\n> Feature Importance is the (normalized) total reduction of the impurity criterion brought by that feature. The higher, the more important the feature.","287389c8":"No outlier observed!\n","ed515bf0":"# Churn Prediction","e026f66c":"## Checking and Handling Null Values","c8c749b0":"TotalCharges, MonthlyCharges, and tenure are in the top 3 of feature importance list.","fc7e6c82":"## k-Fold Cross-Validation\n> It is a standard resampling method for estimating the performance of a machine learning model on data set. Shortly, this approach involves randomly dividing the data set into k groups or folds, of approximately equal size. Each folds is treated as a validation set, while the model is fit on the remaining k \u2212 1 folds.\n\n\n## Why Cross-Validation?\n> It results in a ***less biased*** or ***less optimistic*** estimate of the model performance than other methods, such as a simple train\/test split. Because it ensures that every observation from the original data set has the ***chance of appearing in training and test set***.","00651559":"## Checking Outliers\nOutliers are data points that are far from other data points. In other words, they\u2019re unusual values in dataset. Outliers are problematic for many statistical analyses because they can cause tests to either miss significant findings or distort real results.\nTwo of the most common methods to detect outliers are using ***boxplots*** and ***z-score*** test.\nIn boxplot, The data points less than (Q1 - 1.5 * IQR) and greater than (Q3 + 1.5 * IQR) are recognized as outliers and will be illustrated as individual points in boxplot.\nZ-score method assumes that data follows normal probability distribution. With such an assumption, it finds the points out of the range [mean - 3 * std , mean + 3 * std] as outliers. In our data set, the histograms of real-value attributes show that the probability distribution of the data is not normal. So, Z-score is not applicable to our case as the following histograms approve:","8de9f456":"The shape is (7043, 21) in 3 types and no null values so far.","ed196b3a":"The histograms of the real-value attributes approve that they do not follow normal probalility distribution. We can continue outlier detection with the boxplots.","d87d9417":"## Correlation\nAt this step, data is cleaned. It is time to check the correlation between the attributes:","b3a2bd91":"Employing various classifiers with considering hyperparameter tuning for this data set shows that AdaBoostClassifier results in one of the highest performance measure with a fair computational expense.","9fab17f2":"![image.png](attachment:image.png)","63365d3e":"# Exploratory Data Analysis & Feature Engineering\nIt's time to go through the EDA steps to inspect the data and have an overview of column's data types, values, missing values, outliers, etc.","4523298f":"In order to get the gist of the data, kaggle provided sufficient description of each attribute and the business case itself.\n\n![image.png](attachment:image.png)","b3e4b11e":"## Evaluation Measures","3e30c1ee":"# Building and Evaluating Model","925481c1":"> Churn prediction is detecting which customers are likely to cancel their subscription to a service based on how they use the service. By leveraging Machine Learning techniques, you will be able to anticipate potential churners who are about to abandon your services and take an appropriate marketing action for each individual customer to maximize the chance of retaining the customer.","5c6e865e":"Clearly, the data is imbalance and needs consideration of choosing classifier and evaluation measures.\nAccording to attributes' description and type of data from the primary check, there are 3 attributes of numerical type namely 'tenure', 'MonthlyCharges', 'TotalCharges'.\nTotalCharges is string in data set and needs to be converted into numerical type as follows:","5d8f7c90":"## Visualization\nTo have a better data understanding:","ad9fe6d9":"Finally, the third investigation shows that less than only 10% of customers with low MonthlyCharges will abandon the company. In other words, more than 90% of the customers whose MonthlyCharges is 20 are ready to keep on subscribing the services of the business.","71e491bb":"## Attribute Encoding\nMost of the attributes of the data frame are categorical ones and need to be numeric values in order to be handled by many ML classifiers. LabelEncoding is an appropriate teqnique for such a transformation. LabelEncoding replaces categorical feature values with integers. Another solution is One-hot Encoding in which a categorical attribute is transformed to one or more binary attributes. ","8994fcf2":"# Javad M. Rad","5be35d54":"The ROC curve above implies that the ratio TP rate\/FP rate is fairly good.","e3bf20cb":"## Shape, Columns, Types, and Null Values","ddcf9cc6":"A short inspection shows that 'TotalCharges' = 'tenure' * 'monthlyCharges' and it makes sense. In current situation, as shown above, for all the rows with Nan TotalCharges value, tenure is equal to 0. It mean that such customers are new ones. With such reasoning, the most appropriate value for Nan values of 'TotalCharges' would be 0. Dropping these rows can be an alternative. Change them all to 0 as follows and re-check if it is done:","82db7a3c":"# Business Insight\nIn order to see the relation between the real-value attributes and class labels, the percentage of churned customers are checked versus cumulative TotalCharges. Interesting related facts! First diagram, the relation between the percentage of churned customers and their cumulative TotalCharges shows that almost 53% of the customers whose TotalCharges is less than 100 are churned, that is, the less customer's TotalCharges implies the higher churn rate.","ca5a6dfc":"# Importing packages\nLet's start with importing required packages."}}