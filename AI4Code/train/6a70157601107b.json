{"cell_type":{"83d08050":"code","68e78848":"code","9f80be2b":"code","9f45f51b":"code","5b403a45":"code","5a8bd97a":"code","f49ec182":"code","434c15fd":"code","34d5bb6a":"code","a965ca96":"code","c92ba0e3":"code","e1f841d0":"code","d63c31c2":"code","49f8d2ec":"code","66dc2973":"code","45f20a08":"code","e146ef5c":"code","23d10a56":"code","05079ad6":"code","41f7cbad":"code","406fd9b7":"code","370cabd5":"code","6e434673":"code","6afbf4cc":"code","fee69601":"code","4329843f":"code","a24bc64c":"markdown","186d4ddc":"markdown","ea4f1287":"markdown","935a71dc":"markdown","6b4f589c":"markdown","393c344d":"markdown","566d06d8":"markdown","1ea9b17f":"markdown","6c274989":"markdown","6b6a89e8":"markdown","2a48ad9b":"markdown","c470a349":"markdown","0ca46d45":"markdown","64bf773b":"markdown","7d5cafd4":"markdown","f12ef17b":"markdown","d4d9d1d0":"markdown","77d76a0e":"markdown","b2c69e89":"markdown","4ea38159":"markdown","f1a76e4f":"markdown","10f05641":"markdown"},"source":{"83d08050":"## Importing all necessary libraries \nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras import backend as K","68e78848":"# Reading the folder architecture of Kaggle to get the dataset path.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9f80be2b":"# Reading the Datasets.\nmnist_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nmnist_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","9f45f51b":"# Printing the dimensions\/shape of the given data\nprint(mnist_train.shape, mnist_test.shape)","5b403a45":"# Preliminary analysis\nmnist_train.head()","5a8bd97a":"mnist_train.describe()","f49ec182":"mnist_train.isna().any().any()","434c15fd":"# dividing the data into the input and output features to train make the model learn based on what to take in and what to throw out.\nmnist_train_data = mnist_train.loc[:, \"pixel0\":]\nmnist_train_label = mnist_train.loc[:, \"label\"]\n\n# Normalizing the images array to be in the range of 0-1 by dividing them by the max possible value. \n# Here is it 255 as we have 255 value range for pixels of an image. \nmnist_train_data = mnist_train_data\/255.0\nmnist_test = mnist_test\/255.0","34d5bb6a":"# Let's make some beautiful plots.\ndigit_array = mnist_train.loc[3, \"pixel0\":]\narr = np.array(digit_array) \n\n#.reshape(a, (28,28))\nimage_array = np.reshape(arr, (28,28))\n\ndigit_img = plt.imshow(image_array, cmap=plt.cm.binary)\nplt.colorbar(digit_img)\nprint(\"IMAGE LABEL: {}\".format(mnist_train.loc[3, \"label\"]))","a965ca96":"# Let's make some beautiful plots.\ndigit_array = mnist_train.loc[4, \"pixel0\":]\narr = np.array(digit_array) \n\n#.reshape(a, (28,28))\nimage_array = np.reshape(arr, (28,28))\n\ndigit_img = plt.imshow(image_array, cmap=plt.cm.binary)\nplt.colorbar(digit_img)\nprint(\"IMAGE LABEL: {}\".format(mnist_train.loc[4, \"label\"]))","c92ba0e3":"# Let's build a count plot to see the count of all the labels.\nsns.countplot(mnist_train.label)\nprint(list(mnist_train.label.value_counts().sort_index()))","e1f841d0":"# Converting dataframe into arrays\nmnist_train_data = np.array(mnist_train_data)\nmnist_train_label = np.array(mnist_train_label)","d63c31c2":"# Reshaping the input shapes to get it in the shape which the model expects to recieve later.\nmnist_train_data = mnist_train_data.reshape(mnist_train_data.shape[0], 28, 28, 1)\nprint(mnist_train_data.shape, mnist_train_label.shape)","49f8d2ec":"# But first import some cool libraries before getting our hands dirty!! \n# TensorFlow is Google's open source AI framework and we are using is here to build model.\n# Keras is built on top of Tensorflow and gives us\n# NO MORE GEEKY STUFF, Know more about them here:  https:\/\/www.tensorflow.org     https:\/\/keras.io\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Lambda, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, AvgPool2D\nfrom tensorflow.keras.optimizers import Adadelta\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n### When the accuracy or loss starts to plateau during training we can implement the following callbacks \n#### to lower the learning rate and hence make smaller steps as it gets closer to the global optimum\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import LearningRateScheduler","66dc2973":"# Encoding the labels and making them as the class value and finally converting them as categorical values.\nnclasses = mnist_train_label.max() - mnist_train_label.min() + 1\nmnist_train_label = to_categorical(mnist_train_label, num_classes = nclasses)\nprint(\"Shape of y_train after encoding: \", mnist_train_label.shape)","45f20a08":"# This function builds the CNN Necessary for recognition, detailed explanation in the comments\n\ndef build_model(input_shape=(28, 28, 1)):\n    model = Sequential()     ## We need a sequential model obviously (don't require bidirectional, etc)\n    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = input_shape)) # First 2D Convolutional layer\n    model.add(BatchNormalization()) # Activation is Rectified Linear Unit of ReLU for all layers\n    model.add(Conv2D(32, kernel_size = 3, activation='relu')) # Batch Normalization is used along with Dropout\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    ## Dropout Regularization of 0.4 in order to avoid overfitting\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax')) ## Softmax activation used as this is a multiclass classification task\n    return model    ## The number of units is 10 as there are 10 different classes of digits","e146ef5c":"def compile_model(model, optimizer='adam', loss='categorical_crossentropy'):\n    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"]) # using adam optimization, RMSProp works fine too\n    ## Categorical crossentropy is used as the multiclass loss\n    \ndef train_model(model, train, test, epochs, split):\n    history = model.fit(train, test, shuffle=True, epochs=epochs, validation_split=split)\n    return history ## Data is shuffled during training to avoid inherent bias to the sequence of occurence of an image","23d10a56":"# Training the model using the above function built to build, compile and train the model\ncnn_model = build_model((28, 28, 1)) ## The input is an image odf size 28 X 28\ncompile_model(cnn_model, 'adam', 'categorical_crossentropy')\n\n# train the model for as many epochs as you want but I found training it above 100 will not help us and eventually \n## increase overfitting.\nmodel_history = train_model(cnn_model, mnist_train_data, mnist_train_label, 50, 0.2)","05079ad6":"def plot_model_performance(metric, validations_metric):\n    plt.plot(model_history.history[metric],label = str('Training ' + metric))\n    plt.plot(model_history.history[validations_metric],label = str('Validation ' + metric))\n    plt.legend()","41f7cbad":"### Plotting the accuracy's\nplot_model_performance('accuracy', 'val_accuracy')","406fd9b7":"## Plotting the loss\nplot_model_performance('loss', 'val_loss')","370cabd5":"# reshaping the test arrays as we did to train images above somewhere.\nmnist_test_arr = np.array(mnist_test)\nmnist_test_arr = mnist_test_arr.reshape(mnist_test_arr.shape[0], 28, 28, 1)\nprint(mnist_test_arr.shape)","6e434673":"# Now, since the model is trained, it's time to find the results for the unseen test images.\npredictions = cnn_model.predict(mnist_test_arr)","6afbf4cc":"# Finally, making the final submissions\npredictions_test = []\n\nfor i in predictions:\n    predictions_test.append(np.argmax(i))","fee69601":"## Submitting in the required format\nsubmission =  pd.DataFrame({\n        \"ImageId\": mnist_test.index+1,\n        \"Label\": predictions_test\n    })\n\nsubmission.to_csv('submission.csv', index=False)","4329843f":"submission","a24bc64c":"## Import Libraries","186d4ddc":"### Model Building Process\n> Training a neural network with one input layer, one hidden layer and one output layer for learning the digits in images.","ea4f1287":"### Prediction & Submission","935a71dc":"### Transforming testing data","6b4f589c":"#### NOTE:\n* Data is totally clean in this case (since the final result says `False` which means it has no missing values)\n* There is no empty field. Data is clean already.","393c344d":"### Data Normalization","566d06d8":"### Utility Functions ","1ea9b17f":"### Visulaize a single digit with an array","6c274989":"## Data Normalization and Cleaning","6b6a89e8":"Edit 1: Making the cells more meaningful and clearer with comments\n\nEdit 2: Adding submission file and few more visualizations.","2a48ad9b":"The distribution of data across the classes of digits is pretty much the same","c470a349":"### Kindly upvote if it was helpful! \ud83d\ude03","0ca46d45":"### Encoding train labels","64bf773b":"### Countplot for each of the 10 digits.","7d5cafd4":"The training accuracy steadily increased and plateaued while validation accuracy is also consistent. This clearly shows the model is robust!","f12ef17b":"Training for 50 epochs","d4d9d1d0":"### Building a Sequential Model","77d76a0e":"We see that there are 42000 train and 28000 test examples","b2c69e89":"# MNIST Handwritten Digits Recognition & Classification\n\n> This notebook gives a first hand Convolutional Neural Network approach using tensorflow and keras for the task of Handwritten digits recognition and classification<br><br>\n> This dataset is pretty much the 'Hello World!' of Computer Vision<br>\n> Hope this helps beginners interested in Image recognition and classification to have a jump start!","4ea38159":"## Notes:\n* Kaggle Competition Link for dataset: https:\/\/www.kaggle.com\/c\/digit-recognizer\n* About the dataset: The dataset consists of 10 classes of handwritten numbers between 0-9 as Images.\n* Jump in! \ud83d\ude03","f1a76e4f":"I hope you liked it.<br>","10f05641":"### Model Performance Analysis"}}