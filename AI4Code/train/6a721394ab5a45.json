{"cell_type":{"9303cf2c":"code","737b236e":"code","351ec4cf":"code","61557066":"code","44829643":"code","a2eff8a6":"code","7364fffb":"code","9be0663f":"code","d9722c21":"code","dd50f7bc":"code","03aaf23e":"code","d73b1e6f":"code","59067728":"code","6f588757":"code","ea2b0d0d":"code","81268dbf":"code","1dea2b2d":"code","5fa9ae49":"code","3b2a132a":"code","4ac1c2a9":"code","54e2cb0b":"code","aafcdbbf":"code","e9c48f3a":"code","ef10a2aa":"code","c6e32168":"code","5e2f0bd7":"code","5c3c8c86":"code","1d109a5b":"code","390dce03":"code","83c23ab4":"code","6cafc6fb":"code","9b24f3d7":"code","f6d287ed":"code","bd98e9fc":"code","ac80e9b3":"code","18b4145c":"code","a6365ae1":"code","2a10c4d2":"code","17804aa9":"code","47a4fd36":"code","a07ca349":"code","5235089f":"code","681edb74":"code","7d3ee1c9":"code","0c920e69":"code","049aeb76":"code","63f76190":"code","221a42cf":"code","8ab7f2e9":"code","c1ce2284":"code","ed32340f":"code","c7a64752":"code","5fb87671":"code","8c22bab3":"code","bbec043e":"code","97924bb4":"code","3318da5d":"code","62a9fe2e":"code","35c2a51c":"code","261c80a8":"code","8de890ce":"code","d7dd1a6d":"code","eec9ba52":"code","5420df9f":"code","0213ac16":"code","a3bf76a6":"code","4226863e":"markdown","47ed5867":"markdown","2ca2dd20":"markdown","a4a764f0":"markdown","953bd839":"markdown","66b5ccce":"markdown","bfd20dbe":"markdown","d255a201":"markdown","dd43ddda":"markdown","3fe083a7":"markdown","4b841c8b":"markdown","e22392f5":"markdown","900301d1":"markdown","47c8c9dd":"markdown","13ebcce1":"markdown","5fc92995":"markdown","03a3707b":"markdown"},"source":{"9303cf2c":"!nvidia-smi","737b236e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport zipfile\nimport csv\nimport sys\nimport os\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import optimizers\n\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.models as Model\nfrom tensorflow.keras.layers import *\nimport tensorflow.keras.layers as layers\n\nfrom tensorflow.keras.applications.xception import Xception\n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport PIL\nfrom PIL import ImageOps, ImageFilter\n\n#\u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n\n#\u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432 svg \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u0442\u043a\u0438\u043c\u0438\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nprint(os.listdir(\"..\/input\"))\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)\nprint('Keras        :', tf.keras.__version__)","351ec4cf":"!pip freeze > requirements.txt","61557066":"# \u0412 setup \u0432\u044b\u043d\u043e\u0441\u0438\u043c \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u0442\u0430\u043a \u0443\u0434\u043e\u0431\u043d\u0435\u0435 \u0438\u0445 \u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u0442\u044c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c.\n\nEPOCHS               = 10  # \u044d\u043f\u043e\u0445 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\nBATCH_SIZE           = 16 # \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u043c batch \u0435\u0441\u043b\u0438 \u0441\u0435\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0430\u044f, \u0438\u043d\u0430\u0447\u0435 \u043d\u0435 \u0432\u043b\u0435\u0437\u0435\u0442 \u0432 \u043f\u0430\u043c\u044f\u0442\u044c \u043d\u0430 GPU\nLR                   = 1e-4\nVAL_SPLIT            = 0.15 # \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 \u0442\u0435\u0441\u0442 = 15%\n\nCLASS_NUM            = 10  # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\nIMG_SIZE             = 224 # \u043a\u0430\u043a\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043f\u043e\u0434\u0430\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u0441\u0435\u0442\u044c\nIMG_CHANNELS         = 3   # \u0443 RGB 3 \u043a\u0430\u043d\u0430\u043b\u0430\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n\nDATA_PATH = '..\/input\/'\nPATH = \"..\/working\/car\/\" # \u0440\u0430\u0431\u043e\u0447\u0430\u044f \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f","44829643":"# \u0423\u0441\u0442\u0430\u043d\u0430\u043b\u0438\u0432\u0430\u0435\u043c \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 random seed \u0434\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438\nos.makedirs(PATH,exist_ok=True)\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)  \nPYTHONHASHSEED = 0","a2eff8a6":"train_df = pd.read_csv(DATA_PATH+\"train.csv\")\nsample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\ntrain_df.head()","7364fffb":"train_df.info()","9be0663f":"sns.barplot(y=train_df.Category.value_counts().values,\n           x=train_df.Category.value_counts().index,\n           color='r')\n# \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0435 - \u044d\u0442\u043e \u0445\u043e\u0440\u043e\u0448\u043e","d9722c21":"print('\u0420\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438')\n# Will unzip the files so that you can see them..\nfor data_zip in ['train.zip', 'test.zip']:\n    with zipfile.ZipFile(\"..\/input\/\"+data_zip,\"r\") as z:\n        z.extractall(PATH)\n        \nprint(os.listdir(PATH))","dd50f7bc":"os.listdir(PATH + 'train\/'+'1')","03aaf23e":"print('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a (random sample)')\nplt.figure(figsize=(12,8))\n\nrandom_image = train_df.sample(n=9)\nrandom_image_paths = random_image['Id'].values\nrandom_image_cat = random_image['Category'].values","d73b1e6f":"for index, path in enumerate(random_image_paths):\n    im = PIL.Image.open(PATH+f'train\/{random_image_cat[index]}\/{path}')\n    plt.subplot(3,3, index+1)\n    plt.imshow(im)\n    plt.title('Class: '+str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","59067728":"image = PIL.Image.open(PATH+'\/train\/0\/100380.jpg')\nimgplot = plt.imshow(image)\nplt.show()\nimage.size","6f588757":"! pip install git+https:\/\/github.com\/mjkvaak\/ImageDataAugmentor","ea2b0d0d":"import tensorflow as tf\nfrom ImageDataAugmentor.image_data_augmentor import *\nimport albumentations","81268dbf":"os.listdir(PATH+'train\/')","1dea2b2d":"AUGMENTATIONS = albumentations.Compose([\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.Rotate(limit=30, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n    albumentations.OneOf([\n        albumentations.CenterCrop(height=224, width=200),\n        albumentations.CenterCrop(height=200, width=224),\n    ],p=0.5),\n    albumentations.OneOf([\n        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n    ],p=0.5),\n    albumentations.GaussianBlur(p=0.05),\n    albumentations.HueSaturationValue(p=0.5),\n    albumentations.RGBShift(p=0.5),\n    albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n])","5fa9ae49":"# dataloaders\ntrain_datagen = ImageDataAugmentor(\n        rescale=1.\/255,\n        augment=AUGMENTATIONS,\n        validation_split=VAL_SPLIT)\n\ntest_datagen = ImageDataAugmentor(rescale=1. \/ 255,\n                                 validation_split=VAL_SPLIT\n                                 )\n\n# train_datagen = ImageDataGenerator(rescale=1. \/ 255, \n#                                     rotation_range = 30,\n#                                     shear_range=0.1,\n#                                     zoom_range=[0.85,1.15],\n#                                     brightness_range=[0.5, 1.5],\n#                                     width_shift_range=0.1,\n#                                     height_shift_range=0.1,\n#                                     horizontal_flip=True,\n#                                     validation_split=0.2) #  \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442\n\n\n# test_datagen = ImageDataGenerator(rescale=1. \/ 255,\n#                                  validation_split=0.2)","3b2a132a":"# data generators\ntrain_generator = train_datagen.flow_from_directory(\n        PATH+'train\/',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True, \n        seed=RANDOM_SEED,\n        subset='training')\n\ntest_generator = test_datagen.flow_from_directory(\n        PATH+'train\/',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True, \n        seed=RANDOM_SEED,\n        subset='validation')\n\ntest_sub_generator = test_datagen.flow_from_dataframe( \n    dataframe=sample_submission,\n    directory=PATH+'test_upload\/',\n    x_col=\"Id\",\n    y_col=None,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,)","4ac1c2a9":"# \u00a0\u0413\u043b\u044f\u043d\u0435\u043c \u043d\u0430 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u0440\u0438\u0442\u043d\u043a\u0438\n\nfrom skimage import io\n\ndef imshow(image_RGB):\n  io.imshow(image_RGB)\n  io.show()\n\nx,y = train_generator.next() # \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c \u0442\u0440\u0435\u0439\u043d-\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\nprint('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 train_generator')\nplt.figure(figsize=(12,8))\n\nfor i in range(0,6):\n    image = x[i]\n    plt.subplot(3,3, i+1)\n    plt.imshow(image)\n    #plt.title('Class: '+str(y[i]))\n    #plt.axis('off')\nplt.show()","54e2cb0b":"x,y = test_generator.next() # \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c \u0442\u0435\u0441\u0442-\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\nprint('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 test_generator')\nplt.figure(figsize=(12,8))\n\nfor i in range(0,6):\n    image = x[i]\n    plt.subplot(3,3, i+1)\n    plt.imshow(image)\n    #plt.title('Class: '+str(y[i]))\n    #plt.axis('off')\nplt.show()","aafcdbbf":"!pip install efficientnet","e9c48f3a":"import efficientnet.tfkeras as efn","ef10a2aa":"base_model = efn.EfficientNetB6(weights='imagenet', include_top=False, input_shape=input_shape)","c6e32168":"#base_model.summary()","5e2f0bd7":"# \u0417\u0430\u043c\u043e\u0440\u0430\u0436\u0438\u0432\u0430\u0435\u043c \u0432\u0435\u0441\u0430 \u0432 \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\nbase_model.trainable = False","5c3c8c86":"# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n# steps_per_execution=32\n\nmodel2 = Model.Sequential()\nmodel2.add(base_model)\n# model2.add(layers.Conv2D(32, 3, activation = 'relu', padding = 'same', \n#                         input_shape = input_shape))\n# model2.add(layers.BatchNormalization())\n\nmodel2.add(layers.GlobalAveragePooling2D())\nmodel2.add(layers.BatchNormalization()) \n#model2.add(layers.Dropout(0.25))\n   \nmodel2.add(layers.Dense(256, activation='relu'))\nmodel2.add(layers.Dropout(0.25))\n\n#model2.add(layers.BatchNormalization())\n#     model2.add(layers.Dropout(0.25))\n\nmodel2.add(layers.Dense(CLASS_NUM, activation='softmax'))\n\nmodel2.compile(loss=\"categorical_crossentropy\", \n               optimizer=optimizers.Adam(lr=LR), \n               metrics=[\"accuracy\"],\n               )","1d109a5b":"# \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0435\u0432\nprint(len(model2.layers))","390dce03":"# \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0435\u0432 \u043e\u0431\u0443\u0447\u0438\u043b\u043e\u0441\u044c\nprint(len(model2.trainable_variables))","83c23ab4":"\ncheckpoint = ModelCheckpoint('best_model.hdf5', \n                             monitor = ['val_accuracy'], \n                             verbose = 1, \n                             mode = 'max')\n\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                             patience=3, \n                                             restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25,\n                              patience=2, min_lr=0.0000001, verbose=1,\n                             mode='auto')\n\ncallbacks_list = [checkpoint,earlystop,reduce_lr] # lrscheduler,lr_callback","6cafc6fb":"history = model2.fit(\n        train_generator,\n        steps_per_epoch = train_generator.samples\/\/train_generator.batch_size,\n        validation_data = test_generator, \n        validation_steps = test_generator.samples\/\/test_generator.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)\n","9b24f3d7":"# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0438\u0442\u043e\u0433\u043e\u0432\u0443\u044e \u0441\u0435\u0442\u044c \u0438 \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044e \u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 (best_model)\nmodel2.save('..\/working\/model_last.hdf5')\nmodel2.load_weights('best_model.hdf5')","f6d287ed":"scores = model2.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","bd98e9fc":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","ac80e9b3":"round(model2.optimizer.lr.numpy(), 5)","18b4145c":"# \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u043f\u043e\u0441\u0442\u0435\u043f\u0435\u043d\u043d\u0443\u044e \u0440\u0430\u0437\u043c\u043e\u0440\u043e\u0437\u043a\u0443 \u0441\u043b\u043e\u0435\u0432 \u0434\u043b\u044f \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nbase_model.trainable = True\n\n# Fine-tune from this layer onwards\nfine_tune_at = len(base_model.layers)\/\/2 # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043b\u043e\u0435\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False","a6365ae1":"\nmodel2.compile(loss=\"categorical_crossentropy\", \n               optimizer=optimizers.Adam(lr=LR), \n               metrics=[\"accuracy\"])","2a10c4d2":"history = model2.fit(\n        train_generator,\n        steps_per_epoch = train_generator.samples\/\/train_generator.batch_size,\n        validation_data = test_generator, \n        validation_steps = test_generator.samples\/\/test_generator.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","17804aa9":"scores = model2.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","47a4fd36":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","a07ca349":"# train_generator_2 = train_datagen_2.flow_from_directory(\n#         PATH+'train\/',\n#         target_size=(IMG_SIZE, IMG_SIZE),\n#         batch_size=8,\n#         class_mode='categorical',\n#         shuffle=True, \n#         seed=RANDOM_SEED,\n#         subset='training')\n\n# test_generator_2 = train_datagen_2.flow_from_directory(\n#         PATH+'train\/',\n#         target_size=(IMG_SIZE, IMG_SIZE),\n#         batch_size=8,\n#         class_mode='categorical',\n#         shuffle=True, seed=RANDOM_SEED,\n#         subset='validation')\n\n# test_sub_generator = test_datagen.flow_from_dataframe( \n#     dataframe=sample_submission,\n#     directory=PATH+'test_upload\/',\n#     x_col=\"Id\",\n#     y_col=None,\n#     shuffle=False,\n#     class_mode=None,\n#     seed=RANDOM_SEED,\n#     target_size=(IMG_SIZE, IMG_SIZE),\n#     batch_size=8)","5235089f":"base_model.trainable = True\n\nLR = 1e-5\nmodel2.compile(loss=\"categorical_crossentropy\", \n               optimizer=optimizers.Adam(lr=LR), \n               metrics=[\"accuracy\"])\n\nhistory = model2.fit(\n        train_generator,\n        steps_per_epoch = train_generator.samples\/\/train_generator.batch_size,\n        validation_data = test_generator, \n        validation_steps = test_generator.samples\/\/test_generator.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","681edb74":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","7d3ee1c9":"EPOCHS               = 6\nBATCH_SIZE           = 4 # \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u043c batch \u0435\u0441\u043b\u0438 \u0441\u0435\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0430\u044f, \u0438\u043d\u0430\u0447\u0435 \u043d\u0435 \u0432\u043b\u0435\u0437\u0435\u0442 \u0432 \u043f\u0430\u043c\u044f\u0442\u044c \u043d\u0430 GPU\nLR                   = 1e-5\n\nIMG_SIZE             = 512 # \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\nIMG_CHANNELS         = 3\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)","0c920e69":"AUGMENTATIONS = albumentations.Compose([\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.Rotate(limit=30, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n\n])\n\n# dataloaders\ntrain_datagen_2 = ImageDataAugmentor(\n        rescale=1.\/255,\n        augment=AUGMENTATIONS,\n        validation_split=VAL_SPLIT)\n\ntest_datagen_2 = ImageDataAugmentor(rescale=1. \/ 255,\n                                 validation_split=VAL_SPLIT\n                                 )","049aeb76":"# data generators\ntrain_generator_2 = train_datagen_2.flow_from_directory(\n        PATH+'train\/',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True, \n        seed=RANDOM_SEED,\n        subset='training')\n\ntest_generator_2 = test_datagen_2.flow_from_directory(\n        PATH+'train\/',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True, \n        seed=RANDOM_SEED,\n        subset='validation')\n\ntest_sub_generator = test_datagen.flow_from_dataframe( \n    dataframe=sample_submission,\n    directory=PATH+'test_upload\/',\n    x_col=\"Id\",\n    y_col=None,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,)","63f76190":"# x,y = train_generator_2.next() # \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c \u0442\u0440\u0435\u0439\u043d-\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\n# print('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 train_generator')\n# plt.figure(figsize=(12,8))\n\n# for i in range(0,6):\n#     image = x[i]\n#     plt.subplot(3,3, i+1)\n#     plt.imshow(image)\n#     #plt.title('Class: '+str(y[i]))\n#     #plt.axis('off')\n# plt.show()","221a42cf":"# x,y = test_generator_2.next() # \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c test-\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\n# print('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 test_generator')\n# plt.figure(figsize=(12,8))\n\n# for i in range(0,6):\n#     image = x[i]\n#     plt.subplot(3,3, i+1)\n#     plt.imshow(image)\n#     #plt.title('Class: '+str(y[i]))\n#     #plt.axis('off')\n# plt.show()","8ab7f2e9":"# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0431\u0430\u0437\u043e\u0432\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\n\nbase_model = efn.EfficientNetB6(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# # \u0423\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u043d\u043e\u0432\u0443\u044e \"\u0433\u043e\u043b\u043e\u0432\u0443\" (head)\n\nmodel2 = Model.Sequential()\nmodel2.add(base_model)\n\nmodel2.add(layers.GlobalAveragePooling2D())\nmodel2.add(layers.BatchNormalization())  \n\nmodel2.add(layers.Dense(256, activation='relu'))\nmodel2.add(layers.Dropout(0.25))\n\nmodel2.add(layers.Dense(CLASS_NUM, activation='softmax'))\n\nmodel2.compile(loss=\"categorical_crossentropy\", \n               optimizer=optimizers.Adam(lr=LR), \n               metrics=[\"accuracy\"],\n               )","c1ce2284":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0435\u0441\u0430 \u0443\u0436\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\nmodel2.load_weights('best_model.hdf5')","ed32340f":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                              patience=2, min_lr=0.0000001, verbose=1,\n                             mode='auto')\n\ncallbacks_list = [checkpoint,earlystop,reduce_lr]","c7a64752":"\nhistory = model2.fit(\n        train_generator_2,\n        steps_per_epoch = train_generator_2.samples\/\/train_generator_2.batch_size,\n        validation_data = test_generator_2, \n        validation_steps = test_generator_2.samples\/\/test_generator_2.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","5fb87671":"scores = model2.evaluate_generator(test_generator_2, steps=len(test_generator_2), verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","8c22bab3":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","bbec043e":"from sklearn.metrics import accuracy_score","97924bb4":"test_sub_generator.samples","3318da5d":"test_sub_generator.reset()\npredictions = model2.predict_generator(test_sub_generator, steps=len(test_sub_generator), verbose=1) \npredictions = np.argmax(predictions, axis=-1) #multiple categories\nlabel_map = (train_generator_2.class_indices)\nlabel_map = dict((v,k) for k,v in label_map.items()) #flip k,v\npredictions = [label_map[k] for k in predictions]","62a9fe2e":"filenames_with_dir=test_sub_generator.filenames\nsubmission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\nsubmission['Id'] = submission['Id'].replace('test_upload\/','')\nsubmission.to_csv(PATH+'submission.csv', index=False)\nprint('Save submit')\n","35c2a51c":"submission.to_csv('submission.csv', index=False)","261c80a8":"submission.head()","8de890ce":"model2.load_weights('best_model.hdf5')","d7dd1a6d":"\nAUGMENTATIONS = albumentations.Compose([\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.Rotate(limit=30, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n    albumentations.OneOf([\n        albumentations.CenterCrop(height=224, width=200),\n        albumentations.CenterCrop(height=200, width=224),\n    ],p=0.5),\n    albumentations.OneOf([\n        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n    ],p=0.5),\n    albumentations.GaussianBlur(p=0.05),\n    albumentations.HueSaturationValue(p=0.5),\n    albumentations.RGBShift(p=0.5),\n    albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n])\n\n# dataloaders\ntest_datagen = ImageDataAugmentor(\n        rescale=1.\/255,\n        augment=AUGMENTATIONS,\n        validation_split=VAL_SPLIT)\n\n\ntest_sub_generator = test_datagen.flow_from_dataframe( \n    dataframe=sample_submission,\n    directory=PATH+'test_upload\/',\n    x_col=\"Id\",\n    y_col=None,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,)","eec9ba52":"tta_steps = 10\npredictions = []\n\nfor i in range(tta_steps):\n    preds = model2.predict_generator(test_sub_generator, verbose=1) \n    predictions.append(preds)\n\npred = np.mean(predictions, axis=0)","5420df9f":"predictions = np.argmax(pred, axis=-1) #multiple categories\nlabel_map = (train_generator_2.class_indices)\nlabel_map = dict((v,k) for k,v in label_map.items()) #flip k,v\npredictions = [label_map[k] for k in predictions]\n\nfilenames_with_dir=test_sub_generator.filenames\nTTA_submission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\nTTA_submission['Id'] = TTA_submission['Id'].replace('test_upload\/','')\nTTA_submission.to_csv('TTA_submission.csv', index=False)\nprint('Save submit')","0213ac16":"TTA_submission.head(5)","a3bf76a6":"# Clean PATH\n# import shutil\n# shutil.rmtree(PATH)","4226863e":"## \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\n#### **\u0412\u043f\u0438\u043b\u0438\u043c \u043d\u043e\u0432\u044b\u0439 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0442\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445**","47ed5867":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438 \u0438\u0445 \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0447\u0442\u043e\u0431 \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c \u043a\u0430\u043a \u0438\u0445 \u043b\u0443\u0447\u0448\u0435 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0438 \u0441\u0436\u0438\u043c\u0430\u0442\u044c.","2ca2dd20":"# \u041e\u0431\u0443\u0447\u0430\u0435\u043c. \u042d\u0442\u0430\u043f 1:","a4a764f0":"# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435. \u042d\u0442\u0430\u043f 2","953bd839":"# \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n\n### \u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0438\u0434\u0435\u044f \u044d\u0442\u043e\u0433\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u044f: \u0432\u0437\u044f\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u0443\u044e \u043d\u0430 ImageNet \u0441\u0435\u0442\u044c EfficientNet \u0438 \u0434\u043e\u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043f\u043e\u0434 \u043d\u0430\u0448\u0443 \u0437\u0430\u0434\u0430\u0447\u0443. \n","66b5ccce":"#  \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435. \u042d\u0442\u0430\u043f 4. \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a","bfd20dbe":"# \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","d255a201":"# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","dd43ddda":"# EDA \/ \u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445","3fe083a7":"# TTA","4b841c8b":"\u0414\u043e\u0431\u0430\u0432\u0438\u043c ModelCheckpoint \u0447\u0442\u043e\u0431 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u043f\u043e\u0442\u043e\u043c \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0438 \u0434\u043e\u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c.","e22392f5":"\u041f\u0440\u043e\u0442\u0435\u0441\u0442\u0438\u0442\u0443\u0435\u043c, \u043a\u0430\u043a \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c EfficientNetB4","900301d1":"## \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","47c8c9dd":"# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435. \u042d\u0442\u0430\u043f 3","13ebcce1":"# \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","5fc92995":"# \u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438","03a3707b":"**\u0420\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0441 Tensorflow v2**"}}