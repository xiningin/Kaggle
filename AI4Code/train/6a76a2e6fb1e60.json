{"cell_type":{"20ee1d70":"code","4de067a7":"code","1efe3d96":"code","17f65db8":"code","02511108":"code","0ed413d6":"code","97a39a40":"code","a501907b":"code","f16e25cd":"code","2f1683ac":"code","d6c98a8d":"code","99554961":"code","6399fb6e":"code","cbaaf78e":"code","d8dfd99c":"code","11c4aadf":"code","eb9cba05":"code","6f04e1e3":"code","dc284f3a":"code","0b94beca":"code","68a9f66d":"code","a26fc514":"code","4ea5abc2":"code","2aeb4cdc":"code","02668eb6":"code","2a3b2192":"code","26bad54c":"code","6de2c9ca":"code","ca1f071f":"code","f0e3ac08":"code","6391eb4b":"code","0e487bf2":"code","14ef34cb":"code","1916e9bd":"code","db7bff9c":"code","dfbed551":"code","870bd5d3":"code","54e3edb3":"markdown","0a2bb742":"markdown","c52a2fd7":"markdown","39814fa6":"markdown","c3b29ef1":"markdown","b54a0910":"markdown","24213846":"markdown","547bbc16":"markdown","7d07549a":"markdown","2433fa97":"markdown","6c35deb8":"markdown","a04d02e4":"markdown","37ae4806":"markdown","bfe5598f":"markdown","85cecda5":"markdown","a7f1941b":"markdown","a8cb9a82":"markdown","e05baaea":"markdown","3b8e7e5f":"markdown","8699f520":"markdown","19b44b2e":"markdown","b7d798e2":"markdown","b8dc175a":"markdown","118d0907":"markdown","56d779d6":"markdown","297e5ec4":"markdown"},"source":{"20ee1d70":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n#import time for training details\nfrom time import time\nt0 = time()","4de067a7":"# Importing and Reading the Dataset\ndf_hp_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_hp_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","1efe3d96":"df_hp_train_row_count, df_hp_train_column_count=df_hp_train.shape\nprint('Total number of rows (train):', df_hp_train_row_count)\nprint('Total number of columns (train):', df_hp_train_column_count)\n\ndf_hp_test_row_count, df_hp_test_column_count=df_hp_test.shape\nprint('Total number of rows (test):', df_hp_test_row_count)\nprint('Total number of columns (test):', df_hp_test_column_count)","17f65db8":"df_hp_train.describe()","02511108":"df_hp_train.info()","0ed413d6":"df_hp_train.isna().sum().sort_values(ascending=False)","97a39a40":"df_hp_test.info()","a501907b":"df_hp_test.isna().sum().sort_values(ascending=False)","f16e25cd":"# training data\nTotal_tr = df_hp_train.isnull().sum()\npercent_tr = (df_hp_train.isnull().sum() \/ df_hp_train.isnull().count())\nmissing_data_tr = pd.concat([Total_tr, round(percent_tr*100, 2)], axis=1, keys=['Total', 'Percent'])\n\n#considering missing data >5%\nresult_tr=missing_data_tr[percent_tr>0.05] \nresult_tr.sort_values('Percent',ascending=False)","2f1683ac":"# test data\nTotal_tt = df_hp_test.isnull().sum()\npercent_tt = (df_hp_test.isnull().sum()\/df_hp_test.isnull().count())\nmissing_data_tt = pd.concat([Total_tt, round(percent_tt*100,2)], axis=1, keys=['Total', 'Percent'])\n\n#considering missing data >5%\nresult_tt=missing_data_tt[percent_tt>0.05]\nresult_tt.sort_values('Percent',ascending=False)","d6c98a8d":" sns.distplot(df_hp_train['SalePrice'])","99554961":"#importing required library and package to fix the skew \nimport math\nfrom scipy.stats import norm, skew\n\ndf_hp_train['SalePrice'] = np.log1p(df_hp_train['SalePrice'])\nsns.distplot(df_hp_train['SalePrice'], fit=norm);","6399fb6e":"#correlation map\ncorr = df_hp_train.corr()\n\n# finding highest correlation between features and SalesPrice\n\nhighest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5] #>0.5 would indicate a stronger correlation\nplt.figure(figsize=(12,12))\ng = sns.heatmap(df_hp_train[highest_corr_features].corr(),annot=True,linewidths=.5, fmt= '.1f',cmap=\"icefire\")","cbaaf78e":"# understanding correlation values\nround(corr['SalePrice'].sort_values(ascending=False),2)","d8dfd99c":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\nsns.pairplot(df_hp_train[cols])","11c4aadf":"#columns are same, we first separate the SalePrice column from the train dataset\n\ny_train = df_hp_train['SalePrice']\n\n#concatenating the data\ndf_merge = [df_hp_train, df_hp_test]\nmerged_data = pd.concat(df_merge, sort=False)\n\n# check the dataset after merge\nmerged_data.shape","eb9cba05":"#dropping Id and target variable columns\nmerged_data = merged_data.drop(['Id', 'SalePrice'], axis=1)\n\n# check the dataset again\nmerged_data.shape","6f04e1e3":"# total count of missing values for each feature\ntotal = merged_data.isnull().sum().sort_values(ascending=False)\n\n#percentage of missing values for each feature\npercent = round((merged_data.isnull().sum() \/ merged_data.isnull().count())*100,2).sort_values(ascending=False)\n\n# missing values information\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n#considering missing data >5%\nresult=missing_data[percent>0.05]\nresult.head(30)","dc284f3a":"# drop the columns \nmerged_data.drop((missing_data[missing_data['Total'] > 5]).index, axis=1, inplace=True)","0b94beca":"# features and total missing values \ntotal = merged_data.isnull().sum().sort_values(ascending=False)\ntotal.head(30)","68a9f66d":"# categorical data\ncat_missed = ['Electrical','Exterior1st', 'Exterior2nd','KitchenQual','MSZoning','SaleType',]\n\nfor value in cat_missed:\n    merged_data[value] = merged_data[value].fillna(merged_data[value].mode()[0])\n    \n# numerical data\nnum_missed = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','BsmtFullBath','BsmtHalfBath','GarageArea','GarageCars','TotalBsmtSF']\n\nfor value in num_missed:\n    merged_data[value] = merged_data[value].fillna(0)   ","a26fc514":"merged_data.isnull().sum().sort_values(ascending=False).iloc[:10]","4ea5abc2":"#  remaining missing values can be filled with common values for that feature\n\nmerged_data['Functional'] = merged_data['Functional'].fillna('Typ')\nmerged_data['Utilities'] = merged_data['Utilities'].fillna('AllPub')","2aeb4cdc":"merged_data.isnull().sum().sort_values(ascending=False).iloc[:3]","02668eb6":"#numerical features\nnumeric_features = merged_data.dtypes[merged_data.dtypes != 'object'].index\nskewed_features = merged_data[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nh_skew = skewed_features[abs(skewed_features) > 0.5]\nprint (h_skew)","2a3b2192":"for value in h_skew.index:\n    merged_data[value] = np.log1p(merged_data[value])\n\nmerged_data.shape","26bad54c":"#new column\nmerged_data['totalSF'] = merged_data['TotalBsmtSF'] + merged_data['1stFlrSF'] + merged_data['2ndFlrSF']","6de2c9ca":"# one-hot encoding\nmerged_data = pd.get_dummies(merged_data)\nmerged_data.head()","ca1f071f":"#Defining train and test datasets\n\nX_train =merged_data[:len(y_train)]\nX_test = merged_data[len(y_train):]\n\nX_train.shape,X_test.shape","f0e3ac08":"#import required libraries and packages\n\nfrom sklearn import preprocessing, svm \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression \n\n#linear regression module\n\nregr = LinearRegression() \nregr.fit(X_train, y_train)\n\ntrain_score = regr.score(X_train, y_train)\n\n#print training score\nprint(\"training score: \", train_score)\n\n# making predictions on test data\n\ny_pred=regr.predict(X_test)  \ntest_score = regr.score(X_test, y_pred)\nprint(\"test score:\", test_score )","6391eb4b":"# perform cross-validation\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef CV_train(data):\n    kf = KFold(5,shuffle=True,random_state=42).get_n_splits(x_train.values)\n    rmse = np.sqrt(-cross_val_score(model, x_train, y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef CV_test(data):\n    kf = KFold(5,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, x_test, y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)","0e487bf2":"# using XGBoost \n\nimport xgboost as XGB\n\nthe_model = XGB.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.01, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state =7, nthread = -1)\nthe_model.fit(X_train, y_train)","14ef34cb":"# Predicting SalePrice\n\ny_pred = np.floor(np.expm1(the_model.predict(X_test)))\ny_pred\n\n#print predicted values\n\nprint(\"Predicted SalePrice:\", y_pred)","1916e9bd":"# generating csv for predicted prices \n\npred = pd.DataFrame()\n\n# defining id\ntest_id = df_hp_test['Id']\npred['Id'] = test_id\n\npred['SalePrice'] = y_pred\npred.to_csv('predicted_SalePrice.csv',index=False)","db7bff9c":"from catboost import CatBoostRegressor\nmodel1 = CatBoostRegressor(random_state=42,iterations = 10000,learning_rate=0.005,\n                           early_stopping_rounds=50,max_depth=3)\nmodel1.fit(X_train, y_train, verbose=1)\n","dfbed551":"# Predicting SalePrice\n\ny_pred1 = np.floor(np.expm1(model1.predict(X_test)))\ny_pred1\n\n#print predicted values\n\nprint(\"Predicted SalePrice:\", y_pred1)","870bd5d3":"# generating csv for predicted prices \n\npred1 = pd.DataFrame()\n\n# defining id\ntest_id = df_hp_test['Id']\npred1['Id'] = test_id\n\npred1['SalePrice'] = y_pred1\npred1.to_csv('predicted_SalePrice1.csv',index=False)","54e3edb3":"### Filling the missing data\n\n- We fill the categorical data first and then the numerical data\n- Categorical missing values can be easily replaced with the most frequently occuring value i.e. mode\n- Numerical missing values can either be replaced by mean or by 0. Since these features do not have a strong correlation with target variable, we replace the missing values with 0\n\nIt is also possible to do the same with a SimpleImputer class from scikit-learn but here we use fillna to handle missing values.","0a2bb742":"### Check again to see if there any missing values.","c52a2fd7":"### Handling the Missing Data: \nIt might be challenging to work on the missing data in train and test files separately. \nSo, for the sake of convenience, we are concatenating the train and test datasets and after preprocessing, we divide them again.","39814fa6":"### <h3 style=\"background-color:#4cc9f0;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Importing & Reading the dataset \ud83d\udcdd <\/centre><\/strong><\/h3>","c3b29ef1":"### <h3 style=\"background-color:#4cc9f0;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Applying ML algorithm for prediction \ud83c\udfd8 <\/centre><\/strong><\/h3>","b54a0910":"### <h3 style=\"background-color:#4cc9f0;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Importing Libraries & Packages \ud83d\udcda <\/centre><\/strong><\/h3>","24213846":"Exploring the dataset","547bbc16":"Now, all the missing values have been fixed and the dataset is ready for feature engineering.","7d07549a":"- This correlation heat map shows that 'OverQual'has highest correlation of 0.8. \n- Similarly,'GarageCars' & 'GarageArea' have a correlation of 0.7 each and they are also sharing a correlation of 0.9 with each other.  \n- Next, we can see that, 'TotalBsmtSF' & '1stFlrSF' have a correlation of 0.6 and their correlation with each other is 0.8, which also high. This means, it might be a good idea to add the '1stFlrSF' to  'TotalBsmtSF' for convenience. \n- Finally, 'TotRmsAbvGrd' & 'GrLivArea' each have 0.5 correlation value with the 'SalePrice' and have 0.8 correlation with each other.","2433fa97":"### <h3 style=\"background-color:#4cc9f0;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Checking for Datatypes & missing values\u270f\ufe0f <\/centre><\/strong><\/h3>","6c35deb8":"### Converting the categorical entries to numerical\n- This we can achieve easily using the pandas get_dummies() function which replaces categorical columns with their one-hot representations.","a04d02e4":"The target variable 'SalePrice' seems to be positively skewed. This is undesirable while making predictions. Hence, normalization is required to adjust the skew of this variable","37ae4806":"![Orange and Blue Geometric Pattern General Linkedin Banner(1).jpg](attachment:9d2d0c6d-5213-4b82-a4c0-f1bb137cdff6.jpg)","bfe5598f":"With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. ","85cecda5":"Data preprocessing is complete. Next, we need to split the data back to train and test sets.","a7f1941b":"11 features have missing data >5%","a8cb9a82":"### <h3 style=\"background-color:#4cc9f0;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Checking for skewness of the target variable <\/centre><\/strong><\/h3>","e05baaea":"Two categorical columns still have missing values. We replace 'Functional' column missing data with most frequent entry 'Typ' while we replace 'Utilities' with 'AllPub'","3b8e7e5f":"#### After adjusting the skew, next we need to check for correlation of the features with the target variable through a heatmap.","8699f520":"None of the above features with missing values are important features as their correlation is not > 0.5 as per the heatmap, so it is possible to delete them without affecting the prediction accuracy.","19b44b2e":"Knowing the % of missing data for each feature will give a better idea and select how to handle missing values before preparing the dataset for the model","b7d798e2":"### We can now explore the top 5 features that have highest correlation with the 'SalePrice'.","b8dc175a":"### <h3 style=\"background-color:#4c9ff0;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Feature Engineering <\/centre><\/strong><\/h3>\n\nSince we fixed the skew in target variable, we also need to fix the skewness in the other features","118d0907":"### Finding missing values","56d779d6":"### With these pairplots, we explored all the data. Based on this, we have a good understanding of the important features and their effect on the target variable.","297e5ec4":"Next, we will add columns 'TotalBsmtSF','1stFlrSF' and '2ndFlrSF' to create a new column 'totalSF' based on the observations from the correlation heatmap."}}