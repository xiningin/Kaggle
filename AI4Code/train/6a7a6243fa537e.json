{"cell_type":{"5c15a9d6":"code","6f9aa822":"code","ea297f94":"code","ecc4298b":"code","d6fee9d8":"code","74378a9a":"code","911a094d":"code","be742e72":"code","32edcaf8":"code","5dac6d33":"code","213ee4e9":"code","1437cdd3":"code","a118703d":"code","8bb45757":"code","ced4e8b5":"code","471c86bf":"code","187aaa23":"code","38bd4059":"code","b2cde7f0":"code","e22bb0b1":"code","8d8407a4":"code","8bfe0f9b":"code","68a091ec":"code","4c6d7942":"code","1ce670d2":"code","be1892cb":"code","9d24cdc8":"code","9954b968":"code","6bd9d586":"code","f3eca448":"code","c17fb4f7":"code","2d8814c0":"code","d776b1e3":"code","d09d5f81":"code","c7d4ab26":"code","ce624855":"code","593be800":"markdown","363527db":"markdown","7aee801e":"markdown","bf739658":"markdown","c1d2a6eb":"markdown"},"source":{"5c15a9d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output.","6f9aa822":"# read csv (comma separated value) into data\ndata = pd.read_csv('..\/input\/column_2C_weka.csv')\n","ea297f94":"data.head()","ecc4298b":"data.info()","d6fee9d8":"data.describe()","74378a9a":"#First I visualize the seaborn heatmap to see  the correlation between the features to choose two with max. corr.\n#and these are pelvic_incidence and sacral_slope\nfig,ax = plt.subplots(figsize=(7,5))\n\nax = sns.heatmap(\ndata.corr(), \nannot=True, annot_kws={'size':8},\nlinewidths=.3,linecolor=\"blue\", fmt= '.2f',square=True,cmap=\"YlGnBu_r\",cbar=False)\nplt.show()","911a094d":"data.loc[:,'class'].value_counts()","be742e72":"# For my kernel I used \"pelvic_incidence\" and \"sacral_slope\" for class \"Normal\"\ndata1 = data.loc[data['class'] =='Normal']\ndata1.head()","32edcaf8":"#numpy array for x_axis\n#numpy array for y_axis\nx = data1.pelvic_incidence.values\ny = data1.sacral_slope.values\n# Scatter\nplt.figure(figsize=[6,6])\nplt.scatter(x,y,color=\"magenta\")\nplt.xlabel('pelvic_incidence',fontsize = 25,color='blue')\nplt.ylabel('sacral_slope',fontsize = 25,color='blue')\nplt.show()\n","5dac6d33":"#Linear Regression\ndenominator= x.dot(x)-x.mean()*x.sum()\na= (x.dot(y)-y.mean()*x.sum())\/denominator\nb= (y.mean()*x.dot(x)-x.mean()*x.dot(y))\/denominator\n    \nYhat= a*x + b  # best fitting line\n\nplt.figure(figsize=[6,6])\nplt.scatter(x,y,color=\"magenta\")\nplt.plot(x,Yhat)\nplt.xlabel('pelvic_incidence',fontsize = 25,color='blue')\nplt.ylabel('sacral_slope',fontsize = 25,color='blue')\nplt.show()","213ee4e9":"#accuracy\nd1= y-Yhat\nd2= y-y.mean()\nr2= 1-(d1.dot(d1)\/d2.dot(d2))\nprint(\"the r-squared is:\",r2)","1437cdd3":"# Linear Regression with sklearn\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nX = x.reshape(-1,1)\nY = y.reshape(-1,1)\nlr.fit(X,Y)\ny_pred = lr.predict(X)  \n\n# plot scatter\nplt.figure(figsize=[6,6])\nplt.scatter(X,Y,color=\"magenta\")\nplt.xlabel('pelvic_incidence',fontsize = 25,color='blue')\nplt.ylabel('sacral_slope',fontsize = 25,color='blue')\n\n#plot regression line\nplt.plot(X, y_pred)\nplt.show()\n\n","a118703d":"#accuracy\nprint(\"R^2 score: \", lr.score(X,Y))\n#or\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \",r2_score(Y,y_pred))","8bb45757":"#Multiple Linear Regression\n#load data (I took lumbar_lordosis_angle as x2)\nx = data1.iloc[:, [0,2]].values\n# I added a column of ones (noise)\nX = np.insert(x,0,1,axis=1)\nY = data1.sacral_slope.values\nprint(X.shape)\nprint(Y.shape)","ced4e8b5":"trace1 = go.Scatter3d(\n    x=X[:,0],\n    y=X[:,1],\n    z=Y,\n    mode='markers',\n    marker=dict(\n        size=8,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    ),\n    \n)\n\ndata_ = [trace1]\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=data_, layout=layout)\n\niplot(fig)\n","471c86bf":"w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, Y))\nYhat = np.dot(X, w)","187aaa23":"#accuracy\nd1 = Y - Yhat\nd2 = Y - Y.mean()\nr2 = 1 - d1.dot(d1) \/ d2.dot(d2)\nprint(\"the r-squared is:\", r2)","38bd4059":"#Decision Tree Regression\n# I took first 15 samples of X and Y\n\nx = data1.iloc[:15,[0]].values.reshape(-1,1)\ny = data1.iloc[:15,[2]].values.reshape(-1,1)\n\nfrom sklearn.tree import DecisionTreeRegressor\ndes_tree_reg = DecisionTreeRegressor()\n\ndes_tree_reg.fit(x,y)\n\n\nx_new = np.linspace(min(x),max(x)).reshape(-1,1)\nx_new1 = np.arange(min(x),max(x),0.01).reshape(-1,1)\nyhat_new = des_tree_reg.predict(x_new)\nyhat_new1 = des_tree_reg.predict(x_new1)\nplt.subplot(111)\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new,yhat_new)\nplt.show()\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new1,yhat_new1)\nplt.show()","b2cde7f0":"#Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nrandom_for = RandomForestRegressor(n_estimators =100, random_state = 42)\n\nrandom_for.fit(x,y)\n\n\nx_new = np.linspace(min(x),max(x)).reshape(-1,1)\nx_new1 = np.arange(min(x),max(x),0.01).reshape(-1,1)\nyhat_new = random_for.predict(x_new)\nyhat_new1 = random_for.predict(x_new1)\nplt.subplot(111)\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new,yhat_new)\nplt.show()\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new1,yhat_new1)\nplt.show()","e22bb0b1":"#accuracy\nfrom sklearn.metrics import r2_score\nyhat_rf = random_for.predict(x)\n\nprint(\"r_square score: \", r2_score(y,yhat_rf))\n# or:\nprint(\"r2: \", random_for.score(x,y))\n","8d8407a4":"# linear regression with gradient descent\n# I took first 15 samples of X and Y and multiply them with 0.1\nx = data1.iloc[:15, [0,2]].values\nx = x * 0.1\n# I added a column of ones (noise)\nX = np.insert(x,0,1,axis=1)\ny = data1.sacral_slope.values\nY = y[:15] * 0.1\n\n\nprint(X.shape)\nprint(Y.shape)\nD = X.shape[1]\nN = X.shape[0]","8bfe0f9b":"#plot the data\nfig = plt.figure()\nax = fig.add_subplot(111, projection =\"3d\")\nax.scatter(X[:,0],X[:,1],Y,c=\"r\",marker=\"o\")\nplt.show()","68a091ec":"cost = []\nw = np.random.randn(D)\/np.sqrt(D)\nlearning_rate = 0.001\n\nfor i in range(300):\n    Yhat = X.dot(w)\n    interval = Yhat - Y\n    w = w - learning_rate * X.T.dot(interval)\n    # mean squared error\n    # 1\/N *(Yhat -Y)        for N ...samples\n    mse = interval.dot(interval) \/ N\n    cost.append(mse)\n\nplt.plot(cost)\nplt.show()","4c6d7942":"print(w)","1ce670d2":"plt.plot(Yhat, label=\"prediction\")\nplt.plot(Y ,label=\"target\")\n\nplt.legend()\nplt.show()","be1892cb":"#accuracy\nd1 = Y - Yhat\nd2 = Y - Y.mean()\nr2 = 1 - d1.dot(d1) \/ d2.dot(d2)\nprint(\"the r-squared is:\", r2)","9d24cdc8":"# linear regression with l2 regularization(Ridge)\n# I took the same data as with gradient descent\nx = data1.iloc[:15, [0,2]].values\nx = x * 0.1\n# I added a column of ones (noise)\nX = np.insert(x,0,1,axis=1)\ny = data1.sacral_slope.values\nY = y[:15] * 0.1\n# I added +20 to the last 2 samples of Y\nY[-1] += 20\nY[-2] += 20\nprint(X.shape)\nprint(Y.shape)\nD = X.shape[1]\nN = X.shape[0]","9954b968":"plt.scatter(X[:,1],Y)\nplt.show()\n","6bd9d586":"# maximum likelihood\nw_ml = np.linalg.solve(X.T.dot(X),X.T.dot(Y))\nYhat_ml = X.dot(w_ml)\n\nplt.scatter(X[:,1],Y)\nplt.plot(sorted(X[:,1]),sorted(Yhat_ml))\nplt.show()","f3eca448":"# l2 regularization (Ridge)\nl2 = 1000.0\nw_ridge = np.linalg.solve(l2*np.eye(3) + X.T.dot(X),X.T.dot(Y))\nYhat_ridge = X.dot(w_ridge)\n\nplt.scatter(X[:,1],Y)\nplt.plot(sorted(X[:,1]),sorted(Yhat_ml), label = \"maximum likelihood\")\nplt.plot(sorted(X[:,1]),sorted(Yhat_ridge), label = \"l2_reg\/ridge\")\nplt.legend()\nplt.show()","c17fb4f7":"#Logistic Regression\ndata[\"class\"] = [\"1\" if each ==\"Normal\" else \"0\" for each in data[\"class\"]]\ndata.head()","2d8814c0":"data.loc[:,'class'].value_counts()","d776b1e3":"y = data[\"class\"].values\nx_ = data.drop([\"class\"],axis=1)\nprint(y.shape,x_.shape)\nx_.head()","d09d5f81":"#Normalisation\nx = (x_ - np.min(x_))\/(np.max(x_) - np.min(x_)).values\nx.head()","c7d4ab26":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.2,random_state=42)\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","ce624855":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ntest_acc = lr.score(x_test,y_test)\nprint(\"accuracy:\",test_acc)","593be800":"# This kernel is about linear regression between some features of database: \" Biomechanical features of orthopedic patients\"\n# I started with data analysis","363527db":"# w = (X\u1d40X)\u00af\u00b9* X\u1d40Y\n# w = np.linalg.solve((X\u1d40X)\u00af\u00b9, X\u1d40Y)\n# Y (predictions) is equal X.w","7aee801e":"# Y = a*x + b\n# a = (n\u2211y\u00a1x\u00a1 - \u2211y\u00a1\u2211x\u00a1) \/ n\u2211x\u00a1\u00b2 - (\u2211x\u00a1)\u00b2\n# b = (\u2211y\u00a1\u2211x\u00a1\u00b2 - \u2211x\u00a1\u2211y\u00a1x\u00a1) \/ n\u2211x\u00a1\u00b2 - (\u2211x\u00a1)\u00b2","bf739658":"# R\u00b2 = 1 -(Residual Sum of Squares \/ Total Sum of Squares)\n# RSS = \u2211(y\u00a1 - y\u00a1^)\u00b2\n# TSS = \u2211(y\u00a1 - mean of y)\u00b2","c1d2a6eb":"# I tried to give a short example about some different methods of linear regression.\n# I hope it was helpful"}}