{"cell_type":{"c711c45a":"code","aa0185e3":"code","a05870a7":"code","9720c285":"code","4cb485e0":"code","049ca150":"code","537c36ef":"code","e7a03a5a":"code","b403ce13":"code","1ea3a244":"code","b7f33874":"code","0c6ba22f":"code","9049c08f":"code","c9080a6c":"code","8ee1d038":"code","18caee96":"code","2c7c369a":"code","2ec2b4b3":"code","4f590cf1":"code","ac7e5d6b":"code","9616b750":"code","11a7f526":"code","b4937f8e":"code","53fdaf43":"code","d5b67ea4":"code","93c12b66":"code","e3033f8d":"code","1962637f":"code","535b093f":"code","ce886db9":"code","e879e3f2":"markdown","ec3003ba":"markdown","b8c7bcc0":"markdown","dd206fa4":"markdown","bd5ae6c8":"markdown","6dc5c9c8":"markdown","797b1e0f":"markdown","4d8d45f3":"markdown","00bbebef":"markdown","59868994":"markdown","4dafa7c2":"markdown","f9fb8fb2":"markdown","e8830e51":"markdown","f8742179":"markdown","ba3e4d7a":"markdown","8855c5c3":"markdown"},"source":{"c711c45a":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/airline-passenger-satisfaction\/train.csv\")","aa0185e3":"df.head()","a05870a7":"df.columns","9720c285":"drop_cols = [\"Unnamed: 0\" , \"id\"]\nnum_cols = [\"Age\" , \"Flight Distance\" ,\"Departure Delay in Minutes\" , \"Arrival Delay in Minutes\"]\ny_col = \"satisfaction\"\ncat_cols = list(set(df.columns).difference(set(drop_cols+num_cols)))","4cb485e0":"from sklearn.preprocessing import OneHotEncoder","049ca150":"ohe = OneHotEncoder(drop=\"first\")\ncat_df = pd.DataFrame(ohe.fit_transform(df[cat_cols]).todense() , columns=ohe.get_feature_names(cat_cols))","537c36ef":"from sklearn.preprocessing import MinMaxScaler","e7a03a5a":"mms = MinMaxScaler()\nnum_df = pd.DataFrame(mms.fit_transform(df[num_cols]) , columns=num_cols)","b403ce13":"X = cat_df.join(num_df)\ny = df[y_col]","1ea3a244":"X = X.fillna(X.mean())","b7f33874":"from sklearn.model_selection import train_test_split","0c6ba22f":"X_train , X_test , y_train  ,y_test = train_test_split(X,y)","9049c08f":"from sklearn.cluster import KMeans","c9080a6c":"kmeans = KMeans(2)\nkmeans.fit(X_train)","8ee1d038":"from sklearn.manifold import TSNE","18caee96":"tsne = TSNE(2 , n_iter=250)\ntransformed_df = tsne.fit_transform(X_train)","2c7c369a":"from plotnine import *","2ec2b4b3":"(\n    ggplot(pd.DataFrame({\"c1\" : transformed_df[: , 0]\n                        ,\"c2\" : transformed_df[: , 1]\n                        ,\"cluster\" : kmeans.labels_}) , aes(x=\"c1\" , y=\"c2\" , fill=\"cluster\"))\n    + geom_point(alpha=0.4 , stroke=0)\n)","4f590cf1":"(\n    ggplot(pd.DataFrame({\"c1\" : transformed_df[: , 0]\n                        ,\"c2\" : transformed_df[: , 1]\n                        ,\"y\" : y_train}) , aes(x=\"c1\" , y=\"c2\" , fill=\"y\"))\n    + geom_point(alpha=0.4 , stroke=0)\n)","ac7e5d6b":"result_df = X_train.copy()\nresult_df[\"cluster\"] = kmeans.labels_","9616b750":"melt_cluster = result_df.groupby(\"cluster\").mean().reset_index().melt(id_vars=\"cluster\")\nmelt_cluster","11a7f526":"melt_cluster = melt_cluster[\"variable\"].str.split(\"_\" , expand=True).join(melt_cluster)\nmelt_cluster = melt_cluster.rename({0 : \"variable_base\" , 1:\"response\"} , axis=1)\nmelt_cluster","b4937f8e":"melt_cluster[\"variable_base\"].unique()","53fdaf43":"melt_cluster[\"cluster\"] = melt_cluster[\"cluster\"].astype(\"category\")","d5b67ea4":"(\n    ggplot(melt_cluster[~melt_cluster[\"response\"].isna()],aes(x=\"response\" , y=\"value\" ,fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(12,10))\n)","93c12b66":"kmeans = KMeans(6)\nkmeans.fit(X_train)","e3033f8d":"def get_melted_clusters(labels):\n    result_df = X_train.copy()\n    result_df[num_cols] = mms.inverse_transform(X_train[num_cols])\n    result_df[\"cluster\"] = labels\n    melt_cluster = result_df.groupby(\"cluster\").mean().reset_index().melt(id_vars=\"cluster\")\n    melt_cluster = melt_cluster[\"variable\"].str.split(\"_\" , expand=True).join(melt_cluster)\n    melt_cluster = melt_cluster.rename({0 : \"variable_base\" , 1:\"response\"} , axis=1)\n    melt_cluster[\"cluster\"] = melt_cluster[\"cluster\"].astype(\"category\") \n    return melt_cluster","1962637f":"mc2 = get_melted_clusters(kmeans.labels_)","535b093f":"(\n    ggplot(mc2[~mc2[\"response\"].isna()],aes(x=\"response\" , y=\"value\" ,fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(12,10))\n)","ce886db9":"(\n    ggplot(mc2[mc2[\"response\"].isna()],aes(x=\"cluster\" , y=\"value\" ,fill=\"cluster\"))\n    + geom_col()\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\" , scales=\"free_x\")\n    + theme(figure_size=(12,5),subplots_adjust={'hspace': 0.5})\n)","e879e3f2":"Let's split the data we have into a training and testing set","ec3003ba":"Now we join our categorical dataframe with our numerical dataframe.","b8c7bcc0":"We can now see that both graphs look pretty similar and therefore we have been able to come up with labels very similar to the ground truth using an `unsupervised approach` , purely based on the data we have.\n\n### Cluster Features Analysis\n\nNow that we have our cluster labels, we can see how these clusters are different from one another in terms of the original feature set that we had.\n\nTo do that we group by the `label` found and then take the mean of each of our variable. This allows us to compare each individual feature for each of the cluster we have found","dd206fa4":"### Finally Down to clustering!","bd5ae6c8":"Thing with KMeans clustering is that it takes as an input parameter `Number of Clusters (k)` to find. Therefore you might want to experiment with different numbers and keep the one that makes most sense while interpreting results.\n\nWe can also use `Elbow Method` to figure out a good `k`.\n\nFor now we first try 2 clusters as there are two labels for `satisfaction` variable","6dc5c9c8":"Performing KMeans clustering is fast enough and therefore different values of `k` can be tried out quickly.\n\nTo visually analyse the results of clustering we use `TSNE`. Like `PCA`, `TSNE` helps reduce the dimensionality of the data at hand. Therefore to be able to plot our individual data points we reduce the dimensionality to `2`.\n\nNOTE : TSNE takes time. Patience is Mandatory","797b1e0f":"From the above chart we can eliminate one other question `checkin service`\n\nLet's try to summarize each cluster\n\n1. Cluster 0:\n    1. Mostly satisfied with their experience\n    2. Key for them seems `Inflight Entertainment`\n    2. Specially Happy with `On-board Service , Cleaniness , Food and Drinks`\n2. Cluster 1:\n    1. Quite Unhappy with their experience\n    2. Key for them `Cleaniness, Seat Comfort , Inflight Entertainment`\n    3. As we will see below , this is the youngest group on average and unsatisfied with most in-flight services\n3. Cluster 2:\n    1. This is the oldest group with avg age being close to 50\n    2. Most satisfied with `Cleaniness and Food and Drinks`\n    3. Mostly approve of the `Seat comfort` and `Inflight Entertainment`\n    4. This is also the group that travels mostly in long distance flights and does not experience `delays` (likley because they get moved to a different flight being a business customer)\n4. Cluster 3:\n    1. This is the `'middle'` cluster , where people have mostly rated 3\n5. Cluster 4:\n    1. This is the group of people that despite experiencing delays is mostly satisfied with their experience\n6. Cluster 5:\n    1. This group is unhappy with most of their experience","4d8d45f3":"Any missing values are filled using `Mean Imputation` below.","00bbebef":"# Let's do Clustering Analysis\n\n* Goal is to be able to segment the population and see what features drive their satisfaction.\n* First we read in the training data (won't focus on the test set)","59868994":"We can see that the labels found mostly don't overlap and therefore we have a good separation of the data points using the `KMeans` labels. To compare this to the `ground truth` labels we have in `satisfaction` variable, let's do another plot.","4dafa7c2":"We have around 25 different features available","f9fb8fb2":"### Interpreting the results\n\nLooking at the above graphs, it turns out that our cluster `0` has most people rating `1 or 2` to the survey questions and cluster `1` has most people rating `4 or 5` for most of the questions. That isn't very helpful. BUT it does help us eliminate few of the questions like \n1. `Departure\/Arrival Time convenience` and \n2. `Gate Location` \n3. `Ease of Online Booking`\n\nbecause irrespective of the cluster label we see the response spread all over `1-5`.\n\nAnother important deduction is that people in the `'unsatisfied'` category are mostly the personal travel folks whereas the business travel people are mostly satisified.\n\n\nTo do even better at the interpretation. Let's re-run KMeans clustering, this time for 6 clusters (arbitrary).","e8830e51":"Now we one hot encode all the categorical columns.\n* This is primarily because we are going to use a clustering algorithm which will require all columns to be numerical and also of similar scale.\n* One Hot Encoding will convert all categorical columns to numerical columns with values `0 or 1`\n\nFor Numerical Columns we use `MinMaxScaler` from sklearn to squeeze them into `0-1`","f8742179":"Let's separate them out into numerical and categorical columns and the columns we need to drop. The column that holds the labels is `satisfaction` , we save it as `y_col`","ba3e4d7a":"Now that we have our reduced data, we can plot to see our cluster labels are compared to the true labels `(satisfaction) column`.\n\nLet's call the 2 dimensions of the tsne returned data `c1` and `c2`. We take the labels found from `KMeans` using `kmeans.labels_`. We then plot `c1` on `xaxis`, `c2` on `y-axis` and `color` using `labels` that we have found.","8855c5c3":"To be able to plot easily, we also melt the dataframe so that we get two columns, one containing the original feature name and the other containing the mean value."}}