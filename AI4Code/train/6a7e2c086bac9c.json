{"cell_type":{"4ff72799":"code","fd5d34a0":"code","53fbd3e2":"code","b7ce6b79":"code","e3b40434":"code","67fc2af9":"code","13092eba":"code","f34f88f5":"code","a6e28a6d":"code","372b2f14":"code","e37e5987":"code","c3a8366e":"code","0ae91992":"code","6b1dfb98":"code","e141313b":"code","16cd87aa":"code","28ab7365":"code","6f3491ad":"code","6a2050bf":"code","6d03c1c1":"code","e2cf843a":"code","3edd07c2":"code","5ca2649b":"code","c0f5197c":"code","482b2911":"code","f5832a9b":"code","df38e80b":"code","50564878":"code","83e477b7":"code","f09ae2e7":"code","c062da31":"code","556bffbf":"code","11e80100":"code","74829e58":"code","7e6ca1da":"markdown","2c0e0e84":"markdown","3480341c":"markdown","3380135f":"markdown","a0b51758":"markdown","ee644a5f":"markdown","4e5717b4":"markdown","b7acc288":"markdown","35ba6e24":"markdown","cc477610":"markdown","1395bda2":"markdown","8085e2c2":"markdown","6d4426ad":"markdown","fe0b86cb":"markdown","576053fe":"markdown","07a6a5f2":"markdown"},"source":{"4ff72799":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd5d34a0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","53fbd3e2":"stroke_df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n\nstroke_df.head()","b7ce6b79":"#Drop the 'id' column as it is irrelevant in the prediction\nstroke_df = stroke_df.drop('id', axis=1)\nstroke_df.head()","e3b40434":"stroke_df['smoking_status'].replace('Unknown', np.nan, inplace=True)","67fc2af9":"stroke_df.isna().sum()","13092eba":"stroke_df['bmi'].fillna(stroke_df['bmi'].mean(), inplace=True)\nstroke_df['smoking_status'].fillna(stroke_df['smoking_status'].mode()[0], inplace = True)\n\nstroke_df.isna().sum()","f34f88f5":"from sklearn.preprocessing import LabelEncoder\ndef label_encoded(feat):\n    le = LabelEncoder()\n    le.fit(feat)\n    print(feat.name,le.classes_)\n#     print(le.classes_)\n    return le.transform(feat)","a6e28a6d":"for col in stroke_df.columns:\n    stroke_df[str(col)] = label_encoded(stroke_df[str(col)])","372b2f14":"stroke_df","e37e5987":"#People who have not had a stroke\nstroke_False = stroke_df[stroke_df['stroke'] == 0]\n\nstroke_False.head()","c3a8366e":"print('People who have not had a stroke in percentage-',len(stroke_False)\/len(stroke_df)*100,'%')","0ae91992":"#People who have had a stroke\nstroke_True = stroke_df[stroke_df['stroke'] == 1]\n\nstroke_True.head()","6b1dfb98":"print('People who have had a stroke in percentage-',len(stroke_True)\/len(stroke_df)*100,'%')","e141313b":"#Correlation\nplt.figure(figsize=(10,10))\nsns.heatmap(stroke_df.corr(), vmin=-1, cmap='coolwarm', annot=True);","16cd87aa":"#VISUALIZE\nsns.pairplot(stroke_df, hue = 'stroke', vars=[\n'gender','age','hypertension','heart_disease','ever_married','work_type','Residence_type','avg_glucose_level','bmi','smoking_status','stroke'])","28ab7365":"stroke_df","6f3491ad":"#Drop the target label i.e. the stroke column\nX = stroke_df.drop(['stroke'],axis =1)\n\nX","6a2050bf":"#output target class\ny = stroke_df['stroke']\n\ny","6d03c1c1":"#Now we need to split trained data and split data i.e. divide data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)","e2cf843a":"#3577 sample data points for training\nX_train.shape\ny_train.shape","3edd07c2":"#1533 samples for testing\nX_test.shape\ny_test.shape","5ca2649b":"#Train a decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()   #Instantiate an object out of our class\ndecision_tree.fit(X_train,y_train)","c0f5197c":"from sklearn.metrics import classification_report , confusion_matrix\nfrom sklearn.metrics import accuracy_score","482b2911":"#Plot the confusion matrix for the testing data\ny_predict_test = decision_tree.predict(X_test)\n\ny_predict_test","f5832a9b":"y_test","df38e80b":"cm = confusion_matrix(y_test, y_predict_test)","50564878":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n# labels, title and ticks\nax.set_xlabel('True', fontsize=20)\nax.xaxis.set_label_position('top') \nax.xaxis.set_ticklabels(['Did not suffer stroke','suffered stroke'], fontsize = 10)\nax.xaxis.tick_top()\n\nax.set_ylabel('Predicted', fontsize=20)\nax.yaxis.set_ticklabels(['Did not suffer stroke', 'suffered stroke'], fontsize = 10)\nplt.show()","83e477b7":"print(classification_report(y_test, y_predict_test))","f09ae2e7":"#Random forest classifier to improve the model\nfrom sklearn.ensemble import RandomForestClassifier\nRandomForest = RandomForestClassifier(n_estimators = 150)\nRandomForest.fit(X_train, y_train)","c062da31":"#predicting on test data\ny_predict_test = RandomForest.predict(X_test)","556bffbf":"#creating confusion matrix for test prediction\ncm = confusion_matrix(y_test, y_predict_test)","11e80100":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n# labels, title and ticks\nax.set_xlabel('True', fontsize=20)\nax.xaxis.set_label_position('top') \nax.xaxis.set_ticklabels(['Did not suffer stroke','suffered stroke'], fontsize = 10)\nax.xaxis.tick_top()\n\nax.set_ylabel('Predicted', fontsize=20)\nax.yaxis.set_ticklabels(['Did not suffer stroke', 'suffered stroke'], fontsize = 10)\nplt.show()","74829e58":"print(classification_report(y_test, y_predict_test))","7e6ca1da":"#### There are certain factors which influence the chances of getting a stroke. This dataset contains a person's information like gender, age, hypertension, heart_disease, ever_married, work_type, Residence_type, avg_glucose_level, bmi, smoking_status and we have to predict whether they will get a stroke or not.","2c0e0e84":"#### There are \"Unknown\" values in the smoking_status cloumn. We will treat them as missing values.","3480341c":"# Importing Libraries","3380135f":"# Dropping unnecessary columns","a0b51758":"# Using LabelEncoder()","ee644a5f":"# Filling missing values","4e5717b4":"# TESTING AND TRAINING DATA","b7acc288":"## Importing data","35ba6e24":"# EVALUATING MODEL","cc477610":"# Missing Values","1395bda2":"# TRAINING MODEL using RandomForestClassifier","8085e2c2":"#### We will encode target labels with values between 0 and n_classes-1.","6d4426ad":"![image.png](attachment:image.png)","fe0b86cb":"# ABOUT DATA","576053fe":"### **The accuracy improved from 91% to 95% when RandomForestClassifier was used instead of DecisionTreeClassifier and thus, the accuracy score of RandomForestClassifier is better than that of DecisionTreeClassifier.**","07a6a5f2":"# TRAINING DATA using DecisionTreeClassifier"}}