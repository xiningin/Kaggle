{"cell_type":{"04aa29f0":"code","1a396d70":"code","8427bfd2":"code","a75446cf":"code","45280bd4":"code","076f49e5":"code","d34432b7":"code","42074c44":"code","88cb3c74":"code","cca2df51":"code","970486bc":"code","59d3747f":"code","8ffe26d3":"code","01031d7b":"code","dc063a6b":"code","8b8477df":"code","67b2aefa":"code","1b6dd8d9":"code","be0f011d":"code","8fe68595":"code","dbf34fc9":"code","b38de84d":"code","789cbb7d":"code","c3fa7589":"code","c38e5068":"code","54aa15dc":"code","31f0b952":"code","e2ead788":"code","b2dc34ee":"code","6915d6c5":"code","6a8b642e":"code","194317e5":"code","0f5ce016":"code","8883a2c2":"code","714c700b":"code","11c90991":"code","ec7db604":"code","e905dcfd":"code","6e08b70f":"code","24a0cb1c":"code","a44c506f":"code","35cbb71e":"code","9583b716":"code","b0f66729":"code","c9cb6348":"code","c950ec4a":"code","b7eb8176":"code","477f8197":"code","fadc3188":"code","6b567060":"code","717aea65":"code","7ac492f6":"code","db436f52":"code","d736f14e":"code","818cea4e":"markdown","2fe56fa3":"markdown","3dd3b210":"markdown","28e4279e":"markdown","3bea47b8":"markdown","7f3321d4":"markdown","c4d8bf74":"markdown","fef4475f":"markdown","be125a92":"markdown","e643d459":"markdown","f49fc6a1":"markdown","6a51c4b1":"markdown","60137a82":"markdown","487cf192":"markdown"},"source":{"04aa29f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a396d70":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nimport random\n!pip install beautifulsoup4\nfrom bs4 import BeautifulSoup\nimport tqdm\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport time\nimport gc","8427bfd2":"Datathon_Yetenek = pd.read_csv(\"\/kaggle\/input\/datathon-who-gets-the-job\/Datathon_Yetenek.csv\")\nDatathon_Yetenek.head()","a75446cf":"Datathon_Tecrube = pd.read_csv(\"\/kaggle\/input\/datathon-who-gets-the-job\/Datathon_Tecrube.csv\")\nDatathon_Tecrube.head()","45280bd4":"Datathon_Aday = pd.read_csv(\"\/kaggle\/input\/datathon-who-gets-the-job\/Datathon_Aday.csv\")\nDatathon_Aday.head()","076f49e5":"Datathon_Basvuru = pd.read_csv(\"\/kaggle\/input\/datathon-who-gets-the-job\/Datathon_Basvuru.csv\")\nDatathon_Basvuru.head()","d34432b7":"Datathon_ilan = pd.read_csv(\"\/kaggle\/input\/datathon-who-gets-the-job\/Datathon_ilan.csv\")\nDatathon_ilan.head()","42074c44":"test = pd.read_csv(\"\/kaggle\/input\/datathon-who-gets-the-job\/test.csv\")\ntest.head()","88cb3c74":"Datathon_Basvuru_iseAlinanlar = pd.read_csv(\"\/kaggle\/input\/datathon-who-gets-the-job\/Datathon_Basvuru_iseAlinanlar.csv\")\nDatathon_Basvuru_iseAlinanlar.head()","cca2df51":"Datathon_Basvuru","970486bc":"# for train\nrandom.seed(6)\nise_alinanlar = set([(each[0], each[1]) for each in np.array(Datathon_Basvuru_iseAlinanlar)])\nise_alinanlar_test = set(random.sample(ise_alinanlar, 700))\nise_alinanlar_test_ilan = set([each[1] for each in ise_alinanlar_test])\nise_alinanlar_train = ise_alinanlar - ise_alinanlar_test","59d3747f":"\"\"\"\n# for test\nise_alinanlar = set([(each[0], each[1]) for each in np.array(Datathon_Basvuru_iseAlinanlar)])\n#ise_alinanlar_test = set(random.sample(ise_alinanlar, 700))\nise_alinanlar_test_ilan = set([each[0] for each in np.array(test)])\nise_alinanlar_train = ise_alinanlar\n\n\"\"\"","8ffe26d3":"# for train\nprint(\"ise_alinanlar:\", len(ise_alinanlar))\nprint(\"ise_alinanlar_test:\", len(ise_alinanlar_test))\nprint(\"ise_alinanlar_test_ilan:\", len(ise_alinanlar_test_ilan))\nprint(\"ise_alinanlar_train:\", len(ise_alinanlar_train))\n","01031d7b":"Datathon_Basvuru[\"target\"] = [int((each[0], each[1]) in ise_alinanlar_train) for each in np.array(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]])]\nDatathon_Basvuru","dc063a6b":"Datathon_Basvuru[\"target\"].value_counts()","8b8477df":"Datathon_Basvuru = Datathon_Basvuru.sample(frac=1).reset_index(drop=True)\nDatathon_Basvuru","67b2aefa":"jobdescriptions = []\nfor desc in tqdm.tqdm(Datathon_ilan[\"ilanMetni\"].values):\n    jobdescriptions.append(BeautifulSoup(desc).text.replace(u'\\xa0', u' ').translate(str.maketrans(string.punctuation, ' '*len(string.punctuation), \"\")).lower())\nDatathon_ilan[\"ilanMetni_processed\"] = jobdescriptions\njobdescriptions = []\nfor desc in tqdm.tqdm(Datathon_ilan[\"Nitelikler\"].values):\n    jobdescriptions.append(BeautifulSoup(desc).text.replace(u'\\xa0', u' ').translate(str.maketrans(string.punctuation, ' '*len(string.punctuation), \"\")).lower())\nDatathon_ilan[\"Nitelikler_processed\"] = jobdescriptions\nDatathon_ilan[\"text_all\"] =  Datathon_ilan[\"ilanMetni_processed\"] + \" \" + Datathon_ilan[\"Nitelikler_processed\"]","1b6dd8d9":"Datathon_Tecrube[\"Aciklama\"] = Datathon_Tecrube[\"Aciklama\"].astype(str).str.lower() + \" \"\nadayid_to_aciklama_textdata_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"Aciklama\"].sum())\nDatathon_Basvuru[\"Aday_aciklama_text_len\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_aciklama_textdata_dict).str.len().fillna(0)","be0f011d":"aday_positions_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"PozisyonAdi\"].unique())\naday_sektors_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"SektorAdi\"].unique())\naday_sehirs_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"SehirAdi\"].unique())\naday_aciklamas_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"Aciklama\"].unique())\nilan_positions_dict = dict(Datathon_ilan.groupby(\"IlanId\")[\"pozisyonAdi\"].first())\nilan_sektors_dict = dict(Datathon_ilan.groupby(\"IlanId\")[\"sektorAdi\"].first())\nilan_sehirs_dict = dict(Datathon_ilan.groupby(\"IlanId\")[\"lokasyon\"].first())\nilan_aciklamas_dict = dict(Datathon_ilan.groupby(\"IlanId\")[\"text_all\"].first())\n\nismatch_feature = []\nfor each in tqdm.tqdm(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]].values):\n    ismatch_feature.append(int(ilan_positions_dict[each[1]] in aday_positions_dict[each[0]]))\nDatathon_Basvuru[\"is_match\"] = ismatch_feature\n\nismatch_sektor_feature = []\nfor each in tqdm.tqdm(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]].values):\n    ismatch_sektor_feature.append(int(ilan_sektors_dict[each[1]] in aday_sektors_dict[each[0]]))\nDatathon_Basvuru[\"is_match_sektor\"] = ismatch_sektor_feature\n\nismatch_sehir_feature = []\nfor each in tqdm.tqdm(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]].values):\n    ismatch_sehir_feature.append(int(ilan_sehirs_dict[each[1]] in aday_sehirs_dict[each[0]]))\nDatathon_Basvuru[\"is_match_sehir\"] = ismatch_sehir_feature","8fe68595":"aday_positions_words_dict = {}\nfor temp_key in tqdm.tqdm(aday_positions_dict):\n    temp_all_words = []\n    for each_word in aday_positions_dict[temp_key]:\n        temp_all_words += str(each_word).split()\n    aday_positions_words_dict[temp_key] = set(temp_all_words)\n    \naday_sektors_words_dict = {}\nfor temp_key in tqdm.tqdm(aday_sektors_dict):\n    temp_all_words = []\n    for each_word in aday_sektors_dict[temp_key]:\n        temp_all_words += str(each_word).split()\n    aday_sektors_words_dict[temp_key] = set(temp_all_words)\n    \naday_sehirs_words_dict = {}\nfor temp_key in tqdm.tqdm(aday_sehirs_dict):\n    temp_all_words = []\n    for each_word in aday_sehirs_dict[temp_key]:\n        temp_all_words += str(each_word).split()\n    aday_sehirs_words_dict[temp_key] = set(temp_all_words)\n    \naday_aciklamas_words_dict = {}\nfor temp_key in tqdm.tqdm(aday_aciklamas_dict):\n    temp_all_words = []\n    for each_word in aday_aciklamas_dict[temp_key]:\n        temp_all_words += str(each_word).lower().split()\n    aday_aciklamas_words_dict[temp_key] = set(temp_all_words)\n    \nilan_positions_words_dict = {}\nfor temp_key in tqdm.tqdm(ilan_positions_dict):\n    ilan_positions_words_dict[temp_key] = set(str(ilan_positions_dict[temp_key]).split())\n    \nilan_sektors_words_dict = {}\nfor temp_key in tqdm.tqdm(ilan_sektors_dict):\n    ilan_sektors_words_dict[temp_key] = set(str(ilan_sektors_dict[temp_key]).split())\n    \nilan_sehirs_words_dict = {}\nfor temp_key in tqdm.tqdm(ilan_sehirs_dict):\n    ilan_sehirs_words_dict[temp_key] = set(str(ilan_sehirs_dict[temp_key]).split())\n\nilan_aciklamas_words_dict = {}\nfor temp_key in tqdm.tqdm(ilan_aciklamas_dict):\n    ilan_aciklamas_words_dict[temp_key] = set(str(ilan_aciklamas_dict[temp_key]).split())","dbf34fc9":"ismatch_words_feature = []\nfor each in tqdm.tqdm(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]].values):\n    ismatch_words_feature.append(len(ilan_positions_words_dict[each[1]].intersection(aday_positions_words_dict[each[0]])))\nDatathon_Basvuru[\"is_match_words\"] = ismatch_words_feature\n\nismatch_sektor_words_feature = []\nfor each in tqdm.tqdm(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]].values):\n    ismatch_sektor_words_feature.append(len(ilan_sektors_words_dict[each[1]].intersection(aday_sektors_words_dict[each[0]])))\nDatathon_Basvuru[\"is_match_sektor_words\"] = ismatch_sektor_words_feature\n\nismatch_sehir_words_feature = []\nfor each in tqdm.tqdm(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]].values):\n    ismatch_sehir_words_feature.append(len(ilan_sehirs_words_dict[each[1]].intersection(aday_sehirs_words_dict[each[0]])))\nDatathon_Basvuru[\"is_match_sehir_words\"] = ismatch_sehir_words_feature\n\nismatch_aciklama_words_feature = []\nfor each in tqdm.tqdm(Datathon_Basvuru[[\"AdayId\", \"ilanId\"]].values):\n    ismatch_aciklama_words_feature.append(len(ilan_aciklamas_words_dict[each[1]].intersection(aday_aciklamas_words_dict[each[0]])))\nDatathon_Basvuru[\"is_match_aciklama_words\"] = ismatch_aciklama_words_feature","b38de84d":"Datathon_Aday = Datathon_Aday.sort_values([\"AdayId\", \"BitisYili\"], ascending=False).reset_index(drop=True)","789cbb7d":"Datathon_Aday[\"okul_bolum\"] = Datathon_Aday[\"OkulId\"].astype(str)+\"_\"+Datathon_Aday[\"BolumAdi\"].astype(str)","c3fa7589":"aday_to_accept_dict = dict(pd.Series([each[0] for each in ise_alinanlar_train]).value_counts())\nDatathon_Aday[\"AdayId_accept_count\"] = Datathon_Aday[\"AdayId\"].map(aday_to_accept_dict).fillna(0)\nokulid_to_acceptrate_dict = dict(Datathon_Aday.groupby(\"OkulId\")[\"AdayId_accept_count\"].sum())\nDatathon_Aday[\"AdayId_school_accept_rate\"] = Datathon_Aday[\"OkulId\"].map(okulid_to_acceptrate_dict).fillna(0)\nsehir_to_acceptrate_dict = dict(Datathon_Aday.groupby(\"SehirAdi\")[\"AdayId_accept_count\"].mean())\nDatathon_Aday[\"sehir_accept_rate\"] = Datathon_Aday[\"SehirAdi\"].map(sehir_to_acceptrate_dict).fillna(0)\nbolum_to_acceptrate_dict = dict(Datathon_Aday.groupby(\"BolumAdi\")[\"AdayId_accept_count\"].mean())\nDatathon_Aday[\"bolum_accept_rate\"] = Datathon_Aday[\"BolumAdi\"].map(bolum_to_acceptrate_dict).fillna(0)\ntip_to_acceptrate_dict = dict(Datathon_Aday.groupby(\"Tip\")[\"AdayId_accept_count\"].mean())\nDatathon_Aday[\"tip_accept_rate\"] = Datathon_Aday[\"Tip\"].map(tip_to_acceptrate_dict).fillna(0)\nokul_bolum_to_acceptrate_dict = dict(Datathon_Aday.groupby(\"okul_bolum\")[\"AdayId_accept_count\"].sum())\nDatathon_Aday[\"okul_bolum_accept_rate\"] = Datathon_Aday[\"okul_bolum\"].map(okul_bolum_to_acceptrate_dict).fillna(0)","c38e5068":"Datathon_Yetenek[\"AdayId_accept_count\"] = Datathon_Yetenek[\"AdayId\"].map(aday_to_accept_dict).fillna(0)\nyetenek_to_acceptrate_dict = dict(Datathon_Yetenek.groupby(\"Yetenek\")[\"AdayId_accept_count\"].sum())\nDatathon_Yetenek[\"yetenek_accept_rate\"] = Datathon_Yetenek[\"Yetenek\"].map(yetenek_to_acceptrate_dict).fillna(0)","54aa15dc":"okul_bolum_mean_dict = dict(Datathon_Yetenek.groupby(\"AdayId\")[\"yetenek_accept_rate\"].mean())\nDatathon_Basvuru[\"yetenek_mean\"] = Datathon_Basvuru[\"AdayId\"].map(okul_bolum_mean_dict).fillna(-1)\nokul_bolum_median_dict = dict(Datathon_Yetenek.groupby(\"AdayId\")[\"yetenek_accept_rate\"].median())\nDatathon_Basvuru[\"yetenek_median\"] = Datathon_Basvuru[\"AdayId\"].map(okul_bolum_median_dict).fillna(-1)\nokul_bolum_max_dict = dict(Datathon_Yetenek.groupby(\"AdayId\")[\"yetenek_accept_rate\"].max())\nDatathon_Basvuru[\"yetenek_max\"] = Datathon_Basvuru[\"AdayId\"].map(okul_bolum_max_dict).fillna(-1)","31f0b952":"tip_to_number_dict = {'Belirtilmemi\u015f': 0, '\u00d6nlisans': 1, 'Lisans': 2, 'Y.Lisans': 3, 'Doktora': 4}\nDatathon_Aday[\"Tip_number\"] = Datathon_Aday[\"Tip\"].map(tip_to_number_dict).fillna(-1)\naday_to_tip_max_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"Tip_number\"].max())\nDatathon_Basvuru[\"tip_max\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_tip_max_dict).fillna(-1)\naday_to_tip_min_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"Tip_number\"].min())\nDatathon_Basvuru[\"tip_min\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_tip_min_dict).fillna(-1)","e2ead788":"aday_to_bitisyil_max_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"BitisYili\"].max())\nDatathon_Basvuru[\"bitisyil_max\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_bitisyil_max_dict).fillna(-1)\naday_to_bitisyil_min_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"BitisYili\"].min())\nDatathon_Basvuru[\"bitisyil_min\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_bitisyil_min_dict).fillna(-1)","b2dc34ee":"aday_to_okul_count_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"OkulId\"].count())\nDatathon_Basvuru[\"OkulId_count\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_okul_count_dict).fillna(-1)\naday_to_okul_max_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"AdayId_school_accept_rate\"].max())\nDatathon_Basvuru[\"OkulId_max\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_okul_max_dict).fillna(-1)\naday_to_okul_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"OkulId\"].median())\nDatathon_Basvuru[\"OkulId_median\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_okul_dict).fillna(-1)\nokulid_to_target_dict = dict(Datathon_Basvuru.groupby(\"OkulId_median\")[\"target\"].mean())\nDatathon_Basvuru[\"OkulId_median\"] = Datathon_Basvuru[\"OkulId_median\"].map(okulid_to_target_dict).fillna(-1)","6915d6c5":"bolum_max_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"bolum_accept_rate\"].max())\nDatathon_Basvuru[\"bolum_max\"] = Datathon_Basvuru[\"AdayId\"].map(bolum_max_dict).fillna(-1)\nsehir_max_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"sehir_accept_rate\"].max())\nDatathon_Basvuru[\"sehir_max\"] = Datathon_Basvuru[\"AdayId\"].map(sehir_max_dict).fillna(-1)\nokul_bolum_max_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"okul_bolum_accept_rate\"].max())\nDatathon_Basvuru[\"okul_bolum_max\"] = Datathon_Basvuru[\"AdayId\"].map(okul_bolum_max_dict).fillna(-1)\nokul_bolum_median_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"okul_bolum_accept_rate\"].median())\nDatathon_Basvuru[\"okul_bolum_median\"] = Datathon_Basvuru[\"AdayId\"].map(okul_bolum_median_dict).fillna(-1)\nokul_bolum_mean_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"okul_bolum_accept_rate\"].mean())\nDatathon_Basvuru[\"okul_bolum_mean\"] = Datathon_Basvuru[\"AdayId\"].map(okul_bolum_mean_dict).fillna(-1)\nokul_bolum_std_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"okul_bolum_accept_rate\"].std())\nDatathon_Basvuru[\"okul_bolum_std\"] = Datathon_Basvuru[\"AdayId\"].map(okul_bolum_std_dict).fillna(-1)\naday_to_sehir_dict = dict(Datathon_Aday.groupby(\"AdayId\")[\"SehirAdi\"].last())\nDatathon_Basvuru[\"SehirAdi\"] = Datathon_Basvuru[\"AdayId\"].map(aday_to_sehir_dict).fillna(-1)\nsehiradi_to_target_dict = dict(Datathon_Basvuru.groupby(\"SehirAdi\")[\"target\"].mean())\nDatathon_Basvuru[\"SehirAdi\"] = Datathon_Basvuru[\"SehirAdi\"].map(sehiradi_to_target_dict).fillna(-1)","6a8b642e":"adayid_to_target_dict = dict(Datathon_Basvuru.groupby(\"AdayId\")[\"target\"].mean())\nDatathon_Basvuru[\"AdayId_mean\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_target_dict).fillna(-1)","194317e5":"ilanid_to_target_dict = dict(Datathon_Basvuru.groupby(\"ilanId\")[\"target\"].mean())\nDatathon_Basvuru[\"ilanId_mean\"] = Datathon_Basvuru[\"ilanId\"].map(ilanid_to_target_dict).fillna(-1)","0f5ce016":"adayid_to_yetenek_dict = dict(Datathon_Yetenek.groupby(\"AdayId\")[\"Yetenek\"].nunique())\nDatathon_Basvuru[\"AdayId_yetenek_mean\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_yetenek_dict).fillna(-1)","8883a2c2":"adayid_to_ay_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"CalismaAyi\"].sum())\nDatathon_Basvuru[\"AdayId_ay_mean\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_ay_dict).fillna(-1)\nadayid_to_ay_count_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"CalismaAyi\"].count())\nDatathon_Basvuru[\"AdayId_ay_count\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_ay_count_dict).fillna(-1)","714c700b":"adayid_to_ayrilmayil_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"AyrildigiYil\"].max())\nDatathon_Basvuru[\"AdayId_ayrilmayil_max\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_ayrilmayil_dict).fillna(-1)","11c90991":"adayid_to_ayrilmayil_median_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"AyrildigiYil\"].median())\nDatathon_Basvuru[\"AdayId_ayrilmayil_median\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_ayrilmayil_median_dict).fillna(-1)","ec7db604":"adayid_to_ayrilmayil_min_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"AyrildigiYil\"].min())\nDatathon_Basvuru[\"AdayId_ayrilmayil_min\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_ayrilmayil_min_dict).fillna(-1)","e905dcfd":"adayid_to_sektor_count_dict = dict(Datathon_Tecrube.groupby(\"AdayId\")[\"SektorAdi\"].nunique())\nDatathon_Basvuru[\"AdayId_sektor_count\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_sektor_count_dict).fillna(-1)","6e08b70f":"adayid_to_ilan_count_dict = dict(Datathon_Basvuru.groupby(\"AdayId\")[\"ilanId\"].nunique())\nDatathon_Basvuru[\"AdayId_ilan_count\"] = Datathon_Basvuru[\"AdayId\"].map(adayid_to_ilan_count_dict).fillna(-1)","24a0cb1c":"X = Datathon_Basvuru.drop(\"target\", axis=1)\ny = Datathon_Basvuru[\"target\"]\nresult_X = X.copy()","a44c506f":"y.value_counts()","35cbb71e":"Datathon_Basvuru.head()","9583b716":"X.head()","b0f66729":"X.columns","c9cb6348":"bool_mask = np.logical_and((Datathon_Basvuru[\"target\"]==0).values, np.array([each[1] in ise_alinanlar_test_ilan for each in np.array(Datathon_Basvuru)]))\nresult_X_temp = result_X[bool_mask]\nX_changed = X.drop([\"AdayId_mean\", \"ilanId_mean\"], axis=1) # X # X.drop([\"AdayId_mean\", \"ilanId_mean\"], axis=1)\nfor i in np.arange(1, 1002, 100): # [200]: # np.arange(1, 600, 100):\n    result_X_temp = result_X[bool_mask].copy()\n    print(\"Current number of tree:\", i)\n    xgbcls = xgb.XGBClassifier(gamma = 2, reg_lambda= 1.1,min_child_weight = 1, max_depth=3, n_estimators=i,\n                               learning_rate = 0.1, subsample = 0.8, colsample_bytree = 0.7, \n                               nthread = 8, verbosity=0).fit(X_changed, y, \n                            eval_set=[(X_changed, y)],early_stopping_rounds=100, \n                            eval_metric=[\"aucpr\"], verbose=False)\n    \n    result_X_temp[\"preds\"] = xgbcls.predict_proba(X_changed[bool_mask])[:, 1]\n    result_X_temp[\"real_values\"] = pd.Series(list(zip(result_X_temp[\"AdayId\"],result_X_temp[\"ilanId\"]))).isin(ise_alinanlar_test).astype(int).values\n    result_X_temp = result_X_temp.sort_values(by=[\"ilanId\", \"preds\"], ascending=False)\n    result_X_values = result_X_temp.values\n    predictions_set = set()\n    temp_ilan = result_X_values[0][1]\n    predictions_set.add((result_X_values[0][0], result_X_values[0][1]))\n    temp_counter = 1\n\n    for row in result_X_values[1:]:\n        if row[1]!=temp_ilan:\n            temp_ilan = row[1]\n            temp_counter = 1\n            predictions_set.add((row[0], row[1]))\n            continue\n        if temp_counter==5:\n            continue\n        temp_counter += 1\n        predictions_set.add((row[0], row[1]))\n    print(\"correct:\", len(predictions_set.intersection(ise_alinanlar_test)))\n    print()\n    ","c950ec4a":"# 49 - 61 - 72 - 80 - 82 - 89","b7eb8176":"X_changed.columns","477f8197":"feature_important = xgbcls.get_booster().get_score(importance_type='gain')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=True)\ndata.plot(kind='barh', figsize=(12, 12))","fadc3188":"feature_important = xgbcls.get_booster().get_score(importance_type='gain')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=True)\ndata.plot(kind='barh', figsize=(12, 12))","6b567060":"bool_mask = np.logical_and((Datathon_Basvuru[\"target\"]==0).values, np.array([each[1] in ise_alinanlar_test_ilan for each in np.array(Datathon_Basvuru)]))\nresult_X_temp = result_X[bool_mask]\nX_changed = X.drop([\"AdayId_mean\", \"ilanId_mean\"], axis=1)\nfor i in [350]:\n    result_X_temp = result_X[bool_mask].copy()\n    print(\"Current number of tree:\", i)\n    xgbcls = xgb.XGBClassifier(gamma = 2, reg_lambda= 1.1, min_child_weight = 1, max_depth=3, n_estimators=i,\n                               learning_rate = 0.02, subsample = 0.8, colsample_bytree = 0.7, \n                               nthread = 8, verbosity=0).fit(X_changed, y, \n                            eval_set=[(X_changed, y)],early_stopping_rounds=100, \n                            eval_metric=[\"aucpr\"], verbose=False)\n    \n    result_X_temp[\"preds\"] = xgbcls.predict_proba(X_changed[bool_mask])[:, 1]\n    #result_X_temp[\"real_values\"] = pd.Series(list(zip(result_X_temp[\"AdayId\"],result_X_temp[\"ilanId\"]))).isin(ise_alinanlar_test).astype(int).values\n    result_X_temp = result_X_temp.sort_values(by=[\"ilanId\", \"preds\"], ascending=False)\n    result_X_values = result_X_temp.values\n    predictions_set = [] # set()\n    temp_ilan = result_X_values[0][1]\n    predictions_set.append((result_X_values[0][0], result_X_values[0][1])) # predictions_set.add((result_X_values[0][0], result_X_values[0][1]))\n    temp_counter = 1\n\n    for row in result_X_values[1:]:\n        if row[1]!=temp_ilan:\n            temp_ilan = row[1]\n            temp_counter = 1\n            # predictions_set.add((row[0], row[1]))\n            predictions_set.append((row[0], row[1]))\n            continue\n        if temp_counter==5:\n            continue\n        temp_counter += 1\n        # predictions_set.add((row[0], row[1]))\n        predictions_set.append((row[0], row[1]))\n    print(\"Done\")\n    print()","717aea65":"test.head()","7ac492f6":"test_submission = pd.DataFrame([[each[1], each[0]] for each in list(predictions_set)], columns = test.columns)\ntest_submission[\"ilanId\"] = test_submission[\"ilanId\"].astype(int)\ntest_submission[\"AdayId\"] = test_submission[\"AdayId\"].astype(int)\ntest_submission","db436f52":"#Datathon_Basvuru_iseAlinanlar","d736f14e":"test_submission.to_csv(\"sixth_submission350_withcopy_withokulbolum_withoutleak_withtextdata_finetuned.csv\", index=False)","818cea4e":"Calculate length of explanation","2fe56fa3":"Okul and bolum are meaningful together.","3dd3b210":"To see feature importances","28e4279e":"# Library Import","3bea47b8":"For final submission, run # for test instead of # for train above.","7f3321d4":"Calculate aday based features using okul, sehir, bolum okul_bolum, tip etc.","c4d8bf74":"Convert from html to text","fef4475f":"Find texts for both adays and ilans. Calculate whether there is a match or not.","be125a92":"# Create Target","e643d459":"# Train Test Split","f49fc6a1":"Create submission","6a51c4b1":"Calculate how many applications aday has","60137a82":"Full match is not very probable, then calculate how many words there are common for both ilans and adays in related columns","487cf192":"Train xgboost with the given features and calculate how many matches there are in the validation set with size 700. The split between train and validation set are exactly same with the train and test set."}}