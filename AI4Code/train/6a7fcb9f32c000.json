{"cell_type":{"f8afc037":"code","96d64db7":"code","bc2690ad":"code","fcd78b5a":"code","ceab2b1b":"code","f9a9dfae":"code","abdee76c":"code","6ae81655":"code","5ee0073b":"code","b4410da4":"code","0a351377":"code","43c8f27c":"code","abfdbbdd":"code","b6dc6425":"code","554d97b0":"code","83fcb7ac":"code","84ae617f":"code","6f74c871":"code","1d1cc8b0":"code","aa1d9594":"code","4adef7eb":"code","2fe6f763":"code","9c03dde6":"code","38b33950":"code","6c77a88b":"code","0ed87d90":"code","44d27273":"code","fc7f2067":"code","e1275c4a":"code","595e7f7e":"code","6f10a5b7":"code","f92de4a9":"code","6173c126":"code","42e8eaff":"code","4e8f41a8":"code","8c0319b8":"code","5c1112ea":"code","1c8873b5":"code","d5feb57d":"code","40915398":"code","483480dc":"code","c0bf6282":"code","656d143a":"code","b784faf8":"code","936801b5":"code","844b42b4":"code","e55d4855":"code","6e97c44d":"code","665e9bfa":"code","21e2579a":"code","a9d35a7b":"code","ae915c83":"code","e1f96c02":"code","9b60fc5d":"code","790a3755":"code","ea709d90":"code","77b7f30a":"code","7498c81d":"code","b756782b":"code","fb9ad8a0":"code","56305c8e":"code","0a28a758":"code","b7638365":"code","45745b46":"code","347f7593":"code","9e6d6db1":"code","3afbb63e":"code","5c49577f":"code","550e122a":"code","5b9bb471":"code","bd0f3d0a":"code","ff7ad328":"markdown","e61e8a7c":"markdown","5f403a06":"markdown","123d946d":"markdown","41f38963":"markdown","78c3714c":"markdown","1087e1a3":"markdown","9a375249":"markdown","35c4bd1c":"markdown","8dfc241c":"markdown","694cfbf5":"markdown","6c5d453e":"markdown","1496c1e6":"markdown","9eb7c808":"markdown","89021d38":"markdown","ff85a098":"markdown","f91a6a83":"markdown","47a615f5":"markdown","0e108409":"markdown","572996f4":"markdown","f50d59c7":"markdown","183db777":"markdown","950023cc":"markdown","1cc5203a":"markdown","0cf88383":"markdown","0c46d335":"markdown","d1b54b7b":"markdown","012adbf3":"markdown","fb86314a":"markdown","b8ac6a93":"markdown","b914dc72":"markdown","1e9e0d1c":"markdown","2dc9d728":"markdown","a611ef09":"markdown","263026a4":"markdown","3f018716":"markdown"},"source":{"f8afc037":"#Importing basic libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")","96d64db7":"\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, StandardScaler, PowerTransformer, MinMaxScaler, LabelEncoder, RobustScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split, GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve, plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesRegressor, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import plot_tree\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n","bc2690ad":"df = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")\ndf.tail()","fcd78b5a":"df.shape","ceab2b1b":"df.isnull().sum()","f9a9dfae":"print(\"Sex:\",df['Sex'].unique())\nprint(\"RestingECG:\",df['RestingECG'].unique())\nprint(\"ChestPainType:\",df['ChestPainType'].unique())\nprint(\"ExerciseAngina:\",df['ExerciseAngina'].unique())\nprint(\"ST_Slope:\",df['ST_Slope'].unique())","abdee76c":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Pie(labels=df['HeartDisease'].value_counts().index,\n                             values=df['HeartDisease'].value_counts().values,\n                              hole=.5)])\n\nfig.update_layout(\n    title_text=\"Chest pain type \")\nfig.show()","6ae81655":"import plotly.figure_factory as ff\nfrom plotly.offline import iplot\nfig = ff.create_distplot([df.Age],['Age'],bin_size=1)\niplot(fig, filename='Basic Distplot')","5ee0073b":"# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=df['ChestPainType'].value_counts().index,\n                             values=df['ChestPainType'].value_counts().values,\n                             hole=.5,pull=[0.06, 0.02, 0.06, 0.02])])\n\nfig.update_layout(\n    title_text=\"Chest pain type \")\nfig.show()","b4410da4":"plt.figure(figsize = (15, 10))\nsns.displot(df['RestingBP'], color = 'y', kind='kde')\n\nplt.show()","0a351377":"plt.figure(figsize = (20, 10))\nsns.displot(df['Cholesterol'])\n\nplt.show()","43c8f27c":"# prepare data\ndata = df.loc[:,['RestingBP', 'Cholesterol', 'MaxHR']]\ndata[\"index\"] = np.arange(1,len(data)+1)\n# scatter matrix\nfig = ff.create_scatterplotmatrix(data, diag='box', index='index',colormap='Portland',\n                                  height=900, width=900)\niplot(fig)","abfdbbdd":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True, cmap='RdYlBu')","b6dc6425":"#Get list of categorical variables\ns = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables in the dataset:\", object_cols)","554d97b0":"#Label Encoding the object dtypes.\nLE=LabelEncoder()\nfor i in object_cols:\n    df[i]=df[[i]].apply(LE.fit_transform)\n    print(df[i])\n    print(LE.classes_)\n    \nprint(\"All features are now numerical\")","83fcb7ac":"df.head","84ae617f":"X = df.drop([\"HeartDisease\"], axis=1)\ny = df[\"HeartDisease\"]","6f74c871":"#In this classification problems we do not have a balanced number of examples\n#for each class label. As such, we split the dataset into \n#train and test sets in a way that preserves the same proportions of examples\n#in each class as observed in the original dataset.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify = y, random_state = 101)","1d1cc8b0":"scaler = MinMaxScaler()\nscaler","aa1d9594":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","4adef7eb":"# Function for examining scores\n\ndef train_val(y_train, y_train_pred, y_test, y_pred):\n    \n    scores = {\"train_set\": {\"Accuracy\" : accuracy_score(y_train, y_train_pred),\n                            \"Precision\" : precision_score(y_train, y_train_pred),\n                            \"Recall\" : recall_score(y_train, y_train_pred),                          \n                            \"f1\" : f1_score(y_train, y_train_pred)},\n    \n              \"test_set\": {\"Accuracy\" : accuracy_score(y_test, y_pred),\n                           \"Precision\" : precision_score(y_test, y_pred),\n                           \"Recall\" : recall_score(y_test, y_pred),                          \n                           \"f1\" : f1_score(y_test, y_pred)}}\n    \n    return pd.DataFrame(scores)","2fe6f763":"LR_model = LogisticRegression() \nLR_model.fit(X_train_scaled, y_train)\ny_pred = LR_model.predict(X_test_scaled)\ny_train_pred = LR_model.predict(X_train_scaled)\n\nlog_f1 = f1_score(y_test, y_pred)\nlog_acc = accuracy_score(y_test, y_pred)\nlog_recall = recall_score(y_test, y_pred)\nlog_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(LR_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","9c03dde6":"penalty = [\"l1\", \"l2\", \"elasticnet\"]\nl1_ratio = np.linspace(0, 1, 20)\nC = np.logspace(0, 10, 20)\n\nparam_grid = {\"penalty\" : penalty,\n             \"l1_ratio\" : l1_ratio,\n             \"C\" : C}","38b33950":"LR_grid_model = LogisticRegression(solver='saga', max_iter=5000, class_weight = \"balanced\")\n\nLR_grid_model = GridSearchCV(LR_grid_model, param_grid = param_grid)","6c77a88b":"LR_grid_model.fit(X_train_scaled, y_train)","0ed87d90":"y_pred = LR_grid_model.predict(X_test_scaled)\ny_train_pred = LR_grid_model.predict(X_train_scaled)\n\nlog_grid_f1 = f1_score(y_test, y_pred)\nlog_grid_acc = accuracy_score(y_test, y_pred)\nlog_grid_recall = recall_score(y_test, y_pred)\nlog_grid_auc = roc_auc_score(y_test, y_pred)\n\n\nplot_confusion_matrix(LR_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","44d27273":"plot_roc_curve(LR_model, X_test_scaled, y_test, response_method='auto');","fc7f2067":"plot_precision_recall_curve(LR_model, X_test_scaled, y_test);","e1275c4a":"SVM_model = SVC(random_state=42)\nSVM_model.fit(X_train_scaled, y_train)\ny_pred = SVM_model.predict(X_test_scaled)\ny_train_pred = SVM_model.predict(X_train_scaled)\n\nsvm_f1 = f1_score(y_test, y_pred)\nsvm_acc = accuracy_score(y_test, y_pred)\nsvm_recall = recall_score(y_test, y_pred)\nsvm_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(SVM_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","595e7f7e":"param_grid = {'C': [0.1,1, 10, 100, 1000],\n              'gamma': [\"scale\", \"auto\", 1,0.1,0.01,0.001,0.0001],\n              'kernel': ['rbf', 'linear']}","6f10a5b7":"SVM_grid_model = SVC(random_state=42)\n\nSVM_grid_model = GridSearchCV(SVM_grid_model, param_grid, verbose=3, refit=True)","f92de4a9":"SVM_grid_model.fit(X_train_scaled, y_train)","6173c126":"SVM_grid_model.best_estimator_","42e8eaff":"y_pred = SVM_grid_model.predict(X_test_scaled)\ny_train_pred = SVM_grid_model.predict(X_train_scaled)\n\nsvm_grid_f1 = f1_score(y_test, y_pred)\nsvm_grid_acc = accuracy_score(y_test, y_pred)\nsvm_grid_recall = recall_score(y_test, y_pred)\nsvm_grid_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(SVM_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","4e8f41a8":"plot_roc_curve(SVM_grid_model, X_test_scaled, y_test);","8c0319b8":"plot_precision_recall_curve(SVM_grid_model, X_test_scaled, y_test);","5c1112ea":"DT_model = DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)\nDT_model.fit(X_train_scaled, y_train)\ny_pred = DT_model.predict(X_test_scaled)\ny_train_pred = DT_model.predict(X_train_scaled)\n\ndt_f1 = f1_score(y_test, y_pred)\ndt_acc = accuracy_score(y_test, y_pred)\ndt_recall = recall_score(y_test, y_pred)\ndt_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(DT_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","1c8873b5":"param_grid = {\"splitter\":[\"best\", \"random\"],\n              \"max_features\":[None, 3, 5, 7],\n              \"max_depth\": [None, 4, 5, 6, 7, 8, 9, 10],\n              \"min_samples_leaf\": [2, 3, 5],\n              \"min_samples_split\": [2, 3, 5, 7, 9, 15]}","d5feb57d":"DT_grid_model = DecisionTreeClassifier(class_weight = \"balanced\", random_state=42)\n\nDT_grid_model = GridSearchCV(estimator=DT_grid_model,\n                            param_grid=param_grid,\n                            scoring='recall',\n                            n_jobs = -1, verbose = 2).fit(X_train_scaled, y_train)","40915398":"DT_grid_model.best_estimator_","483480dc":"y_pred = DT_grid_model.predict(X_test_scaled)\n\ny_train_pred = DT_grid_model.predict(X_train_scaled)\n\ndt_grid_f1 = f1_score(y_test, y_pred)\ndt_grid_acc = accuracy_score(y_test, y_pred)\ndt_grid_recall = recall_score(y_test, y_pred)\ndt_grid_auc = roc_auc_score(y_test, y_pred)\n\n\nplot_confusion_matrix(DT_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","c0bf6282":"DT_grid_model.best_estimator_","656d143a":"plot_roc_curve(DT_grid_model, X_test_scaled, y_test);","b784faf8":"plot_precision_recall_curve(DT_grid_model, X_test_scaled, y_test);","936801b5":"RF_model = RandomForestClassifier(class_weight=\"balanced\", random_state=101)\nRF_model.fit(X_train_scaled, y_train)\ny_pred = RF_model.predict(X_test_scaled)\ny_train_pred = RF_model.predict(X_train_scaled)\n\nrf_f1 = f1_score(y_test, y_pred)\nrf_acc = accuracy_score(y_test, y_pred)\nrf_recall = recall_score(y_test, y_pred)\nrf_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(RF_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","844b42b4":"param_grid = {'n_estimators':[50, 100, 300],\n             'max_features':[2, 3, 4],\n             'max_depth':[3, 5, 7, 9],\n             'min_samples_split':[2, 5, 8]}","e55d4855":"RF_grid_model = RandomForestClassifier(random_state=101)\n\nRF_grid_model = GridSearchCV(estimator=RF_grid_model, \n                             param_grid=param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 2).fit(X_train_scaled, y_train) ","6e97c44d":"y_pred = RF_grid_model.predict(X_test_scaled)\ny_train_pred = RF_grid_model.predict(X_train_scaled)\n\nrf_grid_f1 = f1_score(y_test, y_pred)\nrf_grid_acc = accuracy_score(y_test, y_pred)\nrf_grid_recall = recall_score(y_test, y_pred)\nrf_grid_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(RF_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","665e9bfa":"plot_roc_curve(RF_grid_model, X_test_scaled, y_test);","21e2579a":"plot_precision_recall_curve(RF_grid_model, X_test_scaled, y_test);","a9d35a7b":"GB_model = GradientBoostingClassifier(random_state=42)\nGB_model.fit(X_train, y_train)\ny_pred = GB_model.predict(X_test)\ny_train_pred = GB_model.predict(X_train)\n\ngb_f1 = f1_score(y_test, y_pred)\ngb_acc = accuracy_score(y_test, y_pred)\ngb_recall = recall_score(y_test, y_pred)\ngb_auc = roc_auc_score(y_test, y_pred)\n \n    \nplot_confusion_matrix(GB_model, X_test, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)\n","ae915c83":"param_grid = {\"n_estimators\":[100, 200, 300],\n             \"subsample\":[0.5, 1], \"max_features\" : [None, 2, 3, 4], \"learning_rate\": [0.2, 0.5, 0.6, 0.75, 0.85, 1.0, 1.25, 1.5]}  # 'max_depth':[3,4,5,6]","e1f96c02":"GB_grid_model = GradientBoostingClassifier(random_state=42)\n\nGB_grid_model = GridSearchCV(GB_grid_model, param_grid, scoring = \"f1\", verbose=2, n_jobs = -1).fit(X_train, y_train)","9b60fc5d":"y_pred = GB_grid_model.predict(X_test_scaled)\ny_train_pred = GB_grid_model.predict(X_train_scaled)\n\ngb_grid_f1 = f1_score(y_test, y_pred)\ngb_grid_acc = accuracy_score(y_test, y_pred)\ngb_grid_recall = recall_score(y_test, y_pred)\ngb_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\n\nplot_confusion_matrix(GB_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","790a3755":"plot_roc_curve(GB_grid_model, X_test, y_test);","ea709d90":"plot_precision_recall_curve(GB_grid_model, X_test, y_test);","77b7f30a":"AB_model = AdaBoostClassifier(n_estimators=50, random_state=101)\nAB_model.fit(X_train, y_train)\ny_pred = AB_model.predict(X_test)\ny_train_pred = AB_model.predict(X_train)\n\nab_f1 = f1_score(y_test, y_pred)\nab_acc = accuracy_score(y_test, y_pred)\nab_recall = recall_score(y_test, y_pred)\nab_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(AB_model, X_test, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","7498c81d":"# Computing the accuracy scores on train and validation sets when training with different learning rates\n\nlearning_rates = [0.05, 0.1, 0.15, 0.25, 0.5, 0.6, 0.75, 0.85, 1]\n\nfor learning_rate in learning_rates:\n    ab = AdaBoostClassifier(n_estimators=20, learning_rate = learning_rate, random_state=42)\n    ab.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(ab.score(X_train, y_train)))\n    print(\"Accuracy score (test): {0:.3f}\".format(ab.score(X_test, y_test)))\n    print()","b756782b":"param_grid = {\"n_estimators\": [15, 20, 100, 500], \"learning_rate\": [0.2, 0.5, 0.6, 0.75, 0.85, 1.0, 1.25, 1.5]}\n\nAB_grid_model = AdaBoostClassifier(random_state=42)\nAB_grid_model = GridSearchCV(AB_grid_model, param_grid, cv=5, scoring= 'f1')\n\n\nAB_grid_model.fit(X_train, y_train)","fb9ad8a0":"y_pred = AB_grid_model.predict(X_test)\ny_train_pred = AB_grid_model.predict(X_train)\n\nab_grid_f1 = f1_score(y_test, y_pred)\nab_grid_acc = accuracy_score(y_test, y_pred)\nab_grid_recall = recall_score(y_test, y_pred)\nab_grid_auc = roc_auc_score(y_test, y_pred)\n\n\n\nplot_confusion_matrix(AB_grid_model, X_test, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","56305c8e":"plot_roc_curve(AB_grid_model, X_test, y_test);","0a28a758":"plot_precision_recall_curve(AB_grid_model, X_test, y_test);","b7638365":"XGB_model = XGBClassifier(random_state=101)\nXGB_model.fit(X_train_scaled, y_train)\ny_pred = XGB_model.predict(X_test_scaled)\ny_train_pred = XGB_model.predict(X_train_scaled)\n\nxgb_f1 = f1_score(y_test, y_pred)\nxgb_acc = accuracy_score(y_test, y_pred)\nxgb_recall = recall_score(y_test, y_pred)\nxgb_auc = roc_auc_score(y_test, y_pred)\n \n    \nplot_confusion_matrix(XGB_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","45745b46":"param_grid = {\"n_estimators\":[100, 300], \n              \"max_depth\":[3,5,6], \n              \"learning_rate\": [0.1, 0.3],\n              \"subsample\":[0.5, 1], \n              \"colsample_bytree\":[0.5, 1]}","347f7593":"XGB_grid_model = XGBClassifier(random_state=42,  tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",use_label_encoder=False)\nXGB_grid_model = GridSearchCV(XGB_grid_model, param_grid, scoring = \"f1\", verbose=2, n_jobs = -1)\n\n","9e6d6db1":"XGB_grid_model.fit(X_train_scaled, y_train)","3afbb63e":"y_pred = XGB_grid_model.predict(X_test_scaled)\ny_train_pred = XGB_grid_model.predict(X_train_scaled)\n\nxgb_grid_f1 = f1_score(y_test, y_pred)\nxgb_grid_acc = accuracy_score(y_test, y_pred)\nxgb_grid_recall = recall_score(y_test, y_pred)\nxgb_grid_auc = roc_auc_score(y_test, y_pred)\n\n\nplot_confusion_matrix(XGB_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","5c49577f":"plot_roc_curve(XGB_grid_model, X_test_scaled, y_test);","550e122a":"plot_precision_recall_curve(XGB_grid_model, X_test_scaled, y_test);","5b9bb471":"compare = pd.DataFrame({\"Model\": [\"Logistic Regression\", \"SVM\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"GradientBoost\", \"XGBoost\"],\n                        \"F1\": [log_f1, svm_grid_f1, dt_grid_f1, rf_grid_f1, ab_grid_f1, gb_f1, xgb_grid_f1],\n                        \"Recall\": [log_recall, svm_grid_recall, dt_grid_recall, rf_grid_recall, ab_grid_recall, gb_recall, xgb_grid_recall],\n                        \"Accuracy\": [log_acc, svm_grid_acc, dt_grid_acc, rf_grid_acc, ab_grid_acc, gb_acc, xgb_grid_acc],\n                        \"ROC_AUC\": [log_auc, svm_grid_auc, dt_grid_auc, rf_grid_auc, ab_grid_auc, gb_auc, xgb_grid_auc]})\n\ndef labels(ax):\n    for p in ax.patches:\n        width = p.get_width()                        # get bar length\n        ax.text(width,                               # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2,      # get Y coordinate + X coordinate \/ 2\n                '{:1.3f}'.format(width),             # set variable to display, 2 decimals\n                ha = 'left',                         # horizontal alignment\n                va = 'center')                       # vertical alignment\n    \nplt.figure(figsize=(14,14))\nplt.subplot(411)\ncompare = compare.sort_values(by=\"F1\", ascending=False)\nax=sns.barplot(x=\"F1\", y=\"Model\", data=compare)\nlabels(ax)\n\nplt.subplot(412)\ncompare = compare.sort_values(by=\"Recall\", ascending=False)\nax=sns.barplot(x=\"Recall\", y=\"Model\", data=compare)\nlabels(ax)\n\nplt.subplot(413)\ncompare = compare.sort_values(by=\"Accuracy\", ascending=False)\nax=sns.barplot(x=\"Accuracy\", y=\"Model\", data=compare)\nlabels(ax)\n\nplt.subplot(414)\ncompare = compare.sort_values(by=\"ROC_AUC\", ascending=False)\nax=sns.barplot(x=\"ROC_AUC\", y=\"Model\", data=compare )\nlabels(ax)\n\nplt.show()","bd0f3d0a":"import pickle\npickle_out = open(\"classifier.pkl\",\"wb\")\npickle.dump(GB_model, pickle_out)\npickle_out.close()","ff7ad328":"#  ****Modeling****","e61e8a7c":"<font size=5>**Correlation matrix**","5f403a06":"**Random Forest (RF) with Best Parameters Using GridSeachCV**","123d946d":"<font size=5>Cholesterol","41f38963":"# Heart failuer prediction \n![](http:\/2.bp.blogspot.com\/-Hl0sSWhmGS8\/TV8dg16i_EI\/AAAAAAAAAAw\/meuKOjBILWE\/s1600\/HomerSimpson_da_www_simpsoncrazy_com.gif)\n# Hope u like my work ...Please upvote it feels good\ud83d\ude01\n","78c3714c":"# 4.Random Forest (RF)","1087e1a3":"**GradientBoosting (GB) with Best Parameters Using GridSeachCV**","9a375249":"# Spliting the data","35c4bd1c":"# Reference","8dfc241c":"# 3.Decision Tree (DT)","694cfbf5":"<font size=5>RestingBP","6c5d453e":"![](http:\/\/underthebluedoor.files.wordpress.com\/2016\/10\/love-or-money.jpg)\n\n\n# Choise is yours","1496c1e6":"## Introduction\n\n<font size=\"4\">In this modern era people are very busy and working hard in order to satisfying their materialistic needs and not able to spend time for themselves which leads to physical stress and mental disorder. There are also reports that heart suffer because of global pandemic corona virus. Inflammation of the heart muscle can be caused by corona virus. Thus heart disease is very common now a day\u2019s particularly in urban areas because of excess mental stress due to corona virus. As a result Heart disease has become one of the most important factors for death of men and women in the so called material world (Sahoo,2020). <\/font>\n\n","9eb7c808":"# 2.Support Vector Machine (SVM)","89021d38":"**Curves**","ff85a098":"# 1.Logistic regression","f91a6a83":"**Modelling Logistic Regression (LR) with Best Parameters Using GridSeachCV**","47a615f5":"# Data Visualization","0e108409":"** Modelling AdaBoosting (AB) with Best Parameters Using GridSearchCV **","572996f4":" **Ploting the curves**","f50d59c7":"# Comapring models","183db777":"* Heart Failure Prediction Dataset\n\n* 1. Age: Age of the patient [years] \n* 2. Sex: Sex of the patient [M: Male, F: Female] \n* 3. ChestPainType: [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic] \n* 4. RestingBP: Resting blood pressure [mm Hg] \n* 5. Cholesterol: Serum cholesterol [mm\/dl] \n* 6. FastingBS: Fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise] \n* 7. RestingECG: Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions  and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by  Estes' criteria] \n* 8. MaxHR: Maximum heart rate achieved [Numeric value between 60 and 202] \n* 9. ExerciseAngina: Exercise-induced angina [Y: Yes, N: No] \n* 10. Oldpeak: ST [Numeric value measured in depression]  \n* 11. ST_Slope: The slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping] \n* 12. HeartDisease: Output class [1: heart disease, 0: Normal] \n\n* Reference: https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction ","950023cc":"# 7.XGBoosting (XGB)","1cc5203a":"# 5.GradientBoosting (GB)","0cf88383":"**Curves**","0c46d335":"## \ud83d\udd3aConclusion\n\n<font size=\"4\">\n    From this comparions of different Machine Learning models we conclude the following points:<\/font>\n    \n    \n  <font size=\"4\">   \n    \ud83d\udc49\ud83c\udffbGradient Boost model(with out hypertuning) performed best.  \n    \ud83d\udc49GB model had higest f1 score(0.921),Accuracy score(0.913),,ROC_AUC score(0.912) and second highest Recall score. \n    \ud83d\udc49Decision tree is our second best model \n<\/font>\n\n","d1b54b7b":"**Curves**","012adbf3":"**Modelling Decision Tree (DT) with Best Parameters Using GridSeachCV**","fb86314a":"**Curves**","b8ac6a93":"**Modelling Support Vector Machine (SVM) with Best Parameters Using GridSeachCV**","b914dc72":"* https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data\n* https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html \n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n* https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/\n* https:\/\/en.wikipedia.org\/wiki\/Dummy_variable_(statistics)\n* https:\/\/www.displayr.com\/what-are-dummy-variables\/\n* https:\/\/stattrek.com\/multiple-regression\/dummy-variables.aspx\n* https:\/\/www.statisticshowto.com\/dummy-variables\/\n* https:\/\/en.wikipedia.org\/wiki\/Feature_scaling\n* https:\/\/www.dataschool.io\/comparing-supervised-learning-algorithms\/\n* https:\/\/machinelearningmastery.com\/handle-missing-data-python\/\n* https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values\n* https:\/\/www.kaggle.com\/karnikakapoor\/fetal-health-classification\n* https:\/\/www.kaggle.com\/karnikakapoor\/heart-failure-prediction-ann\n* https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know\n* https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro\n* https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy\n* https:\/\/www.kaggle.com\/azizozmen\/heart-failure-predict-8-classification-techniques","1e9e0d1c":"## Description of the dataset","2dc9d728":"# 6.AdaBoosting (AB)","a611ef09":"**Modelling XGBoosting (XGB) with Best Parameters Using GridSearchCV**","263026a4":"## Aim\n\n<font size=\"4\">Our main ain is to predict if some one is at high risk of being diagnoised as a heart patient using different machine learning models.<\/span><\/font>\n","3f018716":"# Thank you"}}