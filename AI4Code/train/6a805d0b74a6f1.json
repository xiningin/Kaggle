{"cell_type":{"ed4ef6a1":"code","7c6ed574":"code","1bf567ea":"code","ae2b22c1":"code","69ef611d":"code","5679c663":"code","329a3069":"code","2ae9fd11":"code","5f79c44d":"code","c8df8616":"code","d38edb86":"code","b37b953e":"code","e9a4bc97":"code","f321a963":"code","0875691e":"code","e5e49fe1":"code","bf47a9ba":"code","57d87d4d":"code","159d7c26":"code","331bbe15":"code","a6496882":"code","13b2dd14":"code","6f9f5afb":"code","6101b6c6":"code","23ca7519":"code","8b366063":"code","fe7f53e4":"code","4c9a1d1f":"code","eef48ec0":"code","e4b54e0a":"code","6587cb50":"code","3a3a3222":"code","52ba3851":"code","6010a4f8":"code","07983445":"code","7852349b":"code","a1acfd96":"code","364b432a":"code","a9cc43ce":"code","ff705c65":"code","ec1c756d":"code","3d0c8012":"code","82cf77d1":"code","9c21ad33":"code","e959a846":"code","721e3e40":"code","41a827db":"code","11973bff":"code","eaf8ba7d":"code","ccb80bab":"code","1eba50fd":"code","291c364c":"code","d56dba77":"code","3b12ca5d":"code","595b870a":"code","82f3208b":"code","ecb8cbfa":"code","a78a0833":"code","6f950966":"code","12f996f1":"code","529f77b7":"code","5c32bc4b":"code","642cf5b8":"code","f5bdcdfe":"code","c7d3fd1e":"code","0fe51ba7":"code","2f6a35f8":"code","f79e3375":"code","06c20538":"code","9d04c6b0":"code","2e846a13":"code","918ea438":"code","c0dd3b0e":"code","b298bf62":"code","f4ca5618":"code","dc311499":"code","1649af4f":"code","9e22bf7b":"code","707d14cd":"code","9fba27a3":"code","f573c6ec":"code","a6409afe":"markdown","7d3b219f":"markdown","3b30fdee":"markdown"},"source":{"ed4ef6a1":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nsys.path.append('..\/input\/umaplearn\/umap')\n\n%mkdir model\n%mkdir interim\n\nfrom scipy.sparse.csgraph import connected_components\nfrom umap import UMAP\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\nimport time\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA,FactorAnalysis\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nprint(torch.cuda.is_available())\nimport warnings\n# warnings.filterwarnings('ignore')","7c6ed574":"torch.__version__","1bf567ea":"NB = '25'\n\nIS_TRAIN = True\nMODEL_DIR = \"model\" # \"..\/model\"\nINT_DIR = \"interim\" # \"..\/interim\"\n\nNSEEDS = 5  # 5\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 15\nBATCH_SIZE = 256\nLEARNING_RATE = 5e-3\nWEIGHT_DECAY = 1e-5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nNFOLDS = 5  # 5\n\nPMIN = 0.0005\nPMAX = 0.9995\nSMIN = 0.0\nSMAX = 1.0","ae2b22c1":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","69ef611d":"train_targets_nonscored = train_targets_nonscored.loc[:, train_targets_nonscored.sum() != 0]\nprint(train_targets_nonscored.shape)","5679c663":"# for c in train_targets_scored.columns:\n#     if c != \"sig_id\":\n#         train_targets_scored[c] = np.maximum(PMIN, np.minimum(PMAX, train_targets_scored[c]))\nfor c in train_targets_nonscored.columns:\n    if c != \"sig_id\":\n        train_targets_nonscored[c] = np.maximum(PMIN, np.minimum(PMAX, train_targets_nonscored[c]))","329a3069":"print(\"(nsamples, nfeatures)\")\nprint(train_features.shape)\nprint(train_targets_scored.shape)\nprint(train_targets_nonscored.shape)\nprint(test_features.shape)\nprint(sample_submission.shape)","2ae9fd11":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","5f79c44d":"def seed_everything(seed=1903):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=1903)","c8df8616":"# GENES\nn_comp = 90\nn_dim = 45\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=1903).fit(data[GENES])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\n    umap = UMAP(n_components=n_dim, random_state=1903).fit(data[GENES])\n    pd.to_pickle(umap, f'{MODEL_DIR}\/{NB}_umap_g.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\n    umap = pd.read_pickle(f'{MODEL_DIR}\/{NB}_umap_g.pkl')\n\ndata2 = (fa.transform(data[GENES]))\ndata3 = (umap.transform(data[GENES]))\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'fa_G-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_G-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'fa_G-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_G-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n#CELLS\nn_comp = 50\nn_dim = 25\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=1903).fit(data[CELLS])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\n    umap = UMAP(n_components=n_dim, random_state=1903).fit(data[CELLS])\n    pd.to_pickle(umap, f'{MODEL_DIR}\/{NB}_umap_c.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\n    umap = pd.read_pickle(f'{MODEL_DIR}\/{NB}_umap_c.pkl')\n    \ndata2 = (fa.transform(data[CELLS]))\ndata3 = (umap.transform(data[CELLS]))\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'fa_C-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_C-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'fa_C-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_C-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]","d38edb86":"from sklearn.preprocessing import QuantileTransformer\n\nfor col in (GENES + CELLS):\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = pd.concat([train_features, test_features])[col].values.reshape(vec_len+vec_len_test, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=123, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')        \n\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","b37b953e":"# PCAS = [col for col in train_features.columns if col.startswith('pca_')]\n# UMAPS = [col for col in train_features.columns if col.startswith('umap_')]","e9a4bc97":"# from sklearn.preprocessing import PolynomialFeatures\n# n_deg = 2\n\n# data = pd.concat([pd.DataFrame(train_features[PCAS]), pd.DataFrame(test_features[PCAS])])\n# data2 = (PolynomialFeatures(degree=n_deg, include_bias=False).fit_transform(data[PCAS]))\n\n# # print(data2)\n# # data4 = (UMAP(n_components=n_dim, n_neighbors=5, random_state=1903).fit_transform(data[GENES]))\n# # data5 = (UMAP(n_components=n_dim, min_dist=0.01, random_state=1903).fit_transform(data[GENES]))\n\n# train2 = data2[:train_features.shape[0]]\n# test2 = data2[-test_features.shape[0]:]\n\n# # print(train2.shape)\n# train2 = pd.DataFrame(train2, columns=[f'poly_C-{i}' for i in range(train2.shape[1])])\n# test2 = pd.DataFrame(test2, columns=[f'poly_C-{i}' for i in range(train2.shape[1])])\n\n# # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n# # train_features = pd.concat((train_features, train2, train3, train4, train5), axis=1)\n# # test_features = pd.concat((test_features, test2, test3, test4, test5), axis=1)\n# train_features = pd.concat((train_features, train2), axis=1)\n# test_features = pd.concat((test_features, test2), axis=1)\n\n\n# data = pd.concat([pd.DataFrame(train_features[UMAPS]), pd.DataFrame(test_features[UMAPS])])\n# data2 = (PolynomialFeatures(degree=n_deg, include_bias=False).fit_transform(data[UMAPS]))\n\n# # print(data2)\n# # data4 = (UMAP(n_components=n_dim, n_neighbors=5, random_state=1903).fit_transform(data[GENES]))\n# # data5 = (UMAP(n_components=n_dim, min_dist=0.01, random_state=1903).fit_transform(data[GENES]))\n\n# train2 = data2[:train_features.shape[0]]\n# test2 = data2[-test_features.shape[0]:]\n\n# # print(train2.shape)\n# train2 = pd.DataFrame(train2, columns=[f'poly_C-{i}' for i in range(train2.shape[1])])\n# test2 = pd.DataFrame(test2, columns=[f'poly_C-{i}' for i in range(train2.shape[1])])\n\n# # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n# # train_features = pd.concat((train_features, train2, train3, train4, train5), axis=1)\n# # test_features = pd.concat((test_features, test2, test3, test4, test5), axis=1)\n# train_features = pd.concat((train_features, train2), axis=1)\n# test_features = pd.concat((test_features, test2), axis=1)","f321a963":"print(train_features.shape)\nprint(test_features.shape)","0875691e":"# train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train_features.merge(train_targets_nonscored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n# target = train[train_targets_scored.columns]\ntarget = train[train_targets_nonscored.columns]","e5e49fe1":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","bf47a9ba":"print(target.shape)\nprint(train_features.shape)\nprint(test_features.shape)\nprint(train.shape)\nprint(test.shape)","57d87d4d":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","159d7c26":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=NFOLDS)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","331bbe15":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","a6496882":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","13b2dd14":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","6f9f5afb":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.15)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.3)\n        self.dense2 = nn.Linear(hidden_size, hidden_size)\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","6101b6c6":"def process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n#     data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n#     data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n\n# --------------------- Normalize ---------------------\n#     for col in GENES:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#     for col in CELLS:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#--------------------- Removing Skewness ---------------------\n#     for col in GENES + CELLS:\n#         if(abs(data[col].skew()) > 0.75):\n            \n#             if(data[col].skew() < 0): # neg-skewness\n#                 data[col] = data[col].max() - data[col] + 1\n#                 data[col] = np.sqrt(data[col])\n            \n#             else:\n#                 data[col] = np.sqrt(data[col])\n    \n    return data","23ca7519":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","8b366063":"num_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048\n# hidden_size=4096\n# hidden_size=9192","fe7f53e4":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.3, div_factor=1000, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.2, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    best_loss_epoch = -1\n    \n    if IS_TRAIN:\n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n\n            if valid_loss < best_loss:            \n                best_loss = valid_loss\n                best_loss_epoch = epoch\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{MODEL_DIR}\/{NB}-nonscored-SEED{seed}-FOLD{fold}_.pth\")\n\n            elif(EARLY_STOP == True):\n                early_step += 1\n                if (early_step >= early_stopping_steps):\n                    break\n\n            if epoch % 10 == 0 or epoch == EPOCHS-1:\n                print(f\"seed: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}, best_loss: {best_loss:.6f}, best_loss_epoch: {best_loss_epoch}\")            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"{MODEL_DIR}\/{NB}-nonscored-SEED{seed}-FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    if not IS_TRAIN:\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        oof[val_idx] = valid_preds    \n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","4c9a1d1f":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","eef48ec0":"SEED = range(NSEEDS)  #[0, 1, 2, 3 ,4]#, 5, 6, 7, 8, 9, 10]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_start = time.time()\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n    print(f\"elapsed time: {time.time() - time_start}\")\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n\nprint(oof.shape)\nprint(predictions.shape)","e4b54e0a":"train.to_pickle(f\"{INT_DIR}\/{NB}-train_nonscore_pred.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}-test_nonscore_pred.pkl\")","6587cb50":"len(target_cols)","3a3a3222":"train[target_cols] = np.maximum(PMIN, np.minimum(PMAX, train[target_cols]))\nvalid_results = train_targets_nonscored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_nonscored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","52ba3851":"\nEPOCHS = 25\n# NFOLDS = 5\n","6010a4f8":"# sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n# sub.to_csv('submission.csv', index=False)","07983445":"nonscored_target = [c for c in train[train_targets_nonscored.columns] if c != \"sig_id\"]","7852349b":"nonscored_target","a1acfd96":"train = pd.read_pickle(f\"{INT_DIR}\/{NB}-train_nonscore_pred.pkl\")\ntest = pd.read_pickle(f\"{INT_DIR}\/{NB}-test_nonscore_pred.pkl\")","364b432a":"# use nonscored target in the given file as feature\n# if comment out below, use predicted nonscored target\n# train = train.drop(nonscored_target, axis=1)\n# train = train.merge(train_targets_nonscored, on=\"sig_id\")\n# train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_scored, on='sig_id')\n# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n# test = test[test['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n# target = train[train_targets_scored.columns]\ntarget = train[train_targets_scored.columns]","a9cc43ce":"# from sklearn.preprocessing import QuantileTransformer\n\nfor col in (nonscored_target):\n\n    vec_len = len(train[col].values)\n    vec_len_test = len(test[col].values)\n    raw_vec = train[col].values.reshape(vec_len, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f\"{MODEL_DIR}\/{NB}_{col}_quantile_nonscored.pkl\")\n    else:\n        transformer = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_{col}_quantile_nonscored.pkl\")\n\n    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","ff705c65":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","ec1c756d":"train","3d0c8012":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=NFOLDS)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","82cf77d1":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","9c21ad33":"def process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n#     data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n#     data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n\n# --------------------- Normalize ---------------------\n#     for col in GENES:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#     for col in CELLS:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#--------------------- Removing Skewness ---------------------\n#     for col in GENES + CELLS:\n#         if(abs(data[col].skew()) > 0.75):\n            \n#             if(data[col].skew() < 0): # neg-skewness\n#                 data[col] = data[col].max() - data[col] + 1\n#                 data[col] = np.sqrt(data[col])\n            \n#             else:\n#                 data[col] = np.sqrt(data[col])\n    \n    return data","e959a846":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","721e3e40":"num_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048\n# hidden_size=4096\n# hidden_size=9192","41a827db":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.3, div_factor=1000, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.2, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    best_loss_epoch = -1\n    \n    if IS_TRAIN:\n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n\n            if valid_loss < best_loss:            \n                best_loss = valid_loss\n                best_loss_epoch = epoch\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{MODEL_DIR}\/{NB}-scored-SEED{seed}-FOLD{fold}_.pth\")\n\n            elif(EARLY_STOP == True):\n                early_step += 1\n                if (early_step >= early_stopping_steps):\n                    break\n\n            if epoch % 10 == 0 or epoch == EPOCHS-1:\n                print(f\"seed: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}, best_loss: {best_loss:.6f}, best_loss_epoch: {best_loss_epoch}\")            \n   \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"{MODEL_DIR}\/{NB}-scored-SEED{seed}-FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    if not IS_TRAIN:\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        oof[val_idx] = valid_preds    \n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","11973bff":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","eaf8ba7d":"SEED = range(NSEEDS)  #[0, 1, 2, 3 ,4]#, 5, 6, 7, 8, 9, 10]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_start = time.time()\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n    print(f\"elapsed time: {time.time() - time_start}\")\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","ccb80bab":"train.to_pickle(f\"{INT_DIR}\/{NB}-train-score-pred.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}-test-score-pred.pkl\")","1eba50fd":"len(target_cols)","291c364c":"train[target_cols] = np.maximum(PMIN, np.minimum(PMAX, train[target_cols]))\n\nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","d56dba77":"# train = pd.read_pickle(f\"..\/interim\/23-train-score-pred.pkl\")\n# test = pd.read_pickle(f\"..\/interim\/23-test-score-pred.pkl\")","3b12ca5d":"train = pd.read_pickle(f\"{INT_DIR}\/{NB}-train-score-pred.pkl\")\ntest = pd.read_pickle(f\"{INT_DIR}\/{NB}-test-score-pred.pkl\")","595b870a":"EPOCHS = 25\n# NFOLDS = 5","82f3208b":"PMIN = 0.0005\nPMAX = 0.9995\nfor c in train_targets_scored.columns:\n    if c != \"sig_id\":\n        train_targets_scored[c] = np.maximum(PMIN, np.minimum(PMAX, train_targets_scored[c]))","ecb8cbfa":"train_targets_scored.columns","a78a0833":"train = train[train_targets_scored.columns]\ntrain.columns = [c + \"_pred\" if (c != 'sig_id' and c in train_targets_scored.columns) else c for c in train.columns]","6f950966":"test = test[train_targets_scored.columns]\ntest.columns = [c + \"_pred\" if (c != 'sig_id' and c in train_targets_scored.columns) else c for c in test.columns]","12f996f1":"train","529f77b7":"# use nonscored target in the given file as feature\n# if comment out below, use predicted nonscored target\n# train = train.drop(nonscored_target, axis=1)\n# train = train.merge(train_targets_nonscored, on=\"sig_id\")\n# train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_scored, on='sig_id')\n# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n# test = test[test['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n# target = train[train_targets_scored.columns]\ntarget = train[train_targets_scored.columns]","5c32bc4b":"# train[\"cp_time\"] = train_features[train_features[\"cp_type\"]==\"trt_cp\"].reset_index(drop=True)[\"cp_time\"]\n# train[\"cp_dose\"] = train_features[train_features[\"cp_type\"]==\"trt_cp\"].reset_index(drop=True)[\"cp_dose\"]\n# test[\"cp_time\"] = test_features[test_features[\"cp_type\"]==\"trt_cp\"].reset_index(drop=True)[\"cp_time\"]\n# test[\"cp_dose\"] = test_features[test_features[\"cp_type\"]==\"trt_cp\"].reset_index(drop=True)[\"cp_dose\"]","642cf5b8":"from sklearn.preprocessing import QuantileTransformer\n\nscored_target_pred = [c + \"_pred\" for c in train_targets_scored.columns if c != 'sig_id']\n\nfor col in (scored_target_pred):\n\n#     transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    vec_len = len(train[col].values)\n    vec_len_test = len(test[col].values)\n    raw_vec = train[col].values.reshape(vec_len, 1)\n#     transformer.fit(raw_vec)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f\"{MODEL_DIR}\/{NB}_{col}_quantile_scored.pkl\")\n    else:\n        transformer = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_{col}_quantile_scored.pkl\")\n\n    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","f5bdcdfe":"# train = train.drop('cp_type', axis=1)\n# test = test.drop('cp_type', axis=1)","c7d3fd1e":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","0fe51ba7":"train","2f6a35f8":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=NFOLDS)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","f79e3375":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","06c20538":"folds","9d04c6b0":"def process_data(data):\n    \n#     data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n#     data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2, 0:0, 1:1, 2:2})\n#     data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1, 0:0, 1:1})\n\n# --------------------- Normalize ---------------------\n#     for col in GENES:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#     for col in CELLS:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#--------------------- Removing Skewness ---------------------\n#     for col in GENES + CELLS:\n#         if(abs(data[col].skew()) > 0.75):\n            \n#             if(data[col].skew() < 0): # neg-skewness\n#                 data[col] = data[col].max() - data[col] + 1\n#                 data[col] = np.sqrt(data[col])\n            \n#             else:\n#                 data[col] = np.sqrt(data[col])\n    \n    return data","2e846a13":"feature_cols = [c for c in folds.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","918ea438":"feature_cols","c0dd3b0e":"folds","b298bf62":"EPOCHS = 25\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1024\n# hidden_size=4096\n# hidden_size=9192","f4ca5618":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.3, div_factor=1000, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.2, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    best_loss_epoch = -1\n    \n    if IS_TRAIN:\n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n\n            if valid_loss < best_loss:            \n                best_loss = valid_loss\n                best_loss_epoch = epoch\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{MODEL_DIR}\/{NB}-scored2-SEED{seed}-FOLD{fold}_.pth\")\n            elif(EARLY_STOP == True):\n                early_step += 1\n                if (early_step >= early_stopping_steps):\n                    break\n\n            if epoch % 10 == 0 or epoch == EPOCHS-1:\n                print(f\"seed: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}, best_loss: {best_loss:.6f}, best_loss_epoch: {best_loss_epoch}\")                           \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"{MODEL_DIR}\/{NB}-scored2-SEED{seed}-FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    if not IS_TRAIN:\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        oof[val_idx] = valid_preds     \n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","dc311499":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","1649af4f":"SEED = range(NSEEDS)  # [0, 1, 2, 3 ,4]#, 5, 6, 7, 8, 9, 10]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_start = time.time()\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n    print(f\"elapsed time: {time.time() - time_start}\")\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","9e22bf7b":"train.to_pickle(f\"{INT_DIR}\/{NB}-train-score-stack-pred.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}-test-score-stack-pred.pkl\")","707d14cd":"train[target_cols] = np.maximum(PMIN, np.minimum(PMAX, train[target_cols]))\nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\ny_pred = np.minimum(SMAX, np.maximum(SMIN, y_pred))\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","9fba27a3":"# for c in test.columns:\n#     if c != \"sig_id\":\n#         test[c] = np.maximum(PMIN, np.minimum(PMAX, test[c]))\n\nsub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission_kibuna_nn.csv', index=False)","f573c6ec":"sub","a6409afe":"- CV log_loss:  0.014761779358699672\n- CV log_loss:  0.014519859174255039\n- CV log_loss:  0.014525173864593479\n- CV log_loss:  0.014354930596928602 # 3 umap features\n- CV log_loss:  0.014353604854355429 # more umap features\n- CV log_loss:  0.01436484670778641 # more hidden nodes\n- CV log_loss:  0.014344688083211073\n  - using predicted unscored targets as feature \n- CV log_loss:  0.013368097791623873\n  - using given unscored targets as feature\n  - bad in public lb\n- CV log_loss:  0.01434373547175235\n  - rankgauss predicted unscored targets\n- CV log_loss:  0.014346100008158216\n  - unscored targets pca\/umap\n- CV log_loss:  0.014328486629791769\n  - NFOLDS=10, Epoch=20\n- CV log_loss:  0.014299741080816082\n  - NFOLDS=10, Epoch=20, 25\n- CV log_loss:  0.014311301224480969\n  - NFOLDS=10, Epoch=25\n- CV log_loss:  0.01429269446076626\n  - NFOLDS=10, Epoch=15, 25","7d3b219f":"- a notebook to save preprocessing model and train\/save NN models\n- all necessary ouputs are stored in MODEL_DIR = output\/kaggle\/working\/model\n    - put those into dataset, and load it from inference notebook","3b30fdee":"CV log_loss:  0.014761779358699672\nCV log_loss:  0.014519859174255039\nCV log_loss:  0.014525173864593479\nCV log_loss:  0.014354930596928602 # 3 umap features\nCV log_loss:  0.014353604854355429 # more umap features\nCV log_loss:  0.01436484670778641 # more hidden nodes"}}