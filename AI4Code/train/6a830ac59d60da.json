{"cell_type":{"68bb6ae6":"code","d166822e":"code","c8ead2bd":"code","2f41aeff":"code","0e10f77f":"code","a947f721":"code","6d27ec77":"code","bab09c11":"code","c53962ba":"code","ab2ba239":"code","f88260a2":"code","299ab9b9":"code","063088db":"code","8961816c":"code","2d28aa59":"code","5873b541":"code","07b2fe3d":"code","ce1cf357":"code","b45628f9":"code","e2ac8be2":"code","6fba322f":"code","cb827bdd":"code","6301b536":"code","526f68d8":"code","61897607":"code","711969c4":"code","2819ab9d":"code","481b3cd6":"code","511bb551":"code","46a43fb8":"code","c97500c8":"code","d1306a2e":"code","30f950f4":"code","d06a9d07":"code","d6015d75":"code","c90b6bc7":"code","c18324d7":"code","ddc1797c":"code","742b4a11":"code","6c60aaff":"code","32044460":"code","cd5bb024":"code","80ba4df8":"code","c99639d7":"code","b1a701ba":"code","ef6ef26d":"code","69081f56":"code","11e74eb3":"code","e87e6c23":"code","093f0f59":"code","06b83f4e":"code","4ecbd7b5":"code","02076674":"code","ea03ef93":"code","f9aae94d":"code","5ef86884":"code","6b3505b8":"code","16ab04ae":"code","c39e1b6d":"code","92aa26b6":"markdown","9f754ad6":"markdown","5a3973f2":"markdown","74b126c1":"markdown","e6db956f":"markdown","5e4efac1":"markdown","4585a3e9":"markdown","cb60f783":"markdown","f4675e73":"markdown","50da90d9":"markdown","adb16fd0":"markdown","6f63c477":"markdown","54406758":"markdown","6fe096fb":"markdown","73ed1c9f":"markdown","116d1014":"markdown","1e2d0374":"markdown","0e6b8a58":"markdown","a8cd2b14":"markdown","9af772a0":"markdown","a8131269":"markdown","e3a60b2f":"markdown","e5beb8fb":"markdown","241c90c0":"markdown","a509ef79":"markdown","3a0b30b4":"markdown","2488974d":"markdown","cd93029d":"markdown","fbdfba89":"markdown","4ffcb7d0":"markdown","e655a1e3":"markdown","718202f9":"markdown","775c85e8":"markdown","f618ed60":"markdown","111ab568":"markdown","54952bc0":"markdown","b76e83b9":"markdown"},"source":{"68bb6ae6":"ls","d166822e":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns","c8ead2bd":"df=pd.read_csv(\"stores.csv\")\ndf.head()","2f41aeff":"df.shape","0e10f77f":"df.describe()","a947f721":"df_fts=pd.read_csv(\"features.csv\")\ndf_fts.head()","6d27ec77":"df_fts.describe()","bab09c11":"df_fts[\"MarkDown1\"][df_fts[\"MarkDown1\"]<0]=0\ndf_fts[\"MarkDown2\"][df_fts[\"MarkDown2\"]<0]=0\ndf_fts[\"MarkDown3\"][df_fts[\"MarkDown3\"]<0]=0\ndf_fts[\"MarkDown5\"][df_fts[\"MarkDown5\"]<0]=0","c53962ba":"df_fts.describe()","ab2ba239":"df_train=pd.read_csv(\"train.csv\")\ndf_train.head()","f88260a2":"df_train.describe()","299ab9b9":"df_train[\"Weekly_Sales\"][df_train[\"Weekly_Sales\"]<0]=0","063088db":"df_train.describe()","8961816c":"df_test=pd.read_csv(\"test.csv\")\ndf_test.head()","2d28aa59":"df_full=pd.merge(df_train,df,how='left',on='Store').merge(df_fts,how='inner',on=['Store','IsHoliday','Date'])\ndf_full.head()","5873b541":"df_full['CPI'] = df_full['CPI'].fillna(df_full['CPI'].mean())\ndf_full['Temperature'] = df_full['Temperature'].fillna(df_full['Temperature'].mean())\ndf_full['Unemployment'] = df_full['Unemployment'].fillna(df_full['Unemployment'].mean())","07b2fe3d":"df_full.fillna(0,inplace=True)","ce1cf357":"df_full.head()","b45628f9":"sample_data=pd.concat([df_full['Type'],df_full['Size']],axis=1)\nfig=sns.boxplot(x='Type',y='Size',data=sample_data)","e2ac8be2":"sample_data=pd.concat([df_full['Type'],df_full['Weekly_Sales']],axis=1)\nfig=sns.boxplot(x='Type',y='Weekly_Sales',data=sample_data,showfliers=False)","6fba322f":"data = pd.concat([df_full['Store'], df_full['Weekly_Sales'], df_full['IsHoliday']], axis=1)\nf, ax = plt.subplots(figsize=(25, 8))\nfig = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=\"IsHoliday\")","cb827bdd":"#encoding for IsHoliday\ndf_full.IsHoliday=df_full.IsHoliday.astype(int)","6301b536":"#encode Type Feature\nle=preprocessing.LabelEncoder().fit(df_full['Type'])\n\nle.classes_\n\ndf_full.Type=le.transform(df_full['Type'])","526f68d8":"df_full.head()","61897607":"#split Date into Day-Month-Year\n\ndf_full[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(df_full.Date)]\ndf_full[\"month\"] = [t.month for t in pd.DatetimeIndex(df_full.Date)]\ndf_full['year'] = [t.year for t in pd.DatetimeIndex(df_full.Date)]\n","711969c4":"df_full.drop(\"Date\",axis=1,inplace=True)","2819ab9d":"df_full.head()","481b3cd6":"# Plotting correlation between all important features\ncorr = df_full.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr, annot=True)\nplt.plot()","511bb551":"#removing one of the highly correlated features \ndf_full=df_full.drop([\"MarkDown4\",\"year\",\"Size\"],axis=1)","46a43fb8":"y=np.array(df_full['Weekly_Sales'])\n\nX=np.array(df_full.drop(['Weekly_Sales'],axis=1))","c97500c8":"#train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","d1306a2e":"import statistics as st\nimport itertools\nn=np.size(y_train)\ny_mean=st.mean(y_train)\ny_pred=list(itertools.repeat(y_mean,n))","30f950f4":"from sklearn.metrics import median_absolute_error,r2_score\nprint(\"MAD Score\",median_absolute_error(y_pred,y_train))\nprint(\"R2 Score\",r2_score(y_pred,y_train))","d06a9d07":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nparam_grid={'fit_intercept':[True,False],'normalize':[True,False]}\nlr=LinearRegression(n_jobs=-1)\nmodel=GridSearchCV(lr,param_grid,scoring='neg_median_absolute_error',n_jobs=-1,pre_dispatch='2*n_jobs').fit(X_train,y_train)","d6015d75":"print(\"Best Hyperparam Values\",model.best_params_)\nprint(\"Median cross-validated score \",model.best_score_) ","c90b6bc7":"Lr_model=LinearRegression(fit_intercept=False,normalize=True,n_jobs=-1).fit(X_train,y_train)","c18324d7":"import matplotlib.pyplot as plt\ny_pred=Lr_model.predict(X_test)\nplt.scatter(y_test,y_pred)\nplt.title(\"Linear Regression Plot\")\nplt.xlabel(\"Y_Test\")\nplt.ylabel(\"Y_Predict\")\nplt.show()","ddc1797c":"from sklearn.metrics import median_absolute_error\nprint(\"MAD score : \",median_absolute_error(y_test,y_pred))\nprint(\"R2 Score : \",Lr_model.score(X_test,y_test))","742b4a11":"from sklearn.linear_model import SGDRegressor\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nparam_grid={'loss':['squared_loss','huber','epsilon_insensitive','squared_epsilon_insensitive'],\n'penalty':['l1','l2','elasticnet'],'alpha':[0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10],\n'learning_rate':['constant','optimal','invscaling','adaptive'] ,        \n'early_stopping':[True,False]  }\nsvm_model=SGDRegressor()\nmodel_svm=GridSearchCV(svm_model,param_grid,scoring='neg_median_absolute_error',n_jobs=-1,pre_dispatch='2*n_jobs').fit(X_train,y_train)","6c60aaff":"print(\"Best Hyperparam Values\",model_svm.best_params_)\nprint(\"Median cross-validated score \",model_svm.best_score_) ","32044460":"svm_model=SGDRegressor(alpha=0.0001,early_stopping=False,learning_rate='optimal', loss='huber', penalty= 'l2')\nsvm_model.fit(X_train,y_train)","cd5bb024":"y_pred=svm_model.predict(X_test)\nplt.scatter(y_test,y_pred)\nplt.title(\"SVM Regression Plot\")\nplt.xlabel(\"Y_Test\")\nplt.ylabel(\"Y_Predict\")\nplt.show()","80ba4df8":"from sklearn.metrics import median_absolute_error\nprint(\"MAD score : \",median_absolute_error(y_test,y_pred))\nprint(\"R2 Score : \",svm_model.score(X_test,y_test))","c99639d7":"from sklearn.tree import DecisionTreeRegressor \nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nparam_grid={'max_depth':[1,5,10,15,20,25,30],\n 'max_features': ['auto', 'sqrt','log2'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10], }\ndt=DecisionTreeRegressor()\nmodel_dt=GridSearchCV(dt,param_grid,scoring='neg_median_absolute_error',n_jobs=-1,pre_dispatch='2*n_jobs').fit(X_train,y_train)","b1a701ba":"print(\"Best Hyperparam Values\",model_dt.best_params_)\nprint(\"Median cross-validated score \",model_dt.best_score_) ","ef6ef26d":"DTR_model=DecisionTreeRegressor(max_depth=25,max_features='auto',min_samples_leaf=4,min_samples_split=10).fit(X_train,y_train)","69081f56":"y_pred=DTR_model.predict(X_test)\nplt.scatter(y_test,y_pred)\nplt.title(\"Decision Tree Regression Plot\")\nplt.xlabel(\"Y_Test\")\nplt.ylabel(\"Y_Predict\")\nplt.show()","11e74eb3":"from sklearn.metrics import median_absolute_error\nprint(\"MAD score : \",median_absolute_error(y_test,y_pred))\nprint(\"R2 Score : \",DTR_model.score(X_test,y_test))","e87e6c23":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nparam_grid={'max_depth':[1,5,10,15,20,25,30],'n_estimators':[20,50,100],\n  'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10], }\nRFR=RandomForestRegressor()\nmodel_rf=RandomizedSearchCV(RFR,param_grid,scoring='neg_median_absolute_error',n_jobs=-1,pre_dispatch='2*n_jobs').fit(X_train,y_train)","093f0f59":"print(\"Best Hyperparam Values\",model_rf.best_params_)\nprint(\"Median cross-validated score \",model_rf.best_score_) ","06b83f4e":"random_frgr=RandomForestRegressor(n_estimators=100,min_samples_split=5,min_samples_leaf=2,max_depth=30,n_jobs=-1).fit(X_train,y_train)","4ecbd7b5":"y_pred=random_frgr.predict(X_test)\nplt.scatter(y_test,y_pred)\nplt.title(\"Random Forrest Tree Regression Plot\")\nplt.xlabel(\"Y_Test\")\nplt.ylabel(\"Y_Predict\")\nplt.show()","02076674":"from sklearn.metrics import median_absolute_error\nprint(\"MAD score : \",median_absolute_error(y_test,y_pred))\nprint(\"R2 Score : \",random_frgr.score(X_test,y_test))","ea03ef93":"from xgboost import XGBRegressor\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nparam_grid={'max_depth':[1,5,10,15,20,25,30],'n_estimators':[20,50,100],'learning_rate':[0.001,0.01,0.1,1]}\nxgb=XGBRegressor(n_jobs=-1)\nmodel_rf=RandomizedSearchCV(xgb,param_grid,scoring='neg_median_absolute_error',n_jobs=-1,pre_dispatch='2*n_jobs').fit(X_train,y_train)","f9aae94d":"print(\"Best Hyperparam Values\",model_rf.best_params_)\nprint(\"Median cross-validated score \",model_rf.best_score_) ","5ef86884":"xgbr=XGBRegressor(n_estimators=100,max_depth=25,learning_rate=0.1,n_jobs=-1).fit(X_train,y_train)","6b3505b8":"y_pred=xgbr.predict(X_test)\nplt.scatter(y_test,y_pred)\nplt.title(\"Gradient Boosted Tree Regression Plot\")\nplt.xlabel(\"Y_Test\")\nplt.ylabel(\"Y_Predict\")\nplt.show()","16ab04ae":"from sklearn.metrics import median_absolute_error\nprint(\"MAD score : \",median_absolute_error(y_test,y_pred))\nprint(\"R2 Score : \",xgbr.score(X_test,y_test))","c39e1b6d":"#http:\/\/zetcode.com\/python\/prettytable\/\nfrom prettytable import PrettyTable\nx=PrettyTable()\nprint(\"Machine Learning Models\")\nx.field_names=['Model','MAD Score','R2 Score',]\nx.add_row(['Random Model',12576.21,-3.166053952252629e+30])\nx.add_row(['Linear Regression',9475.04,-0.4290])\nx.add_row(['SGD Regressor',3958.41,-0.2867])\nx.add_row(['Decision Tree Regressor',648.028,0.9483])\nx.add_row(['Random Forrest Regressor',573.85,0.9619])\nx.add_row(['Gradient Boosted Regressor',554.25,0.9664])\nprint(x)","92aa26b6":"# Decision Tree Regressor","9f754ad6":" Observation: Looking at all the models currently Decision Tree Regressor Looks Like the Best Model. Even R2 score is close to 1.","5a3973f2":"Observation: Plot looks even better","74b126c1":"# Exploratory Data Analysis","e6db956f":"Observation: Plot looks similar as that of a decision tree plot.","5e4efac1":"References \nhttps:\/\/www.kaggle.com\/yepp2411\/walmart-prediction-1-eda-with-time-and-space\n\n\nhttps:\/\/www.kaggle.com\/jevonlee001\/walmat-rfr","4585a3e9":"Note: MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.","cb60f783":"Observation: Hyperparameter tuning was taking a lot of time hence we took lesser features yet we can see that Random Forrest is out performing all the models.","f4675e73":"# Machine Learning Models","50da90d9":"<p style=\"font-size:36px;text-align:center\"> <b>Walmart Recruiting<\/b> <\/p>","adb16fd0":"<b>Data Description<\/b>\n\nYou are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments, and you are tasked with predicting the department-wide sales for each store.\n\nIn addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete\/ideal historical data.\n\n<b>stores.csv<\/b>\n\nThis file contains anonymized information about the 45 stores, indicating the type and size of store.\n\n<b>train.csv<\/b>\n\nThis is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:\n\n    Store - the store number\n    Dept - the department number\n    Date - the week\n    Weekly_Sales -  sales for the given department in the given store\n    IsHoliday - whether the week is a special holiday week\n\n<b>test.csv<\/b>\n\nThis file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.\n\n<b>features.csv<\/b>\n\nThis file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n\n    Store - the store number\n    Date - the week\n    Temperature - average temperature in the region\n    Fuel_Price - cost of fuel in the region\n    MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n    CPI - the consumer price index\n    Unemployment - the unemployment rate\n    IsHoliday - whether the week is a special holiday week\n\nFor convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\nSuper Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\nLabor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\nThanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\nChristmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","6f63c477":"https:\/\/www.researchgate.net\/post\/What_is_the_acceptable_R-squared_in_the_information_system_research_Can_you_provide_some_references\nThe (R-squared) , (also called the coefficient of determination), which is the proportion of variance (%) in the dependent variable that can be explained by the independent variable. Hence, as a rule of thumb for interpreting the strength of a relationship based on its R-squared value (use the absolute value of the R-squared value to make all values positive):\n- if  R-squared value < 0.3 this value is generally considered a None or Very weak effect size,\n- if R-squared value 0.3 < r < 0.5 this value is generally considered a weak or low effect size,\n- if R-squared value 0.5 < r < 0.7 this value is generally considered a Moderate effect size,\n- if R-squared value r > 0.7 this value is generally considered strong effect size,\nRef: Source: Moore, D. S., Notz, W. I, & Flinger, M. A. (2013). The basic practice of statistics (6th ed.). New York, NY: W. H. Freeman and Company. Page (138).\n\nSource: Zikmund, William G. (2000). Business research methods (6th ed). Fort Worth: Harcourt College Publishers. (Page 513)","54406758":"Observation: Again Weekly_sales cannot have a negative value.","6fe096fb":"Type can be an estimate on weekly sales","73ed1c9f":"# Reading Train Data","116d1014":"# Reading Features Data","1e2d0374":"Observation: The SVM Regression Model performs better than Linear Regression still not a desirable model.","0e6b8a58":"Observation: We can see here that a negative markdown value exists,which shouldn't be the case hence we need to remove all values <0","a8cd2b14":"# Reading Stores Data","9af772a0":"Observation: Just by looking at the plot we can sense that there is an improvement in performance with this model","a8131269":"Observation: Gradient Boosted Regression Trees perform better compared to all the models.","e3a60b2f":"Observation: There are missing values in CPI, Unemployment, Temperature,we have filled values by mean imputation","e5beb8fb":"# Random Model","241c90c0":"Ignore the negative sign for all subsequent models","a509ef79":"Observations\n\n1. You can see that there is a slight increase in weekly sales on a holiday as compared to non holiday","3a0b30b4":"Observations\n1. Not many features are correlated apart from MarkDown1 and MarkDown4 and Fuel_Price and year\n2. Size and Type are negative correlated\n3. Correlated Features should be deleted","2488974d":"# Feature Engineering","cd93029d":"Observations\n\n1. The median of A is the highest and C is the lowest\n2. Stores with more sizes have higher sales record","fbdfba89":"Observations\n1. We can infer that Store A is the largest and C is the smallest\n2. There is no overlapped area in size among A,B,C\n","4ffcb7d0":"Observation: Performance of the Linear Regression Model is better than the Random model but the R2 Score is negative  which means the model isn't a really good fit model but it's better than the random model. An ideal R2 model score should be around +1.","e655a1e3":"# SGD Regressor","718202f9":"# Linear Regression","775c85e8":"# Merging Train+Store+Features","f618ed60":"# Reading Test Data","111ab568":"# Gradient Boosted Regressor","54952bc0":"# Conclusion","b76e83b9":"# Random Forrest Regressor"}}