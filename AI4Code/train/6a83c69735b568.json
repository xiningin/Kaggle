{"cell_type":{"a698fbaf":"code","fc626e00":"code","d60a9264":"code","512af779":"code","09e00a72":"code","5c6cb1a8":"code","0ab49467":"code","4466af90":"code","a62427c3":"code","c836564f":"code","c7b693a4":"code","8714dad5":"code","a3e985cd":"code","7c8017dd":"code","41cd783a":"code","7c14a815":"code","92435831":"code","d96a8a6c":"code","e9ebb80a":"code","e17ca7b9":"code","952fa65e":"code","29fd1513":"code","7775bc3b":"code","df6294bc":"code","7730b1fb":"code","ac9304ed":"code","81b18d21":"code","6d4f3c2b":"code","c223da11":"code","258b867d":"code","2f789bb9":"code","2a2fd131":"code","320494b4":"code","22b4eddf":"code","fcb01be3":"code","21825199":"code","a4959c2c":"code","086a8f9e":"code","803a7f9c":"code","441336d0":"code","04b824b9":"code","218150dc":"code","05a22490":"code","ecad7752":"code","dcb4b458":"code","22bbf15e":"code","a5dedffb":"code","16697362":"code","ffd84bba":"code","eecdee3f":"code","4d6f6c7b":"code","03e8a686":"code","a9023247":"code","a3b04ba2":"code","ba144d25":"code","b4734c80":"code","27a58edb":"code","78e1f2e0":"code","cad8c504":"code","be52557c":"code","ef0b5050":"code","8ec19c3f":"code","6dee1f3e":"code","10266f75":"code","223894ce":"code","4f5a688f":"code","164bca7b":"code","1872ceb2":"code","edc590d2":"code","83801b49":"code","6b1e083a":"code","23a04d44":"code","266dadf6":"code","dd115adc":"code","e5938169":"code","ab0ff057":"code","bf2c84c4":"code","aeb5f675":"code","489aa14c":"code","964b2480":"code","7ba5b2b0":"code","09856c58":"code","53087683":"code","89522e16":"code","ebe4335f":"code","58bc0f67":"code","46987e64":"code","e669cca9":"code","289e6cfd":"code","a5faf08f":"code","2154361b":"code","79777a26":"code","1246c656":"code","49d4732d":"code","1f62e44e":"code","a0bf4835":"code","e7e58bff":"code","03b88fa0":"code","b37d4ebe":"code","91b055cb":"code","3f809bf1":"code","cb1b224c":"code","8df46f89":"code","8d7e4bb4":"code","86367715":"code","15dbb8ec":"code","f6e02fe9":"code","94ea302d":"code","b7aa4299":"code","0141f574":"code","c8dd3845":"code","5df725ab":"code","95f75d63":"code","17dbc84b":"code","800bef20":"code","c8c0e3c5":"code","3513b366":"code","4ab70daa":"code","b3444ede":"code","1ad48a5e":"code","7bed2e27":"code","eaf79e43":"code","0caa50fb":"code","e0bfc44f":"code","916f8647":"code","ee1d7527":"code","a60d6e8c":"code","e2eed42d":"code","cdc2bfa5":"code","5742463e":"code","45c625d4":"code","f9b03e8e":"code","e67f4bee":"code","47ca5230":"code","7130afcb":"code","0066d4e7":"code","405793be":"code","4199a3f2":"code","1a71fd34":"code","68851804":"code","18c4cb0b":"code","b690f59f":"code","feac1267":"code","41b68f5e":"code","06b474e9":"code","bf383c5d":"code","9850e10d":"code","e690ff04":"code","305d882c":"code","bd36ea18":"code","d358ee73":"code","4529b655":"code","efa7a647":"code","82fbd79a":"code","15b337aa":"code","293cdec4":"code","e7de31ab":"code","8e78b5c4":"code","49a7a313":"code","43a904a1":"code","17640a94":"code","e0f32adb":"code","0b740c83":"code","80848894":"code","015aa547":"code","10ffc243":"code","2c500947":"code","570336aa":"code","26f894ff":"code","66a741ee":"code","c86f1318":"code","3bbb4498":"code","b200ba7e":"code","5dfa7628":"code","709626c3":"code","398ddb37":"code","39fe9b16":"code","c4274adc":"code","0018cc73":"code","71d80a55":"code","33da799d":"code","a63528ac":"code","5f338b1b":"code","bce7e8f3":"code","9ce31876":"code","1595233a":"code","c31e989c":"code","4adbe8d8":"code","64fff690":"markdown","2d12efc1":"markdown","c03e7ff1":"markdown","d80cab13":"markdown","e7255cff":"markdown","cd10e2e2":"markdown","5aa30cf5":"markdown","5c7c38ed":"markdown","3f9d2a24":"markdown","57370be6":"markdown","552cabf9":"markdown","cc80f72c":"markdown","33a2ad21":"markdown","858e15cc":"markdown","8864ca0f":"markdown","4bdda310":"markdown","629a1815":"markdown","67305208":"markdown"},"source":{"a698fbaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc626e00":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\n%matplotlib inline","d60a9264":"# get your data\nfuelcon = pd.read_csv('\/kaggle\/input\/fuelconsumptionco2\/FuelConsumptionCo2.csv')\nfuelcon.head()","512af779":"fuelcon.describe()","09e00a72":"fuelcon.describe(include=['object'])","5c6cb1a8":"cdf = fuelcon[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]\ncdf.head(9)","0ab49467":"cdf.hist()\nplt.show()","4466af90":"plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS,color='blue')\nplt.xlabel('fuelconsumption_comb')\nplt.ylabel('emission')\nplt.show()","a62427c3":"plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,color='blue')\nplt.xlabel('fuelconsumption_comb')\nplt.ylabel('emission')\nplt.show()","c836564f":"plt.scatter(cdf.CYLINDERS, cdf.CO2EMISSIONS,color='blue')\nplt.xlabel('fuelconsumption_comb')\nplt.ylabel('emission')\nplt.show()","c7b693a4":"# train and test dataset\nmsk = np.random.rand(len(fuelcon)) < 0.8\nmsk","8714dad5":"train = cdf[msk]\ntest = cdf[~msk]\nprint('train set: ',train.shape[0],'test set: ',test.shape[0])","a3e985cd":"# simple regression model\n\n#train data distribution\nplt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue')\nplt.xlabel('engine size')\nplt.ylabel('emission')\nplt.show()","7c8017dd":"from sklearn import linear_model\nlr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['ENGINESIZE']])\ntrain_y = np.asanyarray(train[['CO2EMISSIONS']])\nlr.fit(train_x,train_y)\nprint('coef: ' , lr.coef_,'intercept: ',lr.intercept_)","41cd783a":"# plot outputs\nplt.scatter(train.ENGINESIZE,train.CO2EMISSIONS,color='blue')\nplt.plot(train_x,lr.coef_[0][0]*train_x+lr.intercept_[0],'-r')\nplt.xlabel('engine size')\nplt.ylabel('emission')\nplt.show()","7c14a815":"#evaluation\nfrom sklearn.metrics import r2_score\n\ntest_x = np.asanyarray(test[['ENGINESIZE']])\ntest_y = np.asanyarray(test[['CO2EMISSIONS']])\ntest_yhat = lr.predict(test_x)\n\nprint('mean absolute error: %.2f' % np.mean(np.absolute(test_y-test_yhat)))\nprint('residual sum of squares(MSE): %.2f' % np.mean((test_y-test_yhat)**2))\nprint('R2-score: %.2f' % r2_score(test_y,test_yhat))","92435831":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","d96a8a6c":"# degree of 1\nx = np.arange(-5.0,5.0,0.1)\n\ny = 2*(x)+3\ny_noise = 2*np.random.normal(size=x.size)\nydata = y + y_noise\n\nplt.plot(x,ydata,'bo')\nplt.plot(x,y,'r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()","e9ebb80a":"# quadratic , degree of 2\nx = np.arange(-5.0,5.0,0.1)\n\ny = np.power(x,2)\ny_noise = 2*np.random.normal(size=x.size)\nydata = y + y_noise\n\nplt.plot(x,ydata,'bo')\nplt.plot(x,y,'r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()","e17ca7b9":"# cubic, degree of 3\nx = np.arange(-5.0,5.0,0.1)\n\ny = x**3 + x**2 + x + 3\ny_noise = 20*np.random.normal(size=x.size)\nydata = y + y_noise\n\nplt.plot(x,ydata,'bo')\nplt.plot(x,y,'r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()","952fa65e":"# exponential, y = a + b*c^x\nx = np.arange(-5.0,5.0,0.1)\n\ny = np.exp(x)\n\nplt.plot(x,y,'r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()","29fd1513":"# logarithmic\nx = np.arange(-5.0,5.0,0.1)\n\ny = np.log(x)\n\nplt.plot(x,y,'r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()","7775bc3b":"# sigmoidal\/ logistic\nx = np.arange(-5.0,5.0,0.1)\n\ny = 1-4\/(1+np.power(3,x-2))\n\nplt.plot(x,y,'r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()","df6294bc":"# example\n\n# step 1: get your data\nimport numpy as np\nimport pandas as pd\ngdp = pd.read_csv('\/kaggle\/input\/china-gdp\/china_gdp.csv')\ngdp.head()","7730b1fb":"# step 2: plot your data\nplt.figure(figsize=(8,5))\nx_data,y_data = (gdp['Year'].values,gdp['Value'].values)\nplt.plot(x_data,y_data,'ro')\nplt.xlabel('year')\nplt.ylabel('gdp')\nplt.show()","ac9304ed":"# step 3: choosing a model : logistic may be good like below\n\nx = np.arange(-5.0,5.0,0.1)\n\ny = 1\/(1+np.exp(-x))\n\nplt.plot(x,y,'r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()","81b18d21":"# step 4: building the model\n\n# function with beta_1, beta_2 parameters for a logistic model\ndef sigmoid(x,beta_1,beta_2):\n    y = 1\/(1+np.exp(-beta_1*(x-beta_2)))\n    return y","6d4f3c2b":"beta_1 = 0.1\nbeta_2 = 1990\n\ny_hat = sigmoid(x_data,beta_1,beta_2)\n\nplt.plot(x_data,y_hat*15000000000000)\nplt.plot(x_data,y_data,'ro')\nplt.show","c223da11":"# step 5: find best parameters for model\n\n# normalize data\nxdata = x_data\/max(x_data)\nydata = y_data\/max(y_data)","258b867d":"# use curve_fit , popt are optimized parameters\n\nfrom scipy.optimize import curve_fit\npopt,pcov = curve_fit(sigmoid,xdata,ydata)\n\nprint('beta_1 = %f,beta_2 = %f' % (popt[0],popt[1]))","2f789bb9":"# step 6: plot the resulting model\n\nx =  np.linspace(1960,2015,55)\nx = x\/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x,*popt)\nplt.plot(xdata,ydata,'ro',label='data')\nplt.plot(x,y,linewidth=3.0,label='fit')\nplt.legend(loc='best')\nplt.xlabel('year')\nplt.ylabel('gdp')\nplt.show()","2a2fd131":"# step 7: evaluate the resulting model\n\n# split data into train and test\nmsk = np.random.rand(len(gdp)) < 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n#build the model using train data\npopt,pcov = curve_fit(sigmoid,train_x,train_y)\n\n#predict using test data\ny_hat = sigmoid(test_x,*popt)\n\n#evaluation\nprint('mean absolute error: %.2f' % np.mean(np.absolute(y_hat - test_y)))\nprint('residual sum of squares(MSE): %.2f'% np.mean((y_hat-test_y)**2))\nprint('R2-Score: %.2f'% r2_score(y_hat,test_y))","320494b4":"import itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing\n%matplotlib inline","22b4eddf":"# step 1: get your data\ntele = pd.read_csv('\/kaggle\/input\/telecust1000t\/teleCust1000t.csv')\ntele","fcb01be3":"# step 2: visualize\ntele.custcat.value_counts()\n# 281 pus service, 266 basic service, 236 total service, 217 e-service customers","21825199":"tele.income.hist(bins=50)","a4959c2c":"# step 3: feature set\ntele.columns","086a8f9e":"# to use scikit-learn library, we have to convert the pandas data frame into a numpy array\nx = tele[['region', 'tenure', 'age', 'marital', 'address', 'income', 'ed','employ', 'retire', 'gender', 'reside']].values          \nx[:5]","803a7f9c":"y = tele.custcat.values\ny[:5]","441336d0":"# step 4: normalize data\nx = preprocessing.StandardScaler().fit(x).transform(x.astype(float))\nx[:5]","04b824b9":"# step 5: train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=4)\nprint('train set',x_train.shape,y_train.shape)\nprint('test set',x_test.shape,y_test.shape)","218150dc":"# step 6: set up the model\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nk = 4\nneigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train)\nyhat = neigh.predict(x_test)\nyhat[:5]","05a22490":"# step 7: evaluation the model\n\nfrom sklearn import metrics\nprint('train set accuracy: ',metrics.accuracy_score(y_train,neigh.predict(x_train)))\nprint('test set accuracy\uff1a',metrics.accuracy_score(y_test,yhat))","ecad7752":"# step 8: model tune\n\n# if k =6\nk =6 \nneigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train)\nyhat = neigh.predict(x_test)\nprint('train set accuracy: ',metrics.accuracy_score(y_train,neigh.predict(x_train)))\nprint('test set accuracy: ',metrics.accuracy_score(y_test,neigh.predict(x_test)))","dcb4b458":"# calc the accuracy for different Ks\nks = 10\nmean_acc = np.zeros(ks-1)\nstd_acc = np.zeros(ks-1)\nconfusionmx = []\nfor n in range(1,ks):\n    neigh = KNeighborsClassifier(n_neighbors = n).fit(x_train,y_train)\n    yhat= neigh.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test,yhat)\n    std_acc[n-1] = np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\nmean_acc","22bbf15e":"# plot model accuracy for different number of neighbors(k)\nplt.plot(range(1,ks),mean_acc,'g')\nplt.fill_between(range(1,ks),mean_acc-1*std_acc,mean_acc+1*std_acc,alpha=0.1)\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.xlabel('number of nabors(k)')\nplt.ylabel('accuracy')\nplt.tight_layout()\nplt.show()","a5dedffb":"print('the best accuracy was with: ',mean_acc.max(),'with k is ',mean_acc.argmax()+1)","16697362":"from sklearn.tree import DecisionTreeClassifier","ffd84bba":"# step 1: get your data\ndrug = pd.read_csv('\/kaggle\/input\/drug200\/drug200.csv')\ndrug","eecdee3f":"# step 2: preprocessing\nX = drug[['Age','Sex','BP','Cholesterol','Na_to_K']].values\nX[:5]","4d6f6c7b":"# sex and bp are catergorical which SKlearn deicisiontree do not handle. \nfrom sklearn import preprocessing\nle_sex = preprocessing.LabelEncoder()\nle_sex.fit(['F','M'])\nX[:,1] = le_sex.transform(X[:,1])\n\nle_BP = preprocessing.LabelEncoder()\nle_BP.fit(['LOW','NORMAL','HIGH'])\nX[:,2] = le_BP.transform(X[:,2])\n\nle_chol = preprocessing.LabelEncoder()\nle_chol.fit(['NORMAL','HIGH'])\nX[:,3] = le_chol.transform(X[:,3])\n\nX[:5]","03e8a686":"Y = drug['Drug']\nY[:5]","a9023247":"# step 3: setting up the model\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=3)\nprint('x_train and x_test:',x_train.shape,x_test.shape)\nprint('y_train and y_test:',y_train.shape,y_test.shape)\n\ndrugtree = DecisionTreeClassifier(criterion='entropy',max_depth=4)\ndrugtree.fit(x_train,y_train)\npretree = drugtree.predict(x_test)\nprint(pretree[:5])\nprint(y_test[:5].values)","a3b04ba2":"# step 4: evaluation\n\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nprint('decission tree accuracy: ',metrics.accuracy_score(y_test,pretree))","ba144d25":"# step 5: visualization\n\nfrom six import StringIO\n# import pydotplus\nimport graphviz\nfrom sklearn.tree import export_graphviz\nimport matplotlib.pyplot as plt\n\nimport matplotlib.image as mpimg\nfrom sklearn import tree\n%matplotlib inline\n\n# from io import StringIO\n# dot_data = StringIO()\n# filename = 'drugtree.png'\n# featurenames = my_data.columns[:5]\n# targetnames = my_dat['Drug'].unique().tolist()\n# out = tree.export_graphviz(drugtree,feature_names=featurenames,out_file=dot_data,class_name=np.unique(y_train),filled=True,special_characters=True,rotate=False)\n# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n# graph.write_png(filename)\n# plt.figure(figsize=(100,200))\n# plt.imshow(img,interpolation='nearest')\n\nfeat_names = drug.columns[:5]\ntarg_names = drug['Drug'].unique().tolist()\n\ndata = export_graphviz(drugtree,out_file=None,feature_names=feat_names,class_names=targ_names,filled=True,rounded=True,special_characters=True)\ngraph = graphviz.Source(data)\ngraph","b4734c80":"import pandas as pd\nimport pylab as pl\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\n%matplotlib inline\nimport matplotlib.pyplot as plt","27a58edb":"# get your data\nchurn = pd.read_csv('..\/input\/churndata\/ChurnData.csv')\nchurn","78e1f2e0":"churn.columns","cad8c504":"# step 1: preprocessing and selection\nchurn.dtypes","be52557c":"churn = churn[['tenure','age','address','income','ed','employ','equip','callcard','wireless','churn']].copy()\nchurn['churn'] = churn['churn'].copy().astype('int')\nchurn","ef0b5050":"churn.shape","8ec19c3f":"X = np.asarray(churn[['tenure','age','address','income','ed','employ','equip']])\nX[:5]","6dee1f3e":"Y = np.asarray(churn['churn'])\nY[:5]","10266f75":"# normalize the dataset\n\nfrom sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)\nX[:5]","223894ce":"# step 2: train\/set dataset\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=4)\nprint('train set: ' ,x_train.shape, y_train.shape)\nprint('test set: ',x_test.shape, y_test.shape)","4f5a688f":"# step 3: modeling(logistic regression with scikit_learn)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nlr = LogisticRegression(C=0.01,solver='liblinear').fit(x_train,y_train)\nlr","164bca7b":"yhat = lr.predict(x_test)\nyhat","1872ceb2":"# this method returns the probability of class 0 in first column, and class 1 in second column\nyhat_prob = lr.predict_proba(x_test)\nyhat_prob","edc590d2":"# step 4: evaluation \n\n# jacard index\n\nfrom sklearn.metrics import jaccard_score\njaccard_score(y_test,yhat)","83801b49":"# confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm,classes,normalize=False,title='Confution matrix',cmap=plt.cm.Blues):\n    '''\n    this function prints and plots the confusion matrix\n    normalization can be applied by setting 'normalize=True'\n    '''\n    if normalize:\n        cm = cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n        print('normalized confussion matrix')\n    else:\n        print('confusion matrix, without normalization')\n    \n    print(cm)\n    \n    plt.imshow(cm,interpolation='nearest',cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks,classes,rotation=45)\n    plt.yticks(tick_marks,classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max()\/2.\n    for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,format(cm[i,j],fmt),horizontalalignment='center',color='white' if cm[i,j]>thresh else 'black')\n    plt.tight_layout()\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')\nprint(confusion_matrix(y_test,yhat,labels=[1,0]))","6b1e083a":"# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test,yhat,labels=[1,0])\nnp.set_printoptions(precision=2)\n# plot non_normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['churn=1','churn=0'])","23a04d44":"# alternative way to plot\nimport seaborn as sns\nsns.heatmap(cnf_matrix,annot=True,annot_kws={'size':16})\nplt.show()\n\n# the ticks seems wrong tho","266dadf6":"print(classification_report(y_test,yhat))\n# precision is a measure of the accuracy provided that a class label has been predicted.TP\/(TP+FP)\n# recall is true positive rate. TP\/(TP+FN)","dd115adc":"#log loss\nfrom sklearn.metrics import log_loss\nlog_loss(y_test,yhat_prob)","e5938169":"import pandas as pd\nimport pylab as pl\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nimport matplotlib.pyplot as plt","ab0ff057":"# step 1: get your data\ncell = pd.read_csv('..\/input\/cellsamples\/cell_samples.csv')\ncell\n# this dataset consists of several hundred human cell sample records,\n# each of which contains the values of a set of cell characteritics\n# the class field contains the diagosis, as to weather the samples are benigh(2),or malignant(4)","bf2c84c4":"ax = cell[cell['Class']==4][:50].plot(kind='scatter',x='Clump',y='UnifSize',color='DarkBlue',label='malignant');\ncell[cell['Class']==2][:50].plot(kind='scatter',x='Clump',y='UnifSize',color='Yellow',label='benigh',ax=ax);","aeb5f675":"# step 2: data preprocessing and selection\ncell.dtypes","489aa14c":"# drop rows where BareNuc is not numerical\ncell = cell[pd.to_numeric(cell['BareNuc'],errors='coerce').notnull()]\ncell['BareNuc']=cell['BareNuc'].copy().astype('int')\ncell.dtypes","964b2480":"cell.columns","7ba5b2b0":"feature_cell = cell[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize','BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\nx = np.asarray(feature_cell)\nx[:5]","09856c58":"y = cell['Class'].values\ny[:5]","53087683":"# step 3: train\/test dataset\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=4)\nprint('train set: ',x_train.shape,y_train.shape)\nprint('test set: ',x_test.shape,y_test.shape)","89522e16":"# step 4: modeling\n\n# the svm algorithm offers a choice of kernel functions for performing its processing\n# mapping data into a higher dimensional space is called kernelling\n# the mathematicla function used fro the tranformation is know as the kernel function,and can be of different types:\n# 1. linear, 2. polynormial, 3. radial bais function(RDF), 4. sigmoid\n\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(x_train,y_train)","ebe4335f":"yhat = clf.predict(x_test)\nyhat[:5]","58bc0f67":"# step 5: evaluation\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\n# compute confusion matrix\ncnf_matraix = confusion_matrix(y_test,yhat,labels=[2,4])\nnp.set_printoptions(precision=2)\n\nprint(classification_report(y_test,yhat))\n\n# plot non_normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['benign(2)','malignant(4)'])","46987e64":"# f1 score\nfrom sklearn.metrics import f1_score\nf1_score(y_test,yhat,average='weighted')","e669cca9":"# jaccard index\n# the pos_label tells 2 is the positive label\n\nfrom sklearn.metrics import jaccard_score\njaccard_score(y_test,yhat,pos_label=2)","289e6cfd":"import random\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n%matplotlib inline","a5faf08f":"# k-means on a randomly generated dataset\n\n# step 1: make your data\nnp.random.seed(0)\nx,y = make_blobs(n_samples=5000,centers=[[4,4],[-2,-1],[2,-3],[1,1]],cluster_std=0.9)","2154361b":"x","79777a26":"y","1246c656":"plt.scatter(x[:,0],x[:,1],marker='.')\nplt.show()","49d4732d":"# step 2: setting up k-means model\n\nk_means = KMeans(init = 'k-means++',n_clusters = 4,n_init = 12)\nk_means.fit(x)\nk_means_labels = k_means.labels_ # predict\nk_means_labels","1f62e44e":"k_means_cluster_centers = k_means.cluster_centers_\nk_means_cluster_centers","a0bf4835":"# step 3: creating the visual plot\n\n# initialize the plot with the specified dimensions\nfig = plt.figure(figsize=(6,4))\n# colors uses a color map, which will produce an array of colors base on \n# the numbers of labels there are. we use set(k_means_labels) to get the unique labels.\ncolors = plt.cm.Spectral(np.linspace(0,1,len(set(k_means_labels))))\n# create a plot\nax = fig.add_subplot(1,1,1)\n# for loop that plots the data points and centroids\n# k will range fron0-3,which will match the possible clusters that each datapoint is in\nfor k,col in zip(range(len([[4,4],[-2,-1],[2,-3],[1,1]])),colors):\n    # create a list of all data points,where the data points that are in the cluster(ex. cluster 0 )are labeled as true,else they are labeled as false\n    my_members = (k_means_labels ==k)\n    # define the centroid, or the cluster center\n    cluster_center = k_means_cluster_centers[k]\n    #plots the datapoints with color col.\n    ax.plot(x[my_members,0],x[my_members,1],'w',markerfacecolor=col,marker='.')\n    #plots the centroids with specified color, but with a darker outline\n    ax.plot(cluster_center[0],cluster_center[1],'o',markerfacecolor=col,markeredgecolor='k',markersize=6)\nax.set_title('kmeans')\n#remove x-axis ticks\nax.set_xticks(())\nax.set_yticks(())\nplt.show()","e7e58bff":"# make it 3 clusters\n\nkmeans3 = KMeans(init='k-means++',n_clusters=3,n_init=12)\nkmeans3.fit(x)\nfig = plt.figure(figsize=(6,4))\ncolors = plt.cm.Spectral(np.linspace(0,1,len(set(kmeans3.labels_))))\nax = fig.add_subplot(1,1,1)\nfor k,col in zip(range(len(kmeans3.cluster_centers_)),colors):\n    my_members = (kmeans3.labels_==k)\n    cluster_center = kmeans3.cluster_centers_[k]\n    ax.plot(x[my_members,0],x[my_members,1],'w',markerfacecolor=col,marker='.')\n    ax.plot(cluster_center[0],cluster_center[1],'o',markerfacecolor=col,markeredgecolor='k',markersize=6)\nplt.show()","03b88fa0":"# k-means on customer segmentation\n\n# step 1: get your data\ncust_seg = pd.read_csv('..\/input\/customerseg\/Cust_Segmentation.csv')\ncust_seg","b37d4ebe":"# step 2: preprocessing\n# kmeans aldorithm isn't directly applicable to categorical variables\nseg = cust_seg.drop('Address',axis=1)\nseg.head()","91b055cb":"# normalizing over the standard deviation\nfrom sklearn.preprocessing import StandardScaler\nx = seg.values[:,1:]\nx = np.nan_to_num(x) # Replace NaN with zero\nclus_dataset = StandardScaler().fit_transform(x)\nclus_dataset","3f809bf1":"# step 3: modeling\n\nclusternum = 3\nkmeans = KMeans(init='k-means++',n_clusters = clusternum,n_init = 12)\nkmeans.fit(x)\nlabels = kmeans.labels_\nprint(labels)","cb1b224c":"# step 4: insight\n\nseg['clust_km'] = labels\nseg.head()","8df46f89":"seg.groupby('clust_km').mean()","8d7e4bb4":"x","86367715":"area = np.pi*(x[:,1])**2\nplt.scatter(x[:,0],x[:,3],s= area,c = labels.astype(np.float),alpha=0.5)\nplt.xlabel('age',fontsize=18)\nplt.ylabel('income',fontsize=16)\nplt.show()","15dbb8ec":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(1,figsize=(8,6))\nplt.clf()\nax= Axes3D(fig,rect=[0,0,0.95,1],elev=48,azim=134)\n\nplt.cla()\nax.set_xlabel('education')\nax.set_ylabel('age')\nax.set_zlabel('income')\n\nax.scatter(x[:,1],x[:,0],x[:,3],c=labels.astype(np.float))","f6e02fe9":"import numpy  as np\nimport pandas as pd\nfrom scipy import ndimage\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial import distance_matrix\nfrom matplotlib import pyplot as plt\nfrom sklearn import manifold,datasets\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\n%matplotlib inline","94ea302d":"# step 1: make your data\nx1,y1 = make_blobs(n_samples=50,centers=[[4,4],[-2,-1],[1,1],[10,4]],cluster_std=0.9)\ny1","b7aa4299":"x1","0141f574":"plt.scatter(x1[:,0],x1[:,1],marker='o')","c8dd3845":"# step 2: agglomerative clustering\n\nagglom = AgglomerativeClustering(n_clusters=4,linkage='average')\nagglom.fit(x1,y1)","5df725ab":"plt.figure(figsize=(6,4))\n#scale data point down,or else the data points will be scattered very far apart\nx_min,x_max = np.min(x1,axis=0),np.max(x1,axis=0)\nprint(x_min,x_max)","95f75d63":"# get the average distance for x1\nx1 =(x1-x_min)\/(x_max-x_min)\n\nfor i in range(x1.shape[0]):\n    plt.text(x1[i,0],x1[i,1],str(y1[i]),\n            color=plt.cm.nipy_spectral(agglom.labels_[i]\/10.),\n            fontdict={'weight':'bold','size':9})\nplt.scatter(x1[:,0],x1[:,1],marker='.')\nplt.show()","17dbc84b":"# dendrogram associated for the agglomerative hierarchical clustering\ndist_matrix = distance_matrix(x1,x1)\nprint(dist_matrix)","800bef20":"a = hierarchy.linkage(dist_matrix,'complete')","c8c0e3c5":"dendro = hierarchy.dendrogram(a)","3513b366":"# another case : vehicle\n\n# step 1 : get your data\ncars = pd.read_csv('..\/input\/carsclust\/cars_clus.csv')\ncars","4ab70daa":"# step 2: data preprocessing\n\ncars.size","b3444ede":"cars.dtypes","1ad48a5e":"cars.columns","7bed2e27":"cars[['sales', 'resale', 'type', 'price', 'engine_s', 'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt',\n      'fuel_cap', 'mpg', 'lnsales']]=cars[['sales', 'resale', 'type', 'price', 'engine_s', \n      'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg', \n      'lnsales' ]].apply(pd.to_numeric,errors ='coerce')\ncars = cars.dropna()\ncars = cars.reset_index(drop=True)\ncars","eaf79e43":"featureset = cars[['engine_s','horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]","0caa50fb":"from sklearn.preprocessing import MinMaxScaler\nx = featureset.values\nmin_max_scaler = MinMaxScaler()\nfeature_mtx = min_max_scaler.fit_transform(x)\nfeature_mtx[:5]","e0bfc44f":"# step 3\uff1a clustering using scipy\n\n# calculate distance\nimport scipy \nleng = feature_mtx.shape[0]\nD = np.zeros([leng,leng])\nfor i in range(leng):\n    for j in range(leng):\n        D[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i],feature_mtx[j])","916f8647":"import pylab \nimport scipy.cluster.hierarchy\nz = hierarchy.linkage(D,'complete')","ee1d7527":"from scipy.cluster.hierarchy import fcluster\nmax_d = 3\nclusters =fcluster(z,max_d,criterion='distance')\nclusters","a60d6e8c":"from scipy.cluster.hierarchy import fcluster\nclusters =fcluster(z,5,criterion='maxclust')\nclusters","e2eed42d":"fig = pylab.figure(figsize=(18,50))\ndef llf(id):\n   return '[%s %s %s]' % (cars['manufact'][id], cars['model'][id], int(float(cars['type'][id])) )\ndendro = hierarchy.dendrogram(z,leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')\n","cdc2bfa5":"# step 3.1\uff1a clustering using scikit-learn\ndist_matrix = distance_matrix(feature_mtx,feature_mtx)\nprint(dist_matrix)","5742463e":"agglom = AgglomerativeClustering(n_clusters=6,linkage= 'complete')\nagglom.fit(feature_mtx)\nagglom.labels_","45c625d4":"cars['cluster_'] = agglom.labels_\ncars.head()","f9b03e8e":"import matplotlib.cm as cm\nn_clusters = max(agglom.labels_)+1\ncolors = cm.rainbow(np.linspace(0,1,n_clusters))\ncluster_labels = list(range(0,n_clusters))\n\n# create a figure of size 6 inches by 4 inches\nplt.figure(figsize=(16,14))\n\nfor color,label in zip(colors,cluster_labels):\n    subset = cars[cars.cluster_==label]\n    for i in subset.index:\n        plt.text(subset.horsepow[i],subset.mpg[i],str(subset['model'][i]),rotation=25)\n    plt.scatter(subset.horsepow,subset.mpg,s=subset.price*10,c=color,label='cluster'+str(label),alpha=0.5)\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')\n    ","e67f4bee":"cars.groupby(['cluster_','type'])['cluster_'].count()","47ca5230":"agg_cars = cars.groupby(['cluster_','type'])['horsepow','engine_s','mpg','price'].mean()\nagg_cars","7130afcb":"plt.figure(figsize=(16,10))\nfor color,label in zip(colors,cluster_labels):\n    subset = agg_cars.loc[(label,),]\n    for i in subset.index:\n        plt.text(subset.loc[i][0]+5,subset.loc[i][2],'type='+str(int(i))+' price='+str(int(subset.loc[i][3]))+'k')\n    plt.scatter(subset.horsepow,subset.mpg,s=subset.price*20,c=color,label='cluster'+str(label))\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')","0066d4e7":"import numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline","405793be":"# step 1: data generation\ndef createdatapoints(centroidlocation,numsamples,clusterdeviation):\n    # create random data and store in feature matrix x and response vector y\n    x,y = make_blobs(n_samples=numsamples,centers=centroidlocation,cluster_std=clusterdeviation)\n    # standardize features by removing the mean and scaling to unit variance\n    x = StandardScaler().fit_transform(x)\n    return x,y","4199a3f2":"x,y = createdatapoints([[4,3], [2,-1], [-1,4]],1500,0.5)","1a71fd34":"# step 2 : modeling\nepsilon = 0.3\nminimumsamples = 7\ndb = DBSCAN(eps = epsilon,min_samples= minimumsamples).fit(x)\nlabels = db.labels_\nlabels","68851804":"labels[:100]","18c4cb0b":"# step 3: distinguish outliers\n\n# first, create an array of booleans using labels from db\ncore_samples_mask = np.zeros_like(db.labels_,dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\ncore_samples_mask","b690f59f":"# number of clusters in labels, ignoring noise if present\nn_clusters_ = len(set(labels))-(1 if -1 in labels else 0)\nn_clusters_ ","feac1267":"# remove repetition in labels by returning it into a set\nunique_labels = set(labels)\nunique_labels","41b68f5e":"# step 4 : data visualization\n\n# creat colors for the clusters\ncolors = plt.cm.Spectral(np.linspace(0,1,len(unique_labels)))\ncolors","06b474e9":"# plot the points with colors\nfor k,col in zip(unique_labels,colors):\n    if k ==-1:\n        # black used for noise\n        col = 'k'\n        \n    class_member_mask = (labels==k)\n        \n    # plot the datapoints that are clustered\n    xy = x[class_member_mask & core_samples_mask]\n    plt.scatter(xy[:,0],xy[:,1],s=50,c=col,marker=u'o',alpha = 0.5)\n        \n    # plot the outliers\n    xy = x[class_member_mask & ~core_samples_mask]\n    plt.scatter(xy[:,0],xy[:,1],s=50,c=col,marker=u'o',alpha = 0.5)\n        ","bf383c5d":"# step 0: try to cluster using k-means\n\nfrom sklearn.cluster import KMeans\nk=3\nk_means3 = KMeans(init='k-means++',n_clusters=k,n_init=12)\nk_means3.fit(x)\nfig = plt.figure(figsize=(6,4))\nax = fig.add_subplot(1,1,1)\nfor k,col in zip(range(k),colors):\n    my_members = (k_means3.labels_==k)\n    plt.scatter(x[my_members,0],x[my_members,1],c=col,marker = u'o',alpha=0.5)\nplt.show()","9850e10d":"# step 1: loading data\nimport csv\nimport pandas as pd \nimport numpy as np\n\nweathers = pd.read_csv('..\/input\/weather\/weather-stations20140101-20141231.csv')\nweathers.head()","e690ff04":"# step 2: cleaning data\nweathers = weathers[pd.notnull(weathers['Tm'])]\nweathers = weathers.reset_index(drop=True)\nweathers.head()","305d882c":"# step 3: visualization\nimport mpl_toolkits\nfrom mpl_toolkits.basemap import Basemap\n\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize']=(14,10)\n\nllon = -140\nulon = -50\nllat = 40\nulat = 65\n\nweathers = weathers[(weathers['Long']>llon)& (weathers['Long']<ulon)& (weathers['Lat']>llat)&(weathers['Lat']<ulat)]\n\nmy_map = Basemap(projection ='merc',\n                resolution = 'l',area_thresh = 1000,\n                llcrnrlon=llon ,llcrnrlat=llat,#min longitude(llcrnrlon) and latitude (llacrnrlat)\n                urcrnrlon=ulon , urcrnrlat=ulat)#mas longitude(urcrnrlon) and latitude (urcrnrlat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\nmy_map.fillcontinents(color='white',alpha=0.3)\nmy_map.shadedrelief()\n\n# to collect data based on stations\nxs,ys = my_map(np.asarray(weathers.Long),np.asarray(weathers.Lat))\nweathers['xm']= xs.tolist()\nweathers['ym']= ys.tolist()\n\n# visualization\nfor index,row in weathers.iterrows():\n    my_map.plot(row.xm,row.ym,markerfacecolor=([1,0,0]),marker='o',markersize=5,alpha=0.75)\nplt.show()\n","bd36ea18":"# step 4 : clustering of stations based on their location Lat&Lon\n\nfrom sklearn.cluster import DBSCAN\nimport sklearn.utils\nfrom sklearn.preprocessing import StandardScaler\n\nsklearn.utils.check_random_state(1000)\nclus_dataset = weathers[['xm','ym']]\nclus_dataset = np.nan_to_num(clus_dataset)\nclus_dataset = StandardScaler().fit_transform(clus_dataset)\n\n# compute DBSCAN\ndb = DBSCAN(eps=0.15,min_samples =10).fit(clus_dataset)\ncore_sample_mask = np.zeros_like(db.labels_,dtype=bool)\ncore_sample_mask[db.core_sample_indices_]= True\nlabels = db.labels_\nweathers['Clus_Db'] = labels\n\nrealclusternum=len(set(labels))-(1 if -1 in labels else 0)\nclusternum = len(set(labels))\n\n# a sample of clusters\nweathers[['Stn_Name','Tx','Tm','Clus_Db']].head()","d358ee73":"# step 5 : visualization of clusters based on location \nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt \nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize']=(14,10)\n\nmy_map = Basemap(projection='merc',resolution='l',area_thresh=1000,\n                llcrnrlon=llon,llcrnrlat=llat,\n                urcrnrlon=ulon,urcrnrlat=ulat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\nmy_map.fillcontinents(color='white',alpha = 0.3)\nmy_map.shadedrelief()\n\ncolors = plt.get_cmap('jet')(np.linspace(0,10,clusternum))\n\nfor clus_number in set(labels):\n    c=(([0.4,0.4,0.1]) if clus_number==-1 else colors[np.int(clus_number)])\n    clus_set = weathers[weathers.Clus_Db ==clus_number]\n    my_map.scatter(clus_set.xm,clus_set.ym,color = c , marker='o',s =20,alpha =0.85)\n    if clus_number !=-1:\n        cenx = np.mean(clus_set.xm)\n        ceny = np.mean(clus_set.ym)\n        plt.text(cenx,ceny,str(clus_number),fontsize=25,color = 'red',)\n        print('cluster'+str(clus_number)+', avg temp:'+str(np.mean(clus_set.Tm)))\n        ","4529b655":"# step 6: clustering of stations based on their location ,mean, max and min temperature\n\nfrom sklearn.cluster import DBSCAN\nimport sklearn.utils\nfrom sklearn.preprocessing import StandardScaler\nsklearn.utils.check_random_state(1000)\n\nclus_dataset = weathers[['xm','ym','Tx','Tm','Tn']]\nclus_dataset = np.nan_to_num(clus_dataset)\nclus_dataset = StandardScaler().fit_transform(clus_dataset)\n\ndb = DBSCAN(eps= 0.3, min_samples = 10).fit(clus_dataset)\ncore_samples_mask = np.zeros_like(db.labels_,dtype = bool)\ncore_samples_mask[db.core_sample_indices_]= True\nlabels = db.labels_\nweathers['clus_db'] = labels\n\nrealclusternum = len(set(labels))-(1 if -1 in labels else 0)\nclusternum = len(set(labels))\n\nweathers[['Stn_Name','Tx','Tm','clus_db']].head()","efa7a647":"# step 7: visualization of clusters based on lacation and temperature\n\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = (14,10)\n\nmy_map = Basemap(projection  = 'merc',\n                resolution = 'l',area_thresh = 1000,\n                llcrnrlon = llon ,llcrnrlat = llat,\n                urcrnrlon = ulon, urcrnrlat = ulat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\nmy_map.fillcontinents(color = 'white',alpha = 0.3)\nmy_map.shadedrelief()\n\ncolors = plt.get_cmap('jet')(np.linspace(0,1,clusternum))\n\nfor clust_number in set(labels):\n    c = (([0.4,0.4,0.4]) if clust_number==-1 else colors[np.int(clust_number)])\n    clust_set  = weathers[weathers.clus_db==clust_number]\n    my_map.scatter(clust_set.xm,clust_set.ym,color=c,marker = 'o',s=20,alpha = 0.85)\n    if clust_number !=-1:\n        cenx = np.mean(clust_set.xm)\n        ceny = np.mean(clust_set.ym)\n        plt.text(cenx,ceny,str(clust_number),fontsize = 25,color = 'red')\n        print('cluster'+ str(clust_number)+', avg temp: '+str(np.mean(clust_set.Tm)))\n        ","82fbd79a":"# step 1: get your data \n\nimport pandas as pd # daraframe manipulation library\nfrom math import sqrt # math functions\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nmovies = pd.read_csv('..\/input\/movies\/ml-latest\/movies.csv')\nratings = pd.read_csv('..\/input\/movies\/ml-latest\/ratings.csv')\nmovies","15b337aa":"# step 2: cleaning data \n\n# using regular expression to find a year stored between parentheses\n# specify the parentheses so we don't conflict with movies that have years in their titles\nmovies['year'] = movies.title.str.extract('(\\(\\d\\d\\d\\d\\))',expand=False)\n# removing the parentheses\nmovies['year'] = movies.year.str.extract('(\\d\\d\\d\\d)',expand=False)\n# removing the years from the title\nmovies['title'] = movies.title.str.replace('(\\(\\d\\d\\d\\d\\))','')\n# applying the strip function to get rid of any ending whitespace characters\nmovies['title']= movies['title'].apply(lambda x: x.strip())\nmovies","293cdec4":"# we use genre similarity to recommend. that is to use the genre preference of user to recommend\n# first, calculate weight of each genre according to user ratings. \n# second, calculate sum weight of every movie . \n# third, recommend the highest 20 movies\n\n# every genre is separated by |\nmovies['genres'] = movies.genres.str.split('|')\nmovies","e7de31ab":"# copying the movie dataframe into a new one since we won't need to use the genre information \nmoviewithgenres = movies.copy()\n\n# for every row in the dataframe ,iterate through the list of genres and place a into the corresponding column\nfor index, row in movies.iterrows():\n    for genre in row['genres']:\n        moviewithgenres.at[index,genre]=1\n# filling in the nan values with 0 to show that a movie doesn't have that column's genre\nmoviewithgenres = moviewithgenres.fillna(0)\nmoviewithgenres","8e78b5c4":"ratings","49a7a313":"# drop removes a specified row or colomn from a dataframe\nratings = ratings.drop('timestamp',1)\nratings","43a904a1":"# step 2: building models of content based recommendation system\n\n# creating an input user to recommend movies to \nuserinput = [\n    {'title':'Breakfast Club, The','rating':5},\n    {'title':'Toy Story','rating':3.5},\n    {'title':'Jumanji','rating':2},\n    {'title':'Pulp Fiction','rating':5},\n    {'title':'Akira','rating':4.5}\n]\ninputmovies = pd.DataFrame(userinput)\ninputmovies","17640a94":"# add movieid to input user\n# filtering our the movied by title \ninputid  = movies[movies['title'].isin(inputmovies['title'].tolist())]\ninputid = inputid[['movieId','title']]\n# then merging it so we get the movieid \ninputmovies = pd.merge(inputmovies,inputid)\n# dropping information we won't use from the input dataframe\n# inputmovies = inputmovies.drop('genres',1).drop('year',1)\ninputmovies","e0f32adb":"# filtering out the movies from the input\nusermovies = moviewithgenres[moviewithgenres['movieId'].isin(inputmovies['movieId'])]\nusermovies","0b740c83":"# resetting the index to avoid future issues\nusermovies = usermovies.reset_index(drop = True)\n# dropping unecessary issues due to save memory and to avoid issues \nusergenretable = usermovies.drop('movieId',1).drop('title',1).drop('genres',1).drop('year',1)\nusergenretable","80848894":"inputmovies['rating']","015aa547":"# dot product to get weights\nuserprofile = usergenretable.transpose().dot(inputmovies['rating'])\nuserprofile","10ffc243":"# get the genres of every movie in the original dataframe \ngenretable = moviewithgenres.set_index(moviewithgenres['movieId'])\n# drop the unecessary information \ngenretable = genretable.drop('movieId',1).drop('title',1).drop('genres',1).drop('year',1)\ngenretable","2c500947":"genretable.shape","570336aa":"# multiply the genres by the weights and then take the weighted average \nrecommendationtable = ((genretable*userprofile).sum(axis=1))\/(userprofile.sum())\nrecommendationtable ","26f894ff":"# sort recommendations in descending order\nrecommendationtable = recommendationtable.sort_values(ascending=False)\nrecommendationtable","66a741ee":"# the final recommendation table \nmovies.loc[movies['movieId'].isin(recommendationtable.head(20).keys())]\n","c86f1318":"# step 1: get your data\nmovies = pd.read_csv('..\/input\/movies\/ml-latest\/movies.csv')\nratings = pd.read_csv('..\/input\/movies\/ml-latest\/ratings.csv')\nmovies","3bbb4498":"# using regular expression to find a year stored between parentheses\n# specify the parentheses so we don't conflict with movies that have years in their titles\nmovies['year'] = movies.title.str.extract('(\\(\\d\\d\\d\\d\\))',expand=False)\n# removing the parentheses\nmovies['year'] = movies.year.str.extract('(\\d\\d\\d\\d)',expand=False)\n# removing the years from the title\nmovies['title'] = movies.title.str.replace('(\\(\\d\\d\\d\\d\\))','')\n# applying the strip function to get rid of any ending whitespace characters\nmovies['title']= movies['title'].apply(lambda x: x.strip())\nmovies","b200ba7e":"# step 2 : collaborative filtering \n# also known as user-user filtering. to find users that have similar preferences,reco that that user liked. \n# to find similar user, we use method based on pearson correlation function\n\n# select a user with the movies the user has watched\n# based on his rating to movies,find the top x neighbours\n# get the watched movie record of the user for each neighbour\n# calculate a similarity score using some formula\n# recommend the items with the highest score\n","5dfa7628":"\n# creating an input user to recommend movies to \nuserinput = [\n    {'title':'Breakfast Club, The','rating':5},\n    {'title':'Toy Story','rating':3.5},\n    {'title':'Jumanji','rating':2},\n    {'title':'Pulp Fiction','rating':5},\n    {'title':'Akira','rating':4.5}\n]\ninputmovies = pd.DataFrame(userinput)\ninputmovies","709626c3":"# add movieid to input user\n# filtering our the movied by title \ninputid  = movies[movies['title'].isin(inputmovies['title'].tolist())]\ninputid = inputid[['movieId','title']]\n# then merging it so we get the movieid \ninputmovies = pd.merge(inputmovies,inputid)\n# dropping information we won't use from the input dataframe\n# inputmovies = inputmovies.drop('genres',1).drop('year',1)\ninputmovies","398ddb37":"# the user who has seen the same movies\n\n# filtering out userd that have watched movies that the input has watched and sroing it\nusersubset = ratings[ratings['movieId'].isin(inputmovies['movieId'].tolist())]\nusersubset","39fe9b16":"usersubsetgroup = usersubset.groupby(['userId'])\nusersubsetgroup.get_group(1130)","c4274adc":"# sorting it so users with movie most in common with the input will have priority\nusersubsetgroup = sorted(usersubsetgroup,key=lambda x: len(x[1]),reverse = True)\nusersubsetgroup[:3]","0018cc73":"# similarity of users to input user \nusersubsetgroup = usersubsetgroup[:100]","71d80a55":"# store the pearson correlation in a dictionary , where the key is the user id and the value is the coefficient\npearsonCorrelationDict = {}\n\n# for every user group in subset\nfor name,group in usersubsetgroup:\n    # sorting the input and current user group so the values aren't mixed up\n    group = group.sort_values(by = 'movieId')\n    inputmovies = inputmovies.sort_values(by = 'movieId')\n    # get the n for the formula\n    nratings = len(group)\n    # get the review scores for the movies that they both have in common\n    temp = inputmovies[inputmovies['movieId'].isin(group['movieId'].tolist())]\n    # score them in a temporary buffer variable in a list format to facilitate future using\n    tempratinglist  = temp['rating'].tolist()\n    # let's also put the current user group reviews in a list format\n    tempgrouplist = group['rating'].tolist()\n    # calc the pearson correlation between two users, so called x,y\n    sxx = sum([i**2 for i in tempratinglist])-pow(sum(tempratinglist),2)\/float(nratings)\n    syy = sum([i**2 for i in tempgrouplist])-pow(sum(tempgrouplist),2)\/float(nratings)\n    sxy = sum(i*j for i,j in zip(tempratinglist,tempgrouplist))-sum(tempratinglist)*sum(tempgrouplist)\/float(nratings)\n    \n    if sxy !=0 and syy !=0:\n        pearsonCorrelationDict[name] = sxy\/sqrt(sxx*syy)\n    else:\n        pearsonCorrelationDict[name] = 0\npearsonCorrelationDict.items()","33da799d":"pearsondf = pd.DataFrame.from_dict(pearsonCorrelationDict,orient= 'index')\npearsondf.columns = ['similarityindex']\npearsondf['userId'] = pearsondf.index\npearsondf.index = range(len(pearsondf))\npearsondf.head()","a63528ac":"# the top x similar users to input user\ntopuser = pearsondf.sort_values(by = 'similarityindex',ascending = False)[:50]\ntopuser","5f338b1b":"# rating of selected users to all movies\ntopuserrating = topuser.merge(ratings,left_on='userId',right_on='userId',how = 'inner')\ntopuserrating","bce7e8f3":"# multiplies the similarity by the user's ratings\ntopuserrating['weightedrating'] = topuserrating['similarityindex']*topuserrating['rating']\ntopuserrating","9ce31876":"# applies a sum to the topusers ofter grouping \ntemptopuserrating = topuserrating.groupby('movieId').sum()[['similarityindex','weightedrating']]\ntemptopuserrating.columns= ['sum_similarityindex','sum_weightedrating']\ntemptopuserrating\n","1595233a":"recommendationdf = pd.DataFrame()\nrecommendationdf['weighted average recommendaion score'] = temptopuserrating['sum_weightedrating']\/temptopuserrating['sum_similarityindex']\nrecommendationdf['movieId'] = temptopuserrating.index\nrecommendationdf","c31e989c":"recommendationdf = recommendationdf.sort_values(by = 'weighted average recommendaion score',ascending = False)\nrecommendationdf.head(10)","4adbe8d8":"movies.loc[movies['movieId'].isin(recommendationdf.head(10)['movieId'].tolist())]\n","64fff690":"# 10. Content-Based filtering \n\ncontent based recommendation systems ","2d12efc1":"# 8. Hierarchical Clustering \nlooking at bottom up approach agglomerative hierarchical clustering","c03e7ff1":"kmeans will partion customers into mutually exclusive groups fror example, into 3 clusters, the customers in each cluster are similar to each other demographically. we can create a profile for each group ,considering the common characterristics of each cluster:\n* affluent,education and old aged \n* middle aged and middle income\n* young and low income","d80cab13":"# 6. SVM(support vector machines)\nsvm works by mapping data to a high_dimensiional feature space so that data points can be categorized, even when the data are not otherwise linearly separable.a separator between the categories is found, then the data are tranformed in such a way that the separator could be drawn as a hyperplane. following this , characteristics of new data can be used to predict the group to which a new record should belong.","e7255cff":"# 2. Non Linear Regression Analysis","cd10e2e2":"# 11. Collaborative filtering\n\nrecommend items to userd based on information taken from the user.","5aa30cf5":"* **Disclaimer: This is a case from Coginitive Class: https:\/\/courses.cognitiveclass.ai\/courses\/course-v1:CognitiveClass+ML0101ENv3+2018\/course\/**\n\nThis notebook is for personal exercises only.","5c7c38ed":"# 1. Simple Linear Regression\nFuel consumption and carbon dioxide emission of cars.","3f9d2a24":"Advantages\n* learn user's preferences\n* highly personalized for the user \n\nDisadvantages\n* doesn't take into account what others think of the item,so low quality item recommendations might happen\n* extracting data is not always intuitive\n* determing what characteristics of the item the user dislikes or likes is not always obvious","57370be6":"# 3. K-Nearest Neighbors\nTry to build a model to classify telecommunication provider's customers","552cabf9":"advantages\n* take other user's ratings into consideration\n* doesn't need to study or extract information from the recommended item\n* adpats to the user'd interests which might change over time\n\ndisadvantaged\n* approcimation function can be slow \n* there might be a low of amount of users to approximatr\n* privacy issues when trying to learn the user's preference\n","cc80f72c":"* k-means on a randomly generated dataset\n* k-means on customer segmentation","33a2ad21":"# 9. Density-Based Clustering \n\nwork with tasks with arbitraty shape clusters, or clusters within cluster\n\nlocates regions of high density that are separated from one another by regions of low density.","858e15cc":"example with weather station, using DBSCAN and scikit-learn\n\n1. loading data\n2. overview data\n3. data cleaning\n4. data selection \n5. clustering \n\nabout the dataset\n\n<h4 align = \"center\">\nEnvironment Canada    \nMonthly Values for July - 2015\t\n<\/h4>\n<html>\n<head>\n<style>\ntable {\n    font-family: arial, sans-serif;\n    border-collapse: collapse;\n    width: 100%;\n}\n\ntd, th {\n    border: 1px solid #dddddd;\n    text-align: left;\n    padding: 8px;\n}\n\ntr:nth-child(even) {\n    background-color: #dddddd;\n}\n<\/style>\n<\/head>\n<body>\n\n<table>\n  <tr>\n    <th>Name in the table<\/th>\n    <th>Meaning<\/th>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>Stn_Name<\/font><\/td>\n    <td><font color = \"green\"><strong>Station Name<\/font<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>Lat<\/font><\/td>\n    <td><font color = \"green\"><strong>Latitude (North+, degrees)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>Long<\/font><\/td>\n    <td><font color = \"green\"><strong>Longitude (West - , degrees)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>Prov<\/td>\n    <td>Province<\/td>\n  <\/tr>\n  <tr>\n    <td>Tm<\/td>\n    <td>Mean Temperature (\u00b0C)<\/td>\n  <\/tr>\n  <tr>\n    <td>DwTm<\/td>\n    <td>Days without Valid Mean Temperature<\/td>\n  <\/tr>\n  <tr>\n    <td>D<\/td>\n    <td>Mean Temperature difference from Normal (1981-2010) (\u00b0C)<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"black\">Tx<\/font><\/td>\n    <td><font color = \"black\">Highest Monthly Maximum Temperature (\u00b0C)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>DwTx<\/td>\n    <td>Days without Valid Maximum Temperature<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"black\">Tn<\/font><\/td>\n    <td><font color = \"black\">Lowest Monthly Minimum Temperature (\u00b0C)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>DwTn<\/td>\n    <td>Days without Valid Minimum Temperature<\/td>\n  <\/tr>\n  <tr>\n    <td>S<\/td>\n    <td>Snowfall (cm)<\/td>\n  <\/tr>\n  <tr>\n    <td>DwS<\/td>\n    <td>Days without Valid Snowfall<\/td>\n  <\/tr>\n  <tr>\n    <td>S%N<\/td>\n    <td>Percent of Normal (1981-2010) Snowfall<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>P<\/font><\/td>\n    <td><font color = \"green\"><strong>Total Precipitation (mm)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>DwP<\/td>\n    <td>Days without Valid Precipitation<\/td>\n  <\/tr>\n  <tr>\n    <td>P%N<\/td>\n    <td>Percent of Normal (1981-2010) Precipitation<\/td>\n  <\/tr>\n  <tr>\n    <td>S_G<\/td>\n    <td>Snow on the ground at the end of the month (cm)<\/td>\n  <\/tr>\n  <tr>\n    <td>Pd<\/td>\n    <td>Number of days with Precipitation 1.0 mm or more<\/td>\n  <\/tr>\n  <tr>\n    <td>BS<\/td>\n    <td>Bright Sunshine (hours)<\/td>\n  <\/tr>\n  <tr>\n    <td>DwBS<\/td>\n    <td>Days without Valid Bright Sunshine<\/td>\n  <\/tr>\n  <tr>\n    <td>BS%<\/td>\n    <td>Percent of Normal (1981-2010) Bright Sunshine<\/td>\n  <\/tr>\n  <tr>\n    <td>HDD<\/td>\n    <td>Degree Days below 18 \u00b0C<\/td>\n  <\/tr>\n  <tr>\n    <td>CDD<\/td>\n    <td>Degree Days above 18 \u00b0C<\/td>\n  <\/tr>\n  <tr>\n    <td>Stn_No<\/td>\n    <td>Climate station identifier (first 3 digits indicate   drainage basin, last 4 characters are for sorting alphabetically).<\/td>\n  <\/tr>\n  <tr>\n    <td>NA<\/td>\n    <td>Not Available<\/td>\n  <\/tr>\n\n\n<\/table>\n\n<\/body>\n<\/html>\n\n \n","8864ca0f":"# 5. Logistic Regression\npredict if a customer is leaving for a competitor,so that we can take some action to retain the customers\n\nlogistic regression is a variation od linear regression, useful when the observed dependent variable, y, is categorical. it is a comma 's' shape(sigmoid curve).","4bdda310":"# 7. K-Means Clustering","629a1815":"|Field name|Description|\n|--- |--- |\n|ID|Clump thickness|\n|Clump|Clump thickness|\n|UnifSize|Uniformity of cell size|\n|UnifShape|Uniformity of cell shape|\n|MargAdh|Marginal adhesion|\n|SingEpiSize|Single epithelial cell size|\n|BareNuc|Bare nuclei|\n|BlandChrom|Bland chromatin|\n|NormNucl|Normal nucleoli|\n|Mit|Mitoses|\n|Class|Benign or malignant|","67305208":"# 4. Dicision Trees\nPredict the class of an unknow patient , or to find a proper drug for a new patient."}}