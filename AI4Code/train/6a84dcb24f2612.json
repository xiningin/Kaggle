{"cell_type":{"b39f3750":"code","6ee6f100":"code","da19b50b":"code","14f80f19":"code","398e70a2":"code","4c2372f8":"code","ae6561b7":"code","69a6775e":"code","db0504ec":"code","36fd54ab":"code","44d23606":"code","d72006a2":"code","2122d214":"code","7865cd1f":"code","ce0ecc3e":"code","509c45af":"code","f8cda9ee":"code","f14b7a36":"code","3f9583a9":"code","3f428e14":"code","74e7bd44":"code","91530441":"code","7e54b6ad":"code","856d7074":"code","b758c95b":"markdown"},"source":{"b39f3750":"import numpy as np\nimport pandas as pd\nimport catboost as catb\nfrom sklearn.ensemble import RandomForestRegressor\nimport shutil\nimport os","6ee6f100":"data_train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ndata_test  = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nsubmission = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ny = data_train['target']\nerror = data_train['standard_error']","da19b50b":"data_train","14f80f19":"data_test","398e70a2":"data_train = data_train.drop('target', axis=1)\ndata_train = data_train.drop('standard_error', axis=1)","4c2372f8":"data_train","ae6561b7":"data_test","69a6775e":"all_data = pd.DataFrame(np.vstack((data_train, data_test)))","db0504ec":"all_data","36fd54ab":"import nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nstopword = stopwords.words('english')\nwn = nltk.WordNetLemmatizer()\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom xgboost import XGBRegressor","44d23606":"import string\nstring.punctuation","d72006a2":"def remove_punct(text):\n    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n    return text_nopunct\n\ndef tokenize_(text):\n    tokens = re.split('\\W+', text)\n    return tokens\n\ndef remove_stopwords(tok_list):\n    text = [word for word in tok_list if word not in stopword]\n    return text\n\ndef lemmatizing(tok_text):\n    text = [wn.lemmatize(word) for word in tok_text]\n    return text\n\ndef joini(text):\n    words = [\" \".join(text) for word in text]\n    return words","2122d214":"%%time\nall_data[3]  = pd.DataFrame(all_data[3].apply(lambda x: remove_punct(x)))\nall_data[3]  = all_data[3].apply(lambda x: tokenize_(x.lower()))\nall_data[3]  = all_data[3].apply(lambda x: remove_stopwords(x))\nall_data[3]  = all_data[3].apply(lambda x: lemmatizing(x)) \n#all_data[3]  = all_data[3].apply(lambda x: joini(x)) ","7865cd1f":"for i in range(len(all_data[3])):\n    all_data[3][i] = joini(all_data[3][i])","ce0ecc3e":"for i in range(len(all_data[3])):\n    all_data[3][i] = all_data[3][i][0]","509c45af":"all_data[3][2]","f8cda9ee":"Tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\nall_data_vect = pd.DataFrame(Tfidf_vectorizer.fit_transform(all_data[3]).toarray())\nall_data_vect","f14b7a36":"X = all_data_vect[:2834]\nX_test = all_data_vect[2834:2841]","3f9583a9":"%%time\ncatboost_est = catb.CatBoostRegressor(task_type=\"GPU\")\n#est = RandomForestRegressor()\n#catboost_est.fit(X, y)\ncatboost_est.fit(X, y)","3f428e14":"catboost_est.predict(X_test)\n#est.predict(X_test)","74e7bd44":"submission['target'] = catboost_est.predict(X_test)","91530441":"submission.to_csv(\"submission.csv\", index=False)","7e54b6ad":"submission","856d7074":"#shutil.rmtree(\"\/kaggle\/working\/catboost_info\")\nos.remove(\"kaggle\/working\/catboost_info\/time_left.tsv\")\nos.remove(\"kaggle\/working\/catboost_info\/learn_error.tsv\")\nos.remove(\"kaggle\/working\/catboost_info\/learn\/events.out.tfevents\")\nos.remove(\"kaggle\/working\/catboost_info\/catboost_training.json\")","b758c95b":"# Make some preparations"}}