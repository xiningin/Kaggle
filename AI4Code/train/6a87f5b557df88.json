{"cell_type":{"3b892b0e":"code","c662baa9":"code","9043a193":"code","9dd194e5":"code","858dd719":"code","1a5088f3":"code","df9697a8":"code","07977968":"code","3f82ae5f":"code","f70c3f09":"code","d4463a32":"code","001c7e28":"code","5a91ee48":"code","4275a892":"code","725ba856":"code","ab4b1f9e":"code","dd3738ed":"code","bf8cb4c4":"code","f628a75c":"code","59a9394e":"code","99fece6a":"code","df3044d7":"code","cf8f99c2":"code","5b50e062":"code","f86e1ee1":"code","74c931ed":"code","12c7731e":"code","36540570":"code","a4ae6a24":"code","f8bd1948":"code","ffe744cf":"code","2e11523a":"code","8a7c2b6b":"code","804d889c":"code","f0e40e0a":"code","13f747d8":"code","a1447df9":"code","aea307c0":"code","3bbd48b5":"code","56b1ec47":"code","90fa67c8":"code","1a04245e":"code","76eeb9b8":"code","6582f463":"code","7231b831":"code","6afcc15a":"code","57ebc797":"code","f88a7170":"code","2eac6256":"code","2d2c0fe8":"code","403d9191":"code","1457287e":"code","7ac3afba":"code","f79f9fe7":"code","70a4ffa6":"code","09f50941":"code","45be0ceb":"code","74364026":"code","6ccba480":"code","cd639afe":"code","18ca2cb3":"code","8429f319":"code","2255d1fe":"code","52893ee3":"code","4544d21a":"code","9f39149e":"code","2b957aed":"code","d4ef471f":"code","4ae7b54a":"code","c22825ea":"code","3172d374":"code","0cef0d05":"code","9b3af35e":"code","de58a076":"markdown","f30fc6f7":"markdown","e377995d":"markdown","b16080b3":"markdown","13423e36":"markdown","6ade9441":"markdown","d238a8b3":"markdown","20caecc0":"markdown","a0ad83b6":"markdown","60ae9b2b":"markdown","3c778b17":"markdown","b2f4a50a":"markdown","4b4a5e92":"markdown","bb1b45a3":"markdown","6baca302":"markdown","d1f97166":"markdown","af482aa7":"markdown","3d0adcf1":"markdown","b59af62c":"markdown","17d9562a":"markdown","e2189daa":"markdown","ed8af0fd":"markdown","a6f80f17":"markdown","b4694794":"markdown","f21a1910":"markdown","b614b3ce":"markdown","4ea54ee9":"markdown","2da569a5":"markdown","ddbfc841":"markdown","f65afd35":"markdown","8c6a0bfe":"markdown","d277c9c1":"markdown","f88dae3b":"markdown","4d611fbe":"markdown","490b94a0":"markdown","4016debe":"markdown","00b469e8":"markdown","e2cfc20b":"markdown","48422a67":"markdown","14e20023":"markdown","92a826db":"markdown","47a251a1":"markdown","e3fb0144":"markdown","4e294780":"markdown","fc76b6d3":"markdown","b0db0192":"markdown","34ad7a43":"markdown","326e8932":"markdown","2e9a679d":"markdown","16d31e42":"markdown","5cd55ef4":"markdown","2884b250":"markdown","c57d87b1":"markdown","8d7c9667":"markdown","1dc39b1c":"markdown","b50618e4":"markdown","fad586b6":"markdown","b4f5ea86":"markdown","f45de973":"markdown","7683b2d0":"markdown","20e05f3b":"markdown","e4a8ab66":"markdown","6a93d842":"markdown","83f115bc":"markdown","f1d6b817":"markdown","9e4feca7":"markdown","5f0c2f5a":"markdown","72b9a085":"markdown","cc978238":"markdown","cc9f6513":"markdown","f57b9a4c":"markdown","b68af3ca":"markdown","a0539fc7":"markdown","7004325c":"markdown","cb58dfe2":"markdown","ded35ad2":"markdown","f606ecc5":"markdown","717c2db3":"markdown","4dd42bfa":"markdown","f8d7ba76":"markdown","16770f86":"markdown","967ee9b1":"markdown","4e15441a":"markdown","e371964c":"markdown","e002c0b3":"markdown","7ca3d82d":"markdown","0d870125":"markdown"},"source":{"3b892b0e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set_palette('husl'); sns.set_style('ticks');\nimport pickle\nfrom tqdm import tqdm\n\n%matplotlib inline\n\n# display all figures to 2 decimal places\npd.options.display.float_format = '{:.2f}'.format\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c662baa9":"# start by loading the competition data into pandas dataframes\nos.chdir('..\/input\/competitive-data-science-predict-future-sales')\n\nsales_train    = pd.read_csv('sales_train.csv')\nitems           = pd.read_csv('items.csv')\nitem_categories = pd.read_csv('item_categories.csv')\nshops           = pd.read_csv('shops.csv')\nsample_submission = pd.read_csv('sample_submission.csv')\ntest = pd.read_csv('test.csv')\n         \nos.chdir('..\/..\/working')","9043a193":"sales_train.info()","9dd194e5":"sales_train.head(5)","858dd719":"sales_train.tail(5)","1a5088f3":"# splits the date feature into month and year\nannual_trends = sales_train[['date','item_cnt_day']].copy()\nannual_trends['month'] = annual_trends.date.apply(\n    lambda x: int(x.split('.')[1])\n)\nannual_trends['year'] = annual_trends.date.apply(\n    lambda x: int(x.split('.')[2])\n)\n\n# group the transactions by individual item per month and year\nannual_trends = annual_trends[['year','month','item_cnt_day']].\\\n                    groupby(['year','month'], as_index=False).sum()\n\n# visualise the sales patterns\nfig, ax = plt.subplots(figsize=(15,8))\nsns.lineplot(x='month', \n             y='item_cnt_day', \n             hue='year', \n             data=annual_trends, \n             ax=ax, \n             palette=['yellow','orange','red'], \n             linewidth=3)\nsns.despine()","df9697a8":"random_items = [21904, 20553, 15228, 18221,  3327,  7882, 18847,  6225, 16592, 7218]\n\n# ### uncomment the below to see a random selection of items ###\n# random_items = np.random.choice(sales_train.item_id.unique(), 10)\n\nitem_mask = (sales_train.item_id.isin(random_items))\nfeatures = ['date_block_num', 'item_id', 'item_cnt_day']\nitem_trends = sales_train[item_mask][features].copy()\n\nitem_trends = item_trends.groupby(['date_block_num', 'item_id'], \n                                  as_index=False).sum()\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nfor item in random_items:\n    mask = item_trends.item_id == item\n    plot_data = item_trends[mask]\n    sns.lineplot(x='date_block_num', \n                 y='item_cnt_day',\n                 label=item,\n                 data=plot_data,\n                 ax=ax,\n                 linewidth=2)\n\nsns.despine()","07977968":"sales_train.describe()","3f82ae5f":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,5))\nsns.violinplot(sales_train.item_cnt_day, ax=ax1, palette=['green'])\n\nsns.violinplot(sales_train.item_price, ax=ax2, palette=['blue'])","f70c3f09":"sales_over_10 = len(sales_train[sales_train.item_cnt_day >= 10])\nprint(f\"{sales_over_10} transactions or {sales_over_10 * 100 \/ len(sales_train)}% \"\n      + \"of all the transactions feature the sale of 10 or more items\")","d4463a32":"negative_sales = len(sales_train[sales_train.item_cnt_day < 0])\nprint(f\"{negative_sales} transactions or {negative_sales * 100 \/ len(sales_train)}% \"\n      + \"of all the transactions represent returns or a negative item_cnt_day\")","001c7e28":"drop_mask = (sales_train.item_cnt_day < 500) & (sales_train.item_cnt_day >= 0)\nsales_train = sales_train[drop_mask]","5a91ee48":"# creating a plot to show the items with high values\ndef plot_hv_items():\n    # create list of items valued over 5000\n    high_value_mask = sales_train.item_price > 5000\n    high_value_items = sales_train[high_value_mask].item_id.unique()\n\n    # collect the transactions relating to the above items, \n    mask = sales_train.item_id.isin(high_value_items)\n    high_value_prices = (\n        # group them by item\n        sales_train[mask].groupby('item_id')\n        # capture the min, median, and max prices\n        .agg({'item_price': ['min','median','max']})\n    )\n\n    # remove the multi-level column structure in the grouped dataframe\n    high_value_prices.columns = high_value_prices.columns.droplevel()\n    # order the values by median item_price\n    high_value_prices = high_value_prices.sort_values('median').reset_index()\n\n    # plot each of min \/ median \/ max by the ordered index\n    fig, ax = plt.subplots(figsize=(15,10))\n    \n    for agg_method in ['min','median','max']:\n        sns.lineplot(x=high_value_prices.index, \n                     y=agg_method,\n                     label=agg_method,\n                     data=high_value_prices, \n                     ax=ax, \n                     linewidth=2)\n\n    plt.xlabel('items in price order low-high')\n    plt.ylabel('price')\n    sns.despine()\n    \nplot_hv_items()","4275a892":"sales_train = sales_train[sales_train.item_price < 50000]","725ba856":"plot_hv_items()","ab4b1f9e":"sales_train['revenue'] = sales_train.item_cnt_day * sales_train.item_price","dd3738ed":"sales_train['month'] = (sales_train.date_block_num % 12)","bf8cb4c4":"# create quick dataframe containing months and their lengths\nmonth_lengths = pd.DataFrame({\n    'month': range(0,12),\n    'month_length': [31,28,31,30,31,30,31,31,30,31,30,31]\n})\n\n# merge the month_lengths into the sales_train data using the month feature\nsales_train = sales_train.merge(month_lengths, on='month', how='left')\nsales_train['month_length'] = sales_train.month_length","f628a75c":"shops.info()","59a9394e":"shops.head(10)","99fece6a":"# define new features\nshops['city'] = \"\"\nshops['shop_cat'] = \"\"\n\n# iterate through entries in shops \nfor idx in shops.index:\n    # split name into individual tokens\n    full_name = shops.loc[idx].shop_name.split(' ')\n    # and define city as first token, shop as second\n    shops.loc[idx, 'city'] = full_name[0]\n    shops.loc[idx, 'shop_cat'] = full_name[1]","df3044d7":"# selection of shops chosen to exemplify the new features \n# and highlight the required cleaning mentioned below\nshop_idxs = [0,1,57,58,52,54,46]\nshops.iloc[shop_idxs]","cf8f99c2":"shops.loc[0:1, 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops.loc[46, 'shop_cat'] = '\u0422\u0426'","5b50e062":"# use pandas 'categorical' datatype to encode features\nshops['city_id'] = shops.city.astype('category').cat.codes\nshops['shop_cat_id'] = shops.shop_cat.astype('category').cat.codes","f86e1ee1":"shops.head(10)","74c931ed":"duplicate_shops = [0, 57, 1, 58, 11, 10, 40, 39]\nshops.loc[duplicate_shops]","12c7731e":"print(\"Duplicate shops in test data: \"\n      f\"{[shop for shop in duplicate_shops if shop in test.shop_id.unique()]}\")","36540570":"# create dict of obsolete shops and replacement shop id\nduplicate_shops = {0:57, 1:58, 11:10, 40:39}\n\n \n# apply the function to ammend duplicate shop ids to sales_train\n# (note: this would not change the training data features \n# if applied to the shops dataset)\nsales_train['shop_id'] = sales_train.shop_id.apply(\n    lambda x: duplicate_shops[x] if x in duplicate_shops.keys() else x\n)","a4ae6a24":"item_categories.head(10)","f8bd1948":"filepath = '..\/input\/english-categories-for-predict-future-sales\/english_categories.csv'\nenglish_categories = pd.read_csv(filepath)\nitem_categories['item_category_name'] = english_categories.item_category_name","ffe744cf":"item_categories.head(10)","2e11523a":"# declare new subcat features\nitem_categories['subcat_a'] = \"\"\nitem_categories['subcat_b'] = \"\"\n\n# iterate through each item category\nfor idx in item_categories.index:\n    # split category name into two strings either side of ' - '\n    cat_name = item_categories.loc[idx].item_category_name.split(' - ')\n    # avoid creating new categories for hyphenated names\n    if len(cat_name) == 2:\n        # use indexes of cat_name variable to define new features\n        item_categories.loc[idx, 'subcat_a'] = cat_name[0]\n        item_categories.loc[idx, 'subcat_b'] = cat_name[1]","8a7c2b6b":"mask = (item_categories.subcat_a == \"\") | (item_categories.subcat_b == \"\")\n\nitem_categories[mask]","804d889c":"item_categories.loc[81:82, 'subcat_a'] = 'Blank media'\nitem_categories.loc[81, 'subcat_b'] = 'spire'\nitem_categories.loc[82, 'subcat_b'] = 'piece'\nitem_categories.loc[32, 'subcat_a'] = 'Payment Cards'\nitem_categories.loc[32, 'subcat_b'] = 'Cinema, Music, Games'","f0e40e0a":"item_categories[item_categories.subcat_a == \"\"]","13f747d8":"for idx in item_categories[item_categories.subcat_a == \"\"].index:\n    item_categories.loc[idx, ['subcat_a', 'subcat_b']] = (\n        item_categories.loc[idx, 'item_category_name']\n    )","a1447df9":"for feature in ['subcat_a', 'subcat_b']:\n    item_categories[feature + '_id'] = (\n        item_categories[feature].astype('category').cat.codes\n    )","aea307c0":"gaming_categories = [0,1,7,8,9,11,14]\nitem_categories[item_categories.subcat_a_id.isin(gaming_categories)]","3bbd48b5":"item_categories['gaming'] = 0\n\nfor idx in item_categories.index:\n    if item_categories.loc[idx, 'subcat_a_id'] in gaming_categories:\n        item_categories.loc[idx, 'gaming'] = 1","56b1ec47":"item_categories.head(10)","90fa67c8":"items.head()","1a04245e":"items.loc[1411:1421]","76eeb9b8":"import re\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nitems['item_name_corrected'] = items['item_name'].apply(\n    lambda x: name_correction(x)\n)","6582f463":"unique_item_names = len(items.item_name.unique())\nunique_corrected_item_names = len(items.item_name_corrected.unique())\n\nprint(f\"{unique_item_names} unique item names, \" \n      + f\"{unique_corrected_item_names} unique corrected item_names\")","7231b831":"items.loc[1411:1421]","6afcc15a":"items['item_name_id'] = items.item_name_corrected.astype('category').cat.codes","57ebc797":"test.info()","f88a7170":"test.head()","2eac6256":"def compare_unique_features(feature):\n    unique_train = sales_train[feature].unique()\n    unique_test = test[feature].unique()\n    common = test[test[feature].isin(unique_train)][feature].unique()\n    print(f\"{feature.upper()}: Training data contains {len(unique_train)}, \"\n          f\"Test data {len(unique_test)}, \"\n          f\"{len(common)} common to both.\")\n    \ncompare_unique_features('shop_id')\ncompare_unique_features('item_id')","2d2c0fe8":"shop_item = ['shop_id','item_id']\ntrain_shop_item_combos = sales_train[shop_item + ['item_cnt_day']].\\\n                            groupby(shop_item, as_index=False).mean()\ntrain_shop_item_combos['featured'] = 1\ntest = test.merge(train_shop_item_combos[shop_item + ['featured']],\n                  on=shop_item,\n                  how='left')\ntest.featured.fillna(0, inplace=True)\nnot_featured = (test.featured == 0)\nitem_info = (test.item_id.isin(sales_train.item_id.unique()))\nprint(\"We have shop and item train_sales information on \" \n      + f\"{len(test[~not_featured])} examples in the test set.\")\nprint(f\"Of the remaining test examples:\")\nprint(\"   * we have item only train_sales information for \" \n      + f\"{len(test[not_featured & item_info])}\")\nprint(\"   * and no train_sales information for \"\n      + f\"{len(test[not_featured & ~item_info])}\")","403d9191":"sample_submission.head()","1457287e":"example_sales = pd.DataFrame({\n    'shop':['a','a','a', 'b', 'b', 'c'],\n    'item':['x','y','z','x','z','y'],\n    'sales':[1,2,1, 3,1,2]\n})\nexample_sales","7ac3afba":"example_with_zero_sales = pd.DataFrame([[shop, item] \n                           for shop in example_sales.shop.unique()\n                           for item in example_sales.item.unique()],\n                                       columns = ['shop','item'])\nexample_with_zero_sales.merge(example_sales,\n                             on = ['shop','item'],\n                             how='left').fillna(0)","f79f9fe7":"train_data = []\n\nfor month in range(34):\n    month_data = sales_train[sales_train.date_block_num == month]\n    train_data += ([[month, shop, item] \n                      for shop in month_data.shop_id.unique() \n                      for item in month_data.item_id.unique()])\n    \ndate_shop_item = ['date_block_num', 'shop_id', 'item_id']\ntrain_data = pd.DataFrame(train_data, columns=date_shop_item)\n\ntrain_data.head()","70a4ffa6":"test_copy = test.copy()\ntest_copy['date_block_num'] = 34\ntest_copy.drop(['ID','featured'], axis=1, inplace=True)\n\ntrain_test_data = train_data.append(test_copy).reset_index(drop=True)","09f50941":"date_shop_item = ['date_block_num', 'shop_id', 'item_id']\nsales_features = ['item_cnt_day', 'revenue']\n\nmonthly_sales_train = sales_train[date_shop_item + sales_features].\\\n                            groupby(date_shop_item, as_index=False).sum()\nmonthly_sales_train.rename(columns={'item_cnt_day':'item_cnt_month'}, \n                           inplace=True)\n\ntrain_test_data = train_test_data.merge(monthly_sales_train, \n                                        on=date_shop_item, \n                                        how='left').fillna(0)","45be0ceb":"date_item = ['date_block_num','item_id']\nfeatures = ['item_cnt_day','revenue']\navg_prices = sales_train[date_item + features].\\\n                    groupby(date_item, as_index=False).sum()\n\navg_prices['month_avg_price'] =  avg_prices.revenue \/ avg_prices.item_cnt_day\ntrain_test_data = train_test_data.merge(avg_prices[date_item + ['month_avg_price']], \n                                        on=date_item, \n                                        how='left').fillna(0)","74364026":"# easier to add the month \/ month_length features fresh than merge from sales_train\ntrain_test_data['month'] = (train_test_data.date_block_num % 12)\n\ntrain_test_data = train_test_data.merge(month_lengths, \n                                        on='month', \n                                        how='left')\n\ntrain_test_data['month_length'] = train_test_data.month_length\n\nitem_features = ['item_id', 'item_category_id', 'item_name_id']\ntrain_test_data = train_test_data.merge(items[item_features], \n                                        on='item_id', \n                                        how='left')\n\ncategory_features = ['item_category_id','subcat_a_id', 'subcat_b_id','gaming']\ntrain_test_data = train_test_data.merge(item_categories[category_features], \n                                        on='item_category_id', \n                                        how='left')\n\nshop_features = ['city_id', 'shop_cat_id','shop_id']\ntrain_test_data = train_test_data.merge(shops[shop_features], \n                                        on='shop_id', \n                                        how='left')","6ccba480":"train_test_data.head()","cd639afe":"train_test_data.info()","18ca2cb3":"np.iinfo(np.int64)","8429f319":"dtypes = {'date_block_num': 'int8', \n          'item_id': 'int16',\n          'shop_id': 'int8', \n          'item_cnt_month': 'int8', \n          'revenue': 'float32', \n          'month_avg_price': 'float32',\n          'month': 'int8', \n          'month_length': 'int8', \n          'item_category_id': \n          'int8', 'gaming': 'int8'}\n          \nfor feature, dtype in dtypes.items():\n    train_test_data[feature] = train_test_data[feature].astype(dtype)\n    \ntrain_test_data.info()","2255d1fe":"# group sales_train by item_id and take minimum date_block_num as feature\ndate_first_sold = (\n    sales_train[date_item].\n    groupby('item_id', as_index=False).min()\n    .rename(columns={'date_block_num': 'date_block_first_sold'})\n)\n# merge date_block_first_sold with train_test_data\ntrain_test_data = train_test_data.merge(\n    date_first_sold, \n    on='item_id', \n    how='left'\n)\n# downcast\ntrain_test_data['date_block_first_sold'] = (\n    train_test_data.date_block_first_sold.\n    fillna(0).astype('int8')\n)\n# create months_since_item_first_sale feature by subtracting\n# current date_block_num from date_block_first_sold\ntrain_test_data['months_since_item_first_sale'] = (\n    train_test_data.date_block_num \n    - train_test_data.date_block_first_sold\n)\n# drop redundant feature\ntrain_test_data.drop('date_block_first_sold', axis=1, inplace=True)","52893ee3":"# create dict for data\nmonths_sold = {'date_block_num':[],\n               'item_id':[],\n               'months_since_item_last_sale':[]}\n# create list of item_ids in training data\ntrain_items = (\n    sales_train[sales_train.date_block_num < 34]\n    .item_id.unique()\n)\n# create dataframe of transactions by date_block_num and item_id\nlast_sold_data = (\n    sales_train[date_item + ['item_cnt_day']]\n    .groupby(date_item, as_index=False).min()\n)\nfor item in tqdm(train_items):\n    # find data relating to item\n    item_data = last_sold_data[last_sold_data.item_id == item]\n    # find first month item appears in data\n    item_first_sold = int(\n        date_first_sold[date_first_sold.item_id == item].date_block_first_sold\n    )\n    # loop through months the item was sold\n    for month in range(item_first_sold, 34):\n        # add date_block_num and item_id details to data\n        months_sold['date_block_num'].append(month + 1)\n        months_sold['item_id'].append(item)\n        # find entry in item_data for month\n        item_sales_month = item_data[item_data.date_block_num == month]\n        # if entry exists add zero\n        if len(item_sales_month):\n            months_sold['months_since_item_last_sale'].append(0)\n        # else accumulate months item hasn't sold\n        else:\n            months_sold['months_since_item_last_sale'].append(\n                months_sold['months_since_item_last_sale'][-1] + 1\n            )\n                \nmonths_sold = pd.DataFrame(months_sold)\ntrain_test_data = train_test_data.merge(\n    months_sold, \n    on=['item_id','date_block_num'], \n    how='left'\n)\ntrain_test_data['months_since_item_last_sale'] = (\n    train_test_data.months_since_item_last_sale\n    .fillna(0).astype('int8')\n)","4544d21a":"def add_lag_features(df, lag_months, group_features, feature_name, target_feature, agg_method, cumulative=True, dtype='float32'):\n    \"\"\"\n    Adds lagged features to monthly sales dataset\n    \n    Parameters:\n    df (pandas DataFrame): DataFrame containing monthly transaction data to which lag data is added\n    lag_months (list): list containing the lag(s) in months required\n    group_features (list): list containing the feature names the lag feature should be grouped on\n    feature_name (string): Name of new feature column when added to df\n    target_feature (string): Name of the feature in df to be aggregated in lag data\n    agg_method (Pandas.DataFrame.agg function): Name of aggregation method used by Pandas.groupby().agg()\n    cumulative (bool): If True feature is calculated cumulatively over specified months\n    dtype (data type): Option to specify datatype to downcast new features to\n    \n    Returns:\n    DataFrame: df with lagged feature data added\n    \"\"\"\n    # collect required data in grouped dataframe\n    feature_data = (\n        train_test_data[group_features + [target_feature]].\n        groupby(group_features, as_index=False).agg(agg_method)\n    )\n    # create list to store names of new features added\n    features_created = []\n    for lag_month in tqdm(range(1, max(lag_months) + 1)):\n        # skip this month if we aren't accumulating the data for every month\n        # up to the max lag_month\n        if not cumulative and lag_month not in lag_months:\n            continue\n        # create a copy of feature data that can be manipulated\n        temp_data = feature_data[group_features + [target_feature]].copy()\n        temp_data['date_block_num'] += lag_month\n        lag_feature_name = 'lag_' + str(lag_month) + 'm_' + feature_name\n        # add the feature to lag_months if it is one of the months requested\n        if lag_month in lag_months:\n            features_created.append(lag_feature_name)\n        temp_data.rename(\n            columns={target_feature: lag_feature_name}, \n            inplace=True\n        )\n        # merge the lag feature with the overall feature data\n        feature_data = feature_data.merge(\n            temp_data, \n            on=group_features, \n            how='left'\n        ).fillna(0)\n        # downcast the new feature\n        feature_data.iloc[:,-1] = feature_data.iloc[:,-1].astype(dtype)\n        if lag_month > 1 and cumulative:\n            # sum the new feature with the last one created to calculate a cumulative figure\n            feature_data.iloc[:,-1] = (\n                feature_data.iloc[:,-2:].sum(axis=1).astype(dtype)\n            )\n\n    # if agg_method is mean, the columns will currently contain a sum of the\n    # mean for each month\n    if cumulative and agg_method == 'mean':\n        for lag_month in lag_months:\n            # dividing by lag_month will result in the overall mean\n            feature_data.iloc[:,len(group_features) + lag_month] \/= lag_month\n\n    features_required = features_created + group_features\n    return df.merge(feature_data[features_required], \n                    on=group_features, \n                    how='left')","9f39149e":"train_test_data = add_lag_features(\n    df=train_test_data,\n    lag_months=[1,6,12],\n    group_features=['date_block_num', 'shop_id'],\n    feature_name='sales_by_shop',\n    target_feature='item_cnt_month',\n    agg_method='sum',\n    cumulative=False,\n    dtype='int32'\n)","2b957aed":"train_test_data = add_lag_features(\n    df=train_test_data,\n    lag_months=[1,6,12],\n    group_features=['date_block_num'],\n    feature_name='mean_sales',\n    target_feature='item_cnt_month',\n    agg_method='mean',\n    cumulative=False,\n    dtype='float32'\n)","d4ef471f":"train_test_data = add_lag_features(\n    df=train_test_data,\n    lag_months=[1,6,12],\n    group_features=['date_block_num', 'shop_id', 'item_id'],\n    feature_name='cum_sales_by_item_shop',\n    target_feature='item_cnt_month',\n    agg_method='mean',\n    cumulative=False,\n    dtype='int16'\n)","4ae7b54a":"train_test_data.info()","c22825ea":"drop_lag_months_mask = train_test_data.date_block_num > 11\ntrain_test_data = train_test_data[drop_lag_months_mask]","3172d374":"train_data_mask = train_test_data.date_block_num < 34\ntrain_data = (\n    train_test_data[train_data_mask]\n    .drop('revenue', axis=1).reset_index(drop=True)\n)\ntrain_data['item_cnt_month'] = train_data.item_cnt_month.clip(0,20)\ntrain_data = (\n    train_data.reindex(np.random.permutation(train_data.index))\n    .reset_index(drop=True)\n)","0cef0d05":"drop_features = ['item_cnt_month', 'revenue']\ntest_data = train_test_data[~train_data_mask].drop(drop_features, axis=1)\ntest_data = test_data.merge(test, on=['shop_id','item_id'], how='left')\ntest_data = test_data.set_index('ID')","9b3af35e":"train_data.to_pickle('train_data.pkl')\ntest_data.to_pickle('test_data.pkl')","de58a076":"# Basic Data Exploration and Feature Engineering","f30fc6f7":"This <code><b>item_name_corrected<\/b><\/code> feature could prove useful in highlighting trends between similar items, much in the same way as the features in <code><b>shops<\/b><\/code> and <code><b>item_categories<\/b><\/code> - as with these datasets, we'll create an ordinal encoding:","e377995d":"Now that we've added some good features, we'll put the finishing touches to our training data \/ test data and output both of them.\n\nFirst, we've added a whole bunch of lag features that look back over the previous 12 months of data. This means the first 12 months of data isn't all that useful to us anymore, as it doesn't include the full complement of these lag features. We have plenty of data besides so we can comfortably drop these examples:","b16080b3":"The competition data is broken down as follows:\n\n<ul>\n    <li><code><b>sales_train.csv<\/b><\/code>: training data with transaction information showing the date, time, shop, item and price of each transaction\n    <li><code><b>items.csv<\/b><\/code>: names of each item and the category they belong to\n    <li><code><b>item_categories.csv<\/b><\/code>: a short description of each category\n    <li><code><b>shops.csv<\/b><\/code>: name details for each shop\n    <li><code><b>test.csv<\/b><\/code>: item \/ shop pairings for which we are required to predict sales\n    <li><code><b>sample_submission.csv<\/b><\/code>: an example of the competition submission format\n<\/ul>\n\nto start, we'll go through each of the dataframes and familiarise ourselves with the information they contain, and have a crack at creating some simple features if there's any obvious low-hanging fruit to be had:","13423e36":"Further inspection of the data also shows that some of the shops appear to be duplicates:","6ade9441":"Then we can create an ordinal encoding for these features. It should be noted that the features themselves aren't ordinal - they don't have a natural order (e.g. small -> medium -> large). Creating an ordinal encoding, however, makes it easier when fitting the data to different model types, and allows you to reduce the amount of space these features take up in memory (more on this later):","d238a8b3":"The most significant breakthroughs I experienced in this competition have come from different approaches when adding entries for items that haven't sold. \n\nWe should start by asking <b>if<\/b> it's even necessary to add zero transactions. We can quickly answer this question by making a submission in which we predict <b>1.0<\/b> for every entry, and another where we predict <b>0.5<\/b>. You will find the score for the <b>0.5<\/b> predictions is considerably higher, leading us to believe that either some of the targets should be zero, or that the competition organisers have lied to us and some of the targets are fractions. Lets assume the former! Therefore, we can assume we'll need to feed our model some zeros, or it's not going to do a good job of making predictions itself. \n\nWith the <b>if<\/b> answered we can move onto the <b>how<\/b>. We are making some big assumptions when we choose any approach that adds examples to our training data. When we add zero transactions we are effectively telling our model <b>\"this item was on the shelves in this shop, and nobody picked it up and bought it\"<\/b>.  With this we can make a huge difference to the effectiveness of our model, positive or negative:\n\nInitially, I began by adding a zero entry for every month \/ shop \/ item in the training data that didn't already feature. That's a lot of entries,  ~ 41.5M to be exact! This was just unmanageably large even without engineering any further features. To overcome this, I removed any of the items that don't feature in the test set and went about engineering new features from there. The model predictions from this data were always overly cautious - very few predictions rose above the 0.4-0.6 range. The resulting RMSEs weren't fantastic (around the 1.0 mark). \n\nIt is clear from the above results, that far too many zero-sales transactions had been added. Indeed, after some more exploration of the data I found that certain shops and the majority of items don't make their initial appearance in <code><b>date_block_num<\/b><\/code> zero and often appear for the first time much later. By removing any zero transactions for shops or items that have yet to feature in the <code><b>sales_train<\/b><\/code> in a given month, the results improved dramatically - dropping to a RMSE of approximately 0.92 in the best case. Whilst this was a much better result, it wasn't particularly close to the top end of the leaderboard which was pretty frustrating.\n\nI stagnated at the previous 'stage' of my results for a long time. The final breakthrough came in two stages. Firstly I noted that the majority of my predictions were still very tentative - that is to say, closer to 0-1 than above, and that my best results were achieved when my models tended to make more 'daring' predictions above 1. I then went back to the data, looking at the pattern of sales for different items. Several items exhibit gaps in their sales patterns: some miss the odd month here or there, others don't feature for several months at a time and then reappear. Several of the items drop off entirely and then make a sudden and significant resurgence (if you'd like to see this for yourself, have a play around with the graph of item sales in the exploration section above, as it is a similar graph that helped me make this discovery). \n\nIt became clear to me that my assumption <b>\"once an item featured in the data it was on sale from then on in every shop\"<\/b> may have been a problem. Items may have been removed from shelves, meaning they had no opportunity to sell,  before being added again. Alternatively, there may have been gaps in the training data and I was superimposing zero sales of items that did in fact sell. At any rate, by removing any zero examples that didn't feature at all in a given month of <code><b>sales_train<\/b><\/code>, i.e. either the shop or item were totally absent, removed a huge number of zero transactions. This allowed me to re-introduce the dropped examples, those that didn't feature in the test data, back into the training data. The result was an immediate improvement in my score, and this approach has continued to produce my best scores (just over 0.90 at the time of writing).","20caecc0":"And then we can try this out by creating the <b>12 month 'lagged' sales by shop<\/b> feature we discussed above. Given the above function allows you to specify several \"month lags\", let's get a little more for our money and ask for the sales by shop for the previous <b>1, 6 and 12 months<\/b>:","a0ad83b6":"If we recall from the initial exploration of the <code><b>sales_train<\/b><\/code> dataframe, there appeared to be strong temporal trends within the data. That is to say, for example, that more general information about sales of an item categories, shop sales , or sales in a given month may be very useful in making predictions about future sales. Being mindful of this, we can engineer features that capture some of the details of the trends in the overall data in each example, that will help any model to make more accurate predictions.\n\nAs mentioned before, we should be very careful when using details from other examples in our data to engineer new features. The task is to make predictions of the sales in the month following the last in the training data, so we must be mindful that we should not introduce information in our training data that isn't available to us in the test data. We should also be very careful not to accidentally introduce information about an example's label into any of these features. This is perhaps best explained with an example:\n\nImagine we thought it useful for our model to know the average sales for each month, and added the average sales for every month to every example. When training, the model could then make a prediction on an example from month 25 using the sales average from the preceding months, but also month 25 itself (which includes details of this example's label) and the months in the future. When making predictions for the test data, however, it would not have this information; only sales averages for months past - if it had learned when training to make predictions by looking into the future, it's unlikely to perform well without this information.\n\nBering this information in mind, we shall tread carefully.","60ae9b2b":"and then we group the <code><b>sales_train<\/b><\/code> data by <code><b>date_block_num<\/b><\/code>, <code><b>shop_id<\/b><\/code> and <code><b>item_id<\/b><\/code> and merge the mean  <code><b>item_cnt_day<\/b><\/code> and <code><b>revenue<\/b><\/code> into our data.","3c778b17":"We are told in the competition description that each entry represents sales for the month following the training data, month 34. This can be seen in the submission format, as it requires the predictions to be labeled <code><b>item_cnt_month<\/b><\/code>. \n\nThe observant among us will immediately see a few challenges posed by this submission format: \n\n<ul>\n<li> The format of the data in <code><b>sales_train<\/b><\/code> is <b>daily<\/b> not <b>monthly<\/b>. We need to train our model on a dataset that reflects the distribution of the test data. As such, we can't simply join all of our training dataframes to form a training dataset -  we will have to significantly reformat <code><b>sales_train<\/b><\/code> into a dataset of monthly sales.<\/li>\n\n<li>As things stand, we have information on completed transactions - presumably we will also be required to predict when items <b>don't<\/b> sell as often (or perhaps more often) than when they do. At some point, therefore, we will have to decide how we are going to add training examples for the items that haven't sold.<\/li>\n\n<li>We are also being asked to make predictions on data <b>outside<\/b> of the time-frame we are given in the training data. This means that:<ol>\n<li>Our training and test distributions will not match, which will make model evaluation more challenging: Our evaluation metrics may produce dramatically different scores between train, test and validation sets, even if our model is making reliable predictions.<\/li>\n<li>We cannot use standard techniques to partition our data into training and validation sets: standard cross-validation \/ K-fold techniques would result in train \/ validation sets being taken from within the time-frame of the training data, which does not match that of the test data - we require our model to make predictions on examples taken from outside the time-frame on which it is trained.<\/li>\n<li>We must be <b>very<\/b> careful when engineering features, so as to only include information that is available to us for month 34 (the testing month), and that we avoid our model to 'snooping' on future information it shouldn't have access to (e.g. making a prediction on an example from month 33 using mean sales for month 33)<\/li><\/ol><\/li><\/ul>","b2f4a50a":"Some of these examples with missing <code><b>subcat_a<\/b><\/code> \/ <code><b>subcat_b<\/b><\/code> info obviously fit well with existing categories and so can be manually amended as such:","4b4a5e92":"Then we separate the training data. This is easily done by selecting only month 34 (or not the data in <code><b>train_data_mask<\/b><\/code>). Don't forget to drop both <code><b>revenue<\/b><\/code> and <code><b>item_cnt_month<\/b><\/code> as we don't have any labels for our test data - at any rate, they're all zeros anyway! We'll merge in the ID feature from the <code><b>test<\/b><\/code> dataframe as this will be required when we submit our predictions:","bb1b45a3":"Further exploration of the <code><b>item_cnt_day<\/b><\/code> target feature doesn't seem to reveal anything obvious that points to examples that should \/ shouldn't be removed or changed. From experience the most effective approach has been to remove any transactions outside the [0, 500] <code><b>item_cnt_day<\/b><\/code> range. No satisfyingly scientific reasoning behind this;  just that trial and error with a range different approaches has shown this to produce the best results:","6baca302":"As with <code><b>shops<\/b><\/code> ordinal categories are then created:","d1f97166":"There are a whole host of different potential features that can be added by taking different groupings of <code><b>shop_id<\/b><\/code> \/ <code><b>item_category_id<\/b><\/code> \/ <code><b>subcat_a_id<\/b><\/code> \/ <code><b><\/b>city_id<\/code> etc. The limit is only the number of permutations between the different features available (which is an impractically large number!). Forking the notebook and having a go at making some features is positively encouraged. ","af482aa7":"similarly, a very small minority of transactions feature a negative <code><b>item_cnt_day<\/b><\/code>:","3d0adcf1":"Next, we can separate the training data. Since the competition requires us to make predictions in the range 0-20, we can clip the target feature <code><b>item_cnt_month<\/b><\/code> so that our model learns to make predictions in this range. We should also drop the <code><b>revenue<\/b><\/code> feature, as our model will have a pretty easy time of making predictions if we keep it! (have a think about how it's prepared if you're unsure why):","b59af62c":"Firstly, credit for this section goes to the brillant <a href=\"https:\/\/www.kaggle.com\/kyakovlev\">Konstantin Yakovlev<\/a>, with his excellent notebook <a href=\"https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data\"> 1st place solution - Part 1 - \"Hands on Data\"<\/a> which comes highly recommended. The <code><b>name_correction<\/b><\/code> function below is taken directly from his work.\n\nAs with <code><b>shops<\/b><\/code> and <code><b>item_categories<\/b><\/code> the <code><b>items<\/b><\/code> table is a relatively uninteresting list of each item with its <code><b>item_naame<\/b><\/code>, <code><b>item_id<\/b><\/code> and <code><b>item_category_id<\/b><\/code>:","17d9562a":"Digging further into the test data, we can explore the the shops and items featured in <code><b>test<\/b><\/code> and compare them with those featured in <code><b>sales_train<\/b><\/code>:","e2189daa":"With the test data, we are provided only<code><b>ID<\/b><\/code>, <code><b>shop_id<\/b><\/code>, and <code><b>item_id<\/b><\/code> features: ","ed8af0fd":"# Appendix - adding zero transactions","a6f80f17":"# Managing Memory","b4694794":"At first glance this information appears pretty mundane. As it stands each of the <code><b>shop_name<\/b><\/code> strings are unique and so the feature provides no more information than the <code><b>shop_id<\/b><\/code> that already features in <code><b>sales_train<\/b><\/code>. Closer inspection though, shows repetition of several of the 'words' in each of the name strings:","f21a1910":"Easy! There's obviously potential here for a huge number of engineered features. For the moment though, it's better to explore the other data we've been provided and return when we are ready to draw all the information together in one training dataset:","b614b3ce":"That seems to be about all we can do with the <code><b>shops<\/b><\/code> dataset for the moment. Moving on we have:","4ea54ee9":"We can also add a monthly average of the sales prices for each item. <b>Note:<\/b> <code><b>item_price<\/b><\/code> should not be included! - The price varies from transaction to transaction and so an average item price for the month is certainly useful. But, if a mean value of the <code><b>item_price<\/b><\/code> feature itself is used, then the sales volume of each transaction won't be taken into account:\n\ni.e. 4 items sold at \\$100 and one item at \\$150 will be counted as an average price of \\$125 and not the correct value of \\$110\n\nInstead, the revenue per transaction should be used to calculate the average item price for each month:","2da569a5":"Immediately, we can see some pretty strong seasonal trends at work here. There are distinct months in which there are consistent higher sales than others. \n\nWe can take a more detailed look at some of the individual items, to see if there are any trends at the item level:","ddbfc841":"Boom! We've reduced the memory usage by over 2 \/ 3 and didn't lose any information in the process. We'll have to keep an eye on this as we add more features, but that's just a case of downcasting them as and when they are added to the training data.","f65afd35":"That about does it for building a functioning training dataset. We won't be fitting a model here, as this has already been a pretty epic journey! At any rate, It's recommended to prep your data and fit your model separately for this competition. For one thing, Jupyter notebooks (on which Kaggle notebooks are based) and pandas both have some quirks when it comes to memory that mean a large amount of RAM is eaten up as you prep data that is neither required, nor is it easy to free up. It's much easier and quicker to pickle your train and test datasets and then load a fresh notebook to build and test models so you don't run into memory issues. This has the added benefit of keeping your data prep and model stages separate, so they can be versioned much more clearly when you commit, and you can organise several separate models should you wish to ensemble at some point. \n\nThis competition really is about the data. Better data cleaning and feature engineering can improve scores dramatically - comparatively, model selection and parameter tuning are far less important. The training data prepared in this model, with a few decent lagged features added (that aren't provided above - that would be too easy!) will provide competitors with data that can easily score in the very low <b>0.9 RMSE<\/b> area when fed to any of your bog-standard XGBoost \/ LightGBM \/ RandomForestRegressor models, more or less out of the box.\n\nTo summarise, we have:\n\n<ul>\n    <li>Cleaned the data - eliminating outliers and repeat data items etc.<\/li>\n    <li>Looked more closely at the categorical features to engineer some useful sub-categories<\/li>\n    <li>Grouped the sales data so it represents monthly transactions<\/li>\n    <li>Added zero transactions for items and shops that feature in a month, but represent no transactions in combination<\/li>\n    <li>Managed memory footprint very carefully and downcast features where possible<\/li>\n    <li>Created a feature that tells us how long it is since items first \/ last sold<\/li>\n    <li>Created lag features to point your model to past sales performance in different categories<\/li>\n<\/ul>\n\n\nAll that remains is a warning from the author. As mentioned above, the temporal element of this particular data presents a number of challenges, none more so than when it comes to model evaluation. All of the usual train validation methods, your CV splits and your K-folds, are not welcome here. Using a random sample of the training data from the whole timeline to evaluate a model will really encourage it to dig deep, and remember the performance of each item in each month to make the best predictions on the validation data. When it comes to the test data however, it wont ever have seen data from month 34, and so all this memorising will have been for nought. Put more simply and scientifically, a model will massively overfit if we validate this way. \n\nInsted we should set up your train \/ validation split to a group of the earlier months for training and then the month immediately following for the validation. This will ensure that our model is optimised to make predictions on data it hasn't seen before, and that it can't cheat by using data from the same month it's being tested on. Doing this, we should also be mindful that our train \/ validation \/ test distributions will differ and so we may well end up with quite substantial differences in RMSE scoring. As such, validating the model several times using different ultimate months is encouraged i.e. 'walking forward' using month 30, then 31, then 32, and so on as validation data, and the prior 12 months to train.\n\nWith that, I bid you farewell. Sincere thanks to those that made it to the end - I would really welcome any feedback or comments. Anybody interested in the next stage, model fitting, may wish to take a look at the following:\n\n<a href='https:\/\/www.kaggle.com\/dustyturner\/dense-nn-with-categorical-embeddings'>Dense NN With Categorical Embeddings<\/a>","8c6a0bfe":"Before we can continue with our data preparation, we need to reformat the sales_train information so that it matches the submission format i.e. so that it represents monthly, rather than daily transactions. This can be easily done by grouping the transactions by month, shop, and item. The far bigger challenge is to add transactions for those items that <b>haven't<\/b> sold in a given month, and was somewhat of an epic journey for the author! I'll go ahead and spoil the ending now for the sake of brevity, but a more detailed explanation and (attempted) justification is included in an appendix (at the end of the notebook) for those interested.\n\nThe most effective way of adding examples for unsold items is to find the items and shops that feature in each individual month of the data, and add zero sales for any combination thereof that isn't present. For example if we had the following transactions:","d277c9c1":"# Lag Features","f88dae3b":"# - sales_train","4d611fbe":"# Intro","490b94a0":"# - shops","4016debe":"A good place to start is recalling the trends we saw in the sales of individual items. It appeared clear that items sold best in the month or the month after they were introduced (or that we first saw recorded sales to be pedantic). As such, we can create a feature that signposts this to our model:","00b469e8":"So we can consider different approaches to making predictions for the three different groups of examples above.\n\nSpeaking of predictions, we are also provided with a sample submission file:","e2cfc20b":"Though certainly worth keeping in mind, the tails shouldn't be hugely surprising. Given the scope of the dataset and the huge number of stores and items, we would expect to see individual sales in far greater numbers than multiple sales, and the odd hugely popular item. Similarly, considering the sales prices of the stock in a shop we would expect to see a minority of very high-value items. The negative values for <code><b>item_cnt_day<\/b><\/code> most likely represent the return of unwanted or faulty items. \n\nWe must carefully consider how to treat outlying examples. It's possible that the very small minority of data points that exhibit very high sales \/ prices or negative values may end up hiding some of the useful variance between the items within a more normal range of values.\n\nStarting with <code><b>item_cnt_day<\/b><\/code>, it can be seen that a tiny fraction of the data features in the long tail of the distribution:","48422a67":"With <code><b>item_price<\/b><\/code> there are some interesting features of the outliers that merit a closer look. Isolating the higher valued items, ordering them from lowest to highest median value, and then plotting their min, median, and max price we get the following:","14e20023":"Then we would add examples representing zero sales resulting in the following:","92a826db":"From here we have a whole host of different potential features at our fingertips. For example, we can create a feature that tries to capture the trends we saw earlier in overall sales for each month:","47a251a1":"Checking the new features, we can see a few examples where the <code><b>item_category_name<\/b><\/code> is not separated by the characters <code><b>\" - \"<\/b><\/code>:","e3fb0144":"# - Test","4e294780":"Given that we have a relatively small number of categories, these can all be examined by eye to see if there are any other features common to them that have not been captured in the data. One obvious feature of the <code><b>item_categories<\/b><\/code> is that several relate to gaming:","fc76b6d3":"We also know that as items get older sales slow down. As sales slow, certain of the later months in a product's lifetime feature no sales at all. It may be useful for our model to know this so it can pick up on when items may have gone 'out of fashion':","b0db0192":"and all that remains is to pickle our <code><b>train_data<\/b><\/code> and <code><b>test_data<\/b><\/code> datasets and pat ourselves on the back for a job well done!","34ad7a43":"A feature that identifies each month of the year may also be useful, as this may help a predictive model identify some of the obvious trends in the above graph of sales:","326e8932":"Next, we can feed our model details of past trends in the form of a 'lagged' feature. This can be a clever way of providing our model with really helpful information - effectively we're using the labels from information earlier in the dataset to help our model make predictions on data later on. For example, we could provide the model with information about the mean sales in each shop for the same time the previous year - this would be a <b>12 month 'lagged' sales by shop<\/b> feature.\n\nTo create a lagged feature you can follow a pretty simple recipe:\n<ol><li>Choose a feature, or group of features that you want to group your new feature by; this will always include <code><b>date_block_num<\/b><\/code> as we are lagging the features by a specified number of months - in our example we would add <code><b>shop_id<\/b><\/code><\/li>\n<li>Choose your target feature - in our example this would be <code><b>item_cnt_month<\/b><\/code><\/li> \n<li>Decide how you want to aggregate the grouped data - so our example would be a <b>mean<\/b> of the <code><b>item_cnt_month<\/b><\/code> for each shop, but this could just as easily be a <b>median<\/b> or <b>sum<\/b><\/li>\n<li>Add the number of months you are lagging to the <code><b>date_block_num<\/b><\/code> feature - so our example would be <b>12<\/b><\/li>\n<li>Merge this data with your training dataset<\/li><\/ol>\n\neasy.....\n\nSo easy, in fact, that we can define a function to do this for us:","2e9a679d":"We can see that the training data contains considerably more shops and items than are required in the test set. We should certainly consider keeping hold of this data, as (in general) the more data we have the better inferences our model will be able to draw. Worryingly though, we are missing some of the items in the test set, and so we'll have to make some predictions without prior sales info.\n\nIf we look at the numbers involved, it's clear that we are required to make a prediction for each unique shop and item combination in the test set:\n\n   <b>42 unique shops * 5100 unique items = 214,200 (the number of items in the test dataset)<\/b>\n    \nLets compare this to the shop \/ item combinations available in the training data:","16d31e42":"And let's not forget all of the other features we've already engineered:","5cd55ef4":"To start with, lets visualise the overall annual sales figures to see what we are working with:","2884b250":"With that, let's crack on and get our training data up to spec:","c57d87b1":"For those of us that don't read Russian, a bit of google translating and formatting and the names can be translated to english. This can make it a tad easier to parse out useful information, as well as making this explanation a fair bit easier and more interesting! To save time and space, a csv file with the translations is included in the notebook data:","8d7c9667":"With a peek at the test data, it is clear that only one out of the two shops in these pairs makes an appearance:","1dc39b1c":"The Predict Future Sales competition challenges Kagglers to (somewhat unsurprisingly) predict the sales of a Russian software firm: 1C Company.\n\nCompetitors are provided with transaction data, information about the items being sold, and info about the stores in which said transactions have taken place - they are to use this information to make predictions about the sales of these (and other items) in the month following the period covered in the data.\n\nIn this notebook you will find an exploration of the competition data, a few illuminating insights, and a spot of feature engineering that will hopefully set any prediction models off to a good start. Lets take a look....\n\n![](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fvortini.com%2Fwp-content%2Fuploads%2F2017%2F05%2FSalesForecast.png&f=1&nofb=1)","b50618e4":"# Final Thoughts \/ TLDR","fad586b6":"We can use python's string and regex methods to standardize each name to lowercase characters only and remove the bracketed text and special characters, which saves us the daunting task of wading through all of the data ourselves:","b4f5ea86":"Having dealt with the distribution of the existing features, lets see if there are any we can quickly add. Firstly, given that for each sale we have a price (<code><b>item_price<\/b><\/code>) and a sales volume (<code><b>item_cnt_day<\/b><\/code>), we can quickly create a revenue feature (i.e. number of items sold multiplied by the sales price):","f45de973":"The table is far too large to do much in the way of manual inspection, but with a bit of digging around it can be seen that some of the items share relatively similar names:","7683b2d0":"The shops data is a small table of the 60 shops in the dataset, each with only a <code><b>shop_name<\/b><\/code> and corresponding <code><b>shop_id<\/b><\/code>:","20e05f3b":"There are even more granular features you can create using the <code><b>item_category_name<\/b><\/code> strings should you wish. The above are those that have proved most successful when training subsequent models.\n\nLet's move onto the next of our datasets:","e4a8ab66":"In the sales data comprises the primary transaction data we'll be using to make sales predictions. We have ~2.9M transactions records over a period of 34 months. Individual records include a <code><b>date<\/b><\/code> (broken down into individual days - this is <b>very<\/b> important to remember for later), <code><b>date_block_num<\/b><\/code>, <code><b>shop_id<\/b><\/code>, <code><b>item_id<\/b><\/code>, <code><b>item_price<\/b><\/code>, and the target variable <code><b>item_cnt_day<\/b><\/code> which are all pretty self explanatory:","6a93d842":"# Match Training Data to Submission Format","83f115bc":"and a feature that highlights how well an item has sold in a particular shop in the past will certainly be useful - we could make this feature cumulative, to tell our model the total sum of sales over the last 6 and 12 months:","f1d6b817":"As with <code><b>shops<\/b><\/code> an initial look at the <code><b>item_categories<\/b><\/code> data shows a relatively uninteresting table with an <code><b>item_category_name<\/b><\/code> and an <code><b>item_category_id<\/b><\/code> for each example:","9e4feca7":"and with that, we've explored all of the training data we have been provided. Before we start to draw it all together into one big training dataset, let's take a look at the test data to see what we have been tasked with:","5f0c2f5a":"A check of the new features shows a little bit of manual cleaning is required:\n\n<ul>\n<li>two of the '\u042f\u043a\u0443\u0442\u0441\u043a' examples are prefixed with a <b>'!'<\/b> - this needs removing as the model will otherwise assume they represent a different city<\/li>\n<li>example <b>id 46<\/b> is <b>'\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'<\/b> - the subcategory has been set to <b>'\u041f\u043e\u0441\u0430\u0434'<\/b> when it should be <b>'\u0422\u0426'<\/b><\/li>\n<\/ul>\nOf course, we could make the above code more sophisticated to account for these inconsistencies. Given their small number however, we can be lazy and just manually change them:","72b9a085":"Finally, it may also be useful for our model to know how long each month is, as more days mean more potential sales:","cc978238":"Now that we have begun to add more features to our data, it's worth considering the size of our training dataset. Managing the amount of space our data takes up in memory is one of the main challenges in this competition. Bearing in mind the training data will have to be loaded into RAM, usually multiple times, to train our prediction model we must remain mindful of the size of the dataset we are creating:","cc9f6513":"Having done this, it's clear that a lot of the simplified names match:","f57b9a4c":"# Finishing Touches","b68af3ca":"We can assume, therefore, that these shops are indeed duplicates and must have changed names at some point in the data. As such, we can go through the training data and amend the offending <code><b>shop_id<\/b><\/code>s accordingly:","a0539fc7":"<small><i>Disclaimer: though a fixed group of items have been used for the above, this is primarily to avoid random selection resulting in an unattractive or unreadable graph, where one high selling item dominates and the other items clump near the bottom. The items above were originally chosen at random by numpy - forking the kernel, running the above with the commented section, and analysing different items is encouraged.<\/i><\/small>\n\nThe data is a lot noisier at this level (i.e. it's a mess!), but there are still some clear trends that appear: sales tend to peak within the first two months of the item's introduction and tail off as the item gets older. Also, note that none of the above items feature sales in every month of the data - many are introduced much later on in the timeline and many stop selling fairly early. It is clear that there are strong temporal trends right the way through the data, and so we should think carefully about this when building our training dataset and when training our model. \n\nIf we move on to look at the distribution of examples within each feature, we see some hefty tails on <code><b>item_price<\/b><\/code> and <code><b>item_cnt_day<\/b><\/code>), and some negative values for each:","7004325c":"Re-running the above graph cell will show the prices now fall within a more reasonable min \/ mean \/ max distribution.","cb58dfe2":"# More Advanced Features","ded35ad2":"The remaining examples don't fit with any of the other <code><b>subcat_a<\/b><\/code> \/ <code><b>subcat_b<\/b><\/code> categories, and so are filled with their <code><b>item_category_name<\/b><\/code> in both subcategory features to signify that they are unique:","f606ecc5":"We can manually create a feature to reflect this, by using a one-hot-encoding - that is, to set the feature to 1 if the category relates to gaming and 0 otherwise:","717c2db3":"# - item_categories","4dd42bfa":"In the case of our <code><b>date_block_num<\/b><\/code> feature, this is obviously overkill as we only need months 0 through 34! Indeed, we don't have much need for over 9 x 10^18 values for any of our current features. \n\nTo avoid wasting huge amounts of memory describing very simple features, we can 'downcast' the datatype to a more appropriate size. You can use the above code block to check the ranges of the different dtypes. We can save a ton of memory by choosing the minimum amount of reserved space we need to ensure all of the values in a feature fit into that datatype. Pandas has already helpfully done this for us with the ordinal categories we have created:  <code><b>subcat_a_id<\/b><\/code>, <code><b>subcat_b_id<\/b><\/code>, <code><b>city_id<\/b><\/code>, <code><b>shop_cat_id<\/b><\/code> were all created as <code><b>int8<\/b><\/code>s and <code><b>item_name_id<\/b><\/code> was created as an  <code><b>int16<\/b><\/code>.\n\nLets downcast the rest of our features and see how much memory we can save:","f8d7ba76":"For the most part, the <code><b>item_price<\/b><\/code> seems  to do what we would expect i.e. it varies pretty uniformly around the median <code><b>item_price<\/b><\/code> for each item. There are, however, a couple of pretty blatant outliers:\n<ol>\n<li>At the very start of the graph, there is an odd spike in the max value of one of the items. A quick look at the data, and a bit of google translating shows this example to be a sale of \u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 (EMS) or 'Shipping'. The prices of the other sales of shipping all fall within a much smaller range than the current max, so this appears to be an outlier.\n\n<li>At the end of the graph there is a sudden explosion of the gradient - this turns out to be only one transaction 'Radmin 3 - 522 persons' valued at 307980. Apparently Radmin 3 is a piece of software used by IT professionals, and it seems this is some sort of bulk transaction (probably some sort of enterprise \/  corporate arrangement or suchlike that found its way onto the books)<\/li>\n<\/ol>\n\nA couple of outlying examples are unlikely to affect our predictions dramatically, but we can remove them anyway:","16770f86":"For those unfamiliar with the cyrillic alphabet and \/ or the geography of Russia, some quick googling will show you that the first token (or word) in the <code><b>shop_name<\/b><\/code> string is a city i.e. <b>'\u042f\u043a\u0443\u0442\u0441\u043a'<\/b> is <b>Yakutsk<\/b>. Thus, we can conclude that we have several different shops in each city. There's also a lot of repetition in the second token \/ word, with the likes of <b>'\u0422\u0426'<\/b> and <b>'\u0422\u0420\u0426'<\/b> appearing numerous times - these turn out to be things like <b>'shopping centre'<\/b> and <b>'shopping and entertainment centre'<\/b>.\n\nWe can easily extract this information as new features:","967ee9b1":"Like shops, there are common features of the item_category_name feature that can be used to engineer new features. They are (hopefully) pretty obvious after a quick look at the data, and seem to be well divided between the '-' character in each name string. We'll use the sections of the string either side of this '-' as new features:","4e15441a":"# Item first \/ last sold","e371964c":"Our dataset takes up nearly 1GB in memory. Given the number of examples in our training data we are already fast approaching an unmanageably large dataset, even with the few features we have already engineered. Thankfully, there are ways to reduce the footprint of our dataset without compromising the amount of information it contains. \n\nIf you look, each feature in the info table above is accompanied by a datatype i.e. <code><b>int64<\/b><\/code> or <code><b>float64<\/b><\/code>: this signifies the type of data stored by that feature and the amount of space in memory reserved for each individual value. So, for example, an <code><b>int64<\/b><\/code> is an integer value with 64 bits of memory reserved for its storage. The amount of memory reserved for a data point dictates the number of possible values it can take - so, continuing with our <code><b>int64<\/b><\/code> example, this can take the following values:","e002c0b3":"Most importantly, <b>we shouldn't be adding any zero transactions for items or shops that don't appear in a given month<\/b>.\n\nTo achieve this, we first make a DataFrame with only the <code><b>date_block_num<\/b><\/code>, <code><b>shop_id<\/b><\/code> and <code><b>item_id<\/b><\/code> for each example:","7ca3d82d":"We then concat the <code><b>test<\/b><\/code> examples to the bottom of this dataframe, adding the relevant <code><b>date_block_num<\/b><\/code> along the way. This is a shortcut that means we don't have to add any engineered features to the test set separately - this is particularly useful when we start adding lagged data later:","0d870125":"# - items"}}