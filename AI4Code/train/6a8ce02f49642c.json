{"cell_type":{"2deb3dc0":"code","a21dc9f8":"code","bde595bc":"code","3d4aee42":"code","b0f6996b":"code","7b426a27":"code","64ca1a7c":"code","0596bab4":"code","4573d294":"code","15f37522":"code","ea4c341d":"code","8902684c":"code","d4a90b9a":"code","2810ca0f":"markdown","00908499":"markdown","06558d04":"markdown","3c522c71":"markdown","0ae1c77d":"markdown","e08d1264":"markdown","dbfc8296":"markdown","6d2e4df4":"markdown","09d52792":"markdown","e9770780":"markdown"},"source":{"2deb3dc0":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","a21dc9f8":"df_train_10M = pd.read_csv('..\/input\/train.csv', nrows=200000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\ndf_train_10M.head()","bde595bc":"df_train_10M[\"index_obs\"] = (df_train_10M.index.astype(float)\/150000).astype(int)","3d4aee42":"train_set = df_train_10M.groupby('index_obs').agg({'acoustic_data': 'std', 'time_to_failure': 'mean'})","b0f6996b":"train_set.columns = ['acoustic_data_std', 'time_to_failure_mean']\ntrain_set.head()","7b426a27":"del df_train_10M","64ca1a7c":"train_set[train_set['time_to_failure_mean'].diff() > 0].head()","0596bab4":"train_set[\"acoustic_data_transform\"] = train_set[\"acoustic_data_std\"].clip(-20, 12).rolling(10, min_periods=1).median()","4573d294":"fig, ax1 = plt.subplots(figsize=(20,12))\nax2 = ax1.twinx()\nax1.plot(train_set[\"acoustic_data_transform\"])\nplt.axvline(x=38, color='r')\nplt.axvline(x=334, color='r')\nplt.axvline(x=697, color='r')\nplt.title('Smoothed standard deviation of acoustic_data vs. time', size=20)\nplt.show()","15f37522":"from sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nregr.fit(train_set[[\"acoustic_data_transform\"]], train_set[\"time_to_failure_mean\"])\n\nprint('Coefficients: \\n', regr.coef_)\nprint('Intercept: \\n', regr.intercept_)","ea4c341d":"submission_file = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission_file.head()","8902684c":"for index, seg_id in enumerate(submission_file['seg_id']):\n    seg = pd.read_csv('..\/input\/test\/' + str(seg_id) + '.csv')\n    x = seg['acoustic_data'].values\n    std_x = max(-20, min(12, np.std(x)))\n    submission_file.loc[index, \"time_to_failure\"] = max(0, regr.intercept_ + regr.coef_ * std_x)\n    del seg","d4a90b9a":"submission_file.to_csv('submission.csv', index=False)","2810ca0f":"We check at which indices the earthquakes happen. These are the indices when the difference between 2 'time_to_failure' is positive.","00908499":"Let's predict the results on test set and submit the results.","06558d04":"Let's check the format of the train data.","3c522c71":"Following our first intuition, if \"acoustic_data_std\" is a linear function of  \"time_to_failure_mean\", as we can see on the graph, then \"time_to_failure_mean\" is also a linear function of \"acoustic_data_std\".\n\nLet's use a simple linear regressor to predict the \"time_to_failure_mean\" from the smooth version of \"acoustic_data_std\".","0ae1c77d":"This kernel aims at finding a solution that makes sense by using basic models and features.\nI find it useful to understand roughly how the 'acoustic_data' signal is behaving.\nWe are using here the standard deviation of 'acoustic_data' as the only predictive feature.","e08d1264":"# Predict time of earthquake with only one feature","dbfc8296":"We plot the STD of 'acoustic_data' over time, the 3 red vertical lines correspond to the 3 first earthquakes.\nAt first glance, you can see that the (smoothed) STD of 'acoustic_data' approximately increases linearly until the time at which the earthquakes happen.","6d2e4df4":"We smooth out a bit the STD of \"acoustic_data\" using rolling mean.","09d52792":"As the submission consists of samples of 150,000 data points, we divide the train set into samples of 150,000 data points as well. \nFor each of these small samples, we calculate the standard deviation of the input data \"acoustic_data\".\nLet's assume that the \"time_to_failure\" can be considered as nearly constant within 150,000 data points, we calculate the mean of \"time_to_failure\" for each of these smaller samples.","e9770780":"We delete the original training dataset to free up some memory."}}