{"cell_type":{"6472b561":"code","dcf0925f":"code","072d6c98":"code","432c5c05":"code","57c51c44":"code","2ca638ef":"markdown"},"source":{"6472b561":"from optiver_features import generate_test_df\nfrom fastai.tabular.all import *","dcf0925f":"test_df = generate_test_df()\ntrain_df = pd.read_csv('..\/input\/optiver-train-features\/train_with_features.csv')","072d6c98":"def pred_tabular_nn(train_df, test_df):\n    train_df = train_df.drop(['time_id', 'row_id'], axis=1).fillna(0)\n    train_df.stock_id = train_df.stock_id.astype('category')\n    cont_nn,cat_nn = cont_cat_split(train_df,  dep_var='target')\n    dls = TabularPandas(train_df, [Categorify, Normalize], cat_nn, cont_nn, y_names='target').dataloaders(2048)\n    test_dl = dls.test_dl(test_df.fillna(0))\n    learn = tabular_learner(dls, y_range=(0,.1), layers=[1000,500,200], n_out=1, path = '..\/input\/optiver-models\/')\n    res = torch.zeros(len(test_df))\n    for idx in range(5):\n        learn.load(f'nn_fold{idx}')\n        preds, _ = learn.get_preds(dl=test_dl)\n        res += preds.squeeze() \/ 5\n    return res.numpy()","432c5c05":"def pred_lgb(test_df):\n    test_df = test_df.drop(['row_id', 'time_id'], axis=1)\n    res = np.zeros(len(test_df))\n    for idx in range(10):\n        filename = f'..\/input\/optiver-models\/models\/lgb_fold{idx}.pickle'\n        model = pickle.load(open(filename, 'rb'))\n        preds = model.predict(test_df)\n        res += preds \/ 10\n    return res","57c51c44":"nn_preds = pred_tabular_nn(train_df, test_df)\nlgb_preds = pred_lgb(test_df)\nrate = 0.570\ntest_df['target']=(1-rate) * lgb_preds + rate * nn_preds\ntest_df[['row_id', 'target']].to_csv('submission.csv', index =False)\npd.read_csv('submission.csv').head()","2ca638ef":"This notebook doesn't do anything novel in terms of features, nor architecture. Instead I want to show how you can structure your code and data to run experiments in a fast and concise manner. \n\nThe preprocessing code and LGB models were taken from [here](https:\/\/www.kaggle.com\/tatudoug\/stock-embedding-ffnn-features-of-the-best-lgbm) and based on [this](https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline)\n\nThis is how it works:\n- The training set with features is cached and loaded from https:\/\/www.kaggle.com\/slawekbiel\/optiver-train-features\n- The code to generate those features is saved in an Utility Script: https:\/\/www.kaggle.com\/slawekbiel\/optiver-features and used to process the test data.\n- fast.ai library handles defining the NN model and preparing the data for it (normalization, embeddings, batching etc)\n- Both fastai nad LGB models are trained locally, serialized and then pushed to the dataset: https:\/\/www.kaggle.com\/slawekbiel\/optiver-models"}}