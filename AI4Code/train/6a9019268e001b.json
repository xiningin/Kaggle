{"cell_type":{"a823b41f":"code","ef555c07":"code","26a861bf":"code","2c973d76":"code","dcad570c":"code","1d63b30d":"code","70bb3adb":"code","050a9e31":"code","7c67d76b":"code","501dee03":"code","ed947157":"code","43d7f5ed":"code","24e7ce96":"code","76a92101":"code","224cf7f1":"code","1a2831e7":"code","4a999db0":"code","c307d715":"code","84846f67":"markdown","aadaab0a":"markdown","04ad8fa0":"markdown","2d8705cb":"markdown","84cb1b28":"markdown","594ae79d":"markdown"},"source":{"a823b41f":"import os\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n\nfrom sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.feature_selection import RFE\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display","ef555c07":"Datasets = dict()\nfor ds in ['train', 'test']:\n    dataset = pd.read_csv(f\"..\/input\/{ds}.csv.zip\", sep=',', header=0,\n                          names=['Store', 'Dept', 'Date', 'weeklySales', 'isHoliday'] if ds=='train'\n                           else ['Store', 'Dept', 'Date', 'isHoliday'])\n    features = pd.read_csv(\"..\/input\/features.csv.zip\", sep=',', header=0,\n                           names=['Store', 'Date', 'Temperature', 'Fuel_Price', \n                                  'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', \n                                  'CPI', 'Unemployment', 'IsHoliday']).drop(columns=['IsHoliday'])\n    stores = pd.read_csv(\"..\/input\/stores.csv\", names=['Store', 'Type', 'Size'], sep=',', header=0)\n    dataset = dataset.merge(stores, how='left').merge(features, how='left')\n\n    dataset['Date'] = pd.to_datetime(dataset['Date'])\n    # dataset[\"isTomorrowHoliday\"] = dataset[\"isHoliday\"].shift(-1).fillna(False)\n    display(dataset.head())\n    \n    Datasets[ds] = dataset","26a861bf":"Datasets['train'].dtypes","2c973d76":"def describe_missing_values(df: pd.DataFrame):\n    miss_val = df.isnull().sum()\n    miss_val_percent = 100 * df.isnull().sum() \/ len(df)\n    miss_val_table = pd.concat([miss_val, miss_val_percent], axis=1)\n    miss_val_table_ren_columns = miss_val_table.rename(\n        columns = {0: 'Missing Values', \n                   1: '% of Total Values',}\n    )\n    miss_val_table_ren_columns = miss_val_table_ren_columns[\n        miss_val_table_ren_columns.iloc[:,1] != 0\n    ].sort_values('% of Total Values', ascending=False).round(1)\n    \n    print(f\"Dataframe has {df.shape[1]} columns,\")\n    print(f\"\\t\\t {miss_val_table_ren_columns.shape[0]} columns that have missing values.\")\n\n    return miss_val_table_ren_columns\n\n\ndef visualize_distribution_of_missing_values(df: pd.DataFrame):\n    df_nan_check = df.isna().sum().sort_values()\n    df_nan_check = df_nan_check.to_dict()\n    df_not_nan = []\n\n    nan_cols = 0\n\n    for key, value in df_nan_check.items():\n        df_nan_check[key] = int(value\/len(df)*100)\n        if df_nan_check[key] >= 80:\n            nan_cols += 1\n        else:\n            df_not_nan.append(key)\n\n    # Visualize\n    plt.figure(figsize=(9, 6))\n    plt.suptitle('Distribution of Empty Values', fontsize=19)\n    plt.bar(df_nan_check.keys(), df_nan_check.values())\n    plt.xticks(rotation=69)\n    plt.show()\n    \n\nfor ds in ['train', 'test']:\n    print(f'\\n\\n{ds}-set:')\n    print(describe_missing_values(Datasets[ds]))\n    # visualize_distribution_of_missing_values(dataset)","dcad570c":"def scatter(dataset, column):\n    plt.figure()\n    plt.scatter(dataset[column] , dataset['weeklySales'], alpha=0.169)\n    plt.ylabel('weeklySales')\n    plt.xlabel(column)","1d63b30d":"for col in ['Fuel_Price', 'Size', 'CPI', 'Type', 'isHoliday', 'Unemployment', 'Temperature', 'Store', 'Dept']:\n    scatter(Datasets['train'], col)","70bb3adb":"fig = plt.figure(figsize=(18, 14))\ncorr = Datasets['train'].corr()\nc = plt.pcolor(corr)\nplt.yticks(np.arange(0.5, len(corr.index), 1), corr.index)\nplt.xticks(np.arange(0.5, len(corr.columns), 1), corr.columns, rotation=45)\nfig.colorbar(c)","050a9e31":"for ds in Datasets.keys():\n    # make holidays more specific\n    Datasets[ds]['Holiday_Type'] = None\n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==2), 'Holiday_Type'] = 'Super_Bowl'\n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==9), 'Holiday_Type'] = 'Labor_Day'\n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==11), 'Holiday_Type'] = 'Thanksgiving' \n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==12), 'Holiday_Type'] = 'Christmax'\n    Datasets[ds].drop(columns=['isHoliday'], inplace=True)\n    \n    # 1-hot encoding for categorical features\n    Datasets[ds] = pd.get_dummies(Datasets[ds], columns=[\"Type\", \"Holiday_Type\"])\n    \n    # data imputation\n    Datasets[ds].fillna(value=0, inplace=True)\n    display(Datasets[ds].head())","7c67d76b":"Datasets['train'].Store.value_counts(sort=False)","501dee03":"STORE_ID = 44","ed947157":"holidays = pd.DataFrame({\n    'holiday': ['Super_Bowl']*4 + ['Labor_Day']*4 + ['Thanksgiving']*4 + ['Christmas']*4,\n    'ds': pd.to_datetime(['12-02-2010', '11-02-2011', '10-02-2012', '08-02-2013',\n                          '10-09-2010', '09-09-2011', '07-09-2012', '06-09-2013',\n                          '26-10-2010', '25-10-2011', '23-10-2012', '29-10-2013',\n                          '31-12-2010', '30-12-2011', '28-12-2012', '27-12-2013',]),\n    'lower_window': 0,\n    'upper_window': 1,\n})","43d7f5ed":"data_train, data_test = Datasets['train'].copy(), Datasets['test'].copy()\ndata_train.rename(columns={'Date': 'ds', 'weeklySales': 'y'}, inplace=True)\ndata_train.head()","24e7ce96":"from fbprophet import Prophet\n\nModels = dict()\n\nfor name, group in data_train.groupby([\"Store\", \"Dept\"]):\n    \n    if name[0] != STORE_ID:\n        continue\n        \n    data_grouped = group.drop(columns=[\"Store\", \"Dept\"])\n    print(f\"\\n Training Facebook's Prophet for store={name[0]}, dept={name[1]} with {len(data_grouped)} samples ...\")\n    if len(data_grouped) < 3:\n        print(f\"\\t\\t Number of samples must be larger than 2 !!!\")\n        Models[name] = [None, np.mean(group['y'])]\n        continue\n    \n    # Creating model\n    model = Prophet(\n        growth='linear', # linear or logistic\n        changepoints=None, # list of dates at which to include potential changepoints\n        n_changepoints=11, # number of potential changepoints\n        changepoint_range=0.69, # proportion of history in which trend changepoints will be estimated\n        yearly_seasonality='auto',\n        weekly_seasonality='auto',\n        daily_seasonality='auto',\n        holidays=holidays,\n        seasonality_mode='additive',\n        seasonality_prior_scale=6.9,\n        holidays_prior_scale=6.9,\n        changepoint_prior_scale=0.169,\n        mcmc_samples=0, # if > 0: Bayesian inference with number of MCMC samples, else: MAP estimation\n        interval_width=0.69, # width of the uncertainty intervals provided for the forecast\n        uncertainty_samples=690 # number of simulated draws used to estimate uncertainty intervals\n    )\n\n    for col in ['Size', 'CPI', 'Unemployment']:\n        model.add_regressor(name=col, prior_scale=None, standardize='auto', mode='additive')\n        \n    # Training model        \n    t1 = time.time()\n    model.fit(data_grouped)\n    t2 = time.time()\n    print(f\"\\t\\t ... in {round(t2-t1, 3)} seconds\")\n\n    Models[name] = [model, np.mean(group['y'])]","76a92101":"data_test.rename(columns={'Date': 'ds'}, inplace=True)\nfor col in data_train.columns:\n    if col in ['ds', 'y']:\n        continue\n    if col not in list(data_test.columns):\n        data_test[col] = 0\n\nresult = []\nfor name, group in data_test.groupby([\"Store\", \"Dept\"]):\n\n    if name[0] != STORE_ID:\n        continue\n        \n    data_grouped = group.drop(columns=[\"Store\", \"Dept\"])\n    print(f\"\\n Predicting Facebook's Prophet for store={name[0]}, dept={name[1]} with {len(data_grouped)} samples ...\")\n    if name not in list(Models.keys()):\n        forecast = group.copy()\n        forecast['yhat'] = np.mean(data_train.y)\n    else:\n        model, mean_value = Models[name]\n        if model is None:\n            forecast = group.copy()\n            forecast['yhat'] = mean_value\n        else:\n            try:\n                t1 = time.time()\n                forecast = model.predict(df=data_grouped)\n                t2 = time.time()\n                print(f\"\\t ... in {round(t2-t1, 3)} seconds\")\n                forecast[\"Store\"] = name[0]\n                forecast[\"Dept\"] = name[1]\n            except Exception as e:\n                print(e)\n                forecast = group.copy()\n                forecast['yhat'] = mean_value\n            \n    # 1st-date: Friday, 05-02-2010\n    # Models[name].plot_components(forecast, weekly_start=5, yearly_start=31+5)\n    forecast['yhat'][forecast['yhat']<0] = 20\n    result.append(forecast[['Store', 'Dept', 'ds', 'yhat']])","224cf7f1":"result = pd.concat(result, axis=0, ignore_index=True)\nresult.rename(columns={'yhat': 'Weekly_Sales'}, inplace=True)\nresult['Id'] = result['Store'].apply(str) + '_' + result['Dept'].apply(str) + '_' + result['ds'].dt.strftime('%Y-%m-%d')\n\nresult[['Id', 'Weekly_Sales']].to_csv(f'submission_Prophet_store={STORE_ID}.csv', index=False)\n\ndisplay(result.head())","1a2831e7":"data_train, data_test = Datasets['train'].copy(), Datasets['test'].copy()\n\ndata_train.set_index(keys='Date', drop=True, inplace=True)\ndata_test.set_index(keys='Date', drop=True, inplace=True)\n\nfor col in data_train.columns:\n    if col in ['weeklySales']:\n        continue\n    if col not in list(data_test.columns):\n        data_test[col] = 0\n\ndisplay(data_train[(data_train.Store==1) & (data_train.Dept==1)].tail())\ndisplay(data_test[(data_test.Store==1) & (data_test.Dept==1)].head())","4a999db0":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nsteps = -1\nModels = dict()\nresults = list()\n\nfor name, group_test in data_test.groupby([\"Store\", \"Dept\"]):\n        \n    if name[0] != STORE_ID:\n        continue\n    \n    print(f\"\\n SARIMAX for store={name[0]}, dept={name[1]}\")\n    \n    test_size = len(group_test)\n    if test_size < 1:\n        continue\n    \n    # preparing data\n    group = data_train[(data_train[\"Store\"]==name[0]) & (data_train[\"Dept\"]==name[1])]\n    train_size = len(group)\n    if train_size < 1:\n        predictions = group_test.copy()\n        predictions['Weekly_Sales'] = np.mean(data_train['weeklySales'])\n    else:\n        data_grouped = group.copy()\n        data_grouped.drop(columns=[\"Store\", \"Dept\"], inplace=True)\n        data_grouped.index = pd.DatetimeIndex(data=data_grouped.index.values,\n                                              freq=data_grouped.index.inferred_freq)\n        features_imposed = data_grouped['weeklySales'].shift(steps)\n        features_exposed = data_grouped.drop(columns=['weeklySales'])\n        seasonal_order = 52 if train_size>52 else 1\n\n        try:\n            # Creating model\n            model = SARIMAX(endog=features_imposed, \n                            exog=features_exposed, \n                            order=(1, 0, 0), # p,d,q - number of AR parameters, differences, and MA parameters\n                            seasonal_order=(0, 1, 0, seasonal_order), # P,D,Q,s - AR parameters, differences, MA parameters, and periodicity\n                            trend='ct', # c: const - t: time\n                            enforce_invertibility=False, \n                            enforce_stationarity=False)\n\n            # Training model\n            print(f\"\\t Training with {train_size} samples ...\")\n            t1 = time.time()\n            forecaster = model.fit()\n            t2 = time.time()\n            print(f\"\\t\\t ... in {round(t2-t1, 3)} seconds\")\n            # print(forecaster.summary())\n\n            # Models[name] = forecaster\n\n            # Predicting\n            features = group_test.copy()\n            features.drop(columns=[\"Store\", \"Dept\"], inplace=True)\n            features.index = pd.DatetimeIndex(data=features.index.values,\n                                              freq=features.index.inferred_freq)\n\n            print(f\"\\t Predicting with {test_size} samples ...\")\n            t1 = time.time()\n            predictions = forecaster.predict(start=train_size, \n                                             end=train_size+test_size-1, \n                                             exog=features)\n            t2 = time.time()\n            print(f\"\\t\\t ... in {round(t2-t1, 3)} seconds\")\n\n            predictions = predictions.to_frame(name='Weekly_Sales')\n            predictions[\"Store\"] = name[0]\n            predictions[\"Dept\"] = name[1]\n\n        except Exception as e:\n            print(e)\n            predictions = group_test.copy()\n            predictions['Weekly_Sales'] = np.mean(group['weeklySales'])\n               \n    predictions = predictions[[\"Store\", \"Dept\", 'Weekly_Sales']]\n    predictions.reset_index(inplace=True)\n    predictions['Date'] = group_test.index\n    predictions['Weekly_Sales'][predictions['Weekly_Sales']<0] = np.mean(group['weeklySales'])\n    \n    assert len(predictions)==test_size, f\"[xxx] #predictions = {len(predictions)} != {test_size}\"\n    results.append(predictions)","c307d715":"result = pd.concat(results, axis=0, ignore_index=True)\n# result.to_csv('test.csv', index=False)\nresult['Date'] = pd.to_datetime(result.Date, format='%Y-%m-%d %H:%M:%S')\nresult['Id'] = result['Store'].astype(int).apply(str) + '_' + result['Dept'].astype(int).apply(str) + '_' + result['Date'].dt.strftime('%Y-%m-%d')\nresult[['Id', 'Weekly_Sales']].to_csv(f'submission_SARIMAX_store={STORE_ID}.csv', index=False)\n\ndisplay(result.head())","84846f67":"# **Modeling**","aadaab0a":"# **Data Manipulation**","04ad8fa0":"## **SARIMAX** - **S**easonal **A**uto**R**egressive **I**ntegrated **M**oving **A**verage with e**X**ogenous regressors","2d8705cb":"# **Data Loading**","84cb1b28":"## Facebook Prophet","594ae79d":"# **Data Exploration**"}}