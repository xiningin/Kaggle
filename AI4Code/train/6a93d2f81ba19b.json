{"cell_type":{"7fa55344":"code","9e3ef44b":"code","412e69a2":"code","219fbdf7":"code","be358ece":"code","71ebbdba":"code","85b9ab8e":"code","5d52f78e":"code","fe49e751":"code","42915999":"code","9d9a986c":"code","c1b8a101":"code","a201577a":"code","6e3c3af6":"code","db087aea":"code","0462ed34":"code","23ea922f":"code","e8beb45b":"code","b67d3df3":"code","5d1b72c1":"code","1d76d3fc":"markdown","3410077d":"markdown","143f0cf4":"markdown","fcdf3019":"markdown","17250208":"markdown","a4a64fc7":"markdown","42a3ca7a":"markdown","cc3e25c7":"markdown","e6849e33":"markdown","ecb8509b":"markdown","356b79f6":"markdown"},"source":{"7fa55344":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e3ef44b":"df = pd.read_csv(\"..\/input\/szeged-weather\/weatherHistory.csv\")\ndf.head(3)","412e69a2":"df.isnull().sum()","219fbdf7":"df.fillna(method=\"bfill\",inplace=True)\ndf.isnull().sum()","be358ece":"df.shape","71ebbdba":"df.describe().T","85b9ab8e":"df.corr() ","5d52f78e":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf[\"Summary\"] = le.fit_transform(df[\"Summary\"])\n# Converting \"Summary\" variable to numeric values.","fe49e751":"df.head(3)","42915999":"df[\"Formatted Date\"] = pd.to_datetime(df[\"Formatted Date\"], format = \"%Y-%m-%d %H:%M:%S.%f %z\") \n#It seems that the \"Formatted Date\" variable is in the form of a date. \n#Therefore, we will do the separation of the date as day, month, year with the \"to_datetime\" method in the \"pandas\" module.","9d9a986c":"df[\"year\"] = df[\"Formatted Date\"].apply(lambda x: x.year)\ndf[\"month\"] = df[\"Formatted Date\"].apply(lambda x: x.month)\ndf[\"day\"] = df[\"Formatted Date\"].apply(lambda x: x.day)\n#We assign values \u200b\u200bto new variables.","c1b8a101":"dms = pd.get_dummies(df[\"Precip Type\"])\ndf = pd.concat([df,dms[[\"rain\",\"snow\"]]],axis=1)\ndf.drop([\"Formatted Date\",\"Summary\",\"Daily Summary\",\"Precip Type\",\"Loud Cover\",\"Apparent Temperature (C)\"],axis=1,inplace=True)","a201577a":"df.rename(columns={\"Temperature (C)\": \"temperature\",\"Humidity\":\"humidity\",\"Wind Speed (km\/h)\":\"wind_speed\",\n                   \"Wind Bearing (degrees)\":\"wind_bearing\",\"Visibility (km)\":\"visibility\",\"Pressure (millibars)\":\"pressure\"},inplace=True)\ndf.head()","6e3c3af6":"x = df.drop(\"temperature\",axis=1)\ny= df[\"temperature\"]\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler().fit(x)\nx = pd.DataFrame(sc.transform(x))\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=42)\nx.head()","db087aea":"from sklearn.linear_model import Ridge,Lasso,ElasticNet,RidgeCV,LassoCV,ElasticNetCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nridge_model = Ridge().fit(x_train,y_train)\ny_pred = ridge_model.predict(x_test)\nprint(\"Ridge Regression Test R2 Score : %\",r2_score(y_test, y_pred)*100)\nlasso_model = Lasso().fit(x_train,y_train)\ny_pred = lasso_model.predict(x_test)\nprint(\"Lasso Regression Test R2 Score : %\",r2_score(y_test, y_pred)*100)\nelastic_model = ElasticNet().fit(x_train,y_train)\ny_pred = elastic_model.predict(x_test)\nprint(\"ElasticNet Regression Test R2 Score : %\",r2_score(y_test, y_pred)*100)","0462ed34":"ridge_model = Ridge().fit(x_train,y_train)\ny_pred = ridge_model.predict(x_train)\nprint(\"Ridge Regression Train R2 Score : %\",r2_score(y_train, y_pred)*100)\nlasso_model = Lasso().fit(x_train,y_train)\ny_pred = lasso_model.predict(x_train)\nprint(\"Lasso Regression Train R2 Score : %\",r2_score(y_train, y_pred)*100)\nelastic_model = ElasticNet().fit(x_train,y_train)\ny_pred = elastic_model.predict(x_train)\nprint(\"ElasticNet Regression Train R2 Score : %\",r2_score(y_train, y_pred)*100)","23ea922f":"lamdbalar = 10**np.linspace(10,-2,100)*0.5","e8beb45b":"ridge_cv = RidgeCV(alphas = lamdbalar).fit(x_train,y_train)\nlasso_cv = LassoCV(alphas = lamdbalar).fit(x_train,y_train)\nelastic_cv = ElasticNetCV(alphas = lamdbalar).fit(x_train,y_train)","b67d3df3":"print(ridge_cv.alpha_)\nprint(lasso_cv.alpha_)\nprint(elastic_cv.alpha_)","5d1b72c1":"ridge_tuned = Ridge(alpha=ridge_cv.alpha_).fit(x_train,y_train)\ny_pred = ridge_tuned.predict(x_test)\nprint(\"Tuned Ridge Regression R2 Score : %\",r2_score(y_test, y_pred)*100)\n\nlasso_tuned = Lasso(alpha=lasso_cv.alpha_).fit(x_train,y_train)\ny_pred = lasso_tuned.predict(x_test)\nprint(\"Tuned Lasso Regression R2 Score : %\",r2_score(y_test, y_pred)*100)\n\nelastic_tuned = ElasticNet(alpha=elastic_cv.alpha_).fit(x_train,y_train)\ny_pred = elastic_tuned.predict(x_test)\nprint(\"Tuned ElasticNet Regression R2 Score : %\",r2_score(y_test, y_pred)*100)","1d76d3fc":"In Ridge, Lasso and ElasticNet, the \"alphas\" parameter is valuable. So we try all of these parameters and use whichever is best.","3410077d":"## Train Accuracy","143f0cf4":"# Modelling","fcdf3019":"# Missing Value Analysis","17250208":"The \"Precip Type\" variable is a categorical variable. In order for us to put this variable in our model, it should contain numerical values. To do this, we need the \"get_dummies\" method. This method creates new variables by converting categorical variable data to numeric values. We add these new variables to the data set. We discard the transformed old categorical variable from the data set. In addition, we remove variables that will not work for us while modeling from the dataset.","a4a64fc7":"In this dataset we will estimate the \"Temperature\" variable. In the correlation table, we see that the \"Apparent Temperature (C)\" variable has a very close correlation value to our dependent variable. So we will drop this variable from the dataset later.","42a3ca7a":"After selecting the dependent and independent variables, we scale the arguments. Then we divide our model into 2 different datasets: \"train\" set to train and \"test\" to measure success. We do this division to be 25% to the \"test\" data.","cc3e25c7":"## Test Accuracy","e6849e33":"I chose to change the variable names to make it more useful.","ecb8509b":"# Model Tuning","356b79f6":"# Categorical Variables"}}