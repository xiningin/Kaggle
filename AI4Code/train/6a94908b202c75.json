{"cell_type":{"7c411b41":"code","bcc64550":"code","c0fa6139":"code","0744fe95":"code","d6574270":"code","d5b74626":"code","614228f0":"code","52787296":"code","28d0f410":"code","40c6fff5":"code","379a0054":"code","4dc9866a":"code","3ac44ec9":"code","6bdb8609":"code","597cc002":"code","29f434eb":"code","8c6171f9":"code","81e565a7":"code","18b1a5df":"code","8db7baba":"code","a43118cf":"code","31fc7368":"code","1bb55d6f":"code","dfa0f417":"code","6a1444e7":"code","c34f05ad":"code","e5cb6808":"code","329ec7ca":"code","0a0391af":"code","7344de8f":"code","cd2d74c0":"code","a0923bee":"code","d39a6b76":"code","522f6d59":"code","58c301d2":"code","c8c5e1a9":"code","45a76fa2":"code","906c8671":"code","b7228170":"code","80d629c3":"code","7cabb62b":"code","2c0b7050":"code","f164fec9":"code","06b10a67":"code","acd14188":"code","6ed3dd76":"code","f1b1da66":"code","8b3d9652":"code","da6bc124":"code","81381c09":"code","d6dbbed0":"code","3f45ff8b":"code","605c5d68":"code","4648f025":"code","b548a2a1":"code","cc66038c":"code","98b19f50":"code","435a2de2":"code","04e9d873":"code","74ad3320":"code","13e6cdac":"code","69af59fe":"code","65de33e9":"code","948466a4":"code","38f2be51":"code","746f8dfe":"code","3d3a6661":"code","c9974cf8":"code","27e1d548":"code","f090d525":"code","270b57de":"code","2b83e9f5":"code","623cd67b":"code","eb50982e":"code","15e28357":"code","62299935":"code","4a787e43":"code","316a45ca":"code","72cf96b7":"code","fdad1479":"code","0455e2d2":"code","2d60d792":"markdown","9043c56b":"markdown","302711f9":"markdown","07c5ccf6":"markdown","904ab1a2":"markdown","28d16c46":"markdown","0432af54":"markdown","55c97508":"markdown","696f6092":"markdown","f6e3a09b":"markdown","3a0e4b00":"markdown","8d480cd2":"markdown","52fe43ee":"markdown","27f54e2e":"markdown","8d43ff27":"markdown","45585334":"markdown","1fd6b701":"markdown","a92edf1b":"markdown","7ca6c004":"markdown","4a51d0aa":"markdown","4c95a899":"markdown","91e0030b":"markdown","a58c6651":"markdown","26c01588":"markdown","d03fceef":"markdown","3c078f88":"markdown","bc5a38f2":"markdown","eea6264b":"markdown","6c2287dd":"markdown","0ceba83a":"markdown","ed234069":"markdown","1c4beb23":"markdown","b2d84ba2":"markdown","2a5a4bfd":"markdown","c3d324b3":"markdown","d4225a30":"markdown","68daf9c1":"markdown","162fda8a":"markdown","fb6819c2":"markdown","4d183f30":"markdown","ae7b33de":"markdown","1cad57af":"markdown","b24f21c9":"markdown","03579ea0":"markdown","4117f11d":"markdown","4a4cca26":"markdown","815e9c31":"markdown","198d9b8e":"markdown","52a6325f":"markdown","073d3ae6":"markdown","39ed8144":"markdown"},"source":{"7c411b41":"import numpy as np\nfrom numpy import savetxt \nimport pandas as pd \nimport re\nimport gc\nimport random\nimport os\nimport tensorflow as tf\n\nimport torch\nimport transformers\nimport spacy\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport codecs\nfrom gensim.models import Word2Vec\n\nimport nltk\nfrom nltk.corpus import stopwords as nltk_stopwords\nfrom gensim.models import Word2Vec\nfrom tqdm import notebook\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.simplefilter('ignore')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import plot_confusion_matrix, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedShuffleSplit\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelEncoder, Binarizer, OneHotEncoder\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.gaussian_process.kernels import RBF\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nimport lightgbm as lgb  \n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback","bcc64550":"torch.cuda.is_available()","c0fa6139":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsamp_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","0744fe95":"RND_ST = 2202","d6574270":"train.info()","d5b74626":"train.head()","614228f0":"train['keyword'].unique()[:5]","52787296":"train['keyword'].value_counts()","28d0f410":"len(train['keyword'].unique())","40c6fff5":"train.groupby('keyword')['keyword'].count().head(20)","379a0054":"def space_code_removing(df):\n    \n    for i in range(df.shape[0]):\n        df.loc[i, 'keyword'] = re.sub(r'%20', ' ', str(df.loc[i, 'keyword']))","4dc9866a":"space_code_removing(train)\nspace_code_removing(test)","3ac44ec9":"train['keyword'].unique()[:5]","6bdb8609":"def lemmatize(df):\n    \n    lemmatizer = spacy.load('en_core_web_sm')\n    \n    for i in range(df.shape[0]):\n        lemma = lemmatizer(str(df.loc[i, 'keyword']))\n        df.loc[i, 'keyword_lemma'] = \" \".join([token.lemma_ for token in lemma])","597cc002":"topics = (train.groupby('keyword')['target']\n        .agg(['count','sum'])\n        .reset_index()\n        .sort_values(by='count', ascending=False))\n        \ntopics['fake'] = topics['count'] - topics['sum']\n        \ntopics.rename(columns={'count':'total', 'sum':'true'}, inplace=True)\n\ntopics","29f434eb":"lemmatize(topics)\n\nlen(topics['keyword_lemma'].unique())","8c6171f9":"topics.head()","81e565a7":"topics = topics.drop('keyword', axis=1)\n\ntopics = topics.groupby('keyword_lemma')[['total','true','fake']].sum().reset_index()\n\ntopics['true_prcntg'] = (topics['true'] * 100 \/ topics['total']).round(2)\ntopics['fake_prcntg'] = (100 - topics['true_prcntg'])\n\ntopics","18b1a5df":"real_topics = topics[['keyword_lemma','true_prcntg','total']].sort_values(by='true_prcntg', ascending=False).head(10)\nreal_topics","8db7baba":"fake_topics = topics[['keyword_lemma','fake_prcntg','total']].sort_values(by='fake_prcntg', ascending=False).head(10)\nfake_topics","a43118cf":"sns.set_style('whitegrid')\n\nfig, axes = plt.subplots(1, 2, figsize=(17,5))\n\nsns.barplot(x='true_prcntg', y='keyword_lemma', data=real_topics, color='royalblue', ax=axes[0])\nsns.barplot(x='fake_prcntg', y='keyword_lemma', data=fake_topics, color='salmon', ax=axes[1])\n\naxes[1].set_ylabel('')\naxes[0].set_xlabel('Real news percentage')\naxes[1].set_xlabel('Fake news percentage')\n\nplt.suptitle('Top 10 Real and Fake news topics in train dataset', size=18);","31fc7368":"cntrv_topics = topics[['keyword_lemma','true_prcntg']].query('48 <= true_prcntg <= 52').sort_values(by='true_prcntg', ascending=False)","1bb55d6f":"cntrv_topics","dfa0f417":"temp = pd.DataFrame(train['target'].value_counts())\nname = pd.Series(['real', 'fake'], name='name')\ntemp = temp.join(name)\ntemp\n\nfig, ax = plt.subplots(figsize=(6,6))\nax.vlines(x=temp.name, ymin=0, ymax=temp['target'], color='dimgrey', alpha=0.85, linewidth=2)\nax.scatter(x=temp.name, y=temp['target'], s=75, color='firebrick', alpha=0.85)\n\nfor row in temp.itertuples():\n    ax.text(row.Index, row.target+100, s=row.target, \n            horizontalalignment= 'center', verticalalignment='bottom', fontsize=10)\n\nax.set_title('Fake and real tweets in train dataset', size=15, y=(1.02))\nax.set_ylabel('Tweets')\nax.set_ylim(0, 5000)\n\n#plt.tight_layout\nplt.show()","6a1444e7":"for i in range(50):\n    print(train.loc[i, 'text'])\n    print()","c34f05ad":"def text_processing(df):\n    \n    text = df['text'].values\n    \n    documents = []\n\n    lemmatizer = spacy.load('en_core_web_sm')\n    \n    df_new = df.copy()\n\n    for sen in range(0, len(text)):\n        # remove special symbols\n        document = re.sub(r'\\W', ' ', str(text[sen]))\n    \n        # remove individual symbols\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n    \n        # remove individual symbols from the start of the tweet\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n    \n        # replace few spaces to a single one\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n    \n        # remove 'b'\n        document = re.sub(r'^b\\s+', '', document)\n    \n        # convert all letters to a lower case\n        document = document.lower()\n    \n        # spacy lemmarization\n        lemma = lemmatizer(str(document))\n        document = \" \".join([token.lemma_ for token in lemma])\n        \n        # remove spacy pronouns lemmas\n        #document = re.sub(r'-PRON-', '', document)\n        \n        df_new.loc[sen, 'text_lemm'] = document\n        \n    return df_new","e5cb6808":"def text_processing_02(df):\n    \n    text = df['text'].values\n    \n    df_new = df.copy()\n\n    for sen in range(0, len(text)):\n      \n        ## removing part\n        \n        # remove hyperlinks\n        document = re.sub(r'http\\S+', '', str(text[sen]))\n        # remove hashtags\n        document = re.sub(r'#\\S+', ' ', document)\n        # remove special symbols\n        document = re.sub(r'\\W', ' ', document)\n        # remove individual symbols\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n        # remove individual symbols from the start of the tweet\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n        # replace few spaces to a single one\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n        # remove 'b'\n        document = re.sub(r'^b\\s+', '', document)\n        #remove \u00fb\u00f3\n        document = re.sub(r'\u00fb\u00f3', '', document)\n        \n        # convert all letters to a lower case\n        document = document.lower()\n        \n        ## replacing part\n        document = re.sub(r'hwy', 'highway', document)\n        document = re.sub(r'nsfw', 'not safe for work', document)\n        \n        \n        df_new.loc[sen, 'text_lemm'] = document\n        \n    return df_new","329ec7ca":"temp = train.iloc[46:49].reset_index()\n\ntemp_prep = text_processing_02(temp)\n\ntemp_prep","0a0391af":"%%time\ntrain_lemm = text_processing_02(train)","7344de8f":"for i in range(50):\n    print(train_lemm.loc[i, 'text_lemm'])\n    print()","cd2d74c0":"%%time\ntest_lemm = text_processing_02(test)","a0923bee":"train_lemm = train_lemm.drop(['id','keyword','location','text'], axis=1)\ntest_lemm = test_lemm.drop(['id','keyword','location','text'], axis=1)","d39a6b76":"X_train = train_lemm['text_lemm']\ny_train = train_lemm['target']\n\nX_test = test_lemm['text_lemm']","522f6d59":"X_train.head()","58c301d2":"#tfidfconverter = TfidfVectorizer(max_features=1000, \n                                 #min_df=3, max_df=0.5, \n                                 #ngram_range=(1,1),\n                                 #stop_words=STOP_WORDS)\n\n#tfidfconverter.fit(X_train)","c8c5e1a9":"#X_train_tf = tfidfconverter.transform(X_train)\n#X_test_tf = tfidfconverter.transform(X_test)","45a76fa2":"def grid_search(model, params, features, target):\n    \n    search = GridSearchCV(model, params, verbose=1, cv=3, scoring='f1', n_jobs=-1)\n    search.fit(features, target)\n    \n    print(search.best_score_)\n    print(search.best_params_)  ","906c8671":"def cross_val(model, feat, target):\n    \n    cvs = cross_val_score(model, feat, target, cv=5, scoring='f1').mean()\n    \n    return(cvs)","b7228170":"sgd = SGDClassifier(random_state=RND_ST)\n\nsgd_params = dict(alpha=[1e-03, 1e-04, 1e-05, 1e-06],\n                  penalty=['l1','l2'], \n                  tol=[1e-03, 1e-04, 1e-05])","80d629c3":"#grid_search(sgd, sgd_params, X_train_tf, y_train)","7cabb62b":"#sgd = SGDClassifier(alpha=0.0001, penalty='l2', tol=0.001,  random_state=RND_ST)#","2c0b7050":"#cross_val(sgd, X_train_tf, y_train)","f164fec9":"svm = LinearSVC(random_state = RND_ST)\n\nsvm_params = dict(C=[0.01,0.1,1,10,100],\n                  max_iter=[100,250,500,1000,2000])","06b10a67":"#grid_search(svm, svm_params, X_train_tf, y_train)","acd14188":"#cross_val(svm, X_train_tf, y_train)","6ed3dd76":"svm = LinearSVC(C=1, max_iter=1000, random_state=RND_ST)","f1b1da66":"train_bert = train[['target','text']]\ntest_bert = test[['text']]\n\ny_train = train['target']","8b3d9652":"### Downloading model and tokenizer\n\nmodel_class, tokenizer_class, pretrained_weights = (\n    transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","da6bc124":"#tokenized_train = train_bert['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n#tokenized_test = test_bert['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\ntokenized_train = X_train.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\ntokenized_test = X_test.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","81381c09":"def bert_features(tokenized):\n\n    max_len = 0\n    for i in tokenized.values:\n        if len(i) > max_len:\n            max_len = len(i)\n\n    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n    \n    attention_mask = np.where(padded != 0, 1, 0)\n    \n    batch_size = 1\n    embeddings = []\n    for i in notebook.tqdm(range(padded.shape[0] \/\/ batch_size)):\n            batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n            attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n        \n            with torch.no_grad():\n                batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n        \n            embeddings.append(batch_embeddings[0][:,0,:].numpy())\n    \n    features = np.concatenate(embeddings)\n    \n    return(features)","d6dbbed0":"X_train_bert = bert_features(tokenized_train) ","3f45ff8b":"X_test_bert = bert_features(tokenized_test)","605c5d68":"del tokenized_train, tokenized_test","4648f025":"#from numpy import savetxt \n#savetxt('\/kaggle\/working\/X_train_bert.csv', X_train_bert, delimiter=',')\n#savetxt('\/kaggle\/working\/X_test_bert.csv', X_test_bert, delimiter=',')","b548a2a1":"X_train_bert = np.loadtxt('\/kaggle\/input\/distilbert-preprocessed\/X_train_bert.csv', delimiter=',')\nX_test_bert = np.loadtxt('\/kaggle\/input\/distilbert-preprocessed\/X_test_bert.csv', delimiter=',')\n\ny_train = train['target']","cc66038c":"lr = LogisticRegression(random_state=RND_ST)\n\nlr_params = {'C': np.linspace(0.0001, 100, 20),\n             'max_iter':[50,100,200,500]}","98b19f50":"grid_search(lr, lr_params, X_train_bert, y_train)","435a2de2":"lr_final = LogisticRegression(C=15.789557894736841, max_iter=50, random_state=RND_ST)","04e9d873":"def cat_classifier(features, target):\n    \n    data = Pool(data = features, \n            label = target)\n    \n    scores = cv(data,\n            cbc_params,\n            fold_count=3, \n            plot=\"False\")","74ad3320":"cbc_params = dict(loss_function='Logloss',\n                    iterations=300,\n                    learning_rate=0.07,\n                    depth=4,\n                    subsample=0.7,\n                    verbose=100, \n                    random_state=RND_ST)","13e6cdac":"cat_classifier(X_train_bert, y_train)","69af59fe":"### learn: 0.1747454\ttest: 0.4294865\tbest: 0.4257702 (218)\ttotal: 1m 39s\tremaining: 0us\n\n### learn: 0.2388240\ttest: 0.4301056\tbest: 0.4288780 (350)\ttotal: 1m 36s\tremaining: 0us\n\n### learn: 0.2695160\ttest: 0.4249459\tbest: 0.4249238 (295)\ttotal: 1m 26s\tremaining: 0us","65de33e9":"cbc = CatBoostClassifier(loss_function='Logloss',\n                    iterations=400,\n                    learning_rate=0.09,\n                    depth=4,\n                    subsample=0.8,\n                    verbose=100, \n                    random_state=RND_ST)\n\n#cross_val(cbc, X_train_bert, y_train)","948466a4":"train_lgb = lgb.Dataset(X_train_bert, label=y_train, free_raw_data=False)\n\nlgb_param = {'num_leaves': 70, \n         'objective':'binary',\n         'min_data_in_leaf':23,\n         'max_depth':4,\n         'learning_rate':0.1,\n         'num_iterations':96,\n         'max_bin':3000,\n         'verbosity':0,\n         #'min_split_gain':90,\n         'random_state':RND_ST\n        }\n\n#NUM_ROUNDS = 500","38f2be51":"lgb_history = lgb.cv(params=lgb_param, \n                     train_set=train_lgb, \n                     metrics='cross_entropy', \n                     early_stopping_rounds=5)\n\nlen(lgb_history['cross_entropy-mean'])","746f8dfe":"lgb_history['cross_entropy-mean'][-1]","3d3a6661":"### best 0.4277043669978033","c9974cf8":"import tensorflow as tf\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam, SGD, RMSprop\n\nfrom keras.callbacks import EarlyStopping","27e1d548":"def plot_hist(history):\n\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()","f090d525":"optimizer = Adam(lr=0.0001)","270b57de":"optimizer = SGD(lr=0.0001)","2b83e9f5":"X_train_bert.shape","623cd67b":"try:\n    del model\n    print('refined')\nexcept:\n    print('next')\n\nmodel = Sequential()\n\nmodel.add(Dense(50, input_dim=768, activation='relu', kernel_initializer='lecun_uniform'))\nmodel.add(Dense(50, activation='relu', kernel_initializer='lecun_uniform'))\n\nmodel.add(Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])","eb50982e":"history = model.fit(X_train_bert, y_train, epochs=1500, validation_split=0.1, batch_size=300, verbose=0)","15e28357":"plot_hist(history)","62299935":"def submission(model, train, target, test):\n    \n    model.fit(train, target)\n    \n    pred = model.predict(test)\n    \n    submission = samp_sub.copy()\n    submission['target'] = pred\n    \n    submission.to_csv('\/kaggle\/working\/cbcbert_15.csv', index=False)","4a787e43":"submission(cbc, X_train_bert, y_train, X_test_bert)","316a45ca":"def submission_lgb(params, data, test_features):\n    \n    lgbm = lgb.train(params, data)\n    pred = lgbm.predict(test_features).round().astype('int')\n    \n    submission = samp_sub.copy()\n    submission['target'] = pred\n    \n    submission.to_csv('\/kaggle\/working\/lgbm_02.csv', index=False)","72cf96b7":"submission_lgb(lgb_param, train_lgb, X_test_bert)","fdad1479":"del train_lgb","0455e2d2":"def nn_pred(model, X_test):\n\n    prediction_nn = model.predict(X_test).round().astype('int')\n    submission = samp_sub.copy()\n    submission['target'] = prediction_nn\n    \n    submission.to_csv('\/kaggle\/working\/nn_003.csv', index=False)\n\nnn_pred(model, X_test_bert)","2d60d792":"**Keep moving**","9043c56b":"*Best score on the test dataset  \n0.80815*","302711f9":"Not so good. Anyway, let's make the first submission.","07c5ccf6":"# Cleaning and lemmatization\n\nOk, let's start text processing.\n\n*13.08.20 upd - more symbols and word were replaced.*","904ab1a2":"Make a special function for RNN.","28d16c46":"People never lie about the wreckage, derailment and debris. Also, they tell the truth about outbreaks, typhoons and oil spills.","0432af54":"# BERT text preprocessing","55c97508":"Start with simple logistic regression.","696f6092":"## NN is the future \n\nToday I will use keras for build a simple NN. I'll use torch a bit later.","f6e3a09b":"# Text vectorization \n\nWe will use tf-idf vectorization for the first attempt.  \nAnd keep only unigrams (default ngram_range parameter).","3a0e4b00":"Continue with CatBoost by Yandex.","8d480cd2":"*0.78884 - submission score. Worse than a SGD*","52fe43ee":"# Hallo my beautiful Kagglers!\n\nWelcome back to my notebooks. \nAfter getting into the top 13% at the Digit Recognizing competition, I would like to improve my NLP skills.  \nAs usual, I start to learn the topic from scratch. From basic sklearn models to neural networks like BERT.  \n\nIf you have any comments or suggestions, please, don't hesitate to put them in the comments section below!\n\nHere we go!","27f54e2e":"There are special symbols (#), Capital Letters, dots and commas. Time to remove them all.","8d43ff27":"Let's tokenize the tweets.","45585334":"Drop all necessary columns. Split datasets into features and target.","1fd6b701":"# Scoreboard \n\n\n0.81550 \/ Rank 394 \/ cbc with bert preprocessing  \n0.81121 \/ Rank 426 \/ svm_bert with distill bert preprocessing    \n0.80478 \/ Rank 498 \/ sgd_bert with distill bert preprocessing  \n0.79313 \/ Rank 870 \/ sgd model  ","a92edf1b":"Checking the function on three samples.","7ca6c004":"Try to apply DistillBERT. \n\nThis neural model is a light version of BERT. It requires less time and CPU resources.  \n\nStart with making train and test datasets for our model.","4a51d0aa":"*This approach gave me 0.79313 point on the leaderboard*","4c95a899":"At least, *bioterror\/bioterrorism*, *annihilated\/annihilation* and *blaze\/blazing* mean the same. What if we apply lemmatization for this column? How much can we decrease the number of unique values?  \nWe also need to remove '%20' symbols.","91e0030b":"#### lightGBM","a58c6651":"Make a special function for LightGBM.","26c01588":"To increase iteration time, I will create a table grouped by keywords. Then I will lemmatize 222 rows instead of ~7600.  \nThis dataframe will consist of the numbers of tweets of each topic. ","d03fceef":"Dive into the data. \n\n# Data analysys","3c078f88":"For somebody, storms and open wounds are the real disasters. Other people don't afraid it. \n\nWhat about balance between fake and real news?","bc5a38f2":"FATALITY. FLAWLESS VICTORY.  \nSorry. \n\nSeems like we can use keywords for prediction. First, I would like to make a classification with the tweets text only.  \n\nLet's see, what topic consisnts more fake tweets. \n\nHow many different topics do we have?","eea6264b":"Is that real, 222 unique value without any duplicates?","6c2287dd":"Which topics are the most controversional?","0ceba83a":"Start with simple sklearn models.   \nAs usual create a function for a GridSearch and Cross validation score first. ","ed234069":"Models are ready for learning!","1c4beb23":"Now we have a nice and descriptive table.   \nLet's find the top 10 true and fake topics.","b2d84ba2":"*NB*  \n\nI rerun this notebook often, so I've just turn some rows of code into *#this form*.  \nIf you want, please, remove the 'sharp' characters and run the whole notebook.","2a5a4bfd":"OMG, we have missing values!","c3d324b3":"# Submisson","d4225a30":"Seems like we didn't remove all repeated topics, but reduce the number of unique values from 222 to 186.  ","68daf9c1":"Make attention mask, embbeddings and updated features.","162fda8a":"Nice, it works.  \nLemmatize tweets for test and train datasets.","fb6819c2":"#### Libs and data import","4d183f30":"### BERT preprocessed model selection","ae7b33de":"Aftershocks, ruins and body bags are the most fake topics in Twitter.  \n\nAdd some visualisation to our chart.","1cad57af":"### LinearSVM","b24f21c9":"*I have preprocessed the text and just load it to increase re-iteration time.*","03579ea0":"Set the seed","4117f11d":"# First attempt of model selection","4a4cca26":"*This approach gave me the highest position (best score = 0.81550) on the leadearbord.*","815e9c31":"Not great, not terrible. First evaluation we will try to make with the currant statement.","198d9b8e":"We will make a special cross validation function for catboost classifier.","52a6325f":"### SGD Classifier","073d3ae6":"0.7613775148606625  \n{'C': 15.789557894736841, 'max_iter': 50}","39ed8144":"Bags have been transformed into Bag. Good job!  \n\nDrop the keyword columns, regroup the topics and add percentage of Real and Fake tweets."}}