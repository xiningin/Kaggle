{"cell_type":{"44b61c87":"code","97827069":"code","7f095746":"code","67aa6dcb":"code","444e94d0":"code","2aa9bc7d":"code","5b9b5abe":"code","0e186003":"code","91a2bb0e":"code","977e08cf":"code","3e11d665":"code","bf949796":"code","509bf6b6":"code","2361b785":"code","cd8ad282":"code","ca467337":"code","02d2cb44":"code","e3f01f62":"code","fa3f0238":"code","3d8acc1c":"code","caeafe05":"code","67e7044a":"markdown"},"source":{"44b61c87":"# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns\nimport plotly.express as px\nimport os","97827069":"# Reading Data\ndf = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","7f095746":"# Understanding Data\nprint(\"Rows, columns: \" + str(df.shape))\ndf.head()","67aa6dcb":"df.describe()","444e94d0":"# Missing Values\nprint(df.isna().sum())","2aa9bc7d":"# Histogram of quality\nfig = px.histogram(df,x='quality')\nfig.show()","5b9b5abe":"# Correlation Matrix\ncorr = df.corr()\nplt.pyplot.subplots(figsize=(15,10))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","0e186003":"# Create Classification version of target variable\ndf['goodquality'] = [1 if x >= 7 else 0 for x in df['quality']]","91a2bb0e":"# See proportion of good vs bad wines\ndf['goodquality'].value_counts()","977e08cf":"# Separate feature variables and target variable\nX = df.drop(['quality','goodquality'], axis = 1)\ny = df['goodquality']","3e11d665":"# Normalize feature variables\nfrom sklearn.preprocessing import StandardScaler\nX_features = X\nX = StandardScaler().fit_transform(X)","bf949796":"# Splitting the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)","509bf6b6":"# Model 1: Decision Tree\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel1 = DecisionTreeClassifier(random_state=1)\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nprint(classification_report(y_test, y_pred1))","2361b785":"# Model 2: Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nmodel2 = RandomForestClassifier(random_state=1)\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(classification_report(y_test, y_pred2))","cd8ad282":"# Model 3: AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\nmodel3 = AdaBoostClassifier(random_state=1)\nmodel3.fit(X_train, y_train)\ny_pred3 = model3.predict(X_test)\n\nprint(classification_report(y_test, y_pred3))","ca467337":"# Model 4: Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel4 = GradientBoostingClassifier(random_state=1)\nmodel4.fit(X_train, y_train)\ny_pred4 = model4.predict(X_test)\n\nprint(classification_report(y_test, y_pred4))","02d2cb44":"# Model 5: XGBoost\nimport xgboost as xgb\nmodel5 = xgb.XGBClassifier(random_state=1)\nmodel5.fit(X_train, y_train)\ny_pred5 = model5.predict(X_test)\n\nprint(classification_report(y_test, y_pred5))","e3f01f62":"# Feature Importance: Random Forest\nfeat_importances = pd.Series(model2.feature_importances_, index=X_features.columns)\nfeat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))","fa3f0238":"# Feature Importance: XGBoost\nfeat_importances = pd.Series(model5.feature_importances_, index=X_features.columns)\nfeat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))","3d8acc1c":"df_temp = df[df['goodquality']==1]\ndf_temp.describe()","caeafe05":"df_temp2 = df[df['goodquality']==0]\ndf_temp2.describe()","67e7044a":"# Wine Prediction: Comparing Several Classification Algorithms\n\n## Decision Trees, Random Forests, AdaBoost, Gradient Boost, and XGBoost"}}