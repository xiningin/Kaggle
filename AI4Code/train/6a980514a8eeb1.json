{"cell_type":{"bb0ec584":"code","e2dc4434":"code","cfb0978c":"code","fa10fb1f":"code","89e50959":"code","215a67e9":"code","ee70dcfc":"code","86d8d6fa":"code","8c531c4e":"code","136d72d3":"code","b244c252":"code","a863906e":"code","dcb19f39":"code","6d607e17":"code","6d90a88d":"code","de00b403":"code","072685ce":"code","0f19a160":"code","bfa07dad":"code","deb62059":"code","54e8d775":"code","5e2ca491":"code","4b13ad4f":"code","8ff0698b":"code","2bb8b2c2":"code","f34b7bd5":"code","8c3c8478":"code","f42ea39f":"code","c9c09eb1":"code","7298b8be":"code","677421e1":"code","60a24ec5":"code","c3c8bcfa":"code","1ab29ab8":"markdown","678e24cc":"markdown","790ebb3e":"markdown","bb71d8fd":"markdown"},"source":{"bb0ec584":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse.linalg import svds\nimport scipy\nimport sklearn\nimport numpy as np\nimport math\nimport os\n\n\nimport nltk\nnltk.download('stopwords')\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e2dc4434":"df_art = pd.read_csv('..\/input\/articles-sharing-reading-from-cit-deskdrop\/shared_articles.csv')\ndf_art","cfb0978c":"print(df_art.lang.nunique())","fa10fb1f":"print(df_art.lang.unique())","89e50959":"df_art.lang.value_counts()","215a67e9":"px.pie(df_art, df_art.lang.value_counts().index, df_art.lang.value_counts().values, title='Article Languages')","ee70dcfc":"df_art[df_art.lang.isin(['la', 'es', 'ja'])]","86d8d6fa":"df_art[df_art.lang.isin(['la', 'es', 'ja'])].text.tolist()","8c531c4e":"df_art = df_art[df_art.lang.isin(['en', 'pt'])]\ndf_art","136d72d3":"df_int = pd.read_csv('..\/input\/articles-sharing-reading-from-cit-deskdrop\/users_interactions.csv')\ndf_int","b244c252":"df_int.eventType.unique()","a863906e":"px.pie(df_int, df_int.eventType.value_counts().index,df_int.eventType.value_counts().values, title='Person Interaction Type')","dcb19f39":"stop_words = stopwords.words('english') + stopwords.words('portuguese')","6d607e17":"vectorizer = TfidfVectorizer(analyzer='word',\n                     ngram_range=(1, 2),\n                     min_df=0.005,\n                     max_df=0.8,\n                     max_features=5000,\n                     stop_words=stop_words)\n\nitem_ids = df_art['contentId'].tolist()\ntfidf_matrix = vectorizer.fit_transform(df_art['title'] + \"\" + df_art['text'])\ntfidf_feature_names = vectorizer.get_feature_names()\ntfidf_matrix","6d90a88d":"event_strength = {\n   'VIEW': 1.0,\n   'LIKE': 2.0, \n   'BOOKMARK': 3.0, \n   'FOLLOW': 4.0,\n   'COMMENT CREATED': 5.0,  \n}\n\ndf_int['eventStrength'] = df_int['eventType'].apply(lambda x: event_strength[x])","de00b403":"df_int","072685ce":"df_int_count = df_int.groupby(['personId', 'contentId']).size().groupby('personId').size().reset_index().rename(columns={0:'IntCount'})\nprint('# of people: %d' % len(df_int_count))","0f19a160":"df_int_count.sort_values('IntCount', ascending=False)","bfa07dad":"df_2int_count = df_int_count[df_int_count.IntCount > 2]\nprint('# of people with at least 3 interactions: %d' % len(df_2int_count))","deb62059":"print('# of interactions: %d' % len(df_int))\ndf_int_tot = pd.merge(df_int, df_2int_count, how = 'right', left_on = 'personId', right_on = 'personId')\nprint('# of interactions from people with at least 3 interactions: %d' % len(df_int_tot))","54e8d775":"df_int.info()","5e2ca491":"df_int_tot.info()","4b13ad4f":"df_int_full = df_int_tot.groupby(['personId', 'contentId'])['eventStrength'].sum().apply(lambda x: math.log(1+x, 2)).reset_index()\nprint('# of unique person\/item interactions: %d' % len(df_int_full))\ndf_int_full.head(15)","8ff0698b":"int_df = df_int_full[df_int_full['contentId'].isin(df_art['contentId'])].set_index('personId')\nperson_profiles = {}\nfor person_id in int_df.index.unique():\n    \n    int_person_df = int_df.loc[person_id]\n\n    item_profiles_list = [tfidf_matrix[item_ids.index(x):item_ids.index(x)+1] for x in int_person_df['contentId']]\n    person_item_profiles = scipy.sparse.vstack(item_profiles_list)\n\n    person_item_strengths = np.array(int_person_df['eventStrength']).reshape(-1,1)\n    person_item_strengths_weighted_avg = np.sum(person_item_profiles.multiply(person_item_strengths), axis=0) \/ np.sum(person_item_strengths)\n    person_profile_norm = sklearn.preprocessing.normalize(person_item_strengths_weighted_avg)\n    person_profiles[person_id] = person_profile_norm","2bb8b2c2":"person_profiles[3609194402293569455]","f34b7bd5":"len(person_profiles)","8c3c8478":"print( person_profiles[3609194402293569455].shape)","f42ea39f":"pd.DataFrame(sorted(zip(tfidf_feature_names, \n                        person_profiles[3609194402293569455].flatten().tolist()), key=lambda x: -x[1]),\n             columns=['token', 'relevance'])","c9c09eb1":"df_int_full.personId.nunique()","7298b8be":"def produce_person_vector(person_id_number):\n    person_id =  df_int_full.personId.unique()[person_id_number]\n    person_profile = person_profiles[person_id]\n    df = pd.DataFrame(sorted(zip(tfidf_feature_names, \n                                 person_profile.flatten().tolist()), key=lambda x: -x[1]),\n                      columns=['token', 'relevance'])\n    return df","677421e1":"produce_person_vector(150)","60a24ec5":"df = produce_person_vector(0).rename(columns={'token':'token1', 'relevance':'relevance1'})\ndf","c3c8bcfa":"for i in range(1, df_int_full.personId.nunique()):\n    df['token'+str(i+1)] = produce_person_vector(i).token\n    df['relevance'+str(i+1)] = produce_person_vector(i).relevance\ndf","1ab29ab8":"This code is the reproduction of the great kernel on recommender systems on Kaggle [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101?), particularly of the content-based filtering part, based on the [CI&T DeskDrop Dataset](https:\/\/www.kaggle.com\/gspmoreira\/articles-sharing-reading-from-cit-deskdrop).","678e24cc":"To characterize the content of the articles, transforming text corpus to feature vectors through utilizing the tf\u2013idf with vectors size 5000 along with unigrams and bigrams in the corpus, ignoring stopwords:","790ebb3e":"# Content-Based Filtering","bb71d8fd":"It seems that the articles in 'la', 'es' and 'ja' languages are extremely rare, I might as well exclude them. However, I will choose to drop them. Also, their overall content seems to be trivial: Although I cannot speak these languages, the last entry in the above list is pretty familiar to me 'Lorem ipsum dolor sit...' :)"}}