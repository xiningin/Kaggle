{"cell_type":{"bdd757ce":"code","02e07fa2":"code","53d79cb5":"code","2ff6be41":"code","2e912b11":"code","bacd27b6":"markdown"},"source":{"bdd757ce":"import torch\nimport math\n\nclass Polynomial3(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate four parameters and assign them as\n        member parameters.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. \n        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n        \"\"\"\n        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'","02e07fa2":"# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)","53d79cb5":"# Both x and y will have torch.Size([2000]).\nx.size(), y.size()","2ff6be41":"# Construct our model by instantiating the class defined above.\nmodel = Polynomial3()","2e912b11":"# Construct our loss function and an Optimizer. \n# The call to model.parameters() in the SGD constructor will contain the \n# learnable parameters (defined with torch.nn.Parameter) which are members of the model.\n\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n\nfor t in range(2000):\n    # Forward pass: Compute predicted y by passing x to the model.\n    y_pred = model(x)\n\n    # Compute and print loss.\n    loss = criterion(y_pred, y)\n    if t % 100 == 99: # print every 100 mini-batches\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')","bacd27b6":"# Learning PyTorch: Custom nn Modules\n\nSource: https:\/\/pytorch.org\/tutorials\/beginner\/pytorch_with_examples.html#pytorch-custom-nn-modules\n\nA third order polynomial, trained to predict `y=sin(x)` from `\u2212\u03c0` to `pi` by minimizing squared Euclidean distance.\n\nThis implementation defines the model as a custom Module subclass. Whenever you want a model more complex than a simple sequence of existing Modules you will need to define your model this way.\n\nWe will use a problem of fitting `y=sin(x)` with a third order polynomial as our running example. The network will have four parameters, and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output.\n\nThe `forward` function computes output Tensors from input Tensors. The `backward` function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n\nIf `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value."}}