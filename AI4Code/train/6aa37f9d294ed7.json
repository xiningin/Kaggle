{"cell_type":{"6c74064e":"code","6ac68ff7":"code","9bc11759":"code","f7ddbfad":"code","609d441e":"code","5169abc6":"code","a7b8af14":"code","46a2a2d7":"code","c054a17d":"code","7956cdf0":"code","d4521395":"code","79ed5c1d":"code","4dd4976d":"code","474cda55":"code","83f8c36a":"code","7cd1a36f":"code","4397cc48":"code","e060f015":"code","232c68ee":"code","f9bf390b":"code","da87bda2":"code","178117e0":"code","07a1237f":"code","64e89b8a":"code","295408ae":"code","70be3864":"code","4fde5dc0":"markdown","244de856":"markdown","4500277e":"markdown","4d807d2e":"markdown","f0a6526e":"markdown","3f8a83e3":"markdown","9fe4316f":"markdown","66b6ac13":"markdown","910ec30c":"markdown","7d67574a":"markdown","1bca3e60":"markdown"},"source":{"6c74064e":"TRAIN = True\nDEBUG = False","6ac68ff7":"!pip install -q pycocotools\n!pip uninstall -yq albumentations\n!pip install -q albumentations==0.5.2\n!pip install -q git+https:\/\/github.com\/qubvel\/segmentation_models.pytorch\n!pip install -q git+https:\/\/github.com\/qubvel\/ttach\n!pip install -q crfseg","9bc11759":"import os\nimport random\nimport time\nimport json\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\n\nimport numpy as np\nimport pandas as pd\n\n# Python package for pre-processing \nfrom pycocotools.coco import COCO\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\nfrom tqdm import tqdm\n# Python package for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom crfseg import CRF\nimport segmentation_models_pytorch as smp\nimport ttach as tta\n\nplt.rcParams['axes.grid'] = False\n\nprint('Pytorch version: {}'.format(torch.__version__))\nprint('Is GPU available: {}'.format(torch.cuda.is_available()))\nif torch.cuda.is_available():\n  print(torch.cuda.get_device_name(0))\n  print('The number of GPUs available: {}'.format(torch.cuda.device_count()))\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n\nprint('CPU count: {}'.format(os.cpu_count()))  # 2","f7ddbfad":"if not os.path.exists('\/kaggle\/temp\/input'):\n    !wget https:\/\/aistages-public-junyeop.s3.amazonaws.com\/app\/Competitions\/000015\/data\/data.tar.gz\n    !mkdir \/kaggle\/temp\n    !mkdir \/kaggle\/temp\/input\n    !tar -xzf data.tar.gz -C \/kaggle\/temp\/input\n    !rm data.tar.gz","609d441e":"batch_size = 4   # Mini-batch size\nnum_epochs = 10\nif DEBUG:\n    num_epochs = 1\nlearning_rate = 1e-04","5169abc6":"# fix the seed\nrandom_seed = 42\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.cuda.manual_seed_all(random_seed) # if use multi-GPU\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(random_seed)\nrandom.seed(random_seed)","a7b8af14":"%matplotlib inline\n\ndataset_path = '\/kaggle\/temp\/input'\nanns_file_path = os.path.join(dataset_path, 'train_all.json')\n\n# Read annotations\nwith open(anns_file_path, 'r') as f:\n    dataset = json.loads(f.read())\n\ncategories = dataset['categories']\nanns = dataset['annotations']\nimgs = dataset['images']\nnr_cats = len(categories)\nnr_annotations = len(anns)\nnr_images = len(imgs)\n\n# Load categories and super categories\ncat_names = []\nsuper_cat_names = []\nsuper_cat_ids = {}\nsuper_cat_last_name = ''\nnr_super_cats = 0\nfor cat_it in categories:\n    cat_names.append(cat_it['name'])\n    super_cat_name = cat_it['supercategory']\n    # Adding new supercat\n    if super_cat_name != super_cat_last_name:\n        super_cat_names.append(super_cat_name)\n        super_cat_ids[super_cat_name] = nr_super_cats\n        super_cat_last_name = super_cat_name\n        nr_super_cats += 1\n\nprint('Number of super categories:', nr_super_cats)\nprint('Number of categories:', nr_cats)\nprint('Number of annotations:', nr_annotations)\nprint('Number of images:', nr_images)","46a2a2d7":"from sklearn.model_selection import GroupKFold\nimport datetime\nfrom tqdm import tqdm\n\ninfo = dataset['info']\nlicenses = dataset['licenses']\n\nx = pd.DataFrame(imgs)\nx['image_id'] = x['id']\ny = x.merge(pd.DataFrame(anns), on = 'image_id')\ny['fold'] = -1\nskf = GroupKFold(n_splits = 5)\nfor fold, (trn_ids, val_ids) in enumerate(skf.split(y,groups = y['file_name'].values.tolist())):   \n    y.loc[val_ids,'fold'] = fold\n    \nnow = datetime.datetime.now()\n\ndata = dict(\n    info=info,\n    licenses=[licenses],\n    images=[\n        # license, url, file_name, height, width, date_captured, id\n    ],\n    annotations=[\n        # segmentation, area, iscrowd, image_id, bbox, category_id, id\n    ],\n    categories=dataset['categories'],\n)","c054a17d":"for fold in range(5):\n    train_df_ = y[y['fold'] != fold]\n    val_df_ = y[y['fold'] == fold]\n    train_ids = train_df_.file_name.unique()\n    val_ids = val_df_.file_name.unique()\n    print(f\"\\nFold: {fold}\\n Train images count: {len(train_ids)}, Val images count: {len(val_ids)}\")\n\n    coco_dir = f'\/kaggle\/temp\/input'\n    os.makedirs(coco_dir, exist_ok=True)\n\n    ## Setting the output annotation json file paths\n    train_out_file = f'{coco_dir}\/train_annotations_fold{fold}.json'\n    val_out_file = f'{coco_dir}\/val_annotations_fold{fold}.json'\n    if os.path.exists(train_out_file) and os.path.exists(val_out_file):continue\n    data_train = data.copy()\n    data_train['images'] = []\n    data_train['annotations'] = []\n    data_val = data.copy()\n    data_val['images'] = []\n    data_val['annotations'] = []\n\n    for i, img_id in tqdm(enumerate(train_ids), total=len(train_ids)):\n\n        data_train['images'].append(dict(license=0,\n                                         url=None,\n                                         file_name=img_id,\n                                         height=512,\n                                         width=512,\n                                         date_captured=None,\n                                         id=i\n                                        ))\n\n        img_annotations = train_df_[train_df_.file_name==img_id]\n        boxes = img_annotations[['bbox']].to_numpy()\n        box_labels = img_annotations[['category_id']].to_numpy()\n        segs = img_annotations[['segmentation']].to_numpy().tolist()\n        areas = img_annotations[['area']].to_numpy()\n\n        for bbox, label,seg,area in zip(boxes, box_labels,segs,areas):\n            data_train['annotations'].append(dict(id=len(data_train['annotations']),\n                                                  image_id=i,\n                                                  category_id=int(label),\n                                                  area=float(area[0]),\n                                                  bbox=bbox[0],\n                                                  segmentation=seg[0],\n                                                  iscrowd=0))\n    with open(train_out_file, 'w') as f:\n        json.dump(data_train, f, indent=4)\n\n    for i, img_id in tqdm(enumerate(val_ids), total=len(val_ids)):\n        data_val['images'].append(dict(license=0,\n                                         url=None,\n                                         file_name=img_id,\n                                         height=512,\n                                         width=512,\n                                         date_captured=None,\n                                         id=i\n                                        ))\n\n        img_annotations = val_df_[val_df_.file_name==img_id]\n        boxes = img_annotations[['bbox']].to_numpy()\n        box_labels = img_annotations[['category_id']].to_numpy()\n        segs = img_annotations[['segmentation']].to_numpy()\n        areas = img_annotations[['area']].to_numpy()\n\n        for bbox, label,seg,area in zip(boxes, box_labels,segs,areas):\n\n            data_val['annotations'].append(dict(id=len(data_val['annotations']),\n                                                  image_id=i,\n                                                  category_id=int(label),\n                                                  area=float(area[0]),\n                                                  bbox=bbox[0],\n                                                  segmentation=seg[0],\n                                                  iscrowd=0))             \n    with open(val_out_file, 'w') as f:\n        json.dump(data_val, f, indent=4)","7956cdf0":"# import shutil\n# shutil.make_archive('\/kaggle\/working\/data', \"zip\", \"\/kaggle\/temp\/input\/\")","d4521395":"# Count annotations\ncat_histogram = np.zeros(nr_cats,dtype=int)\nfor ann in anns:\n    cat_histogram[ann['category_id']] += 1\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(5,5))\n\n# Convert to DataFrame\ndf = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\ndf = df.sort_values('Number of annotations', 0, False)\n\n# Plot the histogram\nplt.title(\"category distribution of train set \")\nplot_1 = sns.barplot(x=\"Number of annotations\", y=\"Categories\", data=df, label=\"Total\", color=\"b\")","79ed5c1d":"sorted_temp_df = df.sort_index()\nsorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\nsorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)\nsorted_df","4dd4976d":"category_names = list(sorted_df.Categories)\n\ndef get_classname(classID, cats):\n    for i in range(len(cats)):\n        if cats[i]['id']==classID:\n            return cats[i]['name']\n    return \"None\"\n\nclass CustomDataset(Dataset):\n    \"\"\"COCO format\"\"\"\n    def __init__(self, data_dir, mode = 'train', transform = None):\n        super().__init__()\n        self.mode = mode\n        self.transform = transform\n        self.coco = COCO(data_dir)\n        \n    def load_image (self, index:int):\n        # Get the image_info using coco library\n        image_id = self.coco.getImgIds(imgIds=index)\n        image_infos = self.coco.loadImgs(image_id)[0]\n\n        # Load the image using opencv\n        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n        images \/= 255.0\n        \n        if (self.mode in ('train', 'val')):\n            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n            anns = self.coco.loadAnns(ann_ids)\n            # print(\"image_infos['id'] : {}\".format(image_infos['id']) )\n            # Load the categories in a variable\n            cat_ids = self.coco.getCatIds()\n            cats = self.coco.loadCats(cat_ids)\n\n            # masks_size : height x width            \n            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n            # Background = 0, Unknown = 1, General trash = 2, ... , Cigarette = 11\n            for i in range(len(anns)):\n                className = get_classname(anns[i]['category_id'], cats)\n                pixel_value = category_names.index(className)\n                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n            masks = masks.astype(np.float32)\n            return images, masks, image_infos\n        \n        return images, image_infos\n        \n    def rand_bbox(self,size = (512,512) , lamb = 0.3):\n        \"\"\" Generate random bounding box \n        Args:\n            - size: [width, breadth] of the bounding box\n            - lamb: (lambda) cut ratio parameter\n        Returns:\n            - Bounding box\n        \"\"\"\n        W = size[0]\n        H = size[1]\n        cut_rat = np.sqrt(1. - lamb)\n        cut_w = np.int(W * cut_rat)\n        cut_h = np.int(H * cut_rat)\n\n        # uniform\n        cx = np.random.randint(W)\n        cy = np.random.randint(H)\n\n        bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n        bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n        bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n        bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n        return bbx1, bby1, bbx2, bby2\n        \n    def mixup_images(self, images, masks):\n        rnd_images, rnd_masks, _ = self.load_image(random.randint(0,self.__len__() - 1))\n        lam = np.clip(np.random.beta(1.0,1.0), 0.35,0.65)\n        images = lam * images + (1-lam) * rnd_images\n        masks = lam * masks + (1-lam) * rnd_masks\n        return images, masks\n    \n    def generate_mixup_image(self, images, masks):\n        lam = np.clip(np.random.beta(1.0,1.0), 0.35,0.65)\n        bbx1, bby1, bbx2, bby2 = self.rand_bbox()\n        rand_idx = random.randint(0,self.__len__()-1)\n        images_updated, masks_updated, _ = self.load_image(rand_idx)\n        images[bbx1:bbx2, bby1:bby2, :] = (1 - lam) * images_updated[bbx1:bbx2, bby1:bby2, :] + (lam) * images[bbx1:bbx2, bby1:bby2, :]\n        masks[bbx1:bbx2, bby1:bby2] = (1 - lam) * masks_updated[bbx1:bbx2, bby1:bby2] + (lam) * masks[bbx1:bbx2, bby1:bby2]\n        return images_updated, masks_updated\n    \n    def generate_cutmix_image(self, images, masks):\n        bbx1, bby1, bbx2, bby2 = self.rand_bbox()\n        rand_idx = random.randint(0,self.__len__()-1)\n        images_updated, masks_updated, _ = self.load_image(rand_idx)\n        images[bbx1:bbx2, bby1:bby2, :] = images_updated[bbx1:bbx2, bby1:bby2, :]\n        masks[bbx1:bbx2, bby1:bby2] = masks_updated[bbx1:bbx2, bby1:bby2]\n        return images_updated, masks_updated\n    \n    def __getitem__(self, index: int):\n        \n        images, masks, image_infos = self.load_image(index)\n#         if self.mode == 'train':\n#             if random.random() > 0.75:\n#                 images, masks = self.mixup_images(images, masks)\n#             elif random.random() > 0.50 and random.random() < 0.75:\n#                 images, masks = self.generate_cutmix_image(images, masks)\n#             elif random.random() > 0.25 and random.random() < 0.50:\n#                 images, masks = self.generate_mixup_image(images, masks)\n            \n        if self.transform is not None:\n            transformed = self.transform(image=images, mask=masks)\n            images = transformed[\"image\"]\n            masks = transformed[\"mask\"]\n            masks =  masks.squeeze()\n        \n        return images, masks, image_infos\n    \n    def __len__(self) -> int:        \n        return len(self.coco.getImgIds())","474cda55":"from skimage.transform import resize\n\nclass CustomDataset2(Dataset):\n    def __init__(self, data_dir, transform = None):\n        super().__init__()\n        df = pd.read_csv('..\/input\/upstage-2-submission\/submission (82).csv')\n        self.data_dir = data_dir\n        self.image_ids = df['image_id'].values\n        self.label = df['PredictionString'].values\n        self.transform = transform\n        \n    def __getitem__(self, index: int):\n        img   = cv2.cvtColor(cv2.imread(self.data_dir + '\/' +self.image_ids[index]),cv2.COLOR_BGR2RGB)\n        mask = self.str2arr(self.label[index])\n        plt.imshow(mask)\n        plt.show()\n        if self.transform is not None:\n            trans = self.transform(image = img, mask = mask)\n            img = trans['image']\n            mask = trans['mask']\n        return img, mask.squeeze(),None\n    \n    def __len__(self) -> int:        \n        return len(self.image_ids)\n    \n    def str2arr(self,d):\n        return resize(np.array([int(e) for e in d.split(' ')]).reshape(256,256),(512,512))","83f8c36a":"# collate_fn needs for batch\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_transform = A.Compose([  \n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.7,\n                         border_mode=cv2.BORDER_REFLECT),\n#         A.OneOf([\n#             A.ElasticTransform(p=.3),\n#             A.GaussianBlur(p=.3),\n#             A.GaussNoise(p=.3),\n#             A.OpticalDistortion(p=0.3),\n#             A.GridDistortion(p=.1),\n#             A.IAAPiecewiseAffine(p=0.3),\n#         ], p=0.3),\n        A.OneOf([\n            A.HueSaturationValue(15,25,0),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        ], p=0.3),\n    ToTensor(),\n], p = 1.0)\n\nval_transform = A.Compose([ \n    ToTensor(),\n])\n\ntest_transform = A.Compose([    \n    ToTensor(),\n])","7cd1a36f":"class ModelWithCRF(nn.Module):\n    def __init__(self,):\n        super(ModelWithCRF, self).__init__()\n        self.backbone = smp.Unet(encoder_name='efficientnet-b1', classes=12 , encoder_weights=\"imagenet\", activation=None)\n        self.crf = CRF(n_spatial_dims = 2)\n    def forward(self, x):\n        output = self.backbone(x)\n        return self.crf(output)","4397cc48":"# model \ubd88\ub7ec\uc624\uae30\n# \ucd9c\ub825 \ub808\uc774\ube14 \uc218 \uc815\uc758 (classes = 12)\nmodel = smp.Unet(encoder_name='efficientnet-b1', classes=12 , encoder_weights=\"imagenet\", activation=None)\nmodel = model.to(device)","e060f015":"def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device, fold,verbose):\n    print('Start training..')\n    best_loss = 9999999\n    weights = [0.7,0.1,0.1,0.1]\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for step, (images, masks, _) in enumerate(data_loader):\n            images = torch.stack(images)\n            masks = torch.stack(masks).long()\n            \n            images, masks = images.to(device), masks.to(device)                  \n            \n            outputs= model(images)\n            \n            loss = sum([criterion[i](outputs, masks)*weights[i] for i in range(len(criterion))])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # print the loss at 100 step intervals.\n            if verbose != 0 and type(verbose) is int:\n                if (step + 1) % verbose == 0:\n                    print('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}'.format(\n                        epoch+1, num_epochs, step+1, len(train_loader), loss.item()))\n        \n        # print the loss and save the best model at val_every intervals.\n        if (epoch + 1) % val_every == 0:\n            avrg_loss = validation(epoch + 1, model, val_loader, criterion, device)\n            if avrg_loss < best_loss:\n                print('Best performance at epoch: {}'.format(epoch + 1))\n                print('Save model in', saved_dir)\n                best_loss = avrg_loss\n                save_model(model, saved_dir,fold)","232c68ee":"def validation(epoch, model, data_loader, criterion, device):\n    print('Start validation #{}'.format(epoch))\n    model.eval()\n    weights = [0.7,0.1,0.1,0.1]\n\n    with torch.no_grad():\n        total_loss = 0\n        cnt = 0\n        mIoU_list = []\n        for step, (images, masks, _) in enumerate(data_loader):\n            \n            images = torch.stack(images)       # (batch, channel, height, width)\n            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n\n            images, masks = images.to(device), masks.to(device)            \n\n            outputs = model(images)\n#             loss = criterion(outputs, masks)\n            loss = sum([criterion[i](outputs, masks)*weights[i] for i in range(len(criterion))])\n            total_loss += loss\n            cnt += 1\n            \n            outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n\n            mIoU = label_accuracy_score(masks.detach().cpu().numpy(), outputs, n_class=12)[2]\n            mIoU_list.append(mIoU)\n            \n        avrg_loss = total_loss \/ cnt\n        print('Validation #{}  Average Loss: {:.4f}, mIoU: {:.4f}'.format(epoch, avrg_loss, np.mean(mIoU_list)))\n\n    return avrg_loss","f9bf390b":"transforms = tta.Compose(\n   [\n    tta.HorizontalFlip(),\n#     tta.Rotate90(angles=[0,90]),\n#     tta.Scale(scales=[1, 2]),\n#     tta.Multiply(factors=[0.9, 1]),\n    ]\n)\n\ndef test(model, data_loader, device):\n    size = 256\n    transform = A.Compose([A.Resize(256, 256)])\n    print('Start prediction.')\n    model = tta.SegmentationTTAWrapper(model, transforms)\n    model.eval()\n    file_name_list = []\n    preds_array = np.empty((0, size*size), dtype=np.long)\n    \n    with torch.no_grad():\n        for step, (imgs, image_infos) in tqdm(enumerate(test_loader), total = len(test_loader)):\n\n            # inference (512 x 512)\n            outs = model(torch.stack(imgs).to(device))\n            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n            \n            # resize (256 x 256)\n            temp_mask = []\n            for img, mask in zip(np.stack(imgs), oms):\n                try:\n                    transformed = transform(image=img, mask=mask)\n                    mask = transformed['mask']\n                    temp_mask.append(mask)\n                except:\n                    mask = np.empty((size,size))\n                    mask[:] = -1\n                    temp_mask.append(mask)\n\n            oms = np.array(temp_mask)\n            \n            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n            preds_array = np.vstack((preds_array, oms))\n            \n            file_name_list.append([i['file_name'] for i in image_infos])\n    print(\"End prediction.\")\n    file_names = [y for x in file_name_list for y in x]\n    \n    return file_names, preds_array","da87bda2":"# define the evaluation function\n# https:\/\/github.com\/wkentaro\/pytorch-fcn\/blob\/master\/torchfcn\/utils.py\nimport numpy as np\n\ndef _fast_hist(label_true, label_pred, n_class):\n    mask = (label_true >= 0) & (label_true < n_class)\n    hist = np.bincount(\n        n_class * label_true[mask].astype(int) +\n        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n    return hist\n\n\ndef label_accuracy_score(label_trues, label_preds, n_class):\n    \"\"\"Returns accuracy score evaluation result.\n      - overall accuracy\n      - mean accuracy\n      - mean IU\n      - fwavacc\n    \"\"\"\n    hist = np.zeros((n_class, n_class))\n    for lt, lp in zip(label_trues, label_preds):\n        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n    acc = np.diag(hist).sum() \/ hist.sum()\n    with np.errstate(divide='ignore', invalid='ignore'):\n        acc_cls = np.diag(hist) \/ hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        iu = np.diag(hist) \/ (\n            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n        )\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) \/ hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    return acc, acc_cls, mean_iu, fwavacc","178117e0":"val_every = 1 \n\nsaved_dir = '.\/saved'\nif not os.path.isdir(saved_dir):                                                           \n    os.mkdir(saved_dir)\n    \ndef save_model(model, saved_dir,fold, file_name='best_model.pt'):\n    check_point = {'net': model.state_dict()}\n    file_name = file_name.split('.')[0] + f'_{fold}.pt'\n    output_path = os.path.join(saved_dir, file_name)\n    torch.save(model, output_path)","07a1237f":"criterion = [ nn.CrossEntropyLoss(),\n              smp.losses.DiceLoss   (mode = 'multiclass', from_logits=True, smooth=0.0, eps=1e-07),\n              smp.losses.LovaszLoss (mode = 'multiclass', from_logits=True),\n              smp.losses.TverskyLoss(mode = 'multiclass', from_logits=True, smooth=0.0, eps=1e-07, alpha=0.5, beta=0.5, gamma=1.0)\n             ]","64e89b8a":"# test dataset\ntest_path = os.path.join(dataset_path, f'test.json')\ntest_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                              batch_size=batch_size,\n                                              num_workers=1,\n                                              collate_fn=collate_fn)\nfor i in range(3):\n    print(f'\\n\\n********************** FOLD [{i+1}\/{5}] ************************')\n    torch.cuda.empty_cache()    \n    train_path = os.path.join(dataset_path, f'train_annotations_fold{i}.json')\n    val_path = os.path.join(dataset_path, f'val_annotations_fold{i}.json')\n    \n#     train_dataset = CustomDataset(data_dir=train_path, mode='train', transform=train_transform)\n#     val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=val_transform)\n    train_dataset = CustomDataset2(data_dir=dataset_path, transform=train_transform)\n    val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=val_transform)\n    # DataLoader\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,  batch_size=batch_size,shuffle=True,num_workers=4,collate_fn=collate_fn)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size,shuffle=True,num_workers=4,collate_fn=collate_fn)\n#     model = ModelWithCRF()\n    model = smp.Unet(encoder_name='efficientnet-b3', classes=12 , encoder_weights=None, activation=None)\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'..\/input\/d\/morizin\/coding-test-2-upstage\/saved\/best_model_{i}.pt').state_dict())\n    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)  \n    train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_dir, val_every, device, i,100)   \n    train_dataset = CustomDataset(data_dir=train_path, mode='train', transform=train_transform)\n    val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=val_transform)\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,  batch_size=batch_size,shuffle=True,num_workers=4,collate_fn=collate_fn)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size,shuffle=True,num_workers=4,collate_fn=collate_fn)\n    train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_dir, val_every, device, i,100)   \n\n#     model_path = f'.\/saved\/best_model_{i}.pt'\n\n#     # initialize the model\n#     model = smp.Unet(encoder_name='efficientnet-b1', classes=12 , encoder_weights=None, activation=None).to(device)\n\n#     # load the saved best model\n#     checkpoint = torch.load(model_path, map_location=device)\n#     state_dict = checkpoint.state_dict()\n#     model.load_state_dict(state_dict)\n\n#     # switch to evaluation mode\n#     model.eval()\n#     print('')    \n    \n#     # inference\n#     file_names, preds = test(model, test_loader, device)\n#     submission = pd.DataFrame()\n\n#     submission['image_id'] = file_names\n#     submission['PredictionString'] = [' '.join(str(e) for e in sting.tolist()) for sting in preds]\n#     # save submission.csv\n#     submission.to_csv(f\"submission_{i}.csv\", index=False)","295408ae":"# import pandas as pd\n# import numpy as np\n# from tqdm.auto import tqdm\n\n# weights = [0.33,0.33,0.34]\n# subs = [\n#     pd.read_csv('..\/input\/coding-test-2-upstage\/submission_0.csv'),\n#     pd.read_csv('..\/input\/coding-test-2-upstage\/submission_1.csv'),\n#     pd.read_csv('..\/input\/coding-test-2-upstage\/submission_2.csv'),\n# ]\n# def str2arr(d):\n#     return [np.array([float(e) for e in sting.split(' ')]) for sting in tqdm(d)]\n\n# for ix,sub in tqdm(enumerate(subs), total = len(subs)):\n#     subs[ix]['PredictionString'] = str2arr(sub['PredictionString'])\n\n# sam_sub = subs[1].copy()\n# sam_sub['PredictionString'] = [np.zeros(256*256) for _ in range(sam_sub.shape[0])]\n# for ix,sub in enumerate(subs):\n#     for i in range(sam_sub.shape[0]):\n#         sam_sub.loc[i,'PredictionString'] += subs[ix].loc[i,'PredictionString'] * weights[ix]\n        \n# sam_sub['PredictionString'] = [' '.join(str(int(e)) for e in sting.tolist()) for sting in tqdm(sam_sub['PredictionString'].values)]\n# sam_sub.to_csv('submission.csv', index = False)","70be3864":"# i = 835\n# plt.imshow(str2arr(sam_sub.iloc[i:i+1,1])[0].reshape(256,256))\n# plt.show()\n# plt.imshow(subs[1].iloc[i,1].reshape(256,256))\n# plt.show()","4fde5dc0":"# Create instances of CustomDataset and Assign it to DataLoader","244de856":"# Baseline model\n- Here, we show you the UNet with EfficientNet-B0 encoder model. \n- We just built this baseline model with using a small model, no image augmentation, no fine-tuned hyper-parameters, etc.\n- you can build your model based on this baseline model and you can improve the model in your own way.","4500277e":"# Define the function to save the model","4d807d2e":"# Define the train, validation, test functions","f0a6526e":"# Train","3f8a83e3":"# Create instances of loss function and optimizer","9fe4316f":"# EDA of the training data","66b6ac13":"# This baseline code requires a GPU device. If you have not selected the GPU runtime type, you can change the runtime type to enable GPU.","910ec30c":"# Make a CustomDataset class","7d67574a":"# Prepare data","1bca3e60":"# Set the hyperparameters and fix the seed"}}