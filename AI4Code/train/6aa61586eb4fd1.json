{"cell_type":{"e122e470":"code","1a473ff3":"code","45122d9d":"code","9bae53ba":"code","e84cf498":"code","da763a42":"code","7351271f":"code","7debd729":"code","c63cad60":"code","89b5939b":"code","00b6ed51":"code","1018680f":"code","229c797b":"code","6f48ed51":"code","efeaebff":"code","891dfb08":"code","944913cd":"code","c98cb31a":"markdown","55320934":"markdown","6abb040a":"markdown","78a373ae":"markdown","29c5b945":"markdown","923e6bb7":"markdown","f99204f7":"markdown","375352de":"markdown","d5b1f167":"markdown","cc518340":"markdown","2a45f73a":"markdown"},"source":{"e122e470":"import os\nimport cv2\nimport math\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, fbeta_score\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation, BatchNormalization\n\n# Set seeds to make the experiment more reproducible.\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(0)\nseed(0)\n\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")","1a473ff3":"train = pd.read_csv('..\/input\/imet-2019-fgvc6\/train.csv')\nlabels = pd.read_csv('..\/input\/imet-2019-fgvc6\/labels.csv')\ntest = pd.read_csv('..\/input\/imet-2019-fgvc6\/sample_submission.csv')\n\ntrain[\"attribute_ids\"] = train[\"attribute_ids\"].apply(lambda x:x.split(\" \"))\ntrain[\"id\"] = train[\"id\"].apply(lambda x: x + \".png\")\ntest[\"id\"] = test[\"id\"].apply(lambda x: x + \".png\")\n\nprint('Number of train samples: ', train.shape[0])\nprint('Number of test samples: ', test.shape[0])\nprint('Number of labels: ', labels.shape[0])\ndisplay(train.head())\ndisplay(labels.head())","45122d9d":"# Parameters\nBATCH_SIZE = 64\nEPOCHS = 200\nLEARNING_RATE = 0.0001\nHEIGHT = 128\nWIDTH = 128\nCANAL = 3\nN_CLASSES = labels.shape[0]\nES_PATIENCE = 5\nDECAY_DROP = 0.5\nDECAY_EPOCHS = 10\nclasses = list(map(str, range(N_CLASSES)))","9bae53ba":"def f2_score_thr(threshold=0.5):\n    def f2_score(y_true, y_pred):\n        beta = 2\n        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold), K.floatx())\n\n        true_positives = K.sum(K.clip(y_true * y_pred, 0, 1), axis=1)\n        predicted_positives = K.sum(K.clip(y_pred, 0, 1), axis=1)\n        possible_positives = K.sum(K.clip(y_true, 0, 1), axis=1)\n\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        recall = true_positives \/ (possible_positives + K.epsilon())\n\n        return K.mean(((1+beta**2)*precision*recall) \/ ((beta**2)*precision+recall+K.epsilon()))\n    return f2_score\n\ndef step_decay(epoch):\n    initial_lrate = LEARNING_RATE\n    drop = DECAY_DROP\n    epochs_drop = DECAY_EPOCHS\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n    \n    return lrate","e84cf498":"train_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train,\n    directory=\"..\/input\/imet-2019-fgvc6\/train\",\n    x_col=\"id\",\n    y_col=\"attribute_ids\",\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    class_mode=None,\n    target_size=(HEIGHT, WIDTH))\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_datagen.flow_from_dataframe(  \n        dataframe=test,\n        directory = \"..\/input\/imet-2019-fgvc6\/test\",    \n        x_col=\"id\",\n        target_size=(HEIGHT, WIDTH),\n        batch_size=1,\n        shuffle=False,\n        class_mode=None)","da763a42":"model_vgg = VGG16(weights=None, include_top=False)\nmodel_vgg.load_weights('..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')","7351271f":"STEP_SIZE_TRAIN = train_generator.n \/\/ train_generator.batch_size\ntrain_data = model_vgg.predict_generator(train_generator, STEP_SIZE_TRAIN)","7debd729":"train_labels = []\nfor label in train['attribute_ids'][:train_data.shape[0]].values:\n    zeros = np.zeros(N_CLASSES)\n    for label_i in label:\n        zeros[int(label_i)] = 1\n    train_labels.append(zeros)\n    \ntrain_labels = np.asarray(train_labels)","c63cad60":"X_train, X_val, Y_train, Y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=0)","89b5939b":"model = Sequential()\nmodel.add(Flatten(input_shape=train_data.shape[1:]))\nmodel.add(Dense(2048, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(N_CLASSES, activation=\"sigmoid\"))\n\noptimizer = optimizers.Adam(lr=LEARNING_RATE)\nthresholds = [0.1, 0.15, 0.2, 0.25, 0.28, 0.3, 0.4, 0.5]\nmetrics = [\"accuracy\", \"categorical_accuracy\", f2_score_thr(0.1), f2_score_thr(0.15), f2_score_thr(0.2), \n           f2_score_thr(0.25), f2_score_thr(0.28), f2_score_thr(0.3), f2_score_thr(0.4), f2_score_thr(0.5)]\nlrate = LearningRateScheduler(step_decay)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=ES_PATIENCE)\ncallbacks = [es]\nmodel.compile(optimizer=optimizer, loss=\"binary_crossentropy\",  metrics=metrics)","00b6ed51":"history = model.fit(x=X_train, y=Y_train,\n                    validation_data=(X_val, Y_val),\n                    epochs=EPOCHS,\n                    batch_size=BATCH_SIZE,\n                    callbacks=callbacks,\n                    verbose=2)","1018680f":"sns.set_style(\"whitegrid\")\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex='col', figsize=(20,7))\n\n\nax1.plot(history.history['loss'], label='Train loss')\nax1.plot(history.history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Loss')\n\nax2.plot(history.history['acc'], label='Train Accuracy')\nax2.plot(history.history['val_acc'], label='Validation accuracy')\nax2.legend(loc='best')\nax2.set_title('Accuracy')\n\nax3.plot(history.history['categorical_accuracy'], label='Train Cat Accuracy')\nax3.plot(history.history['val_categorical_accuracy'], label='Validation Cat Accuracy')\nax3.legend(loc='best')\nax3.set_title('Cat Accuracy')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","229c797b":"fig, axes = plt.subplots(4, 2, sharex='col', figsize=(20,7))\n\naxes[0][0].plot(history.history['f2_score'], label='Train F2 Score')\naxes[0][0].plot(history.history['val_f2_score'], label='Validation F2 Score')\naxes[0][0].legend(loc='best')\naxes[0][0].set_title('F2 Score threshold 0.1')\n\naxes[0][1].plot(history.history['f2_score_1'], label='Train F2 Score')\naxes[0][1].plot(history.history['val_f2_score_1'], label='Validation F2 Score')\naxes[0][1].legend(loc='best')\naxes[0][1].set_title('F2 Score threshold 0.15')\n\naxes[1][0].plot(history.history['f2_score_2'], label='Train F2 Score')\naxes[1][0].plot(history.history['val_f2_score_2'], label='Validation F2 Score')\naxes[1][0].legend(loc='best')\naxes[1][0].set_title('F2 Score threshold 0.2')\n\naxes[1][1].plot(history.history['f2_score_3'], label='Train F2 Score')\naxes[1][1].plot(history.history['val_f2_score_3'], label='Validation F2 Score')\naxes[1][1].legend(loc='best')\naxes[1][1].set_title('F2 Score threshold 0.25')\n\naxes[2][0].plot(history.history['f2_score_4'], label='Train F2 Score')\naxes[2][0].plot(history.history['val_f2_score_4'], label='Validation F2 Score')\naxes[2][0].legend(loc='best')\naxes[2][0].set_title('F2 Score threshold 0.28')\n\naxes[2][1].plot(history.history['f2_score_5'], label='Train F2 Score')\naxes[2][1].plot(history.history['val_f2_score_5'], label='Validation F2 Score')\naxes[2][1].legend(loc='best')\naxes[2][1].set_title('F2 Score threshold 0.3')\n\naxes[3][0].plot(history.history['f2_score_6'], label='Train F2 Score')\naxes[3][0].plot(history.history['val_f2_score_6'], label='Validation F2 Score')\naxes[3][0].legend(loc='best')\naxes[3][0].set_title('F2 Score threshold 0.4')\n\naxes[3][1].plot(history.history['f2_score_7'], label='Train F2 Score')\naxes[3][1].plot(history.history['val_f2_score_7'], label='Validation F2 Score')\naxes[3][1].legend(loc='best')\naxes[3][1].set_title('F2 Score threshold 0.5')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","6f48ed51":"best_thr = 0\nbest_thr_val = history.history['val_f2_score'][-1]\nfor i in range(1, len(metrics)-2):\n    if best_thr_val < history.history['val_f2_score_%s' % i][-1]:\n        best_thr_val = history.history['val_f2_score_%s' % i][-1]\n        best_thr = i\n\nthreshold = thresholds[best_thr]\nprint('Best threshold is: %s' % threshold)","efeaebff":"test_generator.reset()\nSTEP_SIZE_TEST = test_generator.n\/\/test_generator.batch_size\n# Pass the test data through the pre-trained model to extract features\nbottleneck_preds = model_vgg.predict_generator(test_generator, steps=STEP_SIZE_TEST)\n# Make prediction using the second model\npreds = model.predict(bottleneck_preds)","891dfb08":"predictions = []\nfor pred_ar in preds:\n    valid = ''\n    for idx, pred in enumerate(pred_ar):\n        if pred > threshold:\n            if len(valid) == 0:\n                valid += str(idx)\n            else:\n                valid += (' %s' % idx)\n    if len(valid) == 0:\n        valid = str(np.argmax(pred_ar))\n    predictions.append(valid)","944913cd":"filenames = test_generator.filenames\nresults = pd.DataFrame({'id':filenames, 'attribute_ids':predictions})\nresults['id'] = results['id'].map(lambda x: str(x)[:-4])\nresults.to_csv('submission.csv',index=False)\nresults.head(10)","c98cb31a":"### Load data","55320934":"#### Pass all the train data through the pre-trained model to extract features","6abb040a":"### Apply model to test set and output predictions","78a373ae":"#### Recreate the train labels","29c5b945":"### Bottleneck model - VGG16 (feature extractor)","923e6bb7":"### Dependencies","f99204f7":"#### Split the new train data into train and validation for the next model","375352de":"### Second model - Deep Learning MLP","d5b1f167":"### Model graph loss","cc518340":"### Find best threshold value","2a45f73a":"![](https:\/\/raw.githubusercontent.com\/visipedia\/imet-fgvcx\/master\/assets\/banner.png)\n<h1><center>Using the bottleneck features of a pre-trained network<\/center><\/h1>\n\n#### I'm sharing this code since it can get a little tricky to beginners do this, especially finding the right methods, APIs and models, I hope this can help someone.\n\n#### Using the bottleneck features of a pre-trained model is basically using the models as a preprocessing step on your data, so you get a model (VGG16 in this case) and pass your data through it, the output will be the representation of your data according to the model. Then take these features and use on any other model (another deep learning model or even SVM).\n\n#### What you need to know:\n- Similar to fine-tuning you need to remove the top of the pre-trained model.\n- Essentially it works as a pipeline with two models.\n- To have good results with this approach your data need to be similar to the same data used to train the model (ImageNet).\n- This will make your training a lot faster since you only need to pass all data once through the big model (VGG 16) than just train a smaller one on a less complex data."}}