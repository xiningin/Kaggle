{"cell_type":{"49f3afab":"code","12996fe9":"code","577589d7":"code","79e47d2a":"code","fd4d36f1":"code","f189eac7":"code","b2900b5c":"code","6f68528a":"code","4cae50d5":"code","23db256f":"code","dba063ce":"code","59a83835":"code","bf0160d5":"code","a7031e65":"code","0175bd40":"code","ddad3874":"code","75f0b16e":"code","58c8f646":"code","a1daa7e3":"code","03519944":"code","fa3db540":"code","8e2898e2":"code","8d6a16ee":"code","38ed8e18":"code","ae76e512":"code","30e971fc":"code","a715e3b3":"code","5cece2ec":"code","166ff6fd":"code","8d8f6823":"code","8de70a68":"code","87e6bb6b":"code","6bc92f7a":"code","0a554f09":"code","193d6694":"code","7e97c700":"code","544e47f9":"code","2e9de6e5":"code","51b0e85b":"code","09d0d745":"code","4c2898b0":"code","24767283":"code","ad34ca63":"code","047ff311":"code","2e301b82":"code","35fbeaeb":"code","8b4906e7":"code","d63c70ff":"code","b5598a04":"code","f00fdabe":"code","ade59aff":"code","384e2378":"code","3c8b68e8":"code","75b651fe":"code","885e1a40":"code","37600015":"code","c6386ca7":"code","d11e77df":"code","c4c0e0f8":"code","c0674223":"code","c39a7901":"code","e7eac966":"code","4014fe9f":"code","e276b89e":"code","99ab9252":"code","9cb6cb8e":"code","484f3ce5":"code","d4617320":"code","076aa9d2":"code","d5a0f974":"code","b0f4866b":"code","4efeb592":"code","a4ed3a46":"code","722f7481":"code","bd193452":"code","005bf0ec":"code","62b0f508":"code","6ad5bc02":"code","77809841":"code","a121b361":"code","57a0d7cf":"code","657a2a05":"code","800e9df6":"code","b07084da":"markdown","bd98a39a":"markdown","e5101199":"markdown","68f4d3a8":"markdown","83de7605":"markdown","25712a9d":"markdown","720fdf4b":"markdown","275aea36":"markdown","04518cbc":"markdown","ccfafe74":"markdown","d6f66480":"markdown","532c1172":"markdown","253bb11b":"markdown","70b553f3":"markdown","3a8f3e1a":"markdown","5d558ec2":"markdown","8d6853de":"markdown","d856e6cb":"markdown","b1ac8d5c":"markdown","4ff05eea":"markdown","466048e1":"markdown","8c2cf08f":"markdown","12e4192b":"markdown","11eea237":"markdown","518163aa":"markdown","3a651bec":"markdown","e91d08dd":"markdown","56878c5a":"markdown","8c814c9f":"markdown","746b78d7":"markdown","c6640aee":"markdown","d81ad1a0":"markdown","43f4a799":"markdown","e4c0569a":"markdown","dc06362a":"markdown","4bf7b4bf":"markdown","ca8ca671":"markdown"},"source":{"49f3afab":"tasks=[\n    [\"Seasonality of transmission\"],\n    [\"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic)\"],\n    [\"Natural history of the virus and shedding of it from an infected person\"],\n    [\"Implementation of diagnostics and products to improve clinical processes\"],\n    [\"Disease models, including animal models for infection, disease and transmission\"],\n    [\"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"],\n    [\"Immune response and immunity\"],\n    [\"Role of the environment in transmission\"]\n]","12996fe9":"import pandas as pd\nimport numpy as np\nimport json\nimport math\nimport glob\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.spatial import distance\nfrom wordcloud import WordCloud\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk import pos_tag\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport spacy\nfrom collections import  Counter\nimport os\nimport multiprocessing\nimport re\n\nimport matplotlib.style as style\n","577589d7":"style.use('fivethirtyeight')","79e47d2a":"stopwords = [\"chapter\", \"fig\", \"peer\", \"researchers\",\" ma\", \" ma\", \"study\" \"review\", \"peer-reviewed\", \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\",\n             \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\",\n             \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\",\n             \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \n             \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \n             \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\",\n             \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \n             \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \n             \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \n             \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\",\n             \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\",\n             \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\",\n             \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \n             \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \n             \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \n             \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \n             \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\",\n             \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\",\n             \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\",\n             \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n             \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n             \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n             \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n             \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\",\n             \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \n             \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]","fd4d36f1":"stemmer = PorterStemmer()\nlem = WordNetLemmatizer()","f189eac7":"data = pd.read_csv(\"..\/input\/cord19\/CORD-19.csv\")","b2900b5c":"data.head()","6f68528a":"data['text'][7][:300]","4cae50d5":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]  # \"first article\" , \"second article\" , ...\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)                                            # \"first articlesecond article ...\" \n    \n    # Now just remove any stopwords\n    return [word for word in nopunc.split() if word.lower() not in stopwords]","23db256f":"data = data[data['has_full_text'] != False]","dba063ce":"text = data['text']\n\nlow_text = text.apply(lambda x: str(x).lower())\nlow_text.head()","59a83835":"text_pre = low_text.apply(text_process) \ntext_pre.head()","bf0160d5":"text_lem = text_pre.apply(lambda x: [lem.lemmatize(y) for y in x if len(x)>2]) #integrate this step in function \ntext_lem.head()","a7031e65":"#os.chdir(\"\/home\/leon\/Documents\/python\/challenge\")","0175bd40":"data = pd.read_csv('..\/input\/final-clean\/final_clean.csv')","ddad3874":"data.head()","75f0b16e":"title = data['clean_title']\ntext = data['clean_text']","58c8f646":"print(title[:5])\nprint('\\n')\nprint(text[:5])","a1daa7e3":"ls= []\n\nfor i in title:\n    ls.append(str(i).split())","03519944":"fdist = FreqDist()\n\nfor sentence in ls:\n    for token in sentence:\n        fdist[token] +=1","fa3db540":"top_title = fdist.most_common(20)","8e2898e2":"top_title[:5]","8d6a16ee":"ls = []\nfor i in top_title:\n    ls.append({'Word': i[0], 'Num': i[1]})\n\ndf = pd.DataFrame(ls)","38ed8e18":"df.iloc[:5]","ae76e512":"plt.figure(figsize=(15,10))\n\nsns.barplot(data = df, y = 'Word', x = 'Num')\nplt.xlabel('Ocurrences', fontsize=14)","30e971fc":"ls= []\n\nfor i in text:\n    ls.append(str(i).split())","a715e3b3":"no = []  #create one list containing every word in the titles\n\nfor i in ls:\n    for j in i:\n        no.append(j)","5cece2ec":"def remove_special_characters(text):\n    text = re.sub('[0-9]', '', text)\n    return text","166ff6fd":"no_num = []\n\nfor i in no:\n    no_num.append(remove_special_characters(i))","8d8f6823":"#ls_text = []\n#\n#for i in text[:100]:\n#    ls_text.append(str(i).split())","8de70a68":"fdist = FreqDist()\n\nfor token in no_num:\n        fdist[token] +=1","87e6bb6b":"top_text = fdist.most_common(21)","6bc92f7a":"top_text[:5]","0a554f09":"text_topwords = []\nfor i in top_text:    # iterate through tuples to create dataframe\n    text_topwords.append({'Word': i[0],\n                          'Num': i[1],\n                         '10K' : i[1]\/10000})\n\ndf = pd.DataFrame(text_topwords)","193d6694":"df.drop(0, axis=0,inplace=True) #remove white space which was used as replacement for numbers","7e97c700":"plt.figure(figsize=(15,10))\n\nsns.barplot(data = df, y = 'Word', x = '10K')\nplt.xlabel('Ocurrences in Thousands', fontsize=14)","544e47f9":"title","2e9de6e5":"ls= []\n\nfor i in title:\n    ls.append(str(i).split())","51b0e85b":"ls[:2] # split text into tokens which are stored in lists","09d0d745":"no = []  #create one list containing every word in the titles\n\nfor i in ls:\n    for j in i:\n        no.append(j)","4c2898b0":"from wordcloud import WordCloud","24767283":"plt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(no))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","ad34ca63":"ls= []\n\nfor i in title:\n    ls.append(str(i).split())","047ff311":"data['n'] = ls","2e301b82":"data.iloc[:2]","35fbeaeb":"pos = data['n'][:5].apply(lambda x: nltk.pos_tag(x)) #integrate this step in function \npos.head()","8b4906e7":"title","d63c70ff":"w2v= []#create list of lists\n\nfor i in title:\n    w2v.append(str(i).split())","b5598a04":"w2v[:3] ","f00fdabe":"# sg=1 -> skipgram\n# big window size to catch the general environment of the center word\n\nmodel = Word2Vec(w2v,\n                 min_count=1,\n                 size= 100,\n                 workers=multiprocessing.cpu_count()-1,\n                 window = 5,\n                 sg = 1,\n                 iter=100,\n                 negative=5,\n                 sample=10e-6)","ade59aff":"model.most_similar('respiratory') ","384e2378":"documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(w2v)]","3c8b68e8":"\n# dm=1 -> distributed memory\nmodel_ = Doc2Vec(documents=documents,\n                dm=1,\n                vector_size=300,\n                window_size=10,\n                epochs = 50,\n                workers=multiprocessing.cpu_count() -1,\n                min_count=1,\n                negative=5,\n                sample=10e-6,\n                min_alpha=0.0001,\n                alpha=0.025)","75b651fe":"tokens = ('seasonality of transmission').split()  # Seasonality of transmission\n\nnew_vector = model_.infer_vector(tokens)\nsims = model_.docvecs.most_similar([new_vector])\nsims[:5] # now we can see the indices of the most similar papers","885e1a40":"articles = [i[0] for i in sims]","37600015":"pd.options.display.max_colwidth = 100\n\ndata.iloc[articles]['title'] # research papers which are related to the passed string above","c6386ca7":"title = title.dropna()\n\nlen(title)","d11e77df":"w2v= []   #create list of lists\n\nfor i in title:\n    w2v.append(str(i).split())","c4c0e0f8":"no = []  #create one list containing every word in the titles\n\nfor i in w2v:\n    for j in i:\n        no.append(j)","c0674223":"no[:10]","c39a7901":"bow_transformer = CountVectorizer().fit(no) \n\n# Print total number of vocab words\nprint(len(bow_transformer.vocabulary_))","e7eac966":"# bow_transformer.vocabulary_","4014fe9f":"data_bow = bow_transformer.transform(title)","e276b89e":"print('Shape of Sparse Matrix: ', data_bow.shape)# 300 rows (each a document) x 41023 columns (each a unique word)\nprint('Amount of Non-Zero occurences: ', data_bow.nnz) ","99ab9252":"print(data_bow) # first doc contains the words in rows: 1, 287, 288, 431...\nprint('\\n')\nprint(data_bow.shape)","9cb6cb8e":"print(bow_transformer.get_feature_names()[5085])\nprint(bow_transformer.get_feature_names()[12023])","484f3ce5":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(data_bow)","d4617320":"data_tfidf = tfidf_transformer.transform(data_bow)\nprint(data_tfidf.shape)","076aa9d2":"from sklearn.cluster import KMeans","d5a0f974":"kmeans = KMeans(n_clusters=6).fit(data_tfidf)\n","b0f4866b":"kmeans.inertia_","4efeb592":"# it took too long so i printed the output below\n\n\n\n#   SSE = []\n#   for cluster in range(1,20):\n#       kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++')\n#       kmeans.fit(data_tfidf)\n#       SSE.append(kmeans.inertia_)\n#   \n#   # converting the results into a dataframe and plotting them\n#   frame = pd.DataFrame({'Cluster':range(20,40), 'SSE':SSE})\n#   plt.figure(figsize=(12,6))\n#   plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n#   plt.xlabel('Number of clusters')\n#   plt.ylabel('Inertia')","a4ed3a46":"kmeans = KMeans(n_jobs = -1, n_clusters = 25, init='k-means++')\nkmeans.fit(data_tfidf)\npred = kmeans.predict(data_tfidf)","722f7481":"kmeans.inertia_","bd193452":"frame = pd.DataFrame(data_tfidf)\nframe['cluster'] = pred\nframe['cluster'].value_counts()","005bf0ec":"title","62b0f508":"topic_input = []\n\nfor i in title:\n    topic_input.append(str(i).split())","6ad5bc02":"topic_input[:50]","77809841":"# create dictionary -> every unique word in lemmatized titles\ndic=gensim.corpora.Dictionary(topic_input)\n\n# create corpus -> term document frequency\n# doc2bow() simply counts the number of occurrences of each distinct word, \n# converts the word to its integer word ID and returns the result as a sparse vector.\nbow_corpus = [dic.doc2bow(doc) for doc in topic_input]","a121b361":"print(len(bow_corpus[0]))  #term frequency\nprint('\\n')\nprint(len(bow_corpus))  #lemma words","57a0d7cf":"print(dic)\nprint('\\n')\nprint('The first document\/title contains the following (words\/ID , amount of times):', bow_corpus[0][:3])\n\n#documents are split in lists","657a2a05":"lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n                                           id2word=dic,\n                                           num_topics=25, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           iterations=1,\n                                           per_word_topics=True)","800e9df6":"lda_model.show_topics()","b07084da":"Using algorithms like **Word2Vec, Doc2Vec** and **FastText**, similar documents can be recognized and sorted.\n\nWord2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that cosine similarity can be calculated to identify related words that share common contexts in the corpus.\n\n\n\nBy applying topic modeling and clustering we can better understand how the current body of literature is divided and what the common themes for those papers are.","bd98a39a":"# Corona Text Mining","e5101199":"# Doc2Vec\n\n### To compare documents and find related ones","68f4d3a8":"## Titles","83de7605":"### How many papers are in the same cluster?","25712a9d":"Pick the most common words","720fdf4b":"# SSE for 1-20 clusters:\n\n- 27505.811443138384,\n- 27345.925525524854,\n- 27245.215028068233,\n- 27147.642084227915,\n- 27068.668335396258,\n- 27028.02880759061,\n- 26952.062151422502,\n- 26891.496983984216,\n- 26836.947145644648,\n- 26749.776461986112,\n- 26729.37520808019,\n- 26694.768607619822,\n- 26664.429563103273,\n- 26609.221275837324,\n- 26550.978657363143,\n- 26501.241635956863,\n- 26492.45400413421,\n- 26454.65331154789,\n- 26397.757836280794\n \n ## The graph was steady going down but I couldnt compute to a higher amount of clusters because it took too long","275aea36":"First split the strings into lists of tokens","04518cbc":"### Again, here i only ran 1 iteration, after 100 iterations I had following output:\n\n\n0,\n  '0.000*\"genotyping\" + 0.000*\"endonuclease\" + 0.000*\"postpcr\" + 0.000*\"rejection\" + 0.000*\"allograft\" + 0.000*\"immunosuppressive\" + 0.000*\"cd4foxp3\" + 0.000*\"cd8cd122\" + 0.000*\"spongederived\" + 0.000*\"fungus\"'),\n  \n1,\n  '0.073*\"virus\" + 0.047*\"coronavirus\" + 0.028*\"cell\" + 0.027*\"protein\" + 0.026*\"human\" + 0.017*\"rna\" + 0.017*\"influenza\" + 0.012*\"characterization\" + 0.011*\"role\" + 0.010*\"activity\"'),\n  \n2,\n  '0.091*\"viral\" + 0.050*\"response\" + 0.044*\"china\" + 0.036*\"health\" + 0.027*\"development\" + 0.024*\"ebola\" + 0.023*\"lung\" + 0.020*\"strategy\" + 0.016*\"pathogenesis\" + 0.016*\"peptide\"'),\n  \n3,\n  '0.000*\"genotyping\" + 0.000*\"endonuclease\" + 0.000*\"postpcr\" + 0.000*\"rejection\" + 0.000*\"allograft\" + 0.000*\"immunosuppressive\" + 0.000*\"cd4foxp3\" + 0.000*\"cd8cd122\" + 0.000*\"spongederived\" + 0.000*\"fungus\"'),\n  \n4,\n  '0.000*\"genotyping\" + 0.000*\"endonuclease\" + 0.000*\"postpcr\" + 0.000*\"rejection\" + 0.000*\"allograft\" + 0.000*\"immunosuppressive\" + 0.000*\"cd4foxp3\" + 0.000*\"cd8cd122\" + 0.000*\"spongederived\" + 0.000*\"fungus\"'),\n  \n5,\n  '0.070*\"disease\" + 0.040*\"study\" + 0.038*\"outbreak\" + 0.031*\"epidemic\" + 0.025*\"model\" + 0.023*\"patient\" + 0.022*\"approach\" + 0.021*\"clinical\" + 0.021*\"middle\" + 0.020*\"merscov\"'),\n  \n6,\n  '0.086*\"analysis\" + 0.063*\"infectious\" + 0.036*\"2019\" + 0.033*\"case\" + 0.026*\"global\" + 0.024*\"sars\" + 0.024*\"factor\" + 0.020*\"epidemiology\" + 0.019*\"fever\" + 0.019*\"investigation\"'),\n  \n7,\n  '0.062*\"covid19\" + 0.054*\"syndrome\" + 0.039*\"pneumonia\" + 0.031*\"severe\" + 0.024*\"sarscov\" + 0.021*\"potential\" + 0.019*\"sarscov2\" + 0.019*\"therapeutic\" + 0.019*\"treatment\" + 0.018*\"pathogen\"'),\n  \n8,\n '0.047*\"antiviral\" + 0.046*\"gene\" + 0.030*\"identification\" + 0.022*\"replication\" + 0.019*\"evaluation\" + 0.019*\"design\" + 0.018*\"genetic\" + 0.018*\"evolution\" + 0.017*\"reverse\" + 0.016*\"risk\"'),\n  \n9,\n  '0.106*\"infection\" + 0.092*\"respiratory\" + 0.033*\"acute\" + 0.029*\"antibody\" + 0.026*\"vaccine\" + 0.024*\"detection\" + 0.022*\"east\" + 0.020*\"child\" + 0.019*\"tract\" + 0.018*\"immune\"')\n\n\n\nTopic 0 Biological processes \n\nTopic 1 about the actual virus and its cell structures\n\nTopic 2 Origin of the virus ('china + 'pathogenesis') \n\nTopic 3 Genotype and biological processes of the virus\n\nTopic 5 human reaction to epidemics\n\nTopic 6 emperical research about virus on global scale\n\nTopic 7 effects of virus + comparison to sars\n\nTopic 8 identification of viruses\n\nTopic 9 more specific effects of infection\n","ccfafe74":"# Load data","d6f66480":"Iterate over list of tuples and create dataframe with word and frequency respectively","532c1172":"Now the model is trained and we can look for similar terms","253bb11b":"# Use Titles to apply topic modeling","70b553f3":"## Lets see the most frequent words in titles and text","3a8f3e1a":"Then calculate the frequency of each token","5d558ec2":"### Make text lowercase","8d6853de":"# Preprocessing NLTK\n\n### This function will remove punctuation and stopwords.","d856e6cb":"# Same for main text","b1ac8d5c":"## RAM will be a problem with 30 mio tokens\n\nIf i dont find a proper solution to run the models, downsampling would be an option.","4ff05eea":"# POS tagging","466048e1":"# Wordcloud","8c2cf08f":"![corona-virus-still-1000x562-1.jpg](attachment:corona-virus-still-1000x562-1.jpg)","12e4192b":"first remove all numbers","11eea237":"### Transform column of lists to column of strings","518163aa":"## With the above model, one can find out many sources of information about a topic of choice.","3a651bec":"## Word2Vec (for now with only the lemmatized text)\n\nI will use Word2Vec (sg = 1 (Skipgram)), to identify the context words around a specific center word. This way I can compute similar words to an input word of my choice.\n\nWord2Vec requires a format of \u2018list of lists\u2019 for training where every document is contained in a list and every list contains lists of tokens of that document.","e91d08dd":"### Text mining applied to research papers to gain insights on viruses, their transmission and possibly uncover some hidden knowledge","56878c5a":"## Apply Elbow method to find optimal k","8c814c9f":"# k-means","746b78d7":"## I did the same steps for the titles and then saved the clean output due to RAM issues as csv, which I am loading in below","c6640aee":"Word2Vec applies cosine similarity between the word-vectors in the weight matrix to compute similarity.","d81ad1a0":"# TF\/IDF","43f4a799":"### ... and apply text_process (remove stopwords and punctuation)","e4c0569a":"# Bag of Words","dc06362a":"# Clustering\n\n\nHere **Bag-Of-Words** and **TF\/IDF** weighting applied, to do clustering later on.","4bf7b4bf":"# Lemmatization","ca8ca671":"## Please note that the outputs of the models are not representative, because they were trained on a low number of epochs, to commit the notebook in a reasonable time."}}