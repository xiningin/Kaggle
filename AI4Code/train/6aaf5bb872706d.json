{"cell_type":{"eedc0541":"code","25243563":"code","bfa160f5":"code","f32b9f6c":"code","bb6590d3":"code","99f348e6":"code","3fbfe12c":"code","c30a6260":"code","51539215":"code","43f1a999":"code","ab5190f8":"code","9df20ff8":"code","f1958876":"code","3aff566c":"code","b4ba9686":"code","bbb81a9c":"code","6b3904fd":"code","06bcc2cf":"code","c3d214a7":"code","bb5a55b1":"code","fb746374":"code","cdb46210":"code","13add74b":"code","59fd3986":"code","ee2fe86f":"markdown","68a21d7d":"markdown","e823ce0b":"markdown","722d4961":"markdown","6f437c05":"markdown","d34767c6":"markdown","a91b314e":"markdown","338cc693":"markdown","5550215b":"markdown","0c1d0ba4":"markdown","c2d995a9":"markdown","50cbe105":"markdown","4ce4c585":"markdown","9ef66f5d":"markdown","a7c97ec9":"markdown","5e46ccd8":"markdown","4dd94f39":"markdown","100ff749":"markdown","70ed67fb":"markdown","01067a84":"markdown","5c309bf7":"markdown","e415dbf4":"markdown","c6b7c865":"markdown","b80786b5":"markdown","4d91031a":"markdown","c48ff611":"markdown","cbebb74d":"markdown","c1d9795a":"markdown","b216210a":"markdown","eb3723e0":"markdown","b929105f":"markdown","3a0a3788":"markdown","ec016d04":"markdown","eb29573b":"markdown","80b284f5":"markdown","f42d0ef2":"markdown","19f8b339":"markdown"},"source":{"eedc0541":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To filter warning by ignoring\nimport warnings\nwarnings.filterwarnings('ignore')","25243563":"# Load dataset\ndiabetes_df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","bfa160f5":"# View the data as dataframe\ndiabetes_df","f32b9f6c":"# To check more information of data\ndiabetes_df.info()","bb6590d3":"# To check distribution of target class\nsns.countplot(diabetes_df['Outcome']);","99f348e6":"# To get information regarding distributions of values in columns\ndiabetes_df.describe()","3fbfe12c":"# Getting names of columns into cols, will be useful in plotting\ncols = diabetes_df.columns\nprint(cols)","c30a6260":"#Plotting histograms for different column values:\nfig, axes = plt.subplots(3,3, figsize=(10,10), gridspec_kw = dict(hspace=0.5, wspace=0.6))\nfig.suptitle('Frequency plot for different column values')\nfor col, az in zip(cols, axes.flat):\n    sns.histplot(diabetes_df[col], ax = az)","51539215":"#To check the zero entries for each column\n(diabetes_df[cols]== 0).sum()","43f1a999":"#replacing zero values with np.NaN\ndiabetes_df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0, np.NaN)\n(diabetes_df[cols]== 0).sum()","ab5190f8":"#Replacing missing values\ndiabetes_df['BloodPressure'] = diabetes_df['BloodPressure'].fillna(diabetes_df['BloodPressure'].mean())\ndiabetes_df['BMI'] = diabetes_df['BMI'].fillna(diabetes_df['BMI'].mean())\ndiabetes_df['Glucose'] = diabetes_df['Glucose'].fillna(diabetes_df['Glucose'].median())\ndiabetes_df['SkinThickness'] = diabetes_df['SkinThickness'].fillna(diabetes_df['SkinThickness'].median())\ndiabetes_df['Insulin'] = diabetes_df['Insulin'].fillna(diabetes_df['Insulin'].median())","9df20ff8":"# To find the corresponding row\ndiabetes_df[diabetes_df['SkinThickness'] > 90]","f1958876":"# Replace wrong entry with median\ndiabetes_df['SkinThickness'].loc[579] = diabetes_df['SkinThickness'].median()\ndiabetes_df.loc[579]","3aff566c":"# Checking for number of pregnancies vs age for more than 10 values\ndiabetes_df[(diabetes_df['Pregnancies'] > 10) & (diabetes_df['Age'] < 30)].shape","b4ba9686":"# To get information regarding distributions of values in columns\ndiabetes_df.describe()","bbb81a9c":"#Pair plots\nsns.pairplot(diabetes_df, hue = 'Outcome', height = 2);","6b3904fd":"#plotting heatmap for correlation\nplt.figure(figsize=(10,10))\nsns.heatmap(diabetes_df.corr(), square=True, linewidths=.5, annot=True, cbar=False);","06bcc2cf":"# To split data into training and test sets\nfrom sklearn.model_selection import train_test_split\n\nX = diabetes_df.drop('Outcome', axis = 1) #drop target column to get X\ny = diabetes_df['Outcome'] #target column is y\n\nX_train, X_test, y_train,  y_test = train_test_split(X, y, test_size = 0.25, stratify = y)","c3d214a7":"print('X_train shape : ', X_train.shape)\nprint('y_train shape : ', y_train.shape)\nprint('X_test shape  : ', X_test.shape)\nprint('y_test shape  : ', y_test.shape)","bb5a55b1":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","fb746374":"# Import metrics to check performance of models\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report","cdb46210":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint('Accuracy : ' + '{:.2f}'.format(accuracy_score(y_test, y_pred)*100) +\" %\")\nprint('F1 score : ' + '{:.2f}'.format(f1_score(y_test, y_pred)))\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","13add74b":"#Support vector machines\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel = 'rbf')\n\nsvc.fit(X_train, y_train)\n\ny_pred = svc.predict(X_test)\n\nprint('Accuracy : ' + '{:.2f}'.format(accuracy_score(y_test, y_pred)*100) +\" %\")\nprint('F1 score : ' + '{:.2f}'.format(f1_score(y_test, y_pred)))\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","59fd3986":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\n\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\n\nprint('Accuracy : ' + '{:.2f}'.format(accuracy_score(y_test, y_pred)*100) +\" %\")\nprint('F1 score : ' + '{:.2f}'.format(f1_score(y_test, y_pred)))\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","ee2fe86f":"### **4. Building Models**","68a21d7d":"### **5. Conclusion**","e823ce0b":"### Task 3","722d4961":"### 4.3. Logistic Regression Model","6f437c05":"Since our eight features are having quite different ranges, we need to do feature scaling. This will ensure that during training of our model more weight is not given to features having higher values. \nHere we will use StandardScaler which standardize features by removing the mean and scaling to them to unit variance.\ni.e. rescale them to distribution of 0 mean and 1 standard deviation.","d34767c6":"We can see that columns other than Age has minimum value of '0'. And these are to be handled. To better understand how values are distributed we can plot histograms for columns.","a91b314e":"Let's check the number of rows having more than 10 pregnancies and less than 30 age.","338cc693":"From the plot we have observed a maximum correlation of 0.56 only and looking at values, we can conclude that the variables are weekly correlated.","5550215b":"## Let's Start","0c1d0ba4":"The whole 768 rows of dataset have all non-null entries. Also all columns have either integer of float values. Now we will check the Outcome column's distribution. ","c2d995a9":"### Task 1\nFirst we replace all zero entries of above mentioned columns with np.NaN. And then replace them with mean or median of column with the helps of histogram plots we have already plotted.","50cbe105":"The examples are to be split into training and test set keeping thier relative class frequencies approximately same. For this we need set `stratify` as `y` in `train_test_split`. We can use our test data to evaluate performance of our Machine Learning models. Here 25% of data is taken as test examples.","4ce4c585":"Random Forest uses multiple decision trees for prediction. These decision trees trained with randomly selected subset of training set and gives their prediction on test set. By majority vote Random Forest combines these predictions and give model prediction. ","9ef66f5d":"## PIMA INDIANS DIABETES | EXPLORATORY DATA ANALYSIS | MODEL BUILDING","a7c97ec9":"Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for classification. They work well with high-dimensional data in finding a decision boundary. Now we will use it on our data.","5e46ccd8":"Now we will look at pair plots and try to get some insights.","4dd94f39":"No entries with more than 10 pregnancies for age below 30. So the number of pregnancies column is error free. We have no task to complete here.","100ff749":"### Task 2","70ed67fb":"We can see that the column minimum values have changed.","01067a84":"The zero values are now replacced from the required colums.\n\nBy looking at the histogram plots one can conclude that BloodPressure and BMI have somewhat symmetric plots and Glucose, SkinThickness and Insulin have skewed plots. So we will now replace the missing values for BloodPressure and BMI with **mean** and Glucose, SkinThickness and Insulin with **median**.\n","5c309bf7":"### 4.2. Feature Scaling ","e415dbf4":"We can assume that the missing values are replaced with zeros in the data collection stage considering huge share of zeros. \n\nThinking of handling these zero entries, since there are very large number of '0' for columns SkinThickness and Insulin, dropping of rows is not a good idea since we have only 768 rows of data. \n\nSo we are ready to do the following data handling tasks.\n1. Deal with entries equal to zero in columns Glucose, BloodPressure, SkinThickness, Insulin and BMI.\n2. Replace the unlikely SkinThickness value close to 100.\n3. Check and correct wrong entries in Pregnancies column if any.","c6b7c865":"Looking at the pair plots we can observe few things: \n* Higher glucose level, higher BMI and greater age have more associated with positive diabetic. \n* Effects are much evident in case of glucose level. \n* The diabetisPredictionFunctionVariable is not showing any high influeces on diabetes chances.\n\nNow if we look at the correlation.","b80786b5":"Once again we will look at the distribution of values in columns to see effect of our updating.","4d91031a":"### **1. Import Required Libraries and Load Dataset**","c48ff611":"### **3. Exploratory Data Analysis**","cbebb74d":"Let's find out the index of high value entry in SkinThickness and replace it with median.","c1d9795a":"* By looking at pair plot we found that higher glucose level much likely for women with diabetic condition. \n* Likelihood of diabetes based on family history was not evident from plots.\n* Performance of three algorithms namely Logistic Regression, Support vector Machines, Random Forest on the dataset is shown.\n* Based on requirement we need to select performance metric for models. That is if we do not want to miss any positive diagnosis we may require higher precision on positive cases.","b216210a":"There is some considerable difference between positive and negative examples (but not skewed), this is to be taken care of later while splitting examples for training and testing. Its time to look at individual columns and get an idea about central tendency, dispersion etc of them.","eb3723e0":"We list some of observation from above plots:\n* It can be observed that many '0' values appear for columns Glucose, Insulin, BloodPressure and BMI. \n* Also we can observe a SkinThickness value close to 100, which is an unlikely.\n* The columns Pregnancies, DiabetesPedigreeFunction and Age looks fine. \n* Although Number of pregnancies above 10 is also observed, this is to be cross checked with age.\n\nTo be more clear of zero entries we will count the number of zeros in each column.","b929105f":"The real word datasets normally required preparation and cleaning before performing any analysis and the same is carried out in this section. As a first step, we will have a quick look at the data as a pandas dataframe.","3a0a3788":"So this is our data it has 768 rows and 9 columns. Now we need more information about the dataframe including the data types and columns, non-null values etc and we can use the following method.","ec016d04":"### **2. Data Preparation and Cleaning**","eb29573b":"## What is Diabetes?\nDiabetes is a medical condition which a significant percentage of population has to undergo. It impairs the body\u2019s ability to process blood glucose, otherwise known as blood sugar. In the absence of careful attention, highly diabetic condition can increase risk to complicated health problems such as stroke, heart disease etc. \n\n## Dataset and Objective\nThe datasets here consists of several medical predictor variables and a variable indicating diabetes condition (i.e. positively diagnosed or not). Our objective with the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset.\n\nThe columns of dataset are:\n1. Pregnancies - Number of times pregnant\n2. Glucose - Plasma glucose concentration a 2 hours in an oral glucose tolerance\n3. BloodPressure - Diastolic blood pressure (mm Hg)\n4. SkinThickness - Triceps skin fold thickness (mm)\n5. Insulin - 2 Hour serum insulin (mu U\/ml)\n6. BMI - Body mass index (weight in kg\/(height in m)^2)\n7. DiabetesPedigreeFunction - Diabetes pedigree function (indicates likelihood of diabetes based on family history)\n8. Age -  Age (years)\n9. Outcome - Class variable (0 or 1), 1 for diabetic and 0 for non diabetic","80b284f5":"### 4.1. Data Splitting","f42d0ef2":"### 4.5. Random Forest Classifier Model","19f8b339":"### 4.4. Support Vector Machines Model"}}