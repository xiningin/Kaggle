{"cell_type":{"a9112d5c":"code","e52c9d0d":"code","e22d5687":"code","c2b73ae0":"code","d8c92e34":"code","75260190":"code","c0525d96":"code","7d4cbed0":"code","bfade394":"code","5ed73766":"code","3f460e0f":"code","13771759":"code","967cffb5":"code","1da5967b":"code","7b851f9c":"code","99e00917":"code","8ef64c6d":"code","c47e8174":"code","aedc3128":"code","b2bf2f47":"code","e33b865a":"code","b6bee79e":"code","cf0b5cec":"code","1cf71ef7":"code","e299739b":"code","a624f937":"code","919ce663":"code","75682a01":"code","e8d6981b":"code","2edc1038":"code","962d487c":"code","4600fe29":"code","bca49de8":"code","cab1a4ea":"code","90aaa574":"code","5ede9224":"code","06b7edb9":"code","cdc42b14":"code","6cc34993":"code","2b5370d3":"code","3c695a0d":"code","c6e815c5":"code","8f31aff3":"code","62f30e8b":"code","c145f9b9":"code","325e8718":"code","be75c62a":"code","bdba34ee":"code","73b100cd":"code","0f651f63":"code","e1c42be6":"code","e51d2b1e":"code","e72c501d":"code","e4831d4f":"code","b05c2b19":"code","565b6d10":"code","ba641d20":"code","2697770f":"code","c37a7516":"code","7545d921":"code","4a6aae32":"code","da2eae0c":"code","f9f1d5db":"code","89ad2fa3":"code","37c88e6a":"code","da476425":"code","d8b59a6b":"code","4ce0dd53":"code","f63486f0":"code","55d5f1e9":"code","f26d2a69":"code","5c6a4734":"code","4cd770de":"code","bf5b2183":"code","960bbf74":"code","3ad94261":"code","f0945576":"code","35511357":"code","73260dee":"code","55b6f9a6":"code","0ad41d17":"code","37fb61f5":"code","c8e97f88":"code","0aba6294":"code","4ca7ce95":"code","ac4ec47d":"code","65fdac7c":"code","e924fcf6":"code","26e14a9d":"markdown","fa369b3b":"markdown","a81bd287":"markdown","6af21462":"markdown","40acd6a5":"markdown","074e9132":"markdown","42543d8f":"markdown","df48c118":"markdown","49842b4f":"markdown","fdef0cbc":"markdown","2b886bfc":"markdown","aee2f634":"markdown","6ce9d06a":"markdown","197b80ce":"markdown","da2bf418":"markdown","a6160a9d":"markdown","cc6817da":"markdown","53f1c211":"markdown","f8e0c093":"markdown","4a051d1e":"markdown","4ea91d49":"markdown","4ef1e35b":"markdown","b32f91a3":"markdown","f0189572":"markdown","7515bef0":"markdown","a9df6c54":"markdown","fdd51cf6":"markdown","fe77754f":"markdown","0dd4e21b":"markdown","e4c8e45b":"markdown","07634a1b":"markdown","124e8f31":"markdown","fdad4060":"markdown","79b6896c":"markdown","2e6140e4":"markdown","a95b30b1":"markdown","b2e646b1":"markdown","8f4354aa":"markdown","a07cdd41":"markdown","926f434f":"markdown","90c0ea14":"markdown","115b90a2":"markdown","f4ef3d59":"markdown","1a569160":"markdown","85fcd62f":"markdown","dbc16da4":"markdown","d9159771":"markdown","310c29cb":"markdown"},"source":{"a9112d5c":"#Basic\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 1000)\n\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nseq_col_brew = sns.color_palette(\"Accent\", 10)\nsns.set_palette(seq_col_brew)\n\n#Sklearn models and metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n\nfrom scipy.stats import spearmanr\n\n#For random seed\nfrom datetime import datetime\nimport time\n\n%matplotlib inline","e52c9d0d":"\ndef lines(n):\n    print('-'*n)\n    \ndef plot_feature_importance(model,features):\n    importances = model.feature_importances_\n    indices = np.argsort(importances)\n    \n\n    feature_importance = pd.DataFrame()\n    feature_importance['feature'] = [features[i] for i in indices]\n    feature_importance['importance'] = importances[indices]\n    feature_importance\n\n    plt.figure(figsize=(16, 12));\n    plt.title('Feature Importances')\n\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance.sort_values(by=\"importance\", ascending=False));\n\n    plt.xlabel('Relative Importance')\n    plt.show()\n\ndef groupby_recomendation(df,col,sort_by='recommendation'):\n    \n    a = df[[col,'recommendation']].groupby(col)['recommendation'].mean().to_frame()\n    b = df[[col,'recommendation']].groupby(col).count()\n    c = pd.merge(left=a,right=b,left_on  =col, right_on=col, suffixes=['_x','_y'])\n    c.columns = ['recommendation', 'group_size']\n\n    c['relation'] = c['group_size'].apply( lambda x: x\/c['group_size'].sum() )\n    c['relation_mean'] = c['relation'] * c['recommendation']\n    c.sort_values(by=sort_by,ascending=False,inplace=True)\n    return c\n\ndef function_team(df,function='',sort_by='',target='two_years',threshold=0.03,plot=True):\n    \n    if (sort_by == ''):\n        sort_by = target\n        \n    base_ = base[ base['function'] == function]\n    gb = groupby(base_,'team',sort_by=sort_by,target=target)\n    \n    mean_target = gb[target].mean()\n    threshold = mean_target - (threshold*mean_target) \n    gb_less_than = gb.loc[ gb[target] < threshold,[target,'group_size','base percentage (%)'] ]\n       \n    if(plot):\n        plot_function_team(gb,target=target,title=f'function \"{function.upper()}\" with target \"{target.upper()}\"')\n    \n    return gb,gb_less_than\n\ndef groupby(df,col,sort_by='recommendation',target='recommendation'):\n    \n    a = df[[col,target]].groupby(col)[target].mean().to_frame()\n    b = df[[col,target]].groupby(col).count()\n    c = pd.merge(left=a,right=b,left_on  =col, right_on=col, suffixes=['_x','_y'])\n    c.columns = [target, 'group_size']\n\n    c['base percentage (%)'] = c['group_size'].apply( lambda x: round( x\/c['group_size'].sum() * 100, 2) )\n    #c['relation_mean'] = c['relation'] * c[target]\n    c.sort_values(by=sort_by,ascending=False,inplace=True)\n    return c   \n\ndef plot_function_team(df,target='recommendation',title=\"\"):\n    plt.figure(figsize=(10,5))\n    \n    g = sns.barplot(x = df.index, y = df[target] )\n    plt.title(title)\n    ax = sns.lineplot(x = list(range(0,len(df))),y= df[target].mean(),label='mean ({:0.2f})'.format(df[target].mean()))\n    g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n    plt.show()\n    \ndef dump_analysis(df,function='Xpeer',targets=['recommendation']):\n    print(f'\\n\\nAnalisis of function {function} groupby team with targets {targets}\\n\\n')\n    for target in targets:\n        _, less = function_team(df,function,target=target) \n        display(_)\n        print(f'Critical Areas based in {target} : Function {function} groupby *team* ')\n        display(less)\n        \ndef calculate_correlation_all(df,x,method='pearson',threshold =0.5):\n    columns = list( df.columns[1:33] )\n    base_ = df[columns].copy()\n    base_ = base_.fillna(-1)\n\n    lb = LabelEncoder()\n    for cat in columns:\n        base_[cat] = lb.fit_transform(base_[cat].astype(str))\n\n    for column in base_.columns[1:]:\n        \n        corr = get_corr(base_,x,column,method=method)\n        \n        if (abs(corr) > threshold and column != x):\n            print(f'Correlation of {x} with {column} is {corr:0.2f} which mean {corr_info(corr)}')\n\ndef corr_info(x):\n    x = abs(x)\n    msg = ''\n    if (x >= 0.9):\n        msg= ('indicates a very \\033[1mstrong correlation')\n    elif(x >= 0.7 or x > 0.9):\n        msg=  ('indicates a \\033[1mstrong correlation')\n    elif(x >= 0.5 or x > 0.7):\n        msg=  ('indicates a \\033[1mmoderate correlation')\n    elif(x >= 0.3 or x > 0.5):\n        msg=  ('indicates a \\033[1mweak correlation')\n    elif(x >= 0 or x > 0.3):\n        msg=  ('indicates a \\033[1mnegligible correlation')\n    \n    return msg + '\\033[0;0m'\n\n\n\ndef get_corr(df,x,y,method='pearson'):\n    \n    if (method == 'pearson'):\n        return np.corrcoef(x=df[x],y=df[y])[0][1]\n    elif (method == 'spearman'):\n        return spearmanr(df[x],df[y])[0]\n\n\ndef plot_regression(df,x,y,xlim=5,ylim=5):\n\n    sns.set_style(\"whitegrid\")\n    sns.jointplot(x=x,y=y,data=df,kind=\"reg\", color=\"m\",xlim=[1,xlim],ylim=[1,ylim],height=7) \n    sns.boxplot(x=x,y=y,data=df)\n    plt.subplots_adjust(top=0.7)\n    plt.show()\n    pearson = get_corr(df,x,y,method='pearson')\n    spearman = get_corr(df,x,y,method='spearman')\n    print(f'Pearson Correlation Coefficient {pearson} mean {corr_info(pearson)}: ')\n    print(f'Spearman Correlation Coefficient {spearman} mean {corr_info(spearman)}: ')\n\n\ndef grouped_function_team(df,col = 'satisfied',threshold=0.1,fn = 'mean',groupby=['function','team'] ):\n    mean = 0\n    if (fn == 'mean'):\n        mean = df.sort_values(by=col, ascending=False)[col].mean()\n    elif(fn == 'median'):\n        mean = df.sort_values(by=col, ascending=False)[col].median()\n    else:\n        return 'None'\n    \n    base_ = df.copy()\n        \n    if (threshold >= 0 ):\n        base_ = df.loc[ df [col] < (mean - mean * threshold)]\n      \n    \n    a = pd.DataFrame()\n    \n    groupby_ = groupby[:]\n    groupby_.append(col)\n    print(groupby_)\n    \n    if (fn == 'mean'):\n        a = base_[groupby_].groupby(groupby)[col].mean().to_frame()\n    elif(fn == 'median'):\n        a = base_[groupby_].groupby(groupby)[col].median().to_frame()\n\n    \n    b = base_[groupby_].groupby(groupby).count()\n\n    c = a.copy()\n    c['size'] = b[col]\n    m = 0\n    m_a = 0\n    if (fn == 'mean'):\n        m = c[col].mean()\n        m_a = df[col].mean()\n    elif(fn == 'median'):\n        m = c[col].median()\n        m_a = df[col].median()\n        \n    print( 'Total Employees in this group: ',c['size'].sum() )\n    print( f'Total {fn} of {m_a:0.2f} ')\n    print( f'{fn} of {col} is: {m:0.2f}' )\n    print( 'Total Employees in this group: {:0.2f}% of total data'.format(c['size'].sum() \/ len(df) * 100) )\n    return c","e22d5687":"base = pd.read_csv('..\/input\/data.csv')\n","c2b73ae0":"base.head(5)","d8c92e34":"base.shape","75260190":"columns_descriptions = list(base.columns)\ncolumns_descriptions","c0525d96":"columns_shorten = ['timestamp',\n 'function',\n 'team',\n 'location',\n 'work_time',\n 'age',\n 'gender',\n 'proud_to_work',\n 'opportunity',\n 'recognition',\n 'support',\n 'learn_and_grow',\n 'myself',\n 'opinions',\n 'take_risks',\n 'expectations',\n 'oKRs',\n 'job_mission',\n 'care_as_person',\n 'career',\n 'opportunities',\n 'satisfied',\n 'two_years',\n 'positive_actions',\n 'comp_fans',\n 'comp_status_quo',\n 'comp_like_owners',\n 'comp_teams',\n 'comp_efficiency',\n 'recommendation']","7d4cbed0":"col_renames = {}\ncol_description = {}\nfor desc,short in zip(columns_descriptions,columns_shorten):\n    print('\\033[1m'+short,':\\033[0;0m')\n    print('\\tDescription: ', desc)\n    col_renames[desc] = short\n    col_description[short] = desc    ","bfade394":"base.rename( columns=col_renames, inplace=True)","5ed73766":"base.head()","3f460e0f":"base['recommendation'].mean()","13771759":"base.isnull().sum().sort_values(ascending=False)[:5]","967cffb5":"base.fillna('None',inplace=True)","1da5967b":"\nbase_kmeans = pd.read_csv('..\/input\/base_kmeans.csv')\nbase['k-means'] = base_kmeans['k-means']\nbase['k-means'].replace({1:'a',0:'b'},inplace=True)\nbase['k-means'].replace({'a':0,'b':1},inplace=True)","7b851f9c":"\nbase['function'].value_counts().plot(kind='pie', autopct='%.2f%%',figsize=(20,10))\nplt.axis('equal');","99e00917":"base['team'].value_counts().plot(kind='pie', autopct='%.2f%%',figsize=(20,10))\nplt.axis('equal');","8ef64c6d":"base['location'].value_counts().plot(kind='pie', autopct='%.2f%%',figsize=(20,10))\nplt.axis('equal');","c47e8174":"base['work_time'].value_counts().plot(kind='pie', autopct='%.2f%%',figsize=(20,10))\nplt.axis('equal');","aedc3128":"base['age'].value_counts().plot(kind='pie', autopct='%.2f%%',figsize=(20,10))\nplt.axis('equal');","b2bf2f47":"base['gender'].value_counts().plot(kind='pie', autopct='%.2f%%',figsize=(20,10))\nplt.axis('equal');","e33b865a":"#for Team\ngroup = groupby_recomendation(base,'team')\ngroup","b6bee79e":"#Check Relation (Group Size and Recommendation)\nplt.figure(figsize=(10,7))\nsns.regplot(y='group_size',x='recommendation',data=group)","cf0b5cec":"#for  Function\ngroup = groupby_recomendation(base,'function')\ngroup","1cf71ef7":"c3 = group.sort_values(by='group_size',ascending=False).iloc[:3]\nprint(f'Top 3 size : {c3[\"group_size\"].sum()} colaborators has mean equals {c3[\"recommendation\"].mean()}')\nc3","e299739b":"c3 = group.sort_values(by='group_size',ascending=True).iloc[:3]\nprint(f'Last 3 : {c3[\"group_size\"].sum()} colaborators has mean equals {c3[\"recommendation\"].mean()}')\nc3","a624f937":"plt.figure(figsize=(10,7))\nsns.regplot(y='group_size',x='recommendation',data=group)","919ce663":"group = groupby_recomendation(base,'location')\ngroup","75682a01":"group = groupby_recomendation(base,'work_time')\ngroup","e8d6981b":"plt.figure(figsize=(10,5))\nsns.barplot(group.index,group['recommendation'])","2edc1038":"group = groupby_recomendation(base,'age','recommendation')\ngroup","962d487c":"plt.figure(figsize=(20,5))\nsns.barplot(group.index,group['recommendation'])","4600fe29":"group = groupby_recomendation(base,'gender')\ngroup","bca49de8":"base.head()","cab1a4ea":"answering_columns = list (base.columns[7:29])\n\nrows = round( len(answering_columns)\/\/2)\nfig, axarr = plt.subplots(rows, 2, figsize=(20, 4 * rows))\n\nc = 0\nr = 0\nanswers = {'Strongly Agree': 5, 'Agree': 4, 'Neither Agree or Disagree': 3, 'Disagree': 2,'Strongly Disagree':1 }\n\n\n#sns.set_palette(sns.cubehelix_palette(5, start=.5, rot=-.75))\nsns.set_palette(sns.diverging_palette(10, 133, sep=80, n=5))\n\nsize = len(base)\nfor count,col in enumerate(answering_columns):\n    \n    #sns.countplot(base[col].sort_values(), ax=axarr[r][c])\n    \n    ax = sns.countplot( base.iloc[base[col].map(answers).argsort()][col], ax=axarr[r][c] )\n    axarr[r][c].set_title( col_description[col], fontsize=11)\n    \n\n    #ax = (base[col].value_counts()\/len(base)*100).sort_index().plot(kind=\"bar\", rot=0)\n    #ax.set_yticks(np.arange(0, 900, 10))\n\n    #ax2 = ax.twinx()\n    #ax2.set_yticks(np.arange(0, 110, 10)*len(base)\/100)\n    \n    for p in ax.patches:\n        ax.annotate('{:.2f}%'.format(p.get_height()\/len(base) * 100), (p.get_x()+0.15, p.get_height()+1))\n    \n    \n    if c >= 1:\n        c = 0\n        r = r + 1\n    else:\n        c = c + 1\nplt.subplots_adjust(hspace=.3)\nsns.despine()","90aaa574":"base_agree = base[:]\n\n\nanswers = {'Strongly Agree': 5, 'Agree': 4, 'Neither Agree or Disagree': 3, 'Disagree': 2,'Strongly Disagree':1 }\ngenders = {'Male': 3,'Female':2,'I would rather not disclose':1}\nages = {'Over 50':6,'Between 40 and 50 years old':5,'Between 36 and 40 years old':4,'Between 31 and 35 years old':3,'Between 26 and 30 years old':2 ,'Less than 26 years old':1}\nwtime = {'More than 3 years':4,'Between 2 and 3 years':3,'Between 1 and 2 years':2,'Less than 1 year':1}\n\n\ncategorical = {}\ncategorical.update(answers)\ncategorical.update(genders)\ncategorical.update(ages)\ncategorical.update(wtime)\ncategorical\n\nbase_agree = base_agree.replace(categorical)\nbase_agree.head()\n\nanswering_columns = list ( base_agree.columns[7:29] )\n    \nfor count,col in enumerate(answering_columns):\n    base_agree.loc[ base_agree[col] < 4,col] = 0\n    base_agree.loc[ base_agree[col] >= 4,col] = 1\n\nfor count,col in enumerate(answering_columns):\n    base_agree.loc[ base_agree[col] == 0 ,col] = 'Disagree'\n    base_agree.loc[ base_agree[col] == 1,col] = 'Agree'\n    \nanswering_columns_ = ['recognition','take_risks','career','satisfied','opportunities','two_years']\n    \nrows = round( len(answering_columns_)\/\/2)\nfig, axarr = plt.subplots(rows, 2, figsize=(20, 4 * rows))\n\nc = 0\nr = 0\nanswers = {'Strongly Agree': 5, 'Agree': 4, 'Neither Agree or Disagree': 3, 'Disagree': 2,'Strongly Disagree':1 }\n\n\n#sns.set_palette(sns.cubehelix_palette(5, start=.5, rot=-.75))\nsns.set_palette(sns.diverging_palette(10, 133, sep=1, n=2))\n\n\nfor count,col in enumerate(answering_columns_):\n    \n    \n\n    \n    #sns.countplot(base[col].sort_values(), ax=axarr[r][c])\n    \n    ax=sns.countplot( base_agree.iloc[base_agree[col].map(answers).argsort()][col], ax=axarr[r][c])\n    axarr[r][c].set_title( col_description[col], fontsize=11)\n    \n    for p in ax.patches:\n        ax.annotate('{:.2f}%'.format(p.get_height()\/len(base) * 100), (p.get_x()+0.15, p.get_height()+1))\n        \n    if c >= 1:\n        c = 0\n        r = r + 1\n    else:\n        c = c + 1\nplt.subplots_adjust(hspace=.3)\nsns.despine()","5ede9224":"answering_columns.append('recommendation')\nbase[answering_columns][:5]","06b7edb9":"unique_col = {}\nfor col in base.columns:\n    unique_col[col] = base[col].nunique() \n    \npd.DataFrame.from_dict(unique_col,columns=['Uniqueness'],orient='index')","cdc42b14":"display ( base['proud_to_work'].unique() )\ndisplay (  base['gender'].unique() ) \ndisplay (  base['age'].unique() )\ndisplay (  base['work_time'].unique() )","6cc34993":"answers = {'Strongly Agree': 5, 'Agree': 4, 'Neither Agree or Disagree': 3, 'Disagree': 2,'Strongly Disagree':1 }\ngenders = {'Male': 3,'Female':2,'I would rather not disclose':1}\nages = {'Over 50':6,'Between 40 and 50 years old':5,'Between 36 and 40 years old':4,'Between 31 and 35 years old':3,'Between 26 and 30 years old':2 ,'Less than 26 years old':1}\nwtime = {'More than 3 years':4,'Between 2 and 3 years':3,'Between 1 and 2 years':2,'Less than 1 year':1}\n\n\ncategorical = {}\ncategorical.update(answers)\ncategorical.update(genders)\n#categorical.update(ages)\ncategorical.update(wtime)\ncategorical\n\nbase = base.replace(categorical)\nbase.head()\n\n'''base=base.drop('timestamp',axis=1)\nbase = pd.get_dummies(base)'''","2b5370d3":"base.head()","3c695a0d":"#T-Score = (Your Organization's Raw Score - Average Score) \/ Standard Deviation * 10 + 50\nresults = {}\n\nanswering_columns_ = answering_columns[:]\n\n\nfor ans in answering_columns_:\n    t_s = (base[ans].mean() - base[ans].max()) \/ base[ans].std() *10 + 50\n    results[ans] = t_s\n    print(ans,base[ans].mean())\n    \nvalues_sorted = sorted(results, key=results.get)\nvalues_ = []\nfor r in values_sorted:\n    values_.append( float( '{:0.2f}'.format(results[r])  ) )\n\nprint(values_)\n\n#sns.scatterplot(x=list(results.keys() ),y= list(results.values()) )\nplt.figure(figsize=(15,5))\nax = sns.barplot(x=values_sorted,y= values_ )\n#plt.title(title)\n#ax = sns.lineplot(x = list(range(0,len(df))),y= df[target].mean(),label='mean ({:0.2f})'.format(df[target].mean()))\nfor p in ax.patches:\n    _x = p.get_x() + p.get_width() \/ 2\n    _y = p.get_y() + p.get_height()+1\n    value = str( int(p.get_height() )  )\n    ax.text(_x, _y, value, ha=\"center\")\n                \n    #ax.text(_x, _y, value, ha=\"left\")\n    \nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.ylabel('T-Score (%)')\nplt.title('Features that most differ from the Desired Response.')\nplt.plot() ","c6e815c5":"base['recommendation'].describe()","8f31aff3":"'''base['super_fan'] = np.zeros(len(base))\nbase.loc[ base['recommendation'] >= 9, 'super_fan'] = 1\ndisplay(base['super_fan'].value_counts() )\nbase.head()'''","62f30e8b":"base['agreement_score'] = np.zeros(len(base))\n\nanswering_columns_ = answering_columns[:]\n\nanswering_columns_.remove('recommendation')\n\nfor col in answering_columns_:\n    base['agreement_score']  = base['agreement_score']  + base[col]\n\nplt.figure(figsize=(15,5))\nsns.regplot(y=base['agreement_score'],x=base['recommendation'])","c145f9b9":"sns.distplot(base['recommendation'])","325e8718":"plt.figure(figsize=(15,6))\nsns.distplot(base['agreement_score'])","be75c62a":"groupby(base,'function',sort_by='agreement_score',target='agreement_score')","bdba34ee":"grouped_function_team(base,col='agreement_score',fn = 'mean')","73b100cd":"#groupby(base,col='k-means',fn = 'mean',threshold = 0)\nbase_k1 = base.loc[ base['k-means'] == 0]\nprint(len(base_k1))\ngrouped_function_team(base_k1,col='agreement_score',fn = 'mean',threshold = -1)","0f651f63":"base_k1 = base.loc[ base['k-means'] == 0]\ngrouped_function_team(base_k1,col='agreement_score',fn = 'mean',threshold = -1,groupby=['age'] )","e1c42be6":"base_k1 = base.loc[ base['k-means'] == 0]\nprint(len(base_k1))\n\ngrouped_function_team(base_k1,col='agreement_score',fn = 'mean',threshold = -1,groupby=['function','team'] )","e51d2b1e":"grouped_function_team(base,col='satisfied',fn = 'mean')","e72c501d":"grouped_function_team(base,col='opportunities',fn = 'mean')","e4831d4f":"base['two_years_yes_no'] = np.zeros(len(base))\nbase.loc[ base['two_years'] >=4,'two_years_yes_no'] = 1\nbase.loc[ base['two_years'] < 4,'two_years_yes_no'] = 0\n\ngb = groupby(base,'function',sort_by='two_years_yes_no',target='two_years_yes_no')\ngb","b05c2b19":"print('Number of Employees in mean last than 7 is', gb.loc[ gb['two_years_yes_no'] < 0.7,'group_size'].sum() )\nprint('Mean of that group is', gb.loc[ gb['two_years_yes_no'] < 0.7,'two_years_yes_no'].mean() )\ngb.loc[ gb['two_years_yes_no'] < 0.7,'two_years_yes_no'].to_frame()","565b6d10":"base['two_years_yes_no'] = np.zeros(len(base))\nbase.loc[ base['two_years'] >=4,'two_years_yes_no'] = 1\nbase.loc[ base['two_years'] < 4,'two_years_yes_no'] = 0\n\ngb = groupby(base,'team',sort_by='two_years_yes_no',target='two_years_yes_no')\ngb","ba641d20":"print('Number of Employees in mean last than 7 is', gb.loc[ gb['two_years_yes_no'] < 0.7,'group_size'].sum() )\nprint('Mean of that group is', gb.loc[ gb['two_years_yes_no'] < 0.7,'two_years_yes_no'].mean() )\ngb.loc[ gb['two_years_yes_no'] < 0.7,'two_years_yes_no'].to_frame()","2697770f":"gb = groupby(base,'function',sort_by='two_years_yes_no',target='two_years_yes_no')\ngb = gb.loc[ gb['two_years_yes_no'] < 0.8,'two_years_yes_no'].to_frame()\ngb","c37a7516":"'''gb = groupby(base,'function',sort_by='two_years_yes_no',target='two_years_yes_no')\ngb = gb.loc[ gb['two_years_yes_no'] < 0.8,'two_years_yes_no'].to_frame()\ngb'''\n\nfor function in list(gb.index):\n    print(f'Analysis for {function}')\n    #dump_analysis(base,function=function,targets=['recommendation','two_years','two_years_yes_no','k-means'])   \n    dump_analysis(base,function=function,targets=['two_years','two_years_yes_no'])   ","7545d921":"columns = list (base.columns[1:34] )\nbase_ = base[ columns ].copy()\nbase_ = base_.fillna(-1)\n\nlb = LabelEncoder()\nfor cat in columns:\n    base_[cat] = lb.fit_transform(base_[cat].astype(str))\n    \n #spearman is alternative to pearson for non parametric data(Ordinal like likert scale)   \ncorrmat = base_.corr(method='spearman')\nplt.figure(figsize=(20,20))\nsns.heatmap(corrmat,annot=True,cmap=\"RdYlGn\")","4a6aae32":"corr_matrix = base.corr(method='spearman').abs()\n\nthreshold = 0.5\n    \nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\ncolumns = [column for column in upper.columns if any(upper[column] > threshold)]\ncolumns.append('satisfied')\ncolumns.append('recommendation')\n\nplt.figure(figsize=(15,10))\nsns.heatmap(base[columns].copy().corr(),annot=True,cmap=\"RdYlGn\")","da2eae0c":"plot_regression(base_,x='satisfied',y='two_years')","f9f1d5db":"plot_regression(base_,x='satisfied',y='opportunities')","89ad2fa3":"plot_regression(base_,x='satisfied',y='work_time')","37c88e6a":"plot_regression(base_,x='two_years',y='agreement_score',ylim=10)\n#sns.boxplot(x='satisfied',y='recommendation',data=base_)","da476425":"plot_regression(base_,x='satisfied',y='recommendation',ylim=10)","d8b59a6b":"plot_regression(base,x='two_years_yes_no',y='k-means',ylim=1)","4ce0dd53":"calculate_correlation_all(base,'two_years',method='pearson')","f63486f0":"calculate_correlation_all(base,'two_years',method='spearman')","55d5f1e9":"calculate_correlation_all(base,'opportunities')","f26d2a69":"calculate_correlation_all(base,'opportunities',method='spearman')","5c6a4734":"calculate_correlation_all(base,'satisfied')","4cd770de":"calculate_correlation_all(base,'satisfied',method='spearman')","bf5b2183":"calculate_correlation_all(base,'k-means',method='spearman')","960bbf74":"columns = list( base.columns[1:33] )\nbase_ = base[ columns].copy()\nbase_ = base_.fillna(-1)\n\nlb = LabelEncoder()\nfor cat in columns:\n    base_[cat] = lb.fit_transform(base_[cat].astype(str))\n    \n    \ncorr_matrix = base_.corr(method='spearman')\n\nsns.clustermap(corr_matrix, method='ward', cmap='RdYlGn', annot=True,\n               vmin=-1, vmax=1, figsize=(20,20))\n\nplt.title(\"Correlations Between Desired Benefits\")\nplt.tight_layout()\n\nplt.show()","3ad94261":"base = base.drop('timestamp',axis=1)\nbase = base.fillna(-1)\n\nlb = LabelEncoder()\nfor cat in list ( base.describe(include=['O']).columns ):\n    base[cat] = lb.fit_transform(base[cat].astype(str))","f0945576":"base.head()","35511357":"'''X = base.drop(['agreement_score'],axis=1)\nX = X.drop('recommendation',axis=1)\ny = base['super_fan'].values\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 10)\n\nprint('Training Features Shape:', X_train.shape)\nprint('Training Target Shape:', y_train.shape)\nprint('Validation Features Shape:', X_val.shape)\nprint('Validation Target Shape:', y_val.shape)\n\nmodel = RandomForestClassifier(n_estimators = 1000, random_state = 42,max_depth=2)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_val)\nacc = accuracy_score(y_val, y_pred)\nprint(\"The acc is {:0.2f}%\".format(acc))\n\nplot_feature_importance(model, X.columns.to_list())'''","73260dee":"from sklearn.ensemble import RandomForestRegressor\n\nX = base.drop(['recommendation','agreement_score'],axis=1)\ny = base['recommendation'].values\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 10)\n                                                  \nprint('Training Features Shape:', X_train.shape)\nprint('Training Target Shape:', y_train.shape)\nprint('Validation Features Shape:', X_val.shape)\nprint('Validation Target Shape:', y_val.shape)\n\n\nmodel = RandomForestRegressor(n_estimators = 10000, random_state = int(time.time()),max_depth=2)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_val)\n\nprint('Erro M\u00e9dio Absoluto', mean_absolute_error(y_val, y_pred)  )\n\nplot_feature_importance(model, list ( X.columns ))","55b6f9a6":"X = base.drop(['recommendation','two_years','agreement_score','two_years_yes_no'],axis=1)\ny = base['two_years'].values\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 10)\n                                                  \nprint('Training Features Shape:', X_train.shape)\nprint('Training Target Shape:', y_train.shape)\nprint('Validation Features Shape:', X_val.shape)\nprint('Validation Target Shape:', y_val.shape)\n\n\nmodel = RandomForestRegressor(n_estimators = 10000, random_state = int(time.time()),max_depth=9)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_val)\n\n\nprint('Erro M\u00e9dio Absoluto', mean_absolute_error(y_val, y_pred)  )\n\nplot_feature_importance(model, list ( X.columns ))","0ad41d17":"X = base.drop(['recommendation','agreement_score','two_years','two_years_yes_no'],axis=1)\ny = base['two_years'].values\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 10)\n                                                  \nprint('Training Features Shape:', X_train.shape)\nprint('Training Target Shape:', y_train.shape)\nprint('Validation Features Shape:', X_val.shape)\nprint('Validation Target Shape:', y_val.shape)\n\n\nmodel = RandomForestClassifier(n_estimators = 10000, random_state = int(time.time()),max_depth=5)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_val)\n\n\ny_pred = model.predict(X_val)\nacc = accuracy_score(y_val, y_pred)\nprint(\"The acc is {:0.2f}%\".format(acc))\n\nplot_feature_importance(model, list( X.columns ))","37fb61f5":"X.head()","c8e97f88":"'''base['two_years_yes_no'] = np.zeros(len(base))\nbase.loc[ base['two_years'] >=4,'two_years_yes_no'] = 1\nbase.loc[ base['two_years'] < 4,'two_years_yes_no'] = 0\n\n\n\nX = base.drop(['recommendation','k-means','two_years','agreement_score','two_years_yes_no','k-means'],axis=1)\n\nbase['function'] = base_kmeans['function']\n#X = base.loc[base['function'] == 'Xpeer']\nX['two_years_yes_no'] = base['two_years_yes_no'].values\n\nX = X.loc[X['function'] == 'Xpeer']\nX['function'] = lb.fit_transform(X['function'])\ny = X.drop(['two_years_yes_no'],axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 10)\n                                                  \nprint('Training Features Shape:', X_train.shape)\nprint('Training Target Shape:', y_train.shape)\nprint('Validation Features Shape:', X_val.shape)\nprint('Validation Target Shape:', y_val.shape)\n\n\nmodel = RandomForestClassifier(n_estimators = 10000, random_state = int(time.time()),max_depth=9)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_val)\n\nacc = accuracy_score(y_val, y_pred)\nprint(\"The acc is {:0.2f}%\".format(acc))\n\nplot_feature_importance(model, X.columns.to_list())'''","0aba6294":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import tree\nfrom sklearn.datasets import load_wine\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display                               \nfrom ipywidgets import interactive\n\nX = base.drop(['recommendation','two_years','agreement_score','two_years_yes_no'],axis=1)\ny = base['two_years'].values\n\nlabels = X.columns\n\ndef plot_tree(crit, split, depth=2):\n    estimator = DecisionTreeClassifier(random_state = 0 \n          , criterion = crit\n          , splitter = split\n          , max_depth = depth)\n    estimator.fit(X, y)\n    graph = Source(tree.export_graphviz(estimator\n          , out_file=None\n          , proportion = True\n          , rounded = True\n          , feature_names=labels\n          , class_names=['Strongly Disagree','Disagree','Neither Agree or Disagree','Agree','Strongly Agree']                                        \n          , filled = True))\n    display(SVG(graph.pipe(format='svg')))\n    return estimator\n\ninter=interactive(plot_tree \n   , crit = [\"gini\", \"entropy\"]\n   , split = [\"best\", \"random\"]\n   , depth=(1,10) )\n\ndisplay(inter)","4ca7ce95":"def plot_history(history, label):\n    plt.plot(history.history['loss'],label='loss')\n    plt.plot(history.history['binary_accuracy'],label='acc')\n    plt.title('Loss for %s' % label)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(loc='upper left')\n    plt.show()","ac4ec47d":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\n\nbase_ = pd.read_csv('..\/input\/data.csv')\nbase_.rename( columns=col_renames, inplace=True)\n\nX_ = base_.drop(['timestamp','recommendation','two_years'],axis=1)\nresults = {}\n\n#for col in X_.columns:\n#X = X_[[col]].copy()\n\n#X = X[['satisfied','opportunities','proud_to_work','positive_actions']].copy()\n#satisfied = 85% acc\n#opportunities = 94% acc\n#proud_to_work = 0.92% acc\n#positive_actions = 0.93% acc\n#positive_actions = 0.93% acc\n\nX = pd.get_dummies(X)\ny = base['two_years_yes_no'].values\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 10)\n\nclassify = Sequential()\nclassify.add( Dense( units=300, activation='relu', input_dim=X.shape[1]  ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\nclassify.add(Dropout(0.2))\nclassify.add( Dense( units=300, activation='relu' ) )\n\nclassify.add(Dense(units=1, activation='sigmoid'))\n\nclassify.compile( optimizer='adam',loss='binary_crossentropy', metrics=['binary_accuracy'])\n\nhistory = classify.fit(x_train, y_train, batch_size=100, epochs=20,verbose=0)\ny_pred = classify.predict( x_test )\ny_pred = y_pred > 0.5\n\nacc = accuracy_score(y_test, y_pred)\n#results[col] = acc\n\nprint(\"The Accuracy of\",col,' is ', acc)\nplot_history(history, '')\n#results ","65fdac7c":"results = {'function': 0.7676767676767676,\n 'team': 0.7676767676767676,\n 'location': 0.7676767676767676,\n 'work_time': 0.7676767676767676,\n 'age': 0.7676767676767676,\n 'gender': 0.7676767676767676,\n 'proud_to_work': 0.7575757575757576,\n 'opportunity': 0.8383838383838383,\n 'recognition': 0.7676767676767676,\n 'support': 0.8080808080808081,\n 'learn_and_grow': 0.7777777777777778,\n 'myself': 0.7878787878787878,\n 'opinions': 0.7272727272727273,\n 'take_risks': 0.7676767676767676,\n 'expectations': 0.7676767676767676,\n 'oKRs': 0.7676767676767676,\n 'job_mission': 0.7878787878787878,\n 'care_as_person': 0.7575757575757576,\n 'career': 0.7676767676767676,\n 'opportunities': 0.8585858585858586,\n 'satisfied': 0.8585858585858586,\n 'positive_actions': 0.8080808080808081,\n 'comp_fans': 0.7272727272727273,\n 'comp_status_quo': 0.7777777777777778,\n 'comp_like_owners': 0.8080808080808081,\n 'comp_teams': 0.7676767676767676,\n 'comp_efficiency': 0.7676767676767676,\n 'satisfied and opportunities': 0.94\n}","e924fcf6":"\nresults = {'function': 0.7676767676767676,\n 'team': 0.7676767676767676,\n 'location': 0.7676767676767676,\n 'work_time': 0.7676767676767676,\n 'age': 0.7676767676767676,\n 'gender': 0.7676767676767676,\n 'proud_to_work': 0.7575757575757576,\n 'opportunity': 0.8383838383838383,\n 'recognition': 0.7676767676767676,\n 'support': 0.8080808080808081,\n 'learn_and_grow': 0.7777777777777778,\n 'myself': 0.7878787878787878,\n 'opinions': 0.7272727272727273,\n 'take_risks': 0.7676767676767676,\n 'expectations': 0.7676767676767676,\n 'oKRs': 0.7676767676767676,\n 'job_mission': 0.7878787878787878,\n 'care_as_person': 0.7575757575757576,\n 'career': 0.7676767676767676,\n 'opportunities': 0.8585858585858586,\n 'satisfied': 0.8585858585858586,\n 'positive_actions': 0.8080808080808081,\n 'comp_fans': 0.7272727272727273,\n 'comp_status_quo': 0.7777777777777778,\n 'comp_like_owners': 0.8080808080808081,\n 'comp_teams': 0.7676767676767676,\n 'comp_efficiency': 0.7676767676767676,\n 'satisfied and opportunities': 0.94\n}\n\nvalues_sorted = sorted(results, key=results.get)\nvalues_ = []\nfor r in results:\n    values_.append( float( '{:0.2f}'.format(results[r])  )*100 )\nplt.figure(figsize=(15,5))\nax = sns.barplot(x=values_sorted,y= values_ )\nfor p in ax.patches:\n    _x = p.get_x() + p.get_width() \/ 2\n    _y = p.get_y() + p.get_height()+1\n    value = str( int(p.get_height() )  )+'%'\n    ax.text(_x, _y, value, ha=\"center\")\n                \nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.ylabel('Accuracy Score (%)')\nplt.title('Most important features for the Neural Network')\nplt.show()","26e14a9d":"## two_years: Testing Classifier","fa369b3b":"### Check GroupBy Recomendations","a81bd287":"# Checking the Answers","6af21462":"# Table of Contents:\n\n**1. [Problem Definition](#id1)** <br>\n**2. [Load the Data](#id2)** <br>\n**3. [Exploratory Data Analysis](#id3)** <br>\n**4. [Model](#id4)** <br>\n**5. [Answers](#id5)** <br>","40acd6a5":"# New Feature: two_years_yes_no\n\n## Groupby Function","074e9132":"### Shorten column name\n\nI'll shorten the column name to work better. Below is the dictionary of shortened terms:","42543d8f":"## Groupby team","df48c118":"It's seems there's no relationship between recommendation and group_size for groupby team.","49842b4f":"#### GroupBy work_time","fdef0cbc":"### Conclusions\n# Como era de se esperar, a recomenda\u00e7\u00e3o final n\u00e3o tem muito a ver com as outras respostas do question\u00e1rio. Vamos ver melhor isso plotando gr\u00e1ficos de correla\u00e7\u00e3o individuais","2b886bfc":"# Analysis Routine","aee2f634":"<a id=\"id1\"><\/a> <br> \n# **1. Problem Definition:** ","6ce9d06a":"# Prediting Super Fan (Classification)","197b80ce":"# Checking some relations","da2bf418":"### Get 10% below the Agreement Score","a6160a9d":"# Nubank's People Analytics\n\nNubank's People Analytics team is having a hard time analyzing the results of their last engagement survey and have asked for your help. \n","cc6817da":"# Feature Engineering: SuperFan","53f1c211":"#### GroupBy Function","f8e0c093":"Context: Nubank's People Analytics team is having a hard time analyzing the results of their last engagement survey and have asked for your help. \n\n\n> Dive into the data and answer the following questions:\n\n**1) Survey Content**<br>\nDo you see any issues with the questions that were asked?<br>\n\n\n**2) Survey Analysis**<br>\nWhat are some of the main insights that you draw from the results? (see historical data tab for past results)<br>\nAre there any areas of concern?<br>\n\n**3) Attrition deep-dive**<br>\n*There has been an increase in regrettable attrition and we'd like to get a better understanding on whether certain areas are more at risk of leaving the company. You decide to take a closer look at the question: \"I see myself still working at Nubank in two years' time.\"*<br>\n\nAre there any factors that help determine the likelihood of someone seeing themselves at Nubank in 2 yrs' time or not?<br>\n\n**4) Next steps**<br>\nBased on your findings in (2) and (3), what suggestions would you provide to Nubank to improve the main painpoints? What studies would you do to test your hypotheses?<br>\nAnything else?<br>\n\n*Obs. Xpeers = Customer Service Agents*","4a051d1e":"<a id=\"id5\"><\/a> <br> \n# **5. Answers:**\n\nSlides","4ea91d49":"Some initial conclusions:<br>\nIt can be concluded that there are 5 alternatives to most of the questions. Except for Timestamp, function, team, location, work_time, age, gender and recomendation\n","4ef1e35b":"<a id=\"id4\"><\/a> <br> \n# **4. Model Creation:** \n","b32f91a3":"The regression curve is slightly negative. <br>\nPerhaps the data suggest that the smaller the group size, the higher the recommendation.<br>\n","f0189572":"<a id=\"id2\"><\/a> <br> \n# **2. Load the Data:** ","7515bef0":"Verificando o coeficiente de correla\u00e7\u00e3o\n* 0.9 para mais ou para menos indica uma correla\u00e7\u00e3o **muito forte**.\n* 0.7 a 0.9 positivo ou negativo indica uma correla\u00e7\u00e3o **forte**.\n* 0.5 a 0.7 positivo ou negativo indica uma correla\u00e7\u00e3o **moderada**.\n* 0.3 a 0.5 positivo ou negativo indica uma correla\u00e7\u00e3o **fraca**.\n* 0 a 0.3 positivo ou negativo indica uma correla\u00e7\u00e3o **desprez\u00edvel**.","a9df6c54":"# Prediting Recommendation (Regression)","fdd51cf6":"# Prediting TwoYears\n\n\n\"I see myself still working at Nubank in two years' time.\"\n## Testing Regression","fe77754f":"### All Functions used in this notebook","0dd4e21b":"<p style=\"color:red\">N\u00e3o sei como os dados foram coletados, n\u00e3o sei se o question\u00e1rio foi nomeado.<br>\nMas acho que \u00e9 baixa a probabilidade da coluna 'recommendation' significar a realidade do pensamento do funcion\u00e1rio. <br>\nEm um question\u00e1rio como esse o funcion\u00e1rio pode achar que isso <br>\nEm um question\u00e1rio desse tipo (principalmente se for nomeado) a pergunta 'alvo' deveria estar mascarada.","e4c8e45b":"# Specific Group Analisys\n\nVamos analisar grupos com m\u00e9dia abaixo de 0.8 quando perguntado se ficaram na nubank por at\u00e9 2     ","07634a1b":"# Findings\n\nFor some reason, Engineer have the very low recommendation average.<br>\nIn addition, this function has a considerable number of employees, which further strengthens the metric.<br>\n","124e8f31":"#### GroupBy Team","fdad4060":"### Imports","79b6896c":"# Features that most differ from the Desired Response","2e6140e4":"### Step : Check the uniqueness of the data","a95b30b1":"It can be concluded that areas with values below 7 score have a high risk of dissatisfaction. We can conclude that this group is the group where most of the employees that do not see working in nubank in the next two years.","b2e646b1":"#### Replacing Categorical Values\n> I chose to directly replace the categorical variables to better control the substituted values. Higher values means more positive values, eg. Agree = 4, Disagree = 3\n","8f4354aa":"<a id=\"id3\"><\/a> <br> \n# **3. EDA (Exploratory Data Analysis):** ","a07cdd41":"# Grouped with K-Means","926f434f":"#### Observations\nThe team **Public Policy** is the team with the best recommendation and **Mgmt Team Support** is the team with the lowest average.<br>\nHowever, **Public Policy** and **Mgmt Team Support** has a very small team and any low vote impacts a lot on the average. <br>\nChargeback already has a considerable number of employees and even so the average recommendation is below 9<br>\n\n","90c0ea14":"<hr>","115b90a2":"#### GroupBy Location","f4ef3d59":"In this part, the average value of the recommendation attribute will be analyzed according to values given in the attributes.<br>\nThis analysis answers questions such as: Which teams gave the highest recommendation value on average?\n","1a569160":"### Groupby ","85fcd62f":"### Satisfied x TwoYears","dbc16da4":"# Feature Engineering: Agreement Score","d9159771":"# Checking Correlations","310c29cb":"## New Feature: Two years yes or no\n\n\nA new feature called 'two_years_yes_no' was created, in this new feature, if the employee answers 5 or 4 in question 'I see myself still working at Nubank in two years' time', it is considered that he will be two years. "}}