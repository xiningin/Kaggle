{"cell_type":{"2f35dec4":"code","2916c82b":"code","406c3b04":"code","1843f0c7":"code","1ba4bb73":"code","521d84c8":"code","1355bd07":"code","c5a50915":"code","aceb9dd1":"code","e841e71f":"code","acce4c57":"code","1a7814c4":"code","2ffb46bc":"code","f8caed2d":"code","14b8d9cb":"code","7c90a09d":"code","a13463d4":"code","41f0e953":"code","67e0873c":"code","88a442d5":"code","d4288a4a":"code","997951e4":"code","58c97ee5":"code","bf5fc7ec":"code","a9328e7c":"code","07c2095a":"code","d6635968":"code","5de6a76f":"code","bb70fb02":"code","cca6b0eb":"code","517c6d9b":"code","31f81af0":"code","342e105d":"code","e6c036be":"code","9999f897":"code","a163d8e7":"code","48bf628d":"code","043316d7":"code","aa2a8b8f":"code","9b768ffe":"code","9a96c7c8":"code","48042f06":"code","4f7b272f":"code","a96ae77b":"code","ebcc27ee":"code","7ed5e6c8":"code","344d224e":"code","93bf8407":"code","55c2b642":"code","88e2faa5":"code","3bd75d07":"code","001a0925":"code","eafe33ee":"code","35ce88d3":"code","524a9a0b":"code","8363ea8c":"code","ac65634e":"code","e0efa1c4":"code","963ba016":"code","02bcd44a":"code","3c781be5":"code","09709b63":"code","73db9239":"code","829cd56a":"code","9cbe0ce2":"code","3801e92b":"code","45a349a9":"code","106c3f9f":"code","995af047":"markdown","3750eaf1":"markdown","33112be2":"markdown","76c6aca7":"markdown","c5964939":"markdown","77df07b3":"markdown","5d754de0":"markdown","a30665c3":"markdown","1fe07897":"markdown","6472a922":"markdown","e589097e":"markdown","fba77f6d":"markdown","65876980":"markdown","7b4bf45a":"markdown","73cdce87":"markdown","76422440":"markdown","85ce5d9f":"markdown","1fa15fbf":"markdown","ce406583":"markdown","e56b995e":"markdown","c66401bf":"markdown","a05e31bd":"markdown","52f8d6f8":"markdown","a029c7a6":"markdown","1d75daa3":"markdown","e04df727":"markdown"},"source":{"2f35dec4":"!pip install bioinfokit","2916c82b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas\nimport os\nimport gc\nimport pylab \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr, probplot, norm, shapiro\n%matplotlib inline\nimport statsmodels.api as sm\n\npal = sns.color_palette()\npd.set_option(\"display.max_columns\", 50)\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly.tools as tls\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n#Performing the Multiple Linear Regression\n\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\n\nfrom sklearn import linear_model\nimport tkinter as tk \nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n\n#---------------------------------------------------LDA with Python from scratch\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom sklearn.model_selection import train_test_split\nstyle.use('fivethirtyeight')\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# get ANOVA table as R like output\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# #------------------------------------------------- evaluate lda with naive bayes algorithm for classification\n# from numpy import mean\n# from numpy import std\n# from sklearn.datasets import make_classification\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import RepeatedStratifiedKFold\n# from sklearn.pipeline import Pipeline\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# from sklearn.naive_bayes import GaussianNB\n\nimport scipy.stats as stats\n\n#---------------------------pip install bioinfokit\nfrom bioinfokit.analys import stat\n\nfrom scipy.stats import chi2_contingency \n\n\n#-----------------------------------------------libraries required\n#nltk.download('stopwords') install\n#nltk.download('wordnet')  install\n\nimport io\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport random\n\n#------------------------------------------------------------Word Cloud\n#pip install wordcloud\n\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","406c3b04":"#data description of DFP\ndf1= pd.read_excel(r\"\/kaggle\/input\/dash-assignment\/DFP.xlsx\",\"Data\")\ndf1","1843f0c7":"df=df1\ndf.head(3)","1ba4bb73":"# position column category has top,bottom,middle,leaderboard,passback\ndf['position']=np.nan\nlist=['top']\nval=df['AD_UNIT_NAME'].apply(lambda x: any([k in x for k in list]))\nmy_ind=[i for i in val.index if val[i]]\ndf['position'].iloc[my_ind]='top'\nlist=['bottom']\nval=df['AD_UNIT_NAME'].apply(lambda x: any([k in x for k in list]))\nmy_ind=[i for i in val.index if val[i]]\ndf['position'].iloc[my_ind]='bottom'\nlist=['middle']\nval=df['AD_UNIT_NAME'].apply(lambda x: any([k in x for k in list]))\nmy_ind=[i for i in val.index if val[i]]\ndf['position'].iloc[my_ind]='middle'\nlist=['leaderboard']\nval=df['AD_UNIT_NAME'].apply(lambda x: any([k in x for k in list]))\nmy_ind=[i for i in val.index if val[i]]\ndf['position'].iloc[my_ind]='leaderboard'\nlist=['passback']\nval=df['AD_UNIT_NAME'].apply(lambda x: any([k in x for k in list]))\nmy_ind=[i for i in val.index if val[i]]\ndf['position'].iloc[my_ind]='passback'","521d84c8":"# checking position value\ndf[df['position']!='top'][df['position']!='bottom'][df['position']!='middle'][df['position']!='leaderboard'][df['position']!='passback']\n","1355bd07":"#story column\ndf['story']=np.nan\ndf['story'] = df.AD_UNIT_NAME.str.split(\"_\",expand=True)[2] ","c5a50915":"# amp_or_non_amp column\ndf['amp_or_non_amp']=\"Nonamp\"\nlist=['amp']\nval=df['AD_UNIT_NAME'].apply(lambda x: any([k in x for k in list]))\nmy_ind=[i for i in val.index if val[i]]\ndf['amp_or_non_amp'].iloc[my_ind]='Amp'\n","aceb9dd1":"#Day's Name\ndays_name={1:\"Monday\",\n              2:\"Tuesday\",\n              3:\"Wednesday\",\n              4:\"Thursday\",\n              5:\"Friday\",\n              6:\"Saturday\",\n              7:\"Sunday\",\n           }\ndf['days_name']=df['DAY'].map(days_name)\n","e841e71f":"#data description of DFP\ndf1= pd.read_csv(r\"\/kaggle\/input\/dash-assignment\/Actual_eCPM.csv\")\ndf1","acce4c57":"df=pd.merge(df, df1, on='LINE_ITEM_NAME', how='right')\nval=df[df['Actual_eCPM']=='-'].index\ndf['Actual_eCPM'].iloc[val]=0\ndf['Actual_eCPM']=df['Actual_eCPM'].astype(str).astype(\"float64\")\ndf['Actual_Revenue']=0\nTotal_Impr=df['Impressions'].sum()\ndf['Actual_Revenue']=Total_Impr*df['Actual_eCPM']","1a7814c4":"df.dtypes","2ffb46bc":"df","f8caed2d":"# df.to_csv('DFP_solution.csv',index=False)","14b8d9cb":"df1=df[df['amp_or_non_amp']=='Amp'][df['eCPM']>77][df['Revenue']>455]['position']\n\ndf2=df[df['amp_or_non_amp']=='Nonmp'][df['eCPM']>77][df['Revenue']>455]['position']\n\nframes=[df1,df2]\n\ndff=pd.concat(frames,axis=0,ignore_index=True)\n","7c90a09d":"dff","a13463d4":"# dff.to_csv('Adpos.csv',index=False)\n","41f0e953":"#data description of DFP\ndf= pd.read_excel(r\"\/kaggle\/input\/dash-assignment\/DFP.xlsx\",\"Data\")\ndf","67e0873c":"data=df\ndata.columns","88a442d5":"# data = df[['DAY', 'Tags_served', 'Impressions','Clicks','CTR','Revenue','eCPM']].copy()\ndata=df\ndata=data.reindex(columns=['DAY','Tags_served','Impressions','Clicks','CTR','Revenue','eCPM','AD_UNIT_NAME',\n                          'ORDER_NAME','ADVERTISER_NAME','LINE_ITEM_NAME','DATE'])\n","d4288a4a":"data['DATE']=data['DATE'].astype(str)\n","997951e4":"fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15,20))\nax= axes.flatten()\nfor i, val in enumerate(data.columns.values[:7]):\n    sm.qqplot(data[val], fit = True, line='q', ax=ax[i])\n    ax[i].legend([val])\nplt.show()","58c97ee5":"#Dimension of the data\nprint(data.shape)","bf5fc7ec":"# datatype of each attribute\ndata.dtypes","a9328e7c":"# descriptive statistics\ndata.describe()","07c2095a":"# class distributions (classification only)\nclass_counts = data.groupby('DAY').size()\nprint(class_counts)","d6635968":"# correlation\npandas.set_option('display.width', 100)\npandas.set_option('precision', 3)\ncorrelations = data.corr(method='pearson')\nprint(correlations)","5de6a76f":"# To find the covariance  \ndf.cov() ","bb70fb02":"# skewness of univariate distributions\nskew = data.skew()\nprint(skew)","cca6b0eb":"plt.scatter(df['DAY'], df['Impressions'], color='red')\nplt.title('Impressions  Vs DAY', fontsize=14)\nplt.xlabel('DAY', fontsize=14)\nplt.ylabel('Impressions', fontsize=14)\nplt.grid(True)\nplt.show()","517c6d9b":"plt.scatter(df['DAY'], df['Clicks'], color='green')\nplt.title('Clicks Vs DAY', fontsize=14)\nplt.xlabel('DAY', fontsize=14)\nplt.ylabel('Clicks', fontsize=14)\nplt.grid(True)\nplt.show()","31f81af0":"plt.scatter(df['DAY'], df['Revenue'], color='blue')\nplt.title('Revenue Vs DAY', fontsize=14)\nplt.xlabel('DAY', fontsize=14)\nplt.ylabel('Revenue', fontsize=14)\nplt.grid(True)\nplt.show()","342e105d":"X = df[['Impressions', 'Clicks']]\nY = df['DAY']\n\n# with sklearn\n# regr = linear_model.LinearRegression()\n# regr.fit(X, Y)\n\nregr = LinearRegression().fit(X, Y)\n\nprint('Intercept: \\n', regr.intercept_)\nprint('Coefficients: \\n', regr.coef_)\n\n# prediction with sklearn\nNew_Impressions = 8000\nNew_Clicks = 4000\n\nprint ('Predicted Stock Index Price: \\n', regr.predict([[New_Impressions ,New_Clicks]]))\n\n# with statsmodels\nX = sm.add_constant(X) # adding a constant\n \nmodel = sm.OLS(Y, X).fit()\npredictions = model.predict(X) \n \nprint_model = model.summary()\nprint(print_model)","e6c036be":"X = df[['Impressions', 'Clicks']]\nY = df['DAY']\n# with sklearn\nregr = linear_model.LinearRegression()\nregr.fit(X, Y)\n\nprint('Intercept: \\n', regr.intercept_)\nprint('Coefficients: \\n', regr.coef_)\n\n#----------------------------------------------------------------------------------------------------- tkinter GUI\n# root= tk.Tk()\n\n# canvas1 = tk.Canvas(root, width = 500, height = 300)\n# canvas1.pack()\n\n# # with sklearn\n# Intercept_result = ('Intercept: ', regr.intercept_)\n# label_Intercept = tk.Label(root, text=Intercept_result, justify = 'center')\n# canvas1.create_window(260, 220, window=label_Intercept)\n\n# # with sklearn\n# Coefficients_result  = ('Coefficients: ', regr.coef_)\n# label_Coefficients = tk.Label(root, text=Coefficients_result, justify = 'center')\n# canvas1.create_window(260, 240, window=label_Coefficients)\n\n# # New_Interest_Rate label and input box\n# label1 = tk.Label(root, text='Type Impressions: ')\n# canvas1.create_window(100, 100, window=label1)\n\n# entry1 = tk.Entry (root) # create 1st entry box\n# canvas1.create_window(270, 100, window=entry1)\n\n# # New_Unemployment_Rate label and input box\n# label2 = tk.Label(root, text=' Type Clicks: ')\n# canvas1.create_window(120, 120, window=label2)\n\n# entry2 = tk.Entry (root) # create 2nd entry box\n# canvas1.create_window(270, 120, window=entry2)\n\n# def values(): \n#     global New_Impressions #our 1st input variable\n#     New_Impressions = float(entry1.get()) \n    \n#     global New_Clicks #our 2nd input variable\n#     New_Clicks = float(entry2.get()) \n    \n#     Prediction_result  = ('Predicted Day: ', regr.predict([[New_Impressions ,New_Clicks]]))\n#     label_Prediction = tk.Label(root, text= Prediction_result, bg='orange')\n#     canvas1.create_window(260, 280, window=label_Prediction)\n    \n# button1 = tk.Button (root, text='Predict Day',command=values, bg='orange') # button to call the 'values' command above \n# canvas1.create_window(270, 150, window=button1)\n \n# #plot 1st scatter \n# figure3 = plt.Figure(figsize=(5,4), dpi=100)\n# ax3 = figure3.add_subplot(111)\n# ax3.scatter(df['DAY'].astype(float),df['Impressions'].astype(float), color = 'r')\n# scatter3 = FigureCanvasTkAgg(figure3, root) \n# scatter3.get_tk_widget().pack(side=tk.RIGHT, fill=tk.BOTH)\n# ax3.legend(['Day']) \n# ax3.set_xlabel('Impressions')\n# ax3.set_title('Impressions Vs. Day')\n\n# #plot 2nd scatter \n# figure4 = plt.Figure(figsize=(5,4), dpi=100)\n# ax4 = figure4.add_subplot(111)\n# ax4.scatter(df['DAY'].astype(float),df['Clicks'].astype(float), color = 'g')\n# scatter4 = FigureCanvasTkAgg(figure4, root) \n# scatter4.get_tk_widget().pack(side=tk.RIGHT, fill=tk.BOTH)\n# ax4.legend(['Day']) \n# ax4.set_xlabel('Clicks')\n# ax4.set_title('Clicks Vs. Day')\n\n# root.mainloop()","9999f897":"# 0. Load in the data and split the descriptive and the target feature\n\nX = df[['Impressions','Clicks','Revenue','eCPM']].copy()\ntarget = df['DAY'].copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X,target,test_size=0.3,random_state=0) \n\n\n# 1. Standardize the data\nfor col in X_train.columns:\n    X_train[col] = StandardScaler().fit_transform(X_train[col].values.reshape(-1,1))\n\n\n\n# 2. Compute the mean vector mu and the mean vector per class mu_k\nmu = np.mean(X_train,axis=0).values.reshape(4,1) # Mean vector mu --> Since the data has been standardized, the data means are zero \n\n\nmu_k = []\n\nfor i,orchid in enumerate(np.unique(df['DAY'])):\n    mu_k.append(np.mean(X_train.where(df['DAY']==orchid),axis=0))\nmu_k = np.array(mu_k).T\n\n\n# 3. Compute the Scatter within and Scatter between matrices\ndata_SW = []\nNc = []\nfor i,orchid in enumerate(np.unique(df['DAY'])):\n    a = np.array(X_train.where(df['DAY']==orchid).dropna().values-mu_k[:,i].reshape(1,4))\n    data_SW.append(np.dot(a.T,a))\n    Nc.append(np.sum(df['DAY']==orchid))\nSW = np.sum(data_SW,axis=0)\n\nSB = np.dot(Nc*np.array(mu_k-mu),np.array(mu_k-mu).T)\n   \n# 4. Compute the Eigenvalues and Eigenvectors of SW^-1 SB\neigval, eigvec = np.linalg.eig(np.dot(np.linalg.inv(SW),SB))\n\n\n    \n# 5. Select the two largest eigenvalues \neigen_pairs = [[np.abs(eigval[i]),eigvec[:,i]] for i in range(len(eigval))]\neigen_pairs = sorted(eigen_pairs,key=lambda k: k[0],reverse=True)\nw = np.hstack((eigen_pairs[0][1][:,np.newaxis].real,eigen_pairs[1][1][:,np.newaxis].real)) # Select two largest\n\n\n# 6. Transform the data with Y=X*w\nY = X_train.dot(w)\n\n# Plot the data\nfig = plt.figure(figsize=(10,10))\nax0 = fig.add_subplot(111)\nax0.set_xlim(-3,3)\nax0.set_ylim(-4,3)\n\nfor l,c,m in zip(np.unique(y_train),['r','g','b'],['s','x','o']):\n    ax0.scatter(Y[0][y_train==l],\n                Y[1][y_train==l],\n               c=c, marker=m, label=l,edgecolors='black')\nax0.legend(loc='upper right')\n\n\n# Plot the voroni spaces\nmeans = []\n\nfor m,target in zip(['s','x','o'],np.unique(y_train)):\n    means.append(np.mean(Y[y_train==target],axis=0))\n    ax0.scatter(np.mean(Y[y_train==target],axis=0)[0],np.mean(Y[y_train==target],axis=0)[1],marker=m,c='black',s=100)\n   \n\n\nmesh_x, mesh_y = np.meshgrid(np.linspace(-3,3),np.linspace(-4,3)) \nmesh = []\n\n\nfor i in range(len(mesh_x)):\n    for j in range(len(mesh_x[0])):\n        date = [mesh_x[i][j],mesh_y[i][j]]\n        mesh.append((mesh_x[i][j],mesh_y[i][j]))\n\n\nNN = KNeighborsClassifier(n_neighbors=1)\nNN.fit(means,['r','g','b'])        \npredictions = NN.predict(np.array(mesh))\n\nax0.scatter(np.array(mesh)[:,0],np.array(mesh)[:,1],color=predictions,alpha=0.3)\n\n\nplt.show()","a163d8e7":"classifier = RandomForestClassifier(max_depth=2, random_state=0)\n\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Accuracy' + str(accuracy_score(y_test, y_pred)))","48bf628d":"df.dtypes","043316d7":"fvalue, pvalue = stats.f_oneway(df['DAY'], df['Impressions'], df['Clicks'], df['Revenue'])\nprint(fvalue, pvalue)\n# 17.492810457516338 2.639241146210922e-05\n\n# reshape the d dataframe suitable for statsmodels package \nd_melt = pd.melt(df.reset_index(), id_vars=['index'], value_vars=['DAY', 'Impressions', 'Clicks', 'Revenue'])\n# replace column names\nd_melt.columns = ['index', 'treatments', 'value']\n# Ordinary Least Squares (OLS) model\nmodel = ols('value ~ C(treatments)', data=d_melt).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\nanova_table","aa2a8b8f":"#---------------------------pip install bioinfokit\n#-----To know the pairs of significant different treatments, we will perform multiple pairwise comparison (Post-hoc comparison) analysis using Tukey HSD test.\n# we will use bioinfokit (v1.0.1 or later) for performing tukey HSD test\n# check documentation here https:\/\/reneshbedre.github.io\/blog\/howtoinstall.html\n# perform multiple pairwise comparison (Tukey HSD)\n# unequal sample size data, tukey_hsd uses Tukey-Kramer test\nres = stat()\nres.tukey_hsd(df=d_melt, res_var='value', xfac_var='treatments', anova_model='value ~ C(treatments)')\nres.tukey_summary","9b768ffe":"#---------------------The Shapiro-Wilk test can be used to check the normal distribution of residuals\nw, pvalue = stats.shapiro(model.resid)\nprint(w, pvalue)","9a96c7c8":"#-------------------------------------use Bartlett\u2019s test to check the Homogeneity of variances.\nw, pvalue = stats.bartlett(df['DAY'], df['Impressions'], df['Clicks'], df['Revenue'])\nprint(w, pvalue)","48042f06":"\n#-----------------------1-sample t-test: testing the value of a population mean in terms of revenue\n\nstats.ttest_1samp(df['Revenue'], 0)   \n","4f7b272f":"#------------------------2-sample t-test: testing for difference across populations in terms of revenue on Monday and Sunday\n\nDAY1 = df[df['DAY'] == 1]['Revenue']\nDAY2 = df[df['DAY'] == 7]['Revenue']\nstats.ttest_ind(DAY1, DAY2)  ","a96ae77b":"\n# # defining the table \ndata = [[207, 282, 241], [234, 242, 232]] \nstat, p, dof, expected = chi2_contingency(data) \n\n# interpret p-value \nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n\tprint('Dependent (reject H0)') \nelse: \n\tprint('Independent (H0 holds true)') \n","ebcc27ee":"df=pd.read_excel(r'\/kaggle\/input\/dash-assignment\/Review_Data_All.xlsx',\"Review Data\")\ndf","7ed5e6c8":"#------------------------------------------------------------lower case conversion\nlength=len(df['Review Paragarph'])\nfor i in range(length):\n    df['Review Paragarph'].iloc[i]=df['Review Paragarph'].iloc[i].lower()\n\nlength=len(df['Review Title'])\nfor i in range(length):\n    df['Review Title'].iloc[i]=df['Review Title'].iloc[i].lower()\n\nlength=len(df['Site'])\nfor i in range(length):\n    df['Site'].iloc[i]=df['Site'].iloc[i].lower()\n\n","344d224e":"#------------------------------------------------------------removing punctuation\npunctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n\nlength=len(df['Review Paragarph'])\n\nfor i in range(length):\n    no_punct = \"\"\n    for char in df['Review Paragarph'].iloc[i]:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    df['Review Paragarph'].iloc[i]=no_punct   \n\nlength=len(df['Review Title'])\n\nfor i in range(length):\n    no_punct = \"\"\n    for char in df['Review Title'].iloc[i]:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    df['Review Title'].iloc[i]=no_punct   \n    \nlength=len(df['Site'])\n\nfor i in range(length):\n    no_punct = \"\"\n    for char in df['Site'].iloc[i]:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    df['Site'].iloc[i]=no_punct  ","93bf8407":"#------------------------------------------------------------removing stopwords\nenglish_stop_words = stopwords.words('english')\ndef remove_stop_words(corpus):\n    removed_stop_words = []\n    for review in corpus:\n        removed_stop_words.append(\n            ' '.join([word for word in review.split() \n                      if word not in english_stop_words])\n        )\n    return removed_stop_words\n\ndf['Review Paragarph']=remove_stop_words(df['Review Paragarph'])\ndf['Review Title']=remove_stop_words(df['Review Title'])\ndf['Site']=remove_stop_words(df['Site'])\n","55c2b642":"#------------------------------------------------------------Stemming, Lemmatization\ndef get_stemmed_text(corpus):\n    from nltk.stem.porter import PorterStemmer\n    stemmer = PorterStemmer()\n    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n\ndf['Review Paragarph'] = get_stemmed_text(df['Review Paragarph'])\ndf['Review Title'] = get_stemmed_text(df['Review Title'])\ndf['Site'] = get_stemmed_text(df['Site'])\n\n\ndef get_lemmatized_text(corpus):\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n\ndf['Review Paragarph']  = get_lemmatized_text(df['Review Paragarph'])\ndf['Review Title'] = get_lemmatized_text(df['Review Title'])\ndf['Site'] = get_lemmatized_text(df['Site'])","88e2faa5":"df","3bd75d07":"\nfor i in df['Review Paragarph'].index:\n    wordcloud = WordCloud(background_color=\"black\", stopwords = set(STOPWORDS)).generate(df['Review Paragarph'].iloc[i])\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","001a0925":"\nfor i in df['Site'].index:\n    wordcloud = WordCloud(background_color=\"white\", stopwords = set(STOPWORDS)).generate(df['Site'].iloc[i])\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","eafe33ee":"\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(df['Review Paragarph'], 10)\ndf1 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])\n\nfig = go.Figure([go.Bar(x=df1['unigram'], y=df1['count'])])\nfig.update_layout(title=go.layout.Title(text=\"Top 10 unigrams in the word document after removing stop words and lemmatization\"))\nfig.show()\n","35ce88d3":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_bigram(df['Review Paragarph'], 10)\ndf2 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n\nfig = go.Figure([go.Bar(x=df2['bigram'], y=df2['count'])])\nfig.update_layout(title=go.layout.Title(text=\"Top 10 bigrams in the word document after removing stop words and lemmatization\"))\nfig.show()","524a9a0b":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_trigram(df['Review Paragarph'], 10)\ndf3 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n\nfig = go.Figure([go.Bar(x=df3['trigram'], y=df3['count'])])\nfig.update_layout(title=go.layout.Title(text=\"Top 10 trigrams in the word document\"))\nfig.show()","8363ea8c":"\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(df['Review Title'], 10)\ndf1 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])\n\nfig = go.Figure([go.Bar(x=df1['unigram'], y=df1['count'])])\nfig.update_layout(title=go.layout.Title(text=\"Top 10 unigrams for Review Title\"))\nfig.show()\n","ac65634e":"#------------------------------------------------------------BiGram Frequency for Review Title \n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_bigram(df['Review Title'], 10)\ndf2 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n\nfig = go.Figure([go.Bar(x=df2['bigram'], y=df2['count'])])\nfig.update_layout(title=go.layout.Title(text=\"Top 10 bigrams for Review Title\"))\nfig.show()","e0efa1c4":"#------------------------------------------------------------TriGram Frequency for Review Title \n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_trigram(df['Review Title'], 10)\ndf3 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n\nfig = go.Figure([go.Bar(x=df3['trigram'], y=df3['count'])])\nfig.update_layout(title=go.layout.Title(text=\"Top 10 trigrams for Review Title\"))\nfig.show()","963ba016":"df","02bcd44a":"df[df['Review Date'].isnull()==True]","3c781be5":"# Create quick lambda functions to find the polarity of each review\n#Terminal \/ Anaconda Navigator: conda install -c conda-forge textblobfrom textblob \n# import textblob as TextBlob\n# df['Text']= df['Text'].astype(str) #Make sure about the correct data type\n\n\npol = lambda x: TextBlob(x).sentiment.polarity\ndf['polarity'] = df['Review Paragarph'].apply(pol) # depending on the size of your data, this step may take some time.\n\nnum_bins = 50\nplt.figure(figsize=(10,6))\nn, bins, patches = plt.hist(df.polarity, num_bins, facecolor='blue', alpha=0.5)\nplt.xlabel('Polarity')\nplt.ylabel('Number of Reviews')\nplt.title('Histogram of Polarity Score')\nplt.show();","09709b63":"#---------------------------------------------------------------------------------polarity score tagging\ndf['polarity_score']=np.nan\nval=df[df['polarity']>0].index\ndf['polarity_score'].iloc[val]='Positive'\nval=df[df['polarity']==0].index\ndf['polarity_score'].iloc[val]='Neutral'\nval=df[df['polarity']<0].index\ndf['polarity_score'].iloc[val]='Negative'","73db9239":"#-------------------------------------------------------------------------------% of positive,negative and neutral\nsum_tot=df['polarity_score'].count()\nsum_pos=df[df['polarity_score']=='Positive']['polarity_score'].count()\nsum_neg=df[df['polarity_score']=='Negative']['polarity_score'].count()\nprint(\"Percentage of Positive Reviews: \"+\"{:.2%}\".format(sum_pos\/sum_tot));\nprint(\"Percentage of Negative Reviews: \"+\"{:.2%}\".format(sum_neg\/sum_tot));\n","829cd56a":"#------------------------------------------------------------------------------- positive feedback\nsearch_value=1\nresult_index = df['polarity'].sub(search_value).abs().idxmin()\ndf.iloc[result_index]['Review Paragarph']","9cbe0ce2":"#------------------------------------------------------------------------------- negative feedback\nsearch_value=-1\nresult_index = df['polarity'].sub(search_value).abs().idxmin()\ndf.iloc[result_index]['Review Paragarph']","3801e92b":"#------------------------------------------------------------------------------- neutral feedback\nsearch_value=0\nresult_index = df['polarity'].sub(search_value).abs().idxmin()\ndf.iloc[result_index]['Review Paragarph']","45a349a9":"#------------------------------------------------------------------------------- Binary Encoding\ndf['target']=0\nval=df[df['polarity']>0].index\ndf['target'].iloc[val]=1","106c3f9f":"df","995af047":"### 2. Use the DAY column to map the day's name in the week. Example:\nMap 1 -> Monday up to 7 -> Sunday\n","3750eaf1":"### Google Ad Manager Data ( DFP.xlsx )\nIn this task, we want to test your familiarity with different statistical and modeling concepts.\n        1. Analyze the dataset in as many ways possible ways, including multivariate analysis, to generate insights for predictive modeling. (Other suggestions for analysis \u2013 [ Correlation, Covariance, ANOVA, Regression analysis, Hypothesis testing: Student\u2019s t-test, chi-square test (Generate Null and Alternate hypothesis and find the significant relation)].","33112be2":"# A. Data Munging","76c6aca7":"### Multiple Discriminant Analysis","c5964939":"### 1. Google Ad Manager Data ( DFP.xlsx )\n\n        1. Create the following columns, from the Ad_unit_name column in the dataset:\n            a. amp_or_non_amp\n            b. story\n            c. position\n\nExample:\n        1.1 CarToq_ad_first_story_pos_top (122380182) story = first, position = top, Nonamp\n        1.2 amp-cartoq-bottom (21684306640) story = None, position = bottom, Amp","77df07b3":"### Regression  ","5d754de0":"## 3 polarity of score of review data","a30665c3":"## 2 Frequency of words\n### UNI GRAM for top 10 words from Review Paragraph ","1fe07897":"## Multivariate Analysis ","6472a922":"### 3. Here, eCPM for some data is incorrect, Merge Actual_eCPM data from the other sheet ( Actual_eCPM.csv ) such that new column for Actual_eCPM is created. Create another column Actual_Revenue which is defined as:\nActual_Revenue = Total Impression * Actual_eCPM\n\n(if not provided, take the given revenue in the dataset as Actual_Revenue to fill the remaining Actual_Revenue column data with it.)\n\nAfter\tcompleting the above 3 tasks save the results as\nDFP_solution.csv","e589097e":"# B. Statistical Analysis:","fba77f6d":"## Univariate  Analysis\n","65876980":"please let me know if you have any idea on how to implement the above.....","7b4bf45a":"### 4 Most positive negative and neutral based on reviews","73cdce87":"# C. Text Processing and Sentiment Analysis: \nDataset: Review data ( Review_Data_All.xlsx )","76422440":"## Multivariate Analysis of Variance and Covariance\n### ANOVA","85ce5d9f":"## reading Actual_eCPM\n","1fa15fbf":"### Interactive Tkinter tool to display in a window","ce406583":"### Python \u2013 Pearson\u2019s Chi-Square Test\n","e56b995e":"### Hypothesis Testing","c66401bf":"###  4. Identify the best-performing Ad position in terms of eCPM and revenue, separately in amp and non-amp case. ( hint : while merging the data keep in mind that [ eCPM = Actual Revenue * 1000 \/ Total Impressions ] and [ CTR = clicks \/ Total impressions] , aggregating directly won\u2019t help.)\nSubmit this result as Adpos.csv","a05e31bd":" As the P-value is non significant, we fail to reject null hypothesis and conclude\n that data is drawn from normal distribution.","52f8d6f8":"# Trying to do logistic regression to identify the words with high coefficients value.","a029c7a6":"## 1 Basic Text Processing","1d75daa3":"### 5 target variable Binary Encode","e04df727":"As the P-value (0.0) is non significant, we fail to reject null hypothesis and \nconclude that treatments have equal variances."}}