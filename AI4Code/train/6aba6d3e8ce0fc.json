{"cell_type":{"93bde6f2":"code","d8c7aec1":"code","3a6ee2fe":"code","c479c0e2":"code","414adae2":"code","79bb9991":"code","ad4ed9fb":"code","bac0e481":"code","82e5056b":"code","c3f2e882":"code","8b9bf959":"code","cdd6c6df":"code","7fc9459b":"code","89f701e2":"code","a5a9d385":"code","96e3edd5":"code","4af433ca":"code","15359be3":"code","7c6f2d4f":"code","fb802772":"code","6d426584":"code","f83009c8":"code","d0385970":"markdown","19b62f0c":"markdown","dc5c8f3c":"markdown","f3a3768a":"markdown","fbe241fd":"markdown","fd35440e":"markdown","ffacb11e":"markdown","e6c26307":"markdown","91d847cc":"markdown","2baf0d1f":"markdown","1d6a184a":"markdown","ae004ff0":"markdown","bf01bc08":"markdown","44dc053f":"markdown","c1faec17":"markdown","fb69fa6b":"markdown","83a4109e":"markdown","bfc1d8a0":"markdown","07718ab4":"markdown","8c4b2bb7":"markdown"},"source":{"93bde6f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8c7aec1":"#Data Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Data Preprocessing libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport re\n\n#modelling and validation libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n#warning\nimport warnings\nwarnings.filterwarnings('ignore')","3a6ee2fe":"df_train = pd.read_csv('..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\ndf_test = pd.read_csv('..\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv')\n\n#training data overview\ndf_train.info()","c479c0e2":"df_train.describe()","414adae2":"df_train.head()","79bb9991":"df_train['Profession'].unique()","ad4ed9fb":"df_train['CITY'].unique()","bac0e481":"df_train['STATE'].unique()","82e5056b":"df_train['House_Ownership'].unique()","c3f2e882":"df_train.select_dtypes(include='object')","8b9bf959":"sns.set_palette('bright')\nfig = sns.countplot(df_train['Risk_Flag'])\nfig.bar_label(fig.containers[0])","cdd6c6df":"f, ax = plt.subplots(figsize=(12,6))\nhm = sns.heatmap(df_train.corr(), cbar=False, annot=True, square=True, fmt='.2f', ax=ax)\nplt.show()","7fc9459b":"fig = sns.scatterplot(x=df_train['Experience'], y=df_train['CURRENT_JOB_YRS'], hue=df_train['Risk_Flag'])","89f701e2":"col = ['Married\/Single', 'House_Ownership', 'Car_Ownership']\nfor i in col:\n    df_train[i] = LabelEncoder().fit_transform(df_train[i])\ndf_train = pd.get_dummies(df_train, columns=['CITY', 'Profession', 'STATE'])\ndf_train = df_train.drop(columns=['Id'])","a5a9d385":"df_train.head()","96e3edd5":"regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\ndf_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df_train.columns.values]","4af433ca":"y = df_train.pop('Risk_Flag')\ntrain_X, val_X, train_y, val_y = train_test_split(df_train, y, test_size=0.2, random_state=415)","15359be3":"space = {'max_depth':hp.quniform('max_depth',550,650,5),\n         'min_samples_leaf':hp.uniform('min_samples_leaf',0.4,0.5),\n         'min_samples_split':hp.uniform('min_samples_split',0.8,1),\n         'n_estimators':hp.quniform('n_estimators',1800, 2500, 100)}","7c6f2d4f":"def objective(space):\n    model = RandomForestClassifier(criterion = 'entropy', \n                                   max_depth = space['max_depth'],\n                                   max_features = 'auto',\n                                   min_samples_leaf = space['min_samples_leaf'],\n                                   min_samples_split = space['min_samples_split'],\n                                   n_estimators = int(space['n_estimators']))\n    model.fit(train_X, train_y)\n    pred = model.predict(val_X)\n    accuracy = accuracy_score(val_y, pred)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }","fb802772":"trials = Trials()\n\nbest_params = fmin(fn = objective,\n            space = space,\n            algo = tpe.suggest,\n            max_evals = 100,\n            trials = trials)","6d426584":"best_params","f83009c8":"best_score = objective(best_params)","d0385970":"Get the parameters that performing the best.","19b62f0c":"From the heatmap, we can see that most of the features have no correlation with each other except `Experience` and `CURRENT_JOB_YRS`.<br> Let's take a look of these two features.","dc5c8f3c":"Split the training set and testing set.","f3a3768a":"Define the model with predefined ranges of parameters and return the validation score.","fbe241fd":"And our accuracy score is approximately 88%.","fd35440e":"Use Hyperopt to find the best parameters.","ffacb11e":"Looks like there are some linear constraints of how experienced the observations are based on how many years of their current jobs.\nFrom the scatter plot, we can infer some constraints:<br>\n$\n\\begin{cases}\ny \\ge 3 \\\\\nx \\ge y \\\\\nx \\le 20 \\\\\ny \\le 14\n\\end{cases}\n$<br>\nand containing a straight line: $x=y$ where $y \\le 14$.\n<br> Moreover, there is no obvious pattern of target variable `Risk_Flag` shown in this graph.","e6c26307":"We can see that the amount of value `0` is approximately 7 times of the amount of value `1`. <br> Class `1` is relatively small in the dataset.","91d847cc":"# Import Libraries","2baf0d1f":"Get the lowest point of loss function from 100 trials.","1d6a184a":"### Encoding for Categorical Features","ae004ff0":"Since there are some data containing unwanted special characters, I would replace them with `_`.","bf01bc08":"So there are a total of 252000 rows of observations without missing variables and 11 features.","44dc053f":"# Exploratory Data Analysis\n","c1faec17":"# Import Data","fb69fa6b":"# Modelling","83a4109e":"# Feature Engineering","bfc1d8a0":"First move, take a view of our target variable `Risk_Flag`.","07718ab4":"From the data description:\n<br>income \t\t\t\t- Income of the user\n<br>age   \t\t\t\t- Age of the user\n<br>experience \t\t\t- Professional experience of the user in years\n<br>profession\t\t\t- Profession\n<br>married\t\t\t\t- Whether married or single\n<br>house_ownership\t\t- Owned or rented or neither\n<br>car_ownership\t\t- Does the person own a car\n<br>risk_flag\t\t\t- Defaulted on a loan\n<br>currentjobyears\t\t- Years of experience in the current job\n<br>currenthouseyears\t- Number of years in the current residence\n<br>city\t\t\t\t- City of residence\n<br>state\t\t\t\t- State of residence\n\n<br>Now we have a rough understanding of the data. Let's probe into the features.","8c4b2bb7":"With Random Forest Classifier, I would like to try tuning the hyperparameters with HYPEROPT. Thanks to this kernel [A Guide on XGBoost hyperparameters tuning](https:\/\/www.kaggle.com\/prashant111\/a-guide-on-xgboost-hyperparameters-tuning) by Prashant Banerjee for an informative guide."}}