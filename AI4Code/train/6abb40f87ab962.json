{"cell_type":{"98bf7d8e":"code","28883f40":"code","b963d1bf":"code","b53c62aa":"code","b3233bb2":"code","684fb0ec":"code","a6df1eec":"code","91dc646e":"code","5715921f":"code","cb1ba1f0":"code","d7acced6":"code","35a863b9":"code","8ca04641":"code","83a49703":"code","ace76300":"code","7f92077f":"code","99b115c6":"code","f5c2074f":"code","f4377604":"code","f1161268":"code","22843065":"code","916a215e":"code","10989b1e":"code","7f52b016":"code","5f3c5f98":"code","cd13ad71":"code","beb475d0":"code","78a8de3b":"code","e5ae9c27":"code","82dd414c":"code","1af393a8":"code","a3cba1c1":"code","694f7940":"markdown","294a857e":"markdown","736bae51":"markdown","3e9c28ba":"markdown","5d7507c2":"markdown","ea48b5a4":"markdown","ec001eda":"markdown","122c6cb4":"markdown","5e968904":"markdown","c1d72317":"markdown","5f387cb8":"markdown","8ba2da37":"markdown","21bc9ce7":"markdown","bf2924a3":"markdown","a69d0cab":"markdown","a4865619":"markdown"},"source":{"98bf7d8e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\nimport os\nprint(os.listdir(\"..\/input\"))","28883f40":"bike_df = pd.read_csv('..\/input\/train.csv')\nbike_df.shape","b963d1bf":"bike_df.info()","b53c62aa":"bike_df.head()","b3233bb2":"# Transform string into datetime type\nbike_df['datetime'] = bike_df['datetime'].apply(pd.to_datetime)\n\n# Extract year, month, day and time from the datetime type\nbike_df['year'] = bike_df['datetime'].apply(lambda x : x.year)\nbike_df['month'] = bike_df['datetime'].apply(lambda x : x.month)\nbike_df['day'] = bike_df['datetime'].apply(lambda x : x.day)\nbike_df['hour'] = bike_df['datetime'].apply(lambda x : x.hour)\nbike_df.head()","684fb0ec":"bike_df.drop(['datetime', 'casual', 'registered'], axis=1, inplace=True)","a6df1eec":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Calculate RMSLE (Root Mean Square Log Error) with using not log() but log1p() due to issues including NaN\ndef rmsle(y, pred):\n    log_y = np.log1p(y)\n    log_pred = np.log1p(pred)\n    squared_error = (log_y - log_pred) ** 2\n    rmsle = np.sqrt(np.mean(squared_error))\n    return rmsle\n\n# Calculate RMSE\ndef rmse(y, pred):\n    return np.sqrt(mean_squared_error(y, pred))\n\n# Calculate MSE, RMSE and RMSLE\ndef evaluate_regr(y, pred):\n    rmsle_val = rmsle(y, pred)\n    rmse_val = rmse(y, pred)\n    mse_val = mean_absolute_error(y, pred)\n    print('RMSLE: {:.3f}, RMSE: {:.3f}, MSE: {:.3f}'.format(rmsle_val, rmse_val, mse_val))","91dc646e":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\n\ny_target = bike_df['count']\nX_features = bike_df.drop(['count'], axis=1, inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=2019)\n\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\npred = lr_reg.predict(X_test)\n\nevaluate_regr(y_test, pred)","5715921f":"def get_top_error_data(y_test, pred, n_tops=5):\n    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\n    result_df['predicted_count'] = np.round(pred)\n    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])\n    \n    print(result_df.sort_values('diff', ascending=False)[:n_tops])\n    \nget_top_error_data(y_test, pred, n_tops=5)","cb1ba1f0":"y_target.hist()","d7acced6":"y_log_transform = np.log1p(y_target)\ny_log_transform.hist()","35a863b9":"y_target_log = np.log1p(y_target)\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=2019)\n\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\npred = lr_reg.predict(X_test)\n\n# Convert the transformed y_test values into the original values\ny_test_exp = np.expm1(y_test)\n\n# Convert the transformed predicted values into the original values\npred_exp = np.expm1(pred)\n\nevaluate_regr(y_test_exp, pred_exp)","8ca04641":"coef = pd.Series(lr_reg.coef_, index=X_features.columns)\ncoef_sort = coef.sort_values(ascending=False)\nsns.barplot(x=coef_sort.values, y=coef_sort.index)","83a49703":"X_features_ohe = pd.get_dummies(X_features, columns=['year', 'month', 'hour', 'holiday', 'workingday', 'season', 'weather'])","ace76300":"X_features_ohe.head()","7f92077f":"X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=2019)\n\ndef get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False):\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    if is_expm1:\n        y_test = np.expm1(y_test)\n        pred = np.expm1(pred)\n    print('###', model.__class__.__name__, '###')\n    evaluate_regr(y_test, pred)\n\nlr_reg = LinearRegression()\nridge_reg = Ridge(alpha=10)\nlasso_reg = Lasso(alpha=0.01)\n\nfor model in [lr_reg, ridge_reg, lasso_reg]:\n    get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=True)","99b115c6":"coef = pd.Series(lr_reg.coef_, index=X_features_ohe.columns)\ncoef_sort = coef.sort_values(ascending=False)[:15]\nsns.barplot(x=coef_sort.values, y=coef_sort.index)","f5c2074f":"X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=2019)\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=500)\ngbm_reg = GradientBoostingRegressor(n_estimators=500)\nxgb_reg = XGBRegressor(n_estimators=500)\nlgbm_reg = LGBMRegressor(n_estimators=500)\n\nfor model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]:\n    get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=True)","f4377604":"submission = pd.read_csv('..\/input\/sampleSubmission.csv')","f1161268":"submission.shape","22843065":"submission.head()","916a215e":"X_test = pd.read_csv('..\/input\/test.csv')\nX_test.head()","10989b1e":"X_test.shape","7f52b016":"# Transform string into datetime type\nX_test['datetime'] = X_test['datetime'].apply(pd.to_datetime)\n\n# Extract year, month, day and time from the datetime type\nX_test['year'] = X_test['datetime'].apply(lambda x : x.year)\nX_test['month'] = X_test['datetime'].apply(lambda x : x.month)\nX_test['day'] = X_test['datetime'].apply(lambda x : x.day)\nX_test['hour'] = X_test['datetime'].apply(lambda x : x.hour)\nX_test.head()","5f3c5f98":"X_test.drop(['datetime'], axis=1, inplace=True)\nX_test.head()","cd13ad71":"X_test.shape","beb475d0":"X_test_ohe = pd.get_dummies(X_test, columns=['year', 'month', 'hour', 'holiday', 'workingday', 'season', 'weather'])\nX_test_ohe.head()","78a8de3b":"prediction = lgbm_reg.predict(X_test_ohe)","e5ae9c27":"prediction","82dd414c":"submission['count'] = np.round(prediction, 0).astype(int)","1af393a8":"submission.head()","a3cba1c1":"submission.to_csv('.\/My_submission.csv', index=False)","694f7940":"Skewness is somewhat improved.\n\nLet's train the model again!","294a857e":"I will create a prediction performance evaluation function. ","736bae51":"As I mentioned, prediction errors are so high. \n\nLet's check Target distribution. ","3e9c28ba":"Regression prediction performances are improved!","5d7507c2":"## 1. Data Loading and Preprocessing","ea48b5a4":"Season, month and weather features have high coefficient values.\n\nLet's apply Regression Tree models including RandomForest, GBM, XGBoost and LightGBM.","ec001eda":"The Target distribution is skewed!\n\nLog transformation fot the Target seems to be needed. ","122c6cb4":"## 2. Log Transformation, Feature Encoding and Model training\/prediction\/evaluation","5e968904":"Considering count values, such prediction errors seems to be relatively high. \n\nLet's check actual and predicted values in top 5 errors!","c1d72317":"The 'datetime' feature needs to be transformed as an object and to be splited into year, month, day and time. ","5f387cb8":"# Regression with Bike Sharing Demand Data","8ba2da37":"After one-hot encoding, prediction performance is improved. \n\nLet's check features with high coefficient values.","21bc9ce7":"Now, I will delete the datetime feature. \n\nI will also delete casual and registered features as the sum of casual and registered equals to count.","bf2924a3":"RMSLE is lower, but RMSE is higher. \n\nLet's check coefficient values of features!","a69d0cab":"## 3. Prediction on Test Set","a4865619":"Year feature has an exceptional coefficient value, which results from being int type. \n\nSo, categorical features including year need to be transformed into one-hot encoding. "}}