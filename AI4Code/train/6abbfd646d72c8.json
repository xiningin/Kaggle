{"cell_type":{"d46216ed":"code","744f6bbc":"code","0333e7e3":"code","82d7a928":"code","e93ceea5":"code","ff8d0f02":"code","4bb6fb02":"code","6b6e63bb":"code","70487f2e":"code","c399326d":"code","862cf142":"code","ad5fb6c1":"code","0aff29b8":"code","5aab76c1":"code","bcb18da4":"code","242aec65":"code","ca7b4166":"code","dab99268":"code","d9d4c5d1":"code","e1da8cc7":"code","228581bd":"code","209753fd":"code","689deee7":"code","220df0e6":"code","c8179839":"code","f4d8cb07":"code","43177d1c":"code","3489d074":"code","b9a90f39":"code","b64cec22":"code","b14cfa64":"code","b16d13b6":"code","92de2e35":"code","2260bff2":"code","78cd38d1":"code","99bf8875":"code","9cb571cf":"code","c9f99b9a":"code","3b8068bc":"code","44fc8a81":"code","3ae9703d":"code","ec96b209":"code","7b4d876d":"code","e3cc6b73":"code","ed157ef1":"code","2e1c9c8a":"code","3f9c33af":"code","4ec5e54c":"code","5b9f6df4":"code","e2499a1f":"code","4f20a95c":"code","b0f05eca":"code","797753ad":"code","c53fba69":"code","41a37c86":"code","b5d96dda":"markdown","fcede032":"markdown","4557e67a":"markdown","3207e3b7":"markdown","2159622f":"markdown","71f4c264":"markdown","558454d2":"markdown","3476dad7":"markdown","5b783847":"markdown","ba13861e":"markdown","f8db16a5":"markdown","ba0813fc":"markdown","0c97e35d":"markdown","d4234f9e":"markdown","7d87dd3b":"markdown","f91151cb":"markdown","4c267876":"markdown","51ec8bc8":"markdown","5044d3ab":"markdown","37f81b65":"markdown","76319723":"markdown","68acccc1":"markdown","74789a9e":"markdown","9f75e7c4":"markdown","c67eae52":"markdown","565b30e4":"markdown","de2a91fd":"markdown","7612a91c":"markdown","cf196a32":"markdown","cbb7ecbd":"markdown","ae78e102":"markdown","019fe055":"markdown","930afb60":"markdown","88d594aa":"markdown","2ee0fc30":"markdown"},"source":{"d46216ed":"# Import libraries and tools\n# Data preprocessing and linear algebra\nimport os, re, random\nfrom os.path import join\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom sklearn.datasets import load_files\nimport pandas as pd\nimport numpy as np\nnp.random.seed(2)\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\n# Tools for cross-validation, error calculation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom keras.utils.np_utils import to_categorical\n\n# Machine Learning\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import MaxPooling2D, GlobalAveragePooling2D, Activation\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras import optimizers\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","744f6bbc":"print(os.listdir('..\/input\/'))","0333e7e3":"INPUT_PATH = '..\/input\/flowers-recognition\/flowers\/flowers\/'\nprint(os.listdir(INPUT_PATH))","82d7a928":"img_folders = [join(INPUT_PATH, dir) for dir in os.listdir(INPUT_PATH)]\nlist(img_folders)","e93ceea5":"# Load images into NumPy array\nimages = load_files(INPUT_PATH, random_state=42, shuffle=True)\nX = np.array(images['filenames'])\ny = np.array(images['target'])\nlabels = np.array(images['target_names'])\n\n# Remove unnecessary .pyc and .py files\npyc_file = (np.where(file==X) for file in X if file.endswith(('.pyc','.py')))\nfor i in pyc_file:\n    X = np.delete(X, i)\n    y = np.delete(y, i)","ff8d0f02":"# Our array summary\nprint(f'Target labels (digits) - {y}')\nprint(f'Target labels (names) - {labels}')\nprint(f'Number of uploaded images : {X.shape[0]}')","4bb6fb02":"# Draw random image directly from dataset for aesthetic reasons only\nimg = plt.imread('..\/input\/flowers-recognition\/flowers\/daisy\/100080576_f52e8ee070_n.jpg')\nplt.imshow(img);","6b6e63bb":"# Check our target y variable\nflowers = pd.DataFrame({'species': y})\nflowers.count()","70487f2e":"# Correspond species and flowers and form digit labels\nflowers['flower'] = flowers['species'].astype('category')\nlabels = flowers['flower'].cat.categories","c399326d":"labels","862cf142":"# Let's implement a constant - standard image size for Inception model input, which is 150 px\nimage_size = 150","ad5fb6c1":"# Write images into NumPy array using sklearn's img_to_array() method\ndef imageLoadConverter(img_paths):\n    # Load\n    images = [load_img(img_path, target_size=(image_size, image_size)) for img_path in img_paths]\n    # Write into array\n    images_array = np.array([img_to_array(img) for img in images])\n    \n    return(images_array)\n\n# Convert into NumPy array\nX = np.array(imageLoadConverter(X))\n# Print result\nprint(f'Function worked with following output (images, width, height, color): {X.shape}')","0aff29b8":"# Convert classes in digit form\nnum_classes = len(np.unique(y))\nprint(f'Classes: {num_classes} and corresponding labels: {labels}')","5aab76c1":"# One-Hot Encoding\ny = to_categorical(y, num_classes)\nprint(y.shape)","bcb18da4":"# Split data on train, validation and test subsets\n# Using 10% or 20% from train data is classical approach\n\n# First, split X into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=2)\n\n# Second, split test into test and validation subsets in equal proportion\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=True, random_state=2)","242aec65":"# Count number of elements in subsets\ntotal_X_train = X_train.shape[0]\ntotal_X_val = X_val.shape[0]\ntotal_X_test = X_test.shape[0]","ca7b4166":"print(f'Train: {total_X_train}')\nprint(f'Validation: {total_X_val}')\nprint(f'Test: {total_X_test}')","dab99268":"# Delete X since it will not be needed further\ndel X","d9d4c5d1":"# By default, the InceptionV3 model expects images as input with the size 150x150 px with 3 channels\ninput_shape = (image_size, image_size, 3)","e1da8cc7":"# Define model constants\nbatch_size = 8\nepochs = 20","228581bd":"# Define our pre-trained model, downloading weights from Imagenet\n# pre_trained_model = InceptionV3(input_shape = input_shape, include_top = False, weights = 'imagenet')\n\n# Define our pre-trained model, using weights, uploaded from Kaggle's Keras Inception dataset\nlocal_weights = \"..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\npre_trained_model = InceptionV3(input_shape = input_shape, include_top = False, weights = None)","209753fd":"# Load weights into network\npre_trained_model.load_weights(local_weights)","689deee7":"# Print models summary table\nprint(pre_trained_model.summary())","220df0e6":"# Print number of models layers\nlen(pre_trained_model.layers)","c8179839":"# Set layers to be not trainable since they are already are\nfor layer in pre_trained_model.layers:\n     layer.trainable = False","f4d8cb07":"# Add custom layers\nx = pre_trained_model.output\n# Add Pooling layer\nx = Flatten()(x)\n# Add a fully connected layer with 1024 nodes and ReLU activation\nx = Dense(1024, activation=\"relu\")(x)\n# Add a dropout with rate 0.5\nx = Dropout(0.2)(x)\n# Specify final output layer with SoftMax activation\npredictions = Dense(5, activation=\"softmax\")(x)","43177d1c":"pre_trained_model.input","3489d074":"predictions","b9a90f39":"# Build the final model \ninception_model = Model(inputs=pre_trained_model.input, \n                        outputs=predictions\n                       )","b64cec22":"# Compile model\ninception_model.compile(loss='categorical_crossentropy',\n                        optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n                        metrics=['accuracy']\n                       )","b14cfa64":"# Implement train ImageDataGenerator and specify some preprocessing\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)","b16d13b6":"# Upload and peprocess images\ntrain_generator = train_datagen.flow(\n        X_train, y_train, \n        batch_size=batch_size,\n        shuffle=False)  ","92de2e35":"# Implement validation ImageDataGenerator\nvalidation_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","2260bff2":"validation_generator = validation_datagen.flow(\n        X_val, y_val,\n        batch_size=batch_size,\n        shuffle=False) ","78cd38d1":"test_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","99bf8875":"test_generator = test_datagen.flow(\n        X_test, y_test,\n        batch_size=batch_size,\n        shuffle=False\n)","9cb571cf":"# Stop model learning after 10 epochs in which val_loss value not decreased\nearly_stop = EarlyStopping(patience=10, \n                          verbose=1, \n                          mode='auto'\n                         )","c9f99b9a":"# Reduce the learning rate when accuracy, for example, not increase for two continuous steps\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001\n                                           )","3b8068bc":"# Save callbacks\ncallbacks = [early_stop, learning_rate_reduction]\ncallbacks","44fc8a81":"hist = inception_model.fit_generator(\n    train_generator, \n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=total_X_val\/\/batch_size,\n    steps_per_epoch=total_X_train\/\/batch_size,\n    callbacks=callbacks\n)","3ae9703d":"# poch 1\/20\n# 432\/432 [==============================] - 25s 57ms\/step - loss: 0.9134 - accuracy: 0.6843 - val_loss: 0.7519 - val_accuracy: 0.7616 - lr: 1.0000e-04\n# Epoch 2\/20\n# 432\/432 [==============================] - 23s 53ms\/step - loss: 0.6030 - accuracy: 0.7841 - val_loss: 0.6113 - val_accuracy: 0.7801 - lr: 1.0000e-04\n# Epoch 3\/20\n# 432\/432 [==============================] - 23s 54ms\/step - loss: 0.5084 - accuracy: 0.8154 - val_loss: 0.5370 - val_accuracy: 0.8079 - lr: 1.0000e-04\n# Epoch 4\/20\n# 432\/432 [==============================] - 22s 52ms\/step - loss: 0.4568 - accuracy: 0.8336 - val_loss: 0.5346 - val_accuracy: 0.8009 - lr: 1.0000e-04\n# Epoch 5\/20\n# 432\/432 [==============================] - 24s 54ms\/step - loss: 0.4375 - accuracy: 0.8435 - val_loss: 0.5041 - val_accuracy: 0.8148 - lr: 1.0000e-04\n# Epoch 6\/20\n# 432\/432 [==============================] - 22s 52ms\/step - loss: 0.3996 - accuracy: 0.8528 - val_loss: 0.4885 - val_accuracy: 0.8218 - lr: 1.0000e-04\n# Epoch 7\/20\n# 432\/432 [==============================] - 24s 54ms\/step - loss: 0.3680 - accuracy: 0.8620 - val_loss: 0.5028 - val_accuracy: 0.8194 - lr: 1.0000e-04\n# Epoch 8\/20\n# 432\/432 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.8667\n# Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n# 432\/432 [==============================] - 25s 57ms\/step - loss: 0.3690 - accuracy: 0.8667 - val_loss: 0.5066 - val_accuracy: 0.8194 - lr: 1.0000e-04\n# Epoch 9\/20\n# 432\/432 [==============================] - 23s 52ms\/step - loss: 0.3142 - accuracy: 0.8945 - val_loss: 0.4857 - val_accuracy: 0.8287 - lr: 5.0000e-05\n# Epoch 10\/20\n# 432\/432 [==============================] - 24s 55ms\/step - loss: 0.2965 - accuracy: 0.8867 - val_loss: 0.4750 - val_accuracy: 0.8380 - lr: 5.0000e-05\n# Epoch 11\/20\n# 432\/432 [==============================] - 23s 54ms\/step - loss: 0.2845 - accuracy: 0.9043 - val_loss: 0.4753 - val_accuracy: 0.8403 - lr: 5.0000e-05\n# Epoch 12\/20\n# 432\/432 [==============================] - 23s 53ms\/step - loss: 0.2874 - accuracy: 0.8939 - val_loss: 0.4844 - val_accuracy: 0.8310 - lr: 5.0000e-05\n# Epoch 13\/20\n# 431\/432 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9030\n# Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n# 432\/432 [==============================] - 25s 57ms\/step - loss: 0.2744 - accuracy: 0.9032 - val_loss: 0.4928 - val_accuracy: 0.8356 - lr: 5.0000e-05\n# Epoch 14\/20\n# 432\/432 [==============================] - 23s 54ms\/step - loss: 0.2689 - accuracy: 0.9058 - val_loss: 0.4824 - val_accuracy: 0.8333 - lr: 2.5000e-05\n# Epoch 15\/20\n# 432\/432 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9064\n# Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n# 432\/432 [==============================] - 24s 56ms\/step - loss: 0.2675 - accuracy: 0.9064 - val_loss: 0.4757 - val_accuracy: 0.8333 - lr: 2.5000e-05\n# Epoch 16\/20\n# 432\/432 [==============================] - 23s 53ms\/step - loss: 0.2451 - accuracy: 0.9130 - val_loss: 0.4748 - val_accuracy: 0.8333 - lr: 1.2500e-05\n# Epoch 17\/20\n# 432\/432 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9145\n# Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-05.\n# 432\/432 [==============================] - 23s 53ms\/step - loss: 0.2615 - accuracy: 0.9145 - val_loss: 0.4769 - val_accuracy: 0.8287 - lr: 1.2500e-05\n# Epoch 18\/20\n# 432\/432 [==============================] - 24s 57ms\/step - loss: 0.2516 - accuracy: 0.9107 - val_loss: 0.4685 - val_accuracy: 0.8333 - lr: 1.0000e-05\n# Epoch 19\/20\n# 432\/432 [==============================] - 24s 55ms\/step - loss: 0.2439 - accuracy: 0.9186 - val_loss: 0.4724 - val_accuracy: 0.8333 - lr: 1.0000e-05\n# Epoch 20\/20\n# 432\/432 [==============================] - 24s 55ms\/step - loss: 0.2454 - accuracy: 0.9145 - val_loss: 0.4674 - val_accuracy: 0.8426 - lr: 1.0000e-05","ec96b209":"# Plot accuracy and loss curves\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 7))\n\nax1.plot(hist.history['loss'], color='r', label=\"Train loss\")\nax1.plot(hist.history['val_loss'], color='b', label=\"Validation loss\")\nax1.set_xticks(np.arange(1, epochs, 1))\nlegend = ax1.legend(loc='best', shadow=True)\n\nax2.plot(hist.history['accuracy'], color='r', label=\"Train accuracy\")\nax2.plot(hist.history['val_accuracy'], color='b',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, epochs, 1))\nlegend = ax2.legend(loc='best', shadow=True)\n\nplt.tight_layout()\nplt.show()","7b4d876d":"# Predict on validation X_val_resnet\ny_pred_val = inception_model.predict_generator(validation_generator)","e3cc6b73":"# Prepare y_true and y_pred on validation by taking the most likely class\ny_true_val = y_val.argmax(axis=1)\ny_pred_val = y_pred_val.argmax(axis=1)","ed157ef1":"# Check datatypes\nprint(f'y_true datatype: {y_true_val.dtype}')\nprint(f'y_pred datatype: {y_pred_val.dtype}')","2e1c9c8a":"# Evaluate on validation dataset\nloss, acc = inception_model.evaluate_generator(validation_generator, verbose=0)\nprint(f'Validation loss: {loss:.2f}%')\nprint(f'Validation accuracy: {acc*100:.2f}%')","3f9c33af":"# Compute and plot the Confusion matrix\nconfusion_mtx_resnet = confusion_matrix(y_true_val, y_pred_val) \n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx_resnet, annot=True, fmt='d', cmap=plt.cm.Blues)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Validation (aka True) Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","4ec5e54c":"samples = total_X_test","5b9f6df4":"predict = inception_model.predict_generator(test_generator, steps=np.ceil(samples\/batch_size))","e2499a1f":"predict.shape","4f20a95c":"X_test.shape","b0f05eca":"# Evaluate on test dataset\nloss, acc = inception_model.evaluate_generator(test_generator, verbose=0)\nprint(f'Test loss: {loss:.2f}%')\nprint(f'Test accuracy: {acc*100:.2f}%')","797753ad":"# Get most likely class as y_pred and y_test\ny_pred = predict.argmax(axis=1)\ny_true = y_test.argmax(axis=1)\n","c53fba69":"# Show classification report\nprint(metrics.classification_report(y_true, y_pred))","41a37c86":"# Compute and plot the Confusion matrix\nconfusion_mtx = confusion_matrix(y_true, y_pred) \n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Validation (aka True) Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","b5d96dda":"#### Training DataGenerator","fcede032":"The way in which we plan to pass our images to the input of the model depends on the data organization within directories. Or vice versa - a dichotomous question.\nThere are two main options for organizing images in the original dataset.\n\n**1.Images are sorted into folders according to classes.**   \nFor example, images of cars - in the folder \"Cars\", images of trains - in the folder \"Trains\". This is the most convenient structure for passing to the model input.\n\nIn this case, the standard Keras ImageDataGenerator() method is suitable. However, in order to make it able to \"see\" the images, we will need to create a subdirectory inside each directory and moving the images into it. In Kaggle environment it means that we need to create appropriate subfolder hierarchy in ..\/output and then copy all images from ..\/input into it, not forgetting to divide them into the train, valid and test subfolders.\n\n**2.Images are interspersed and a metadata file with description attached.**\n\nIn this case there are few ways to go for.  \nThe first: to write a small python script, which yields batches of images and labels while reading the csv.  \nThe second: to write your custom method, which extracts images from their subfolders, assign labels to them, and finally writes them down into common X and Y subsets. (Truth be sayed, this approach is universal and also can be used in case of dataset, devided on category subfolders).    \nThe third - use another high-level API such as Dataset.","4557e67a":"### Load images into array","3207e3b7":"A few notes about dropout rate. We empyrically choosed 0,2 rate since typical 0,5 given less accuracy. A good and gentle dropout rate tuning can be found in [3].","2159622f":"### Visualize prediction on validation data","71f4c264":"We solved a flower classification problem using Machine Learning method - a CNN (Inception v3 type) using Transfer Learning approach with 90% accuracy. During Machine Learning process we devided flowers dataset into three parts: train, validation and test. First shown model validation data and then made final prediction on test. This is a classical approach. Howewer, accuaracy can be increased using more epochs or tuning model's architecture or\/and its hyperparameters.","558454d2":"#### Test DataGenerator","3476dad7":"### Model description","5b783847":"[1] https:\/\/arxiv.org\/abs\/1512.00567  \n[2] https:\/\/machinelearningmastery.com\/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks\/  \n[3] https:\/\/machinelearningmastery.com\/dropout-for-regularizing-deep-neural-networks\/","ba13861e":"# Intro\nRefer to one of classical classification problems - flowers recognition and try to address it using on of Machine Learning methods.\n\nWe choose a CNN network since this neural networks are indistrial standard in computer vision. Among many CNN architectures we choose Inception v3 [1] for learning purpose.  \n\nLet's get down to our task and remember: the days are long, the years - short.","f8db16a5":"#### Learning Rate Reduction","ba0813fc":"#### Early Stop","0c97e35d":"### Model implementation","d4234f9e":"Lets:   \na) choose **Inception V3** model which is one of the ImageNet winners with very high accuracy and low computer resources;  \nb) use a Transfer Learning paradigm.  \n\nTransfer learning refers to a technique for predictive modeling on a different but somehow similar problem that can then be reused to accelerate the training and improve the performance of a new model. If we examine the process of **deep learning**, it becomes clear that on the first layers network is trying to grasp the most basic laws (edges), on the middle layers - the most important (shapes), and on the last layer - specific details (high level features).\n\nIn deep learning, this means reusing the weights in one or more layers from a pre-trained network model in a new model and either keeping the weights fixed, fine tuning them, or adapting the weights entirely when training the model [2]\n\nPre-trained models trained with a million level data by hundreds of researchers having strong computation power (CPU, GPU and TPU). We can use pre-trained model and train it with a small set of our data. The trick is here to freeze or lock the early layers and let the final layers to be trained. In this way, our new model can know the facial patterns in middle and high level features as well.","7d87dd3b":"### Model fit","f91151cb":"#### Validation DataGenerator","4c267876":"### Predict on test data","51ec8bc8":"### Callbacks","5044d3ab":"### Visualize accuracy and loss after model fit","37f81b65":"Inception V3 [1] is a type of Convolutional Neural Networks.  It consists of many convolution and max pooling layers and includes fully connected neural networks. Network's conceptual scheme:\n![image.png](attachment:image.png)","76319723":"# Import libraries and tools","68acccc1":"### Split data on train and validation subsets ####","74789a9e":"### DataGenerators","9f75e7c4":"A few words about models parameters.  \n**input_shape:** self-descriptive, defined earlier as images with, height and color code;  \n**include_top = False:** we are going to use all the layers in the model except for the last fully connected layer as it is specific to the ImageNet competition;  \n**weights = 'imagenet':** download pre-trained weights trained on Imagenet extra-big dataset.  \n**weights = None**: load pure model and then upload weights from local machine.\n","c67eae52":"# References\n","565b30e4":"# Data load","de2a91fd":"We have 5 classes of species and 5 labels for each of them (0,1,2,3,4,5). In order to pass them on network inputs we should make some preparation known as One-Hot Encoding, which takes a single integer and produces a vector where a single element is 1 and all other elements are 0, like [0, 1, 0, 0].  \nThere are several ways in Python to do it, we will choose Keras's to_categorical() popular implementation.","7612a91c":"Our pre-trained model goes without top and output layers, we should specify them manually.\n\nUsually Flatten() or GlobalAveragePoolingXD() layers are placed at the end of the CNN to get a shape that works with dense layers. What is the difference between them?  \n\n**Flattening** a tensor means to remove all of the dimensions except for one. In other words, a flatten operation on a tensor *reshapes* the tensor to have the shape that is equal to the number of elements contained in tensor non including the batch dimension.  \n\n**GlobalAveragePooling** is a methodology used for better representation of your vector. It can be 1D\/2D\/3D. The main idea is to pool the data by averaging it (GlobalAveragePooling) or picking maximum value (GlobalMaxPooling). Padding is required to take the corner cases into the account.","cf196a32":"# Machine learning","cbb7ecbd":"# Conclusion","ae78e102":"Before we start training our model we should care about avoiding of model overfitting. Callback functions will be very helpfull.  \n\n> A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\n\nThere are two useful ones: **Early Stop** (Keras EarlyStop() method) and **Learning Rate Reduction** (Keras ReduceLROnPlateau() method). \n\n**Nota Bene**  \nIn Kaggle enviroment we should import EarlyStop and ReduceLROnPlateau from tensorflow.keras.callbacks, not from keras.callbacks, since the last doesn't accepted in model.fit().","019fe055":"### Label encoding","930afb60":"We obtain accuracy 90 %. Good result. Howewer, it can be improved by playing with model hyperpaameters.","88d594aa":"### Predict on validation data","2ee0fc30":"### Model choose"}}