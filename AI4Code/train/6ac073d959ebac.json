{"cell_type":{"0c26c44c":"code","83957771":"code","b479fb09":"code","6a5e7f7f":"code","b30598f5":"code","efd50c12":"code","4b7858f2":"code","0e87e907":"code","c46b7781":"code","21752537":"code","946c8029":"code","627dfed3":"code","aa126b20":"code","b06b8899":"code","3b851cd8":"code","addd6a12":"code","ed3d74c5":"code","c31db152":"code","bfa58ce4":"code","a4ad47ac":"code","58d67a5b":"code","13abe8f0":"code","d1355883":"code","a28cb4d8":"code","6c6823b3":"code","f19d4454":"code","27ee0131":"code","c7f72deb":"code","296f4be1":"code","ca014ddb":"code","25030815":"code","fc97e202":"code","41f6ff7b":"code","44032e3d":"code","5126f645":"code","471006cf":"code","0ca0f1bf":"code","8e8807a1":"code","86e2963f":"code","86a82dab":"code","45d2a6c5":"code","73bcc01a":"code","c37acc9d":"code","cd4732e3":"code","2b017f7c":"code","a18f84af":"code","7627a92c":"code","bae18c17":"code","193f8670":"code","0c7c3652":"code","8fe06d83":"code","d7a282aa":"code","902c2e49":"code","87fbc5c6":"code","b5f94296":"markdown","a58090de":"markdown","3b84ddb3":"markdown","df35ee87":"markdown","094fce18":"markdown","acf69102":"markdown","713daa1b":"markdown","7b4fead3":"markdown"},"source":{"0c26c44c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","83957771":"train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b479fb09":"train.shape,test.shape","6a5e7f7f":"train.head(5)","b30598f5":"train_ID=train['Id']\ntest_ID=test['Id']\ntrain.drop(\"Id\",axis=1,inplace=True)\ntest.drop(\"Id\",axis=1,inplace=True)","efd50c12":"#delete \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(30,30))\nsns.heatmap(train.corr(),annot=True)","4b7858f2":"train=train[train[\"GrLivArea\"]<4500]\ntrain.reset_index(drop=True,inplace=True)","0e87e907":"train.columns","c46b7781":"train['SalePrice']=np.log1p(train['SalePrice'])\ny=train['SalePrice']\ntrain_features=train.drop('SalePrice',axis=1)\ntest_features=test\n","21752537":"train_features.shape,test_features.shape","946c8029":"features=pd.concat([train_features,test_features],axis=0)\nfeatures.shape","627dfed3":"numeric_t = [f for f in features.columns if features.dtypes[f] != 'object']\nchar_t = [f for f in features.columns if features.dtypes[f] == 'object']\nnumeric_t ","aa126b20":"char_t","b06b8899":"features['MoSold'].value_counts()","3b851cd8":"for col in numeric_t:\n    if features[col].isnull().sum()>0:\n        print(\"{} is lack of {}\".format(col,features[col].isnull().sum()))","addd6a12":"for col in char_t:\n    if features[col].isnull().sum()>0:\n        print(\"{} is lack of {}\".format(col,features[col].isnull().sum()))","ed3d74c5":"features['MSSubClass'] = features['MSSubClass'].astype(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","c31db152":"features['Functional']=features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")","bfa58ce4":"features['Exterior1st']=features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd']=features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])","a4ad47ac":"features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")","58d67a5b":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')","13abe8f0":"features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","d1355883":"objects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\nfeatures.update(features[objects].fillna('None'))","a28cb4d8":"features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","6c6823b3":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))","f19d4454":"numerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)","27ee0131":"from scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","c7f72deb":"features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","296f4be1":"features['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])","ca014ddb":"features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","25030815":"print(features.shape)","fc97e202":"final_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)","41f6ff7b":"X = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(X):, :]\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","44032e3d":"outliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","5126f645":"overfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()","471006cf":"overfit","0ca0f1bf":"print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","8e8807a1":"import numpy as np  # linear algebra\nimport pandas as pd  #\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","86e2963f":"# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","86a82dab":"from sklearn.model_selection import GridSearchCV\nkfolds=KFold(n_splits=10,shuffle=True,random_state=42)\nscale=RobustScaler().fit(X)\nX1=scale.transform(X)\nfrom sklearn.linear_model import Ridge\n\nmodel=Ridge()\nrid_param_grid = {\"alpha\":[19.8]}\ngrid_search= GridSearchCV(model,param_grid=rid_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nrid_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","45d2a6c5":"from sklearn.linear_model import Lasso\nmodel=Lasso()\nlas_param_grid = {\"alpha\":[0.0005963623316594642]}\ngrid_search= GridSearchCV(model,param_grid=las_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nlas_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","73bcc01a":"from sklearn.linear_model import ElasticNet\nmodel=ElasticNet()\nela_param_grid = {\"alpha\":[0.0006951927961775605],\n                 \"l1_ratio\":[0.90]}\ngrid_search= GridSearchCV(model,param_grid=ela_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nela_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","c37acc9d":"from sklearn.svm import SVR\nmodel=SVR()\nsvr_param_grid = {\"C\":[66],\n                 \"gamma\":[6.105402296585326e-05]}\ngrid_search= GridSearchCV(model,param_grid=svr_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nsvr_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","cd4732e3":"grid_search.best_params_","2b017f7c":"model=GradientBoostingRegressor()\ngbdt_param_grid = {\"n_estimators\":[2200],\n                 \"learning_rate\":[0.05],\n                   \"max_depth\":[3],\n                   \"max_features\":[\"sqrt\"],\n                   \"min_samples_leaf\":[5],\n                   \"min_samples_split\":[12],\n                   \"loss\":[\"huber\"]\n                  }\n                   \n                   \n                   \ngrid_search= GridSearchCV(model,param_grid=gbdt_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\ngbdt_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","a18f84af":"model=LGBMRegressor()\nlgbm_param_grid = {\n                   'objective':['regression'], \n                   'max_depth':[5],\n                   'num_leaves':[12],\n                   'learning_rate':[0.005], \n                    'n_estimators':[5500],\n                    'max_bin':[190], \n                    'bagging_fraction':[0.2],\n                    'feature_fraction':[0.2]                  \n                  }\n                   \n                   \n                   \ngrid_search= GridSearchCV(model,param_grid=lgbm_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nlgbm_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","7627a92c":"from sklearn.model_selection import GridSearchCV\nkfolds=KFold(n_splits=5,shuffle=True,random_state=42)\nscale=RobustScaler().fit(X)\nX1=scale.transform(X)\nmodel=XGBRegressor()\nxgb_param_grid = {\"n_estimators\":[3000],\n                 \"learning_rate\":[0.01],\n                   \"max_depth\":[3],\n                   \"subsample\":[0.8],\n                \"colsample_bytree\":[0.8],\n                 \"gamma\":[0],\n                \"objective\":['reg:linear'],\n                \"min_child_weight\":[2], \n                \"reg_alpha\":[0.1],\n                \"reg_lambda\":[0.5]\n                  }\n                   \n                   \n                   \ngrid_search= GridSearchCV(model,param_grid=xgb_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nxgb_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","bae18c17":"stack_gen = StackingCVRegressor(regressors=(rid_best, las_best, ela_best,\n                                            gbdt_best, xgb_best, lgbm_best),\n                                meta_regressor=xgb_best,\n                                use_features_in_secondary=True)\nstack_gen.fit(np.array(X1), np.array(y))","193f8670":"X_sub=scale.transform(X_sub)","0c7c3652":"def blend_models_predict(X):\n    return ((0.1 * ela_best.predict(X)) + \\\n            (0.1 * las_best.predict(X)) + \\\n            (0.1 * rid_best.predict(X)) + \\\n            (0.1 * svr_best.predict(X)) + \\\n            (0.1 * gbdt_best.predict(X)) + \\\n            (0.1 * xgb_best.predict(X)) + \\\n            (0.1 * lgbm_best.predict(X)) + \\\n            (0.3 * stack_gen.predict(np.array(X))))\n            \nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X1)))\n\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))\n","8fe06d83":"submission.head()","d7a282aa":"print('Blend with Top Kernels submissions\\n')\nsub_1 = pd.read_csv('..\/input\/top-10-0-10943-stacking-mice-and-brutal-force\/House_Prices_submit.csv')\nsub_2 = pd.read_csv('..\/input\/hybrid-svm-benchmark-approach-0-11180-lb-top-2\/hybrid_solution.csv')\nsub_3 = pd.read_csv('..\/input\/lasso-model-for-regression-problem\/lasso_sol22_Median.csv')\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(blend_models_predict(X_sub)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))","902c2e49":"submission.to_csv(\"submission.csv\", index=False)","87fbc5c6":"submission.head(5)","b5f94296":"print('TEST score on CV')\n\nscore = cv_rmse(ridge)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","a58090de":"print('RMSLE score on train data:')\nprint(rmsle(y,las_best.predict(X1)))\nX_sub=scale.transform(X_sub)\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(las_best.predict(X_sub)))","3b84ddb3":"def blend_models_predict(X):\n    return ((0.4 * ela_best.predict(X)) + \\\n            (0.3 * las_best.predict(X)) + \\\n            (0.3 * rid_best.predict(X))) \n            \n            \nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X1)))\n\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.expm1(blend_models_predict(X_sub))\n","df35ee87":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))\n            \nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))\n\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))\n","094fce18":"LotFrontage is lack of 486   q\nMasVnrArea is lack of 23\nBsmtFinSF1 is lack of 1\nBsmtFinSF2 is lack of 1\nBsmtUnfSF is lack of 1\nTotalBsmtSF is lack of 1\nBsmtFullBath is lack of 2\nBsmtHalfBath is lack of 2\nGarageYrBlt is lack of 159   q\nGarageCars is lack of 1      q \nGarageArea is lack of 1      q","acf69102":"# setup models    \nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds,))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, random_state=42, l1_ratio=e_l1ratio))\n                                        \nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n                                   \n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\n                                       \n\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006, random_state=42)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","713daa1b":"print('START Fit')\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\nprint(datetime.now(), 'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'svr')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","7b4fead3":"MSZoning is lack of 4                q\nAlley is lack of 2719                w\nUtilities is lack of 2               w\nExterior1st is lack of 1             w\nExterior2nd is lack of 1             w\nMasVnrType is lack of 24             w\nBsmtQual is lack of 81               q \nBsmtCond is lack of 82               q\nBsmtExposure is lack of 82           q\nBsmtFinType1 is lack of 79           q\nBsmtFinType2 is lack of 80           q\nElectrical is lack of 1              q\nKitchenQual is lack of 1             q\nFunctional is lack of 2              q\nFireplaceQu is lack of 1420          w\nGarageType is lack of 157            q\nGarageFinish is lack of 159          q\nGarageQual is lack of 159            q\nGarageCond is lack of 159            q\nPoolQC is lack of 2908               q\nFence is lack of 2346                w\nMiscFeature is lack of 2812          w\nSaleType is lack of 1                w"}}