{"cell_type":{"f7a4a001":"code","4c5a4ff4":"code","6a98378d":"code","e40b81c5":"code","1c956965":"code","6824209c":"code","d76ff02b":"code","d7d95cf2":"code","5ff79f10":"code","6f3ff05c":"code","c47921bb":"code","65ae9ea0":"code","c7d2982c":"code","4a019b71":"code","d525abbd":"code","1ad6b494":"code","afb08d1c":"code","c9acfeaf":"code","6f106043":"code","11097505":"code","b692d6ac":"code","f533f5e7":"code","1b84a718":"code","d837c86e":"code","835ead91":"code","2ae62f13":"code","dbaa766c":"code","9194955c":"code","6002b297":"code","6fe1f64b":"code","7ec4fff6":"code","3e6a47f7":"markdown","a9099fd2":"markdown","267808b4":"markdown","ddbfee5d":"markdown","629e75f9":"markdown","0abaafd7":"markdown","a7781112":"markdown","96d652b8":"markdown","143752bb":"markdown","0007f86e":"markdown","5e6952df":"markdown","67560f45":"markdown","892f73e4":"markdown","33fffdc5":"markdown","849858fd":"markdown","df93eae7":"markdown"},"source":{"f7a4a001":"# Import Required Python Packages :\n\n# Scientific and Data Manipulation Libraries :\n\nimport numpy as np\nimport pandas as pd\n\n# Data Viz & Regular Expression Libraries :\n\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n# Scikit-Learn ML Libraries :\n\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n\n# Garbage Collection Libraries :\n\nimport gc\n\n# Boosting Algorithm Libraries :\n\nfrom xgboost                          import XGBClassifier\nfrom catboost                         import CatBoostClassifier\nfrom lightgbm                         import LGBMClassifier\nfrom sklearn.ensemble                 import RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics                  import accuracy_score\nfrom sklearn.model_selection          import StratifiedKFold,KFold","4c5a4ff4":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6a98378d":"# Load data \n\ntrain = pd.read_csv('..\/input\/customer\/Train.csv')\ntest = pd.read_csv('..\/input\/customer\/Test.csv')\nsub = pd.read_csv('..\/input\/customer\/sample_submission.csv')","e40b81c5":"# Python Method 1 : Displays Data Information :\n\ndef display_data_information(data, data_types, dataframe_name):\n    print(\" Information of \",dataframe_name,\": Rows = \",data.shape[0],\"| Columns = \",data.shape[1],\"\\n\")\n    data.info()\n    print(\"\\n\")\n    for VARIABLE in data_types :\n        data_type = data.select_dtypes(include=[ VARIABLE ]).dtypes\n        if len(data_type) > 0 :\n            print(str(len(data_type))+\" \"+VARIABLE+\" Features\\n\"+str(data_type)+\"\\n\"  )        \n\n# Display Data Information of \"train\" :\n\ndata_types  = [\"float32\",\"float64\",\"int32\",\"int64\",\"object\",\"category\",\"datetime64[ns]\"]\ndisplay_data_information(train, data_types, \"train\")","1c956965":"# Display Data Information of \"test\" :\n\ndisplay_data_information(test, data_types, \"test\")","6824209c":"# Python Method 2 : Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table) :\n\ndef display_head_tail(data, head_rows, tail_rows):\n    display(\"Data Head & Tail :\")\n    display(data.head(head_rows).append(data.tail(tail_rows)))\n#     return True\n\n# Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table)\n# Pass Dataframe as \"train\", No. of Rows in Head = 3 and No. of Rows in Tail = 2 :\n\ndisplay_head_tail(train, head_rows=3, tail_rows=2)","d76ff02b":"display_head_tail(test, head_rows=3, tail_rows=2)","d7d95cf2":"# Python Method 3 : Displays Data Description using Statistics :\n\ndef display_data_description(data, numeric_data_types, categorical_data_types):\n    \n    print(\"Data Description :\")\n    display(data.describe( include = numeric_data_types))\n    print(\"\")\n    display(data.describe( include = categorical_data_types))\n\n# Display Data Description of \"train\" :\n\ndisplay_data_description(train, data_types[0:4], data_types[4:7])","5ff79f10":"# Display Data Description of \"test\" :\n\ndisplay_data_description(test, data_types[0:4], data_types[4:7])","6f3ff05c":"# Checking Percentage(%) of Common ID's  between train and test data using Unique train values :\n\nprint(np.intersect1d(train['ID'], test['ID']).shape[0]\/train['ID'].nunique())\ncommon_ids = len(set(test['ID'].unique()).intersection(set(train['ID'].unique())))\nprint(\"Common IDs : \",common_ids)\n\n# Data Leak as out of 2627 Rows , there are 2332 ID's in Common\n\nprint(\"Unique IDs : \",test.shape[0] - common_ids)","c47921bb":"testx = pd.merge(test,train,how='left', on = 'ID')","65ae9ea0":"# Python Method 4 : Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\n\ndef remove_duplicate(data):\n    \n    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n    data.drop_duplicates(keep=\"first\", inplace=True) \n    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n    \n    return data\n\n# Remove Duplicates from \"train\" data :\n\ntrain = remove_duplicate(train)\n\n# No Duplicates at all !!!","c7d2982c":"# # Python Method 5 : Fills or Imputes Missing values with Various Methods : \n\n# def fill_missing_values(data, fill_value, fill_types, columns, dataframe_name):\n    \n#     print(\"Missing Values BEFORE REMOVAL in \",dataframe_name,\" data\")\n#     display(data.isnull().sum())\n#     for column in columns :\n        \n#         # Fill Missing Values with Specific Value :\n#         if \"Value_Fill\" in fill_types :\n#             data[ column ] = data[ column ].fillna(fill_value)\n# #             print(\"Value_Fill\")\n\n#         # Fill Missing Values with Forward Fill  (Previous Row Value as Current Row in Table) :\n#         if \"Forward_Fill\" in fill_types :\n#             data[ column ] = data[ column ].ffill(axis = 0)\n# #             print(\"Forward_Fill\")\n\n#         # Fill Missing Values with Backward Fill (Next Row Value as Current Row in Table) :\n#         if \"Backward_Fill\" in fill_types :\n#             data[ column ] = data[ column ].bfill(axis = 0)\n# #             print(\"Backward_Fill\")\n    \n#     print(\"Missing Values AFTER REMOVAL in \",dataframe_name,\" data\")\n#     display(data.isnull().sum())\n    \n#     return data\n\n# fill_types = [ \"Forward_Fill\"]\n# fill_value = 0\n# # Fills or Imputes Missing values in \"Registration_Date\" Column with \"Forward_Fill\" Method in \"train\" : \n# train = fill_missing_values(train, fill_value, fill_types, [\"Registration_Date\"],\"train\")\n\n# # Fills or Imputes Missing values in \"Registration_Date\" Column with \"Forward_Fill\" Method in \"train\" :\n# test  = fill_missing_values(test, fill_value, fill_types, [\"Registration_Date\"],\"test\")","4a019b71":"# Let LightGBM Classifier Handle the Issues :","d525abbd":"# Python Method 6 : Displays Unique Values in Each Column of the Dataframe(Table) :\n\ndef display_unique(data):\n    for column in data.columns :\n        \n        print(\"No of Unique Values in \"+column+\" Column are : \"+str(data[column].nunique()))\n        print(\"Actual Unique Values in \"+column+\" Column are : \"+str(data[column].sort_values(ascending=True,na_position='last').unique() ))\n        print(\"NULL Values :\")\n        print(data[ column ].isnull().sum())\n        print(\"Value Counts :\")\n        print(data[column].value_counts())\n        print(\"\")\n        \n# Displays Unique Values in Each Column of \"train\" :\n# Check \"train\" data for Values of each Column - Long Form :\n\ndisplay_unique(train)\n\n# Display this info in a Table Format - Improvements coming In Part 2","1ad6b494":"# Check \"train\" data for Values of each Column - Short Form :\n# Use Whichever you feel good working with :\n\nfor i in train:\n    print(f\"column {i} unique values {train[i].unique()}\")","afb08d1c":"# Concatenate train and test data into single DataFrame - df :\n\ntrain['is_train'] = 1\ntest['is_train'] = 0\ndf = pd.concat([train,test])","c9acfeaf":"# Convert 2 Categorical(String) Columns 'City_Type','Employer_Category' using Label Encode Technique :\n# Docs : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n# Label encode category values\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfor i in ['Gender' , 'Ever_Married' , 'Graduated','Profession','Spending_Score','Var_1'  ]:\n    le = LabelEncoder()\n    df[i] = le.fit_transform(df[i].astype('str'))","6f106043":"# Mapping Values to Label ENCODED Values :\n\ndf['Segmentation'] = df['Segmentation'].map({'A':0,'B':1,'C':2,'D':3})","11097505":"# Get Back train data from df with a condition on column is_train == 1 :\n\ntrain = df[df['is_train'] == 1]","b692d6ac":"# split train into 5 folds and apply random forest and check accuracy of each fold\n\npredictor_train = train.drop(['Segmentation','is_train','ID'],axis=1)\ntarget_train    = train['Segmentation']","f533f5e7":"predictor_test = test.drop(['is_train','ID'],axis=1)","1b84a718":"def data_encoding( encoding_strategy , encoding_data , encoding_columns ):\n    \n    if encoding_strategy == \"LabelEncoding\":\n        Encoder = LabelEncoder()\n        for column in encoding_columns :\n            encoding_data[ column ] = Encoder.fit_transform(tuple(encoding_data[ column ]))\n        \n    elif encoding_strategy == \"OneHotEncoding\":\n#         display(encoding_data[encoding_columns])\n        encoding_data = pd.get_dummies( encoding_data  )\n        \n    elif encoding_strategy == \"TargetEncoding\":\n        ## Code Coming soon\n        print(\"TargetEncoding\")\n\n    else :\n        encoding_data = pd.get_dummies( encoding_data[encoding_columns]  )\n        \n    dtypes_list =['float64','float32','int64','int32']\n    # BEST CODE : 0.6872386379302422\n#     encoding_data.astype( dtypes_list[0] ).dtypes # UNCOMMENTED EARLIER\n    # NEW CODE : 0.6872386379302422 - NO CHANGE !!!\n    # encoding_data.astype( dtypes_list[0] ).dtypes - COMMENTED NOW\n    \n    return encoding_data\n\nencoding_columns  = [ \"Gender\", \"Ever_Married\" , \"Graduated\", \"Profession\" , \"Spending_Score\", \"Var_1\" ]\nencoding_strategy = [ \"OneHotEncoding\", \"LabelEncoding\", \"TargetEncoding\", \"ELSE\"]\n\npredictor_train_encode = data_encoding( encoding_strategy[1] , predictor_train , encoding_columns )\npredictor_test_encode  = data_encoding( encoding_strategy[1] , predictor_test ,  encoding_columns )","d837c86e":"print(\"predictor_train_encode SHAPE   : \",predictor_train_encode.shape)\ndisplay(\"predictor_train_encode COLUMNS : \",predictor_train_encode.head())\n\nprint(\"predictor_test_encode SHAPE   : \",predictor_test_encode.shape)\ndisplay(\"predictor_test_encode COLUMNS : \",predictor_test_encode.head())","835ead91":"# Mention Categorical Values of the Light GBM Model to Handle :\ncategorical_features = [\"Gender\", \"Ever_Married\" ,\"Graduated\" ,\"Profession\" ,\"Spending_Score\" ,\"Var_1\" ]\n\nlgb_model = LGBMClassifier()\n\n# Apply Stratified K-Fold Cross Validation where K=5 or n_splits=5 :\nkf = StratifiedKFold(n_splits=5,shuffle=True,random_state=10)\nacc = []\n\n# Pass predictor_train,target_train for Cross Validation :\nfor fold,(t_id,v_id) in enumerate(kf.split(predictor_train,target_train)):\n    \n    # Split train and validation data :\n    tx = predictor_train.iloc[t_id]; ty = target_train.iloc[t_id]\n    vx = predictor_train.iloc[v_id]; vy = target_train.iloc[v_id]\n    \n    # Train\/Fit the Data to LighGBM Model :\n    lgb_model.fit(tx,ty, categorical_feature = categorical_features )\n    \n    # Predict the Validation Data to Train LighGBM Model :\n    val_y = lgb_model.predict(vx)\n    \n    # Get Accuracy Score on Validation Data for Each Fold :\n    acc_score = accuracy_score(vy,val_y)\n    acc.append(acc_score)\n    print(f\"fold {fold} accuracy {acc_score}\")\n\n# Get Mean of Accuracy Score on Validation Data for All 5 Folds :\nprint(f\"Mean accuracy score {np.mean(acc)}\")","2ae62f13":"# Tuned the Hyperparameters of LighGBM Classifier :\nlgb_model = LGBMClassifier(\n                                   boosting_type='gbdt', \n                                   max_depth=15, \n                                   learning_rate=0.15, \n                                   objective='multiclass', # Multi Class Classification\n                                   random_state=100,  \n                                   n_estimators=1000 ,\n                                   reg_alpha=0, \n                                   reg_lambda=1, \n                                   n_jobs=-1\n                                 )","dbaa766c":"# Mention Categorical Values of the Light GBM Model to Handle :\ncategorical_features = [\"Gender\", \"Ever_Married\" ,\"Graduated\" ,\"Profession\" ,\"Spending_Score\" ,\"Var_1\" ]\n\n# Apply Stratified K-Fold Cross Validation where K=5 or n_splits=5 :\nkf = StratifiedKFold(n_splits=5,shuffle=True,random_state=10)\nacc = []\n\n# Pass predictor_train,target_train for Cross Validation :\nfor fold,(t_id,v_id) in enumerate(kf.split(predictor_train,target_train)):\n    \n    # Split train and validation data :\n    tx = predictor_train.iloc[t_id]; ty = target_train.iloc[t_id]\n    vx = predictor_train.iloc[v_id]; vy = target_train.iloc[v_id]\n    \n    # Train\/Fit the Data to LighGBM Model :\n    lgb_model.fit(tx,ty, categorical_feature = categorical_features )\n    \n    # Predict the Validation Data to Train LighGBM Model :\n    val_y = lgb_model.predict(vx)\n    \n    # Get Accuracy Score on Validation Data for Each Fold :\n    acc_score = accuracy_score(vy,val_y)\n    acc.append(acc_score)\n    print(f\"fold {fold} accuracy {acc_score}\")\n\n# Get Mean of Accuracy Score on Validation Data for All 5 Folds :\nprint(f\"Mean accuracy score {np.mean(acc)}\")","9194955c":"def model_train_predict_submit( Classifiers_model_name, model_name ,X_train, y_train, X_test, target):\n    \n    categorical_features = [\"Gender\", \"Ever_Married\" ,\"Graduated\" ,\"Profession\" ,\"Spending_Score\" ,\"Var_1\"  ]\n    Classifiers_model_name.fit( X_train, y_train , categorical_feature = categorical_features )\n    final_predictions = Classifiers_model_name.predict( X_test )\n    print(final_predictions)  \n   \n    Result_Promoted = pd.DataFrame({'ID': sub['ID'], target : final_predictions})\n    Result_Promoted[ target ]=Result_Promoted[ target ].map({0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\" })\n    print(Result_Promoted[ target ].unique())\n    Result_Promoted.to_csv(model_name +\"_Labelling=Yes_Scaling=Yes\"+\".csv\",index=False)\n    return Result_Promoted\n\nmodel_name       = \"LGBM_Tuned_BEST\"\nmodel_classifier = lgb_model\nsub = model_train_predict_submit( model_classifier, model_name, predictor_train_encode,target_train, predictor_test_encode, target = 'Segmentation')","6002b297":"sub1 = pd.merge(sub,testx,how='left',on='ID')\nsub1.head()","6fe1f64b":"sub['segmentation2'] = sub1['Segmentation_y']\nsub.head()","7ec4fff6":"sub['segmentation2'] = sub['segmentation2'].fillna('x')\nfor i in range(len(sub)):\n    if sub.iloc[i,2] != 'x':\n        sub.iloc[i,1] = sub.iloc[i,2]\n        \nsub[['ID','Segmentation']].to_csv('FINAL_LGBM_BEST_SUBMISSION_TUNED.csv',index = False)","3e6a47f7":"## Steps for Applied Machine Learning (ML) for Hackathons :\n\n1.  Understand the Problem Statement & Import Packages and Datasets.  \n\n2.  Perform EDA (Exploratory Data Analysis) - Understanding the Datasets :\n\n       *       Explore Train and Test Data and get to know what each Column \/ Feature denotes.\n       *       Check for Imbalance of Target Column in Datasets.\n       *       Visualize Count Plots & Unique Values to infer from Datasets.\n            \n3.  Remove Duplicate Rows from Train Data if present.\n\n4.  Fill\/Impute Missing Values Continuous - Mean\/Median\/Any Specific Value & Categorical - Others\/ForwardFill\/BackFill.\n\n5.  Feature Engineering \n\n      *       Feature Selection - Selection of Most Important Existing Features.\n      *       Feature Creation  - Creation  of New Feature from the Existing Features.\n      \n6.  Split Train Data into Train and Validation Data with Predictors(Independent) & Target(Dependent).      \n7.  Data Encoding - Label Encoding, OneHot Encoding and Data Scaling - MinMaxScaler, StandardScaler, RobustScaler\n8.  Create Baseline ML Model for Multi Class Classification Problem\n9.  Improve ML Model,Fine Tune with MODEL Evaluation METRIC - \"Accuracy\" and Predict Traget \"Outcome\"\n10. Result Submission, Check Leaderboard & Improve \"Accuracy\" Score","a9099fd2":"## 2. Perform EDA (Exploratory Data Analysis) - Understanding the Datasets :    \n\n### 2.1 Explore Train and Test Data and get to know what each Column \/ Feature denotes :","267808b4":"## 9. Improve ML Model,Fine Tune with MODEL Evaluation METRIC - \"Accuracy\" and Predict Target \"Segmentation\" :","ddbfee5d":"## 4.  Fill\/Impute Missing Values Continuous - Mean\/Median\/Any Specific Value & Categorical - Others\/ForwardFill\/BackFill :","629e75f9":"## 3.  Remove Duplicate Rows from Train data if present :","0abaafd7":"### Multi - Class Classification Problem - Target has more than 2 Categories - \n### Target - Segmentation has 4 Values of Customers ['D' 'A' 'B' 'C']","a7781112":"![Customer_Seg.jpg](attachment:Customer_Seg.jpg)","96d652b8":"![Customer_RANK.jpg](attachment:Customer_RANK.jpg)","143752bb":"## 7.  Data Encoding - Label Encoding :","0007f86e":"## 8.  Create Baseline ML Model :","5e6952df":"## 10. Result Submission, Check Leaderboard & Improve \"ACCURACY\" :","67560f45":"## 6.  Split Train Data into Train and Validation Data with Predictors(Independent) & Target(Dependent) :","892f73e4":"### DATASET can be downloaded here -> https:\/\/www.kaggle.com\/vetrirah\/customer","33fffdc5":"### **<center>\ud83d\ude0a Reached Rank 6 - Top 5 Private Score - Thanks for reading Friends. See you all in Part 2 for more Analysis and Modelling - ENCOURAGE if you liked this Notebook \ud83d\ude0a<\/center>**\n\n### **<center>\ud83d\ude0a For Learning Purpose - You can still participate in your free time to see your Public and Private Scores & Rank, though it won't reflect on Leaderboard \ud83d\ude0a<\/center>**\n\n### **<center>\ud83d\ude0a Ask your doubts & Share your thoughts, ideas & feedbacks in Comments below \ud83d\ude0a<\/center>**","849858fd":"## 1.  Understand the Problem Statement & Import Packages and Datasets :","df93eae7":"## 5.  Feature Engineering\n\n### 5.1 Feature Selection - Selection of Most Important Existing Features\n### 5.2 Feature Creation  - Creation  of New Features from the Existing Features \/ Predictors :"}}