{"cell_type":{"dd945d3a":"code","45bed7de":"code","a64df240":"code","0ea7ce24":"code","b85b1fcc":"code","fecdd63a":"code","7f2b6a45":"code","ff96dea1":"code","85226d19":"code","6997e2ec":"code","02561eee":"code","21c4eea5":"code","5240cda2":"code","49320959":"code","2ae66a1b":"code","bcb7dc65":"code","6c61eaa3":"code","6c968561":"code","da01802c":"code","ada8001d":"code","21089fa8":"code","2df82864":"code","faf35e2b":"code","7e5763ea":"code","d91e8035":"code","342c0970":"code","77df442b":"markdown","23c79264":"markdown","bbe39ec6":"markdown","77d1bba5":"markdown","d7e5e41e":"markdown","c287c4af":"markdown","f1f282ed":"markdown","acb1ff71":"markdown","ca040dd2":"markdown","20dd9657":"markdown","c8567c6a":"markdown","b9033956":"markdown","81594fcd":"markdown","349bc844":"markdown","4b253eac":"markdown","c3057670":"markdown","48145626":"markdown","59ef603e":"markdown","af36721f":"markdown","41ac4a5b":"markdown"},"source":{"dd945d3a":"import tensorflow as tf\ntry:\n   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n   tpu = None\nif tpu:\n   tf.config.experimental_connect_to_cluster(tpu)\n   tf.tpu.experimental.initialize_tpu_system(tpu)\n   strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n   strategy = tf.distribute.get_strategy()","45bed7de":"!pip install nlp\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","a64df240":"%%time\n%autosave 60\n\nimport os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport gc\ngc.enable()\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm \n\nimport nlp\nimport transformers\nfrom transformers import (AdamW, \n                          XLMRobertaTokenizer, \n                          XLMRobertaModel, \n                          get_cosine_schedule_with_warmup)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.serialization as xser\nimport torch_xla.version as xv\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint('PYTORCH:', xv.__torch_gitrev__)\nprint('XLA:', xv.__xla_gitrev__)","0ea7ce24":"train = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\nsample_submission = pd.read_csv('..\/input\/contradictory-my-dear-watson\/sample_submission.csv')","b85b1fcc":"TRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 4\nMAX_LEN = 80\n# Scale learning rate to 8 TPU's\nLR = 2e-5 * xm.xrt_world_size() \nMETRICS_DEBUG = True\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')","fecdd63a":"# mnli data\nmnli = nlp.load_dataset(path='glue', name='mnli', split='train[:5%]')\n\n# xnli data\nxnli = nlp.load_dataset(path='xnli')\nxnli = nlp.concatenate_datasets([xnli['test'], xnli['validation']])\n\n# snli data\nsnli = nlp.load_dataset(path='snli', split='train[:5%]')","7f2b6a45":"print(\"#\"*25)\nprint(\"  MNLI\"); print(\"#\"*25)\nprint(\"Shape: \", mnli.shape)\nprint(\"Num of Samples: \", mnli.num_rows)\nprint(\"Num of Columns: \", mnli.num_columns)\nprint(\"Column Names: \", mnli.column_names)\nprint(\"Features: \", mnli.features)\nprint(\"Num of Classes: \", mnli.features['label'].num_classes)\nprint(\"Split: \", mnli.split)\nprint(\"Description: \", mnli.description)\nprint(f\"Labels Count - 0's:{len(mnli.filter(lambda x: x['label']==0))}, 1's:{len(mnli.filter(lambda x: x['label']==1))}, 2's: 0's:{len(mnli.filter(lambda x: x['label']==2))}\")\nprint()\nprint(\"#\"*25)\nprint(\"  XNLI\"); print(\"#\"*25)\nprint(\"Shape: \", xnli.shape)\nprint(\"Num of Samples: \", xnli.num_rows)\nprint(\"Num of Columns: \", xnli.num_columns)\nprint(\"Column Names: \", xnli.column_names)\nprint(\"Features: \", xnli.features)\nprint(\"Split: \", xnli.split)\nprint(\"Description: \", xnli.description)\nprint(f\"Labels Count - 0's:{len(xnli.filter(lambda x: x['label']==0))}, 1's:{len(xnli.filter(lambda x: x['label']==1))}, 2's: 0's:{len(xnli.filter(lambda x: x['label']==2))}\")\nprint()\nprint(\"#\"*25)\nprint(\"  SNLI\"); print(\"#\"*25)\nprint(\"Shape: \", snli.shape)\nprint(\"Num of Samples: \", snli.num_rows)\nprint(\"Num of Columns: \", snli.num_columns)\nprint(\"Column Names: \", snli.column_names)\nprint(\"Features: \", snli.features)\nprint(\"Num of Classes: \", snli.features['label'].num_classes)\nprint(\"Split: \", snli.split)\nprint(\"Description: \", snli.description)\nprint(f\"Labels Count - 0's:{len(snli.filter(lambda x: x['label']==0))}, 1's:{len(snli.filter(lambda x: x['label']==1))}, 2's: 0's:{len(snli.filter(lambda x: x['label']==2))}\")","ff96dea1":"# encoding\ndef convert_to_features(batch):\n    input_pairs = list(zip(batch['premise'], batch['hypothesis']))\n    encodings = tokenizer.batch_encode_plus(input_pairs, \n                                            add_special_tokens=True, \n                                            pad_to_max_length=True, \n                                            max_length=MAX_LEN, \n                                            truncation=True, \n                                            return_attention_mask=True, \n                                            return_token_type_ids=True)\n    return encodings","85226d19":"# function to preprocess special structure of xnli\ndef preprocess_xnli(example):\n    premise_output = []\n    hypothesis_output = []\n    label_output = []\n    for prem, hyp, lab in zip(example['premise'],  example['hypothesis'], example[\"label\"]):\n        label = lab\n        langs = hyp['language']\n        translations = hyp['translation']\n        hypothesis = {k: v for k, v in zip(langs, translations)}\n        for lang in prem:\n            if lang in hypothesis:\n                premise_output += [prem[lang]]\n                hypothesis_output += [hypothesis[lang]]\n                label_output += [label]\n    return {'premise':premise_output, 'hypothesis':hypothesis_output, 'label':label_output}","6997e2ec":"# encode mnli and convert to torch tensor\nmnli_encoded = mnli.map(convert_to_features, batched=True, remove_columns=['idx', 'premise', 'hypothesis'])\nmnli_encoded.set_format(\"torch\", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label'])","02561eee":"# preprocess xnli, encode and convert to torch tensor\nxnli_processed = xnli.map(preprocess_xnli, batched=True)\nxnli_encoded = xnli_processed.map(convert_to_features, batched=True, remove_columns=['premise', 'hypothesis'])\nxnli_encoded.set_format(\"torch\", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) ","21c4eea5":"# encode snli and convert to torch tensor\nsnli_encoded = snli.map(convert_to_features, batched=True, remove_columns=['premise', 'hypothesis'])\nsnli_encoded.set_format(\"torch\", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) ","5240cda2":"print(mnli_encoded.column_names)\nprint(snli_encoded.column_names)\nprint(xnli_encoded.column_names)\n\nprint(mnli_encoded.num_rows)\nprint(snli_encoded.num_rows)\nprint(xnli_encoded.num_rows)","49320959":"train_dataset = nlp.load_dataset('csv', data_files=['..\/input\/contradictory-my-dear-watson\/train.csv'])['train']\n\nprint(train_dataset.num_rows)\nprint(train_dataset.column_names)\ndrop_columns = train_dataset.column_names[:-1]\n\nencoded_train_dataset = train_dataset.map(convert_to_features, batched=True, remove_columns=drop_columns)\nencoded_train_dataset.set_format(\"torch\", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) \nprint(encoded_train_dataset.num_rows)\nprint(encoded_train_dataset.column_names)","2ae66a1b":"train_dataset = nlp.concatenate_datasets([mnli_encoded, \n                                          xnli_encoded, \n                                          snli_encoded,\n                                          encoded_train_dataset\n                                         ])\n\nprint(train_dataset.num_rows)\nprint(train_dataset.column_names)","bcb7dc65":"train_dataset.cleanup_cache_files()\ndel mnli, mnli_encoded\ndel xnli, xnli_encoded, xnli_processed\ndel snli, snli_encoded\ngc.collect()","6c61eaa3":"class DatasetRetriever(Dataset):\n    def __init__(self, dataset:nlp.arrow_dataset.Dataset):\n        self.dataset = dataset\n        self.ids = self.dataset['input_ids']\n        self.mask = self.dataset['attention_mask']\n        self.type_ids = self.dataset['token_type_ids']\n        self.targets = self.dataset[\"label\"]\n        \n    def __len__(self):\n        return self.dataset.num_rows\n    \n    def __getitem__(self, index):   \n        ids = self.ids[index]\n        mask = self.mask[index]\n        type_ids = self.type_ids[index]\n        targets = self.targets[index]\n        return {\n            'ids':torch.tensor(ids),\n            'mask':torch.tensor(mask),\n            'type_ids':torch.tensor(type_ids),\n            'targets':targets\n        }","6c968561":"class XLMRoberta(nn.Module):\n    def __init__(self, num_labels, multisample):\n        super(XLMRoberta, self).__init__()\n        output_hidden_states = False\n        self.num_labels = num_labels\n        self.multisample= multisample\n        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\", \n                                                       output_hidden_states=output_hidden_states, \n                                                       num_labels=1)\n        self.layer_norm = nn.LayerNorm(1024*2)\n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)        \n        self.classifier = nn.Linear(1024*2, self.num_labels)\n    \n    def forward(self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None):\n        outputs = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask,\n                               inputs_embeds=inputs_embeds)\n        average_pool = torch.mean(outputs[0], 1)\n        max_pool, _ = torch.max(outputs[0], 1)\n        concatenate_layer = torch.cat((average_pool, max_pool), 1)\n        normalization = self.layer_norm(concatenate_layer)\n        if self.multisample:\n            # Multisample Dropout\n            logits = torch.mean(\n                torch.stack(\n                    [self.classifier(self.dropout(normalization)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            logits = self.dropout(normalization)\n            logits = self.classifier(logits)       \n        outputs = logits\n        return outputs  ","da01802c":"class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches \/\/ 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '\/' + fmt.format(num_batches) + ']'\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res","ada8001d":"def get_model_optimizer(model):\n    # Differential Learning Rate\n    def is_backbone(name):\n        return \"roberta\" in name\n    \n    optimizer_grouped_parameters = [\n       {'params': [param for name, param in model.named_parameters() if is_backbone(name)], 'lr': LR},\n       {'params': [param for name, param in model.named_parameters() if not is_backbone(name)], 'lr': 1e-3} \n    ]\n    \n    optimizer = AdamW(\n        optimizer_grouped_parameters, lr=LR, weight_decay=1e-2\n    )\n    \n    return optimizer","21089fa8":"def loss_fn(outputs, targets):\n    return nn.CrossEntropyLoss()(outputs, targets)","2df82864":"def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n    # Train\n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, data_time, losses, top1],\n        prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n    )\n    model.train()\n    end = time.time()\n    for i, data in enumerate(train_loader):\n        data_time.update(time.time()-end)\n        ids, mask, type_ids, targets = data[\"input_ids\"], data[\"attention_mask\"], data['token_type_ids'], data[\"label\"]\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        type_ids = type_ids.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids = ids,\n            attention_mask = mask,\n            token_type_ids = type_ids\n        )\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        loss = loss_fn(outputs, targets)\n        acc1= accuracy(outputs, targets, topk=(1,))\n        losses.update(loss.item(), ids.size(0))\n        top1.update(acc1[0].item(), ids.size(0))\n        scheduler.step()\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % 50 == 0:\n            progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","faf35e2b":"def eval_loop_fn(validation_loader, model, device):\n    #Validation\n    model.eval()\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    learning_rate = AverageMeter('LR',':2.8f')\n    progress = ProgressMeter(\n        len(validation_loader),\n        [batch_time, losses, top1],\n        prefix='[xla:{}]Validation: '.format(xm.get_ordinal()))\n    with torch.no_grad():\n        end = time.time()\n        for i, data in enumerate(validation_loader):\n            ids, mask, type_ids, targets = data[\"input_ids\"], data[\"attention_mask\"], data['token_type_ids'], data[\"label\"]\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            type_ids = type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(\n                input_ids = ids,\n                attention_mask = mask,\n                token_type_ids = type_ids\n            )\n            loss = loss_fn(outputs, targets)\n            acc1= accuracy(outputs, targets, topk=(1,))\n            losses.update(loss.item(), ids.size(0))\n            top1.update(acc1[0].item(), ids.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if i % 50 == 0:\n                progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","7e5763ea":"WRAPPED_MODEL = xmp.MpModelWrapper(XLMRoberta(num_labels=3, multisample=False))\n\ndataset = train_dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset['train']\nvalid_dataset = dataset['test']\ntrain_dataset.set_format(\"torch\", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) \nvalid_dataset.set_format(\"torch\", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) ","d91e8035":"def _run():\n    xm.master_print('Starting Run ...')\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Train Loader Created.')\n    \n    valid_sampler = DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Valid Loader Created.')\n    \n    num_train_steps = int(len(train_dataset) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size())\n    device = xm.xla_device()\n    model = WRAPPED_MODEL.to(device)\n    xm.master_print('Done Model Loading.')\n    optimizer = get_model_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps = 0,\n        num_training_steps = num_train_steps * EPOCHS\n    )\n    xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n    \n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Training ...')\n        train_loop_fn(para_loader.per_device_loader(device),\n                      model,  \n                      optimizer, \n                      device, \n                      scheduler, \n                      epoch\n                     )\n        \n        xm.master_print(\"Finished training epoch {}\".format(epoch))\n            \n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Validating ...')\n        eval_loop_fn(para_loader.per_device_loader(device), \n                     model,  \n                     device\n                    )\n        \n        # Serialized and Memory Reduced Model Saving\n        if epoch == EPOCHS-1:\n            xm.master_print('Saving Model ..')\n            xm.save(model.state_dict(), \"model.bin\")\n            xm.master_print('Model Saved.')\n            \n    if METRICS_DEBUG:\n      xm.master_print(met.metrics_report(), flush=True)","342c0970":"def _mp_fn(rank, flags):\n    # torch.set_default_tensor_type('torch.FloatTensor')\n    _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","77df442b":"### Training","23c79264":"### Run","bbe39ec6":"### Data Files","77d1bba5":"### Model and Dataset Config","d7e5e41e":"### Metrics Factory","c287c4af":"### Dataset Factory","f1f282ed":"#### Helper Functions","acb1ff71":"### External Datasets\n\nHuggingFace [`nlp`](https:\/\/huggingface.co\/nlp\/index.html) library contains datasets and evaluation metrics for natural language processing.\nIt is compatible with NumPy, Pandas, PyTorch and TensorFlow. \n\nWe will be using three external datasets:\n\n1. [XNLI](https:\/\/huggingface.co\/nlp\/viewer\/?dataset=xnli) is a subset of a few thousand examples from MNLI which has been translated into a 14 different languages (some low-ish resource). \n2. [Glue](https:\/\/huggingface.co\/nlp\/viewer\/?dataset=glue&config=mnli) the General Language Understanding Evaluation benchmark (https:\/\/gluebenchmark.com\/) is a collection of resources for training, evaluating, and analyzing natural language understanding systems.\n3. [SNLI](https:\/\/huggingface.co\/nlp\/viewer\/?dataset=snli) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE).\n\n*Note: Three datasets combined reach more than 100k examples and hence will be difficult to train within the assigned resources. Since this is a multi-lingual based competition we will fetch **100% test, validation data** from XNLI, **25% train data** from Glue\/MNLI and **25% train data** from SNLI.*","ca040dd2":"#### Encoded Data Statistics","20dd9657":"#### Encode Datasets","c8567c6a":"### CONFIG","b9033956":"### Setup Dependencies","81594fcd":"### Loss Factory","349bc844":"#### Cocatenate and shuffle all datasets","4b253eac":"#### External Data Statistics","c3057670":"### Optimizer Factory","48145626":"### Check TPU is available","59ef603e":"#### Competitions Data - Convert & Encode ","af36721f":"### Evaluation","41ac4a5b":"### Model Factory"}}