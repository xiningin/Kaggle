{"cell_type":{"22a95f38":"code","a1633e87":"code","a413a8a4":"code","c3d36d90":"code","f85732e5":"code","9b29139f":"code","724f704d":"code","e58915ec":"code","fdfedef3":"code","d60a92f4":"code","730a3d7e":"code","cca3e244":"code","f8a584e4":"code","1321c61a":"code","3b098e04":"code","90f46009":"code","9cf566b8":"code","136de0fb":"code","24ae8bc0":"code","7f9435e3":"code","b2e00e95":"code","e009f98c":"code","aa0207ed":"code","bd89897b":"code","e2eb981c":"code","bba0fc87":"code","12d1d7fe":"code","e6bcd80f":"code","a16b7d89":"code","572499fd":"code","6e5ee58f":"code","d5941eb9":"code","e05eeeca":"code","4b7c4414":"code","c7e68709":"code","9106c7b7":"code","031c2fc6":"code","16b100e5":"code","8eff32b1":"code","c372bf28":"code","c356724a":"code","b9270be6":"code","8e55ac42":"code","75ef7004":"code","f8e3257c":"code","7aa2032f":"code","49cd3766":"code","ae310578":"code","54e5ef3f":"code","d249a2b1":"code","96a9ced3":"code","7c4f9ea6":"code","37f358fe":"code","ba49ceff":"code","7e647111":"code","9f855830":"code","a4597d98":"code","d6906e70":"code","476e37c7":"code","d8c6a947":"code","e0fa53be":"code","f32a3b1b":"code","207fad10":"code","a9d99f06":"code","8cd23675":"code","b62dcb51":"code","848ceb3e":"code","063c2aee":"code","f1bdaaaa":"code","cd5c9138":"code","7527e71c":"code","1ed919e3":"code","9c1ab0fe":"code","0166cdf9":"code","ad6f29af":"code","eab0f753":"code","30b52527":"code","6a41d8c7":"code","1f5383a1":"code","ffb2efe2":"code","aa06fe0a":"code","93015836":"code","9352cc7c":"code","3645980f":"code","e28e1e1d":"code","d6cfe7ea":"code","bf689c1e":"code","ff32ae93":"code","4209910c":"code","8d4bbb4a":"code","99b9035b":"code","03eb6737":"code","e9616090":"code","23204257":"code","2c142f09":"code","5ce45ced":"code","7debbf98":"code","8067f3e7":"code","c596265f":"code","e60da410":"code","94e6f5cb":"code","b2981dbc":"code","1f732499":"code","b5073a8c":"code","a2b94dc9":"code","725317dd":"code","214a01eb":"code","a80e32a5":"code","029e1048":"code","f595e6c9":"code","2bd9d426":"code","209011ce":"code","fc95dcd9":"code","8622c6fa":"code","237ae84f":"code","d89cfd94":"code","611e929b":"code","78585766":"code","26233d9a":"code","81a5aac1":"code","7340a197":"code","3587fdc8":"code","a5dadd0c":"code","32e1d74f":"code","6d607796":"code","6932c012":"code","ee1a8dbb":"code","3153238f":"code","8e30ec35":"code","40476d67":"code","866ecf51":"code","e7cbe28a":"code","de2c0946":"code","de193577":"code","02972e7a":"markdown","2da35eef":"markdown","38e32921":"markdown","6c92270f":"markdown","da3adc62":"markdown","35e41df5":"markdown","40e7c143":"markdown","ade3ac86":"markdown","e33e05af":"markdown","bfe95b56":"markdown","e6e04fb9":"markdown","2b452ead":"markdown","a4700fb3":"markdown","97fb73c6":"markdown","e7c5487a":"markdown","072c607c":"markdown","830ffbcb":"markdown","dc8415a1":"markdown","d3449354":"markdown","9f040549":"markdown","f254e5e7":"markdown","c6ca2511":"markdown","1e1493fa":"markdown","d678b47f":"markdown","ffda34c9":"markdown","714e5d5f":"markdown","658377ea":"markdown","7600a4cb":"markdown","1f6fe741":"markdown","bf5f0fc9":"markdown","0f016cdd":"markdown","e4a43d83":"markdown","4c5311e0":"markdown","5355eef0":"markdown","f4dabfab":"markdown","5c23ad4a":"markdown","d2d3d37c":"markdown","da49b99f":"markdown","56e0a7f3":"markdown","e80d670d":"markdown","b9fcafd2":"markdown","7f0b4235":"markdown","c02f1059":"markdown","3edaa2b0":"markdown","fb143690":"markdown","fe067ab3":"markdown","86a16316":"markdown","fb62cb52":"markdown","d10c45ca":"markdown","31ead5af":"markdown","b1b17c55":"markdown","248e780f":"markdown","a5af24e2":"markdown","c1b436ab":"markdown","493ce81c":"markdown"},"source":{"22a95f38":"#Importing stuff that I will need in this notebook\n\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nimport seaborn as sns \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport matplotlib\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom geopy.distance import geodesic, vincenty\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom itertools import product\nfrom sklearn.metrics import make_scorer,  r2_score\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, BayesianRidge, Lasso\nfrom scipy import stats\nfrom sklearn.pipeline import make_pipeline\nfrom geopy.geocoders import Nominatim\nfrom sklearn import feature_extraction, model_selection, metrics\nfrom sklearn.metrics import accuracy_score, classification_report, mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","a1633e87":"import warnings\nwarnings.filterwarnings('ignore')","a413a8a4":"train = pd.read_csv( \"..\/input\/califdata\/train.csv\")\ntestTOsubmit = pd.read_csv(\"..\/input\/califdata\/test.csv\" )","c3d36d90":"print (\"train size: \", train.shape)\nprint (\"test size: \", testTOsubmit.shape)","f85732e5":"train.head()","9b29139f":"train.info()","724f704d":"train.isnull().sum()","e58915ec":"testTOsubmit.info()","fdfedef3":"testTOsubmit.isnull().sum()","d60a92f4":"train.describe()","730a3d7e":"def grafico(col):\n    plt.figure(figsize=(16,5))\n    plt.subplot(121)\n    sns.distplot(col);\n\n    plt.subplot(122)\n    sns.boxplot(col);","cca3e244":"# Plot for target variable\n\ngrafico(train['median_house_value'])","f8a584e4":"# Let's check out the numerical variables at one place in a grid like plot\n\ndf = train.dropna()","1321c61a":"# Select only numerical \"independent\" columns\n\nnumerical_columns = df.drop(['Id', 'median_house_value', 'longitude', 'latitude'], axis=1).columns\n\nplt.figure(figsize=(27,12))\nfor k,v in enumerate(numerical_columns):\n    plt.subplot(2,4,k+1)\n    sns.distplot(df[v])\n    plt.tight_layout();","3b098e04":"grafico(train['median_age']);","90f46009":"grafico(train['total_rooms']);","9cf566b8":"grafico(train['total_bedrooms']);","136de0fb":"grafico(train['population']);","24ae8bc0":"grafico(train['households']);","7f9435e3":"grafico(train['median_income']);","b2e00e95":"corr_matrix = train.corr()\nprint(corr_matrix)","e009f98c":"# And, here, more information about the target related to the others columns\n\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","aa0207ed":"# Ploting the correlation between two columns\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"median_age\"]\nscatter_matrix(train[attributes], figsize=(18, 12))","bd89897b":"train.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.2, figsize=(10,10))","e2eb981c":"plt.figure(figsize=(15,5))\nplt.subplots_adjust(hspace = .25)\nplt.subplot(1,2,1)\nplt.title('Corelation between longtitude and median_house_value')\nplt.xlabel('longitude',fontsize=12)\nplt.ylabel('median_house_value',fontsize=12)\nplt.scatter(train['longitude'].head(100),train['median_house_value'].head(100),color='g')\nplt.subplot(1,2,2)\nplt.title('Corelation between latitude and median_house_value')\nplt.xlabel('latitude',fontsize=12)\nplt.ylabel('median_house_value',fontsize=12)\nplt.scatter(train['latitude'].head(100),train['median_house_value'].head(100),color='r')","bba0fc87":"sns.distplot(train['median_house_value'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['median_house_value'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Median House Value Distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['median_house_value'], plot=plt)\nplt.show()","12d1d7fe":"testTOsubmit['median_house_value'] = np.nan","e6bcd80f":"data = train.append(testTOsubmit, ignore_index = True)","a16b7d89":"data.info()","572499fd":"data[\"rooms_per_household\"] = data[\"total_rooms\"]\/data[\"households\"]","6e5ee58f":"data[\"bedrooms_per_room\"] = data[\"total_bedrooms\"]\/data[\"total_rooms\"]","d5941eb9":"data[\"population_per_household\"]= data[\"population\"]\/data[\"households\"]","e05eeeca":"data[\"population_per_room\"]= data[\"population\"]\/data[\"total_rooms\"]","4b7c4414":"# Reading these data files\n\n# A list of the latitude and longitude of cities in California\ncity_lat_long = pd.read_csv( '..\/input\/califdata\/cal_cities_lat_long.csv')\n\n# Historical population data for cities in California (including the year 1990, which is the year the original housing price data)\ncity_pop_data = pd.read_csv( '..\/input\/califdata\/cal_populations_city.csv')\n\n# Historical population data for countries in California (including the year 1990, which is the year the original housing price data)\ncounty_pop_data = pd.read_csv( '..\/input\/califdata\/cal_populations_county.csv')","c7e68709":"city_coords = {}\nfor dado in city_lat_long.iterrows():\n    row = dado[1]\n    if row['Name'] not in city_pop_data['City'].values:   \n        continue           \n    else: \n        city_coords[row['Name']] = (float(row['Latitude']), float(row['Longitude']))","9106c7b7":"def closest_point(location, location_dict):\n    closest_location = None\n    for city in location_dict.keys():\n        distance = vincenty(location, location_dict[city]).kilometers\n        if closest_location is None:\n            closest_location = (city, distance)\n        elif distance < closest_location[1]:\n            closest_location = (city, distance)\n    return closest_location","031c2fc6":"# Let's use the April 1990 population data below, because the California Housing was collected at this time\ncity_pop_dict = {}\nfor dado in city_pop_data.iterrows():\n    row = dado[1]\n    city_pop_dict[row['City']] =  row['pop_april_1990']\n\n    \n# For big cities, we will separate in an another coordinate dictonary\nbig_cities = {}\nfor key, value in city_coords.items():\n    if city_pop_dict[key] > 500000:\n        big_cities[key] = value","16b100e5":"# Adding the data relating to the points to the closest city\n\ndata['close_city'] = data.apply(lambda x: closest_point((x['latitude'],x['longitude']),city_coords), axis = 1)\ndata['close_city_name'] = [x[0] for x in data['close_city'].values]\ndata['close_city_dist'] = [x[1] for x in data['close_city'].values]\ndata['close_city_pop'] = [city_pop_dict[x] for x in data['close_city_name'].values]","8eff32b1":"data = data.drop('close_city', axis=1)","c372bf28":"# Adding the data relating to the points to the closest big city\n\ndata['big_city'] = data.apply(lambda x: closest_point((x['latitude'],x['longitude']),big_cities), axis = 1)\ndata['big_city_name'] = [x[0] for x in data['big_city'].values]\ndata['big_city_dist'] = [x[1] for x in data['big_city'].values]","c356724a":"data = data.drop('big_city', axis=1)","b9270be6":"# Importing geographical image of the State of California\ncalifornia_img = Image(url= \"https:\/\/ibb.co\/eOSUtL\")\nhousing_plot = data[['longitude','population','latitude',\n                      'close_city_name','big_city_name','big_city_dist','median_house_value']]\n\n#Ploting the map\nhousing_plot.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n                  s=housing_plot['population']\/100, label='population', figsize=(10,7),\n                  c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\n\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.legend()\nplt.show()","8e55ac42":"# Graph of vectors connecting points to their nearest city\n\ncity_lat_long.plot(kind='scatter', x='Longitude', y='Latitude',  alpha=0.4,\n                   s=housing_plot['population']\/100, label='population', figsize=(10,7))\n\nfor line in data.iterrows():\n    dat = line[1]\n    x1 = dat['longitude']\n    y1 = dat['latitude']\n    p2 = city_coords[dat['close_city_name']]\n    x2 = p2[1]\n    y2 = p2[0]\n    plt.plot([x1,x2],[y1, y2], 'k-',linewidth=0.1)\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.show()","75ef7004":"# Ploting map of the vectors connecting districts to the nearest major city\n\ncity_lat_long.plot(kind='scatter', x='Longitude', y='Latitude',  alpha=0.4,\n                   s=housing_plot['population']\/100, label='population', figsize=(10,7))\n\nfor line in data.iterrows():\n    dat = line[1]\n    x1 = dat['longitude']\n    y1 = dat['latitude']\n    p2 = big_cities[dat['big_city_name']]\n    x2 = p2[1]\n    y2 = p2[0]\n    plt.plot([x1,x2],[y1, y2], 'k-',linewidth=0.1)\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.show()","f8e3257c":"# Plot a correlation matrix\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","7aa2032f":"encoder = LabelEncoder()","49cd3766":"# Encoder the ocean close city name\ndata['close_city_name'] = encoder.fit_transform(data['close_city_name'])\n\n# Encoder the ocean big city name\ndata['big_city_name'] = encoder.fit_transform(data['big_city_name'])","ae310578":"#log transform skewed numeric features\n\nskewed = data.apply(lambda x: skew(x.dropna())) #compute skewness\nskewed = skewed[skewed > 0.75]\nskewed = skewed.index\n\ndata[skewed] = np.log1p(data[skewed])","54e5ef3f":"data.head(10)","d249a2b1":"data.describe()","96a9ced3":"scaler = MinMaxScaler()","7c4f9ea6":"# Here, we will separate the 'Id' and 'median_house_value' to preservate its real number and apply the feature scaling\n\ndataID = data['Id']\ndataMEDIAN = data['median_house_value']\ndata_scaled = data.apply(lambda x:(x.astype(float) - min(x))\/(max(x)-min(x)), axis = 0)\ndata_scaled.head(10)\ndata = data_scaled\ndata['Id2'] = dataID \ndata['median_house_value2'] = dataMEDIAN ","37f358fe":"data.head()","ba49ceff":"data.head()","7e647111":"data.shape","9f855830":"#Separate what is test to submit of the rest\n\nSPLIT = data[data['median_house_value'].notnull()]\ntestTOsubmit = data[data['median_house_value'].isnull()]","a4597d98":"#Split the train and test to measure the quality of each type of regression\n\ntrain, test = train_test_split(SPLIT, test_size=0.2)","d6906e70":"trainID = train['Id2']\ntrainMEDIAN = train['median_house_value2']\ntestID = test['Id2']\ntestMEDIAN = test['median_house_value2']\ntestTOsubmitID = testTOsubmit['Id2']\ntestTOsubmitMEDIAN = testTOsubmit['median_house_value2']","476e37c7":"train = train.drop('Id2', axis=1)\ntrain = train.drop('median_house_value2', axis=1)\ntest = test.drop('Id2', axis=1)\ntest = test.drop('median_house_value2', axis=1)\ntestTOsubmit = testTOsubmit.drop('Id2', axis=1)\ntestTOsubmit = testTOsubmit.drop('median_house_value2', axis=1)","d8c6a947":"train","e0fa53be":"test","f32a3b1b":"testTOsubmit","207fad10":"print (\"train size: \", train.shape)\nprint (\"test size: \", test.shape)\nprint (\"test to submit size: \", testTOsubmit.shape)","a9d99f06":"Ytrain = train['median_house_value']\nXtrain = train.drop('median_house_value', axis=1)\nYtest = test['median_house_value']\nXtest = test.drop('median_house_value', axis=1)","8cd23675":"Xtrain.shape,Xtest.shape,Ytrain.shape,Ytest.shape","b62dcb51":"# Defining the function of cross vaidation for test and train\n\nn_folds = 5\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,Xtrain,Ytrain,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,Xtest,Ytest,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)","848ceb3e":"# Getting the predictions to Linear Regression\n\nlr = LinearRegression()\nlr.fit(Xtrain, Ytrain)\ntest_pre = lr.predict(Xtest)\ntrain_pre = lr.predict(Xtrain)","063c2aee":"# Calculating the Linear Regression RMSE\n\nprint(\"Linear Regression RMSE on Training set :\", rmse_CV_train(lr).mean())\nprint(\"Linear Regression RMSE on Test set :\", rmse_CV_test(lr).mean())","f1bdaaaa":"score = cross_val_score(lr,Xtrain, Ytrain, cv = 5)\nprint('Score is: '+ str(np.mean(score)))","cd5c9138":"# Plot between predicted values and residuals\n\nplt.scatter(train_pre, train_pre - Ytrain, c = \"red\",  label = \"Training data\")\nplt.scatter(test_pre,test_pre - Ytest, c = \"green\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.plot([0, 1.1], [0, 0], c = \"black\")\n\nplt.show()","7527e71c":"# Plot predictions (real values)\n\nplt.scatter(train_pre, Ytrain, c = \"red\",  label = \"Training data\")\nplt.scatter(test_pre, Ytest, c = \"green\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([0.1, 0.95], [0, 1], c = \"black\")\nplt.show()","1ed919e3":"# Defining list of alphas\n\nalphas = [0.00025, 0.0003, 0.0005, 0.0007, 0.001, 0.002, 0.0025, 0.003, 0.004, 0.005, 0.006,\n          0.007, 0.008, 0.009, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.5, 2]","9c1ab0fe":"# Setting alphas and fitting with ridge\n\nridge = RidgeCV(alphas = alphas)\nridge.fit(Xtrain,Ytrain)","0166cdf9":"alpha = ridge.alpha_\nprint('The best alpha found was: ',alpha)","ad6f29af":"print(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],cv = 5)\nridge.fit(Xtrain, Ytrain)\nalpha = ridge.alpha_\nprint(\"Best alpha : \", alpha)","eab0f753":"model_ridge = Ridge()\ncv_ridge = [rmse_CV_train(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.xlim((0.00225, 0.0026))\nplt.ylim((0.0853, 0.0855))","30b52527":"# Calculating the Ridge RMSE\n\nprint(\"Ridge RMSE on Training set :\", rmse_CV_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_CV_test(ridge).mean())","6a41d8c7":"score = cross_val_score(ridge,Xtrain, Ytrain, cv = 5)\nprint('Score is: '+ str(np.mean(score)))","1f5383a1":"Ytrain_ridge = ridge.predict(Xtrain)\nYtest_ridge = ridge.predict(Xtest)","ffb2efe2":"# Plot between predicted values and residuals\n\nplt.scatter(Ytrain_ridge, Ytrain_ridge -Ytrain, c = \"red\",  label = \"Training data\")\nplt.scatter(Ytest_ridge, Ytest_ridge -Ytest, c = \"green\", marker = \"v\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.plot([0, 1.2], [-0.6, 0.6], c = \"black\")\nplt.show()","aa06fe0a":"# Plot predictions - Real values\n\nplt.scatter(Ytrain_ridge, Ytrain, c = \"red\",  label = \"Training data\")\nplt.scatter(Ytest_ridge, Ytest, c = \"green\",  label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\") \nplt.plot([0.27, 0.93], [0.2, 1], c = \"black\")\nplt.show()","93015836":"# Defining another list of alphas\n\nalphas2 = [0.025, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2, 0.25, 0.3, 1, 1.25, 1.5, 1.75, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 5,\n           10, 13, 15, 17, 20, 23, 25, 30, 35, 40, 50]","9352cc7c":"# Setting alphas and fitting with lasso\n\nlasso = LassoCV(alphas = alphas2).fit(Xtrain, Ytrain)","3645980f":"alpha2 = lasso.alpha_\nprint('The best alpha found was: ',alpha2)","e28e1e1d":"print(\"Try again for more precision with alphas centered around \" + str(alpha2))\nlasso = LassoCV(alphas = [alpha2 * .6, alpha2 * .65, alpha2 * .7, alpha2 * .75, alpha2 * .8, alpha2 * .85, \n                          alpha2 * .9, alpha2 * .95, alpha2, alpha2 * 1.05, alpha2 * 1.1, alpha2 * 1.15,\n                          alpha2 * 1.25, alpha2 * 1.3, alpha2 * 1.35, alpha2 * 1.4],cv = 5)\nlasso.fit(Xtrain, Ytrain)\nalpha2 = lasso.alpha_\nprint(\"Best alpha : \", alpha2)","d6cfe7ea":"# Calculating the Ridge RMSE\n\nprint(\"Lasso RMSE on Training set :\", rmse_CV_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_CV_test(lasso).mean())","bf689c1e":"#Setting Alphas and L1-ratio\n\nalphas = [0.0005, 0.001, 0.01, 0.03, 0.05, 0.1]\nl1_ratios = [1.5, 1.1, 1, 0.9, 0.8, 0.7, 0.5]\nENet = ElasticNet()","ff32ae93":"# Defining the CV Eastic Net\n\ncv_elastic = [rmse_CV_train(ElasticNet(alpha = alpha, l1_ratio=l1_ratio)).mean() \n            for (alpha, l1_ratio) in product(alphas, l1_ratios)]","4209910c":"# Plot the relation between the alphas, l1 and the RMSE\n\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nidx = list(product(alphas, l1_ratios))\np_cv_elastic = pd.Series(cv_elastic, index = idx)\np_cv_elastic.plot(title = \"Validation\")\nplt.xlabel(\"Alpha - L1_ratio\")\nplt.ylabel(\"RMSE\")","8d4bbb4a":"# Let's zoom in to the first 10 parameter pairs\n\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nidx = list(product(alphas, l1_ratios))[:10]\np_cv_elastic = pd.Series(cv_elastic[:10], index = idx)\np_cv_elastic.plot(title = \"Validation\")\nplt.xlabel(\"Alpha - L1_ratio\")\nplt.ylabel(\"RMSE\")","99b9035b":"score = rmse_CV_train(ENet)\nprint(\"Elastic Net RMSE on Train set: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","03eb6737":"elastic = ElasticNet(alpha=0.0005, l1_ratio=0.9)\nelastic.fit(Xtrain, Ytrain)\nElasticNet(alpha=0.0005, copy_X=True, fit_intercept=True, l1_ratio=0.9,\n      max_iter=1000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)","e9616090":"# Let's look at the residuals\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":elastic.predict(Xtrain), \"true\":Ytrain})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","23204257":"score = cross_val_score(elastic,Xtrain, Ytrain, cv = 5)\nprint('Score is: '+ str(np.mean(score)))","2c142f09":"print('R^2 train: %.3f' %  r2_score(preds['true'], preds['preds']))","5ce45ced":"coef = pd.Series(elastic.coef_, index = Xtrain.columns)\nprint(\"Elastic Net picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","7debbf98":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])","8067f3e7":"matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Elastic Net Model\")","c596265f":"RF = RandomForestRegressor(max_depth=30, n_estimators=500, max_features = 15, oob_score=True, random_state=1234)\nscore = cross_val_score(RF,Xtrain, Ytrain, cv = 5, n_jobs = -1)\nprint(\"Random Forest RMSE on Training set :\", rmse_CV_train(RF).mean())\nprint(\"Random Forest RMSE on Test set :\", rmse_CV_test(RF).mean())\nprint('Score is: '+ str(np.mean(score)))","e60da410":"# Setting the K numbers\n\nneighbors = [3,5,7,9,11,13,15,20,25,30]","94e6f5cb":"# Here, we define a function that test (with CV equal to 3) various K-nn and yours misclassification error\n\nprint('With CV = 3')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=3)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","b2981dbc":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 3)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","1f732499":"# Here, we define a function that test (with CV equal to 5) various K-nn and yours misclassification error\n\nprint('With CV = 5')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=5)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","b5073a8c":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 5)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","a2b94dc9":"# Here, we define a function that test (with CV equal to 7) various K-nn and yours misclassification error\n\nprint('With CV = 7')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=7)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","725317dd":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 7)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","214a01eb":"# Here, we define a function that test (with CV equal to 9) various K-nn and yours misclassification error\n\nprint('With CV = 9')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=9)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","a80e32a5":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 9)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","029e1048":"# Here, we define a function that test (with CV equal to 11) various K-nn and yours misclassification error\n\nprint('With CV = 11')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=11)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","f595e6c9":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 11)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","2bd9d426":"# Here, we define a function that test (with CV equal to 13) various K-nn and yours misclassification error\n\nprint('With CV = 13')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=13)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","209011ce":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 13)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","fc95dcd9":"# Here, we define a function that test (with CV equal to 15) various K-nn and yours misclassification error\n\nprint('With CV = 15')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=15)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","8622c6fa":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 15)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","237ae84f":"# Here, we define a function that test (with CV equal to 20) various K-nn and yours misclassification error\n\nprint('With CV = 20')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=20)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","d89cfd94":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 20)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","611e929b":"# Here, we define a function that test (with CV equal to 25) various K-nn and yours misclassification error\n\nprint('With CV = 25')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=25)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","78585766":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 25)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","26233d9a":"# Here, we define a function that test (with CV equal to 30) various K-nn and yours misclassification error\n\nprint('With CV = 30')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=k,weights='uniform')\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=30)\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","81a5aac1":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 30)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","7340a197":"# Set the K-nn with K=7 and CV=20\n\nknn = KNeighborsRegressor(n_neighbors=9)\nknn.fit(Xtrain,Ytrain)\nscores = cross_val_score(knn, Xtrain, Ytrain, cv=20).mean()\nprint(scores)","3587fdc8":"# Open the file that contains a sample how to send ours predictions\n\nsample = pd.read_csv('..\/input\/atividade-3-pmr3508\/sample_sub_1.csv')\nprint (\"submission size: \", sample.shape)","a5dadd0c":"sample.head()","32e1d74f":"Ytrain['median_house_value'] = trainMEDIAN\nYtest['median_house_value'] = testMEDIAN","6d607796":"Y = Ytrain + Ytest","6932c012":"Xtrain['Id'] = trainID\nXtest['Id'] = testID","ee1a8dbb":"X = Xtrain + Xtest","3153238f":"testTOsubmit['Id'] = testTOsubmitID","8e30ec35":"# Let's fit with thw Random Forest\n\nRF.fit(X,Y)","40476d67":"# Drop the target column with NaN values\n\ntestTOsubmit = testTOsubmit.dropna(axis=1, how='all')","866ecf51":"predictionsTOsubmit = RF.predict(testTOsubmit)","e7cbe28a":"# Sending the predictions\n\nstr(predictionsTOsubmit)\nids = testTOsubmitID\nsubmission = pd.DataFrame({'Id':ids,'median_house_value':predictionsTOsubmit[:]})\nsubmission.to_csv(\"predictionsT3.csv\", index = False)","de2c0946":"submission.shape","de193577":"submission","02972e7a":"## 10. Random Forest Regressor\n - Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.","2da35eef":" - As we can see above, in some specific latitudes and longitudes, the median house value can be very diverse (these are, probably, big cities or populous places).","38e32921":"## References\n\n\n - [Cross Validation Score](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html)\n - [Confusion Matrix](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html)\n - [K-Nearest Neighbors](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsRegressor.html)\n - [Naive Bayes](http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html)\n - [Pandas](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/)\n - [ROC Curve](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html)\n - [Linear Regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html)\n - [Geopy](https:\/\/github.com\/geopy\/geopy)\n - [Geospatial Feature Engineering and Visualization](https:\/\/www.kaggle.com\/camnugent\/geospatial-feature-engineering-and-visualization)\n - [Lasso Regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html)\n - [Split Data](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.drop.html)\n - [Ridge Regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html)\n - [Wikipedia](https:\/\/pt.wikipedia.org\/wiki\/Wikip\u00e9dia:P\u00e1gina_principal)\n - [Random Forest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","6c92270f":"- We find in the kaggle the same database with an additional feature (called ocean_proximity), so, let's use it! For integrate this feature in our database, we will merge by the location of each house","da3adc62":"## 3. Future Engeneering\n\n Here, we will use the domain knowledge of the data to create features that make machine learning algorithms work. Our intent is to increase the predictive power of machine learning algorithms by creating features from raw data that help facilitate the machine learning process.","35e41df5":" - Then, let's try some numbers to the K and CV and find the best performance","40e7c143":" - Let's join the 'train' and 'test' for the future adjustments be made in both (then we can separate).","ade3ac86":" - Here, let's plot a correlation matrix to see the correlation coefficients between variables. Each cell in the table shows the correlation between two variables.","e33e05af":" - As we can see above, the features are related to an especif house and our target is the house price. Also, below, we can see that the database does not have missing data (which is good for a better analysis).","bfe95b56":" - Here, we will determine the distance between two sets of coordinates and, then, try all pairwise comparisons and figure out which city is the closest.","e6e04fb9":"## Table of Contents\n\n- [Introduction](#introduction)\n- [Initial Data Exploration](#initial)\n- [Future Engeneering](#future)\n- [Deeper Data Exploration](#deeper)\n- [Adjustments before starting the regression tests](#adjustments)\n- [Linear Regression](#classify0)\n- [Ridge Regression](#classify1)\n- [Lasso Regression](#classify2)\n- [Elastic Net Regression](#classify3)\n- [Random Forest Regressor](#classify4)\n- [KNN Regression](#classify5)\n- [Conclusions](#conclusions)\n- [References](#references) ","2b452ead":"### Read Data File\n\nThe first task is to read data from .csv file. Most of the features are frequency of some especif word.","a4700fb3":"### Split Data","97fb73c6":"## 12. Conclusion\n\n - After we analyze all the data and look at various methods of classification, we concluded that we will predict our \"testTOsubmit\" with the Random Forest Regressor.","e7c5487a":" - Firstly, we will explore the data and your relation with the target","072c607c":" - Here, we will attribute some variables","830ffbcb":" ### Geoespatial Feature Engeneering \n \n - In this subsection, we use other datas that are useful to complement and understand the locality of the californian houses and their relationship with the prices","dc8415a1":"## 6. Linear Regression Model (without Regularization)\n - In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables).","d3449354":" - Typically, ML algorithms don't perform well when the input numerial attributes have very different scales. So, let's see if the dataset has very different scales between its features.","9f040549":"## 7. Ridge Regression\n - Now, we will use a regression called Ridge. It is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution.","f254e5e7":"## 1. Introduction\n\nIn this kernel, we will build a model that predicts the median price of a house in a California region.\nThe goal of this task is that we test all regression models known to us and choose the most suitable for this task.","c6ca2511":" - As we can see, the higher frequency of house prices is between 10000 and 20000 and this is a Skewed Dataset (with a positive skew).","1e1493fa":"## 4. Deeper data exploration\n\n In this moment, we will continue exploring the data with the feature engeneering adjusted.\n \n( OBS.: N\u00e3o sei o que ocorreu, por\u00e9m, n\u00e3o consigo colocar imagens no Kaggle, esse erro s\u00f3 ocorre dentro da pataforma...)","d678b47f":"### Data Visualization","ffda34c9":"## 11. KNN (k-nearest neighbors)\n - For now, let's try the k-nearest neighbors algorithm (k-NN) in regression. This output is the property value for the object. This value is the average of the values of its k nearest neighbors.","714e5d5f":"## 2. Initial data exploration\n\n In this section, we will explore the data in order to understand what each of them means in the database and how they relate to the target variable.","658377ea":" - Most ML algorithms work with numbers better. Therefore, we will convert text attributes into numerical attributes.","7600a4cb":" - Here we will adjust the 'Id' and 'median_house_value' to the predictions","1f6fe741":" - As we can see above, there are some \"horizontal lines\" in the plot, one clear one at  500,000, other at 450,000 and another one at 350,000, and a few other ones. If we cannot figure out the reason, removing those data points might be a good idea before feeding the data to the algorithms.","bf5f0fc9":" - Let's plot them individually and check the presence of ouliers with the help of boxplots","0f016cdd":" - In this section, we will create new features with the relation of some of them","e4a43d83":"## 8. LASSO Regression\n - In this section we will use an especific algorithm called LASSO. It is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.","4c5311e0":" - Just confirming that everything is right!","5355eef0":" - We can conclude that our best perfomace with K-NN was with CV equal to 20 and the optimal number of neighbors is 9. So, let's use it and take more useful info of your classification.","f4dabfab":"## 9. Elastic Net\n- In the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the lasso and ridge methods. ","5c23ad4a":" - Now, let's predict in our test file and send the results","d2d3d37c":" - Let's get the correlation between two columns","da49b99f":" - Just analyzing better the relationship between median income and median house value (let's plot this graph bigger)","56e0a7f3":" - Here, let's get a deeply analysis between the relationship of coordinates and the median house value","e80d670d":" - As we can se above, 'median_income' is very related to the 'median_house_value'. It is easy to understand this high relationship between the two variables, because the more money the person has, the better the home and standard of living he will want to have. Therefore, more expensive will be the value and his home.","b9fcafd2":" - Now, we will plot a map that provides a visualization of three of the features added to the data. The black lines represent the vectors connecting the districts to the nearest town. Based on these lines, the distance to the nearest town (length of black line), the population of the nearest town (point the black line arrives at) and a categorical for the name of the nearest town were assigned to the district.These features help to capture the general trend of houses getting cheaper the further away from downtown one gets (a fairly universal pattern... with plenty of exceptions).","7f0b4235":" - Ploting in the map of California our location and population of housing districts, along with a heatmap to show where homes are the most expensive","c02f1059":"### Feature Scaling","3edaa2b0":" - As we can see above, in this housing dataset, you can see, for example, 'median_income' ranges from 4999\n to 150001 while 'total_rooms' is between 2 and 39320 (note that scaling the target values is typically not required). Let's do the feature scaling now!","fb143690":" - Now, let's iterates through the coordinates dataframe and builds a dictonary with the city names as the keys and a tuple of the corresponding (latitude, longitude) as the values.","fe067ab3":" - Now, let's see the median house value distribution.","86a16316":" - Now, we will analyse the houses and their distances to the big cities (San Francisco, San Jose, San Diego or Los Angeles). We used a cutoff for 'big city' of 500,000 people (that is quite big to 1990). I decided to add these features in because the proximity to a large city is somthing I intuitively thought would impact a house price (this definitely separates which are cosmopolitan houses or not).","fb62cb52":"### Enconde text attributes","d10c45ca":"### Environment Setting\n\nBefore we start we need set up the environment, please make sure those packages are installed in your computer when you copy this code to the local file.","31ead5af":"## 5. Adjustments before starting the regression tests","b1b17c55":" - As we can see above, everything has occurred correctly and we are with our features between 0 and 1","248e780f":" - As we can see above, this clustering also has an interesting side effect of effectively splitting California into quarters. So having the categorical San Francisco as the closest big city is roughly equivalent to being from the northern 1\/4 of the state, while the San Diego category is roughly equivalent to the bottom 1\/4 of California. So this feature may be adding some extra structure to the data in ways I had not initially intended.","a5af24e2":"### log Transformation\n - The best way to fix the Skewed Dataset is to perform a log transform of the same data, with the intent to reduce the skewness. After taking logarithm of the same data the curve seems to be normally distributed.","c1b436ab":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n**Tarefa 3 - Regress\u00e3o**\n\nHASH: f20977e697\n","493ce81c":"- Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit."}}