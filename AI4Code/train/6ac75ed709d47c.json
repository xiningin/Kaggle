{"cell_type":{"e12ac97f":"code","7b44c766":"code","10ad207d":"code","a78abb28":"code","cd6dbe94":"code","a3c617c0":"code","3f6bb5c0":"code","417c8337":"code","91ba6312":"code","06c30843":"code","4e4c780b":"code","06a0b3f8":"code","334516fe":"code","7b547703":"code","ec438c0f":"code","49b99116":"code","0d2a38b1":"code","cf012850":"code","0e3de96e":"code","61c30df8":"code","3b01e329":"code","e7688ff9":"code","ae11b402":"code","66f54e93":"markdown","cb251bfb":"markdown","5d23fba9":"markdown","675ae629":"markdown","4e9f4cb6":"markdown","1dd06ee0":"markdown","501a0523":"markdown","c22f1bab":"markdown","3c617d9f":"markdown","0a3be0f8":"markdown","96596b86":"markdown","26a50125":"markdown","02ee8dcc":"markdown","96cbc5a3":"markdown","a1839d24":"markdown","c38fc502":"markdown"},"source":{"e12ac97f":"import keras as k\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Dense\nfrom keras.layers import SpatialDropout1D\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom sklearn.model_selection import train_test_split\nimport re\nimport io\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","7b44c766":"df = pd.DataFrame(columns = ['review', 'estimation'])","10ad207d":"path = \"..\/input\/movie-review\/txt_sentoken\/txt_sentoken\"\npos_reviews = os.listdir(path + '\/pos\/')\nfor i in range(len(pos_reviews)):\n    with io.open(path+'\/pos\/'+pos_reviews[i], \"r\") as f:\n        text = f.read().lower()\n        df = df.append({'review':text, 'estimation': 1}, ignore_index=True)\n        \nneg_reviews = os.listdir(path + '\/neg\/')\nfor i in range(len(pos_reviews)):\n    with io.open(path+'\/neg\/'+neg_reviews[i], \"r\") as f:\n        text = f.read().lower()\n        df = df.append({'review':text, 'estimation': 0}, ignore_index=True)","a78abb28":"df['review'] = df['review'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))   ","cd6dbe94":"tokenizer = k.preprocessing.text.Tokenizer(num_words=40000, split=' ')\ntokenizer.fit_on_texts(df['review'].values)\nX = tokenizer.texts_to_sequences(df['review'].values)\nX = k.preprocessing.sequence.pad_sequences(X)\nsequence_dict = tokenizer.word_index;","a3c617c0":"Y = df['estimation'].values","3f6bb5c0":"output_dim = 30\nlstm_units = 30\ndropoutLSTM = 0.5\nbatch_size = 128\nepochs = 30\noptimizer = k.optimizers.Adam(lr=0.01, decay=0.01)","417c8337":"model = Sequential()\nmodel.add(Embedding(40000, output_dim = output_dim, input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(LSTM(lstm_units, recurrent_dropout = dropoutLSTM))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])","91ba6312":"history = model.fit(X, Y, validation_split=0.1, batch_size = batch_size, epochs = epochs)","06c30843":"model.save('model.h5')","4e4c780b":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","06a0b3f8":"embeddings_index = dict();\nwith open('..\/input\/glove6b\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;","334516fe":"vocab_size = len(sequence_dict);\nembeddings_matrix = np.zeros((vocab_size+1, 100));\nfor word, i in sequence_dict.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","7b547703":"lstm_units = 30\ndropoutLSTM = 0.5\nbatch_size = 64\nepochs = 30\noptimizer = k.optimizers.Adam(lr=0.01, decay=0.01)","ec438c0f":"model = Sequential()\nmodel.add(Embedding(embeddings_matrix.shape[0], output_dim = 100, input_length = X.shape[1], weights=[embeddings_matrix], trainable=False))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(LSTM(lstm_units, recurrent_dropout = dropoutLSTM))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])","49b99116":"history = model.fit(X, Y, validation_split=0.1, batch_size = batch_size, epochs = epochs)","0d2a38b1":"model.save('model_GloVe.h5')","cf012850":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","0e3de96e":"lstm_units1 = 150\nlstm_units2 = 100\ndropoutLSTM = 0.3\ndense_units = 30\nbatch_size = 32\nepochs = 30\noptimizer = k.optimizers.Adam(lr=0.01, decay=0.0001, clipnorm=1)","61c30df8":"model = Sequential()\nmodel.add(Embedding(embeddings_matrix.shape[0], output_dim = 100, input_length = X.shape[1], weights=[embeddings_matrix], trainable=False))\nmodel.add(LSTM(lstm_units1, recurrent_dropout = dropoutLSTM, return_sequences=True))\nmodel.add(SpatialDropout1D(0.1))\nmodel.add(LSTM(lstm_units2, recurrent_dropout = dropoutLSTM))\nmodel.add(Dense(dense_units, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])","3b01e329":"history = model.fit(X, Y, validation_split=0.1, batch_size = batch_size, epochs = epochs)","e7688ff9":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ae11b402":"model.save('model_improved.h5')","66f54e93":"Gathering all reviews from different folders into one csv file. Assigning \"1\" to positive reviews and \"0\" to negative reviews:","cb251bfb":"Fitting model:","5d23fba9":"Setting up the model:","675ae629":"Creating embedding matrix:","4e9f4cb6":"Saving  model:","1dd06ee0":"In this kernel we will create a recurrent neural network with LSTM layer for predicting positive or negative estimations based on movie reviews.","501a0523":"Creating new data frame with two columns: first column for movie reviews, and the second one for estimations.","c22f1bab":"Setting parameters:","3c617d9f":"Finally, we've reached 100% on the test subset. Training accuracy is lower due to dropout regularization that uses only a fraction of neurons to estimate train accuracy. <a href=\"https:\/\/keras.io\/getting-started\/faq\/#why-is-the-training-loss-much-higher-than-the-testing-loss\">Keras FAQ: Why is the training loss much higher than the testing loss?!<\/a>","0a3be0f8":"Setting up, fitting and plotting the new model:","96596b86":"Adding second LSTM layer and tweaking hyperparameters:","26a50125":"Another posible way to fix this problem is to use another algorithms of word embedding. Here we will use GloVe model with 100 dimensional vectors:","02ee8dcc":"Removing all characters that are not letters or numbers:","96cbc5a3":"We can see that with GloVe embedding the variance becomes lower.","a1839d24":"The main problem of this model is that accuracy on the train subset is much bigger than that on the test subset. Unfortunately, our dataset is too small to avoid overfitting even by means of various regularization technics such as reducing network nodes and dropout.","c38fc502":"Defining our vocabulary with 40000 most common words and assigning a unique index (token) to each. 0 is a reserved index that won't be assigned to any word. Then replacing words with tokens, leaving out those not present in the vocabulary. Finally, creating an array containing token sequences with paddings and an extra dimension holding a number of timesteps."}}