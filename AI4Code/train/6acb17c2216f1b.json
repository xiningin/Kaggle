{"cell_type":{"ebc393ad":"code","d4d36244":"code","d595047b":"code","1043c468":"code","dad2900e":"code","a3ef71b1":"code","1ff57a36":"code","e0091989":"code","4d1b58b9":"code","63cf9b8d":"code","b4ea4d52":"code","56e132bf":"code","b9c97966":"code","7f6de1cb":"code","a9ba5190":"code","b6eae4f2":"code","ebba7913":"code","7e782d02":"code","a8fcf337":"code","e213479c":"code","99de4668":"code","c7ccc622":"code","76ab08c9":"code","9ec865ad":"code","57b87521":"code","2ed0a112":"code","a65eba5f":"code","b76500ff":"code","9f0f613f":"code","4f0897eb":"code","2a53a408":"code","026c9611":"code","2c8cc3b1":"code","2bc2deb2":"code","de4ea29a":"code","5a2c79d9":"code","48bbe151":"code","e457c73b":"code","2acb5c41":"code","346e4edd":"code","294586c7":"code","94b23e19":"code","226cfe42":"code","7f0ec83f":"code","cd676afa":"code","f1c18d72":"code","30662f97":"code","77b53f6c":"code","9003a645":"code","5669360a":"code","154b9e51":"code","3072b10c":"code","ae600890":"code","ab3407db":"code","a5f322bd":"code","9248f526":"code","5c3037cd":"code","63ef1fc9":"code","a2bfec9f":"code","2f9a1fcd":"code","09586d52":"code","48a78c7e":"code","adc1f888":"code","c1d374db":"code","e4a9f171":"code","dbd019f7":"code","d552973b":"code","f356a7cc":"code","65fb4783":"code","0be1d432":"code","c51ab4e4":"code","2d3a1d73":"code","493d77a2":"code","36fbc840":"code","aaebc6dd":"code","41ed6298":"code","2b011023":"code","a5493cc8":"code","55fd6a73":"code","f7e289d1":"code","8c51d588":"code","e1bc644c":"code","909431b1":"code","f8d79d9d":"code","f0b9db02":"code","50a142eb":"code","c7102dd8":"code","59f04a40":"code","d7940e99":"code","21ccf0a0":"code","9332a632":"code","4d0a0cc9":"code","f72ff3be":"code","e786de20":"code","9ddb8313":"code","9d89ddfb":"code","560e7b98":"code","64daad7d":"code","49cdde80":"code","9bc53e53":"code","b3f6293d":"code","02502f56":"code","dc757c7d":"code","cd631806":"code","de021be9":"code","a537d625":"code","6e68c91c":"code","3879125a":"code","c5749a37":"code","e38a4f10":"code","eb26064b":"code","f32188dd":"code","88bcc704":"code","2041fe9d":"code","8d22a527":"code","69372aa5":"code","295a91dd":"code","2ec0e074":"code","afab15b3":"code","9396d205":"code","34347934":"code","a726b843":"code","66a78463":"code","4b45a58b":"code","ff65024e":"code","4a66e743":"code","b3fa3f11":"code","f7d23d0a":"code","40a2835e":"code","a6cfbdc4":"code","31a5cd4e":"code","54ffbf5a":"code","b91e502e":"code","2cae89fc":"code","90353d7e":"code","8f02bea7":"code","45795395":"code","92dc03e1":"code","a55629fd":"code","5ab6d1e0":"code","5ffe9cd8":"code","08d40994":"code","451eb218":"code","83bc8da7":"code","4b9550a7":"code","cc0bacf4":"code","03ff22bd":"code","34996725":"code","506df164":"code","1ecac10e":"code","c64950bf":"code","0b85e409":"code","85828c8f":"code","d3fd55b9":"code","8f84d54e":"code","603f3082":"code","64ab72d1":"code","9f8bb87f":"code","209ccc7b":"code","2b19543c":"code","9cf11d7f":"code","856932fb":"code","44e56496":"code","6a142c9b":"code","79bb60d8":"code","054962fb":"code","48a598b9":"code","793ab8e7":"code","c4c0d1ba":"code","154a571c":"code","8e04ff2d":"code","2f6ff00b":"code","7e57be1f":"code","66d56e4d":"code","9948a480":"code","ccf993ea":"code","b81c46b2":"code","da17557f":"code","268b4322":"code","0227e5c3":"code","e24181a7":"code","4012c31a":"code","17de0192":"code","8299f44e":"code","fbaec171":"code","ed46c287":"code","1bae3c5e":"code","66f062fe":"code","e3d07999":"code","d3afee19":"code","7599d596":"code","89205c81":"code","a2896bda":"code","d916fc82":"code","1c89abd0":"code","ce632c33":"code","f2cf6747":"code","5e312def":"code","b785634d":"code","e48e4489":"code","12265c6e":"code","30bbef5d":"code","d276381a":"code","13a7baab":"code","e4dd427d":"code","7a893516":"code","d0fbdba2":"code","41531312":"code","c7d3f331":"code","c6f5376c":"code","3e29bedf":"code","737ae74b":"code","24445749":"code","6471fe38":"code","f8ab44d1":"code","6e675466":"code","9b29af1d":"code","19160418":"code","68f1ec25":"code","882ec114":"code","daff6d05":"code","bd6407d9":"code","65a9b037":"code","d1d7c6a6":"code","14793348":"code","5844f2fb":"code","d5708bc9":"code","0b31fe2e":"code","4fb2f9a7":"code","201a4c1c":"code","2f7716f1":"code","3e6d2649":"code","583733e6":"code","c1b89e30":"code","234376ec":"code","01f3254b":"code","77f3e0e1":"code","b1a21897":"code","153c08ad":"code","ab8e10d2":"code","7aff6f26":"code","6f197284":"code","e7c1bb80":"code","996e476e":"code","8d0f6b94":"code","c42c33eb":"code","caf30521":"code","09656899":"code","7c0c735d":"code","e43a8ec3":"code","35d26bf5":"code","78809af7":"code","1ab95b5e":"code","f6e42182":"code","af6bfd2e":"code","34654d9b":"code","991bf7e2":"code","b75e5d53":"code","745f5ffc":"code","1f682761":"code","ddf005a2":"code","02adb431":"code","bc400c57":"code","a45f753f":"code","82f3efc5":"code","6de45afa":"code","78e91862":"code","3688e2fc":"code","34a5ba6a":"code","ffa58c92":"code","345c2f96":"code","bfc4cd85":"code","205e9284":"code","1db695bb":"code","7fba53c6":"code","b364c4b0":"code","378bf87c":"code","7fefd250":"code","34f8f192":"code","0be97834":"code","122fd1a1":"code","ef4e4ceb":"code","3d139484":"code","4db294a9":"code","bfb3d6ad":"code","92bc66fe":"code","b608d3e8":"code","20376ff2":"code","bbaf9bcc":"code","5a71c36e":"markdown","109e9e04":"markdown","4eac687a":"markdown","d36e6a43":"markdown","565c1220":"markdown","af7d80d5":"markdown","c1540dcb":"markdown","1e0f1440":"markdown","4e88a2fc":"markdown","acd153fe":"markdown","e6f130e5":"markdown","62959d61":"markdown","8255b108":"markdown","ba000af6":"markdown","6964a603":"markdown","4b14237b":"markdown","5ce4c0b8":"markdown","48f9602b":"markdown","4a15dd53":"markdown","5cf7f8f5":"markdown","ceee22f0":"markdown","aefea2d3":"markdown","64dca4d2":"markdown","5dc24306":"markdown","f06642b6":"markdown","43fe6999":"markdown","40b136c5":"markdown","34417d7b":"markdown","de312a87":"markdown","b1dc1d5b":"markdown","b4926ddf":"markdown","7e38a7d6":"markdown","a363a403":"markdown","afc9dcf5":"markdown","209baa8a":"markdown","3190d748":"markdown","9386c0cb":"markdown","ef537e70":"markdown","064ed348":"markdown","ba2865fd":"markdown","051a9ffb":"markdown","0d09f355":"markdown","65ebc1c0":"markdown","ff5ca719":"markdown","0eae559b":"markdown","84a4869f":"markdown","0ea607cb":"markdown","0c0dcdfa":"markdown","67f7c8d6":"markdown","0f1b4c09":"markdown","6540313d":"markdown","77118912":"markdown","9bddf8b6":"markdown"},"source":{"ebc393ad":"# Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport statsmodels.api as sm\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import r2_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d4d36244":"pc = pd.read_csv(\"C:\/\/Users\/pchadha\/Boosting_Kaggle_Practice\/Prudential_Life_insurance\/train.csv\")","d595047b":"pc_test = pd.read_csv(\"C:\/\/Users\/pchadha\/Boosting_Kaggle_Practice\/Prudential_Life_insurance\/test.csv\")","1043c468":"pd.set_option('display.max_row', 1000)\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_colwidth',5000)","dad2900e":"pc.head()","a3ef71b1":"pc.Response.value_counts()","1ff57a36":"# The 'Response' shows that potential customers have been classified into 8 categories\n# Most have been classified as level '8', followed by '6' and '7'","e0091989":"pc.info(verbose = True)","4d1b58b9":"# It can be seen that there are 18 float type, 109 int type and 1 object type variables in the dataset. \n#Total columns are 128, where 'Id' and 'Response' will not be part of model learning as they are customer 'id' and 'Target' \n# values respectively ","63cf9b8d":"cols4 = pc.select_dtypes(include=['int64']).columns.values\ncols5 = pc.select_dtypes(include = ['float64']).columns.values","b4ea4d52":"round((pc.isnull().sum()\/len(pc.index))*100,2)","56e132bf":"# As we have lot of columns, will remove columns with above 50% missing values\ncol = []\nfor i in pc.columns:\n    if round((pc[i].isnull().sum()\/len(pc.index))*100,2) >= 50:\n        col.append(i)\nprint(col)\nprint(len(col))\n    ","b9c97966":"# Removing these columns\npc = pc.drop(col, axis = 1)","7f6de1cb":"round((pc.isnull().sum()\/len(pc.index))*100,2)","a9ba5190":"# Focussing only on columns that have missing values\ncol = []\nfor i in pc.columns:\n    if round((pc[i].isnull().sum()\/len(pc.index))*100,2) != 0:\n        col.append(i)\nprint(col)\nprint(len(col))\n","b6eae4f2":"# Analysing 'Employment_Info_1' feature\npc[\"Employment_Info_1\"].describe()","ebba7913":"pc[\"Employment_Info_1\"].value_counts()","7e782d02":"pc[\"Employment_Info_1\"].max()","a8fcf337":"# These are normalized values related employment history and since there's insignificant number of missing rows in this case,\n# will remove the missing rows rather than imputing them\npc = pc[~pd.isnull(pc[\"Employment_Info_1\"])]","e213479c":"round((pc[col].isnull().sum()\/len(pc.index))*100,2)","99de4668":"pc[\"Employment_Info_4\"].describe()","c7ccc622":"pc[\"Employment_Info_4\"].value_counts()","76ab08c9":"# We can see that '0' value dominates the distribution of values within this feature. Therefore, imputing value '0' \n# for missing values in this case\npc.loc[pd.isnull(pc[\"Employment_Info_4\"]),\"Employment_Info_4\"] = 0 ","9ec865ad":"round((pc[col].isnull().sum()\/len(pc.index))*100,2)","57b87521":"pc.index = pd.RangeIndex(1, len(pc.index) + 1)","2ed0a112":"pc[\"Employment_Info_6\"].describe()","a65eba5f":"pc[\"Employment_Info_6\"].value_counts()","b76500ff":"pc[\"Insurance_History_5\"].describe()","9f0f613f":"pc[\"Insurance_History_5\"].value_counts()","4f0897eb":"pc[\"Family_Hist_2\"].describe()","2a53a408":"pc[\"Family_Hist_2\"].value_counts()","026c9611":"pc[\"Family_Hist_4\"].describe()","2c8cc3b1":"pc[\"Family_Hist_4\"].value_counts()","2bc2deb2":"pc[\"Medical_History_1\"].describe()","de4ea29a":"pc[\"Medical_History_1\"].value_counts()","5a2c79d9":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","48bbe151":"round((pc[col].isnull().sum()\/len(pc.index))*100,2)","e457c73b":"#Before imputing the values, will first analyze the 'object' feature \npc[\"Product_Info_2\"].value_counts()","2acb5c41":"lc = LabelEncoder()","346e4edd":"pc[\"Product_Info_2\"] = lc.fit_transform(pc[\"Product_Info_2\"])","294586c7":"pc[\"Product_Info_2\"].describe()","94b23e19":"pc[\"Product_Info_2\"].value_counts()","226cfe42":"# Imputing values using Iterative Imputer\ncols2 = pc.columns","7f0ec83f":"pc_imp = pd.DataFrame(IterativeImputer().fit_transform(pc))","cd676afa":"pc_imp.columns = cols2","f1c18d72":"pc_imp.head()","30662f97":"round((pc_imp[col].isnull().sum()\/len(pc_imp.index))*100,2)","77b53f6c":"pc_imp[cols4] = pc_imp[cols4].astype(int)","9003a645":"pc_imp.info(verbose = True)","5669360a":"pc_imp[\"Product_Info_2\"] = pc_imp[\"Product_Info_2\"].astype(int)","154b9e51":"# Removing 'Id' variable as it will not be used for model learning\npc_imp = pc_imp.drop('Id', axis =1)","3072b10c":"pc_imp.head()","ae600890":"# Box plots for outlier analysis\ncol = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\"]\ncoln = [\"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\", \"Insurance_History_5\"]\ncol2 = [\"Family_Hist_2\", \"Family_Hist_4\"]","ab3407db":"sns.boxplot(data = pc_imp[col], orient = 'v')","a5f322bd":"# We can see that there are few outliers in the case of features: 'Ht', 'Wt' and 'BMI'","9248f526":"plt.figure(figsize = (20,12))\nsns.boxplot(data = pc_imp[coln], orient = 'v')","5c3037cd":"# Outlier removal for 'BMI' and 'Employment_Info_6'\nQ1= pc_imp['BMI'].quantile(0.5)\nQ3= pc_imp['BMI'].quantile(0.95)\nRange=Q3-Q1\nprint(Range)\npc_imp= pc_imp[(pc_imp['BMI'] >= Q1-1.5*Range) & (pc_imp['BMI'] <= Q3+1.5*Range) ]","63ef1fc9":"Q1= pc_imp['Employment_Info_6'].quantile(0.5)\nQ3= pc_imp['Employment_Info_6'].quantile(0.95)\nRange=Q3-Q1\npc_imp= pc_imp[(pc_imp['Employment_Info_6'] >= Q1-1.5*Range) & (pc_imp['Employment_Info_6'] <= Q3+1.5*Range) ]","a2bfec9f":"plt.figure(figsize = (10,10))\nsns.boxplot(data = pc_imp[col2], orient = 'v')","2f9a1fcd":"# pairplot analysis to understand correlation between continous variables\ncolc = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\", \"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\", \"Insurance_History_5\", \"Family_Hist_2\",\"Family_Hist_4\"]","09586d52":"sns.pairplot(pc_imp[colc])","48a78c7e":"colc = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\", \"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\", \"Insurance_History_5\", \"Family_Hist_2\",\"Family_Hist_4\"]","adc1f888":"plt.figure(figsize = (15, 15))\nsns.heatmap(pc_imp[colc].corr(), annot = True, square = True, cmap=\"YlGnBu\")\nplt.show()","c1d374db":"pc_imp = pc_imp.drop([\"Ht\", \"Wt\"], axis = 1)","e4a9f171":"sns.boxplot(x = \"Response\",y = \"Family_Hist_2\", data = pc_imp)","dbd019f7":"sns.boxplot(x = \"Response\",y = \"Family_Hist_4\", data = pc_imp)","d552973b":"sns.boxplot(x = \"Response\",y = \"Ins_Age\", data = pc_imp)","f356a7cc":"sns.boxplot(x = \"Response\",y = \"BMI\", data = pc_imp)","65fb4783":"pc_imp = pc_imp.drop(\"Family_Hist_4\", axis =1)","0be1d432":"pc_imp[\"Product_Info_1\"].value_counts()","c51ab4e4":"pc_imp[\"Medical_History_8\"].value_counts()","2d3a1d73":"pc_imp[\"Medical_History_30\"].value_counts()","493d77a2":"pc_imp[\"Medical_History_1\"].value_counts()","36fbc840":"pc_imp[\"Medical_History_2\"].value_counts()","aaebc6dd":"pc_imp.shape","41ed6298":"pc_imp.info(verbose = True)","2b011023":"#Preparing the data in the 'test' dataset as well now","a5493cc8":"# round((pc_test.isnull().sum()\/len(pc_test.index))*100,2)","55fd6a73":"#First deleting the columns in 'test' dataset that have been deleted in the 'train' dataset\n#colt = ['Family_Hist_3', 'Family_Hist_5', 'Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32','Ht','Wt','Family_Hist_4','Id']","f7e289d1":"#pc_test_rev = pc_test.drop(colt, axis = 1)","8c51d588":"#pc_test_rev.shape","e1bc644c":"# pc_test_rev.info(verbose = True)","909431b1":"#Splitting data between test and train data sets\npc_imp_train, pc_imp_test = train_test_split(pc_imp, train_size = 0.7, test_size = 0.3, random_state = 100)","f8d79d9d":"y_pc_imp_train = pc_imp_train.pop(\"Response\")","f0b9db02":"X_pc_imp_train = pc_imp_train","50a142eb":"y_pc_imp_test = pc_imp_test.pop(\"Response\")","c7102dd8":"X_pc_imp_test = pc_imp_test","59f04a40":"X_pc_imp_train.shape","d7940e99":"X_pc_imp_test.shape","21ccf0a0":"# Will now do PCA to reduce dimensionality\npca = PCA(svd_solver='randomized', random_state=42)","9332a632":"pca.fit(X_pc_imp_train)","4d0a0cc9":"pca.components_","f72ff3be":"colnames = list(X_pc_imp_train.columns)","e786de20":"#Dataframe with features and respective first two Prinicple components\npca_df = pd.DataFrame({'PC1':pca.components_[0], 'PC2':pca.components_[1], 'Features': colnames})","9ddb8313":"pca_df.head()","9d89ddfb":"%matplotlib inline\nfig = plt.figure(figsize = (15,15))\nplt.scatter(pca_df.PC1, pca_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pca_df.Features):\n    plt.annotate(txt, (pca_df.PC1[i],pca_df.PC2[i]))\nplt.tight_layout()\nplt.show()","560e7b98":"pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1]","64daad7d":"# Plotting the cummulative variance and number of PCs graph to identify the correct number of PCs required to explain 95% of variance\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cummulative variance\")\nplt.show()","49cdde80":"# Base Random Forest result\nrfc=RandomForestClassifier()","9bc53e53":"rfc.fit(X_pc_imp_train, y_pc_imp_train)","b3f6293d":"Tr_predict = rfc.predict(X_pc_imp_train)","02502f56":"# Let's check the report of our default model\nprint(classification_report(y_pc_imp_train,Tr_predict))","dc757c7d":"# Printing confusion matrix\nprint(confusion_matrix(y_pc_imp_train,Tr_predict))","cd631806":"test_predict = rfc.predict(X_pc_imp_test)","de021be9":"# Let's check the report of default model on test dataset\nprint(classification_report(y_pc_imp_test,test_predict))","a537d625":"# Max depth tuning using CV an\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(10, 80, 5)}\n\n# instantiate the base model\nrf_m = RandomForestClassifier()\n\n\n# fit tree on training data\nrf_m = GridSearchCV(rf_m, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_m.fit(X_pc_imp_train, y_pc_imp_train)","6e68c91c":"# scores of GridSearch CV\nscores = rf_m.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","3879125a":"# n_estimators tuning using CV and keeping tuned 'max_depth' of 15\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(300, 2400, 300)}\n\n# instantiate the base model\nrf_e = RandomForestClassifier(max_depth=15)\n\n\n# fit tree on training data\nrf_e = GridSearchCV(rf_e, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_e.fit(X_pc_imp_train, y_pc_imp_train)","c5749a37":"sc = pd.DataFrame(scores)","e38a4f10":"sc.head()","eb26064b":"# scores of GridSearch CV\nscores = rf_e.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","f32188dd":"# tuning Max_features\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [10,20,30,40,50]}\n\n# instantiate the model\nrf_mx = RandomForestClassifier(max_depth=15, n_estimators = 1250)\n\n\n# fit tree on training data\nrf_mx = GridSearchCV(rf_mx, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_mx.fit(X_pc_imp_train, y_pc_imp_train)","88bcc704":"scores = rf_mx.cv_results_","2041fe9d":"sc = pd.DataFrame(scores)","8d22a527":"sc.head()","69372aa5":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","295a91dd":"# tuning min_samples_leaf\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(50, 1000, 50)}\n\n# instantiate the model\nrf_sl = RandomForestClassifier(max_depth=15, max_features = 30, n_estimators = 1250)\n\n\n# fit tree on training data\nrf_sl = GridSearchCV(rf_sl, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_sl.fit(X_pc_imp_train, y_pc_imp_train)","2ec0e074":"scores = rf_sl.cv_results_","afab15b3":"sc = pd.DataFrame(scores)","9396d205":"sc.head()","34347934":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","a726b843":"rf_f = RandomForestClassifier(max_depth=15, n_estimators = 1250, max_features = 30, min_samples_leaf = 800)","66a78463":"rf_f.fit(X_pc_imp_train, y_pc_imp_train)","4b45a58b":"rf_pred = rf_f.predict(X_pc_imp_train)","ff65024e":"print(classification_report(y_pc_imp_train,rf_pred))","4a66e743":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred))","b3fa3f11":"print(metrics.precision_score(y_pc_imp_train, rf_pred, average = 'weighted'))","f7d23d0a":"print(metrics.recall_score(y_pc_imp_train, rf_pred, average = 'weighted'))","40a2835e":"rf_f = RandomForestClassifier(max_depth=15, n_estimators = 1600, max_features = 40, min_samples_leaf = 40)","a6cfbdc4":"rf_f.fit(X_pc_imp_train, y_pc_imp_train)","31a5cd4e":"rf_pred = rf_f.predict(X_pc_imp_train)","54ffbf5a":"print(classification_report(y_pc_imp_train,rf_pred))","b91e502e":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred))","2cae89fc":"print(metrics.precision_score(y_pc_imp_train, rf_pred, average = 'weighted'))","90353d7e":"print(metrics.recall_score(y_pc_imp_train, rf_pred, average = 'weighted'))","8f02bea7":"rf_pred = rf_f.predict(X_pc_imp_test)","45795395":"print(classification_report(y_pc_imp_test,rf_pred))","92dc03e1":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred))","a55629fd":"print(metrics.precision_score(y_pc_imp_test, rf_pred, average = 'weighted'))","5ab6d1e0":"print(metrics.recall_score(y_pc_imp_test, rf_pred, average = 'weighted'))","5ffe9cd8":"rf_f_1 = RandomForestClassifier(max_depth=15, n_estimators = 1600, max_features = 50, min_samples_leaf = 30)","08d40994":"rf_f_1.fit(X_pc_imp_train, y_pc_imp_train)","451eb218":"rf_pred = rf_f_1.predict(X_pc_imp_train)","83bc8da7":"print(classification_report(y_pc_imp_train,rf_pred))","4b9550a7":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred))","cc0bacf4":"print(metrics.precision_score(y_pc_imp_train, rf_pred, average = 'weighted'))","03ff22bd":"print(metrics.recall_score(y_pc_imp_train, rf_pred, average = 'weighted'))","34996725":"rf_pred_test = rf_f_1.predict(X_pc_imp_test)","506df164":"print(classification_report(y_pc_imp_test,rf_pred_test))","1ecac10e":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test))","c64950bf":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test, average = 'weighted'))","0b85e409":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test, average = 'weighted'))","85828c8f":"rf_f_2 = RandomForestClassifier(max_depth=15, n_estimators = 1600, max_features = 50, min_samples_leaf = 20)","d3fd55b9":"rf_f_2.fit(X_pc_imp_train, y_pc_imp_train)","8f84d54e":"rf_pred_train_2 = rf_f_2.predict(X_pc_imp_train)","603f3082":"print(classification_report(y_pc_imp_train,rf_pred_train_2))","64ab72d1":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_2))","9f8bb87f":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_2, average = 'weighted'))","209ccc7b":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_2, average = 'weighted'))","2b19543c":"rf_pred_test_2 = rf_f_2.predict(X_pc_imp_test)","9cf11d7f":"print(classification_report(y_pc_imp_test,rf_pred_test_2))","856932fb":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_2))","44e56496":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_2, average = 'weighted'))","6a142c9b":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_2, average = 'weighted'))","79bb60d8":"rf_f_3 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 25)","054962fb":"rf_f_3.fit(X_pc_imp_train, y_pc_imp_train)","48a598b9":"rf_pred_train_3 = rf_f_3.predict(X_pc_imp_train)","793ab8e7":"print(classification_report(y_pc_imp_train,rf_pred_train_3))","c4c0d1ba":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_3))","154a571c":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_3, average = 'weighted'))","8e04ff2d":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_3, average = 'weighted'))","2f6ff00b":"rf_pred_test_3 = rf_f_3.predict(X_pc_imp_test)","7e57be1f":"print(classification_report(y_pc_imp_test,rf_pred_test_3))","66d56e4d":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_3))","9948a480":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_3, average = 'weighted'))","ccf993ea":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_3, average = 'weighted'))","b81c46b2":"rf_f_4 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20)","da17557f":"rf_f_4.fit(X_pc_imp_train, y_pc_imp_train)","268b4322":"rf_pred_train_4 = rf_f_4.predict(X_pc_imp_train)","0227e5c3":"print(classification_report(y_pc_imp_train,rf_pred_train_4))","e24181a7":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_4))","4012c31a":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_4, average = 'weighted'))","17de0192":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_4, average = 'weighted'))","8299f44e":"rf_pred_test_4 = rf_f_4.predict(X_pc_imp_test)","fbaec171":"print(classification_report(y_pc_imp_test,rf_pred_test_4))","ed46c287":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_4))","1bae3c5e":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_4, average = 'weighted'))","66f062fe":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_4, average = 'weighted'))","e3d07999":"rf_f_5 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 15)","d3afee19":"rf_f_5.fit(X_pc_imp_train, y_pc_imp_train)","7599d596":"rf_pred_train_5 = rf_f_5.predict(X_pc_imp_train)","89205c81":"print(classification_report(y_pc_imp_train,rf_pred_train_5))","a2896bda":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_5))","d916fc82":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_5, average = 'weighted'))","1c89abd0":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_5, average = 'weighted'))","ce632c33":"rf_pred_test_5 = rf_f_5.predict(X_pc_imp_test)","f2cf6747":"print(classification_report(y_pc_imp_test,rf_pred_test_5))","5e312def":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_5))","b785634d":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_5, average = 'weighted'))","e48e4489":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_5, average = 'weighted'))","12265c6e":"rf_f_6 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 12)","30bbef5d":"rf_f_6.fit(X_pc_imp_train, y_pc_imp_train)","d276381a":"rf_pred_train_6 = rf_f_6.predict(X_pc_imp_train)","13a7baab":"print(classification_report(y_pc_imp_train,rf_pred_train_6))","e4dd427d":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_6))","7a893516":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_6, average = 'weighted'))","d0fbdba2":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_6, average = 'weighted'))","41531312":"rf_pred_test_6 = rf_f_6.predict(X_pc_imp_test)","c7d3f331":"print(classification_report(y_pc_imp_test,rf_pred_test_6))","c6f5376c":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_6))","3e29bedf":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_6, average = 'weighted'))","737ae74b":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_6, average = 'weighted'))","24445749":"rf_f_7 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 8)","6471fe38":"rf_f_7.fit(X_pc_imp_train, y_pc_imp_train)","f8ab44d1":"rf_pred_train_7 = rf_f_7.predict(X_pc_imp_train)","6e675466":"print(classification_report(y_pc_imp_train,rf_pred_train_7))","9b29af1d":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_7))","19160418":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_7, average = 'weighted'))","68f1ec25":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_7, average = 'weighted'))","882ec114":"rf_pred_test_7 = rf_f_7.predict(X_pc_imp_test)","daff6d05":"print(classification_report(y_pc_imp_test,rf_pred_test_7))","bd6407d9":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_7))","65a9b037":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_7, average = 'weighted'))","d1d7c6a6":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_7, average = 'weighted'))","14793348":"rf_f_8 = RandomForestClassifier(max_depth=20, n_estimators = 1800, max_features = 30, min_samples_leaf = 15)","5844f2fb":"rf_f_8.fit(X_pc_imp_train, y_pc_imp_train)","d5708bc9":"rf_pred_train_8 = rf_f_8.predict(X_pc_imp_train)","0b31fe2e":"print(classification_report(y_pc_imp_train,rf_pred_train_8))","4fb2f9a7":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_8))","201a4c1c":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_8, average = 'weighted'))","2f7716f1":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_8, average = 'weighted'))","3e6d2649":"rf_pred_test_8 = rf_f_8.predict(X_pc_imp_test)","583733e6":"print(classification_report(y_pc_imp_test,rf_pred_test_8))","c1b89e30":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_8))","234376ec":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_8, average = 'weighted'))","01f3254b":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_8, average = 'weighted'))","77f3e0e1":"# Max depth tuning using CV an\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(10, 120, 5)}\n\n# instantiate the base model\nrf_m = RandomForestClassifier(n_estimators = 1600, max_features = 30, min_samples_leaf = 15)\n\n\n# fit tree on training data\nrf_m = GridSearchCV(rf_m, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_m.fit(X_pc_imp_train, y_pc_imp_train)","b1a21897":"# scores of GridSearch CV\nscores = rf_m.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","153c08ad":"rf_f_9 = RandomForestClassifier(max_depth=18, n_estimators = 1800, max_features = 25, min_samples_leaf = 15)","ab8e10d2":"rf_f_9.fit(X_pc_imp_train, y_pc_imp_train)","7aff6f26":"rf_pred_train_9 = rf_f_9.predict(X_pc_imp_train)","6f197284":"print(classification_report(y_pc_imp_train,rf_pred_train_9))","e7c1bb80":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_9))","996e476e":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_9, average = 'weighted'))","8d0f6b94":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_9, average = 'weighted'))","c42c33eb":"rf_pred_test_9 = rf_f_9.predict(X_pc_imp_test)","caf30521":"print(classification_report(y_pc_imp_test,rf_pred_test_9))","09656899":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_9))","7c0c735d":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_9, average = 'weighted'))","e43a8ec3":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_9, average = 'weighted'))","35d26bf5":"round((pc_test.isnull().sum()\/len(pc_test.index))*100,2)","78809af7":"#First deleting the columns in 'test' dataset that have been deleted in the 'train' dataset\ncolt = ['Family_Hist_3', 'Family_Hist_5', 'Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32','Ht','Wt','Family_Hist_4','Id']","1ab95b5e":"pc_test_rev = pc_test.drop(colt, axis = 1)","f6e42182":"pc_test_rev.shape","af6bfd2e":"round((pc_test_rev.isnull().sum()\/len(pc_test_rev.index))*100,2)","34654d9b":"# Focussing only on columns that have missing values\ncol = []\nfor i in pc_test_rev.columns:\n    if round((pc_test_rev[i].isnull().sum()\/len(pc_test_rev.index))*100,2) != 0:\n        col.append(i)\nprint(col)\nprint(len(col))\n","991bf7e2":"# Employment_Info_1 and Employment_Info_4 have quite low percentage of missing values so will just remove the rows\npc_test_rev = pc_test_rev[~pd.isnull(pc_test_rev[\"Employment_Info_1\"])]","b75e5d53":"pc_test_rev = pc_test_rev[~pd.isnull(pc_test_rev[\"Employment_Info_4\"])]","745f5ffc":"round((pc_test_rev[col].isnull().sum()\/len(pc_test_rev[col].index))*100,2)","1f682761":"pc_test_rev.index = pd.RangeIndex(1, len(pc_test_rev.index) + 1)","ddf005a2":"# Will treat missing values via iterative imputer for rest of columns. Will first encode the column 'Product_Info_2'","02adb431":"lc = LabelEncoder()","bc400c57":"pc_test_rev[\"Product_Info_2\"] = lc.fit_transform(pc_test_rev[\"Product_Info_2\"])","a45f753f":"# Imputing values using Iterative Imputer\ncolt2 = pc_test_rev.columns","82f3efc5":"pc_test_imp = pd.DataFrame(IterativeImputer().fit_transform(pc_test_rev))","6de45afa":"pc_test_imp.columns = colt2","78e91862":"pc_test_imp.head()","3688e2fc":"round((pc_test_imp[col].isnull().sum()\/len(pc_test_imp.index))*100,2)","34a5ba6a":"colt4 = np.delete(cols4,0)","ffa58c92":"print(len(colt4))","345c2f96":"colt4 = np.delete(colt4,107)","bfc4cd85":"print(colt4)","205e9284":"pc_test_imp[colt4] = pc_test_imp[colt4].astype(int)","1db695bb":"pc_test_imp[\"Product_Info_2\"] = pc_test_imp[\"Product_Info_2\"].astype(int)","7fba53c6":"pc_test_imp.head()","b364c4b0":"rf_pred_sub = rf_f_9.predict(pc_test_imp)","378bf87c":"rf_pred_sub","7fefd250":"rf_pred_sub.reshape(-1)","34f8f192":"pred_rk = pd.DataFrame({'Risk': rf_pred_sub})","0be97834":"pred_rk.head()","122fd1a1":"pred_rk.index = pd.RangeIndex(1, len(pred_rk.index) + 1)","ef4e4ceb":"len(pred_rk)","3d139484":"pc_test.index = pd.RangeIndex(1, len(pc_test.index) + 1)","4db294a9":"pc_test.head()","bfb3d6ad":"fn_dt = pd.DataFrame()","92bc66fe":"fn_dt['Id'] = pc_test['Id'] ","b608d3e8":"fn_dt['Response'] = pred_rk['Risk']","20376ff2":"# Final dataframe to be submitted is:\nfn_dt.head()","bbaf9bcc":"# Writing this to csv file for submission\nfn_dt.to_csv(\"C:\/\/Users\/pchadha\/Boosting_Kaggle_Practice\/Prudential_Life_insurance\/submission_file.csv\")","5a71c36e":"Analysing few continuous features first\nContinuous variables are as follows:\nProduct_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5","109e9e04":"We can see the max_depth of around 18 results in best test score and difference between train and test at this point is also minimum. Will keep the max_depth as '18'","4eac687a":"Preparing the data in the 'test' dataset as well now so that can predict the risk values using the selected model. ","d36e6a43":"Performance of this model is above 55% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data","565c1220":"Performance of this model is again near 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data","af7d80d5":"As per the apprehension, model is overfitted as the scores on test dataset are quite poor. \nRandomforest usually do not overfit but we used default model without any tuninng so overfitting was not avoided.\nWe will now tune the model for following hyper parameters:\nMax_depth\nn_estimators\nMax_features\nmin_smaples_leaf\n\nWill use grid search to estimate appropriate hyper parameters to get to best possible model","c1540dcb":"As seen by the plots, correlation is observed between the same features. Since BMI is calculated using 'Wt' and 'Ht' details and so would be sufficient by itslef to help in identifying the risk level, will drop the 'Wt' and 'Ht' features. \n","1e0f1440":"It can be seen that all null values have been treated. However, side-effect of Iterative computer is that it converts all columns to float type while processing them. Will now convert the columns originally integer type to 'int' ","4e88a2fc":"Analyse now few categorical variables","acd153fe":"Will use PCA to see if dimensionality can be reduced to only the most important feature set","e6f130e5":"The model with parameters selected is not giving good results on train data itself. Lets try now with lower values of 'min_sample_leaf'","62959d61":"Few outliers in the case of Family_Hist_2 and Family_Hist_4 features observed","8255b108":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores","ba000af6":"Performance of this model is near 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data","6964a603":"Performance of this model is lot better with scores around 75% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data","4b14237b":"Creating the required output file","5ce4c0b8":"It can be seen that only two dimensions or features,  contrbute heavily in terms of variance along two principal components","48f9602b":"Even after performing multiple iterations, it can be seen that the test accuracy and recall scores are around 53%, while precision is at 50%. Will stick with this model then as this is giving best performance so far with train scores around 60% and test at 53%. \nI am submitting this project with modelling performed by using Randomforest model though we could get better performance using models such as XGBOOST. However, for this submission I am sticking randomforest based model: 'rf_f_9'","4a15dd53":"The best 'test' score is observed at 'max_depth' of 15. Lets now tune other hyper parameters","5cf7f8f5":"Performance of this model is about 57% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data","ceee22f0":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores","aefea2d3":"Will first treat the columns (if any) with missing values ","64dca4d2":"So far the last three models have given similar performances. Will now see if we can get better performance by optimizing 'Max_depth' now  ","5dc24306":"From the pair plots, we can see that few variables such as \"Family_Hist_2\",\"Ins_Age\" and \"Family_Hist_4\", \"BMI\", \"Wt\" and \"Ht\" are correlated to each other. \nWill now plot heatmap to understand correlation in details between these variables","f06642b6":"Model Building\nWill use Random Forest with default features and estimate performance parameters such as 'Precision', 'Accuracy', 'f-score', 'Recall' to judge the model performance\nThen will use GridSearch, along with Cross-validation, to tune few key parameters such as 'Max_depth', 'min_samples_leaf', 'n_estimators', 'max_features' and get the best possible result\n","43fe6999":"Will now predict the risk classifiers using the model 'rf_f_9' for the above test data set for submission","40b136c5":"It's observed that the 'target' accuracy becomes maximum at about 30 'max_features' and difference between 'training' and 'test' accuracy scores are minimum at this point too.  Therefore, will consider 'max_features' to be 30. ","34417d7b":"Again for similar reasons, will impute using Iterative Imputer","de312a87":"We have the following ideal values of the hyper parameters that were selected for tuning:\n1. max_depth: 15\n2. n_estimators: 1250\n3. max_features: 30\n4. min_sample_leaf: 800\n\nWill now create the model with these parameters and analyse the result","b1dc1d5b":"The scores achieved are all quite high but also show strong signs of overfitting. Lets see the results on test data","b4926ddf":"Again, this feature too is normalized and has no dominant value. Therefore, here too will impute with use of Iterative imputer","7e38a7d6":"We can see from above plots that 'Family_Hist_2', 'Family_Hist_4' and 'Ins_Age' show similar distribution w.r.t 'Response' feature, with 'Family_Hist_2', 'Family_Hist_4' almost the same!. Therefore, can drop either of the features, will drop 'Family_Hist_4' for this iteration. \n'All' the features have least 'median' and 'max' (not considering outliers) values for 'Response' value -'8'\nThe distribution of the above analysed features is similar for 'Response' values -'3', '4' and '8' ","a363a403":"From the above graph, it can be observed that 'n_estimators' value of above '1250' seem to be most appropriate as the 'test' accuracy' is maximum at this value. ","afc9dcf5":"It can be seen from the above plot too that PCA would reduce features to less than 10 as per the variance contribution. While this may be good in the ease of model calculation, we would loose certain important features that may not be important in terms of just considering variance, still would help in model being applicable over different set of data.\nTherefore, will not use PCA reduced feature\/dimension set and instead, use the in-build feature filtering within RandomForest model to avoid multi-collinearity  ","209baa8a":"It can be seen that all null values have been treated. However, side-effect of Iterative computer is that it converts all columns to float type while processing them. Will now convert the columns originally integer type to 'int' ","3190d748":"Will now analyse 'Family_Hist_2', 'Ins_Age', 'Family_Hist_4' and 'BMI' w.r.t 'Response' variable","9386c0cb":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores","ef537e70":"Both train and test scores are closer with test scores above 50%. Will execute one more model with 'min_leaf_samples' less than 10, i.e., 8, and then select the best model out of the lot so far","064ed348":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores","ba2865fd":"Again, the scores of test and train are still not close enough for model to be considered an optimized one. Will try one last model with use of 'min_samples_leaf'. If model performance is not better, will not set the value of 'min_samples_leaf'","051a9ffb":"Will first consider min_samples_leaf value of 800 as both of the accuracy scores are first closest at this point. However, it can be seen that test and train accuracy scores are best with 'min_sample_leaf' values being around 50. So will consider value to be 50 or lower in case model does not give optimum results with following parameters:","0d09f355":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores","65ebc1c0":"Visualizing features and their relations using univariate and bivariate analysis","ff5ca719":"Again for similar reasons, will impute using Iterative Imputer","0eae559b":"In the above case, all features have outliers. Now all these variables are normalized so scaling is not required. \nHowever, will remove outliers using IQR for variables 'BMI', 'Employment_Info_6' to avoid data getting skewed for these variables ","84a4869f":"This feature has no dominant value so will use iterative imputer to fill the null values here\n","0ea607cb":"Model performance for test data is quite better for this model as compared to original model. Still, will try few more models and see if the gap between test and train results can be reduced further","0c0dcdfa":"Again, there's not much improvement in the model. Will now run the model without setting the parameter 'min_samples_leaf'","67f7c8d6":"Again, no one particular dominant value and even 'mean' is not suitable here as there's significant diference between the median and the mean. Will impute using Iterative Imputer function here as well","0f1b4c09":"The test and train scores are closer to each other, with test scores being above 50%. Lets try optimizing further","6540313d":"It can be seen that this is a categorical value that can be converted to numeric type using encoder. Will use 'labelencoder' to avoid creating multiple features using 'Dummy variables' as we already have lot of features.","77118912":"The test scores are best of any model so far but the difference between the train and test scores is quite huge. Will now select larger value of 'min_samples_leaf' than the default '1' to reduce overfitting","9bddf8b6":"Results are bit closer for train and test data. However, the precision score is less than 50% which needs to be better. Lets try and optimize model bit more"}}