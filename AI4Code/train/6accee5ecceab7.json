{"cell_type":{"63aeeb3c":"code","a95fe9ff":"code","fc3c7da3":"code","b03a5ed5":"code","a4b5cde3":"code","2910f0fb":"code","aaf5a964":"code","5958a591":"code","de673e4d":"code","3907d83c":"code","b42f8cf4":"code","6214010b":"code","d57c1a00":"code","82f8dce6":"code","e7b07864":"code","1901741a":"code","ef13361d":"code","ebc613fd":"code","eb76e365":"code","913b99d0":"code","06b5bbcc":"code","6d7c132d":"code","efa191c5":"code","f2d9a7b4":"code","12b9e40b":"code","7f5f1765":"code","e1012707":"code","e0f951f1":"code","cac2edbf":"code","8ce77b09":"code","21ab0a2e":"code","ebf1d799":"markdown","66582181":"markdown","c1090572":"markdown","730aae47":"markdown","18d2f0b9":"markdown","d636caf4":"markdown","ba08a434":"markdown","115d3b16":"markdown","33280d47":"markdown","9b9ff553":"markdown","0d7bc8a5":"markdown","189e7f6d":"markdown","14fe67e7":"markdown","365214e9":"markdown","64329b5f":"markdown","2e19499d":"markdown","838ec244":"markdown","be443c26":"markdown","98bec3d0":"markdown","fd82d6d9":"markdown","512ebc75":"markdown","c9be5dcc":"markdown","15e5924d":"markdown","15dff706":"markdown","f55d6f36":"markdown","32daea4d":"markdown","2869b9e1":"markdown","60be7c7f":"markdown","1a1e31a0":"markdown","0105987d":"markdown","989f7592":"markdown","f92cf261":"markdown","7b97cb2e":"markdown"},"source":{"63aeeb3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom scipy.sparse import csr_matrix\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nfrom nltk.corpus import stopwords\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport nltk\nimport string\nimport re\n\nfrom collections import Counter\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a95fe9ff":"data = pd.read_csv(\"..\/input\/spam.csv\", encoding='latin-1')\ndata.head()","fc3c7da3":"data.info()","b03a5ed5":"unimportant_col = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"]\nuseful_data = data.drop(unimportant_col, axis=1)","a4b5cde3":"useful_data = useful_data.rename(columns={\"v1\": \"Type\", \"v2\": \"Text\"})\nuseful_data.head()","2910f0fb":"train_data, test_data = train_test_split(useful_data, test_size=0.2, random_state=42)","aaf5a964":"train_data.info()","5958a591":"train_data.describe()","de673e4d":"train_data.groupby(\"Type\").describe()","3907d83c":"train_data.Type.value_counts().plot.pie();","b42f8cf4":"train_data['Length'] = train_data['Text'].apply(len)\ntrain_data.head()","6214010b":"train_data.hist(column='Length',by='Type',bins=60,figsize=(12,4))","d57c1a00":"findCapitalCount = lambda x: sum(map(str.isupper, x['Text'].split())) \ntrain_data[\"CapitalCount\"] = train_data.apply(findCapitalCount, axis=1)\ntrain_data.head()","82f8dce6":"train_data.hist(column='CapitalCount',by='Type',bins=30,figsize=(12,4))","e7b07864":"findWordCount = lambda x: len(x['Text'].split()) \ntrain_data[\"WordCount\"] = train_data.apply(findWordCount, axis=1)\ntrain_data.head()","1901741a":"train_data[\"CapitalRate\"] = train_data[\"CapitalCount\"] \/ train_data[\"WordCount\"]\ntrain_data.head()","ef13361d":"train_data.hist(column='CapitalRate',by='Type',bins=30,figsize=(12,4))","ebc613fd":"stopwords.words(\"english\")","eb76e365":"stemmer = nltk.PorterStemmer()\nfor word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n    print(word, \"=>\", stemmer.stem(word))","913b99d0":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attr_names = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.rename(columns={\"v1\": \"Type\", \"v2\": \"Text\"})\n        return X[self.attr_names]","06b5bbcc":"class AddExtraAttr(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attr_name = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.values\n        X = pd.DataFrame(X, columns=[self.attr_name])\n        X['Length'] = X[self.attr_name].apply(len)\n#         findCapitalCount = lambda x: sum(map(str.isupper, x[self.attr_name].split())) \n#         X[\"CapitalCount\"] = X.apply(findCapitalCount, axis=1)\n#         findWordCount = lambda x: len(x[self.attr_name].split()) \n#         X[\"WordCount\"] = X.apply(findWordCount, axis=1)\n#         X[\"CapitalRate\"] = X[\"CapitalCount\"] \/ X[\"WordCount\"]\n        return X[[\"Length\"]]","6d7c132d":"class FindCount(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attr_name = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        Y = []\n        for index, row in X.iterrows():\n            row[self.attr_name] = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', row[self.attr_name])\n            no_punc = [ch for ch in row[self.attr_name] if ch not in string.punctuation]\n            punc = [(\" \" + ch + \" \") for ch in row[self.attr_name] if ch in string.punctuation]\n            word_list = \"\".join(no_punc + punc).split()\n            useful_words = [word.lower() for word in word_list if word.lower() not in stopwords.words(\"english\")]\n            word_counts = Counter(useful_words)\n            stemmed_word_counts = Counter()\n            for word, count in word_counts.items():\n                stemmed_word = stemmer.stem(word)\n                stemmed_word_counts[stemmed_word] += count\n            word_counts = stemmed_word_counts\n            Y.append(word_counts)\n        return Y","efa191c5":"class ConvertToVector(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += count\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.most_common_ = most_common\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))","f2d9a7b4":"cat_attr = [\"Text\"]\n\nmain_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attr)),\n        ('find_count', FindCount(cat_attr[0])),\n        ('convert_to_vector', ConvertToVector()),\n    ])\n\nextra_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attr)),\n        ('add_extra', AddExtraAttr(cat_attr[0])),\n    ])\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('main_pipeline', main_pipeline),\n    ('extra_pipeline', extra_pipeline),\n])","12b9e40b":"train_prepared = full_pipeline.fit_transform(train_data)","7f5f1765":"y_train = train_data[\"Type\"]\nencoder = LabelEncoder()\ntype_encoded = encoder.fit_transform(y_train)","e1012707":"log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nscore = cross_val_score(log_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","e0f951f1":"sgd_clf = SGDClassifier(random_state=42)\nscore = cross_val_score(sgd_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","cac2edbf":"mnb_clf = MultinomialNB()\nscore = cross_val_score(mnb_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","8ce77b09":"bnb_clf = BernoulliNB()\nscore = cross_val_score(bnb_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","21ab0a2e":"from sklearn.metrics import precision_score, recall_score\n\ny_test = test_data[\"Type\"]\nencoder = LabelEncoder()\ntype_encoded = encoder.fit_transform(y_test)\n\nX_test_transformed = full_pipeline.transform(test_data)\n\nbnb_clf = BernoulliNB()\nbnb_clf.fit(train_prepared, y_train)\n\ny_pred = bnb_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision BernoulliNB: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall BernoulliNB: {:.2f}%\\n\".format(100 * recall_score(type_encoded, y_pred)))\n\nmnb_clf = MultinomialNB()\nmnb_clf.fit(train_prepared, y_train)\n\ny_pred = mnb_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision MultinomialNB: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall MultinomialNB: {:.2f}%\\n\".format(100 * recall_score(type_encoded, y_pred)))\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(train_prepared, y_train)\n\ny_pred = sgd_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision SGD: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall SGD: {:.2f}%\\n\".format(100 * recall_score(type_encoded, y_pred)))\n\nlog_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nlog_clf.fit(train_prepared, y_train)\n\ny_pred = log_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision Logistic Regression: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall Logistic Regression: {:.2f}%\".format(100 * recall_score(type_encoded, y_pred)))","ebf1d799":"This feature can be useful too.\n\nAs visualized below, spam messages are more likely to have these kind of words.","66582181":"# Train Model","c1090572":"Now, we convert the count data to vector, and create a sparse matrix.\n\nThis will show us, the number of occurances of each word in each text.","730aae47":"We want to gain some info about data.","18d2f0b9":"As mentioned above, we can add useful attributes such as Length, and CapitalRate.","d636caf4":"It's better to rename the columns names for better understanding.","ba08a434":"At this step, we replace the sequence of integers with a NUMBER string. Then we seperate the text from punctuation for easier analysis. Then we need to eliminate the common english words from SMS content, as they can easily mislead us, and finally stem the words.\n\nNow we can count the words.","115d3b16":"# Add UpperCase Count Feature","33280d47":"### BernoulliNB","9b9ff553":"# Add Length Feature","0d7bc8a5":"The number of words in general or the number of uppercase words may not be accurate separately and may not lead to preceise results. \n\nIt is better to get the ratio of the uppercase words count to all words.","189e7f6d":"### Logistic Regression","14fe67e7":"It is better to analyze the root of words as they can come in different forms.\n\nStemmer can do the job for us!","365214e9":"We now want to analyze the number of words which are fully in uppercase.","64329b5f":"# Create a Test Set","2e19499d":"# Add UpperCase Rate Feature","838ec244":"We need to define some transformers, in order to create a pipeline.","be443c26":"# Common Words","98bec3d0":"# Get Data","fd82d6d9":"Using the transformers above, we can make a pipeline to automate the process.","512ebc75":"At the beginning, we choose a part of data for test and we will put it aside. So, our computation will not depend on it.","c9be5dcc":" ### SGD","15e5924d":"As the types are \"spam\" and \"ham\", we will use LabelEncoder to encode these values to integers.\n\nThen we try a few models.","15dff706":"# Analyze Data","f55d6f36":"The 3 last columns seem to be useless, as they do not mostly contain a value and are null.","32daea4d":"As seen below, ham messages are usually much shorter than the spam ones.","2869b9e1":"### MultinomialNB","60be7c7f":"# Pipeline","1a1e31a0":"Now, we will add new features and check if they are useful.","0105987d":"Some words such as 'and', 'it', etc. are very common in english texts and can be misleading. We can delete them from our text.","989f7592":"# Add Word Count Feature","f92cf261":"# Stemmer","7b97cb2e":"At this step, we use the test data and analyze our recall and precision."}}