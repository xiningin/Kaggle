{"cell_type":{"6642e048":"code","c85d1a91":"code","645caa10":"code","32b75b56":"code","5a327e5a":"code","460bff6d":"code","14433bdf":"code","5aa99ba3":"code","66ae9c9e":"code","21c3fa67":"code","a946ef27":"code","5023753e":"code","9e2a8991":"code","51ccc289":"markdown"},"source":{"6642e048":"# Get the pydotplus package\n# !pip install pydotplus","c85d1a91":"# Standard Libraries\nimport os\nimport numpy as np \nimport pandas as pd \n\n# Visualization libraries\n#import pydotplus\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style({\"axes.facecolor\": \".95\"})\n\n# Modeling and Machine Learning\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals.six import StringIO  \nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.linear_model import LogisticRegression\n\n\n# Specify Paths\nBASE_PATH = '..\/input\/Kannada-MNIST\/'\nTRAIN_PATH = BASE_PATH + 'train.csv'\nTEST_PATH = BASE_PATH + 'test.csv'\n\n# Seed for reproducability\nseed = 1234\nnp.random.seed(seed)","645caa10":"# File sizes and specifications\nprint('\\n# Files and file sizes')\nfor file in os.listdir(BASE_PATH):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(BASE_PATH + file) \/ 1000000, 2))))","32b75b56":"# Load in training and testing data\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\ntest_df.rename(columns={'id':'label'}, inplace=True)\nconcat_df = pd.concat([train_df, test_df])\nsample_sub = pd.read_csv(BASE_PATH + 'sample_submission.csv');","5a327e5a":"def acc(y_true, y_pred):\n    \"\"\"\n        Calculates the accuracy score between labels and predictions.\n        \n        :param y_true: The true labels of the data\n        :param y_pred: The predictions for the data\n        \n        :return: a float denoting the accuracy\n    \"\"\"\n    return round(accuracy_score(y_true, y_pred) * 100, 2)","460bff6d":"test_df.head()","14433bdf":"test_df.shape","5aa99ba3":"train_df.head()","66ae9c9e":"features = [col for col in train_df.columns if col.startswith('pixel')]\n","21c3fa67":"tsvd = TruncatedSVD(n_components=50).fit_transform(concat_df[features])\ntsne = TSNE(n_components=3)\ntransformed = tsne.fit_transform(tsvd)  \n# Split up the t-SNE results in training and testing data\ntsne_train = pd.DataFrame(transformed[:len(train_df)], columns=['component1', 'component2', 'component3'])\ntsne_test = pd.DataFrame(transformed[len(train_df):], columns=['component1', 'component2', 'component3'])","a946ef27":"# Perform another split for t-sne feature validation\nX_train, X_val, y_train, y_val = train_test_split(tsne_train, \n                                                  train_df['label'], \n                                                  test_size=0.25, \n                                                  random_state=seed)\n# Train model with t-sne features\nclf = DecisionTreeClassifier(max_depth=10, random_state=seed)\nclf.fit(X_train, y_train)","5023753e":"train_preds = clf.predict(X_train)\nval_preds = clf.predict(X_val)\nacc_tsne_train = acc(train_preds, y_train)\nacc_tsne_val = acc(val_preds, y_val)\nprint(f'Training accuracy with t-SNE features: {acc_tsne_train}%')\nprint(f'Validation accuracy with t-SNE features: {acc_tsne_val}%')","9e2a8991":"# Make predictions and save submission file\npredictions = clf.predict(tsne_test)\nsample_sub['label'] = predictions\nsample_sub.to_csv('submission.csv', index=False)","51ccc289":"This is a simple adoptation of the following MNIST notebook: https:\/\/www.kaggle.com\/carlolepelaars\/97-on-mnist-with-a-single-decision-tree-t-sne\nPlease upvote it if you find this notebook helpful."}}