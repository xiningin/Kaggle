{"cell_type":{"b4abd08d":"code","682ecdb8":"code","07b7bced":"code","2d01bcca":"code","067d3b27":"code","e6deef60":"code","3b72c153":"code","0d19fd3b":"code","e53bf578":"code","ebcc6595":"code","ad325ade":"code","679eddb5":"code","b5ae1db2":"code","888984cd":"code","bb90b4cc":"code","5508e515":"code","d3b6beb6":"code","de67eedf":"code","2ce973e5":"code","723749d6":"code","5cdee11a":"code","10ccd6ff":"code","ea27736b":"code","f6db4b0e":"code","cd12358a":"code","19f6e1c4":"code","cf8ab85a":"code","8357f999":"code","4c9914ad":"markdown","0360aa6a":"markdown","58e97f24":"markdown","261754b3":"markdown","88a96008":"markdown","71bcd337":"markdown","451e0848":"markdown","defc59f6":"markdown","c7838fe2":"markdown","cf332e02":"markdown","d3e984e1":"markdown","f5292442":"markdown","4f6112ec":"markdown","5d616f96":"markdown","ce1c24ac":"markdown","040ed933":"markdown","0099436c":"markdown","93e8be4c":"markdown","4fdf35e1":"markdown","4996e07f":"markdown","4d339534":"markdown","b317288e":"markdown","08486f01":"markdown","e65fe3b1":"markdown","54e1e547":"markdown","d4251910":"markdown"},"source":{"b4abd08d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom scipy.stats import skew\nfrom math import sqrt\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_columns', 300)","682ecdb8":"df_train = pd.read_csv('..\/input\/train.csv', index_col=\"Id\")\ndf_test = pd.read_csv('..\/input\/test.csv', index_col=\"Id\")\ny_train = df_train.SalePrice\ndf_train = df_train.drop(columns='SalePrice')\n\nprint(len(df_train._get_numeric_data().columns))\nprint(len(df_train.select_dtypes(include='object').columns))","07b7bced":"_, axes = plt.subplots(figsize=(14, 6))\nmissing = df_train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","2d01bcca":"categorical_features = df_train.select_dtypes(include='object').columns\ndf_train[categorical_features] = df_train[categorical_features].fillna('None')\ndf_test[categorical_features] = df_test[categorical_features].fillna('None')","067d3b27":"from sklearn.preprocessing import Imputer\n\n# Replace NaN with corresponding Neighborhood Median\ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\ndf_test[\"LotFrontage\"] = df_test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n    \n# Replace NaN with Feature Median \nnumerical_features = df_train.select_dtypes(include='number').columns\nmy_imputer = Imputer()\ndf_train[numerical_features] = my_imputer.fit_transform(df_train[numerical_features])\ndf_test[numerical_features] = my_imputer.transform(df_test[numerical_features])\n","e6deef60":"idx_split = df_train.shape[0]\ndf_train = df_train.append(df_test)","3b72c153":"NumStr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\nfor col in NumStr:\n    df_train[col]=df_train[col].astype(str)\n    \nnon_ordinal_features = ['LandSlope','Utilities','MSZoning', 'Street','Alley','LotShape','LandContour','LotConfig','Neighborhood','Condition1','Condition2','BldgType','HouseStyle', 'RoofStyle','RoofMatl','Exterior1st', 'Exterior2nd','MasVnrType','Foundation', 'BsmtExposure','BsmtFinType1','BsmtFinType2', 'Heating', 'CentralAir', 'Electrical','Functional', 'GarageType','GarageFinish','PavedDrive', 'Fence','MiscFeature','SaleType','SaleCondition', \"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\nordinal_features = df_train.select_dtypes(include='object').columns.drop(non_ordinal_features)","0d19fd3b":"dummies = pd.get_dummies(df_train.loc[:,non_ordinal_features], drop_first=True)\ndf_train = pd.concat([df_train,dummies], axis=1)\ndf_train = df_train.drop(non_ordinal_features,axis=1)","e53bf578":"def cat_to_num(x):\n    if x=='Ex':\n        return 5\n    if x=='Gd':\n        return 4\n    if x=='TA':\n        return 3\n    if x=='Fa':\n        return 2\n    if x=='Po':\n        return 1\n    if x=='None':\n        return 0\n\ndf_train.loc[:,ordinal_features] = df_train.loc[:,ordinal_features].applymap(cat_to_num)","ebcc6595":"X = df_train.loc[:idx_split, :]\nX_test = df_train.loc[idx_split+1:, :]","ad325ade":"X = X.assign(SalePrice=y_train)","679eddb5":"sns.set()\n_, axes = plt.subplots(figsize=(12, 4))\nsns.distplot(X.SalePrice)","b5ae1db2":"_, ax = plt.subplots(ncols=1, figsize=(12,8))\ncorr_matrix = X.drop(dummies,axis=1).corr()\nsns.heatmap(corr_matrix);","888984cd":"_, ax = plt.subplots(ncols=1, figsize=(10,7))\ncols = corr_matrix.nlargest(11, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(X[cols].values.T)\nsns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)","bb90b4cc":"_, axes = plt.subplots(figsize=(14, 6))\nsns.boxplot(x='OverallQual', y='SalePrice', data=X)","5508e515":"sns.set()\nsns.jointplot(kind='scatter',data=X, x='GrLivArea', y='SalePrice')","d3b6beb6":"sns.set()\nsns.jointplot(kind='scatter',data=X, x='TotalBsmtSF', y='SalePrice')","de67eedf":"sns.set()\nsns.jointplot(kind='scatter',data=X, x='1stFlrSF', y='SalePrice')","2ce973e5":"X = X.drop(columns='SalePrice')","723749d6":"X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \nX[\"TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\nX[\"GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n\n\nX_test[\"TotalHouse\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]   \nX_test[\"TotalHouse_OverallQual\"] = X_test[\"TotalHouse\"] * X_test[\"OverallQual\"]\nX_test[\"GrLivArea_OverallQual\"] = X_test[\"GrLivArea\"] * X_test[\"OverallQual\"]\n","5cdee11a":"print(X[(X['GrLivArea']>4000) & (y_train<300000)].index)\nX = X.drop(X[(X['GrLivArea']>4000) & (y_train<300000)].index)\ny_train = y_train.drop([524, 1299])","10ccd6ff":"numeric_feats = X.dtypes[X.dtypes != \"object\"].index\nnumeric_feats = numeric_feats.drop(dummies)\nnumeric_feats = numeric_feats.drop(ordinal_features)\n\nskewed_feats = X[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    X[feat] = boxcox1p(X[feat], lam)\n    X_test[feat] = boxcox1p(X_test[feat], lam)","ea27736b":"y_train = np.log(y_train)","f6db4b0e":"multicol_features = ['GarageCars', 'PoolQC', 'FireplaceQu', 'GarageCond']\nX = X.drop(multicol_features, axis = 1)\nX_test = X_test.drop(multicol_features, axis = 1)","cd12358a":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nscaler.fit(X)\n\nX[X.columns] = scaler.transform(X[X.columns])\nX_test[X_test.columns] = scaler.transform(X_test[X_test.columns])","19f6e1c4":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_log_error\n\nrgr_ls = Lasso(fit_intercept = True)\nparam_grid = {\n    \"alpha\": [0.0002, 0.0003, 0.0005, 0.0008, 0.001]\n}\nsearchCV = GridSearchCV(rgr_ls, cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nsearchCV.fit(X, y_train)\nnp.sqrt(searchCV.best_score_*-1)","cf8ab85a":"lasso = searchCV.best_estimator_\ncoefs = pd.Series(lasso.coef_,index=X.columns)\n\nplt.figure(figsize=(10,18))\ncoefs[coefs.abs()>0.03].sort_values().plot.barh()\nplt.title('Coefficients with magnitude greater than 0.03')","8357f999":"y_test_pred = searchCV.predict(X_test)\ny_test_pred = np.exp(y_test_pred)\nmy_submission = pd.DataFrame({'Id': X_test.index.astype(int), 'SalePrice': y_test_pred})\nmy_submission.to_csv('submission.csv', index=False)","4c9914ad":"## 5.4 Feature Scaling","0360aa6a":"### 2.3.2 Factorizing","58e97f24":"## 3.2 Target Variable\nThe target variable is right skewed, this makes sense since expensive houses are rarer. Having skewed distributions may negatively impact model performance, so we'll take care of this during data cleaning","261754b3":"## 3.5 Predictors-Target Split\nHere I dropped the target again, because we don't want to have our target as a predictor when we're modelling.","88a96008":"### 3.4.2 GrLivArea","71bcd337":"### 3.3.2 Correlation with Target\nHere we plot the 10 most correlated features with our target to have an idea of which might be the best predictors, below are the descriptions of the features.\n\n1. **OverallQual:** Rating of the material and finish of the house.\n2. **GrLivArea:** Above ground living area in square feet, it's basically the area of the house excluding the basement. This means that it is related with two other features in the following manner: GrLivArea = 1stFlrSF + 2ndFlrSF\n3. **ExterQual:** Evaluates the quality of the material on the exterior, it's interesting that although the description is similar to OverallQual they are not very correlated as seen on the previous plot.\n4. **KitchenQual:** Just the general kitchen quality.\n5. **GarageCars:**  Size of garage in car capacity.\n6. **GarageArea:** Size of garage in square feet, hence why it is correlated with GarageCars, they measure pretty much the same thing using different metrics.\n7. **TotalBsmtSF:** Area of the Basement in square feet.\n8. **1stFlrSF:** Area of the First Floor in square feet.\n9. **BsmtQual:** Evaluates the height of the basement.\n10. **FullBath:** Number of full bathrooms above the ground.","451e0848":"# 3 Exploratory Data Analysis\n\nThis EDA has the objectives described below, any actions will be taken during the Data Cleaning and Feature Engineering phases.\n\n- Get a general feel for the dataset\n- Discover predictors that are correlated between them which may lead to **multicollinearity issues**\n- Find out which predictors are most correlated with the target, these are the **top potential predictors** and are good candidates for feature engineering. It will also be interesting to verify if these are actually the ones the model gives more weight\n- Inspect the data distributions and identify **skewed distributions** and **potential outliers** which can negatively impact the model performance\n\n## 3.1 Predictors-Target Join\nDoing EDA is easier if the predictors and the labels are in the same dataframe","defc59f6":"## 2.2 Missing Data\nHere the objective is to detect and handle the missing values in our data, it's important to check the dataset description. Below are some conclusions:\n- The categorical NaNs are not missing values, they simply mean the house doesn't have that particular feature, this means they are an extra category. For this reason they were replaced with the category \"None\"\n- The numerical NaNs are indeed missing values and were imputed with the mean of that particular feature.\n- A particular case was with LotFrontage, insteading of imputing with the mean of the feature I imputed by using the mean of thet corresponding neighborhood.\n\n### 2.2.1 What Data is Missing?","c7838fe2":"### 3.4.4  1stFlrSF","cf332e02":"### 2.2.2 Imputing Categorical Features","d3e984e1":"# 6 Modelling\nSo we finally reached the modelling phase, I tried other regression models like Ridge and ElasticNet but Lasso obtained the best results, probably due to the high ammount of non-important features which Lasso handles very well.\n\nLinear models are very easy to interpret we simply look at the weights that were assigned to each feature and we understand its influence in the predictive power of the model, below are some conclusions we can reach by looking at Lassos' coefficients:\n\n-  The most important feature is **TotalHouse_OverallQual**, which we created. This makes sense since it account for both the area of the house and materials and finishes.\n- The other top 7 features are all related with areas of the house and overall quality and condition, so these are clearly the most important features for determining the price of a house.\n- Other very important features are the one-hot encoded neighborhoods, there are both positive and negative coefficients corresponding to the most and least expensive neighborhoods.\n\n## 6.1 Lasso Regression\n","f5292442":"## 5.3 Avoiding Multicollinearity","4f6112ec":"### 2.3.5 Train-Test Split","5d616f96":"## 3.3 Correlation Analysis\n### 3.3.1 General Correlation\nThere are highly correlated features with each other this may result in multicollineariy during the modelling phase, these are the most troublesome features:\n- GarageCars and GarageArea are highly correlated.\n- PoolQC and PoolArea are highly correlated.\n- Fireplaces and FireplaceQu are highly correlated.\n- GarageCond and GarageCond are highly correlated","ce1c24ac":"### 2.3.3 Non-Ordinal Variables","040ed933":"## 2.3 Categorical Encoding\n\nHere the objective is to encode categorical features and also factorize numerical features that should be categorical, these are some of the conclusions reached:\n\n- First we join the train and test sets to guarantee they'll have te same dimension, because if one of the datasets had a category that the other didn't have One-hot encoding would output train and test sets with different dimensions which would break our model.\n- There are some numerical features which make more as categoricals, so they are turned into type str and then one-hot encoded\n- Categorical features that have no ordinal relation in their categories are one-hot encoded\n- Categorical features that have an ordinal relation in their categories are manually encoded to maintain their ordinal relation\n\n### 2.3.1 Train-Test Join","0099436c":"## 5.2 Feature Normalization","93e8be4c":"# 4 Feature Engineering\n\nHere the objective is to engineer some extra features to help our model understand relations between them:\n\n- **TotalHouse:** The total area of the house including the basement\n- **TotalHouse_OverallQual:** The total area of the house times the overall quality, this proved to be the most weighted feature for Lasso\n- **GrLivArea_OverallQual:** The area of the house excluding the basement times the overall quality","4fdf35e1":"### 2.3.4 Ordinal Variables","4996e07f":"### 2.2.3 Imputing Numerical Features","4d339534":"### 3.4.3 TotalBsmtSF","b317288e":"## 3.4 Numerical Features\n\nThe objective here is to plot the Numerical Features in order to understand their distributions and if they have outliers, below are some of the reached conclusions:\n\n- In most numerical features there are some outliers, in fact they are the same points for all features. This are indeed unwanted outliers as can be seen in the dataset description, so they were removed.\n- There are some skewed distributions so we might have to take care of this.\n\n### 3.4.1 OverallQual","08486f01":"# 5 Data Cleaning\n\nHere the objective is to clean the data based on the EDA we did previously, here are some conclusions:\n- The previously detected outliers were removed\n- All features with skewness above 0.75 are boxcox transformed\n- The four multicollinear features we identified previously are removed\n- All features are scaled using MinMaxScaling since it gave better results comparing to Standard and Robust\n\n## 5.1 Handling Outliers","e65fe3b1":"If you found any mistakes or have any tips please be free to comment.","54e1e547":"## 6.2 Model Submission","d4251910":"# 1 Introduction\n\nIn first place I'd like to say my notebook was inspired by many kernels that have been posted, in fact there's probably nothing here that you won't see on other kernels. My objective was to build an **effective and well documented solution that balances accuracy and interpretability**. So here you'll see some ideas and techniques that are split between other kernels, I decided not to use any extreme feature engineering, dimensionality reduction or ensembles which may increase accuracy but would reduce the interpretability of the model.\n\n- [Data Preprocessing](#Data-Preprocessing)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n- [Feature Engineering](#Feature-Engineering)\n- [Data Cleaning](#Data-Cleaning)\n- [Modelling](#Modelling)\n\n# 2 Data Preprocessing\n\n## 2.1 Data Loading"}}