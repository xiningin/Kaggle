{"cell_type":{"974eb4bd":"code","c3f34386":"code","48c1b4b5":"code","baac34f6":"code","a7cd0971":"code","14c0ee8d":"code","060b00d1":"code","b7903097":"code","12da0964":"code","eede4535":"code","6752ee2c":"code","6eadda27":"code","2780242c":"markdown"},"source":{"974eb4bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import word2vec\nfrom gensim.models import FastText\nfrom sklearn.manifold import TSNE\nfrom nltk.cluster import KMeansClusterer\nimport nltk\nfrom sklearn import metrics\nfrom nltk.cluster import KMeansClusterer\nimport nltk\nimport matplotlib.pyplot as plt\nfrom sklearn import cluster\nfrom sklearn.cluster import KMeans\nimport re\nfrom string import punctuation\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english')) \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c3f34386":"train=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ntest=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\nsample=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","48c1b4b5":"train.shape","baac34f6":"train.sentiment.value_counts()","a7cd0971":"def text_to_wordlist(text, remove_stop_words=True, stem_words=False,lemmatize_words=False):\n    # Clean the text, with the option to remove stop_words and to stem words.\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n#     text = re.sub(r\"<\/p>\", \"\", text)\n#     text = re.sub(r\"<p>\", \"\", text)\n    text = re.sub(r\"what's\", \"\", text)\n    text = re.sub(r\"\\n\", \"\", text)\n    text = re.sub(r\"What's\", \"\", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\" m \", \" am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = re.sub(r\" J K \", \" JK \", text)\n    # Remove punctuation from text\n    text = ''.join([c.lower() for c in text if c not in punctuation])\n\n    # Optionally, remove stop words\n    if remove_stop_words:\n        text = text.split()\n        text = [w for w in text if not w in stop_words]\n        text = \" \".join(text)\n\n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n\n    if lemmatize_words:\n        # stemmed_words = [lemmatizer.lemmatize(word) for word in text]\n        # text = \" \".join(stemmed_words)\n        pass\n    # Return a list of words\n    return (text)\n# text_to_wordlist(stack['body'])","14c0ee8d":"for i,val in enumerate(train['text']):\n    try:\n        train['text'][i]=text_to_wordlist(train['text'][i])\n    except:\n        continue\n# train['text']    \n# text_to_wordlist(train['text'][2])","060b00d1":"train['text']","b7903097":"corpus=[]\nfor i in train['text'].values:\n    corpus.append(str(i).split(\" \"))\ncorpus[:1]","12da0964":"\nmodel = FastText(corpus, size=100, workers=4,window=5)","eede4535":"print(model.wv.most_similar('shit'))\nprint('******')\nprint(model.wv.most_similar('crap'))\nprint('******')\nprint(model.wv.most_similar('good'))\n","6752ee2c":"model = word2vec.Word2Vec(corpus, size=100, workers=4,window=5)\n","6eadda27":"# print(model.wv.most_similar('shit')) gives error because shit word not present in vocabulary\nprint('******')\n# print(model.wv.most_similar('crap'))  gives error because crap word not present in vocabulary\nprint('******')\nprint(model.wv.most_similar('good'))","2780242c":"what do you guys think ?\n\ngoing forward i'll probably use these embeddings for classfication , until then search is on . if you like this give it a thumbs up . would like to collaborate with everyone of ya !!"}}