{"cell_type":{"532296f1":"code","7fa3dcec":"code","bd729bf1":"code","447e6578":"code","99f8efe7":"code","0a9f0fd7":"code","0870dca1":"code","c4a3e178":"code","8cf8fe30":"code","302d141e":"code","bd963260":"code","d5239362":"code","ea72222a":"code","b3d96d93":"code","37ed943a":"code","60f0e7fa":"code","d42eb019":"code","0e5056f6":"code","3816377b":"code","6987e24f":"code","ad145cec":"code","f8eb6f55":"code","b85f3fe7":"code","23f61cf5":"code","01d130bc":"code","f50e3983":"code","7c26f447":"code","9db94834":"code","89527e14":"code","30a14009":"code","4787893c":"code","5c17d33e":"code","1c5b83e3":"code","6a6ce622":"markdown","7b8dbe90":"markdown","19ac9a8f":"markdown","d6eabd51":"markdown","da708ee6":"markdown","f40c3e9f":"markdown","524561dd":"markdown","87895736":"markdown","859a9408":"markdown","3a2747f6":"markdown","116d6575":"markdown","99a74597":"markdown","46b32063":"markdown","0ccf0892":"markdown","32fe6844":"markdown","6ade9506":"markdown","443febc7":"markdown","3176b786":"markdown"},"source":{"532296f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns #for confusion matrix\n#For data visualization\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7fa3dcec":"data=pd.read_csv(\"..\/input\/kidney_disease.csv\")","bd729bf1":"data.info() #data types and feature names. \n            #I'll change the data types of some parameters in the following codes","447e6578":"data.head() #first 5 samples in dataset","99f8efe7":"data.classification.unique() ","0a9f0fd7":"data.classification=data.classification.replace(\"ckd\\t\",\"ckd\") ","0870dca1":"data.classification.unique() #problem solved.","c4a3e178":"data.drop(\"id\",axis=1,inplace=True) ","8cf8fe30":"data.head() #id parameter dropped.","302d141e":"data.classification=[1 if each==\"ckd\" else 0 for each in data.classification]","bd963260":"data.head()","d5239362":"data.isnull().sum() ","ea72222a":"\ndf=data.dropna(axis=0)\nprint(data.shape)\nprint(df.shape) \ndf.head()","b3d96d93":"df.index=range(0,len(df),1)\ndf.head()","37ed943a":"#you can see that the values have changed.\ndf.wc=df.wc.replace(\"\\t6200\",6200)\ndf.wc=df.wc.replace(\"\\t8400\",8400) \nprint(df.loc[11,[\"wc\"]])\nprint(df.loc[20,[\"wc\"]])","60f0e7fa":"df.pcv=df.pcv.astype(int)\ndf.wc=df.wc.astype(int)\ndf.rc=df.rc.astype(float)\ndf.info()","d42eb019":"dtype_object=df.select_dtypes(include=['object'])\ndtype_object.head()","0e5056f6":"for x in dtype_object.columns:\n    print(\"{} unique values:\".format(x),df[x].unique())\n    print(\"*\"*20)","3816377b":"dictonary = {\n        \"rbc\": {\n        \"abnormal\":1,\n        \"normal\": 0,\n    },\n        \"pc\":{\n        \"abnormal\":1,\n        \"normal\": 0,\n    },\n        \"pcc\":{\n        \"present\":1,\n        \"notpresent\":0,\n    },\n        \"ba\":{\n        \"notpresent\":0,\n        \"present\": 1,\n    },\n        \"htn\":{\n        \"yes\":1,\n        \"no\": 0,\n    },\n        \"dm\":{\n        \"yes\":1,\n        \"no\":0,\n    },\n        \"cad\":{\n        \"yes\":1,\n        \"no\": 0,\n    },\n        \"appet\":{\n        \"good\":1,\n        \"poor\": 0,\n    },\n        \"pe\":{\n        \"yes\":1,\n        \"no\":0,\n    },\n        \"ane\":{\n        \"yes\":1,\n        \"no\":0,\n    }\n}\n\n","6987e24f":"#We used categorical values as numerical to replace them.\ndf=df.replace(dictonary)\n","ad145cec":"df.head() #All values are numerical.","f8eb6f55":"#HEAT MAP #correlation of parameters \nf,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True,fmt=\".2f\",ax=ax,linewidths=0.5,linecolor=\"orange\")\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\nplt.show()\n","b85f3fe7":"#box-plot\ntrace0 = go.Box(\n    y=df.bp,\n    name = 'Bp',\n    marker = dict(\n        color = 'rgb(12, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=df.sod,\n    name = 'Sod',\n    marker = dict(\n        color = 'rgb(12, 128, 128)',\n    )\n)\ndata = [trace0, trace1]\niplot(data)","23f61cf5":"#Line plot\ndf2=df.copy()\ndf2[\"id\"]=range(1,(len(df.ba)+1),1)\ndf2[\"df2_bp_norm\"]=(df2.bp-np.min(df2.bp))\/(np.max(df2.bp)-np.min(df2.bp))\ndf2[\"df2_hemo_norm\"]=(df2.hemo-np.min(df2.hemo))\/(np.max(df2.hemo)-np.min(df2.hemo))\n#Line Plot\ntrace1 = go.Scatter(\n        x = df2.id,\n        y = df2.df2_bp_norm,\n        mode = \"lines\",\n        name = \"Blood Press.\",\n        marker = dict(color = 'rgba(16, 112, 2, 0.8)'),\n        text= df.age)\ntrace2 = go.Scatter(\n        x = df2.id,\n        y = df2.df2_hemo_norm,\n        mode = \"lines+markers\",\n        name = \"Hemo\",\n        marker = dict(color = 'rgba(80, 26, 80, 0.8)'),\n        text= df.age)\ndata=[trace1,trace2]\nlayout=dict(title=\"Blood Press and Hemoglobin values according the age\",\n            xaxis=dict(title=\"\u0130d\",ticklen=5,zeroline=False))\nfig=dict(data=data,layout=layout)\niplot(fig)","01d130bc":"score=[] #these variables will be used to show the algorithm name and its successes.\nalgorithms=[] ","f50e3983":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\ny=df[\"classification\"].values\nx_data=df.drop([\"classification\"],axis=1)\n\n#Normalization\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n\n#Preparing the test and training set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.3)\n\n#model and accuracy\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nknn.predict(x_test)\nscore.append(knn.score(x_test,y_test)*100)\nalgorithms.append(\"KNN\")\nprint(\"KNN accuracy =\",knn.score(x_test,y_test)*100)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=knn.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\" KNN Confusion Matrix\")\nplt.show()\n#%%\n","7c26f447":"#Navie-Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\n\n#Training\nnb.fit(x_train,y_train)\n#Test\nscore.append(nb.score(x_test,y_test)*100)\nalgorithms.append(\"Navie-Bayes\")\nprint(\"Navie Bayes accuracy =\",nb.score(x_test,y_test)*100)\n\n#Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ny_pred=nb.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Navie Bayes Confusion Matrix\")\nplt.show()","9db94834":"#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,random_state=1)\nrf.fit(x_train,y_train)\nscore.append(rf.score(x_test,y_test)*100)\nalgorithms.append(\"Random Forest\")\nprint(\"Random Forest accuracy =\",rf.score(x_test,y_test))\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=rf.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Random Forest Confusion Matrix\")\nplt.show()","89527e14":"#Support Vector Machine\nfrom sklearn.svm import SVC\nsvm=SVC(random_state=1)\nsvm.fit(x_train,y_train)\nscore.append(svm.score(x_test,y_test)*100)\nalgorithms.append(\"Support Vector Machine\")\nprint(\"svm test accuracy =\",svm.score(x_test,y_test)*100)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=svm.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Support Vector Machine Confusion Matrix\")\nplt.show()","30a14009":"#Decision Tree \nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"Decision Tree accuracy:\",dt.score(x_test,y_test)*100)\nscore.append(dt.score(x_test,y_test)*100)\nalgorithms.append(\"Decision Tree\")\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=dt.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Decision Tree Confusion Matrix\")\nplt.show()","4787893c":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nscore.append(lr.score(x_test,y_test)*100)\nalgorithms.append(\"Logistic Regression\")\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=lr.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.show()","5c17d33e":"trace1 = {\n  'x': algorithms,\n  'y': score,\n  'name': 'score',\n  'type': 'bar'\n}","1c5b83e3":"data = [trace1];\nlayout = {\n  'xaxis': {'title': 'Classification Algorithms'},\n  'title': 'Comparison of the accuracy of classification algorithms'\n};\nfig = go.Figure(data = data, layout = layout)\niplot(fig)\n","6a6ce622":"**8) I corrected some of the parameters.**","7b8dbe90":"**2) The \"id\" parameter will not work for classification, so I'm removing this parameter from the data set.**","19ac9a8f":"**CONCLUSION**\n\nIn this study, there were 400 samples and 26 parameters.However, some samples had no parameter values. For this reason, I prepared the data to use the classification algorithms. I did data visualization work.  I applied classification algorithms and compared success rates with each other.   It was a nice work for me. I hope you like it.\nIf you have questions and suggestions, you can comment. Because your questions and suggestions are very valuable for me.\n     ","d6eabd51":"**Data cleaning&Classification algorithms comparison**","da708ee6":"                       **How to Handle Missing Data,what did I do?**\n\nPandas provides the dropna() function that can be used to drop either columns or rows with missing data. We can use dropna() to remove all rows with missing data\nRemoving rows with missing values can be too limiting on some predictive modeling problems, an alternative is to impute missing values.\nImputing refers to using a model to replace missing values.\n\nThere are many options we could consider when replacing a missing value, for example:\n\n    A constant value that has meaning within the domain, such as 0, distinct from all other values.\n    A value from another randomly selected record.\n    A mean, median or mode value for the column.\n    A value estimated by another predictive model.    \n\nPandas provides the fillna() function for replacing missing values with a specific value.\n\nFor example, we can use fillna() to replace missing values with the mean value for each column,\nFor example; dataset.fillna(dataset.mean(), inplace=True)","f40c3e9f":"**CLASSIFICATION ALGORITHMS**","524561dd":"**12)The ordinal values to integers, we can use the pandas DataFrame method replace() to\"rbc\",\"pc\",\"pcc\",\"ba\",\"htn\",\"dm\",\"cad\",\"appet\",\"pe\" and \"ane\" to appropriate numeric values**","87895736":"**7) indexes are not sequential as you can see in the table above. I used the following code to sort indexes.**","859a9408":"\n**5)Sometimes instead of Nan in the data set, \"?\" value can be found. To solve this problem ; **df = data [(data! = '?'). all (axis = 1)]** can be used.**\n","3a2747f6":"**PREPARE DATA**\n\n**1) 3 unique values appear in the data set. However, there is no value called  \"ckd\\t. \"  I have written the following code to solve this problem.**","116d6575":"**11)display a sample row to get a better sense of how the values in each column are formatted.**","99a74597":"**3) I changed the target parameter values to 1 and 0 to be able to use the classification algorithms. If the value is \"ckd\" is 1, if not equal to 0. **","46b32063":"**10)Keep in mind, the goal in this section is to have all the columns as numeric columns (int or float data type), and containing no missing values. We just dealt with the missing values, so let's now find out the number of columns that are of the object data type and then move on to process them into numeric form.**","0ccf0892":"**4) I used the following code to find out how many \"NaN\" values in which parameter.**","32fe6844":"**In this study, chronic kidney disease was estimated using classification algorithms. You'll find this kernel;**\n            * The codes I think will be useful for data cleaning\n            * How to Handle Missing Data,what did I do?\n            * Data Visualization\n            *  Classification Algorithms\n                -KNN\n                -Navie-Bayes\n                -Logistic Regression\n                -Decision Tree\n                -Random Forest\n                -Support Vector Machine          \n            * Success rate of classification algorithms\n            * Conclusion\n\n","6ade9506":"**6) I can use dropna() to remove all rows with missing data**\n  \nThere were 25 parameters of 400 samples before writing this code. After writing this code, there are 25 parameters left in 158 examples.\n\nThe number of samples decreased but the reliability of the model increased.","443febc7":"**VISUALIZATION**","3176b786":"**9) I'll change the data types of some parameters **"}}