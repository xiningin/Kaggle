{"cell_type":{"5619650a":"code","33863188":"code","39605dd0":"code","8b546fdb":"code","9f333ff4":"code","770603ea":"code","619a253c":"code","37e43266":"code","ffda5123":"code","7078b789":"code","58a7ed96":"code","a8d4e745":"code","68ef4061":"code","07f40a92":"code","1750ba11":"code","b650c3ea":"markdown","00305ece":"markdown","08a43a60":"markdown","d0c79c9e":"markdown","c55ad40c":"markdown","29beeefe":"markdown","c0d1082a":"markdown","cd3a3f92":"markdown"},"source":{"5619650a":"import numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nfrom keras.models import Sequential\nimport matplotlib.pyplot as plt\nimport cv2\nfrom keras.preprocessing.image import img_to_array\n","33863188":"training = keras.preprocessing.image_dataset_from_directory(\n    '..\/input\/cat-and-dog\/training_set\/training_set\/',\n    validation_split=0.25,\n    seed=123,\n    interpolation='nearest',\n    subset='training',\n    batch_size=32,\n    image_size=(256,256)\n)","39605dd0":"validation_data = keras.preprocessing.image_dataset_from_directory(\n    '..\/input\/cat-and-dog\/test_set\/test_set\/',\n    validation_split=0.15,\n    seed=123,\n    interpolation='nearest',\n    subset='training',\n    batch_size=32,\n    image_size=(256,256)\n)","8b546fdb":"classes = training.class_names\nprint(classes)","9f333ff4":"\nplt.figure(figsize=(15,15))\nfor img, label in training.take(1):\n    for i in range(9):\n        ax = plt.subplot(3,3,i+1)\n        a = np.random.randint(1,30)\n        plt.imshow(img[a])\n        plt.title(classes[label[a]])\n        plt.axis('off')","770603ea":"model = tf.keras.models.Sequential()\nmodel.add(layers.Conv2D(filters = 64, kernel_size = (3,3), strides = 2, input_shape = (256,256,3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU())\n\nmodel.add(layers.Conv2D(filters = 64, kernel_size = (3,3), strides = 2, input_shape = (256,256,3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU())\n\nmodel.add(layers.Conv2D(filters = 128, kernel_size = (3,3), strides = 2, input_shape = (256,256,3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU())\n\nmodel.add(layers.Conv2D(filters = 128, kernel_size = (3,3), strides = 2, input_shape = (256,256,3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU())\n\nmodel.add(layers.Conv2D(filters = 256, kernel_size = (3,3), strides = 2, input_shape = (256,256,3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU())\n\nmodel.add(layers.Conv2D(filters = 256, kernel_size = (3,3), strides = 2, input_shape = (256,256,3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU())\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(32,activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n","619a253c":"def show_what_cnn_sees(path,layer, model):\n    img = cv2.imread(path,1)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(256,256))\n    img = img \/ 255.0\n    img_arr = []\n    img_arr.append(img_to_array(img))\n    img_arr = np.array(img_arr).reshape(1,256,256,3)\n    \n    inputs = model.inputs\n    output = model.layers[layer].output\n    \n    new_model = tf.keras.Model(inputs, output)\n    \n    featureMaps = new_model.predict(img_arr)\n    \n    ## Plotting Features\n    for maps in featureMaps:\n        plt.figure(figsize=(20,20))\n        pltNum = 1\n        \n        for a in range(7):\n            for b in range(7):\n                plt.subplot(7, 7, pltNum)\n                plt.imshow(maps[: ,: ,pltNum - 1], cmap='gray')\n                pltNum += 1\n        \n        plt.show()","37e43266":"path = '..\/input\/cat-and-dog\/test_set\/test_set\/cats\/cat.4018.jpg'\nshow_what_cnn_sees(path,3,model)","ffda5123":"show_what_cnn_sees(path,6,model)","7078b789":"show_what_cnn_sees(path,9,model)","58a7ed96":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])","a8d4e745":"model.fit(training, epochs = 5,validation_data = validation_data)","68ef4061":"show_what_cnn_sees(path,3,model)","07f40a92":"show_what_cnn_sees(path,6,model)","1750ba11":"show_what_cnn_sees(path,9,model)","b650c3ea":"## Model","00305ece":"## Before training","08a43a60":"## After training","d0c79c9e":"## Visualize data","c55ad40c":"## function to visualize what cnn sees","29beeefe":"# Thank you","c0d1082a":"## Import libraries","cd3a3f92":"## Load data"}}