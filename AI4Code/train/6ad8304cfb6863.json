{"cell_type":{"5a9a46bb":"code","c704dc05":"code","a3c51cf8":"code","1f35924d":"code","2522332a":"code","0a95949d":"code","f51d9c66":"code","dce1d591":"code","5c429d1f":"code","eb0159f0":"code","165c33de":"code","9937fc9c":"code","7b792211":"code","4e0ab2a2":"code","623d1920":"code","67accee3":"code","7ee51c2f":"code","f6369bfc":"code","5df57972":"code","2fc028fc":"code","57176a6f":"code","53c38649":"code","cfdb14e0":"code","6212050f":"code","bc6db328":"code","268002a4":"code","a915ff2a":"code","1ce0ac4d":"code","4107ebd5":"code","682814f3":"code","307c0245":"code","abc6ebf0":"code","d1f3e7da":"code","b5a21713":"code","d356c1f9":"code","cfe5e4a0":"code","e89e83bd":"code","a9d1feed":"code","89211d07":"code","a0b7c631":"code","f9c83cd5":"code","9d6126ab":"code","11e82965":"markdown","00f583f4":"markdown","3ebffaef":"markdown","27587276":"markdown","164093ac":"markdown","bb57e434":"markdown","4accb32a":"markdown","6e387f3d":"markdown","b57d5329":"markdown","2cce667d":"markdown","5929705c":"markdown","25ac87ba":"markdown","7ddb6ba2":"markdown","a40b2f8a":"markdown","4b95bdd5":"markdown","9f09580e":"markdown","a9f7c654":"markdown","ce785b2e":"markdown","4e3ba627":"markdown","6d1eaa02":"markdown","7b33b38d":"markdown","d225ba0d":"markdown","d5ee693a":"markdown","6ca735fc":"markdown","9c4fb631":"markdown","f7fce12c":"markdown","c7bc3977":"markdown","194a5415":"markdown","2d0d6ad5":"markdown","f0214275":"markdown","11f40332":"markdown","2959f132":"markdown"},"source":{"5a9a46bb":"import numpy as np\nimport pandas as pd\nfrom time import strftime\nimport itertools\nimport random\nimport os\nfrom os import walk\nfrom os.path import join","c704dc05":"import matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.image as mpimg\nimport cv2","a3c51cf8":"import tensorflow as tf\nfrom keras.layers import Dense, Activation, Dropout","1f35924d":"METADATA_PATH = '\/kaggle\/input\/brian-tumor-dataset\/metadata.csv'\nTUMOR_IMG_PATH = '\/kaggle\/input\/brian-tumor-dataset\/Brain Tumor Data Set\/Brain Tumor Data Set\/Brain Tumor\/'\nHEALTHY_IMG_PATH = '\/kaggle\/input\/brian-tumor-dataset\/Brain Tumor Data Set\/Brain Tumor Data Set\/Healthy\/'\nDF_DIR_PATH = '\/kaggle\/working\/brian-tumor-dataset\/'\nDF_PATH = DF_DIR_PATH + 'preprocessed-dataset.csv'\nMODEL_PATH = DF_DIR_PATH + 'pretrained-model.csv'\nCLASS_TUMOR = 1\nCLASS_HEALTHY = 0\nNOISE_FLOOR = 10;\nWHITE_THRE = 30;\nBLACK_THRE = 1;\nTARGET_SIZE = 64;\nTOTAL_INPUTS = TARGET_SIZE ** 2","2522332a":"metadata_df = pd.read_csv(METADATA_PATH)\nmetadata_df.shape","0a95949d":"metadata_df.head()","f51d9c66":"fig = plt.figure(figsize=[10, 10])\nfig.patch.set_facecolor('white')\nfor i in range(1, 7):\n    plt.subplot(2, 3, i)\n    rand_pick = random.randint(1, 1000)\n    random_img = str(rand_pick)\n    img = mpimg.imread(TUMOR_IMG_PATH + 'Cancer (' + random_img + ').jpg')\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel('Cancer (' + str(random_img) + ').jpg', fontsize=14)\nfig.subplots_adjust(hspace=-0.4)\nplt.show()","dce1d591":"EXAMPLE_IMG_PATH = HEALTHY_IMG_PATH + \"Not Cancer  (122).jpg\"\nimg1 = mpimg.imread(EXAMPLE_IMG_PATH)\nfig = plt.figure(figsize=[6,6])\nfig.patch.set_facecolor('black')\nplt.imshow(img1)\nplt.xticks([])\nplt.yticks([])\nplt.show()","5c429d1f":"def rgb2gray(rgb_img):\n    if rgb_img.ndim == 3:\n        R, G, B = rgb_img[:, :, 0], rgb_img[:, :, 1], rgb_img[:, :, 2]\n        output_img = 0.2989 * R + 0.5870 * G + 0.1140 * B\n    else:\n       output_img = rgb_img\n    if output_img.max() < 1.1:\n        output_img = output_img * 255\n    return output_img","eb0159f0":"print(f\"Shape before: {img1.shape}\")","165c33de":"img2 = rgb2gray(img1)\nprint(f\"Shape after: {img2.shape}\")","9937fc9c":"def reduce_noise(noisy_img, noise_floor=NOISE_FLOOR):\n    clean_img = np.array(noisy_img)\n    clean_img[noisy_img < NOISE_FLOOR] = 0\n    return clean_img","7b792211":"print(\"Top left corner before:\")\nimg2[:6, :6]","4e0ab2a2":"img3 = reduce_noise(img2)\nprint(\"Top left corner after:\")\nimg3[:6, :6]","623d1920":"def remove_white_borders(img_with_borders, threshold=WHITE_THRE):\n    # left\n    if np.mean(img_with_borders[:, 0]) > threshold:\n        img_with_borders[:, 0] = 0;\n    # right\n    if np.mean(img_with_borders[:, -1]) > threshold:\n        img_with_borders[:, -1] = 0;\n    # top\n    if np.mean(img_with_borders[0, :]) > threshold:\n        img_with_borders[0, :] = 0;\n    # bottom\n    if np.mean(img_with_borders[-1, :]) > threshold:\n        img_with_borders[-1, :] = 0;\n    return img_with_borders","67accee3":"print(\"Top left corner before:\")\nimg3[:6, :6]","7ee51c2f":"img4 = remove_white_borders(img3)\nprint(\"Top left corner after:\")\nimg4[:6, :6]","f6369bfc":"def remove_black_padding(img_with_padding, threshold=BLACK_THRE):\n    # Sides\n    vert_mean = np.mean(img_with_padding, axis=0)\n    vert_map = vert_mean > BLACK_THRE;\n    vert_matches = np.where(vert_map == True)\n    img_no_padding = img_with_padding[:, vert_matches[0][0]:vert_matches[0][-1]]\n    # Top & bottom\n    hori_mean = np.mean(img_no_padding, axis=1)\n    hori_map = hori_mean > BLACK_THRE;\n    hori_matches = np.where(hori_map == True)\n    img_no_padding = img_no_padding[hori_matches[0][0]:hori_matches[0][-1], :]\n    return img_no_padding","5df57972":"fig = plt.figure(figsize=[4, 4])\nfig.patch.set_facecolor('white')\nplt.title(\"Image before\")\nplt.imshow(img4, cmap=\"gray\")\nplt.show()","2fc028fc":"img5 = remove_black_padding(img4)\nfig = plt.figure(figsize=[4, 4])\nfig.patch.set_facecolor('white')\nplt.title(\"Image after\")\nplt.imshow(img5, cmap=\"gray\")\nplt.show()","57176a6f":"def square_image(rect_img):\n    X, Y = rect_img.shape\n    if X != Y:\n        if X > Y:\n            out_img = np.zeros((X, X))\n            offset = int(np.ceil((X-Y) \/ 2))\n            out_img[:, offset : (offset + Y)] = rect_img\n        else:\n            out_img = np.zeros((Y, Y))\n            offset = int(np.ceil((Y-X) \/ 2))\n            out_img[offset : (offset + X), :] = rect_img\n        return out_img\n    else:\n        return rect_img","53c38649":"img6 = square_image(img5)\nfig = plt.figure(figsize=[4, 4])\nfig.patch.set_facecolor('white')\nplt.imshow(img6, cmap=\"gray\")\nplt.title(f\"Squared image side={img6.shape[0]}\")\nplt.show()","cfdb14e0":"img7 = cv2.resize(img6, dsize=(TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_CUBIC)","6212050f":"fig = plt.figure(figsize=[4, 4])\nfig.patch.set_facecolor('white')\nplt.imshow(img7, cmap=\"gray\")\nplt.title(f\"Squared image side={img7.shape[0]}\")\nplt.show()","bc6db328":"fig = plt.figure(figsize=[10, 10])\nfig.patch.set_facecolor('white')\nplt.subplot(2, 3, 1)\nplt.imshow(img1)\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(\"Original\")\n\nplt.subplot(2, 3, 2)\nplt.imshow(img2, cmap=\"gray\")\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(\"Grayscale\")\n\nplt.subplot(2, 3, 3)\nplt.imshow(img3, cmap=\"gray\")\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(\"Black floor\")\n\nplt.subplot(2, 3, 4)\nplt.imshow(img5, cmap=\"gray\")\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(\"Lines and padding removed\")\n\nplt.subplot(2, 3, 5)\nplt.imshow(img6, cmap=\"gray\")\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(\"Square shaped\")\n\nplt.subplot(2, 3, 6)\nplt.imshow(img7, cmap=\"gray\")\nplt.xlabel(\"Resized\")\nplt.xticks([])\nplt.yticks([])\nfig.subplots_adjust(hspace=-0.4)\nplt.show()","268002a4":"def preprocesss_img(input_img):\n    output_img = rgb2gray(input_img)\n    output_img = reduce_noise(output_img)\n    output_img = remove_white_borders(output_img)\n    output_img = remove_black_padding(output_img)\n    output_img = square_image(output_img)\n    output_img = cv2.resize(output_img, dsize=(TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR)\n    return output_img","a915ff2a":"features = np.zeros((1, TARGET_SIZE ** 2))\ntarget = np.array([])\ndebug=1\n# Fill with tumor images\nfor root, dirnames, filenames in walk(TUMOR_IMG_PATH):\n    n_total = len(filenames)\n    n = 1\n    for filename in filenames:\n        file_path = join(root, filename)\n        if n % 200 == 0:\n            print(f\"File {n}\/{n_total}\\t {filename}\")\n        n = n + 1        \n        img = mpimg.imread(file_path)\n        img = preprocesss_img(img)\n        features = np.vstack([features, np.reshape(img, -1)])\n        target = np.append(target, CLASS_TUMOR)\nfeatures = features[1:, :] # Removing the first zeros row","1ce0ac4d":"# Fill with healthy images\nfor root, dirnames, filenames in walk(HEALTHY_IMG_PATH):\n    n_total = len(filenames)\n    n = 1\n    for filename in filenames:\n        file_path = join(root, filename)\n        if n % 200 == 0:\n            print(f\"File {n}\/{n_total}\\t {filename}\")\n        n = n + 1        \n        img = mpimg.imread(file_path)\n        img = preprocesss_img(img)\n        features = np.vstack([features, np.reshape(img, -1)])\n        target = np.append(target, CLASS_HEALTHY)","4107ebd5":"df = pd.DataFrame(features)\ndf[\"target\"] = target","682814f3":"# os.mkdir(DF_DIR_PATH)\n# df.to_csv(DF_PATH)","307c0245":"# df.to_csv(DF_PATH)\n# df = df.loc[:, ~df.columns.str.contains('^Unnamed')]","abc6ebf0":"average_healthy_brain = np.array(df[df[\"target\"] == CLASS_HEALTHY].drop(\"target\", axis=1).mean()).reshape(64,64)\naverage_tumor_brain = np.array(df[df[\"target\"] == CLASS_TUMOR].drop(\"target\", axis=1).mean()).reshape(64,64)\nplt.figure(figsize=[8, 4])\nplt.subplot(1,2,1)\nplt.imshow(average_healthy_brain)\nplt.title('Average healthy brain')\nplt.subplot(1,2,2)\nplt.imshow(average_tumor_brain)\nplt.title('Average tumor brain')\nplt.show()","d1f3e7da":"target = df[\"target\"]\nfeatures = df.drop(\"target\", axis=1)\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.1, random_state=10)","b5a21713":"# validation dataset\nVALIDATION_SIZE = 600\nx_val = x_train[:VALIDATION_SIZE]\ny_val = y_train[:VALIDATION_SIZE]","d356c1f9":"model_1 = Sequential([\n    Dense(units=256, input_dim=TOTAL_INPUTS, activation='relu', name='m1_hidden1'),\n    Dense(units=128, activation='relu', name='m1_hidden2'),\n    Dense(units=64, activation='relu', name='m1_hidden3'),\n    Dense(units=16, activation='relu', name='m1_hidden4'),\n    Dense(units=2, activation='softmax', name='m1_output')\n])\nasdf = tf.keras.optimizers.Adam\nmodel_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","cfe5e4a0":"model_1.summary()","e89e83bd":"samples_per_batch = 100\nn_epochs = 60\n# logdir = LOG_DIR + f\"model_1 at {strftime('%H:%M:%S')}\"\n# tb_train_dir = logdir + \"\/train\"","a9d1feed":"model_1.fit(x=x_train, y=y_train, \n            epochs=n_epochs, \n            batch_size=samples_per_batch, \n            validation_data=(x_val, y_val))","89211d07":"test_loss, test_accuracy = model_1.evaluate(x_test, y_test)\nprint(f\"loss: {test_loss:0.3}\\t accuracy: {test_accuracy:0.1%}\")","a0b7c631":"# predict the test dataset\npredict_x_test = model_1.predict(x_test) \nclasses_x_test = np.argmax(predict_x_test, axis=1)\nconf_matrix = confusion_matrix(y_true=y_test, y_pred=classes_x_test)","f9c83cd5":"plt.figure(figsize=[4, 4])\nplt.imshow(conf_matrix, cmap=\"bone_r\")\nplt.title(\"Confusion matrix\", fontsize=16)\nplt.ylabel(\"Actual classes\")\nplt.xlabel(\"Predicted classes\")\ntick_marks = np.arange(2)\nplt.yticks(tick_marks, [0,1])\nplt.xticks(tick_marks, [0,1])\nplt.colorbar()\nfor i, j in itertools.product(range(2), range(2)):\n    plt.text(j, i, conf_matrix[i][j], horizontalalignment=\"center\", \n             color=\"white\" if conf_matrix[i][j] > 100 else \"black\")\nplt.show()","9d6126ab":"model_1.save(\"MODEL_PATH\")","11e82965":"# Model creation","00f583f4":"This notebook is not so much about the machine learning process, which uses a simple perceptron model, but about how to process and normalize a rather irregular image dataset. The source images are present in different sizes, noise levels and other characteristics, and homogeneizing them not only reduces the storage size and training time, using a much simpler model, but also might be capable of improving the accuracy results.\n\nThe source dataset for this notebook can be downloaded on Kaggle [Brain Tumor Dataset](https:\/\/www.kaggle.com\/preetviradiya\/brian-tumor-dataset), compiled by [Preed Vidariya](https:\/\/www.kaggle.com\/preetviradiya)","3ebffaef":"### ML libraries","27587276":"# Analyze results","164093ac":"# Saving and loading the pre processed dataset","bb57e434":"### Visualization and image processing libraries","4accb32a":"The chosen image is \"Not Cancer  (122).jpg\" from the healthy folder since it has all of the irregularities\n1. It's an RGB image.\n2. There are irregularities in the levels of black around the background.\n3. It has a white border on the left and top sides.\n4. It has black padding, particularly on the sides.\n5. The image is not a square.\n6. The dimensions are not a power of two","6e387f3d":"### Basic libraries","b57d5329":"# Detecting brain tumor diagnosis from X-ray data","2cce667d":"# Conclussions","5929705c":"### Explore single images","25ac87ba":"## Batching the whole dataset into a pandas dataframe","7ddb6ba2":"# Preprocessing and cleaning the dataset","a40b2f8a":"### 5) Make the image a square","4b95bdd5":"### 2) level the depth of black","9f09580e":"# Pixel data exploration","a9f7c654":"The brain scans display a number of variants that can be easily addresssed and fixed:\n 1. There are images saved as RGB (three channels) while just grayscale (one channel) is necessary\n 2. The black background has some random noise\n The levels of black differ from one image to other\n 3. Some images have white lines in the borders\n 4. Some images have a big black background as padding, all those pixels contain no relevant information\n 5. Some images are rectangles, other are squares\n 6. The dimensions vary depending on the image","ce785b2e":"# Training","4e3ba627":"# Exploring the dataset","6d1eaa02":"We can fix those irregularities by:\n 1. Converting all the images to grayscale.\n 2. Converting the random noise in the black background to pure black.\n 3. Detecting the presence of white borders and remove them by turning them to black.\n 4. Reduce the dimensions of the image to completely remove the black background padding\n 5. Making all the images rectangles without stretching the original image.\n 6. Turn the dimension to a power of 2, as most TPU used for training will work faster on those dimensions.","7b33b38d":"The dataset comes with two .csv files with the description of the contents. We'll focus on the complete one \"metadata.csv\"","d225ba0d":"## Testing the preprocesss sequence on a single image","d5ee693a":"### Explore the dataset as whole","6ca735fc":"### 4) Remove black padding","9c4fb631":"### Constants and paths","f7fce12c":"Normalizing all the dataset images can greatly simplify the required model to make a classifier. \nBy removing all the unnecessary information from the images and modifying it to adapt to the target acceleration engine the training time is greatly shortened, delivering results in just a few epoches.","c7bc3977":"# Importing the data and libraries","194a5415":"### 3) Remove white borders","2d0d6ad5":"Just in these first five entries we can see the disparity in the files. The images come in different formats (jpeg, png, tif), and in different modes (rgb, monochrome), in different resolutions, some of them are squares but others are irregular rectangles.","f0214275":"### 1) Turn image into grayscale","11f40332":"### 6) Redimension to a power of two","2959f132":"## Preprocessing summary"}}