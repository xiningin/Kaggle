{"cell_type":{"375a3375":"code","b7f35519":"code","17b7b8bd":"code","c47d0ea1":"code","6433838c":"code","60ecd0fe":"code","d6020115":"code","9efb285f":"code","be93f51a":"code","f19a5654":"code","f654a38c":"code","2c8040b7":"code","ac666742":"code","d3be210c":"code","c15eca76":"code","37b61b91":"code","8a513532":"code","4e53768c":"code","9d7c3a28":"markdown","6e7566e9":"markdown","d790cd66":"markdown","bd2f7a38":"markdown","9bde64c7":"markdown","60553f15":"markdown","13c24eb1":"markdown","678d11b7":"markdown"},"source":{"375a3375":"# Data science libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport itertools\n\n#Scikit-learn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve,auc\n\n#Keras API Tensorflow 2 libraries\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, LeakyReLU\nfrom keras.layers.noise import AlphaDropout\nfrom keras.optimizers import Adam\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\nfrom keras.utils.np_utils import to_categorical\nprint('Tensorflow version:', tf.__version__)","b7f35519":"#loading dataset\ndf = np.load('..\/input\/sheri-facial-recognition\/ORL_faces.npz') \ndf.files","17b7b8bd":"# Loading train and test dataset (data is already split into these datasets)\nx_train = df['trainX']\ny_train = df['trainY']\nx_test = df['testX']\ny_test = df['testY']","c47d0ea1":"# understanding the shape of input features\nx_train.shape","6433838c":"# image pixel value are in the range 0-255, need to be normalised\nx_train[40].max()","60ecd0fe":"# Normalizing each image as each image is between 0-255 pixels\nx_train = x_train.astype(np.float32) \/ 255.0\nx_test = x_test.astype(np.float32) \/ 255.0\nprint('Training dataset shape: ',x_train.shape)\nprint('Testing dataset shape: ',x_test.shape)","d6020115":"x_train, x_valid, y_train, y_valid = train_test_split(x_train,y_train,test_size=0.1,random_state=42)","9efb285f":"# Shape of image definition\nrows = 112\ncolumns = 92\nimage_shape = (rows,columns,1)","be93f51a":"# Reshape function\nx_train = x_train.reshape(x_train.shape[0],*image_shape)\nx_test = x_test.reshape(x_test.shape[0],*image_shape)\nx_valid = x_valid.reshape(x_valid.shape[0],*image_shape)\n\nprint('Training dataset modified shape: ',x_train.shape)\nprint('Testing dataset modified shape: ',x_test.shape)\nprint('Validating dataset modified shape: ',x_valid.shape)\n","f19a5654":"# We will initialize our cnn model with activation function, dropout rate, optimizer\ndef cnn_model(activation,dropout_rate,optimizer):\n    \n    model = Sequential() #initialize Sequential model\n \n    if(activation == 'selu'):\n        model.add(Conv2D(32, kernel_size=3,activation=activation, input_shape=image_shape, kernel_initializer='lecun_normal')) \n        #32 filter with kernel size of 3 with input shape\n        model.add(MaxPooling2D(pool_size=2))\n        \n        model.add(Conv2D(64, 3, activation=activation, kernel_initializer='lecun_normal')) #64 filter with kernel size of 3 x 3\n        model.add(MaxPooling2D(pool_size=2)) #Max pool with size of 2\n        \n        model.add(Flatten()) \n        model.add(Dense(2024, activation=activation, kernel_initializer='lecun_normal'))\n        model.add(AlphaDropout(0.5))\n        \n        model.add(Dense(1024, activation=activation, kernel_initializer='lecun_normal'))\n        model.add(AlphaDropout(0.5))\n        \n        model.add(Dense(512, activation=activation, kernel_initializer='lecun_normal'))\n        model.add(AlphaDropout(0.5))\n        \n        model.add(Dense(20, activation='softmax')) #Output layer\n    else:\n        model.add(Conv2D(32, kernel_size=3,activation=activation, input_shape=image_shape))\n        \n        #32 filter with kernel size of 3 x 3 with input shape\n        model.add(MaxPooling2D(pool_size=2)) \n        \n        model.add(Conv2D(64,3, activation=activation)) #64 filter with kernel size of 3 x 3\n        model.add(MaxPooling2D(pool_size=2)) #Max pool with size of 2\n        \n        model.add(Flatten())\n        \n        model.add(Dense(2024, activation=activation))\n        model.add(Dropout(0.5))\n        \n        model.add(Dense(1024, activation=activation))\n        model.add(Dropout(0.5))\n        \n        model.add(Dense(512, activation=activation))\n        model.add(Dropout(0.5))\n        \n        model.add(Dense(20, activation='softmax')) #Output layer\n    \n    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n    \n    #compile model with loss, optimizer chosen and accuracy as metrics\n    \n    return model","f654a38c":"#For Leaky-Rely function we need to define aplha parameters using get_custom_objects\nget_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})\n\n# Defining the type of activation functions to be tested\nactivation_function = ['relu', 'elu', 'leaky-relu', 'selu']\n","2c8040b7":"activation_results = [] #creating an empty matrix for storing results for activations\nfor activation in activation_function:\n    print('\\nTraining with {0} activation function\\n'.format(activation))\n\n    model = cnn_model(activation=activation,dropout_rate=0.2,optimizer=Adam(clipvalue=0.5)) \n    #using 'adam' optimizer with clipvalue\n\n    history = model.fit(np.array(x_train), np.array(y_train),batch_size=256,epochs=100,verbose=1,validation_data=(np.array(x_valid),np.array(y_valid)))\n\n    activation_results.append(history) #store results\n\n    K.clear_session()\n    del model\n    print(activation_results)","ac666742":"# Plot the Model accuracy and Model loss for each activation function used above\n# Just to make sure, we don't change the above data, so we store it in new matrix\n\nactivation_list = activation_function[0:]\nresults_new = activation_results[0:]\n\ndef plot_results(activation_results,activation_functions_new =[]):\n    \n    plt.figure(figsize=(8,6))\n    \n    # Model accuracy values plot\n    for activation_function in activation_results:\n        plt.plot(activation_function.history['val_accuracy'])\n    \n    plt.title('Model accuracy')\n    plt.ylabel('Test Accuracy')\n    plt.xlabel('No. of Epochs')\n    plt.legend(activation_functions_new)\n    plt.grid()\n    plt.show()\n    \n    # Model loss values plot\n    \n    plt.figure(figsize=(8,6))\n    \n    for activation_function in activation_results:\n        plt.plot(activation_function.history['val_loss'])\n    \n    plt.title('Model Loss')\n    plt.ylabel('Test Loss')\n    plt.xlabel('No. of Epochs')\n    plt.legend(activation_functions_new)\n    plt.grid()\n    plt.show()","d3be210c":"plot_results(results_new, activation_list)","c15eca76":"activation_func_final ='leaky-relu'\n\nmodel_final = cnn_model(activation=activation_func_final,\n                    dropout_rate=0.2,\n                    optimizer=Adam(clipvalue=0.5)) #using 'adam' optimizer with clipvalue of 0.5\n\nhistory_final = model_final.fit(np.array(x_train), np.array(y_train), \n                    batch_size=256,\n                    epochs=100, \n                    verbose=1,\n                    validation_data=(np.array(x_valid),np.array(y_valid)))","37b61b91":"# Data in history\n\nprint(history_final.history.keys())\n\n# Plotting Accuracy for final model\nplt.plot(history_final.history['accuracy'])\nplt.plot(history_final.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel(' No. of Epochs')\nplt.legend(['Train', 'Valid'])\nplt.grid()\nplt.show()\n\n# Plotting Loss for Final Model\nplt.plot(history_final.history['loss'])\nplt.plot(history_final.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('No. of Epochs')\nplt.legend(['Train', 'Valid'])\nplt.grid()\nplt.show()","8a513532":"result_score = model_final.evaluate(np.array(x_test),np.array(y_test),verbose=0)\n\nprint('Test Loss {:.4f}'.format(result_score[0]))\nprint('Test Accuracy {:.4f}'.format(result_score[1]))","4e53768c":"# Save the model for future pridiction\nmodel_final.save('\/kaggle\/working\/Sheri_face_detection_model')","9d7c3a28":"**Step #4 : Transform the images into equal sizes to input into the CNN model**\n\nWhen we feed images in CNN the size of each image must be same.\nWe will define the shape of image in terms of rows, columns\nTo make equal size of all images (train, test, and valid dataset), we will use Reshape function","6e7566e9":"**Step #2 : Load the Dataset and Preprocess the data**","d790cd66":"**Step #1 : Import the Libraries**","bd2f7a38":"**Step 5: Building a CNN Model that has 3 main layers**\n\n1. Convolutional Layer\n2. Pooling Layer\n3. Fully Connected Layer\n","9bde64c7":"<h4> Conclusion <\/h4>\n\nHere in this project we analyzed ORL faces images (train and test sets were given). \n\nWe used CNN method to build the model and train it.\n\nThe analysis for different activation functions is fisrt observed to find that 'leaky-relu' activation function is one of the activation functions that can be used for the final model\n\nThe model training is done using x_train and y_train with validation data as x_valid and y_valid. owever for evaluating model, we use x_test and y_test which gives us <b> loss ~0.3173 <\/b>  with <b> an accuracy of 93.75% <\/b>","60553f15":"**Step #3 : Split the dataset**\n\nSplit is done from Xtrain dataset into x_train and x_valid dataset\nHere we considered only 10 % of the training dataset as validation dataset as number of images overall is very low (240)\n\nHere point to note is that normalising is done prior to split. \nIn traditonal sklearn ML algorithem we use fit_transform for train data and just transform with test data when using standardscaller or normalizer\n\nBut here normalisation is just dividing by 255 the maximum value, so it won't effect doing it before spliting or after splitting","13c24eb1":"## Project: Perform Facial Recognition with Deep Learning in Keras Using CNN\n#### Submitted by: Sheri Prashanth Reddy\n\n\n\n**Problem Statement:**\n    \nFacial recognition is a biometric alternative that measures unique characteristics of a human face. Applications available today include flight check in, tagging friends and family members in photos, and \u201ctailored\u201d advertising. You are a computer vision engineer who needs to develop a face recognition programme with deep convolutional neural networks.\n\n\n**Objective:**\nUse a deep convolutional neural network to perform facial recognition using Keras.\n\n**Dataset Details:**\nORL face database composed of 400 images of size 112 x 92. There are 40 people, 10 images per person. The images were taken at different times, lighting and facial expressions. The faces are in an upright position in frontal view, with a slight left-right rotation.\n\n**Steps to be followed:**\n1. Input the required libraries\n2. Load the dataset after loading the dataset, you have to normalize every image.\n3. Split the dataset\n4. Transform the images to equal sizes to feed in CNN\n5. Build a CNN model that has 3 main layers: \n    i. Convolutional Layer \n    ii. Pooling Layer \n    iii. Fully Connected\n\nLayer\n6. Train the model\n7. Plot the result\n8. Iterate the model until the accuracy is above 90%\n","678d11b7":"Here it is seen that <b>'leaky-relu'<\/b> and <b>'relu'<\/b> both perform well with minimum loss at lower epochs as compared to other activation functions\n\nLooking at the plots above all activation functions converge with minimum loss and high accuracy at training and validation set but <b>'leaky-relu' <\/b> is able to converge for higher accuracy at lower epochs with minimum loss, so we choose <b> 'leaky-relu' <\/b> for final model training and plotting results."}}