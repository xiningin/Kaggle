{"cell_type":{"a9e33969":"code","0f062158":"code","81cfa806":"code","6d4eed08":"code","391948f7":"code","3d198701":"code","02a2bf91":"code","baa231c2":"code","965fbcea":"code","143c1ba0":"code","8c0f1238":"code","9d7f163b":"code","4537f48f":"code","d0f5ccf9":"code","bb19eefc":"code","6c616c06":"code","59c84712":"code","f409b815":"code","d09abdfa":"code","93163b06":"code","8407b870":"code","993b6f3e":"code","b6bb7e26":"code","9dcb986b":"code","f66aa251":"code","88af9b6e":"code","462b4b95":"code","16a1817e":"code","22646fb2":"code","ca554d1c":"code","bb6f0897":"code","81dc2923":"code","8ea0f1b1":"code","9e2200ea":"code","f4592d1c":"code","195037e7":"code","0864f2c6":"code","59f8eae4":"code","4a3bea96":"code","ad27693c":"code","f3a61b2b":"code","7541833c":"code","0af0dc58":"code","3d4e4529":"code","63196540":"code","c313ec42":"code","174b396f":"code","8f4da363":"code","45f9ea25":"code","afb9ab8b":"code","8b6c1a11":"code","409502b7":"code","4acd59a4":"code","a6628440":"code","a049ccfa":"code","ac1e4ce0":"code","bf519bee":"code","57bff11d":"code","28e8cdbd":"code","7c684ab2":"code","f8d5c1fc":"code","14a73933":"code","7e0f7bcb":"code","527c67bb":"code","c9ffd950":"code","4682cb90":"code","976e34f7":"markdown","297334c8":"markdown","b3091bfd":"markdown","f76dd07a":"markdown","66c70ae0":"markdown","8644e344":"markdown","745ac3bb":"markdown"},"source":{"a9e33969":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f062158":"# importing dataset: training data\ntrain_df=pd.read_csv('..\/input\/churn-risk-rate-hackerearth-ml\/train.csv',index_col='customer_id', na_values=['?'])\nlen(train_df.columns) #25 columns in total","81cfa806":"# dataframe view\ntrain_df.head()\ntrain_df.shape","6d4eed08":"train_df.churn_risk_score.value_counts()\n# we should get rid the extra category (-1)","391948f7":"train_df.drop(train_df[ train_df['churn_risk_score'] == -1 ].index, axis=0, inplace=True)","3d198701":"train_df.columns","02a2bf91":"# separating X and y\nfrom sklearn.model_selection import train_test_split\n\nX = train_df.copy()\ny = train_df.churn_risk_score\ntrain_df.drop(['churn_risk_score'], axis = 1,inplace=True)\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_df, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=1)","baa231c2":"# importing dataset: test data\ntest_df=pd.read_csv('..\/input\/churn-risk-rate-hackerearth-ml\/test.csv',index_col='customer_id', na_values=['?'])\ntest_df.head()\ntest_df.columns","965fbcea":"test_df.shape","143c1ba0":"# dropping unnecessary columns\n# Name - as it does not specify or contribute anything special in predicting churn_risk_score\n\n# variable: referral_id\ntrain_df.referral_id.dtype # dtype('O')\ntrain_df.referral_id.unique() # variaous random numbers \nlen(train_df.referral_id.unique()) #11359 - unique values\n\n\n# variable: security_no\nlen(train_df['security_no'].unique()) # 36992 - all unique values\n\n# I think this unique (almost unique) IDs will contribute less to the predictions, so will drop\nX_train.drop(['referral_id', 'security_no', 'Name'], axis=1, inplace=True)\nX_valid.drop(['referral_id', 'security_no', 'Name'], axis=1, inplace=True)\ntest_df.drop(['referral_id', 'security_no', 'Name'], axis=1, inplace=True)","8c0f1238":"X_train.info()","9d7f163b":"%matplotlib inline\nsns.displot(X['age'], kde=True, bins=50) #seaborn histogram\n# age seems to be uniformly distributed","4537f48f":"sns.boxplot(y=X['age'], hue=X['churn_risk_score']) \n# no outliers at all\n# median age of customers around 40","d0f5ccf9":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nsns.regplot(x='churn_risk_score',y='age', scatter=True, fit_reg=True, data=X, ax=ax)#seaborn scatter plot\nax.set(xlim=(0,6)) #to change axis ranges\nax.set_ylim(0,80)\nplt.show()","bb19eefc":"sns.countplot(x='churn_risk_score', hue='gender', data=X, palette=\"tab10\")\n# there is some variations over gender counts: thus gender is important variable","6c616c06":"sns.countplot(x='churn_risk_score', hue='region_category', data=X, palette=\"tab10\")\n# there is some variations over region_category counts: thus region_category is important variable","59c84712":"sns.countplot(x='churn_risk_score', hue='membership_category', data=X, palette=\"tab10\")\n# there is some variations over membership_category counts: thus membership_category is important variable","f409b815":"sns.countplot(x='churn_risk_score', hue='joined_through_referral', data=X, palette=\"tab10\")\n# there is some variations over joined_through_referral counts: thus joined_through_referral is important variable\n# we have replaced '?' as na_value: cause ? might be representing not_known","d09abdfa":"sns.countplot(x='churn_risk_score', hue='preferred_offer_types', data=X, palette=\"tab10\")\n# there is some variations over preferred_offer_types counts: thus preferred_offer_types is important variable","93163b06":"sns.countplot(x='churn_risk_score', hue='medium_of_operation', data=X, palette=\"tab10\")\n# there is some variations over medium_of_operation counts: thus medium_of_operation is important variable","8407b870":"sns.countplot(x='churn_risk_score', hue='internet_option', data=X, palette=\"tab10\")\n# there is some variations over internet_option counts: thus internet_option is important variable","993b6f3e":"#days_since_last_login: int64  \nsns.displot(X['days_since_last_login'], kde=True, bins=5) #seaborn histogram\n#seems like there are negative values","b6bb7e26":"sns.boxplot(y=X['days_since_last_login'], hue=X['churn_risk_score'])\n# the outlier is -999","9dcb986b":"X['days_since_last_login'].value_counts().sort_values()\n# I think negative values will give wrong indication.","f66aa251":"# replacing - with median, but with a trick\ndays_since_last_login__df = X[['days_since_last_login']]\ntype(days_since_last_login__df) #pandas.core.frame.DataFrame\n\n\ndays_since_last_login__df.drop(days_since_last_login__df[ days_since_last_login__df['days_since_last_login'] == -999 ].index, axis=0, inplace=True)\nreplace_median = np.median(days_since_last_login__df['days_since_last_login'])\n\n# replacing with median\nX_train['days_since_last_login'].replace(-999, int(replace_median), inplace=True)\nX_valid['days_since_last_login'].replace(-999, int(replace_median), inplace=True)\nX['days_since_last_login'].replace(-999, int(replace_median), inplace=True)","88af9b6e":"test_df['days_since_last_login'].unique()","462b4b95":"# if test data also holds negative value\ntest_df['days_since_last_login'] = test_df['days_since_last_login'] .apply(lambda x : x if x > 0 else replace_median)","16a1817e":"sns.boxplot(y=X['days_since_last_login'], hue=X['churn_risk_score'])\n# the outlier is -999: which has been removed with median","22646fb2":"#avg_time_spent: float64  \nsns.displot(X['avg_time_spent'], kde=True, bins=5) #seaborn histogram","ca554d1c":"sns.boxplot(y=X['avg_time_spent'], hue=X['churn_risk_score'])\n# a lot of outliers: mainly because of negative values","bb6f0897":"\n# will replace the negative avg time spent with abs values\nX_train['avg_time_spent'] = X_train['avg_time_spent'].apply(lambda x : x if x > 0 else abs(x))\nX_valid['avg_time_spent'] = X_valid['avg_time_spent'].apply(lambda x : x if x > 0 else abs(x))\nX['avg_time_spent'] = X['avg_time_spent'].apply(lambda x : x if x > 0 else abs(x))\ntest_df['avg_time_spent'] = test_df['avg_time_spent'].apply(lambda x : x if x > 0 else abs(x))","81dc2923":"sns.boxplot(y=X['avg_time_spent'], hue=X['churn_risk_score'])\n# a lot of outliers still but this are positive values and will have a positive impact on target variable","8ea0f1b1":"fig, ax = plt.subplots()\nsns.regplot(x='churn_risk_score',y='avg_time_spent', scatter=True, fit_reg=False, data=X, ax=ax)#seaborn scatter plot\nax.set(xlim=(0,6)) #to change axis ranges\nax.set_ylim(0,3200)\nplt.show()\n\n# avg_time_spent has some effect on target variable","9e2200ea":"sns.displot(X['avg_transaction_value'], kde=True, bins=5) #seaborn histogram","f4592d1c":"sns.boxplot(y=X['avg_transaction_value'], hue=X['churn_risk_score'])","195037e7":"fig, ax = plt.subplots()\nsns.regplot(x='churn_risk_score',y='avg_transaction_value', scatter=True, fit_reg=False, data=X, ax=ax)#seaborn scatter plot\nax.set(xlim=(0,6)) #to change axis ranges\nax.set_ylim(0,100000)\nplt.show()\n\n# avg_transaction_value has some effect on target variable, as it is low for last 3 categories of target variable","0864f2c6":"#avg_frequency_login_days: object -- but it shouldn't be object\nX.avg_frequency_login_days.nunique() #1587\nX.avg_frequency_login_days.value_counts()\n# it is object because of 'Error' value\n# but there are negative values too -- that also needs to be cleared\n\n\na=X.avg_frequency_login_days.unique()\n#print(list(a))","59f8eae4":"# declaring temp_df to find median\n\ntemp_df = X[['avg_frequency_login_days']]\ntemp_df.drop(temp_df[ temp_df['avg_frequency_login_days'] == 'Error' ].index, axis=0, inplace=True)\ntemp_df = temp_df.astype('float64') # changing type to float for whole datatype\n# df[[\"a\", \"b\"]] = df[[\"a\", \"b\"]].apply(pd.to_numeric)\n\n\n# negative valued converted to pos\ntemp_df['avg_frequency_login_days'] = temp_df['avg_frequency_login_days'].apply(lambda x : x if x > 0 else abs(x))\n\n# converting negative values to abs\nreplace_mean = np.mean(temp_df['avg_frequency_login_days'])\n\n#transforming the columns\nX_train['avg_frequency_login_days'].replace('Error', str(replace_mean), inplace=True)\nX_train['avg_frequency_login_days'] = X_train['avg_frequency_login_days'].apply(pd.to_numeric)\nX_train['avg_frequency_login_days'] = X_train['avg_frequency_login_days'].apply(lambda x : x if x > 0 else abs(x))\n\nX_valid['avg_frequency_login_days'].replace('Error', str(replace_mean), inplace=True)\nX_valid['avg_frequency_login_days'] = X_valid['avg_frequency_login_days'].apply(pd.to_numeric)\nX_valid['avg_frequency_login_days'] = X_valid['avg_frequency_login_days'].apply(lambda x : x if x > 0 else abs(x))\n\nX['avg_frequency_login_days'].replace('Error', str(replace_mean), inplace=True)\nX['avg_frequency_login_days'] = X['avg_frequency_login_days'].apply(pd.to_numeric)\nX['avg_frequency_login_days'] = X['avg_frequency_login_days'].apply(lambda x : x if x > 0 else abs(x))\n\n\ntest_df['avg_frequency_login_days'].replace('Error', str(replace_mean), inplace=True) #no errors will be thrown if \"Error\" didn't existed\ntest_df['avg_frequency_login_days'] = test_df['avg_frequency_login_days'].apply(pd.to_numeric)\ntest_df['avg_frequency_login_days'] = test_df['avg_frequency_login_days'].apply(lambda x : x if x > 0 else abs(x))\n","4a3bea96":"sns.boxplot(y=X['avg_frequency_login_days'], hue=X['churn_risk_score'])","ad27693c":"# points_in_wallet              \nsns.boxplot(y=X['points_in_wallet'], hue=X['churn_risk_score'])\n# presence of outliers but the value can be negative","f3a61b2b":"fig, ax = plt.subplots()\nsns.regplot(x='churn_risk_score',y='points_in_wallet', scatter=True, fit_reg=False, data=X, ax=ax)#seaborn scatter plot\nax.set(xlim=(0,6)) #to change axis ranges\nax.set_ylim(0,3000)\nplt.show()\n\n# avg_transaction_value has some effect on target variable, as it is low for last 3 categories of target variable","7541833c":"sns.displot(X['points_in_wallet'], kde=True, bins=5) #seaborn histogram","0af0dc58":"#used_special_discount         \nsns.countplot(x='churn_risk_score', hue='used_special_discount', data=X, palette=\"tab10\")\n# there is some variations over used_special_discount counts: thus used_special_discount is important variable","3d4e4529":"#offer_application_preference           \nsns.countplot(x='churn_risk_score', hue='offer_application_preference', data=X, palette=\"tab10\")\n# there is some variations over offer_application_preference counts: thus offer_application_preference is important variable","63196540":"#past_complaint                           \nsns.countplot(x='churn_risk_score', hue='past_complaint', data=X, palette=\"tab10\")\n# there is some variations over past_complaint counts: thus past_complaint is important variable","c313ec42":"#complaint_status                                         \nsns.countplot(x='churn_risk_score', hue='complaint_status', data=X, palette=\"tab10\")\n# there is some variations over complaint_status counts: thus complaint_status is important variable; although not that important","174b396f":"# changing the joining_date\nX['joining_date'].unique()","8f4da363":"X_valid['joining_date'].unique()","45f9ea25":"def strToDate(df, col_name):\n    temp_df = df\n    day = []\n    month = []\n    year = []\n    for i in range(len(df)):\n        #print(df['joining_date'][i][:4])\n        year.append(int(df['joining_date'][i][:4])) #0..3 represents year\n        month.append(int(df['joining_date'][i][5:7])) # 5..6 represents month\n        day.append(int(df['joining_date'][i][8:10])) # 8..9 represents the day\n    \n    # adding new columns in dataframe\n    temp_df['day_of_joining']=day\n    temp_df['month_of_joining']=month\n    temp_df['year_of_joining']=year\n    return temp_df","afb9ab8b":"#changing the column data to year, month, day\nX_valid = strToDate(X_valid, 'joining_date')\nX_train = strToDate(X_train, 'joining_date')\nX = strToDate(X, 'joining_date')\ntest_df = strToDate(test_df, 'joining_date')\n\n","8b6c1a11":"#dropping the parent str joining_date column\nX_valid.drop('joining_date',axis = 1,inplace=True)\nX_train.drop('joining_date',axis = 1,inplace=True)\nX.drop('joining_date',axis = 1,inplace=True)\ntest_df.drop('joining_date',axis = 1,inplace=True)","409502b7":"# checking if the time series column has been updated\nX['day_of_joining'].unique()\nX['day_of_joining'].dtype","4acd59a4":"# checking if the time series column last_visit_time\nprint(X['last_visit_time'].unique())\nX['last_visit_time'].dtype","a6628440":"# changing the last_visit_time variable\n\ndef strToTime(df, col_name):\n    temp_df = df\n    hour = []\n    minute = []\n    second = []\n    for i in range(len(temp_df)):\n        hour.append(int(temp_df[col_name][i][:2]))\n        minute.append(int(temp_df[col_name][i][3:5]))\n        second.append(int(temp_df[col_name][i][6:8]))\n        \n    temp_df['minute_last_time_visit']=minute\n    temp_df['hour_last_time_visit']=hour\n    temp_df['sec_last_time_visit']=second\n    \n    temp_df.drop(col_name,axis=1,inplace=True) #deleting the column inside funtion itself\n    \n    return temp_df","a049ccfa":"# changing the time series data 'last_visit_time'\nX_valid = strToTime(X_valid, 'last_visit_time')\nX_train = strToTime(X_train, 'last_visit_time')\nX = strToTime(X, 'last_visit_time')\ntest_df = strToTime(test_df, 'last_visit_time')","ac1e4ce0":"# checking if the time series column has been updated\nprint(X['minute_last_time_visit'].unique())\nX['minute_last_time_visit'].dtype","bf519bee":"X['feedback']","57bff11d":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\n\n#below method to do label encoding (as of now) on feedback column\ndef nlpProcessor(df, col_name):\n    feedbacks = []\n    for line in df[col_name]: # extracting all individual feedback\n        feedbacks.append(line)\n        \n    lemmatizer = WordNetLemmatizer()\n\n    feedback_corpus = []\n\n    for i in range(len(feedbacks)):\n        sentence = re.sub('[^a-zA-Z]',' ',feedbacks[i]) #repacing all other characters than alphabet\n        sentence = sentence.lower() #converting every feedback to lowercase\n        #print(sentence)\n        \n        sentence = sentence.split() #splitting sentence into words\n        sentence = [lemmatizer.lemmatize(word) for word in sentence if not word in set(stopwords.words('english'))]\n    \n        sentence = ' '.join(sentence)\n        feedback_corpus.append(sentence)\n        \n    \n    temp_df = pd.DataFrame(feedback_corpus) #dataframe of unique feedbacks\n\n    temp_df.columns=['feedback']\n\n    unique_feedbacks = temp_df['feedback'].unique()\n\n    #SCOPE OF IMPORVEMENT: we can replace feedbacks based upon sentiments of words\n    # here we will just do encoding (kind of label encoding)\n    for i in range(len(unique_feedbacks)):\n        temp_df.replace(unique_feedbacks[i],i,inplace=True)\n\n\n    df['feedback'] = temp_df['feedback']\n    \n    return df","28e8cdbd":"# changing the time series data 'last_visit_time'\nX_valid = nlpProcessor(X_valid, 'feedback')\nX_train = nlpProcessor(X_train, 'feedback')\nX = nlpProcessor(X, 'feedback')\ntest_df = nlpProcessor(test_df, 'feedback')","7c684ab2":"categorical_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\nnumerical_cols = [col for col in X_train.columns if X_train[col].dtype in [\"int64\", \"float64\"]]\nlow_cardinality_categorical_cols = [col for col in categorical_cols if X_train[col].nunique() < 15]","f8d5c1fc":"# keeping only low category cols\n# Keep selected columns only\nmy_cols = low_cardinality_categorical_cols + numerical_cols\nX_train_selected = X_train[my_cols].copy() #deep copy\nX_valid_selected = X_valid[my_cols].copy() #deep copy\n","14a73933":"test_df_selected = test_df[my_cols].copy()","7e0f7bcb":"# pipeline\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","527c67bb":"from sklearn.metrics import accuracy_score, confusion_matrix\n\n\n# Bundle preprocessing and modeling code in a pipeline\ndef fit_predict_score(model):\n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)\n                         ])\n\n    # Preprocessing of training data, fit model \n    clf.fit(X_train_selected, y_train)\n\n    # Preprocessing of validation data, get predictions\n    preds = clf.predict(X_valid_selected)\n    \n    return preds\n#     print(\"accuracy score:\", round(model.score_*100,2))\n\n    \n    #confusion matrix\n#     confusion_matrix_ = confusion_matrix(y_valid, preds)\n#     print(\"confusion matrix:\", confusion_matrix_)\n    # principle diagonal element gives the count for correct (True positive, True negative)\n\n#     #accuracy score\n#     accuracy_score_ = accuracy_score(y_valid, preds)\n#     print(\"accuracy score:\", round(accuracy_score_*100,2))\n\n        \n\n","c9ffd950":"\n\n# Defining model: \nmodels = [XGBClassifier(n_estimators=100, random_state=0),\n          CatBoostClassifier(verbose=0), LGBMClassifier()]\n          \n\nmodel_name = ['XGBClassifier', 'CatBoostClassifier', 'LGBMClassifier']\n\n# find the best model among them\nfor idx in range(3):\n    print(model_name[idx])\n    preds = fit_predict_score(models[idx])\n    from sklearn.metrics import f1_score\n    print(f1_score(y_valid, preds, average='weighted'))\n\n\n# XGBClassifier: 0.737558536346357\n# CatBoostClassifier: 0.730955372571863\n# LGBMClassifier: 0.7306710840943358","4682cb90":"clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', models[0])\n                         ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train_selected, y_train)\n\ntest_preds = clf.predict(test_df_selected) # as XGBClassifier seems to have higher score\n\n# The lines below shows how to save predictions in format used for competition scoring\n\n\noutput = pd.DataFrame({'customer_id': test_df_selected.index,\n                      'churn_risk_score': test_preds})\noutput.to_csv('submission.csv', index=False)","976e34f7":"Same data is available on:\nhttps:\/\/www.kaggle.com\/imsparsh\/churn-risk-rate-hackerearth-ml","297334c8":"#### variable - region_category","b3091bfd":"#### Working on time series data","f76dd07a":"#### variable - gender","66c70ae0":"#### variable - Age","8644e344":"### Working with the feedback variable: needs Natural language processing","745ac3bb":"## EDA"}}