{"cell_type":{"cf71a62a":"code","d171c591":"code","f9ce9cc4":"code","0eb10287":"code","99bf3d54":"code","61b90966":"code","52fc92d6":"code","f0027328":"code","0bcdbfe3":"code","7a80993f":"code","54e73bb8":"code","04ef5af0":"code","d0ff1ff9":"code","a825d5c5":"code","29cf62f6":"code","8f280340":"code","58bed1f0":"code","f332bb7c":"code","b5481a2e":"code","8442d485":"code","9719fa39":"code","c93b03d5":"code","13e99868":"code","ea9cd6b5":"code","69883f0f":"code","b9ce2f17":"code","0e617e43":"code","faae1ba1":"code","fde7b282":"code","2aee9c39":"code","81034b80":"code","f4e64bef":"code","e2f90f39":"code","8e3517e9":"code","9ce7d390":"code","48e099fb":"code","5aecadfc":"code","5934b015":"code","c61c641c":"code","202fc677":"code","58706c28":"code","a7011bf8":"code","e1255ac4":"code","15123276":"code","5a37869d":"code","5c62024b":"code","19ed7d73":"code","d9716c75":"code","6bc8e9d5":"code","79ea0cc2":"markdown","fa1cbe19":"markdown","e01b3f83":"markdown","fc4053f2":"markdown","461b5b3d":"markdown","8d405ec9":"markdown","f5eae830":"markdown","c6a6abcd":"markdown","cbe37d88":"markdown","e956f59e":"markdown","ff74b18c":"markdown","785a749d":"markdown","88d803f6":"markdown","bc155158":"markdown","52489c35":"markdown","017a7b4e":"markdown"},"source":{"cf71a62a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d171c591":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.head()","f9ce9cc4":"df.describe()","0eb10287":"df.info()","99bf3d54":"(df['Sex']=='female').sum()","61b90966":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n\n\nsns.distplot(a=df[df['Sex']=='male']['Fare'], label='Men', bins=10, kde=False, norm_hist=True);\nsns.distplot(a=df[df['Sex']=='female']['Fare'], label='women', bins=10, kde=False, norm_hist=True);\nplt.legend();\n","52fc92d6":"df.replace(['male','female'],['M','F'],inplace=True)\ndf['Sex'].head()","f0027328":"df.info()","0bcdbfe3":"df[['Sex', 'Age', 'SibSp', 'Parch', 'Embarked']] =df[['Sex', 'Age', 'SibSp', 'Parch', 'Embarked']].astype('category')","7a80993f":"df.info()","54e73bb8":"pd.pivot_table(df,values='Survived',index=['Pclass'],columns=['Sex'])","04ef5af0":"import seaborn as sns; sns.set()\nax=pd.pivot_table(df,values='Survived',index=['Pclass'],columns=['Sex']).plot(kind='bar')\nax.set_ylabel('Survival_Rate');\n","d0ff1ff9":"pivot=pd.pivot_table(df,index='SibSp',values='Survived')\nax=pivot.plot(kind='bar')\nax.set_ylabel('Survival_Rate')","a825d5c5":"pivot = pd.pivot_table(df,index='Embarked', values='Survived')\nax=pivot.plot(kind='bar')\nax.set_ylabel('Survival_Rate');","29cf62f6":"pivot = pd.pivot_table(df, index='Parch',values='Survived')\nax=pivot.plot(kind='bar')\nax.set_ylabel('Survival_Rate')","8f280340":"df['cut']=pd.cut(df['Age'],6)\npivot = pd.pivot_table(df,index='cut',values='Survived')\npivot.plot(kind='bar')","58bed1f0":"srs=df['cut'].value_counts(normalize=False)\nsrs.plot(kind='bar')","f332bb7c":"#filling null values with random values with each category have a probability proportional to the number of it in the column.\nsrs = df['cut'].value_counts(normalize=True)\nfilling = pd.Series(np.random.choice(srs.index,p=srs,size=len(df)))\ndf['cut'].fillna(filling,inplace=True)\ndf['cut'].isna().any()","b5481a2e":"titanic_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ntitanic_df.info()","8442d485":"titanic_df.dropna(axis=0,inplace=True,subset=['Embarked'])\ntitanic_df.drop(['Cabin','Ticket','Name'],axis=1,inplace=True)\ntitanic_df.set_index('PassengerId',inplace=True)\ntitanic_df.info()","9719fa39":"titanic_df=pd.get_dummies(titanic_df, columns=['Sex','Embarked'])","c93b03d5":"titanic_df.info()","13e99868":"titanic_df['Age'].fillna(titanic_df['Age'].mean(),axis=0,inplace=True)\ntitanic_df.info()","ea9cd6b5":"titanic_df=pd.get_dummies(titanic_df,columns=['Pclass'])\ntitanic_df.info()","69883f0f":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass AttributeAdder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X['FareAge'] = X['Fare']\/X['Age']\n        return X\n\n    \n","b9ce2f17":"X = titanic_df.drop('Survived',axis=1)\ny = titanic_df['Survived']\nX.head()","0e617e43":"#Addind a pipe for scaling\nprocpipe = Pipeline([\n    \n    ('attradd', AttributeAdder()),\n    ('minmaxscale', MinMaxScaler()),\n    ]\n)","faae1ba1":"#Support Vector Classifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nsvcpipe = Pipeline([\n    ('procpipe',procpipe),\n    ('svc',SVC())\n])\n\ny_predict = cross_val_predict(svcpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the Support Vector Classifier using an rbf kernel is:',accuracy_score(y_predict, y))\n\n","fde7b282":"from sklearn.ensemble import RandomForestClassifier\n\nrfcpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('randomforestregressor',RandomForestClassifier())\n])\ny_predict = cross_val_predict(rfcpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the Random Forest Classifier is:',accuracy_score(y_predict, y))\n\n","2aee9c39":"from sklearn.linear_model import LogisticRegression\n\nlrpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('logisticregressor',LogisticRegression())\n])\ny_predict = cross_val_predict(lrpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the Logistic Regression is:',accuracy_score(y_predict, y))","81034b80":"#Models to use. They are classification algorithims\n#SVC, DecisionTreeClassifier, RandomForestRegressor, SGDClassifier\n\nfrom sklearn.linear_model import SGDClassifier\n\nSGDpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('sgdclassifier', SGDClassifier())\n])\n\n\ny_predict = cross_val_predict(SGDpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the Stochastic Gradient Descent Classifier is:',accuracy_score(y_predict, y))\n","f4e64bef":"\nfrom sklearn.naive_bayes import GaussianNB\n\nGNBpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('sgdclassifier', GaussianNB())\n])\n\n\ny_predict = cross_val_predict(GNBpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the Naive Gaussian Bayes  is:',accuracy_score(y_predict, y))\n","e2f90f39":"\nfrom sklearn.naive_bayes import MultinomialNB\n\nMTNpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('sgdclassifier', MultinomialNB())\n])\n\n\ny_predict = cross_val_predict(MTNpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the Naive Multinomial Bayes  is:',accuracy_score(y_predict, y))\n\n","8e3517e9":"\nfrom sklearn.naive_bayes import BernoulliNB\n\nBNLpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('sgdclassifier', BernoulliNB())\n])\n\n\ny_predict = cross_val_predict(BNLpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the Naive Bernoulli Bayes  is:',accuracy_score(y_predict, y))\n","9ce7d390":"from sklearn.ensemble import AdaBoostClassifier\n\nABCpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('sgdclassifier', AdaBoostClassifier())\n])\n\n\ny_predict = cross_val_predict(ABCpipe, X, y, cv=4,n_jobs=-1)\n\nprint('The accuracy for the AdaBoostClassifier  is:',accuracy_score(y_predict, y))\n","48e099fb":"import xgboost as xgb\n\nxgbmodel = xgb.XGBClassifier(objective='binary:logistic')\n\nXGBpipe = Pipeline([\n    ('procpipe', procpipe),\n    ('xgbclassifier', xgbmodel)\n])\n\ny_pred = cross_val_predict(XGBpipe,X,y,cv=6);\nprint('The accuracy for the XGBClassifier is:',accuracy_score(y_predict, y))","5aecadfc":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'svc__C':[0.5, 1, 10, 100, 300, 350],\n    'svc__gamma':['scale', 'auto', 1,10,100,300],\n    \n}\n\ngrid = GridSearchCV(svcpipe,param_grid, cv=6, n_jobs=-1).fit(X,y)\n\n\n\n\n","5934b015":"print('The best params for SVC are:\\n',grid.best_params_)","c61c641c":"print('The best score was',grid.best_score_)","202fc677":"m=titanic_df.corr()\nplt.figure(figsize = (16,16))\nsns.heatmap(m,annot=True)","58706c28":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf.head()\n\ndf.info()","a7011bf8":"titanic_df = df\n\ntitanic_df.drop(['Cabin','Ticket','Name'],axis=1,inplace=True)\ntitanic_df.set_index('PassengerId',inplace=True)\ntitanic_df=pd.get_dummies(titanic_df, columns=['Sex','Embarked'])\n\ntitanic_df.head()\n\n","e1255ac4":"titanic_df['Age'].fillna(titanic_df['Age'].mean(),axis=0,inplace=True)\ntitanic_df.info()","15123276":"titanic_df.loc[titanic_df['Fare'].isna(),'Fare']=titanic_df['Fare'].mean()","5a37869d":"titanic_df.info()","5c62024b":"titanic_df=pd.get_dummies(titanic_df,columns=['Pclass'])","19ed7d73":"bestModel = grid.best_estimator_\n\ny_predict = bestModel.predict(titanic_df)\ny_predict","d9716c75":"data = {titanic_df.index.name: titanic_df.index, 'Survived': y_predict}\ntheframe = pd.DataFrame(data)\ntheframe.head()","6bc8e9d5":"theframe.to_csv(\".\/submission.csv\")","79ea0cc2":"<h1> Titanic Dataset <\/h1>\n\nThis is my first tryhard notebook. Firstly, i'm going to train my data analysis skills, by trying to find relationships between variables. Then, i'll do some data cleaning. Then i'll show the data processing\/feature engineering, and finally, test various models and then do a grid search on the best one.\n","fa1cbe19":"Survival Rate for different intervals of Age:","e01b3f83":"Since the SVC model was the best, i'll be using it for parameter tuning.","fc4053f2":"Plotting the histogram for Fares. We can see that there's a slight difference between the proportion of men\/women against the price of the fares. Men pay proportionally more cheaper fares while proportionally more women are paying for more expensive fares. ","461b5b3d":"Making a pipe for data processing. After that, i'm going to make pipes for each model and print the accurace of each one of them. I'll also be using cross validation on each one of them.","8d405ec9":"Correlation between numerical values","f5eae830":"Here we see that having 0 siblings and and greater than 2 siblings means you have a low chance of survival, while having 1\/2 siblings means you have the most chance of survival.","c6a6abcd":"Transforming to OneHotEncoding some categorical attributes.","cbe37d88":"We can see 2 things from this pivot table: \n1. Females survived way more then men\n2. Class 3 were the ones who least survived.\n\nBelow is the visual representation of the results","e956f59e":"Replacing male\/female categories to M, F respectively. Then transforming some attributes into categorical for more efficiency in memory. (No need to do this for this small dataset, but I wanted to learn more about categorical types in pandas).","ff74b18c":"Distribution of ages","785a749d":"I didn't changed the Cabin into categorical because it will be deleted because of the amount of null values.","88d803f6":"We can see some null values: On Column 5 (Age), Column 10 (Cabin) and Columns 11 (Embarked)","bc155158":"Making a transformer that adds the ratio of Fare and Age","52489c35":"Now that we've trained a bit of data analysis, now we'll train some models. First, we'll clean up the data. We'll delete the cabin columns, then delete the rows with null embarked, then we'll input the age through the mean.","017a7b4e":"People who embark from C has the most chance of survival, while Q and S have a lower chance of survival relatively speaking"}}