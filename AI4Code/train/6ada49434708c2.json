{"cell_type":{"d3dcea0b":"code","b3bc3be3":"code","3b3e4ba2":"code","fb9d5409":"code","8533ef6d":"code","cd9e2cc7":"code","8145ef5c":"code","676bfbec":"code","8094e124":"code","2c5097c1":"code","91b4ad4d":"code","b4c735db":"code","968511ff":"code","94ffb5f4":"code","132344e0":"code","625a8c5f":"code","1e39af76":"code","d5d99d33":"code","fc69d9fd":"code","90a436aa":"code","f21246f1":"code","6ae37253":"code","813fc4da":"code","c170056d":"code","de349fbc":"code","47948858":"code","cf845d8c":"code","6c5b5148":"code","da353a61":"code","17262431":"code","d0c9eb13":"code","ab4d592b":"code","94e6e063":"code","7fac0577":"code","88c561fb":"code","9b1c16a1":"code","907b7a01":"code","af2c8e00":"code","b0d94e42":"code","8dbfa280":"code","7e9c498b":"code","1117acbc":"code","e4c22c4c":"code","43a10e0b":"code","059e0ce2":"code","67e818eb":"code","07b5dd2e":"code","d668cb9b":"markdown","713b6950":"markdown","b8b9c1ee":"markdown","475e9fd8":"markdown","da0ee1c0":"markdown","fb1f258f":"markdown","a4ad8e91":"markdown","9ae6cdfe":"markdown","2d59d0fc":"markdown","0fdf8b0d":"markdown","773f9f79":"markdown","ddc587fe":"markdown","08a8177e":"markdown","6ac625c0":"markdown","1c5faf40":"markdown","675665db":"markdown","fe65a7a6":"markdown","acadb854":"markdown","391f2e69":"markdown"},"source":{"d3dcea0b":"!pip install dataprep","b3bc3be3":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\nfrom dataprep.eda import create_report\nfrom dataprep.eda import plot_missing\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","3b3e4ba2":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ndf = data.copy()\npd.set_option('display.max_row',df.shape[0])\npd.set_option('display.max_column',df.shape[1]) \ndf.head()","fb9d5409":"plot_missing(df)","8533ef6d":"print('There is' , df.shape[0] , 'rows')\nprint('There is' , df.shape[1] , 'columns')","cd9e2cc7":"df.duplicated().sum()","8145ef5c":"df['Survived'].value_counts(normalize=True) #Classes d\u00e9s\u00e9quilibr\u00e9es","676bfbec":"for col in df.select_dtypes(include=['float64','int64']):\n    plt.figure()\n    sns.displot(df[col],kind='kde',height=3)\n    plt.show()","8094e124":"X = df.drop('Survived',axis=1)\ny = df['Survived']","2c5097c1":"survived = df[y == 1]\ndied = df[y == 0]","91b4ad4d":"plt.figure(figsize=(4,4))\nsns.pairplot(data,height=1.5)\nplt.show()","b4c735db":"corr = df.corr(method='pearson').abs()\n\nfig = plt.figure(figsize=(8,6))\nsns.heatmap(corr, annot=True, cmap='tab10', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\nprint (df.corr()['Survived'].abs().sort_values())","968511ff":"for col in df.select_dtypes(include=['float64','int64']):\n    plt.figure(figsize=(4,4))\n    sns.distplot(survived[col],label='High Risk')\n    sns.distplot(died[col],label='Low Risk')\n    plt.legend()\n    plt.show()","94ffb5f4":"for col in X.select_dtypes(include=['float64','int64']):\n    plt.figure(figsize=(4,4))\n    sns.lmplot(x='Pclass', y=col, hue='Survived', data=df)","132344e0":"create_report(df)","625a8c5f":"for col in df.select_dtypes(include=['object']):\n    print(f'{col :-<50} {df[col].unique()}')","1e39af76":"def feature_engineering(df):\n    useless_columns = ['PassengerId','Name','Ticket','Cabin'] # Let's consider we want to use all the features\n    df = df.drop(useless_columns,axis=1)\n    return df\n\ndef encoding(df):\n    code = {\n        'male':0,\n        'female':1,\n        'S':0,\n        'C':1,\n        'Q':2,\n           }\n    for col in df.select_dtypes('object'):\n        df.loc[:,col]=df[col].map(code)\n        \n    return df\n\ndef imputation(df):\n    \n    df = df.dropna(axis=0) # There are no NaN anyways\n    \n    return df\n\ndef preprocessing(df):\n    df = feature_engineering(df)\n    df = encoding(df)\n    df = imputation(df)\n    \n    X = df.drop('Survived',axis=1)\n    y = df['Survived']    \n      \n    return df,X,y","d5d99d33":"df=data.copy()\ndf,X,y = preprocessing(df)\ndf.head()","fc69d9fd":"df = data.copy()\ntrainset, valset = train_test_split(df, test_size=0.2, random_state=0)\nprint(trainset['Survived'].value_counts())\nprint(valset['Survived'].value_counts())","90a436aa":"_, X_train, y_train = preprocessing(trainset)\n_, X_val, y_val = preprocessing(valset)","f21246f1":"preprocessor = make_pipeline(MinMaxScaler())\n\nPCAPipeline = make_pipeline(StandardScaler(), PCA(n_components=2,random_state=0))\n\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=0,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression(solver='sag'))","6ae37253":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X))\nPCA_df = pd.concat([PCA_df, y], axis=1)\nPCA_df.head()","813fc4da":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df['Survived'],palette=sns.color_palette(\"tab10\", 2))\nplt.show()","c170056d":"dict_of_models = {'RandomForest': RandomPipeline,\n'AdaBoost': AdaPipeline,\n'SVM': SVMPipeline,\n'KNN': KNNPipeline,\n'LR': LRPipeline}","de349fbc":"def evaluation(model):\n    model.fit(X_train, y_train)\n    # calculating the probabilities\n    y_pred_proba = model.predict_proba(X_val)\n\n    # finding the predicted valued\n    y_pred = np.argmax(y_pred_proba,axis=1)\n    print('Accuracy = ', accuracy_score(y_val, y_pred))\n    print('-')\n    print(confusion_matrix(y_val,y_pred))\n    print('-')\n    print(classification_report(y_val,y_pred))\n    print('-')\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train, \n                                               cv=4, scoring='f1', \n                                               train_sizes=np.linspace(0.1,1,10))\n    plt.figure(figsize=(12,8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()","47948858":"for name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    evaluation(model)","cf845d8c":"RandomPipeline.fit(X_train, y_train)\ny_proba = RandomPipeline.predict_proba(X_val)\ny_pred = np.argmax(y_proba,axis=1)\n\nprint(\"RandomForest : \", accuracy_score(y_val, y_pred))","6c5b5148":"y_pred_prob = RandomPipeline.predict_proba(X_val)[:,1]\n\nfpr,tpr,threshols=roc_curve(y_val,y_pred_prob)\n\nplt.plot(fpr,tpr,label='RandomForest ROC Curve')\nplt.xlabel(\"False Survivor Rate\")\nplt.ylabel(\"True SurvivorR Rate\")\nplt.title(\"andomForest ROC Curve\")\nplt.show()","da353a61":"err = []\n  \nfor i in range(1, 40):\n    \n    model = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = i))\n    model.fit(X_train, y_train)\n    pred_i = model.predict(X_val)\n    err.append(np.mean(pred_i != y_val))\n  \nplt.figure(figsize =(10, 8))\nplt.plot(range(1, 40), err, color ='blue',\n                linestyle ='dashed', marker ='o',\n         markerfacecolor ='blue', markersize = 8)\n  \nplt.title('Mean Err = f(K)')\nplt.xlabel('K')\nplt.ylabel('Mean Err')","17262431":"KNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = 4))\nKNNPipeline.fit(X_train, y_train)\ny_proba = KNNPipeline.predict_proba(X_val)\ny_pred = np.argmax(y_proba,axis=1)\n\nprint(\"KNN : \", accuracy_score(y_val, y_pred))","d0c9eb13":"best_classifier = RandomPipeline","ab4d592b":"from sklearn.model_selection import RandomizedSearchCV\nbest_classifier.get_params().keys()","94e6e063":"hyper_params = {\n    'randomforestclassifier__n_estimators':[10,100,150,250,400,600],\n    'randomforestclassifier__criterion':['gini','entropy'],\n    'randomforestclassifier__min_samples_split':[2,6,12],\n    'randomforestclassifier__min_samples_leaf':[1,4,6,10],\n    'randomforestclassifier__max_features':['auto','srqt','log2',int,float],\n    'randomforestclassifier__n_jobs':[-1],\n    'randomforestclassifier__verbose':[0,1,2],\n    'randomforestclassifier__class_weight':['balanced','balanced_subsample'],\n    'randomforestclassifier__n_jobs':[-1],\n    'randomforestclassifier__n_jobs':[-1],\n}","7fac0577":"grid = RandomizedSearchCV(best_classifier,hyper_params,scoring='accuracy',n_iter=40)","88c561fb":"grid.fit(X_train,y_train)","9b1c16a1":"print(grid.best_params_)\nprint(grid.best_score_)","907b7a01":"ypred = grid.predict(X_val)\nprint(classification_report(y_val,ypred))","af2c8e00":"evaluation(grid.best_estimator_)","b0d94e42":"import xgboost as xgb\ngbm = xgb.XGBClassifier(\n learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n eval_metric = 'logloss',\n nthread= -1,\n scale_pos_weight=1).fit(X_train, y_train)\nevaluation (gbm)","8dbfa280":"best_classifier = gbm # grid.best_estimator_","7e9c498b":"df_test=pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test.head()","1117acbc":"def encoding(df):\n    code = {\n        'male':0,\n        'female':1,\n        'S':0,\n        'C':1,\n        'Q':2,\n           }\n    for col in df.select_dtypes('object'):\n        df.loc[:,col]=df[col].map(code)\n        \n    return df\n\ndef feature_engineering(df):\n    useless_columns = ['PassengerId','Name','Ticket','Cabin'] # Let's consider we want to use all the features\n    df = df.drop(useless_columns,axis=1)\n    return df\n\ndef imputation(df):\n    \n    df = df.fillna(df.mean())\n    \n    return df\n\ndef preprocessing(df):\n    df = feature_engineering(df)\n    df = encoding(df)\n    df = imputation(df)\n\n    return df","e4c22c4c":"X_test = preprocessing(df_test)\nX_test.head()","43a10e0b":"predictions = best_classifier.predict(X_test)\npredictions_proba = best_classifier.predict_proba(X_test)\nprint(predictions_proba)","059e0ce2":"thresholds = [0.3,0.4,0.5,0.6,0.7,0.8]\nbest_t = 0.3\nbest_acc = 0\nfor t in thresholds:\n    y_pred = (best_classifier.predict_proba(X_val)[:,1] >= t).astype(int)\n    acc = accuracy_score(y_val, y_pred)\n    if acc > best_acc:\n        best_acc=acc\n        best_t=t\nprint(best_acc)\nprint(best_t)","67e818eb":"predictions = (best_classifier.predict_proba(X_test)[:,1] >= 0.4).astype(int)\nsubmission = pd.DataFrame({'PassengerId':df_test['PassengerId'],\n                          'Survived':predictions})\nsubmission.head()","07b5dd2e":"submission.to_csv('gender_submission.csv', index=False)","d668cb9b":"## Detailed Analysis","713b6950":"# If you like please upvote !\n## Also check my other notebooks :\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83d\udc01Mice Trisomy (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83c\udf97\ufe0fBreast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### \ud83c\udf26\ud83c\udf21 Weather Forecasting \ud83d\udcc8 (98% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/weather-forecasting-98-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Heart Attack \ud83e\ude7a\ud83d\udc93 (90% Acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Mobile price (95.5% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83e\udde0 Stroke (74% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-stroke-74-acc","b8b9c1ee":"# Predictions","475e9fd8":"## PCA Analysis","da0ee1c0":"### Comments\n\nIt looks like we have some very useful features here, with a correlation > 0.25.\nThe following features seems promising for predicting wether a patient would be dead or not :\n- **Pclass**\n- **Fare**\n\nWe can also notice that **Pclass** and **Fare** looks correlated, let's find out !","fb1f258f":"<h1><center>\ud83d\udea2Titanic Data Analysis\ud83d\udd0e<\/center><\/h1>\n<h3><center>\ud83d\udc80(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/280px-RMS_Titanic_3.jpg\" alt =\"Titanic\" style='width: 400px;'><\/center>\n\n<h3>Context<\/h3>\n<p>\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean on 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking at the time one of the deadliest of a single ship and the deadliest peacetime sinking of a superliner or cruise ship to date. With much public attention in the aftermath, the disaster has since been the material of many artistic works and a founding material of the disaster film genre.\n\nRMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.\n<\/p>\n","a4ad8e91":"## Dataset Analysis","9ae6cdfe":"## XGBoost","2d59d0fc":"## Best classifier","0fdf8b0d":"# Using RandomForest","773f9f79":"# Modelling","ddc587fe":"### Comments\n#### All 5 models look promising, but **RandomForest** has a slightly better accuracy **(85%)****","08a8177e":"# Exploratory Data Analysis\n\n## Aim :\n- Understand the data (\"A small step forward is better than a big one backwards\")\n- Begin to develop a modelling strategy","6ac625c0":"## Classification problem","1c5faf40":"## RandomForest Optimization","675665db":"## KNN Optimization","fe65a7a6":"## Visualising Target and Features","acadb854":"## Making Prediction","391f2e69":"# A bit of data engineering ..."}}