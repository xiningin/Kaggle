{"cell_type":{"b179905b":"code","2088ea92":"code","446b2716":"code","68c3c50c":"code","8ec62db7":"code","88452969":"code","676ef9fc":"code","c4bd8de8":"code","d3c5b46c":"code","640e0626":"code","8a5e9b6a":"code","a90037d2":"code","37d9c682":"code","1783c2f9":"code","8882d50a":"code","fa66ed2a":"code","2821859d":"code","aa5f3577":"code","26b998ee":"code","7bf32041":"code","a24ab0b7":"code","a2205c90":"code","1d4e4ffe":"code","b7611479":"code","d1acad34":"code","3b6a74f1":"code","96789810":"code","376bdd12":"code","e6d6cf08":"code","efd68184":"code","bcc43e11":"code","4962bc13":"code","b165e762":"code","e44f0f3d":"code","8170e954":"code","5e45603c":"code","945cd24b":"code","226e5711":"code","e64b2adc":"code","e20b2de7":"code","be41bc94":"code","f1cc0583":"code","494e3cb4":"code","431fa573":"code","9040a5a4":"code","9c402444":"code","854ad527":"code","87c66473":"code","3f3bfdb1":"code","f7d80247":"code","d56f1913":"code","5e4fb331":"code","5b8d1802":"code","fee25f11":"code","33926dbf":"code","2ba927d9":"code","39dfe851":"code","02ac573a":"code","4dc9aafe":"code","cabd18a4":"code","93f3111e":"code","9f515ca2":"code","bd667d06":"code","a98e017b":"code","414e7349":"code","91229ef2":"code","9c9ad942":"code","846dd7ad":"code","e39493c6":"code","4b353936":"code","e25e1b99":"code","301e364f":"code","5f03fc6b":"code","8fe6191f":"code","97b26c1b":"code","3778c452":"code","ca790717":"code","ea93f8fe":"code","1d211fae":"code","cbe53cab":"code","fbbc11fb":"code","10ccdb22":"code","95ca836b":"code","a4f75e80":"code","2774bb61":"code","f0d24139":"markdown","a225fd93":"markdown","4b586b5d":"markdown","ad2590fc":"markdown","c8981b30":"markdown","8f2519d5":"markdown","166507a5":"markdown","5a22fe70":"markdown","7d0a14d5":"markdown","a21f5feb":"markdown","e40344d7":"markdown","fd56a642":"markdown","44e6d4bb":"markdown","b202d874":"markdown","2885388f":"markdown","bdd0cdb1":"markdown","198bb6f2":"markdown","8c22261d":"markdown","eb865f26":"markdown","22ed496b":"markdown","83aca1de":"markdown","442e9fa7":"markdown","48e98da1":"markdown","4fb839c1":"markdown","b75e5f45":"markdown","55ac260d":"markdown","79ccc348":"markdown","a5fc745b":"markdown","7d5c00a1":"markdown","d8ed47d7":"markdown","40173e9f":"markdown","723ff4f7":"markdown","cbb76dce":"markdown","a3e085d6":"markdown","1f0f1c81":"markdown","9237a831":"markdown","5c108238":"markdown","1298563c":"markdown","2e2b2e52":"markdown","a89de13a":"markdown","76212ed2":"markdown","6531d844":"markdown","b0633e6b":"markdown","0c54bbbb":"markdown","081dd089":"markdown","c6266fd2":"markdown","88e81a84":"markdown","65e60786":"markdown","27fc9b30":"markdown","fc1fb847":"markdown","6d2af55e":"markdown","15b6afd9":"markdown","45c247e0":"markdown","444226c6":"markdown","eb0f3665":"markdown","677be919":"markdown"},"source":{"b179905b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.feature_selection import SelectFromModel\n\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2088ea92":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","446b2716":"train.head()","68c3c50c":"test.head()","8ec62db7":"train.info()\nprint('_'*40)\ntest.info()","88452969":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n# Train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n# Test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","676ef9fc":"#missing amount for train set\nmissing= train.isnull().sum().sort_values(ascending=False)\npercentage = (train.isnull().sum()\/ train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([missing, percentage], axis=1, keys=['Missing', '%'])\nmissing_data.head(3)","c4bd8de8":"#missing amount for test set\nmissing= test.isnull().sum().sort_values(ascending=False)\npercentage = (test.isnull().sum()\/ test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([missing, percentage], axis=1, keys=['Missing', '%'])\nmissing_data.head(3)","d3c5b46c":"train.Embarked.fillna(value='S', inplace=True)","640e0626":"train['Embarked'].value_counts()","8a5e9b6a":"isn = pd.isnull(test['Fare'])\ntest[isn]","a90037d2":"average_of_fare= test.groupby('Pclass')['Fare'].mean()\nprint('The mean fare for the Pclass (for missing fare data) is:',average_of_fare[3])","37d9c682":"# filling the missing by mean\ntest.Fare.fillna(value=average_of_fare[3], inplace=True)","1783c2f9":"mean_age = train.groupby('Pclass')[['Age']].mean()\nmean_age","8882d50a":"#defining a function 'impute_age'\ndef impute_age(age_pclass): # passing age_pclass as ['Age', 'Pclass']\n    # Passing age_pclass[0] which is 'Age' to variable 'Age'\n    Age = age_pclass[0]\n    # Passing age_pclass[2] which is 'Pclass' to variable 'Pclass'\n    Pclass = age_pclass[1]\n    #applying condition based on the Age and filling the missing data respectively \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 38\n        elif Pclass == 2:\n            return 30\n        else:\n            return 25\n    else:\n        return Age","fa66ed2a":"#train data\ntrain['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)\n#test data\ntest['Age'] = test[['Age','Pclass']].apply(impute_age,axis=1)","2821859d":"# train\ntrain['Cabin']=train['Cabin'].notnull().astype('int')\ntrain['Cabin'].unique()","aa5f3577":"# test\ntest['Cabin']=test['Cabin'].notnull().astype('int')\ntest['Cabin'].unique()","26b998ee":"# Sex & Age\ng = sns.FacetGrid(train, hue = 'Survived', col = 'Sex', height = 3, aspect = 2)\ng.map(plt.hist, 'Age', alpha = .5, bins = 20)\ng.add_legend()\nplt.show()","7bf32041":"#Change the data types\ntrain['Age'] = train['Age'].astype(int)\ntest['Age'] = test['Age'].astype(int)","a24ab0b7":"def age_range(df):\n    df['Age'].loc[df['Age'] <= 16 ] = 0\n    df['Age'].loc[(df['Age'] > 16) & (df['Age'] <= 32)] = 1\n    df['Age'].loc[(df['Age'] > 32) & (df['Age'] <= 48)] = 2\n    df['Age'].loc[(df['Age'] > 48) & (df['Age'] <= 64)] = 3\n    df['Age'].loc[df['Age'] > 64] = 4   \nage_range(train)\nage_range(test)","a2205c90":"# Creating title dictionary in train data\ntitles = set()\nfor name in train['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\nTitle_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\ntrain['Title'] = train['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())  \n# Mapping Titles\ntrain['Title'] = train.Title.map(Title_Dictionary)","1d4e4ffe":"# Creating Title dictionary in test data\ntitles = set()\nfor name in test['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\nTitle_Dictionary_test = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\ntest['Title'] = test['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())    \n# Mapping Titles\ntest['Title'] = test.Title.map(Title_Dictionary_test)","b7611479":"# Missing values\ntest[test['Title'].isnull()]","d1acad34":"# Filling missing values in title\ntest['Title'].fillna(value='Mr', inplace=True)","3b6a74f1":"test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1","96789810":"train['FamilySize'] = train['FamilySize'].astype(int)\ntest['FamilySize'] = test['FamilySize'].astype(int)\ndef family_range(df):\n    df['FamilySize'].loc[df['FamilySize'] <= 1 ] = 0\n    df['FamilySize'].loc[(df['FamilySize'] >= 2) & (df['FamilySize'] <= 4)] = 1\n    df['FamilySize'].loc[df['FamilySize'] >= 5] = 2   \nfamily_range(train)\nfamily_range(test)","376bdd12":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n# Train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n# Test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","e6d6cf08":"# Train Data\ntrain = pd.get_dummies(train, columns=['Sex','Embarked','Title'],drop_first=True)","efd68184":"# Test Data\ntest= pd.get_dummies(test, columns=['Sex','Embarked','Title'],drop_first=True)\ntest['Title_Royalty'] = 0    # adding Title_Royalty column to match columns in the train df","bcc43e11":"fig=plt.figure(figsize=(18,10))\nax = fig.gca()\nsns.heatmap(train.corr(), annot=True,ax=ax, cmap=plt.cm.YlGnBu)\nax.set_title('The correlations between all numeric features')\npalette =sns.diverging_palette(80, 110, n=146)\nplt.show","4962bc13":"# correlation with the target\ncorr_matrix = train.corr()\ncorr_matrix[\"Survived\"].sort_values(ascending=False)","b165e762":"g = sns.factorplot('Survived',data=train,kind='count',hue='Pclass')\ng._legend.set_title('Pclass')\n# replace labels\nnew_labels = ['1st class', '2nd class', '3rd class']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)","e44f0f3d":"g = sns.factorplot('Pclass',data=train,hue='Sex_male',kind='count')\ng._legend.set_title('Sex')\n# replace labels\nnew_labels = ['Female', 'Male']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)","8170e954":"g = sns.factorplot('Survived',data=train,kind='count',hue='FamilySize')\ng._legend.set_title('Family Size')\n# replace labels\nnew_labels = ['Small', 'Single', 'Large']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)","5e45603c":"# Train data\nfeatures_drop = ['PassengerId','Name', 'Ticket', 'Survived','SibSp','Parch']","945cd24b":"selected_features = [x for x in train.columns if x not in features_drop]","226e5711":"# Test data\nfeatures_drop_test = ['PassengerId','Name', 'Ticket','SibSp','Parch']","e64b2adc":"selected_features_test = [x for x in test.columns if x not in features_drop_test]","e20b2de7":"# Train data\nX = train[selected_features]\ny = train['Survived']","be41bc94":"# Test data\ntesting = test[selected_features_test]","f1cc0583":"ss = StandardScaler()\nXs =ss.fit_transform(X)","494e3cb4":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=.3,random_state=55, stratify=y) ","431fa573":"tree= DecisionTreeClassifier()\ntree.fit(X_train, y_train)\nprint('test score' , tree.score(X_train, y_train))\nprint('test score' , tree.score(X_test, y_test))","9040a5a4":"y_pred =tree.predict(testing)","9c402444":"dt = DecisionTreeClassifier()\ndt_en = BaggingClassifier(base_estimator=dt, n_estimators=100, max_features=10)\ndt_en.fit(X_train, y_train)\nprint('test score' , dt_en.score(X_train, y_train))\nprint('test score' , dt_en.score(X_test, y_test))","854ad527":"y_pred = dt_en.predict(testing) ","87c66473":"param = { 'max_features': [0.3, 0.6, 1],\n        'n_estimators': [50, 150, 200], \n         'base_estimator__max_depth': [3, 5, 20]}","3f3bfdb1":"model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), oob_score=True)\nmodel_gs = GridSearchCV(model,param, cv=6, verbose=1, n_jobs=-1 )\nmodel_gs.fit(X_train, y_train)","f7d80247":"model_gs.best_params_","d56f1913":"model_gs.best_estimator_.oob_score_","5e4fb331":"randomF = RandomForestClassifier(max_depth=350, n_estimators=9, max_features=11, random_state=14, min_samples_split=3)\nrandomF.fit(X_train, y_train)\nprint('Train score :',randomF.score(X_train, y_train))\nprint('Ttest score :',randomF.score(X_test, y_test))","5b8d1802":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(randomF, X, y, cv=cv).mean()","fee25f11":"y_pred=randomF.predict(testing)","33926dbf":"et = ExtraTreesClassifier(n_estimators=66, min_samples_split=7)\net.fit(X_train, y_train)\nprint('Train score :',et.score(X_train, y_train))\nprint('Ttest score :',et.score(X_test, y_test))","2ba927d9":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(et, X, y, cv=cv).mean()","39dfe851":"y_pred =et.predict(testing)","02ac573a":"knn_classifier = KNeighborsClassifier(n_neighbors=7, leaf_size=48, weights='uniform',p=1)  \nknn_classifier.fit(X_train, y_train)\nprint(knn_classifier.score(X_train, y_train))\nprint (knn_classifier.score(X_test, y_test))","4dc9aafe":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(knn_classifier, X, y, cv=cv).mean()","cabd18a4":"y_pred = knn_classifier.predict(testing) ","93f3111e":"knn = KNeighborsClassifier()\nknn_en = BaggingClassifier(base_estimator=knn, n_estimators=45, oob_score=True, max_features=9, random_state=99)\nknn_en.fit(X_train, y_train)\n\nprint(knn_en.score(X_train, y_train))\nprint(knn_en.score(X_test, y_test))","9f515ca2":"y_pred = knn_en.predict(testing) ","bd667d06":"knn_en.estimators_[12]","a98e017b":"svm_l = svm.SVC(kernel='linear', C=33)\nsvm_l.fit(X_train, y_train)\nprint('Train : ', svm_l.score(X_train, y_train))\nprint('Test: ', svm_l.score(X_test, y_test))","414e7349":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(svm_l, Xs, y, cv=cv).mean()","91229ef2":"cross_val_score(randomF, X, y, cv=cv)","9c9ad942":"y_pred = svm_l.predict(testing) ","846dd7ad":"svm_p = svm.SVC(kernel='poly', C=3)\nsvm_p.fit(X_train, y_train)\nprint(svm_p.score(X_train, y_train))\nprint(svm_p.score(X_test, y_test))","e39493c6":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(svm_p, Xs, y, cv=cv).mean()","4b353936":"y_pred = svm_p.predict(testing) ","e25e1b99":"svm_rbf = svm.SVC(kernel='rbf', C=4)\nsvm_rbf.fit(X_train, y_train)\nprint(svm_rbf.score(X_train, y_train))\nprint(svm_rbf.score(X_test, y_test))","301e364f":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(svm_rbf, Xs, y, cv=cv).mean()","5f03fc6b":"y_pred = svm_rbf.predict(testing) ","8fe6191f":"logreg = LogisticRegression(max_iter=300)\nlogreg.fit(X_train, y_train)\nprint('train score' , logreg.score(X_train, y_train))\nprint('test score' , logreg.score(X_test, y_test))","97b26c1b":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(logreg, X, y, cv=cv).mean()","3778c452":"cross_val_score(randomF, X, y, cv=cv)","ca790717":"y_pred = logreg.predict(testing) ","ea93f8fe":"adaboost = AdaBoostClassifier(n_estimators=67)\nadaboost.fit(X_train, y_train)\nprint('Train accuracy:', adaboost.score(X_train, y_train))\nprint('Test accuracy:',adaboost.score(X_test, y_test))","1d211fae":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(adaboost, X, y, cv=cv).mean()","cbe53cab":"y_pred = adaboost.predict(testing) ","fbbc11fb":"thesubmission = gender_submission.copy()\nthesubmission['Survived'] = y_pred\nthesubmission['Survived'].head()\nthesubmission.to_csv('thesubmission.csv', index=False)","10ccdb22":"list_of_Scores = list()","95ca836b":"# Decision Tree Classifier\nresults = {'Model':'Decision Tree Classifier',\n           'Train Score':tree.score(X_train, y_train),\n           'Test Score':tree.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n\n# Bagging Classifier with Decision Tree \nresults = {'Model':'Bagging with Decision Tree ',\n           'Train Score':dt_en.score(X_train, y_train),\n           'Test Score':dt_en.score(X_test, y_test),\n           'Kaggle Score':0.75598}\nlist_of_Scores.append(results)\n\n# Random Forest Classifier\nresults = {'Model':'Random Forest Classifier',\n           'Train Score': randomF.score(X_train, y_train),\n           'Test Score':randomF.score(X_test, y_test),\n           'Kaggle Score':0.77990\n}\nlist_of_Scores.append(results)\n\n# Extra Trees Classifier\nresults = {'Model':'Extra Trees Classifier',\n           'Train Score':et.score(X_train, y_train),\n           'Test Score': et.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n\n# KNeighbors Classifier\nresults = {'Model':'KNeighbors Classifier',\n           'Train Score':knn_classifier.score(X_train, y_train),\n           'Test Score':knn_classifier.score(X_test, y_test),\n           'Kaggle Score':0.77511}\nlist_of_Scores.append(results)\n\n# Bagging Classifier with a Knn \nresults = {'Model':'Bagging Classifier with Knn ',\n           'Train Score': knn_en.score(X_train, y_train),\n           'Test Score':knn_en.score(X_test, y_test),\n           'Kaggle Score':0.66507}\nlist_of_Scores.append(results)\n\n# SVM with Linear\nresults = {'Model':'SVM with Linear',\n           'Train Score': svm_l.score(X_train, y_train),\n           'Test Score':svm_l.score(X_test, y_test),\n           'Kaggle Score':0.80382}\nlist_of_Scores.append(results)\n\n\n# SVM with Poly\nresults = {'Model':'SVM with Poly',\n           'Train Score':svm_p.score(X_train, y_train),\n           'Test Score':svm_p.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n\n# SVM with Rbf \nresults = {'Model':\"SVM with Rbf\",\n           'Train Score':svm_rbf.score(X_train, y_train),\n           'Test Score':svm_rbf.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results) \n\n\n# Logistic Regression\nresults = {'Model':'Logistic Regression',\n           'Train Score':logreg.score(X_train, y_train),\n           'Test Score':logreg.score(X_test, y_test),\n           'Kaggle Score':0.80382}\nlist_of_Scores.append(results)\n\n# AdaBoost Classifier\nresults = {'Model':'AdaBoost Classifier ',\n           'Train Score':adaboost.score(X_train, y_train),\n           'Test Score':adaboost.score(X_test, y_test),\n           'Kaggle Score':0.77511}\nlist_of_Scores.append(results)","a4f75e80":"df_results = pd.DataFrame(list_of_Scores)","2774bb61":"df_results","f0d24139":"#### We fill the mean age with respect to each Pclass.","a225fd93":"## Importing packages","4b586b5d":"**The mean age of each Pclass in the train data.**","ad2590fc":"### Dummies\n##### Creating Dummies For Categorical Columns.","c8981b30":"Train data have Survived (dependent variable) and other predictor variables.\nTest data include the same variables that in train data, but without Survived (dependent variable) because this data will be submitted to kaggle.","8f2519d5":"- In this Kaggle competition, we aim to predict which passengers survived the Titanic shipwreck according to economic status (class), sex, age .\n\n- In this competition, we face in binary classification problem and we try to solve this problem by using:-\n\n     - Random Forest Classifier.\t\n     - KNeighbors Classifier.\n     - Support Vector Classification.\n     - Gaussian Process Classifier.\n     - Decision Tree Classifier.\n     - AdaBoost Classifier.\n     - ExtraTreesClassifier \n     - Logistic Regression","166507a5":"### Kaggle Score","5a22fe70":"### 4 - Build KNeighbors Classifier\tModel","7d0a14d5":"- The graph shows that the survival rate in the  3rd class was lowest than the 1st and 2nd class.","a21f5feb":"- The graph shows that the reason for the high death rate of men than women because most of them were in the 3rd class.  ","e40344d7":"### Submission","fd56a642":"- #### Splitting and Standardizing Train Data to Obtain Test Scores","44e6d4bb":"### 7- Build AdaBoost Classifier Model","b202d874":"- The graph shows that the death rate of males was higher than females\n- The graph shows that older passengers had less chance of survival.","2885388f":"### 5 - Build SVM Model","bdd0cdb1":"- #### SVM with Rbf","198bb6f2":"- #### Dropping Some Columns","8c22261d":"![kaggle_score.png](attachment:kaggle_score.png)","eb865f26":"- #### Filling missing Fare values in test dataset ","22ed496b":"### Analyze by visualizing data","83aca1de":" - ##### Age Feature","442e9fa7":"- ### These datasets include 11 explanatory variables:","48e98da1":"- #### SVM with Linear","4fb839c1":"#### Model Prep: Create X  and y variables","b75e5f45":"#### Let's see how to teat the Age column !","55ac260d":"- #### Family Size Features","79ccc348":"Now let's take a look at the most important variables, which will have strong linear releationship with \n<b>Survived<\/b> variable .<br><br>","a5fc745b":"- ####  Title Feature","7d5c00a1":"### 6- Build Logistic Regression Model\n","d8ed47d7":"## Exploring the Data","40173e9f":"In this modeling we use cross-validation to evaluate the results after data cleaning. According to the Logistic Regression and SVM important featrues, we inference those featrues can play a major part in prediction. The most important featrues are: Fare,Title_Mr and they gave us a good predect of Survived feature. According to the two models's important featrues, we inference those featrues can play a major part in prediction. As we got these result:\n\nLogistic Regression resulte:\n\n        Train Score: 0.8154093097913323\n        Test  Score: 0.8619402985074627\n\nSVM resulte:\n\n        Train Score: 0.8234349919743178\n        Test  Score: 0.8544776119402985\n\nAnd when we tested the Corss Validation of Logestic, the results were:\n\nLogistic Regression resulte:\n\n        \"[0.79329609, 0.80898876, 0.83146067, 0.82022472, 0.85393258]\"\n\nSVM resulte:\n\n        \"[0.79329609, 0.80898876, 0.83146067, 0.82022472, 0.85393258]\"\n        \n        \nWith an average of: \"0.8215805661917017\" ~ 0.82 of Logistic Regression and \"0.8316740945326722\" ~ 0.83 of SVM this is a good ratio, as it means that the model can generalize any new data that can enter the model at 82-83 percent of accuracy, as this result indicates that the model is right fit because a low  viariance of it.\n\nIn the picture below the first three points score we get in this  modling in kaggle which it show the same public score for the two model a Logistic Regression and SVM.","723ff4f7":"- The graph shows that the number of deaths in singles was more than the families due to the plan followed in the rescue, which depends on the rescue of families first and then singles. ","cbb76dce":"### Modeling ","a3e085d6":"- #### SVM with Poly","1f0f1c81":"### 3- Build Extra Trees Classifier Model","9237a831":"- #### Grid Search for Bagging Classifiers","5c108238":" - ##### Cabin Feature","1298563c":"### 2 - Build Random Forest Classifier Model","2e2b2e52":"- #### Now, separate the selected column in X_train and Survived in y_train","a89de13a":"\n\n# <center> Titanic <\/center>\n\n\n\n\n\n\n### Group Number: 8\n### Group members:\n- Abdulrahman ALQannas\n- Doaa Alsenani\n- Ghida Qahtan\n- Moayad Magadmi\n---\n\n","76212ed2":"   - #### The Pclass of missing fare in test dataset","6531d844":" #### Filling A Few Missing Values\n - #### Embarked Feature  in train dataset ","b0633e6b":"### Feature Engineering","0c54bbbb":"-  #### Data Dictionary\n\n|Feature|Dataset|Description|\n|-------|---|---|\n|Survival|Train|The number of survived the Titanic shipwreck| \n|Pclass|Train\/Test|Economic status (class)| \n|Sex|Train\/Test|male or female.| \n|Age|Train\/Test|Age in years| \n|Sibsp\/Parch|Train\/Test|The number of siblings, spouses, or children aboard the Titanic.| \n|ticket|Train\/Test|Ticket number.| \n|Fare|Train\/Test|Passenger fare| \n|Cabin|Train\/Test|Cabin number| \n|Embarked|Train\/Test|Port of Embarkation| \n\n","081dd089":"### 1- Build Decision Tree Classifier Model","c6266fd2":"## Loading the Titanic","88e81a84":" - #### Fit a BaggingClassifier with a decision tree base estimator","65e60786":"### Making several new features ","27fc9b30":"##  Results","fc1fb847":"# Evaluation","6d2af55e":"## Introduction","15b6afd9":"- #### This table provides all the scores that we got from each model.","45c247e0":"- #### Survived Correlation Matrix ","444226c6":"#### No more missing data","eb0f3665":"- #### Fit a BaggingClassifier with a Knn base estimator","677be919":"### Check Missing Values"}}