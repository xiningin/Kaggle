{"cell_type":{"47c37c97":"code","8b30d83c":"code","4a483b30":"code","5772ffd8":"code","615b24a0":"code","4cbfd105":"code","03c6917c":"code","64a03a80":"code","4cf68c21":"code","fc01ae6b":"code","0933f7eb":"code","40e201b1":"code","e0ab58e6":"code","07124d38":"code","1bd83594":"code","4efb0c87":"code","7fd07630":"code","4a97c2c0":"code","2d8d1e8b":"code","3405ad91":"code","cb2968b7":"code","a22eb81e":"code","7c8d9a0e":"code","a0e64b53":"code","e9d40365":"code","f7bd6a89":"code","95f54919":"code","df550824":"code","5b45e7c4":"code","119a980f":"code","b7f10590":"code","e0d8a52b":"code","e8ecb028":"code","4d9111c2":"code","6d2a5960":"code","4c279ae9":"code","5904a41c":"code","283a2cdb":"code","8b64a0ba":"code","b1ae6f65":"code","156d84c8":"code","7c13417f":"code","f8116c96":"code","688b825e":"code","414d2ec9":"code","9a995f7a":"markdown","28f7cb16":"markdown","ed67b077":"markdown","641336c4":"markdown","d939e626":"markdown","3ecf648d":"markdown","ec9a69b9":"markdown","d751f2ff":"markdown","27eb81e6":"markdown","3d4feddd":"markdown","47771ff0":"markdown","6473f4b4":"markdown","45a8ea21":"markdown","3a6cdbd8":"markdown","d5ebbcd2":"markdown","4047da3b":"markdown","c357ec35":"markdown","c7963b65":"markdown","093aed7f":"markdown","b23261e0":"markdown","e5b2d077":"markdown","2756b60e":"markdown","e3b08fcb":"markdown","96cdb8fb":"markdown","37606858":"markdown","b1fe5d71":"markdown","71800f6d":"markdown","f9bdafca":"markdown","cbec78eb":"markdown","4a0624bc":"markdown","43a0b4c3":"markdown"},"source":{"47c37c97":"#Load General packages\nimport os\nimport numpy as np\nimport pandas as pd # for data processing\nimport matplotlib.pyplot as plt #for vizualization of data\nfrom tqdm import tqdm #for progress information\ntqdm.pandas()","8b30d83c":"#Load \"train\" dataset\ndata = pd.read_csv(\"..\/input\/train.csv\")\n\n#Load \"test\" dataset\nsubmission_data = pd.read_csv(\"..\/input\/test.csv\")","4a483b30":"#Load \"glove.840B.300d\" data\nembeddings_index = {}    #creates empty list\nglove = open('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt') #opens the test document for reading\nfor line in tqdm(glove): #for every line in this text do the following        (tqdm: and show the progress)\n    values = line.split(\" \")  #splits the string every time there is a space into seperate strings\n    word = values[0] #the first string in this text file is always the word\n    coefs = np.asarray(values[1:], dtype='float32') # the following strings are the \"explanation\"\n    embeddings_index[word] = coefs #the list is now filled with entries consisting of the word and the respective \"explanations\" (word vectors)\nglove.close() #closes the file such that is not possible to read it anymore\n\nprint('The dictionary contains %s word vectors.' % len(embeddings_index))","5772ffd8":"#Example on how Glove represents the word \"kitchen\"\nword = \"kitchen\"\nprint(\"The vector of\", word, \"in the dictionary is\", embeddings_index[word])\n","615b24a0":"data.head() # shows the first 5 rows of a dataset","4cbfd105":"data.info() #presents general information to the dataset","03c6917c":"pd.options.display.max_colwidth = 300 # for setting the width of the table longer\n\n#Sincere questions\ndata.loc[data['target'] == 0].head(10)","64a03a80":"#Insincere questions\ndata.loc[data['target'] == 1].head(10)","4cf68c21":"fig1, ax1 = plt.subplots()\nax1.pie(data[\"target\"].value_counts(), explode=(0, 0.3), labels= [\"Sincere\", \"Insincere\"], autopct='%1.1f%%',\n        shadow=True, startangle=45)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","fc01ae6b":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, #performing the split\n                 test_size = 0.3)\ntrain = train.reset_index(drop=True) #thus the df counts from 0 to x and does not spring from one number to another number\ntest = test.reset_index(drop = True)\nprint (\"The training dataset has the shape:\" , train.shape)\nprint (\"The test dataset has the shape:\", test.shape)","0933f7eb":"X_train = train.iloc[:,1] #Takes all rows of the first column as new dataset\nY_train = np.array(train.iloc[:, 2]) #Takes all rows of the second column as new dataset\nX_test = test.iloc[:,1]\nY_test = np.array(test.iloc[:, 2])\n\nprint(X_train.shape)\nprint(Y_train.shape)","40e201b1":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences \nfrom keras.preprocessing import sequence \n\n#features\ntokenizer = Tokenizer(filters='', lower=False) #To ensure that no preprocessing is done at all\ntokenizer.fit_on_texts(list(data[\"question_text\"]))\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nsequences = tokenizer.texts_to_sequences(data[\"question_text\"])\nmaxlen = len(max(sequences, key = len)) #max number of words in a question (l\u00e4ngste sequenz aus tokenisierten w\u00f6rtern)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq =tokenizer.texts_to_sequences(X_test)\nX_train_seq = pad_sequences(X_train_seq, maxlen=maxlen)\nX_test_seq = pad_sequences(X_test_seq, maxlen=maxlen)","e0ab58e6":"word = \"kitchen\"\nprint(\"The index of\", word, \"in the vocabulary is\", word_index[word], \".\")","07124d38":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","1bd83594":"oov = check_coverage(tokenizer.word_counts,embeddings_index)","4efb0c87":"oov[:10]","7fd07630":"#dictionary\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]","4a97c2c0":"#Creation of the improved data\nt1 = Tokenizer(filters = puncts,lower = False)\nt1.fit_on_texts(list(data[\"question_text\"]))\noov = check_coverage(t1.word_counts,embeddings_index)","2d8d1e8b":"oov[:10]","3405ad91":"t2 = Tokenizer(filters = puncts, lower = True)\nt2.fit_on_texts(list(data[\"question_text\"]))\noov = check_coverage(t2.word_counts,embeddings_index)","cb2968b7":"oov[:10]","a22eb81e":"tokenizer_2 = Tokenizer(filters = puncts, lower = False)\ntokenizer_2.fit_on_texts(list(data[\"question_text\"]))\nword_index_2 = tokenizer_2.word_index\nprint('Found %s unique tokens.' % len(word_index_2))\n\nsequences_2 = tokenizer_2.texts_to_sequences(data[\"question_text\"])\nmaxlen_2 = len(max(sequences_2, key = len)) #max number of words in a question (l\u00e4ngste sequenz aus tokenisierten w\u00f6rtern)\n\nX_train_seq_2 = tokenizer_2.texts_to_sequences(X_train)\nX_test_seq_2 =tokenizer_2.texts_to_sequences(X_test)\nX_train_seq_2 = pad_sequences(X_train_seq_2, maxlen=maxlen_2)\nX_test_seq_2 = pad_sequences(X_test_seq_2, maxlen=maxlen_2)","7c8d9a0e":"#Compute Embedding Matrix for the old data\nembed_dim = 300 #da glove.840B.300d.txt bedeutet, dass 300d. vektor\nembedding_matrix = np.zeros((len(word_index) + 1, embed_dim)) #creation of the numpy array\nfor word, i in tqdm(word_index.items()): #loop going through each word in word_index\n    embedding_vector = embeddings_index.get(word) #for each word the programm takes the respective vector and calls it embedding_vector\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector # vector gets inserted into the numpy array at the place where the word would stand according to the index.","a0e64b53":"#Compute Embedding Matrix for the tweaked data\nembed_dim = 300 #da glove.840B.300d.txt bedeutet, dass 300d. vektor\nembedding_matrix_2 = np.zeros((len(word_index_2) + 1, embed_dim)) #creation of the numpy array\nfor word, i in tqdm(word_index_2.items()): #loop going through each word in word_index\n    embedding_vector = embeddings_index.get(word) #for each word the programm takes the respective vector and calls it embedding_vector\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix_2[i] = embedding_vector # vector gets inserted into the numpy array at the place where the word would stand according to the index.","e9d40365":"from keras.layers.embeddings import Embedding\n\n#Load into Keras Embedding layer\nembedding_layer = Embedding(len(word_index) + 1,\n                            embed_dim,\n                            weights=[embedding_matrix],\n                            input_length=maxlen,\n                            trainable=False)","f7bd6a89":"#Load into Keras Embedding layer\nembedding_layer_2 = Embedding(len(word_index_2) + 1,\n                            embed_dim,\n                            weights=[embedding_matrix_2],\n                            input_length=maxlen_2,\n                            trainable=False)","95f54919":"#Metric: F1 score: F1: wikipedia, umsetzung https:\/\/github.com\/keras-team\/keras\/blob\/53e541f7bf55de036f4f5641bd2947b96dd8c4c3\/keras\/metrics.py\n\ndef fmeasure (y_true, y_pred):\n    \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    \n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    \n    f1 = 5 * (precision*recall) \/ (4*precision+recall+K.epsilon())\n    \n    return f1","df550824":"#Metric: F1 score: F1: wikipedia, umsetzung https:\/\/github.com\/keras-team\/keras\/blob\/53e541f7bf55de036f4f5641bd2947b96dd8c4c3\/keras\/metrics.py\n\ndef loss_f1 (y_true, y_pred):\n    \n    true_positives = K.sum(y_true * y_pred)\n    predicted_positives = K.sum(y_pred)\n    possible_positives = K.sum(y_true)\n    \n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    \n    f1 = 5 * (precision*recall) \/ (4*precision+recall+K.epsilon())\n    \n    return 1-f1","5b45e7c4":"#Attention layer \nfrom keras import initializers, regularizers, constraints\nfrom keras.engine.topology import Layer\nimport keras.backend as K #to use math functions like \"keras.backend.sum\"\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        \n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","119a980f":"from keras.models import Model #to build the Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation,Bidirectional, CuDNNGRU, CuDNNLSTM # the layers we will use","b7f10590":"# First Model: Unprocessed Data, Basic Structure\nsequence_input = Input(shape=(maxlen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nX = LSTM(128, return_sequences=True)(embedded_sequences)\nX = Dropout(0.5)(X)\nX = LSTM(128)(X)\nX = Dropout(0.5)(X)\nX = Dense(1, activation = \"sigmoid\")(X)\nX = Activation('sigmoid')(X)\n\nmodel = Model(inputs=sequence_input, outputs=X)\nmodel.summary()","e0d8a52b":"# Second Model: Unprocessed Data, Improved Structure\nsequence_input_2 = Input(shape=(maxlen,), dtype='int32')\nembedded_sequences_2 = embedding_layer(sequence_input_2)\nX_2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(embedded_sequences_2)\nX_2 = Dropout(0.5)(X_2)\nX_2 = Bidirectional(CuDNNGRU(128, return_sequences=False))(X_2)\nX_2 = Dropout(0.5)(X_2)\nX_2 = Dense(1)(X_2)\nX_2 = Activation('sigmoid')(X_2)\n\nmodel_2 = Model(inputs = sequence_input_2, outputs=X_2)\nmodel_2.summary()","e8ecb028":"# Third Model: Preprocessed Data, Improved Structure\nsequence_input_3 = Input(shape=(maxlen_2,), dtype='int32')\nembedded_sequences_3 = embedding_layer_2(sequence_input_3)\nX_3 = Bidirectional(CuDNNGRU(128, return_sequences=True))(embedded_sequences_3)\nX_3 = Dropout(0.5)(X_3)\nX_3 = Bidirectional(CuDNNGRU(128, return_sequences=False))(X_3)\nX_3 = Dropout(0.5)(X_3)\nX_3 = Dense(1)(X_3)\nX_3 = Activation('sigmoid')(X_3)\n\nmodel_3 = Model(inputs = sequence_input_3, outputs=X_3)\nmodel_3.summary()\n\nmodel_5 = Model(inputs = sequence_input_3, outputs=X_3)","4d9111c2":"# Fourth Model: Preprocessed Data, With Attention Layer\nsequence_input_4 = Input(shape=(maxlen_2,), dtype='int32')\nembedded_sequences_4 = embedding_layer_2(sequence_input_4)\nX_4 = Bidirectional(CuDNNGRU(128, return_sequences=True))(embedded_sequences_4)\nX_4 = Bidirectional(CuDNNGRU(64, return_sequences=True))(X_4)\nX_4 = Attention(maxlen_2)(X_4)\nX_4 = Dense(64, activation = \"relu\")(X_4)\nX_4 = Dense(1, activation = \"sigmoid\")(X_4)\n\nmodel_4 = Model(inputs = sequence_input_4, outputs = X_4)\nmodel_4.summary()","6d2a5960":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_fmeasure', min_delta=0.0001, patience=2, mode='max')\n","4c279ae9":"model.compile(loss= \"binary_crossentropy\", optimizer='adam', metrics= [fmeasure])\nhistory = model.fit(X_train_seq, Y_train, validation_data=(X_test_seq, Y_test),\n          epochs= 8, batch_size = 512, shuffle = True, callbacks = [early_stopping], verbose = 2)","5904a41c":"model_2.compile(loss= \"binary_crossentropy\", optimizer='adam', metrics= [fmeasure])\nhistory_2 = model_2.fit(X_train_seq, Y_train, validation_data=(X_test_seq, Y_test),\n          epochs= 8, batch_size = 512, shuffle = True, callbacks = [early_stopping], verbose = 2)","283a2cdb":"model_3.compile(loss= \"binary_crossentropy\", optimizer='adam', metrics= [fmeasure])\nhistory_3 = model_3.fit(X_train_seq_2, Y_train, validation_data=(X_test_seq_2, Y_test),\n          epochs= 8, batch_size = 512, shuffle = True, callbacks = [early_stopping], verbose = 2)","8b64a0ba":"model_4.compile(loss= \"binary_crossentropy\", optimizer='adam', metrics= [fmeasure])\nhistory_4 = model_4.fit(X_train_seq_2, Y_train, validation_data=(X_test_seq_2, Y_test),\n          epochs= 8, batch_size = 512, shuffle = True, callbacks = [early_stopping], verbose = 2)","b1ae6f65":"# F1-Measure as loss-function\nmodel_5.compile(loss= loss_f1, optimizer='adam', metrics= [fmeasure])\nhistory_5 = model_5.fit(X_train_seq_2, Y_train, validation_data=(X_test_seq_2, Y_test),\n          epochs= 8, batch_size = 512, shuffle = True, callbacks = [early_stopping], verbose = 2)","156d84c8":"print(history.history.keys())\nplt.plot(history.history[\"fmeasure\"])\nplt.plot(history.history[\"val_fmeasure\"])\nplt.title('Model 1')\nplt.show()","7c13417f":"print(history_2.history.keys())\nplt.plot(history_2.history[\"fmeasure\"])\nplt.plot(history_2.history[\"val_fmeasure\"])\nplt.title('Model 2 -  with improved structure')\nplt.show()","f8116c96":"print(history_3.history.keys())\nplt.plot(history_3.history[\"fmeasure\"])\nplt.plot(history_3.history[\"val_fmeasure\"])\nplt.title('Model 3 -  with improved structure and data preparation')\nplt.show()","688b825e":"print(history_4.history.keys())\nplt.plot(history_4.history[\"fmeasure\"])\nplt.plot(history_4.history[\"val_fmeasure\"])\nplt.title('Model 4 -  with data preparation and attention')\nplt.show()","414d2ec9":"print(history_5.history.keys())\nplt.plot(history_5.history[\"fmeasure\"])\nplt.plot(history_5.history[\"val_fmeasure\"])\nplt.title('Model 5 -  with improved structure, data preparation and f-measure as loss function')\nplt.show()","9a995f7a":"# Comparison of different Models\n","28f7cb16":"An enquiry into the most used words, not represented in the embedding layer, shows that the deviations are largely due to vernacular. ","ed67b077":"Apparently lowecasing worsened and not improved the text quality. As seen in the next box, some words, like *Etherum*, where only not known, because they were lowecased but are names.","641336c4":"#### Normal Keras tokenization\n\nSome of the preprocessing is already done by the tokenizer which was used above (e.g. the text is set lowecase and and some of the punctuation is taken out of the text). The following code shows how equal the text is to the embedding layer after this sort of preprocessing.","d939e626":"The table shows the first rows of a set as a table with the following coloumns: quid (unique question identifier, question_text (guess what) and target (the whether it is an sincere question (0) or not (1).","3ecf648d":"In the end there are now two kinds of datasets which will be used. The dataset which resulted only from the keras tokenizer and the dataset which results from the tweaked tokenizer. The second kind of datasets will be created in the next box","ec9a69b9":"### Finalising the Model","d751f2ff":"However there are still some words unknown to the embedding. Yet these words seem to be names or rather young concepts, therefore it does not seem as if anything could be made against this words.","27eb81e6":"### Fitting the dataset to the embedding layer\nIt is sayed that the text should be preprocessed especially to make it as similar as possible to the Embedding layer (for example [here](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings)). This will be tested in the following, using some of the methods shown in the kaggle just linked. The following code helps in testing on similarities:","3d4feddd":"### Short Data Analysis\nTo understand the Data and get a grab on how to build the Neural Network, a short analyzis of the data set seems necessary. At first the structure of the dataset should be seen.","47771ff0":"### Creating embedding matrix\nNow the two dictionaries word_index and embeddings_index get  joined together into a numpy array, so that it can be loaded into the keras framework. Throught this the  the data mass gets reduced.","6473f4b4":"Because the computer can not compute strings, the words will encoded as numbers. This is done through the Keras Tokenizer. How that works is explained [here](https:\/\/machinelearningmastery.com\/prepare-text-data-deep-learning-keras\/).","45a8ea21":"1. The code for the attention layer was taken from [here](https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043)","3a6cdbd8":"In the following cells, the Glove Embedding layer is loaded into a dictionary. ","d5ebbcd2":"#### Removal of Interpunctiation\nThe keras tokenizer automatically removes [\"all punctuation, plus tabs and line breaks, minus the ' character\"](http:\/\/https:\/\/keras.io\/preprocessing\/text\/). But as shown above, this is not enough. Especially because the \u2018 character gets not removed. Here again [PyTorch Starter](https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter) offers a wider range of possibilities. If the dictionary of this authors is used, the difference to the tokenizer filter is quite large.","4047da3b":"The info function shows amongst other whether there are empty entries (which would require some feature engineering). Here all entries are suitable. Therefore no engineering is necessary. Till now.","c357ec35":"To further improve performance Early stopping is used. When the validation measure does not improve anymore, the model stops learning (thos overfitting is prevented)","c7963b65":"## 3. The Models\n### Definitions","093aed7f":"#### Lower case\/ Upper case\nIn reviewing the keras tokenizer, it was noticable that the tokenizer automatically sets all words in lower case. This might be helpful if a small embedding only knows words in lower case, however it also can be argued that usefull information about these words get lost through the lower casing. Consider a text writing about a hotel called \"Pearl\". If this word would be lowercased, the network could think that a pearl would be meant. That Lowercasing worsenes the score can be seen next","b23261e0":"## 2. Preprocessing data and building the embedding layer\n### Preprocessing\nIn Machine Learning it is common to train the model on one dataset and to test in on another dataset, to ensure that the model is not only reproducing the datra but learning the \"function\" behind the data. This Train-Test_split is done next.","e5b2d077":"At second it seems interesting how sincere questions and how insincere questions look like. ","2756b60e":"The definition of the metric put forward in the paper gets translated into code","e3b08fcb":"Following this Training and Test Sets get divided into X and Y sets. Where X contains the information used to predict the class and Y contains the information which class the question belongs to. The result will be 4 Datasets with one column.","96cdb8fb":"At third a short look on the distribution of sincere and insincere questions should be done. This shows that only a minor fraction of the data has a lable 1. This could lead to aproblem later, because if the system would optimize only according to accuracy (percantage of \"right\" choices), the model would reach a high accuracy with predicting only Sincere (93.8%). Fortunately accuracy is not used in the loss function. ","37606858":"### Building the Structure of the Models","b1fe5d71":"## 1. Loading & Analyzing\n\n### Loading\n","71800f6d":"## 4. The Results\n\n1. How did the f1_score change on the training data and how did it change on the test data","f9bdafca":"#### Correction of vernacular\nIn [PyTorch Starter](https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter) the authors already builded a dictionary with words which need to be replaced and wrote the text to use it. However with the glove embedding this is not necessary, as it recognizes this words, when the points are removed. Glove know's words like arent, couldnt or didnt. Therefore it is only important to remove al this interpunctuation.[](http:\/\/)","cbec78eb":"This array will now be transfered into a Keras Embedding layer","4a0624bc":"The above definition does not work as a loss for a neural network. Therefore it must be tweaked a bit","43a0b4c3":"The following code starts the learning procedure the respective models"}}