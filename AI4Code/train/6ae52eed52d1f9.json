{"cell_type":{"3cf93a48":"code","0732f0b1":"code","05740278":"code","ead3c201":"code","7e361ad3":"code","e12bdc97":"code","6c35246e":"code","d5fbeb8b":"code","ffd3487e":"code","6aef0476":"code","66c96a74":"code","09f591ef":"code","a9fda077":"code","a9b4da35":"code","2709275a":"code","dc0ca4dc":"code","c6843682":"code","82c6a9d4":"code","f426ee51":"code","c9c07a7f":"code","f21d8ffc":"code","f271ab04":"code","140a68ad":"code","6439598c":"code","9d2c90a9":"code","ea2a3bd2":"code","b2afe784":"code","076b9d80":"code","802a4089":"code","c9d9b763":"code","a084f4c4":"code","e211149b":"code","d697b915":"code","02422b6a":"code","7366bf92":"code","54ca3ba1":"code","37651d45":"code","b7a2e05c":"code","9d936bf6":"code","fb8f0364":"code","a9e54b2e":"code","a0806f4f":"code","b8c1080b":"code","a8073922":"code","bda45f39":"code","a6bf5511":"code","2949ecba":"code","d824442d":"code","7e81d07f":"code","2df1cbf4":"code","a7a691a4":"code","29e44f5b":"markdown","5a40fda7":"markdown","400af832":"markdown","49ac9175":"markdown","ceb60d29":"markdown","f3758703":"markdown","a016049c":"markdown","bfb13c9f":"markdown","c884cfbb":"markdown","56db6f72":"markdown","d9b37d56":"markdown","297f66df":"markdown","278df9a6":"markdown","517f5513":"markdown","0b44eaa0":"markdown","54665614":"markdown","54d68878":"markdown","560c3c27":"markdown","8db50067":"markdown","6a84106a":"markdown","9a7bd0d7":"markdown","aa3776c6":"markdown","ab34a90b":"markdown","01372e2f":"markdown","383912f2":"markdown","ff6acf0b":"markdown","43b829a2":"markdown","1b9353bb":"markdown","920275b7":"markdown"},"source":{"3cf93a48":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0732f0b1":"data = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ndata","05740278":"data = data.sample(frac=1, random_state=42)\ndata.set_index(\"Id\", inplace=True)\ndata.head()","ead3c201":"train, test = data[:120], data[120:]","7e361ad3":"train.info()","e12bdc97":"train.describe()","6c35246e":"train.hist(bins=18, figsize=(15,10))","d5fbeb8b":"plt.figure(figsize=(15,8))\nplt.subplot(221)\nplt.title(\"Sepal Length\")\nsns.boxplot(y=\"SepalLengthCm\", x=\"Species\", data=train)\nplt.subplot(222)\nplt.title(\"Sepal Width\")\nsns.boxplot(y=\"SepalWidthCm\", x=\"Species\", data=train)\nplt.subplot(223)\nplt.title(\"Petal Length\")\nsns.boxplot(y=\"PetalLengthCm\", x=\"Species\", data=train)\nplt.subplot(224)\nplt.title(\"Petal Width\")\nsns.boxplot(y=\"PetalWidthCm\", x=\"Species\", data=train)\nplt.tight_layout()","ffd3487e":"train.corr()","6aef0476":"sns.heatmap(train.corr(), cmap=\"RdBu\", vmin=-1.0, vmax=1.0, annot=True)","66c96a74":"X_train = train.drop(\"Species\", axis=1)\ny_train = train[\"Species\"].copy()\n\nX_test = test.drop(\"Species\", axis=1)\ny_test = test[\"Species\"].copy()","09f591ef":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmy_pipeline = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"impute\", SimpleImputer(strategy=\"mean\"))\n])","a9fda077":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    (\"full\", my_pipeline, X_train.columns)\n])","a9b4da35":"X_train_prepared = full_pipeline.fit_transform(X_train)","2709275a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nknn_clf = KNeighborsClassifier()\n\nscores_knn = cross_val_score(knn_clf, X_train_prepared, y_train, cv=4, scoring=\"accuracy\")\nprint(scores_knn)\nprint(\"mean: \", scores_knn.mean())\nprint(\"Std: \", scores_knn.std())","dc0ca4dc":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier()\n\nscores_forest = cross_val_score(forest_clf, X_train_prepared, y_train, cv=4, scoring=\"accuracy\")\nprint(scores_forest)\nprint(\"mean: \", scores_forest.mean())\nprint(\"Std: \", scores_forest.std())","c6843682":"from sklearn.svm import SVC\n\nsvm_clf = SVC()\n\nscores_svm = cross_val_score(svm_clf, X_train_prepared, y_train, cv=4, scoring=\"accuracy\")\nprint(scores_svm)\nprint(\"mean: \", scores_svm.mean())\nprint(\"Std: \", scores_svm.std())","82c6a9d4":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\n\nscores_log = cross_val_score(log_reg, X_train_prepared, y_train, cv=4, scoring=\"accuracy\")\nprint(scores_log)\nprint(\"mean: \", scores_log.mean())\nprint(\"Std: \", scores_log.std())","f426ee51":"forest_clf.fit(X_train_prepared, y_train)\nprint(\"Feature Importances Random Forest\")\nprint(f\"Sepal Length: {forest_clf.feature_importances_[0]}\")\nprint(f\"Sepal Width: {forest_clf.feature_importances_[1]}\")\nprint(f\"Petal Length: {forest_clf.feature_importances_[2]}\")\nprint(f\"Petal Width: {forest_clf.feature_importances_[3]}\")","c9c07a7f":"y_train.value_counts()","f21d8ffc":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\npreds_log = cross_val_predict(log_reg, X_train_prepared, y_train, cv=4)\npreds_forest = cross_val_predict(forest_clf, X_train_prepared, y_train, cv=4)\npreds_svm = cross_val_predict(svm_clf, X_train_prepared, y_train, cv=4)\npreds_knn = cross_val_predict(knn_clf, X_train_prepared, y_train, cv=4)\n\nprint(f\"Actual predictions: {preds_log[:10]}\")","f271ab04":"print(\"Confusion matrix Random Forest\")\nprint(confusion_matrix(y_train, preds_forest))\nprint(\"______\")\nprint(\"Confusion matrix KNN\")\nprint(confusion_matrix(y_train, preds_knn))\nprint(\"______\")\nprint(\"Confusion matrix SVC\")\nprint(confusion_matrix(y_train, preds_svm))\nprint(\"______\")  \nprint(\"Confusion matrix Logistic Regression\")\nprint(confusion_matrix(y_train, preds_log))","140a68ad":"errors_forest = np.array(y_train == preds_forest)\nerrors_knn = np.array(y_train == preds_knn)\nerrors_svm = np.array(y_train == preds_svm)\nerrors_log = np.array(y_train == preds_log)\n\nerror_idxs_forest = list(np.argwhere(errors_forest==False).flatten())\nerror_idxs_knn = list(np.argwhere(errors_knn==False).flatten())\nerror_idxs_svm = list(np.argwhere(errors_svm==False).flatten())\nerror_idxs_log = list(np.argwhere(errors_log==False).flatten())","6439598c":"errors_overall = error_idxs_forest.copy()\nerrors_overall.extend(error_idxs_knn)\nerrors_overall.extend(error_idxs_svm)\nerrors_overall.extend(error_idxs_log)\nerror_idxs_overall = sorted(set(errors_overall))\nprint(error_idxs_overall)","9d2c90a9":"errors = data.iloc[list(error_idxs_overall)]\nerrors","ea2a3bd2":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curves(estimator, X, y):\n    #Get training sizes\n    training_sizes = np.linspace(1, (len(X) * (3\/4)), 10).round().astype(\"int64\")\n    training_sizes[0] = 5\n    #Learning curve values\n    train_sizes, train_scores, val_scores = learning_curve(estimator, X, y_train, train_sizes=training_sizes, \n                                                       cv=4, scoring=\"accuracy\")\n    #get means of cross validation\n    train_scores_mean = train_scores.mean(axis=1)\n    val_scores_mean = val_scores.mean(axis=1)\n    #plot data\n    plt.figure(figsize=(12,6))\n    plt.plot(train_sizes, train_scores_mean, label=\"Training Score\")\n    plt.plot(train_sizes, val_scores_mean, label=\"Validation Score\")\n    plt.title(f\"{estimator}\")\n    plt.xlabel(\"Training Sizes\")\n    plt.ylabel(\"Accuracy\")\n    plt.xlim(0, 95)\n    plt.legend()\n    print(\"Training score means: \", train_scores_mean)\n    print(\"Validation score means: \", val_scores_mean)","b2afe784":"plot_learning_curves(forest_clf, X_train_prepared, y_train)","076b9d80":"plot_learning_curves(knn_clf, X_train_prepared, y_train)","802a4089":"plot_learning_curves(svm_clf, X_train_prepared, y_train)","c9d9b763":"plot_learning_curves(log_reg, X_train_prepared, y_train)","a084f4c4":"train[\"relations_length\"] = train[\"PetalLengthCm\"]**3 \/ train[\"SepalLengthCm\"]\ntrain[\"relations_petal\"] = train[\"PetalLengthCm\"]**3 \/ train[\"PetalWidthCm\"]\ntrain[\"relations_width\"] = train[\"PetalWidthCm\"]**3 \/ train[\"SepalWidthCm\"]\n\ntest[\"relations_length\"] = test[\"PetalLengthCm\"]**3 \/ test[\"SepalLengthCm\"]\ntest[\"relations_petal\"] = test[\"PetalLengthCm\"]**3 \/ test[\"PetalWidthCm\"]\ntest[\"relations_width\"] = test[\"PetalWidthCm\"]**3 \/ test[\"SepalWidthCm\"]","e211149b":"train[[\"relations_length\", \"relations_petal\", \"relations_width\"]].hist(bins=20, figsize=(12,8))","d697b915":"plt.figure(figsize=(15,20))\nplt.subplot(421)\nsns.boxplot(y=\"SepalLengthCm\", x=\"Species\", data=train)\nplt.subplot(422)\nsns.boxplot(y=\"SepalWidthCm\", x=\"Species\", data=train)\nplt.subplot(423)\nsns.boxplot(y=\"PetalLengthCm\", x=\"Species\", data=train)\nplt.subplot(424)\nsns.boxplot(y=\"PetalWidthCm\", x=\"Species\", data=train)\nplt.subplot(425)\nsns.boxplot(y=\"relations_width\", x=\"Species\", data=train)\nplt.subplot(426)\nsns.boxplot(y=\"relations_petal\", x=\"Species\", data=train)\nplt.subplot(427)\nsns.boxplot(y=\"relations_length\", x=\"Species\", data=train)","02422b6a":"X_train_new = train.drop(\"Species\", axis=1)\nX_test_new = test.drop(\"Species\", axis=1)\n\n#full_pipeline_new put maybe later \nfull_pipeline_new = ColumnTransformer([\n    (\"full\", my_pipeline, X_train_new.columns)\n])\n\nX_train_new_prepared = full_pipeline_new.fit_transform(X_train_new)\nX_test_new_prepared = full_pipeline_new.transform(X_test_new)","7366bf92":"from sklearn.model_selection import GridSearchCV\n\nparam_grids = {\n    \"n_estimators\": [10, 12, 15, 20],\n    \"max_depth\": [15, 20, 30, 40],\n    \"max_leaf_nodes\": [5, 8, 10, 20, 30]\n}\n\ngrid_forest = GridSearchCV(forest_clf, param_grids, scoring=\"accuracy\", cv=4)\n\ngrid_forest.fit(X_train_new_prepared, y_train)","54ca3ba1":"grid_forest.best_estimator_","37651d45":"forest_clf = grid_forest.best_estimator_\n\nscores_forest_new = cross_val_score(forest_clf, X_train_new_prepared, y_train, cv=4, scoring=\"accuracy\")\n\nprint(scores_forest_new)\nprint(scores_forest_new.mean())\nprint(scores_forest_new.std())","b7a2e05c":"forest_clf.fit(X_train_new_prepared, y_train)\n\nprint(\"Feature Importances Random Forest\")\nprint(f\"Sepal Length: {forest_clf.feature_importances_[0]}\")\nprint(f\"Sepal Width: {forest_clf.feature_importances_[1]}\")\nprint(f\"Petal Length: {forest_clf.feature_importances_[2]}\")\nprint(f\"Petal Width: {forest_clf.feature_importances_[3]}\")\nprint(f\"Realtions Width: {forest_clf.feature_importances_[4]}\")\nprint(f\"Relations Petal: {forest_clf.feature_importances_[5]}\")\nprint(f\"Relations Length: {forest_clf.feature_importances_[6]}\")","9d936bf6":"param_grids = {\n    \"n_neighbors\": np.arange(1,11),\n    \"weights\": [\"uniform\", \"distance\"],\n    \"leaf_size\": [20, 30, 40]\n}\n\ngrid_knn = GridSearchCV(knn_clf, param_grids, cv=4, scoring=\"accuracy\")\n\ngrid_knn.fit(X_train_new_prepared, y_train)","fb8f0364":"grid_knn.best_estimator_","a9e54b2e":"knn_clf = grid_knn.best_estimator_\n\nscores_knn_new = cross_val_score(knn_clf, X_train_new_prepared, y_train, cv=4, scoring=\"accuracy\")\nprint(scores_knn_new)\nprint(scores_knn_new.mean())\nprint(scores_knn_new.std())","a0806f4f":"param_grids = {\n    \"C\": [1, 10, 30, 100],\n    \"kernel\": [\"rbf\", \"linear\", \"sigmoid\"],\n    \"gamma\": [\"auto\", \"scale\"],\n    \"decision_function_shape\": [\"ovo\", \"ovr\"]\n}\n\ngrid_svm = GridSearchCV(svm_clf, param_grids, scoring=\"accuracy\", cv=4)\n\ngrid_svm.fit(X_train_new_prepared, y_train)","b8c1080b":"grid_svm.best_estimator_","a8073922":"svm_clf = grid_svm.best_estimator_\n\nscores_svm_new = cross_val_score(svm_clf, X_train_new_prepared, y_train, cv=4, scoring=\"accuracy\")\n\nprint(scores_svm_new)\nprint(scores_svm_new.mean())\nprint(scores_svm_new.std())","bda45f39":"log_reg = LogisticRegression()\n\nparam_grids = {\n    \"C\": [1.0, 3.0, 10.0, 50.0],\n    \"max_iter\": [50, 100, 200, 1000],\n    \"multi_class\": [\"auto\", \"ovr\", \"multinomial\"],\n}\n\ngrid_log = GridSearchCV(log_reg, param_grids, cv=4, scoring=\"accuracy\")\n\ngrid_log.fit(X_train_new_prepared, y_train)","a6bf5511":"grid_log.best_estimator_","2949ecba":"log_reg = grid_log.best_estimator_\n\nscores_log_new = cross_val_score(log_reg, X_train_new_prepared, y_train, cv=4, scoring=\"accuracy\")\n\nprint(scores_log_new)\nprint(scores_log_new.mean())\nprint(scores_log_new.std())","d824442d":"print(f\"Before Tuning: {scores_forest.mean()} After Tuning: {scores_forest_new.mean()} (Random Forest)\")\nprint(f\"Before Tuning: {scores_knn.mean()} After Tuning: {scores_knn_new.mean()} (KNN)\")\nprint(f\"Before Tuning: {scores_svm.mean()} After Tuning: {scores_svm_new.mean()} (SVC)\")\nprint(f\"Before Tuning: {scores_log.mean()} After Tuning: {scores_log_new.mean()} (Logistic Regression)\")","7e81d07f":"classifiers = {\n    \"forest_clf\": forest_clf,\n    \"knn_clf\": knn_clf,\n    \"svm_clf\": svm_clf,\n    \"log_reg\": log_reg\n}","2df1cbf4":"def testset_scores(clf, X_train, y_train, X_test, y_test):\n    for key, value in classifiers.items():\n        clf = value\n        clf.fit(X_train, y_train)\n        y_preds = clf.predict(X_test)\n        print(key,\":\", accuracy_score(y_test, y_preds))","a7a691a4":"from sklearn.metrics import accuracy_score\n\ntestset_scores(classifiers, X_train_new_prepared, y_train, X_test_new_prepared, y_test)","29e44f5b":"Logistic Regression has the highest accuracy of the five algorithms, with Random Forest, KNN and SVC right beneath.\n\nAs a next step we want to analyze the importance of the features used. The easiest way is through Random Forest's in-built feature importance attribute **feature_importances_**. It shows us that the _Petal Length & Width_ features are way more important for the classification of the flowers than the _Sepal Length & Width_ features.","5a40fda7":"#### Logistic Regression","400af832":"# Data Exploration \n\nFirst we want to get an overlook over the distribution of the features in our training data.\n\nOur training set now consists of 120 observations. It also seems to have no null-values and all its features are numerical, which has a big impact on the data preparation step following later on.","49ac9175":"As we can see in the histograms the features _\"SepalLengthCm\"_ and _\"SepalWidthCm\"_ look quite normally (gaussian) distributed. However, the features _\"PetalLengthCm\"_ and _\"PetalWidthCm\"_ have lots of very low values and then a gap with no observations (2 - 3 for _Sepal Length_ and 0.6 - 0.9 for _Sepal Width_) and eventually another rough normal distribution at the high end of the distribution.\n\nThe fact that there are lots of low values and then a gap for both _Sepal Length_ and _Sepal Width_ makes it look like one of the flowers has very different sizes of Petals from the other two flowers.","ceb60d29":"The upper row of the boxplot chart below indicates that the distribution of _Sepal Length_ and _Sepal Width_ for the different flowers are quite similar to each other. Nevertheless, _Iris-setosa_ takes on the most extreme values for both features. \n\nThe bottom row confirms the suggestion we made on the basis of the above histograms. _Iris-setosa_ flower seems to have very different _Petal Length & Width_ than the other two flowers. Additionally, it can be seen that the (Iris-setosa flower has on both these features a very narrow distribution (-> short boxplot) in comparison to the ones of _Iris-versicolor_ and _Iris-virginica_, which have a longer boxplot.\n\nThis findings assume that the _Iris-setosa_ flower should be the easiest to classify as it is very well distinguishable from the other two flowers. However, this remains to be seen.","f3758703":"100% accuracy with the Logistic Regression model! Our feature enineering and hyperparamter tuning was definitely worth it.","a016049c":"Now the first thing we have to do is splitting the data into training and test set. We then have to put the test set away without taking a look at it, as in a real world application with new \"unseen\" data, we also don't know its characteristics.\n\nIn the last step we shuffled our data, which is, why it is no problem to put the first 120 examples in the training and the last 30 examples in the test set as they aren't orderd anymore. These numbers represent the common 80% - 20% partition of the data into training and test set.","bfb13c9f":"Now we need to analyze the errors our models make. In a classification problem this can be perfectly done with the confusion matrix. Although Precision\/Recall is better suited to a classification problem with a less well-balanced target variable than we have (see value count below), we can still analyze what kind of mistakes our models did.\n\nWith **cross_val_predict** we are able to compute the actual classes our model outputs, while with **cross_val_score** we just computed the score of the model (e.g. accuracy in our case). We can use these actual predictions to exactly analyze which errors we made.","c884cfbb":"For the next three models it's different. The training accuracy isn't perfect anymore (-> bias is a bit higher), instead the gap between training and validation accuracy is smaller than for the Random Forest model (-> lower variance). Especially the learning curve for the logistic regression look very promising. In general these two models seem to generalize rather well. However, maybe we can get even a slight improvement out of the hypterparamter tuning, so that the bias lowers but the variance is kept at the same level.","56db6f72":"### Hyperparamter Tuning\n\nNow it's time for hypterparamter tuning with **GridSearchCV**. \n\n#### Random Forest","d9b37d56":"Now we want to identify the actual errors our models made. That way we are able to see if they tend to make lots of different errors or rather the same ones.\n\nFirst we compare the predictions of the models to the solutions in y_train. Then we get the indices of the errors and concatenate them in one single set (--> so values don't appear twice). As we can see, our models made eight different errors and again we get the confirmation that all errors are either _Iris-versicolor or -virginica_.","297f66df":"The Random Forest model performs perfectly on the training set, which speaks for low bias of the model. However, it doesn't perform that well on the validation set. This gap between training and validation accuracy is rather big, which means that the model has high variance. In conclusion, we can say that the Random Forest model overfits.","278df9a6":"Below is the heatmap to the correlation table, where darker values imply higher correlations.","517f5513":"Now it's time to preprocess our data.\n\nAbove we saw that all our features are numerical, which means that we have to make only one Pipeline for numerical values. We also noticed that there aren't any missing values within the training set. However, we can't be sure that in the test set aren't any missing values either. Thus, we have to include an imputation step nontheless.\n\nApart from the mean imputation we also need to standardize our data with the **StandardScaler** class from sklearn.","0b44eaa0":"# Baseline Predictions\n\nNow it's time for baseline predictions on different classification algorithms. From there on we can improve our models through feature engineering and hyperparameter tuning. But one step at a time.\n\nTo check for the accuracy of the different learning algorithms we put the number of cross-validation sets to 4. This means that the size of each cross validation set is 30 examples, which matches with the size of the test data.\n\nWe will train four different classification algorithms: K-Nearest Neighbors (KNN), Random Forest Classifier, Support Vector Machine Classifier (SVC) and Logistic Regression_.","54665614":"As a next step, we can take a look at the correlation between the features. \n\n$Sepal Length <-> Petal Length = 88.5 Percent$\n\n$Sepal Length <-> Petal Width = 84 Percent$\n\n$Petal Length <-> Petal Width = 96.5 Percent$\n\nThese three feature combinations have the highest correlation coefficients ranging from 0.81 to 0.96. When taking a thorough look at the boxplots from above two things are noteworthy.\n\n#### 1) Iris-setosa with the lowest values on the respective features\n\nFirstly, the higher the value for _Sepal Length_ the bigger the chance that it is an _Iris-versicolor_ or _Iris-virginica_. These two flowers also have higher values on _Petal Length_ and _Petal Width_. Therefore, a higher value on _Sepal Length_ is related to a higher value on _Petal Length_ and _Petal Width_.\n\nIn contrast, the lower the value for _Sepal Length_ the bigger the chance that it is an _Iris-setosa_, which also has very low values on _Petal Length_ and _Petal Width_. So, a low value on _Sepal Length_ is related to a lower value on _Petal Length_ and _Petal Width_.\n\n#### 2) Similar pattern in all three features\n\nSecondly, it is evident that the three involved features _Sepal Length_, _Petal Length_ and _Petal Width_ all show a smiliar pattern. While _Iris-virginica_ is on the high end of the distribution in all three features, _Iris-versicolor is in the middle and _Iris-setosa_ at the low end.\n\nAs these patterns are similar in all three mentioned features the correlation has to be very strong because, high, middle or low values in _Sepal Length_ correspond with the respective values for _Petal Length and Width_.","54d68878":"### Learning Curve \n\nTo see whether the models under- or overfit, we plot learning curves for all the models. This way, we can see how well the model performs on the training and on the validation set.","560c3c27":"### Feature engineering\n\nSo, now we have seen that our models perform quite well with accuracies from 95% to 96.6%. We also have seen that our four models make a total of 20 errors on eight different examples. In addition, all these errors are made on _Iris-versicolor_ and _Iris-virginica_ flowers. \n\nLast but not least, we have seen that the Random Forest model overfits, while the models of SVC and Logistic Regression have a slightly higher bias. Therefore, we will try to lower the bias of the latter two models through creating new features. This is no problem for the Random Forest Classifier, because we can just regularize it even more than we would have had to without additional features.\n\nThe goal for the feature engineering has to be that the _Iris-versicolor & -virginica_ flowers are better distinguishable from each other. The distributions of these two flowers are less overlapping on the _Petal_ features than on the _Sepal_ features (see boxplots). Therefore, we raise these two features to higher power and divide them through other features. It seems as the features below are the best.\n\nNow we have to add them to training as well as to the test data.","8db50067":"# Test set Results\n\nNow, the final moment has arrived, the testing. We want to test all our models on the test set and see which one performs best. ","6a84106a":"The dataset is sorted by the target column _\"Species\"_ in the order of Iris-setosa, -versicolor, -viriginica. Therefore, we have to shuffle the dataset to assure an even distribution of classes when splitting the dataset into training and test set. We do this with pandas' **sample()** method by setting the fraction = 1. The parameter _\"frac\"_ indicates which fraction of the dataset it outputs.\n\nAfter executing the code we notice that the order of observations has changed. Perfect!","9a7bd0d7":"# Data Preparation\n\nFirst we have to save our features and our target variable in separate dataframes for both the training and the test set.","aa3776c6":"### Confusion Matrix\n\nIn the confusion matrix the rows represent the actual number of the respective class in the data, while the colummns represent the predictions for each of the classes. \n\nIn all confusion matrices the sum of the rows are the same (naturally, as it is like a value count from above!). This means 43 (for _Iris-setosa_), 39 (for _Iris-versicolor_) and 38 (for _Iris-virginica_).\n\nTo analyze the predictions and the errors, let's take a look at the confusion matrix of the SVC. This model predicted 43 examples as _Iris-setosa_, 40 as _Iris-versicolor_ (37+3) and 37 as _Iris-virginica_ (35+2). In total it made 5 mistakes. It classified three examples as _Iris virginica_ (lowest row, middle column), which in fact were _Iris-versicolor_ (middle row, middle column) and it classified two examples as _Iris-versicolor_ (middle row, right column) which actually were _Iris-virginica_ (low row, right column).\n\nIn the end, the most important message is that the model predicts all _Iris-setosa_ correctly (which we assumed at the beginning looking at the boxplots) while it missclassifies _Iris-versicolor_ as _Iris-virginica_ and vice versa. Therefore, our job for the feature engineering will be to create new features, which make the both mentioned flowers better distinguishable.","ab34a90b":"# Error Analysis","01372e2f":"# Get Data","383912f2":"#### K-Nearest Neighbors ","ff6acf0b":"Because the new features were added to the unprepared training set, we have to preprocess the data again.","43b829a2":"Nice! All our models improved through feature engineering and hyperparamter tuning.","1b9353bb":"#### Support Vector Machine Classifier","920275b7":"Two of the additional features _Relations Width & Length_ seem to be rather important."}}