{"cell_type":{"f3bd9c18":"code","4d4104ab":"code","844078f1":"code","fcb3438d":"code","6ffc2777":"code","d149c683":"code","7c4b6fef":"code","f8d5fcab":"code","efeedec1":"code","0af393e1":"code","0c0b72b5":"code","2b05a382":"code","ab7dfd24":"code","a935ee3b":"code","21d05781":"code","c15fa510":"code","79fe9021":"code","b9c65233":"code","7374916c":"code","aa3c8ab0":"code","7cb4fb1a":"code","44f42186":"code","e808e447":"code","470ea681":"code","50413c71":"code","9fb5116d":"markdown","9c711068":"markdown","7e69956a":"markdown","fdb77b2a":"markdown"},"source":{"f3bd9c18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d4104ab":"nRowsRead = 1000  # specify 'None' if want to read whole file\ndata = pd.read_csv(\n    \"..\/input\/cusersmarildownloadsfostercsv\/foster.csv\",\n    delimiter=\";\",\n    encoding=\"utf8\",\n    nrows=nRowsRead,\n)\ndata.dataframeName = \"foster.csv\"\nnRow, nCol = data.shape\nprint(f\"There are {nRow} rows and {nCol} columns\")\ndata.head()","844078f1":"!pip install nb_black -q","fcb3438d":"%load_ext nb_black","6ffc2777":"import plotly.express as px\nimport plotly.figure_factory as ff\nimport os\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","d149c683":"# To plot numerical column\ndef plot_hist(data, column):\n    fig = px.histogram(data, x=column, color=\"RegionID\")\n    fig.show()\n    fig = ff.create_table(pd.DataFrame(data[column].describe()).T)\n    fig.show()\n\n\n# To plot categorical column\ndef plot_count(data, column):\n    df = data.groupby(column)[\"RegionID\"].value_counts()\n    df = pd.DataFrame(df)\n    df.columns = [\"Count\"]\n    df.reset_index(inplace=True)\n    fig = px.bar(\n        df, x=column, y=\"Count\", color=\"RegionID\", text=\"Count\", barmode=\"group\"\n    )\n    fig.show()","7c4b6fef":"plot_hist(data, \"RegionID\")","f8d5fcab":"plot_count(data, \"AreaID\")","efeedec1":"plot_count(data, \"Area\")","0af393e1":"plot_hist(data, \"Jan\")","0c0b72b5":"plot_hist(data, \"Dec\")","2b05a382":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n\nenc = OneHotEncoder(handle_unknown=\"ignore\")\nstander_scaler = StandardScaler()\nlabel_encoder = LabelEncoder()\n\nX = np.concatenate(\n    (\n        ## OneHotEncoder\n        enc.fit_transform(data[[\"Area\"]]).toarray(),\n        ## Stander Scaler\n        stander_scaler.fit_transform(\n            data[\n                [\n                    \"Jan\",\n                    \"Feb\",\n                    \"Mar\",\n                    \"Apr\",\n                    \"May\",\n                    \"Jun\",\n                    \"Jul\",\n                    \"Aug\",\n                    \"Sep\",\n                    \"Oct\",\n                    \"Nov\",\n                    \"Dec\",\n                ]\n            ]\n        ),\n        ## LabelEncoder\n        label_encoder.fit_transform(data[[\"Area\"]]).reshape(-1, 1),\n        ## No formatation\n        data[[\"RegionID\", \"AreaID\"]].values,\n    ),\n    axis=1,\n)\n\ny = data.RegionID.values\nX.shape","ab7dfd24":"columns = (\n    [el for el in enc.categories_[0]]\n    + [\n        \"Jan\",\n        \"Feb\",\n        \"Mar\",\n        \"Apr\",\n        \"May\",\n        \"Jun\",\n        \"Jul\",\n        \"Aug\",\n        \"Sep\",\n        \"Oct\",\n        \"Nov\",\n        \"Dec\",\n    ]\n    + [\"Area\"]\n    + [\"RegionID\", \"AreaID\"]\n    + [\"RegionID\"]\n)","a935ee3b":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ntable = pd.DataFrame(np.concatenate([X, y.reshape(-1, 1)], axis=1))\ntable.columns = columns\ntable = table.corr()\nwith sns.axes_style(\"white\"):\n    mask = np.zeros_like(table)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(\n        round(table, 2),\n        cmap=\"Reds\",\n        mask=mask,\n        vmax=table.max().max(),\n        vmin=table.min().min(),\n        linewidths=0.5,\n        annot=True,\n        annot_kws={\"size\": 12},\n    ).set_title(\"Correlation Matrix App behavior dataset\")","21d05781":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D\nimport keras\n\n\ndef get_model():\n    return Sequential(\n        [\n            Dense(units=200, input_dim=12, activation=\"relu\"),\n            Dense(150, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(1, activation=\"sigmoid\"),\n        ]\n    )\n\n\ndef train_ann(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.1, random_state=42\n    )\n\n    model = get_model()\n\n    model.compile(\n        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"mse\", \"accuracy\"],\n    )\n\n    # Trainig and returning back the results.\n    history = model.fit(\n        X_train,\n        y_train,\n        batch_size=10,\n        epochs=50,\n        verbose=0,\n        validation_data=(X_test, y_test),\n    )\n    loss, mse, acc = model.evaluate(X_test, y_test, verbose=0)\n    fig = ff.create_table(\n        pd.DataFrame([(loss, mse, acc)], columns=[\"Loss\", \"MSE\", \"Accuracy\"]),\n    )\n    fig.show()","c15fa510":"from imblearn.over_sampling import SMOTE\n\nX, y = SMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","79fe9021":"from imblearn.over_sampling import RandomOverSampler\n\nX, y = RandomOverSampler(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","b9c65233":"from imblearn.over_sampling import BorderlineSMOTE\n\nX, y = BorderlineSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","7374916c":"from imblearn.over_sampling import ADASYN\n\nX, y = ADASYN(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","aa3c8ab0":"from imblearn.over_sampling import KMeansSMOTE\n\nX, y = KMeansSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","7cb4fb1a":"from imblearn.over_sampling import SVMSMOTE\n\nX, y = SVMSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","44f42186":"from imblearn.under_sampling import ClusterCentroids\n\nX, y = ClusterCentroids(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","e808e447":"from imblearn.under_sampling import AllKNN\n\nX, y = AllKNN().fit_resample(X, y)\n\ntrain_ann(X, y)","470ea681":"from imblearn.under_sampling import NeighbourhoodCleaningRule\n\nX, y = NeighbourhoodCleaningRule().fit_resample(X, y)\n\ntrain_ann(X, y)","50413c71":"from imblearn.under_sampling import RandomUnderSampler\n\nX, y = RandomUnderSampler().fit_resample(X, y)\n\ntrain_ann(X, y)","9fb5116d":"Added to my list: learn how to deal with imbalanced data.","9c711068":"#Imbalanced Data\n\nImbalanced data typically refers to a problem with classification problems where the classes are not represented equally.\n\nFor example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n\nThis is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.\n\nYou can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either.\nhttps:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/","7e69956a":"![](https:\/\/miro.medium.com\/max\/450\/1*zsyN08VVrgHbAEdvv27Pyw.png)medium.com","fdb77b2a":"#Codes by Marco Carujo. https:\/\/www.kaggle.com\/mcarujo\/churn-prediction-ann-over-under-sampling"}}