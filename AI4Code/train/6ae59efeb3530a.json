{"cell_type":{"4b8f84df":"code","330a0c01":"code","75a7122c":"code","f0aff33e":"code","c694459d":"code","affb2d0a":"code","c5c462cb":"code","17c1eff3":"code","b4de585a":"code","1e0b5753":"code","b9f6a658":"code","71a39efc":"code","6cd752f8":"markdown"},"source":{"4b8f84df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","330a0c01":"train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id')","75a7122c":"import os\nimport numpy as np \nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f0aff33e":"def addFeatures(df):\n    #horizontal and vertical distance to hydrology can be easily combined\n    cols = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology']\n    df['distance_to_hydrology'] = df[cols].apply(np.linalg.norm, axis=1)\n    \n    #adding a few combinations of distance features to help enhance the classification\n    cols = ['Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points',\n            'Horizontal_Distance_To_Hydrology']\n    df['distance_mean'] = df[cols].mean(axis=1)\n    df['distance_sum'] = df[cols].sum(axis=1)\n    df['distance_dif_road_fire'] = df[cols[0]] - df[cols[1]]\n    df['distance_dif_hydro_road'] = df[cols[2]] - df[cols[0]]\n    df['distance_dif_hydro_fire'] = df[cols[2]] - df[cols[1]]\n    \n    #taking some factors influencing the amount of radiation\n    df['cosine_of_slope'] = np.cos(np.radians(df['Slope']) )\n    #X['Diff_azimuth_aspect_9am'] = np.cos(np.radians(123.29-X['Aspect']))\n    #X['Diff_azimuth_aspect_12noon'] = np.cos(np.radians(181.65-X['Aspect']))\n    #X['Diff_azimuth_aspect_3pm'] = np.cos(np.radians(238.56-X['Aspect']))\n\n    #sum of Hillshades\n    shades = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    #df['Sum_of_shades'] = df[shades].sum(1)\n    weights = pd.Series([0.299, 0.587, 0.114], index=cols)\n    df['hillshade'] = (df[shades]*weights).sum(1)\n\n    df['elevation_vdh'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    print('Total number of features : %d' % (df.shape)[1])\n    return df","c694459d":"def preprocessData(train, test):\n\n    y_train = train['Cover_Type']\n    \n    classes = train.Cover_Type.unique()\n    num_classes = len(classes)\n    print(\"There are %i classes: %s \" % (num_classes, classes))\n    train.drop(['Cover_Type'], axis=1, inplace=True)\n\n    train = addFeatures(train)    \n    test = addFeatures(test)\n\n    dtrn_first_ten = train.loc[:,:'Horizontal_Distance_To_Fire_Points']\n    dtrn_wa_st = train.loc[:,'Wilderness_Area1':'Soil_Type40']\n    dtrn_added_features = train.loc[:,'distance_to_hydrology':]\n    dtrn_ = pd.concat([dtrn_first_ten,dtrn_added_features,dtrn_wa_st],axis=1)\n\n    dtst_first_ten = test.loc[:,:'Horizontal_Distance_To_Fire_Points']\n    dtst_wa_st = test.loc[:,'Wilderness_Area1':'Soil_Type40']\n    dtst_added_features = test.loc[:,'distance_to_hydrology':]\n    dtst_ = pd.concat([dtst_first_ten,dtst_added_features,dtst_wa_st],axis=1)\n    \n    # elevation was found to have very different distributions on test and training sets\n    # lets just drop it for now to see if we can implememnt a more robust classifier!\n    #train = train.drop('Elevation', axis=1)\n    #test = test.drop('Elevation', axis=1)    \n\n    return dtrn_, dtst_, y_train","affb2d0a":"X, test, y = preprocessData(train, test)","c5c462cb":"weights = [11.393577400361757, 1.4282825089634368, 0.6063107664752647, 1, 1.916980442614397, \n1.0945477432742674, 1.668754279754504, 1.7520168478233817, 8.207420802921982, \n0.7501841943847916, 1.9971420119714571, 2.72057743717325, 2.0, 1.575220244799055, \n2.0695773922466643, 2.536316322049836, 0.46168425088806536, 0.4420755307264942, \n10.660977569012896, 876.0230240897795, 795.52134403456]","17c1eff3":"from sklearn.neighbors import KNeighborsClassifier\nclf1 = KNeighborsClassifier(n_neighbors=1,p=1)\n\nX_copy = X.copy()\ntest_copy = test.copy()\n\nfor i in range(19):\n    c = X.columns[i]\n    X_copy[c] = weights[i]*X_copy[c]\n    test_copy[c] = weights[i]*test_copy[c]\nfor i in range(19,23):\n    c = X.columns[i]\n    X_copy[c] = weights[19]*X_copy[c]\n    test_copy[c] = weights[19]*test_copy[c]\nfor i in range(23,len(X.columns)):\n    c = X.columns[i]\n    X_copy[c] = weights[20]*X_copy[c]\n    test_copy[c] = weights[20]*test_copy[c]","b4de585a":"from sklearn.ensemble import RandomForestClassifier\nclf2 = RandomForestClassifier(n_estimators=300, max_features='sqrt', bootstrap=False,max_depth=60,\n                              min_samples_split=2,min_samples_leaf=1,random_state=1)\nfrom sklearn.ensemble import ExtraTreesClassifier\nclf3 = ExtraTreesClassifier(n_estimators=400,max_depth=50,min_samples_split=5,\n                             min_samples_leaf=1,max_features=63,random_state=1)\nfrom lightgbm import LGBMClassifier\nclf4 = LGBMClassifier(num_leaves=109,objective='multiclass',num_class=7,\n                       learning_rate=0.2,random_state=1)\n","1e0b5753":"from mlxtend.classifier import EnsembleVoteClassifier\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3, clf4])\nlabels = ['KNeighbors', 'Random Forest', 'Extra Trees', 'LGBM', 'Ensemble']\nfor clf, label in zip([clf1, clf2, clf3, clf4, eclf], labels):\n    scores = cross_val_score(clf, X_copy, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.3f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","b9f6a658":"eclf.fit(X_copy,y)\npreds_test = eclf.predict(test_copy)\nprint(preds_test[:10])","71a39efc":"# Make the submission file\noutput = pd.DataFrame({'Id': test.index,'Cover_type': preds_test})\noutput.to_csv('submission.csv', index=False)","6cd752f8":"# Trying some shared notebooks.\n\n### Desperately trying to improve knn accuracy. This [notebook](https:\/\/www.kaggle.com\/chrisfreiling\/nearest-neighbor-kicks-ass-2) was a definite help."}}