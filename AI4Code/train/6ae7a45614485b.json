{"cell_type":{"c4be7ffc":"code","a68ad96b":"code","8f061d7f":"code","e7373bd6":"code","f58052b8":"code","f8b353bb":"code","b2ff2980":"code","cb371e02":"code","077bed5c":"code","3afe7752":"code","6491cc3b":"code","79f09bab":"code","98e9de75":"code","3d4ebe3b":"code","29c3118b":"code","93d604e5":"code","c2a3308c":"code","8fbd0a8b":"code","5cb1c5fc":"code","a4f7e342":"code","71555fea":"code","96c2b9e5":"markdown","60469b87":"markdown","0e24c4f1":"markdown","0cf5f868":"markdown","14acd374":"markdown","95d44b54":"markdown","5785e6e7":"markdown","3b95352b":"markdown","814aba62":"markdown","c5afbeef":"markdown","0fb4dc05":"markdown","0eddf1ce":"markdown"},"source":{"c4be7ffc":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","a68ad96b":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntrain_df.set_index('id', inplace=True)\nprint(f\"train_df: {train_df.shape}\")\ntrain_df.head()","8f061d7f":"test_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\ntest_df.set_index('id', inplace=True)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","e7373bd6":"feats = []\nfor col in tqdm(train_df.columns):\n    feats.append([\n        col,\n        train_df[col].nunique(),\n        train_df[col].mean(),\n        train_df[col].min(),\n        train_df[col].max(),\n        train_df[col].corr(train_df['target'])\n    ])\n\ndf = pd.DataFrame(feats, columns=['Col','Num Unique','Mean','Min','Max','Corr'])\ndf.sort_values(by=['Corr'], ascending=False, key=abs).head(25)","f58052b8":"cat_cols = df[(df['Num Unique']<5)&(df['Col']!='target')]['Col'].unique().tolist()\nnum_cols = [col for col in test_df.columns if col not in cat_cols]\nprint(f\"cat_cols: {len(cat_cols)} \\nnum_cols: {len(num_cols)}\")\n\ndel df\ngc.collect()","f8b353bb":"plt.figure(figsize=(12,5))\nsns.countplot(x='f22', hue='target', data=train_df)\nplt.title(\"Countplot - f22 vs target\", fontweight='bold', pad=15);","b2ff2980":"plt.figure(figsize=(12,5))\nsns.kdeplot(x='f179', hue='target', data=train_df)\nplt.title(\"Countplot - f179 vs target\", fontweight='bold', pad=15);","cb371e02":"train_df['mean'] = train_df[num_cols].mean(axis=1)\ntrain_df['std'] = train_df[num_cols].std(axis=1)\ntrain_df['min'] = train_df[num_cols].min(axis=1)\ntrain_df['max'] = train_df[num_cols].max(axis=1)\n\ntest_df['mean'] = test_df[num_cols].mean(axis=1)\ntest_df['std'] = test_df[num_cols].std(axis=1)\ntest_df['min'] = test_df[num_cols].min(axis=1)\ntest_df['max'] = test_df[num_cols].max(axis=1)\n\nprint(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")\n\nfeatures = test_df.columns.tolist()\nprint(f\"Num features: {len(features)}\")\n\ncat_cols_indices = [train_df.columns.get_loc(col) for col in cat_cols]\nprint(f\"cat_cols_indices: {cat_cols_indices}\")","077bed5c":"for col in tqdm(num_cols):\n    transformer = QuantileTransformer(n_quantiles=3000, \n                                      random_state=42, \n                                      output_distribution=\"normal\")\n    \n    vec_len = len(train_df[col].values)\n    vec_len_test = len(test_df[col].values)\n\n    raw_vec = train_df[col].values.reshape(vec_len, 1)\n    test_vec = test_df[col].values.reshape(vec_len_test, 1)\n    transformer.fit(raw_vec)\n    \n    train_df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_df[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n\nprint(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")","3afe7752":"INT8_MIN    = np.iinfo(np.int8).min\nINT8_MAX    = np.iinfo(np.int8).max\nINT16_MIN   = np.iinfo(np.int16).min\nINT16_MAX   = np.iinfo(np.int16).max\nINT32_MIN   = np.iinfo(np.int32).min\nINT32_MAX   = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail=1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() \/ (1024*1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    length_interval      = 50\n    length_float_decimal = 4\n\n    print('='*length_interval)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, length_float_decimal)), str(np.round(col_max, length_float_decimal))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                print('='*length_interval)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN\/2) and (col_max < INT8_MAX\/2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('='*length_interval)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n    \n    return data","6491cc3b":"train_df = compress_dataset(train_df)","79f09bab":"test_df = compress_dataset(test_df)","98e9de75":"FOLD = 5\nSEEDS = [29, 47]\n\nxgb_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'gpu_hist', \n    'gpu_id': 0, \n    'predictor': 'gpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01063045229441343, \n    'gamma': 0.24652519525750877, \n    'max_depth': 4, \n    'min_child_weight': 366, \n    'subsample': 0.6423040816299684, \n    'colsample_bytree': 0.7751264493218339, \n    'colsample_bylevel': 0.8675692743597421, \n    'lambda': 0, \n    'alpha': 10,\n    'random_state': 2021\n}\n\ncb_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0,\n    'random_state': 2021\n}\n\nlgb_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'gpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 300, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2,\n    'random_state': 2021\n}","3d4ebe3b":"counter = 0\noof_score = 0\ny_pred_final_xgb = np.zeros((test_df.shape[0], 1))\ny_pred_meta_xgb = np.zeros((train_df.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(train_df[features], train_df['target'])):\n        counter += 1\n\n        train_x, train_y = train_df[features].iloc[train], train_df['target'].iloc[train]\n        val_x, val_y = train_df[features].iloc[val], train_df['target'].iloc[val]\n\n        model = XGBClassifier(**xgb_params)\n\n        model.fit(train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], \n                  early_stopping_rounds=200, verbose=1000)\n        \n        y_pred = model.predict_proba(val_x, iteration_range=(0, model.best_iteration))[:,-1]\n        y_pred_meta_xgb[val] += np.array([y_pred]).T\n        y_pred_final_xgb += np.array([model.predict_proba(test_df, iteration_range=(0, model.best_iteration))[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nXGBoost | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nXGBoost | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_xgb = y_pred_meta_xgb \/ float(len(SEEDS))\ny_pred_final_xgb = y_pred_final_xgb \/ float(counter)\noof_score \/= float(counter)\nprint(\"XGBoost | Aggregate OOF Score: {}\".format(oof_score))","29c3118b":"counter = 0\noof_score = 0\ny_pred_final_cb = np.zeros((test_df.shape[0], 1))\ny_pred_meta_cb = np.zeros((train_df.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(train_df[features], train_df['target'])):\n        counter += 1\n\n        train_x, train_y = train_df[features].iloc[train], train_df['target'].iloc[train]\n        val_x, val_y = train_df[features].iloc[val], train_df['target'].iloc[val]\n\n        model = CatBoostClassifier(**cb_params)\n\n        model.fit(train_x, train_y, eval_set=[(val_x, val_y)],\n                  cat_features=cat_cols_indices,\n                  early_stopping_rounds=200, verbose=500)\n\n        y_pred = model.predict_proba(val_x)[:,-1]\n        y_pred_meta_cb[val] += np.array([y_pred]).T\n        y_pred_final_cb += np.array([model.predict_proba(test_df)[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nCatBoost | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nCatBoost | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_cb = y_pred_meta_cb \/ float(len(SEEDS))\ny_pred_final_cb = y_pred_final_cb \/ float(counter)\noof_score \/= float(counter)\nprint(\"CatBoost | Aggregate OOF Score: {}\".format(oof_score))","93d604e5":"counter = 0\noof_score = 0\ny_pred_final_lgb = np.zeros((test_df.shape[0], 1))\ny_pred_meta_lgb = np.zeros((train_df.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(train_df[features], train_df['target'])):\n        counter += 1\n\n        train_x, train_y = train_df[features].iloc[train], train_df['target'].iloc[train]\n        val_x, val_y = train_df[features].iloc[val], train_df['target'].iloc[val]\n\n        model = LGBMClassifier(**lgb_params)\n        \n        model.fit(train_x, train_y, eval_metric='auc',\n                  eval_set=[(train_x, train_y), (val_x, val_y)],\n                  categorical_feature=cat_cols_indices, \n                  early_stopping_rounds=200, verbose=200)\n\n        y_pred = model.predict_proba(val_x, num_iteration=model.best_iteration_)[:,-1]\n        y_pred_meta_lgb[val] += np.array([y_pred]).T\n        y_pred_final_lgb += np.array([model.predict_proba(test_df, num_iteration=model.best_iteration_)[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nLightGBM | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nLightGBM | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_lgb = y_pred_meta_lgb \/ float(len(SEEDS))\ny_pred_final_lgb = y_pred_final_lgb \/ float(counter)\noof_score \/= float(counter)\nprint(\"LightGBM | Aggregate OOF Score: {}\".format(oof_score))","c2a3308c":"np.savez_compressed('.\/TPS_1021_Meta_Features.npz',\n                    y_pred_meta_xgb=y_pred_meta_xgb, \n                    y_pred_meta_cb=y_pred_meta_cb, \n                    y_pred_meta_lgb=y_pred_meta_lgb, \n                    y_pred_final_xgb=y_pred_final_xgb,\n                    y_pred_final_cb=y_pred_final_cb,\n                    y_pred_final_lgb=y_pred_final_lgb)","8fbd0a8b":"submit_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final_xgb.ravel()\nsubmit_df.to_csv(\"XGB_Submission.csv\", index=False)\nsubmit_df.head()","5cb1c5fc":"submit_df['target'] = y_pred_final_cb.ravel()\nsubmit_df.to_csv(\"CB_Submission.csv\", index=False)\nsubmit_df.head()","a4f7e342":"submit_df['target'] = y_pred_final_lgb.ravel()\nsubmit_df.to_csv(\"LGB_Submission.csv\", index=False)\nsubmit_df.head()","71555fea":"submit_df['target'] = (y_pred_final_xgb * 0.55) + (y_pred_final_cb * 0.15) + (y_pred_final_lgb * 0.3)\nsubmit_df.to_csv(\"WA_Submission.csv\", index=False)\nsubmit_df.head()","96c2b9e5":"## Data Exploration","60469b87":"## Model Hyperparameters","0e24c4f1":"## Save meta features","0cf5f868":"## Import libraries","14acd374":"## XGBoost Model","95d44b54":"## Create submission files","5785e6e7":"## Quantile Transformation","3b95352b":"## Load source datasets","814aba62":"## Feature Engineering","c5afbeef":"## LightGBM Model","0fb4dc05":"## CatBoost Model","0eddf1ce":"## Compress data"}}