{"cell_type":{"3acc9df3":"code","b56b070b":"code","f0de84c1":"code","6359fba8":"code","21979bef":"code","2cd06cfa":"code","7a1b71b1":"code","fc548eaf":"code","93eef463":"code","f3f77393":"code","5cbdcedf":"code","f9fe9d5a":"code","9e9852d9":"code","0c1493d7":"code","5e633b14":"code","90ed0df1":"code","a59c890a":"code","dc90091d":"code","de6b7b28":"code","63998714":"code","a2fe0fd6":"code","a5745e44":"code","3c4b58ef":"code","2b297d30":"code","e8fb591c":"code","c6f00df0":"code","5f299534":"code","b47fa3e5":"code","11925c01":"code","6b2ce71e":"code","b843a6c8":"code","73ebeb6f":"code","a99cbfd7":"code","9da70bcb":"code","f6c89ff0":"code","58137ddb":"code","51c5bb8c":"code","345093a0":"code","b27dbd3f":"code","f6f0f715":"code","c19c0110":"code","dd067eae":"code","35c1652c":"code","b2100a5d":"code","cddb74bc":"markdown","e51ce449":"markdown","0374b380":"markdown","5295dd30":"markdown","b58981d8":"markdown","0fd4884c":"markdown","273790f9":"markdown","e4560df1":"markdown","fe36976c":"markdown","855b3e26":"markdown","b332935f":"markdown","78541e77":"markdown","19928341":"markdown","3856f981":"markdown","ada6f2b7":"markdown","bf11c680":"markdown","cc8ea3a1":"markdown","8aa5d097":"markdown","a5f40a29":"markdown","a644bec9":"markdown","572f6ebd":"markdown","9ab6eea5":"markdown","df63958f":"markdown"},"source":{"3acc9df3":"# Data analysis and wrangling\nimport numpy as np \nimport pandas as pd \n\npd.options.display.max_columns = 100\n\nimport re \nimport random as rnd\nfrom pprint import pprint\n\nSEED = 123\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\");","b56b070b":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_combined = [df_train, df_test]\ndf_all = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set'\n\ndf_submission = pd.DataFrame(columns=['PassengerId', 'Survived'])\ndf_submission['PassengerId'] = df_test['PassengerId']\n\nprint('*'*40)\nprint('Training data shape is {}'.format(df_train.shape))\nprint(df_train.columns)\nprint('*'*40)\nprint('Test data shape is {}'.format(df_test.shape))\nprint(df_test.columns)\nprint('*'*40)\nprint('All data shape is {}'.format(df_all.shape))\n","f0de84c1":"print('*'*40)\nprint(df_train.info())\nprint('*'*40)\nprint(df_test.info())","6359fba8":"print(\"Missing Data in Training Dataset\\n\")\nprint(df_train.isnull().sum())\nprint('*'*100)\nprint(\"Missing Data in Test Dataset\\n\")\nprint(df_test.isnull().sum())","21979bef":"fg = sns.FacetGrid(df_train, col = 'Survived', size=3.2, aspect=1.5)\nfg.map(sns.distplot, 'Age')","2cd06cfa":"#df_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n#df_all.isnull().sum()","7a1b71b1":"df_corr = df_all.corr().abs().unstack().sort_values(ascending = False).reset_index()\ndf_corr[df_corr['level_0'] == 'Fare']","fc548eaf":"df_all['Fare'] = df_all.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))\ndf_all.groupby('Pclass')['Fare'].median()","93eef463":"print(df_all.Embarked.unique())\nprint(\"Most common port of embarkation is :\", df_all.Embarked.mode()[0])\n\n#Find out the details of those who are missing embarkation details\ndf_all.loc[pd.isnull(df_all['Embarked']), 'Embarked'] = 'S'","f3f77393":"# Check to make sure there are no missing values, drop cabin and passengerid\ndf_train.isnull().sum()\ndf_test.isnull().sum()\ndf_all.isnull().sum()","5cbdcedf":"#How many people survived in the training dataset? \nprint(\"{:.2f}% survived\\n\".format(df_train['Survived'].mean()*100.0))\n\n# How many survived by sex (normalized)?\nprint(pd.crosstab(df_train[\"Sex\"], df_train['Survived'], normalize =True).round(2))\nprint(\"\\n\")\n\n# How many survived by class (normalized)?\nprint(pd.crosstab(df_train['Pclass'], df_train['Survived'], normalize = True).round(2))\nprint(\"\\n\")\n\n# How many survived by class and sex (normalized)?\npd.crosstab(df_train['Pclass'], [df_train['Survived'], df_train['Sex']], normalize = True).round(2)\nprint(\"\\n\")\n\n# How many survived by Parch (normalized)?\npd.crosstab(df_train['Parch'], df_train['Survived'], normalize = True).round(2)\nprint(\"\\n\")\n\n# How many survived by SibSp (normalized)?\npd.crosstab(df_train['SibSp'], df_train['Survived'], normalize = True).round(2)\n\ndata_crosstab = pd.crosstab([df_train.Sex, df_train.Pclass, df_train.SibSp], df_train.Survived, margins = False)\nprint(data_crosstab)","f9fe9d5a":"df_train = df_all[:890].reset_index(drop=True)\ndf_test = df_all[891:].reset_index(drop=True)\ndf_test.drop(['Survived'], axis = 1, inplace=True)\ndf_train.isnull().sum()","9e9852d9":"fig, axs = plt.subplots(nrows = 2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","0c1493d7":"fig, axs = plt.subplots(ncols = 1, nrows = 3, figsize = (20, 5))\n\nplt.subplot(1, 3, 1)\nsns.countplot(\"Pclass\", hue = 'Survived', data = df_train)\nplt.xlabel(\"Pclass\", size = 10, labelpad = 5)\nplt.ylabel(\"Passenger Count\", size = 10, labelpad = 5)\nplt.legend([\"died\", \"survived\"], loc =\"upper left\")\n\nplt.subplot(1, 3, 2)\nsns.countplot(\"Parch\", hue = 'Survived', data = df_train)\nplt.xlabel(\"Parch\", size = 10, labelpad = 5)\nplt.ylabel(\"Passenger Count\", size = 10, labelpad = 5)\nplt.legend([\"died\", \"survived\"], loc =\"upper right\")\n\nplt.subplot(1, 3, 3)\nsns.countplot(\"SibSp\", hue = 'Survived', data = df_train)\nplt.xlabel(\"SibSp\", size = 10, labelpad = 5)\nplt.ylabel(\"Passenger Count\", size = 10, labelpad = 5)\nplt.legend([\"died\", \"survived\"], loc =\"upper right\")","5e633b14":"fig = plt.figure(figsize=(20, 5))\nsns.violinplot(x = 'Sex', y='Age', hue = 'Survived', data=df_all[:890], split=True, palette = 'muted', inner = 'quartile'); ","90ed0df1":"fig = plt.figure(figsize=(20, 5))\ng = sns.catplot(x=\"Sex\", y=\"Fare\",\n                hue=\"Survived\", col=\"Pclass\",\n                data=df_all[:890], kind=\"violin\", split=True);","a59c890a":"fig = plt.figure(figsize=(20, 5))\ng = sns.catplot(x=\"Sex\", y=\"Age\",\n                hue=\"Survived\", col=\"Embarked\",\n                data=df_all[:890], kind=\"violin\", split=True);\n","dc90091d":"# What is the breakdown by Pclass, Sex and Age\nfig = plt.figure(figsize=(20, 5))\ng = sns.catplot(x=\"Sex\", y=\"Age\",\n                hue=\"Survived\", col=\"Pclass\",\n                data=df_all[:890], kind=\"violin\", split=True);\n","de6b7b28":"# Creation of new feature representing the members in a family associated with a passenger\ndf_all['Family'] = df_all['SibSp'] + df_all['Parch'] + 1\n    \nfig = plt.figure(figsize=(20, 5))\nsns.violinplot(x = 'Family', y='Age', hue = 'Survived', data=df_all[:890], split=True, palette = 'muted', inner = 'quartile'); \n\n# Introducing other features based on the family size\ndf_all['Singleton']   = df_all['Family'].map(lambda s: 1 if s == 1 else 0)\ndf_all['SmallFamily'] = df_all['Family'].map(lambda s: 1 if 2 <= s <= 5 else 0)\ndf_all['MediumFamily'] = df_all['Family'].map(lambda s: 1 if 6 <= s <= 6 else 0)\ndf_all['LargeFamily'] = df_all['Family'].map(lambda s: 1 if 7 <= s else 0)    ","63998714":"fig = plt.figure(figsize=(20, 5))\nn, bins, patches = plt.hist(x=df_all.groupby(['Ticket'])['Pclass'].count(),\n                            bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5],\n                            alpha=0.7, rwidth=0.85)\n\nplt.xlabel('Passengers per Ticket')\nplt.ylabel('Passenger Count')\nplt.title('Passengers per Ticket Histogram')","a2fe0fd6":"PassengersPerTicket = df_all.groupby('Ticket')['Fare'].count()\ndf_all['SingleFare'] = (df_all['Fare']\/[PassengersPerTicket[ticket] for ticket in df_all['Ticket']]).round(2)\ndf_all['SingleFare'].fillna(df_all.groupby(['Embarked','Pclass'])['SingleFare'].median, inplace = True)\ndf_all['SingleFareBin'] = pd.cut(df_all['SingleFare'].astype(int), 8)\ndf_all['FareBin'] = pd.cut(df_all['Fare'].astype(int), 8)","a5745e44":"mapdict = {\n           \"Master\": \"Master\", \n           \"Jonkheer\": \"Master\",\n           \"Miss\": \"Miss\", \n           \"Mlle\": \"Miss\",  \n           \"Mrs\": \"Mrs\", \n           \"Mme\": \"Mrs\", \n           \"Ms\": \"Mrs\",\n           \"Mr\": \"Mr\", \n           \"Capt\": \"Military\", \n           \"Col\": \"Military\", \n           \"Major\": \"Military\",\n           \"Dr\": \"Professional\", \n           \"Rev\": \"Professional\",        \n           \"the Countess\" : \"Aristocratic\", \n           \"Lady\": \"Aristocratic\", \n           \"Don\": \"Aristocratic\",\n           \"Dona\" : \"Aristocratic\",\n           \"Sir\": \"Aristocratic\"           \n          }\n\ndf_all['Title'] = df_all['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]        \ndf_all['Title'] = df_all.Title.map(mapdict)","3c4b58ef":"grpSPT = df_all.iloc[:891].groupby(['Sex','Pclass','Title'])\ngrpSPT_median = grpSPT.median()\ngrpSPT_median = grpSPT_median.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\ngrpSPT_median","2b297d30":"def add_age(row):\n    return grpSPT_median[(grpSPT_median['Sex'] == row['Sex']) & \n                         (grpSPT_median['Title'] == row['Title']) & \n                         (grpSPT_median['Pclass'] == row['Pclass'])]['Age'].values[0]\n    \n# a function that fills the missing values of the Age variable\ndf_all['Age'] = df_all.apply(lambda row: add_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n\n#df_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\ndf_all['AgeBin'] = pd.cut(df_all['Age'].astype(int), 8)","e8fb591c":"#df_all['Lifestage'] = np.where(df_all['Age']>= 18.0, np.where(df_all['Age'] >= 65.0, \"OAP\", \"Adult\"), \"Child\")","c6f00df0":"df_all['Deck'] = df_all['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')\ndf_all['Deck'].value_counts()","5f299534":"# Works with training data only as survival not available for test\nsns.countplot(x='Deck',hue='Survived', data=df_all[:890], order = ['A', 'B', 'C', 'D', 'E', 'F', 'G'])","b47fa3e5":"pd.crosstab(df_all.loc[:890,'Deck'], df_all.loc[:890,'Survived'], normalize=True).round(2)","11925c01":"df_all['Deck'] = df_all['Deck'].replace(['B', 'D', 'E'], 'BDE')\ndf_all['Deck'] = df_all['Deck'].replace(['C', 'F', 'G'], 'CFG')","6b2ce71e":"features = ['Embarked', 'Sex', 'Deck', 'Title', 'AgeBin', 'FareBin', 'SingleFareBin']\n\nfor feature in features:\n    df_all[feature] = LabelEncoder().fit_transform(df_all[feature])","b843a6c8":"encoded_features = []\n\ncat_features = ['Pclass', 'Deck', 'Sex', 'Embarked', 'Title', 'AgeBin', 'FareBin', 'SingleFareBin']\n\nfor ft in cat_features:\n    encoded = OneHotEncoder().fit_transform(df_all[ft].values.reshape(-1, 1)).toarray()\n    n = df_all[ft].nunique()\n    cols = ['{}_{}'.format(ft, n) for n in range(1, n+1)]\n    encoded_df = pd.DataFrame(encoded, columns= cols)\n    encoded_df.index = df_all.index\n    encoded_features.append(encoded_df)\n\nfor i in range(0, len(cat_features)):\n    df_all = pd.concat([df_all, encoded_features[i]], axis = 1)","73ebeb6f":"df_all.columns","a99cbfd7":"df_all.head()","9da70bcb":"\ndrop_cols = ['PassengerId', 'Name', 'Pclass', 'Sex', 'Embarked', 'Deck', 'Cabin', 'Ticket', 'Title', 'Age', 'AgeBin', 'Fare', 'FareBin', 'SingleFare', 'SingleFareBin']\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_train = df_all[:891]\ndf_train_y = df_train['Survived']\ndf_train.drop([\"Survived\"], axis=1, inplace = True)\n\ndf_test = df_all[891:]\ndf_test.drop([\"Survived\"], axis=1, inplace=True)","f6c89ff0":"X_train = StandardScaler().fit_transform(df_train)\ny_train = df_train_y.values\nX_test = StandardScaler().fit_transform(df_test)","58137ddb":"df_all.columns","51c5bb8c":"probs = pd.DataFrame(np.zeros((len(X_test), 10)), columns = ['Fold_{}_Prob_{}'.format(i, j) for i in range(1,6) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], 5)), columns = ['Fold_{}'.format(i) for i in range(1,6)], index=df_train.columns)\n\nacc_score = []\n\n#Implementing cross validation\nk = 5\nkf = KFold(n_splits=k)\n\nmodel = RandomForestClassifier(criterion = 'gini', \n                      n_estimators = 100,\n                      max_samples = 0.3,\n                      max_depth = 5, \n                      min_samples_split = 6, \n                      min_samples_leaf = 6, \n                      max_features = 24, \n                      oob_score = True, \n                      random_state = SEED, \n                      n_jobs = -1, \n                      verbose = 0)\n\n\nfor fold, (train_index, test_index) in enumerate(kf.split(y_train), 1):\n    X_train_fld, y_train_fld = X_train[train_index], y_train[train_index]    \n    X_test_fld, y_test_fld = X_train[test_index], y_train[test_index]\n    \n    model.fit(X_train_fld,y_train_fld)\n    pred_values = model.predict(X_test_fld)\n    \n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = model.predict_proba(X_test)[:, 1]\n    \n    importances.iloc[:, fold-1] = model.feature_importances_\n     \n    acc = accuracy_score(pred_values, y_test_fld)\n    acc_score.append(acc)\n    \navg_acc_score = sum(acc_score)\/k\nprint('Accuracy of each fold - {}'.format(acc_score))\nprint('Avg accuracy : {}'.format(avg_acc_score))","345093a0":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 15))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\nplt.title('Random Forest Algorithm Average Feature Importance')\nplt.show()","b27dbd3f":"rf = RandomForestClassifier(random_state = SEED)\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","f6f0f715":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 7, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 6, 8, 10]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)","c19c0110":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n#rf = RandomForestClassifier()\n\n# Random search of parameters, using 5 fold cross validation, \n# search across 200 different combinations, and use all available cores\n#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, random_state=SEED, n_jobs = -1)\n\n# Fit the random search model\n#rf_random.fit(X_train, y_train)\n\n#rf_random.best_params_","dd067eae":"probs = pd.DataFrame(np.zeros((len(X_test), 10)), columns = ['Fold_{}_Prob_{}'.format(i, j) for i in range(1,6) for j in range(2)])\n\nacc_score = []\n\n#Implementing cross validation\nk = 5\nkf = KFold(n_splits=k)\n\nmodel = RandomForestClassifier(criterion = 'gini', \n                      n_estimators = 1000,                      \n                      max_depth = 10, \n                      min_samples_split = 10, \n                      min_samples_leaf = 2, \n                      max_features = 'auto', \n                      oob_score = True, \n                      random_state = SEED, \n                      n_jobs = -1, \n                      verbose = 0)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(y_train), 1):\n    X_train_fld, y_train_fld = X_train[train_index], y_train[train_index]    \n    X_test_fld, y_test_fld = X_train[test_index], y_train[test_index]\n    \n    model.fit(X_train_fld,y_train_fld)\n    pred_values = model.predict(X_test_fld)\n    \n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = model.predict_proba(X_test)[:, 1]\n     \n    acc = accuracy_score(pred_values, y_test_fld)\n    acc_score.append(acc)\n    \navg_acc_score = sum(acc_score)\/k\nprint('Accuracy of each fold - {}'.format(acc_score))\nprint('Avg accuracy : {}'.format(avg_acc_score))","35c1652c":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nclass_no_survived = [col for col in probs.columns if col.endswith('Prob_0')]\n\nprobs['1'] = probs[class_survived].sum(axis=1)\/5\nprobs['0'] = probs[class_no_survived].sum(axis=1)\/5\nprobs['pred'] = 0\n\npos = probs[probs['1'] > 0.5].index\nprobs.loc[pos, 'pred'] = 1","b2100a5d":"df_submission['Survived'] = probs['pred']\ndf_submission.to_csv('submission.csv', header=True, index=False)","cddb74bc":"### 2.5.1 Continuous Features","e51ce449":"### 2.5.2 Categorical Features","0374b380":"\n# 2. Project Stages\nIt's useful to have a systematic way to work through a project. Typically, this is the structure I use. \n\n### Exploratory Data Analysis\nExploratory Data Analysis (EDA) is used to determine the nature of the data and its characteristics. Pre-processing prepares the data for modelling or the creation of new features. This process can involve the removal of noise, the handling of missing data or outliers. In the titanic dataset, there are categorical, ordinal and numerical data types. Categorical and ordinal features need to be converted to numerical format for machine learning algorithms to work effectively.\n\n### Feature Engineering\nFeature engineering involves the creation of new features that we hope will correlate with the target feature or complement those we already have.\n\n### Modelling & Evaluation\nModelling involves determining and building the models we should use to predict survival. There are many different types of model and each have characteristics suited to different aspects of the problem. Evaluation involves determining what metrics we should we use to measure model performance e.g. ROC, AUC, MSE or RMSE.","5295dd30":"## 3.0.2 Fare Feature\nA new feature based on the fare paid per person is created. ","b58981d8":"## 3.0.4 Age Feature","0fd4884c":"Fares and Class are both correlated with survival as expected with fare acting as a surrogate for class. Sibsp is positively correlated with Parch as expected. Age and class are negatively correlated with eachother which would seem to suggest that lower classes had younger people.","273790f9":"## 2.1 Exploratory Data Analysis\n\nGiven the historical knowledge available concerning the titanic, a number of hypotheses are prevalent such as females having a higher survival rate than males and 1st class passengers having a higher survival rate than lower class passengers.  Lets have a look at the data.\n\n","e4560df1":" ## 3.0.5 Deck Feature\n It's clear that some decks had a better survival rate than others with decks B, C, D and E having the greatest.","fe36976c":"## 2.3 Target Distribution","855b3e26":"# 4.0 Modelling & Evaluation\n\n## 4.1 Random Forest Algorithm\n","b332935f":"### 2.2.4 Cabin\nThe cabin feature is highly incomplete with 687 missing in training and 327 in test. This isn't something that can be corrected easily if at all. However, it may be possible to use cabin details to determine deck information which may be an indicator of survival. \n\nThe alphanumeric cabin feature show quite a few missing values and at first look doesn't appear to provide any useful information. It is made up of a letter and number combination, with the letter signifying the deck. According to the titanic fandom website, A-Deck was last to flood. This was the second deck from the highest deck in the titanic [https:\/\/titanic.fandom.com\/wiki\/A_Deck]. \n\nMost of the Third Class cabins were located on F Deck, with a few on the forward G Deck. They were noticeably less comfortable and spacious then their First and Second-class counterparts, which were located in higher decks. This area was the first to flood during the sinking, because of their location in the lowest decks in the bow [https:\/\/titanic.fandom.com\/wiki\/Third_Class_cabins].","78541e77":"## 2.5 Target Distribution in Features","19928341":"# 5.0 References\n- https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\/notebook#2.-Feature-Engineering\n- https:\/\/www.ahmedbesbes.com\/blog\/kaggle-titanic-competition\n- https:\/\/seaborn.pydata.org\/generated\/seaborn.violinplot.html\n- https:\/\/medium.com\/mlearning-ai\/how-i-improved-my-score-on-kaggles-titanic-competition-by-using-advanced-classification-techniques-a2f5f63f7194\n- https:\/\/machinelearningmastery.com\/rfe-feature-selection-in-python\/","3856f981":"## 2.2 Missing Data\nAs you can see from the dataset there are missing data from the training and test datasets.\n\n    Training data is missing: Age, Cabin and Embarked\n    Testing data is missing: Age, Cabin, and Fare\n\nA number of methods can be used to do this e.g. dropping the feature completely, dropping the observation or row that is missing data or imputing new values based on mean, median or context. Age and Cabin features contain missing data in the training and test datasets and should be dealt with using the same approaches in both cases for consistency. Embarked and Fare features are missing from training and test datasets respectively. It's clear that PassengerId should be dropped from the training dataset as it does not contribute to survival. \n","ada6f2b7":"### 2.3.1 Correlations","bf11c680":"### 2.2.3 Embarked\nThe Embarked field in the training data is missing 2 values but is complete in the test data. These values could be imputed e.g. by checking if anyone else is on the same ticket or determining some other corroborating feature.","cc8ea3a1":"### 2.2.1 Age\nBoth Training and Test datasets are missing Age values. One option is to use the mean or median of Age overall all the dataset, but it might be more accurate to include contextual information, such as the Pclass and the extracted title, before estimating a median Age. A title will incorporate elements of class, gender and age. From the plots below a number of things can be said. If you were 5 or below, you had a good chance of surviving. If you were over 60, you had poor chance of surviving. ","8aa5d097":"### 2.2.2 Fare\nIt should be noted that the fares quoted are not per person, but rather per ticket. One fare value is missing from the test dataset. Fare is highly correlated with Pclass. \n","a5f40a29":"## 3.1 Frequency Encoding & Scaling","a644bec9":"## 4.2 Hyperparameter Tuning","572f6ebd":"## 3.0.1 Family Feature","9ab6eea5":"# 3.0 Feature Engineering\n\nFeature extraction will create features relating to family, decks, age, cabin, fares and nicknames\n","df63958f":"# 1. Introduction\nRMS Titanic sank in the early morning hours of 15 April 1912 in the North Atlantic Ocean, four days into her maiden voyage from Southampton to New York City, with an estimated 2,224 people on board. \nAlmost all those who jumped or fell into the water drowned or died within minutes due to the effects of cold shock and incapacitation. It is estimated that between 1,490\u20131,635 (66.9% to 73.5%) out of 2,224 people on board didn't survive, making it one of the deadliest peacetime maritime disasters in history. The problem is to predict who survived based on the data sets provided. This kernel has 3 main sections; Exploratory Data Analysis, Feature Engineering, Modelling & Evaluation.  If you have any suggestions on how to improve this kernel please let me know. For further information please refer to: https:\/\/en.wikipedia.org\/wiki\/Sinking_of_the_Titanic.\n"}}