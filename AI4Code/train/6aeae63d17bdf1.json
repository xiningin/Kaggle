{"cell_type":{"9b175c6c":"code","cfe9da46":"code","7db22a51":"code","adcc0620":"code","9787ae5b":"code","df7db095":"code","f4dafb94":"code","cc918752":"code","cd19db96":"code","739deaf4":"code","bc33ebd5":"code","e7aea48d":"code","5b1834cd":"code","c898074c":"code","01a225b4":"code","609080dc":"code","d069d30c":"code","032f5856":"code","94c657f0":"code","74866e98":"code","7c0123e6":"code","a4cb822f":"code","bfef8248":"code","b2f36f18":"code","4a399519":"code","e9987b44":"code","6feeac3b":"code","3b695039":"code","34cd8cd0":"code","30aa366c":"markdown","99e119f4":"markdown","796e5b79":"markdown","31328766":"markdown","10c47132":"markdown","9a6d6308":"markdown","d3771cf7":"markdown","2e0404f3":"markdown","2d80efa8":"markdown","9770ed6b":"markdown","7372ba1e":"markdown","5f2f0e9d":"markdown","dc35cf34":"markdown","88e54ca3":"markdown","897d394c":"markdown","54f1ee2a":"markdown","da2c9a0a":"markdown","e7f0b584":"markdown","d89b50d6":"markdown","e82993fa":"markdown","0d08db9f":"markdown","c008c6e3":"markdown","31e1b4d2":"markdown"},"source":{"9b175c6c":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns # For visualization\nimport matplotlib.pyplot as plt","cfe9da46":"data = pd.read_csv(\"\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv\")\ndata.head()","7db22a51":"data.describe().T","adcc0620":"data.shape","9787ae5b":"data.drop(columns = [\"RowNumber\",\"CustomerId\",\"Surname\"], inplace = True) #3 Columns dropped.\ndata.head()","df7db095":"data.Exited.value_counts(),data.isnull().sum()","f4dafb94":"plt.figure(figsize=(12,10))\nsns.heatmap(data.corr(),annot = True)","cc918752":"fig,ax = plt.subplots(nrows=2, ncols=2 , figsize=(25,10))\nsns.barplot(x= pd.cut(data.CreditScore,bins = 5).unique().sort_values(),y = data.groupby(pd.cut(data.CreditScore,bins = 5)).mean()[\"Exited\"], ax = ax[0,0]),ax[0,0].set(xlabel='Credit Score')\nsns.barplot(x=np.sort(data.Geography.unique()),y = data.groupby(data.Geography).mean()[\"Exited\"],ax = ax[0,1])\nsns.barplot(x=np.sort(data.Gender.unique()),y = data.groupby(data.Gender).mean()[\"Exited\"],ax = ax[1,0])\nsns.barplot(x= pd.cut(data.Age,bins = 20).unique().sort_values(),y = data.groupby(pd.cut(data.Age,bins = 20)).mean()[\"Exited\"],ax = ax[1,1]),ax[1,1].set(xlabel='Customer Age')","cd19db96":"fig,ax1 = plt.subplots(nrows=2, ncols=2 , figsize=(25,10))\nsns.barplot(x = data.Tenure.unique(),y = data.groupby(data.Tenure).mean()[\"Exited\"] , ax = ax1[0,0]),ax1[0,0].set(xlabel='Tenure')\nsns.barplot(x = data.NumOfProducts.unique(),y = data.groupby(data.NumOfProducts).mean()[\"Exited\"]*100 , ax = ax1[0,1]),ax1[0,1].set(xlabel=\"Number Of Products\")\nsns.boxplot(data.Balance, ax = ax1[1,0], color = \"lime\"),ax1[1,0].set(xlabel=\"Checking for outliers at Balance column\")\nsns.barplot(x = pd.cut(data.Balance , bins = 10).unique().sort_values(),y = data.groupby(pd.cut(data.Balance, bins = 10)).mean()[\"Exited\"]*100, ax=ax1[1,1]),ax1[1,1].set(xlabel=\"Balance column's customer loss percent\")","739deaf4":"data.drop(columns = [\"Tenure\"],inplace = True)# I dropped \"Tenure\" column here.\nfig,ax2 = plt.subplots(nrows=2, ncols=2 , figsize=(25,10))\nsns.barplot(x = data.HasCrCard.unique(),y = data.groupby(data.HasCrCard).mean()[\"Exited\"] , ax = ax2[0,0]),ax2[0,0].set(xlabel='Has Credit Card')\nsns.barplot(x = data.IsActiveMember.unique(),y = data.groupby(data.IsActiveMember).mean()[\"Exited\"], ax = ax2[0,1]),ax2[0,1].set(xlabel=\"Is Active Member Column\")\nsns.boxplot(data.EstimatedSalary, ax = ax2[1,0], color = \"purple\"),ax2[1,0].set(xlabel=\"Checking for outliers at Estimated Salary column\")\nsns.barplot(x = pd.cut(data.EstimatedSalary , bins = 10).unique().sort_values(),y = data.groupby(pd.cut(data.EstimatedSalary, bins = 10)).mean()[\"Exited\"]*100, ax=ax2[1,1]),ax2[1,1].set(xlabel=\"Estimated Salary column's customer loss percent\")","bc33ebd5":"#data.drop(columns = [\"HasCrCard\",\"EstimatedSalary\"],inplace = True)","e7aea48d":"from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n\ndata.Gender = LabelEncoder().fit_transform(data.Gender) # Gender label encoding\ndata = pd.concat((pd.DataFrame(OneHotEncoder(sparse = False).fit_transform(pd.DataFrame(data.Geography)),columns = [\"FromFrance\",\"FromSpain\",\"FromGermany\"]),data),axis = 1) #Geography onehotencoding\ndata.drop(columns= [\"Geography\"],inplace = True)\ndata.head()","5b1834cd":"from sklearn.preprocessing import MinMaxScaler\ndata[[\"CreditScore\",\"Age\",\"Balance\",\"EstimatedSalary\"]] = MinMaxScaler(copy = False).fit_transform(data[[\"CreditScore\",\"Age\",\"Balance\",\"EstimatedSalary\"]])\ndata.head()","c898074c":"from sklearn.model_selection import train_test_split\nX = data.iloc[:,:-1]\ny = data.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25,random_state = 10)\nvalitrain, X_test, valitest, y_test = train_test_split(X_test,y_test,test_size = 0.5)","01a225b4":"import tensorflow as tf\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(32 , activation = \"relu\",input_shape=(X_train.shape[1],)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(1,activation = \"sigmoid\")\n])\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),loss = tf.losses.BinaryCrossentropy(),metrics= [\"accuracy\",'binary_crossentropy'])","609080dc":"history = model.fit(X_train, y_train , validation_data = (valitrain,valitest),epochs = 100)","d069d30c":"y_pred = model.predict(X_test)\ny_pred = np.round_(y_pred)\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test,y_pred)\ncm1","032f5856":"model.evaluate(X_test, y_test,batch_size=10)","94c657f0":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","74866e98":"hist = pd.DataFrame(history.history)","7c0123e6":"f, ax = plt.subplots(1, 1,figsize = (15,8))\nsns.lineplot(x = range(0,100),y = hist.binary_crossentropy , color = \"green\",label='Training binary crossentropy')\nsns.lineplot(x = range(0,100),y = hist.val_binary_crossentropy , color = \"blue\",label='Validation binary crossentropy')\nsns.lineplot(x = range(0,100),y = hist.accuracy , color = \"red\",label='Training Accuracy')\nsns.lineplot(x = range(0,100),y = hist.val_accuracy , color = \"purple\",label='Validation Accuracy')\n","a4cb822f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","bfef8248":"le = LogisticRegression()\nsvc = SVC()\nrf = RandomForestClassifier()","b2f36f18":"le.fit(X_train,y_train)\nsvc.fit(X_train,y_train)\nrf.fit(X_train,y_train)","4a399519":"y_predle = le.predict(X_test)\ny_predsvc = svc.predict(X_test)\ny_predrf = rf.predict(X_test)","e9987b44":"print(classification_report(y_test,y_predle))","6feeac3b":"print(classification_report(y_test,y_predsvc))","3b695039":"print(classification_report(y_test,y_predrf))","34cd8cd0":"cmrf = confusion_matrix(y_test, y_predrf)\ncmsvc = confusion_matrix(y_test, y_predsvc)\ncmrf , cm1 , cmsvc","30aa366c":"Lets visualize results.","99e119f4":"\"HasCrCard\" and \"EstimatedSalary\" columns have no pattern at \"Exited\" column.We might drop these columns too. Active members chance of leaving company is double times.","796e5b79":"<a id=\"part4\"><\/a>\n# 4.Normalization(minmax scaling) and Split the dataset:","31328766":"<a id=\"part6\"><\/a>\n# 6.Training the Neural Network:","10c47132":"<a id=\"part5\"><\/a>\n# 5.Building the Neural Network Model:","9a6d6308":"Good dataset looks better now.Before training one last thing left to do. Lets scale Balance, CreditScore and EstimatedSalary columns.Because we're going to perform Neural Network for training and NN works better with scaled data.I'am going to apply MinMaxScaler to our data for better neural network training.","d3771cf7":"We finished preprocessing stage.Now We're going to create a neural network and use it for training.","2e0404f3":"<a id=\"part3\"><\/a>\n# 3.Encoding:\n\nIn dataset we have 2 categorical columns. For geography column we're going to apply OneHotEncoding and use Label encoder for the gender column.","2d80efa8":"Random Forest Classifier and ANN gave us best results.Svc and Logistic regression seems failed a little bit.That because we are dealing with imbalanced dataset.To improve SVC and LogReg results.We can try smote for upsampling and train again.But i'm not going to do that. If you want to learn how to implement upsampling you can check my other notebooks.I tried couple things at ANN but i can't improve the results and \u0131 don't know what to expect accuracy at loss customers. So do you have any suggestions to improve ANN result. Thanks for your time.","9770ed6b":"Seems like lower credit score means company is more likely to lose customer. Also being at age range between 45-65 means also company is going to lose customer.At figure [1,0] male customers seems like more loyal than female customers.Let's dig a little bit more.","7372ba1e":"<a id=\"part8\"><\/a>\n# 8.Conclusion:","5f2f0e9d":"I also want to try some traditional ML methods like SVC ,Random Forest and Logistic Regression.","dc35cf34":"When i dropped these columns its effectted NN negatively but randomforest worked same.So i decided to not drop.","88e54ca3":"Second, Lets check balance of data and NaN values.","897d394c":"<a id=\"part0\"><\/a>\n# 0.Abstract:\n\nIn this analysis after numerically understand data.I visualized the dataset and tried to understand relations between columns.And i dropped tenure column after EDA. I transformed categorical values into numerical.Gender column label encoded and Geography column OneHotEncoded.To perform neural network better. I applied min max scaling to data.And I split dataset into 3 parts,Training ,Test and validation.NN model has 1 input layer and 5 hidden layer.I experimented a little bit at neural network architecture.Model i used gave best result in my experiences. I also tried Logistic Regression ,SVC and Random Forest Classifier. Because of our dataset is imbalanced. Result of Logistic regression and SVC are bad. But Random Forest Classifier gave almost same results with neural network.\n\n**Used Libraries:**\n* 1. Pandas\n* 2. Numpy\n* 3. Seaborn\n* 4. Matplotlib\n* 5. Sklearn\n* 6. Tensorflow\n\n![](https:\/\/ml8ygptwlcsq.i.optimole.com\/fMKjlhs.f8AX~1c8f3\/w:1200\/h:675\/q:auto\/https:\/\/www.unite.ai\/wp-content\/uploads\/2019\/11\/artificial-neural-network-3501528_1920.png)\n\n**Table of contents:**\n\n* [0.Abstract:](#part0)\n* [1.Understanding Dataset:](#part1)\n* [2.Exploratory Data Analysis:](#part2)\n* [3.Encoding:](#part3)\n* [4.Normalization(minmax scaling) and Splitting the dataset:](#part4)\n* [5.Building the Neural Network Model:](#part5)\n* [6.Training the Neural Network:](#part6)\n* [7.Traditional Methods:](#part7)\n* [8.Conclusion:](#part8)","54f1ee2a":"At first graph, Company customer losses has no relation with customer membership duration. We can drop this column for better training.Second graph sayin, customers who has 2 and 4 products is more likely to exit.Last 2 figures about balance column. First one is for outliers at balance column and second one is customer loss rate.Customers with balance of +200k, have higher chance to exit.","da2c9a0a":"<a id=\"part7\"><\/a>\n# 7.Traditional Methods:","e7f0b584":"Seems like dataset has no Nan values but dataset is unbalanced. %80 of target value is 0.We can balance it but we are going to use neural network for training.We don't need to balance dataset.","d89b50d6":"Seems like 30 epochs is better for training.","e82993fa":"85 percent accuracy not too bad. But recall percentage = 0.55 means we are predicting lost customers with %55 accuracy.","0d08db9f":"Dataset has 10.000 rows and 14 columns.For training we don't need \"RowNumber\",\"CustomerId\" and \"Surname\" columns. Lets start with dropping these columns.","c008c6e3":"<a id=\"part2\"><\/a>\n# 2.Exploratory Data Analysis:\nLets discover relations between Exited and rest of the data.","31e1b4d2":"<a id=\"part1\"><\/a>\n# 1.Understand the Dataset:\n\nLets understand data with numbers.\n"}}