{"cell_type":{"90ab85d8":"code","5f73af86":"code","d273f2fb":"code","78fd961b":"code","fd9a1789":"code","ba739cb6":"code","66250c82":"code","2e3af1c7":"code","ddc16b9b":"code","c0e60ee9":"code","a8dbb3ff":"code","871b6fcc":"code","7082b740":"code","0f820984":"code","4bf23b86":"code","0cc70a04":"code","277aa3bd":"code","edd18684":"code","6c460359":"code","1ea6ecd4":"code","6104e5ee":"code","38396f9a":"code","49dd868a":"code","f2542852":"code","eaaaba03":"code","6255088f":"code","ba329891":"code","91f018af":"code","ec0d9936":"code","31cc30a7":"code","c106f2f4":"code","7b639b66":"code","33a65981":"code","0bc35a4f":"code","446f8817":"code","4f66f319":"code","2c84619a":"code","0922be5e":"code","33680cef":"markdown","67eae3e5":"markdown","56245e06":"markdown","d7becf03":"markdown","24858273":"markdown","5d18c873":"markdown","352452cd":"markdown","483d8092":"markdown","25cc18b7":"markdown","17fa4a20":"markdown","aa9d7f8a":"markdown","539e1f96":"markdown","7cfd65c1":"markdown","012662b5":"markdown","cf7172ca":"markdown"},"source":{"90ab85d8":"#______________________________________________________import libraries___________________________________________________#\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom scipy import optimize as op # Advanced optimization\nimport warnings \nwarnings.filterwarnings('ignore') #ignore's warnings\nfrom math import ceil \nimport numpy as np\nimport math\nimport operator\n\n#________________________________________________________Visualization___________________________________________________#\n\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.metrics import confusion_matrix #Confusion matrix\nfrom sklearn.metrics import accuracy_score # Accuracy score\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree #Decision tree\nfrom sklearn.svm import SVC\n\n#____________________________________________________Spliting training and testing_______________________________________#\n\nfrom sklearn.model_selection import train_test_split\n\n","5f73af86":"# Loading the data\n\npath = '..\/input\/iris-flower-dataset\/IRIS.csv'\ndf_iris = pd.read_csv(path, header=0)\ndf_iris","d273f2fb":"df_iris.info()","78fd961b":"df_iris.describe()","fd9a1789":"df_iris['species'].value_counts()","ba739cb6":"df_iris.isnull().values.any()\n","66250c82":"print('Iris-setosa')\nsetosa = df_iris['species'] == 'Iris-setosa'\nprint(df_iris[setosa].describe())\nprint('\\nIris-versicolor')\nversicolor = df_iris['species'] == 'Iris-versicolor'\nprint(df_iris[versicolor].describe())\nprint('\\nIris-virginica')\nvirginica = df_iris['species'] == 'Iris-virginica'\nprint(df_iris[virginica].describe())","2e3af1c7":"# The Histogram representation of the univariate plots for each measurement\nnp = df_iris\nnp.hist(edgecolor='blue', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\n","ddc16b9b":"# Plotting scatter plot with respect to sepal length\nsepalPlt = sb.FacetGrid(df_iris, hue=\"species\", size=6).map(plt.scatter, \"sepal_length\", \"sepal_width\")\nplt.legend(loc='upper right');\nplt.title(\"sepal Length VS width\")","c0e60ee9":"# Using seaborn pairplot to see the bivariate relation between each pair of features\n\nimport seaborn as sns\nsns.set_palette('husl')\n\nnl = df_iris\nb = sns.pairplot(nl,hue=\"species\", diag_kind=\"kde\", markers='+',size =3);\nplt.show()","a8dbb3ff":"# setting our data\nimport numpy as np\nspecies = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n# Number of examples\nm = df_iris.shape[0]\n# Features\nn = 4\n# Number of classes\nk = 3\n\nX = np.ones((m,n + 1))\ny = np.array((m,1))\nX[:,1] = df_iris['petal_length'].values\nX[:,2] = df_iris['petal_width'].values\nX[:,3] = df_iris['sepal_length'].values\nX[:,4] = df_iris['sepal_width'].values\n\n# Labels\ny = df_iris['species'].values\n\n# Mean normalization\nfor j in range(n):\n    X[:, j] = (X[:, j] - X[:,j].mean())","871b6fcc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 11)\n# it shows 80% of data is split for training and 20% of the data goes to testing.\n\nX = df_iris.drop(['species'], axis=1)\ny = df_iris['species']\n# print(X.head())\nprint(X_train.shape)\n# print(y.head())\nprint(y_test.shape)","7082b740":"# Sigmoid function\n\ndef sigmoid(z):\n    return 1.0 \/ (1 + np.exp(-z))\n\n#_____________________________________________________Regularized cost function_________________________________________#\n\ndef reglrCostFunction(theta, X, y, lambda_s = 0.1):\n    m = len(y)\n    h = sigmoid(X.dot(theta))\n    J = (1 \/ m) * (-y.T.dot(np.log(h)) - (1 - y).T.dot(np.log(1 - h)))\n    reg = (lambda_s\/(2 * m)) * np.sum(theta**2)\n    J = J + reg\n \n    return J\n#_____________________________________________________Regularized gradient function____________________________________#\n\ndef reglrGradient(theta, X, y, lambda_s = 0.1):\n    m, n = X.shape\n    theta = theta.reshape((n, 1))\n    y = y.reshape((m, 1))\n    h = sigmoid(X.dot(theta))\n    reg = lambda_s * theta \/m\n    gd = ((1 \/ m) * X.T.dot(h - y)) \n    gd = gd + reg\n\n    return gd\n#_________________________________________________Optimizing logistic regression (theta)_____________________________#\n\ndef logisticRegression(X, y, theta):\n    result = op.minimize(fun = reglrCostFunction, x0 = theta, args = (X, y), method = 'TNC', jac = reglrGradient)\n    \n    return result.x","0f820984":"# Training\nall_theta = np.zeros((k, n + 1))\n\n# One vs all\ni = 0\nfor flower in species:\n    tmp_y = np.array(y_train == flower, dtype = int)\n    optTheta = logisticRegression(X_train, tmp_y, np.zeros((n + 1,1)))\n    all_theta[i] = optTheta\n    i += 1","4bf23b86":"# Predictions\nProb = sigmoid(X_test.dot(all_theta.T)) # probability for each flower\npred = [species[np.argmax(Prob[i, :])] for i in range(X_test.shape[0])]\n\nprint(\" Test Accuracy \", accuracy_score(y_test, pred) * 100 , '%')","0cc70a04":"# Confusion Matrix\ncnfm = confusion_matrix(y_test, pred, labels = species)\n\nsb.heatmap(cnfm, annot = True, xticklabels = species, yticklabels = species);","277aa3bd":"# classification report \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","edd18684":"#__________________________________________________KNN using Euclidian Distance______________________________________#\n\ndef euclidianDistance(data1, data2, length):\n    distance = 0\n    for x in range(length):\n        distance += np.square(data1[x] - data2[x])\n       \n    return np.sqrt(distance)","6c460359":"def knn(trainingSet, testInstance, k):\n \n    distances = {}\n    sort = {}\n \n    length = testInstance.shape[1]\n    \n    for x in range(len(trainingSet)):\n        \n        dist = euclidianDistance(testInstance, trainingSet.iloc[x], length)\n\n        distances[x] = dist[0]\n      \n    sorted_d = sorted(distances.items(), key=operator.itemgetter(1))\n   \n \n    neighbors = []\n\n   \n    for x in range(k):\n        neighbors.append(sorted_d[x][0])\n    classVotes = {}\n    \n  \n    for x in range(len(neighbors)):\n        response = trainingSet.iloc[neighbors[x]][-1]\n \n        if response in classVotes:\n            classVotes[response] += 1\n        else:\n            classVotes[response] = 1\n\n\n    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n    return(sortedVotes[0][0], neighbors)","1ea6ecd4":"testSet = [[1.4, 3.6, 5.6, 1.2]]\ntest = pd.DataFrame(testSet)\nresult,neigh = knn(df_iris, test, 5)#here we gave k=4\nprint(\"And the flower is:\",result)\nprint(\"the neighbors are:\",neigh)","6104e5ee":"# K-Nearest Neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier(n_neighbors=8)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","38396f9a":"# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","49dd868a":"# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","f2542852":"#___________________________________________Cross validation using Brute Force_________________________________________#\nfrom sklearn.model_selection import cross_val_score\ncv_scores = []\nneighbors = list(np.arange(3,50,2))\nfor n in neighbors:\n    knn = KNeighborsClassifier(n_neighbors = n,algorithm = 'brute')\n    \n    cross_val = cross_val_score(knn,X_train,y_train,cv = 5 , scoring = 'accuracy')\n    cv_scores.append(cross_val.mean())\n    \nerror = [1-x for x in cv_scores]\noptimal_n = neighbors[ error.index(min(error)) ]\nknn_optimal = KNeighborsClassifier(n_neighbors = optimal_n,algorithm = 'brute')\nknn_optimal.fit(X_train,y_train)\npred = knn_optimal.predict(X_test)\nacc = accuracy_score(y_test,pred)*100\nprint(\"The accuracy for optimal k = {0} using brute is {1}\".format(optimal_n,acc))","eaaaba03":"print(\"classification_report using brute force\")\nprint(classification_report(y_test,pred))","6255088f":"\nclf = SVC(kernel = 'linear').fit(X_train,y_train)\nclf.predict(X_train)\ny_pred = clf.predict(X_test)\n# Creates a confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['setosa','versicolor','virginica'], \n                     columns = ['setosa','versicolor','virginica'])\n\nsns.heatmap(cm_df, annot=True)\nplt.title('Accuracy using brute:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","ba329891":"#__________________________________________________________________________________________________#\n#____________Applying different Classification Models using \"sklearn\"______________________________#\n#__________________________________________________________________________________________________#\n\n\n# Seperating the data into dependent and independent variables\n\nX = df_iris.iloc[:, :-1].values\ny = df_iris.iloc[:, -1].values","91f018af":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","ec0d9936":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","31cc30a7":"# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","c106f2f4":"# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","7b639b66":"# Support Vector Machine's \nfrom sklearn.svm import SVC\n\nclassifier = SVC()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","33a65981":"# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","0bc35a4f":"# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","446f8817":"# Decision Tree's\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier()\n\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","4f66f319":"# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","2c84619a":"# Accuracy score\n\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","0922be5e":"#Decision tree figure with analysis\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\nplt.figure(figsize=(5, 3.5), dpi=300)\nclassifier = DecisionTreeClassifier().fit(iris.data, iris.target)\nplot_tree(classifier, feature_names=iris.feature_names,  \n                     class_names=iris.target_names,  \n                     filled=True, rounded=True)\nplt.show()\n\n","33680cef":"****K-Nearest Neighbour:\n\n* It is very easy to implement and executes very quickly, but it also has some disadvantage I.e., it sometimes misses the shortest paths which can be easily pointed by us.\n* Here we perform KNN Algorithm by different nearest neighbours\n\n![Clustering_average.png](attachment:ed6ad768-72ed-48d9-aeef-045a051455cb.png)\n\n\n![knn.png](attachment:cd220494-ee44-4287-9d05-6f51c1a45ec5.png)\n\nThe above picture we can clearly visualize the three different cells that are compared with one point and by selecting different \u201cK\u201d values we can get different nearest neighbours as shown in figure: \n\n","67eae3e5":"# 7. Naive Bayes.","56245e06":"# 8. SVM","d7becf03":"**At every stage it shows five different aspects to clearly justify the further steps based \non the present steps. Initially compares with the sepals and petals length and width. Here \n\u201cgini\u201d is the impurity level that plotted wrong (labelled wrong). Followed by samples count in \na step. Value is the set with three different species and finally the class it belongs**","24858273":"* These can be determined by finding the distance between the two different points. Here we are determined this distance by \u201cEuclidean Distance\u201d.\n* Euclidean Distance: \u201cIt is the distance between the two points in its own space (Euclidean Space)\u201d.\n\n![Euclidian distance.png](attachment:91ac010c-159b-4deb-bc36-654f5e4a2290.png)","5d18c873":"# 9. Decision Tree: \n\n* It is mainly used for classification of regression. In Decision Tree at every level, we need to rectify the splitting attribute.\n* A Decision Tree is made up of diving the data set into sub data sets with homogenous values. Standard division is used to calculate the similarity of samples in data sets.\n* We had loaded the trained and test data to scikit learn library for this algorithm and we had obtained 100% accuracy rate:","352452cd":"****Classification Report for KNN:\n\nOn a per-class basis, the classification report shows a representation of the major classification metrics. This gives the classifier's actions a deeper insight into global accuracy. In terms of true and false positives and negatives, the metrics are described. In this case, positive and negative are common names for the classes of a problem with a binary classification. We will consider true and false occupied and true and false unoccupied in the example above. Therefore, when the real class is positive, as is the predicted class, a true positive is. A false positive is when it is negative for the real class, but positive for the predicted class. Metrics are described using this terminology as follows:\n\n****Precision:\n\nPrecision is a classifier's ability not to mark a positive example that is negative. It is defined as the ratio of true positives to the sum of true and false positives for each class. \"To be simple, what percent was right for all instances classified positive.\n\n****Recall:\n\nRecall is a classifier's ability to identify all positive instances. It is defined as the ratio of true positives to the sum of true positives and false negatives for each class. \"To simplify, \"What percent was categorized correctly for all instances that were actually positive.\n\n****\u201cF1\u201d score:\n\nA weighted harmonic mean of accuracy is the F1 value, noting that the best score is 1.0 and the worst is 0.0. As they integrate accuracy and recall into their computation, F1 scores are lower than precision scales. As a rule of thumb, the F1 weighted average should \nbe used, not global precision, to compare classifier models.\n\n","483d8092":"# 1. Basic Imports","25cc18b7":"****Support Vector Machine and Naive Bayes. \n\n* These two algorithms are performed by making use of the libraries(scikit learn). Because the only reason for these two algorithms is to check the results with different algorithms and also to learn about the library we had used (scikit learn). \n\n****Support Vector Machine:\n\n* It is also known as \u201csuper vector network\u201d. The main aim of these algorithm is to analyse the data for classification and regression. It was developed at AT&T lab. It is one of the most famous prediction methods based on statical learning framework. It does not only perform in linear classification but also perform in non-linear classification efficiently by using \u201ckernel trick\u201d.\n\n****Naive Bayes:\n\n* This algorithm mainly works by applying \u201cBayes Theorem\u201d with independence assumptions. This algorithm is very high scalable in learning problem. It can also be called as simple bayes (or) independence bayes.","17fa4a20":"# 6. Cross Validation using Brute Force.","aa9d7f8a":"# 3. Visualisations","539e1f96":"# 4. Regression\n\n**Sigmoid function:**\n\n* By using a link function, we can convert a logistic model to a predictor. For our model, we will be using the sigmoid function. The bellow images show the S-curve and the Sigmoid function equation.\n\n\n![sigmoid function.png](attachment:9da645e3-8051-4167-b42a-d18c9f1a4cae.png)\n\n\n* So, we can observe the change from zero to one where zero is the least probability which is false and the one is about true case. So, the values that are right to the curve are true cases.\n\n\n**Cost function:**\n\n* In cost function J(0) represents the optimization objective. The main aim of cost function is to minimize the cost with this we can develop accurate model with very high error less percentage. \n\n![Regularised cost function.png](attachment:043357fa-8562-406e-b23e-824172b8fc4a.png)\n\n\n** Gradient function:**\n\n* Gradient function helps in reducing the cost value of the items. It is the partial derivative of regularized cost gradient function, so it reduces the value\n\n![Regularised gradient.png](attachment:e301f940-c31b-4c59-aa40-20a9c78a7559.png)\n\n\n\n\n\n","7cfd65c1":"> ##**Thank you...! For Visiting,please upvote my page and encourage.**\n\n\n![image.png](attachment:7dae070d-a997-4109-83c1-1b8738ad1e4e.png)","012662b5":"# 5. KNN Using Euclidian Distance.","cf7172ca":"# 2. Dataset Loading"}}