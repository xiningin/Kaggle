{"cell_type":{"5fee92f1":"code","28e39e8c":"code","ad7fd4d6":"code","a6a544b0":"code","6fffacae":"code","0522493f":"code","6b998192":"code","14d9b89c":"code","529a2ad2":"code","ee58dde2":"code","3fb4acc4":"code","3d5206d2":"markdown","eb759d10":"markdown","42204e0b":"markdown","5ca768c4":"markdown","c054eddc":"markdown","9011cfc1":"markdown","250a98e4":"markdown","9600e647":"markdown","e259920c":"markdown","ead0fc6a":"markdown","1d1f7336":"markdown","e4eac566":"markdown","428ef212":"markdown"},"source":{"5fee92f1":"\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","28e39e8c":"from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.applications.vgg19 import VGG19\nimport matplotlib.pyplot as plt\nfrom glob import glob","ad7fd4d6":"train_path = \"\/kaggle\/input\/cell-images-parasitized-or-not\/cell_images\/train\/\"\ntest_path = \"\/kaggle\/input\/cell-images-parasitized-or-not\/cell_images\/test\/\"","a6a544b0":"img = load_img(train_path + \"parasitized\/C68P29N_ThinF_IMG_20150819_134326_cell_172.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","6fffacae":"x = img_to_array(img) # I have been converted image to matrix\n\nprint(x.shape)\n\nnumberOfClass = len(glob(train_path+\"\/*\"))\nprint(\"Number of Class: \",numberOfClass)","0522493f":"vgg = VGG19()\n\n\nprint(vgg.summary())\n","6b998192":"print(type(vgg))\n\n\nvgg_layer_list = vgg.layers\nprint(vgg_layer_list)\n\nmodel = Sequential()\nfor i in range(len(vgg_layer_list)-1):\n    model.add(vgg_layer_list[i])\n\n\nprint(model.summary())\n","14d9b89c":"for layers in model.layers:\n    layers.trainable = False\n    \nmodel.add(Dense(numberOfClass,activation=\"softmax\"))\n\nprint(model.summary())\n\n\nmodel.compile(loss = \"categorical_crossentropy\",\n              optimizer = \"rmsprop\",\n              metrics = [\"accuracy\"]\n              )","529a2ad2":"train_data = ImageDataGenerator().flow_from_directory(train_path,target_size = (224,224))\ntest_data = ImageDataGenerator().flow_from_directory(test_path,target_size = (224,224))\n\nbatch_size = 32\n\nhist = model.fit_generator(train_data,\n                           steps_per_epoch=3200\/\/batch_size,\n                           epochs = 100,\n                           validation_data=test_data,\n                           validation_steps=800\/\/batch_size\n                           )","ee58dde2":"print(hist.history.keys())\nplt.figure(figsize=(15,15))\nplt.plot(hist.history[\"loss\"],label = \"training loss\")\nplt.plot(hist.history[\"val_loss\"],label = \"validation loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(15,15))\nplt.plot(hist.history[\"accuracy\"],label = \"training acc\")\nplt.plot(hist.history[\"val_accuracy\"],label = \"validation acc\")\nplt.title(\"Training Accuracy vs Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","3fb4acc4":"results = model.evaluate(test_data)","3d5206d2":"# Read the Dataset","eb759d10":"# Training the Model","42204e0b":"# Exploratory Data Analysis - 1","5ca768c4":"# RMSPROP (OPTIMIZER)\n\nRMSprop is a gradient-based optimization technique used in training neural networks. It was proposed by the father of back-propagation, Geoffrey Hinton. Gradients of very complex functions like neural networks have a tendency to either vanish or explode as the data propagates through the function (refer to vanishing gradients problem). Rmsprop was developed as a stochastic technique for mini-batch learning. RMSprop deals with the above issue by using a moving average of squared gradients to normalize the gradient. This normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding and increasing the step for small gradients to avoid vanishing. Simply put, RMSprop uses an adaptive learning rate instead of treating the learning rate as a hyperparameter. This means that the learning rate changes over time.\n\n![](https:\/\/miro.medium.com\/max\/1174\/1*bdgqAQdEgpyBZscgNgx6hQ.png)\n","c054eddc":"# Accuracy Calculation","9011cfc1":"At this project , I'm going to use VGG19.Let's look at the VGG19","250a98e4":"# VGG19 MODEL (Transfer Learning CNN STRUCTURE)","9600e647":"# Exploratory Data Analysis - 2","e259920c":"# How is Transfer Learning Used?\nUsing a pre-trained model significantly reduces the time required for feature engineering and training. The first step is to select a source model, ideally one with a large dataset to train with. Many research institutions release these models and datasets as open-sourced projects, so it\u2019s not necessary to create your own. The next step is to decide which layers to reuse in your own network. The goal is create a framework that is at least better than a na\u00efve model, so you can be assured some new feature learning takes place. Typically deeper layers are reused as these tend to be more general whereas the top layers tend to more finely tuned to a particular problem.\n![](\nhttps:\/\/www.aismartz.com\/blog\/wp-content\/uploads\/2019\/11\/Concept-of-Transfer-learning.jpg)","ead0fc6a":"# What is the VGG19 ?\nVGG-19 is a trained Convolutional Neural Network, from Visual Geometry Group, Department of Engineering Science, University of Oxford. The number 19 stands for the number of layers with trainable weights. 16 Convolutional layers and 3 Fully Connected layers.\n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-a1965831bd123c677eeb6cb22bb85bd4)\n\n![](https:\/\/miro.medium.com\/max\/700\/1*_Lg1i7wv1pLpzp2F4MLrvw.png)","1d1f7336":"# What is the Softmax ?\nThe softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1. The input values can be positive, negative, zero, or greater than one, but the softmax transforms them into values between 0 and 1, so that they can be interpreted as probabilities. If one of the inputs is small or negative, the softmax turns it into a small probability, and if an input is large, then it turns it into a large probability, but it will always remain between 0 and 1.\n\nThe softmax function is sometimes called the softargmax function, or multi-class logistic regression. This is because the softmax is a generalization of logistic regression that can be used for multi-class classification, and its formula is very similar to the sigmoid function which is used for logistic regression. The softmax function can be used in a classifier only when the classes are mutually exclusive.\n\nMany multi-layer neural networks end in a penultimate layer which outputs real-valued scores that are not conveniently scaled and which may be difficult to work with. Here the softmax is very useful because it converts the scores to a normalized probability distribution, which can be displayed to a user or used as input to other systems. For this reason it is usual to append a softmax function as the final layer of the neural network.\n\n![](https:\/\/lh6.googleusercontent.com\/3vcfJ5hJhsMZAMFIbQOEycfVW1t6rh1CXt62DeMk8RPPXVzV4vCcURNm_z_F7618uAeSHT7qT7wE_UiK5Ic0b-Eeuunn6iTGeHWbpAaUAP6-G2ePubeGWCb4_TmSapeaimZqvuUs)","e4eac566":"# Transfer Learning\nTransfer or inductive learning is a supervised learning technique that reuses parts of a previously trained model on a new network tasked for a different but similar problem. In computer vision, for example, some feature extractors from a nudity detection model could be used to speed up the learning process for a new facial recognition model.","428ef212":"# LIBRARIES"}}