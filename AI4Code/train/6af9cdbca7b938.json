{"cell_type":{"5410404d":"code","748acfd0":"code","bc19ba73":"code","bb5a234e":"code","2b7ad0b7":"code","e5533e42":"code","09ba6201":"code","e8ddbbd0":"code","90a20640":"code","1eba0325":"code","c1ebbabd":"code","d20799a4":"code","85f3a85c":"code","b34a5754":"code","fa9f21a4":"code","6fa5d357":"code","9b940596":"code","9ae7b756":"code","7d9a0b1e":"code","9e26864f":"code","e0f7046b":"code","b12fce27":"code","f6b56f43":"code","f540a048":"code","18fb0a29":"code","fb29669e":"code","9a723a18":"code","971d4f5b":"code","d42a21fd":"code","2df8ddf1":"code","e52f48d3":"code","ff2463ee":"code","7b71cbea":"markdown"},"source":{"5410404d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport gc\nimport random\nimport seaborn as sns\nimport xgboost\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix    \nimport matplotlib.pyplot as plt \nfrom catboost import CatBoostClassifier, cv, Pool\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nnp.random.seed(64)\nrandom.seed(64)","748acfd0":"data_root = os.environ.get('KAGGLE_DIR', '..\/input')\ntrain = pd.read_parquet(f'{data_root}\/tpsdec2021parquet\/train.pq')\n#pseudo = pd.read_csv(f'{data_root}\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv', dtype=train.dtypes.to_dict())\ntest = pd.read_parquet(f'{data_root}\/tpsdec2021parquet\/test.pq')","bc19ba73":"train.head()","bb5a234e":"train.describe().T","2b7ad0b7":"test.head()","e5533e42":"#pseudo.head()","09ba6201":"train['Cover_Type'].value_counts()","e8ddbbd0":"all_df = pd.concat([train.assign(ds=0), test.assign(ds=2)]).reset_index(drop=True).drop(\n    columns=['Soil_Type7', 'Soil_Type15'] # 0 variance\n)","90a20640":"train = train.drop(columns=['Soil_Type7', 'Soil_Type15'])\ntest = test.drop(columns=['Soil_Type7', 'Soil_Type15'])\ntrain = train.astype({'Cover_Type': np.int8})\n","1eba0325":"# types = {'Cover_Type': np.int8}\n# train = all_df.loc[all_df.ds == 0].astype(types).drop(columns=['ds'])\n# #pseudo = all_df.loc[all_df.ds == 1].astype(types).drop(columns=['ds'])\n# test = all_df.loc[all_df.ds == 2].drop(columns=['Cover_Type', 'ds'])","c1ebbabd":"label_encoder = LabelEncoder()\n\ntrain = train.loc[train.Cover_Type != 5] # Excluding label 5 because there is only 1 training example\n\nX = train.drop(columns=['Id', 'Cover_Type']).astype(np.float32).to_numpy()\ny = label_encoder.fit_transform(train.Cover_Type)\nfeat_names = train.columns.drop(['Id', 'Cover_Type'])\ncolum_names = train.columns\ndel train\nnum_class = len(label_encoder.classes_)\n\n# X_pseudo = pseudo.drop(columns=['Id', 'Cover_Type']).astype(np.float32).to_numpy()\n# y_pseudo = label_encoder.transform(pseudo.Cover_Type)\n# del pseudo\n\nX_test = test.drop(columns=['Id']).astype(np.float32).to_numpy()\ndel test\n\n\nXGB_col_names = [f'{cls}' for cls in label_encoder.classes_] # Column names will be used to save results \nCAT_col_names = [f'{cls}' for cls in label_encoder.classes_]","d20799a4":"params = {\n    'num_class': num_class,\n    'objective': 'multi:softprob',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'eval_metric': ['merror', 'mlogloss'],\n    'learning_rate': .05,\n    'max_depth': 0,\n    'subsample': .20,\n    'sampling_method': 'gradient_based',\n    'seed': 64,\n    'grow_policy': 'lossguide',\n    'max_leaves': 255,\n    'lambda': 75,\n}","85f3a85c":"%%time\nXGB_val_proba = np.zeros((len(y), num_class), dtype=np.float32)\nXGB_test_proba = np.zeros((len(X_test), num_class), dtype=np.float32)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=64)\nfor train_idx, val_idx in cv.split(X, y):\n    gc.collect()\n    dval = xgboost.DMatrix(X[val_idx], label=y[val_idx])\n    booster: xgboost.Booster = xgboost.train(\n        params=params,\n        dtrain= xgboost.DMatrix(X[train_idx], label=y[train_idx]),\n        num_boost_round= 1500,\n        evals=[(dval, 'val')],\n        early_stopping_rounds=100,\n        verbose_eval=100,\n    )\n    XGB_val_proba[val_idx] = booster.predict(\n        dval, iteration_range=(0, booster.best_iteration + 1)\n    )\n    XGB_test_proba += booster.predict(\n        xgboost.DMatrix(X_test), iteration_range=(0, booster.best_iteration + 1)\n    ) \/ cv.n_splits","b34a5754":"y_pred = label_encoder.inverse_transform(XGB_val_proba.argmax(axis=1))\ny_true = label_encoder.inverse_transform(y)\n\nprint(classification_report(y_true, y_pred))","fa9f21a4":"accuracy_score(y_true, y_pred)","6fa5d357":"sns.set(style='darkgrid', context='notebook', rc={'figure.figsize': (18, 9), 'figure.frameon': False})\n\nfscore = booster.get_fscore()\nfscore = pd.DataFrame({'value': fscore.values()}, index=feat_names[np.arange(len(fscore))])\nfscore.plot.barh();","9b940596":"XGB_test_preds = pd.DataFrame(XGB_test_proba, columns=XGB_col_names)","9ae7b756":"#y_pred = label_encoder.inverse_transform(test_proba.argmax(axis=1))\n#sub = pd.read_parquet(f'{data_root}\/tpsdec2021parquet\/test.pq', columns=['Id']).assign(Cover_Type=y_pred)\n#sub.head()\n#sub.to_csv('submission.csv', index=False)\n#sub.shape","7d9a0b1e":"catboost = CatBoostClassifier(use_best_model = True, random_seed = 64,eval_metric='AUC',iterations=1500)","9e26864f":"%%time\nCAT_test_proba = np.zeros((len(X_test), num_class), dtype=np.float32)\nCAT_val_proba = np.zeros((len(y), num_class), dtype=np.float32)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=64)\nfor train_idx, val_idx in cv.split(X, y):\n    gc.collect()\n    catboost.fit(X[train_idx], y[train_idx], eval_set= (X[val_idx], y[val_idx]), early_stopping_rounds=100,verbose_eval = 100)\n    XGB_val_proba[val_idx] = catboost.predict_proba(X[val_idx])  \n    CAT_prediction = catboost.predict_proba(X_test)\n    CAT_test_proba += CAT_prediction\/5","e0f7046b":"#y_pred = label_encoder.inverse_transform(val_proba.argmax(axis=1))\n#y_true = label_encoder.inverse_transform(y)\n\n#print(classification_report(y_true, y_pred))","b12fce27":"CAT_test_preds = pd.DataFrame(CAT_test_proba, columns=CAT_col_names)","f6b56f43":"final_results = pd.DataFrame(columns = ['XGB_Output', 'XGB_Prob', 'CAT_Output', 'CAT_Prob', 'Final_Pred'])\n\nfinal_results['XGB_Output'] = XGB_test_preds.idxmax(axis =1)\nfinal_results['XGB_Prob'] = XGB_test_preds.max(axis =1)\nfinal_results['CAT_Output'] = CAT_test_preds.idxmax(axis =1)\nfinal_results['CAT_Prob'] = CAT_test_preds.max(axis =1)","f540a048":"check = final_results[final_results['XGB_Output'] != final_results['CAT_Output']]","18fb0a29":"check1 = check[check['XGB_Prob']<check['CAT_Prob']]","fb29669e":"check1.head(25)","9a723a18":"XGB_test_preds.head(50)","971d4f5b":"def returnFinalCover(row):\n    if row[1] > row[3]:\n        return row[0]\n    else:\n        return row [2]  ","d42a21fd":"final_results['Final_Pred'] = final_results.apply(returnFinalCover,axis =1)","2df8ddf1":"final_results.head(25)","e52f48d3":"y_pred = final_results['Final_Pred'].astype(np.int8)\nsub = pd.read_parquet(f'{data_root}\/tpsdec2021parquet\/test.pq', columns=['Id']).assign(Cover_Type=y_pred)\nsub.head()","ff2463ee":"sub.to_csv('submission.csv', index=False)","7b71cbea":"Softprob classifier: To output probabilities instead of classes\n\nSubsample is the fraction of the samples that will be used at each iteration of the booster.\n\nlambda is l2 regularization parameter. \n\nFor more detailed explanation - do refer to https:\/\/www.kaggle.com\/kaaveland\/tps202112-reasonable-xgboost-model\n\n"}}