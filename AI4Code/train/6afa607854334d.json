{"cell_type":{"7d5f1343":"code","12b5684e":"code","22a1a532":"code","e5d4cc65":"code","02c3fc06":"code","7df76e8a":"code","ac8bb232":"code","81dbd2a6":"code","bae2af96":"code","16710c83":"code","4f6dc06f":"code","c28e1b74":"code","54620cb9":"code","d57c5e73":"code","d5d293a7":"code","24e22243":"code","23917030":"code","5012a530":"code","21f596a6":"code","358d9e8c":"code","9de7f68b":"code","3c4daa3e":"code","de9d1487":"code","7c4dd0cf":"code","1f0907cc":"code","7c86e1bb":"code","e265c30c":"code","62cfa2b3":"code","6545e9cc":"code","40b325f5":"code","b42587e1":"code","ff8797cb":"code","5bad9d0b":"markdown","11c72706":"markdown","d5b271bc":"markdown","554db429":"markdown","70da5667":"markdown","0be66a02":"markdown","d778adf2":"markdown","7e512cbb":"markdown","a6e9abf0":"markdown","d2080d2f":"markdown","804947d9":"markdown","85094d82":"markdown","e6626ee1":"markdown","261ebf06":"markdown","e313769b":"markdown","d7f9d292":"markdown","caf08ddf":"markdown","e6e85aec":"markdown"},"source":{"7d5f1343":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12b5684e":"#load libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","22a1a532":"df = pd.read_csv(\"\/kaggle\/input\/predicting-pulsar-starintermediate\/pulsar_data_train.csv\")","e5d4cc65":"df.head()","02c3fc06":"#let's check for missing\/ unique values\ndef dfaux (df):\n    cant = df.isnull().sum()\n    df_aux = pd.DataFrame(index = df.columns, data =\n                         {'type': df.dtypes,\n                          'unique_values': df.nunique(),\n                          'have_null?': df.isnull().any(),\n                          'how many?' : cant,\n                          'per' : cant\/df.shape[0]*100 })\n    return df_aux","7df76e8a":"dfaux(df)","ac8bb232":"df.columns","81dbd2a6":"#replacing null values\ndf[\" Excess kurtosis of the integrated profile\"] = df[\" Excess kurtosis of the integrated profile\"].replace(np.NaN, df[\" Excess kurtosis of the integrated profile\"].mean())","bae2af96":"#replacing null values\ndf[\" Standard deviation of the DM-SNR curve\"] = df[\" Standard deviation of the DM-SNR curve\"].replace(np.NaN, df[\" Standard deviation of the DM-SNR curve\"].mean())\ndf[\" Skewness of the DM-SNR curve\"] = df[\" Skewness of the DM-SNR curve\"].replace(np.NaN, df[\" Skewness of the DM-SNR curve\"].mean())","16710c83":"#lets check again\ndfaux(df)","4f6dc06f":"df.shape","c28e1b74":"df.info()","54620cb9":"df.describe()","d57c5e73":"#correlation\ncorr = df.corr()\nsns.heatmap(data=df.corr(),annot=True,cmap=\"coolwarm\",linewidths=1,fmt=\".2f\",linecolor=\"gray\")","d5d293a7":"#Correlation with target variable\ncor_target = abs(corr[\"target_class\"])#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0]\nrelevant_features.nlargest(n=12)","24e22243":"sns.pairplot(data=df,\n             palette=\"husl\",\n             hue=\"target_class\",\n             vars=[\" Mean of the integrated profile\",\n                   \" Excess kurtosis of the integrated profile\",\n                   \" Skewness of the integrated profile\",\n                   \" Mean of the DM-SNR curve\",\n                   \" Excess kurtosis of the DM-SNR curve\",\n                   \" Skewness of the DM-SNR curve\"])\n\nplt.suptitle(\"PairPlot of Data Without Std. Dev. Fields\",fontsize=18)\n\nplt.tight_layout()\nplt.show()   # pairplot without standard deviaton fields of data","23917030":"#feature scaling\nfeatures = df.drop(\"target_class\", axis=1)\nscaler = MinMaxScaler(feature_range=(0,1))\nfscaled = scaler.fit_transform(features)","5012a530":"#separating input and target variable\nX = df.drop(\"target_class\", axis=1)\nY = df[\"target_class\"]","21f596a6":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 0 )","358d9e8c":"#1. SGD\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_test)\nacc_sgd = round(sgd.score(x_train, y_train)*100, 2)","9de7f68b":"print(acc_sgd)","3c4daa3e":"#2. Random Forest\nrforest = RandomForestClassifier(n_estimators = 100)\nrforest.fit(x_train, y_train)\ny_pred2 = rforest.predict(x_test)\n\nacc_rf = round(rforest.score(x_train, y_train)* 100, 2)\nprint(acc_rf)","de9d1487":"#3. Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred3 = logreg.predict(x_test)\n\nacc_lr = round(logreg.score(x_train, y_train) * 100, 2)\nprint(acc_lr)","7c4dd0cf":"#4. KNN\nknn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(x_train, y_train)\ny_pred4 = knn.predict(x_test)\nacc_knn = round(knn.score(x_train, y_train)*100 ,2)\nprint(acc_knn)","1f0907cc":"#5. GNB\ngauss = GaussianNB()\ngauss.fit(x_train, y_train)\ny_pred5 = gauss.predict(x_test)\nacc_gauss = round(gauss.score(x_train, y_train)*100,2)\nprint(acc_gauss)","7c86e1bb":"#6. Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ny_pred6 = dt.predict(x_test)\nacc_dt = round(dt.score(x_train, y_train)*100,2)\nprint(acc_dt)","e265c30c":"importances = pd.DataFrame({'feature':x_train.columns, 'importance':np.round(dt.feature_importances_, 3)})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\nimportances.head(15)","62cfa2b3":"importances.plot.bar()","6545e9cc":"#confusion matrix\nfrom sklearn.metrics import precision_score, recall_score\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(dt, x_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","40b325f5":"#f1 score\nfrom sklearn.metrics import f1_score\nf1_score(y_train, predictions)","b42587e1":"#roc-auc\ny_scores = dt.predict_proba(x_train)","ff8797cb":"y_scores= y_scores[:,1]\nfrom sklearn.metrics import roc_auc_score\nrascore  = roc_auc_score(y_train, y_scores)\nprint(rascore)","5bad9d0b":"A lot of the columns shows positive correlation","11c72706":"# EDA","d5b271bc":"There are quite many null values in three of the total features. Let's replace the null values with mean","554db429":"# Optimization","70da5667":"# Data Cleaning","0be66a02":"We can use out-of-bag sample to estimate generalization of the accuracy. It is as accurate as using the test set.","d778adf2":"# Load Data","7e512cbb":"All the features are of float type, even the target class","a6e9abf0":"# Load Libraries","d2080d2f":"The mean of the target variable is 9.2%, hence we can say that only 9.2% stars are pulsar stars ","804947d9":"# Feature Importance","85094d82":"Clearly, decision tree is the winner here","e6626ee1":"Data is quite separable on most of the columns. Since I do not have the description of these features, I won't drop any of these columns and carry on ","261ebf06":"Our model predicts 79% of the time, a star's identification as pulsar correctly\n\nOur model tells us that it predicted the survival of 81% of the the stars who were actually pulsar stars","e313769b":"# Modeling","d7f9d292":"Skewness of integrated profile plays a humongous role to predict the target variable. ","caf08ddf":"Perfect, let's get to EDA now!","e6e85aec":"12528 -> rows\n\n9 -> columns(features)"}}