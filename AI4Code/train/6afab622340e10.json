{"cell_type":{"31dc82fa":"code","7302969b":"code","82ef342e":"code","eb44c12b":"code","4e350d03":"code","3149f934":"code","d755c69f":"code","94970b12":"code","dde6d625":"code","04000f94":"code","cf969a30":"code","a6b42003":"code","0264e732":"code","e1e521cc":"code","eec6b426":"code","9319d853":"code","87cdb3fc":"code","f021d049":"code","c387b51a":"code","35bc432a":"code","c2328f9e":"code","7a79cea5":"code","96f2fe56":"code","019f04d3":"code","5627c20f":"code","10fd2dae":"code","1bebdf09":"code","a27fde36":"code","05eb500f":"code","449668e9":"code","3fc1d24b":"code","53d413f2":"code","8790d9e8":"code","1a71f069":"code","e152cb99":"code","ddd84151":"code","b76e811a":"code","88126481":"code","9525f905":"code","4f1e4102":"code","2d5fcddd":"code","a4f8d5fe":"code","6f64ba7f":"code","8eafbc8f":"code","846733be":"code","11855a4a":"code","bd6af257":"code","a83baf02":"markdown","b9fb7fae":"markdown","19bad618":"markdown","953fa126":"markdown","6dce062b":"markdown","ba9e9933":"markdown","4e2ac307":"markdown","b95689cf":"markdown","3d3f62c5":"markdown","168b607d":"markdown","0c04a04d":"markdown","d5f08f12":"markdown"},"source":{"31dc82fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport statistics as st\nimport matplotlib.pyplot as plt \nimport json\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import plot_roc_curve\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7302969b":"files=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file1 = os.path.join(dirname, filename)\n        files.append(file1)\n# files","82ef342e":"# files_json = [f for f in files if f[-4:] == 'json']\nfiles_csv = [f for f in files if f[-3:] == 'csv']\nprint('There are total ', len(files_csv), 'files')","eb44c12b":"files_csv.sort()\nfiles_csv","4e350d03":"df = pd.DataFrame()\nshp = np.zeros(len(files_csv))\ni=0\n\nfor f in files_csv:\n    \n    data = pd.read_csv(f, names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\", \"f10\", \"f11\", \"f12\", \"f13\"])\n    shp[i] = len(data)\n    df = df.append(data, sort=False)\n    i = i+1\n    \n \n    \nprint(df.shape)\ndf.to_csv(r'\/kaggle\/working\/file1.csv', index = False)","3149f934":"def convert_data(pd_data):\n    pdd = pd_data.loc[:,:].values\n    return(pdd)","d755c69f":"def export_training_data(d, lb, cn):\n    d1 = pd.DataFrame(d)\n    lb1 = pd.DataFrame(lb)\n    exporting_data = pd.concat([d1, lb1], axis=1, sort=False) \n    return(exporting_data)","94970b12":"def join_array(d3, dnew):\n    return(np.concatenate((d3, dnew), axis=0))","dde6d625":"def find_vars(X_1):\n    wa = np.zeros((X_1.shape[0],1))\n    for i in range (1,X_1.shape[0]):\n        wa[i] = st.mean(X_1[i,:])\n\n    plt.plot(wa)\n    vvr = np.zeros((wa.shape[0],1))\n    for i in range (0, wa.shape[0]-15):\n        vvr[i] = st.variance(wa[i:i+14,0])\n    return(vvr)","04000f94":"def apply_pca(da):\n    from sklearn.decomposition import PCA \n\n    pca = PCA(n_components = 1) \n\n    X_1 = pca.fit_transform(da) \n    # X_2 = pca.transform(X_test) \n\n    # explained_variance = pca.explained_variance_ratio_ \n    X_1.shape\n    return(X_1)","cf969a30":"def predict_knn(X1, X2, y1, y2, kn):\n    classifier = KNeighborsClassifier(n_neighbors=kn, algorithm='brute')\n    classifier.fit(X1, y1)\n    predicted = classifier.predict(X2)\n    print(\"KNN: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"KNN is \", (correct), \"percent accurate\")\n    return predicted, correct","a6b42003":"def predict_nb(X1, X2, y1, y2):\n    classifier = GaussianNB()\n    classifier.fit(X1, y1)\n    predicted = classifier.predict(X2)\n    print(\"Naive Bayes: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"Naive Bayes is \", (correct), \"percent accurate\")\n    return predicted, correct","0264e732":"def predict_lda(X1, X2, y1, y2, n):\n    classifier = LinearDiscriminantAnalysis(n_components=n, store_covariance=False)\n    classifier.fit(X1,y1)\n    lda = classifier.fit_transform(X1,y1)\n    predicted = classifier.predict(X2)\n    print(\"LDA: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"LDA is \", (correct), \"percent accurate\")\n    return predicted, correct, lda","e1e521cc":"def apply_linear_reg(X1, X2, y1, y2):\n    classifier = LinearRegression(normalize=True)\n    classifier.fit(X1, y1)\n    predicted = classifier.predict(X2)\n    print(\"Linear Regression: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"Linear Regression is \", (correct), \"percent accurate\")\n    return predicted, correct","eec6b426":"def apply_lg(X1, X2, y1, y2):\n    \n    classifier = LogisticRegression()\n\n    classifier.fit(X1, y1)\n    predicted=classifier.predict(X2)\n    print(\"Logistic Regression: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"Logistic Regression is \", (correct), \"percent accurate\")\n    return predicted, correct","9319d853":"def predict_svm(X1, X2, y1, y2):\n    classifier = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n    classifier.fit(X1, y1)\n    predicted = classifier.predict(X2)\n    print(\"SVM: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"SVM is \", (correct), \"percent accurate\")\n    return predicted, correct","87cdb3fc":"def predict_dt(X1, X2, y1, y2):\n    classifier = tree.DecisionTreeClassifier(criterion='gini', splitter='best')\n    classifier.fit(X1, y1)\n    tree.plot_tree(classifier)\n    predicted = classifier.predict(X2)\n    print(\"Decision Tree: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"Decision Tree is \", (correct), \"percent accurate\")\n    return predicted, correct","f021d049":"def predict_rf(X1, X2, y1, y2):\n    classifier = RandomForestClassifier(n_estimators = 200, oob_score = True, criterion = \"gini\", random_state = 0)\n    classifier.fit(X1, y1)\n    predicted = classifier.predict(X2)\n    print(\"Random Forrest: Number of mislabeled points out of a total %d points : %d\",\n          (predicted.shape, (y2 != predicted).sum()))\n    correct = 100-(y2 != predicted).sum()\/predicted.shape*100\n    print(\"Random Forrest is \", (correct), \"percent accurate\")\n    return predicted, correct","c387b51a":"def cluster_data(d):\n    kmeans = KMeans(n_clusters=5, max_iter=500)\n    kmeans.fit(d)\n    labels = kmeans.predict(d)\n    centroids = kmeans.cluster_centers_\n    return(labels, centroids)","35bc432a":"def split_for_kfold(t_data, lbs, kd):\n    kfold = KFold(n_splits=kd)\n    for train_index1, test_index1 in kfold.split(t_data):\n        print(\"TRAIN:\", train_index1.shape, \"TEST:\", test_index1.shape)\n        X_train, X_test = t_data[train_index1], t_data[test_index1]\n    for train_index2, test_index2 in kfold.split(lbs):\n        print(\"TRAIN labels:\", train_index2.shape, \"TEST labels:\", test_index2.shape)\n        y_train, y_test = lbs[train_index2], lbs[test_index2]\n#     t_data.shape\n    return X_train, X_test, y_train, y_test","c2328f9e":"# def apply_all(Xt, yt, kn, n, kd):\n#     predicted = np.zeros((0, 6))\n#     correct = np.zeros((0, 6))\n#     X1, X2, y1, y2 = split_for_kfold(Xt, yt, kd)\n#     predicted[0], correct[0] = predict_knn(X1, X2, y1, y2, kn)\n#     predicted[1], correct[1] = predict_nb(X1, X2, y1, y2)\n#     predicted[2], correct[2] = predict_lda(X1, X2, y1, y2, n)\n#     predicted[3], correct[3] = predict_svm(X1, X2, y1, y2)\n#     predicted[4], correct[4] = predict_dt(X1, X2, y1, y2)\n#     predicted[5], correct[5] = predict_rf(X1, X2, y1, y2)\n    \n#     predictions = pd.DataFrame()\n#     predictions['knn_score'] = predicted[0]\n#     predictions['nb_score'] = predicted[1]\n#     predictions['svm_score'] = predicted[3]\n#     predictions['dt_score'] = predicted[4]\n#     predictions['rf_score'] = predicted[5]\n#     predictions\n\n#     return predict","7a79cea5":"# Training Data\ntraincv_data = convert_data(df)\ntraincv_data = traincv_data[:,1:]\ndf.shape\ntraincv_data.shape\ndf","96f2fe56":"features = [\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\", \"f10\", \"f11\", \"f12\", \"f13\"]\nplt.plot(df)","019f04d3":"traincv_data.shape\nav = np.zeros((traincv_data.shape[0],1))\nfor i in range (1,traincv_data.shape[0]):\n    av[i] = st.mean(traincv_data[i,:])\n\n# av\nplt.plot(av[:,:])","5627c20f":"av_pd = pd.DataFrame(av[:,:])\nsns.pairplot(av_pd, palette='OrRd')","10fd2dae":"X_pca = apply_pca(traincv_data) #In 1\nvvr = find_vars(X_pca)\nplt.plot(vvr)","1bebdf09":"plt.plot(traincv_data, 'green')\nplt.plot(X_pca, 'red')","a27fde36":"vvr","05eb500f":"#Clustering data\nmydata = convert_data(df)\nmydata = mydata[:,1:]\n# labels, centroids = cluster_data(mydata)\n# traincv1_data = traincv_data[700+116:,:]\n# labels, centroids = cluster_data(traincv_data)\nlabels, centroids = cluster_data(vvr)\ntraining_data = export_training_data(mydata, labels, centroids)\n# training_data.to_csv(r'\/kaggle\/working\/training_data.csv', index = False)\nlabels\nplt.plot(labels)\n# plt.plot(vvr)","449668e9":"v_l = pd.DataFrame(vvr)\nv_l[\"Labels\"] = labels\nv_l\nsns.pairplot(v_l, hue='Labels', palette='OrRd')","3fc1d24b":"df[\"Pre\"] = labels\ndf[\"vvr\"] = vvr\nsns.pairplot(df, hue='Pre', palette='OrRd')","53d413f2":"X_train, X_test, y_train, y_test = split_for_kfold(traincv_data, labels, kd=9)","8790d9e8":"X_train.shape\ny_train.shape","1a71f069":"X_pca_testing = apply_pca(X_test)\nvvr_testing = find_vars(X_pca_testing)\nX_pca_training = apply_pca(X_train)\nvvr_training = find_vars(X_pca_training)","e152cb99":"# Training & Prediction of the model\nknn_pred, knn_score = predict_knn(vvr_training, vvr_testing, y_train, y_test, kn=3)\nknn_pred\nplt.plot(knn_pred)","ddd84151":"nb_pred, nb_score = predict_nb(vvr_training, vvr_testing, y_train, y_test)\nnb_pred\nplt.plot(nb_pred)","b76e811a":"lng_pred, lng_score = apply_linear_reg(vvr_training, vvr_testing, y_train, y_test)\nplt.plot(lng_pred)","88126481":"lg_pred, lg_score = apply_lg(vvr_training, vvr_testing, y_train, y_test)\nplt.plot(lg_pred)","9525f905":"lda_pred, lda_score, lda = predict_lda(X_train, X_test, y_train, y_test, n=2)\nplt.plot(X_train, 'green')\nplt.plot(lda, 'red')","4f1e4102":"plt.plot(lda_pred, 'red')","2d5fcddd":"svm_pred, svm_score = predict_svm(vvr_training, vvr_testing, y_train, y_test)\nsvm_pred\nplt.plot(svm_pred)","a4f8d5fe":"dt_pred, dt_score = predict_dt(vvr_training, vvr_testing, y_train, y_test)","6f64ba7f":"plt.plot(dt_pred)","8eafbc8f":"rf_pred, rf_score = predict_rf(vvr_training, vvr_testing, y_train, y_test)\nrf_pred\nplt.plot(rf_pred)","846733be":"k1 = 9\npredictions = pd.DataFrame()\npredictions['Kfold'] = [k1]\npredictions['knn_score'] = knn_score\npredictions['nb_score'] = nb_score\npredictions['svm_score'] = svm_score\npredictions['dt_score'] = dt_score\npredictions['rf_score'] = rf_score\npredictions","11855a4a":"for i in range (5, 9):\n    print('\\nKfold = ', i)\n    Xt1, Xt2, yt1, yt2 = split_for_kfold(traincv_data, labels, kd=i)\n    knnpred, knnscore = predict_knn(Xt1, Xt2, yt1, yt2, kn=5)\n    nbpred, nbscore = predict_nb(Xt1, Xt2, yt1, yt2)\n    # ldapred, ldascore = predict_lda(X1, X2, y1, y2, n)\n    svmpred, svmscore = predict_svm(Xt1, Xt2, yt1, yt2)\n    dtpred, dtscore = predict_dt(Xt1, Xt2, yt1, yt2)\n    rfpred, rfscore = predict_rf(Xt1, Xt2, yt1, yt2)\n    new_row = {'Kfold':i,'knn_score':knnscore, 'nb_score':nbscore, 'svm_score':svmscore, 'dt_score':dtscore, 'rf_score':rfscore}\n    predictions = predictions.append(new_row, ignore_index=True)","bd6af257":"predictions","a83baf02":"**Assignment#1 Applying PCA**","b9fb7fae":"**Assignment#4 Applying LDA**","19bad618":"**Quiz#1 Applying SVM**","953fa126":"**Quiz#4 Applying K-Means Clustering**","6dce062b":"**Assignment#2 Applying KNN**","ba9e9933":"**Quiz#6 Validate All**","4e2ac307":"**Quiz#2 Applying Decision Trees**","b95689cf":"**Assignment#5 Applying Linear Regression**","3d3f62c5":"**Quiz#5 k-Fold cross validation**","168b607d":"**Assignment#6 Applying Logistic Regression**","0c04a04d":"**Quiz#3 Applying Random Forest**","d5f08f12":"Assignment#3 Applying Naive Bayes"}}