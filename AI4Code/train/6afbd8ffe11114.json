{"cell_type":{"246dc89e":"code","f0141820":"code","ab73517a":"code","afe7b26d":"code","bc1b6491":"code","89eabe4d":"code","bf138b42":"code","fbeb820a":"code","08b26269":"code","972e2764":"code","2aaa81e8":"code","9799b3a2":"code","6eccda7b":"code","cd518623":"code","4f6f2908":"code","b5b401a0":"code","a367f25a":"code","67324058":"code","4bcb1792":"code","ec1c193d":"code","31880c58":"code","0cbc1cc4":"code","d7cae5f1":"code","cc2d8070":"code","83902992":"code","b99a747d":"code","2c711b43":"code","308ed4af":"code","9323a559":"code","dc2f9c8c":"code","8f490fd6":"code","160146af":"code","7415421a":"code","c4eb2d0a":"code","d3616d45":"code","b8408dff":"code","8f121d7e":"code","ed0a62e9":"code","167f648f":"code","098fcaa6":"code","188d4e3a":"code","b952556f":"code","4252d8ae":"code","e325838d":"code","639fc8fd":"code","ca8d0112":"code","2f159b6d":"code","3ba07722":"code","d4132003":"markdown","c0a20bd2":"markdown","f59ad7f4":"markdown","ebcca417":"markdown","2e84c5b4":"markdown","16cf62de":"markdown","dc53958f":"markdown","50367444":"markdown","9a55693c":"markdown","c126b6d9":"markdown","1efe2175":"markdown","075fdce4":"markdown","bcb50786":"markdown","24358b9d":"markdown","3660cbd6":"markdown","16c6c6b9":"markdown","ee5c518c":"markdown","f377bf03":"markdown","a9ccfc48":"markdown","d8a5ed9a":"markdown","5a5528e3":"markdown","218a1c6c":"markdown"},"source":{"246dc89e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nignore=True\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0141820":"df=pd.read_csv(\"\/kaggle\/input\/predicting-profitable-customer-segments\/customerTargeting.csv\")","ab73517a":"df.head()","afe7b26d":"df.shape","bc1b6491":"df.info()","89eabe4d":"df.describe()","bf138b42":"fig=plt.figure(figsize=(12,7))\nsns.countplot(x=df[\"target\"], palette = 'rocket_r')","fbeb820a":"from sklearn.feature_selection import VarianceThreshold\nX=df.drop(['target'],axis=1)\nY=df['target']","08b26269":"var_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X)","972e2764":"sum(var_thres.get_support()) #Counting columns with variance threshold by grt_support method","2aaa81e8":"constant_columns = [column for column in X.columns #Checking for contsant columns \n                    if column not in X.columns[var_thres.get_support()]]","9799b3a2":"X.drop(constant_columns,axis=1)#Dropping constant columns ","6eccda7b":"#Feature Correlation\ncor_target =df.corr().abs()\nTarget_Corr = cor_target.corr()['target'].to_frame().reset_index() #Feature Correlation related to SalePrice\nFeature_corr =cor_target.unstack().to_frame(name='Correlation') # Feature Relation\nFeature = Feature_corr[(Feature_corr['Correlation']>=0.80)&(Feature_corr['Correlation']<1)].sort_values(by='Correlation', ascending = False).reset_index()\ndisplay(Feature)","cd518623":"cor_target =df.corr()\n# Select upper triangle of correlation matrix\nupper = cor_target.where(np.triu(np.ones(cor_target.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.82\nto_drop = [column for column in upper.columns if any(upper[column] > 0.82)]\ndf=df.drop(df[to_drop], axis=1)","4f6f2908":"my_corr=df.corr() # Correlation of newly created dataframe \ncor_target = abs(my_corr[\"target\"]) # Correlation with respect to target column \n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.2] # Columns whose correlation is greater than 0.5\ndisplay(relevant_features) # getting the correlation values with Correlation greater than 0.5","b5b401a0":"X=df.drop(['target'],axis=1)\nY=df['target']","a367f25a":"from statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nX_vif = add_constant(X)\nvif = pd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)","67324058":"display(vif.sort_values(ascending = False).head(10))","4bcb1792":"df.shape","ec1c193d":"from sklearn.feature_selection import mutual_info_classif\nmutual_info = mutual_info_classif(X,Y)\nmutual_info","31880c58":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X.columns\nmutual_info.sort_values(ascending=False)","0cbc1cc4":"# Selecting Top 10 Columns using kbest\nfrom sklearn.feature_selection import SelectKBest\nsel_cols = SelectKBest(mutual_info_classif, k=10)\nsel_cols.fit(X,Y)\nX.columns[sel_cols.get_support()]","d7cae5f1":"X=df[['g1_1','g1_10', 'g1_5', 'g2_1', 'g2_19', 'c_2', 'c_3', 'c_10','c_11','c_25']]\nY=df[\"target\"]","cc2d8070":"display(X.shape)\ndisplay(Y.shape)","83902992":"X.describe()","b99a747d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","2c711b43":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler(feature_range=(0,1))\nX_train = mms.fit_transform(X_train)\nX_test = mms.fit_transform(X_test)","308ed4af":"#!pip install pycaret","9323a559":"# Importing PyCaret Module based on type of Problem\nfrom pycaret import classification\nfrom pycaret.classification import * ","dc2f9c8c":"# Setting up of variables depending on its nature (continuous or Categorical) and also selecting those feature which are not important \n# for further analysis.\nclassification_setup=setup(data=df,target='target',numeric_features=['g1_1','g1_10','g1_5','g2_1','g2_19','c_2',\n                                                                     'c_3','c_11','c_10','c_25'],\n                          ignore_features=['g1_2', 'g1_3', 'g1_11', 'g1_7', 'g1_15','g1_16', \n                           'g1_17', 'g1_18', 'g1_19', 'g1_20', 'g1_21','g2_2','g2_3', 'g2_5', 'g2_7', \n                           'g2_10', 'g2_15', 'g2_16', 'g2_17','g2_18', 'g2_11', 'g2_20', 'c_1', 'c_5', \n                           'c_4','c_7', 'c_8', 'c_9', 'c_13', 'c_14', 'c_15', 'c_16','c_22', 'c_23', \n                           'c_24', 'c_27', 'c_28','c_6'],silent = True)","8f490fd6":"compare_models() # Here pycaret will build different model based on selections we have made above. It will show accuracy and other metrics \n# that how has different model performed on data.","160146af":"gbc  = create_model('gbc') # Based on Above Summarized result Gradient Boost Performs best ","7415421a":"tuned_gbc = tune_model(gbc,n_iter = 100,optimize = 'AUC')#Optimizing based on AUROC Scores","c4eb2d0a":"plot_model(estimator = tuned_gbc, plot = 'learning')#Plotting Learning curve","d3616d45":"plot_model(estimator = tuned_gbc, plot = 'auc')#Plotting AUC Scores","b8408dff":"plot_model(estimator = tuned_gbc, plot = 'confusion_matrix')#Plotting Confusion Matrix","8f121d7e":"plot_model(estimator = tuned_gbc, plot = 'feature')#Feature Importances Based on GBModel","ed0a62e9":"evaluate_model(tuned_gbc)","167f648f":"#interpret_model(tuned_gbc)","098fcaa6":"#predict_model(tuned_gbc, data=X_test)","188d4e3a":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import  AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8),random_state = 42)\nparameters = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[5,10,15,20,100],\n              \"learning_rate\":  [0.05, 0.5, 1]}\nada_clf = GridSearchCV(ada_clf, parameters, cv=3, scoring=\"accuracy\")\nada_clf.fit(X_train, y_train)\nprint(f'Best parameters {ada_clf.best_params_}')\nprint('-----')\nprint(f'Mean cross-validated accuracy score of the best_estimator: '+f'{ada_clf.best_score_:.3f}')","b952556f":"print(\"Test Accuracy:\",ada_clf.score(X_test, y_test))","4252d8ae":"from sklearn.metrics import classification_report , confusion_matrix , accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\n\nY_Pred=ada_clf.predict(X_test)\ncnf_mat=confusion_matrix(y_test, Y_Pred)\nfig, ax = plot_confusion_matrix(conf_mat=cnf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","e325838d":"from yellowbrick.classifier import ConfusionMatrix\nfig=plt.figure(figsize=(8,8))\nclasses=[\"0\", \"1\", \"2\"]\ncnf_mat=ConfusionMatrix(ada_clf, classes=classes,label_encoder={0: 'non_Profitable', 1: 'Group_1', 2: 'Group_2'})\ncnf_mat.fit(X_train, y_train)\ncnf_mat.score(X_test, y_test)\ncnf_mat.show()","639fc8fd":"fig=plt.figure(figsize=(12,7))\nfrom yellowbrick.classifier import ClassificationReport\nclasses=[\"0\", \"1\", \"2\"]\nvisualizer = ClassificationReport(ada_clf, classes=classes, support=True)\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()       ","ca8d0112":"fig=plt.figure(figsize=(15,8))\nfrom yellowbrick.classifier import PrecisionRecallCurve\nviz = PrecisionRecallCurve(ada_clf,per_class=True,\n    cmap=\"Set1\")\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test)\nviz.show()","2f159b6d":"fig=plt.figure(figsize=(18,8))\nfrom yellowbrick.classifier import ROCAUC\nvisualizer = ROCAUC(ada_clf, classes=[\"0\", \"1\", \"2\"])\nvisualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show() ","3ba07722":"from yellowbrick.classifier import ClassPredictionError\nfig=plt.figure(figsize=(18,8))\nvisualizer = ClassPredictionError(ada_clf,per_class=True,cmap=\"Set1\")\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.show()","d4132003":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Class Prediction Error<\/h1>","c0a20bd2":"**Content**\nIn order to enable machine learning experimentation, this dataset has been structured as follows:\n\nEach row is a comparison between two groups of potential customers:<br>\n1. Column names starting with \"g1\" represent characteristics of the first customer group (these were known before the campaign was run).\n2. Column names starting with \"g2\" represent characteristics of the second customer group (these were known before the campaign was run)\n3. Column names starting with \"c_\" are features representing some comparison of the two groups (also known before the campaign was run)\n\nThe last column, named \"target\", is categorical, with 3 categories:<br>\n0 - none of the two groups were profitable<br>\n1 - group1 turned out to be more profitable<br>\n2 - group2 turned out to be more profitable<br>\n\n**Inspiration**\nCan you build a machine learning classifier that accurately predicts which of the 2 groups (if any) will turn out to be more profitable?","f59ad7f4":"<h1 style=\"background-color:Crimson;color:white;text-align:center;\">Model<\/h1>","ebcca417":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">AUROC<\/h1>","2e84c5b4":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Confusion Matrix<\/h1>","16cf62de":"Selecting K-Best Features based on Target","dc53958f":"<h1 style=\"background-color:Crimson;color:white;text-align:center;\">Mutual Info Gain<\/h1>","50367444":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">ADABoost Model<\/h1>","9a55693c":"<h1 style=\"background-color:Crimson;color:white;text-align:center;\">Checking VIF<\/h1>","c126b6d9":"# <h1 style=\"background-color:Crimson;color:white;text-align:center;\">Analysing Target<\/h1>","1efe2175":"<h1 style=\"background-color:Crimson;color:white;text-align:center;\">Getting Correlation Among Features & with Target<\/h1>","075fdce4":"Mutual information (MI) is a measure of the amount of information between two random variables is symmetric and non-negative, and it could be equal to zero if and only if two random variables are independent, and higher values mean higher dependency. Mutual information measures the dependency between the variables.","bcb50786":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Observations<\/h1>","24358b9d":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Implementing PYCARET (An AutoML Approach)<\/h1>","3660cbd6":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Train-Test Split<\/h1>","16c6c6b9":"<div class=\"alert alert-info\" role=\"alert\">\nDropping Columns based on Correlation<\/div>","ee5c518c":"Based on my Analysis model would suggest to go for Customers in Group 1. Reason being:\n1. Our Model Accuracy being 59% says about 59% of time it correctly labelled the customer in respective groups.\n2. The precision of Model for Class1(0.63) > Class 0(0.36) & Class 2(0.57) which says among the labelled customers as Class 1. Model correctly predicts them as class 1 with score of 63% i.e 63 out of every 100 class 1 customers are correctly predicted.\n3. In this scenario we want our assumptions to be more correct rather than false negatives and true negative( i.e we want to be more confident in our decisions so based on True Positive rates we can see 80% True Positive Scores.). Though it would also depend on customer behaviour and other demographic and social factors.\n4. Also the Recall Score of Class 1 Group is better than other two group which says (False Positive better than False Negative). \n<br>\n\n**Suggestion:** Should perform A\/B Testing. \n\n<br>\n**Still Room for Improvement**","f377bf03":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Min-Max Scaling<\/h1>","a9ccfc48":"Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. Mathematically, the VIF for a regression model variable is equal to the ratio of the overall model variance to the variance of a model that includes only that single independent variable. This ratio is calculated for each independent variable. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model.\n\n1. A variance inflation factor (VIF) provides a measure of multicollinearity among the independent variables in a multiple regression model.\n2. Detecting multicollinearity is important because while multicollinearity does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables. \n3. A large variance inflation factor (VIF) on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.","d8a5ed9a":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Classification Report<\/h1>","5a5528e3":"<h1 style=\"background-color:Crimson;color:white;text-align:center;\">Dropping Feature(Variance Threshold)<\/h1>\n<div class=\"alert alert-info\">Dropping Columns based on low Variance since they do not contribute much in prediction. This method removes features with variation below a certain cutoff.<\/div>","218a1c6c":"<h1 style=\"background-color:lightgreen;color:white;text-align:center;\">Precision-Recall Curve<\/h1>"}}