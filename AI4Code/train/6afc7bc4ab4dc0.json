{"cell_type":{"c0cbe7d9":"code","ff40604e":"code","984d31ad":"code","7568679c":"code","1fe2d2c4":"code","724e714a":"code","445184a7":"code","91372175":"code","e82402f0":"code","b67fc599":"code","9e8d80b1":"code","aa608c4a":"code","7c05094f":"code","52f09f1e":"code","2429c9b0":"code","acf8639f":"code","64faca9f":"code","ba80eaf3":"code","b93f5b9d":"code","693f449a":"code","0ce20bdd":"code","01ef5441":"code","af0183f0":"code","1622303b":"code","f93da83f":"code","151a8b4c":"markdown","fc67a8c2":"markdown","973e133e":"markdown","2af6b526":"markdown","91964ef2":"markdown","bcfd905b":"markdown","80fdd503":"markdown","82580c33":"markdown","217deaaa":"markdown","253164c0":"markdown"},"source":{"c0cbe7d9":"# LIBRARIES\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nstdScaler = StandardScaler()\nrobScaler = RobustScaler()","ff40604e":"# DATA\ndf = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","984d31ad":"# COLUMNS IN THE DATASET\nprint(df.columns)","7568679c":"# MISSING VALUES\nprint('# Missing (or NaN) Values: ', df.isnull().sum().max())\n\n# PERCENT OF CLASSES\nvalueCounts = df['Class'].value_counts()\nprint('# of No Fraud data: ', round(valueCounts[0]\/len(df) * 100, 2), '%')\nprint('# of No Fraud data: ', round(valueCounts[1]\/len(df) * 100, 2), '%')\n# PLOT THIS PERCENT\nsns.countplot('Class', data=df)\nplt.title('Class Distribution\\n[ 0 - Not Fraud | 1 - Fraud ]')\n\n# ### Conclusion : The dataset is highly imbalanced!","1fe2d2c4":"print(df['Class'].value_counts())\nprint(len(df['Class']))","724e714a":"# SCALING THE FEATURES\n# DATA DESCRIPTION (Mean, Std Dev, Min, Max)\ndf.describe()","445184a7":"# INITIAL DATA SET\ndf.head()","91372175":"# From the df.describe() and df.head(), we can easily see that the Vx values have already been scaled. We only need to scale the Time and Amount cols appropriately\n\n# SCALING AMOUNT AND TIME FIELDS\nscaledAmt = robScaler.fit_transform(df['Amount'].values.reshape(-1, 1))\nscaledTime = robScaler.fit_transform(df['Time'].values.reshape(-1, 1))\n\n# DROPPING ORIGINAL UN-SCALED FIELDS\ndf.drop(['Amount', 'Time'], axis=1, inplace=True)\n\n# INSERT THESE NEW SCALED COLS INTO THE DATAFRAME\ndf.insert(0, 'scaledAmount', scaledAmt)\ndf.insert(1, 'scaledTime', scaledTime)\n\n# VERIFY\ndf.head()","e82402f0":"# # HEATMAP - 1 --> Not useful. Hence the below heatmap was plotted\n# f, ax = plt.subplots(figsize=(15,15))\n# sns.heatmap(df, yticklabels=False, ax=ax)\n# ax.set_title('Heat Map for original DF')","b67fc599":"# HEAT MAP - 2\nf, ax = plt.subplots(figsize=(24,20))\nsns.heatmap(df.corr(), cmap='YlGnBu', annot_kws={'size':20}, ax=ax)\nax.set_title(\"Heatmap for Correlation Matrix of DF\", fontsize=14)\n\n# Conclusion : V2, V4, V11 give high correlation (positive) with the Class field - So we can use them to perform further analysis","9e8d80b1":"# BOX PLOTS\nfor i in range(7):\n    f, axs = plt.subplots(ncols=4, figsize=(20,4))\n    for j in range(4):\n        v_num = 4*i + j + 1\n        ftr_name = 'V' + str(v_num)\n        sns.boxplot(x='Class', y=ftr_name, data=df, ax=axs[j])\n        axs[j].set_title('Checking distribution for field: ' + ftr_name)","aa608c4a":"# FREQUENCY PLOTS\nfor i in range(7):\n    f, axs = plt.subplots(ncols=4, figsize=(20,4))\n    for j in range(4):\n        v_num = 4*i + j + 1\n        ftr_name = 'V' + str(v_num)\n        sns.distplot(df[ftr_name], ax=axs[j])\n        axs[j].set_title('Checking frequency distribution for field: ' + ftr_name)","7c05094f":"# Print the fraction of class just to ensure later\nclass_vals = df['Class'].value_counts()\nprint('Number of `No Fraud`: ', class_vals[0], '. Percentage of Dataset that is `No Fraud`: ', round(100 * class_vals[0] \/ len(df), 3))\nprint('Number of `Fraud`: ', class_vals[1], '. Percentage of Dataset that is `Fraud`: ', round(100 * class_vals[1] \/ len(df), 3))\nprint('')\n\n# Get X and Y\nX = df.drop('Class', axis=1)\nY = df['Class']\n\n# init the vars\n# Xtrain_prime = []\n# Ytrain_prime = []\n# Xtest_prime = []\n# Ytest_prime = []\n\n# Split proportionately to create as much randomness as possible \nfor train_index, test_index in StratifiedKFold(n_splits=5, random_state=None, shuffle=False).split(X, Y):\n    print('Train Index: ', train_index, 'Test Index: ', test_index)\n    Xtrain_prime, Xtest_prime = X.iloc[train_index], X.iloc[test_index]\n    Ytrain_prime, Ytest_prime = Y.iloc[train_index], Y.iloc[test_index]\n\n# Converting the DataFrame into arrays\nXtrain_prime = Xtrain_prime.values\nYtrain_prime = Ytrain_prime.values\nXtest_prime = Xtest_prime.values\nYtest_prime = Ytest_prime.values\n\n\n# Ensure Train and Test sets are equally distributes for both 1 and 0 cases\n_, number_of_trains = np.unique(Ytrain_prime, return_counts=True)\n_, number_of_tests = np.unique(Ytest_prime, return_counts=True)\nprint(100 * number_of_trains \/ len(Ytrain_prime))\nprint(100 * number_of_tests \/ len(Ytest_prime))","52f09f1e":"# Shuffle the original DataFrame\ndf = df.sample(frac = 1)\n\n# Equalize # of Frauds and No Frauds\ndf_fraud = df[df['Class'] == 1]\ndf_no_fraud = df[df['Class'] == 0][:class_vals[1]]\n\ndf_equalized = pd.concat([df_fraud, df_no_fraud])\n\n# Shuffle again\ndf_equalized = df_equalized.sample(frac = 1, random_state = 1)\n\n# Acknowledge\ndf_equalized.describe()\n\nvalueCounts = df_equalized['Class'].value_counts()\nprint('# of No Fraud data: ', round(valueCounts[0] \/ len(df_equalized) * 100, 2), '%')\nprint('# of No Fraud data: ', round(valueCounts[1] \/ len(df_equalized) * 100, 2), '%')\n\n# Plot this too\nsns.countplot('Class', data=df_equalized)\nplt.title('Class Distribution\\n[ 0 - Not Fraud | 1 - Fraud ]')\n","2429c9b0":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df_equalized.corr(), ax=ax)","acf8639f":"# Visualize data with just 3 features - V2, V4, V11\ndf_selected = df_equalized[['V2', 'V4', 'V11', 'Class']]\nprint(df_selected.columns)\n\nsns.pairplot(data=df_selected, hue='Class')","64faca9f":"# Trying 3D view\n# https:\/\/www.geeksforgeeks.org\/three-dimensional-plotting-in-python-using-matplotlib\/\nfig = plt.figure(figsize=(10,10))\nax = plt.axes(projection='3d')\nax.scatter(df_selected['V2'], df_selected['V4'], df_selected['V11'], c=df_selected['Class'])\nax.set_xlabel('V2')\nax.set_ylabel('V4')\nax.set_zlabel('V11')\nax.legend(['0', '1'])","ba80eaf3":"# Get the X and Y data\nX = df_equalized.drop('Class', axis=1)\nY = df_equalized['Class']\n\n# Split Train and Test data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\nx_train = x_train.values\nx_test = x_test.values\ny_train = y_train.values\ny_test = y_test.values","b93f5b9d":"# Simple Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodels = {\n    'LogRes' : LogisticRegression(),\n    'KNN' : KNeighborsClassifier(),\n    'SVM' : SVC(),\n    'DecTree' : DecisionTreeClassifier()\n}\n","693f449a":"from sklearn import metrics\n\n# Store the accuracy scores\nmodels_performance = []\n\n# SVM kernels\n# https:\/\/towardsdatascience.com\/a-guide-to-svm-parameter-tuning-8bfe6b8a452c\nsvm_kernel = {\n    0 : 'linear',\n    1 : 'rbf',\n    2 : 'poly'\n}\n\n# Check for different hyperparams for the models\nfor i in range(20):\n    # 1. Logistic Regression\n    model = LogisticRegression(C = (i + 1) \/ 10)\n    model.fit(x_train, y_train)\n    prediction = model.predict(x_test)\n    models_performance.append(['LogRes', (i + 1) \/ 10, metrics.accuracy_score(prediction, y_test)])\n    # 2. KNN\n    model = KNeighborsClassifier(n_neighbors = i + 1)\n    model.fit(x_train, y_train)\n    prediction = model.predict(x_test)\n    models_performance.append(['KNN', i + 1, metrics.accuracy_score(prediction, y_test)])\n    # 3. Decision Tree\n    model = DecisionTreeClassifier(max_depth=i+1)\n    model.fit(x_train, y_train)\n    prediction = model.predict(x_test)\n    models_performance.append(['DecTree', i + 1, metrics.accuracy_score(prediction, y_test)])\n    # 4. SVM\n    model = SVC(kernel=svm_kernel[i % 3])\n    model.fit(x_train, y_train)\n    prediction = model.predict(x_test)\n    models_performance.append(['SVM', i%3, metrics.accuracy_score(prediction, y_test)])\n\n# Convert the models_performance to a DataFrame\nmodels_performance = pd.DataFrame(models_performance, columns=['Model', 'Param', 'Accuracy'])\n\n# And verify\nmodels_performance.head()","0ce20bdd":"# Visualize the model performance\nsns.lineplot(data=models_performance, x='Param', y='Accuracy', hue='Model')","01ef5441":"# Using a Neural Network for the same\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\nn_inputs = x_train.shape[1]\n\nsimple_mlp_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\nsimple_mlp_model.summary()\nsimple_mlp_model.compile(Adam(lr=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","af0183f0":"history = simple_mlp_model.fit(x_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True)\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.xlabel('Epochs')\nplt.show()","1622303b":"prediction = model.predict(x_test)\nprint(prediction)\ni = 0\ntacc = 0\ntloss = 0\nfor true_label in y_test:\n    if true_label == prediction[i]:\n        tacc+=1\n    else:\n        tloss+=1\n    i+=1\n\nprint('total: ', len(y_test), ' acc: ', tacc\/len(y_test), ' loss: ', tloss\/len(y_test))\nprint(len(x_test))","f93da83f":"print(len(x_train))\nprint(len(x_test))","151a8b4c":"## Some Considerations\n1. We can easily see that due to extremely low Fraud cases in the dataset, if we directly apply an ML model, our model might predict `No Fraud` for almost all the cases, due to ***OVERFITTING***.\n2. So, we must perform some sort of data balancing\n    - Either remove the `No Fraud` cases, or,\n    - Add more `Fraud` cases","fc67a8c2":"## Conclusion\nWe can observe from the heatmap that features `V2`, `V4` and `V11` have better correlation with respect to the Class","973e133e":"## Conclusion\n1. V2, V4, V11 give high correlation (positive) with the Class field\n2. so, we can probably use these 3 fields in some use case","2af6b526":"Step 1 - Splitting data to Train and Test sets","91964ef2":"## References\n1. SK-Learn methods: https:\/\/scikit-learn.org\/stable\/modules\/classes.html\n2. Seaborn methods: https:\/\/seaborn.pydata.org\/api.html\n3. Correlation Heatmaps: https:\/\/towardsdatascience.com\/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n4. Histo plots: https:\/\/etav.github.io\/python\/count_basic_freq_plot.html\n5. KNN concept: https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n6. Good repo of methods: https:\/\/machinelearningmastery.com\/start-here\/#python","bcfd905b":"# APPROACH:\n## Reduce number of No Fraud cases (Random Under Sampling)","80fdd503":"Building another Heatmap of the new equalized dataframe","82580c33":"# MLDA_1 COURSE : FINAL REPORT\n\n|  |  |\n| --- | --- |\n| **NAME** | Sourabha Bharadwaj BM |\n| **ROLL NO** | 17CS01001 |\n| **COURSE** | BTech 4th Year |\n| **DATASET** | Credit Card Fraud Detection |","217deaaa":"As we can see, the `Fraud` and `No Fraud` cases are well distributed in the Train and Test sets, with maximum randomization too.\n\nNow, we can perform the Random Undersampling in order to equalize the number of `Fraud` and `No Fraud` cases","253164c0":"## Conclusion: \n1. The dataset has no missing values\n2. The dataset is **highly** skewed."}}