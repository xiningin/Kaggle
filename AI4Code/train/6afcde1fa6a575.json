{"cell_type":{"11171684":"code","af6d10e9":"code","9feac024":"code","c559aca4":"code","b7b51989":"code","0da8fb8e":"code","30fea509":"code","0876ea76":"code","6517e62c":"code","33a0af91":"code","212b101e":"code","0216d2b0":"code","9de3e77c":"code","8372da72":"code","c02c2c4f":"code","f88e4903":"code","534ff29a":"code","b3c43e98":"code","4fb26ad3":"code","96d152af":"code","f964b841":"code","a371512e":"code","daeaf09c":"code","057d39dc":"code","47c76be5":"code","a0efb3e0":"code","460a850e":"code","f2fdcd3e":"code","4bf51cc3":"code","bf88dab4":"code","9c3423e5":"markdown"},"source":{"11171684":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af6d10e9":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#sl.__version__\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","9feac024":"dados=pd.read_csv('\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv')","c559aca4":"dados.head(3)","b7b51989":"#id do carro nao tem relevancia\ndados = dados.drop('car_ID',axis=1)","0da8fb8e":"# converter risco_seguro pois \u00e9 uma vari\u00e1vel categoria\ndados[\"symboling\"] = dados[\"symboling\"].astype(str)","30fea509":"# separar Fabricante do nome do carro\ndados[\"fabricante\"] = dados['CarName'].str.split(' ', expand=True)[0]","0876ea76":"dados.groupby('fabricante').size()","6517e62c":"# corrigindo erros nos nomes dos fabricantes\ndados[\"fabricante\"] = dados[\"fabricante\"].replace({\"maxda\":\"mazda\",\n                               \"Nissan\":\"nissan\",\n                               \"porcshce\":\"porsche\",\n                               \"toyouta\":\"toyota\",\n                               \"vokswagen\":\"volkswagen\",\n                               \"vw\":\"volkswagen\"})","33a0af91":"# Valores unicos de Fabricantes\ndados[\"fabricante\"].unique()","212b101e":"# o nome do carro nao tem relevancia\ndados.drop(columns=\"CarName\", inplace=True)","0216d2b0":"# Dividindo carros em categoria popular e superior\ndados[\"categoria_preco\"] = dados[\"price\"].apply(lambda x: \"popular\" if x <= 18500 else \"superior\")","9de3e77c":"colunas_numerica = list(dados.select_dtypes(exclude=\"object\"))\ncolunas_categoricas = list(dados.select_dtypes(include=\"object\"))","8372da72":"# visualizar distribui\u00e7\u00e3o por Fabricante\nplt.figure(figsize=(15,6))\ndados[\"fabricante\"].value_counts().sort_values(ascending=False).plot.bar()\nplt.xticks(rotation=90)\nplt.xlabel(\"Fabricante\", fontweight=\"bold\")\nplt.ylabel(\"Qtde\", fontweight=\"bold\")\nplt.title(\"Quantidade de carros por Fabricante\", fontweight=\"bold\")\nplt.show()","c02c2c4f":"# Observando variaveis categoricas\nplt.figure(figsize=(15,20))\nfor i,col in enumerate(colunas_categoricas[:-2], start=1):\n    plt.subplot(5,2,i)\n    sns.countplot(dados[col])\n    plt.xlabel(col, fontweight=\"bold\")\nplt.show()","f88e4903":"# mapa de calor para visualizar a correla\u00e7\u00e3o de pearson entre pre\u00e7o e outras vari\u00e1veis num\u00e9ricas\nplt.figure(figsize=(12,8))\nsns.heatmap(dados.corr(), annot=True, cmap=\"RdYlGn\", square=True, mask=np.triu(dados.corr(), k=1))\nplt.show()","534ff29a":"variables=dados.drop(columns=[\"price\"])","b3c43e98":"# converter vari\u00e1veis categoricas em num\u00e9ricas\nle = LabelEncoder()\ndf_encoded=dados\ndf_encoded[colunas_categoricas] = df_encoded[colunas_categoricas].apply(lambda col: le.fit_transform(col))\ndf_encoded.head()","4fb26ad3":"# Coletando x e y\nX = df_encoded.drop(columns=[\"price\"])\ny = df_encoded['price']","96d152af":"# Criando um Correlation Plot\ndef visualize_correlation_matrix(data, hurdle = 0.0):\n    R = np.corrcoef(data, rowvar = 0)\n    R[np.where(np.abs(R) < hurdle)] = 0.0\n    heatmap = plt.pcolor(R, cmap = mpl.cm.coolwarm, alpha = 0.8)\n    heatmap.axes.set_frame_on(False)\n    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor = False)\n    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor = False)\n    heatmap.axes.set_xticklabels(variables, minor = False)\n    plt.xticks(rotation=90)\n    heatmap.axes.set_yticklabels(variables, minor = False)\n    plt.tick_params(axis = 'both', which = 'both', bottom = 'off', top = 'off', left = 'off', right = 'off') \n    plt.colorbar()\n    plt.show()","f964b841":"# Visualizando o Plot\nplt.figure(figsize=(12,8))\nvisualize_correlation_matrix(X, hurdle = 0.5)","a371512e":"# Gerando os dados\nobservations = len(df_encoded)\nvariables = df_encoded.columns","daeaf09c":"# Aplicando Padroniza\u00e7\u00e3o\nstandardization = StandardScaler()\nXst = standardization.fit_transform(X)\noriginal_means = standardization.mean_\noriginanal_stds = standardization.scale_","057d39dc":"# Gerando X e Y\nXst = np.column_stack((Xst,np.ones(observations)))\ny  = df_encoded['price'].values","47c76be5":"from sklearn.linear_model import LinearRegression\nmodelo = LinearRegression(normalize = False, fit_intercept = True)","a0efb3e0":"def r2_est(X,y):\n    return r2_score(y, modelo.fit(X,y).predict(X))","460a850e":"# Gera o impacto de cada atributo no R2\nr2_impact = list()\nfor j in range(X.shape[1]):\n    selection = [i for i in range(X.shape[1]) if i!=j]\n    r2_impact.append(((r2_est(X,y) - r2_est(X.values[:,selection],y)), X.columns[j]))\n    \nfor imp, varname in sorted(r2_impact, reverse = True):\n    print ('%6.3f %s' %  (imp, varname))","f2fdcd3e":"XX=X[['fabricante','enginelocation','enginesize','stroke','carbody',\n      'horsepower','drivewheel','carlength','highwaympg','citympg']]","4bf51cc3":"X_train, X_test, Y_train, Y_test  = train_test_split(XX, y, test_size=0.2, random_state=42)","bf88dab4":"from sklearn.ensemble import RandomForestRegressor\n# Criando o modelo\nmodelo = RandomForestRegressor()\n\n# Treinando o modelo\nmodelo.fit(X_train, Y_train)\n\n# Fazendo previs\u00f5es\nY_pred = modelo.predict(X_test)\n\n# Resultado\nY_pred = modelo.predict(X_test)\nprint(\"R-squared:\", r2_score(Y_pred, Y_test))","9c3423e5":"**R-squared: 0.95**\n* Vari\u00e1veis relevantes: `fabricante`,`localiacao_motor`,`cilindrada`,`ciclo_pistao`,`carroceria`,`potencia_hp`,`tracao`,`comprimento`,\n`consumo_estrada_mpg`,`consumo_cidade_mpg`"}}