{"cell_type":{"d3b280df":"code","b6b84f23":"code","b7c000a6":"code","6f7b9740":"code","84888bba":"code","abf29694":"code","c1e81c82":"code","9c83b21d":"code","dfd0adb6":"code","81b8fbeb":"code","6e0d73fa":"code","7a9cde4f":"code","b0d2deb7":"code","8cb5e6a7":"code","c1af4082":"code","a84a3faf":"code","0d62eaa6":"code","ad8a3f47":"code","e9797405":"code","2fb0cdc4":"code","6555b070":"code","39b933ed":"code","89d303f9":"code","5ae095cf":"code","82d95696":"markdown","5c361f23":"markdown","92110afd":"markdown","dcf67f60":"markdown","464e3051":"markdown"},"source":{"d3b280df":"from tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing import image\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom tensorflow.python.client import device_lib\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec \nplt.style.use('dark_background')\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth=True\nsession = tf.compat.v1.InteractiveSession(config=config)\nfrom datetime import datetime","b6b84f23":"data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","b7c000a6":"data.head()","6f7b9740":"data.shape","84888bba":"lb = LabelBinarizer()\nlb.fit(data['label'])\ndata = data.join(pd.DataFrame(lb.transform(data['label']), columns=lb.classes_))\ndata.drop(columns=['label'], inplace=True)","abf29694":"data.shape","c1e81c82":"x_data = data.drop(columns=list(range(0,10))).values.reshape(-1,28,28)\/255.","9c83b21d":"y_data = data[list(range(0,10))]","dfd0adb6":"x_train, x_test, y_train, y_test =  train_test_split(x_data, y_data, test_size=0.1)","81b8fbeb":"plt.imshow(np.array(x_train[10]), cmap='gray')","6e0d73fa":"classifier = Sequential()\nimage_shape = (28, 28, 1)\nworkers = 10\nepochs = 10","7a9cde4f":"classifier = Sequential([\n    tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n    Conv2D(256, kernel_size=(3, 3), strides = 1, input_shape = image_shape, activation = 'relu', padding='same'),\n    MaxPooling2D(pool_size = (2, 2), strides = 1),\n    Dropout(0.05),\n    Conv2D(64, kernel_size=(3, 3), activation = 'relu', padding='same'),\n    MaxPooling2D(pool_size = (2, 2), strides=1),\n    Dropout(0.05),\n    Conv2D(16, kernel_size=(3, 3), activation = 'relu', padding='same'),\n    MaxPooling2D(pool_size = (2, 2), strides=1),\n    Dropout(0.05),\n    Conv2D(32, kernel_size=(3, 3), activation = 'relu', padding='same'),\n    AveragePooling2D(pool_size = (2, 2), strides=1),\n    Dropout(0.05),\n    Flatten(),\n    Dense(units = 10, activation = 'sigmoid')])\n","b0d2deb7":"classifier.compile(optimizer = 'adam', loss = categorical_crossentropy, metrics = ['accuracy'], )","8cb5e6a7":"print(classifier.summary())","c1af4082":"checkpoint = ModelCheckpoint(filepath='digits_best.hdf5', \n                             monitor='val_loss', \n                             verbose=1, \n                             save_best_only=True)","a84a3faf":"cnn = classifier.fit(x=x_train, y=y_train,\n                     validation_split=0.1,\n                     shuffle=True,\n                     epochs=epochs,\n                     use_multiprocessing=True,\n                     workers=workers, \n                     callbacks=[checkpoint])","0d62eaa6":"classifier.load_weights('digits_best.hdf5')\nclassifier.save('digits_best.h5')\ncnn_best = load_model('digits_best.h5')","ad8a3f47":"plt.figure(figsize=(15, 6))\nplt.subplot(1,2,1)\nplt.plot(cnn.epoch, cnn.history[\"loss\"], label=\"Train loss\")\nplt.plot(cnn.epoch, cnn.history[\"val_loss\"], label=\"Validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"yield\")\nplt.ylim(0,0.5)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(cnn.epoch, cnn.history[\"accuracy\"], label=\"Train accuracy\")\nplt.plot(cnn.epoch, cnn.history[\"val_accuracy\"], label=\"Validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"yield\")\nplt.legend()\n\nplt.show()","e9797405":"evaluation = cnn_best.evaluate(x=x_train, y=y_train,\n                               steps=len(x_train),\n                               verbose=1,\n                               workers=10)","2fb0cdc4":"pd.DataFrame(np.round(evaluation,2), cnn_best.metrics_names, columns=[\"model evaluation\"])","6555b070":"predictions = cnn_best.predict(x_test,\n                               steps=len(x_test),\n                               verbose=1, \n                               workers=10)","39b933ed":"y_test_true = lb.inverse_transform(np.asarray(y_test))\ny_test_pred = predictions.argmax(1)\nclasses = [str(i) for i in range(0,10)]\nprint(classification_report(y_test_true, \n                      y_test_pred, \n                      target_names = classes))","89d303f9":"cm_test = pd.DataFrame(confusion_matrix(y_test_true, y_test_pred), \n                       columns=classes, \n                       index=classes)\ncm_test","5ae095cf":"plt.figure(figsize=(12,9))\nsns.heatmap(cm_test, annot=True, fmt='d',\n            xticklabels=classes,\n            yticklabels=classes)\nplt.yticks(rotation=0)\nplt.ylabel('True')\nplt.xlabel('Prediction')\nplt.ylim(0, 10)","82d95696":"### Here is the main \"trick\" - reshape the columns to be a numpy ndarray, with the shape of (28,28) normalized to  be with values between (0,1)","5c361f23":"### Prepare the data set to be multi-class output (10 columns of 1 class each, just like a dummy feature)","92110afd":"### Below is another little trick, to choose the best point to save the classifier, by a chosen parameter (here it is minimum validation loss)","dcf67f60":"### Create a CNN model without going through creating the images data set\n\n- This notebook won't bring you the best score, few trials indicats +98%.\n- This notebook will save you disk and time, by avoiding going through the recreation of images from the tabular data set\n- Each epoch takes about 40-45 seconds on Kaggle GPU free account, on GTX1060 with CUDA, it takes 15 seconds per epoch","464e3051":"### Remeber, 2 boxes back, I saved the best classifier? Now i load it to use for the validation and the prediction"}}