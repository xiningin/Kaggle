{"cell_type":{"b01ff9f5":"code","55c27cd1":"code","23f5eb2a":"code","ffa9a688":"code","c7b3adcb":"code","fd1214ba":"code","38e43658":"code","f4e14d25":"code","1debaea3":"code","298b7b64":"code","eefd54e8":"code","7713435d":"code","c3408f2e":"code","31ed92ec":"code","1fd73768":"code","429a94cd":"code","530624eb":"code","95e507c4":"code","5ab5b24d":"code","5e027181":"code","8de41f3a":"code","7d15c339":"code","147dd748":"code","4febb9e6":"code","b88a9a85":"code","47239ab8":"code","53fbbe3f":"code","bba88e8a":"code","3ec808ac":"code","92d82f45":"markdown","400f6b4b":"markdown","0b763e2d":"markdown","64bbbc3a":"markdown","3cb01b90":"markdown","c38b263d":"markdown","8ccd9d5a":"markdown","71387ecf":"markdown","f95027a2":"markdown","c402f211":"markdown","7511a8b3":"markdown"},"source":{"b01ff9f5":"#Import Libraries \nfrom datetime import datetime\nimport numpy as np             #for numerical computations like log,exp,sqrt etc\nimport pandas as pd            #for reading & storing data, pre-processing\nimport matplotlib.pylab as plt #for visualization\n#for making sure matplotlib plots are generated in Jupyter notebook itself\n%matplotlib inline             \nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 10, 6","55c27cd1":"#Importing the dataset\ndataset = pd.read_csv(\"..\/input\/AirPassengers.csv\")\ndataset.head(5)","23f5eb2a":"#Parse strings to datetime type\ndataset['Month'] = pd.to_datetime(dataset['Month'],infer_datetime_format=True) #convert from string to datetime\nindexedDataset = dataset.set_index(['Month'])\nindexedDataset.head(5)","ffa9a688":"# plot graph\nplt.xlabel('Date')\nplt.ylabel('Number of air passengers')\nplt.plot(indexedDataset)","c7b3adcb":"#Determine rolling statistics\nrolmean = indexedDataset.rolling(window=12).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\nrolstd = indexedDataset.rolling(window=12).std()\nprint(rolmean,rolstd)","fd1214ba":"#Plot rolling statistics\norig = plt.plot(indexedDataset, color='blue', label='Original')\nmean = plt.plot(rolmean, color='red', label='Rolling Mean')\nstd = plt.plot(rolstd, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)","38e43658":"#Perform Augmented Dickey\u2013Fuller test:\nprint('Results of Dickey Fuller Test:')\ndftest = adfuller(indexedDataset['#Passengers'], autolag='AIC')\n\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)","f4e14d25":"#Log Scale Transformation \n#Estimating trend\nindexedDataset_logScale = np.log(indexedDataset)\nplt.plot(indexedDataset_logScale)","1debaea3":"#The below transformation is required to make series stationary\nmovingAverage = indexedDataset_logScale.rolling(window=12).mean()\nmovingSTD = indexedDataset_logScale.rolling(window=12).std()\nplt.plot(indexedDataset_logScale)\nplt.plot(movingAverage, color='red')","298b7b64":"datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)","eefd54e8":"#Remove NAN values\ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(12)","7713435d":"def test_stationarity(timeseries):\n    \n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #Plot rolling statistics\n    orig = plt.plot(timeseries, color='blue', label='Original')\n    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')\n    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey\u2013Fuller test:\n    print('Results of Dickey Fuller Test:')\n    dftest = adfuller(timeseries['#Passengers'], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n    \ntest_stationarity(datasetLogScaleMinusMovingAverage)","c3408f2e":"#Exponential Decay Transformation \nexponentialDecayWeightedAverage = indexedDataset_logScale.ewm(halflife=12, min_periods=0, adjust=True).mean()\nplt.plot(indexedDataset_logScale)\nplt.plot(exponentialDecayWeightedAverage, color='red')","31ed92ec":"datasetLogScaleMinusExponentialMovingAverage = indexedDataset_logScale - exponentialDecayWeightedAverage\ntest_stationarity(datasetLogScaleMinusExponentialMovingAverage)","1fd73768":"#Time Shift Transformation \ndatasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()\nplt.plot(datasetLogDiffShifting)","429a94cd":"datasetLogDiffShifting.dropna(inplace=True)\ntest_stationarity(datasetLogDiffShifting)","530624eb":"decomposition = seasonal_decompose(indexedDataset_logScale) \n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(indexedDataset_logScale, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(seasonal, label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\n\nplt.tight_layout()\n\n\n#There can be cases where an observation simply consisted of trend & seasonality. \n#In that case, there won't be \n#any residual component & that would be a null or NaN. Hence, we also remove such cases.\n\n\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","95e507c4":"decomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","5ab5b24d":"#Plotting ACF & PACF \n#ACF & PACF plots\n\nlag_acf = acf(datasetLogDiffShifting, nlags=20)\nlag_pacf = pacf(datasetLogDiffShifting, nlags=20, method='ols')\n\n#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()","5e027181":"#Building Models\n#AR Model\n#making order=(2,1,0) gives RSS=1.5023\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,0))\nresults_AR = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting AR model')","8de41f3a":"#MA Model\nmodel = ARIMA(indexedDataset_logScale, order=(0,1,2))\nresults_MA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting MA model')","7d15c339":"# AR+I+MA = ARIMA model\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting ARIMA model')","147dd748":"#Prediction & Reverse transformations \npredictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())","4febb9e6":"#Convert to cumulative sum\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum)","b88a9a85":"#log scale\npredictions_ARIMA_log = pd.Series(indexedDataset_logScale['#Passengers'].iloc[0], index=indexedDataset_logScale.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","47239ab8":"# Inverse of log is exp.\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(indexedDataset)\nplt.plot(predictions_ARIMA)","53fbbe3f":"indexedDataset_logScale","bba88e8a":"#We have 144(existing data of 12 yrs in months) data points. \n#And we want to forecast for additional 120 data points or 10 yrs.\nresults_ARIMA.plot_predict(1,264) \n#x=results_ARIMA.forecast(steps=120)    #Execute in Jupyter\/spyder\n#x\n","3ec808ac":"#print(x[1])\n#print(len(x[1]))\n#print(np.exp(x[1]))","92d82f45":"**For a Time series to be stationary, its ADCF test should have:**\n\n**1.p-value to be low (according to the null hypothesis)**\n\n**2.The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics**\n\n**From the above ADCF test result, we see that p-value(at max can be 1.0) is very large. Also critical values are no where close to the Test Statistics. Hence, we can safely say that our Time Series at the moment is not stationary**","400f6b4b":"**We observe that the Time Series is stationary & also the series for moving avg & std. dev. is almost parallel to x-axis thus they also have no trend.**\n\n**Also,**\n**1.p-value has decreased from 0.022 to 0.005.**\n**2.Test Statistic value is very much closer to the Critical values.**\n\n**Both the points say that our current transformation is better than the previous logarithmic transformation. Even though, we couldn't observe any differences by visually looking at the graphs, the tests confirmed decay to be much better.**\n\n**But lets try one more time & find if an even better solution exists. We will try out the simple time shift technique, which is simply:**\n\n**Given a set of observation on the time series:\nx0,x1,x2,x3,....xn**\n\n**The shifted values will be:**\n\n**null,x0,x1,x2,....xn <---- basically all xi's shifted by 1 pos to right**\n\n**Thus, the time series with time shifted values are:\nnull,(x1\u2212x0),(x2\u2212x1),(x3\u2212x2),(x4\u2212x3),....(xn\u2212xn\u22121)**","0b763e2d":"**We see that our predicted forecasts are very close to the real time series values indicating a fairly accurate model.**","64bbbc3a":"**From above 2 graphs, we can see that, visually this is the best result as our series along with rolling statistic values of moving avg & moving std. dev. is very much flat & stationary.**\n\n**But, the ADCF test shows us that:**\n\n**1.p-value of 0.07 is not as good as 0.005 of exponential decay.**\n\n**2.Test Statistic value not as close to the critical values as that for exponential decay.**\n\n**We have thus tried out 3 different transformation: log, exp decay & time shift. **\n\n**For simplicity, we will go with the log scale. The reason for doing this is that we can revert back to the original scale during forecasting.**\n\n**Let us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part**","3cb01b90":"**From the above graph, we see that rolling mean itself has a trend component even though rolling standard deviation is fairly constant with time. For our time series to be stationary, we need to ensure that both the rolling statistics ie: mean & std. dev. remain time invariant or constant with time. Thus the curves for both of them have to be parallel to the x-axis, which in our case is not so.**\n\n**To further augment our hypothesis that the time series is not stationary, let us perform the ADCF test.**","c38b263d":"**From above graph, we see that even though rolling mean is not stationary, it is still better than the previous case, where no transfromation were applied to series. So we can atleast say that we are heading in the right direction.**\n\n**We know from above graph that both the Time series with log scale as well as its moving average have a trend component. Thus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both.**\n\n**Its like:**\n\n**logscaleL=stationarypart(L1)+trend(LT) **\n\n**movingavgoflogscaleA=stationarypart(A1)+trend(AT)**\n\n**resultseriesR=L\u2212A=(L1+LT)\u2212(A1+AT)=(L1\u2212A1)+(LT\u2212AT)**\n\n**Since, L & A are series & it moving avg, their trend will be more or less same, Hence\nLT-AT nearly equals to 0**\n\n**Thus trend component will be almost removed and we have, R=L1\u2212A1 , our final non-trend curve**","8ccd9d5a":"**From the ACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2 From the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2**\n\n**ARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model**","71387ecf":"**Data Transformation to achieve Stationarity **\n\n**There are a couple of ways to achieve stationarity through data transformation like taking  log10 , loge , square, square root, cube, cube root, exponential decay, time shift and so on ...**\n\n**In our notebook, lets start of with log transformations. Our objective is to remove the trend component. Hence, flatter curves( ie: paralle to x-axis) for time series and rolling mean after taking log would say that our data transformation did a good job.**","f95027a2":"**From above graph, we observe that our intuition that \"subtracting two related series having similar trend components will make the result stationary\" is true.**\n\n**We find that:**\n\n**1.p-value has reduced from 0.99 to 0.022.**\n\n**2.The critical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic. Thus, from above 2 points, we can say that our given series is stationary.**\n\n**But, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.**\n\n**Let us try out Exponential decay.**","c402f211":"**From above graph, it seems that exponential decay is not holding any advantage over log scale as both the corresponding curves are similar. But, in statistics, inferences cannot be drawn simply by looking at the curves. Hence, we perform the ADCF test again on the decay series below.**","7511a8b3":"**By combining AR & MA into ARIMA, we see that RSS value has decreased from either case to 1.0292, indicating ARIMA to be better than its individual component models.**\n\n**With the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data.**"}}