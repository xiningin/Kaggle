{"cell_type":{"5e4cd8e4":"code","37cc1fab":"code","9b6a00a1":"code","20b7fca5":"code","4c777782":"code","89280beb":"code","f8104bb4":"code","f97e1fa0":"code","69fc8d6f":"code","6e2fb6d5":"code","4a9b51fc":"code","f4fe0c5f":"code","6e8df379":"code","60da4071":"code","4fd0db69":"code","c6c1907d":"code","10039be0":"code","7169d6db":"code","4a3e2de0":"code","4cbb454f":"code","f16534a5":"code","bda79c49":"code","65734b63":"code","533304f1":"code","1daa4a1c":"code","dc28e4f2":"code","9619758d":"code","073616dc":"code","bb0b4d8b":"markdown","73d87540":"markdown","ff7d7079":"markdown","8b980e72":"markdown","40aa9792":"markdown","e392fcfe":"markdown","36c3717a":"markdown","ef8afa2a":"markdown","c169875d":"markdown","592e2fe7":"markdown","2a752afa":"markdown","202b7190":"markdown","d9c5f1b5":"markdown","d0cd2a1f":"markdown","596ab2fd":"markdown","f8adcbf4":"markdown","a9c479e8":"markdown","b176f9ad":"markdown","b68f735b":"markdown","1ac5608f":"markdown","666f1e37":"markdown","fd0ec498":"markdown","c2a55221":"markdown","09e5e29f":"markdown","5be5557b":"markdown","be822bc9":"markdown","e7f4b655":"markdown","561d4263":"markdown","d35405d0":"markdown","27765b75":"markdown"},"source":{"5e4cd8e4":"from IPython.display import Image\nfrom datetime import datetime\n\n# runtime\nimport timeit\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n%matplotlib inline\n\nnp.warnings.filterwarnings('ignore')","37cc1fab":"bank = pd.read_csv('..\/input\/bank-marketing-campaigns-dataset\/bank-additional-full.csv', sep=\";\")\ndisplay(bank.head(3))\nprint(bank.info())\nprint(bank.describe())\nprint(bank.shape)\n","9b6a00a1":"sns.heatmap(bank.isnull())\nplt.show()","20b7fca5":"bank['month'] = pd.to_datetime(bank['month'], format='%b').dt.month\nbank['month'].value_counts().index","4c777782":"print(bank['day_of_week'].value_counts().index)","89280beb":"d = {'thu':4, 'mon':1, 'wed':3, 'tue':2, 'fri':5}\nbank['day_of_week'] = bank['day_of_week'].map(d)\nbank['day_of_week'].value_counts().index","f8104bb4":"def pcontacted(x):\n    if x == 999:\n        return 'no'\n    else:\n        return 'yes'\n\nbank['pdays'] = bank['pdays'].apply(pcontacted)\nbank.rename(columns={\"pdays\": \"bcontacted\"}, inplace=True)\nbank['bcontacted'].value_counts()","f97e1fa0":"# Drops nulls\ndef drop(column):\n\n    bank[column].replace('unknown', np.nan, inplace=True)\n    bank.dropna(inplace=True)\n\ndrop('job')\ndrop('marital')\ndrop('education')\ndrop('housing')\n\n","69fc8d6f":"sns.countplot(data=bank, x='y',palette='GnBu')\nplt.show()","6e2fb6d5":"cat = bank.select_dtypes('object').columns.to_list()\ncat = cat[:-1]\ncat","4a9b51fc":"fig, ax = plt.subplots(3,4, figsize=(20,17))\n\ncat = bank.select_dtypes('object').columns.to_list()\ncat = cat[:-1]\n\nax = ax.ravel()\nposition = 0\n\nfor i in cat:\n    \n    order = bank[i].value_counts().index\n    sns.countplot(data=bank, x=i, ax=ax[position], hue='y', palette='GnBu', order=order)\n    ax[position].tick_params(labelrotation=90)\n    ax[position].set_title(i, fontdict={'fontsize':17})\n    \n    position += 1\n\nplt.subplots_adjust(hspace=0.7)\n\nplt.show()","f4fe0c5f":"bank['month'] = bank['month'].astype('str')\nbank['day_of_week'] = bank['day_of_week'].astype('str')\n\nnumbers = bank.select_dtypes(['int64', 'float64']).columns.to_list()\nlen(numbers)","6e8df379":"bank.hist(figsize=(20,10), edgecolor='white', color='#00afb9')\nplt.show()\n\ndisplay(bank[numbers].describe())","60da4071":"TotalLog = np.log(bank['campaign'] + 1)\nTotalLog.hist(edgecolor='white', color='#00afb9')\n\nplt.show()","4fd0db69":"bank.drop(['duration'], axis=1, inplace=True)\nnumbers = bank.select_dtypes(['int64', 'float64']).columns.to_list()","c6c1907d":"fig, ax = plt.subplots(figsize=(14,9))\nsns.heatmap(bank[numbers].corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True, linewidths=.5)\nplt.show()","10039be0":"numbers","7169d6db":"fig, ax = plt.subplots(7,4, figsize=(20,27))\nax = ax.ravel()\nposition = 0\n\na = 0\nb = 1\n\n\nwhile a < len(numbers):\n    try:\n        sns.scatterplot(x=numbers[a], y=numbers[b], data=bank, hue='y', ax=ax[position])\n        position += 1\n        if b < len(numbers) - 1:\n            b += 1\n        else:\n            a += 1\n            b = a + 1\n    except:\n        break\n\nplt.subplots_adjust(hspace=0.5)\nplt.show()","4a3e2de0":"bank['month'] = bank['month'].astype('int')\nbank['day_of_week'] = bank['day_of_week'].astype('int')","4cbb454f":"#bank['month']\nfig, ax = plt.subplots(1,2, figsize=(15,5))\n\norder = bank['month'].value_counts().sort_index().index\nsns.countplot(bank['month'], palette='Set3', ax=ax[0])\nsns.countplot(bank['month'], data=bank, palette='Set3', ax=ax[1], hue='y')\nplt.show()\n\ndf = pd.DataFrame(bank['month'].value_counts().sort_index())\ndf['%'] = np.around(df['month']\/df['month'].sum() * 100, 2)\ndf['Yes'] = bank.groupby(['y','month']).size()['yes'].values\ndf['%Yes'] = np.around(df['Yes']\/df['month']*100,2)\ndisplay(df.T)\n\n","f16534a5":"bank_features = bank.iloc[:,:-1]\nbank_features = pd.get_dummies(bank_features)\ndisplay(bank_features.head(5))","bda79c49":"X = bank_features.values\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","65734b63":"# determining optimal number of features\nn_features = [5, 10, 15, 20, 25, 30, 35, 40, 45]\n\nstart = timeit.default_timer()\n\nfor i in n_features:\n    # Building the model based feature selection\n    select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=i)\n\n    select.fit(X_train, y_train)\n\n    mask = select.get_support()\n\n    X_train_rfe = select.transform(X_train)\n    X_test_rfe = select.transform(X_test)\n\n    train = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n    score = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_train_rfe, y_train)\n    \n    print(\"Train score: {:.3f}\".format(train), \"Test score: {:.3f}\".format(score), \" number of features: {}\".format(i))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)  ","533304f1":"select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=20)\n\nselect.fit(X_train, y_train)\n\nmask = select.get_support()\n\nX_train_rfe = select.transform(X_train)\nX_test_rfe = select.transform(X_test)\n\nscore = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n\nprint(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(20))\n\nfeatures = pd.DataFrame({'features':list(bank_features.keys()), 'select':list(mask)})\nfeatures = list(features[features['select']==True]['features'])\nfeatures","1daa4a1c":"X = bank_features[features].values\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nbest_n = 0\nbest_training = 0\nbest_test = 0\n\nfor i in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    \n    training = knn.score(X_train, y_train)\n    test = knn.score(X_test, y_test)\n    \n    if test > best_test:\n        best_n = i\n        best_training = training\n        best_test = test\n\nprint(\"best number of neighbors: {}\".format(best_n))\nprint(\"best training set score : {:.3f}\".format(best_training))\nprint(\"best test set score: {:.3f}\".format(best_test))","dc28e4f2":"start = timeit.default_timer()\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(knn.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)","9619758d":"X = bank_features[features].values\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nstart = timeit.default_timer()\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\n\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) ","073616dc":"X = bank_features\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nstart = timeit.default_timer()\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\n\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) ","bb0b4d8b":"### 2.2.1 Distribution","73d87540":"## 3.1. Dummies","ff7d7079":"- I was hoping to find a trend in the age group since that would have helped us create a customer profile but that won't be the case since the graphs don't show any evident trend\n- 'campaign's 'yes' are located in  lower values, that means that calling the customer more won't lead to a yes (during the same campaign). But, according to feature 'previous', the customer tend to say 'yes' if he was more than 4 time previously contacted (during different campaigns). This puts in evidence the importance of customer relationship building\n- when 'emp.var.rate' is negative, customers tend to say yes. This is a variable outside the control of the bank, but it could be telling us something since it reveals a trend. It should be included in the model\n- When 'nr.employed' is less tha 5100, customers tend to say 'yes'\n- The remaining relationships don't show any evident trend","8b980e72":"* Most of the clients were contacted during summer (May-Jul)\n* November and October was pretty still until november when it peaked again before Chirstmas and January-February, which don't have any observations. ","40aa9792":"# Introduction\n\nThe dataset is about a bank's marketing campaign that offers clients a term deposit in the bank. The company approched it's clients mostly by telephon and a target variable was assigned according to the client's answer: 'yes' if they want to make the investment and 'no' otherwise.\n\nA term deposit is a fixed-term investment that includes the deposit of money into an account at a financial institution. Term deposit investments usually carry short-term maturities ranging from one month to a few years and will have varying levels of required minimum deposits [https:\/\/www.investopedia.com\/terms\/t\/termdeposit.asp](http:\/\/).\n\n## Table of Contents\n\n1. [Data Loading & Data Cleaning](#1.-Data-Loading-&-Data-Cleaning)\n2. [Descriptive Analysis and EDA](#2.-Descriptive-Analysis-and-EDA)\n3. [Feature Engineering](#3.-Feature-Engineering)\n4. [Classification model](#4.-Classification-model)\n6. [Part 2 next week](#Part-2-next-week)\n\n\n## Input variables:\n### bank client data:\n1. 1 - age (numeric)\n1. 2 - job : type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown')\n1. 3 - marital : marital status (categorical: 'divorced', 'married', 'single', 'unknown'; note: 'divorced' means divorced or widowed)\n1. 4 - education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n1. 5 - default: has credit in default? (categorical: 'no', 'yes', 'unknown')\n1. 6 - housing: has housing loan? (categorical: 'no', 'yes', 'unknown')\n1. 7 - loan: has personal loan? (categorical: 'no', 'yes', 'unknown')\n\n### related with the last contact of the current campaign:\n1. 8 - contact: contact communication type (categorical: 'cellular', 'telephone')\n1. 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n1. 10 - day_of_week: last contact day of the week (categorical: 'mon', 'tue', 'wed', 'thu', 'fri')\n1. 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n### other attributes:\n1. 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n1. 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n1. 14 - previous: number of contacts performed before this campaign and for this client (numeric)\n1. 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure', 'nonexistent', 'success')\n\n### social and economic context attributes\n1. 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n1. 17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n1. 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n1. 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n1. 20 - nr.employed: number of employees - quarterly indicator (numeric)\n\n### Output variable (desired target):\n1. 21 - y - has the client subscribed a term deposit? (binary: 'yes', 'no')","e392fcfe":"## Classification model Conclusions\n- We were able to build a simple Knn modelwith a set score of 0.891. Fairly good but the gradient models were more effective\n- We used the reduced dataset and the complete dataset to build the model, obtaining the same results (test score of 0.90). the reduced dataset, thought, run a bit faster (1.72 vs 2.07)","36c3717a":"## 3.2 Model Based Feature Selection\nModel based feature selection uses a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones. For this case, we are going to use a random forest classifier, since it usually yields good results and because this is a classification task","ef8afa2a":"## Descriptive Analysis and EDA Conclusions\n- Overall, most of the features don't show an evident trend towards 'yes'. The found trends were based mainly on absolute values and not on proportion. The fact that we have imbalanced data doesn't help either.\n- Most of the categorical features are distributed around 90% 'no' and 10% 'yes'. \n- **'poutcome'** and **'bcontacted'** are the only features where 'yes' is bigger than 'no' in one of their values. This tells us something important as both are based on previous results and previous contacts. Seems like customer relation plays an important role\n- Speaking about numbers, **'campaign'** shows the most evident trend, where 'yes' tends to be located in smaller values.\n- Surprisingly, one of the external variables, **'emp.var.rate'** shows a trend where when it's negative, customers tend to say 'yes'\n\n","c169875d":"## 2.1. Categories\nLet's take a look at the categories and how the 'y' is distributed among them.\n\nFrom the first round, i notice that some features instead of having \"null\" values, they have \"unknow\" values, so i'm going to drop them before visualizing the findings","592e2fe7":"# 2. Descriptive Analysis and EDA\n","2a752afa":"Changing the names of the month and day of the week will alow us to include this data in our machine learning model","202b7190":"# 1. Data Loading & Data Cleaning","d9c5f1b5":"- We have a clear imbalanced data problem: the target feature 'y' is dominated by 'no's. When building the classification algorithm we will have to apply an oversampling method to avoid the model predicting based on size.\n- All of the features follow the same trend, there is a group that is mostly targeted and then it descends. The proportion of 'yes' and 'no', follow the same pattern as it descends, (around 10% of the sample)\n- The value, where 'yes' overcome 'no' is located in the feature  'poutcoume', where if there is a success in previous outcomes, the client will say 'yes' to the marketing campaign. The sample, though is extremely low.\n- 'yes' in 'bcontacted' is also bigger in this feature, but the sample is also quite small","d0cd2a1f":"Thankfully, there aren't any null values in the dataset.\n\nLet's change the datatypes","596ab2fd":"## Feature Engineering Conclusions\n\n- As found in the EDA section, the model selected features based on majority. Features like:  'job_admin.', 'job_technician', 'marital_married', 'marital_single', 'education_high.school', 'education_university.degree'. This is why an oversampling technique is needed\n- We also were right about 'bcontacted', 'campaign', and 'emp.var.rate'. These features, mentioned on the EDA section ended to be important in the feature selection\n- 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed' were also selected. Seems like the economic situation also plays a role in the model","f8adcbf4":"# 4. Classification model\n* in this section we are gong to build a simple model (knn) with the reduced dataset, then a more complex one with the reduced data (Gradient Boosting classifier), and finally another one with the complete dataset and a Gradient Boosting classifier","a9c479e8":"# 3. Feature Engineering\nThis section will focu on converting the categorical variables itno dummy variables to fit it into the model.\n\nWe are also going to be ending with lot's of features, so we are going to apply a reature reduction technique to properly analyze the model\n\nWe are also goinf to apply a log formula to 'campaign' to see if we can squeez some additional score values.","b176f9ad":"* Except from 'previous', the variables that are under the control of the bank ('age' and 'campaign') don't show any high correlation. Thankfully, we don't have any continous target variable that we can apply a regression model to.\n* high correlation pairs are not relevant since they are not target variables.\n\nlet's take a look at the combinations of the variables with scatter plots and color them with the 'y' column to see if we can find any pattern.\n\n'duration' column is going to be discarted in this part since all '0' values are a 'no' and that include unanswered calls. ","b68f735b":"Couple of things could be said about these distributions.\n\n* **age.** The targetted age tend to be between 32 and 47 (interquantile range) which is the most productive age. It also has the most 'normal' distribution, this will be key for our machine learning algorithm\n* **duration.** Duration's mean (258.20) drastically differs from the median (180). This is because some high duration calls (outliers) that push the distribution to the right\n* **campaign.** clients tend to be contacted two times (median = 2) during the campaign\n* **previous.** most of the values are 0. Similarly to the 'pdays' attribute, this means that most of the clients haven't been contated before\n* **emp.var.rate** values tend to be between -1.8 and 1.4\n* **cons.price.idx.** and **cons.conf.idx** don't vary too much\n* **euribor3m.** The clients were contacted usually when the Euribor3 rate was between 1.3 and 4.96 \n* social and economic context attributes: 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m' and 'nr.employed' don't show any apparent distribution, my intuition tells me that they won't be that useful for the model, taking also into account that they aren\u00c4t under the control of the bank, but lets leave them there until we finish exploring.\n\nMoreover, we can see that 'duration' and 'campaign' distributions have most the values located in the first bin and then it descends. To make this distributions a bit more useful for our machine learning model, we could transform them with the a log formula to make them more 'normal'. An example below","1ac5608f":"## 4.3 Gradient Boosting (Complete Dataset)","666f1e37":"As mentioned in the introduction, if we want realistic results, we will have to drop 'duration'. Lets do that now.","fd0ec498":"### 2.2.2 correlations","c2a55221":"## 2.2 Numbers","09e5e29f":"'pdays' has value 999 if the client was not previously contacted, this variable will make our analysis harder to interpret. For this reason, we are going to change this feature to a categorical one, if the value is 999, then it will be replaced with a 'no' which means that the client was not previously contacted, else, it will have a 'yes'","5be5557b":"# Part 2 next week\nWhat is missing:\n- feature engineering: we found that some values could be improved with log formulas\n- imbalanced data: we should apply an oversampling method to get a more realistic result\n- Visualize results\n- building a ML application. So... will your customer accept your offer?","be822bc9":"With a training score of 0.886 and a test score of 0.991, 20 features is a good enough choiche for me.\nLet's check what features the model selects","e7f4b655":"## 2.3 Dates","561d4263":"All the datatypes are correct. We can convert the dates columns (\"month\", \"day_of_week\") for further analysis\n\nLet's begin by taking a look at the nulls. Then, we are going to change the data type of the month and day_of_week columns.\n\nlet's see where the nulls are located.","d35405d0":"## 4.2 Gradient Boosting (Reduced Dataset)","27765b75":"## 4.1. Knn"}}