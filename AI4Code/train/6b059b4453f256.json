{"cell_type":{"d6f2c780":"code","e1b3fb6e":"code","06fa360a":"code","afa4a4cd":"code","5b08d0db":"code","596d8de6":"code","b17f686f":"code","a2c86096":"code","d9b5450c":"code","4de1f577":"code","5a184331":"code","0bdb1a2d":"code","3900567d":"code","10d2e1c1":"code","ac62066b":"code","7d8feb9f":"code","e6848413":"code","8d7dd65f":"code","8913a047":"code","bae080bd":"code","c8dd843d":"code","44a980b3":"code","40589b78":"code","784f9588":"code","8f498b8a":"code","2c97b692":"code","f504ecef":"code","09ac9b54":"code","6ca17dfb":"code","f28af176":"code","a1d4e380":"code","198b55c8":"code","8fea74e1":"code","266bac82":"code","e62e79cd":"code","64cfe535":"markdown","0b1b3ca5":"markdown","48ea8fbc":"markdown","5c74e8ba":"markdown","7c9c1b01":"markdown","9a06b3fc":"markdown","c95d49c5":"markdown","778b3957":"markdown","1b11965b":"markdown","a8b4575d":"markdown","e8bba031":"markdown","2101b483":"markdown","b2dc93a6":"markdown","53aff378":"markdown","0fe999c9":"markdown","4240f2ed":"markdown","10883ca3":"markdown","acff55f4":"markdown","d8dd0001":"markdown","7cc53b98":"markdown","06ff9caf":"markdown","b365df04":"markdown","02a4d192":"markdown","e9fb4a3e":"markdown","049c7c43":"markdown","3cd77b93":"markdown","e6ad54a4":"markdown","2d874d9a":"markdown","fe9f58b6":"markdown","32baebfa":"markdown","3751881b":"markdown"},"source":{"d6f2c780":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1b3fb6e":"import numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","06fa360a":"df = pd.read_csv('\/kaggle\/input\/wine-quality\/winequalityN.csv')","afa4a4cd":"df.sample(5)","5b08d0db":"df.isna().sum()","596d8de6":"#Replacing null values in fixed acidity with median\ndf['fixed acidity'].fillna(df['fixed acidity'].median(), inplace=True)\ndf['fixed acidity'].isna().sum()","b17f686f":"#Replacing null values in volatile acidity with mean\ndf['volatile acidity'].fillna(df['volatile acidity'].mean(), inplace=True)\ndf['volatile acidity'].isna().sum()","a2c86096":"#Replacing null values in citric acid with mean\ndf['citric acid'].fillna(df['citric acid'].mean(), inplace=True)\ndf['citric acid'].isna().sum()","d9b5450c":"#Replacing null values in residual sugar with mean\ndf['residual sugar'].fillna(df['residual sugar'].mean(), inplace=True)\ndf['residual sugar'].isna().sum()","4de1f577":"#Replacing null values in chlorides with median\ndf['chlorides'].fillna(df['chlorides'].median(), inplace=True)\ndf['chlorides'].isna().sum()","5a184331":"#Replacing null values in pH with mean\ndf['pH'].fillna(df['pH'].mean(), inplace=True)\ndf['pH'].isna().sum()","0bdb1a2d":"#Replacing null values in sulphates with median\ndf['sulphates'].fillna(df['sulphates'].median(), inplace=True)\ndf['sulphates'].isna().sum()","3900567d":"df.isna().sum()","10d2e1c1":"df['quality'].min()\ndf['quality'].value_counts()","ac62066b":"#Mapping values of target variable quality to 'low', 'medium' and 'high' categories for classification\ndf['quality']=df['quality'].map({3:'low', 4:'low', 5:'medium', 6:'medium', 7:'medium', 8:'high', 9:'high'})","7d8feb9f":"df['quality']=df['quality'].map({'low':0,'medium':1,'high':2})","e6848413":"df.sample(5)","8d7dd65f":"sn.set()\nplt.figure(figsize=(30,15))\nsn.boxplot(data=df)\nplt.show()","8913a047":"fig, ax =plt.subplots(1,3)\nplt.subplots_adjust(right=2.5, top=1.5)\nsn.boxplot(df['residual sugar'], df['type'], ax=ax[0])\nsn.boxplot(df['free sulfur dioxide'], df['type'], ax=ax[1])\nsn.boxplot(df['total sulfur dioxide'], df['type'], ax=ax[2])\nplt.show()","bae080bd":"#Removing outliers in residual sugar\nlower = df['residual sugar'].mean()-3*df['residual sugar'].std()\nupper = df['residual sugar'].mean()+3*df['residual sugar'].std()\ndf = df[(df['residual sugar']>lower) & (df['residual sugar']<upper)]\n\n#Removing outliers in free sulfur dioxide\nlower = df['free sulfur dioxide'].mean()-3*df['free sulfur dioxide'].std()\nupper = df['free sulfur dioxide'].mean()+3*df['free sulfur dioxide'].std()\ndf = df[(df['free sulfur dioxide']>lower) & (df['free sulfur dioxide']<upper)]\n\n#Removing outliers in total sulfur dioxide\nlower = df['total sulfur dioxide'].mean()-3*df['total sulfur dioxide'].std()\nupper = df['total sulfur dioxide'].mean()+3*df['total sulfur dioxide'].std()\ndf = df[(df['total sulfur dioxide']>lower) & (df['total sulfur dioxide']<upper)]","c8dd843d":"dummies = pd.get_dummies(df['type'], drop_first=True)\ndf = pd.concat([df, dummies], axis=1)\ndf.drop('type', axis=1, inplace=True)","44a980b3":"#Checking relationship between features\ncor=df.corr()\nplt.figure(figsize=(20,10))\nsn.heatmap(cor,xticklabels=cor.columns,yticklabels=cor.columns,annot=True)\ncor","40589b78":"X = df.loc[:,df.columns!='quality']\ny = df['quality']\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20, random_state=0)","784f9588":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nprint(rfc.get_params())","8f498b8a":"# Fit the model\nrfc.fit(X_train,y_train)","2c97b692":"y_pred=rfc.predict(X_test)\naccuracy_score(y_test,y_pred)","f504ecef":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start=90, stop=200, num=12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start=10, stop=110, num=11)]\nmax_depth.append(None)\nmin_samples_split=[2, 5, 10]\nmin_samples_leaf=[1, 2, 4]\nbootstrap=[True, False]","09ac9b54":"random_search_grid = {'n_estimators': n_estimators,\n                      'max_features': max_features,\n                      'max_depth': max_depth,\n                      'min_samples_split': min_samples_split,\n                      'min_samples_leaf': min_samples_leaf,\n                      'bootstrap': bootstrap}\nprint(random_search_grid)","6ca17dfb":"rfc=RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator=rfc, param_distributions = random_search_grid, n_iter=100, \n                          cv=3, verbose=2, random_state=0, n_jobs=-1)","f28af176":"rf_random.fit(X_train, y_train)","a1d4e380":"rf_random.best_params_","198b55c8":"rfc = RandomForestClassifier(n_estimators=90, min_samples_split=2, min_samples_leaf=1, \n                             max_features='auto', max_depth=50, bootstrap=True,\n                             random_state = 42)","8fea74e1":"rfc.fit(X_train,y_train)","266bac82":"y_pred=rfc.predict(X_test)\naccuracy_score(y_test,y_pred)","e62e79cd":"print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint('\\nClassification Report:\\n', classification_report(y_test, y_pred))","64cfe535":"There are null values present in the following columns:\n'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'pH', 'sulphates'","0b1b3ca5":"I will now use RandomizedSearchCV for searching over and performing 3-fold cross validation on the grid of parameters that can be used for the Random Forest model for this dataset.","48ea8fbc":"## Train-Test split\n\nI will be splitting the dataset into training and testing sets in the ratio of 0.80:0.20","5c74e8ba":"We can now get the best set of parameters from the function the evaluated grid.","7c9c1b01":"Let's fit the model once again with the updated parameters.","9a06b3fc":"#### 94.71%","c95d49c5":"## Correlation between features","778b3957":"## Importing necessary packages","1b11965b":"<b>There seems to be a slight improvement. Nevertheless, we get around 95% accuracy, which is awesome!<\/b>\n\nNow I will be showing the confusion matrix and the classification report.","a8b4575d":"Now, since we're predicting the target variable quality, we'll have to categorize the numbers into low, medium and high and then encode it to 0,1 and 2 for classification","e8bba031":"We can see the default parameters used by the classifier","2101b483":"## Reading the data","b2dc93a6":"I will now be plotting a boxplot to view the general distribution of data across all features to check for outliers.","53aff378":"I will be fitting the sklearn's <b>RandomForestClassifier<\/b> model.","0fe999c9":"Before I begin, my work here is an improvement to an existing approach. So, credits to https:\/\/www.kaggle.com\/taha07\/wine-quality-prediction-data-analysis for his accuracy of 93%","4240f2ed":"### 1-Hot encoding","10883ca3":"For each column, instead of dropping rows with null values, I will instead be replacing them with either median or mean. Simply dropping the rows will considerably reduce the size of the dataset and hence might degrade performance of the models","acff55f4":"## I hope you found this useful :)","d8dd0001":"### Removal of Outliers","7cc53b98":"### Acknowledgement","06ff9caf":"In these three columns we can notice significant outliers. Therefore, they must be removed from the respective columns.","b365df04":"#### I will be using Random Forest Classifier to predict the quality of wine, which is the target variable","02a4d192":"Outliers are extreme cases of data that may severely affect the prediction capailities of the machine learning models. Therefore, its critical that we remove them.","e9fb4a3e":"We have achieved an accuracy score of 0.946 (94.6%). That's great! But the performance can further be enhanced by tuning the parameters.","049c7c43":"Now we check for total no. of null values in each column","3cd77b93":"## Hyperparameter Tuning","e6ad54a4":"The 'type' column must be 1-hot encoded for classification. 1-hot encoding creates a binary column for each category. Here we use pd.get_dummies() to remove the first category and essentially bring it to one column of 1's and 0's where 1 denotes white wine and 0 denotes not white (red wine).","2d874d9a":"#### Now I will be taking you through the steps","fe9f58b6":"## Model Fitting","32baebfa":"## Data Cleaning and Preprocessing\n\nBefore performing any analysis on data, it's important to deal with null values as they are prone to major errors and inconsistencies","3751881b":"No more null values."}}