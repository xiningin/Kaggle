{"cell_type":{"ca9907f4":"code","dfd09bad":"code","f67b738d":"code","98a8ccb3":"code","f94ed3bc":"code","362c87c2":"code","d2f8bb94":"code","8050c5f9":"code","33248ab8":"code","fee24dfc":"code","208dada9":"code","a23e982b":"code","fbfe9f96":"code","2f07fbb5":"code","acc9a629":"code","2ca24440":"code","00ce2d73":"code","29abb52b":"code","09553d0a":"code","522076dd":"markdown","54846fec":"markdown"},"source":{"ca9907f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dfd09bad":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_palette('husl')\nimport matplotlib.pyplot as plt\n%matplotlib inline","f67b738d":"#Load Data\niris = pd.read_csv('..\/input\/iris\/Iris.csv')\niris.head()","98a8ccb3":"iris.info()","f94ed3bc":"iris['Species'].value_counts()","362c87c2":"iris1 = iris.drop('Id', axis=1)\ng = sns.pairplot(iris1, hue='Species', markers='+')\nplt.show()","d2f8bb94":"# sepal length vs sepal width.\nfig = iris1[iris1.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\niris1[iris1.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\niris1[iris1.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.show()","8050c5f9":"# petal length vs petal width.\nfig = iris1[iris1.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='Setosa')\niris1[iris1.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\niris1[iris1.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.show()","33248ab8":"g = sns.violinplot(y='Species', x='SepalLengthCm', data=iris1, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='SepalWidthCm', data=iris1, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalLengthCm', data=iris1, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalWidthCm', data=iris1, inner='quartile')\nplt.show()","fee24dfc":"plt.figure(figsize=(10,8)) \nsns.heatmap(iris1.corr(),annot=True,cmap='cubehelix_r')\nplt.show()","208dada9":"X = iris.drop(['Id', 'Species'], axis=1)\ny = iris['Species']\n# print(X.head())\nprint(X.shape)\n# print(y.head())\nprint(y.shape)","a23e982b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","fbfe9f96":"from sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import metrics #for checking the model accuracy","2f07fbb5":"logr = LogisticRegression()\nlogr.fit(X_train,y_train)\ny_pred = logr.predict(X_test)\nacc_log = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the Logistic Regression is', acc_log)","acc9a629":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\nacc_dt = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the Decision Tree is', acc_dt)","2ca24440":"sv = svm.SVC() #select the algorithm\nsv.fit(X_train,y_train) # we train the algorithm with the training data and the training output\ny_pred = sv.predict(X_test) #now we pass the testing data to the trained algorithm\nacc_svm = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the SVM is:', acc_svm)","00ce2d73":"knc = KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nknc.fit(X_train,y_train)\ny_pred = knc.predict(X_test)\nacc_knn = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the KNN is', acc_knn)","29abb52b":"a_index = list(range(1,11))\na = pd.Series()\nx = [1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    kcs = KNeighborsClassifier(n_neighbors=i) \n    kcs.fit(X_train,y_train)\n    y_pred = kcs.predict(X_test)\n    a=a.append(pd.Series(metrics.accuracy_score(y_pred,y_test)))\nplt.plot(a_index, a)\nplt.xticks(x)","09553d0a":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Support Vector Machines',\n              'K-Nearest Neighbours'],\n    'Score': [acc_log, acc_dt, acc_svm, acc_knn]})\nmodels.sort_values(by='Score', ascending=False)","522076dd":"**Classification Algorithms: **","54846fec":"Split"}}