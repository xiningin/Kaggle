{"cell_type":{"5e4e68c3":"code","7a7fd0ae":"code","6c3e740a":"code","d6542d50":"code","426bd0b8":"code","4d585866":"code","49502d2d":"code","7c5f6bc0":"code","ab7a267b":"code","35a022af":"code","c7373406":"code","1fbfdaec":"code","112f5ed0":"code","151c3fe5":"code","0c702896":"code","40ae01be":"code","f4b9b493":"code","1a1ae517":"code","66a99317":"code","9adbcea7":"code","8f084a77":"markdown","09dfb7c3":"markdown","7bf1d8b6":"markdown","f92d6835":"markdown"},"source":{"5e4e68c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport torch\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport matplotlib.patches as patches\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# xml library for parsing xml files\nfrom xml.etree import ElementTree as et\nimport cv2\n\nimport os\nimport glob","7a7fd0ae":"# defining the files directory and testing directory\nimages_dir = '..\/input\/face-mask-detection\/images\/'\nannotations_dir = '..\/input\/face-mask-detection\/annotations\/'\n\n\nclass FaceMaskDataset(torch.utils.data.Dataset):\n\n    def __init__(self, images_dir, annotation_dir,width, height, transforms=None):\n        self.transforms = transforms\n        self.images_dir = images_dir\n        self.annotation_dir = annotation_dir\n        self.height = height\n        self.width = width\n        \n        # sorting the images for consistency\n        # To get images, the extension of the filename is checked to be jpg\n        self.imgs = [image for image in sorted(os.listdir(images_dir))]\n        self.annotate = [image for image in sorted(os.listdir(annotation_dir))]\n        \n        # classes: 0 index is reserved for background\n        self.classes = [_, 'without_mask','with_mask','mask_weared_incorrect']\n\n    def __getitem__(self, idx):\n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.images_dir, img_name)\n\n        # reading the images and converting them to correct size and color    \n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        # dividing by 255\n        img_res \/= 255.0\n        \n        # annotation file\n        annot_filename = self.annotate[idx]\n        annot_file_path = os.path.join(self.annotation_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # cv2 image gives size as height x width\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # box coordinates for xml files are extracted and corrected for image size given\n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # bounding box\n            xmin = int(member.find('bndbox').find('xmin').text)\n            xmax = int(member.find('bndbox').find('xmax').text)\n            \n            ymin = int(member.find('bndbox').find('ymin').text)\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            \n            xmin_corr = (xmin\/wt)*self.width\n            xmax_corr = (xmax\/wt)*self.width\n            ymin_corr = (ymin\/ht)*self.height\n            ymax_corr = (ymax\/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # convert boxes into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # getting the areas of the boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        # image_id\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n            \n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# check dataset\ndataset = FaceMaskDataset(images_dir, annotations_dir, 224, 224)\nprint('length of dataset = ', len(dataset), '\\n')\n\n# getting the image and target for a test index.  Feel free to change the index.\nimg, target = dataset[78]\nprint('Image shape = ', img.shape, '\\n','Target - ', target)","6c3e740a":"# Function to visualize bounding boxes in the image\n\ndef plot_img_bbox(img, target):\n    # plot the image and bboxes\n    # Bounding boxes are defined as follows: x-min y-min width height\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(5,5)\n    a.imshow(img)\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Draw the bounding box on top of the imag\n        a.add_patch(rect)\n    plt.show()\n    \n# plotting the image with bboxes. Feel free to change the index\nimg, target = dataset[35]\nplot_img_bbox(img, target)","d6542d50":"# Send train=True fro training transforms and False for val\/test transforms\ndef get_transform(train):\n    \n    if train:\n        return A.Compose([\n                            #A.HorizontalFlip(0.5),\n                            #A.RandomBrightnessContrast(p=0.2),\n                            #A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n                     # ToTensorV2 converts image to pytorch tensor without div by 255\n                            ToTensorV2(p=1.0) \n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","426bd0b8":"def collate_fn(batch):\n    return tuple(zip(*batch))","4d585866":"# use our dataset and defined transformations\ndataset = FaceMaskDataset(images_dir, annotations_dir, 480, 480, transforms= get_transform(train=True))\ndataset_test = FaceMaskDataset(images_dir, annotations_dir, 480, 480, transforms= get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\n# train test split\ntest_split = 0.2\ntsize = int(len(dataset)*test_split)\ndataset = torch.utils.data.Subset(dataset, indices[:-tsize])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=10, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","49502d2d":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","7c5f6bc0":"# to train on gpu if selected.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_classes = 4\n\n# get the model using our helper function\nmodel = get_model_instance_segmentation(num_classes)\n\nnum_epochs = 5\n\n# move model to the right device\nmodel.to(device)\n\n\n    \n# parameters construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\nlen_dataloader = len(data_loader)\n\nfor epoch in range(num_epochs):\n    model.train()\n    i = 0    \n    epoch_loss = 0\n    for imgs, annotations in data_loader:\n        i += 1\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        loss_dict = model(imgs, annotations)\n        losses = sum(loss for loss in loss_dict.values())        \n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step() \n#         print(f'Iteration: {i}\/{len_dataloader}, Loss: {losses}')\n        epoch_loss += losses.item()\n    print('Epoch_loss = ',epoch_loss)\n","ab7a267b":"# the function takes the original prediction and the iou threshold.\n\ndef apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return transforms.ToPILImage()(img).convert('RGB')","35a022af":"# pick one image from the test set\nimg, target = dataset_test[4]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","c7373406":"print('EXPECTED OUTPUT')\nplot_img_bbox(torch_to_pil(img), target)","1fbfdaec":"print('MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), prediction)","112f5ed0":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), nms_prediction)","151c3fe5":"def plot_image(img_tensor, annotation,predict=True):\n    \n    fig,ax = plt.subplots(1)\n    fig.set_size_inches(18.5, 10.5)\n    img = img_tensor.cpu().data\n    mask_dic = {1:'without_mask', 2:'with_mask', 3:'mask_weared_incorrect'}\n\n    # Display the image\n    ax.imshow(img.permute(1, 2, 0))\n    \n    for i,box in enumerate(annotation[\"boxes\"]):\n        xmin, ymin, xmax, ymax = box\n\n        # Create a Rectangle patch\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        label = mask_dic[int(annotation['labels'][i].data)]\n        if predict:\n            score = int((annotation['scores'][i].data) * 100)\n            ax.text(xmin, ymin, f\"{label} : {score}%\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='b')\n        else:\n            score=''\n            ax.text(xmin, ymin, f\"{label}\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='b')\n    plt.show()","0c702896":"for imgs, annotations in data_loader:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        break","40ae01be":"model.eval()\npreds = model(imgs)","f4b9b493":"nms_prediction = apply_nms(preds[4], iou_thresh=0.2)\nprint(\"Prediction\")\nplot_image(imgs[4], nms_prediction)\nprint(\"Target\")\nplot_image(imgs[4].to('cpu'), annotations[4],False)","1a1ae517":"nms_prediction = apply_nms(preds[7], iou_thresh=0.2)\nprint(\"Prediction\")\nplot_image(imgs[7], nms_prediction)\nprint(\"Target\")\nplot_image(imgs[7].to('cpu'), annotations[7],False)","66a99317":"nms_prediction = apply_nms(preds[0], iou_thresh=0.2)\nprint(\"Prediction\")\nplot_image(imgs[0], nms_prediction)\nprint(\"Target\")\nplot_image(imgs[0].to('cpu'), annotations[0],False)","9adbcea7":"nms_prediction = apply_nms(preds[2], iou_thresh=0.2)\nprint(\"Prediction\")\nplot_image(imgs[2], nms_prediction)\nprint(\"Target\")\nplot_image(imgs[2].to('cpu'), annotations[2],False)","8f084a77":"# Function to plot image","09dfb7c3":"# Make a dataset and dataloader","7bf1d8b6":"# Train Model","f92d6835":"# Model"}}