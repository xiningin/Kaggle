{"cell_type":{"fa9012b4":"code","20db7c23":"code","0a39aac5":"code","80c15024":"code","b54f90a3":"code","eb68ab8a":"code","13986b7a":"code","4f02358c":"code","4d631270":"code","a36ac198":"code","adeae328":"code","8f95ed09":"code","fec273ac":"code","bf70eb0f":"markdown","291cbc72":"markdown","4b29f4cf":"markdown","e5193766":"markdown","b597c8b8":"markdown","fcf31bde":"markdown","0f11bea2":"markdown","7eb5402e":"markdown","1388d5b1":"markdown","260c7418":"markdown","1996bcd8":"markdown"},"source":{"fa9012b4":"import pandas as pd\nimport numpy as np\n\nimport plotly.express as px\n\nimport glob\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2","20db7c23":"df_train= pd.read_csv(\"..\/input\/landmark-recognition-2020\/train.csv\")\ndf_submission = pd.read_csv(\"..\/input\/landmark-recognition-2020\/sample_submission.csv\")\nTRAIN_PATH = \"..\/input\/landmark-recognition-2020\/train\"\nTEST_PATH = \"..\/input\/landmark-recognition-2020\/test\"","0a39aac5":"train_list = glob.glob('..\/input\/landmark-recognition-2020\/train\/*\/*\/*\/*')\ntest_list = glob.glob('..\/input\/landmark-recognition-2020\/test\/*\/*\/*\/*')\n\nprint( 'Images in Train Folder:', len(train_list))\nprint( 'Images in Test Folder:', len(test_list))","80c15024":"df_train.head()","b54f90a3":"df_train.shape[0], df_train.id.nunique()","eb68ab8a":"df_train.landmark_id.nunique()","13986b7a":"df_train.isna().sum()","4f02358c":"df_image_counts = df_train.groupby(\"landmark_id\").agg(images = (\"id\",\"nunique\")).reset_index()\ndf_image_counts.head()","4d631270":"px.box(df_image_counts, x= \"images\",width=1000, height=300)","a36ac198":"def plot_images(image_list,rows,cols,title):\n    fig,ax = plt.subplots(rows,cols,figsize = (25,5*rows))\n    ax = ax.flatten()\n    for i, image_id in enumerate(image_list):\n        image = cv2.imread(TRAIN_PATH+'\/{}\/{}\/{}\/{}.jpg'.format(image_id[0],image_id[1],image_id[2],image_id))\n        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        ax[i].imshow(image)\n        ax[i].set_axis_off()\n        ax[i].set_title(image_id)\n    plt.suptitle(title)","adeae328":"plot_images(df_train.loc[df_train.landmark_id==df_image_counts[df_image_counts.images == 6272][\"landmark_id\"].values[0],\"id\"].values[:10],2,5,\"Images of Landmark - 138982 (10 out of 6k images of this landmark)\")","8f95ed09":"plot_images(df_train.loc[df_train.landmark_id==df_image_counts[df_image_counts.images == 2231][\"landmark_id\"].values[0],\"id\"].values[:10],2,5,\"Images of Landmark with second highest number of images (10 out of 2 k images of this landmark)\")","fec273ac":"plot_images(df_train.loc[df_train.landmark_id==df_image_counts[df_image_counts.images == 10][\"landmark_id\"].values[0],\"id\"].values[:10],2,5,\"Images of Landmark:\" + str(df_image_counts[df_image_counts.images == 10][\"landmark_id\"].values[0]))","bf70eb0f":"* This seems to be the default class meaning all the images that donot have a class or a landmark in it are put into this class","291cbc72":"In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The training set is available in the train\/ folder, with corresponding landmark labels in train.csv. The test set images are listed in the test\/ folder. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a\/b\/c\/abcdef.jpg).","4b29f4cf":"Please Upvote if you found this helpful :)","e5193766":"Submissions are evaluated using Global Average Precision (GAP) at k, where k=1. This metric is also known as micro Average Precision (\u03bc\n\nAP), as per [1,2]. It works as follows:\n\nFor each test image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions (sorted in descending order by confidence scores), and computes the Average Precision based on this list.\n\nIf a submission has N\n\npredictions (label\/confidence pairs) sorted in descending order by their confidence scores, then the Global Average Precision is computed as:\n\nGAP=(1\/M)\u2211P(i)rel(i)\n\nwhere:\n\n* N is the total number of predictions returned by the system, across all queries\n* M is the total number of queries with at least one landmark from the training set visible in it (note that some queries may not depict landmarks)\n* P(i) is the precision at rank i\n* rel(i) denotes the relevance of prediciton i: it\u2019s 1 if the i-th prediction is correct, and 0 otherwise\n\nFor each id in the test set, you can predict at most one landmark and its corresponding confidence score. Some images contain no landmarks. You may decide not to predict any result for a given query, by submitting an empty prediction. ","b597c8b8":"# 1. Introduction","fcf31bde":"What we have:\n* We have ~1.6M images in train data with ~81 k landmarks (classes) \n* We have a minimum of 2 images per landmark and a exptionally high 6272 images of a landmark. The upper threshold of the boxplot is 42 meaning most of images have less than or equal to 42 images per landmark (Though it would be interesting to see which is the landmark with 6272 images \ud83d\ude1b)","0f11bea2":"## Importing Libraries","7eb5402e":"## Looking at the dataset","1388d5b1":"## 1.b Evaluation Metrics","260c7418":"## Reading files","1996bcd8":"## 1.a Data Description"}}