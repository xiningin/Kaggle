{"cell_type":{"010c6c2f":"code","b2676ea6":"code","65dfede4":"code","d7a5c924":"code","74285bf0":"code","7e3d3fa9":"code","d9fc96f5":"code","133c14e5":"code","27b0fa01":"code","4b1af56a":"code","57e2e047":"code","70c2cdc1":"code","1488ff29":"code","99a7c478":"code","fd286e06":"code","769a457c":"code","dae32001":"code","191ef293":"code","e1b97010":"code","a3f4ff94":"code","01aaa3ed":"code","211f4e0f":"code","c1cdd637":"code","57b25f90":"code","cfbd4a0d":"code","54373694":"code","f900aefc":"code","e56b2409":"code","50955711":"code","86c9cbfe":"code","baa7a942":"code","509b3a78":"code","6a09b9bc":"code","1b0673be":"code","a61b0368":"code","2a0d2655":"markdown","244189de":"markdown","896644c1":"markdown","418691df":"markdown","15c64ca7":"markdown","5213d831":"markdown","587793e3":"markdown","e60c19a0":"markdown","d4675d61":"markdown","50270a29":"markdown","39213791":"markdown","07da9b4f":"markdown","7ab25ad7":"markdown","d18c0aca":"markdown","6d9acabe":"markdown","a49d0168":"markdown","1a0e4df4":"markdown","1390197d":"markdown","495df712":"markdown","e1e897ec":"markdown","c7338f55":"markdown","05f31948":"markdown","e555a3d8":"markdown","8657bd91":"markdown","707c4cf2":"markdown","1385a1a2":"markdown","c0660931":"markdown","4dbd8770":"markdown","f6c5587c":"markdown","33659c28":"markdown","193a305c":"markdown","4312730e":"markdown","da730fdc":"markdown","c942c5d7":"markdown","3e702eef":"markdown","892083f3":"markdown","aa88bc4c":"markdown","439451a1":"markdown","068c8571":"markdown","71f696a7":"markdown","1e0fdf00":"markdown","c467e4f8":"markdown","5a8fbe7c":"markdown","2d33d336":"markdown"},"source":{"010c6c2f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV as randomCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn import feature_selection\nfrom sklearn.linear_model import LinearRegression as l_reg\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\nfrom plotnine import *\nfrom matplotlib import gridspec\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_decomposition import PLSRegression as pls_reg","b2676ea6":"energy_df=pd.read_csv(r'..\/input\/eergy-efficiency-dataset\/ENB2012_data.csv')\nenergy_df.head()","65dfede4":"energy_df.columns=[\"relative_compactness\",\"surface_area\",\"wall_area\",\"roof_area\",\"overall_height\",\"orientaion\",\n                   \"glazing_area\",\"glazing_area_dist\",\"heating_load\",\"cooling_load\"]","d7a5c924":"energy_df[\"log_heating_load\"]=np.log(energy_df[\"heating_load\"])\nenergy_df[\"log_cooling_load\"]=np.log(energy_df[\"cooling_load\"])\n\nenergy_df_f=energy_df.copy()\nenergy_df_f.drop([\"heating_load\",\"cooling_load\"],axis=1,inplace=True)\n#energy_df_f.drop([\"log_heating_load\",\"cooling_load\"],axis=1,inplace=True)\n\nenergy_X=energy_df_f.iloc[:,:-2]\nenergy_Y=energy_df_f.loc[:,[\"log_heating_load\"]]\n#energy_Y=energy_df_f.loc[:,[\"heating_load\"]]\n\nstd_scale=StandardScaler()\n\nenergy_X_std=std_scale.fit_transform(energy_X)\n\nenergy_train_X,energy_test_X,energy_train_Y,energy_test_Y=\\\ntrain_test_split(energy_X_std,energy_Y,test_size=0.20,random_state=48)\n\nenergy_train_r_X,energy_test_r_X,energy_train_Y,energy_test_Y=\\\ntrain_test_split(energy_X,energy_Y,test_size=0.20,random_state=48)","74285bf0":"def pca_regressor_cv(data_X,data_Y,no_features,seed,cv_no):\n    pcr=make_pipeline(PCA(n_components=no_features,random_state=seed),l_reg())\n    cv_results=cross_validate(pcr,data_X,data_Y,cv=cv_no,\n                              scoring=[\"neg_root_mean_squared_error\",\"r2\"],return_train_score=True)\n    return cv_results\n    ","7e3d3fa9":"rmse_list_train=[]\nrmse_list_test=[]\nr2_list_train=[]\nr2_list_test=[]\nfor i in range(1,9):\n    cv_results_temp=pca_regressor_cv(energy_train_X,energy_train_Y,no_features=i,seed=48,cv_no=5)\n    mean_rmse_train=np.mean(cv_results_temp[\"train_neg_root_mean_squared_error\"])\n    mean_r2_train=np.mean(cv_results_temp[\"train_r2\"])\n    mean_rmse_test=np.mean(cv_results_temp[\"test_neg_root_mean_squared_error\"])\n    mean_r2_test=np.mean(cv_results_temp[\"test_r2\"])\n    rmse_list_train.append(mean_rmse_train)\n    r2_list_train.append(mean_r2_train)\n    rmse_list_test.append(mean_rmse_test)\n    r2_list_test.append(mean_r2_test)\n    rmse_df=pd.DataFrame(zip(rmse_list_train,rmse_list_test,r2_list_train,r2_list_test))\n    rmse_df.columns=[\"Mean RMSE Train\",\"Mean RMSE Test\",\"Mean R2 Train\",\"Mean R2 Test\"]\n    rmse_df.index=rmse_df.index+1","d9fc96f5":"rmse_df","133c14e5":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=rmse_df.iloc[:,:2],ax=ax[0])\nsns.lineplot(data=rmse_df.iloc[:,2:],ax=ax[1])\nax[0].set_title(\"Mean Negative RMSE \\n Based on Number of Features\")\nax[1].set_title(\"Mean R2 Based on Number of Features\")\nplt.show()","27b0fa01":"pca=PCA(n_components=6,random_state=48)\nenergy_PCA_train_X=pd.DataFrame(pca.fit_transform(energy_train_X))\ncum_variance=np.cumsum(pca.explained_variance_ratio_)\ncum_variance_df=pd.DataFrame(zip(pca.explained_variance_ratio_,cum_variance))","4b1af56a":"cum_variance_df.columns=[\"Variance\",\"Cumulative Variance\"]\ncum_variance_df.index=cum_variance_df.index+1\ncum_variance_df","57e2e047":"fig,ax=plt.subplots(1,1,figsize=(5,5))\nsns.lineplot(data=cum_variance_df,ax=ax)\nax.set_title(label=\"Explained Variance \\n for Each Principal Component\")\nax.set_xlabel(xlabel=\"Kth Principal Component\")\nax.set_ylabel(ylabel=\"Explained Variance\")\nplt.show()","70c2cdc1":"l_reg_pca=l_reg()\nl_reg_pca.fit(energy_PCA_train_X,np.ravel(energy_train_Y))\npred_train_pca=l_reg_pca.predict(energy_PCA_train_X)\nenergy_PCA_test_X=pd.DataFrame(pca.transform(energy_test_X))\npred_test_pca=l_reg_pca.predict(energy_PCA_test_X)","1488ff29":"print(\"RMSE for Train set:\",MSE(pred_train_pca,energy_train_Y,squared=False))\nprint(\"RMSE for Test set:\",MSE(pred_test_pca,energy_test_Y,squared=False))","99a7c478":"print(\"R2 for Train set:\",r2_score(pred_train_pca,energy_train_Y))\nprint(\"R2 for Test set:\",r2_score(pred_test_pca,energy_test_Y))","fd286e06":"actual_y_pca=[np.ravel(energy_train_Y),np.ravel(energy_test_Y)]\npredict_y_pca=[pred_train_pca,pred_test_pca]","769a457c":"def residual_plot(actual_y,predict_y,title_label):\n    fig,ax=plt.subplots(1,len(actual_y),figsize=(10,5))\n    for i,col in enumerate(actual_y,0):\n        sns.residplot(x=actual_y[i], y=predict_y[i], lowess=True, color=\"g\",ax=ax[i])\n        ax[i].set_title(title_label[i])\n    return fig,ax","dae32001":"residual_plot(actual_y_pca,predict_y_pca,[\"Train\",\"Test\"])\nplt.show()","191ef293":"raw_pred_err_list_pca=[]\n\nfor i in range(0,len(actual_y_pca)):\n    list_temp=[]\n    list_temp=actual_y_pca[i]-predict_y_pca[i]\n    raw_pred_err_list_pca.append(list_temp)\nraw_pred_err_label=[\"Raw Prediction Errors (Train)\",\"Raw Prediction Errors (Test)\"]","e1b97010":"def raw_predict_err_hist(err_predict_list,bin_no,title_label):\n    fig,ax=plt.subplots(1,len(err_predict_list),figsize=(10,5))\n    for i,col in enumerate(err_predict_list,0):\n        sns.histplot(x=err_predict_list[i],bins=bin_no,kde=True,ax=ax[i])\n        ax[i].set_title(title_label[i])\n    return fig,ax","a3f4ff94":"raw_predict_err_hist(raw_pred_err_list_pca,bin_no=7,title_label=raw_pred_err_label)\nplt.show()","01aaa3ed":"def pls_regressor_cv(data_X,data_Y,no_features,seed,cv_no):\n    data_X=data_X.iloc[:,:no_features]\n    pls_rg=pls_reg(n_components=no_features)\n    no_var=no_features\n    cv_results=cross_validate(pls_rg,data_X,data_Y,cv=cv_no,\n                              scoring=[\"neg_root_mean_squared_error\",\"r2\"],return_train_score=True)\n    return cv_results","211f4e0f":"rmse_pls_list_train=[]\nrmse_pls_list_test=[]\nr2_pls_list_train=[]\nr2_pls_list_test=[]\nfor i in range(0,8):\n    cv_results_temp=pls_regressor_cv(energy_train_r_X,energy_train_Y,no_features=i+1,seed=48,cv_no=5)\n    mean_rmse_train=np.mean(cv_results_temp[\"train_neg_root_mean_squared_error\"])\n    mean_r2_train=np.mean(cv_results_temp[\"train_r2\"])\n    mean_rmse_test=np.mean(cv_results_temp[\"test_neg_root_mean_squared_error\"])\n    mean_r2_test=np.mean(cv_results_temp[\"test_r2\"])\n    rmse_pls_list_train.append(mean_rmse_train)\n    r2_pls_list_train.append(mean_r2_train)\n    rmse_pls_list_test.append(mean_rmse_test)\n    r2_pls_list_test.append(mean_r2_test)\n    rmse_pls_df=pd.DataFrame(zip(rmse_pls_list_train,rmse_pls_list_test,r2_pls_list_train,r2_pls_list_test))\n    rmse_pls_df.columns=[\"Mean RMSE Train\",\"Mean RMSE Test\",\"Mean R2 Train\",\"Mean R2 Test\"]\n    rmse_pls_df.index=rmse_pls_df.index+1","c1cdd637":"rmse_pls_df","57b25f90":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=rmse_pls_df.iloc[:,:2],ax=ax[0])\nsns.lineplot(data=rmse_pls_df.iloc[:,2:],ax=ax[1])\nax[0].set_title(\"Mean Negative RMSE \\n Based on Number of Features\")\nax[1].set_title(\"Mean R2 Based on Number of Features\")\nplt.show()","cfbd4a0d":"pls_rg_f=pls_reg(n_components=7)\npls_rg_f.fit(energy_train_r_X,np.ravel(energy_train_Y))\npred_train_pls=pls_rg_f.predict(energy_train_r_X)\npred_test_pls=pls_rg_f.predict(energy_test_r_X)","54373694":"print(\"RMSE for Train set:\",MSE(pred_train_pls,energy_train_Y,squared=False))\nprint(\"RMSE for Test set:\",MSE(pred_test_pls,energy_test_Y,squared=False))","f900aefc":"print(\"R2 for Train set:\",r2_score(pred_train_pls,energy_train_Y))\nprint(\"R2 for Test set:\",r2_score(pred_test_pls,energy_test_Y))","e56b2409":"actual_y_pls=[np.ravel(energy_train_Y),np.ravel(energy_test_Y)]\npredict_y_pls=[np.ravel(pred_train_pls),np.ravel(pred_test_pls)]","50955711":"residual_plot(actual_y_pls,predict_y_pls,[\"Train\",\"Test\"])\nplt.show()","86c9cbfe":"raw_pred_err_list_pls=[]\n\nfor i in range(0,len(actual_y_pls)):\n    list_temp=[]\n    list_temp=actual_y_pls[i]-predict_y_pls[i]\n    raw_pred_err_list_pls.append(list_temp)\nraw_pred_err_label=[\"Raw Prediction Errors (Train)\",\"Raw Prediction Errors (Test)\"]","baa7a942":"raw_predict_err_hist(raw_pred_err_list_pls,bin_no=7,title_label=raw_pred_err_label)\nplt.show()","509b3a78":"pca_components=pd.DataFrame(pca.components_.T)\npca_components.columns=pca_components.columns+1\npca_components.index=energy_X.columns\npca_components","6a09b9bc":"dict(zip([\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\"],np.exp(l_reg_pca.coef_)))","1b0673be":"pls_x_loading=pd.DataFrame(pls_rg_f.x_loadings_)\npls_x_loading.columns=[\"CP1\",\"CP2\",\"CP3\",\"CP4\",\"CP5\",\"CP6\",\"CP7\"]\npls_x_loading.index=energy_test_r_X.columns\npls_x_loading","a61b0368":"pls_rg_coef_f=dict(zip(pls_x_loading.columns,np.ravel(np.exp(pls_rg_f.coef_))))\npls_rg_coef_f","2a0d2655":"The histograms indicate that the residuals are normally distributed. ","244189de":"# Interpretation on Regression Coefficients ","896644c1":"This section will use the PCA-transformed data to fit into linear regression with log heating log as dependent variable to find out which principal components have high contribution to log heating load.","418691df":"## Searching for Optimal Number of Components for PCA Regression","15c64ca7":"This section will do cross-validation for PCA regressor to find the optimal number of principal components to be used in terms of negative root mean squared error (RMSE) and R2. ","5213d831":"Based on the plots above, PCA regression seems to be a good fit to explain how the combination of the building features affect log heating load.","587793e3":"Based on the plots above, PLS regression seems to be a good fit to explain how the combination of the building features affect log heating load.","e60c19a0":"## Model Diagnostics","d4675d61":"# Data Loading","50270a29":"PCA regressor is set with a seed number of 48 and 5-folds cross validation. The for loop above calculates negative RMSE and R2 for train and test dataset based on number of principal components used and store values for RMSE and R2 into a dataframe.","39213791":"Based on the histograms above, the residuals are normally distributed with a slight long left tail.","07da9b4f":"## PCA Transformation on Independent Variables","7ab25ad7":"R2 for both train and test are slightly higher by approximately 0.001 or 0.002 in PLS regression compared to PCA regression .","d18c0aca":"Looking at the PLS loading table above, CP1 has tall buildings that have strong compactness but small surface area and roof area. CP2 consists of buildings that have weak compactness but large in surface area, wall area and glazing area. CP3 comprises buildings which have small glazing area but high glazing area distribution and large wall area. CP4 is related to buildings that have weak compactness, low orientation value and small glazing area but high in height. CP5 comprises buildings that are with high orientation value and big glazing area distribution. CP6 consists of buildings that with big wall area and high orientation value but small in glazing area and glazing area distribution. The last component has buildings that are with strong compactness, large glazing area, roof area and glazing area distribution but with low orientation value and small wall area. ","6d9acabe":"# Using Energy Efficiency Dataset for Principal Component Analysis and Partial Least Square Linear Regressions","a49d0168":"This section will use residual plots such as histogram and scatter plot to find out whether there are any hidden trend in the residual plots.","1a0e4df4":"This section will do cross-validation for PLS regressor to find the optimal number of principal components to be used in terms of negative root mean squared error (RMSE) and R2.","1390197d":"As a conclusion, PCA and PLS regressions both seem to be good models to find out which combination of independent variables affect log heating load. While PCA's transformation is only focused on independent variables, PLS's transformation uses the relationships between independent variables and dependent variable to do transformation on independent variables. Opinions from domain experts are required to determine whether PCA or PLS provides a better explanation. ","495df712":"Looking at the differences in RMSE, the difference is quite small, around 0.01. ","e1e897ec":"Based on the principal component (PC) loading table above, PC1 comprises buildings with strong compactness, taller height but smaller surface area and roof area. PC2 consists of buildings that have large wall area. PC3 has buildings that have small glazing area and distribution but with large wall area. PC4 is heavily focused on orientation. PC5 comprises buildings that have small glazing area but more glazing area distribution. PC6 is similar to PC1 but lower height.","c7338f55":"The data source is from https:\/\/archive.ics.uci.edu\/ml\/datasets\/Energy+efficiency\n\nThis notebook will explore the dataset and use linear regression to explain the relationship between the independent variables and dependent variable (heating load) with principal component analysis (PCA) and partial least squares (PLS) to be used as the models for dimensionality reduction.\n\nIt is a further extension from the previous notebook.\n(https:\/\/www.kaggle.com\/ariosliew92\/energy-efficiency-linear-regression)\n\nBased on the previous notebook, there are some dependent variables that are highly correlated such as relative compactness with surface area, roof area with surface area and et cetera. Therefore, this notebook will explore using PCA and PLS to reduce dimensionality of the dataset and fit PCA or PLS transformed data into linear regression.","05f31948":"Based on the coefficients above from PCA regressor, it shows that PC1 and PC2 can greatly increase the heating load by at least 22 percent. This shows that a building with strong compactness and high height but smaller surface area or a building with big wall area and glazing area need more heating load to warm up the indoor temperature. PC6 can reduce the heating load by 27 percent indicating that the building with lower height does not require a lot of energy to warm up the indoor environment compared to higher building.","e555a3d8":"This section will explore partial least square (PLS) regression to compare the results between PCA and PLS regressor. \n\nPLS is different from PCA as PLS uses information in both independent variables (Xs) and dependent variable (Y) to generate the components in PLS by searching multidimensional direction among independent variables that explains maximum variance in dependent variable while PCA only uses information in independent variables to generate the components in PCA by searching multiple orthogonal axes that maximise variance. PLS's transformation is only on independent variables while dependent variable remains the same.  \n\nThe detail explanation for PLS regression is in the link below:\nhttps:\/\/en.wikipedia.org\/wiki\/Partial_least_squares_regression","8657bd91":"## Regression Coefficients from PLS Regression","707c4cf2":"## Model Fitting","1385a1a2":"Based on the graphs above, the most optimal number of components for PLS regression is 7 as it has the larger negative RMSE and R2 compared to number of components 6 and below. Despite the larger negative RMSE and R2 when using 8 components, it does not show much difference.  ","c0660931":"## Model Fitting","4dbd8770":"## Model Diagnostics","f6c5587c":"Looking at R2, the model performance is quite good as both test and train sets show that the model is able to explain at least 90% of the variation in the data. ","33659c28":"## Regression Coefficients from PCA Regression","193a305c":"This section will do PCA with 6 principal components according to the previous section on data with independent variables to look at the weights of the independent variables in each principal component.","4312730e":"The function above is to create a model fitting pipeline by doing PCA transformation before linear regression model fitting. The inputs will be data for independent variables and dependent variable, number of features, seed to set the result of PCA transformation to be static and number of folds of cross validation.","da730fdc":"The residual plots above look the same as the residual plots in PCA regression and they do not indicate any trends in the residuals. ","c942c5d7":"Looking at the coefficients for each CP, CP5 has the highest coefficient value followed by CP7, CP2, CP3, CP1 and so on. \nCP5 and CP7 share 1 common characteristic that they both have large glazing area distribution but CP5 has largest orientation value compared to other components while CP7 has strong compactness. CP2 and CP3 both have large wall area but CP3 has smaller glazing area but larger glazing area distribution compared to CP2. \n\nThe high coefficient values due to the combination of large orientation value and glazing area distribution indicates that the combination of large orientation value and glazing area distribution causes high heating load. ","3e702eef":"# PLS Regression","892083f3":"Based on the table and graphs above, the most optimal number of principal components for principal component regressor is 6 based on the graphs above as means for negative RMSE and R2 for 7 principal component do not show significant improvement. ","aa88bc4c":"This section will be using PCA Regression approach to reduce dimensionality of the data by computing k number of principal components that is less than number of independent variables. \n\nPCA uses orthogonal linear transformation to find the maximum variance that lies on each principal component. To do that, PCA uses singular value decomposition to calculate eigenvalues and eigenvectors for each principal component. Therefore, each principal component contains a combination of independent variables with different weights. PCA only uses independent variables for the transformation.\n\nThe detailed explanation for PCA can be found in the link below:\nhttps:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis","439451a1":"Explained variance ratios for each principal component are extracted to know the proportion of variance explained by each principal component.","068c8571":"Comparing RMSE for test with PCA regression, PLS regression is slightly higher compared to PCA regression which is 0.1289.","71f696a7":"# Data Transformation and Splitting","1e0fdf00":"Looking at the residual plots above, both indicates that the model is underestimated the values for log heating load and no hidden trend in the residuals. There might be some outliers as some residuals are greater than 0.3 or less than -0.3.","c467e4f8":"## Searching for Optimal Number of Components for PLS Regression","5a8fbe7c":"Looking at the explained variance for each principal component, the first two principal components consist of most of the variance in the data as the first principal component consists of at least 45% of the variance followed by the second principal component which consists of at least 15% of the variance. ","2d33d336":"# PCA Regression"}}