{"cell_type":{"6f66e634":"code","b0846e4b":"code","b3fe3cd5":"code","b2660e4b":"code","6eb6b8b2":"code","5c33ae38":"code","309b3d48":"code","d6235ffa":"code","e1baa4d4":"code","30a967a1":"code","10c210f5":"code","a8b3293b":"code","d7e22eb8":"code","b57e974a":"code","e8196ca3":"code","02c51563":"code","68052f6d":"code","a6826e7a":"code","69015f65":"code","bc038b74":"code","d8e45600":"markdown","6dadcc4c":"markdown","eaef7a86":"markdown","718b07ce":"markdown","7a284e0a":"markdown","59ad44d0":"markdown","1d9ab72e":"markdown","d2ea2b4f":"markdown","12f8d8ac":"markdown","2272c544":"markdown","cbb9ba4a":"markdown","40dd64f0":"markdown","f8bbd6ee":"markdown","c0fddcd5":"markdown","b0a7e84f":"markdown","9979524f":"markdown","ca0a169a":"markdown"},"source":{"6f66e634":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os","b0846e4b":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata = pd.read_csv('\/kaggle\/input\/restaurant-reviews\/Restaurant_Reviews.tsv' , sep='\\t', quoting=3) #quoting removes '' \ndata.head()","b3fe3cd5":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 1000):\n  review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corpus.append(review)","b2660e4b":"print(corpus)","6eb6b8b2":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values","5c33ae38":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","309b3d48":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","d6235ffa":"y_pred = classifier.predict(X_test)\n#print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","e1baa4d4":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","30a967a1":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","10c210f5":"y_pred = classifier.predict(X_test)\n#print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","a8b3293b":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","d7e22eb8":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","b57e974a":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","e8196ca3":"from sklearn.svm import SVC\nfrom sklearn import *\nclassifier=SVC(kernel=\"linear\", random_state=0)\nclassifier.fit(X_train, y_train)","02c51563":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","68052f6d":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","a6826e7a":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","69015f65":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","bc038b74":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","d8e45600":"## Making the Confusion Matrix","6dadcc4c":"## Splitting the dataset into the Training set and Test set","eaef7a86":"# 4.  Training the  SVM  on the Training set\n","718b07ce":"## 1. Training the Naive Bayes model on the Training set","7a284e0a":"# Summary","59ad44d0":"## Importing the libraries","1d9ab72e":"## Creating the Bag of Words model","d2ea2b4f":"## Cleaning the texts","12f8d8ac":"The more accurate model to train our data is Support Vector Machine with the accuracy 0.79 and Naive Logistic Regression with 0.78","2272c544":"# Natural Language Processing\n","cbb9ba4a":"## Predicting the Test set results","40dd64f0":"# 2.  Training the Naive Logistic Regression on the Training set\n","f8bbd6ee":"# 6.  Training the  Random Forest Regression  on the Training set\n","c0fddcd5":"## Importing the dataset","b0a7e84f":"Test different classifications to find the one with the highest accuracy","9979524f":"# 3.  Training the Naive K_nearest_neighbours on the Training set\n","ca0a169a":"# 5.  Training the  Decision Tree regression on the Training set\n"}}