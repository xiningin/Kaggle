{"cell_type":{"9cc694a7":"code","5058ff94":"code","b42c0642":"code","6d60a183":"code","97fd24b9":"code","7ea8f842":"code","1d64f9cd":"code","77312ce1":"code","be92af01":"code","19864b44":"code","c733c1d7":"code","a6f5f654":"code","406a3918":"code","5408f29d":"code","fb2182aa":"code","ee12a760":"code","45efb08d":"code","a4f01cf6":"code","47a540b7":"code","a6ea5bfd":"code","c8f40636":"code","20e8b470":"code","a7309697":"code","8a17ac96":"code","28e66e66":"code","8f441f2f":"code","c6e220b9":"code","c14137a9":"code","9e8d689c":"code","af461a84":"code","104f9083":"code","ac37ca15":"code","90b8599b":"code","b6768381":"code","8fa3b064":"code","4ba24595":"code","397751b1":"code","f38eae4b":"code","e560f165":"code","f648fef0":"code","9749cdf7":"code","f39d2ddb":"code","dc4854e3":"code","10f036eb":"code","418c78c7":"code","75ed8a7d":"code","036e7e22":"code","a1b024eb":"code","585aa49a":"code","b069d104":"code","c3df347d":"code","5a97a8b1":"code","681b0d14":"code","02a9dea0":"code","444d1b58":"code","43d28ec8":"code","7aae5931":"code","723b01b3":"code","c94b0591":"code","e804f66b":"markdown","45aa7b94":"markdown","10b8158b":"markdown","672a8a08":"markdown","587b7fb2":"markdown","d71804de":"markdown","68436b5c":"markdown","c0e1acfb":"markdown","344e2259":"markdown","f3b54752":"markdown","f592b3e3":"markdown","75c87717":"markdown","2329174a":"markdown","5e475f86":"markdown","596246f5":"markdown","3ad880ad":"markdown"},"source":{"9cc694a7":"# Import all the tools we need\n\n# Regular EDA (exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we want our plots to appear inside the notebook\n%matplotlib inline \n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score ,accuracy_score\nfrom sklearn.metrics import plot_roc_curve","5058ff94":"df=pd.read_csv(\"..\/input\/titanic\/train.csv\")","b42c0642":"df\n","6d60a183":"df.isna().sum()","97fd24b9":"df.info()","7ea8f842":"highest_fare = df[\"Fare\"].max()\nhighest_fare","1d64f9cd":"bins = pd.cut(df['Fare'], [0, 10, 40,75 ,150 ,200 ,600])\ndf.groupby(bins)[\"Survived\"].value_counts().unstack()","77312ce1":"df.isna().sum()","be92af01":"bins_1 = pd.cut(df['Age'], [0, 3, 5,15 ,20 ,50 ,90])\ndf.groupby(bins_1)[\"Survived\"].value_counts().unstack()","19864b44":"df[\"Survived\"].value_counts()","c733c1d7":"df[\"Age\"].fillna(df['Age'].median(), inplace = True)\ndf['Embarked'].fillna(value=\"S\", inplace=True)\ndf['Cabin'].fillna(value=\"Unkn\", inplace=True)","a6f5f654":"df.isna().sum()","406a3918":"bins_1 = pd.cut(df['Age'], [0, 3, 5,15 ,20 ,50 ,90])\ndf.groupby(bins_1)[\"Survived\"].value_counts().unstack()","5408f29d":"df","fb2182aa":"def show_values(axs, orient=\"v\", space=.01):\n    def _single(ax):\n        if orient == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n                value = '{:.2f}'.format(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif orient == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n                value = '{:.2f}'.format(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _single(ax)\n    else:\n        _single(axs)","ee12a760":"df","45efb08d":"##Analysing Various Labels Using countplot\n\n\nfig, ax = plt.subplots(3,2,figsize=(10,20)) # creating multi plot (3*2) with figure size of (10,14)\nfig.subplots_adjust(hspace=0.4, wspace=0.25) #Adjusting  space between charts\n\nshow_values(sns.countplot(x='Sex',data=df,ax=ax[0,0],hue =\"Sex\"))\nax[0,0].set(xlabel = \"Gender \")\n\nshow_values(sns.countplot(x='Pclass',data=df,ax=ax[0,1]))\nax[0,1].set(xlabel = \"Ticket class\")\n\nshow_values(sns.countplot(x='Survived',data=df,ax=ax[1,0]))\nax[1,0].set(xlabel = \"Survived\")\n\nshow_values(sns.countplot(x='Embarked',data =df , ax= ax[1,1]))\n\nsns.histplot(x = 'Age' ,data = df , ax = ax[2,0])\n\n\nsns.histplot(x = 'Fare' ,data = df , ax = ax[2,1])\n","a4f01cf6":"fig , ax =plt.subplots(figsize = (15,8))\nsns.heatmap(df.corr(),annot = True , cmap =\"rocket\")","47a540b7":"sns.pairplot(df , hue=\"Survived\" ,vars=[\"Fare\",\"Age\"] , height =7 , palette =\"plasma\")","a6ea5bfd":"df.info()","c8f40636":"for label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","20e8b470":"for label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        df[label] = content.astype(\"category\").cat.as_ordered()","a7309697":"for label, content in df.items():\n    if pd.api.types.is_float_dtype(content):\n        df[label] = content.astype(\"int64\")","8a17ac96":"df.info()","28e66e66":"\nfor label, content in df.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # We add the +1 because pandas encodes missing categories as -1\n        df[label] = pd.Categorical(content).codes+1  ","8f441f2f":"df","c6e220b9":"#split data into X and y\nX = df.drop(\"Survived\" , axis=1)\n\ny = df[\"Survived\"]","c14137a9":"X","9e8d689c":"y","af461a84":"np.random.seed(42)\n\n#split data into train & test \nX_train ,X_test,y_train,y_test = train_test_split (X,y,test_size = 0.2)","104f9083":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(solver='liblinear'),\n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier(),\n          \"gnb\" : GaussianNB(),\n          \"decision_tree\" : DecisionTreeClassifier(random_state=0, max_depth=2)}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of differetn Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","ac37ca15":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\n\nmodel_scores","90b8599b":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nshow_values(model_compare.T.plot.bar());","b6768381":"np.random.seed(42)\ncv_acc_log = cross_val_score(LogisticRegression(solver='liblinear'), X, y, cv=20, scoring=\"accuracy\").mean()\ncv_acc_gnb =cross_val_score(GaussianNB(), X, y, cv=20, scoring=\"accuracy\").mean()\ncv_acc_rnd =cross_val_score(RandomForestClassifier(), X, y, cv=20, scoring=\"accuracy\").mean()","8fa3b064":"cross_val = { \"Logisticregression\" : cv_acc_log ,\n              \"Gaussiannb\" :cv_acc_gnb,\n               \"RandomForestclssifier \" : cv_acc_rnd }\n\ncross_val","4ba24595":"# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","397751b1":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=rf_grid,\n                           cv=10,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","f38eae4b":"# Evaluate the randomized search RandomForestClassifier model\nrs_rf.score(X_test, y_test)","e560f165":"rs_rf.best_params_","f648fef0":"# Make predictions with tuned model\ny_preds = rs_rf.predict(X_test)\ny_preds","9749cdf7":"# Plot ROC curve and calculate and calculate AUC metric\nplot_roc_curve(rs_rf, X_test, y_test)","f39d2ddb":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    \nplot_conf_mat(y_test, y_preds)","dc4854e3":"print(classification_report(y_test, y_preds))","10f036eb":"def evaluate_preds(y_true, y_preds):\n    \"\"\"\n    Performs evaluation comparison on y_true labels vs. y_pred labels\n    on a classification.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_preds)\n    precision = precision_score(y_true, y_preds)\n    recall = recall_score(y_true, y_preds)\n    f1 = f1_score(y_true, y_preds)\n    metric_dict = {\"accuracy\": round(accuracy, 2),\n                   \"precision\": round(precision, 2),\n                   \"recall\": round(recall, 2),\n                   \"f1\": round(f1, 2)}\n    print(f\"Acc: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 score: {f1:.2f}\")\n    \n    return metric_dict","418c78c7":"baseline_metrics = evaluate_preds(y_test, y_preds)\nbaseline_metrics","75ed8a7d":"clf = RandomForestClassifier(n_estimators= 310,\n min_samples_split= 12,\n min_samples_leaf= 15,\n max_depth= None)\n\nclf.fit(X_train, y_train);","036e7e22":"from sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\nimportance = clf.feature_importances_\n\nfor i,v in enumerate(importance):\n  print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","a1b024eb":"X","585aa49a":"\ndf.groupby(\"Sex\")[\"Survived\"].value_counts().unstack()","b069d104":"plt.figure(figsize=(20, 25))\n\n# Scatter with postivie examples\nplt.scatter(df.Age[df.Survived==1],\n            df.Fare[df.Survived==1],\n            c=\"salmon\")\n\n# Scatter with negative examples\nplt.scatter(df.Age[df.Survived==0],\n            df.Fare[df.Survived==0],\n            c=\"lightblue\")\n\n\nplt.title(\"Comparison of Age & Fare \")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Fare\")\nplt.legend([\"Survived\", \"Not Survived\"]);","c3df347d":"df_1 = pd.read_csv(\"..\/input\/titanic\/test.csv\")","5a97a8b1":"df_1.isna().sum()","681b0d14":"df_1['Fare'].fillna(value = np.mean(df_1['Fare']), inplace=True)\ndf_1[\"Age\"].fillna(df_1['Age'].median(), inplace = True)\ndf_1['Cabin'].fillna(value=\"Unkn\", inplace=True)","02a9dea0":"for label, content in df_1.items():\n    if pd.api.types.is_string_dtype(content):\n        df_1[label] = content.astype(\"category\").cat.as_ordered()\n        \nfor label, content in df_1.items():\n    if pd.api.types.is_float_dtype(content):\n        df_1[label] = content.astype(\"int64\")\n\nfor label, content in df_1.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # We add the +1 because pandas encodes missing categories as -1\n        df_1[label] = pd.Categorical(content).codes+1          ","444d1b58":"df_1","43d28ec8":"df_1.info()","7aae5931":"# Make predictions on the test dataset using the best model\ntest_preds = rs_rf.predict(df_1)","723b01b3":"test_preds","c94b0591":"# Create DataFrame compatible with Kaggle submission requirements\ndf1_preds = pd.DataFrame()\ndf1_preds[\"PassengerId\"] = df_1[\"PassengerId\"]\ndf1_preds[\"Survived\"] = test_preds\ndf1_preds","e804f66b":"# Evaluation ","45aa7b94":"# Converting Object to Category && Float to int64","10b8158b":"In Sex\n 1-Female ,\n 2-Male\n \nIN Survived\n 0 - Not Survived,\n 1 - Survived \n \n ","672a8a08":"# Filling Missing Values","587b7fb2":"# Confusion Matrix","d71804de":"# Hyperparameter Tuning","68436b5c":"# DATA DICTIONARY\n\n1.Survived => 0 : no \/ 1 : yes\n\n2.Pclass (Class Ticket) => 1 : 1st \/ 2 : 2nd \/ 3 : 3rd\n\n3.Name\n\n4.Sex => 0 : man \/ 1 : woman\n\n5.Age\n\n6.Sibsp (of siblings \/ spouses aboard the Titanic)\n\n7.Parch (of parents \/ children aboard the Titanic)\n\n8.Ticket (Ticket Number)\n\n9.Fare (Passenger Fare)\n\n10.Cabin (cabin Number)\n\n11.Embarked (Port of Embarkation) => C : Cherbourg \/ Q = Queenstown \/ S = Southampton\n\n","c0e1acfb":"From above table its confirm that Female have (233\/314)= 74% chance of survival on Other hand male have only 18% of survival","344e2259":"# Checking Feature importance","f3b54752":"# Feature Importance","f592b3e3":"# Modelling","75c87717":"# Roc Curve","2329174a":"# Preprocessing the Test data","5e475f86":"# Turn categorical variables into numbers","596246f5":"# Data Visualization","3ad880ad":"# Classification Report"}}