{"cell_type":{"5bcf7391":"code","5a6738d5":"code","576e4bca":"code","ed51cb1e":"code","56044a02":"code","1d471633":"code","89bae749":"code","42919237":"code","b1cf5932":"code","ce698bec":"code","60db8911":"code","53fed48a":"code","8a169c2e":"code","b9b3e79b":"code","fdf0ceb2":"code","a56a0a83":"code","69ecb833":"code","0300b631":"code","20e2c4c1":"code","597344f7":"code","f136b25d":"code","4d06cf2b":"code","4f76c4e1":"code","9cd96f5d":"code","ff5e1175":"code","6733230f":"code","ee2bf23c":"code","12d8ccde":"code","6cf14894":"code","07f39e9d":"code","cdc8a814":"code","52789760":"code","ba1a9674":"code","236847e7":"code","16b8d0e3":"code","e9c7e5dc":"code","732ddfb4":"code","1fe8b7aa":"code","b90b3530":"code","80e54136":"code","5b4f1c97":"code","982a58cf":"code","58dba95d":"code","8b957873":"code","26de6402":"code","4a0e680a":"code","9aa4e01a":"code","3bf8135e":"code","8fb6b317":"markdown","a3896597":"markdown","091ea7f2":"markdown","4e2ce41d":"markdown","834c1022":"markdown","ef2ebc13":"markdown","59170fb7":"markdown","1f2132b8":"markdown"},"source":{"5bcf7391":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5a6738d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport re","576e4bca":"# Column names list\n\ncolumn_list= ['Column'+str(i) for i in range(15)]\ncolumn_list.insert(0, 'Review')\ncolumn_list","ed51cb1e":"# Reading the data\ndata= pd.read_csv('..\/input\/sentisum\/sentisum-evaluation-dataset.csv', names= column_list)","56044a02":"# Creating Column16 that would contain all the sentiments joined with ',' for each review.\n\ndata['Column16'] = data[data.columns[1:]].apply(lambda x: ', '.join(x.dropna().astype(str)),axis=1)","1d471633":"# Dataset contains redacted company or application names therefore replaced it with 'the company' and created a new column 'Review0'.\n\ndata['Review0']= data.apply(lambda x: x['Review'].replace('[REDACTED]', 'the company'), axis=1)","89bae749":"data['Column16'].isnull().sum()\n\n#Inference: No null row in Column16 (Hurrayyy!!)","42919237":"# Creating a feature to check the presence of [REDACTED] in review.\n\ndata['is_redacted']= data.apply(lambda x: '[REDACTED]' in x['Review'], axis=1)","b1cf5932":"data['is_redacted'].value_counts()\n\n#Inference: 1631 rows have redacted names","ce698bec":"data.head()","60db8911":"data.columns","53fed48a":"# Creating a review DataFrame for gaining an insight on total type of reviews present in the original dataset\n\nreview= pd.DataFrame(data['Column0'].value_counts())\n\nfor i in data.columns[2:16]:\n    review= pd.concat([review, pd.DataFrame(data[i].value_counts())])","8a169c2e":"review.shape\n\n#Inference: 282 type of sentiments are present in the entire dataset.","b9b3e79b":"# 'Sum' column contains the count of each sentiment with their polarity tags \n\nreview['Sum']= review.fillna(0).apply(lambda x: sum(x), axis=1)\nreview.reset_index(inplace= True)\nreview.rename(columns= {'index':'Reviews'}, inplace= True)","fdf0ceb2":"review.head()","a56a0a83":"pos= review[review['Reviews'].str.contains(\"positive\")]\nneg= review[review['Reviews'].str.contains(\"negative\")]\nprint('Total no. of sentiments with positive polarity tag : {}'.format(pos['Sum'].sum()))\nprint('Total no. of sentiments with negative polarity tag : {}'.format(neg['Sum'].sum()))\nprint('Total size of data: {}'.format(data.shape[0]))\n\n\n# Inference: Clearly there is class imbalance. We need to be more careful regarding negative sentiments since they are very less. \n# Also, there are many rows with overlapping reviews i.e. they contain both negative and positive.\n","69ecb833":"y= [pos['Sum'].sum(), neg['Sum'].sum()]\nx= ['Total no. of sentiments with positive polarity tag', 'Total no. of sentiments with negative polarity tag']\n\nplt.figure(figsize = (10,5))\nplt.barh(x,y, color = \"m\")\nplt.grid(True)","0300b631":"labels= list(review.iloc[:10]['Reviews'])\nsize= list(review.iloc[:10]['Sum'])\nexplode= [0.1, 0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.3]\n\nplt.figure(figsize = (9,9))\nplt.pie(size, labels = labels, shadow = True, startangle = 90, autopct='%1.1f%%', explode= explode )\nplt.title(\"Reviews Distribution\", y=1.095)\nplt.legend()\n\n#Inference: The data is biased towards positive polarity and 'value for money' is the most used sentiment in terms of frequency.","20e2c4c1":"x= data[data['Column16'].str.contains('positive')].shape[0]\ny= data[data['Column16'].str.contains('negative')].shape[0]\nz= data[~data['Column16'].str.contains('positive')].shape[0]\n\nprint('Total rows containing at least one positive sentiment: ', x)\nprint('Total rows containing at least one negative sentiment: ', y)\nprint('Total rows containing neither positive nor negative sentiment: ', z-y)","597344f7":"ax= [x,y,z]\nordinate= ['Total rows containing at least one positive sentiment', 'Total rows containing at least one negative sentiment','Total rows containing neither positive nor negative sentiment']\n\nplt.figure(figsize = (10,5))\nplt.barh(ordinate,ax, color = \"b\")\nplt.grid(True)","f136b25d":"# For eg. this cell contains no explicit positive or negative tag\n\ndata['Column16'][10130]","4d06cf2b":"# Dropping the rows where polarity tag is not provided since it would make the trained model less impactful\n \ndata.drop(data[~data['Column16'].str.contains('negative|positive')].index, inplace= True)","4f76c4e1":"data.tail(3)  # Indexing is wrong, reset is required","9cd96f5d":"data.reset_index(inplace=True)\ndata= data.drop(['index'],axis=1)","ff5e1175":"data= data.drop([])","6733230f":"data.head(3)","ee2bf23c":"# Sterilization function for cleaning the data and making it more suitable for the model.\n\ndef sterilization(data):\n    \n    data = re.sub('https?:\/\/\\S+|www\\.\\S+', '', data) #remove any HTTP link\n    data = re.sub('<*>', '', data)\n    emoj = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    emoj.sub(r'', data)\n    data = re.sub(r'\\w*\\d\\w*','', data) #remove any number of alphanumeric shortcuts used in comments\n    data = re.sub(r'\\s+', ' ', data)    #remove white space character\n\n    return data","12d8ccde":"# Cleaning data...\ndata['Review0']=data['Review0'].apply(lambda x : sterilization(x))","6cf14894":"# Changing column order for better view of dataset\n\ndata= data.drop(['Column0', 'Column1', 'Column2', 'Column3', 'Column4',\n       'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10',\n       'Column11', 'Column12', 'Column13', 'Column14' ], axis=1)\n","07f39e9d":"data.tail()","cdc8a814":"import glob\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader","52789760":"!pip install pytorch_lightning==0.8.1","ba1a9674":"import argparse\nimport pytorch_lightning as pl","236847e7":"from transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    get_linear_schedule_with_warmup\n)","16b8d0e3":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","e9c7e5dc":"# Dividing dataset for train, validation and testing\n\ntrain= data[:7001]\nval= data[7001:8101]\nref= data[8101:]\ntrain.to_csv('train.csv', index=False)\nval.to_csv('valid.csv', index= False)\nref.to_csv('ref.csv', index=False)","732ddfb4":"class T5FineTuner(pl.LightningModule):\n    def __init__(self, hparams):\n        super(T5FineTuner, self).__init__()\n        self.hparams = hparams\n\n        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n\n    def is_logger(self):\n        return True\n\n    def forward(\n            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n    ):\n        return self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            lm_labels=lm_labels,\n        )\n\n    def _step(self, batch):\n        lm_labels = batch[\"target_ids\"]\n        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n        outputs = self(\n            input_ids=batch[\"source_ids\"],\n            attention_mask=batch[\"source_mask\"],\n            lm_labels=lm_labels,\n            decoder_attention_mask=batch['target_mask']\n        )\n\n        loss = outputs[0]\n\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._step(batch)\n\n        tensorboard_logs = {\"train_loss\": loss}\n        return {\"loss\": loss, \"log\": tensorboard_logs}\n\n    def training_epoch_end(self, outputs):\n        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def validation_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        return {\"val_loss\": loss}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        tensorboard_logs = {\"val_loss\": avg_loss}\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def configure_optimizers(self):\n        \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n        model = self.model\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.hparams.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n        self.opt = optimizer\n        return [optimizer]\n\n    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n        if self.trainer.use_tpu:\n            xm.optimizer_step(optimizer)\n        else:\n            optimizer.step()\n        optimizer.zero_grad()\n        self.lr_scheduler.step()\n\n    def get_tqdm_dict(self):\n        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n\n        return tqdm_dict\n\n    def train_dataloader(self):\n        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n                                num_workers=4)\n        t_total = (\n                (len(dataloader.dataset) \/\/ (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n                \/\/ self.hparams.gradient_accumulation_steps\n                * float(self.hparams.num_train_epochs)\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n        )\n        self.lr_scheduler = scheduler\n        return dataloader\n\n    def val_dataloader(self):\n        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"valid\", args=self.hparams)\n        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)","1fe8b7aa":"logger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n        def on_validation_end(self, trainer, pl_module):\n            logger.info(\"***** Validation results *****\")\n            if pl_module.is_logger():\n                  metrics = trainer.callback_metrics\n                  # Log results\n                  for key in sorted(metrics):\n                    if key not in [\"log\", \"progress_bar\"]:\n                      logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n        def on_test_end(self, trainer, pl_module):\n            logger.info(\"***** Test results *****\")\n\n            if pl_module.is_logger():\n                metrics = trainer.callback_metrics\n\n                  # Log and save results to file\n                output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n                with open(output_test_results_file, \"w\") as writer:\n                    for key in sorted(metrics):\n                          if key not in [\"log\", \"progress_bar\"]:\n                            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n                            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))","b90b3530":"args_dict = dict(\n    data_dir=\"\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='t5-base',\n    tokenizer_name_or_path='t5-base',\n    max_seq_length=200,\n    learning_rate=3e-4,\n    weight_decay=0.0,\n    adam_epsilon=1e-8,\n    warmup_steps=0,\n    train_batch_size=16,\n    eval_batch_size=16,\n    num_train_epochs=2,\n    gradient_accumulation_steps=8,\n    n_gpu=1,\n    early_stop_callback=False,\n    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n    opt_level='O1', # you can find out more on optimisation levels here https:\/\/nvidia.github.io\/apex\/amp.html#opt-levels-and-properties\n    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    seed=42,\n)","80e54136":"class SentimentDataset(Dataset):\n    def __init__(self, tokenizer, data_dir, type_path, max_len=200):\n        self.path = os.path.join(data_dir, type_path + '.csv')\n\n        self.question = \"Review0\"             # Review0 column as input\n        self.target_column = \"Column16\"       # Column16 column as output\n        self.data = pd.read_csv(self.path)\n\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.inputs = []\n        self.targets = []\n\n        self._build()\n\n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, index):\n        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n        target_ids = self.targets[index][\"input_ids\"].squeeze()\n\n        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n\n    def _build(self):\n        for idx in range(len(self.data)):\n            target,question= self.data.loc[idx, self.target_column], self.data.loc[idx, self.question]\n\n            input_ = \"Comment: %s <\/s>\" % (question)\n            target = \" %s <\/s>\" %(target)\n\n            # tokenize inputs\n            tokenized_inputs = self.tokenizer.batch_encode_plus(\n                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n            )\n            # tokenize targets\n            tokenized_targets = self.tokenizer.batch_encode_plus(\n                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n            )\n\n            self.inputs.append(tokenized_inputs)\n            self.targets.append(tokenized_targets)","5b4f1c97":"# Pretrained tokenizer is used\n\ntokenizer = T5Tokenizer.from_pretrained('t5-base')","982a58cf":"# Gentle check if everything is according to the plan or not\n\ndataset = SentimentDataset(tokenizer, '\/kaggle\/working', 'valid', 200)\n\ndata = dataset[200]\nprint(tokenizer.decode(data['source_ids']))\nprint(tokenizer.decode(data['target_ids']))","58dba95d":"!mkdir result","8b957873":"# We will train the model for 6 epochs. This value is decided through iterative process.\n\nargs_dict.update({'data_dir': '\/kaggle\/working', 'output_dir': '\/kaggle\/working\/result', 'num_train_epochs':1,'max_seq_length':200})\nargs = argparse.Namespace(**args_dict)\nprint(args_dict)","26de6402":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    \n    period =1,filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n)\n\ntrain_params = dict(\n    accumulate_grad_batches=args.gradient_accumulation_steps,\n    gpus=args.n_gpu,\n    max_epochs=args.num_train_epochs,\n    early_stop_callback=False,\n    precision= 16 if args.fp_16 else 32,\n    amp_level=args.opt_level,\n    gradient_clip_val=args.max_grad_norm,\n    checkpoint_callback=checkpoint_callback,\n    callbacks=[LoggingCallback()],\n)","4a0e680a":"def get_dataset(tokenizer, type_path, args):\n    return SentimentDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)","9aa4e01a":"print (\"Initialize model\")\nmodel = T5FineTuner(args)\n\ntrainer = pl.Trainer(**train_params)","3bf8135e":"print (\" Training model\")\ntrainer.fit(model)\n\nprint (\"training finished\")\n\nprint (\"Saving model\")\nmodel.model.save_pretrained(\"\/kaggle\/working\/result\")\n\nprint (\"Saved model\")","8fb6b317":"# **SentiSum**\n\nThis notebook tries to solve the problem presented by SentiSum. A dataset of ~10k rows is given containing the Reviews about garage services provided to customers. These reviews are tagged with corresponding sentiments. Each sentiment is further tagged with corresponding polarity i.e. positive or negative.\n\nThe proposed solution involves required EDA and Feature Engineering of the dataset and finally the tweaked dataset is used to train a T5 model for predicting the sentiments and polarity tags for a given review.\n\nNOTE: I've tried to give a short explaination of each code snippet with the possible inference. Enjoy!!\n\nPS: The notebook for testing the model and further inferencing can be found [here](https:\/\/www.kaggle.com\/bunnyyy\/sentisum-inferencing).\n\n![image.png](attachment:image.png)\n","a3896597":"### Hyperparameter setting","091ea7f2":"### Review dataframe is created for exploring the types and distribution of sentiments across the data.","4e2ce41d":"### Add prefix 'Comment: ' to each review before passing through the model","834c1022":"### T5 inbuilt sentencepiece tokenizer is used. AdamW optimizer is used, similar to the paper.","ef2ebc13":"# EDA and Feature Engineering starts here...","59170fb7":"### Visualization of distribution of top 10 sentiments according to their frequencies.","1f2132b8":"# Training of T5 model starts..."}}