{"cell_type":{"ff21a229":"code","372f9626":"code","935fad23":"code","93d4fdb3":"code","a5cbc925":"code","0ff83819":"code","bace6a0e":"code","50b80a64":"code","58da629d":"code","99580ccc":"code","fc6528a7":"code","d9022c58":"code","40547df7":"code","008bb3f4":"code","104d74db":"code","73523f67":"code","fde3b1cd":"code","5d4d45bf":"code","6745d00a":"code","3eb8710b":"code","34126775":"code","85360ee8":"code","47c61b7b":"code","f3e91b3e":"code","b9fbfba7":"code","eff27968":"code","5cd877af":"code","01299fc0":"code","709f6bc4":"code","69442781":"code","874e6550":"code","1616203c":"code","8d0c5d9d":"code","34cbd498":"code","535ebf3b":"markdown","3ac83d1a":"markdown","082f2d79":"markdown","626d6cb4":"markdown","e6fb338d":"markdown","b995687f":"markdown","6cf1e1a0":"markdown","c5800b9c":"markdown","0d530c24":"markdown","6948bf5a":"markdown","904b82ae":"markdown","932eeae0":"markdown","2f53d449":"markdown","6d13022c":"markdown","c847e98e":"markdown","25eb1ca5":"markdown","85bd7989":"markdown","f30691bf":"markdown","2ce91693":"markdown","d90aec3a":"markdown","935ab4c4":"markdown","71bdd734":"markdown","43b036aa":"markdown","5efe7932":"markdown","093d23d9":"markdown","a1ee1581":"markdown","8b3d423e":"markdown","d90898be":"markdown","a6f0fbd9":"markdown","dd106640":"markdown","d6fac016":"markdown","ea617644":"markdown"},"source":{"ff21a229":"import numpy as np\nimport pandas as pd\nimport gensim\nimport matplotlib.pyplot as plt\nimport random\nimport gc\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nrandom.seed(1234)","372f9626":"train_df = pd.read_csv('..\/input\/google-quest-challenge\/train.csv')\ntest_df = pd.read_csv('..\/input\/google-quest-challenge\/test.csv')\n\n# the dataset size\nprint('train set size is %d' % len(train_df))\nprint('test set size is %d' % len(test_df))","935fad23":"feat_cols = [\n    'question_title', \n    'question_body', \n    'question_user_name', \n    'question_user_page', \n    'answer', \n    'answer_user_name', \n    'answer_user_page', \n    'url', \n    'category', \n    'host']\ntarget_cols = [\n    'question_asker_intent_understanding', \n    'question_body_critical', \n    'question_conversational', \n    'question_expect_short_answer', \n    'question_fact_seeking', \n    'question_has_commonly_accepted_answer',\n    'question_interestingness_others', \n    'question_interestingness_self', \n    'question_multi_intent', \n    'question_not_really_a_question', \n    'question_opinion_seeking', \n    'question_type_choice',\n    'question_type_compare', \n    'question_type_consequence', \n    'question_type_definition', \n    'question_type_entity', \n    'question_type_instructions', \n    'question_type_procedure', \n    'question_type_reason_explanation', \n    'question_type_spelling',\n    'question_well_written', \n    'answer_helpful', \n    'answer_level_of_information', \n    'answer_plausible', \n    'answer_relevance', \n    'answer_satisfaction', \n    'answer_type_instructions', \n    'answer_type_procedure', \n    'answer_type_reason_explanation',\n    'answer_well_written'\n]\n\nprint('we have %d feature columns and %d target columns' % (len(feat_cols) , len(target_cols)))","93d4fdb3":"peek = train_df.sample()\ntext_cols = [\n    'question_title',\n    'question_body',\n    'answer'\n]\n\nfor col_name in feat_cols + target_cols:\n    print(col_name)\n    print('='* 10)\n    print(str(peek[col_name].values[0]) + '\\n')","a5cbc925":"train_df.groupby('question_title').count()['qa_id'].sort_values(ascending=False)","0ff83819":"# take a closer look to the most popular question\ntrain_df[train_df['question_title'] == 'What is the best introductory Bayesian statistics textbook?'][feat_cols]","bace6a0e":"train_df.groupby('category').count()['qa_id'].sort_values(ascending=False).plot(kind='bar', alpha=0.5)","50b80a64":"train_df.groupby('host').count()['qa_id'].sort_values(ascending=False).plot(kind='bar', figsize=(16, 6), fontsize=15, alpha=0.5)","58da629d":"train_df.groupby('answer_user_name').count()['qa_id'].sort_values(ascending=False)[:5]","99580ccc":"train_df[train_df['answer_user_name'] == 'Scott'][['answer_user_name', 'question_title', 'category', 'host']].sort_values(by='category')","fc6528a7":"train_df.groupby(['question_user_name']).count()['qa_id'].sort_values(ascending=False).iloc[:5]","d9022c58":"train_df[train_df['question_user_name'] == 'Mike'][['question_user_name', 'question_title', 'category', 'host']].sort_values(by=['category', 'question_title'])","40547df7":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\nTOKENIZER = RegexpTokenizer(r'\\w+')\nSTOPWORDS = set(stopwords.words('english'))","008bb3f4":"train_df['question_title_len'] = train_df['question_title'].map(lambda x: len(TOKENIZER.tokenize(x)))\ntrain_df['question_body_len'] = train_df['question_body'].map(lambda x: len(TOKENIZER.tokenize(x)))\ntrain_df['answer_len'] = train_df['answer'].map(lambda x: len(TOKENIZER.tokenize(x)))","104d74db":"train_df[['answer_len', 'question_body_len', 'question_title_len']].plot(kind='box', showfliers=False)","73523f67":"def gen_word_cloud(col):\n    rows = train_df[col].map(lambda x: TOKENIZER.tokenize(x)).values.tolist()\n    words = []\n    for row in rows:\n        for w in row:\n            if w not in STOPWORDS:\n                words.append(w.lower())\n    \n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join(words))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n\ngen_word_cloud('question_title')","fde3b1cd":"gen_word_cloud('question_body')","5d4d45bf":"gen_word_cloud('answer')","6745d00a":"import re\nquestion_related_target_cols = [ col for col in target_cols if re.search('^question_', col)]\nanswer_related_target_cols = [ col for col in target_cols if re.search('^answer_', col)]","3eb8710b":"train_df[answer_related_target_cols[:5]].plot(kind='hist', figsize=(12, 6), alpha=0.5)","34126775":"train_df[question_related_target_cols[:5]].plot(kind='hist', figsize=(12, 6), alpha=0.5)","85360ee8":"plt.figure(figsize=(12, 12))\nsns.heatmap(data=train_df[answer_related_target_cols].corr(), \n            square=True, \n            annot=True,\n            linewidths=1, \n            cmap=sns.color_palette(\"Blues\"))","47c61b7b":"plt.figure(figsize=(12, 12))\nsns.heatmap(data=train_df[question_related_target_cols].corr(), \n            square=True, \n            linewidths=1, \n            cmap=sns.color_palette(\"Blues\"))","f3e91b3e":"std = train_df.groupby('category')[question_related_target_cols[:8]].std()\ntrain_df.groupby('category')[question_related_target_cols[:8]].mean().plot(kind='bar', figsize=(16, 8), \n                                                                           yerr=std)","b9fbfba7":"std = train_df.groupby('category')[answer_related_target_cols].std()\ntrain_df.groupby('category')[answer_related_target_cols].mean().plot(kind='bar', figsize=(16, 8), \n                                                                           yerr=std)","eff27968":"frequent_hosts = set(train_df.groupby('host').count()['qa_id'].sort_values(ascending=False)[:10].index)\nidx = train_df['host'].map(lambda x: x in frequent_hosts)\ntrain_subset = train_df[idx]\n\nstd = train_subset.groupby('host')[answer_related_target_cols].std()\ntrain_subset.groupby('host')[answer_related_target_cols].mean().plot(kind='bar', figsize=(16, 8), yerr=std)","5cd877af":"std = train_subset.groupby('host')[question_related_target_cols[:8]].std()\ntrain_subset.groupby('host')[question_related_target_cols[:8]].mean().plot(kind='bar', figsize=(16, 8), yerr=std)","01299fc0":"plt.figure(figsize=(8, 6))\ntext_len_cols = ['question_title_len', 'question_body_len', 'answer_len']\ncorr_with_text_len = train_df[answer_related_target_cols + text_len_cols].corr().loc[text_len_cols, answer_related_target_cols]\nsns.heatmap(data=corr_with_text_len.T, \n            square=True, \n            linewidths=1, \n            annot=True,\n            cmap=sns.color_palette(\"Blues\"))","709f6bc4":"plt.figure(figsize=(12, 12))\ncorr_with_text_len = train_df[question_related_target_cols + text_len_cols].corr().loc[text_len_cols, question_related_target_cols]\nsns.heatmap(data=corr_with_text_len.T, \n            square=True, \n            linewidths=1, \n            annot=True,\n            cmap=sns.color_palette(\"Blues\"))","69442781":"# load w2v model, this might take a few moments, grab a coffee and relax\nw2v_model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/word2vec-google\/GoogleNews-vectors-negative300.bin', binary=True, unicode_errors='ignore')","874e6550":"TFIDF_SVD_WORDVEC_DIM = 300\n\ndef get_text_feats(df, col):\n\n    def tokenize_downcase_filtering(x):\n        words = TOKENIZER.tokenize(x)\n        lower_case = map(lambda w: w.lower(), words)\n        content_words = filter(lambda w: w not in STOPWORDS, lower_case)\n        return ' '.join(content_words)\n\n    rows = df[col].map(tokenize_downcase_filtering).values.tolist()\n    tfidf = TfidfVectorizer(tokenizer=lambda x: x.split(' '))  # dont use sklearn default tokenization tool \n    tfidf_weights = tfidf.fit_transform(rows)\n    svd = TruncatedSVD(n_components=TFIDF_SVD_WORDVEC_DIM, n_iter=10)  # reduce dimensionality\n    dense_tfidf_repr_mat = svd.fit_transform(tfidf_weights)\n    \n    word2vec_repr_mat = np.zeros((len(df), w2v_model.vector_size))\n    for i, row in enumerate(rows):\n        word2vec_accum = np.zeros((w2v_model.vector_size, ))\n        word_cnt = 0\n        for w in row.split(' '):\n            if w in w2v_model.wv:\n                word2vec_accum += w2v_model.wv[w]\n                word_cnt += 1\n\n        # compute the average for the wordvec of each non-sptop word\n        if word_cnt != 0:\n            word2vec_repr_mat[i] = word2vec_accum \/ word_cnt\n\n    return  np.concatenate([word2vec_repr_mat, dense_tfidf_repr_mat], axis=1)  # word2vec + tfidf\n\n\ndef one_hot_feats(df, col):\n    return pd.get_dummies(df['host'], prefix='host', drop_first=True).values\n\n\n# let's build features\ndf_all = pd.concat((train_df, test_df))\ndf_all['question_title_len'] = df_all['question_title'].map(lambda x: len(TOKENIZER.tokenize(x)))\ndf_all['question_body_len'] = df_all['question_body'].map(lambda x: len(TOKENIZER.tokenize(x)))\ndf_all['answer_len'] = df_all['answer'].map(lambda x: len(TOKENIZER.tokenize(x)))\n\ndata = []\nfor col in text_cols:\n    data.append(get_text_feats(df_all, col))\n\nfor col in ['category', 'host']:\n    data.append(one_hot_feats(df_all, col))\n\ndata.append(df_all[text_len_cols].values)\ndata = np.concatenate(data, axis=1)\n\ntrain_feats = data[:len(train_df)]\ntest_feats = data[len(train_df):]\n\n# del w2v_model\n# gc.collect()\nprint(train_feats.shape)","1616203c":"# code from https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.model_selection import KFold\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\n\nnum_folds = 5\nfold_scores = []\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=9102)\n\ntest_preds = np.zeros((len(test_feats), len(target_cols)))\nfor train_index, val_index in kf.split(train_feats):\n    train_X = train_feats[train_index, :]\n    train_y = train_df[target_cols].iloc[train_index]\n    \n    val_X = train_feats[val_index, :]\n    val_y = train_df[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(512, input_shape=(train_feats.shape[1],)),\n        Activation('tanh'),\n        Dense(256),\n        Activation('tanh'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs=50, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    overall_score = 0\n    print('-'* 10)\n    for i, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, i], val_y[col].values).correlation \/ len(target_cols)\n        print('%s\\t%.5f' % (col, spearmanr(preds[:, i], val_y[col].values).correlation))\n\n    fold_scores.append(overall_score)\n    test_preds += model.predict(test_feats) \/ num_folds\n    \nprint(fold_scores)","8d0c5d9d":"sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\nfor i, col in enumerate(target_cols):\n    sub[col] = test_preds[:, i]\nsub.to_csv(\"submission.csv\", index = False)","34cbd498":"sub","535ebf3b":"## 2. Build a DNN model","3ac83d1a":"<font size=4> **Scott** seems not just one person.","082f2d79":"# Benchmark with Word2Vec\n","626d6cb4":"# Inspect Feature Variables","e6fb338d":"<font size=4>Now see the word distribution","b995687f":"<font size=4>A closer look to Mike<\/font>","6cf1e1a0":"<font size=4> the correlation between answer_level_of_information and answer_lne is 0.31.. make sense hah.","c5800b9c":"## 4. Answer User Name & Question User Name\n<font size=4> Who is the most active user <\/font>","0d530c24":"------------------","6948bf5a":"# Inspect Target Variable","904b82ae":"## 3. Submit","932eeae0":"<font size=4> We compute the averages of several target values for each category, and display them with std as error bars. It seems some question related target variables are slighted affected by the category, while the question related ones are not. ","2f53d449":"**NOTE:**\n<font size=4>\n    <p>*question_title*, *question_body*, *question_user_page*, *url* and etc. are bounded together.<\/p>\n    <p>ALSO The asker might answer his own question [see row 1647 in the above example].<\/p>\n<\/font>","6d13022c":"<font size=4> <p> The techniques for text vectorization we use in this part are:\n* TFIDF \n* SVD -- dimension reduction for the TFIDF weights --> the dimension for the dense representation is 300d\n* Word2Vec (300d google news) -- we use the average over the word-vectors of each non-stop word in each passage of text <\/p>\n\n<p> We also include the length of text as a feature. <\/p>","c847e98e":"----------------\n<font size=4 >Here lists the top5 active question user","25eb1ca5":"<font size=4>Now a quick look for one sample<\/font>","85bd7989":"<font size=4> Here are many duplicated question titles. The higest one named 'What is the best introductory Bayesian statistics textbook?' attracts 12 answers in the trainset.<\/font>","f30691bf":"## 1. Variable Distribution","2ce91693":"-------------","d90aec3a":"## 2. Variable Correlations","935ab4c4":"## 2. Category & Host\n<font size=4> The Distribution over category and host<\/font>","71bdd734":"## 3. target vs Text Len","43b036aa":"# Target vs Features","5efe7932":"# Intro\n<font size=4> This Notebook gives u guys a gentle introduction towards the QA labeling Competition, which includes a detailed EDA about the dataset and a benchmark Keras DNN model with Word2Vec. Wish you Happy Kaggling!","093d23d9":"## 2. target vs Host\n<font size=4> we filter out some unfrequent host at first","a1ee1581":"## 1. target vs Category","8b3d423e":"## 1. Feature Engineering","d90898be":"## 5. Text","a6f0fbd9":"<font size=4> Still, some question realted target (*question_converstional*, *qustion_interestingness_self*, etc.) are sensitive to the *host*","dd106640":"<font size=4> Let's see the distribution of text length","d6fac016":"## 1. question_title","ea617644":"<font size=4> Here lists the top5 active answer user"}}