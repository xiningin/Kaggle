{"cell_type":{"0cc13eea":"code","0ab93842":"code","659eb995":"code","4ab02841":"code","e58562df":"code","0727b87b":"code","07cee584":"code","8ef6beb2":"code","3e903f0a":"code","6a42ff71":"code","7af8c5ed":"code","83708941":"code","db30e6b4":"code","85e03016":"code","6cf509da":"code","42ed4dca":"code","3561d2a2":"code","78dc63a9":"code","97d984b2":"code","13549b84":"code","473fb3d3":"code","935925e9":"code","ee7bb1df":"code","74424f93":"code","8790da9a":"code","7c8f2edc":"code","326607a9":"code","9a7c3914":"code","a99183fa":"code","c4a920a4":"code","97c22cb3":"code","21ddcd51":"code","0a08c1cc":"code","c78d35d0":"code","3f980619":"code","a62eda87":"code","fdfda872":"code","35e65410":"code","c5b96f39":"code","e35c9f97":"code","d928366b":"code","d0a6a2fd":"code","97117877":"code","c6ba719f":"code","6ccee123":"code","04470684":"code","fa18e58e":"code","665485c9":"code","532714c4":"code","06cdf917":"code","2784a805":"code","4ad5831c":"markdown"},"source":{"0cc13eea":"\nimport torch\nimport numpy as np\nimport transformers\nimport spacy\nnlp = spacy.load('en_core_web_sm')","0ab93842":"if torch.cuda.is_available():\n    device = torch.device('cuda')","659eb995":"device.type","4ab02841":"import pandas as pd\nimport os\nprint(os.listdir(\"..\/input\/training-dataset\"))","e58562df":"# Load dataset\nsentences = pd.read_csv('..\/input\/training-dataset\/cleaned_train_fina.csv')['user_review'].values\nlabels = pd.read_csv('..\/input\/training-dataset\/train.csv')['user_suggestion'].values","0727b87b":"#! wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","07cee584":"#!unzip uncased_L-12_H-768_A-12.zip","8ef6beb2":"#!  'uncased_L-12_H-768_A-12.zip' 'bert-base-uncased'","3e903f0a":"from transformers import RobertaConfig,RobertaForSequenceClassification,RobertaModel,RobertaTokenizer\n# Load tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)","6a42ff71":"config = RobertaConfig('roberta-base')","7af8c5ed":"# For an example try for single sentence\ninput_ids = tokenizer.encode(sentences[0])\n# these ids come out from pretrained bert embeddings model which we have just loaded and this represent index of unique tokens or in \n# other words it's just a index of each token to represent that particular token. NOTHING MORE THAN THAT.\n\n# But we don't only need input_ids instead we also need attention mask ids where attention mask ids are those ids which differentiate\n# padded or truncated ids with original input ids of given sentences.Let's see how attention mask ids look like but how can we get \n# those ids because encode don't contain those ids? so for that we will use it's next version encode_plus BAM!\n\nids_dictionary = tokenizer.encode_plus(sentences[0],return_attention_mask=True,return_token_type_ids=True,max_length=100,pad_to_max_length=True) # for now we are discarding padding,later i will explain you this with padding also\n","83708941":"print('input_ids',ids_dictionary['input_ids'],'\\nattention_mask',ids_dictionary['attention_mask']) # for now we didn't do padding\n                                                                                                   # but if we do padding then we can differentiate it from input_ids\n\n# So attention mask ids basically differentiating padded sequences with original input sequence by binary representation where\n# 1 means true indexes of input tokens where 0 represents padded tokens","db30e6b4":"import torch","85e03016":"# Now do this for whole training data\ninput_ids = [] # list of input_ids of all sentences\nattention_mask = [] # list of attention_mask of all sentences\nfor sent in sentences:\n    # create a dictionary which will return input_ids and attention_mask of sentences\n    # find maximum length of sentence from all sentences\n    ids_dictionary = tokenizer.encode_plus(str(sent),add_special_tokens=True,return_tensors='pt',return_attention_mask=True,return_token_type_ids=True,max_length =300 ,pad_to_max_length = True)\n    input_ids.append(ids_dictionary['input_ids'])\n    attention_mask.append(ids_dictionary['attention_mask'])\n    \n\n    \n    ","6cf509da":"from transformers import BertForSequenceClassification,AdamW,BertConfig\nclassifier = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2,output_attentions=False,output_hidden_states=False)","42ed4dca":"input_ids","3561d2a2":"input_ids = torch.cat(input_ids,dim=0)\nattention_mask = torch.cat(attention_mask,dim=0)\nlabels = torch.tensor(labels)","78dc63a9":"input_ids[:4]","97d984b2":"from torch.utils.data import TensorDataset,random_split\ndataset = TensorDataset(input_ids,attention_mask,labels)\nprint(len(dataset))\ntrain_size =int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\nprint(train_size)\ntrain_dataset,val_dataset = random_split(dataset,[int(train_size),int(val_size)])\n","13549b84":"# print model parameters and transfer model to cuda\nparams = classifier.named_parameters()\nfor param in params:\n    print('layers',param[0],'parameters_size',param[1].shape)\n    ","473fb3d3":"classifier.to(device)","935925e9":"from sklearn.metrics import f1_score\n","ee7bb1df":"# convert given dataset into tensordataset which consist of input sente\nfrom torch.utils.data import DataLoader,random_split,RandomSampler,SequentialSampler\ntrain_loader = DataLoader(train_dataset,batch_size = 32 , sampler=RandomSampler(train_dataset))\nval_loader = DataLoader(val_dataset,batch_size=32,\n                       sampler= SequentialSampler(val_dataset)) # in validation order doesn't bother\n","74424f93":"\nfrom torch.utils.data import DataLoader,RandomSampler,SequentialSampler","8790da9a":"# define optimizer\nfrom transformers import AdamW\n# here ADAMW is a optimizer with weight decaying\noptimizer = AdamW(classifier.parameters(),lr=2e-5,eps=1e-8)","7c8f2edc":"# set scheduler for learning rate, for that calculate total steps\nfrom transformers import get_linear_schedule_with_warmup\nepochs = 4 # researchers suggest to take epochs should be in range of 4-7 for fine tuning pretrained model as we have concern \n# just for last layer which is untrained classification layer.\ntotal_steps = len(train_loader) * epochs\n# scheduler take care of linear schedule of learning rate \nscheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n","326607a9":"len(train_loader)","9a7c3914":"# Start training \nepochs = 2\nimport random\n\n\n# total loss calculate for each epochs stores in this list\ntraining_stats = []\nfor epoch in range(epochs):\n    \n    total_train_loss = 0\n    classifier.train()\n    \n    for step,batch in enumerate(train_loader):\n        \n        # print out batch[0],batch[1] and label to get input_ids,attention_mask and labels\n        if step%40==0 and not step==0: # this is just to show elapsed time with number of steps\n            print('steps',step) # so here find steps for every 40 batches\n        b_input_ids = batch[0].to(device)\n        b_attention_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        # now before calculating loss, first zero all previously calculated gradients\n        classifier.zero_grad()\n        \n        # now calculate loss and logits and add loss as a loss.item\n        loss,logits = classifier(token_type_ids=None,input_ids=b_input_ids,attention_mask = b_attention_mask,labels = b_labels)\n        total_train_loss+=loss.item()\n        \n        # now backward pass your loss and then update parameters using optimizer\n        loss.backward()\n        # to prevent gradient exploding use grad_norm\n        torch.nn.utils.clip_grad_norm_(classifier.parameters(),1.0)\n        # now to optimize loss for next batch of size 32(this size is recommended by researchers)\n        optimizer.step() # here step is a function which updates model parameters which we have just passed in optimizer function\n        \n        scheduler.step()\n    average_loss_batch = total_train_loss \/ len(train_loader) # here train_loader helps to arrange data randomly or sequentially depends\n                                                              # on sampler and batch size.\n    \n    print('average_loss_batch:',average_loss_batch)\n    \n    # now evaluate model for validation dataset\n    classifier.eval()\n    # so side by side calculate validation loss, here don't need to forward pass for the inputs\n    total_val_loss = 0 \n    total_accuracy = 0\n    for step,batch in enumerate(val_loader):\n        # remember train_loader or val_loader is used to represent data in form of batches\n        bt_input_ids = batch[0].to(device)\n        bt_attention_mask = batch[1].to(device)\n        bt_labels = batch[2].to(device)\n        \n        # tell torch not to bother forward pass instead of just backprop\n        with torch.no_grad():\n            (loss,logits) = classifier(token_type_ids=None,input_ids = bt_input_ids,attention_mask = bt_attention_mask,labels = bt_labels)\n        total_val_loss += loss.item()\n        # calculate accuracy as well for val_batch examples and true labels correspondingly.\n        # detach true labels and logits and pass to cpu\n        label = bt_labels.detach().cpu().numpy()\n        predicted_labels = logits.detach().cpu().numpy()\n        \n        accuracy = f1_score(np.argmax(predicted_labels,axis=1).flatten(),label.flatten())\n        total_accuracy+= accuracy\n    # now find average loss per val_loader\n    average_val_loss_batch = total_val_loss\/len(val_loader)\n    # now find average accuracy as well \n    average_val_accuracy = total_accuracy\/len(val_loader)\n    training_stats.append({'average_loss_batch':average_loss_batch,\n                          'average_val_loss_batch':average_val_loss_batch,\n                          'average_val_accuracy':average_val_accuracy})\n    \n    \nprint('Training Complete')       ","a99183fa":"#import torch, gc\n#gc.collect()\n\n#torch.cuda.empty_cache()","c4a920a4":"print(predicted_labels)","97c22cb3":"#! nvidia-smi clear","21ddcd51":"training_stats","0a08c1cc":"# now finally we have trained model and now we just want to predict ","c78d35d0":"# now save weights of a trained model\ntorch.save(classifier,'fine-tuned-robert-SA.pth')","3f980619":"# now evaluate for testing dataset\n# read testing dataset\ntest_user_review = pd.read_csv('..\/input\/testingdataset\/cleaned_test_fina.csv')['user_review'].values\n","a62eda87":"classifier","fdfda872":"type(test_user_review[0])","35e65410":"# now prepare testing dataset i.e. in form of tensors and then load using Data Loader\n# First encode testing data as we did in case of training data\ntest_input_ids =[]\ntest_attention_mask = []\n\nfor sent in test_user_review:\n    encode_dictionary = tokenizer.encode_plus(str(sent),max_length=300,pad_to_max_length = True,return_tensors='pt', return_token_type_ids=True,return_attention_mask=True,add_special_tokens=True)\n    test_ids = encode_dictionary['input_ids']\n    attention_mask = encode_dictionary['attention_mask']\n    test_input_ids.append(test_ids)\n    test_attention_mask.append(attention_mask)\n    \n    ","c5b96f39":"# now integrate all testing dataset using TensorDataset\ntest_dataset = TensorDataset(torch.cat(test_input_ids),torch.cat(test_attention_mask)) # torch.cat function is used to concatenate all tensors \n                                                                                        # generated into one dim=0 like(4,5) -> (8,5) if there is two tensors","e35c9f97":"len(test_dataset)","d928366b":"# now load dataloader\ntest_loader = DataLoader(test_dataset,batch_size = 32,sampler= SequentialSampler(test_dataset))","d0a6a2fd":"# start evaluating model named classifier\nclassifier.eval()\n","97117877":"epochs = 1\ntotal_accuracy = 0\nfor epoch in range(epochs):\n    \n    final_predictions = []\n    classifier.eval()\n    for step,batch in enumerate(test_loader):\n        if step%40==0:\n            print(step)\n        test_input_ids = batch[0].to(device)\n        test_attention_mask = batch[1].to(device)\n        with torch.no_grad():\n            output = classifier(input_ids= test_input_ids ,token_type_ids=None, attention_mask = test_attention_mask)\n        logits = output[0].detach().cpu().numpy()\n        final_predictions.append(logits)\n        ","c6ba719f":"# then we will compare predictions with actual test labels using f1score or mcc score","6ccee123":"final_predictions","04470684":"predict = []\nfor predictions in final_predictions:\n    for pred in predictions:\n        predict.append(np.argmax(pred,axis=0).flatten())\nlen(predict)","fa18e58e":"predict = pd.DataFrame({'predictions':predict}).to_csv('predictions.csv')","665485c9":"torch.save(classifier.state_dict(),'weights-fine-tuned-robert.ckpt')","532714c4":"os.listdir('..\/output1')","06cdf917":"classifier.save_pretrained('..\/output1')","2784a805":"model = torch.load('fine-tuned-robert-SA.pth')","4ad5831c":"# Steps to be followed to perform sentimental analysis using BERT\n\n* Load pretrained Bert model \"bert-uncased model\" for small letter characters otherwise \"bert-cased model\" which is specific     for only capital letters. \n* Load BertTokenizer and BertForSentenceClassification to classify text into binary or multiple categories.\n* Load pretrained bert-uncased-model using BertforSequenceClassification class to classify reviews and then evaluate on validation dataset\n* Then get predictions for your own test dataset and calculate MCC or F1score."}}