{"cell_type":{"74f46eca":"code","40f14b4a":"code","21419441":"code","526f14fa":"code","e3259809":"code","62da3761":"code","7aec7b7f":"code","d337e8c0":"code","01baa15b":"code","d6e016ca":"code","c5eed8b7":"code","7099ffaa":"code","7102dd8e":"code","4f140eef":"code","3b555303":"code","3555aac1":"code","ca09638d":"code","16c296f4":"code","e5cd2454":"code","e7f86a01":"code","aa38b317":"code","32d50bd8":"code","551a2481":"code","1ca74b47":"code","3829fba7":"code","cf411852":"code","08db5898":"code","4b44ed2f":"code","64fa3437":"code","045a5a28":"code","f291004d":"code","0657020f":"code","1024c04e":"code","b63030a4":"code","213853bc":"code","70207def":"code","5f688a6e":"code","fe0c525f":"code","6207b067":"code","480d9b2b":"code","a6def9b8":"code","b1fafde6":"code","59bfe561":"code","c2246484":"code","45730523":"code","7335347d":"code","f64ee985":"code","a6d93f58":"code","4c1024f7":"code","2a81c1c6":"markdown","5d55bbb7":"markdown","cc4e81b3":"markdown","50eed359":"markdown","8fae57f9":"markdown","5cc655e5":"markdown","edcdf1e7":"markdown","770d015e":"markdown","8a6df0d7":"markdown","1c60682b":"markdown","fac3953f":"markdown","a42367ad":"markdown","649c015e":"markdown","b2a4e0bd":"markdown","e9b3dc5f":"markdown","02691b6b":"markdown","f54d8efc":"markdown","5cbbc2a8":"markdown","5cc318ba":"markdown","1dae7c47":"markdown","73379f14":"markdown","d195c54d":"markdown","20e8abcf":"markdown","8d445729":"markdown","6407fba6":"markdown","d725fc0e":"markdown","59078bbe":"markdown","2f190a67":"markdown"},"source":{"74f46eca":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D, SpatialDropout2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve\nfrom tensorflow.keras.preprocessing import image\nfrom PIL import Image","40f14b4a":"# Selecting Dataset Folder Paths\ndir_ = Path('..\/input\/vehicle-detection-image-set')\nfilepaths = list(dir_.glob(r'**\/*.png'))\n# Mapping the labels\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\n# Paths & labels femalee eyes\nfilepaths = pd.Series(filepaths, name = 'File').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenating...\ndf = pd.concat([filepaths, labels], axis=1)\n\ndf = df.sample(frac = 1, random_state = 56).reset_index(drop = True)","21419441":"vc = df['Label'].value_counts()\nplt.figure(figsize = (9, 5))\nsns.barplot(x = vc.index, y = vc)\nplt.title(\"Number of images for each category in the Training Dataset\", fontsize = 11)\nplt.show()","526f14fa":"plt.style.use(\"dark_background\")","e3259809":"figure = plt.figure(figsize=(2,2))\nx = plt.imread(df[\"File\"][34])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(df[\"Label\"][34])","62da3761":"figure = plt.figure(figsize=(2, 2))\nx = plt.imread(df[\"File\"][15])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(df[\"Label\"][15])","7aec7b7f":"fig, axes = plt.subplots(nrows = 5,\n                        ncols = 5,\n                        figsize = (7, 7),\n                        subplot_kw = {\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df[\"File\"][i]))\n    ax.set_title(df[\"Label\"][i])\nplt.tight_layout()\nplt.show()","d337e8c0":"trainset_df, testset_df = train_test_split(df, train_size = 0.90, random_state = 42)\n\ndisplay(trainset_df.head())\n\ntestset_df.head()","01baa15b":"# converting the Label to a numeric format for testing later...\nLE = LabelEncoder()\n\ny_test = LE.fit_transform(testset_df[\"Label\"])","d6e016ca":"# Viewing data in training dataset\nprint('Training Dataset:')\n\nprint(f'Number of images: {trainset_df.shape[0]}')\n\nprint(f'Number of images with malee eyes: {trainset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with femalee eyes: {trainset_df[\"Label\"].value_counts()[1]}\\n')\n\n# Viewing data in test dataset\nprint('Test Dataset:')\n\nprint(f'Number of images: {testset_df.shape[0]}')\n\nprint(f'Number of images with malee eyes: {testset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with femalee eyes: {testset_df[\"Label\"].value_counts()[1]}\\n')","c5eed8b7":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                    shear_range = 0.2,\n                                    zoom_range = 0.1,\n                                    rotation_range = 20,\n                                    width_shift_range = 0.1,\n                                    height_shift_range = 0.1,\n                                    horizontal_flip = True,\n                                    vertical_flip = True,\n                                    validation_split = 0.1)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","7099ffaa":"print(\"Preparing the training dataset ...\")\ntraining_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (75, 75),\n    color_mode = \"rgb\",\n    class_mode = \"binary\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"training\")\n\nprint(\"Preparing the validation dataset ...\")\nvalidation_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (75, 75),\n    color_mode =\"rgb\",\n    class_mode = \"binary\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"validation\")\n\nprint(\"Preparing the test dataset ...\")\ntest_set = test_datagen.flow_from_dataframe(\n    dataframe = testset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (75, 75),\n    color_mode =\"rgb\",\n    class_mode = \"binary\",\n    shuffle = False,\n    batch_size = 32)\n\nprint('Data generators are ready!')","7102dd8e":"# Callbacks\ncb = [EarlyStopping(monitor = 'loss', mode = 'min', patience = 5, restore_best_weights = True)]","4f140eef":"CNN_base_inc = InceptionV3(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')","3b555303":"for layer in CNN_base_inc.layers:\n    layer.trainable = False","3555aac1":"x = layers.Flatten()(CNN_base_inc.output)","ca09638d":"x = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\n\nCNN_inc = Model(CNN_base_inc.input, x)","16c296f4":"# Compilation\nCNN_inc.compile(optimizer = RMSprop(lr = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_inc_history = CNN_inc.fit(training_set, epochs = 25, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_inc = end - start\nprint ('\\nTraining and validation time is: ', time_CNN_inc)","e5cd2454":"acc = CNN_inc_history.history['accuracy']\nval_acc = CNN_inc_history.history['val_accuracy']\nloss = CNN_inc_history.history['loss']\nval_loss = CNN_inc_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","e7f86a01":"score_inc = CNN_inc.evaluate(test_set)\nprint(\"Test Loss:\", score_inc[0])\nprint(\"Test Accuracy:\", score_inc[1])","aa38b317":"y_pred_inc = CNN_inc.predict(test_set)\ny_pred_inc = np.round(y_pred_inc)\n\nrecall_inc = recall_score(y_test, y_pred_inc)\nprecision_inc = precision_score(y_test, y_pred_inc)\nf1_inc = f1_score(y_test, y_pred_inc)\nroc_inc = roc_auc_score(y_test, y_pred_inc)","32d50bd8":"print(classification_report(y_test, y_pred_inc))","551a2481":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_inc),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","1ca74b47":"CNN_base_xcep = Xception(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')\nCNN_base_xcep.trainable = False","3829fba7":"CNN_xcep = Sequential()\nCNN_xcep.add(CNN_base_xcep)\nCNN_xcep.add(GlobalAveragePooling2D())\nCNN_xcep.add(Dense(128))\nCNN_xcep.add(Dropout(0.1))\nCNN_xcep.add(Dense(1, activation = 'sigmoid'))\n\nCNN_xcep.summary()","cf411852":"plot_model(CNN_xcep, show_layer_names = True , show_shapes = True)","08db5898":"# Compilation\nCNN_xcep.compile(optimizer='adam', loss = 'binary_crossentropy',metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_xcep_history = CNN_xcep.fit(training_set, epochs = 25, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_xcep = end - start\nprint ('\\nTraining and validation time: ', time_CNN_xcep)","4b44ed2f":"acc = CNN_xcep_history.history['accuracy']\nval_acc = CNN_xcep_history.history['val_accuracy']\nloss = CNN_xcep_history.history['loss']\nval_loss = CNN_xcep_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","64fa3437":"score_xcep = CNN_xcep.evaluate(test_set)\nprint(\"Test Loss:\", score_xcep[0])\nprint(\"Test Accuracy:\", score_xcep[1])","045a5a28":"y_pred_xcep = CNN_xcep.predict(test_set)\ny_pred_xcep = np.round(y_pred_xcep)\n\nrecall_xcep = recall_score(y_test, y_pred_xcep)\nprecision_xcep = precision_score(y_test, y_pred_xcep)\nf1_xcep = f1_score(y_test, y_pred_xcep)\nroc_xcep = roc_auc_score(y_test, y_pred_xcep)","f291004d":"print(classification_report(y_test, y_pred_xcep))","0657020f":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_xcep),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","1024c04e":"CNN_base_mobilenet = MobileNet(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')","b63030a4":"for layer in CNN_base_mobilenet.layers:\n    layer.trainable = False","213853bc":"CNN_mobilenet = Sequential()\nCNN_mobilenet.add(BatchNormalization(input_shape = (75, 75, 3)))\nCNN_mobilenet.add(CNN_base_mobilenet)\nCNN_mobilenet.add(BatchNormalization())\nCNN_mobilenet.add(GlobalAveragePooling2D())\nCNN_mobilenet.add(Dropout(0.5))\nCNN_mobilenet.add(Dense(1, activation = 'sigmoid'))\n\nCNN_mobilenet.summary()","70207def":"plot_model(CNN_mobilenet, show_layer_names = True , show_shapes = True)","5f688a6e":"# Compilation\nCNN_mobilenet.compile(optimizer='adam',loss = 'binary_crossentropy', metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_mobilenet_history = CNN_mobilenet.fit(training_set, epochs = 25, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_mobilenet = end - start\nprint ('\\nTraining and validation time: ', time_CNN_mobilenet)","fe0c525f":"acc = CNN_mobilenet_history.history['accuracy']\nval_acc = CNN_mobilenet_history.history['val_accuracy']\nloss = CNN_mobilenet_history.history['loss']\nval_loss = CNN_mobilenet_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","6207b067":"score_mn = CNN_mobilenet.evaluate(test_set)\nprint(\"Test Loss:\", score_mn[0])\nprint(\"Test Accuracy:\", score_mn[1])","480d9b2b":"y_pred_mn = CNN_mobilenet.predict(test_set)\ny_pred_mn = np.round(y_pred_mn)\n\nrecall_mn = recall_score(y_test, y_pred_mn)\nprecision_mn = precision_score(y_test, y_pred_mn)\nf1_mn = f1_score(y_test, y_pred_mn)\nroc_mn = roc_auc_score(y_test, y_pred_mn)","a6def9b8":"print(classification_report(y_test, y_pred_mn))","b1fafde6":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_mn),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","59bfe561":"models= [('Inception', time_CNN_inc, np.mean(CNN_inc_history.history['accuracy']), np.mean(CNN_inc_history.history['val_accuracy'])),\n         ('Xception', time_CNN_xcep, np.mean(CNN_xcep_history.history['accuracy']), np.mean(CNN_xcep_history.history['val_accuracy'])),\n         ('MobileNet', time_CNN_mobilenet, np.mean(CNN_mobilenet_history.history['accuracy']), np.mean(CNN_mobilenet_history.history['val_accuracy']))]\n\ndf_all_models = pd.DataFrame(models, columns = ['Model', 'Time', 'Training accuracy (%)', 'Validation Accuracy (%)'])\n\ndf_all_models","c2246484":"models = [('Inception', score_inc[1], recall_inc, precision_inc, f1_inc, roc_inc),\n          ('Xception', score_xcep[1], recall_xcep, precision_xcep, f1_xcep, roc_xcep),\n          ('MobileNet', score_mn[1], recall_mn, precision_mn, f1_mn, roc_mn)]\n\ndf_all_models_testset = pd.DataFrame(models, columns = ['Model', 'Test accuracy (%)', 'Recall (%)', 'Precision (%)', 'F1 (%)', 'AUC'])\n\ndf_all_models_testset","45730523":"plt.subplots(figsize=(12, 10))\nsns.barplot(y = df_all_models_testset['Test accuracy (%)'], x = df_all_models_testset['Model'], palette = 'icefire')\nplt.xlabel(\"Models\")\nplt.title('Accuracy')\nplt.show()","7335347d":"r_probs = [0 for _ in range(len(y_test))]\nr_auc = roc_auc_score(y_test, r_probs)\nr_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n\nfpr_inc, tpr_inc, _ = roc_curve(y_test, y_pred_inc)\nfpr_xcep, tpr_xcep, _ = roc_curve(y_test, y_pred_xcep)\nfpr_mn, tpr_mn, _ = roc_curve(y_test, y_pred_mn)","f64ee985":"sns.set_style('darkgrid')\n\nplt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)\n\nplt.plot(fpr_inc, tpr_inc, marker='.', label='Inception (AUROC = %0.3f)' % roc_inc)\nplt.plot(fpr_xcep, tpr_xcep, marker='.', label='Xception (AUROC = %0.3f)' % roc_xcep)\nplt.plot(fpr_mn, tpr_mn, marker='.', label='MobileNet (AUROC = %0.3f)' % roc_mn)\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend() \nplt.show()","a6d93f58":"test_set.class_indices","4c1024f7":"plt.style.use(\"dark_background\")\n\n\nfig, axes = plt.subplots(nrows = 4,\n                         ncols = 4,\n                         figsize = (15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(testset_df[\"File\"].iloc[i]))\n    ax.set_title(f\"True: {testset_df.Label.iloc[i]}\\nInception: {y_pred_inc[i]}\\nXception: {y_pred_xcep[i]}\\nMobileNet: {y_pred_mn[i]}\")\nplt.tight_layout()\nplt.show()","2a81c1c6":"## 3. Observing the images","5d55bbb7":"###### Step 8 - Viewing results and generating forecasts","cc4e81b3":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","50eed359":"## 1. Imports from libraries","8fae57f9":"## 5. Directory of training, validation and test images\n\nHere we make the division of the image bases for training, validation and testing of the model, for that we use the [flow_from_dataframe](https:\/\/keras.io\/api\/preprocessing\/image\/#flowfromdataframe-method)\n\nParameters of ``flow_from_directory``:\n\n    dataframe - Dataframe containing the images directory\n    x_col - Column name containing the images directory\n    y_col - Name of the column containing what we want to predict\n    target_size - size of the images (remembering that it must be the same size as the input layer)\n    color_mode - RGB color standard\n    class_mode - binary class mode (cat\/dog)\n    batch_size - batch size (32)\n    shuffle - Shuffle the data\n    seed - optional random seed for the shuffle\n    subset - Subset of data being training and validation (only used if using validation_split in ImageDataGenerator)","5cc655e5":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","edcdf1e7":"## Detection of images containing vehicles or not using Convolutional Neural Networks\n\n\n#### Dataset information:\n\n- The data was collected to train a model to distinguish between images containing vehicles and images without vehicles, so the whole problem is binary classification.\n\n\nThe data is divided into 2 folders:\n- The folder `` non-vehicles`` contains 8968 images and the folder `` vehicles`` contains 8792 images for training and testing the model.\n\nThe dataset can be found on the `` Kaggle`` platform at the link below:\n\n- https:\/\/www.kaggle.com\/brsdincer\/vehicle-detection-image-set","770d015e":"## 4. Generating batches of images\nIn this part we will generate batches of images increasing the training data, for the test database we will just normalize the data using [ImageDataGenerator](https:\/\/keras.io\/api\/preprocessing\/image\/#imagedatagenerator-class)\n\nParameters of ``ImageDataGenerator``:\n\n    rescale - Transform image size (normalization of data)\n    shear_range - Random geometric transformations\n    zoom_range - Images that will be zoomed\n    rotation_range - Degree of image rotation\n    width_shift_range - Image Width Change Range\n    height_shift_range - Image height change range\n    horizontal_flip - Rotate images horizontally\n    vertical_flip - Rotate images vertically\n    validation_split - Images that have been reserved for validation (0-1)","8a6df0d7":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","1c60682b":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","fac3953f":"## 3. Dividing into training and testing sets\nNow we need to convert our data into training and testing sets. We will use 90% of the images as our training data and test our model on the remaining 10% with Scikit-learn's train_test_split function.","a42367ad":"## 2. Organizing Training and Testing Dataframes","649c015e":"Use of callbacks to monitor models and see if metrics will improve, otherwise training is stopped.\n\n``EarlyStopping`` parameters:\n\n    monitor - Metrics that will be monitored\n    patience - Number of times without improvement in the model, after these times the training is stopped\n    restore_best_weights - Restores best weights if training is interrupted","b2a4e0bd":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (75, 75, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","e9b3dc5f":"## 9. Viewing the results of all models","02691b6b":"###### Step 8 - Viewing results and generating forecasts","f54d8efc":"## 6. Construction of the first model (Inception)\nThe [InceptionV3](https:\/\/keras.io\/api\/applications\/inceptionv3\/) model proposed by Szegedy et al. (2015), is a CNN architecture that seeks to solve several large-scale image recognition problems and can also be used in transfer learning problems. Its differential is the presence of convolutional characteristics extractor modules. These modules have the functionality to learn with fewer parameters that contain a greater range of information.\n\n<p><img src = \"https:\/\/cloud.google.com\/tpu\/docs\/images\/inceptionv3onc--oview.png?hl=pt-br\" alt><\/p>","5cbbc2a8":"###### Step 1 - Base model creation\n    input_shape - Setting the height\/width and RGB channels (75, 75, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","5cc318ba":"###### Step 6 - Viewing results and generating forecasts","1dae7c47":"## 8. Construction of the third model (MobileNet)\nThe MobileNet model proposed by Howard et al. (2017), is a CNN architecture that were created to perform computer vision tasks on mobile devices and embedded systems. They are based on in-depth separable convolution operations, which lessens the burden of operations in the first layers.\n\n<p><img src = \"https:\/\/nitheshsinghsanjay.github.io\/images\/mobtiny_fig.PNG\" alt><\/p>","73379f14":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","d195c54d":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","20e8abcf":"## 7. Construction of the second model (Xception)\nThe [Xception](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/Xception) model proposed by Chollet et al.(2016), is a CNN architecture similar to the Inception described above and, has the difference that the initiation modules were replaced by separable convolutions in depth. Xception has the same amount of parameters as InceptionV3 with a total of 36 convolutional layers. Thus, having a more efficient use of parameters.\n\n<p><img src = \"https:\/\/miro.medium.com\/max\/1688\/1*J8dborzVBRBupJfvR7YhuA.png\" alt><\/p>","8d445729":"###### Step 3 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","6407fba6":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","d725fc0e":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","59078bbe":"###### Step 2 - Flattening\n    Transforming the matrix to a vector to enter the Artificial Neural Network layer","2f190a67":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (75, 75, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet"}}