{"cell_type":{"fbee6b02":"code","228f3710":"code","7ac6c8b2":"code","3eafdf65":"code","c73e6410":"code","5ca36f0d":"code","3d994b35":"code","59796ecc":"code","2955c661":"code","7aa15a85":"code","0949f43e":"code","6ac690bd":"code","1c7bbd1b":"code","9b66d1fb":"code","911b5a94":"code","0e159eec":"code","259a605f":"code","39bae9fd":"code","dd4b4d0c":"code","0d33bb83":"code","49f55163":"code","5c63839e":"markdown","fdfb4524":"markdown","121df736":"markdown","de593578":"markdown","e60530ce":"markdown","af4bcf7e":"markdown","1196b506":"markdown","b90bb959":"markdown","f2f108aa":"markdown","d1377bc5":"markdown","f4bf308e":"markdown","f3469e5a":"markdown"},"source":{"fbee6b02":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nsns.set(style=\"ticks\", palette=\"pastel\")","228f3710":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain = train.drop('Id', axis = 1)\n\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nids_test = test['Id']\n\ntest = test.drop('Id', axis = 1)","7ac6c8b2":"correlation_target = train.corr()['SalePrice']\npd.DataFrame(correlation_target[np.abs(correlation_target) > 0.6].sort_values(ascending = False))","3eafdf65":"attributes = pd.DataFrame(correlation_target[np.abs(correlation_target) > 0.6].sort_values(ascending = False)).index.tolist()\n\ncorrelation_attributes = train.corr()[attributes]\n\ngrid_kws = {\"width_ratios\": (.9, .05), \"wspace\": 0.2}\nf, (ax1, cbar_ax) = plt.subplots(1, 2, gridspec_kw=grid_kws, figsize = (9, 9))\n\ncmap = sns.diverging_palette(220, 8, as_cmap=True)\n\nax1 = sns.heatmap(correlation_attributes, vmin = -1, vmax = 1, cmap = cmap, ax = ax1, square = False, linewidths = 0.5, yticklabels = True, \\\n    cbar_ax = cbar_ax, cbar_kws={'orientation': 'vertical', \\\n                                 'ticks': [-1, -0.5, 0, 0.5, 1]})\nax1.set_xticklabels(ax1.get_xticklabels(), size = 10); \nax1.set_title('Correlation Heatmap', size = 15);\ncbar_ax.set_yticklabels(cbar_ax.get_yticklabels(), size = 12);","c73e6410":"#As we know, MSubClass is not a number, it is a class. Let's transform it\ntrain['MSSubClass'] = train['MSSubClass'].astype(object)\ntest['MSSubClass'] = test['MSSubClass'].astype(object)\n\ndtypes_ = pd.DataFrame(train.dtypes)\n\nprint('We have {:.0f} different categorical features'.format(dtypes_[dtypes_[0] == 'object'].count().iloc[0]))","5ca36f0d":"f, axes = plt.subplots(44, 1, figsize=(15,250))\n\ncounter_for_axes = 0\n\nfor item in dtypes_[dtypes_[0] == 'object'][0].index.tolist():\n    sns.boxplot(y='SalePrice', x=item, data=train,  orient='v' , ax=axes[counter_for_axes])\n    counter_for_axes = counter_for_axes + 1","3d994b35":"missing_ = pd.DataFrame(train.isna().sum()\/len(train)).sort_values(0, ascending = False)\nmissing_values = missing_[missing_[0] > 0]\n\nplt.figure(figsize=(15,5))\nplt.bar(range(len(missing_values)),missing_values[0])\nplt.xticks(range(len(missing_values)), missing_values.index, rotation = 'vertical')\nplt.title('Missing Values %')\n\nmissing_values.index.tolist()","59796ecc":"from sklearn.pipeline import make_pipeline, FeatureUnion, Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n#Here I will make a class to select some columns according to their types\nclass SelectType(BaseEstimator, TransformerMixin):\n    def __init__(self, dtype):\n        self.dtype = dtype\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        return X.select_dtypes(include = [self.dtype])","2955c661":"#Here I am selecting the right columns\n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n\n        try:\n            return X[self.columns]\n        except KeyError:\n            cols_error = list(set(self.columns) - set(X.columns))\n            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)","7aa15a85":"#Here I am applying the pipeline and testing the first model, the ridge one.\n\nfrom sklearn.pipeline import make_pipeline\n\npreprocessing_pipeline = make_pipeline(\n    ColumnSelector(columns=['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley',\n       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n       'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n       'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal',\n       'MoSold', 'YrSold', 'SaleType', 'SaleCondition']),\n    FeatureUnion(transformer_list = [\n    ('Numbers', make_pipeline(\n        SelectType(np.number), SimpleImputer(strategy='constant', fill_value = 0), StandardScaler())),\n    ('Object', make_pipeline(\n        SelectType('object'), SimpleImputer(strategy='constant', fill_value = 'No'), OneHotEncoder(handle_unknown=\"ignore\")))\n])\n)\n\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nclassifier_pipeline_ridge = make_pipeline(preprocessing_pipeline,\n                                    Ridge()\n)\n\nparam_grid = {\"ridge__alpha\": [0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 100]}\n\nX = train.iloc[:,:79]\ny = train.iloc[:,79:]\n\nclassifier_model_ridge = GridSearchCV(classifier_pipeline_ridge, param_grid, cv=5, scoring = 'neg_mean_squared_error')\nclassifier_model_ridge.fit(X,y)","0949f43e":"#Let's go for the Lasso\n\nfrom sklearn.linear_model import Lasso\n\nclassifier_pipeline_lasso = make_pipeline(preprocessing_pipeline,\n                                      Lasso())\n\nparam_grid = {\"lasso__alpha\": [0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200]}\n\nclassifier_model_lasso = GridSearchCV(classifier_pipeline_lasso, param_grid, cv=5, scoring = 'neg_mean_squared_error')\nclassifier_model_lasso.fit(X,y)\nclassifier_model_lasso.best_score_","6ac690bd":"#Now its RF time\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nclassifier_pipeline_rf = make_pipeline(preprocessing_pipeline,\n                                      RandomForestRegressor())\n\nparam_grid = {\"randomforestregressor__max_depth\": [2,3,4]}\n\nclassifier_model_rf = GridSearchCV(classifier_pipeline_rf, param_grid, cv=5, scoring = 'neg_mean_squared_error')\nclassifier_model_rf.fit(X,y)\nclassifier_model_rf.best_score_","1c7bbd1b":"#And Kernel Ridge\n\nfrom sklearn.kernel_ridge import KernelRidge\n\nclassifier_pipeline_krr = make_pipeline(preprocessing_pipeline,\n                                      KernelRidge())\n\nparam_grid = {\"kernelridge__alpha\": [0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 100]}\n\nclassifier_model_krr = GridSearchCV(classifier_pipeline_krr, param_grid, cv=5, scoring = 'neg_mean_squared_error')\nclassifier_model_krr.fit(X,y)\nclassifier_model_krr.best_score_\n","9b66d1fb":"#Taking a look at the scores\n\nkrr = {'model':'krr','result':classifier_model_krr.best_score_}\nridge = {'model':'ridge','result':classifier_model_ridge.best_score_}\nlasso = {'model':'lasso','result':classifier_model_lasso.best_score_}\nrandom_forest = {'model':'random_forest','result':classifier_model_rf.best_score_}\n\nresult = pd.DataFrame([krr,ridge,lasso,random_forest])\nresult.sort_values('result', ascending = False)","911b5a94":"#It looks we are going for the lasso.\n\npd.DataFrame(classifier_model_lasso.cv_results_)","0e159eec":"#Getting ready\nclassifier_pipeline_lasso_2 = make_pipeline(preprocessing_pipeline,\n                                      Lasso(alpha = 200))","259a605f":"#Fitting it\nclassifier_pipeline_lasso_2.fit(X,y)","39bae9fd":"#Going for the test\nans = pd.DataFrame(classifier_pipeline_lasso_2.predict(test))","dd4b4d0c":"#Just making it according to the sample\nans['Id'] = ids_test\nans2 = ans.set_index('Id')","0d33bb83":"#Final adjustments\nans2.rename(columns={0:'SalePrice'}, inplace = True)","49f55163":"#Done!\nans2","5c63839e":"# Datifying House Prices\nHey folks! Our goal here is to predict the price of some houses around Ames, Iowa. To do it so, I will follow some steps recommended by \"Introduction to Machine Learning with Python\", by Andreas C. Muller & Sarah Guido. The book provived my a great initial vision of data science and I would definitely recommend it!\nPlease, feel free to comment and criticize this notebook! I would love to hear from you guys, as I am just starting around ML projects and this is my first notebook :)","fdfb4524":"From now, let's just create the Pipeline to run cross-validations and select the best model:","121df736":"## 4. Prepare the Data and Running the Models\nFirst of all, we are taking a look at missing values and understanding each way","de593578":"After taking a look at the correlation of these, we can notice that there are a lot of red stuff around. It means that there are some relations like:\n* Simple internal design options (like screen\/enclosed\/three porch, miscellaneous stuff, kitcken area, low quality finished square feet) does not look to hard impact, as it could be easily remodelled if needed. In the other hand, external stuff (like the open porch, lot frontage, wood decks, masonry veneer) would be a bit harder to 'construct' it and it seems like having higher impact.\n* Overall Conditions does not look that relevant and it was a surprise for me. There could be several reasons, like the criteria was not well trained so it would not make sense. This idea would make some sense especially because there is a great relation between YearBuilt and the SalePrice, so we can understand that new houses tend to be in better conditions. However,  as we are talking about a lot of money invested (ye, we are buying a house), it would be fine to invest some money to make it better. This would be way cheaper than the 'heavy' part as we talked above.","e60530ce":"## 3.  Explore the data\nI was really wondering how I would do this systematically through this dataset, and I thought it would be the ideal: first going for the numeric data, then the caterogical data and finish with the target.","af4bcf7e":"### X.X Categorical Data\nGreat. For now I'd like to study a bit the distribution of the sale price among the different cattegories. I know it is a lot, but I understood that this experience really made me feel comfy with the data and I really believe it worths.","1196b506":"* For MiscFeatures, Alley, Fence, Fireplace, PoolQC: the lack of information means there is not something of this feature in the house\n* LotFrontage made me think about two situations: just missing the info or not having it. I will go with the second option. We will put it as 0.\n* The garage missing values are the same for all the features. It is probably telling us there is no garage. The GarageCond, GarageType, GarageFinish and GarageQual will take the same destiny as the first group we said. I will take YearBuilt as 0.\n* Same for basement info and for MasVnr\n* Electrical are boolean, so we will go with most frequent values","b90bb959":"Main takeaways for our initial exploration:\n* We have found several outliers through different features, and it really made me think about this for the next exploration\n* The general zoning and the neighbourhood are features I was really expecting differences, and I am satisfied from what I see.\n* As we saw in in the previous topic, external structure seems to show some difference, like paved or graved alley acess to propriety, the house style, the roof, so we are running in the same direction.\n* Until now, we were looking especially for big and bold structural features as relevant, however the central air, heather and fireplace showed us some potential. The common thing is that those are features that would not be that 'easy' to install in a house.\n* Some features are related to the same classes, but just separated due to different combinations (like Condition 1 and Condition 2). We will have to treat this after.\n* This is the very first moment we are talking about a commercial feature: the sale type and condition seem to be relevant for this decision also.\n","f2f108aa":"### 3.1 Numeric Data\nSo here i went for the direct correlations. I got the highest scores atm to understand a bit how it'd make sense. Then, I'd get those features and understand a bit how they relate to all the others. My goal here is to understand the data and get familiar with it.","d1377bc5":"* Taking a look at the documentation, Overall Quality is the big shot and it is related to the material and finish of the house. It is such an interesting thing for me, specially because I'd say the feet square area would be a better shot for the first position.\n* Also, checking the above grade (ground) living area seems important - and it seems perfectly normal.\n* The garage cars and garage area look like the same information, as we can think that largers areas can fit more cars. It makes us think that maybe we have similar information between different features\n* After this, we are talking about the basement and the squaree feet of the first floor - at it talks a lot about the living area. Again, it makes me guess (at the moment) that a bit of features can nicely answer our questions","f4bf308e":"## 1. Framing the problem\nOur goal, as we said, is to predict the value of  houses based on a range of features. The solution can be used as part of the analysis process for real estate investments, as example.\nI understand this challenge as a supervised problem, which will be solved as a batch learning system.\nBy now, the sucessfull measurement will be the RMSE.","f3469e5a":"## 2. Get the data"}}