{"cell_type":{"7edad608":"code","fa7731b3":"code","7a07dc62":"code","25288584":"code","b1b51c94":"code","28dca59a":"code","06527b0b":"code","c40ec752":"code","c1d455e5":"code","2d0ab62b":"code","04f1886a":"code","de7f0088":"code","d7c2faa6":"code","49aa4f55":"code","39921fe9":"code","c8d5339e":"code","07caf64d":"code","c8fba77f":"code","5177dbff":"code","49a1246d":"code","4b905bc5":"code","ae294ff8":"code","af5ba138":"code","96b14859":"code","c6d26c3a":"code","9cb860a6":"code","cb3cfcea":"code","a1b4e058":"code","03ee777a":"code","451e31fd":"code","29cc4fc5":"code","57a4c190":"code","7e7d98a6":"code","961c8875":"code","41af2ec6":"code","177427cb":"code","6927e931":"code","50759d37":"code","442b98e5":"code","047e000c":"code","a33fbede":"code","1163b2a8":"markdown","e3a8bc98":"markdown","6d080885":"markdown","a2222056":"markdown","c68e9f4c":"markdown","b7c2bcd8":"markdown","e3356ef6":"markdown","d53c3617":"markdown","596adf75":"markdown","b504d390":"markdown","7b911793":"markdown","0f74bb45":"markdown","3bac91d3":"markdown","536b9388":"markdown","aa5b640a":"markdown","8fbb53be":"markdown","757623bb":"markdown","55e609f5":"markdown"},"source":{"7edad608":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa7731b3":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","7a07dc62":"df.info()","25288584":"df.describe()","b1b51c94":"for i in df.columns:\n\n    sns.boxplot(df[i])\n    plt.show()","28dca59a":"for i in df.columns:\n    for each in df[i].values:\n        \n        if each > 1 or each < -1:\n            df[i] = (df[i] - np.min(df[i]))\/(np.max(df[i]) - np.min(df[i]))\n        else:\n            pass","06527b0b":"y = df[\"target\"].values\nx = df.drop([\"target\"],axis = 1)","c40ec752":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)","c1d455e5":"lr = LogisticRegression(random_state = 42).fit(x_train,y_train)\nlr.score(x_test,y_test)","2d0ab62b":"# GridSearchCV\nparams = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nlr_model = LogisticRegression(random_state = 42)\nlr_cv = GridSearchCV(lr_model,params,cv = 3).fit(x_train,y_train)\nlr_cv.best_params_","04f1886a":"lr_tuned = LogisticRegression(C = 1,random_state = 42).fit(x_train,y_train)\nlr_tuned.score(x_test,y_test)","de7f0088":"y_pred_log = lr_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_log),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","d7c2faa6":"knn = KNeighborsClassifier(n_neighbors = 2).fit(x_train,y_train)\nknn.score(x_test,y_test)","49aa4f55":"params = {\"n_neighbors\": range(1,50)}\nknn_model = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn_model, params, cv = 10, n_jobs = -1).fit(x_train,y_train)\nknn_cv.best_params_","39921fe9":"knn_tuned = KNeighborsClassifier(n_neighbors = 7).fit(x_train,y_train)\nknn_tuned.score(x_test,y_test)","c8d5339e":"y_pred_knn = knn_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_knn),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","07caf64d":"tree = DecisionTreeClassifier(random_state = 42).fit(x_train,y_train)\ntree.score(x_test,y_test)","c8fba77f":"params = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50))}\ntree_model = DecisionTreeClassifier(random_state = 42)\ntree_cv = GridSearchCV(tree_model, params, cv = 10, n_jobs = -1).fit(x_train,y_train)\ntree_cv.best_params_","5177dbff":"tree_tuned = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 7, min_samples_split = 2,random_state = 42).fit(x_train,y_train)\ntree_tuned.score(x_test,y_test)","49a1246d":"y_pred_tree = tree_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_tree),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","4b905bc5":"rf = RandomForestClassifier(random_state = 42).fit(x_train,y_train)\nrf.score(x_test,y_test)","ae294ff8":"params = {\"max_depth\": range(1,15),\n         \"min_samples_split\" : [2,5,6,7,10],\n         \"min_samples_leaf\": [2,5,7,8],\n         \"max_features\" : [2,3,5,7,9,12],\n         }\nrf_model = DecisionTreeClassifier(random_state = 42)\nrf_cv = GridSearchCV(rf_model, params, cv = 10, n_jobs = -1).fit(x_train,y_train)\nrf_cv.best_params_","af5ba138":"rf_tuned = RandomForestClassifier(max_depth = 5, max_features  = 9, min_samples_leaf = 8, min_samples_split = 2,random_state = 42).fit(x_train,y_train)\nrf_tuned.score(x_test,y_test)","96b14859":"y_pred_rf = rf_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_rf),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","c6d26c3a":"svm = SVC(random_state = 42).fit(x_train,y_train)\nsvm.score(x_test,y_test)","9cb860a6":"params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\nsvm_model = SVC(random_state = 42)\nsvm_cv = GridSearchCV(svm_model,params,cv = 10,n_jobs = -1).fit(x_train,y_train)\nsvm_cv.best_params_","cb3cfcea":"svm_tuned = SVC(C = 10, degree = 1, gamma = 0.1, random_state = 42).fit(x_train,y_train)\nsvm_tuned.score(x_test,y_test)","a1b4e058":"y_pred_svm = svm_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_svm),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","03ee777a":"gbm = GradientBoostingClassifier(random_state = 42).fit(x_train,y_train)\ngbm.score(x_test,y_test)","451e31fd":"gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,1000],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\ngbm_model = GradientBoostingClassifier(random_state = 42)\ngbm_cv = GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1).fit(x_train,y_train)\ngbm_cv.best_params_","29cc4fc5":"gbm_tuned = GradientBoostingClassifier(max_depth = 2, learning_rate = 0.01, min_samples_split = 45, n_estimators = 100, random_state = 42).fit(x_train,y_train)\ngbm_tuned.score(x_test,y_test)","57a4c190":"y_pred_gbm = gbm_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_gbm),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","7e7d98a6":"ada = AdaBoostClassifier(random_state = 42).fit(x_train,y_train)\nada.score(x_test,y_test)","961c8875":"params = {\"n_estimators\":[10,100,200,300,500,1000],\"learning_rate\":[0.0001,0.001,0.01,0.1,0.2,0.3,0.7]}\nada_model = AdaBoostClassifier(random_state = 42)\nada_cv = GridSearchCV(ada_model,params,cv = 10,n_jobs = -1).fit(x_train,y_train)\nada_cv.best_params_","41af2ec6":"ada_tuned = AdaBoostClassifier(learning_rate = 0.01,n_estimators = 1000,random_state = 42).fit(x_train,y_train)\nada_tuned.score(x_test,y_test)","177427cb":"y_pred_ada = ada_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_ada),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","6927e931":"bag = BaggingClassifier(random_state = 42).fit(x_train,y_train)\nbag.score(x_test,y_test)","50759d37":"params = {\"n_estimators\": range(1,50)}\nbag_model = BaggingClassifier(random_state = 42)\nbag_cv = GridSearchCV(bag_model, params, cv = 10, n_jobs = -1).fit(x_train,y_train)\nbag_cv.best_params_","442b98e5":"bag_tuned = BaggingClassifier(n_estimators = 45,random_state = 42).fit(x_train,y_train)\nbag_tuned.score(x_test,y_test)","047e000c":"y_pred_bag = bag_tuned.predict(x_test)\nsns.heatmap(confusion_matrix(y_test,y_pred_bag),annot = True)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_test\")\nplt.show()","a33fbede":"pred_list = [lr_tuned,knn_tuned,tree_tuned,rf_tuned,gbm_tuned,svm_tuned,ada_tuned,bag_tuned]\n\nfor i in pred_list:\n    print(\"Score : \",i.score(x_test,y_test))\n    y_pred = i.predict(x_test)\n    sns.heatmap(confusion_matrix(y_test,y_pred),annot = True)\n    plt.xlabel(\"Y_pred\")\n    plt.ylabel(\"Y_test\")\n    plt.title(i)\n    plt.show()","1163b2a8":"<a id = \"3\"><\/a><br>\n# Determine X and Y (Train-test split)","e3a8bc98":"Briefly we seleceted to GBM and SVM. They scores are 88.5","6d080885":"<a id = \"7\"><\/a><br>\n## Decision Tree","a2222056":"<a id = \"12\"><\/a><br>\n## Bagging","c68e9f4c":"# Introduction\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to\nthis date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n<font color=\"blue\">\nWhat will we do?\n\n1. [Load and Check of Dataset](#1)\n1. [Normalization](#2)\n1. [Determine X and Y (Train-test split)](#3) \n1. [Machine Learning (Classification)](#4)\n    * [Logistic Regression](#5)\n    * [KNN (K Neighbors)](#6)\n    * [Decision Tree](#7)\n    * [Random Forest](#8)\n    * [SVM](#9)\n    * [GBM](#10)\n    * [Adaboost](#11)\n    * [Bagging](#12)\n1. [Compare Algorithms](#13)\n","b7c2bcd8":"If we want to watch easily road, we can use Label Encoder. I wanted to show how we do in for loop.","e3356ef6":"<a id = \"9\"><\/a><br>\n## SVM","d53c3617":"<a id = \"10\"><\/a><br>\n## GBM","596adf75":"<a id = \"8\"><\/a><br>\n## Random Forest","b504d390":"<a id = \"11\"><\/a><br>\n## Adaboost","7b911793":"I prefered GridSearch because it's easy for me. That's selection is yours.","0f74bb45":"<a id = \"1\"><\/a><br>\n# Load and Check of Dataset","3bac91d3":"<a id = \"4\"><\/a><br>\n# Machine Learning (Classification)\n\n1. Logistic Regression\n1. KNN(K Neighbors)\n1. Decision Tree\n1. Random Forests\n1. SVM\n1. GBM (Gradient Boosting Machine)\n1. AdaBoost\n1. Bagging","536b9388":"We have checked the dataset and, we didn't see any blank data. Some data has outliers but, they can be ignored. Why didn't we see any blank data? Because medical data don't have any blank data. Why? Because this data is the hospital's data so they can't be blank. This info gave from the patients.","aa5b640a":"<a id = \"5\"><\/a><br>\n## Logistic Regression","8fbb53be":"<a id = \"2\"><\/a><br>\n# Normalization\n\nWe did it because we need to normalize. Why? Because our data has distribution. Some features have a big value, some feature has between of 0 and 1. But they're the same distribution and for the numbers are different from each other, we need to normalize. ","757623bb":"<a id = \"6\"><\/a><br>\n## KNN (K Neighbors)","55e609f5":"<a id = \"13\"><\/a><br>\n# Compare Algorithms"}}