{"cell_type":{"8520b8a6":"code","b19e94f0":"code","2c5c0d05":"code","eba3a8bd":"code","44723c4f":"code","f1caf79f":"code","3a6639e9":"code","59b1387b":"code","d9e1c9ce":"code","9583a3d1":"code","bd488fdc":"code","408037b9":"code","6a20ec47":"code","933df452":"code","5fbed5d9":"code","41d5a02a":"code","52da2b9f":"code","0253da85":"code","e4a4bd26":"code","d175ec19":"code","596a61e6":"code","8e719a48":"code","1222a313":"markdown","11d77967":"markdown","e010cef2":"markdown","447433f6":"markdown","e5baadd1":"markdown","17e11426":"markdown","92499edf":"markdown","ed1411d6":"markdown","5d7250d4":"markdown","b7e87581":"markdown","9d90a978":"markdown","ffe2d9f5":"markdown","c374c67c":"markdown","ed36ea33":"markdown"},"source":{"8520b8a6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","b19e94f0":"from IPython.display import display\npd.options.display.max_columns = None","2c5c0d05":"train = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\", index_col=0)\nX_test = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\", index_col=0)","eba3a8bd":"X = train.iloc[:, :-1]\ny = train.iloc[:, -1:]","44723c4f":"cols_with_missing = X.isnull().any()\ncols_with_missing = cols_with_missing[cols_with_missing].index\n\ncols_missing_factor = pd.Series(X[cols_with_missing].isnull().sum() \/ len(X) * 100,\n                                index=cols_with_missing)\n\nMISSING_VALUES_THRESHOLD = 50","f1caf79f":"values = ((col, round(perc, 1), perc >= MISSING_VALUES_THRESHOLD) for col, perc in cols_missing_factor.iteritems())\ncolumns = [\"Columns with missing values\", \"percentage of missing values\", \"to drop\"]\n\npd.DataFrame(values, columns=columns)","3a6639e9":"cols_to_drop = cols_missing_factor[cols_missing_factor >= MISSING_VALUES_THRESHOLD].index.to_list()\nX = X.drop(columns=cols_to_drop)\nX_test = X_test.drop(columns=cols_to_drop)","59b1387b":"X.columns = range(X.shape[1])\nX_test.columns = range(X.shape[1])\n\nnum_cols = X.select_dtypes(include=np.number).columns.to_list()\ncat_cols = X.select_dtypes(exclude=np.number).columns.to_list()","d9e1c9ce":"from sklearn.impute import SimpleImputer\n\nnum_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\nX[num_cols] = num_imputer.fit_transform(X[num_cols])\nX_test[num_cols] = num_imputer.transform(X_test[num_cols])\n\ncat_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\nX[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\nX_test[cat_cols] = cat_imputer.transform(X_test[cat_cols])","9583a3d1":"X = X.values\ny = y.values\nX_test = X_test.values","bd488fdc":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=0)","408037b9":"from sklearn.preprocessing import StandardScaler\n\nX_std_scaler = StandardScaler()\nX_train[:, num_cols] = X_std_scaler.fit_transform(X_train[:, num_cols])\nX_val[:, num_cols] = X_std_scaler.transform(X_val[:, num_cols])\n\ny_std_scaler = StandardScaler()\ny_train = y_std_scaler.fit_transform(y_train)\ny_val = y_std_scaler.transform(y_val)","6a20ec47":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncol_trans = ColumnTransformer([(\"encoder\", OneHotEncoder(), cat_cols)], remainder=\"passthrough\")\ncol_trans.fit(X)\nX_train = col_trans.transform(X_train).toarray()\nX_val = col_trans.transform(X_val).toarray()","933df452":"pd.DataFrame(X_train)","5fbed5d9":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\nlin_reg = LinearRegression().fit(X_train, y_train)\n\ny_pred = lin_reg.predict(X_val)\nlin_reg_score = mean_absolute_error(y_val, y_pred)\nprint(\"MAE:\", lin_reg_score)","41d5a02a":"from sklearn.preprocessing import PolynomialFeatures\n\nX_train_poly = X_train[:, num_cols]\nX_train_poly = PolynomialFeatures(degree=2).fit_transform(X_train)\nX_train_poly = np.concatenate((X_train_poly, np.delete(X_train_poly, num_cols, axis=1)), axis=1)\n\nX_val_poly = X_val[:, num_cols]\nX_val_poly = PolynomialFeatures(degree=2).fit_transform(X_val)\nX_val_poly = np.concatenate((X_val_poly, np.delete(X_val_poly, num_cols, axis=1)), axis=1)","52da2b9f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\npoly_reg = LinearRegression().fit(X_train_poly, y_train)\n\ny_pred = poly_reg.predict(X_val_poly)\npoly_reg_score = mean_absolute_error(y_val, y_pred)\nprint(\"MAE:\", poly_reg_score)","0253da85":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error\n\nsvr = SVR(kernel=\"rbf\").fit(X_train, y_train)\n\ny_pred = svr.predict(X_val)\nsvr_score = mean_absolute_error(y_val, y_pred)\nprint(\"MAE:\", svr_score)","e4a4bd26":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\ntree = DecisionTreeRegressor().fit(X_train, y_train)\n\ny_pred = tree.predict(X_val)\ntree_score = mean_absolute_error(y_val, y_pred)\nprint(\"MAE:\", tree_score)","d175ec19":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest = RandomForestRegressor().fit(X_train, y_train)\n\ny_pred = forest.predict(X_val)\nforest_score = mean_absolute_error(y_val, y_pred)\nprint(\"MAE:\", forest_score)","596a61e6":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nxgb = XGBRegressor().fit(X_train, y_train)\n\ny_pred = xgb.predict(X_val)\nxgb_score = mean_absolute_error(y_val, y_pred)\nprint(\"MAE:\", xgb_score)","8e719a48":"from keras import models, layers\nfrom sklearn.metrics import mean_absolute_error\n\nnet = models.Sequential()\nnet.add(layers.Dense(128, input_shape=(X_train.shape[1],), activation=\"relu\"))\nnet.add(layers.Dropout(0.2))\nnet.add(layers.Dense(64, activation=\"relu\"))\nnet.add(layers.Dense(1, activation=None))\n\nnet.compile(optimizer=\"rmsprop\", loss=\"mae\", metrics=[\"mae\"])\n\nhistory = net.fit(X_train, y_train, batch_size=32, validation_data=(X_val, y_val), epochs=100, verbose=0)\n\ny_pred = net.predict(X_val)\nnn_score = mean_absolute_error(y_val, y_pred)\nprint(\"MAE:\", nn_score)","1222a313":"## Decision tree regression","11d77967":"### Preview","e010cef2":"## Support vector regression","447433f6":"### Splitting to train and validation set\n","e5baadd1":"## Data preprocessing\n### Missing values","17e11426":"## Linear regression","92499edf":"## Neural network ","ed1411d6":"# Housing prices","5d7250d4":"## Random forest regression","b7e87581":"## Gradient boosting","9d90a978":"### Features standarization","ffe2d9f5":"## Data loading","c374c67c":"## Polynomial regression","ed36ea33":"### Encoding categorical features"}}