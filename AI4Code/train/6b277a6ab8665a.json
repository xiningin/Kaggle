{"cell_type":{"77be6a20":"code","99d74b50":"code","f3e3ccc0":"code","98981846":"code","685e4670":"code","35dadbe8":"code","3b935d5a":"code","1906f83e":"code","e0eab8f4":"code","513b1f64":"code","c05a68c0":"code","4f5858b9":"code","14f14b34":"code","e16d548b":"code","b334477f":"code","e4f0168d":"markdown","90406410":"markdown","8dc59e8d":"markdown","4d9458f7":"markdown","b58b8eaa":"markdown","cd976e35":"markdown","bc7c79e0":"markdown","844c26ee":"markdown","f9091952":"markdown","775046ed":"markdown"},"source":{"77be6a20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99d74b50":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC","f3e3ccc0":"#Load in the train and test datasets \ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)","98981846":"train['Ticket_type'] = train['Ticket'].apply(lambda x:x[0:3])\ntrain['Ticket_type'] = train['Ticket_type'].astype('category')  #astype = change data type \ntrain['Ticket_type'] = train['Ticket_type'].cat.codes  #cat.codes = Return series of codes as well as the index\n\ntest['Ticket_type'] = test['Ticket'].apply(lambda x:x[0:3])\ntest['Ticket_type'] = test['Ticket_type'].astype('category')\ntest['Ticket_type'] = test['Ticket_type'].cat.codes\n\ntrain.head(3)\n","685e4670":"full_data = [train, test]\n\n# Some extra features, not necessarily important\n# Gives the length of the name\n# train['Name_length'] = train['Name'].apply(len)\n# test['Name_length'] = test['Name'].apply(len)\ntrain['Words_Count'] = train['Name'].apply(lambda x: len(x.split()))\ntest['Words_Count'] = test['Name'].apply(lambda x: len(x.split()))\n\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","35dadbe8":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","3b935d5a":"train.head(3)","1906f83e":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Person Correlation of Features', y = 1.05, size = 15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n#linewidths = line between cells \n#vmax = maximum\n#cmap = color of the heatmap\n#annot = Whether or not the value of each cell is marked, and the data type is set","e0eab8f4":"y_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'],axis=1)\nx_train = train.values #Creates an array of the train data \nx_test = test.values ","513b1f64":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\nxgb_predictions = gbm.predict(x_test)\n\nxgb.plot_importance(gbm)","c05a68c0":"StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': xgb_predictions })","4f5858b9":"df1 = pd.read_csv('..\/input\/91-genetic-algorithms-explained-using-geap\/submission.csv')\ndf2 = pd.read_csv('..\/input\/titanic-eda-fe-3-model-decision-tree-viz\/submission_GA.csv')\ndf3 = pd.read_csv('..\/input\/91-genetic-algorithms-explained-using-geap\/submission.csv')\nx1=0.4; x2=0.3; df1.head()","14f14b34":"df_ensemble = df1.copy()\nk = 'Survived'\ndf_ensemble[k] =  x1*df1[k] + x2*df2[k] + (1.0-x1-x2)*df3[k]\ndf_ensemble[k] = df_ensemble[k].apply(lambda f: 1 if f>=0.5 else 0)\ndf_ensemble.to_csv('df_ensemble_pre.csv', index=False)\ndf_ensemble.head()","e16d548b":"import numpy as np\nimport pandas as pd\n\nimport os\nimport re\nimport warnings\n\nimport io\nimport requests\nurl=\"https:\/\/github.com\/thisisjasonjafari\/my-datascientise-handcode\/raw\/master\/005-datavisualization\/titanic.csv\"\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode('utf-8')))\n \ntest_data_with_labels = c\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\nwarnings.filterwarnings('ignore')\n\nfor i, name in enumerate(test_data_with_labels['name']):\n    if '\"' in name:\n        test_data_with_labels['name'][i] = re.sub('\"', '', name)\n        \nfor i, name in enumerate(test_data['Name']):\n    if '\"' in name:\n        test_data['Name'][i] = re.sub('\"', '', name)\n        \nsurvived = []\n\nfor name in test_data['Name']:\n    survived.append(int(test_data_with_labels.loc[test_data_with_labels['name'] == name]['survived'].values[-1]))\n\n    \nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = survived\nsubmission.to_csv('ground_truth.csv', index=False)","b334477f":"# Generate Submission File \nStackingSubmission.to_csv(\"XGB.csv\", index=False)\nStackingSubmission.head()","e4f0168d":"# 5. Submission file\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:","90406410":"# Ensemble Modelling\nEnsemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model. Most of the practical data mining solutions utilize ensemble modeling techniques.","8dc59e8d":"# 1. Load Data","4d9458f7":"# 4. Evaluation\n\n### Generating Ground truth \n\n\n\n\"ground truth\"-> refers to the accuracy of the training set's classification for supervised learning techniques.","b58b8eaa":"# 2. Plots\n\n### 1. Person Correlation Heatmap \n \nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows","cd976e35":"**XGBoost parameters used in the model:**\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting","bc7c79e0":"# Introduction\nIt will be the best pieces of all required parts of this challenge. Add the data you need by adding the data in my notebook. \n\nWe will start by \n1. Loading data\n2. EDA\n3. Modelling \n4. Evaluation \n5. Submission file\n\nPlease upvote the notebook!!! \nThanks, I hope it might be useful for all of you guys. ","844c26ee":"### 2. Takeaway form the Plots\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.","f9091952":"Define df1, df2, df3","775046ed":"# 3. Models\n\n### [XGBoost]\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. We call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"}}