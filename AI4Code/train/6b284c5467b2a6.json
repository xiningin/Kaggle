{"cell_type":{"2303adaf":"code","f9d2566a":"code","7bbc69ac":"code","d73f4ed3":"code","0e3377fb":"code","f33c0c8f":"code","3bcd98d6":"code","7fa1afda":"code","50608d61":"code","2ef4a1ba":"code","bc4ef927":"code","03eff5a7":"code","00d10f10":"code","de880c91":"code","b09edaf4":"code","53e63fa8":"code","48153aaa":"code","b385b709":"code","2cde2f95":"code","025eb873":"code","0a18c073":"code","f7838a68":"code","34de9b2f":"code","0dfc48c1":"code","29994628":"code","8241f62e":"code","a6db122c":"code","edba5518":"code","12d7e657":"code","2112c060":"code","5f6ae39f":"code","397e3324":"code","0e9b7e7f":"code","88e378ae":"code","b729e086":"code","5ad651ed":"code","1e40fb3c":"code","520e3b98":"code","a6ae3153":"code","b5ce7e3f":"code","3d272b06":"code","95f10d75":"code","3bcee407":"code","0f9cfb63":"markdown","137eccaf":"markdown","24eea0a5":"markdown","0cf692c2":"markdown","f9e45c49":"markdown","7ccec3a5":"markdown","8a637f38":"markdown","a1169946":"markdown","b8648811":"markdown","77a0a67c":"markdown","8b6766d8":"markdown","a560d62e":"markdown","cf9ac49c":"markdown","7752dd0a":"markdown","2ca23808":"markdown","7b9c481a":"markdown","aff44060":"markdown","e4aa92b7":"markdown","7a08852d":"markdown"},"source":{"2303adaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport math\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport pandas_profiling\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","f9d2566a":"train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')\nprint(\"done\")","7bbc69ac":"train.info()","d73f4ed3":"# pandas_profiling.ProfileReport(train)","0e3377fb":"train.head()","f33c0c8f":"#The Features\n\n#remove Id column so it won't participate in calculations as a feature\nX = train[set(train.columns)-set(['Cover_Type','Id'])]\n#The target\ny = train.Cover_Type\n\n# REMOVAL OF THE ID COLUMN FROM TEST SET\nid_test = test['Id']\ntest = test.drop(columns='Id')\nprint(\"done\")","3bcd98d6":"features_names = train.columns\n# print(features_names)\n# train[[name for name in train.columns if 'Wild'in name]].head()\nnum_col_names = [ 'Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\nonehot_col_names = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nx_numeric = X[num_col_names]\nx_onehot = X[onehot_col_names]\nprint(\"done\")","7fa1afda":"colsNum = len(x_numeric.columns)\nfig, ax = plt.subplots(colsNum, figsize =(5,30))\n\nfor  i, name in enumerate(x_numeric.columns,0): #starts counting from 0\n        sns.boxplot(x=y, y =X[name], ax = ax[i]).set_title(name)\n\nplt.tight_layout()\nprint(\"done\")","50608d61":"train.head()","2ef4a1ba":"fig = plt.figure(figsize= (25,10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\n\nwildernessAreas_sums = x_onehot.groupby(y)['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4'].sum()\nsoiltypes_sums = x_onehot.groupby(y)[list(name for name in onehot_col_names if 'Soil' in name)].sum()\nsns.heatmap(data= wildernessAreas_sums, annot = True, ax= ax1, fmt='g')\nsns.heatmap(data= soiltypes_sums, annot = True, ax = ax2, fmt='g')\n# print(soiltypes_sums)\n\n","bc4ef927":"### Lets see if there are some features we can remove right away:\n\n#Categorical features:\nprint('Percentage of values in categorical features:')\ncategorical_percent_value = 100 * x_onehot.sum()\/len(x_onehot)\n# creates a series in which the index is the list of one-hot-encoded \n# features names and their information percentage\nprint(categorical_percent_value)","03eff5a7":"plt.figure(figsize = (40,40))\ncorr = x_numeric.corr()\n# rule_of_thumb = 2\/math.sqrt(len(x_numeric)) # = ~ 0.016\n# every correlation that its absolute value is above that value is a correlation \n# that can be a target for dimentionality reduction\ncorr = corr.abs()\ncorrelation_matrix = corr.style.background_gradient(cmap = 'coolwarm')\ncorrelation_matrix","00d10f10":"correlated_features = set()\n\nfor i in range(len(corr.columns)):\n    for j in range(i):\n        if abs(corr.iloc[i,j]) > 0.6:\n            colname = corr.columns[i]\n            correlated_features.add(colname)\n            \ncorrelated_features","de880c91":"# a scatter plot that is not very informative:\n\nplt.scatter(x = X['Horizontal_Distance_To_Hydrology'],\n            y = X['Vertical_Distance_To_Hydrology'],\n            alpha = 0.5, marker = 'o', c = y)\nplt.xlabel('Horizontal')\nplt.ylabel('Vertical')\nplt.title('Hydrology map')\n\nplt.show()\nprint(\"done\")","b09edaf4":"#imports \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n## for ML\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsClassifier\n\n## for ML model evaluation\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \n\n\n## for Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklearn.compose import ColumnTransformer\nprint(\"done\")","53e63fa8":"train_X, val_X, train_y, val_y = train_test_split(X,y)\nprint(\"done\")","48153aaa":"#This Function Returns the accuracy score of RF of n-estimators\n\ndef get_accuracy_RF(n_estimators, train_X, train_y, val_X, val_y):\n    model = RandomForestClassifier(n_estimators= n_estimators,random_state=1)\n    model.fit(train_X,train_y)\n    preds = model.predict(val_X)\n    acc = accuracy_score(val_y, preds)\n    return acc\n\nprint(\"done\")","b385b709":"#OPTIMIZATION OF RF N_ESTIMATORS\n# Here I find the 'best' n_estimators for the RF model\n# acc stands for accuracy\n\n# n_estimators = [50,100,200,300,400,500, 600, 700]\n# acc = []\n# for n in n_estimators:\n#     acc_new = get_accuracy_RF(n, train_X, train_y, val_X, val_y)\n#     acc.append(acc_new)\n#     print(\"for \",n,\" accuracy: \",acc_new)\n\nprint(\"done\")","2cde2f95":"# index_of_max = acc.index(max(acc))\nn_estimators=500\n# print(index_of_max)\nchosen_n_estimators = n_estimators#[index_of_max]\nprint('best n_estimators for RF: ',chosen_n_estimators)","025eb873":"#Define:\n# RFmodel = RandomForestRegressor(n_estimators= 100,random_state=1)\nRFmodel = RandomForestClassifier(n_estimators= chosen_n_estimators,random_state=1)\n# #Fit\n# clf = RFmodel.fit(train_X,train_y)\n# #Predict\n# val_predictions_RF = RFmodel.predict(val_X)\n# #Evaluate\n# clf.score(val_X,val_y) #accuracy","0a18c073":"model = RFmodel\n# preprocessor = ColumnTransformer(transformers=[])\nRF_pipeline = Pipeline(steps = [('model', model)])\nprint(\"done\")","f7838a68":"rfp = RF_pipeline.fit(train_X , train_y)\npreds= RF_pipeline.predict(val_X)\nrfp.score(val_X, val_y) #accuracy\nprint(\"done\")","34de9b2f":"mask = categorical_percent_value < 3\ncategorical_feat_to_remove = categorical_percent_value.index[mask]\nprint(categorical_feat_to_remove)\n","0dfc48c1":"### Remove the chosen categorical columns an\u05d2 re-train the RF model:\n# Data for output2\n\ntrain_X_proccessed = train_X[set(train_X.columns)-set(categorical_feat_to_remove)]\nval_X_proccessed = val_X[train_X_proccessed.columns]\ntest_proccessed = test[train_X_proccessed.columns]\nprint(test_proccessed.columns)\nprint(train_X_proccessed.shape, val_X_proccessed.shape)","29994628":"# Train Data for output2\n\nrfpp = RF_pipeline.fit(train_X_proccessed, train_y)\npreds = RF_pipeline.predict(val_X_proccessed)\nacc_rfpp = rfpp.score(val_X_proccessed, val_y)\nprint('Accuracy: ',acc_rfpp)\n","8241f62e":"# a peek at the val_y values\nval_y.unique()\nnp.unique(preds)","a6db122c":"confusion_matrix(val_y.values,preds)","edba5518":"print('\\nClassification Report:\\n', classification_report(val_y.values,preds))\n# print('F= ', (2*))","12d7e657":"# the following function returns a copy of data with one shuffled column\ndef shuffle_one_feature(data,feature_to_shuffle):\n    shuffled_feature = np.random.permutation(data[feature_to_shuffle])\n    data[feature_to_shuffle] = shuffled_feature\n    return data\nprint(\"done\")","2112c060":"train_X.columns\nval_X.columns\nprint(train_X.shape)\nprint(val_X.shape)","5f6ae39f":"#compare to acc_rfpp =  0.858\nfeatures = val_X_proccessed.columns\nscore_after_feature_shuffle = pd.DataFrame(index = [1])\n\nfor feature in features:\n    data_to_shuffle = val_X_proccessed.copy()\n    one_shuffled = shuffle_one_feature(data_to_shuffle, feature)\n    preds = RFmodel.predict(one_shuffled)\n    acc = accuracy_score(val_y,preds)\n#     print(acc)\n    score_after_feature_shuffle[feature] = acc\n\nscore_after_feature_shuffle_T = score_after_feature_shuffle.transpose()\nscore_after_feature_shuffle_T[score_after_feature_shuffle_T>0.801]\nprint(score_after_feature_shuffle_T[1])\n# only 2 features show a significant change inaccuracy\n# if their valus are shuffles in the validation part\n# Let's assume that only those two features are important and train the \n# RF model over the data that includes these 2 and the important one-hot features\n","397e3324":"# find names of numerical features that show significant difference after shuffling\n\nsignificant_feat = []\nfeatures_num = len(score_after_feature_shuffle_T[1])\nfor feature in features: \n    feature_acc = score_after_feature_shuffle_T[1][feature]\n    if (feature_acc\/acc_rfpp <0.95):\n       significant_feat.append(feature)\n \nset_to_remove = set(train_X.columns)-set(significant_feat)-set(onehot_col_names)\n","0e9b7e7f":"# Data for output3  - removed correlated features\n# Reproccess the data so the correlated features are removed: \ncorrelated_features\n\ntrain_X_proccessed = train_X[set(train_X.columns)-set(categorical_feat_to_remove)-correlated_features]\nval_X_proccessed = val_X[train_X_proccessed.columns]\ntest_proccessed = test[train_X_proccessed.columns]\nprint(test_proccessed.columns)\nprint(train_X_proccessed.shape, val_X_proccessed.shape)","88e378ae":"# Train Data for output3\nrfpp = RF_pipeline.fit(train_X_proccessed, train_y)\npreds = RF_pipeline.predict(val_X_proccessed)\nacc_rfpp = rfpp.score(val_X_proccessed, val_y)\nprint('Accuracy: ',acc_rfpp)","b729e086":"# Data for output4  - removed correlated features and insignificant features\n# Reproccess the data so the correlated features are removed: \ncorrelated_features\n\ntrain_X_proccessed = train_X[set(train_X.columns)-set(categorical_feat_to_remove)-correlated_features- set_to_remove]\nval_X_proccessed = val_X[train_X_proccessed.columns]\ntest_proccessed = test[train_X_proccessed.columns]\nprint(test_proccessed.columns)\nprint(train_X_proccessed.shape, val_X_proccessed.shape)","5ad651ed":"# Train Data for output4\nrfpp = RF_pipeline.fit(train_X_proccessed, train_y)\npreds = RF_pipeline.predict(val_X_proccessed)\nacc_rfpp = rfpp.score(val_X_proccessed, val_y)\nprint('Accuracy: ',acc_rfpp)","1e40fb3c":"#building pipeline for PCA - at work\nmodel = RFmodel\npreprocessor = ColumnTransformer(transformers=[])\nPCA_pipeline = Pipeline(steps = [('preprocessor', preprocessor),('model', model)])\nprint(\"PCA not done\")","520e3b98":"# kernels = ['linear', 'poly', 'rbf','sigmoid', 'precomputed']\nmodelSVC = svm.SVC(kernel = 'rbf', C=1500)\nscaler = StandardScaler().fit(train_X)\n# preprocessor = ColumnTransformer(transformers=[('scaler',scaler)])\nSVC_pipeline = Pipeline(steps = [('scaler',scaler),('model', modelSVC)])\nprint(\"done\")","a6ae3153":"# Train Data for output5\nsvcPipe= SVC_pipeline.fit(train_X_proccessed, train_y)\npreds = SVC_pipeline.predict(val_X_proccessed)\nacc_svcPipe = svcPipe.score(val_X_proccessed, val_y)\nprint('Accuracy: ',acc_svcPipe)","b5ce7e3f":"modelKNN = KNeighborsClassifier(n_neighbors=3)\nscaler = StandardScaler().fit(train_X)\n# preprocessor = ColumnTransformer(transformers=[('scaler',scaler)])\nKNN_pipeline = Pipeline(steps = [('scaler',scaler),('model', modelKNN)])\nprint(\"done\")","3d272b06":"### Train Data for output6\nknnPipe= KNN_pipeline.fit(train_X_proccessed, train_y)\npreds = KNN_pipeline.predict(val_X_proccessed)\nacc_knnPipe = knnPipe.score(val_X_proccessed, val_y)\nprint('Accuracy: ',acc_knnPipe)\n\n# n_neighbors =3 gave the best results for KNN but to sum up, KNN downgraded the submittion score","95f10d75":"# Original test set\n#test_preds = RF_pipeline.predict(test)\n\n# Removed a few categorical features\ntest_proccessed_preds = RF_pipeline.predict(test_proccessed)\n# test_proccessed_preds = SVC_pipeline.predict(test_proccessed)\n# test_proccessed_preds = KNN_pipeline.predict(test_proccessed)\n\nprint(test_proccessed_preds)","3bcee407":"# RF Classifier over all the features\n# output1 = pd.DataFrame({'Id': id_test,\n#                     'Cover_Type': test_preds}) # Score1 = 0.10209\n\n# RF Classifier after ramoval of a few categorical features (categorical_feat_to_remove)\n# output2 = pd.DataFrame({'Id': id_test,\n#                       'Cover_Type': test_proccessed_preds}) # Score2 = 0.73541\n# output3 = pd.DataFrame({'Id': id_test,\n#                       'Cover_Type': test_proccessed_preds}) # Score3 = 0.74278\n# output4 = pd.DataFrame({'Id': id_test,\n#                       'Cover_Type': test_proccessed_preds}) # Score4 = 0.74831\n# output5 = pd.DataFrame({'Id': id_test,\n#                       'Cover_Type': test_proccessed_preds}) # Score5 = 0.68290\n# output6 = pd.DataFrame({'Id': id_test,\n#                       'Cover_Type': test_proccessed_preds}) # Score6 = 0.66705\noutput = pd.DataFrame({'Id': id_test,\n                      'Cover_Type': test_proccessed_preds}) # Score7 =\noutput.to_csv('submission.csv', index=False)","0f9cfb63":"### Split the data into train and validation","137eccaf":"# Bivariate","24eea0a5":"# Split features from target","0cf692c2":"# Univariate","f9e45c49":"# KNN","7ccec3a5":"### Lets see the correlation matrix in colors","8a637f38":"# Load tha data","a1169946":"### logistic regression\n### XGboost\n### light GBM\n### complexity plot (score as complexity changes)\n### feature importance","b8648811":"## Starting with RF on *all* features to get a feeling ","77a0a67c":"### Since the target feature is categorical, this is a classification problem!","8b6766d8":"### Train the RF classifier on the train_X_proccessed data:","a560d62e":"# Trying pipeline for practice","cf9ac49c":"### The steps to building and using a model are:\n\n### Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n### Fit: Capture patterns from provided data. This is the heart of modeling.\n### Predict: Just what it sounds like\n### Evaluate: Determine how accurate the model's predictions are.","7752dd0a":"# The output for submission","2ca23808":"# Running ML models","7b9c481a":"# SVM - SVC","aff44060":"# Predictions:","e4aa92b7":"# Take a peek at the data","7a08852d":"### Finding feature importance by shuffling"}}