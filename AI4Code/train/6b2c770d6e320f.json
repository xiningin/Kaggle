{"cell_type":{"96b80557":"code","fc4cae9b":"code","ff5a2be1":"code","2c75914c":"code","749b01a3":"code","04111866":"code","a911e8bc":"code","5dcb00d5":"code","5630f205":"code","9a2854d4":"code","0503bc28":"code","8e6eec95":"code","d7266a94":"code","2eaab03e":"code","7c633266":"code","57fc623d":"code","d0326e08":"code","d3bc2810":"code","1fef1778":"code","c6e0fef5":"code","58b2bba4":"markdown","3367aff3":"markdown","e8ac9622":"markdown","6c6a6c6f":"markdown","ea8a6645":"markdown","e70c66cf":"markdown","dda929ed":"markdown","465d2aa7":"markdown"},"source":{"96b80557":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","fc4cae9b":"# install sent2vec\n!pip install git+https:\/\/github.com\/epfml\/sent2vec\n# install annoy\n!pip install annoy","ff5a2be1":"!pip freeze > kaggle_image_requirements.txt","2c75914c":"# Input data files are available in the \"..\/input\/\" directory.\n# Any results you write to the current directory are saved as output.\n\n!ls ..\/input","749b01a3":"import re\nimport time\n\nstart = time.time()\nenglish_sentences = []\nwith open(\"..\/input\/jw300entw\/jw300.en-tw.en\") as f:\n    for line in f:\n        english_sentences.append(re.sub(r'[\\W\\d]', \" \",line.lower())) # clean and normalize\nend = time.time()\nprint(\"Loading the english sentences took %d seconds\"%(end-start))","04111866":"print(\"A sample of the english sentences is:\")\nprint(english_sentences[:10])\nprint(\"The length of the list is:\")\nprint(len(english_sentences))","a911e8bc":"twi_sentences = []\nwith open(\"..\/input\/jw300entw\/jw300.en-tw.tw\") as f:\n    for line in f:\n        twi_sentences.append(re.sub(r'[\\W\\d]', \" \", line.lower())) # clean and normalize","5dcb00d5":"print(\"A sample of the twi sentences is:\")\nprint(twi_sentences[:10])\nprint(\"The length of the list is:\")\nprint(len(twi_sentences))","5630f205":"MAXNSENT = 10000 # how many sentences to take from top of data for now (small experiment)","9a2854d4":"import time\nimport sent2vec\n\nmodel = sent2vec.Sent2vecModel()\nstart=time.time()\nmodel.load_model('..\/input\/sent2vec\/wiki_unigrams.bin')\nend = time.time()\nprint(\"Loading the sent2vec embedding took %d seconds\"%(end-start))","0503bc28":"def assemble_embedding_vectors(data):\n    out = None\n    for item in data:\n        vec = model.embed_sentence(item)\n        if vec is not None:\n            if out is not None:\n                out = np.concatenate((out,vec),axis=0)\n            else:\n                out = vec                                            \n        else:\n            pass\n        \n    return out","8e6eec95":"start=time.time()\nEmbeddingVectors = assemble_embedding_vectors(english_sentences[:MAXNSENT])\nend = time.time()\nprint(\"Computing all embeddings took %d seconds\"%(end-start))\nprint(EmbeddingVectors)","d7266a94":"print(\"The shape of embedding matrix:\")\nprint(EmbeddingVectors.shape)","2eaab03e":"# Save embeddings for later use\nnp.save(\"english_sent2vec_vectors_jw.npy\",EmbeddingVectors)","7c633266":"from annoy import AnnoyIndex\n\nstart = time.time()\ndimension = EmbeddingVectors.shape[1] # Length of item vector that will be indexed\nenglish_NN_index = AnnoyIndex(dimension, 'angular')  \nfor i in range(EmbeddingVectors.shape[0]): # go through every embedding vector\n    english_NN_index.add_item(i, EmbeddingVectors[i]) # add to index\n\nenglish_NN_index.build(10) # 10 trees\nenglish_NN_index.save('en_sent2vec_NN_index.ann') # save index\nend = time.time()\nprint(\"Building the NN index took %d seconds\"%(end-start))","57fc623d":"test_english_NN_index = AnnoyIndex(dimension, 'angular')\ntest_english_NN_index.load('en_sent2vec_NN_index.ann') # super fast, will just mmap the file","d0326e08":"translation_idx = 5 # choose index of sentence to focus on in english_sentences\/twi_sentences\n\nannoy_out = test_english_NN_index.get_nns_by_item(translation_idx, 5) # will 5 nearest neighbors to the very first sentence","d3bc2810":"print(annoy_out)","1fef1778":"print(\"- The sentence we are finding nearest neighbors of:\\n\")\nprint(english_sentences[annoy_out[0]])\nprint(\"\\n\\n- The 4 nearest neighbors found:\\n\")\nfor i in range(1,5):\n    print(str(i) + \". \"+ english_sentences[annoy_out[i]])","c6e0fef5":"print(\"- In other words, if we were translating the english sentence:\\n\")\nprint(english_sentences[annoy_out[0]])\nprint(\"  where the known correct translation is:\")\nprint(twi_sentences[annoy_out[0]])\nprint(\"\\n\\n- The 4 top translation suggested by our sparse retrieval system above are:\\n\")\nfor i in range(1,5):\n    print(str(i) + \". \"+ twi_sentences[annoy_out[i]])","58b2bba4":"Test the built index","3367aff3":"# Build and Test Index w\/ Annoy for fast Neareast-Neighbor Retrieval\n\nFirst build the annoy index for the available English sent2vec vectors","e8ac9622":"# Load The Raw Data","6c6a6c6f":"Write requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n\nLatest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https:\/\/github.com\/azunre\/transfer-learning-for-nlp","ea8a6645":"# Vectorize Subset of English Sentences with sent2vec","e70c66cf":"# Preliminaries","dda929ed":"This seems to work! Now we need to just scale it out to the whole dataset and test with random input!","465d2aa7":"Check what is in the input folder"}}