{"cell_type":{"c7d067a8":"code","a08a8e06":"code","e4cee0d1":"code","145d2b31":"code","b63ccc56":"code","71de0c07":"code","e28c8da5":"code","8891e6ad":"code","41b1bb3d":"code","e3746926":"code","b585e005":"code","502a6678":"code","2f88b3e7":"code","95495a4a":"markdown","a7e7ea04":"markdown"},"source":{"c7d067a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a08a8e06":"import math\nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime    \nsns.set_style(\"whitegrid\")\nfrom pandas.plotting import autocorrelation_plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")","e4cee0d1":"data = pd.read_csv('..\/input\/mrvlstock\/MRVL.csv')\ndata.head()","145d2b31":"data.reset_index(drop=True, inplace=True)\ndata.fillna(data.mean(), inplace=True)\ndata.head()","b63ccc56":"ma_day = [10,50,100]\n\nfor ma in ma_day:\n    column_name = \"MA for %s days\" %(str(ma))\n    data[column_name]=pd.DataFrame.rolling(data['Close'],ma).mean()\n\ndata['Daily Return'] = data['Close'].pct_change()\n\ndf = data","71de0c07":"from sklearn.model_selection import train_test_split\n\nX = []\nY = []\nwindow_size=100\nfor i in range(1 , len(df) - window_size -1 , 1):\n    first = df.iloc[i,2]\n    temp = []\n    temp2 = []\n    for j in range(window_size):\n        temp.append((df.iloc[i + j, 2] - first) \/ first)\n    temp2.append((df.iloc[i + window_size, 2] - first) \/ first)\n    X.append(np.array(temp).reshape(100, 1))\n    Y.append(np.array(temp2).reshape(1, 1))\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n\ntrain_X = np.array(x_train)\ntest_X = np.array(x_test)\ntrain_Y = np.array(y_train)\ntest_Y = np.array(y_test)\n\ntrain_X = train_X.reshape(train_X.shape[0],1,100,1)\ntest_X = test_X.reshape(test_X.shape[0],1,100,1)\n\nprint(len(train_X))\nprint(len(test_X))","e28c8da5":"# For creating model and training\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Bidirectional, TimeDistributed\nfrom tensorflow.keras.layers import MaxPooling1D, Flatten\nfrom tensorflow.keras.regularizers import L1, L2\nfrom tensorflow.keras.metrics import Accuracy\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\nmodel = tf.keras.Sequential()\n\n\n# CNN layers\n# kernel size is one of the hyperparameters\nmodel.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu', input_shape=(None, 100, 1))))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Flatten()))\n\n\n# LSTM layers\n# Dropouts is one of the hyperparameters\nmodel.add(Bidirectional(LSTM(100, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(100, return_sequences=False)))\nmodel.add(Dropout(0.5))\n\n#Final layers\nmodel.add(Dense(1, activation='linear'))\n# Adam optimizer is one of the \nmodel.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n\nhistory = model.fit(train_X, train_Y, validation_data=(test_X,test_Y), epochs=40,batch_size=40, verbose=1, shuffle =True)","8891e6ad":"#checking how much loss decrease while training (using mse)\nplt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","41b1bb3d":"#checking how much loss decrease while training (using mae)\nplt.plot(history.history['mae'], label='train mae')\nplt.plot(history.history['val_mae'], label='val mae')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","e3746926":"# After the model has been constructed, we'll summarise it\nfrom tensorflow.keras.utils import plot_model\nprint(model.summary())\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","b585e005":"model.evaluate(test_X, test_Y)","502a6678":"from sklearn.metrics import explained_variance_score, mean_poisson_deviance, mean_gamma_deviance\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import max_error\n\n# predict probabilities for test set\nyhat_probs = model.predict(test_X, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\n\nvar = explained_variance_score(test_Y.reshape(-1,1), yhat_probs)\nprint('Variance: %f' % var)\n\nr2 = r2_score(test_Y.reshape(-1,1), yhat_probs)\nprint('R2 Score: %f' % var)\n\nvar2 = max_error(test_Y.reshape(-1,1), yhat_probs)\nprint('Max Error: %f' % var2)","2f88b3e7":"predicted  = model.predict(test_X)\ntest_label = test_Y.reshape(-1,1)\npredicted = np.array(predicted[:,0]).reshape(-1,1)\nlen_t = len(train_X)\nfor j in range(len_t , len_t + len(test_X)):\n    temp = data.iloc[j,3]\n    test_label[j - len_t] = test_label[j - len_t] * temp + temp\n    predicted[j - len_t] = predicted[j - len_t] * temp + temp\nplt.plot(predicted, color = 'green', alpha=0.3, label = 'Predicted  Stock Price')\nplt.plot(test_label, color = 'red', alpha=0.2,label = 'Real Stock Price')\nplt.title(' Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel(' Stock Price')\nplt.legend()\nplt.show()","95495a4a":"# **Training of Model**","a7e7ea04":"Fine tuning for the model"}}