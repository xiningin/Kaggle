{"cell_type":{"1c5f75b4":"code","2e14ab6c":"code","9e3dc710":"code","f1059b38":"code","a32edb96":"code","57559703":"code","06ef4d69":"code","d0f53a00":"code","75ad661c":"code","8667866e":"code","b93714f3":"code","16fbc97d":"code","4d0fffc2":"code","648c039c":"code","d1ddf4a7":"code","6f3d9d07":"code","8e1b219c":"code","d0dec189":"code","77ea966f":"code","14ba86cb":"code","a7d2aef3":"code","20403c12":"code","9597df04":"code","e2b1f34d":"code","3076c50c":"code","48b34c1a":"code","43be5313":"code","e4d089d9":"code","08175507":"code","77992f6a":"markdown","51a54fe1":"markdown","1c5d9130":"markdown","7073a737":"markdown","edb7c04f":"markdown","0b3bb99b":"markdown","165b2b94":"markdown","9dcdd4a6":"markdown","7de9d87e":"markdown","5703de6b":"markdown","63fc85e9":"markdown","f45a8bf3":"markdown","8e36eeaa":"markdown","adb1fe49":"markdown","5ba193a1":"markdown","a5bd951d":"markdown","c2159f82":"markdown","cdabdee8":"markdown","f11b8357":"markdown","72b58fe6":"markdown","5be53257":"markdown","c5e106b0":"markdown","cbe6121a":"markdown","b8c95ca9":"markdown","de021f43":"markdown","fdb8abf8":"markdown","d17be0fe":"markdown","fed1ac21":"markdown","d43d6972":"markdown"},"source":{"1c5f75b4":"!pip install seaborn==0.11.0\n\n# General Data Manipulation Library\nimport pandas as pd\nimport numpy as np\nimport re\n\n\n# Plotting LIbraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nfrom IPython.display import clear_output\nwarnings.filterwarnings('ignore')\nimport random\n\nrandom.seed(1455)\nnp.random.seed(1455)\n\nfrom sklearn.metrics import accuracy_score\n\nsns.set_theme()\nclear_output()","2e14ab6c":"# Load in the train and test datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntest.head(5)","9e3dc710":"# Dealing with the Ticket\ntrain['Ticket_type'] = train['Ticket'].apply(lambda x: x[0:4])\ntrain['Ticket_type'] = train['Ticket_type'].astype('category')\ntrain['Ticket_type'] = train['Ticket_type'].cat.codes\n\ntest['Ticket_type'] = test['Ticket'].apply(lambda x: x[0:4])\ntest['Ticket_type'] = test['Ticket_type'].astype('category')\ntest['Ticket_type'] = test['Ticket_type'].cat.codes\n\ntrain['Ticket_type'].value_counts().plot.bar(figsize=(30, 5), title = 'Ticket Code Conversion', xlabel = 'Ticket Code', ylabel = 'People Count')","f1059b38":"train['Words_Count'] = train['Name'].apply(lambda x: len(x.split()))\ntest['Words_Count'] = test['Name'].apply(lambda x: len(x.split()))\n\nfig = sns.displot(data=train, x=\"Words_Count\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Words_Count feature vs Survived')","a32edb96":"# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\nfig = sns.displot(data=train, x=\"Has_Cabin\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Has_Cabin feature vs Survived')","57559703":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n\nfig = sns.displot(data=train, x=\"FamilySize\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='FamilySize feature vs Survived')","06ef4d69":"train['IsAlone'] = 0\ntrain.loc[train['FamilySize'] == 1, 'IsAlone'] = 1\n\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1","d0f53a00":"# Remove all NULLS in the Fare column and create a new feature CategoricalFare\ntrain['Fare'] = train['Fare'].fillna(train['Fare'].median())\ntest['Fare'] = test['Fare'].fillna(train['Fare'].median())\n\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)","75ad661c":"whole_data  = [train, test]\n\nfor dataset in whole_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n\n\n\n\nfig = sns.displot(data=train, x=\"Age\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Age vs Survived')\n\n\n\n# Mapping Age\ndataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\ndataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\ndataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\ndataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\ndataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\n\nfig = sns.displot(data=train, x=\"Age\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Age Mapped vs Survived')","8667866e":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in whole_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in whole_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n    \nfor dataset in whole_data:\n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \nfig = sns.displot(data=train, x=\"Title\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Title Mapped vs Survived')","b93714f3":"for dataset in whole_data:\n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n# Mapping Sex [Male: 1, Female: 0]\nfor dataset in whole_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\nfig = sns.displot(data=train, x=\"Sex\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Sex vs Survived')\n\n\nfig = sns.displot(data=train, x=\"Fare\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Fare vs Survived')","16fbc97d":"# Taking Care of the Missing Values\ntrain['Embarked'] = train['Embarked'].fillna('S')\ntest['Embarked'] = test['Embarked'].fillna('S')\n    \n# Mapping Embarked\ntrain['Embarked'] = train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntest['Embarked'] = test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\nfig = sns.displot(data=train, x=\"Embarked\", hue=\"Survived\", multiple=\"stack\", kde = True)\nfig.set(title='Embarked feature vs Survived')","4d0fffc2":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\ntrain.head(5)","648c039c":"plt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.3,vmax=1.0, \n            square=True, cmap='PiYG', linecolor='white', annot=True)","d1ddf4a7":"plt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=12)\nsns.heatmap(train.astype(float).corr(method = 'spearman'),linewidths=0.3,vmax=1.0, \n            square=True, cmap='PiYG', linecolor='white', annot=True)","6f3d9d07":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize= (30, 12))\nsns.heatmap(train.astype(float).corr(method = 'pearson'),linewidths=0.3,vmax=1.0, \n            square=True, cmap='PiYG', linecolor='white', annot=True, ax = ax[0])\n\nsns.heatmap(train.astype(float).corr(method = 'spearman'),linewidths=0.3,vmax=1.0, \n            square=True, cmap='PiYG', linecolor='white', annot=True, ax = ax[1])\n\nax[0].title.set_text('Peason Correlation of Features')\nax[1].title.set_text('Spearman Correlation of Features')\n\nfig.show()","8e1b219c":"y_train = train['Survived'].ravel() # Creates an array of the train labels\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creates an array of the test labels","d0dec189":"test_data_with_labels = pd.read_csv(\"https:\/\/github.com\/thisisjasonjafari\/my-datascientise-handcode\/raw\/master\/005-datavisualization\/titanic.csv\")\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\nfor i, name in enumerate(test_data_with_labels['name']):\n    if '\"' in name:\n        test_data_with_labels['name'][i] = re.sub('\"', '', name)\n        \nfor i, name in enumerate(test_data['Name']):\n    if '\"' in name:\n        test_data['Name'][i] = re.sub('\"', '', name)\n        \nsurvived = []\n\nfor name in test_data['Name']:\n    survived.append(int(test_data_with_labels.loc[test_data_with_labels['name'] == name]['survived'].values[-1]))\n\n\n# Ground Label Evaluation\ny_true = np.array(survived)","77ea966f":"import xgboost as xgb\n\n\n\ngbm = xgb.XGBClassifier(n_estimators= 100, \n                        max_depth = 4,\n                        gamma = 0.9,\n                        nthread = -1,\n                        scale_pos_weight=1,\n                       random_state = 3101)\n\n\ngbm.fit(x_train, y_train)\nxgb_predictions = gbm.predict(x_test)\n\nscore_gbm = gbm.score(x_train, y_train)\nprint(f'Random Forest Classifier score (Train Accuracy): {score_gbm}')\n\ntest_acc_gbm = accuracy_score(y_true, xgb_predictions)\nprint(f'Random Forest Classifier score (Test Accuracy): {test_acc_gbm}')\n\n\n# Making Submission DataFrame\nxgb_submission = pd.DataFrame({ 'PassengerId': PassengerId,\n                                   'Survived': xgb_predictions })\n\n# Writing to csv file\nxgb_submission.head()\nxgb_submission.to_csv('xgb_submission.csv', index = False)","14ba86cb":"xgb.plot_importance(gbm)","a7d2aef3":"xgb.to_graphviz(gbm, num_trees=2)","20403c12":"#random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators = 4, \n                            max_features  = 5,\n                            random_state = 216)  \nrfc.fit(x_train, y_train)\nscore_rfc = rfc.score(x_train, y_train)\nout_rfc = rfc.predict(x_test)\nprint(f'Random Forest Classifier score (Train Accuracy): {score_rfc}')\n\ntest_acc_rfc = accuracy_score(y_true, out_rfc)\nprint(f'Random Forest Classifier score (Test Accuracy): {test_acc_rfc}')\n\n\n\n# Making Submission DataFrame\nrfc_submission = pd.DataFrame({ 'PassengerId': PassengerId,\n                                   'Survived': out_rfc })\n\n# Writing to csv file\nrfc_submission.head()\nrfc_submission.to_csv('rfc_submission.csv', index = False)","9597df04":"from sklearn.neighbors import KNeighborsClassifier\n\n#knn classifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train, y_train)\nscore_knn = knn.score(x_train, y_train)\nout_knn = knn.predict(x_test)\n \n\nprint(f'K- Nearest Neighbour ClassifierClassifier score (Train Accuracy): {score_knn}')\ntest_acc_knn = accuracy_score(y_true, out_knn)\nprint(f'K- Nearest Neighbour Classifier Classifier score (Test Accuracy): {test_acc_knn}')","e2b1f34d":"from sklearn.svm import SVC\n\n#SVM\n# \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019\nsvc = SVC(C = 5, kernel = 'linear', random_state = 8)\nsvc.fit(x_train, y_train)\nscore_svc = svc.score(x_train, y_train)\nout_svc = svc.predict(x_test)    \nprint(f'Support Vector Machine Classifier score (Train Accuracy): {score_svc}')\ntest_acc_svc = accuracy_score(y_true, out_svc)\nprint(f'K- Support Vector Machine  Classifier Classifier score (Test Accuracy): {test_acc_svc}')","3076c50c":"from sklearn.ensemble import VotingClassifier\n\n#voting classifier    \nvclf = VotingClassifier(estimators=[('gb',gbm), ('rf',rfc),('knn',knn) , ('svm',svc)], voting='hard', weights=[2,3,1,2])\nvclf.fit(x_train, y_train)\nout_vclf = vclf.predict(x_test)\nscore_voting = vclf.score(x_train, y_train)\nprint(f'Voting Classifier score (Train Accuracy): {score_voting}')\ntest_acc_voting = accuracy_score(y_true, out_vclf)\nprint(f'Voting Classifier score (Test Accuracy): {test_acc_voting}')","48b34c1a":"classifier = ['XGBoost', \t'RandomForest', 'KNN', 'SVC', 'VotingEnsemble']\ntrain_acc  = [score_gbm,\tscore_rfc,\tscore_knn, score_svc, score_voting]\ntest_acc   = [test_acc_gbm, \ttest_acc_rfc, \ttest_acc_knn, test_acc_svc, test_acc_voting]\n\nscore_df = pd.DataFrame({'classifier': classifier, 'train_acc': train_acc, 'test_acc': test_acc})\nscore2 = score_df.copy()\nscore_df.train_acc = (np.floor(score_df.train_acc*10000)\/100).map('{:,.2f}'.format)\nscore_df.test_acc = (np.floor(score_df.test_acc*10000)\/100).map('{:,.2f}'.format)\nscore_df","43be5313":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[\n    go.Bar(name='Train Acc', x=score_df.classifier, y=score_df.train_acc, text = score_df.train_acc),\n    go.Bar(name='Test Acc', x=score_df.classifier, y=score_df.test_acc, text = score_df.test_acc)\n])\n\n\n\n# Change the bar mode\nfig.update_layout(barmode='group', \n                  title='Performance Comparison of the Classifiers', title_x = 0.5,\n                  yaxis_title = 'Accuracy (%)',\n                  xaxis_title = 'Classifier')\nfig.update_traces(texttemplate='%{text:.s} %', textposition='outside')\nfig.show()","e4d089d9":"sns.set_context('talk')\n\nf, ax = plt.subplots(figsize = (10,6))\nsns.set_color_codes('pastel')\nsns.barplot(x = 'classifier', y = 'train_acc', data = score2,\n            label = 'Train Acc', color = 'r', edgecolor = 'w')\nsns.set_color_codes('muted')\nsns.barplot(x = 'classifier', y = 'test_acc', data = score2,\n            label = 'Test Acc', color = 'g', edgecolor = 'w')\nax.legend(ncol = 2, loc = 'upper right')\nsns.despine(left = True, bottom = True)\n# plt.show()","08175507":"score_df","77992f6a":"## 2.2 Random Forest Classifier","51a54fe1":"## 2.5 Voting All Classifiers","1c5d9130":"# 3. Evaluation\n\n ","7073a737":"### 5. Synthetic Feature <font color='red'> [CategoricalFare] <\/font>","edb7c04f":"# Loading Data","0b3bb99b":"### 2.1.1 Feature Importance Map\nTo have a look at the importance of the feature elements, you can use the following section","165b2b94":"### Final Processed Data","9dcdd4a6":"### 6. Synthetic Feature <font color='red'> [CategoricalAge] <\/font>","7de9d87e":"## 3.1 Making Ground Truth for Test Set (Evaluation Purpose Only)\nHere I am downloading the output of the test set that produces a score of 1.00 (100% Accurate Prediction). However, I am not going to submit it like others just to get at top 1% submission. Rather I am going to perform a performance analysis.","5703de6b":"### 7. Synthetic Feature <font color='red'> [Title] <\/font>\nWe know that in the early twentith century, title meant social status and in titanic survival peoples' social status played a major role in prioratizing who will board on the life saving ship before or after. ","63fc85e9":"### 4. Synthetic Feature <font color='red'> [IsAlone] <\/font>","f45a8bf3":"## 2.3 K- Nearest Neighbour Classifier ","8e36eeaa":"# More is Coming in Ensemble","adb1fe49":"## 2.4 Support Vector Machine Classifier","5ba193a1":"### 3. Synthetic Feature <font color='red'> [FamilySize] <\/font>","a5bd951d":"### Here is the Data Dictionary to Understand what those Features mean\n\n|Variable|Definition\t|Key|\n|:--|:--|:--|\n|survival|\tSurvival|\t0 = No, 1 = Yes|\n|pclass|\tTicket class|\t1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex|\tSex\t||\n|Age|\tAge in years|\t|\n|sibsp|\t# of siblings \/ spouses aboard the Titanic\t||\n|parch|\t# of parents \/ children aboard the Titanic\t||\n|ticket|\tTicket number\t||\n|fare|\tPassenger fare\t||\n|cabin|\tCabin number|\t|\n|embarked|\tPort of Embarkation|\tC = Cherbourg, Q = Queenstown, S = Southampton|\n\n### Variable Notes\n* **pclass:** A proxy for socio-economic status (SES)\n> 1st = Upper <br>\n> 2nd = Middle <br>\n> 3rd = Lower <br>\n\n* **age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* **sibsp:** The dataset defines family relations in this way...\n> Sibling = brother, sister, stepbrother, stepsister <br>\n> Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* **parch:** The dataset defines family relations in this way...\n> Parent = mother, father <br>\n> Child = daughter, son, stepdaughter, stepson <br>\n> Some children travelled only with a nanny, therefore parch=0 for them.","c2159f82":"### 13. Comparison between Pearson and Spearman Correlation of Features. \nIn order to understand this comparison, please have a look at this [small article](https:\/\/support.minitab.com\/en-us\/minitab-express\/1\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/basics\/a-comparison-of-the-pearson-and-spearman-correlation-methods\/#:~:text=The%20Pearson%20correlation%20evaluates%20the%20linear%20relationship%20between%20two%20continuous%20variables.&text=The%20Spearman%20correlation%20coefficient%20is,evaluate%20relationships%20involving%20ordinal%20variables.)\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\nHowever, in the Spearman Correlation Features, it shows a number of features which are correlated. The reason here is that Spearman mainly finds out the correlation among the ordinal features where Pearson Correlation finds the correlation among the linear data. \n","cdabdee8":"### 1. Dealing with the Ticket Number:\nThough at first look it might seem quite obvious that these bogus ticket numbers are rubbish and they dont mean anything. However, on the careful observatrion, it seems that these ticket numbers have some predictive power. Removing them will be a absolute loss for final prediction. So we will categorize the tickets based on the first 3 chracters of the ticket. The following block of code at first takes first 3 characters of the field `Ticket` and makes a categorical feature from them. ","f11b8357":"### 10. Final Feature Selection: \n","72b58fe6":"# Loading Libraries","5be53257":"### 2. Synthetic Feature <font color='red'> [Has_Cabin] <\/font>\nIt seems that those who had cabin, survival rate among them was higher. ","c5e106b0":"# 2. Classification: \n## 2.1 XGBoost\nHere we choose the eXtremely famous library for boosted tree learning model, [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html). It was built to optimize large-scale boosted tree algorithms. To dig dipper into its use and fine tuning for your custom model feel free to look at the [python package documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/index.html) dedicated for XGBoost. \n\nJust a quick run down of the XGBoost parameters used in the model:\n* `max_depth:` How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n* `gamma:` minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n* `eta:` step size shrinkage used in each boosting step to prevent overfitting\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:","cbe6121a":"### 8. Processing Fare and Sex Feature <font color='red'> [Title] <\/font>","b8c95ca9":"### 2. Synthetic Feature <font color='red'> [Words_Count] <\/font>","de021f43":"# <font color='blue'> Data Cleaning & Feature Engineering","fdb8abf8":"### 9. Processing Feature <font color='red'>Embarked<\/font>","d17be0fe":"### 12. <font color='blue'> Spearman's Rank Correlation <\/font>\nIn statistics, Spearman's rank correlation coefficient or Spearman's $rho\\ (\\rho)$ named after Charles Spearman and often denoted by the Greek letter or as, is a nonparametric measure of rank correlation. It assesses how well the relationship between two variables can be described using a monotonic function. \n\n$$\\rho=1-\\frac{6 \\sum d_{i}^{2}}{n (n^{2}-1)}$$\nwhere, <br>\n$\\rho\t$ =\tSpearman's rank correlation coefficient <br>\n$d_{i}\t$ =\tdifference between the two ranks of each observation <br>\n$n\t$ =\tnumber of observations\n","fed1ac21":"### 11. <font color='blue'> Correlation (Pearson) <\/font>\nA correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables or features. The variables may be two columns of a given data set of observations, often called a sample, or two components of a multivariate random variable with a known distribution. \n\nPearson correlation coefficient `(r)`: \n\n<font color='blue'> $$r =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum\\left(x_{i}-\\bar{x}\\right)^{2} \\sum\\left(y_{i}-\\bar{y}\\right)^{2}}}$$ <\/font>\nwhere <br>\n$r$\t=\tcorrelation coefficient <br>\n$x_{i}$\t=\tvalues of the x-variable in a sample<br>\n$\\bar{x}$\t=\tmean of the values of the x-variable<br>\n$y_{i}$\t=\tvalues of the y-variable in a sample<br>\n$\\bar{y}$\t=\tmean of the values of the y-variable\n\nLet's have a look at the correlation of all the features. \n","d43d6972":"### 2.1.2 Decision Tree Visualization\nTo have a look at the original decision tree boundary, just have a look at the follwoing decision tree. Feel free to play with some parameters. "}}