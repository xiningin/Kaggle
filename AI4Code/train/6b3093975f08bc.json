{"cell_type":{"ecc9564f":"code","82163a32":"code","31530a0b":"code","4a7884b5":"code","623165db":"code","702a4b47":"code","44ef5af0":"code","af1d2d87":"code","0eade274":"code","20653162":"code","f4ebd7e2":"code","be5868f1":"code","8c5a4832":"code","9208e7e9":"code","8b7a8de4":"code","72ba0813":"code","d57be557":"code","91c4d81c":"code","0f75a0bd":"code","2df12b4c":"code","2ffb6a72":"code","3e34e36d":"code","11607145":"code","81ea1948":"code","0d02258c":"markdown","9319ec75":"markdown","e654a755":"markdown","0782abfb":"markdown","50a9c13e":"markdown","a4c63061":"markdown","967d3110":"markdown","54abb291":"markdown","acf45e17":"markdown","fa2f307d":"markdown","6eba2117":"markdown","b0702b22":"markdown","fa94b58b":"markdown","da51e74a":"markdown","9d5a73b6":"markdown","997f262f":"markdown","eea10439":"markdown","584a7d9f":"markdown","465d2614":"markdown","d93d68f1":"markdown","7f02c468":"markdown","8662ea16":"markdown","e421edc8":"markdown","26d288be":"markdown","78b18151":"markdown","79364893":"markdown","fc4ed3f5":"markdown"},"source":{"ecc9564f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82163a32":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom sklearn. preprocessing import MinMaxScaler\nfrom numpy import sort\nfrom sklearn.feature_selection import SelectFromModel\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nimport plotly.graph_objects as go\nimport xgboost as xgb\nimport plotly.graph_objects as go\nfrom sklearn.tree import DecisionTreeRegressor\n","31530a0b":"train_df = pd.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv', dtype={'Id':str})\\\n            .dropna().reset_index(drop=True) # Load train dataFrame\nloading_df = pd.read_csv('..\/input\/trends-assessment-prediction\/loading.csv', dtype={'Id':str})","4a7884b5":"# Lets merge train df with loading_df\ntrain_df = train_df.merge(loading_df, on='Id', how='left')\ntrain_df.head() ","623165db":"train_df.shape","702a4b47":"X_train = train_df.iloc[:,6:]  #Train feature\ntarget = train_df.iloc[:,1:6]   #Target feature\nX_train.head()","44ef5af0":"x_train,x_val,y_train,y_val = train_test_split(X_train,target,test_size=0.33,shuffle=True) #Lets split the data","af1d2d87":"scaler = MinMaxScaler()\nscaler.fit(x_train)  ##Fit on train set\nx_train = scaler.transform(x_train)  ##transform train set\nx_val = scaler.transform(x_val)   ##transform validaion set","0eade274":"x_train = pd.DataFrame(x_train,columns=X_train.columns)  ##Convert numpy into dataframe\nx_val = pd.DataFrame(x_val,columns=X_train.columns)","20653162":"loss_wt = [.3, .175, .175, .175, .175]  ##weight for each target variable in calculatting loss (Given)","f4ebd7e2":"def k_loss(weight,y_pred,y_true):   ##Lets define the loss function\n    s = np.sum(np.abs(y_pred-y_true))\/np.sum(y_true)\n    return weight*s","be5868f1":"#model =  LGBMRegressor(random_state=17)  #Lets define model for feature selection\n#model = xgb.XGBRegressor(tree_method= 'gpu_hist') ##\nmodel = DecisionTreeRegressor() #You can use your own model for feature selection","8c5a4832":"col = target.columns","9208e7e9":"loss_dict ={} #To keep the history of loss\nbest_feat_col ={} #Dictionary to store best feature for each target variable\n\nfor i in range(5): #Iterate for every target feature\n    print(\"Selecting best feature subset for \" +str(col[i])+\".....\")\n    min_loss=1000\n    store =[]\n    best_feature=[]\n    #best_feature\n    for x in range(1,27): ##Iterate for x best subset among 26 feature\n        sfs = SFS(model,k_features=x,forward=True,floating=False,scoring = 'neg_mean_squared_error',cv = 0) #For forward selection set forward to True\n        sfs.fit(x_train, y_train.iloc[:,i])\n        col_n = list((sfs.k_feature_names_))\n        model.fit(x_train[col_n],y_train.iloc[:,i])\n        loss = k_loss(loss_wt[i],y_val.iloc[:,i],model.predict(x_val[col_n]))\n        if(loss<min_loss):\n            min_loss=loss\n            best_feature = col_n\n        store.append(loss)\n    best_feat_col[col[i]]= list(best_feature)\n    loss_dict[col[i]]=store\n            \n    ","8b7a8de4":"fig = go.Figure(data=go.Scatter(x=list(range(1,27)), y=loss_dict['age']))\nfig.update_layout(title='Count of best features vs Loss for \"AGE\"',\n                   xaxis_title='Count of features',\n                   yaxis_title='Loss')\nfig.show()","72ba0813":"fig = go.Figure(data=go.Scatter(x=list(range(1,27)), y=loss_dict['domain1_var1']))\nfig.update_layout(title='Count of best features vs Loss for \"domain1_var1\"',\n                   xaxis_title='Count of features',\n                   yaxis_title='Loss')\nfig.show()","d57be557":"fig = go.Figure(data=go.Scatter(x=list(range(1,27)), y=loss_dict['domain1_var2']))\nfig.update_layout(title='Count of best features vs Loss for \"domain1_var2\"',\n                   xaxis_title='Count of features',\n                   yaxis_title='Loss')\nfig.show()","91c4d81c":"fig = go.Figure(data=go.Scatter(x=list(range(1,27)), y=loss_dict['domain2_var1']))\nfig.update_layout(title='Count of best features vs Loss for \"domain2_var1\"',\n                   xaxis_title='Count of features',\n                   yaxis_title='Loss')\nfig.show()","0f75a0bd":"fig = go.Figure(data=go.Scatter(x=list(range(1,27)), y=loss_dict['domain2_var2']))\nfig.update_layout(title='Count of best features vs Loss for \"domain2_var2\"',\n                   xaxis_title='Count of features',\n                   yaxis_title='Loss')\nfig.show()","2df12b4c":"best_feat_col['age']","2ffb6a72":"best_feat_col['domain1_var1']","3e34e36d":"best_feat_col['domain1_var2']  ##Interesting result","11607145":"best_feat_col['domain2_var1']","81ea1948":"best_feat_col['domain2_var2']","0d02258c":"> ### 1. Import Libraries\n> ### 2. Read the data\n> ### 3. Train_test_split\n> ### 4. Preprocessing\n> ### 5. Load the model and apply feature selection algorithm\n> ### 6. Visualize the count of feature vs Loss for each target variable","9319ec75":"## Lets Plot Count of feature vs different Target variables","e654a755":"![image.png](attachment:image.png)","0782abfb":"#### 4.For domain2_var1","50a9c13e":"### Lets see the best features (Extracted by our algorithm)","a4c63061":"#### 1.For Age","967d3110":"# Acknowledgement","54abb291":"#### 3.For domain1_var2","acf45e17":"### 5. Domain2_var2","fa2f307d":"\n\n1. Feature selection by : [https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e](http:\/\/)\n\n2. Sequential Feature Selector : [http:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/](http:\/\/)","6eba2117":"#### 2.For domain1_var1","b0702b22":"### 5. Lets load the model","fa94b58b":"# Introduction","da51e74a":"### 4. Preprocessing ","9d5a73b6":"> #  If you like this notebook please support me with a upvote :)","997f262f":"### 3. Split the data into train and test set","eea10439":"Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve. Irrelevant or partially relevant features can negatively impact model performance.\n#### what are Benefits of performing feature selection before modeling your data?\n\u00b7 Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n\n\u00b7 Improves Accuracy: Less misleading data means modeling accuracy improves.\n\n\u00b7 Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.\n\nIn this notebook I will explain feature selection by Greedy approach using library Sequential Feature Selector.\nHere i am using only loading feature for this task for sake of simplicity.","584a7d9f":"******************************************************************************","465d2614":"# Content","d93d68f1":"### 3. Domain1_var2","7f02c468":"### 2. Domain1_var1","8662ea16":"### 2.Read the data","e421edc8":"#### 1.For domain2_var2","26d288be":"### 1. Import Libraries","78b18151":"### 4. Domain2_var1","79364893":"### 1. Age","fc4ed3f5":"### 5.1 Apply Feature selection using sequential feature selector\n\n#### Overview:\nSequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d. The motivation behind feature selection algorithms is to automatically select a subset of features that is most relevant to the problem. The goal of feature selection is two-fold: We want to improve the computational efficiency and reduce the generalization error of the model by removing irrelevant features or noise.\n\n#### Pseudo Code:\n\n##### Input: Y={y1,y2,...,yd}\n\nThe SFS algorithm takes the whole d-dimensional feature set as input.\n##### Output: Xk={xj|j=1,2,...,k;xj\u2208Y}, where k=(0,1,2,...,d)\n\nSFS returns a subset of features; the number of selected features k, where k<d, has to be specified a priori.\n##### Initialization: X0=\u2205, k=0\n\nWe initialize the algorithm with an empty set \u2205 (\"null set\") so that k=0 (where k is the size of the subset).\n##### Step 1 (Inclusion):\n\nx+= arg max J(xk+x), where x\u2208Y\u2212Xk\nXk+1=Xk+x+\nk=k+1\nGo to Step 1\n\nin this step, we add an additional feature, x+, to our feature subset Xk.\nx+ is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to Xk.\nWe repeat this procedure until the termination criterion is satisfied.\n##### Termination: k=p\n\nWe add features from the feature subset Xk until the feature subset of size k contains the number of desired features p that we specified a priori.\n\n\n\nSource : [http:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/](http:\/\/)"}}