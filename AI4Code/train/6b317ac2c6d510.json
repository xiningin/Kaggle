{"cell_type":{"ff4af28b":"code","9ea48e9e":"code","af980783":"code","7ac69e0e":"code","14008447":"code","c2355b8c":"code","67f2e056":"code","bbe94187":"code","9d22af84":"code","892030d0":"code","8b48be53":"code","df8af907":"code","d775ebf5":"code","fec5d81d":"code","c1903185":"code","392a2b31":"code","8108ecc7":"code","212321b7":"code","29c0348f":"code","0ac3356c":"code","4f187e42":"code","a8ff1108":"code","a02b72e4":"code","5575a682":"code","a58ca6f9":"code","c00470f4":"markdown","f658867f":"markdown","e0016e49":"markdown","d7fb965c":"markdown","9df8deac":"markdown","f2284101":"markdown","56780210":"markdown","e0cf1254":"markdown","2392771b":"markdown","47be927b":"markdown","15a1dd88":"markdown","f0522692":"markdown","c78a6cb4":"markdown","c864a25b":"markdown","06ca483b":"markdown","04c555ab":"markdown","a7528535":"markdown","d3bb8700":"markdown","0e18d349":"markdown"},"source":{"ff4af28b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport io\nimport os\nimport cv2\nimport shutil\nimport tqdm as tqdm\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Model,save_model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Input, BatchNormalization, Activation, LeakyReLU, Concatenate\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split","9ea48e9e":"path = '..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train'\nimg_mask = plt.imread(path + '\/WithMask\/10.png')\nimg_nomask = plt.imread(path + '\/WithoutMask\/10.png')\ntitles = ['mask','no mask']\nimgs = [img_mask, img_nomask]\nrows = 1\ncols = 2\naxes = []\nfig = plt.figure(figsize = (10,6))\nfor a in range(rows*cols):\n    axes.append(fig.add_subplot(rows, cols, a+1))\n    subplot_title = titles[a]\n    axes[-1].set_title(subplot_title)\n    plt.imshow(imgs[a])\n    \nfig.tight_layout()\nplt.show()","af980783":"data = []\nshape = []\ntitles = []\ncount = 0\ncategories=os.listdir(path)\n# loop through each directory and read in images for info \nfor category in categories:\n    folder_path = os.path.join(path, category)\n    img_names = os.listdir(folder_path)\n    \n    for img_name in img_names:\n        img_path = os.path.join(folder_path, img_name)\n        img = cv2.imread(img_path)\n        data.append(img.shape)\n        # so pandas doesn't unpack the shape tuple\n        shape.append(str(img.shape))\n        titles.append(img_name)","7ac69e0e":"df = pd.DataFrame(data, columns = ['length', 'width', 'channels'])\ndf['title'] = titles\ndf['shape'] = shape\nprint(\"Avg length: {:f}\".format(df.length.mean()))\nprint(\"Avg width: {:f}\".format(df.width.mean()))\ndf","14008447":"df['shape'].value_counts()","c2355b8c":"len(df['shape'].unique())","67f2e056":"df.length.describe()","bbe94187":"df.width.describe()","9d22af84":"df[df['length'] != df['width']]","892030d0":"len(df[df['length'] < 224])","8b48be53":"len(df[df['length'] < 100])","df8af907":"len(df[df['length'] < 90])","d775ebf5":"def create_model():\n    input_shape = (100,100,3)\n    num_classes = 2 # with mask, without mask\n    # input layer\n    input_image = Input(shape=input_shape)\n    # 1st Conv layer\n    model = Conv2D(16, (3, 3), activation='relu', padding='valid', input_shape=input_shape)(input_image)\n    model = MaxPooling2D((2, 2),padding='same')(model)\n    # 2nd Conv layer\n    model = Conv2D(32, (3, 3), activation='relu', padding='same')(model)\n    model = MaxPooling2D((2, 2),padding='same')(model)\n    # 3rd Conv layer\n    model = Conv2D(64, (3, 3), activation='relu', padding='same')(model)\n    model = MaxPooling2D((2, 2),padding='same')(model)\n    # 4th Conv layer\n    model = Conv2D(128, (3, 3), activation='relu', padding='same')(model)\n    model = MaxPooling2D((2, 2),padding='same')(model)\n    # 5th Conv layer\n    model = Conv2D(256, (3, 3), activation='relu', padding='same')(model)\n    model = MaxPooling2D((2, 2),padding='same')(model)\n    # FC layers\n    model = Flatten()(model)\n    model = Dense(1024)(model)\n    model = Dense(64)(model)\n\n    output= Dense(num_classes, activation='sigmoid')(model)\n\n    model = Model(inputs=[input_image], outputs=[output])\n    return model\n\nmodel = create_model()\nmodel.summary()","fec5d81d":"train_dir = '..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train'\ntest_dir = '..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test'\nval_dir = '..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Validation'\n\nimg_shape = (100, 100)\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range = 20,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   horizontal_flip = True,\n                                   fill_mode = 'nearest')\n\ntest_datagen = ImageDataGenerator()\nval_datagen = ImageDataGenerator()\n\ntrain_batchsize = 16\ntest_batchsize = 16\nval_batchsize = 16\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size = img_shape,\n                                                    batch_size = train_batchsize,\n                                                    class_mode = 'categorical')\n\ntest_generator = test_datagen.flow_from_directory(test_dir,\n                                                  target_size = img_shape,\n                                                  batch_size = test_batchsize,\n                                                  class_mode = 'categorical',\n                                                  shuffle = False)\n\nval_generator = val_datagen.flow_from_directory(val_dir,\n                                                  target_size = img_shape,\n                                                  batch_size = val_batchsize,\n                                                  class_mode = 'categorical',\n                                                  shuffle = False)","c1903185":"# Function to visualize performance at the end of training. \ndef plot_history(history, name):\n    plt.figure(figsize=(8,3))\n    plt.subplot(1,2,1)\n    plt.plot(history.history[\"loss\"])\n    plt.plot(history.history[\"val_loss\"])\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.title(\"loss\")\n    # plt.yscale('log')\n\n    plt.subplot(1,2,2)\n    plt.plot(history.history[\"accuracy\"])\n    plt.plot(history.history[\"val_accuracy\"])\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.title(\"metric\")\n\n    plt.savefig(name)","392a2b31":"#optimizer = tfa.optimizers.RectifiedAdam(lr = 1e-3, warmup_proportion = 0.2, min_lr = 1e-6)\n#optimizer = tfa.optimizers.LazyAdam(learning_rate = 0.001)\noptimizer = 'adam'\n#loss = tfa.losses.SigmoidFocalCrossEntropy(from_logits = True)\nloss = 'binary_crossentropy'\nmetrics = 'accuracy'\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('mask_detection.h5',\n                                                monitor = 'val_loss',\n                                                mode = 'min',\n                                                save_best_only = True,\n                                                verbose = 0)\n\nearlystop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n                                             min_delta = 0,\n                                             patience = 10,\n                                             verbose = 0,\n                                             restore_best_weights = True)\n\n#callbacks = [earlystop, checkpoint]\ncallbacks = [checkpoint]\n\n# change metric to f1 score?\nmodel.compile(loss = loss,optimizer = optimizer,metrics = metrics)\n\nn_train_samples = 10000\nn_test_samples = 992\nn_val_samples = 800\nepochs = 30\n#batch_size = 8 * strategy.num_replicas_in_sync\nbatch_size = 32\n\nhistory = model.fit_generator(train_generator,\n                                 steps_per_epoch = n_train_samples \/\/ batch_size,\n                                 epochs = epochs,\n                                 callbacks = callbacks,\n                                 validation_data = val_generator,\n                                 validation_steps = n_val_samples \/\/ batch_size)\n                                 #validation_steps = test_batchsize)\n\nplot_history(history, 'history_%d.png')\n\nmodel.save('mask_detection.h5')","8108ecc7":"from tensorflow.keras.models import load_model\nfrom os import listdir\nfrom os.path import isfile, join\n\n# load model\nclassifier = load_model('mask_detection.h5')\n\n# dictionary of labels\nmask_dict = {'[0]': 'with mask',\n             '[1]': 'without mask'}\n\n# function to draw predicted class on image\ndef draw_test(category, pred, img):\n    classes = mask_dict[str(pred)]\n    BLACK = [0,0,0]\n    expanded_image = cv2.copyMakeBorder(img, 80, 0, 0, 100, cv2.BORDER_CONSTANT, value = BLACK)\n    expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_BGR2RGB)\n    cv2.putText(expanded_image, classes, (0, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n    #cv2.imshow(category, expanded_image)\n    plt.imshow(expanded_image)\n\n# function to get a random image for testing\ndef getRandomImage(path):\n    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n    random_directory = np.random.randint(0, len(folders))\n    path_class = folders[random_directory]\n    print(\"True Class - \" + str(path_class))\n    file_path = path + '\/' + path_class\n    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n    random_file_index = np.random.randint(0, len(file_names))\n    image_name = file_names[random_file_index]\n    print(image_name)\n    return cv2.imread(file_path+\"\/\"+image_name), str(path_class)\n\n# gets a random image, resizes it, then draws predicition\ndef check_model(path, predictor):\n    img, true_class = getRandomImage(path)\n    orig = img.copy()\n    orig = cv2.resize(orig, None, fx = 0.5, fy = 0.5, interpolation = cv2.INTER_LINEAR)\n    img = cv2.resize(img, (100,100), interpolation = cv2.INTER_LINEAR)\n    img = img\/255.\n    img = img.reshape(1,100,100,3)\n    # get prediction\n    res = np.argmax(predictor.predict(img, 1, verbose = 0), axis = 1)\n    # show image with predicted class\n    draw_test(\"Prediction\", res, orig)","212321b7":"check_model(val_dir, classifier)","29c0348f":"results = model.evaluate_generator(test_generator)\nprint('Test Loss:', results[0])\nprint('Test Accuracy:', results[1])","0ac3356c":"from sklearn.metrics import classification_report\n\npreds = model.predict_generator(test_generator)\npreds = np.argmax(preds, axis = 1)\nactual = test_generator.classes\nlabels = ['with_mask', 'without_mask']\nprint(classification_report(actual,preds,target_names = labels))","4f187e42":"def build_xception():\n    # Define input shape\n    input_shape = (100,100,3)\n    # 2 classes for mask and no mask\n    num_classes = 2\n    # Build Model: define input\n    input_image = Input(shape=input_shape)\n    # Xception expects a specific type of preprocessing: https:\/\/keras.io\/api\/applications\/xception\/\n    model = tf.keras.applications.xception.preprocess_input(input_image)\n    # Call Xception model through Keras API\n    model = tf.keras.applications.Xception(include_top = False, weights = 'imagenet', input_shape = input_shape)(model)\n    # Max pooling for dominant feature selection\n    model = MaxPooling2D((2, 2),padding='same')(model)\n    # Normalize inputs\n    model = BatchNormalization()(model)\n    # Flatten inputs to get proper dimensions\n    model = Flatten()(model)\n    # Fully Connected layer \n    model = Dense(1024)(model)\n    #model = Dense(64)(model)\n    # Add some dropout\n    model = Dropout(0.4)(model)\n    # Output\n    output= Dense(num_classes, activation='sigmoid')(model)\n    # instantiate model\n    model = Model(inputs=[input_image], outputs=[output])\n    return model\n\nxception = build_xception()\nxception.get_layer('xception').trainable = False\nxception.summary()","a8ff1108":"#optimizer = tfa.optimizers.RectifiedAdam(lr = 1e-3, warmup_proportion = 0.2, min_lr = 1e-6)\n#optimizer = tfa.optimizers.LazyAdam(learning_rate = 0.001)\noptimizer = 'adam'\n#loss = tfa.losses.SigmoidFocalCrossEntropy(from_logits = True)\nloss = 'binary_crossentropy'\n#loss = 'categorical_crossentropy'\nmetrics = 'accuracy'\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('xception.h5',\n                                                monitor = 'val_loss',\n                                                mode = 'min',\n                                                save_best_only = True,\n                                                verbose = 0)\n\nearlystop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n                                             min_delta = 0,\n                                             patience = 10,\n                                             verbose = 1,\n                                             restore_best_weights = True)\n\n#callbacks = [earlystop, checkpoint]\ncallbacks = [checkpoint]\n\n# change metric to f1 score?\nxception.compile(loss = loss,optimizer = optimizer,metrics = metrics)\n\nn_train_samples = 10000\nn_test_samples = 992\nn_val_samples = 800\nepochs = 30\n#batch_size = 8 * strategy.num_replicas_in_sync\nbatch_size = 32\n\nhistory = xception.fit_generator(train_generator,\n                                 steps_per_epoch = n_train_samples \/\/ batch_size,\n                                 epochs = epochs,\n                                 callbacks = callbacks,\n                                 validation_data = val_generator,\n                                 validation_steps = n_test_samples \/\/ batch_size)\n                                 #validation_steps = test_batchsize)\n\nplot_history(history, 'history_%d.png')\n\nxception.save('xception.h5')","a02b72e4":"check_model(val_dir, xception)","5575a682":"xception.evaluate_generator(test_generator)","a58ca6f9":"preds = xception.predict_generator(test_generator)\npreds = np.argmax(preds, axis = 1)\nactual = test_generator.classes\nlabels = ['with_mask', 'without_mask']\nprint(classification_report(actual,preds,target_names = labels))","c00470f4":"### However, most images seem to fall between (100, 100) and (224, 224).  Since binary classification is fairly simple, let's go with (100, 100) to save on processing time. ","f658867f":"### Length and width are distributed the exact same way. ","e0016e49":"## DataGenerators\n### Continuing, we'll use ImageDatagenerators to feed our image data into the model. ImageDataGenerators augment datasets in real time. Read more here: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\n\n### Since the data directories are set up so nicely, we can use the 'flow_from_directory' method to easily create our training, test, and validation datasets.","d7fb965c":"### Let's fit a CNN from scratch on this dataset to detect whether or not a person is wearing a mask. Once we've done that, we'll compare our simple CNN to a model using transfer learning, specifically Xception with ImageNet weights. ","9df8deac":"### 9994 out of 10k images are less than or equal to (224, 224)","f2284101":"## EDA\n### Let's get a look at a couple of the images in the data set. One with the mask, and one without.","56780210":"## Transfer Learning  \n### Here we'll use the Xception model with weights from ImageNet, and add some more layers so that it can train on this dataset.","e0cf1254":"### About 40% of our training data is 224, 224, 3.","2392771b":"### There are no images where the length does not equal the width.","47be927b":"### 98% test accuracy is quite good. The high test loss implies that the model isn't necessarily confident in the predictions, but the high accuracy shows that the predictions tend to be correct.","15a1dd88":"## Model Training\n### We'll use an adam optimizer for the learning rate and binary_crossentropy for the loss since we only have two categories. ","f0522692":"## Build Model\n### Now, we'll define a function to build our CNN in Keras. We'll be using the Functional API from Keras, which can be found here: https:\/\/keras.io\/guides\/functional_api\/","c78a6cb4":"### Training loss and accuracy look like they have converged to their local extrema, but validation loss in particular seems to still be jumping around after 30 epochs. ","c864a25b":"## Evaluate Model","06ca483b":"### 50% accuracy and very high loss. Looking at the history plots, this model definitely hasn't converged. Also, the hyperparameters may need more tuning to suit Xception, as this model is using the same hyperparameters as the much simper CNN we trained above it. Better hyperparameters may be found here: https:\/\/arxiv.org\/pdf\/1610.02357.pdf. Furthermore, the ImageNet weights were trained to classify 1000 different classes, while this task is only binary classification.  Due to the relative simplicity of the task, transfer learning may require more work than it is worth, and training a simpler network from scratch could be more efficient.","04c555ab":"### Very good precision and recall. Precision is True Positive \/ (True Positive + False Positive), so when the model predicts 'with mask', it is correct 99% of time. Recall is True Positive \/ (True Positive + False Negative), so the model was able to correctly identify 99% of all 'without_mask' images in the test set. ","a7528535":"### The training data has 139 unique image shapes amongst 10,000 images.","d3bb8700":"### Very poor precision and recall. Precision is True Positive \/ (True Positive + False Positive), so when the model predicts 'without mask', it is correct 51% of time. However, given the 4% recall on 'with_mask', that means it correctly identified 4% of all 'with_mask' images, meaning this model mainly chose 'without_mask'. The 'without_mask' recall being so high also indicates that this model mainly predicted 'without_mask'.","0e18d349":"### Next, let's figure out a reasonable image size to feed into the model.\n### We'll start by reading in the image shapes into a dataframe so that we can explore the data."}}