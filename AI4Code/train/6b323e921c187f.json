{"cell_type":{"3efe05c0":"code","893e0004":"code","e41718f4":"code","040bed1f":"code","d1dea885":"code","ee70ff73":"code","939a15c8":"code","373b6e7d":"code","d86a01c4":"code","84f7664d":"code","d2c4b7ad":"code","e0a44246":"code","438225f7":"code","97587ad5":"code","c521dab4":"code","64551f65":"code","622fdb3f":"code","b50bf62b":"code","de6e219d":"code","570dc470":"code","f7935d4e":"code","29dbffd0":"code","b67193ba":"code","57342a63":"code","299a113c":"code","01397eeb":"code","b7712614":"code","b99726b9":"code","e28dae7a":"code","b933509f":"code","70dc8b58":"code","56f640fc":"code","eaf679ca":"markdown","4d12c87d":"markdown","f284f11a":"markdown","b44ebcd6":"markdown","00565f02":"markdown","32f50f94":"markdown","93ace686":"markdown","a35ff2a3":"markdown","3d7d2810":"markdown","8fb61658":"markdown","43645143":"markdown","39931823":"markdown","f4481934":"markdown","6063bf77":"markdown","d86a5f2f":"markdown","d6e91ddb":"markdown","ed2df6a8":"markdown","082bfb5c":"markdown","d07c16d6":"markdown","cdef7a13":"markdown","5efa6df3":"markdown","d24f32a9":"markdown","0c9af048":"markdown","d1c04b76":"markdown","75b0f040":"markdown","1d0b5dab":"markdown","e20bc610":"markdown","b4963573":"markdown","9ec03054":"markdown","f44c4177":"markdown","58aaf05f":"markdown","3120198a":"markdown","d79b1b31":"markdown","257159dd":"markdown","963ae7cf":"markdown","0a740b2d":"markdown","48bd5ce3":"markdown","d3a37918":"markdown","a7286fdd":"markdown","e480e106":"markdown","0045d561":"markdown","4dde4d2d":"markdown","c28fe4cb":"markdown","522c3675":"markdown"},"source":{"3efe05c0":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport itertools\nfrom sklearn import metrics\nfrom scipy.stats import norm, rankdata\n\npd.set_option('display.max_columns', 200)\n# below is to have multiple outputs from same Jupyter cells\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nwarnings.filterwarnings('ignore')","893e0004":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","e41718f4":"print(\"Train Shape\\n\")\ntrain_df.shape\nprint(\"Test Shape\\n\")\ntest_df.shape","040bed1f":"print(\"Train Describe\\n\")\ntrain_df.describe() \nprint(\"Test Describe\\n\")\ntest_df.describe()","d1dea885":"train_df[\"target\"].value_counts()\/train_df.shape[0]*100\nfig,ax= plt.subplots()\nsns.countplot(data=train_df,x=\"target\",ax=ax)\nax.set(xlabel=\"Target\",\n       ylabel=\"Count\", \n       Title = \"Target Distribution\"\n       )","ee70ff73":"train_df.isnull().sum().sum()\ntest_df.isnull().sum().sum()","939a15c8":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","373b6e7d":"def plot_feature_boxplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    \n    ","d86a01c4":"def plot_feature_violinplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    ","84f7664d":"def plot_binned_feature_target_violinplot(df,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(200,1,figsize=(8,380))\n\n    for feature in features:\n        bins = np.nanpercentile(df[feature], range(0,101,10))\n        df[feature+\"_binned\"] = pd.cut(df[feature],bins=bins)\n        i += 1\n        plt.subplot(200,1,i)\n        sns.violinplot(y=df[feature+\"_binned\"], x=target, showfliers=False)\n        plt.title(feature+'_binned & Target', fontsize=12)\n        plt.ylabel('')\n        plt.xlabel('')\n       \n        locs, labels = plt.xticks()\n        plt.xticks([0.0,1.0])\n        plt.tick_params(axis='y', which='major', labelsize=8)\n        #ax.set_xticks([0.15, 0.68, 0.97])\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();","d2c4b7ad":"def add_new_feature_row(df,features):\n    for feature in features:\n        df[feature+\"_pct\"] = df[feature].pct_change()\n        df[feature+\"_diff\"] = df[feature].diff()\n        df.drop(feature,axis=1)\n    return df\n    ","e0a44246":"def normalize_df(df,features):\n    for feature in features:\n        #normalize\n        df[feature+'_norm'] = (df[feature] - df[feature].mean())\/df[feature].std()\n        #percentage change row wise\n        #df[feature+\"_pct\"] = df[feature].pct_change() # didnt give boost\n        #diff change row wise\n        #df[feature+\"_diff\"] = df[feature].diff() # didnt give boost\n        # Square\n        #df[feature+'^2'] = df[feature] * df[feature]\n        # Cube\n        #df[feature+'^3'] = df[feature] * df[feature] * df[feature]\n        # 4th power\n        #df[feature+'^4'] = df[feature] * df[feature] * df[feature] * df[feature]\n        # Cumulative percentile (not normalized)\n        #df[feature+'_cp'] = rankdata(df[feature]).astype('float32')\n        # Cumulative normal percentile\n        #df[feature+'_cnp'] = norm.cdf(df[feature]).astype('float32')\n    return df\n    ","438225f7":"t0 = train_df.loc[train_df['target'] == 0] # segregate in two datasets correseponding to target\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:102] # run time errors forced this step to split into 100 sets\nplot_feature_distribution(t0, t1, '0', '1', features)\n","97587ad5":"features = train_df.columns.values[102:200] \nplot_feature_distribution(t0, t1, '0', '1', features)","c521dab4":"target   = train_df[\"target\"]\nfeatures = train_df.columns.values[2:102]\nplot_feature_boxplot(train_df, test_df, 'train', 'test', features, target)","64551f65":"features = train_df.columns.values[102:200]\nplot_feature_boxplot(train_df, test_df, 'train', 'test', features, target )","622fdb3f":"features = train_df.columns.values[2:102]\nplot_feature_violinplot(train_df, test_df, 'train', 'test', features, target)","b50bf62b":"features = train_df.columns.values[102:200]\nplot_feature_violinplot(train_df, test_df, 'train', 'test', features, target)","de6e219d":"correlations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index() #\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']] # remove corr between same cols\n","570dc470":"#highest correlated features are\ncorrelations.tail(10)","f7935d4e":"correlations = correlations.iloc[-25:,]\nplt.figure()\nfig, ax = plt.subplots(figsize=(10,12))\nsns.heatmap(correlations.pivot_table(index='level_0',columns='level_1'))\nplt.xlabel(\"\")\nplt.ylabel(\"\")\n#plt.xticks([], [])\n#plt.yticks([], [])\nplt.xticks(rotation=70)\nplt.title(\"Corr plot between top 25 vars\",fontsize=14)\nplt.show()","29dbffd0":"#features = train_df.columns.values[2:102]\n#plot_binned_feature_target_violinplot(train_df,features,target)","b67193ba":"#features = train_df.columns.values[102:200]\n#plot_binned_feature_target_violinplot(train_df,features,target)","57342a63":"import gc\ngc.collect()","299a113c":"test_df['target']= np.nan\ncombine_df = train_df.append(test_df,ignore_index=True)","01397eeb":"features = train_df.columns.values[2:]\ncombine_df = normalize_df(combine_df,features)","b7712614":"train_df = combine_df[combine_df['target'].notnull()].reset_index(drop=True)\ntest_df = combine_df[combine_df['target'].isnull()].reset_index(drop=True)\n","b99726b9":"#features = train_df.columns.values[201:]\n#plot_binned_feature_target_violinplot(train_df,features,target)","e28dae7a":"#test_df = test_df.drop(\"target\",axis=1)\npredictors = train_df.columns.values.tolist()[2:]\nnfold = 10\ntarget = 'target'","b933509f":"param = {\n     'num_leaves': 18,\n     'max_bin': 63,\n     'min_data_in_leaf': 5,\n     'learning_rate': 0.010614430970330217,\n     'min_sum_hessian_in_leaf': 0.0093586657313989123,\n     'feature_fraction': 0.056701788569420042,\n     'lambda_l1': 0.060222413158420585,\n     'lambda_l2': 4.6580550589317573,\n     'min_gain_to_split': 0.29588543202055562,\n     'max_depth': 49,\n     'save_binary': True,\n     'seed': 1337,\n     'feature_fraction_seed': 1337,\n     'bagging_seed': 1337,\n     'drop_seed': 1337,\n     'data_random_seed': 1337,\n     'objective': 'binary',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': 'auc',\n     'is_unbalance': True,\n     'boost_from_average': False\n}\n\n\nnfold = 10\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    #print(\"after lgb train\")\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n    #print(\"after lgb test\")\n    nround = 8523\n    clf = lgb.train(param, \n                    xg_train, \n                    nround, \n                    valid_sets = [xg_valid], \n                    early_stopping_rounds=250,\n                    verbose_eval=250)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=nround) \n    #print(\"after lgb fit\")\n    predictions += clf.predict(test_df[predictors], num_iteration=nround) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.4f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","70dc8b58":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(clf, max_num_features=100, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","56f640fc":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"sant_lgb.csv\", index=False)\nsub_df[:10]","eaf679ca":"Separate out train and test. Append new features created to training dataset.","4d12c87d":"<div id=\"LibLink\">\n**Import libraries**\n<\/div>","f284f11a":"Basic statistics for datasets","b44ebcd6":"Boxplot 1-100 features","00565f02":"To plot violinplot of features of two datasets, along with class split  **plot_feature_violinplot**","32f50f94":"This is model lifted and shifted from [Fayaz's](https:\/\/www.kaggle.com\/fayzur\/lightgbm-customer-transaction-prediction) kernel.","93ace686":"Lets separate the dataset for positive and negative class and check if feature distributions give us some signal.Lift and shift from [Gabriel's](https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction) kernel.","a35ff2a3":"Feature Importance as per model","3d7d2810":"None of dataset has any missing values. ","8fb61658":"Violin Plot 100-200 features","43645143":"Looking at output of describe for both df, data seems to be similar in both the datasets (test and train).  Another point is test is of same size as train. we need to find a way to extract some info from test data.","39931823":"### Binning","f4481934":"<div id=\"EDALink\">\n **Basic EDA**\n <\/div>","6063bf77":"### Missing values","d86a5f2f":"Violin Plot 1-100 features","d6e91ddb":"<div id = ModLink>\n** Modeling **\n    <\/div>","ed2df6a8":"Correlation plot also shows not relation between features, looks to be pretty independent of each other.","082bfb5c":"Submission file","d07c16d6":"To plot boxplot of features of two datasets, along with class split  **plot_feature_boxplot**","cdef7a13":"<div id=\"PlotLink\">\n**Plotting**\n    <\/div>","5efa6df3":"To plot violinplot of binned features for training along with class split  **plot_binned_feature_target_violinplot**","d24f32a9":"Distribution of target in training dataset. This shows its a imbalance data set, with 90% of data being 0 and 10% as 1.","0c9af048":"<div id = FeatLink>\n** Feature Engineering **\n    <\/div>","d1c04b76":"Lets find out corr between features, we may be able to drop couple of features if highly correlated. ","75b0f040":"To add new features row wise  **add_new_feature_row**","1d0b5dab":"Boxplot 100-200 features","e20bc610":"To normailize features using combined dataset **normalize_df**","b4963573":"Read files ","9ec03054":"Distplot for 1-100 features","f44c4177":"Lets try to find if data is some sort of time series data. as one of the post was doubting. We will try to add features which will be row wise, like percentage increase from one row to next, difference from one row to next, ratio etc.","58aaf05f":"Distplot for 100-200 features","3120198a":"Lets see how the train and test features affects target. It may give some signal if some feature is more important for target prediction.","d79b1b31":"<div id=\"FuncLink\">\n** Utility Functions for EDA and Feature Engineering **\n    <\/div>","257159dd":"lets try to find if binning of the features shows some trend for predicting target. We will use consistent pattern of using aa utility function and calling with 100 features in one call. ","963ae7cf":"Next we are going to combine is two data sets and try to extract some info from test dataset into train features. This idea is from [William's](https:\/\/www.kaggle.com\/blackblitz\/gaussian-naive-bayes) kernel. Wel will create a ratio \/pct_change\/diff as new features to factor for time series hypothesis.","0a740b2d":"I am trying to analyze data thru diff views to get a clue which features may be impacting target. My intenetion is to keep things simple and easily comprehendable. I myself get lost sometimes in good kernels which are bit low on structure part.  I have tried to keep it structured and scalable for new features and models. ","48bd5ce3":"To plot distributions features of two datasets  **plot_feature_distribution**","d3a37918":"Highest correlation between top 10 features is as follows","a7286fdd":"<div id=CorLink>\n** Correlation and Binning **\n<\/div>","e480e106":"Plot selectively if new any new features give some insight for target prediction.","0045d561":"-  [Import and Read](#LibLink)\n-  [Basic EDA](#EDALink)\n-  [Functions](#FuncLink)\n-  [Plotting](#PlotLink)\n-  [Corr and Bin](#CorLink)\n-  [Features](#FeatLink)\n-  [Model](#ModLink)","4dde4d2d":"All these plots above shows data between train and test is very much similar and mostly normally distributed. We may be able to use test dataset for extracting some info assuming its homogeneous with train. ","c28fe4cb":"Number of rows and columns in Dataset","522c3675":"Binning plots also does not show any different story. "}}