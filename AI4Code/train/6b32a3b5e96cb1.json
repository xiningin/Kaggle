{"cell_type":{"f7956119":"code","d89f61d0":"code","469b69ad":"code","43dc9cb8":"code","65bf0859":"code","cbe5dbcf":"code","9a9bea04":"code","c8ab1c06":"code","377d581e":"code","3f70d087":"code","955509fd":"code","40ab2fd3":"code","dd2925fd":"code","73d22181":"code","c932a51f":"code","f9ac1fe3":"code","46ba4743":"code","6376b413":"code","40008dbf":"code","15cb31c9":"markdown","1cd0cf97":"markdown","2eed63f4":"markdown","e2d6bbd4":"markdown","2019e2ba":"markdown","ac5a2cf3":"markdown","5c6428b1":"markdown","e38f2bb1":"markdown","91bdcc5e":"markdown","e0062705":"markdown","ded81616":"markdown","1abdb029":"markdown","d03d5a44":"markdown","22152b19":"markdown"},"source":{"f7956119":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 500)\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nfrom cycler import cycler # for cycling through colors in a graph\n\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn import neighbors as neigh\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn import metrics\nimport warnings; warnings.filterwarnings('ignore')","d89f61d0":"first_yr = pd.read_csv('..\/input\/world-happiness-report\/2015.csv')\nsecond_yr = pd.read_csv('..\/input\/world-happiness-report\/2016.csv')\nthird_yr = pd.read_csv('..\/input\/world-happiness-report\/2017.csv')\nfourth_yr = pd.read_csv('..\/input\/world-happiness-report\/2018.csv')\nfifth_yr = pd.read_csv('..\/input\/world-happiness-report\/2019.csv')\nsixth_yr = pd.read_csv('..\/input\/world-happiness-report\/2020.csv')","469b69ad":"#Sorting data by happiness ranks and dropping the happiness rank columns\nfirst_yr.sort_values('Happiness Rank', inplace=True)\nfirst_yr.drop('Happiness Rank', inplace=True, axis=1)\nsecond_yr.sort_values('Happiness Rank', inplace=True)\nsecond_yr['Standard Error'] = (second_yr['Upper Confidence Interval'] - second_yr['Lower Confidence Interval'])\/2\nsecond_yr.drop(['Happiness Rank', 'Upper Confidence Interval', 'Lower Confidence Interval'], inplace=True, axis=1)\nthird_yr.sort_values('Happiness.Rank', inplace=True)\nthird_yr['Standard Error'] = (third_yr['Whisker.high'] - third_yr['Whisker.low'])\/2\nthird_yr.drop(['Happiness.Rank', 'Whisker.high', 'Whisker.low'], inplace=True, axis=1)\nfourth_yr.sort_values('Overall rank', inplace=True)\nfourth_yr.drop('Overall rank', inplace=True, axis=1)\nfifth_yr.sort_values('Overall rank', inplace=True)\nfifth_yr.drop('Overall rank', inplace=True, axis=1)\n\n#Changing column names for consistency\nfourth_yr.rename(columns={'Country or region':'Country'}, inplace=True)\nfifth_yr.rename(columns={'Country or region':'Country'}, inplace=True)\nsixth_yr.columns = ['Country', 'Region', 'Happiness Score',\n       'Standard error of ladder score', 'upperwhisker', 'lowerwhisker',\n       'Economy (GDP per Capita)', 'Social support', 'Health (Life Expectancy)',\n       'Freedom', 'Generosity',\n       'Trust (Government Corruption)', 'Dystopia Residual',\n       'Explained by: Log GDP per capita', 'Explained by: Social support',\n       'Explained by: Healthy life expectancy',\n       'Explained by: Freedom to make life choices',\n       'Explained by: Generosity', 'Explained by: Perceptions of corruption',\n       'Dystopia + residual']\n\n#Changing conflicting region names\nsecond_yr.Region.replace({'East Asia':'Eastern Asia', 'South Asia':'Southern Asia', 'Southeast Asia':'Southeastern Asia', 'Middle East and North Africa':'Middle East and Northern Africa'}, inplace=True)\nfirst_yr.Region.replace({'East Asia':'Eastern Asia', 'South Asia':'Southern Asia', 'Southeast Asia':'Southeastern Asia', 'Middle East and North Africa':'Middle East and Northern Africa'}, inplace=True)\nsixth_yr.Region.replace({'East Asia':'Eastern Asia', 'South Asia':'Southern Asia', 'Southeast Asia':'Southeastern Asia', 'Middle East and North Africa':'Middle East and Northern Africa'}, inplace=True)\n\n\n#Adding region names in 2017,2018, 2019 where they were missing\nregions = pd.concat([second_yr[['Country','Region']], first_yr[['Country', 'Region']], sixth_yr[['Country','Region']]])\nregions.drop_duplicates(subset='Country', keep='first')\nthird_yr = third_yr.join(regions.set_index('Country'), on='Country')\nfourth_yr = fourth_yr.join(regions.set_index('Country'), on='Country')\nfifth_yr = fifth_yr.join(regions.set_index('Country'), on='Country')\n\n#Dropping duplicate rows\nthird_yr.drop_duplicates(subset='Country', inplace=True, keep='first')\nfourth_yr.drop_duplicates(subset='Country', inplace=True, keep='first')\nfifth_yr.drop_duplicates(subset='Country', inplace=True, keep='first')\n\n\n#Filling missing Region values\nthird_yr.Region.fillna('Southeastern Asia',inplace=True)\nfourth_yr.Region[fourth_yr.Country == 'Trinidad & Tobago'] = 'Latin America and Caribbean'\nfourth_yr.Region.iloc[fourth_yr.Country == 'Northern Cyprus'] = 'Middle East and Northern Africa'\nfifth_yr.Region[fifth_yr.Country == 'Trinidad & Tobago'] = 'Latin America and Caribbean'\nfifth_yr.Region.iloc[fifth_yr.Country == 'Northern Cyprus'] = 'Middle East and Northern Africa'\nfifth_yr.Region.iloc[fifth_yr.Country == 'North Macedonia'] = 'Western Europe'\n\n#Filling the only missing value remaining\nfourth_yr['Perceptions of corruption'].fillna(method='bfill', inplace=True)\n\n#Making column names same\nthird_yr.columns=['Country', 'Happiness Score', 'Economy (GDP per Capita)',\n       'Family', 'Health (Life Expectancy)', 'Freedom', 'Generosity',\n       'Trust (Government Corruption)', 'Dystopia Residual',\n       'Standard Error','Region']\nfourth_yr.columns = ['Country', 'Happiness Score', 'Economy (GDP per Capita)','Social support', \n       'Health (Life Expectancy)', 'Freedom', 'Generosity',\n       'Trust (Government Corruption)', 'Region']\nfifth_yr.columns = ['Country', 'Happiness Score', 'Economy (GDP per Capita)', 'Social support',\n       'Health (Life Expectancy)', 'Freedom', 'Generosity',\n       'Trust (Government Corruption)',\n       'Region']","43dc9cb8":"#Aggregating ranks of countries over various years\nfirst_yr[\"Rank 2015\"]  = first_yr.index\nsecond_yr[\"Rank 2016\"] = second_yr.index\nthird_yr[\"Rank 2017\"]  = third_yr.index\nfourth_yr[\"Rank 2018\"] = fourth_yr.index\nfifth_yr[\"Rank 2019\"]  = fifth_yr.index\nsixth_yr[\"Rank 2020\"]  = sixth_yr.index\n\n#Setting the index to country names\nfirst_yr.set_index('Country', inplace=True)\nsecond_yr.set_index('Country', inplace=True)\nthird_yr.set_index('Country', inplace=True)\nfourth_yr.set_index('Country', inplace=True)\nfifth_yr.set_index('Country', inplace=True)\nsixth_yr.set_index('Country', inplace=True)","65bf0859":"Rank = pd.concat([first_yr[\"Rank 2015\"], second_yr[\"Rank 2016\"], third_yr[\"Rank 2017\"], fourth_yr[\"Rank 2018\"], fifth_yr[\"Rank 2019\"], sixth_yr[\"Rank 2020\"]], axis=1)","cbe5dbcf":"plt.rc('axes', prop_cycle=(cycler('color', [(0.2,0,0),(0,0,0.2),(0,0.2,0),(0,0,0.5),(0.5,0,0),(0,0.5,0),(0,0,0.9),(0,0.9,0),(0,0.5,0.9),(0.9,0.5,0),(0.5,0,0.9),(0.5,0.9,0),(0,0.9,0.5),(0.9,0,0.5),(0,0.5,0.5),(0.5,0.5,0),(0.5,0,0.5)])))\n\nfor i in np.arange(0,11,1):\n    Rank.iloc[i].plot(figsize=(15,8))\n    plt.text(2.7,Rank.iloc[i,3], s=Rank.index[i])\n\nplt.title('Top 10 happiest countries during 2015-2020', fontsize=28)","9a9bea04":"#Dropping columns which just numerically add up to the happiness index\ndataset = sixth_yr.drop(['Explained by: Log GDP per capita', 'Explained by: Social support',\n       'Explained by: Healthy life expectancy',\n       'Explained by: Freedom to make life choices',\n       'Explained by: Generosity', 'Explained by: Perceptions of corruption',\n       'Dystopia + residual','upperwhisker','lowerwhisker','Standard error of ladder score'],axis=1)\n\n#Renaming columns\ndataset.rename(columns={'Ladder score':'Happiness index'}, inplace=True)\n\n#Score of Dystopia is same for all countries so let's drop it\ndataset.drop(['Dystopia Residual'], axis=1, inplace=True)\n\n#Dropping the rank column as it does not contribute to the happiness index score, it is an outcome\ndataset.drop(['Rank 2020'], axis=1, inplace=True)","c8ab1c06":"#Dropping columns which just numerically add up to the happiness index\ndataset = sixth_yr.drop(['Explained by: Log GDP per capita', 'Explained by: Social support',\n       'Explained by: Healthy life expectancy',\n       'Explained by: Freedom to make life choices',\n       'Explained by: Generosity', 'Explained by: Perceptions of corruption',\n       'Dystopia + residual','upperwhisker','lowerwhisker','Standard error of ladder score'],axis=1)\n\n#Renaming columns\ndataset.rename(columns={'Ladder score':'Happiness index'}, inplace=True)\n\n#Score of Dystopia is same for all countries so let's drop it\ndataset.drop(['Dystopia Residual'], axis=1, inplace=True)\n\n#Dropping the rank column as it does not contribute to the happiness index score, it is an outcome\ndataset.drop(['Rank 2020'], axis=1, inplace=True)\n\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(22,8))\nsns.kdeplot(first_yr[\"Happiness Score\"], shade=True, ax=axes[0,0])\nsns.kdeplot(second_yr[\"Happiness Score\"], shade=True, ax=axes[0,1])\nsns.kdeplot(third_yr[\"Happiness Score\"], shade=True, ax=axes[0,2])\nsns.kdeplot(fourth_yr[\"Happiness Score\"], shade=True, ax=axes[1,0])\nsns.kdeplot(fifth_yr[\"Happiness Score\"], shade=True, ax=axes[1,1])\nsns.kdeplot(dataset[\"Happiness Score\"], shade=True, ax=axes[1,2])\n\naxes[0,0].set_xlabel(\"Happiness score 2015\")\naxes[0,1].set_xlabel(\"Happiness score 2016\")\naxes[0,2].set_xlabel(\"Happiness score 2017\")\naxes[1,0].set_xlabel(\"Happiness score 2018\")\naxes[1,1].set_xlabel(\"Happiness score 2019\")\naxes[1,2].set_xlabel(\"Happiness score 2020\")\n\naxes[0,0].axvline(first_yr[\"Happiness Score\"].mean(), color='black')\naxes[0,1].axvline(second_yr[\"Happiness Score\"].mean(), color='black')\naxes[0,2].axvline(third_yr[\"Happiness Score\"].mean(), color='black')\naxes[1,0].axvline(fourth_yr[\"Happiness Score\"].mean(), color='black')\naxes[1,1].axvline(fifth_yr[\"Happiness Score\"].mean(), color='black')\naxes[1,2].axvline(dataset[\"Happiness Score\"].mean(), color='black')\n\naxes[0,0].text(first_yr[\"Happiness Score\"].mean(),0.3, s=np.round(first_yr[\"Happiness Score\"].mean(),2))\naxes[0,1].text(second_yr[\"Happiness Score\"].mean(),0.3, s=np.round(second_yr[\"Happiness Score\"].mean(),2))\naxes[0,2].text(third_yr[\"Happiness Score\"].mean(),0.3, s=np.round(third_yr[\"Happiness Score\"].mean(),2))\naxes[1,0].text(fourth_yr[\"Happiness Score\"].mean(),0.3, s=np.round(fourth_yr[\"Happiness Score\"].mean(),2))\naxes[1,1].text(fifth_yr[\"Happiness Score\"].mean(),0.3, s=np.round(fifth_yr[\"Happiness Score\"].mean(),2))\naxes[1,2].text(dataset[\"Happiness Score\"].mean(),0.3, s=np.round(dataset[\"Happiness Score\"].mean(),2))\n\n\nplt.suptitle(\"Happiness score distribution across years\")","377d581e":"first_yr[pd.Series(first_yr.Region.unique()).sort_values(0)] = pd.get_dummies(first_yr.Region)\nsecond_yr[pd.Series(second_yr.Region.unique()).sort_values(0)] = pd.get_dummies(second_yr.Region)\nthird_yr[pd.Series(third_yr.Region.unique()).sort_values(0)] = pd.get_dummies(third_yr.Region)\nfourth_yr[pd.Series(fourth_yr.Region.unique()).sort_values(0)] = pd.get_dummies(fourth_yr.Region)\nfifth_yr[pd.Series(fifth_yr.Region.unique()).sort_values(0)] = pd.get_dummies(fifth_yr.Region)\ndataset[pd.Series(sixth_yr.Region.unique()).sort_values(0)] = pd.get_dummies(dataset.Region)\n\nfirst_yr.drop(['Region'], axis=1, inplace=True)\nsecond_yr.drop(['Region'], axis=1, inplace=True)\nthird_yr.drop(['Region'], axis=1, inplace=True)\nfourth_yr.drop(['Region'], axis=1, inplace=True)\nfifth_yr.drop(['Region'], axis=1, inplace=True)\ndataset.drop(['Region'], axis=1, inplace=True)\n\n\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfs1 = SelectKBest(score_func=mutual_info_regression, k='all')\nfs2 = SelectKBest(score_func=mutual_info_regression, k='all')\nfs3 = SelectKBest(score_func=mutual_info_regression, k='all')\nfs4 = SelectKBest(score_func=mutual_info_regression, k='all')\nfs5 = SelectKBest(score_func=mutual_info_regression, k='all')\nfs6 = SelectKBest(score_func=mutual_info_regression, k='all')\n\nfs1.fit(first_yr.drop([\"Happiness Score\"],axis=1),first_yr[\"Happiness Score\"])\nfs2.fit(second_yr.drop([\"Happiness Score\"],axis=1),second_yr[\"Happiness Score\"])\nfs3.fit(third_yr.drop([\"Happiness Score\"],axis=1),third_yr[\"Happiness Score\"])\nfs4.fit(fourth_yr.drop([\"Happiness Score\"],axis=1),fourth_yr[\"Happiness Score\"])\nfs5.fit(fifth_yr.drop([\"Happiness Score\"],axis=1),fifth_yr[\"Happiness Score\"])\nfs6.fit(dataset.drop([\"Happiness Score\"],axis=1),dataset[\"Happiness Score\"])","3f70d087":"f1s = pd.concat([pd.DataFrame(first_yr.columns), pd.DataFrame(fs1.scores_)], axis=1)\nf1s.columns=[\"features\",\"scores 2015\"]\nf1s = f1s[(f1s.features!=\"Happiness Score\") & (f1s.features!=\"Standard Error\")& (f1s.features!=\"Rank 2015\")]\nf2s = pd.concat([pd.DataFrame(second_yr.columns), pd.DataFrame(fs2.scores_)], axis=1)\nf2s.columns=[\"features\",\"scores 2016\"]\nf2s = f2s[(f2s.features!=\"Happiness Score\") & (f2s.features!=\"Standard Error\")& (f2s.features!=\"Rank 2016\")]\nf3s = pd.concat([pd.DataFrame(third_yr.columns), pd.DataFrame(fs3.scores_)], axis=1)\nf3s.columns=[\"features\",\"scores 2017\"]\nf3s = f3s[(f3s.features!=\"Happiness Score\") & (f3s.features!=\"Standard Error\")& (f3s.features!=\"Rank 2017\")]\nf4s = pd.concat([pd.DataFrame(fourth_yr.columns), pd.DataFrame(fs4.scores_)], axis=1)\nf4s.columns=[\"features\",\"scores 2018\"]\nf4s = f4s[(f4s.features!=\"Happiness Score\") & (f4s.features!=\"Standard Error\")& (f4s.features!=\"Rank 2018\")]\nf5s = pd.concat([pd.DataFrame(fifth_yr.columns), pd.DataFrame(fs5.scores_)], axis=1)\nf5s.columns=[\"features\",\"scores 2019\"]\nf5s = f5s[(f5s.features!=\"Happiness Score\") & (f5s.features!=\"Standard Error\")& (f5s.features!=\"Rank 2019\")]\nf6s = pd.concat([pd.DataFrame(dataset.columns), pd.DataFrame(fs6.scores_)], axis=1)\nf6s.columns=[\"features\",\"scores 2020\"]\nf6s = f6s[(f6s.features!=\"Happiness Score\") & (f6s.features!=\"Standard Error\")& (f6s.features!=\"Rank 2020\")]\n\n#Setting index to features\nf1s.set_index('features',inplace=True)\nf2s.set_index('features',inplace=True)\nf3s.set_index('features',inplace=True)\nf4s.set_index('features',inplace=True)\nf5s.set_index('features',inplace=True)\nf6s.set_index('features',inplace=True)","955509fd":"fs_scores=pd.concat([f1s,f2s,f3s,f4s,f5s,f6s],axis=1)","40ab2fd3":"fs_scores = fs_scores[(fs_scores.index!='Dystopia Residual') & (fs_scores.index!='Trust (Government Corruption)')]\nfor i in np.arange(0,17,1):\n    fs_scores.iloc[i].plot(figsize=(15,8))\nplt.legend(loc='lower left')  \nplt.title('Feature importance across years')","dd2925fd":"fs_scores.transpose().describe().iloc[1,:].to_frame().sort_values('mean', ascending=False).style.background_gradient(cmap='Reds')","73d22181":"second_yr = second_yr.iloc[:,:8]","c932a51f":"second_yr","f9ac1fe3":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score as s\n\nss=[]\n\nfor i in range(2,50):\n\n    k=KMeans(n_clusters=i,precompute_distances=True)\n    k.fit(second_yr)\n    ss.append(s(second_yr,k.labels_))\n    ss=pd.DataFrame(ss)\nss.plot(figsize=(20,10))\nplt.axvline(4)","46ba4743":"ss.sort_values(0,ascending=False)","6376b413":"k=KMeans(n_clusters=6,precompute_distances=True)\nk.fit(second_yr)\nsecond_yr['labels'] = k.labels_","40008dbf":"first_yr.iloc[:,:9]\n\nss=[]\n\nfor i in range(2,50):\n\n    k=KMeans(n_clusters=i,precompute_distances=True)\n    k.fit(first_yr)\n    ss.append(s(first_yr,k.labels_))\n    ss=pd.DataFrame(ss)\nss.plot(figsize=(20,10))\nplt.axvline(4)","15cb31c9":"# 1. Initialisation","1cd0cf97":"Important features across the years:\n* Social support\n* Economy (GDP per capita)\n* Family\n* Health (Life expectancy)\n* Being in Southern Asia\n\nFreedom is much less important than perceived","2eed63f4":"Storing the rank data in a column and setting the index to the country names","e2d6bbd4":"### Feature importance statistically:","2019e2ba":"# Introduction\n\n## [World happiness report](https:\/\/worldhappiness.report\/ed\/2020\/)\n\nThe World Happiness Report is a landmark survey of the state of global happiness that ranks 156 countries by how happy their citizens perceive themselves to be. The World Happiness Report 2020 for the first time ranks cities around the world by their subjective well-being and digs more deeply into how the social, urban and natural environments combine to affect our happiness. I urge you to check out their report if you already haven't!\n\n## Data\n\nThe data is available on Kaggle [here](https:\/\/www.kaggle.com\/mathurinache\/world-happiness-report)\n\n## This notebook\n\nIn this notebook we explore what makes the citizens of this planet happy. We also try predicting the happiness score of hypothetical countries not mentioned in this dataset. This is obtained by hyper-parameter tuning of various regression models. \n\n## Intended viewership\n\nI invite data enthusiasts from all regions of the world to tell me something about their countries which is not reflected in the datasets.","ac5a2cf3":"# Table of Contents\n\n1. [Initialisation](#Inititialisation)\n  * [Libraries](#Libraries)\n  * [Data loading](#Data_loading)\n2. [Exploratory data analysis](#Exploratory_data_analysis)\n  * [Data preparation](#Data_preparation)\n  * [Visualization](#Visualization)\n3. [Model development](#Model_development)\n  * [Model testing](#Model_testing)  \n  * [Benchmarking models](#Benchmarking_models)  \n  * [Feature importance](#Feature_importance)\n4. [Conclusion](#Conclusion)  ","5c6428b1":"At ss's index=4, i.e. 6 clusters we are getting the sharpest change.","e38f2bb1":"## Visualization","91bdcc5e":"Making a Rank dataframe which stores ranks of happiness index across the years 2015-2020","e0062705":"## Libraries","ded81616":"# 2. Exploratory_data_analysis","1abdb029":"# Clustering","d03d5a44":"## Data_preparation","22152b19":"## Data_loading"}}