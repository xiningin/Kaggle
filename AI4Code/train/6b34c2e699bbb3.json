{"cell_type":{"39c22450":"code","78b9a1d7":"code","e70dd646":"code","6dcb9571":"code","591c2eb4":"code","05c8022d":"code","0154975e":"code","dd0ee78b":"code","62257535":"code","0e84c300":"code","7ff5ee48":"code","e66aefe9":"code","5af367ae":"code","6a94e5c7":"code","fd8f546e":"code","0b31d42f":"code","a903f067":"code","481a2127":"code","3b336e95":"code","a2988095":"code","67226c37":"code","150dfbf8":"markdown","009aa7a1":"markdown","6ddd8519":"markdown","7bfa8dd6":"markdown","a3235c9a":"markdown","18ae36ab":"markdown","f60408a0":"markdown","55962d3e":"markdown","5b1f1e05":"markdown"},"source":{"39c22450":"TRUNCATED_LEN = 78 # truncating the train \nTRUNCATED_LEN_y = 50 # truncating the target\nSEED_SPLIT = 42\nSEED_VAL = 802\nSIZE_TRAIN = 800\nSN_THRESHOLD = 1.5\nBPPS_THRESHOLD = 1e-2\n\npretrain_dir = None\none_fold = False\nrun_test = False\n\nverbose=1\n\nae_epochs = 40\nae_epochs_each = 5\nae_batch_size = 32\n\nepochs_list =     [30, 10, 10, 5,  5,   5]\nbatch_size_list = [8,  16, 32, 64, 128, 256] \n\n## copy pretrain model to working dir\nimport shutil\nimport json\nimport glob\nif pretrain_dir is not None:\n    for d in glob.glob(pretrain_dir + \"*\"):\n        shutil.copy(d, \".\")\n    \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n%matplotlib inline","78b9a1d7":"train = pd.read_json(\"\/kaggle\/input\/stanford-covid-vaccine\/train.json\",lines=True)\ntrain = train[train.signal_to_noise > SN_THRESHOLD].reset_index(drop = True)\n    \nsub = pd.read_csv(\"\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv\")\nAs = []\nfor id in tqdm(train[\"id\"]):\n    a = np.load(f\"\/kaggle\/input\/stanford-covid-vaccine\/bpps\/{id}.npy\")\n    As.append(a)\nAs = np.array(As)","e70dd646":"targets = list(sub.columns[1:])\nprint(targets)\n\ny_train = []\nseq_len = train[\"seq_length\"].iloc[0]\nseq_len_target = train[\"seq_scored\"].iloc[0]\nignore = -10000\nignore_length = seq_len - seq_len_target\nfor target in targets:\n    y = np.vstack(train[target])\n    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n    y = np.hstack([y, dummy])\n    y_train.append(y)\ny = np.stack(y_train, axis = 2)\nprint(y.shape)","6dcb9571":"## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_inverse_distance_to_loop(sequence, loop_type):\n    '''\n    compute the graph distance of each base to the near loop\n    '''\n    prev = float('-inf')\n    Dist = []\n    for i, x in enumerate(sequence):\n        if x == loop_type: \n            prev = i\n        Dist.append(i - prev)\n\n    prev = float('inf')\n    for i in range(len(sequence) - 1, -1, -1):\n        if sequence[i] == loop_type: prev = i\n        Dist[i] = min(Dist[i], prev - i)\n    Dist = 1\/(np.array(Dist)+1)**2\n    \n    return Dist*(Dist>0.01)\n\n\ndef get_input(train):\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    \n    X_node = np.concatenate([X_node, X_loop], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = sorted(set(a.flatten()))\n    print(vocab)\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    ## distance to loops\n#     loop_types = [\"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n#     dist_inv_to_loops = np.zeros((train.shape[0], As.shape[1], len(loop_types)))\n#     for i in tqdm(range(len(train))):\n#         idx = train.index[i]\n#         for j, s in enumerate(loop_types):\n#             dist_inv_to_loops[i,:,j] = get_inverse_distance_to_loop(train[\"predicted_loop_type\"][idx], s)\n    \n#     X_node = np.concatenate([dist_inv_to_loops, X_node], axis = 2)\n    \n    print(X_node.shape)\n    return X_node\n\nX_node = get_input(train)","591c2eb4":"def get_structure_adj(train):\n    Ss = []\n    for i in tqdm(range(len(train))):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n#                 a_structure[start, i] = 1\n#                 a_structure[i, start] = 1\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_neighbor = np.diag(np.ones(seq_length-1),-1) + np.diag(np.ones(seq_length-1),1)\n#         a_neighbor += np.diag(np.ones(seq_length))\n        a_strc = np.concatenate([a_strc, a_neighbor[...,None]], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n    \n    Ss = np.array(Ss)\n    print(Ss.shape)\n    return Ss\n\nSs = get_structure_adj(train)","05c8022d":"def get_distance_matrix(As):\n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1\/Ds\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n    \n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    Ds[...,-1] = Ds[...,0]*As\n    print(Ds.shape)\n    return Ds\n\nDs = get_distance_matrix(As)\n\n# Ds = Ds[...,1]\n# Ds = Ds[...,None]","0154975e":"## concat adjecent\n# As = np.stack([As, As*(As>=BPPS_THRESHOLD)], axis=3)\n# As = np.concatenate([As, Ss, Ds], axis = 3).astype(np.float32)\nAs = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\ndel Ss, Ds\nprint(As.shape)","dd0ee78b":"import tensorflow as tf\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras import backend as K\n\ndef mcrmse(t, p, seq_len_target = TRUNCATED_LEN_y):\n    ## calculate mcrmse score by using numpy\n    t = t[:, :seq_len_target]\n    p = p[:, :seq_len_target]\n    \n    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis = 1), axis = 0)))\n    return score\n\ndef mcrmse_loss(y_target, y_pred, seq_len_target = TRUNCATED_LEN_y):\n    ## calculate mcrmse score by using tf\n    y_target = y_target[:, :seq_len_target]\n    y_pred = y_pred[:, :seq_len_target]\n    \n    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean((y_target - y_pred) ** 2, \n                                                                axis = 1), \n                                                 axis = 0)))\n    loss += 6e-1*tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean(tf.abs(y_target - y_pred), \n                                                                      axis = 1), axis = 0)))\n    return loss\n\ndef attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) \/ np.sqrt(n_factor))([x_Q, x_KT])\n#     res = tf.expand_dims(res, axis = 3)\n#     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = tf.squeeze(res, axis = 3)\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor \/\/ n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\ndef res(x, unit, kernel = 3, rate = 0.1):\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Dense(unit, None)(x)\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.Dropout(rate)(h)\n#         h = tf.keras.activations.swish(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\ndef adj_attn(x, adj, unit, n = 2, rate = 0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a)\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\n\ndef get_base(config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    \n    adj_learned = L.Dense(2, \"relu\")(adj)\n#     adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n    adj_learned_1 = L.Conv2D(4, 3, padding=\"same\")(adj)\n#     adj_learned_2 = L.Conv2D(2, 15, activation='relu', padding=\"same\")(adj)\n    adj_all = L.Concatenate(axis = 3)([adj, adj_learned, adj_learned_1])\n        \n    xs = []\n    xs.append(node)\n    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n    x = L.Concatenate()([x1, x2, x3, x4])\n    \n    for unit in [64, 32]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.1)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel = 30)\n        \n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n        x = multi_head_attention(x, x, unit, 4, 0.1)\n        xs.append(x)\n        \n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    return model\n\n\ndef get_ae_model(base, config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n\n    x = base([L.SpatialDropout1D(0.3)(node), adj])\n    x = forward(x, 64, rate = 0.3)\n    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n    \n    loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n    \n    opt = adam = tf.optimizers.Adam()\n    model.compile(optimizer = opt, loss = lambda t, y : y)\n    return model\n\n\ndef get_model(base, config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    \n    x = base([node, adj])\n    x = forward(x, 128, rate = 0.4)\n#     x = L.LSTM(32, return_sequences=True, dropout=0.0)(x)\n    x = L.Dense(5, None)(x)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    \n    opt = adam = tf.optimizers.Adam()\n    model.compile(optimizer = opt, loss = mcrmse_loss)\n    return model","62257535":"print('------Before-----')\nprint(X_node.shape)\nprint(As.shape)\nprint(y.shape)","0e84c300":"np.random.seed(SEED_SPLIT)\nidx_tr = np.random.choice(X_node.shape[0], SIZE_TRAIN, replace=False)\nidx_all = np.arange(X_node.shape[0])\nidx_pri = np.setdiff1d(idx_all,idx_tr)\n\n\nX_node, X_node_pri = X_node[idx_tr, :TRUNCATED_LEN], X_node[idx_pri]\nAs, As_pri = As[idx_tr, :TRUNCATED_LEN, :TRUNCATED_LEN], As[idx_pri]\ny, y_pri = y[idx_tr,:TRUNCATED_LEN], y[idx_pri]","7ff5ee48":"set(idx_all) == set(list(idx_tr)+list(idx_pri))","e66aefe9":"print('------After-----\\n')\nprint(X_node.shape)\nprint(As.shape)\nprint(y.shape)\nprint()\nprint(X_node_pri.shape)\nprint(As_pri.shape)\nprint(y_pri.shape)","5af367ae":"config = {} ## not use now\nbase = get_base(config)\nae_model = get_ae_model(base, config)\nae_model.summary()\ngc.collect();\ndel ae_model, base","6a94e5c7":"config = {}\n\nif ae_epochs > 0:\n    base = get_base(config)\n    ae_model = get_ae_model(base, config)\n    ## TODO : simultaneous train\n    for i in range(ae_epochs\/\/ae_epochs_each):\n        print(f\"------ {i} ------\")\n        print(\"--- train ---\")\n        ae_model.fit([X_node, As], [X_node[:,0]],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        print(\"--- private simulated ---\")\n        ae_model.fit([X_node_pri, As_pri], [X_node_pri[:,0]],\n                  epochs = 2*ae_epochs_each,\n                  batch_size = ae_batch_size)\n        gc.collect()\n    print(\"****** save ae model ******\")\n    base.save_weights(\".\/base_ae\")","fd8f546e":"from sklearn.model_selection import KFold\nkfold = KFold(5, shuffle = True, random_state = SEED_VAL)\n\nscores = []\npreds = np.zeros([len(X_node), X_node.shape[1], 5])\n\nfor i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n    print(f\"\\n ------ fold {i} start -----\\n \")\n    print(f\"Train on {len(tr_idx)}\")\n    print(f\"Validate on {len(va_idx)}\")\n    X_node_tr = X_node[tr_idx]\n    X_node_va = X_node[va_idx]\n    As_tr = As[tr_idx]\n    As_va = As[va_idx]\n    y_tr = y[tr_idx]\n    y_va = y[va_idx]\n    \n    base = get_base(config)\n    if ae_epochs > 0:\n        print(\"****** load ae model ******\")\n        base.load_weights(\".\/base_ae\")\n        print(\"****** ae model loaded ******\")\n    model = get_model(base, config)\n    if pretrain_dir is not None:\n        d = f\".\/model{i}\"\n        print(f\"--- load from {d} ---\")\n        model.load_weights(d)\n    for epochs, batch_size in zip(epochs_list, batch_size_list):\n        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n        model.fit([X_node_tr, As_tr], [y_tr],\n                  validation_data=([X_node_va, As_va], [y_va]),\n                  epochs = epochs,\n                  verbose=verbose,\n                  batch_size = batch_size, \n                  validation_freq = 3)\n        \n    model.save_weights(f\".\/model{i}\")\n    p = model.predict([X_node_va, As_va])\n    scores.append(mcrmse(y_va, p))\n    print(f\"fold {i}: mcrmse {scores[-1]}\")\n    preds[va_idx] = p\n        \npd.to_pickle(preds, \"oof.pkl\")","0b31d42f":"model.summary()","a903f067":"print(scores,'\\n')\nprint('CV score is:', np.mean(scores))","481a2127":"print(mcrmse(y_va, p))\nprint(mcrmse(y_va, p, seq_len_target = TRUNCATED_LEN_y))\nprint(mcrmse(y_va, p, seq_len_target = seq_len_target))","3b336e95":"p_pri_simu = 0\n\nfor i in range(5):\n    model.load_weights(f\".\/model{i}\")\n    p_pri_simu += model.predict([X_node_pri, As_pri]) \/ 5","a2988095":"score_pri_simu = mcrmse(p_pri_simu, y_pri,seq_len_target = seq_len_target)\nprint(f'Simulate private score is: {score_pri_simu:.7f}')","67226c37":"score_pri_comp = mcrmse(p_pri_simu, y_pri,seq_len_target = y_pri.shape[1])\nprint(f'Full sequence score for comparison is: {score_pri_comp:.7f}')","150dfbf8":"## Loading train\/test\/targets","009aa7a1":"## Predict the private set simulated","6ddd8519":"## train","7bfa8dd6":"## Model","a3235c9a":"## Edge matrices (adj)","18ae36ab":"## Node","f60408a0":"## Private LB simulation","55962d3e":"## Pretrain the autoencoder","5b1f1e05":"## Baseline model private simulation\n\nThe baseline is based on the greatest kernel among all this competition (IMHO) by @mrkmakr \nhttps:\/\/www.kaggle.com\/mrkmakr\/covid-ae-pretrain-gnn-attn-cnn\nwhich is in turn a hugely cleaned up version of uncle @cpmpml 's NFL big data bowl's graph transformer\nhttps:\/\/www.kaggle.com\/cpmpml\/graph-transfomer\n\nIn this notebook I tested ideas against the baseline. Major simple ideas without major architectual changes are:\n- adding dropouts in attention layers\n- adding an LSTM layer in the regressor\n- adding additional learnable edge feature matrices which are gonna to fed to attention layers\n- adding an L1 penalty to make gradients bigger in later epochs\n- adding bi-\/tri- diagonal matrix in the base edge matrix to model non-paired neighbors\n- Bpps with cut-offs\n- Distance matrices weighted by Bpps matrices\n- More hard-coded nodal features (columns sum of bpps, inverse Hamming distance to the loops)\n\n\nTrain with a truncated length (78), target simulating the 91\/130 ratio (50).\nValidation set is the full train seq_len (107)\n\nBaseline private simulation score:\nVersion 1-9: 0.2585188\n\nBaseline using SNR>1.5 with more AE training (30 epoch with more time on private simulated)\nVersion 10-: 0.2507343\n\n### Change notes\n- Version 1 (&2): CV  0.2585188. no model change from the best public version, just added private simulation\n- Version 3 (&5): CV 0.2619488. add a tridiagonal matrix to Ss, keep only two distance matrices (linear and square)\n- Version 6: CV 0.2624281. baseline+tridiagonal neighbor matrix in Ss, only two distance matrices, diagonal weight changes from 2 to 1 in Ss \n- Version 7: CV 0.2591735. baseline +  truncated As, only two distance matrices, remove the tridiagonal from the Ss\n- Version 8: CV 0.2579629. baseline + truncated As, only inverse square distance matrix, weighted tridiagonal from the Ss\n- Version 9: CV 0.2606475 truncated As, only inverse square distance matrix, only 0.5 weighted bi-diagonal for Ss\n\n### SNR 1.5 versions\n\n- Version 10: Add more AE epoch from baseline\n- Version 11: more AE epochs than version 10\n- Version 12: only inverse square distance + 2 Conv2D 4x4 filters + a new loss function\n- Version 13: (CPU) same with 12, distance to nearest loops nodal feature added\n- Version 15: (GPU) same with 13, more AE epochs to bring down the entropy loss\n- Version 16: (GPU) version 12, bidiagonal weight, inverse linear and square distance, 2 Conv2D 4x4 filters, back to old loss\n- Version 17: (CPU) version 12, + 4 Conv2D 4x4 filter, old loss\n- Version 18: (CPU) 3 Conv2D learnable edge filters, L1 penalty, linear distance weighted by As (is this learnable?)\n- Version 19: (CPU) version 18, stronger L1 penalty, additional linear distance weighted by As (is this learnable?)\n- Version 20 & 21: (CPU) version 19 + LSTM in regressor, learnable edge matrix has now no activation."}}