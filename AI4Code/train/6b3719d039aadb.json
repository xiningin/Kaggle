{"cell_type":{"d3e61e0d":"code","4cce404d":"code","a0704d62":"code","cb5c4b36":"code","df1889fc":"code","acc0af93":"code","c94ce8e0":"code","0ad65ca4":"code","b27c3050":"markdown","52229589":"markdown","bc91b295":"markdown","3eb4dfe0":"markdown","0321af07":"markdown","782661ea":"markdown"},"source":{"d3e61e0d":"import cv2\nimport subprocess\nimport os, gc, glob\n# import numpy as np\nimport cupy as np\nimport pandas as pd\nimport datatable as dtable\nfrom numba import njit\nfrom tqdm.notebook import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom IPython.display import HTML","4cce404d":"file = dtable.fread('..\/input\/davis-240c-datasets\/shapes_rotation\/events.txt', sep = ' ').to_pandas()\nfile.columns = ['timestamp', 'x', 'y', 'polarity']\nfile['polarity'] *= 1\nfile.head()","a0704d62":"file.describe()","cb5c4b36":"def event_agg(timestamp, x, y, polarity, T_r, M, N):\n    T_seq = timestamp.max()\n    T_frames = int((T_seq \/\/ T_r)) + 1\n    \n    frames_0 = np.zeros((T_frames, M, N)) # polarity == 0\n    frames_1 = np.zeros((T_frames, M, N)) # polarity == 1\n    \n    for i in tqdm(range(T_frames)):\n        idx_0 = np.where((timestamp >= i * T_r) & (timestamp < i * T_r + T_r) & (polarity == 0))[0]\n        if len(idx_0) > 0:\n            frames_0[i] = np.bincount(N * x[idx_0] + y[idx_0], minlength = M * N).reshape(M, N)\n        \n        idx_1 = np.where((timestamp >= i * T_r) & (timestamp < i * T_r + T_r) & (polarity == 1))[0]\n        if len(idx_1) > 0:\n            frames_1[i] = np.bincount(N * x[idx_1] + y[idx_1], minlength = M * N).reshape(M, N)\n    \n    superframes = np.concatenate((frames_0, frames_1), axis = -1)\n    print('generated superframes with size:', superframes.shape)\n    return superframes","df1889fc":"T_r = 0.01\nM = 240\nN = 180","acc0af93":"timestamp = np.array(file['timestamp'].values)\nx = np.array(file['x'].values)\ny = np.array(file['y'].values)\npolarity = np.array(file['polarity'].values)\nsuperframes = event_agg(timestamp, x, y, polarity, T_r, M, N)\nsuperframes = superframes.get()","c94ce8e0":"idx = np.random.randint(superframes.shape[0]).get()\nplt.imshow(superframes[idx], cmap = cm.Greys_r)\nplt.show()","0ad65ca4":"frames = [] # for storing the generated images\nfig = plt.figure()\nfor i in range(1000):\n    frames.append([plt.imshow(superframes[i], cmap = cm.Greys_r, animated = True)])\n\nani = animation.ArtistAnimation(fig, frames, interval = 50, blit = False, repeat_delay = 1000)\nani.save('video.mp4')\nHTML(ani.to_jshtml())","b27c3050":"# Video","52229589":"# Image","bc91b295":"# Read Event Data","3eb4dfe0":"T_r: time intervel\n\nM: image length\n\nN: image width","0321af07":"# Generate Superframes\n\n**Reference:** \n\nN. Khan, K. Iqbal and M. G. Martini, \"Time-Aggregation-Based Lossless Video Encoding for Neuromorphic Vision Sensor Data,\" in IEEE Internet of Things Journal, vol. 8, no. 1, pp. 596-609, 1 Jan.1, 2021.","782661ea":"# Libraries"}}