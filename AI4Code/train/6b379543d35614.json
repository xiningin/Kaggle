{"cell_type":{"217ab84f":"code","312cc053":"code","770cce86":"code","602e3f02":"code","0d36da14":"code","9af53005":"code","dd4b3314":"code","c3d64379":"code","4f0b3eb8":"code","d49ef85d":"code","5c0e02d5":"code","89294cb7":"code","f1eadf92":"code","4330f048":"code","436215b8":"code","e07ee313":"code","68d59d3a":"code","f6537685":"code","61fb0ff6":"code","4c9adcb9":"code","ca0fe6d9":"code","d17b61ac":"code","60084daf":"code","5503499d":"code","0b82b519":"code","76765292":"code","da614037":"code","77ae6c8c":"code","30bce141":"code","53c219b1":"code","de720148":"code","3600f26e":"code","a7258021":"code","1a431145":"code","fef72c27":"code","40bd4235":"code","7217f678":"code","28d987b8":"code","5e8bf01f":"code","93a277d4":"code","8d0bf914":"code","f5976d05":"markdown","4932556c":"markdown","5e73a661":"markdown","f398911d":"markdown","84b45413":"markdown","b697ba94":"markdown","fcd85d95":"markdown","90b331db":"markdown","865d6b8e":"markdown","7657a1ab":"markdown","4b1d06aa":"markdown","88e550ff":"markdown","e2ecba23":"markdown","8075ce0d":"markdown","9021f281":"markdown","fc352d2a":"markdown","89e567d1":"markdown","78fe6711":"markdown","9ff29c91":"markdown","64ae4560":"markdown","ae4708a7":"markdown","cd4b03e3":"markdown","f8e5b040":"markdown","a40a1d24":"markdown","09150041":"markdown","a476e3a9":"markdown","fb0e68d8":"markdown","fbc911e3":"markdown","a8211a30":"markdown","d6b11311":"markdown","95fe7ed4":"markdown","c2c0a469":"markdown","94f3227c":"markdown","e6198d1c":"markdown","694a77ab":"markdown"},"source":{"217ab84f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","312cc053":"df = pd.read_csv('..\/input\/advertising-dataset\/advertising.csv')","770cce86":"df.head()","602e3f02":"X = df.drop('Sales',axis=1)","0d36da14":"X.shape","9af53005":"y = df['Sales']","dd4b3314":"y.shape","c3d64379":"#from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score","4f0b3eb8":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","d49ef85d":"model = LinearRegression()","5c0e02d5":"model.fit(X_train,y_train)","89294cb7":"test_predictions = model.predict(X_test)\ntest_predictions","f1eadf92":"mean_absolute_error(y_test,test_predictions)","4330f048":"np.sqrt(mean_squared_error(y_test,test_predictions))","436215b8":"def mape(actual,pred):\n    actual, pred = np.array(actual), np.array(pred)\n    return np.mean(np.abs((actual - pred) \/ actual) * 100)\n\nprint('The Mean absolute percentage error is:- ',mape(y_test,test_predictions))","e07ee313":"r2_score(y_test,test_predictions)","68d59d3a":"test_residuals = y_test - test_predictions\ntest_residuals","f6537685":"plt.figure(figsize=(7,6),dpi=90)\nsns.scatterplot(x=y_test, y=test_residuals)\nplt.ylabel('Residuals from the Linear regression model')\nplt.title('Residual plot of Linear resgression')\nplt.axhline(y=0,color='red');","61fb0ff6":"from sklearn.preprocessing import PolynomialFeatures","4c9adcb9":"train_set_rmse = []\ntest_set_rmse = []\n\nfor d in range(1,10):\n    \n    # Creating polymial features\n    polynomial_converter = PolynomialFeatures(degree = d, include_bias=False)\n    poly_features = polynomial_converter.fit_transform(X)\n    \n    # Creating training and test set\n    X_p_train, X_p_test, y_p_train, y_p_test = train_test_split(poly_features, y , test_size=0.3, random_state=101)\n    \n    # Polynomial Model creation\n    poly_model = LinearRegression()\n    poly_model.fit(X_p_train,y_p_train)\n    \n    # Predictions of both train and test set\n    train_set_preds = poly_model.predict(X_p_train)\n    test_set_preds = poly_model.predict(X_p_test)\n    \n    # Calculating RMSE for both train and test predictions\n    train_rmse = np.sqrt(mean_squared_error(y_p_train,train_set_preds))\n    test_rmse = np.sqrt(mean_squared_error(y_p_test,test_set_preds))\n    \n    # Storing the rmse to be later used for plotting\n    train_set_rmse.append(train_rmse)\n    test_set_rmse.append(test_rmse)","ca0fe6d9":"train_set_rmse","d17b61ac":"test_set_rmse","60084daf":"plt.figure(figsize=(7,6),dpi=90)\nplt.plot(range(1,6),train_set_rmse[0:5],label='Train RMSE')\nplt.plot(range(1,6),test_set_rmse[0:5],label='Test RMSE')\nplt.xlabel('Model complexity')\nplt.ylabel('RMSE values')\nplt.title('Model complexity vs RMSE on both training and testing set')\nplt.legend();","5503499d":"final_poly_converter = PolynomialFeatures(degree=3,include_bias=False)","0b82b519":"final_poly_features = final_poly_converter.fit_transform(X)","76765292":"final_poly_features.shape","da614037":"# training and testing data\nX_f_train, X_f_test, y_f_train, y_f_test = train_test_split(final_poly_features, y , test_size=0.3, random_state=101)","77ae6c8c":"final_poly_model = LinearRegression()","30bce141":"final_poly_model.fit(X_f_train,y_f_train)","53c219b1":"final_poly_predictions = final_poly_model.predict(X_f_test)","de720148":"final_poly_predictions","3600f26e":"print('MAE of Polynomial Regression',mean_absolute_error(y_f_test,final_poly_predictions))","a7258021":"print('RMSE of Polynomial Regression',np.sqrt(mean_squared_error(y_f_test,final_poly_predictions)))","1a431145":"print('The r2 score is',r2_score(y_f_test,final_poly_predictions))","fef72c27":"poly_test_residuals = y_f_test - final_poly_predictions","40bd4235":"plt.figure(figsize=(8,6),dpi=90)\nsns.scatterplot(x=y_f_test, y=poly_test_residuals)\nplt.ylabel('Residuals from the Polynomial Regression model')\nplt.title('Residual plot of Polynomial regression')\nplt.axhline(y=0,color='red');","7217f678":"plt.figure(figsize=(12,5),dpi=90)\nplt.subplot(1,2,1)\nsns.scatterplot(x=y_test, y=test_residuals)\nplt.title('Residual plot of Linear regression')\nplt.ylabel('Residuals from the Linear Regression model')\nplt.axhline(y=0,color='red');\n\nplt.subplot(1, 2, 2) \nsns.scatterplot(x=y_f_test, y=poly_test_residuals)\nplt.title('Residual plot of Polynomial regression')\nplt.ylabel('Residuals from the Polynomial Regression model')\nplt.axhline(y=0,color='red');","28d987b8":"# Saving the model","5e8bf01f":"from joblib import dump","93a277d4":"dump(final_poly_model,'Sales_prediction_model_poly_reg.joblib')","8d0bf914":"dump(final_poly_converter,'Final_sales_features_converter.joblib')","f5976d05":"### We can see that with Linear Regression the model accuracy achieved is 91.85%","4932556c":"We can see that both the error metrics of Polynomial regression are performing way better than Linear regression","5e73a661":"### 5. So we'll go with degree 3 for our final Polymial regression","f398911d":"### Let's visualize the residual plots","84b45413":"#### R-2 score - Polynomial Regression","b697ba94":"### Polynomial Regression Model Accuracy - 98.88%\n\nLinear Regression model accuracy - 91.85%","fcd85d95":"#### RMSE","90b331db":"#### Initially we had just 3 features now as a result of using degree 3 we have the additional features which include the possible squared values of the original values and possible interaction terms","865d6b8e":"#### With the testing set RMSE values we can see that after degree 4 the errors shoot up suddenly to a very high value, giving us an idea where we should stop increasing the order","7657a1ab":"#### Now we have completed the whole process \n\n#### Let's see what the RMSE values are and plot them","4b1d06aa":"#### MAE","88e550ff":"#### MAE - polynomial regression\n\n","e2ecba23":"#### Predicting values the model has never seen before","8075ce0d":"### Linear regression model creation","9021f281":"### 4. Let's visualize them","fc352d2a":"## Hence from above metrics analysis we can clearly see that using higher order polynomial we get best accuracy than Linear Regression.","89e567d1":"#### From the residual plots we can see that with less errors than Linear regression the residuals in Polynomial regression are close to the regression line, telling us that it has less errors.","78fe6711":"#### But we need a way to decide what the order of the polynomial should be to get maximum accuracy.\n\nOne way is to run a for loop for the entire process from polynomial feature creation to finally testing the accuracy using RMSE and then plotting out them out to see which degree gives the least error so that we can create a final model it.\n","9ff29c91":"#### R-2 score","64ae4560":"### 7.Let's test the accuracy of the Polynomial regression model and compare it with Linear Regression","ae4708a7":"#### RMSE - polynomial regression","cd4b03e3":"MAE of Linear regression - 1.21","f8e5b040":"#### From the training set RMSE values we can see that the errors are constantly going down with increase in the order of the polynomial, though at one point we can see that for &th degree it shoots up and then gradually decreases which is kind of a redflag but it decreses after that.","a40a1d24":"### We should keep in mind that this does not apply to every case, i.e. polynomial regression won't always give best accuracy, but we won't find out until we try it for ourselves.","09150041":"### 2.Let's test the model accuracy ","a476e3a9":"### 3. Polynomial Regssion - Selecting the degree of polynomial\n#### Though we have good accuracy with Linear regression, this can be further increased with Polynomial regression","fb0e68d8":"### 6.Final model creation","fbc911e3":"### Usually the training set performs a bit better than the tset test, the same can be seen above\n\n### So even though here we see that degree 4 has less error than degree 3 but we also have to think about whether it is worth the risk to go for degree 4 instead of degree3 since we can clearly see taking up degree 4 has a high risk for shooting up the error very badly so what is suggested is that you should go for degree 3","a8211a30":"#### MAPE","d6b11311":"#### Feature selection","95fe7ed4":"### 1.Linear regression","c2c0a469":"RMSE of Linear regression - 1.51","94f3227c":"## Let's also see the residual plots","e6198d1c":"### Let's visualize the residual plots of Linear regression and Polynomial regression side by side","694a77ab":"#### With the residual plot we can see that the points are normally distributed along the regression line, though points are a bit far off from the line, but we can say that linear regression was a good choice of algo for this dataset."}}