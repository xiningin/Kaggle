{"cell_type":{"5a4a4b6b":"code","6f244691":"code","5b7e525d":"code","ac839a29":"code","67f26ca4":"code","70e5e4a9":"code","da1fad9e":"code","7f8c2500":"code","6bfd7346":"code","d9413b08":"code","0febca90":"code","f3bb1a4e":"code","f133d84b":"code","66820bad":"code","bc415fa7":"code","92ef8d95":"code","82b91898":"code","6ab19152":"code","3b293037":"code","1f32c032":"code","3e09d694":"code","37a51a59":"code","1293047c":"code","f7d6d7c0":"code","944ae44e":"code","7311c37c":"code","cf728c76":"code","9a425c34":"code","596d1511":"markdown","90034172":"markdown","440ef320":"markdown","c6b9a566":"markdown","2d5b40ac":"markdown","f5161f54":"markdown","915ea1ea":"markdown","2dc19abc":"markdown","127180e9":"markdown","fb2d3cb1":"markdown","940090ca":"markdown","75e9a05a":"markdown","8e3835ef":"markdown","2670b97b":"markdown","3e3e6c3d":"markdown","f5b617c6":"markdown","90726453":"markdown","6a978b7d":"markdown","7ef08c29":"markdown"},"source":{"5a4a4b6b":"# vizualization library\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# pytorch library\nimport torch # the main pytorch library\nimport torch.nn.functional as f # the sub-library containing different functions for manipulating with tensors\n\n# huggingface's transformers library\nfrom transformers import BertModel, BertTokenizer","6f244691":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","5b7e525d":"bert_version = 'bert-large-cased'\ntokenizer = BertTokenizer.from_pretrained(bert_version)\nmodel = BertModel.from_pretrained(bert_version)","ac839a29":"model = model.eval()","67f26ca4":"# the .to method allows us to move the model to the provided device\nmodel = model.to(device)","70e5e4a9":"texts  = [\n    'Obama speaks to the media in Illinois',\n    'The president greets the press in Chicago',\n    'Oranges are my favorite fruit',\n]","da1fad9e":"encodings = tokenizer(\n    texts, # the texts to be tokenized\n    padding=True, # pad the texts to the maximum length (so that all outputs have the same length)\n    return_tensors='pt' # return the tensors (not lists)\n)","7f8c2500":"encodings = encodings.to(device)","6bfd7346":"encodings.keys()","d9413b08":"encodings['input_ids']","0febca90":"encodings['attention_mask']","f3bb1a4e":"encodings['token_type_ids']","f133d84b":"for tokens in encodings['input_ids']:\n    print(tokenizer.convert_ids_to_tokens(tokens))","66820bad":"# disable gradient calculations\nwith torch.no_grad():\n    # get the model embeddings\n    embeds = model(**encodings)","bc415fa7":"embeds = embeds[0]","92ef8d95":"embeds","82b91898":"# to get the size of the tensor use .shape (similar as with numpy)\nembeds.shape","6ab19152":"def visualize(distances, figsize=(10, 5), titles=None):\n    # get the number of columns\n    ncols = len(distances)\n    # create the subplot placeholders\n    fig, ax = plt.subplots(ncols=ncols, figsize=figsize)\n    \n    for i in range(ncols):\n        \n        # get the axis in which we will draw the matrix\n        axes = ax[i] if ncols > 1 else ax\n        \n        # get the i-th distance\n        distance = distances[i]\n        \n        # create the heatmap\n        axes.imshow(distance)\n        \n        # show the ticks\n        axes.set_xticks(np.arange(distance.shape[0]))\n        axes.set_yticks(np.arange(distance.shape[1]))\n        \n        # set the tick labels\n        axes.set_xticklabels(np.arange(distance.shape[0]))\n        axes.set_yticklabels(np.arange(distance.shape[1]))\n        \n        # set the values in the heatmap\n        for j in range(distance.shape[0]):\n            for k in range(distance.shape[1]):\n                text = axes.text(k, j, str(round(distance[j, k], 3)),\n                               ha=\"center\", va=\"center\", color=\"w\")\n        \n        # set the title of the subplot\n        title = titles[i] if titles and len(titles) > i else \"Text Distance\"\n        axes.set_title(title, fontsize=\"x-large\")\n        \n    fig.tight_layout()\n    plt.show()","3b293037":"CLSs = embeds[:, 0, :]","1f32c032":"# normalize the CLS token embeddings\nnormalized = f.normalize(CLSs, p=2, dim=1)\n# calculate the cosine similarity\ncls_dist = normalized.matmul(normalized.T)\ncls_dist = cls_dist.new_ones(cls_dist.shape) - cls_dist\ncls_dist = cls_dist.numpy()","3e09d694":"visualize([cls_dist], titles=[\"CLS\"])","37a51a59":"MEANS = embeds.mean(dim=1)","1293047c":"# normalize the MEANS token embeddings\nnormalized = f.normalize(MEANS, p=2, dim=1)\n# calculate the cosine similarity\nmean_dist = normalized.matmul(normalized.T)\nmean_dist = mean_dist.new_ones(mean_dist.shape) - mean_dist\nmean_dist = mean_dist.numpy()","f7d6d7c0":"visualize([mean_dist], titles=[\"MEAN\"])","944ae44e":"MAXS, _ = embeds.max(dim=1)","7311c37c":"# normalize the MEANS token embeddings\nnormalized = f.normalize(MAXS, p=2, dim=1)\n# calculate the cosine similarity\nmax_dist = normalized.matmul(normalized.T)\nmax_dist = max_dist.new_ones(max_dist.shape) - max_dist\nmax_dist = max_dist.numpy()","cf728c76":"visualize([max_dist], titles=[\"MAX\"])","9a425c34":"dist = [cls_dist, mean_dist, max_dist]\ntitles = [\"CLS\", \"MEAN\", \"MAX\"]\nvisualize(dist, titles=titles)","596d1511":"We have have the following sentences for which we want to measure their similarities. While the first two sentences are semantically similar, the third sentence could be considered as random.","90034172":"# Way 3: Compute the max-over-time of the Output Vectors\n\nWe will calculate the max-over-time of all embeddings for each sentence and calculate the cosine distance. The max-over-time is basically getting the maximum value for each dimension of the tokens. For example, if a sentence has two tokens:\n\n$$\nt_{1} = \n\\begin{bmatrix}\n0.4 \\\\\n0.7 \\\\\n\\end{bmatrix}, \\quad\nt_{2} = \n\\begin{bmatrix}\n0.5 \\\\\n0.2 \\\\\n\\end{bmatrix},\n$$\nthen the `max-over-time` would be \n\n$$\n\\max(t_{1},t_{2}) = \n\\begin{bmatrix}\n0.5 \\\\\n0.7 \\\\\n\\end{bmatrix}.\n$$\n\n\nwe will use the `.max` method that is documented [here][max]. The `.max` method returns a tuple where the first value is a tensor of maximum values based on the `dim` argument, and the second value are the indices of the index location of each maximum value found (`argmax`).\n\n[max]: https:\/\/pytorch.org\/docs\/stable\/generated\/torch.max.html#torch.max\n","440ef320":"# Import the Libraries","c6b9a566":"# Embeddings\n\nIn order to get the token embeddings we will send the encodings through the model. Pytorch by default calculates the gradients for every layer of the model which is really handy when training the model but not when we want to use it in production. **Gradients take additional space on CPU or GPU which you don't want to populate with things you don't need.**\n\nWe can disable gradient calculation by adding the `with torch.no_grad():` before the model.\n","2d5b40ac":"# Way 2: Computing the Mean of All Output Vectors\n\nWe will calculate the mean of all embeddings for each sentence and calculate the cosine distance. To get the means we will use the `.mean` method that is documented [here][mean].\n\n[mean]: https:\/\/pytorch.org\/docs\/stable\/generated\/torch.mean.html#torch.mean","f5161f54":"The BERT tokenizer returns three values:\n\n- **input_ids.** The list\/tensor of token ids for each sentence. The id `101` represents the class token `[CLS]`, `102` represents the separator token `[SEP]`, and `0` represents the padding token `[PAD]`.\n- **attention_mask.** This list\/tensor represents which ids to use when generating the tokens (e.g. ignores the `[PAD]` tokens).\n- **token_type_ids.** This list\/tensor represents which tokens correspond to the first and second sentence (used for next sentence prediction).","915ea1ea":"## WordPiece\n\nThe BERT model uses the [WordPiece tokenizer][wordpiece].\n\nLets convert the ids into tokens and output their representaitons.\n\n[wordpiece]: https:\/\/huggingface.co\/transformers\/v3.5.1\/tokenizer_summary.html#wordpiece","2dc19abc":"# Measure the Sentence Similarities\n\nNow that we got the sentence token embeddings we will calculate the sentence similarities. We will od this in three ways:\n\n- Using the `[CLS]` token embeddings.\n- Calculating the mean of all sentence token embeddings.\n- Calculating the max-over-time of the sentence token embeddings.\n\nIn all three ways we will calculate the sentence similarities using the cosine distance.","127180e9":"The output of the model is a tuple of tensors:\n\n- The first tensor (`embeds[0]`) contains the embeddings of the sentence tokens.\n- The second tensor (`embeds[1]`) contains the embeddings of the `[CLS]` token that can be then sent further to train the next sentence prediction. We won't use this tensor in this notebook.","fb2d3cb1":"Get the encodings of the sentences. Since the sentences are of different lenghts we set the `padding` parameter to `True` which will pad the shorter sentences. We also set the `return_tensors` to `'pt'` so that we get the `pytorch` tensors instead of python lists.","940090ca":"# BERT Tokenizer\n\nThe tokenizer transforms a string into lists, tensors, etc. which can be used as the input of the model. Different tokenizers have different inputs. The BERT tokenizer takes the following inputs:\n\n- **text.** The sequence or batch of sequences to be encoded.\n- **text_pair.** The OTHER sequence of batch of sequences to be encoded (used for next sentence prediction). (Default: None)\n- **padding.** Activates and controls padding. If True, pad to the longest sequence in the batch (or no padding if only a single sequence is provided). *Other options are also available.* (Default: False)\n- **truncation.** Activates and controls truncation. If True, Truncate to a maximum length specified with the `max_length` argument. (Default: False)\n- **max_length.** Controls the maximum length to use by one of the truncation\/padding parameters.\n- **return_tensors.** If set, will return tensors instead of list of python integers.\n\nThe full documentation of the BERT tokenizer can be found [here][tokenizer]. **NOTE:** The BERT tokenizer inherits from the `PreTrainedTokenizer` class, which is documented [here][pretrained].\n\n\n**DISCLAIMER:** In this notebook we use the BERT tokenizer. Different models use different tokenizers. For instance, to use the `RoBERTa` model we would need to import the `RobertaTokenizer` which will transform the text in a different way. **Consult the `transformers` documentation on which tokenizer to use.**\n\n[tokenizer]: https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#berttokenizer\n[pretrained]: https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html#transformers.PreTrainedTokenizer","75e9a05a":"# Way 1: Use the [CLS] Embeddings\n\nWe will take the first embedding representing the `[CLS]` token of all three sentences and calculate the cosine distance.","8e3835ef":"# Sentence Similarity using BERT\n\nIn this notebook we will calculate the similarity of different sentences using BERT. Three sentences will be sent through the BERT model to retrieve its embeddings and use them to calculate the sentence similarity by:\n\n- Using the `[CLS]` token of each sentence.\n- Calculating the mean of sentence embeddings.\n- Calculating the max-over-time of the sentence embeddings.\n\nThe python libraries that we will use in this notebook are:\n\n- [transformers][transformers]. The huggingface library containing the general-purpose architectures for NLP.\n- [torch][torch]. An open source machine learning framework used to create and train models on the GPU.\n- [matplotlib][matplotlib]. A comprehensive library for creating static, animated and interactive visualizations in Python.\n\n[transformers]: https:\/\/huggingface.co\/transformers\/index.html\n[torch]: https:\/\/pytorch.org\/\n[matplotlib]: https:\/\/matplotlib.org\/\n\n**NOTE:** The notebook uses `transformers-3.5.1` and `torch-1.7.0` versions.","2670b97b":"**The input tensors need to be on the same device as the model.** In order for us to use the BERT model to get the sentence embeddings we must first move the input tensors to the same device as the model.","3e3e6c3d":"Now, let us specify where the model will do its calculations. We already defined the `device` in the previous lines so lets use that:","f5b617c6":"# Import BERT Tokenizer and Model\n\nNow we will import a pretrained BERT model from Hugging Face. There are a lot of different BERT models that were trained with different hyperparameters and tasks. A full list of official models is available [here][pretrained], but there are also a lot of models that are uploaded by the [community][community].\n\nIn this notebook we will use the `bert-large-cased` model. To use it we will need to import both the `tokenizer` and the `model`: The `tokenizer` will enable us to transform strings into tensors that can be then sent to the `model`, which in turn will give us the embeddings.\n\n[pretrained]: https:\/\/huggingface.co\/transformers\/pretrained_models.html\n[community]: https:\/\/huggingface.co\/models","90726453":"# Compare the Distances\n\nNow lets visualize all three distances. We see that all three approaches are able to measure the semantic similarity. Which one to take depends on the task that you are interested in.","6a978b7d":"# Setting the Device\n\nWith `pytorch` we are able to move the python calculations to the GPU. To do this we define the `device` on which we wish to run the calculations. Depending if `cuda` (the GPU drivers that enable running calculations on the graphic card) is enabled on the machine, we define the device as follows:","7ef08c29":"Each model has two states:\n\n- `train`. If the model is in this state it is configured for training Components such as `Dropout` and other that have been found to improve the models performance are active in this mode.\n- `eval`. If the model is in this state it is configured for production. Components such as `Dropout` are deactivated, since they are used only when training the model.\n\nIf the model is only going to be used in production, then set the model to `eval`. If the model is first going to be fine-tuned, set the model to `train`. After training, set the model to `eval`.  **NOTE:** It is good practice to specify the models state before using it.\n\nIn this notebook we will not train the model, that is why we will set it to `eval`."}}