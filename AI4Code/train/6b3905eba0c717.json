{"cell_type":{"3b787d50":"code","d3056e00":"code","6941f28a":"code","2659d8cc":"code","df02e3e5":"code","ae4f60c1":"code","0e428407":"code","954efb84":"code","14862274":"code","aa30817d":"code","3c0a7d76":"code","b28b78b1":"code","f1cc7c5c":"code","734e83a5":"code","4028a4c7":"code","3235192a":"code","ec9aff41":"code","0aad893c":"code","103b8c42":"code","b9c17176":"code","3bdb80c3":"code","efcb99fb":"code","8430d3e8":"code","6ef35792":"code","3af33ead":"code","0d319787":"code","a74f9e65":"code","04793508":"code","7f4af4d0":"code","f17805f5":"code","78c2949a":"code","cf91b0cf":"code","96002079":"code","07423d63":"code","9d003992":"code","ac82bd68":"markdown","08dd6e3e":"markdown","c19eea34":"markdown","8bdd6abf":"markdown","2c8ef62e":"markdown","bbfa62f2":"markdown","839a37d6":"markdown","ebf2662f":"markdown","282ffaf3":"markdown","090875b3":"markdown","5836da78":"markdown","20b318fe":"markdown","0ae68167":"markdown"},"source":{"3b787d50":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import metrics, model_selection","d3056e00":"%matplotlib inline","6941f28a":"!head ..\/input\/train.csv","2659d8cc":"mnist_df = pd.read_csv(\"..\/input\/train.csv\", delimiter=',')","df02e3e5":"mnist_df.head()","ae4f60c1":"# raw features are between 0 and 255\nmnist_df.describe()","0e428407":"from sklearn import preprocessing","954efb84":"training_target, training_features = mnist_df.iloc[:, 0], mnist_df.iloc[:, 1:]","14862274":"min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nscaled_training_features = min_max_scaler.fit_transform(training_features)","aa30817d":"_, ax = plt.subplots(1,1)\n_ = ax.imshow(scaled_training_features[0].reshape((28, 28)), cmap=\"gray\")","3c0a7d76":"from sklearn import decomposition","b28b78b1":"decomposition.PCA?","f1cc7c5c":"_prng = np.random.RandomState(42)\npca = decomposition.PCA(n_components=2, random_state=_prng)\ntransformed_training_features = pca.fit_transform(scaled_training_features)","734e83a5":"fig, ax = plt.subplots(1, 1)\n_ = ax.scatter(transformed_training_features[:,0], transformed_training_features[:,1], c=training_target, alpha=0.05)\nax.set_xlabel(\"Component 1\")\nax.set_ylabel(\"Component 2\")\nax.set_title(\"PCA\", fontsize=20)","4028a4c7":"from sklearn import manifold, pipeline","3235192a":"manifold.TSNE?","ec9aff41":"# initial PCA to determine n_components that capture 95% of sample variance\n_prng = np.random.RandomState(42)\npca = decomposition.PCA(random_state=_prng)\npca.fit_transform(scaled_training_features)\nn_components = np.sum(pca.explained_variance_ratio_.cumsum() < 0.95)","0aad893c":"_prng = np.random.RandomState(42)\n\nembedding_pipeline = pipeline.make_pipeline(\n    decomposition.PCA(n_components, random_state=_prng),\n    manifold.TSNE(n_components=2, random_state=_prng)\n)","103b8c42":"_prng = np.random.RandomState(34)\n_n_samples, _ = scaled_training_features.shape\nidxs = _prng.choice(_n_samples, _n_samples \/\/ 10, replace=False) ","b9c17176":"_sampled_target = training_target[idxs]\n_sampled_features = scaled_training_features[idxs]\n_transformed_sampled_features = embedding_pipeline.fit_transform(_sampled_features)\n\nfig, ax = plt.subplots(1, 1)\n_ = ax.scatter(_transformed_sampled_features[:,0], _transformed_sampled_features[:,1], c=_sampled_target, alpha=0.05)\nax.set_xlabel(\"Component 1\")\nax.set_ylabel(\"Component 2\")\nax.set_title(\"t-SNE\", fontsize=20)","3bdb80c3":"!lscpu","efcb99fb":"NUMBER_CORES = 4\nNUMBER_THREADS = 2 * NUMBER_CORES","8430d3e8":"from sklearn import linear_model","6ef35792":"linear_model.LogisticRegression?","3af33ead":"_prng = np.random.RandomState(42)\nlogistic_regression_clf = linear_model.LogisticRegression(solver=\"lbfgs\",\n                                                          fit_intercept=False,\n                                                          multi_class=\"multinomial\",\n                                                          max_iter=1000,\n                                                          random_state=_prng)\nlogistic_regression_clf.fit(scaled_training_features, training_target)","0d319787":"from sklearn import model_selection","a74f9e65":"logistic_regression_clf_accuracy = model_selection.cross_val_score(logistic_regression_clf,\n                                                                   scaled_training_features,\n                                                                   training_target,\n                                                                   scoring=\"accuracy\",\n                                                                   cv=NUMBER_CORES,\n                                                                   n_jobs=NUMBER_CORES,\n                                                                   verbose=10)","04793508":"logistic_regression_clf_accuracy.mean()","7f4af4d0":"from sklearn import ensemble","f17805f5":"ensemble.RandomForestClassifier?","78c2949a":"# evaluate a reasonable classifier using CV\nrandom_forest_clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=_prng, n_jobs=NUMBER_CORES)\nrandom_forest_accuracy = model_selection.cross_val_score(random_forest_clf,\n                                                         scaled_training_features,\n                                                         training_target,\n                                                         scoring=\"accuracy\",\n                                                         cv=NUMBER_CORES,\n                                                         n_jobs=NUMBER_CORES,\n                                                         verbose=10)","cf91b0cf":"random_forest_accuracy.mean()","96002079":"# retrain the classifier using the entire dataset\nrandom_forest_clf.fit(scaled_training_features, training_target)\n\n# load the testing features (note we use transform method and NOT fit_transform!)\n_testing_features_df = pd.read_csv(\"..\/input\/test.csv\", delimiter=',')\n_scaled_testing_features = min_max_scaler.transform(_testing_features_df)\n\n# make predictions \npredictions = random_forest_clf.predict(_scaled_testing_features)","07423d63":"# submission format for kaggle\n!head ..\/input\/sample_submission.csv","9d003992":"import time\n\nimport pandas as pd\n\ntimestamp = time.strftime(\"%Y%m%d-%H%M%S\")\nnumber_predictions, = predictions.shape\ndf = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions})\ndf.to_csv(f\"random-forest-submission-{timestamp}.csv\", index=False)","ac82bd68":"\n# Introduction\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. \n\nIn this tutorial, your goal is to correctly identify digits from the Kaggle MNIST dataset of tens of thousands of handwritten images. We will walk through the development process of various ML pipelines using Scikit-Learn to classify handwritten digits to high accuracy.","08dd6e3e":"## Random Forest Classifier","c19eea34":"In order to submit our predictions to Kaggle we first need to produce a correctly formatted submission file.","8bdd6abf":"## Rescale the raw data\n\nData for individual pixels is stored as integers between 0 and 255. Neural network models work best when numerical features are scaled. To rescale the raw features we can use tools from the [Scikit-Learn preprocessing module](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html).","2c8ef62e":"## Logistic Regression","bbfa62f2":"# Visualizing training samples","839a37d6":"# Classic ML Models","ebf2662f":"Once you have successfully submited your predictions then you can check the [Digit-Recognizer competition](https:\/\/www.kaggle.com\/c\/digit-recognizer) website and see how well your best model compares to your peers.","282ffaf3":"# Submit to Kaggle!\n\n## Make predictions","090875b3":"## Visualizing training sample using t-SNE\n\n[t-distributed Stochastic Neighbor Embedding (t-SNE)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. For more details on t-SNE including other use cases see this excellent *Toward Data Science* [blog post](https:\/\/towardsdatascience.com\/an-introduction-to-t-sne-with-python-example-5a3a293108d1)\n\nIt is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples.","5836da78":"As suggested above we will go ahead and use an initial PCA step in our embedding pipeline to reduce the overall number of features which will speed up convergence of the t-SNE algorithm.","20b318fe":"## Visualizing training samples using PCA\n\n[Principal Components Analysis (PCA)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html) can be used as a visualization tool to see if there are any obvious patterns in the training samples.","0ae68167":"# Load the MNIST data"}}