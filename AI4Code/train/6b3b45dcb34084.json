{"cell_type":{"24f3a69d":"code","3814ce06":"code","0489faba":"code","73bff01f":"code","6833c163":"code","fcf31d16":"code","93c5a092":"code","b030c24e":"code","a97469c5":"code","5f5c4883":"code","b842b8ec":"code","cb71424e":"code","4dabbca5":"code","b49d920e":"code","3ef3a65a":"code","a785186b":"code","de3d47e6":"code","454cbcb0":"code","45c71802":"code","af63b4eb":"code","477908c6":"code","7bb970ef":"code","7061dec9":"code","e4348cba":"code","e58d05c8":"code","8525404e":"code","6fd91298":"code","3ca602c4":"code","02d0b992":"code","bdebb7e3":"code","e8e1d4c3":"code","96e13333":"code","d79ece22":"code","a4aaf49b":"code","14b0fb17":"code","60f9ab6a":"code","728efebb":"code","85b3a189":"code","d1f460a4":"code","3036a909":"code","4a87a427":"code","bd9737c5":"code","6c934262":"code","984d1168":"code","a4bbf9ff":"code","300615b6":"code","efbe8309":"code","baa93c38":"code","a9248db8":"code","be17ae4f":"code","d24da60a":"code","c7eab8d8":"code","0884ded7":"code","62e40557":"code","e4e4a8d5":"code","2ee77c98":"code","6b2f24aa":"code","5dc504cd":"code","fcdeb314":"code","a171df42":"code","9789d937":"code","eddaf4c1":"code","a4ecf918":"code","6d5b3428":"code","3f79eb66":"code","dad92fdf":"code","355d4f86":"code","8c645d45":"code","3e2bb625":"code","f6014108":"code","bff10c76":"code","b51b06b1":"code","8a1057f4":"code","89b0502c":"code","2506e8f8":"code","d713da26":"code","478d9207":"code","84024675":"code","33b39618":"code","6bf76974":"code","60ed570c":"code","cee89f1a":"code","6b87735e":"code","b8edcb05":"code","7d248dfa":"code","339db780":"code","53a0e6f2":"code","434687c0":"code","fb6b91d7":"code","fc22bf15":"code","a41de3fd":"code","e44a2a73":"code","5bf4fd6d":"code","d3a1b6e4":"code","4ab57f49":"code","03837359":"code","bb4f2181":"code","9ef83a37":"code","102e841d":"code","2b693e30":"code","65b33766":"code","dcfcdbd7":"code","eb7ec245":"code","36fe12da":"code","e21c0ba9":"code","37d2b693":"code","512c16bf":"code","e181948b":"code","f8292f64":"code","dc1d4254":"code","f3ce84e2":"code","70c00a76":"code","686d8705":"code","1c530863":"code","b92535a7":"code","17f7bcf5":"code","5345db6d":"code","5d5134fa":"code","8e7cc717":"code","80cfbd22":"code","eab604cd":"code","96789c19":"code","ca6b4125":"code","0683439c":"code","70653321":"code","182d0a95":"code","7d0d0089":"code","a1785bda":"code","4a43e722":"code","54e7812f":"code","dcee50b0":"code","04ecc0b8":"code","8f6ab807":"code","a6705a9e":"code","eb45d8fe":"code","2f4927ce":"code","a08fd45e":"code","03f418e5":"code","b787b3ae":"code","bad23657":"code","62eea04f":"code","2d226f43":"code","84e55a9c":"code","1386df4f":"code","3e567938":"code","d8390485":"code","391090a6":"code","06576766":"code","1699895d":"code","b16f0132":"code","a125a43b":"code","65b5ef0d":"code","89ddfc6e":"code","c56a95be":"code","eefa5de4":"code","67baf147":"code","af594e39":"code","d2a34135":"markdown","b33b4e9d":"markdown","1516091a":"markdown","0e3e1c4c":"markdown","2d59dad4":"markdown","ff6f1d18":"markdown","26d18938":"markdown","d3618de7":"markdown","b32a9de0":"markdown","e11d10d3":"markdown","600324ac":"markdown","47cf1608":"markdown","3755f022":"markdown","1dea051e":"markdown","c0efec32":"markdown","746c8967":"markdown","01a90957":"markdown","71aa042c":"markdown","c541dde2":"markdown","56e57554":"markdown","8b788021":"markdown","3c5e46a3":"markdown","91d01fb2":"markdown","ad5fa81f":"markdown","17ee9ac6":"markdown","5371c4a7":"markdown","644377b6":"markdown","772c4950":"markdown","b3be4f66":"markdown","f6d3f543":"markdown","71fe1d07":"markdown","bce4d8be":"markdown","101be1a3":"markdown","4a922fb6":"markdown","f8bf8f98":"markdown","c1ee4c11":"markdown","d55ab1dc":"markdown","c51e5f9f":"markdown","696d1ded":"markdown","6974217d":"markdown","58f235ca":"markdown","229c3e78":"markdown","f1c7a1c2":"markdown"},"source":{"24f3a69d":"import pandas as pd\npd.set_option('max_columns', None)","3814ce06":"path = \"..\/input\/company-bankruptcy-prediction\/\"","0489faba":"df = pd.read_csv(path + 'data.csv')","73bff01f":"df.head()","6833c163":"df.shape","fcf31d16":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","93c5a092":"corr = df.corr()","b030c24e":"fig, ax = plt.subplots(figsize = (15,15))\nsns.heatmap(corr, ax = ax, cmap = 'viridis', linewidth = 0.1)","a97469c5":"df.info()","5f5c4883":"df['Bankrupt?'].value_counts()","b842b8ec":"print('Financially stable:', round(df['Bankrupt?'].value_counts()[0] \/ len(df) * 100,2) ,'%')\nprint('Financially unstable:', round(df['Bankrupt?'].value_counts()[1] \/ len(df) * 100, 2), '%')","cb71424e":"## Visualizing the datas\n\nsns.set_theme(context = 'paper')\n\n\nplt.figure(figsize = (8,8))\nsns.countplot(x = 'Bankrupt?', data = df);\nplt.title('Class Distributions: \\n 0: Financially Stable & 1: Financially Unstable');","4dabbca5":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint(\"Financially Stable:\", round(df['Bankrupt?'].value_counts()[0] \/ len(df) * 100, 2), '% of the dataset')\nprint(\"Financially Unstable:\", round(df['Bankrupt?'].value_counts()[1] \/ len(df) * 100,2),'% of the dataset')\n\nX = df.drop('Bankrupt?', axis = 1)\ny = df['Bankrupt?']\n\nsss = StratifiedKFold(n_splits = 5, random_state = None, shuffle = False)\n\nfor train_index, test_index in sss.split(X,y):\n    print(\"\\n Train\", train_index, \"Test\", test_index)\n    org_Xtrain, org_Xtest = X.iloc[train_index], X.iloc[test_index]\n    org_ytrain, org_ytest = y.iloc[train_index], y.iloc[test_index]\n    ","b49d920e":"import numpy as np","3ef3a65a":"## turn into an array\n\norg_Xtrain = org_Xtrain.values\norg_Xtest = org_Xtest.values\norg_ytrain = org_ytrain.values\norg_ytest = org_ytest.values\n\n## See if both the train and test label distribution are similarly distributed \ntrain_unique_label, train_counts_label = np.unique(org_ytrain, return_counts = True)\ntest_unique_label, test_counts_label = np.unique(org_ytest, return_counts = True)\n\nprint('Label Distirubtions: \\n')\nprint(train_counts_label \/ len(org_ytrain))\nprint(test_counts_label \/ len(org_ytest))","a785186b":"## Lets shuffle the data before creating the subsamples\n\nxdf = df.sample(frac = 1)\n\n## amount of Financially unstable data is 220\n# sdf = Financially stable\n# ndf = Financially unstable\n\nsdf = df.loc[xdf['Bankrupt?'] == 0][:220]\nndf = df.loc[xdf['Bankrupt?']==1]\n\nnormal_distributed_df = pd.concat([sdf, ndf])\n\n# Shuffling again\n\nnxdf = normal_distributed_df.sample(frac = 1, random_state = 42)","de3d47e6":"nxdf.head()","454cbcb0":"## Checking new dataframe\n\nprint(\"Distribution of the Classes in the subsample dataset\")\nprint(nxdf['Bankrupt?'].value_counts() \/ len(nxdf))\n\nsns.countplot('Bankrupt?', data = nxdf)\nplt.title(\"Equally Distributed Class\", fontsize = 14)\nplt.show()","45c71802":"## make sure we use the subsampe in our correlation\n\nf, (ax1, ax2) = plt.subplots(2,1, figsize = (54,50))\n\n## Entire data frame\n\ncorr = df.corr()\nsns.heatmap(corr, cmap = 'coolwarm_r', annot_kws = {'size': 20}, ax= ax1)\nax1.set_title(\"Imbalanced Correlated Matrix \\n\")\n\n\nsub_sample_corr = nxdf.corr()\nsns.heatmap(sub_sample_corr, cmap = 'coolwarm_r', annot_kws = {'size': 20}, ax = ax2)\nax2.set_title(\"SubSample Correlation Matrix\")\nplt.show()","af63b4eb":"nxdf.hist(bins = 50, figsize = (35,20))\nplt.show()","477908c6":"## this is equally sampled dataset (perfectly balanced target)\n\nX = nxdf.drop(['Bankrupt?'],1)\ny = nxdf['Bankrupt?']\n\nrf_fs_Xtrain, rf_fs_Xtest, rf_fs_ytrain, rf_fs_ytest = train_test_split(X,y, test_size = 0.1, random_state = 1)","7bb970ef":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel","7061dec9":"## modelling with balanced traget \n\nmodel = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\nmodel.fit(rf_fs_Xtrain, rf_fs_ytrain)\n\nsel = SelectFromModel(model)","e4348cba":"## balanced target\n\nsel.fit(rf_fs_Xtrain, rf_fs_ytrain)","e58d05c8":"# balanced\n\nselected_feat= rf_fs_Xtrain.columns[(sel.get_support())]\nlen(selected_feat)","8525404e":"selected_feat","6fd91298":"## Creating a dataframe for only selected values to train later\n\nrf_fs = pd.DataFrame()\n\nfor column in selected_feat:\n    if column in nxdf:\n        rf_fs[column] = nxdf[column].values\n        \n","3ca602c4":"rf_fs","02d0b992":"from sklearn.decomposition import PCA","bdebb7e3":"n_components = 2\npca = PCA(n_components = n_components)\npca.fit(nxdf)","e8e1d4c3":"X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(rf_fs_Xtrain.values)","96e13333":"x_pca = pca.transform(nxdf)","d79ece22":"x_pca.shape","a4aaf49b":"# PCA scatter plot\nplt.figure(figsize = (8,8))\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(rf_fs_ytrain == 0), cmap='coolwarm', label= 'Stable_Company', linewidths=2)\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(rf_fs_ytrain == 1), cmap='coolwarm', label= 'Unstable_Company', linewidths=2)\nplt.show()","14b0fb17":"## Splitting dataset for Normal data without feature selection\n\nX_train, X_test, y_train, y_test = train_test_split(nxdf.drop('Bankrupt?', axis = 1), nxdf['Bankrupt?'],test_size = 0.1, random_state = 1)","60f9ab6a":"nxdf.shape","728efebb":"rf_fs.shape","85b3a189":"## Splitting RandomForest Feature Selection dataset\n\nfs_Xtrain, fs_Xtest, fs_ytrain, fs_ytest = train_test_split(rf_fs, nxdf['Bankrupt?'], test_size = 0.1, random_state = 1)","d1f460a4":"model_score = pd.DataFrame(columns = (\"Original_Dataset\",\"Selected_Dataset\"))","3036a909":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, confusion_matrix","4a87a427":"lrmodel1 = LogisticRegression(max_iter = 1000)\nlrmodel1.fit(X_train, y_train)\nscore1 = lrmodel1.score(X_test, y_test)\nlr_pred1 = lrmodel1.predict(X_test)","bd9737c5":"## Accuracy on Original Datset without Feature Selection:\n\nprint(\"Score:\", score1)","6c934262":"lr_cm1 = confusion_matrix(y_test, lr_pred1, labels = (1,0))","984d1168":"lr_cm1","a4bbf9ff":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(lr_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","300615b6":"y_test.value_counts()","efbe8309":"lrmodel2 = LogisticRegression(max_iter = 1000)\nlrmodel2.fit(fs_Xtrain, fs_ytrain)\nscore2 = lrmodel2.score(fs_Xtest, fs_ytest)\nlr_ypred2 = lrmodel2.predict(fs_Xtest)","baa93c38":"print(\"Score\", score2)","a9248db8":"lr_cm2 = confusion_matrix(fs_ytest, lr_ypred2, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", lr_cm2)","be17ae4f":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(lr_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","d24da60a":"model_score = model_score.append(pd.DataFrame({'Original_Dataset':[score1], 'Selected_Dataset': [score2]}, index = ['LogisticRegression']))","c7eab8d8":"model_score","0884ded7":"from sklearn.naive_bayes import GaussianNB","62e40557":"naiveb1 = GaussianNB()","e4e4a8d5":"naiveb1.fit(X_train, y_train)\nscore1 = naiveb1.score(X_test, y_test)\nnb_pred1 = naiveb1.predict(X_test)","2ee77c98":"print(\"Score:\", score1)","6b2f24aa":"nb_cm1 = confusion_matrix(y_test, nb_pred1, labels = (1,0))","5dc504cd":"print(\"Confusion Matrix: \\n\", nb_cm1)","fcdeb314":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(nb_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","a171df42":"naiveb2 = GaussianNB()","9789d937":"naiveb2.fit(fs_Xtrain, fs_ytrain)\nscore2 = naiveb2.score(fs_Xtest, fs_ytest)\nnb_pred2 = naiveb2.predict(fs_Xtest)","eddaf4c1":"print(\"Score:\", score2)","a4ecf918":"nb_cm2 = confusion_matrix(fs_ytest, nb_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", nb_cm2)","6d5b3428":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(nb_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","3f79eb66":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [score1], 'Selected_Dataset': [score2]}, index = ['NaiveBayes']))","dad92fdf":"model_score","355d4f86":"from sklearn.neighbors import KNeighborsClassifier","8c645d45":"knn1 = KNeighborsClassifier(n_neighbors = 7)","3e2bb625":"knn1.fit(X_train, y_train)","f6014108":"score1 = knn1.score(X_test, y_test)\nprint(score1)","bff10c76":"knn_pred1 = knn1.predict(X_test)\nknn_cm1 = confusion_matrix(y_test, knn_pred1, labels = (1,0))\nprint(\"Confusion Matrix:\\n\", knn_cm1)","b51b06b1":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(knn_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","8a1057f4":"### Hyperparameter tuning for KNN","89b0502c":"error_rate = []\n\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    pred_knn = knn.predict(X_test)\n    error_rate.append(np.mean(pred_knn != y_test))","2506e8f8":"plt.figure(figsize = (8,8))\nplt.plot(range(1,40), error_rate, color = 'blue', linestyle = 'dashed', marker = 'o', markerfacecolor = 'red', markersize = 10);\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","d713da26":"## let's see how much difference does it makes\n\ntuned_knn1 = KNeighborsClassifier(n_neighbors = 4)\ntuned_knn1.fit(X_train, y_train)","478d9207":"tuned_score1 = tuned_knn1.score(X_test, y_test)\nprint(tuned_score1)","84024675":"knn2 = KNeighborsClassifier(n_neighbors = 7)\nknn2.fit(fs_Xtrain, fs_ytrain)","33b39618":"score2 = knn2.score(fs_Xtest, fs_ytest)\nprint(score2)","6bf76974":"knn_pred2 = knn2.predict(fs_Xtest)\nknn_cm2 = confusion_matrix(fs_ytest, knn_pred2, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", knn_cm2)","60ed570c":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(knn_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","cee89f1a":"## Hyperparamter tuning for this","6b87735e":"error_rate = []\n\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(fs_Xtrain, fs_ytrain)\n    pred_knn = knn.predict(fs_Xtest)\n    error_rate.append(np.mean(pred_knn != fs_ytest))","b8edcb05":"plt.figure(figsize = (8,8))\nplt.plot(range(1,40), error_rate, color = 'blue', linestyle = 'dashed', marker = 'o', markerfacecolor = 'red', markersize = 10);\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","7d248dfa":"tuned_knn2 = KNeighborsClassifier(n_neighbors = 14)\ntuned_knn2.fit(fs_Xtrain, fs_ytrain)","339db780":"tuned_score2 = tuned_knn2.score(fs_Xtest, fs_ytest)\nprint(tuned_score2)","53a0e6f2":"tuned_knn_pred2 = knn.predict(fs_Xtest)\ntuned_cm2 = confusion_matrix(fs_ytest, tuned_knn_pred2, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", tuned_cm2)","434687c0":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(tuned_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","fb6b91d7":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [tuned_score1], 'Selected_Dataset': [tuned_score2]}, index = ['KNN']))\n","fc22bf15":"model_score","a41de3fd":"from sklearn.tree import DecisionTreeClassifier","e44a2a73":"dt1 = DecisionTreeClassifier()","5bf4fd6d":"dt1 = dt1.fit(X_train, y_train)","d3a1b6e4":"score1 = dt1.score(X_test, y_test)\nprint(score1)","4ab57f49":"dt_pred1 = dt1.predict(X_test)\ndt_cm1 = confusion_matrix(y_test, dt_pred1, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", dt_cm1)","03837359":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(dt_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","bb4f2181":"dt2 = DecisionTreeClassifier()","9ef83a37":"dt2 = dt2.fit(fs_Xtrain, fs_ytrain)","102e841d":"score2 = dt2.score(fs_Xtest, fs_ytest)\nprint(score2)","2b693e30":"dt_pred2 = dt2.predict(fs_Xtest)\ndt_cm2 = confusion_matrix(fs_ytest, dt_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", dt_cm2)","65b33766":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(dt_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","dcfcdbd7":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [score1], 'Selected_Dataset': [score2]}, index = ['DecisionTrees']))\n","eb7ec245":"model_score","36fe12da":"rfclf1 = RandomForestClassifier(n_estimators = 100)","e21c0ba9":"rfclf1.fit(X_train, y_train)","37d2b693":"score1 = rfclf1.score(X_test, y_test)\nprint(score1)","512c16bf":"rf_pred1 = rfclf1.predict(X_test)\nrf_cm1 = confusion_matrix(y_test, rf_pred1, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", rf_cm1)","e181948b":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(rf_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","f8292f64":"rfclf2 = RandomForestClassifier(n_estimators = 100)","dc1d4254":"rfclf2.fit(fs_Xtrain, fs_ytrain)","f3ce84e2":"score2 = rfclf2.score(fs_Xtest, fs_ytest)\nprint(score2)","70c00a76":"rf_pred2 = rfclf2.predict(fs_Xtest)\nrf_cm2 = confusion_matrix(fs_ytest, rf_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", rf_cm2)","686d8705":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(rf_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","1c530863":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)\n               ]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [2,4]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","b92535a7":"# Create the param grid\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(param_grid)","17f7bcf5":"tuned_rf = RandomForestClassifier()","5345db6d":"from sklearn.model_selection import GridSearchCV\nrf_Grid = GridSearchCV(estimator = tuned_rf, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)","5d5134fa":"rf_Grid.fit(X_train, y_train)","8e7cc717":"rf_Grid.best_params_","80cfbd22":"print (f'Train Accuracy - : {rf_Grid.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {rf_Grid.score(X_test,y_test):.3f}')","eab604cd":"tuned_score2 = rf_Grid.score(X_test, y_test)\nprint(tuned_score2)","96789c19":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [score1], 'Selected_Dataset': [tuned_score2]}, index = ['RandomForest']))","ca6b4125":"model_score","0683439c":"from xgboost import XGBClassifier","70653321":"xgb1 = XGBClassifier(n_estimators = 100)\nxgb1.fit(X_train, y_train)","182d0a95":"score1 = xgb1.score(X_test, y_test)\nprint(score1)","7d0d0089":"xgb_pred1 = xgb1.predict(X_test)\nxgb_cm1 = confusion_matrix(y_test, xgb_pred1, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", xgb_cm1)","a1785bda":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(xgb_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","4a43e722":"xgb2 = XGBClassifier(n_estimators = 100)\nxgb2.fit(fs_Xtrain, fs_ytrain)","54e7812f":"score2 = xgb2.score(fs_Xtest, fs_ytest)\nprint(score2)","dcee50b0":"xgb_pred2 = xgb2.predict(fs_Xtest)\nxgb_cm2 = confusion_matrix(fs_ytest, xgb_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", xgb_cm2)","04ecc0b8":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(xgb_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","8f6ab807":"from sklearn.model_selection import RandomizedSearchCV","a6705a9e":"params={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","eb45d8fe":"tuned_xgb = XGBClassifier()","2f4927ce":"random_search = RandomizedSearchCV(tuned_xgb, param_distributions = params, n_iter = 5, scoring = 'roc_auc', n_jobs = 1, cv = 5, verbose = 3)","a08fd45e":"random_search.fit(X_train, y_train)","03f418e5":"random_search.best_estimator_","b787b3ae":"random_search.best_params_","bad23657":"tuned_score1 = random_search.score(X_test, y_test)\nprint(tuned_score1)","62eea04f":"print (f'Train Accuracy - : {random_search.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {random_search.score(X_test,y_test):.3f}')","2d226f43":"tuned_xgb_pred1 = random_search.predict(X_test)\ntuned_xgb_cm1 = confusion_matrix(y_test, tuned_xgb_pred1, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", tuned_xgb_cm1)","84e55a9c":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(tuned_xgb_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","1386df4f":"tuned_xgb2 = XGBClassifier()","3e567938":"random_search2 = RandomizedSearchCV(tuned_xgb2, param_distributions = params, n_iter = 5, scoring = 'roc_auc', n_jobs = 1, cv = 5, verbose = 3)","d8390485":"random_search2.fit(fs_Xtrain, fs_ytrain)","391090a6":"random_search2.best_estimator_","06576766":"random_search2.best_params_","1699895d":"tuned_score2 = random_search2.score(fs_Xtest, fs_ytest)\nprint(tuned_score2)","b16f0132":"print (f'Train Accuracy - : {random_search2.score(fs_Xtrain,fs_ytrain):.3f}')\nprint (f'Test Accuracy - : {random_search2.score(fs_Xtest,fs_ytest):.3f}')","a125a43b":"tuned_xgb_pred2 = random_search2.predict(fs_Xtest)\ntuned_xgb_cm2 = confusion_matrix(fs_ytest, tuned_xgb_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", tuned_xgb_cm2)","65b5ef0d":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(tuned_xgb_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","89ddfc6e":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [tuned_score1], 'Selected_Dataset': [tuned_score2]}, index = ['XGBoost']))\n","c56a95be":"model_score","eefa5de4":"## Checking Classification report of the best model\n\nprint(classification_report(fs_ytest, tuned_xgb_pred2))","67baf147":"## Biased model\n\nprint(classification_report(y_test, tuned_xgb_pred1))","af594e39":"### Checking Classification report of the worst model\n\nprint(classification_report(y_test, lr_ypred2 ))","d2a34135":"### With Original Dataset","b33b4e9d":"Definitely this is biased towards, postive class. Since this is unbalanced dataset. We will hyeprtune with equally balanced dataset ","1516091a":"Let's test with, K = 5","0e3e1c4c":"### Hyperparamter Tuning","2d59dad4":"### Feature Selection Dataset","ff6f1d18":"We see the data is highly skewed towards, Financially stable. If we train the model on this dataset, our prediction will be biased towards Financially stabled.\n\nWe will balance the dataset, to train our model.\n\nNotice: Notice how imbalanced is our original dataset! Most of the comapnies are Financially Stable. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most of the companies are Financially Stable. But we don't want our model to assume, we want our model to detect patterns that give signs of Bankrupt!","26d18938":"## 3. Data Preprocessing\n\n- Split Training and Testing\n- Feature selection with RandomForest\n- PCA","d3618de7":"### 2.1 Checking for Data imbalance","b32a9de0":"### PCA","e11d10d3":"#### Split Training and Testing","600324ac":"#### Splitting the Data (Original DataFrame)\n\n\nBefore proceeding with the <b> RandomUnderSampling <\/b> technique we have to seperate the original dataframe. \n\n<b>Why? <\/b>\n\nfor testing purposes, remeber although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model with the dataframes that were undersample and oversample (in order for our model to detect the patterns) and test it on the original testing set.","47cf1608":"### Original Dataset","3755f022":"#### Correlation Matrices\n\nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample) in order for use to see which features have a high positive or negative correlation with regards to fraud transactions.","1dea051e":"#### Hyperparamter Tuning","c0efec32":"### Original dataset","746c8967":"# KNN","01a90957":"#### Preparing all the dataset for the models\n\n- <b> nxdf <\/b> is the original dataset.\n- <b> rf_fs <\/b> is the dataset with Feature Selection from Random Forest\n","71aa042c":"### Observation\n\n- All the features are numerical (int64 or float64)\n- All the values are scaled between -1 to 1.","c541dde2":"- Since <b> rf_fs <\/b> target feature <b> Bankrupt? <\/b> has already been dropped. We know nxdf and rf_fs has same target value i.e ['Bankrupt'] so we will use the target value from nxdf for splitting Selected Dataset","56e57554":"# Exploratory Data Analysis\n\n- 2.1 Checking for data imbalance\n- 2.2 Outliers\n- 2.3 Filling null values\n","8b788021":"## Attribute Information:\n\nThere are total 95 features. This Dataset has a lot of features. The Dataset description is given on the Data's page itself.\nBefore blindly performing EDA it's important to have information about the data.\n\nhttps:\/\/www.kaggle.com\/fedesoriano\/company-bankruptcy-prediction\n\nFirst we will train the model on raw data, and we will use <b> Feature Selection <\/b> technique to highlight some of the features and train on selected features. Hence, we will compare the models and accuracy.\n\n\n## Our Plan\n\n\n\n- <b> 1. Observe Dataset <\/b>\n\n\n- <b> 2. Exploratory Data Analysis <\/b>\n\n    - 2.1 Datset Cleaning\n    - 2.2 Check for data imbalance\n    \n\n\n- <b> 3. Data Preprocessing <\/b>\n\n    - 3.2 Split Training and testing\n    - 3.2 Feature Selection with RandomForest\n    - 3.3 PCA\n    \n    \n- <b> 4. Models, Hyperparameter Tuning, Cross Validation and Model Evaluation <\/b>\n\n    - 4.1 Logistic Regression\n    - 4.2 Naive Bayes\n    - 4.3 K-Nearest Neighbor\n    - 4.4 Decision Tree\n    - 4.5 Random Forest\n    - 4.6 XGBoost\n    \n","3c5e46a3":"### Feature selection with RandomForest","91d01fb2":"# 1. Observe Dataset","ad5fa81f":"### With Original dataset","17ee9ac6":"#### Random Under-Sampling and OverSampling\n\nIn this phase of the project we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and this avoiding our models to overfitting.\n","5371c4a7":"### With Selected Features","644377b6":"We can see, it's not that different","772c4950":"### Feature Selection Dataset","b3be4f66":"# Logistic Regression","f6d3f543":"### Feature Selection Data","71fe1d07":"# Decision Tree","bce4d8be":"# XGBoost","101be1a3":"### Original Dataset","4a922fb6":"### Original data","f8bf8f98":"# Random Forest","c1ee4c11":"### Feature Selection Dataset","d55ab1dc":"As we can see, <b> XGBoost <\/b> performs best, on <b> selected features <\/b>. ","c51e5f9f":"A bit better but not that great","696d1ded":"### With Selected Dataset","6974217d":"# Naive Bayes","58f235ca":"## Testing our Models\n\nWe will test our all the dataset (i.e normal, random forest feature selection and PCA dataset with each model.\n\nFor comparison we will make a new dataFrame, and comapre which method performed better\n\nAlso as it is classification problem, we will test it with following algorithms\n\n- Logistic Regression\n- Naive Bayes\n- KNN\n- Decision Trees\n- Random Forest\n- SVM","229c3e78":"### Hypertuning for balanced dataset","f1c7a1c2":"We can see there are large number of blue square boxes and red square boxes which indicates, those column are has high or low correlation with one or other. So we will use PCA (Dimensionality Reduction) technqiue. \n\n<b> PCA vs Feature Selection? <\/b>\n\nhttps:\/\/stackoverflow.com\/questions\/16249625\/difference-between-pca-principal-component-analysis-and-feature-selection#:~:text=The%20difference%20is%20that%20PCA,takes%20the%20target%20into%20consideration.&text=PCA%20is%20based%20on%20extracting,data%20shows%20the%20highest%20variability.\n\nJust to add to the very good answers above. The difference is that PCA will try to reduce dimensionality by exploring how one feature of the data is expressed in terms of the other features(linear dependecy). Feature selection instead, takes the target into consideration. It will rank your input variables in terms of how useful they are to predict the target value. This is true for univariate feature selection. Multi variate feature selection can also do something that can be considered a form of PCA, in the sense that it will discard some of the features in the input. But don't take this analogy too far."}}