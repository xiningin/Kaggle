{"cell_type":{"99a6093a":"code","4a73be2c":"code","d4ef12e0":"code","a3ee9dba":"code","862ea9ce":"code","a2c96610":"code","85c67c68":"code","6558b91a":"code","21ee06f3":"code","42ef9f65":"code","13079625":"code","cb483df8":"code","3e432b9d":"code","961712a5":"code","3fe9f1b2":"code","0553456c":"code","1418e030":"code","82590dfa":"code","cd725b8c":"code","f0bfa802":"code","8bce26a0":"code","36cb0182":"code","13c8f93b":"code","d00b3874":"code","02369ac3":"code","93c639cb":"code","c126f385":"code","8f8bef90":"code","45e34bf3":"code","55f253c9":"code","2af8789f":"code","1339f251":"code","75dd8929":"code","ed218974":"code","6d28b25f":"code","823258b0":"code","095342ae":"code","ad04ee12":"code","dbbbae36":"code","0da81f38":"code","b0dad291":"code","4872da89":"code","a4d737e2":"code","43c39600":"code","5b0ea5b1":"code","cb028126":"code","95217376":"code","61d8d3cf":"code","5fea1f70":"code","44f1daac":"code","208d89d2":"code","841be464":"code","45edaf47":"code","705583b1":"code","09dc11f1":"code","e79a273a":"code","dde69278":"code","1ad1186e":"code","162aa621":"markdown","b27882c1":"markdown","55ab329e":"markdown","08b308ea":"markdown","fc66127f":"markdown","52a3ba5a":"markdown","10279aaf":"markdown","d5b51dd8":"markdown","9e74db8d":"markdown","7bf2bdf9":"markdown","750eca70":"markdown","00ab7bdf":"markdown","d186532e":"markdown","364de90b":"markdown","1d45b6c4":"markdown","659963ff":"markdown","6b8cf806":"markdown","04937cff":"markdown","74fba98a":"markdown","2bddb156":"markdown","13a52b23":"markdown","a03faf6c":"markdown","716546a5":"markdown","a2879c06":"markdown","994084cf":"markdown","9530770f":"markdown","08ef33b6":"markdown","40f754fd":"markdown","9a781cdf":"markdown","3cecae23":"markdown","412cbb7c":"markdown","9a51c2e0":"markdown","8c8a5b07":"markdown","d448bfca":"markdown","a73bc84f":"markdown","3f377eee":"markdown","8cb2bdf3":"markdown","ca3e9f63":"markdown","91162cb1":"markdown","8f6d3464":"markdown","e00cd904":"markdown","821f67f6":"markdown","3188ed33":"markdown","0fc34f76":"markdown","ac98f101":"markdown","63a383dd":"markdown","ff2b7702":"markdown","92886a92":"markdown","be04a96a":"markdown","8574a2d5":"markdown","cea29721":"markdown"},"source":{"99a6093a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.api.types import CategoricalDtype\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nfrom matplotlib import style\nstyle.use('seaborn')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline","4a73be2c":"pd.options.display.float_format = '{:,.2f}'.format","d4ef12e0":"PATH = '..\/input\/'","a3ee9dba":"df = pd.read_csv(PATH + 'train.csv', low_memory=False, index_col=0)\ntest = pd.read_csv(PATH + 'test.csv', low_memory=False, index_col=0)\ndf.head()","862ea9ce":"df.shape, test.shape","a2c96610":"df.describe(include='all')","85c67c68":"test.describe(include='all')","6558b91a":"df[['Survived']].groupby(df['Pclass']).mean().plot.bar()\nplt.show()","21ee06f3":"df[['Survived']].groupby(df['Sex']).mean().plot.bar()\nplt.show()","42ef9f65":"bins=[b for b in range(0, 91, 5)]\nfig = plt.figure()\n# ax0 = plt.subplot2grid((1, 2), (0, 0), colspan=2)\n# plt.title(\"Age Histogram\")\n\nax1 = plt.subplot2grid((1, 2), (0, 0))\ndf['Age'][df['Survived'] == 1].hist(bins=bins, color='g')\nplt.title(\"Surviving passengers\")\nax2 = plt.subplot2grid((1, 2), (0, 1), sharey=ax1)\ndf['Age'][df['Survived'] == 0].hist(bins=bins, color='r')\nplt.title(\"Deceased passengers\")\nplt.tight_layout()\nplt.show()","13079625":"threshold = 7\ndf[['Survived']].groupby(df['Age'].apply(lambda x: f'below {threshold}' if x < threshold else f'above {threshold}')).mean().plot.bar()\nplt.show()","cb483df8":"df[['Survived']].groupby(df['Name'].apply(lambda x: x.split(sep = ',')[1].split(sep = \".\")[0].strip())).mean().sort_values(by=\"Survived\").plot.bar()\nplt.show()","3e432b9d":"df[['Survived']].groupby([df['SibSp']]).mean().plot.bar()\nplt.show()","961712a5":"df[['Survived']].groupby([df['Parch']]).mean().plot.bar()\nplt.show()","3fe9f1b2":"df[['Survived']].groupby([df['Parch'] + df['SibSp']]).mean().plot.bar()\nplt.show()","0553456c":"df[['Survived']].groupby([df['Parch'] + df['SibSp']]).count()\/df.shape[0]","1418e030":"df[['Survived']].groupby([df['Embarked']]).mean().plot.bar()\nplt.show()","82590dfa":"df[['Survived']].groupby([df['Embarked']]).count()","cd725b8c":"df.isna().sum()[df.isna().sum() != 0]\/df.shape[0] * 100","f0bfa802":"test.isna().sum()[test.isna().sum() != 0]\/test.shape[0] * 100","8bce26a0":"df['Cabin'].value_counts()","36cb0182":"df['Deck'] = df['Cabin'].fillna(value='NA').apply(lambda x: ''.join(filter(str.isalpha, x))[0] if x != 'NA' else x)\ntest['Deck'] = test['Cabin'].fillna(value='NA').apply(lambda x: ''.join(filter(str.isalpha, x))[0] if x != 'NA' else x)","13c8f93b":"df[['Survived']].groupby(df['Deck']).mean().plot.bar()\nplt.title(\"Survival rate by deck\")\nplt.show()","d00b3874":"df[['Name', 'Sex', 'Age']].sort_values('Age')","02369ac3":"titles = ['Mr.', 'Master.', 'Rev.', 'Dr.', 'Sir.', 'Don.', 'Capt.', 'Lady.', 'Miss.', 'Ms.', 'Mrs.']\n\nfor title in titles:\n    print(f\"Title: {title}, Median of {df[['Name', 'Age']][df['Name'].str.contains(title)].median()}\")","93c639cb":"df[['Name', 'Age']][~df['Name'].str.contains(', M')]","c126f385":"def db_mod(db, title): db.loc[db[db['Name'].str.contains(title) & (db['Age'].isnull())].index, 'Age'] = df[\"Age\"][df['Name'].str.contains(title)].median()\n\nfor title in titles: \n    db_mod(df, title=title)\n    db_mod(test, title=title)","8f8bef90":"df['Embarked'].fillna(value='NA', inplace=True)","45e34bf3":"df[['Pclass', 'Fare']].groupby('Pclass').mean()","55f253c9":"test[test['Fare'].isnull()]","2af8789f":"test.loc[test[(test['Pclass'] == 3) & (test['Fare'].isnull())].index, 'Fare'] = df['Fare'][df['Pclass'] == 3].mean()","1339f251":"df['FamSize'] = df['SibSp'] + df['Parch']\ntest['FamSize'] = test['SibSp'] + test['Parch']","75dd8929":"df['Title'] = df['Name'].apply(lambda x: x.split(sep = ',')[1].split(sep = \".\")[0].strip())\ntest['Title'] = test['Name'].apply(lambda x: x.split(sep = ',')[1].split(sep = \".\")[0].strip())","ed218974":"df.columns","6d28b25f":"cat_list = ['Sex', 'FamSize', 'Pclass']\ndummy_list = ['Embarked', 'Deck', 'Title']\ncont_list = ['Age', 'Fare']\ndep = 'Survived'","823258b0":"train_set = df[cat_list + cont_list + dummy_list + [dep]].copy()\ntest_set = test[cat_list + cont_list + dummy_list].copy()","095342ae":"def process_df(df, is_train=True, train=train_set):\n    for c in cat_list + cont_list + dummy_list:\n        if is_train:\n            if c in cat_list: df[c] = df[c].astype(\"category\").cat.as_ordered()\n        else:\n            if c in cat_list: df[c] = df[c].astype(CategoricalDtype(train[c].cat.categories))\n        if c in cont_list:\n            df[c] = df[c].astype(\"float32\")\n        if c in dummy_list:\n            cols = pd.get_dummies(df[c], prefix=c + \"_\")\n            for col in cols.columns: df[col] = cols[col]\n            df.drop(columns=c, inplace=True)","ad04ee12":"process_df(train_set)","dbbbae36":"process_df(test_set, is_train=False)","0da81f38":"train_num = train_set.copy()\n\nfor c in train_num.columns: train_num[c] = train_num[c].cat.codes if c in cat_list else train_num[c]","b0dad291":"plt.figure(figsize=(20, 20))\nplt.title(\"Correlation table\")\n# sns.heatmap(train_num[[dep] + cat_list + cont_list].corr(), annot=True, cmap=\"seismic\")\nsns.heatmap(train_num.corr(), annot=True, cmap=\"seismic\")\nplt.tight_layout()\nplt.show()","4872da89":"test_num = test_set.copy()\n\nfor c in test_num.columns: test_num[c] = test_num[c].cat.codes if c in cat_list else test_num[c]","a4d737e2":"def acc(targ, pred): return (targ == pred).mean()","43c39600":"r = 0.3\nidxs = np.random.choice(np.arange(len(train_num)), size = int(len(train_num) * 0.3), replace=False)\nidxs_mask = train_num.index.isin(idxs)","5b0ea5b1":"X_train = train_num.drop(columns='Survived')[~idxs_mask]\nX_val = train_num.drop(columns='Survived')[idxs_mask]\ny_train = train_num['Survived'][~idxs_mask]\ny_val = train_num['Survived'][idxs_mask]","cb028126":"X_val.describe(include='all')","95217376":"test_num.describe(include='all')","61d8d3cf":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","5fea1f70":"accurs = []\nmin = 1\nmax = 15\nstep = 1\nfor p in range(min, max, step):\n    m = RandomForestClassifier(n_estimators=45, max_features=0.88, min_samples_leaf=p,\n                              n_jobs=-1, oob_score=True)\n    m.fit(X_train, y_train)\n    preds = m.predict(X_val)\n    print(\"-\"*30, f'''\n    n-estimators: {p}\n    Training score: {m.score(X_train, y_train)*100:.2f}%\n    Validation score: {m.score(X_val, y_val)*100:.2f}%\n    Out-of-Bag score: {m.oob_score_*100:.2f}%\n    Accuracy: {acc(y_val, preds)*100:.2f}%\n    ''')\n    accurs.append([p, acc(y_val, preds)])\naccurs = np.array(accurs)\naccurs[np.unravel_index(accurs[:, 1].argmax(), accurs[:, 1].shape)[0], :]","44f1daac":"m = RandomForestClassifier(n_estimators=70, max_features=0.5, min_samples_leaf=1,\n                          n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\npreds = m.predict(X_val)\nprint(\"-\"*30, f'''\nTraining score: {m.score(X_train, y_train)*100:.2f}%\nValidation score: {m.score(X_val, y_val)*100:.2f}%\nOut-of-Bag score: {m.oob_score_*100:.2f}%\nAccuracy: {acc(y_val, preds)*100:.2f}%\n''')","208d89d2":"def cross_val(X, y, cv=10):\n    accuracies = cross_val_score(estimator = m,\n                                 X = X,\n                                 y = y,\n                                 cv = cv) # k: number of folds - typically 10\n    print(\"Average accuracy:\", round(accuracies.mean()*100,1),\"%\")\n    print(\"Standard deviation:\", round(accuracies.std()*100,1),\"%\")","841be464":"cross_val(X_train, y_train)","45edaf47":"accs = []\ntarg = m.score(X_train, y_train)\nnum_features = 15\n\nfor c in X_train.columns:\n    X = X_train.copy()\n    X[c] = X[[c]].sample(frac=1).set_index(X.index)[c]  # random shuffle of one column\n    accs.append(targ - m.score(X, y_train))\n    \n\nFI = sorted([[c, float(a)] for c, a in zip(X.columns, accs)], key=lambda x: x[1], reverse=True)[:num_features]\npd.DataFrame({'Score loss': [FI[i][1] for i in range(len(FI))], 'Features': [FI[i][0] for i in range(len(FI))]}).set_index('Features').sort_values(by='Score loss', ascending=True).plot.barh()\nplt.show()","705583b1":"top = 8\nselected = [FI[i][0] for i in range(len(FI))][:top]\nXt = X_train[selected].copy()\nXv = X_val[selected].copy()\nt = test_num[selected].copy()","09dc11f1":"m = RandomForestClassifier(n_estimators=70, max_features=0.5, min_samples_leaf=5,\n                          n_jobs=-1, oob_score=True)\nm.fit(Xt, y_train)\npreds = m.predict(Xv)\nprint(\"-\"*30, f'''\nTraining score: {m.score(Xt, y_train)*100:.2f}%\nValidation score: {m.score(Xv, y_val)*100:.2f}%\nOut-of-Bag score: {m.oob_score_*100:.2f}%\nAccuracy: {acc(y_val, preds)*100:.2f}%\n''')\ncross_val(Xt, y_train)","e79a273a":"accs = []\ntarg = m.score(Xt, y_train)\n\nfor c in Xt.columns:\n    X = Xt.copy()\n    X[c] = X[[c]].sample(frac=1).set_index(X.index)[c]  # random shuffle of one column\n    accs.append(targ - m.score(X, y_train))\n    \npd.DataFrame({'Score loss': accs}, index=X.columns).sort_values(by='Score loss', ascending=True).plot.barh()\nplt.title('Feature Importance')\nplt.show() ","dde69278":"m.predict(t).sum()\/t.shape[0]","1ad1186e":"my_submission = pd.DataFrame({'Survived': m.predict(t)}, index=t.index)\nmy_submission.to_csv('submission.csv')","162aa621":"The accuracy is reasonably close to where it has been previously. Let's look at the variable contribution again:","b27882c1":"We are only missing a small number of values for Embarked in the training set, which we will fill with 'NA's","55ab329e":"Then we transform the training set into a numerical representation:","08b308ea":"We will now classify our features thet we'll be using for further analysis as categorical, dummy categorical, continuous and dependant variable:","fc66127f":"We compare the distributions in the validation and test sets:","52a3ba5a":"In order to transform features into numerical values, we transform the appropriate variables in the training set:","10279aaf":"Women had a much higher survival rate - not surprising as they most likely had priority over the male passengers into the lifeboats.","d5b51dd8":"Notable difference between the surviving and deceased passenger age groups is that small children were more significantly more likely to survive.\n\nIn general, the conclusions we draw so far is that indeed, the principle of *\"women and children first\"* was followed.","9e74db8d":"## Feature Selection","7bf2bdf9":"Passengers from Cherbourg managed to survive more frequently for some reason.","750eca70":"It's difficult to see any pattern in the above features. Given the fact they both relate to number of family members, perhaps we should look at them in combination:","00ab7bdf":"# Imports","d186532e":"We do a similar transformation on the test set, making sure the category mapping remains the same as in the training set:","364de90b":"Now, let's look at the full correlation matrix for the selected features:","1d45b6c4":"**SibSp and Parch**","659963ff":"## Modelling","6b8cf806":"The feature distributions appear to be reasonably similar.","04937cff":"As for the missing fare information in the Test dataset, we will substitute it with the Pclass mean from the training set:","74fba98a":"We'll look at the variable contribution to our overall accuracy:","2bddb156":"We start off by adding new features into our dataset:","13a52b23":"**Training \/ validation split**\n\nSince the test set comprises ca. 30% of overall passangers, we will use a similar ratio to obtain a validation set:","a03faf6c":"## Dataset Import","716546a5":"**Random Forest classifier**","a2879c06":"**Other missing values**","994084cf":"## Data Pre-processing","9530770f":"**Age**","08ef33b6":"**Name & title**","40f754fd":"**Missing Values**","9a781cdf":"**Age**","3cecae23":"Turns out this is just a single observation. Let's fill it in:","412cbb7c":"It looks like most variables do not add too much to the overall accuracy of the model. Let's reestimate based only on the top contributors:","9a51c2e0":"## Submission","8c8a5b07":"Let's look at cabin first, where over 77% observations are missing...","d448bfca":"**Pclass**\n\nThe ticket class, indicating wealth and cabin placement on the deck - the higher the class, the closer the assigned cabins were to the life boats.","a73bc84f":"Not surprisingly, the survival rate appears to be decreasing sharply for higher class numbers","3f377eee":"We analyse the percentage of missing values in the dataset features:","8cb2bdf3":"First, we make note the cabin numbers alone most likely introduce little new information to what we already know from analysing the Pclass and the cabin number is too passenger specific anyway. However, from looking at the picture below we see that interesting conclusions can be drawn from the letter idicating the deck as the upper decks are closer to the rescue boats as we hinted earlier:\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/84\/Titanic_cutaway_diagram.png\/687px-Titanic_cutaway_diagram.png\" width=\"400\">\n\nThat's why for the purpose of our analysis, we will create a new feature Deck - the extracted deck information from the Cabin feature.\nIn the process, we also replace the missing values with 'NA's.\n","ca3e9f63":"Anyone with the title Mr. was very likely to die that day - this comes as no surprise after our analysis of passenger sex.","91162cb1":"## Dataset Initial Analysis","8f6d3464":"Age is going to be a little more tricky. Our observation so far has been that children had a higher survival rate. Therefore it would be beneficial for the model accuracy to differentiate between children and adults in the missing age data group.\n\nLet's have a look at the passenger names then:","e00cd904":"It looks like most passengers did not have family members onboard and those who did had a bigger chance of surviving - perhaps families ","821f67f6":"**Embarked**","3188ed33":"Both the training set and the test set numerical features appear to have a similar distribution, which is promising for applying ML solutions.","0fc34f76":"## Analysing the results","ac98f101":"**Cabin**","63a383dd":"The predicted survival rate is in the same order of magnitude as the one observed in our training data.","ff2b7702":"We'll therefore insert the missing age values using the median of the title:","92886a92":"**Sex**","be04a96a":"We extract the passenger title from the Name column:","8574a2d5":"There were a total of 2224 passengers and crew on board the Titanic when the ship left for her maiden voyage. 1502 people died during the catastrophe that ensued after colliding with the iceberg, leaving the survival rate at roughly 32.5%.\n\nIf we only look at passengers though, there were approximately 1300 leaving with the ship and 812 that died, leaving the passenger survival rate at a higher level of 37%, which is close to the one in our training sample - 38%. Thus, we expect the test sample to display a similar survival rate.\n\nLet's look closer at some of the features in our dataset:","cea29721":"It looks like young boys were addressed as *Master* in the register. In general, the median age differs depending on the title:"}}