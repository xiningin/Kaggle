{"cell_type":{"71fc282f":"code","300bd8d2":"code","f665d285":"code","b15f6282":"code","8facd8a8":"code","c1269eb7":"code","ccf4cb7b":"code","317c2867":"code","cf262395":"code","8bbac1a7":"code","02b2d2f0":"code","c4bd769a":"code","ea5d3159":"code","cae673dd":"code","8aad3d05":"code","af746ccb":"code","903436b2":"code","de26e6d6":"code","2e38bd92":"code","5a888dc9":"code","85a241e9":"code","8afc3cb2":"code","36973889":"code","6f5ccdf9":"code","be547b71":"code","81e0d709":"code","24b5fd82":"code","0402379e":"code","79569c3e":"code","7d61e09b":"code","a285eaaa":"code","b9173abb":"code","cd66b422":"code","f4704964":"code","b7ed3326":"code","b5b5a849":"code","e90d605c":"code","a1afec3f":"code","51c6bf84":"code","1300385b":"code","968cf41e":"code","d9ef35be":"code","0b255d34":"code","187bc613":"code","4dad46f4":"code","141d9292":"code","cce9bcfa":"code","52e988f5":"code","ffec4cd1":"code","2f58ee2e":"code","e980bbd4":"code","9dd757f8":"code","b33fcb94":"code","ce965bb2":"code","dd0bdf4e":"code","45fa0321":"code","e0fd9ac3":"code","43414207":"code","568d641b":"code","eb69429e":"markdown","7f55e916":"markdown","595f3a9d":"markdown","b506b921":"markdown","df54e1fa":"markdown","5c7cb43e":"markdown","6caf2115":"markdown","1e8b49e9":"markdown","7309af1b":"markdown","d79b124f":"markdown","5147b53c":"markdown","4611b6fe":"markdown","ee722a68":"markdown","f3888fde":"markdown","55c75ce5":"markdown","9cf1b11a":"markdown","f7a588bb":"markdown","e1a5c4f2":"markdown","f3207145":"markdown","d563fa50":"markdown","a403e1b9":"markdown","3843adea":"markdown","edaa55c1":"markdown","85c20ddb":"markdown","f1eb712b":"markdown","838eb08c":"markdown","595153e4":"markdown","1f499730":"markdown","c08f5c20":"markdown","debfc4aa":"markdown","5066c850":"markdown"},"source":{"71fc282f":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","300bd8d2":"df_fraud = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf_fraud.head()","f665d285":"#observe the different feature type present in the data\n\ndf_fraud.info()","b15f6282":"#Determining the count of Fraud activity and non fraud activity\nclasses=df_fraud['Class'].value_counts()\nnormal_share=classes[0]\/df_fraud['Class'].count()*100\nfraud_share=classes[1]\/df_fraud['Class'].count()*100","8facd8a8":"print(classes)\nprint(normal_share)\nprint(fraud_share)","c1269eb7":"# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n\ndf_fraud['Class'].value_counts().plot.bar()\nplt.show()\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nactivity = ['Non-Fraud','Fraud']\nper = [normal_share,fraud_share]\nax.bar(activity,per)\nplt.show()","ccf4cb7b":"# Create a scatter plot to observe the distribution of classes with Amount\nplt.figure(figsize=(8,6))\nplt.title('Distribution of Transaction Amount', fontsize=14)\nsns.distplot(df_fraud['Amount'], bins=100)\nplt.show()","317c2867":"# Create a scatter plot to observe the distribution of classes with time\nplt.figure(figsize=(8,6))\nplt.title('Distribution of Transaction Time', fontsize=14)\nsns.distplot(df_fraud['Time'], bins=100)\nplt.show()","cf262395":"# distribution of class\n\nfraud=df_fraud[df_fraud['Class']==1]\nnormal=df_fraud[df_fraud['Class']==0]\nprint(fraud.shape,normal.shape)","8bbac1a7":"# Create a scatter plot to observe the distribution of classes with Amount\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","02b2d2f0":"#Since the features V1 to V28 has been scaled beforehand, we will also scale the columns Time and Amount:\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# std_scaler = StandardScaler()\nrob_scaler = RobustScaler() #more robust to outliers\n\ndf_fraud['scaled_amount'] = rob_scaler.fit_transform(df_fraud.Amount.values.reshape(-1,1))\ndf_fraud['scaled_time'] = rob_scaler.fit_transform(df_fraud.Time.values.reshape(-1, 1))\n\ndf = df_fraud.drop(['Time', 'Amount'], axis=1)","c4bd769a":"scaled_amount = df_fraud.scaled_amount\nscaled_time = df_fraud.scaled_time\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)","ea5d3159":"y= df['Class']#class variable\nX=df.drop('Class', axis=1)","cae673dd":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.30, random_state=100)","8aad3d05":"print(np.sum(y))\nprint(np.sum(y_train))\nprint(np.sum(y_test))","af746ccb":"# plot the histogram of a variable from the dataset to see the skewness\n#get correlations of each features in dataset\n\nplt.figure(figsize=(12, 10))\ncorr = df.corr()\nfig = sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})\nfig.set_title('Correlation Matrix', fontsize=14)\nplt.show()","903436b2":"# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\nfrom sklearn.preprocessing import PowerTransformer\npt=PowerTransformer(copy=False)\n\nX_pwr=pt.fit_transform(X)","de26e6d6":"X_pwr_df = pd.DataFrame(X_pwr)","2e38bd92":"# plot the histogram of a variable from the dataset again to see the result \n# plot the histogram of a variable from the dataset again to see the result \ncorrmat =X_pwr_df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(X_pwr_df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","5a888dc9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\n","85a241e9":"\nlog_reg_params = {'penalty': [ 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid_log_reg = GridSearchCV(LogisticRegression(solver='liblinear'), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\nlog_reg = grid_log_reg.best_estimator_\n\n#knears_params = {'n_neighbors': list(range(2, 5, 1)), 'algorithm': ['auto']}# 'ball_tree', 'kd_tree', 'brute'\n#grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n#grid_knears.fit(X_train, y_train)\n#knears_neighbors = grid_knears.best_estimator_\n\n#svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf']}#'poly', 'sigmoid', 'linear'\n#grid_svc = GridSearchCV(SVC(), svc_params)\n#grid_svc.fit(X_train, y_train)\n#svc = grid_svc.best_estimator_\n\n\ntree_params = {'criterion': ['gini'], 'max_depth': list(range(2, 4, 1)), 'min_samples_leaf': list(range(5, 7, 1))}# 'entropy'\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\ntree_clf = grid_tree.best_estimator_\n","8afc3cb2":"xgb_params = {'learning_rate' :[0.3],'max_depth' :[6],'min_child_weight' : [1],'n_estimators' : [100]}#,eval_metric='mlogloss'\ngrid_xgb = GridSearchCV(XGBClassifier(), xgb_params)\ngrid_xgb.fit(X_train, y_train)\nxgb_clf = grid_xgb.best_estimator_","36973889":"#Evaluating  score by cross-validation\nfrom sklearn.model_selection import cross_val_score\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: {}%'.format(round(log_reg_score.mean() * 100, 2)))\n\n#knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n#print('Knears Neighbors Cross Validation Score: {}%'.format(round(knears_score.mean() * 100, 2)))\n\n#svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n#print('Support Vector Classifier Cross Validation Score: {}%'.format(round(svc_score.mean() * 100, 2)))\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score: {}%'.format(round(tree_score.mean() * 100, 2)))\n\nxgb_score = cross_val_score(xgb_clf, X_train, y_train, cv=5)\nprint('XG Boost Classifier Cross Validation Score: {}%'.format(round(xgb_score.mean() * 100, 2)))","6f5ccdf9":"# Generating cross-validated estimates for each input data point\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method='decision_function')\n#knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n#svc_pred = cross_val_predict(svc, X_train, y_train, cv=5, method='decision_function')\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)\nxgb_pred = cross_val_predict(xgb_clf, X_train, y_train, cv=5)","be547b71":"# roc score for the different models\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n\nprint('Logistic Regression:', roc_auc_score(y_train, log_reg_pred))\n#print('K-Nearest Neighbors:', roc_auc_score(y_train, knears_pred))\n#print('Support Vector Classifier:', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier:', roc_auc_score(y_train, tree_pred))\nprint('XG Boost Classifier:', roc_auc_score(y_train, xgb_pred))","81e0d709":"# Plotting roc curve for the different models\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nlog_fpr, log_tpr, log_threshold = roc_curve(y_train, log_reg_pred)\n#knears_fpr, knears_tpr, knears_threshold = roc_curve(y_train, knears_pred)\n#svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\nxgb_fpr, xgb_tpr, xgb_threshold = roc_curve(y_train, xgb_pred)\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, #knears_fpr, knears_tpr, svc_fpr, svc_tpr, \n                             tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    #plt.plot(knears_fpr, knears_tpr, label='K-Nearest Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    #plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_train, xgb_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr,# knears_fpr, knears_tpr, svc_fpr, svc_tpr, \n                         tree_fpr, tree_tpr)\nplt.show()","24b5fd82":"\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = log_reg.predict(X_train)\n\nprint('---' * 40)\nprint('Scores in Logistic regression:')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 40)\n","0402379e":"\ntree_params = {'criterion': ['gini'], 'max_depth': list(range(2, 4, 1)), 'min_samples_leaf': list(range(5, 7, 1))}# 'entropy'\ntree_clf = GridSearchCV(DecisionTreeClassifier(), tree_params)\ntree_clf.fit(X_train, y_train)\ntree_clf = grid_tree.best_estimator_\n\ntree_score = cross_val_score(tree_clf, X_train , y_train , cv=5)\nprint('DecisionTree Classifier Cross Validation Score: {}%'.format(round(tree_score.mean() * 100, 2)))","79569c3e":"var_imp = []\nfor i in tree_clf.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(tree_clf.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(tree_clf.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(tree_clf.feature_importances_)[-3])+1)\n\n# Variable on Index-16 and Index-13 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(tree_clf.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(tree_clf.feature_importances_)[-2])\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nnp.random.shuffle(X_train_0)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n            label='Actual Class-0 Examples')\nplt.legend()","7d61e09b":"len(X_train), len(X_test)","a285eaaa":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import RandomOverSampler \nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\nprint('Original dataset shape %s' % Counter(y))\nros = RandomOverSampler(random_state=42)\nX_ros, y_ros = ros.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_ros))\n","b9173abb":"\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import RandomOverSampler #- import the packages\n\nX, y = make_classification(n_classes=2, class_sep=2,weights=[0.1, 0.9], n_informative=3, n_redundant=1\n                           , flip_y=0,n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\nprint('Original dataset shape %s' % Counter(y_train))\n\nros = RandomOverSampler(random_state=42)\nX_train_ros, y_train_ros = ros.fit_resample(X, y)\n\nprint('Resampled dataset shape %s' % Counter(y_train_ros))","cd66b422":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n\n#Logistic Regression of RandomeOversampling \n\n# create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n#hyper-params\nlog_reg_params = {'penalty': [ 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# perform grid search\ngrid_log_reg = GridSearchCV(LogisticRegression(solver='liblinear'), \n                            param_grid=log_reg_params,\n                            scoring= 'r2', \n                            cv = folds, \n                            verbose = 1,\n                            return_train_score=True)      \n                    \n# fit the model\ngrid_log_reg.fit(X_train_ros, y_train_ros)\nlog_reg_ros = grid_log_reg.best_estimator_","f4704964":"#KNN model of Random Over Sampling \n\n\n# create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n#hyper-params\nknears_params = {'n_neighbors': list(range(2, 5, 1)), 'algorithm': ['auto']}# 'ball_tree', 'kd_tree', 'brute'\n\n# perform grid search\ngrid_knears = GridSearchCV(KNeighborsClassifier(), \n                           param_grid=knears_params,\n                           verbose=1,\n                           scoring= 'r2', \n                           cv = folds,\n                           return_train_score=True)\n\n# fit the model\n\ngrid_knears.fit(X_train_ros, y_train_ros)\nknears_neighbors_ros = grid_knears.best_estimator_","b7ed3326":"#SVC model for Random Over Sampling \n\n\n# create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n#hyper-params\nsvc_params = {'C': [1.0], 'kernel': ['rbf']}#'poly', 'sigmoid', 'linear' , # C : 0.5, 0.7, 0.9, \n\n# perform grid search\ngrid_svc = GridSearchCV(SVC(),\n                        param_grid=svc_params,\n                        verbose=1,\n                        scoring= 'r2', \n                        cv = folds,\n                        return_train_score=True)\n\n# fit the model\n\ngrid_svc.fit(X_train_ros, y_train_ros)\nsvc_ros = grid_svc.best_estimator_","b5b5a849":"#DecisionTreeClassifier model of Random Over Sampling \n\n# create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n#hyper-params\ntree_params = {'criterion': ['gini'], 'max_depth': list(range(2, 4, 1)), 'min_samples_leaf': list(range(5, 7, 1))}# 'entropy'\n\n# perform grid search\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), \n                        param_grid=tree_params,\n                        verbose=1,\n                        scoring= 'r2', \n                        cv = folds,\n                        return_train_score=True)\n\n# fit the model\n\ngrid_tree.fit(X_train_ros, y_train_ros)\ntree_clf_ros = grid_tree.best_estimator_","e90d605c":"#XGBoost model of Random Over Sampling\n\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\n\nxgb_params = {'learning_rate' :[0.3],\n            'max_depth' :[6],\n            'min_child_weight' : [1],\n            'n_estimators' : [100] \n              #,              eval_metric='mlogloss'\n           }\ngrid_xgb_ros = GridSearchCV(XGBClassifier(), xgb_params)\ngrid_xgb_ros.fit(X_train_ros, y_train_ros)\nxgb_clf_ros = grid_xgb.best_estimator_","a1afec3f":"#Evaluating a score by cross-validation\nfrom sklearn.model_selection import cross_val_score\n\n\nlog_reg_score = cross_val_score(log_reg_ros, X_train_ros, y_train_ros, cv=5)\nprint('Logistic Regression Cross Validation Score: {}%'.format(round(log_reg_score.mean() * 100, 2)))\n\nknears_score = cross_val_score(knears_neighbors_ros, X_train_ros, y_train_ros, cv=5)\nprint('Knears Neighbors Cross Validation Score: {}%'.format(round(knears_score.mean() * 100, 2)))\n\nsvc_score = cross_val_score(svc_ros, X_train_ros, y_train_ros, cv=5)\nprint('Support Vector Classifier Cross Validation Score: {}%'.format(round(svc_score.mean() * 100, 2)))\n\ntree_score = cross_val_score(tree_clf_ros, X_train_ros, y_train_ros, cv=5)\nprint('DecisionTree Classifier Cross Validation Score: {}%'.format(round(tree_score.mean() * 100, 2)))\n\nxgb_score = cross_val_score(xgb_clf_ros, X_train_ros, y_train_ros, cv=5)\nprint('XGB Classifier Cross Validation Score: {}%'.format(round(xgb_score.mean() * 100, 2)))","51c6bf84":"# Generating cross-validated estimates for each input data point for Random Over Sampling\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred_ros = cross_val_predict(log_reg_ros, X_train, y_train, cv=5, method='decision_function')\nknears_pred_ros = cross_val_predict(knears_neighbors_ros, X_train, y_train, cv=5)\nsvc_pred_ros = cross_val_predict(svc_ros, X_train, y_train, cv=5, method='decision_function')\ntree_pred_ros = cross_val_predict(tree_clf_ros, X_train, y_train, cv=5)\nxgb_pred_ros = cross_val_predict(xgb_clf_ros, X_train, y_train, cv=5)","1300385b":"# roc score for the different models\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n\nprint('Logistic Regression:', roc_auc_score(y_train, log_reg_pred_ros))\nprint('K-Nearest Neighbors:', roc_auc_score(y_train, knears_pred_ros))\nprint('Support Vector Classifier:', roc_auc_score(y_train, svc_pred_ros))\nprint('Decision Tree Classifier:', roc_auc_score(y_train, tree_pred_ros))\nprint('XGB Classifier:', roc_auc_score(y_train, xgb_pred_ros))","968cf41e":"# Plotting roc curve for the different models\n\nlog_fpr, log_tpr, log_threshold = roc_curve(y_train, log_reg_pred_ros)\nknears_fpr, knears_tpr, knears_threshold = roc_curve(y_train, knears_pred_ros)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred_ros)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred_ros)\nxgb_fpr, xgb_tpr, xgb_threshold = roc_curve(y_train, xgb_pred_ros)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knears_fpr, knears_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve for Random Over Sampling\\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred_ros)))\n    plt.plot(knears_fpr, knears_tpr, label='K-Nearest Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred_ros)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred_ros)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred_ros)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGB Classifier Score: {:.4f}'.format(roc_auc_score(y_train, xgb_pred_ros)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knears_fpr, knears_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,xgb_fpr, xgb_tpr)\nplt.show()","d9ef35be":"from imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","0b255d34":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=0)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_smote.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_smote==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_smote==0)))\n\n","187bc613":"from sklearn.model_selection import train_test_split, StratifiedKFold\n\nX = df.drop('Class', axis=1)\ny = df.Class\n\nskf = StratifiedKFold(n_splits=5, shuffle=False)\n\nfor train_index, test_index in skf.split(X, y):\n    original_X_train, original_X_test = X.iloc[train_index], X.iloc[test_index]\n    original_y_train, original_y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    original_X_train, original_X_test = original_X_train.values, original_X_test.values\n    original_y_train, original_y_test = original_y_train.values, original_y_test.values\n\n    # see if the train and test set have similar distributed labels\n    _, train_label_count = np.unique(original_y_train, return_counts=True)\n    _, test_label_count = np.unique(original_y_test, return_counts=True)\n\n    print('Label Distributions:')\n    print(train_label_count \/ len(original_y_train))\n    print(test_label_count \/ len(original_y_test))","4dad46f4":"#Logistic Regression after SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n\n\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\nlog_reg_params = {'penalty': ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(solver='liblinear'), log_reg_params, n_iter=4)\n\nfor train, test in skf.split(original_X_train, original_y_train):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg)\n    model = pipeline.fit(original_X_train[train], original_y_train[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_X_train[test])\n    \n    accuracy_lst.append(pipeline.score(original_X_train[test], original_y_train[test]))\n    precision_lst.append(precision_score(original_y_train[test], prediction))\n    recall_lst.append(recall_score(original_y_train[test], prediction))\n    f1_lst.append(f1_score(original_y_train[test], prediction))\n    auc_lst.append(roc_auc_score(original_y_train[test], prediction))\n    \nprint(\"Accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"Precision: {}\".format(np.mean(precision_lst)))\nprint(\"Recall: {}\".format(np.mean(recall_lst)))\nprint(\"F1: {}\".format(np.mean(f1_lst)))","141d9292":"from sklearn.metrics import classification_report\n\nlabels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_X_test)\n\nprint(classification_report(original_y_test, smote_prediction, target_names=labels))","cce9bcfa":"#Precision score after SMOTE\nfrom sklearn.metrics import average_precision_score\n\ny_score = best_est.decision_function(original_X_test)\naverage_precision = average_precision_score(original_y_test, y_score)\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","52e988f5":"#Precision recall curve for SMOTE in Logistic regression\nfig = plt.figure(figsize=(12, 6))\nprecision, recall, _ = precision_recall_curve(original_y_test, y_score)\nplt.step(recall, precision)\nplt.fill_between(recall, precision, alpha=0.2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall Curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(average_precision), fontsize=16)\nplt.show()","ffec4cd1":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn import over_sampling\n\nada = over_sampling.ADASYN(random_state=0)\nX_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train.ravel())\n","2f58ee2e":"print(np.sum(X_train_adasyn))","e980bbd4":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n\n#Logistic Regression of ADASYN \n\n# create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n#hyper-params\nlog_reg_params = {'penalty': [ 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# perform grid search\ngrid_log_reg = GridSearchCV(LogisticRegression(solver='liblinear'), \n                            param_grid=log_reg_params,\n                            scoring= 'r2', \n                            cv = folds, \n                            verbose = 1,\n                            return_train_score=True)      \n\n#dt = gs.best_estimator_.get_params()['clf']\n\n\n# fit the model\ngrid_log_reg.fit(X_train_adasyn, y_train_adasyn)\nlog_reg_adasyn = grid_log_reg.best_estimator_\n\nlog_reg_adasyn_score = cross_val_score(log_reg_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('Logistic Regression Cross Validation Score: {}%'.format(round(log_reg_adasyn_score.mean() * 100, 2)))\n\n\n","9dd757f8":"#KNN model of ADASYN \n\n\n# create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n#hyper-params\nknears_params = {'n_neighbors': list(range(2, 5, 1)), 'algorithm': ['auto']}# 'ball_tree', 'kd_tree', 'brute'\n\n# perform grid search\ngrid_knears = GridSearchCV(KNeighborsClassifier(),param_grid=knears_params,verbose=1,scoring= 'r2', cv = folds,return_train_score=True)\n\n# fit the model\n\ngrid_knears.fit(X_train_adasyn, y_train_adasyn)\nknears_neighbors_adasyn = grid_knears.best_estimator_\n\n\nknears_score_adasyn = cross_val_score(knears_neighbors_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('Knears Neighbors Cross Validation Score: {}%'.format(round(knears_score_adasyn.mean() * 100, 2)))\n","b33fcb94":"\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf']}#'poly', 'sigmoid', 'linear'\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train_adasyn, y_train_adasyn)\nsvc_adasyn = grid_svc.best_estimator_\nsvc_score_adasyn = cross_val_score(svc_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('Support Vector Classifier Cross Validation Score: {}%'.format(round(svc_score_adasyn.mean() * 100, 2)))\n\n\ntree_params = {'criterion': ['gini'], 'max_depth': list(range(2, 4, 1)), 'min_samples_leaf': list(range(5, 7, 1))}# 'entropy'\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train_adasyn, y_train_adasyn)\ntree_clf_adasyn = grid_tree.best_estimator_\ntree_score_adasyn = cross_val_score(tree_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('DecisionTree Classifier Cross Validation Score: {}%'.format(round(tree_score_adasyn.mean() * 100, 2)))\n\n#XGBoost\n\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\n\nxgb_params = {'learning_rate' :[0.3],'max_depth' :[6],'min_child_weight' : [1],'n_estimators' : [100] }#,eval_metric='mlogloss'\ngrid_xgb = GridSearchCV(XGBClassifier(), xgb_params)\ngrid_xgb.fit(X_train_adasyn, y_train_adasyn)\nxgb_clf_adasyn = grid_xgb.best_estimator_\nxgb_score_adasyn = cross_val_score(xgb_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('XG Boost Classifier Cross Validation Score: {}%'.format(round(xgb_score_adasyn.mean() * 100, 2)))\n","ce965bb2":"#Evaluating a score by cross-validation\n\n\nprint('Logistic Regression Cross Validation Score: {}%'.format(round(log_reg_adasyn_score.mean() * 100, 2)))\nprint('Knears Neighbors Cross Validation Score: {}%'.format(round(knears_score_adasyn.mean() * 100, 2)))\nprint('Support Vector Classifier Cross Validation Score: {}%'.format(round(svc_score.mean() * 100, 2)))\nprint('DecisionTree Classifier Cross Validation Score: {}%'.format(round(tree_score_adasyn.mean() * 100, 2)))\nprint('XG Boost Classifier Cross Validation Score: {}%'.format(round(xgb_score_adasyn.mean() * 100, 2)))\n","dd0bdf4e":"# Generating cross-validated estimates for each input data point for Random Over Sampling\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred_adasyn = cross_val_predict(log_reg_adasyn, X_train, y_train, cv=5, method='decision_function')\nknears_pred_adasyn = cross_val_predict(knears_neighbors_adasyn, X_train, y_train, cv=5)\nsvc_pred_adasyn = cross_val_predict(svc_adasyn, X_train, y_train, cv=5, method='decision_function')\ntree_pred_adasyn = cross_val_predict(tree_clf_adasyn, X_train, y_train, cv=5)\nxgb_pred_adasyn = cross_val_predict(xgb_clf_adasyn, X_train, y_train, cv=5)\n\n\n\n\n# roc score for the different models\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n\nprint('Logistic Regression:', roc_auc_score(y_train, log_reg_pred_adasyn))\nprint('K-Nearest Neighbors:', roc_auc_score(y_train, knears_pred_adasyn))\nprint('Support Vector Classifier:', roc_auc_score(y_train, svc_pred_adasyn))\nprint('Decision Tree Classifier:', roc_auc_score(y_train, tree_pred_adasyn))\nprint('XGB Classifier:', roc_auc_score(y_train, xgb_pred_adasyn))\n","45fa0321":"# Plotting roc curve for the different models\n\nlog_fpr, log_tpr, log_threshold = roc_curve(y_train, log_reg_pred_adasyn)\nknears_fpr, knears_tpr, knears_threshold = roc_curve(y_train, knears_pred_adasyn)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred_adasyn)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred_adasyn)\nxgb_fpr, xgb_tpr, xgb_threshold = roc_curve(y_train, xgb_pred_adasyn)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knears_fpr, knears_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve for Random Over Sampling\\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred_adasyn)))\n    plt.plot(knears_fpr, knears_tpr, label='K-Nearest Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred_adasyn)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred_adasyn)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred_adasyn)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGB Classifier Score: {:.4f}'.format(roc_auc_score(y_train, xgb_pred_adasyn)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knears_fpr, knears_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,xgb_fpr, xgb_tpr)\nplt.show()","e0fd9ac3":"# perform the best oversampling method on X_train & y_train\n\n#clf = ___  #initialise the model with optimum hyperparameters\n#clf.fit( ) # fit on the balanced dataset\n#print() --> #print the evaluation score on the X_test by choosing the best evaluation metric\n\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import accuracy_score\n\nprecision, recall, threshold = precision_recall_curve(y_train, tree_pred_adasyn)\n\ny_pred = tree_clf_adasyn.predict(X_test)\n\n# overfitting case (undersampling before cross validation)\nprint('---' * 40)\nprint('Scores in Decision Tree:')\nprint('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))\nprint('---' * 40)\n\n","43414207":"var_imp = []\nfor i in tree_clf_adasyn.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(tree_clf_adasyn.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(tree_clf_adasyn.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(tree_clf_adasyn.feature_importances_)[-3])+1)\n\n# Variable on Index-13 and Index-9 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(tree_clf_adasyn.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(tree_clf_adasyn.feature_importances_)[-2])\n\n#X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n#X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n#np.random.shuffle(X_train_0)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n            label='Actual Class-0 Examples')\nplt.legend()","568d641b":"#fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n\nfpr, tpr, thresholds = roc_curve(y_train, xgb_pred_adasyn, pos_label=2)\nthreshold = thresholds[np.argmax(tpr-fpr)]\nprint('Threshold :',threshold)\nprint('Train auc :', roc_auc_score(y_train, xgb_pred_adasyn))","eb69429e":"#### The data is highly imbalanced, so balancing the data to apply the different model algorithms.","7f55e916":"## Exploratory data analysis","595f3a9d":"### Select the oversampling method which shows the best result on a model\n- Apply the best hyperparameter on the model\n- Predict on the test dataset","b506b921":"Class - only contains values 0 and 1, we will use this as our Output Variable\nClass - value of 1 indicates 'Fraud', otherwise 0","df54e1fa":"### If there is skewness present in the distribution use:\n- <b>Power Transformer<\/b> package present in the <b>preprocessing library provided by sklearn<\/b> to make distribution more gaussian","5c7cb43e":"### Print the important features of the best model to understand the dataset","6caf2115":"#### Proceed with the model which shows the best result \n- Apply the best hyperparameter on the model\n- Predict on the test dataset","1e8b49e9":"Conclusion : \nWhen we are building a classifier, both precision and recall are important, but it all comes down to the task we are given. In this particular project, higher recall means we can detect more frauds which leads to a more safer platform, whereas higher precision means that we can correctly detect the fraud cases which leads to customer satisfaction (from avoiding their accounts getting blocked). \n","7309af1b":"##### Logistic Regression gave the best results, so applying hyperparameter on the model ","d79b124f":"## RandomOversampling","5147b53c":"### Print the important features of the best model to understand the dataset\n- This will not give much explanation on the already transformed dataset\n- But it will help us in understanding if the dataset is not PCA transformed","4611b6fe":"## ADASYN","ee722a68":"##### Build models on other algorithms to see the better performing on ADASYN","f3888fde":"##### Build models on other algorithms to see the better performing on SMOTE","55c75ce5":"## Model Building\n- Build different models on the imbalanced dataset and see the result","9cf1b11a":"Here we will observe the distribution of our classes","f7a588bb":"## Model Building\n- Build different models on the balanced dataset and see the result","e1a5c4f2":"## Model building with balancing Classes\n\n##### Perform class balancing with :\n- Random Oversampling\n- SMOTE\n- ADASYN","f3207145":"### Similarly explore other algorithms by building models like:\n- Logistic Regressions\n- KNN\n- SVM\n- Decision Tree\n- XGBoost","d563fa50":"##### Determining the feature importance","a403e1b9":"#### Print the FPR,TPR & select the best threshold from the roc curve","3843adea":"## SMOTE","edaa55c1":"The problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.","85c20ddb":"### Splitting the data into train & test data","f1eb712b":"Print the class distribution after applying SMOTE","838eb08c":"### Print the class distribution after applying ADASYN","595153e4":"##### Variables with feature importance are : V11, V18, V1\nSince data is PCA transforemed, the meaning of these variables from Business wise is not known.","1f499730":"### Plotting the distribution of a variable","c08f5c20":"## Credit Card Fraud Detection\n\nIn this project you will predict fraudulent credit card transactions with the help of Machine learning models. Please import the following libraries to get started.","debfc4aa":"##### Variables with feature importance are : V19, V16, V14\nSince data is PCA transforemed, the meaning of these variables from Business wise is not known.","5066c850":"##### Applying different algorithms using GridSearch cross validation.  Also performing the hyperparameter tuning and printing the evaluation result."}}