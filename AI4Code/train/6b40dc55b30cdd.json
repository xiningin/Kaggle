{"cell_type":{"25f16f5f":"code","278e9f4e":"code","22492ff9":"code","258c0470":"code","48f0de23":"code","11d96142":"code","5850eb35":"code","3160c416":"code","24dbf91d":"code","11a5e6a7":"code","1ac48b58":"code","f2db758d":"code","b37aadc0":"code","144401bd":"code","38d86325":"code","877fa88e":"code","3d562a47":"code","365886e1":"code","0de42247":"code","eedfadd6":"code","68f32003":"code","54f0db8b":"code","9cd434af":"code","580df38f":"code","ad6b4146":"code","345d0d53":"code","48b3f63b":"code","04271ef6":"code","44b6bbce":"code","c088ca85":"code","4d4b60c7":"code","6ab2ae41":"code","db3112e2":"code","3bc2d9fe":"code","ceb5ea00":"code","03c4996e":"code","aced1fea":"code","6a82d54e":"code","9fa24a78":"code","56d02fde":"code","f548ecf4":"code","249460aa":"code","95ef9ffd":"code","9f53654a":"code","abb7f05a":"code","dd71b772":"code","2ec6c693":"code","b4e67460":"code","3d305064":"code","a7ad3693":"code","e7812c96":"code","731b1148":"markdown","b58b5d69":"markdown","c3ee0351":"markdown","c8d911f3":"markdown","b49e8b8c":"markdown","18041f40":"markdown","8f7730f3":"markdown","0046a6f9":"markdown","5a39834c":"markdown","f6065baa":"markdown","80cad335":"markdown","c661ec10":"markdown","8116fbaf":"markdown","7fb1137b":"markdown","6569fbf5":"markdown","1bf66dbc":"markdown","0c24a104":"markdown","9d054c38":"markdown","40259c7a":"markdown","fbef70c1":"markdown","22c945f1":"markdown"},"source":{"25f16f5f":"# Data manipulation libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# Avoid Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n#Common model helpers\nfrom sklearn.preprocessing import (StandardScaler,\n                                   LabelEncoder,\n                                   OneHotEncoder)\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (classification_report, accuracy_score, \n                             auc, \n                             precision_score,\n                             recall_score,\n                             f1_score, \n                             roc_auc_score,\n                             confusion_matrix)\nfrom sklearn.model_selection import (GridSearchCV,\n                                     StratifiedKFold,\n                                     cross_val_score)\n\n\n# dimensionality reduction\nfrom sklearn.decomposition import PCA\nfrom umap import UMAP\nimport pylab as pl\n\n# imbalance dataset handling\n\nfrom imblearn.datasets import make_imbalance\nfrom imblearn.under_sampling import (RandomUnderSampler, \n                                     ClusterCentroids,\n                                     TomekLinks,\n                                     NeighbourhoodCleaningRule,\n                                     EditedNearestNeighbours,\n                                     NearMiss)\n\n\nfrom imblearn.over_sampling import (SMOTE,\n                                    ADASYN)\n# model algorithams\nfrom sklearn.ensemble import (RandomForestClassifier, \n                              AdaBoostClassifier, \n                              GradientBoostingClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n","278e9f4e":"#Reading the csv file in variable \n\ndf = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","22492ff9":"df.head()","258c0470":"df.tail()","48f0de23":"#get the size of dataframe\ndf.shape","11d96142":"df.dtypes\n","5850eb35":"df.info()\n","3160c416":"df.describe().T\n","24dbf91d":"import missingno as msno\n\nmsno.matrix(df)","11a5e6a7":"msno.bar(df, sort = 'descending')\n","1ac48b58":"df['bmi'].nunique()\n","f2db758d":"# handling missing values\ndf['bmi'].fillna(df['bmi'].mean(), inplace=True)\ndf.isnull().sum()","b37aadc0":"fig,axes = plt.subplots(4,2,figsize = (16,16))\nsns.set_style('whitegrid')\nfig.suptitle(\"Count plot for various categorical features\")\n\nsns.countplot(ax=axes[0,0],data=df,x='gender')\nsns.countplot(ax=axes[0,1],data=df,x='hypertension')\nsns.countplot(ax=axes[1,0],data=df,x='heart_disease')\nsns.countplot(ax=axes[1,1],data=df,x='ever_married')\nsns.countplot(ax=axes[2,0],data=df,x='work_type')\nsns.countplot(ax=axes[2,1],data=df,x='Residence_type')\nsns.countplot(ax=axes[3,0],data=df,x='smoking_status')\nsns.countplot(ax=axes[3,1],data=df,x='stroke')\n\nplt.show()","144401bd":"#This is to look at what all unique values have . Just trying to use python\nlist_col=['smoking_status','work_type','Residence_type','gender']\n\n#What are the different types of smking status?\n#What are the different work types?\n#What are the residence types?\n#How many models we have?\n\nfor col in list_col: \n    print('{} :{} ' . format(col.upper(),df[col].unique()))","38d86325":"fig = px.box(data_frame = df,\n            x = \"avg_glucose_level\",\n            width = 800,\n            height = 300)\nfig.update_layout({\"template\":\"plotly_dark\"})\nfig.show()","877fa88e":"## binning of numerical variables\n\ndf['bmi_cat'] = pd.cut(df['bmi'], bins = [0, 19, 25,30,10000], labels = ['Underweight', 'Ideal', 'Overweight', 'Obesity'])\ndf['age_cat'] = pd.cut(df['age'], bins = [0,13,18, 45,60,200], labels = ['Children', 'Teens', 'Adults','Mid Adults','Elderly'])\ndf['glucose_cat'] = pd.cut(df['avg_glucose_level'], bins = [0,90,160,230,500], labels = ['Low', 'Normal', 'High', 'Very High'])","3d562a47":"sns.countplot(x='stroke', data=df)","365886e1":"!pip install pywaffle\nfrom pywaffle import Waffle\n\nstroke_gen = df[df['stroke'] == 1]['gender'].value_counts()\nhealthy_gen = df[df['stroke'] == 0]['gender'].value_counts()\n\nfemale = df['gender'].value_counts().values[0]\nmale =  df['gender'].value_counts().values[1]\n\nstroke_female = int(round (stroke_gen.values[0] \/ female * 100, 0))\nstroke_male = int(round( stroke_gen.values[1] \/ male *100, 0))\nhealthy_female = int(round(healthy_gen.values[0] \/ female * 100, 0))\nhealthy_male = int(round(healthy_gen.values[1] \/ male *100, 0))\n\n\nfemale_per = int(round(female\/(female+male) * 100, 0))\nmale_per = int(round(male\/(female+male)* 100, 0))\n\n\n\nfig = plt.figure(FigureClass = Waffle, \n                 constrained_layout = True,\n                 figsize = (7,7),\n                 facecolor = '#f6f5f5',dpi = 100,\n                 \n                 plots = {'121':\n                          {     \n                           'rows':7,\n                           'columns': 7,\n                           'values' : [healthy_male,stroke_male],\n                            'colors' : ['#512b58','#fe346e'],\n                              'vertical' : True,\n                              'interval_ratio_y': 0.1,\n                              'interval_ratio_x': 0.1,\n                              'icons' : 'male',\n                              'icon_legend': False,\n                               'icon_size':20,\n                              'plot_anchor':'C',\n                              'alpha':0.1\n                          },\n                          \n                          '122' :\n                          { \n                            'rows': 7,\n                            'columns':7,\n                            'values':[healthy_female,stroke_female],         \n                              'colors' : ['#512b58','#fe346e'],\n                              'vertical': True,\n                              'interval_ratio_y': 0.1,\n                              'interval_ratio_x': 0.1,\n                              'icons' : 'female',\n                              'icon_legend' :False,\n                              'icon_size':20,\n                              'plot_anchor':'C',\n                              'alpha':0.1\n                                                      \n                           }\n                         },\n                   \n)\n#fig.text ('asdfasdfasd0', {'font':'Serif', 'size':35, 'color':'black'} )\n\n\nfig.text(0., 0.8, 'Gender Risk for Stroke - effect of gender on strokes?', {'font':'Serif', 'size':20, 'color':'black', 'weight':'bold'})\nfig.text(0., 0.73, 'Risk of stroke in both male and female are same,\\nprove our initial assumption is wrong. ', {'font':'Serif', 'size':13, 'color':'black', 'weight':'normal'}, alpha = 0.7)\nfig.text(0.24, 0.22, 'ooo', {'font':'Serif', 'size':16,'weight':'bold' ,'color':'#f6f5f5'})\nfig.text(0.65, 0.22, 'ooo', {'font':'Serif', 'size':16,'weight':'bold', 'color':'#f6f5f5'})\nfig.text(0.23, 0.28, '{}%'.format(healthy_male), {'font':'Serif', 'size':20,'weight':'bold' ,'color':'#512b58'},alpha = 1,)\nfig.text(0.65, 0.28, '{}%'.format(healthy_female), {'font':'Serif', 'size':20,'weight':'bold', 'color':'#512b58'}, alpha = 1)\nfig.text(0.21, 0.67, 'Male ({}%)'.format(male_per), {'font':'Serif', 'size':14,'weight':'bold' ,'color':'black'},alpha = 0.5,)\nfig.text(0.61, 0.67, 'Female({}%)'.format(female_per), {'font':'Serif', 'size':14,'weight':'bold', 'color':'black'}, alpha = 0.5)\n#fig.text(0., 0.8, 'Assumption was proven wrong', {'font':'Serif', 'size':24, 'color':'black', 'weight':'bold'})\n\nfig.text(0.9,0.73, 'Stroke ', {'font': 'Serif','weight':'bold','Size': '16','weight':'bold','style':'normal', 'color':'#fe346e'})\nfig.text(1.02,0.73, '|', {'color':'black' , 'size':'16', 'weight': 'bold'})\nfig.text(1.035,0.73, 'No Stroke', {'font': 'Serif','weight':'bold', 'Size': '16','style':'normal', 'weight':'bold','color':'#512b58'},alpha = 1)\n\n\nfig.show()","0de42247":"cat_cols = [\"gender\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\",\"stroke\"]\ncont_cols = [\"age\",\"avg_glucose_level\",\"bmi\"]","eedfadd6":"cr = df[cont_cols].corr()\nplt.figure(figsize = (10,10))\nsns.heatmap(cr,cmap=\"viridis\", annot = True)\nplt.show()","68f32003":"bmi = list(df['bmi'].values)\nhist_data = [bmi]\ngroup_labels = [\"bmi\"]\ncolors = ['Red']\nfig = ff.create_distplot(hist_data,group_labels,show_hist = True,colors=colors)\nfig.show()\n","54f0db8b":"df['gender'].value_counts()\n","9cd434af":"df.drop(df[df['gender'] == 'Other'].index, inplace = True)\ndf['gender'].unique()","580df38f":"print(\"The shape before removing the BMI outliers : \",df.shape)\ndf.drop(df[df['bmi'] > 47].index, inplace = True)\nprint(\"The shape after removing the BMI outliers : \",df.shape)","ad6b4146":"bmi = list(df['bmi'].values)\nhist_data = [bmi]\ngroup_labels = [\"bmi\"]\ncolors = ['Red']\nfig = ff.create_distplot(hist_data,group_labels,show_hist = True,colors=colors)\nfig.show()","345d0d53":"# Label Encoding the categorical variables\n\nfrom sklearn.preprocessing import LabelEncoder\nobject_cols = [\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"]\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_encoder.fit(df[col])\n    df[col] = label_encoder.transform(df[col])","48b3f63b":"df.head()\n","04271ef6":"df.drop(['bmi_cat', 'age_cat', 'glucose_cat'], axis=1, inplace=True)","44b6bbce":"# Using SMOTE\nfrom imblearn.over_sampling import SMOTE\nsampler = SMOTE(random_state = 42)\nX = df.drop(['stroke'],axis=1)\ny = df[['stroke']]\nX,y= sampler.fit_resample(X,y['stroke'].values.ravel())\ny = pd.DataFrame({'stroke':y})\nsns.countplot(data = y, x = 'stroke', y= None)\nplt.show()","c088ca85":"df.head()\n","4d4b60c7":"# Joining back dataset\ndf = pd.concat([X,y],axis = 1)\ndf.head()","6ab2ae41":"df = df.sample(frac = 1)\n","db3112e2":"import torch\nimport torch.nn as nn","3bc2d9fe":"cat_cols = [\"gender\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"]\ncont_cols = [\"age\",\"avg_glucose_level\",\"bmi\"]\ny_col = [\"stroke\"]","ceb5ea00":"for cat in cat_cols:\n    df[cat] = df[cat].astype('category')","03c4996e":"df.info()\n","aced1fea":"# stacking the categorical columns\ncats = np.stack([df[col].cat.codes.values for col in cat_cols], 1)\ncats[:5]","6a82d54e":"# converting the stack into tensor\ncats = torch.tensor(cats, dtype = torch.int64)\ncats[:5]","9fa24a78":"# stacking the continuous columns & converting to tensor\nconts = np.stack([df[col].values for col in cont_cols], 1)\nconts = torch.tensor(conts, dtype=torch.float)\nconts[:5]","56d02fde":"# converting target variable to tensor and flattening since CrossEntropyLoss expects a 1-d tensor\ny = torch.tensor(df[y_col].values).flatten()\ny[:5]\n","f548ecf4":"print(cats.shape)\nprint(conts.shape)\nprint(y.shape)","249460aa":"cat_szs = [len(df[col].cat.categories) for col in cat_cols]\nemb_szs = [(size, min(50, (size+1)\/\/2)) for size in cat_szs]\nemb_szs","95ef9ffd":"class TabularModel(nn.Module):\n\n    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n        super().__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layerlist = []\n        n_emb = sum((nf for ni,nf in emb_szs))\n        n_in = n_emb + n_cont\n        \n        for i in layers:\n            layerlist.append(nn.Linear(n_in,i)) \n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            n_in = i\n        layerlist.append(nn.Linear(layers[-1],out_sz))\n        self.layers = nn.Sequential(*layerlist)\n        \n    def forward(self, x_cat, x_cont):\n        embeddings = []\n        for i,e in enumerate(self.embeds):\n            embeddings.append(e(x_cat[:,i]))\n        x = torch.cat(embeddings, 1)\n        x = self.emb_drop(x)\n        \n        x_cont = self.bn_cont(x_cont)\n        x = torch.cat([x, x_cont], 1)\n        x = self.layers(x)\n        return x","9f53654a":"torch.manual_seed(42)\nmodel = TabularModel(emb_szs, conts.shape[1], 2, [400,200,100], p=0.2)\nmodel","abb7f05a":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","dd71b772":"batch_size = 9000\ntest_size = 492\n\ncat_train = cats[:batch_size-test_size]\ncat_test = cats[batch_size-test_size:batch_size]\ncon_train = conts[:batch_size-test_size]\ncon_test = conts[batch_size-test_size:batch_size]\ny_train = y[:batch_size-test_size]\ny_test = y[batch_size-test_size:batch_size]","2ec6c693":"import time\nstart_time = time.time()\n\nepochs = 320\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    y_pred = model(cat_train, con_train)\n    loss = criterion(y_pred, y_train)\n    losses.append(loss)\n    \n    if i%25 == 1:\n        print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'epoch: {i:3}  loss: {loss.item():10.8f}') \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds') ","b4e67460":"plt.plot(range(epochs), losses)\nplt.ylabel('Cross Entropy Loss')\nplt.xlabel('epoch');","3d305064":"with torch.no_grad():\n    y_val = model(cat_test, con_test)\n    loss = criterion(y_val, y_test)\nprint(f'CE Loss: {loss:.8f}')","a7ad3693":"rows = 200\ncorrect = 0\ngroundTruth = []\npredictedValues = []\nprint(f'{\"MODEL OUTPUT\":26} ARGMAX  Y_TEST')\nfor i in range(rows):\n    print(f'{str(y_val[i]):26} {y_val[i].argmax():^7}{y_test[i]:^7}')\n    predictedValues.append(y_val[i].argmax().item())\n    groundTruth.append(y_test[i])\n    if y_val[i].argmax().item() == y_test[i]:\n        correct += 1\nprint(f'\\n{correct} out of {rows} = {100*correct\/rows:.2f}% correct')","e7812c96":"from sklearn.metrics import f1_score\nprint(\"The F1-score is :\", f1_score(groundTruth, predictedValues))\nprint(\"\\n\")\nprint(confusion_matrix(groundTruth, predictedValues))\nprint(\"\\n\")\nprint(classification_report(groundTruth, predictedValues))","731b1148":"Now we can see that our data has been completely transformed into numerical dataset.\n\n","b58b5d69":"**Observation:**\n\navg_glucose_level has large number of outliers present in the dataset.","c3ee0351":"**So Let's get Started**","c8d911f3":"**Import necessary packages**","b49e8b8c":"**Observation:**\n\n* The number of people have heart stroke is actually negligible as compared to the ones not having it.\n* The data is higly unbalanced.\n* So, while modeling and training data, either over sampling or under sampling has to be done to obtain best results.","18041f40":"# **Stroke Prediction Dataset**","8f7730f3":"**Observation:** Since there is only one 'Other' category in gender, we should remove it from our data.","0046a6f9":"**Observation:** We can thus validate our previous assumption that there are null values in bmi column. Let's try filling out those null values.\n\n","5a39834c":"**Observation:** It's quite clear that the data has been completely balanced.\n\n","f6065baa":"**Observation:** The outliers of BMI index have been removed. Let us plot the distribution to see if it is still skewed.","80cad335":"Observation: Only bmi column has certain missing values.\n\n","c661ec10":"**Observation:** We cannot see any noteworthy correlation between the given features!","8116fbaf":"**Observation: There are 5110 rows and 12 columns in the dataset.**","7fb1137b":"Since STROKE is highly imbalanced, there are two ways to deal with it. We can either undersample the majority class or we could oversample the minority class.\n\nWe will be using oversampling technique for this project.\n\nThe simplest approach involves duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short.","6569fbf5":"This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n**Dataset Information:**\n\nhealthcare-data-stroke-data.csv: The csv contains data related to patients who may have heart disease and various attributes which determine that :\n\n* id: unique identifier\n* gender: \"Male\", \"Female\" or \"Other\"\n* age: age of the patient\n* hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n* heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n* ever_married: \"No\" or \"Yes\"\n* work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n* Residence_type: \"Rural\" or \"Urban\"\n* avg_glucose_level: average glucose level in blood\n* bmi: body mass index\n* smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n* stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\n**Objective**\n\n* Visualize the relationships between various Healthy and Unhealthy habits to Heart Strokes, and there by predict the stroke probability with best model and hypertuned parameters.\n\n**Assumptions**\n\n* Smoking can induce Stroke, is it true?\n* Heart with a Heart Disease is prone to Stroke, is it true?\n* Workload results in high blood pressure and that could lead to Stroke, is it true?\n* Males are most susceptible to strokes due to high work related stress, is it true?\n \n**Questions to be answered**\n\n* Does age has impact on strokes? and How is this parameter distributed?\n* Does body mass index and glucose levels in a person, propel a heart stroke?\n* Is there a difference in the rate of heart stroke for smokers and non smokers?\n* Does the type of job, whether stressdul or not, contribute to heart stroke?\n","1bf66dbc":"**Observation:**\n\n* It is interesting to note that although the number of males and females are different in the dataset, but, both of them are at equal risk to heart stroke.\n* Hence proving that our assumption that males are more susciptible to stroke due to work load, as wrong.\n* Please note that the above figure has been taken from Bhuvan's notebook. https:\/\/www.kaggle.com\/bhuvanchennoju\/data-stroytelling-auc-focus-on-strokes","0c24a104":"The prediction part of this notebook has been referred from Naman Manchanda's notebook https:\/\/www.kaggle.com\/namanmanchanda\/stroke-eda-and-ann-prediction\n\nAnd as a main helper to this kaggle notebook , a special Thanks to Aditi Mulye!\nThank you for making such a wonderful notebook! Really helped me learn a lot!","9d054c38":"**Observation:**\n\n* There are a lot of outliers in bmi\n* The outliers make the distribution curve highly skewed towards right\n* Either the outliers can be removed or the distribution curve can be made less-skewed by mapping the values with a log but both cases will lead to loss of the number of datapoints with Stroke = 1","40259c7a":"**Observation:**\n\n* There are 4 different types of people on the basis of smoking category.\n* People can be categorised into 5 types on the basis of type of work.\n* There are only 2 residences - 'Urban' and 'Rural'\n* There are 3 different gender entries.","fbef70c1":"**Observation:**\n\n* id, hypertension, heart_disease and stroke are of int datatype.\n* gender, ever_married, work_type, Residence_type and smoking_status are of object datatype.\n* age, avg_glucose_level and bmi are of float datatype.","22c945f1":"The above section of the code has been taken from Bhuvan Chennoju's notebook https:\/\/www.kaggle.com\/bhuvanchennoju\/data-stroytelling-auc-focus-on-strokes"}}