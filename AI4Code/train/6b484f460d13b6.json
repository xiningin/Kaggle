{"cell_type":{"dcd70888":"code","ec63aae6":"code","ab7893a4":"code","c53d739f":"code","5d6a5b1e":"code","75071d41":"code","5eef5b2f":"code","21833726":"code","0b91cfde":"code","3014bf00":"code","f9d6052b":"code","b273c53d":"code","2b3ebcfc":"code","0a2e7549":"code","15eb3eb7":"code","f0466375":"code","0f49a394":"markdown","945dcd49":"markdown","8f88502d":"markdown","4f633400":"markdown","3418a0f4":"markdown","cc9b7924":"markdown","89c65e8a":"markdown"},"source":{"dcd70888":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\nclass FillUp():\n    def __init__(self):\n        pass\n    \n    # Fill the missing value by mode.\n    # Input the feature matrix and the name of column with missing value\n    def mode(self,df,name):       \n        df[name] = df[name].fillna(df[name].mode()[0])\n        return df\n    \n    # Fill the missing value by median.\n    def median(self,df,name):\n        df[name] = df[name].fillna(df[name].median())\n        return df\n    \n    # Fill the missing value by mean.\n    def mean(self,df,name):\n        df[name] = df[name].fillna(df[name].mean())\n        return df\n    \n    # Fill the missing value by RandomForestRegressor.\n    def RFR(self,df,name):\n        \n        # Get the name of columns without missing value\n        cols = list(df.dropna(axis=1).columns)\n        \n        # Transform 'int64' into 'float64'\n        for col in cols:\n            if df[col].dtype == 'int64':\n                df[col] = df[col].astype('float64')\n                \n        # We can't use 'Object' value in RandomForestRegressor\n        for col in cols:\n            if df[col].dtype == 'O':\n                cols.remove(col)\n                \n        # Insert the name of column which will be filled.\n        cols.insert(0,name)\n        \n        # Define a new feature matrix\n        df_ = df[cols]\n        \n        # To separate the known data and unknown data.\n        known = df_[df_[name].notnull()].values\n        unknown = df_[df_[name].isnull()].values\n        \n        # Get training data\n        y = known[:,0]\n        x = known[:,1:]\n        \n        # Modeling and training\n        rfr = RandomForestRegressor(random_state=0, n_estimators=200, n_jobs=-1)\n        rfr.fit(x, y)\n        \n        # Make a prediction\n        predicted = rfr.predict(unknown[:,1:])\n        \n        # Fill the predicted data back to the original feature matrix\n        df.loc[(df[name].isnull()),name] = predicted\n        return df\n    \n    # Fill the missing value by XgBoostingRegressor.\n    # The follows are the same as above but the model\n    def XGBR(self,df,name):\n        \n        cols = list(df.dropna(axis=1).columns)\n        \n        for col in cols:\n            if df[col].dtype == 'int64':\n                df[col] = df[col].astype('float64')\n        for col in cols:\n            if df[col].dtype == 'O':\n                cols.remove(col)\n                \n        cols.insert(0,name)\n        df_ = df[cols]\n        \n        known = df_[df_[name].notnull()].values\n        unknown = df_[df_[name].isnull()].values\n        y = known[:,0]\n        x = known[:,1:]\n        \n        xgbr = xgb.XGBRegressor()\n        xgbr.fit(x, y)\n        \n        predicted = xgbr.predict(unknown[:,1:])\n        df.loc[(df[name].isnull()),name] = predicted\n        return df","ec63aae6":"import warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","ab7893a4":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","c53d739f":"# Save ID\ntrain_Id = train.loc[:,'PassengerId']\ntest_Id = test.loc[:,'PassengerId']\n\n# Delete useless information\ntrain.drop('PassengerId',axis=1,inplace=True)\ntest.drop('PassengerId',axis=1,inplace=True)\ntrain.drop('Name',axis=1,inplace=True)\ntest.drop('Name',axis=1,inplace=True)\ntrain.drop('Cabin',axis=1,inplace=True)\ntest.drop('Cabin',axis=1,inplace=True)","5d6a5b1e":"#concat the train data and test data\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.Survived\ndf = pd.concat((train,test)).reset_index(drop=True)\ndf.drop(['Survived'],axis=1,inplace=True)","75071d41":"df.head()","5eef5b2f":"df.info()","21833726":"# Percentage of missing values\ndf.isnull().mean()","0b91cfde":"Fup = FillUp()","3014bf00":"# Fill 'Embarked' with mode\nFup.mode(df,'Embarked')","f9d6052b":"df.info()","b273c53d":"# Fill 'Fare' with mean\nFup.mean(df,'Fare')","2b3ebcfc":"df.info()","0a2e7549":"# Fill 'Age' with RandomForestRegessor\nFup.RFR(df,'Age')","15eb3eb7":"df.info()","f0466375":"# Percentage of missing values\ndf.isnull().mean()","0f49a394":"**Help You Do The Simple Data Processing**","945dcd49":"Next, I will show you how to use the above code through an example.\n\nThe data is 'Titanic: Machine Learning from Disaster'.","8f88502d":"* **Concluding**\n\nThanks for reading this kernel.\n\nI hope it will be helpful for you.\n\nThere may be some mistakes in the above codes.\n\nLooking forward to your valuable suggestions\uff01","4f633400":"* **Introduction**\n\nData Processing is an important step in ML. \n\nBut for the freshers, this step is so unfriendly that too many freshers will withdraw from the competition.\n\nIn order to help the freshers to do the data processing. I write this kernel.","3418a0f4":"**The codes are as follows:**","cc9b7924":"*PS : I'm a fresher in Kaggle as well, and this is my first kernel. I hope you will like it.*","89c65e8a":"* **Main Body**\n\nThis is a class named FillUp. I packaged several functions and set them into one class.\n\nAs a fresher ,you can use this class to do the data processing easily.\n\nFinally, I hope you can finish data processing independently.\n\nImproving ourselves is the reason why we take part in Kaggle."}}