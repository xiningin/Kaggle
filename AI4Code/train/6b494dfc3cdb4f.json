{"cell_type":{"f7830d75":"code","693b27c8":"code","cb2547df":"code","0182a46c":"code","f7bafe5a":"code","893cd5b1":"code","104d064b":"code","1cf2fa64":"code","cac87d1d":"code","23d42626":"code","63049b98":"code","93e47926":"code","3257bbe3":"code","ede4df2b":"code","a38ddbce":"code","bde9b7d3":"code","0f3af377":"code","4f135263":"code","9a4251c1":"code","59cf39b2":"code","4234127f":"code","2143eb64":"code","638f1650":"code","13292454":"code","ab0cfbb1":"code","dfbeb918":"code","8ae6c296":"code","baea34c6":"code","bab19fec":"code","74662c3b":"code","9b52f201":"code","9025039d":"code","d68d394a":"code","7b32331f":"code","493136fe":"code","c42d6187":"code","cbc9cda4":"code","1f3c01f9":"code","b613236f":"code","022f1609":"markdown","a18c7fe3":"markdown","7974f548":"markdown","326305b5":"markdown","4e86eaef":"markdown","4f42a7fc":"markdown","c75c36ad":"markdown","7ddd2494":"markdown"},"source":{"f7830d75":"from keras.models import Sequential\nfrom keras.layers import Dense \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve\nfrom keras.utils import to_categorical\nimport random\nfrom datetime import datetime\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report","693b27c8":"def decode(datum):\n    return np.argmax(datum)","cb2547df":"dic={'Male':1,'Female':0,'Yes':1,'No':0,'Positive':1,'Negative':0}","0182a46c":"# We are reading our data\ndf = pd.read_csv(\"..\/input\/early-stage-diabetes-risk-prediction-datasets\/diabetes_data_upload.csv\")\nprint(df.head(),df.shape)\n\ndf.boxplot()","f7bafe5a":"# First 5 rows of our data\ndf.head()","893cd5b1":"all_col=df.columns\n\nfor i in range(df.shape[1]):\n    df.replace\nprint(all_col[-1],len(all_col))\nfor col in range(1,len(all_col)):\n    df=df.replace({all_col[col]: dic})\nprint(df.head())","104d064b":"df['class'].value_counts()","1cf2fa64":"sns.countplot(x=\"class\", data=df, palette=\"bwr\")\nplt.show()","cac87d1d":"countNo = len(df[df['class'] == 0])\ncountYes = len(df[df['class'] == 1])\nprint(\"Percentage of NO: {:.2f}%\".format((countNo \/ (len(df['class']))*100)))\nprint(\"Percentage of Yes: {:.2f}%\".format((countYes \/ (len(df['class'])*100))))","23d42626":"y = df['class'].values\nx_data = df.drop(['class'], axis = 1)","63049b98":"# Normalize\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values","93e47926":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)","3257bbe3":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","ede4df2b":"#initialize\ndef initialize(dimension):\n    \n    weight = np.full((dimension,1),0.01)\n    bias = 0.0\n    return weight,bias","a38ddbce":"def sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head","bde9b7d3":"def forwardBackward(weight,bias,x_train,y_train):\n    # Forward\n    \n    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) \/ x_train.shape[1]\n    \n    # Backward\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n    \n    return cost,gradients","0f3af377":"def update(weight,bias,x_train,y_train,learningRate,iteration) :\n    costList = []\n    index = []\n    \n    #for each iteration, update weight and bias values\n    for i in range(iteration):\n        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n        \n        costList.append(cost)\n        index.append(i)\n\n    parameters = {\"weight\": weight,\"bias\": bias}\n    \n    print(\"iteration:\",iteration)\n    print(\"cost:\",cost)\n\n    plt.plot(index,costList)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","4f135263":"def predict(weight,bias,x_test):\n    z = np.dot(weight.T,x_test) + bias\n    y_head = sigmoid(z)\n\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","9a4251c1":"def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):\n    dimension = x_train.shape[0]\n    weight,bias = initialize(dimension)\n    \n    parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)\n\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"Manuel Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))","59cf39b2":"start_time = datetime.now()\nlogistic_regression(x_train,y_train,x_test,y_test,1,100)\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","4234127f":"accuracies = {}\n\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nacc = lr.score(x_test.T,y_test.T)*100\n\naccuracies['Logistic Regression'] = acc\nprint(\"Test Accuracy {:.2f}%\".format(acc))","2143eb64":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))","638f1650":"start_time = datetime.now()\n# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","13292454":"from sklearn.svm import SVC","ab0cfbb1":"start_time = datetime.now()\nsvm = SVC(random_state = 1,kernel='linear')\nsvm.fit(x_train.T, y_train.T)\n\nacc = svm.score(x_test.T,y_test.T)*100\naccuracies['SVM'] = acc\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","dfbeb918":"start_time = datetime.now()\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\n\nacc = nb.score(x_test.T,y_test.T)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","8ae6c296":"start_time = datetime.now()\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train.T, y_train.T)\n\nacc = dtc.score(x_test.T, y_test.T)*100\naccuracies['Decision Tree'] = acc\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","baea34c6":"start_time = datetime.now()\n# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 2, random_state = 1)\nrf.fit(x_train.T, y_train.T)\n\nacc = rf.score(x_test.T,y_test.T)*100\naccuracies['Random Forest'] = acc\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","bab19fec":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","74662c3b":"# Predicted values\ny_head_lr = lr.predict(x_test.T)\nknn3 = KNeighborsClassifier(n_neighbors = 3)\nknn3.fit(x_train.T, y_train.T)\ny_head_knn = knn3.predict(x_test.T)\ny_head_svm = svm.predict(x_test.T)\ny_head_nb = nb.predict(x_test.T)\ny_head_dtc = dtc.predict(x_test.T)\ny_head_rf = rf.predict(x_test.T)","9b52f201":"from sklearn.metrics import f1_score\nf1_lr = f1_score(y_test,y_head_lr)\nf1_knn = f1_score(y_test,y_head_knn)\nf1_svm = f1_score(y_test,y_head_svm)\nf1_nb = f1_score(y_test,y_head_nb)\nf1_dtc = f1_score(y_test,y_head_dtc)\nf1_rf = f1_score(y_test,y_head_rf)\n\nprint('Logistic Regression F1 score:', f1_lr)\nprint('K Nearest Neighbors F1 score:', f1_knn)\nprint('Support Vector Machine F1 score:', f1_svm)\nprint('Naive Bayes F1 score:', f1_nb)\nprint('Decision Tree s F1 score:', f1_dtc)\nprint('Random Forest F1 score:', f1_rf)","9025039d":"from sklearn.metrics import recall_score\nrc_lr = recall_score(y_test,y_head_lr)\nrc_knn = recall_score(y_test,y_head_knn)\nrc_svm = recall_score(y_test,y_head_svm)\nrc_nb = recall_score(y_test,y_head_nb)\nrc_dtc = recall_score(y_test,y_head_dtc)\nrc_rf = recall_score(y_test,y_head_rf)\n\nprint('Logistic Regression recall:', rc_lr)\nprint('K Nearest Neighbors recall:', rc_knn)\nprint('Support Vector Machine recall:', rc_svm)\nprint('Naive Bayes recall:', rc_nb)\nprint('Decision Tree recall:', rc_dtc)\nprint('Random Forest recall:', rc_rf)","d68d394a":"from sklearn.metrics import precision_score\nprecision_lr = precision_score(y_test,y_head_lr)\nprecision_knn = precision_score(y_test,y_head_knn)\nprecision_svm = precision_score(y_test,y_head_svm)\nprecision_nb = precision_score(y_test,y_head_nb)\nprecision_dtc = precision_score(y_test,y_head_dtc)\nprecision_rf = precision_score(y_test,y_head_rf)\n\nprint('Logistic Regression precision:', precision_lr)\nprint('K Nearest Neighbors precision:', precision_knn)\nprint('Support Vector Machine precision:', precision_svm)\nprint('Naive Bayes precision:', precision_nb)\nprint('Decision Tree precision:', precision_dtc)\nprint('Random Forest precision:', precision_rf)","7b32331f":"from sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_test,y_head_lr)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_svm = confusion_matrix(y_test,y_head_svm)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_rf = confusion_matrix(y_test,y_head_rf)\n","493136fe":"def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n    plt.legend","c42d6187":"from sklearn.metrics import roc_curve, auc\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_head_lr)\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_head_knn)\nfpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_head_svm)\nfpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, y_head_nb)\nfpr_dtc, tpr_dtc, thresholds_dtc = roc_curve(y_test, y_head_dtc)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_head_rf)\n\nroc_auc_lr = auc(fpr_lr, tpr_lr)\nroc_auc_knn = auc(fpr_knn, tpr_knn)\nroc_auc_svm = auc(fpr_svm, tpr_svm)\nroc_auc_nb = auc(fpr_nb, tpr_nb)\nroc_auc_dtc = auc(fpr_dtc, tpr_dtc)\nroc_auc_rf = auc(fpr_rf, tpr_rf)","cbc9cda4":"plt.figure(figsize=(14, 7))\n\n#plt.suptitle(\"ROC\",fontsize=24)\nplt.subplots_adjust(wspace = 0.5, hspace= 0.5)\n\n\nplt.subplot(2,3,1)\nplt.title(\"LR\", fontsize=18)\nplot_roc_curve(fpr_lr, tpr_lr)\nplt.plot(fpr_lr, tpr_lr, label='AUC = %0.4f'% roc_auc_lr)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,2)\nplt.title(\"KNN\", fontsize=18)\nplot_roc_curve(fpr_knn, tpr_knn)\nplt.plot(fpr_knn, tpr_knn, label='AUC = %0.4f'% roc_auc_knn)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,3)\nplt.title(\"SVM\", fontsize=18)\nplot_roc_curve(fpr_svm, tpr_svm)\nplt.plot(fpr_svm, tpr_svm, label='AUC = %0.4f'% roc_auc_svm)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,4)\nplt.title(\"NBC\", fontsize=18)\nplot_roc_curve(fpr_nb, tpr_nb)\nplt.plot(fpr_nb, tpr_nb, label='AUC = %0.4f'% roc_auc_nb)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,5)\nplt.title(\"DT\", fontsize=18)\nplot_roc_curve(fpr_dtc, tpr_dtc)\nplt.plot(fpr_dtc, tpr_dtc, label='AUC = %0.4f'% roc_auc_dtc)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,6)\nplt.title(\"RF\", fontsize=18)\nplot_roc_curve(fpr_rf, tpr_rf)\nplt.plot(fpr_rf, tpr_rf, label='AUC = %0.4f'% roc_auc_rf)\nplt.legend(loc='lower right')","1f3c01f9":"plt.figure(figsize=(24,12))\n\nsns.set(font_scale=2)\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression \",fontsize=26)\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 36})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors \",fontsize=26)\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 36})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine \",fontsize=26)\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 36})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Classifier \",fontsize=26)\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 36})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree \",fontsize=26)\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 36})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest \",fontsize=26)\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 36})\n\nplt.show()","b613236f":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\ntest_size=0.2)\n\n\nmodel=Sequential()\n\nmodel.add(Dense(30,activation='relu',input_dim=X_train.shape[1]))\nmodel.add(Dense(40,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='sgd',loss='binary_crossentropy',metrics=['accuracy'])\n\nhistory=model.fit(X_train,y_train,epochs=100, validation_data=(X_val, y_val))\n\nscores_train=model.evaluate(X_train,y_train)\nprint('Train Acc:',scores_train[1])\nscore_test=model.evaluate(X_test,y_test)\nprint('Test Acc:',score_test[1])\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","022f1609":"K-Nearest Neighbour (KNN) Classification","a18c7fe3":"Support Vector Machine (SVM) Algorithm","7974f548":"Random Forest Classification","326305b5":"Decision Tree Algorithm","4e86eaef":"## Confusion Matrix, F1 Score","4f42a7fc":"Naive Bayes Algorithm","c75c36ad":"Comparing Models","7ddd2494":"**Sklearn Logistic Regression**"}}