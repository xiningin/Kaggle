{"cell_type":{"8a5027e2":"code","92b3b822":"code","d4f22218":"code","a13e995f":"code","3b51362b":"code","689b7986":"code","4564bfa4":"code","12c7bd43":"code","15f50bda":"code","46263b4c":"code","df4b8021":"code","c6d4278d":"code","dc4a1c16":"markdown","c6d72419":"markdown","02e4a59e":"markdown","7071d58a":"markdown","52c06d35":"markdown","50000151":"markdown","ef89f743":"markdown","b917021a":"markdown"},"source":{"8a5027e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport lightgbm as lgb","92b3b822":"ins = pd.read_csv('..\/input\/installments_payments.csv')\nins.head()","d4f22218":"ins['tw1']=np.exp(ins['DAYS_ENTRY_PAYMENT']*0.01)\nins['tw2']=np.exp(ins['DAYS_ENTRY_PAYMENT']*0.05)\nins['tw3']=np.exp(ins['DAYS_ENTRY_PAYMENT']*0.25)","a13e995f":"ts_view=ins[['DAYS_ENTRY_PAYMENT', 'tw1', 'tw2', 'tw3']][ins['DAYS_ENTRY_PAYMENT']>-100]\nts_view.drop_duplicates('DAYS_ENTRY_PAYMENT', inplace=True)\nsns.regplot('DAYS_ENTRY_PAYMENT', 'tw1', data=ts_view, fit_reg=False)\nsns.regplot('DAYS_ENTRY_PAYMENT', 'tw2', data=ts_view, fit_reg=False)\nsns.regplot('DAYS_ENTRY_PAYMENT', 'tw3', data=ts_view, fit_reg=False)","3b51362b":"#Are the payments late\/early?\n\nins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\nins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\nins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\nins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n\n#Now let's weight these new values\n\nins['DPD_tw1']=ins['DPD']*np.exp(ins['DAYS_ENTRY_PAYMENT']*ins['tw1'])\nins['DPD_tw2']=ins['DPD']*np.exp(ins['DAYS_ENTRY_PAYMENT']*ins['tw2'])\nins['DPD_tw3']=ins['DPD']*np.exp(ins['DAYS_ENTRY_PAYMENT']*ins['tw3'])\n\nins['DBD_tw1']=ins['DBD']*np.exp(ins['DAYS_ENTRY_PAYMENT']*ins['tw1'])\nins['DBD_tw2']=ins['DBD']*np.exp(ins['DAYS_ENTRY_PAYMENT']*ins['tw2'])\nins['DBD_tw3']=ins['DBD']*np.exp(ins['DAYS_ENTRY_PAYMENT']*ins['tw3'])\n\n#Let's weight the value of payments made\n\nins['AMT_PAYMENT_tw1']=ins['AMT_PAYMENT']*ins['tw1']\nins['AMT_PAYMENT_tw2']=ins['AMT_PAYMENT']*ins['tw2']\nins['AMT_PAYMENT_tw3']=ins['AMT_PAYMENT']*ins['tw3']\n\n\n\n\n","689b7986":"# Features: Perform aggregations\n# The natural thing to want to do here is to sum the time-weights and weighted values so that we can get weighted-averages\n# You might want to experiment with this a bit more - perhaps other aggregations will prove useful?\naggregations = {\n    'NUM_INSTALMENT_VERSION': ['nunique'],\n    'DPD': ['max', 'mean', 'sum'],\n    'DBD': ['max', 'mean', 'sum'],\n    'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n    'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum'],\n    'DPD_tw1':['sum'],\n    'DPD_tw2':['sum'],\n    'DPD_tw3':['sum'],\n    'DBD_tw1':['sum'],\n    'DBD_tw2':['sum'],\n    'DBD_tw3':['sum'],\n    'tw1':['sum'],\n    'tw2':['sum'],\n    'tw3':['sum'],\n\n\n    'AMT_PAYMENT_tw1':['sum'],\n    \n    'AMT_PAYMENT_tw2':['sum'],\n    'AMT_PAYMENT_tw3':['sum']\n    \n}\n\n    \nins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\nins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n\n","4564bfa4":"\nins_agg['INSTAL_weighted_tw1_DPD_SUM']=ins_agg['INSTAL_DPD_tw1_SUM']\/ins_agg['INSTAL_tw1_SUM']\nins_agg['INSTAL_weighted_tw2_DPD_SUM']=ins_agg['INSTAL_DPD_tw2_SUM']\/ins_agg['INSTAL_tw2_SUM']\nins_agg['INSTAL_weighted_tw3_DPD_SUM']=ins_agg['INSTAL_DPD_tw3_SUM']\/ins_agg['INSTAL_tw3_SUM']\n\nins_agg['INSTAL_weighted_tw1_DBD_SUM']=ins_agg['INSTAL_DBD_tw1_SUM']\/ins_agg['INSTAL_tw1_SUM']\nins_agg['INSTAL_weighted_tw2_DBD_SUM']=ins_agg['INSTAL_DBD_tw2_SUM']\/ins_agg['INSTAL_tw2_SUM']\nins_agg['INSTAL_weighted_tw3_DBD_SUM']=ins_agg['INSTAL_DBD_tw3_SUM']\/ins_agg['INSTAL_tw3_SUM']\n\nins_agg['INSTAL_weighted_tw1_AMTPAY_SUM']=ins_agg['INSTAL_AMT_PAYMENT_tw1_SUM']\/ins_agg['INSTAL_tw1_SUM']\nins_agg['INSTAL_weighted_tw2_AMTPAY_SUM']=ins_agg['INSTAL_AMT_PAYMENT_tw2_SUM']\/ins_agg['INSTAL_tw2_SUM']\nins_agg['INSTAL_weighted_tw3_AMTPAY_SUM']=ins_agg['INSTAL_AMT_PAYMENT_tw3_SUM']\/ins_agg['INSTAL_tw3_SUM']","12c7bd43":"ins_agg.head()","15f50bda":"targets=pd.read_csv('..\/input\/application_train.csv')[['SK_ID_CURR', 'TARGET']]\n\nins_agg=ins_agg.join(targets.set_index('SK_ID_CURR'))\n\ntrain=ins_agg[pd.notnull(ins_agg['TARGET'])]\ntrain=train.fillna(0)\ntarget = np.array(train['TARGET'])\ntrain=train.drop(['TARGET'],axis=1)","46263b4c":"estimator=lgb.LGBMClassifier()\n","df4b8021":"estimator.fit(train, target)","c6d4278d":"import shap\n\nshap_values = shap.TreeExplainer(estimator).shap_values(train[0:2000])\nshap.summary_plot(shap_values, train[0:2000], max_display=800)","dc4a1c16":"I wanted to share a simple and effective way of dealing with the time-series element of this competition. I've found that exponential weighting of values works quite nicely. It is also computationally inexpensive and quite easy to interpret.\n\nI'll show a simple example from the installments_payments data, and leave it to others to refine and apply more widely in their own solutions. Note that I'm showing this for weighting events that happened prior to the application (i.e. negative time values) but you might also want to apply this to features with positive time values (remaining payments on loans for example).","c6d72419":"Let's see what these weights look like","02e4a59e":"Great, some of our time-weighted features look to be pretty useful. There's plenty of scope to improve this method, and to apply it to other data. Good luck!","7071d58a":"Now we do the aggregations of the weighted features, and the weights themselves","52c06d35":"Divide the aggregated weighted features by the sum of the weights for some time-decay-weighted features","50000151":"How do we know if the features are any good? Let's try to predict the target values based only on the data in the frame.\n\nNote - this is only a quick-and-dirty lightgbm run with no tuning, regularisation etc. as I only want a rough idea of feature importance.","ef89f743":"First we'll make some time weights for our features. Intuitively, we want more-recent observations to be wighted more strongly, but we don't know how quickly we want the weights to tail off for older observations. We'll try a few different exponential weights. The higher the multiplier in the exponential the quicker the results will taper off. Note that all of the time values in the data are negative.","b917021a":"Great, this looks like a reasonable selection of weightings. Note that more-recent data are closer to 0 (the right-hand side of this plot). Let's create some features and apply some weighting.\n\nI've borrowed heavily from the following kernel for this part of the code:\n\nhttps:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features"}}