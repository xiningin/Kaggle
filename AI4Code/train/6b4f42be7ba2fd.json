{"cell_type":{"dcb9d13e":"code","34110a0b":"code","9bad504c":"code","78884888":"code","b0f76a97":"code","df1e5d91":"code","e7a7b215":"code","1f6b8473":"code","fa08f9f2":"code","c0e7ed7e":"code","6e4273d8":"code","05869f51":"code","811da21d":"code","e88b0525":"code","b52fe7c2":"code","490987fe":"code","c9e7c13a":"code","1ef2fec4":"code","b2e1726d":"code","b58014e8":"code","dd2ac803":"code","5cf68f28":"code","fe0f37dd":"code","45d31758":"code","2b091e13":"code","7242a98c":"code","246bead3":"code","b2d3d531":"code","70f963b1":"code","e341476b":"code","9fa106cc":"code","4e420218":"code","cf55272e":"code","bfa7ace2":"code","9e2ce97b":"code","7688805f":"code","abc1daf5":"code","4e384d6f":"code","796ae087":"code","c0971004":"code","1f019252":"code","7cd990be":"code","b5d3adf4":"code","1451bcdf":"code","d2f70aa7":"code","6f903d74":"code","48c4d52c":"code","9fc7fe37":"code","2eb4cd5b":"code","639eff36":"code","183271fb":"code","e61c12a7":"code","8ededf94":"code","10726d05":"code","7e69009c":"code","60630c2f":"code","909e959a":"code","99e554f7":"code","8446de4d":"code","39876e30":"code","6043b79a":"code","cef82cb2":"code","e74d5d78":"code","14de3038":"code","695f0655":"code","71090fa9":"code","8a5cc6b7":"code","7049d948":"markdown","62b4308d":"markdown","7ee9ccfc":"markdown","a69e2b92":"markdown","597f3524":"markdown","b8382ef0":"markdown","efa5deaa":"markdown","72f29ae9":"markdown","eab5216d":"markdown","1fed8bc0":"markdown","4ed4cd6e":"markdown","21e54ba0":"markdown","7b010b82":"markdown","d27c9e41":"markdown","875d7d69":"markdown","530eb264":"markdown","6fbb21e2":"markdown","06551cf7":"markdown","87967721":"markdown","1cc267e9":"markdown","17954c28":"markdown","a512ea50":"markdown","9e484bc2":"markdown","22ca3a98":"markdown","8ee0a7ab":"markdown","1b659882":"markdown","857386cb":"markdown","7ba40fde":"markdown"},"source":{"dcb9d13e":"# Import Packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport statsmodels.api as sm\nimport scipy.stats as stats\nimport optuna\n\nfrom sklearn import metrics, ensemble, model_selection, tree\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer, MissingIndicator\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import export_graphviz\nfrom xgboost import XGBClassifier\n\n# Config\n%config InlineBackend.figure_format = 'retina'\n#plt.style.use('default')","34110a0b":"# Read data\npath = Path('\/kaggle\/input\/song-popularity-prediction\/')\ntrain = pd.read_csv(path\/'train.csv'); \ntest = pd.read_csv(path\/'test.csv')\nss = pd.read_csv(path\/'sample_submission.csv')\n# Read folds \ndf = pd.read_csv(\"..\/input\/song-popularity-folds\/train_folds (1).csv\")\ndf_test = pd.read_csv(path\/'test.csv')","9bad504c":"# Take a peek into the data\ntrain.head()","78884888":"# Check missong values & data types.\ntrain.info();","b0f76a97":"# High level overview of the data.\ntrain.describe().T","df1e5d91":"# Missing values in train & test sets.\npd.concat(\n    [\n        train.isnull().sum().to_frame(name='pct_na_train').query('pct_na_train>0')\/len(train) * 100,\n        test.isnull().sum().to_frame(name='pct_na_test').query('pct_na_test>0')\/len(train) * 100\n    ],\n    axis = 1\n)","e7a7b215":"corr = train.corr()\nfig, ax = plt.subplots(figsize=(10,10))   \nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","1f6b8473":"train.loc[:, ['song_duration_ms']].div(1000 * 60).describe().T","fa08f9f2":"# Histogram using matlab style state based interface.\nplt.hist(x='song_duration_ms', bins=30, density=False, cumulative=False, orientation='vertical', log=False, label='Song duration', alpha=1, data=train);\nplt.xlim(0,500_000);\nplt.title('Distribution of Song Duration');\nplt.xlabel('Song Duration');\nplt.ylabel('Frequency');","c0e7ed7e":"# Histogram using pandas plotting.\ntrain.hist(column='song_duration_ms', bins=30, alpha=.4);","6e4273d8":"# Histogram using the seaborn library.\nsns.histplot(data=train, x='song_duration_ms', bins=30, palette='pastel', ax=None);","05869f51":"# Different ways to draw a density plot.\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\ntrain.song_duration_ms.plot.density(ax=ax1);\ntrain.song_duration_ms.plot.kde(ax=ax2);\nsns.kdeplot(train.song_duration_ms, bw_adjust=1, fill=True, ax=ax3);","811da21d":"# CDF\nsns.kdeplot(train.song_duration_ms, cumulative=True);","e88b0525":"# Which distribution matches our data ?\nsm.qqplot(train.dropna().song_duration_ms, line='45', fit=True,dist=stats.norm);","b52fe7c2":"# Overlapping histograms based on another column.\nplt.figure(figsize=(6,4))\nplt.hist(train.query('song_popularity == 0').loc[:, 'song_duration_ms'] , bins=100, density=True, alpha=0.5, label=\"data1\");\nplt.hist(train.query('song_popularity == 1').loc[:, 'song_duration_ms'] , bins=100, density=True, alpha=0.5, label=\"data1\");","490987fe":"fig, ax = plt.subplots(figsize=(6,4))\nsns.kdeplot(data = train, x = 'song_duration_ms', hue = 'song_popularity', common_norm=False, fill=True, label = 'Song Popularity', ax=ax);","c9e7c13a":"train.hist(by='song_popularity', column='song_duration_ms', figsize=(12,4), bins=30, alpha=.5);","1ef2fec4":"train.groupby('song_popularity').song_duration_ms.describe()","b2e1726d":"train.boxplot( column='song_duration_ms', by='song_popularity', figsize=(10,8));","b58014e8":"train.hist(column='acousticness' ,bins=30, alpha=.5);","dd2ac803":"train.boxplot(column='acousticness', by='song_popularity', figsize=(10,5));","5cf68f28":"train.hist(column='danceability' ,bins=30, alpha=.5);","fe0f37dd":"train.boxplot(column='danceability', by='song_popularity', figsize=(10,5));","45d31758":"train.hist(column='energy' ,bins=30, alpha=.5);","2b091e13":"fig, ax = plt.subplots(figsize=(6,4))\nsns.kdeplot(data = train, x = 'energy', hue = 'song_popularity', common_norm=False, fill=True, label = 'Song Popularity', ax=ax);\nax.legend(loc='upper left');","7242a98c":"train.boxplot(column='energy', by='song_popularity', figsize=(10,5));","246bead3":"train.hist(column='instrumentalness' ,bins=30, alpha=.5);","b2d3d531":"train.boxplot(column='instrumentalness', by='song_popularity', figsize=(10,5));","70f963b1":"train.hist(column='key' ,bins=30, alpha=.5);","e341476b":"train.hist(column='liveness' ,bins=30, alpha=.5);","9fa106cc":"train.boxplot(column='liveness', by='song_popularity', figsize=(8,5));","4e420218":"train.hist(column='loudness' ,bins=30, alpha=.5);","cf55272e":"train.boxplot(column='loudness', by='song_popularity', figsize=(8,5));","bfa7ace2":"train.hist(column='audio_mode' ,bins=30, alpha=.5);","9e2ce97b":"train.hist(column='speechiness', bins=30, alpha=.5);","7688805f":"train.boxplot(column='speechiness', by='song_popularity', figsize=(8,5));","abc1daf5":"train.hist(column='tempo', bins=30, alpha=.5);","4e384d6f":"train.boxplot(column='tempo', by='song_popularity', figsize=(8,5));","796ae087":"train.hist(column='time_signature', bins=30, alpha=.5);","c0971004":"train.time_signature.value_counts()","1f019252":"train.hist(column='audio_valence', bins=30, alpha=.5);","7cd990be":"train.boxplot(column='audio_valence', by='song_popularity', figsize=(8,5));","b5d3adf4":"'''\nfig, axs = plt.subplots(5, 3, figsize=(15, 15), sharex=True)\naxs = axs.flatten()\nplt_idx = 0\nfor cat_id, d in train.groupby('categoryId'):\n    title = f'{cat_id}: {category_id_map[cat_id]} - target'\n    d['target'].apply(np.log1p) \\\n        .plot(kind='hist',\n              bins=100,\n              title=title,\n              ax=axs[plt_idx],\n              color=next(color_cycle)\n             )\n    plt_idx += 1\nplt.tight_layout()\nplt.show()\n'''","1451bcdf":"train.head()","d2f70aa7":"# Define features & target\nfeats = train.drop(columns=['id', 'song_popularity']).columns.to_list()\ntarget = ['song_popularity']\n\nX = train[feats]\ny = train[target]\nx_test = test[feats]\n\nx_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.25, shuffle=True, random_state=42)","6f903d74":"# Add missing indicator\ndef add_missing_indicator(x_train, x_test, mode='train'):\n    na_feats = x_train.columns[x_train.isna().any()].to_list()\n    x_train_cp = x_train.copy()\n    x_test_cp = x_test.copy()\n    if mode == 'train':\n        for feat in na_feats:\n            x_train_cp.loc[:, feat+'_ind'] = np.where(x_train.loc[:, feat].isna(), 1, 0)\n        return x_train_cp        \n    else:\n        for feat in na_feats:\n            x_test_cp.loc[:, feat+'_ind'] = np.where(x_test.loc[:, feat].isna(), 1, 0)\n        return x_test_cp        ","48c4d52c":"x_train_ind = add_missing_indicator(x_train, x_val, mode='train')\nx_val_ind = add_missing_indicator(x_train, x_val, mode='test')\nx_test_ind = add_missing_indicator(x_train, x_test, mode='test')\nall_feats = x_train_ind.columns.to_list()","9fc7fe37":"x_train_ind.head()","2eb4cd5b":"# Missing values treatment\nna_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\nx_train_tf = pd.DataFrame(na_imputer.fit_transform(x_train_ind), columns=all_feats)\nx_val_tf = pd.DataFrame(na_imputer.transform(x_val_ind), columns=all_feats)\nx_test_tf = pd.DataFrame(na_imputer.transform(x_test_ind), columns=all_feats)","639eff36":"x_train_tf.head(3)","183271fb":"x_val_tf.head(3)","e61c12a7":"rf = RandomForestClassifier(n_estimators=1000,  max_depth=6, n_jobs=-1)\nrf.fit(x_train_tf, y_train)\npreds_val = rf.predict(x_val_tf)\nprint(\n    f'Train AUC is:{metrics.roc_auc_score(y_train, rf.predict_proba(x_train_tf)[:, 1])}',\n    f'Valid AUC is :{metrics.roc_auc_score(y_val, rf.predict_proba(x_val_tf)[:, 1])}',\n    sep='\\n'\n)","8ededf94":"preds_test_prob =  rf.predict_proba(x_test_tf)[:, 1]","10726d05":"test.head()","7e69009c":"df_preds = pd.DataFrame({'id':test.id , 'song_popularity': preds_test_prob})\ndf_preds.to_csv('sub4.csv' ,index=False)","60630c2f":"def get_fi(model, x):\n    return pd.DataFrame(np.stack([x.columns, model.feature_importances_], axis=1), columns=['features', 'imp']).astype({'imp':np.float32}).sort_values(by='imp', ascending=False).reset_index(drop=True) ","909e959a":"get_fi(rf, x_train_tf)","99e554f7":"# Define features\nuseful_features = [c for c in df.columns if c not in ('id', 'song_popularity', 'kfold')]\nobject_cols = ['key', 'audio_mode', 'time_signature']\ndf_test = df_test[useful_features]","8446de4d":"# Run XGBoost with Cross Validation.\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.song_popularity\n    yvalid = xvalid.song_popularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    na_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n    xtrain = pd.DataFrame(na_imputer.fit_transform(xtrain), columns=useful_features)\n    xvalid = pd.DataFrame(na_imputer.transform(xvalid), columns=useful_features)\n    xtest = pd.DataFrame(na_imputer.transform(xtest), columns=useful_features)\n    \n    model = XGBClassifier(\n        n_estimators = 100,\n        max_depth = 8,\n        learning_rate = 0.05,\n        reg_alpha = 0,\n        reg_lambda = 1,\n        #tree_method='gpu_hist', \n        eval_metric = 'auc', \n        random_state=fold, \n        n_jobs=4, \n        use_label_encoder=False)\n    #model = RandomForestClassifier(n_estimators=100,  max_depth=6, n_jobs=-1)\n    model.fit(xtrain, ytrain)\n    \n    preds_valid_prob = model.predict_proba(xvalid)[:, 1]\n    preds_test_prob = model.predict_proba(xtest)[:, 1]\n    \n    final_predictions.append(preds_test_prob)\n    auc = metrics.roc_auc_score(yvalid, model.predict_proba(xvalid)[:, 1])\n    print(fold, auc)\n    scores.append(auc)\nprint(np.mean(scores), np.std(scores))    \n    ","39876e30":"# Prediction\npreds = np.mean(np.column_stack(final_predictions), axis=1)\ndf_preds = pd.DataFrame({'id':test.id , 'song_popularity': preds})\n# df_preds.to_csv('preds5.csv' ,index=False)","6043b79a":"for col in object_cols:\n    \n    # to be used to append validation dataset for each fold.\n    temp_df = []\n    \n    # to be used for computing the encoded feature value for the test dataset.\n    temp_test_feats = None\n    for fold in range(5):\n        xtrain = df[df['kfold']!= fold].reset_index(drop=True)\n        xvalid = df[df['kfold'] == fold].reset_index(drop=True)\n\n        # calculate the encoding on the training fold and use it for the validation fold\n        feat = xtrain.groupby(col)['song_popularity'].mean()\n        feat = feat.to_dict()\n        xvalid.loc[:, f'{col}_tar_enc'] = xvalid.loc[:, col].map(feat)\n\n        # append the validation data to temp_df so that we can later concat all the\n        # valid dataframes to form our training set. In this training set, \n        # the target encoded feature value for a fold would be determined based on \n        # all folds except that one and would prevent leakage.\n        temp_df.append(xvalid)\n\n        # prepare the target encoded test feature by adding and averaging the output \n        # from each training fold and using that averaged value as the feature value\n        # for the test dataset \n        if temp_test_feats is None: \n            temp_test_feats = df_test[col].map(feat)\n        else : \n            temp_test_feats += df_test[col].map(feat)\n\n    # Here we calculate the encoded feature value for the test set based on the \n    # avg of the value obtained from the out-of-folds data. Each value is \n    # calculated based on the out-of-fold encoded value and an average is taken.\n    temp_test_feats \/= 5\n    df_test.loc[:, f'{col}_target_enc'] = temp_test_feats\n\n    # Here we concat all the validation folds that we have accumulated in the \n    # the temp_df list. This will ensure that the each encoded feature value is \n    # calculated on out-of-fold data and there will not be any leakage.\n    df = pd.concat(temp_df)","cef82cb2":"df = pd.read_csv(\"..\/input\/song-popularity-folds\/train_folds (1).csv\")\ndf_test = pd.read_csv(path\/'test.csv')\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"song_popularity\", \"kfold\")]\nobject_cols = object_cols = ['key', 'audio_mode', 'time_signature']\ndf_test = df_test[useful_features]","e74d5d78":"# Target Encoding\nfor col in object_cols:\n    temp_df = []    \n    temp_test_feats = None\n    for fold in range(5):\n        xtrain = df[df['kfold']!= fold].reset_index(drop=True)\n        xvalid = df[df['kfold'] == fold].reset_index(drop=True)\n\n        feat = xtrain.groupby(col)['song_popularity'].mean()\n        feat = feat.to_dict()\n        xvalid.loc[:, f'{col}_tar_enc'] = xvalid.loc[:, col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feats is None: \n            temp_test_feats = df_test[col].map(feat)\n        else : \n            temp_test_feats += df_test[col].map(feat)\n\n    temp_test_feats \/= 5\n    df_test.loc[:, f'{col}_tar_enc'] = temp_test_feats\n    df = pd.concat(temp_df)\n","14de3038":"useful_features = [c for c in df.columns if c not in (\"id\", \"song_popularity\", \"kfold\")]\nobject_cols = ['key', 'audio_mode', 'time_signature']\ndf_test = df_test[useful_features]\n\ndef run(trial):\n    fold = 0\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.song_popularity\n    yvalid = xvalid.song_popularity\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    na_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n    xtrain = pd.DataFrame(na_imputer.fit_transform(xtrain), columns=useful_features)\n    xvalid = pd.DataFrame(na_imputer.transform(xvalid), columns=useful_features)\n    #xtest = pd.DataFrame(na_imputer.transform(xtest), columns=useful_features)\n\n    model = XGBClassifier(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=1000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth,\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], eval_metric='auc', verbose=1000)\n    auc = metrics.roc_auc_score(yvalid, model.predict_proba(xvalid)[:, 1])\n    return auc\n","695f0655":"#study = optuna.create_study(direction='maximize')\n#study.optimize(run, n_trials=100)\n#study.best_params","71090fa9":"final_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.song_popularity\n    yvalid = xvalid.song_popularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    na_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n    xtrain = pd.DataFrame(na_imputer.fit_transform(xtrain), columns=useful_features)\n    xvalid = pd.DataFrame(na_imputer.transform(xvalid), columns=useful_features)\n    xtest = pd.DataFrame(na_imputer.transform(xtest), columns=useful_features)\n    \n    params = {\n     'learning_rate': 0.01457612376117814,\n     'reg_lambda': 4.172314502824556,\n     'reg_alpha': 1.7569139119225397e-05,\n     'subsample': 0.9315294838214047,\n     'colsample_bytree': 0.9073420039531463,\n     'max_depth': 5}\n    \n    model = XGBClassifier(\n    random_state=42,\n    tree_method=\"gpu_hist\",\n    gpu_id=0,\n    predictor=\"gpu_predictor\",\n    n_estimators=1000,\n    **params)\n    \n    # Fit the model\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    # Make predictions\n    preds_valid_prob = model.predict_proba(xvalid)[:, 1]\n    preds_test_prob = model.predict_proba(xtest)[:, 1]\n    \n    final_predictions.append(preds_test_prob)\n    # Validation AUC\n    auc = metrics.roc_auc_score(yvalid, model.predict_proba(xvalid)[:, 1])\n    print(fold, auc)\n    scores.append(auc)\n    \nprint(np.mean(scores), np.std(scores))","8a5cc6b7":"# Optimized XGBoost Prediction\npreds = np.mean(np.column_stack(final_predictions), axis=1)\ndf_preds = pd.DataFrame({'id':test.id , 'song_popularity': preds})\ndf_preds.to_csv('xgb_preds.csv' ,index=False)","7049d948":"## 3. High level overview of the data","62b4308d":"## 5. Random Forest Baseline","7ee9ccfc":"### danceability","a69e2b92":"### audio_valence","597f3524":"### tempo","b8382ef0":"## 4. Exploratory Data Analysis","efa5deaa":"### liveness","72f29ae9":"### energy","eab5216d":"## 6. Modeling with XGBoost","1fed8bc0":"What are the questions that histogram helps us answer?\n* Which values are the most common? Why?\n* Which values are rare? Why? Does that match your expectations?\n* Can you see any unusual patterns? What might explain them?","4ed4cd6e":"### 6.2 Target Encoding","21e54ba0":"## 1. Import packages & data","7b010b82":"### loudness","d27c9e41":"### instrumentalness","875d7d69":"### key","530eb264":"### 6.4 Optimized XGBoost","6fbb21e2":"### Song Duration","06551cf7":"# [Song Popularity Prediction](https:\/\/www.kaggle.com\/c\/song-popularity-prediction)","87967721":"### 6.3 Hyperparameter Tuning","1cc267e9":"## 4. Data Preparation","17954c28":"### 6.1 XGBoost Baseline","a512ea50":"### 4.1 Study the correlations in the dataset","9e484bc2":"### Acousticness","22ca3a98":"### audio_mode","8ee0a7ab":"### Plotting all at once\n[Credits](https:\/\/www.kaggle.com\/robikscube\/the-stallion-baseline-e01)","1b659882":"## 2. Data descrption\n\nSource - Spotify: \"In Spotify's API is something called Valence, that describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (happy, cheerful, euphoric), while tracks with low valence sound more negative (sad, depressed, angry).\"\n\nFrom very good article explaining Spotify API What Makes a Song Likeable?- https:\/\/towardsdatascience.com\/what-makes-a-song-likeable-dbfdb7abe404 we can read that:\n\nSpotify Audio Features For every track on their platform, Spotify provides data for thirteen Audio Features.The Spotify Web API developer guide defines them as follows:\n\n**Danceability**: Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.\n\n**Valence**: Describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n**Energy**: Represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.\n\n**Tempo**: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece, and derives directly from the average beat duration.\n\n**Loudness**: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.\n\n**Speechiness**: This detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Instrumentalness: Predicts whether a track contains no vocals. \u201cOoh\u201d and \u201caah\u201d sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \u201cvocal\u201d.\n\n**Liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.\n\n**Acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic.\n\n**Key**: The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C\u266f\/D\u266d, 2 = D, and so on.\n\n**Mode**: Indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n\n**Duration**: The duration of the track in milliseconds.\n\n**Time Signature**: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).\n\nData Provided by Remek Kinas SOURCE\n\nCredits: Data description can be found [here](https:\/\/www.kaggle.com\/dataqueenpending\/song-popularity-prediction-dataqueen\/notebook?scriptVersionId=85893941&cellId=5)","857386cb":"### speechiness","7ba40fde":"### time_signature"}}