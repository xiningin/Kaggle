{"cell_type":{"fdc5293a":"code","8fb831bf":"code","b80a4fa7":"code","642a9b8c":"code","aa8f0414":"code","4d6369e7":"code","7256ca68":"code","4585982a":"code","d8141c9a":"code","524cba01":"code","f3224cc8":"code","6899b99d":"code","69d50b33":"code","87d8e506":"code","b05045fe":"code","705f378c":"markdown","b3268092":"markdown","ec155f82":"markdown","5edb5132":"markdown","6213fc29":"markdown","b39971aa":"markdown","ea055f84":"markdown","1b3b5ad9":"markdown"},"source":{"fdc5293a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","8fb831bf":"df = pd.DataFrame({\n    'x': [12, 20, 28, 18, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72],\n    'y': [39, 36, 30, 52, 54, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24]\n})\n\n\nnp.random.seed(200)\nk = 3\n# centroids[i] = [x, y]\ncentroids = {\n    i+1: [np.random.randint(0, 80), np.random.randint(0, 80)]\n    for i in range(k)\n}\n    \ncentroids","b80a4fa7":"# Plotting the data\nfig = plt.figure(figsize=(5, 5))\nplt.scatter(df['x'], df['y'], color='k')\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.show()","642a9b8c":"# Marking the random centroids choosen\nfig = plt.figure(figsize=(5, 5))\nplt.scatter(df['x'], df['y'], color='k')\ncolmap = {1: 'r', 2: 'g', 3: 'b'}\nfor i in centroids.keys():\n    plt.scatter(*centroids[i], color=colmap[i])\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.show()","aa8f0414":"## Assignment Stage\n\ndef assignment(df, centroids):\n    for i in centroids.keys():\n        # sqrt((x1 - x2)^2 - (y1 - y2)^2)\n        df['distance_from_{}'.format(i)] = (\n            np.sqrt(\n                (df['x'] - centroids[i][0]) ** 2\n                + (df['y'] - centroids[i][1]) ** 2\n            )\n        )\n    centroid_distance_cols = ['distance_from_{}'.format(i) for i in centroids.keys()]\n    df['closest'] = df.loc[:, centroid_distance_cols].idxmin(axis=1)\n    df['closest'] = df['closest'].map(lambda x: int(x.lstrip('distance_from_')))\n    df['color'] = df['closest'].map(lambda x: colmap[x])\n    return df\n\ndf = assignment(df, centroids)\nprint(df)\n\nfig = plt.figure(figsize=(5, 5))\nplt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\nfor i in centroids.keys():\n    plt.scatter(*centroids[i], color=colmap[i])\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.show()","4d6369e7":"## Update Stage\n\nimport copy\n\nold_centroids = copy.deepcopy(centroids)\n\ndef update(k):\n    for i in centroids.keys():\n        centroids[i][0] = np.mean(df[df['closest'] == i]['x'])\n        centroids[i][1] = np.mean(df[df['closest'] == i]['y'])\n    return k\n\ncentroids = update(centroids)\n    \nfig = plt.figure(figsize=(5, 5))\nax = plt.axes()\nplt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\nfor i in centroids.keys():\n    plt.scatter(*centroids[i], color=colmap[i])\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nfor i in old_centroids.keys():\n    old_x = old_centroids[i][0]\n    old_y = old_centroids[i][1]\n    dx = (centroids[i][0] - old_centroids[i][0]) * 0.75\n    dy = (centroids[i][1] - old_centroids[i][1]) * 0.75\n    ax.arrow(old_x, old_y, dx, dy, head_width=2, head_length=3, fc=colmap[i], ec=colmap[i])\nplt.show()","7256ca68":"## Repeat Assigment Stage\n\ndf = assignment(df, centroids)\n\n# Plot results\nfig = plt.figure(figsize=(5, 5))\nplt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\nfor i in centroids.keys():\n    plt.scatter(*centroids[i], color=colmap[i])\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.show()","4585982a":"# Continue until all assigned categories don't change any more\nwhile True:\n    closest_centroids = df['closest'].copy(deep=True)\n    centroids = update(centroids)\n    df = assignment(df, centroids)\n    if closest_centroids.equals(df['closest']):\n        break\n\nfig = plt.figure(figsize=(5, 5))\nplt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\nfor i in centroids.keys():\n    plt.scatter(*centroids[i], color=colmap[i])\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.show()\n","d8141c9a":"df = pd.DataFrame({\n    'x': [12, 20, 28, 18, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72],\n    'y': [39, 36, 30, 52, 54, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24]\n})\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(df)","524cba01":"labels = kmeans.predict(df)\ncentroids = kmeans.cluster_centers_","f3224cc8":"labels","6899b99d":"centroids","69d50b33":"fig = plt.figure(figsize=(5, 5))\n\ncolors = map(lambda x: colmap[x+1], labels)\n\nplt.scatter(df['x'], df['y'], color=colors, alpha=0.5, edgecolor='k')\nfor idx, centroid in enumerate(centroids):\n    plt.scatter(*centroid, color=colmap[idx+1])\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.show()","87d8e506":"wcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(df)\n    # inertia method returns wcss for that model\n    wcss.append(kmeans.inertia_)","b05045fe":"plt.figure(figsize=(10,5))\nsns.lineplot(range(1, 11), wcss,marker='o',color='red')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","705f378c":"![](https:\/\/i.imgur.com\/5W63xul.png)\n\nThe Elbow Method is then used to choose the best K value. In the depiction below we can see that after 3 there's no significant decrease in WCSS so 3 is the best here. Therefore there's an elbow shape that forms and it is usually a good idea to pick the number where this elbow is formed. There would be many times when the graph wouldn't be this intuitive but with practice it becomes easier.","b3268092":"References:\n[](http:\/\/)http:\/\/benalexkeen.com\/k-means-clustering-in-python\/","ec155f82":"Sample on real dataset: https:\/\/www.kaggle.com\/karthickaravindan\/k-means-clustering-project","5edb5132":"## Using skLearn","6213fc29":"## k parameter\n\nThe way to evaluate the choice of K is made using a parameter known as WCSS. WCSS stands for Within Cluster Sum of Squares. It should be low. Here's the formula representation for example when K = 3\n\nSummation Distance(p,c) is the sum of distance of points in a cluster from the centroid.","b39971aa":"The following code uses k-means with sklearn and without sklearn.","ea055f84":"K-means clustering is a clustering algorithm that aims to partition n observations into k clusters.\n\nThere are 3 steps:\n\n* Initialisation \u2013 K initial \u201cmeans\u201d (centroids) are generated at random\n* Assignment \u2013 K clusters are created by associating each observation with the nearest centroid\n* Update \u2013 The centroid of the clusters becomes the new mean\n* Repeat \u2013 Assignment and Update are repeated iteratively until convergence\n\nThe end result is that the sum of squared errors is minimised between points and their respective centroids.\n\nWe\u2019ll do this manually first, then show how it\u2019s done using scikit-learn","1b3b5ad9":"## Random centroids selection\n\nSo we saw that even with clear distinction possible visually, wrong randomisation can produce wrong results. There have been researches carried out and one of the most famous ways to initialise centroids is KMeans++.\n\nThe steps to initialize the centroids using K-Means++ are:\n\n1. The first cluster is chosen uniformly at random from the data points that we want to cluster. This is similar to what we do in K-Means, but instead of randomly picking all the centroids, we just pick one centroid here\n1. Next, we compute the distance (D(x)) of each data point (x) from the cluster center that has already been chosen\n1. Then, choose the new cluster center from the data points with the probability of x being proportional to (D(x))2\n1. We then repeat steps 2 and 3 until k clusters have been chosen\n\nMore diagramatic representation here: https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/comprehensive-guide-k-means-clustering\/"}}