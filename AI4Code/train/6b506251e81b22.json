{"cell_type":{"767a4e19":"code","fc620b4e":"code","d072957a":"code","402b6fde":"code","22061e93":"code","259368c5":"code","0f468b70":"code","87669a30":"code","fcf31154":"code","a16278e5":"code","9ae0de6b":"code","8391f27d":"code","92de2c72":"code","f7c757d3":"code","52019003":"code","034627b9":"code","a50e8704":"code","c0fec310":"code","0a660a15":"code","ee4e6aa1":"code","2359c339":"markdown","f965cf70":"markdown","6a57bf3f":"markdown","1433272e":"markdown","d58289c8":"markdown","6cf57759":"markdown","78b58dc7":"markdown","215838ae":"markdown","c119052e":"markdown","66c986fe":"markdown","fb999e23":"markdown","27298b1e":"markdown","34682592":"markdown","c85a88e4":"markdown"},"source":{"767a4e19":"pip install torchsummary ","fc620b4e":"import numpy as np\nimport pandas as pd\nimport os","d072957a":"from torch.utils.data import Dataset\nimport torch\nimport torch.nn.functional as F                          # Functions like ReLU\nimport torch.optim as optim                              # Optimizers like Adam\nimport torch.nn as nn                                    # Neural Network Modules\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch import ToTensorV2            # Coverting image to Tensor\nfrom tqdm import tqdm                                    # To print the Progress bar","402b6fde":"train_data_path = \"..\/input\/cat-and-dog\/training_set\/training_set\"\ntest_data_path=\"..\/input\/cat-and-dog\/test_set\/test_set\"","22061e93":"class CatvsDog(Dataset):\n    \"\"\"\n    This a class for selecting the data for the model\n    Attributes\n    ----------\n    images  : list\n        paths of the images\n    transforms : albumentations.core.composition.Compose\n        transforms that must be applied on the image\n    \"\"\"\n    def __init__(self,data_dir,typew='train',transforms=None):\n        catpaths = data_dir+'\/cats'\n        dogpaths = data_dir+'\/dogs'\n        cats = os.listdir(catpaths)\n        dogs = os.listdir(dogpaths)\n        self.images = [(catpaths+'\/'+cats[i],0) for i in range(len(cats)) if cats[i].endswith('.jpg')]\n        dogs = [(dogpaths+'\/'+dogs[i],1) for i in range(len(dogs)) if dogs[i].endswith('.jpg')]\n        self.transforms = transforms\n        self.images.extend(dogs)\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self,index):\n        img = np.array(Image.open(self.images[index][0]))\n        y = self.images[index][1]\n        if self.transforms is not None:\n            augmentations = self.transforms(image=img)\n            img = augmentations[\"image\"]\n        return img,y\n        ","259368c5":"import albumentations as A\nt1 = A.Compose([\n    A.Resize(224,224),\n    A.augmentations.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","0f468b70":"def get_dogandcat(image_dir,transforms=None,batch_size=1,shuffle=True,pin_memory=True,train=True):\n    data = CatvsDog(image_dir,transforms=t1)\n    if train==False:\n        return torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n    train_size = int(0.8 * data.__len__())\n    test_size = data.__len__() - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n    train_batch = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n    test_batch = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n    return train_batch,test_batch","87669a30":"train_batch,test_batch = get_dogandcat(train_data_path,transforms=t1,batch_size=16,shuffle=True,pin_memory=True)","fcf31154":"for i,j in train_batch:\n    img = np.transpose(i[0,:,:,:], (1,2,0))\n    print(j)\n    plt.imshow(img)\n    break","a16278e5":"class N_conv(nn.Module):\n    \"\"\"\n    This a class for defining the N convolution\n    Attributes\n    ----------\n    conv  : nn.Sequential\n        defines the train model\n    \"\"\"\n    def __init__(self,in_channels,out_channels,N = 2):\n        super(N_conv,self).__init__()\n        model = []\n        model.append(nn.Conv2d(in_channels,out_channels,kernel_size=(3,3),padding=(1,1)))\n        model.append(nn.ReLU(True))\n        for i in range(N-1):\n            model.append(nn.Conv2d(out_channels,out_channels,kernel_size=(3,3),padding=(1,1)))\n            model.append(nn.ReLU(True))\n        model.append(nn.MaxPool2d(kernel_size=(2,2),stride=(2,2)))\n        self.conv = nn.Sequential(*model)\n    def forward(self,x):\n        return self.conv(x)","9ae0de6b":"class Vgg16(nn.Module):\n    \"\"\"\n    This a class for defing the VGG16 model\n    Attributes\n    ----------\n    conv  : nn.Sequential\n        defines the train model\n    \"\"\"\n    def __init__(self,in_channels=3,out_channels=1,init_weights=True):\n        super(Vgg16,self).__init__()\n        self.conv1 = N_conv(3,64)\n        self.conv2 = N_conv(64,128)\n        self.conv3 = N_conv(128,256,N=3)\n        self.conv4 = N_conv(256,512,N=3)\n        self.conv5 = N_conv(512,512,N=3)\n        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n        self.linear1 = nn.Linear(512*7*7,4096)\n        self.linear2 = nn.Linear(4096,4096)\n        self.relu = nn.ReLU(True)\n        self.dropout = nn.Dropout(0.3)\n        self.linear3 = nn.Linear(4096,2)\n        if init_weights:\n            self._initialize_weights()\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x,1)\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear3(x)\n        return x","8391f27d":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","92de2c72":"# Hyperparameters\nlearning_rate = 1e-4\nnum_epochs = 20","f7c757d3":"model = Vgg16(3,2).to(device)","52019003":"from torchsummary import summary\nsummary(model, (3, 224, 224))","034627b9":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","a50e8704":"for epoch in range(num_epochs):\n    loop = tqdm(enumerate(train_batch),total=len(train_batch))\n    loss = 0\n    for batch_idx, (data, targets) in loop:\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        \n\n        # backward\n        optimizer.zero_grad()\n        \n        scores = model(data)\n        loss = criterion(scores, targets)\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n        \n        #Update Progress bar\n        loop.set_description(f'Epoch [{epoch+1}]')\n        loop.set_postfix(loss = loss.item())\n","c0fec310":"def check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n\n    model.train()\n    return num_correct\/num_samples","0a660a15":"check_accuracy(train_batch, model)","ee4e6aa1":"check_accuracy(test_batch, model)","2359c339":"<p>VGG16 was created by K. Simonyan and A. Zisserman (University of Oxford researchers) in 2014.<\/p>\n<p>This model was seen as an upgrade to AlexNet.<\/p>\n<p>VGG16 was modeled in such a way that convolutions would actually look simpler by replacing AlexNet large convolution filters with a 3\u00d73 filter while padding to maintain the same size before a 2\u00d72 MaxPool layer downsamples the image size.<\/p>","f965cf70":"# Architecture","6a57bf3f":"### The dataset class\nIn dataset preparation we write three functions,\n1. **init**    : Used to retrive filepaths of images\n2. **len**     : Returns the length of the dataset\n3. **getitem** : Returns a single image input and its output values","1433272e":"# Training","d58289c8":"<p>The VGG16 represents the model and N_conv represents the the N_convoution operations<\/p>\n<p>The Conv2D filters are initialized to kaiming normal and the linear layers with normal<\/p>","6cf57759":"![1_NNifzsJ7tD2kAfBXt3AzEg.png](attachment:af858c22-0164-41c4-a875-28e6213213c4.png)","78b58dc7":"# DataSet preparation","215838ae":"## A short description of VGG16","c119052e":"To build any model in pytorch, we can broadly divide the code into three parts:\n1. **Dataset**\n2. **Model**\n3. **Train**\n\n","66c986fe":"We set the criterion to CrossEntropy and optimizer to Adam","fb999e23":"<p> And finally we code the training part <\/p>\n<p> We switch to gpu and set the hyperparameters <\/p>","27298b1e":"## Building any model in PyTorch\n","34682592":"The transforms are the operations performed on the image before passing it to the model","c85a88e4":"### Transforms"}}