{"cell_type":{"48a81037":"code","99823161":"code","969ed60a":"code","c119c6b3":"code","9d3500f1":"code","132197d9":"code","091cc583":"code","95ad5b41":"code","3a5c1644":"code","68640306":"code","afa0e4e3":"markdown","7b0b906f":"markdown","20646002":"markdown","29d286a3":"markdown","e36ab364":"markdown","9f8a9996":"markdown","2b9dd811":"markdown","b78416b3":"markdown","12e66c43":"markdown","7d6a894d":"markdown"},"source":{"48a81037":"import pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly_express as px\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","99823161":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')","969ed60a":"fig = ff.create_distplot(\n    [train.target[:10000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])\n\nfig","c119c6b3":"df_69685 = pd.read_csv('..\/input\/dataset\/0.69685-others-submission6.csv')\nfig = ff.create_distplot(\n    [df_69685.target[:10000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])","9d3500f1":"# light GBM optuna tuned params\nlight_opt_best_random = {'n_estimators': 8540,\n 'min_data_per_group': 45,\n 'num_leaves': 126,\n 'max_depth': 20,\n 'learning_rate': 0.0032598944879946414,\n 'subsample_for_bin': 32553,\n 'lambda_l1': 0.11917413918151999,\n 'lambda_l2': 6.857359561808505e-05,\n 'bagging_fraction': 0.8910429482743759,\n 'min_data_in_leaf': 94,\n 'min_sum_hessian_in_leaf': 0.01,\n 'bagging_freq': 2,\n 'feature_fraction': 0.4699812049606955,\n 'min_child_samples': 61}\n\n# xgboost upotuna tuned params\nxgb_opt_best_random = {'learning_rate': 0.004138539806617361, \n 'gamma': 0.020496820582462844, \n 'max_depth': 19, \n 'min_child_weight': 308, \n 'max_delta_step': 9, \n 'subsample': 0.6437442427644592, \n 'colsample_bytree': 0.41845630929589844, \n 'lambda': 0.0038484657676066394, \n 'alpha': 0.09281553090596092, \n 'n_estimators': 4767\n}\n\n# catboost optuna tuned params\ncatboost_opt_best_random= {\n    'n_estimators': 9639, \n    'learning_rate': 0.025621857270512527, \n    'reg_lambda': 0.03261099593456338, \n    'subsample': 0.6319711159148579, \n    'depth': 7, \n    'min_child_samples': 48, \n    'colsample_bylevel': 0.14898612913306458, \n    'langevin': False, \n    'model_shrink_rate': 0.28621265987632455, \n    'model_shrink_mode': 'Decreasing', \n    'model_size_reg': 2.373053070327802\n}","132197d9":"# loading predictions for the 6 models above\nsub26a = pd.read_csv('..\/input\/dataset\/submission26a-optuna-light-sm.csv')\nsub26b = pd.read_csv('..\/input\/dataset\/submission26b-optuna-light-lg.csv')\nsub26c = pd.read_csv('..\/input\/dataset\/submission26c-optuna-xgb-sm.csv')\nsub26d = pd.read_csv('..\/input\/dataset\/submission26d-optuna-xgb-lg.csv')\nsub26e = pd.read_csv('..\/input\/dataset\/submission26e-optuna-catboost-sm.csv')\nsub26f = pd.read_csv('..\/input\/dataset\/submission26f-optuna-catboost-lg.csv')","091cc583":"ls = []\nfor idx, num in enumerate(df_69685.target):\n    \n    if np.mean([sub26b.target[idx], sub26d.target[idx], sub26f.target[idx]]) > 8.1:# change this threshold or swap the if\/elif clause, you will get different shapes\n        ls.append(np.mean([sub26b.target[idx], sub26d.target[idx], sub26f.target[idx]]))\n    elif np.mean([sub26a.target[idx], sub26c.target[idx], sub26e.target[idx]]) < 8.1:\n        ls.append( np.mean([sub26a.target[idx], sub26c.target[idx], sub26e.target[idx]]))\n\n    else:\n        ls.append(num)\n","95ad5b41":"\nfig = ff.create_distplot(\n    [ls[:1000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])","3a5c1644":"ls = []\nfor idx, num in enumerate(df_69685.target):\n    \n    if np.mean([sub26a.target[idx], sub26c.target[idx], sub26f.target[idx]]) < 7.8: # change this threshold or swap the if\/elif clause, you will get different shapes\n        ls.append( np.mean([sub26a.target[idx], sub26c.target[idx], sub26f.target[idx]]))\n    \n    elif np.mean([sub26b.target[idx], sub26d.target[idx], sub26e.target[idx]]) > 7.8 :\n        ls.append(np.mean([sub26b.target[idx], sub26d.target[idx], sub26e.target[idx]]))\n    else:\n        ls.append(num)","68640306":"fig = ff.create_distplot(\n    [ls[:10000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])","afa0e4e3":"So in theory you can generate predictions with bimodal distributions, but their does not seem to be as good as the best model I have, which only has 1 peak. Any thoughts?","7b0b906f":"## Mix 1 - public score = 0.7034","20646002":"# Simulating the shape of target in the train set","29d286a3":"# Generate some bi-modal predictions","e36ab364":"# Generating various bimodal predictions","9f8a9996":"## Mix 2 - public score = 0.70714","2b9dd811":"Sharing some of my predictions that displayed bimodal distribution. Public scores are around 0.70. Tried some ensembles but public score isn't necessarily any better than \"unimodal\" predictions..","b78416b3":"# Distribution of target in train set","12e66c43":"I trained 5 additional models: \n\n- XGBRegressor,  using train set where target is <9. \n- Same XGBRegressor model, except using train set where target >7\n- LightGBM, using train set where target is <9. \n- Same LightGBM model, except using train set where target >7\n- CatBoostRegressor, using train set where target is <9. \n- Same CatBoostRegressor model, except using train set where target >7\n\nAll 6 models were tuned using Optuna. Each model has an RMSE of bewteen 0.3-0.5 for prediction within its specified region. Then I used each of the models to predict the **full** test set","7d6a894d":"# Distribution of predictions for test set (ensemble, public score = 0.69685 )"}}