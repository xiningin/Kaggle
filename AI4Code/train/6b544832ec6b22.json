{"cell_type":{"65713abe":"code","8291fd82":"code","696138fc":"code","36fb65c8":"code","5909aa7f":"code","54e4afd7":"code","60260803":"code","0f052243":"code","386b67dc":"code","826faa08":"code","c36b898f":"code","6a9dd3fa":"code","9a8c355c":"code","665ced6c":"code","dd7c3547":"code","7f247193":"code","f9e34881":"code","635bc490":"code","99552508":"code","592d31fa":"code","747adab7":"code","a1257936":"code","b83a2164":"code","e735e72c":"code","466c97e2":"code","74a54e04":"code","ab83a9b3":"code","cac080c6":"code","0568db76":"code","636cc809":"code","b80ddf76":"code","ade43df2":"code","1148ac9e":"markdown","5688ffc9":"markdown","9cda4e5e":"markdown","bdfdea6b":"markdown","e9ccc715":"markdown","b18b961d":"markdown","f9afee4f":"markdown","3d926fa2":"markdown","7f5a2cbd":"markdown","acbd8877":"markdown","1cc62e7b":"markdown","e6ba90cd":"markdown","02ae7282":"markdown","396445e9":"markdown","996eb2c7":"markdown","5a562984":"markdown","80be9dd3":"markdown","514d6c59":"markdown","1777b42e":"markdown","3924328d":"markdown","0e7e4318":"markdown","11294118":"markdown","f0cc7ecd":"markdown","7332c401":"markdown","affbe819":"markdown","b5ec2d5a":"markdown","051b22dc":"markdown","689e8c0c":"markdown","ae2a0c96":"markdown","cc795387":"markdown","47eeb471":"markdown","2c65ee96":"markdown","01972539":"markdown","632cff2f":"markdown","3b0209d1":"markdown","37e0b65f":"markdown","8dcd112c":"markdown","d67f6166":"markdown","6a1af0f6":"markdown","c7f4ff52":"markdown","0688cd14":"markdown","6b3c9879":"markdown","8fd64bc4":"markdown","9d9b2843":"markdown","6a5dc0b6":"markdown","a7a415a8":"markdown","3c606361":"markdown","1a0fc803":"markdown","367b3807":"markdown"},"source":{"65713abe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8291fd82":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator, FormatStrFormatter\nplt.rcParams[\"figure.figsize\"] = (20, 15)\nimport seaborn as sns\nfrom bs4 import BeautifulSoup","696138fc":"stack_data = pd.read_csv(r'\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv')\nstack_data","36fb65c8":"print(stack_data.info())","5909aa7f":"stack_data_f = stack_data.dropna(subset=['Y'])\nstack_data_f.CreationDate = stack_data_f.CreationDate.astype('datetime64[ns]')\nstack_data_f['year'] = stack_data_f.CreationDate.dt.year\nstack_data_f['month'] = stack_data_f.CreationDate.dt.month\nstack_data_f['day'] = stack_data_f.CreationDate.dt.day\nstack_data_f.info()","54e4afd7":"stack_data_f['date_month'] = pd.to_datetime({'month':stack_data_f.CreationDate.dt.month,\n                                             'year':stack_data_f.CreationDate.dt.year,\n                                             'day':[1 for i in stack_data_f.CreationDate]})\n\nstack_data_gb_d = stack_data_f.groupby(by=stack_data_f.CreationDate.dt.date\n                                    ).agg({'CreationDate':lambda x:(~x.isna()).sum(),\n                                           'date_month': lambda x: x.iloc[0]})\n\nstack_data_gb_m = stack_data_gb_d.groupby(by=['date_month']).agg([np.mean, np.max, np.min, np.sum])\n\n\nstack_data_gb_y = stack_data_f.groupby(by=stack_data_f.CreationDate.dt.year\n                                    ).agg({'CreationDate':lambda x:(~x.isna()).sum(),\n                                           'date_month': lambda x: x.iloc[0]})\n\n\nfig = plt.figure(figsize = (20, 15))\nplt.gcf().subplots_adjust(left = 0.1, bottom = 0.1,\n                       right = 0.9, top = 0.9, wspace = 0, hspace = 0.3)\nwidth = 0.35\n\n# Plot day\nax_d = fig.add_subplot(311)\n\nmajorLocator = MultipleLocator(10)\nmajorFormatter = FormatStrFormatter('%d')\nminorLocator = MultipleLocator(2)\n\nl1, = ax_d.plot(stack_data_gb_m.index, stack_data_gb_m.iloc[:,1],\n            label='Questions per day', alpha=0.5, c='b')\nl2, = ax_d.plot(stack_data_gb_m.index, stack_data_gb_m.iloc[:,2],\n            label='Questions per day', alpha=0.5, c='b')\nl3, = ax_d.plot(stack_data_gb_m.index, stack_data_gb_m.iloc[:,0], c='orange')\n\nax_d.set_ylabel('Number of Questions per day')\nax_d.set_title('Number of questions asked per month')\nax_d.yaxis.set_major_locator(majorLocator)\nax_d.yaxis.set_major_formatter(majorFormatter)\nax_d.yaxis.set_minor_locator(minorLocator)\n\nplt.fill_between(stack_data_gb_m.index, \n                     stack_data_gb_m.iloc[:,1],\n                     stack_data_gb_m.iloc[:,2],\n                     alpha=0.2)\n    \nplt.grid(axis='both', color='0.95')\nplt.legend([l1,l3],['min and max number of questions per day', 'mean'])\n\n# Barplot month\nax_m = fig.add_subplot(3,1,2)\n\nsns.barplot(x=stack_data_gb_m.index.date, y=stack_data_gb_m.iloc[:,3], palette=\"Blues_d\", ax=ax_m)\nplt.setp(ax_m.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\nax_m.set_ylabel('Number of Questions')\n\n# Barplot year\nax_y = fig.add_subplot(3,1,3)\n\nsns.barplot(x=stack_data_gb_y.index, y=stack_data_gb_y['CreationDate'], data=stack_data_gb_y, palette=\"Blues_d\")\n\nax_y.set_xlabel('time')\nax_y.set_ylabel('Number of Questions')\n\n#From matplotlib exemple\ndef autolabel(rects, ax, width=0.35, xpos='center'):\n    \"\"\"\n    Attach a text label above each bar in *rects*, displaying its height.\n\n    *xpos* indicates which side to place the text w.r.t. the center of\n    the bar. It can be one of the following {'center', 'right', 'left'}.\n    \"\"\"\n\n    ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n    offset = {'center': 0, 'right': 1, 'left': -1}\n    i=0\n    for height in rects:\n        ax.annotate('{}'.format(height),\n                    xy=(i , height),\n                    xytext=(offset[xpos]*3, 3),  # use 3 points offset\n                    textcoords=\"offset points\",  # in both directions\n                    ha=ha[xpos], va='bottom', size=8)\n        i+=1\n\n\nautolabel(stack_data_gb_m.iloc[:,3], ax=ax_m)\nautolabel(stack_data_gb_y['CreationDate'], ax=ax_y)\nprint('graphs based on:')\nstack_data_gb_d","60260803":"# Could be better defined\nlang_list = ['( |<)C( |>)','( |<)C[+]','( |<)C[#]','objective-c','Java( |>)','SQL','Javascript','Python','Ruby','PHP','HTML','( |<)R( |>)','MATLAB']\n\nTags = stack_data_f.Tags.str.split('><',expand=True)\nTags = Tags.apply(lambda x: x.str.replace('<|>',''))\n\nlist_tags=pd.Series()\nfor col in Tags:\n    list_tags = pd.concat([list_tags, Tags.loc[:, col]])\n    \nlist_tags = list_tags.dropna().reset_index(drop=True) # List of all the tags\nprint(\"Total number of tags over 60k topics:\", list_tags.shape[0])\n\n# Languages study\nstack_data_l = stack_data_f.copy()\nfor lang in lang_list:\n    stack_data_l[lang] = stack_data.Tags.str.contains(lang, regex=True, case=False)\n    \nstack_data_nb_l = stack_data_l.loc[:,lang_list].sum().sort_values(ascending=False) # Values for each languages\nprint('Total number of references to a language:', stack_data_nb_l.sum())\n\n# Graph\nlgs = stack_data_nb_l.values\nind = stack_data_nb_l.index\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind, lgs, width)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Scores')\nax.set_title('Number of tags by languages between 2016 and 2020 over 60k topics')\nax.set_xticks(ind)\nax.legend()\n\n\n#From matplotlib exemple\ndef autolabel(rects, xpos='center'):\n    \"\"\"\n    Attach a text label above each bar in *rects*, displaying its height.\n\n    *xpos* indicates which side to place the text w.r.t. the center of\n    the bar. It can be one of the following {'center', 'right', 'left'}.\n    \"\"\"\n\n    ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n    offset = {'center': 0, 'right': 1, 'left': -1}\n\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(offset[xpos]*3, 3),  # use 3 points offset\n                    textcoords=\"offset points\",  # in both directions\n                    ha=ha[xpos], va='bottom')\n\n\nautolabel(rects1)\n\n# Proportion of languages per year\n# Same code than the previous point\n\ndef pie_l(periodicity, date, lang_list, ax, data=stack_data_l.copy()):\n    \"\"\"Create a pie on languages proportions according to the periodicity chosen and the date\"\"\"\n    \n    fracs = data.loc[data[periodicity] == date,lang_list].sum().sort_values(ascending=False)\n    fracs = fracs.apply(lambda x: x*100\/fracs.sum())\n    labels = fracs.index\n    ax.pie(fracs, labels=labels, autopct='%1.1f%%', textprops={'fontsize':10},\n                  shadow=True, explode=tuple(0.2 if i==0 \n                                              else 0.1 if i==1\n                                              else 0.05 if i==2 \n                                              else 0 for i,v in enumerate(fracs)))\n    ax.set_title('Proportions of each languages in '+str(date))\n    return fracs\n\n# Make figure and axes\nfig, axs = plt.subplots(3, 2, figsize=(30,30))\nx1 = pie_l('year', 2016, lang_list, ax=axs[0,0])\nx2 = pie_l('year', 2017, lang_list, ax=axs[0,1])\nx3 = pie_l('year', 2018, lang_list, ax=axs[1,0])\nx4 = pie_l('year', 2019, lang_list, ax=axs[1,1])\nx5 = pie_l('year', 2020, lang_list, ax=axs[2,0])\n\npaper_rc = {'lines.linewidth': 1, 'lines.markersize': 8}                  \nsns.set_context(\"paper\", rc = paper_rc)\nsns.lineplot(data=pd.DataFrame([x1,x2,x3,x4,x5], index=pd.date_range('2016', periods=5, freq='Y')),\n             markers=['s', 'o', 'v', '<', '>','s', 'o', 'v', '<', '>','o', 'v', '<' ], dashes=False, ax=axs[2,1])\naxs[2,1].set_title('Languages proportions over time (Same meaning than the kpi)')","0f052243":"fig, ax = plt.subplots(2,3, figsize=(30,20))\n\n# By Classes\nHQ = stack_data_l.loc[stack_data_l.Y=='HQ', lang_list].sum()\nHQ.loc['other'] = 20000 - HQ.sum()\nHQ.sort_values(ascending=False, inplace=True)\n\nLQ = stack_data_l.loc[stack_data_l.Y=='LQ_EDIT', lang_list].sum()\nLQ.loc['other'] = 19999 - LQ.sum()\nLQ.sort_values(ascending=False, inplace=True)\n\nLQC = stack_data_l.loc[stack_data_l.Y=='LQ_CLOSE', lang_list].sum()\nLQC.loc['other'] = 19998 - LQC.sum()\nLQC.sort_values(ascending=False, inplace=True)\n\n# By languages\npy = stack_data_l.loc[stack_data_l.Python==True, 'Y'].value_counts()\njs = stack_data_l.loc[stack_data_l['Java( |>)']==True, 'Y'].value_counts()\nj = stack_data_l.loc[stack_data_l.Javascript==True, 'Y'].value_counts()\n\npie_HQ = ax[0,0].pie(HQ, labels=HQ.index, autopct='%1.1f%%', textprops={'fontsize':10},\n                  shadow=True, explode=tuple(0.2 if i==0 \n                                              else 0.1 if i==1\n                                              else 0.05 if i==2 \n                                              else 0 for i,v in enumerate(HQ)))\n\npie_LQ = ax[0,1].pie(LQ, labels=LQ.index, autopct='%1.1f%%', textprops={'fontsize':10},\n                  shadow=True, explode=tuple(0.2 if i==0 \n                                              else 0.1 if i==1\n                                              else 0.05 if i==2 \n                                              else 0 for i,v in enumerate(LQ)))\n\npie_LQC = ax[0,2].pie(LQC, labels=LQC.index, autopct='%1.1f%%', textprops={'fontsize':10},\n                  shadow=True, explode=tuple(0.2 if i==0 \n                                              else 0.1 if i==1\n                                              else 0.05 if i==2 \n                                              else 0 for i,v in enumerate(LQC)))\n\npie_py = ax[1,0].pie(py, labels=py.index, autopct='%1.1f%%', textprops={'fontsize':10},\n                          shadow=True)\n\npie_js = ax[1,1].pie(js, labels=js.index, autopct='%1.1f%%', textprops={'fontsize':10},\n                          shadow=True)\n\npie_j = ax[1,2].pie(j, labels=j.index, autopct='%1.1f%%', textprops={'fontsize':10},\n                          shadow=True)\n\nax[0,0].set_title('HQ topics')\nax[0,1].set_title('LQ_EDIT topics')\nax[0,2].set_title('LQ_CLOSE topics')\nax[1,0].set_title('Python')\nax[1,1].set_title('JavaScript')\nax[1,2].set_title('Java')","386b67dc":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.multioutput import MultiOutputClassifier\nimport xgboost\nfrom sklearn.model_selection import ParameterGrid\nimport sklearn\nimport eli5\nfrom eli5.lime import TextExplainer","826faa08":"import tensorflow as tf\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing import text\nfrom sklearn.metrics import classification_report\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","c36b898f":"import re","6a9dd3fa":"stack_data_f['text'] = stack_data_f.Title+': '+stack_data_f.Body\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\nstack_data_f.text = stack_data_f.text.apply(clean_text)\n","9a8c355c":"# Best model","665ced6c":"#Split data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(stack_data_f.text.iloc[:50000]\n                                                    , stack_data_f.Y.iloc[:50000], test_size=0.3, random_state=0 )\n\n\n#Try different classifiers\nclassifiers = [\n    LogisticRegression(C=1),\n    MultinomialNB(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier()]\n\nClassifiers_results = pd.Series(name='results')\n\nfor cls in classifiers:\n    text_clf = Pipeline([\n        ('vect', TfidfVectorizer(ngram_range=(1,1))),\n        ('clf', cls)])\n\n    text_clf.fit(X_train, y_train)\n    predicted = text_clf.predict(X_test)\n    print(str(cls) +': ' + str(text_clf.score(X_test, y_test)))","dd7c3547":"def GridSearch(cls, parameters, X, y):\n    \"\"\"Try different parameters. Don't use CV because of huge train dataset\"\"\"\n    \n    results = pd.DataFrame()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n    \n    for ind,par in enumerate(list(ParameterGrid(parameters))):\n        text_clf = Pipeline([\n                ('vect', TfidfVectorizer()),\n                ('clf', classifier(**par))])\n        text_clf.fit(X_train, y_train)\n        predicted = text_clf.predict(X_test)\n        results.loc[str(par),'results'] = text_clf.score(X_test, y_test)\n        results.loc[str(par),'parameters'] = ind\n        ind_best = results.sort_values(by=['results'], ascending=False).iloc[0,1]\n\n    return list(ParameterGrid(parameters))[int(ind_best)]\n\nclassifier = LogisticRegression\nparameters = {\n    'solver':['saga'],\n    'C': [1, 1.5, 2],\n    'penalty': ['l1', 'l2']\n }\n\nresults = GridSearch(classifier, parameters, stack_data_f.text.iloc[:5000], stack_data_f.Y.iloc[:5000])\nresults","7f247193":"classifier = LogisticRegression(**results)\n\ntext_clf = Pipeline([\n                ('vect', TfidfVectorizer()),\n                ('clf', classifier)])\ntext_clf.fit(X_train, y_train)\npredicted = text_clf.predict(X_test)\ntext_clf.score(X_test, y_test)","f9e34881":"te = TextExplainer(random_state=0)\nte.fit(stack_data_f.text.iloc[:50000][0], text_clf.predict_proba)\nte.show_prediction(target_names= stack_data_f.Y.unique().tolist())","635bc490":"Tags = stack_data_f.Tags.str.split('><',expand=True)\nTags = Tags.apply(lambda x: x.str.replace('<|>',''))\n\nf_tags = Tags[0]\ns_tags = Tags[1].fillna(f_tags)\nt_tags = Tags[2].fillna(f_tags)\n\nf_stack_data_f = stack_data_f.text\ns_stack_data_f = stack_data_f.text\nt_stack_data_f = stack_data_f.text\n\n\"\"\"\n# If you want to do multiouput classifier, join the three previous columns\nf_s_tags = pd.DataFrame({'0':f_tags,'1':s_tags})\nbinarizer = MultiLabelBinarizer()\nf_s_tags = binarizer.fit_transform(f_s_tags.values)\"\"\"","99552508":"#Split data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(f_stack_data_f.iloc[:10000]\n                                                    , f_tags[:10000], test_size=0.3, random_state=0 ) # f_s_tags[:10000,:]\n\n\n#Try different classifiers\nclassifiers = [\n    DecisionTreeClassifier(random_state=0),\n    RandomForestClassifier(random_state=0)]\n\nfor cls in classifiers:\n    text_clf = Pipeline([\n        ('vect', TfidfVectorizer()),\n        ('clf', cls)])\n\n    text_clf.fit(X_train, y_train)\n    predicted = text_clf.predict(X_test)\n    print(str(cls) +': ' + str(text_clf.score(X_test, y_test)))","592d31fa":"def GridSearch(cls, parameters, X, y):\n    \"\"\"Try different parameters. Don't use CV because of huge train dataset\"\"\"\n    \n    results = pd.DataFrame()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n    \n    for ind,par in enumerate(list(ParameterGrid(parameters))):\n        text_clf = Pipeline([\n                ('vect', TfidfVectorizer()),\n                ('clf', classifier(**par))])\n        text_clf.fit(X_train, y_train)\n        predicted = text_clf.predict(X_test)\n        results.loc[str(par),'results'] = text_clf.score(X_test, y_test)\n        results.loc[str(par),'parameters'] = ind\n        ind_best = results.sort_values(by=['results'], ascending=False).iloc[0,1]\n        \n    return list(ParameterGrid(parameters))[int(ind_best)]\n    \n#Try different parameters\nclassifier = RandomForestClassifier\n\nparameters = {\n    'random_state': [0],\n    'max_features': [1000, 2000, 3000],\n    'n_estimators': [150, 200, 300],\n }\n\nresults = GridSearch(classifier, parameters, f_stack_data_f.iloc[:5000], f_tags[:5000])\nresults","747adab7":"#Split data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(f_stack_data_f.iloc[:10000]\n                                                        , f_tags[:10000], test_size=0.3, random_state=0)\n\nclassifier = RandomForestClassifier(**results)\n\ntext_clf = Pipeline([\n            ('vect', TfidfVectorizer()),\n            ('clf', classifier)])\n\ntext_clf.fit(X_train, y_train)\npredicted = text_clf.predict(X_test)\nprint('score :' + str(text_clf.score(X_test, y_test)))","a1257936":"text_clf.predict([f_stack_data_f.iloc[32500]])","b83a2164":"f_tags[32500]","e735e72c":"MAX_FEATURES = 20000\nEPOCHS = 20\nBATCH_SIZE = 20","466c97e2":"stack_data_f['text'] = stack_data_f.Title+': '+stack_data_f.Body\n\ndef clean_text(text):\n    text = text.lower()\n    text = BeautifulSoup(text,'html.parser').text\n    text = text.replace('\\n', '').replace('\\r\\n', '').replace('\\r', '').replace(\"\\'\", '')\n    return text\nstack_data_f.text = stack_data_f.text.apply(clean_text)","74a54e04":"f_s_tags = pd.DataFrame({'0':f_tags,'1':s_tags})\ntest = f_s_tags.copy()\nencoder = LabelEncoder()\nencoder.fit(pd.concat([test.iloc[:,0],test.iloc[:,1]],ignore_index=True)) # Transform columns of tag into integers\ntest.iloc[:,0] = encoder.transform(test.iloc[:,0])\ntest.iloc[:,1] = encoder.transform(test.iloc[:,1])\n\ndf = sklearn.utils.shuffle(pd.DataFrame({'text':stack_data_f.text, 'Tags1':test.iloc[:,0], 'Tags2':test.iloc[:,1]}), random_state=0) # Shuffle \nY1 = df.Tags1\nY2 = df.Tags2\ntext_stack = df.text\nY1 =  np_utils.to_categorical(Y1) # Transform integers into binary output. exemple: Let's [1,2,3] be our output, this vector become [[1,0,0],[0,1,0],[0,0,1]]\nY2 =  np_utils.to_categorical(Y2)\n\nX_train = text_stack.values[:50000]\nX_test = text_stack.values[50000:55000]\ny_train1 = Y1[:50000]\ny_test1 = Y1[50000:55000]\ny_train2 = Y2[:50000]\ny_test2 = Y2[50000:55000]","ab83a9b3":"tokens = text.Tokenizer(num_words=MAX_FEATURES, lower=True)\ntokens.fit_on_texts(list(X_train))\nX_train_seq = tokens.texts_to_sequences(X_train)\nX_test_seq = tokens.texts_to_sequences(X_test)\n\nlength = [len(i) for i  in pd.Series(X_train_seq)]\nplt.hist(length)\nprint(np.quantile(length, 0.90))\n# 90% of the questions count more than 230 words \nMAX_LEN = 250\n\nX_train = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='pre')\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='pre')","cac080c6":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n    x = layers.Embedding(MAX_FEATURES, 256)(inputs)\n    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n    x = layers.GlobalMaxPooling1D()(x)\n    outputs = layers.Dense(4970, activation='softmax')(x)\n    outputs2 = layers.Dense(4971, activation='softmax')(x)\n    model = tf.keras.Model(inputs, [outputs,outputs2])\n    model.summary()\n    \n    es_cb = EarlyStopping(monitor='val_loss', min_delta=0,  patience=10, verbose=0, mode='auto')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n    # Momentum permit to our model to cross 'mountains and plateau'\n    SGD = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9) \n    model.compile(loss='categorical_crossentropy', optimizer=SGD ,metrics=[tf.keras.metrics.CategoricalAccuracy()])","0568db76":"#Learning Rate is one of the most important hyperparameter so the following piece of code is a way to find a good LR\nimport keras\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    \n    def __init__(self, K, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n        self.K = K\n        \n    def on_batch_end(self, batch, logs):\n        \n        self.rates.append(self.K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        self.K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n        \n        \ndef bestLearningRate():\n        \n        print(\"\\n\\n********************** Best learning rate calculation ******************\\n\\n\")\n        K = keras.backend\n        model.compile(loss='categorical_crossentropy', optimizer=SGD, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n        expon_lr = ExponentialLearningRate(K,factor=1.0002)\n        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 15, callbacks=[expon_lr])\n        print(\"*************************************************************************\\n\\n\")\n        \n        print(\"********************** Loss as function of learning rate plot displayed ********************\\n\\n\")\n        plt.plot(expon_lr.rates, expon_lr.losses)\n        plt.gca().set_xscale('log')\n        plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n        plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\n        plt.xlabel(\"Learning rate\")\n        plt.ylabel(\"Loss\")\n        \nbestLearningRate()","636cc809":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n    x = layers.Embedding(MAX_FEATURES, 256)(inputs)\n    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n    x = layers.GlobalMaxPooling1D()(x)\n    outputs = layers.Dense(4970, activation='softmax')(x)\n    outputs2 = layers.Dense(4971, activation='softmax')(x)\n    model = tf.keras.Model(inputs, [outputs,outputs2])\n    model.summary()\n    \n    es_cb = EarlyStopping(monitor='val_loss', min_delta=0,  patience=10, verbose=0, mode='auto')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n    # Momentum permit to our model to cross 'mountains and plateau'\n    SGD = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9) \n    model.compile(loss='categorical_crossentropy', optimizer=SGD ,metrics=[tf.keras.metrics.CategoricalAccuracy()])\n\n#training \nhistory = model.fit(X_train, [y_train1, y_train2], batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_test, [y_test1,y_test2]),callbacks = [es_cb, reduce_lr], verbose=1)","b80ddf76":"def tags_pred(test_question):\n    \n    print(test_question)\n    seq = tokens.texts_to_sequences([test_question])\n    padded = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=MAX_LEN, padding='pre')\n    pred = model.predict(padded)\n\n    labels=list(encoder.classes_)\n    pred1 = pred[0][0]\n    pred2 = pred[1][0]\n    for i in range(3): # We get the three most probable tags\n        \n        print('Tags 1 : ' +str(labels[np.argmax(pred1)]) + ' ' + str(pred1[np.argmax(pred1)]))\n        pred1 = np.delete(pred1, np.argmax(pred1), axis=0)\n    for i in range(3):\n\n        print('Tags 2 : ' +str(labels[np.argmax(pred2)]) + ' ' + str(pred2[np.argmax(pred2)]))\n        pred2 = np.delete(pred2, np.argmax(pred2), axis=0)\n    ","ade43df2":"labels=list(encoder.classes_)\nprint('Tag 1 : ' + str(np.argmax(Y1[56311]))+ ' ' + str(labels[np.argmax(Y1[56311])]))\nprint('Tag 2 : ' + str(np.argmax(Y2[56311]))+ ' ' + str(labels[np.argmax(Y2[56311])]))\ntags_pred(text_stack.iloc[56311])","1148ac9e":"# ------------------------------------------------------------------------","5688ffc9":"Explanations: The following tab gives us some explan","9cda4e5e":"We will try to predict the first tags columns # 2 columns is to long","bdfdea6b":"The goal of this part is to make 2 kind of predictions: \n- Classification on Y thanks to title and body topics -> Single classification\n- Classification on Tags thanks to title and body topics -> Multiclass classification\n\nFor this two tasks, we will compare deep learning and other machine learning classificators.\nBefore that, Preprocessing need to be done on body topics to remove tags.","e9ccc715":"# 60k Stack Overflow Questions with Quality Rating ","b18b961d":"Text explanations","f9afee4f":"- val_loss = val_dense_loss + val_dense_1_loss\n- val_dense_1_categorical_accuracy --> val_dense_1_loss\n- val_dense_categorical_accuracy --> val_dense_loss\n\nAround 60% of accuracy for the first column and 30 for the second","3d926fa2":"# If you liked this notebook, please upvoted it!","7f5a2cbd":"We have to define some hyperparameters before start running our model\nFirst:","acbd8877":"#### Conclusion\n\nThe results of Y predictions are not good at all, that's why a deep learning model could be usefull here","1cc62e7b":"### Others functions Usefull","e6ba90cd":"we have to clean our text and we add the title, it could have usefull data","02ae7282":"### To continue <a id=\"g\"><\/a>\n\nIn order to go further on this languages study, we now get the proportions of each language on total HQ, LQ_EDIt and LQ_CLOSE topics over 2016 to 2020.\n- HQ: 20000 lines\n- LQ_EDIT: 19998 lines\n- LQ_CLOSE: 19999 lines","396445e9":"### Learning rate","996eb2c7":"#### Conclusion:\n\nPython, Javascript and java are the three most questioned languages. That would be a shortcut to conclude on the most famous or the more used language only with this study. Moreover, the data have not been taken randomly. However, it gives us an idea of the IT languages landscape. The last graph is very interesting and shows us the increasing of Python.","5a562984":"# Let's predict<a id=\"h\"><\/a>","80be9dd3":"Results","514d6c59":"The text need to be transform into sequences and then they are set at the same size with adding 0 for the sorter one and cut parts of text for the longer one.","1777b42e":"To compare with our previous random forest model, we will predict just the two first tags columns however.","3924328d":"Libr\u00e9aries related to TensorFlow (dl analysis coming soon)","0e7e4318":"#### Conclusion","11294118":"## Summary\n\n   - [Vizualisation Libraries import](#a)\n   - [Data import](#b)\n   - [Cleaning of the data](#c)\n   - [Let's analyse the data](#d)\n     - [Number of questions](#e)\n     - [Languages used](#f)\n     - [To continue](#g)\n   - [Let's predict](#h)\n      - [Import Libraries](#i)\n      - [Preprocessing](#j)\n     - [Predict Y(ML tools)](#k)\n     - [Predict Tags(ML tools)](#l)\n     - [Predict Tags(DL tools)](#m)\n     ","f0cc7ecd":"# Cleaning of the data<a id=\"c\"><\/a>","7332c401":"We refind The three languages Python, JavaScript and Java. They are most composed by LQ_CLOSE.","affbe819":"#### Conclusion:\nWe have more questions from 2016 than 2020. The algorithme could be influenced by old questions, we need to be careful about the influence of the time on our predictions.","b5ec2d5a":"Exemple of prediction","051b22dc":"#### Languages used. <a id=\"f\"><\/a>\n\nIt could be interesting to know which language have the more questions on it (Not the most famous or the more used). To do it, i used a list of arbitrary chosen languages: C, C++, C#, Java, SQL, Java, Script, Python, Ruby, PHP, HTML\/CSS, R, MATLAB.\n\nIn total there are 159871 tags (tag -> <...>) over 60k topics and 43746 are refering to a language in the list above.\n\nFrom discussion, we know that each sentence hasn't been taken randomly. they have been sorted from highest rated to lowest then selected. The following graphs can't give us reliable results on the most questioned language. It can just give us a quick view on the top 3: Python, JavaScript and Java.\n\nIt's also interesting to know the proportions of each languages over time on the 43746 references. We represented them by 5 KPI. The last graph gathered the 5 kpi in lineplot: It's easiest to see the evolution of a language.\n\n\n\nThis kind of study can be done on other subject like the different IDE used: Visual-studio etc... lang_list need to be modified.","689e8c0c":"We delete bad data found in the table.","ae2a0c96":"### Preprocessing<a id=\"j\"><\/a>","cc795387":"## Predict Tags (Machine learning tools)<a id=\"l\"><\/a>","47eeb471":"#### Results","2c65ee96":"Best parameters","01972539":"## Data\n\nThis is an original dataset, made publicly available for researchers.\n\nWe collected 60,000 Stack Overflow questions from 2016-2020 and classified them into three categories:\n\nHQ: High-quality posts with 30+ score and without a single edit.\nLQ_EDIT: Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.\nLQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\nNotes:\n\nQuestions are sorted according to Question Id.\nQuestion body is in HTML format.\nAll dates are in UTC format.\nPlease let me know of any additional information that you may require.\n\n## Task\n\n- Which Stack Overflow questions should be closed?\n- Predict tags according to text and title.\n","632cff2f":"### Import libraries<a id=\"i\"><\/a>","3b0209d1":"#### Best parameters","37e0b65f":"200 NAN in Y to delete.","8dcd112c":"The lowest point give you the LR to choose.","d67f6166":"### Model","6a1af0f6":"# Vizualisation Libraries import<a id=\"a\"><\/a>","c7f4ff52":"## Predict Y (Machine learning tools)<a id=\"k\"><\/a>","0688cd14":"#### Number of questions ? <a id=\"e\"><\/a>\n\nIt's interesting to know the number of question according to time. To do it:\n\n- Column date_month is created to analyse the number of questions less deeper.\n- Stack_data_gb_d: Groupby object created to calculate the number of questions per day.\n- Stack_data_gb_m: Other groupby object created to calculate the number of question per month.\n- Stack_data_gb_y: Other groupby object created to calculate the number of questions per year.\n- It would be a litle bit messy to show the number of questions per day, instead of that we showed the mean, min and max number of questions asked per days over months.\n- We plot the result thanks to matplotlib.\n- MajorLocator, majorFormatter, minorLocator are used to defined more precise yticks.\n- Fill_between is used to fill the area between the max and min plot.\n- Autolabel function is the same than the matplotlib doc on bar labels.","6b3c9879":"# ---------------------------------------------------------------------------------------------------------------","8fd64bc4":"To do it, we can use classifiers which support multilabel output as :\n\n- sklearn.tree.DecisionTreeClassifier\n- sklearn.tree.ExtraTreeClassifier\n- sklearn.ensemble.ExtraTreesClassifier\n- sklearn.neighbors.KNeighborsClassifier\n- sklearn.neural_network.MLPClassifier\n- sklearn.neighbors.RadiusNeighborsClassifier\n- sklearn.ensemble.RandomForestClassifier\n- sklearn.linear_model.RidgeClassifierCV\n\nElse, we can use sklearn.multioutput.MultiOutputClassifier, if you want to use classifiers which do not natively support multi-target classification.\n\nSee https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier for more details","9d9b2843":"Thanks\nGa\u00e9tan","6a5dc0b6":"# Data import<a id=\"b\"><\/a>","a7a415a8":"# Let's analyse the data<a id=\"d\"><\/a>","3c606361":"Libraries related to scikit learning tools","1a0fc803":"#### Best model","367b3807":"## Deep learning <a id=\"m\"><\/a>"}}