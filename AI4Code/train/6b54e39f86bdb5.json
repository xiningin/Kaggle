{"cell_type":{"53b61042":"code","193e8700":"code","d2f541ac":"code","7bf8b63e":"code","620b61c6":"code","f0ec881b":"code","89d2ccdb":"code","d0cb0a48":"code","88c105e8":"code","f04a43cc":"code","3da9ebc0":"code","8d3f9f7f":"code","57d5aa9b":"code","984e60db":"code","c7adaa80":"markdown","9a4f843f":"markdown","f913340d":"markdown","fe14d811":"markdown","af5ae7d7":"markdown","9478833a":"markdown","b8465ae9":"markdown","a76d7362":"markdown","ec3fd1db":"markdown","05a99be4":"markdown","321406b5":"markdown","4c64690b":"markdown","f95923fb":"markdown","1acf9c01":"markdown","0d7da4b1":"markdown"},"source":{"53b61042":"import numpy as np \nimport pandas as pd \nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, preprocessing\nfrom keras.callbacks import ReduceLROnPlateau\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport time\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","193e8700":"data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')","d2f541ac":"data.head()","7bf8b63e":"data_as_np = data.to_numpy()\nlabels = data_as_np[:,0].reshape((42000,1)) #To avoid having issues with arrays of np shape (42000,)\npixels = data_as_np[:,1:].reshape((42000,28,28,1)) \/ 255\n\nlabels, pixels = shuffle(labels, pixels)\n\nr = int(np.random.uniform(0,41999))\ndigit=pixels[r,:,:,0] * 255 #pyplot expects a 2D array for a gray scale image\nplt.imshow(digit, cmap=plt.cm.gray)   ","620b61c6":"X_train, X_CV, y_train, y_CV = train_test_split(pixels, labels, test_size = 0.08 )\n\nprint(f'Training data: \\nX_train shape : {X_train.shape} and y_train shape : {y_train.shape}\\n')\nprint(f'Cross validation data: \\nX_CV shape : {X_CV.shape} and y_CV shape : {y_CV.shape}')","f0ec881b":"inputs = keras.Input(shape = (28,28,1))\n\nconv1 = layers.Conv2D(16, (8,8), strides = (1,1), activation = 'relu', padding='same')\n\nx = conv1(inputs)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(32, (5,5), strides = (1,1), activation = 'relu', padding='same')(x)\nx = layers.MaxPool2D(pool_size=(2,2))(x)\n\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(32, (3,3), strides = (1,1), activation = 'relu', padding='same')(x)\nx = layers.MaxPool2D(pool_size=(2,2))(x)\nx = layers.Dropout(rate = 0.1)(x)\n\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(64, (3,3), strides = (1,1), activation = 'relu', padding='same')(x)\nx = layers.MaxPool2D(pool_size=(2,2))(x)\n\nx = layers.Flatten()(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dense(120, activation = 'relu')(x)\nx = layers.Dropout(rate = 0.2)(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dense(84, activation = 'relu')(x)\n\noutputs = layers.Dense(10, activation = 'softmax')(x)\n\nmodel = keras.Model(inputs = inputs, outputs = outputs, name=\"MNISTv2\")\n\n\n\nmodel.summary()\n\n\nmodel.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n             optimizer = keras.optimizers.Adam(lr = 0.001),\n             metrics = ['accuracy'])","89d2ccdb":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.1, \n                                            min_lr=0.00001)","d0cb0a48":"datagen = preprocessing.image.ImageDataGenerator(\n    featurewise_std_normalization=False,\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=False,\n    zca_whitening=False)\n\ndatagen.fit(X_train)\n\nepochs=2\nbatch_size=10000\n# fits the model on batches with real-time data augmentation:\nmodel.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                    steps_per_epoch=len(X_train) \/\/ batch_size, epochs=epochs,\n                    validation_data=(X_CV, y_CV),\n                    callbacks=[learning_rate_reduction])\n","88c105e8":"test = model.evaluate(X_CV, y_CV)","f04a43cc":"test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntest_as_np = test.to_numpy()\n\npixels_test = test_as_np.reshape((28000,28,28,1)) \/ 255\n\ntic = time.time()\nresult = model.predict(pixels_test, batch_size=32)\ntoc = time.time()\n\nelapsed = (toc-tic)*1000\nprint(f'The inference time for the 28 000 examples batch size is {elapsed} ms')\n\n\nresult = result.argmax(axis=1).reshape((28000,1))","3da9ebc0":"i = int(np.random.uniform(0,27999))\n\ntic = time.time()\ninference = model.predict(pixels_test[i:i+1,:,:,:])\ntoc = time.time()\n\nelapsed = (toc-tic)*1000\n\nprint(f'The result is {inference.argmax(axis=1)} and the inference time is {elapsed} ms')\n\n#print(np.average(pixels_test[i:i+1,:,:,:]))\n#print('this is the shit \\n',np.average(pixels_test[i,:,:,0])) #Theses two lines can be used to make sure we are looking at the same example.\n\ndigit=pixels_test[i,:,:,0] * 255 \nplt.imshow(digit, cmap=plt.cm.gray)  ","8d3f9f7f":"a = np.arange(1,28001).reshape((28000,1))\ntest_result = np.concatenate((a, result), axis = 1)\n\nprint(test_result)","57d5aa9b":"np.savetxt('test.csv', test_result, fmt='%i', delimiter=',', header=\"ImageId,Label\", comments='')","984e60db":"check = pd.read_csv('\/kaggle\/working\/test.csv')\ncheck.head(10)","c7adaa80":"## Time to build the model v2\n\nFor this second version of my submission of the MNIST competition i will build a CNN similar to the LeNet5. Using tensorflow and Keras API. In the second model, I tweaked a bit the architecture to get better performances. I added dropout layers for example.","9a4f843f":"## Loading the data\n\nI start by loading the data onto the notebook by using Pandas as the format is CSV.","f913340d":"Then I quickly visualize the five first rows of the data using the head() method from pandas","fe14d811":"## Training the model using Keras data augmentation API\n\nIn this second version, I also added a data augmentation to increase the training of my model. I have done so with the Keras preprocessing API. I train the model with real-time data augmentation.","af5ae7d7":"## Load the test set to prepare the submission","9478833a":"# CNN for MNIST digits classification (99.285% test accuracy, top 25%)\n\nIn this notebook I will use a modified LeNet5 implementation using tensorflow keras API for the problem of handwritten digits classifcation. This is tribute to he man who created the data set and the performance of this CNN is very satisfying for not an acceptable number of parameters.","b8465ae9":"## Split the data \n\nWe want to split the data we have into a training set and a crossing validation set. I will not use a test set in this application as we do not have a lot of data and we want to maximise the potential of what we have.\n\nThe split ratio is determine by the input test_size.\n","a76d7362":"## Remarks\n\nNow we see that the t dev set accuracy is better than our train set. Thus that mean that we can fit better the training set. There multiple solutions for this:\n* Train a deeper network\n* Train for longer\n* Generate even more data","ec3fd1db":"## References\n\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, november 1998.","05a99be4":"## Convert the panda dataframe into a numpy array\n\nConvert the panda dataframe to a numpy array. Shuffle the data using a sklearn tool and show a random example image.","321406b5":"## Prediction on a single example\n\nWe can compute the inference time on a single example and check if our model work properly. Also, it is time to enjoy our work and see that we have done a good job at classifying handwritten numbers.","4c64690b":"## Save as a CSV file\n\nFinally, we have the data well ordered and everything is ready to create and submit the CSV file.","f95923fb":"## Cross validate the model on an unknown set\n\nNow that our model is trained with a good accuracy on the training dataset. I improved the performance on the dev set thanks to the added regularisation, droupout layers, as suggested in the previous notebook.","1acf9c01":"## Learning rate annealer\n\nI use the Keras method ReduceLROnPlateau to reduce the learning rate when the accuracy on the validation set doesn't increase anymore.","0d7da4b1":"## Postprocessing\n\nWe need to convert the results in a csv files for submission for the kaggle competition. The exact shape should have the id of the test exmaple and the class predcited."}}