{"cell_type":{"dc4aef14":"code","d25764b0":"code","faf19fe3":"code","e083dad5":"code","5fe9282f":"code","e58dece4":"code","e214c392":"code","3c732572":"code","ea1abc82":"code","f0685966":"code","70af1931":"code","02ee69b0":"code","374a7d5e":"code","382ec0be":"code","6c4c2146":"code","f417fdef":"code","774665b1":"code","e6af3005":"code","d6db3b6b":"code","f3f32ec4":"code","f07dd237":"code","0cb1c082":"code","ee9a5415":"code","0f0bba02":"code","db171738":"code","46a7cefb":"code","910edf4c":"code","b02a3f70":"code","01a4e2d7":"code","b3c967c2":"markdown","c4eed4ca":"markdown","3069e224":"markdown","bb81f761":"markdown","66f88a70":"markdown","c0bf0bb6":"markdown","39bc7923":"markdown","cccf7e17":"markdown","adbd3a06":"markdown","23786236":"markdown","3c259a82":"markdown","84df0f58":"markdown","9b8a9425":"markdown"},"source":{"dc4aef14":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n%matplotlib inline","d25764b0":"Kaggle=1\nif Kaggle==0:\n    train=pd.read_csv(\"train.csv\")\n    test=pd.read_csv(\"test.csv\")\n    sample_sub=pd.read_csv(\"sample_submission.csv\")\nelse:\n    train=pd.read_csv(\"..\/input\/train.csv\")\n    test=pd.read_csv(\"..\/input\/test.csv\")\n    sample_sub=pd.read_csv(\"..\/input\/sample_submission.csv\")","faf19fe3":"print(f'Train has {train.shape[0]} rows and {train.shape[1]} columns' )\nprint(f'Test has {test.shape[0]} rows and {test.shape[1]} columns' )","e083dad5":"train.head()","5fe9282f":"train['target'].value_counts()","e58dece4":"from sklearn.model_selection import train_test_split,KFold, cross_val_score, GridSearchCV,StratifiedKFold,StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport feather\nfrom bayes_opt import BayesianOptimization","e214c392":"train_model=train.drop('ID_code',axis=1)\ntest_model=test.drop('ID_code',axis=1)","3c732572":"X=train_model.drop('target',axis=1)\nY=train_model['target']","ea1abc82":"train_x,valid_x,train_y,valid_y=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=100)","f0685966":"print(f'Training has {train_x.shape[0]} rows and {train_x.shape[1]} columns' )\nprint(f'Validation has {valid_x.shape[0]} rows and {valid_x.shape[1]} columns' )","70af1931":"train_y.value_counts()","02ee69b0":"valid_y.value_counts()","374a7d5e":"# folds = StratifiedKFold(n_splits=5,shuffle=True,random_state=40)","382ec0be":"feature_name=[f for f in train_x.columns if f not in ['target']]\nmean_auc=0.0\nN_SPLITS=5","6c4c2146":"train_x.shape,train_y.shape","f417fdef":"# train_x_sample=train_x.iloc[1:1000,]\n# train_y_sample=train_y.iloc[1:1000]","774665b1":"# train_x_sample.shape,train_y_sample.shape","e6af3005":"# def rf_model(**params):\n#     params['min_samples_leaf']=int(params['min_samples_leaf'])\n#     params['max_features']=int(params['max_features'])\n#     params['max_depth']=int(params['max_depth'])\n#     params['n_estimators']=int(params['n_estimators'])\n   \n    \n#     test_pred=np.zeros(train_x.shape[0])\n    \n#     for n_folds,(train_idx,valid_idx) in enumerate(folds.split(train_x,train_y)):\n#         x_train,x_valid=train_x.iloc[train_idx],train_x.iloc[valid_idx]\n#         y_train,y_valid=train_y.iloc[train_idx],train_y.iloc[valid_idx]\n#         clf=RandomForestClassifier(**params,random_state=100,n_jobs=-1,verbose=True)\n#         clf.fit(x_train,y_train)\n#         y_pred_proba=clf.predict_proba(x_valid)\n        \n#         test_pred[valid_idx]=clf.predict_proba(x_valid)[:,1]\n        \n#     gc.collect()\n        \n#     return roc_auc_score(y_valid,test_pred[valid_idx])\n    ","d6db3b6b":"# params ={'n_estimators':(100,1000),\n#           'max_depth':(10,100),\n#           'min_samples_leaf':(1,10),\n#          'max_features':(1,10)}","f3f32ec4":"# bo = BayesianOptimization(rf_model, params)\n# bo.maximize(init_points=5, n_iter=5)","f07dd237":"# bo.max","0cb1c082":"rf_oof_preds=np.zeros(train_x.shape[0])\nrf_sub_preds=np.zeros(test_model.shape[0])","ee9a5415":"folds=StratifiedShuffleSplit(n_splits=5,random_state=100)","0f0bba02":"\nauc_score=[]\nimportance=pd.DataFrame()\n%time\nfor n_folds,(train_idx,valid_idx) in enumerate(folds.split(train_x,train_y)):\n    x_train,x_valid=train_x.iloc[train_idx],train_x.iloc[valid_idx]\n    y_train,y_valid=train_y.iloc[train_idx],train_y.iloc[valid_idx]\n    clf=RandomForestClassifier(n_estimators=720 ,max_depth= 50,min_samples_leaf=9 ,max_features=2 ,n_jobs=-1,random_state=100,verbose=True)\n    clf.fit(x_train,y_train)\n    y_preds_proba=clf.predict_proba(x_valid)\n    rf_oof_preds[valid_idx]=y_preds_proba[:,1]\n    rf_sub_preds=clf.predict_proba(test_model[feature_name])[:,1]\/folds.n_splits\n    auc_score.append(roc_auc_score(y_valid,rf_oof_preds[valid_idx]))\n    \n    print(\"\\n {} fold ROC AUC Score is : {}\".format(n_folds+1,roc_auc_score(y_valid,rf_oof_preds[valid_idx])))\n    \n    importance['feature']=feature_name\n    importance['gini']=clf.feature_importances_\n    importance['fold']=n_folds+1\n    \nprint(\"\\n Average ROC Score is {}\",np.mean(auc_score))\n    \n    \n    ","db171738":"### https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\nlgb_oof_preds=np.zeros(train_x.shape[0])\nlgb_sub_preds=np.zeros(test_model.shape[0])\nauc_valid=[]\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","46a7cefb":"importances=pd.DataFrame()\nfor fold_idx, (train_ids, valid_ids) in enumerate(folds.split(train_x,train_y)):\n    # Split traninig data set.\n    trn_data = lgb.Dataset(train_x.iloc[train_ids],label=train_y.iloc[train_ids])\n    val_data = lgb.Dataset(train_x.iloc[valid_ids],label=train_y.iloc[valid_ids])\n    ## Building the model:\n    num_rounds=10000\n    clf = lgb.train(param, trn_data, num_rounds, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    # Train estimator.\n    \n    # Prediction and evaluation on validation data set.\n    lgb_oof_preds[valid_ids] = clf.predict(train_x.iloc[valid_ids],num_iteration=clf.best_iteration)\n    # Set feature importances.\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = feature_name\n    imp_df['gain'] = clf.feature_importance()\n    imp_df['fold'] = fold_idx + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    \n    # Prediction of testing data set.\n    lgb_sub_preds += clf.predict(test_model[feature_name],num_iteration=clf.best_iteration)\/ folds.n_splits\n    \n    \n    \n    gc.collect()\nprint(\"Mean AUC: %.5f\" % (roc_auc_score(train_y,lgb_oof_preds)))","910edf4c":"# ### Taken from Oliver's awesome kernel - \n\n# def display_importances(feature_importance_df_):\n#     # Plot feature importances\n#     cols = feature_importance_df_[[\"feature\", \"gain\"]].groupby(\"feature\").mean().sort_values(\n#         by=\"gain\", ascending=False)[:50].index\n    \n#     best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n#     plt.figure(figsize=(8,10))\n#     sns.barplot(x=\"gain\", y=\"feature\", \n#                 data=best_features.sort_values(by=\"gain\", ascending=False))\n#     plt.title('LightGBM Features (avg over folds)')\n#     plt.tight_layout()\n#     #plt.savefig('lgbm_importances.png')\n","b02a3f70":"#  display_importances(importances)","01a4e2d7":"## Blending and submitting\n\nsample_submission = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsample_submission[\"target\"] = (0.4*rf_sub_preds)+(0.6*lgb_sub_preds)\nsample_submission.to_csv(\"blend_submission.csv\", index=False)","b3c967c2":"## Loading the data","c4eed4ca":"Split the data as X and Y for modelling.","3069e224":"Submission,","bb81f761":"## Modelling","66f88a70":"Lets drop the ID Code column.","c0bf0bb6":"### Loading the required libraries","39bc7923":"Since it is an imbalanced dataset , lets try out stratified k fold cross validation and train xgboost model to find out the feature importance.","cccf7e17":"# Santander Customer Transaction Prediction","adbd3a06":"In this compeition , we are asked to predict whether a customer will make transaction in future or not irrespective of the amount of money transfered . It is a binary classification task and we have been provided with anonymised dataset of numeric transactions for this.The binary column **target** is what we need to predict and a string column **ID_code** .Lets begin.","23786236":"We find that the data is numeric with id_code being a character column .The task is to predict the target.Lets check this column.","3c259a82":"A lot of codes and ideas have been inspired from fellow kagglers - Oliver , Bojan , Will Koherson .Due credits to them.","84df0f58":"Since we dont know the description of each of the columns , lets quickly create a random forest model and look at the feature importance .After that we can select the important features alone for modelling.","9b8a9425":"We find that the target column is unbalanced with 179902 values being 0 whereas there are only 20098 rows with value 1 ."}}