{"cell_type":{"6c2addbc":"code","0fc4f949":"code","d88ac5ac":"code","cbc3008f":"code","f887f24a":"code","8468625d":"code","25f4912c":"code","abb15469":"code","574476d6":"code","e6aef93e":"code","c55d3c0c":"code","d17fc470":"code","fe1cc3bc":"code","65b35f68":"code","70d202d1":"code","f6ecb5f7":"code","b9703454":"code","73e61f13":"code","ec815722":"code","71b0f401":"code","ba423be9":"code","eb753cbc":"code","1013c289":"code","c1f3e209":"code","ca4c0b24":"code","f1c27bd7":"code","245ac647":"code","51ef5b0f":"code","ebfba736":"code","e652d424":"code","c35b8710":"code","b75fe8e7":"code","351be762":"code","d9722375":"code","702765b5":"code","db579b45":"code","025f6756":"code","1032449d":"code","81bdc9aa":"code","d6829331":"code","f93e0b67":"code","1e436b10":"code","8cc4e5a6":"code","f53e321a":"code","b5c20eae":"code","3c00ea3d":"code","c4f18f9f":"code","99541479":"code","869d6036":"code","fd27a90d":"code","6b7e09fd":"code","30fde090":"code","41624809":"code","14d31795":"code","776a69ac":"code","d6600bba":"code","6ad4a984":"code","d9ec3dbd":"code","954e54bf":"code","62918345":"code","17e8b4c5":"code","53dc8b08":"code","272f5509":"code","904b31c3":"markdown","6b8161aa":"markdown","b3b6dd19":"markdown","4ef4149d":"markdown"},"source":{"6c2addbc":"!pip install imutils","0fc4f949":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport gc\nimport pandas as pd\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.layers import ZeroPadding2D,UpSampling2D,ThresholdedReLU,Conv2DTranspose, Cropping2D, DepthwiseConv2D,add, Activation, LeakyReLU, Dense, Conv2D, GlobalMaxPooling2D , MaxPooling2D, Flatten,Concatenate, Input, Dropout, BatchNormalization, GlobalAveragePooling2D, SeparableConv2D, AveragePooling2D\n\n\nfrom tensorflow.keras.optimizers import RMSprop, Adam, SGD \nfrom tensorflow.keras.metrics import TruePositives, FalsePositives, TrueNegatives, FalseNegatives, AUC, BinaryAccuracy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.activations import softsign\nfrom keras.initializers import Constant\n# import gdown\nimport cv2\nimport math\nfrom tensorflow.keras.regularizers import l1, l2\nfrom PIL import Image\nimport shutil\nfrom tqdm import tqdm\nimport os\nfrom tensorflow.keras.applications import InceptionV3, ResNet101, VGG16\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import LabelEncoder\nfrom skimage import data, img_as_float\nfrom skimage import exposure\nfrom skimage.io import imsave\nfrom imutils import perspective\nfrom imutils import contours\nimport imutils\nfrom scipy.spatial import distance as dist\ndef midpoint(ptA, ptB):\n    return ((ptA[0] + ptB[0]) * 0.5, (ptA[1] + ptB[1]) * 0.5)\nimport matplotlib.gridspec as gridspec\nfrom sklearn.model_selection import KFold","d88ac5ac":"TRAIN_DIR = \".\/train\/\"\nTEST_DIR = \".\/test\/\"\nHAIR_ADD_IMAGE = False\nLINZE = False\nCROP = True\nCROP_SIZE=15 #15\nSHAPE = 256\nSHAPE_RESIZE = 150\nVAR1 = True\nBATCH_SIZE=64\nEPOCH = 30\ntrain_dir = \"..\/input\/jpeg-melanoma-256x256\/train\"\ntest_dir = \"..\/input\/jpeg-melanoma-256x256\/test\"\nos.mkdir(TRAIN_DIR)\nos.mkdir(TEST_DIR)\nDEVICE= \"TPU\"","cbc3008f":"shutil.rmtree(TRAIN_DIR)\nshutil.rmtree(TEST_DIR)","f887f24a":"def HAIR_SORRY_REMOVE(image, clip_hist_percent=1):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    kernel = cv2.getStructuringElement(1,(17,17))\n    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n    _,threshold = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n    final_image = cv2.inpaint(image,threshold,1,cv2.INPAINT_TELEA)\n    return (final_image)\ndef step_assimetry(img):\n    img = HAIR_SORRY_REMOVE(img)\n#     img = cv2.bilateralFilter(img,50,15,15)\n    \n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    ret, thresh =     cv2.threshold(gray,100,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    \n    kernel = np.ones((2,2),np.uint8)\n    opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)\n    kernel = np.ones((2,2),np.uint8)\n    dilate = cv2.erode(opening,kernel,iterations=3)\n#     dilate = cv2.dilate(opening,kernel,iterations=3)\n    blur = dilate\n   \n#     blur = cv2.blur(opening,(15,15))\n    ret, thresh =     cv2.threshold(blur,100,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    \n    contours, hierarchy =     cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n#     images = cv2.drawContours( img, contours, -1, (255,0,0), 3, cv2.LINE_AA, hierarchy, 1 )\n#     plt.imshow(images)\n    \n    delta = 80\n#     if len(contours)<3:\n#         delta = 40\n    S = list()\n#     concat = np.concatenate(contours)\n# hulls = cv2.convexHull(concat)\n    for c in contours:\n        box = cv2.minAreaRect(c)\n        box = cv2.cv.BoxPoints(box)  if imutils.is_cv2() else cv2.boxPoints( box)\n        box = np.array(box, dtype=\"int\")\n        box = perspective.order_points( box)\n        center = img.shape[0]\/2\n       \n        ( tl, tr, br, bl) = box\n        (centerX, centerY) = midpoint( tl, br)\n        (midlx, midly) = midpoint( tl, bl)\n#         print(midlx,\"---\")\n        if (tl[0]>0 and tr[0]<250 and br[1]<255 and centerX>center-delta and centerX<center+delta and centerY>center-delta and centerY<center+delta) or (midlx>0 and br[0]<252 and centerX>center-delta and centerX<center+delta  and centerY>center-delta and centerY<center+delta):\n#             if (centerX>center-delta and centerX<center+delta) and (centerY>center-delta and centerY<center+delta):\n                S.append(c)\n    if len(S)>0:\n        cont = np.vstack([S[i] for i in range(len(S))])\n        hull = cv2.convexHull(cont)\n        S=hull\n    \n#     print([S[i] for i in len(S)])\n    return S","8468625d":"COUNTLEN = 10","25f4912c":"def get_Assimetry2(im):\n    example = [(105, 24), (78, 84), (93, 90), (105, 112), (110, 148), (93, 179), (76, 228), (44, 225), (112, 255), (184, 252), (217, 255), (255, 224), (255, 123), (207, 110), (209, 56), (153, 78)] \n    result = im.copy()\n    orig2 = im.copy()\n    img = im.copy()\n#     img = segmentation_color3(img.copy())\n   \n    S = step_assimetry(img)\n    shape_orig = orig2.shape[0]\n    if len(S)>0:        \n#         cnt = max(S, key=cv2.contourArea)\n        cnt = S\n    else:\n        return \"err\", result, 200, 200\n    box = cv2.minAreaRect(cnt)\n    box = cv2.cv.BoxPoints(box)  if imutils.is_cv2() else cv2.boxPoints( box)\n    box = np.array(box, dtype=\"int\")\n    box = perspective.order_points( box)\n    orig = cv2.drawContours(result, [box.astype(\"int\")] , -1 , ( 0 , 255 , 0 ) , 2)\n    for ( x, y)  in box:\n        cv2.circle(result, (int(x), int(y)) , 5 , ( 0 , 0 , 255 ) , -1)\n    ( tl, tr, br, bl) = box\n    d1 = dist.euclidean(tl, tr)\n    d2 = dist.euclidean(tl, bl)\n    ( centerXX, centerYY) = midpoint( tl, br)\n    (midlx, midly) = midpoint( tl, bl)\n#     print(int(midly),int(midlx))\n    cv2.circle(result,(int(midlx), int(midly)),15,(255,45,12),2)\n    start = (int(centerXX), int( centerYY))\n    count_len = COUNTLEN\n    shape = cnt.shape[0]\n    run = int(shape\/count_len)\n    if shape<count_len:\n        run = 1\n    LENGTH = list()\n    sss = []\n    previos = start\n    \n    for i,r in enumerate(range(0,shape,run)):\n        if i<count_len:\n            end = (int(cnt[r][0][0]),int(cnt[r][0][1]))\n            dA = dist.euclidean(start, end)\n            \n            cv2.line(result, start, end,(0, 0, 0), 2)\n            previos = end\n            \n            \n            \n            LENGTH.append(dA)\n        else:\n            break\n    max_LENGTN = max(LENGTH)\n    min_LENGTN = min(LENGTH)\n    if len(LENGTH)<count_len:\n        razn = count_len - len(LENGTH)\n        for i in range(razn):\n            LENGTH.append(0)\n    if max_LENGTN<28 or min_LENGTN<12:\n        cv2.circle(result,(int(midlx), int(midly)),20,(255,45,12),9)\n        return 250, result, d1,d2\n#         max_LENGTN=55#80\n#     max_LENGTN+=10#15\n    top_crop = centerYY-max_LENGTN\n    bottom_crop = centerYY+max_LENGTN\n    left_crop = centerXX-max_LENGTN\n    right_crop = centerXX+max_LENGTN\n    if top_crop<0:\n        top_crop=0\n    if bottom_crop>256:\n        bottom_crop=256\n    if left_crop<0:\n        left_crop=0\n    if right_crop>256:\n        right_crop=256\n#     result = result[int(top_crop):int(bottom_crop),int(left_crop):int(right_crop),:]\n#     result = cv2.bilateralFilter(result,9,100,100)\n#     result = segmentation_color3(result)\n#     print (sss)\n    return LENGTH, result, d1,d2 #return length border","abb15469":"train = pd.read_csv(\"..\/input\/jpeg-melanoma-256x256\/train.csv\")\ntest =  pd.read_csv(\"..\/input\/jpeg-melanoma-256x256\/test.csv\")","574476d6":"train","e6aef93e":"def constructing_features(DIR,DATAFRAME,save_dir):\n    DATAFRAME[\"pred_pred1\"] = 0\n    DATAFRAME[\"pred_pred2\"] = 0\n    for i, image_name in enumerate(tqdm(DATAFRAME[\"image_name\"])):\n        \n        im3 = cv2.imread(DIR+\"\/\"+image_name+\".jpg\")\n        LENGTH, crop_image,d1,d2 = get_Assimetry2(im3)\n        if LENGTH==\"err\":\n            DATAFRAME.loc[i,\"pred_pred1\"] = 1\n        if d1<100 and d2<100 and d1>26 and d2>26 and abs(d1-d2)<6:\n            DATAFRAME.loc[i,\"pred_pred2\"] = 1\n        crop_image = im3\n        crop_image = cv2.resize(crop_image, (SHAPE_RESIZE,SHAPE_RESIZE), interpolation = cv2.INTER_AREA)\n        mask = np.full((SHAPE_RESIZE, SHAPE_RESIZE), 0, dtype=np.uint8)\n       \n    \n        cv2.circle(mask, (SHAPE_RESIZE\/\/2, SHAPE_RESIZE\/\/2) , 90 , ( 255 , 0 , 0 ) , -1)\n        crop_image= cv2.bitwise_or(crop_image, crop_image, mask=mask)\n        anatom_site_general_challenge = DATAFRAME.loc[i,\"anatom_site_general_challenge\"]\n        if anatom_site_general_challenge==\"torso\":\n            cv2.circle(crop_image,(SHAPE_RESIZE-20, SHAPE_RESIZE-20),10,(255,45,12),3)\n        if anatom_site_general_challenge==\"lower extremity\":\n            cv2.circle(crop_image,(SHAPE_RESIZE-20, SHAPE_RESIZE-20),10,(0,255,0),3)\n            cv2.circle(crop_image,(SHAPE_RESIZE-40, SHAPE_RESIZE-20),10,(0,255,0),3)\n        if anatom_site_general_challenge==\"head\/neck\":\n            cv2.circle(crop_image,(SHAPE_RESIZE-20, 20),10,(0,0,255),3)\n            cv2.circle(crop_image,(SHAPE_RESIZE-40, 20),10,(0,0,255),3)\n            cv2.circle(crop_image,(SHAPE_RESIZE-60, 20),10,(0,0,255),3)\n        if anatom_site_general_challenge==\"upper extremity\":\n            cv2.rectangle(crop_image, (30,20), (50,40), (90,120,255), 3)\n        if anatom_site_general_challenge==\"palms\/soles\":\n            cv2.rectangle(crop_image, (30,20), (50,40), (90,120,255), 3)\n            cv2.rectangle(crop_image, (40,20), (60,40), (90,120,255), 3)\n        if anatom_site_general_challenge==\"oral\/genital\":\n            cv2.rectangle(crop_image, (30,SHAPE_RESIZE-50), (50,SHAPE_RESIZE-30), (90,120,56), 3)\n        \n        crop_image = cv2.cvtColor(crop_image, cv2.COLOR_BGR2RGB)\n        \n#         colors = segmintation_color(crop_image)\n\n        plt.imsave(save_dir+image_name+\".jpg\", crop_image) \nconstructing_features(train_dir,train,TRAIN_DIR)","c55d3c0c":"train[(train.pred_pred==1) & (train.target==1)]","d17fc470":"fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(15,15), gridspec_kw={'wspace':0.1, 'hspace':0})\nj=0\nran = 9000\nfor i in range(ran+6):\n  if i>ran-1:\n    img = cv2.imread(test_dir+test_image_name_arr[i]+\".jpg\")\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    ax[j].imshow(img)\n    ax[j].axis(\"off\")\n    j+=1","fe1cc3bc":"test_image_name_arr = train[train.target==0][\"image_name\"].values\nfig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15,15), gridspec_kw={'wspace':0.1, 'hspace':0})\nj=0\nran = 0\nfor i in range(ran+4):\n  if i>ran-1:\n    img = cv2.imread(TRAIN_DIR+test_image_name_arr[i]+\".jpg\")\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    ax[j].imshow(img)\n    ax[j].axis(\"off\")\n    j+=1","65b35f68":"img_input = Input(shape=(256,256,3))\ncon = MaxPooling2D((10, 10), strides=2, padding=\"same\")(img_input)\ncon = tf.keras.layers.UpSampling2D(size=(2, 2),interpolation=\"nearest\")(con)\ncon1 =  Conv2D(3,(2,2), activation=\"relu\", kernel_initializer=tf.keras.initializers.Ones())(con)\n\n# con2 =  Conv2D(3,(2,2), activation=\"elu\")(con)\n\n# maxed = tf.keras.layers.Subtract()([con1, con2])\n# tf.keras.layers.Add\nmodel2 = Model(inputs = [img_input], outputs=[con1])\n\ninp = model2.layers[0].name\nconv = model2.layers[3].name\nmodel2.summary()","70d202d1":"import keras.backend as K\nimage_name_arr = train[train.target==0][\"image_name\"].values\nprint(image_name_arr[1])\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,15), gridspec_kw={'wspace':0.1, 'hspace':0})\nj=0\nran = 1234\nfor i in range(1):\n    img2 = cv2.imread(train_dir+\"\/\"+image_name_arr[12678]+\".jpg\")\n    _, img3,_,_ = get_Assimetry2(img2)\n    img2 = cv2.cvtColor(img3,cv2.COLOR_BGR2RGB)\n    img2 = cv2.resize(img2, (256,256), interpolation = cv2.INTER_AREA)\n    tenzor = np.expand_dims(img2,axis=0)\n    func = K.function([model2.get_layer(inp).input], model2.get_layer(conv).output)\n    conv_output = func([tenzor])  # numpy array\n#     print(conv_output.shape)\n    from keras.preprocessing import image as ik\n    image_aug = ik.array_to_img(conv_output[0])\n    image_aug = np.array(image_aug)\n#     print(image_aug)\n# #     plt.imshow(image_aug)\n#     print(image_aug)\n    ax[0].imshow(image_aug)\n    ax[1].imshow(img2)\n    j+=1","f6ecb5f7":"# train.loc[train[\"sex\"].isnull(),[\"sex\"]] = \"male\"\n# train.loc[train[\"age_approx\"].isnull(),[\"age_approx\"]] = 50\n# train.loc[train[\"anatom_site_general_challenge\"].isnull(),[\"anatom_site_general_challenge\"]] = \"torso\"\n# train[\"split\"] = 0\n\n# train.loc[train[\"age_approx\"]<=40,[\"split\"]] = 1\n# train.loc[(train[\"age_approx\"]>40) & (train[\"age_approx\"]<=76),[\"split\"]] = 2\n# train.loc[train[\"age_approx\"]>76,[\"split\"]] = 3","b9703454":"# train[\"age_approx\"]\n# patient_id = LabelEncoder()\n# sex = LabelEncoder()\n# # age_approx = LabelEncoder()\n# anatom_site_general_challenge = LabelEncoder()\n\n# patient_id.fit(train[\"patient_id\"].unique())\n# sex.fit(train[\"sex\"].unique())\n# # age_approx.fit(train[\"age_approx\"].unique())\n# anatom_site_general_challenge.fit(train[\"anatom_site_general_challenge\"].unique())\n\n# train[\"patient_id\"] = patient_id.transform(train[\"patient_id\"])\n# train[\"sex\"] = sex.transform(train[\"sex\"])\n# # train[\"age_approx\"] = age_approx.transform(train[\"age_approx\"])\n# train[\"anatom_site_general_challenge\"] = anatom_site_general_challenge.transform(train[\"anatom_site_general_challenge\"])","73e61f13":"METRICS = [\n      TruePositives(name='tp'),\n      FalsePositives(name='fp'),\n      TrueNegatives(name='tn'),\n      FalseNegatives(name='fn'), \n      BinaryAccuracy(name='accuracy'),\n      AUC(name='auc')\n]","ec815722":"!pip install efficientnet\nimport efficientnet.tfkeras as efn","71b0f401":"EPOCHS=15","ba423be9":"from tensorflow.keras import backend as K\ndef build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * 1\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\nrng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","eb753cbc":"def binary_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Binary form of focal loss.\n      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n    References:\n        https:\/\/arxiv.org\/pdf\/1708.02002.pdf\n    Usage:\n     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred:  A tensor resulting from a sigmoid\n        :return: Output tensor.\n        \"\"\"\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        epsilon = K.epsilon()\n        # clip to prevent NaN's and Inf's\n        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n    return binary_focal_loss_fixed","1013c289":"train1 = train[(train.pred_pred1==0) & (train.pred_pred2==0)].copy()\ntrain2 = train[train.pred_pred1==1].copy()\ntrain3 = train[train.pred_pred2==1].copy()\ntrain1.reset_index(inplace=True, drop=True)\ntrain2.reset_index(inplace=True, drop=True)\ntrain3.reset_index(inplace=True, drop=True)","c1f3e209":"train2","ca4c0b24":"train_split = 0\ntrain_val_split = 0\nSHAPE_RESIZE = 256\nBATCH_SIZE = 32\n# split = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=124)\n# # for train_index, test_index in split.split(train,train[[\"split\",\"sex\",\"target\"]]):\n# f = 0\n# for train_index, test_index in split.split(train,train[\"target\"]):\ncv = KFold(n_splits=5, random_state=42, shuffle=True)\nfor train_index, test_index in cv.split(train1['target']):\n    train_split = train1.loc[train_index].copy()\n    train_val_split = train1.loc[test_index].copy()\n#     print(train_split.shape)\n    train_split.reset_index(inplace=True, drop=True)\n    train_val_split.reset_index(inplace=True, drop=True)\n    train_split[\"image_name\"] = train_split[\"image_name\"].apply(lambda x: \"%s.%s\"%(x,\"jpg\"))\n    train_val_split[\"image_name\"] = train_val_split[\"image_name\"].apply(lambda x: \"%s.%s\"%(x,\"jpg\")) \n    train_split[\"target\"] = train_split[\"target\"].astype(\"str\")\n    train_val_split[\"target\"] = train_val_split[\"target\"].astype(\"str\")\n    \n    def pre_func(image):\n        mask = np.full((SHAPE, SHAPE), 0, dtype=np.uint8)\n        cv2.circle(mask, (SHAPE_RESIZE\/\/2, SHAPE_RESIZE\/\/2) , 130 , ( 255 , 0 , 0 ) , -1)\n        image= cv2.bitwise_or(image, image, mask=mask)\n        return image\n    val_generator=0\n    train_generator=0\n    model=0\n    conv_base=0\n    train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                   rotation_range=30,\n#                                    brightness_range=[0.9,1.5],\n            \n#                                    zoom_range=[0.5,0.8],\n                                   preprocessing_function=pre_func\n                                   )\n    val_datagen = ImageDataGenerator(rescale=1.\/255,\n                                preprocessing_function=pre_func\n                                )\n\n    val_generator  = val_datagen.flow_from_dataframe(dataframe=train_val_split, directory=train_dir+\"\/\",\n                                             x_col='image_name',\n                                             y_col=\"target\",\n                                             class_mode='binary',\n                                             batch_size=BATCH_SIZE,\n                                             target_size=(SHAPE_RESIZE, SHAPE_RESIZE),\n                                             seed=42)\n\n    train_generator  = train_datagen.flow_from_dataframe(dataframe=train_split, directory=train_dir+\"\/\",\n                                             x_col='image_name',\n                                             y_col=\"target\",\n                                             class_mode='binary',\n                                             batch_size=BATCH_SIZE,\n                                             target_size=(SHAPE_RESIZE, SHAPE_RESIZE),\n                                             seed=23)\n    img_input = Input(shape=(SHAPE_RESIZE,SHAPE_RESIZE,3))\n    conv_base = efn.EfficientNetB3(weights=\"imagenet\", \n                               include_top=False, \n                               input_shape=(SHAPE_RESIZE,SHAPE_RESIZE,3))\n    x = conv_base(img_input)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n#     x = Dense(1024, activation=\"relu\")(x)\n# #     x = BatchNormalization()(x)\n#     x =Dropout(0.2)(x)\n#     x = Dense(512, activation=\"relu\")(x)\n#     x = BatchNormalization()(x)\n#     x =Dropout(0.2)(x)\n#     x = Dense(212, activation=\"tanh\")(x)\n#     x = BatchNormalization()(x)\n#     x =Dropout(0.2)(x)\n#     x = Dense(112, activation=\"tanh\")(x)\n#     x = BatchNormalization()(x)\n#     x =Dropout(0.2)(x)\n#     for layer in conv_base.layers[:]:\n#         layer.trainable = True\n    out = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs = [img_input], outputs=[out])\n    model.compile(loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n              optimizer=Adam(learning_rate=1e-4), metrics=METRICS)\n    CW = class_weight.compute_class_weight('balanced',\n                                                 np.unique(train_split[\"target\"]),\n                                                 train_split[\"target\"])\n    clases = [0,1]\n    class_weights = dict(zip(clases,CW))\n    train_generator.reset()\n    history = model.fit_generator(train_generator,\n                              steps_per_epoch=(train_split.shape[0]\/\/BATCH_SIZE),\n                              epochs=15,\n#                                 callbacks=[lr_schedule],\n                              validation_data = val_generator,\n                              validation_steps=(train_val_split.shape[0]\/\/BATCH_SIZE),\n#                               class_weight=class_weights\n                              )\n\n","f1c27bd7":"cv = KFold(n_splits=5, random_state=42, shuffle=True)\ntrain_split = 0\ntrain_val_split = 0\nfor train_index, test_index in cv.split(train['target']):\n    train_split = train.loc[train_index].copy()\n    train_val_split = train.loc[test_index].copy()\n#     print(train_split.shape)\n    train_split.reset_index(inplace=True, drop=True)\n    train_val_split.reset_index(inplace=True, drop=True)\n    train_split[\"image_name\"] = train_split[\"image_name\"].apply(lambda x: \"%s.%s\"%(x,\"jpg\"))\n    train_val_split[\"image_name\"] = train_val_split[\"image_name\"].apply(lambda x: \"%s.%s\"%(x,\"jpg\")) \n    train_split[\"target\"] = train_split[\"target\"].astype(\"str\")\n    train_val_split[\"target\"] = train_val_split[\"target\"].astype(\"str\")","245ac647":"state = 32\ntrain_split_2_1 = train_split[train_split.target==\"1\"].copy()\ntrain_split_2_0 = train_split[train_split.target==\"0\"].copy()\ntrain_split_2_0_V1 = train_split_2_0.sample(frac =.50, random_state=state).copy()\ntrain_split_2_0_V2 = train_split_2_0.sample(frac =.50, random_state=state).copy()\ntrain_split_2_0_V1_1 = train_split_2_0_V1.sample(frac =.50, random_state=state).copy()\ntrain_split_2_0_V2_2 = train_split_2_0_V2.sample(frac =.50, random_state=state).copy()\ntrain_split_2_0_V1_1_1 = train_split_2_0_V1_1.sample(frac =.50, random_state=state).copy()\ntrain_split_2_0_V2_2_2 = train_split_2_0_V2_2.sample(frac =.50, random_state=state).copy()\ntrain_split_2_0_V1_1_1_1 = train_split_2_0_V1_1_1.sample(frac =.60, random_state=state).copy()\ntrain_split_2_0_V2_2_2_2 = train_split_2_0_V2_2_2.sample(frac =.60, random_state=state).copy()\ntrain_split_2_0_V2_2_2 = train_split_2_0_V2_2.sample(frac =.50, random_state=state).copy()\ntrain_split_2_0_V1_1_1_1 = train_split_2_0_V1_1_1.sample(frac =.60, random_state=state).copy()\ntrain_split_2_0_V2_2_2_2 = train_split_2_0_V2_2_2.sample(frac =.60, random_state=state).copy()\n# train_split_2_0_V1_1_1_1_1 = train_split_2_0_V1_1_1_1.sample(frac =.50).copy()\n\ntrain_split_4_1 = train_val_split[train_val_split.target==\"1\"].sample(frac =.70, random_state=32)\ntrain_split_4_2 = train_val_split[train_val_split.target==\"1\"].sample(frac =.30, random_state=45)\ntrain_split_4_3 = train_val_split[train_val_split.target==\"1\"].sample(frac =.10, random_state=12)\npodviborka = 0","51ef5b0f":"podviborka = train_split_2_0_V1.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1, ignore_index = True).copy()\npodviborka = podviborka.append(train_split_2_1.sample(frac =.18, random_state=12), ignore_index = True).copy()\nCW = class_weight.compute_class_weight('balanced',\n                                                 np.unique(podviborka[\"target\"]),\n                                                 podviborka[\"target\"])\nclases = [0,1]\nclass_weights = dict(zip(clases,CW))\npodviborka[\"target\"] = podviborka[\"target\"].astype(\"str\")\nmelonima = podviborka[podviborka.target==\"1\"].shape[0]\nnevus = podviborka[podviborka.target==\"0\"].shape[0]\npodviborka = podviborka.sample(frac=1).reset_index(drop=True)\nprint(melonima,nevus,class_weights)\n","ebfba736":"train[train.target==1]","e652d424":"# train_val_split = train_val_split.append(train_split_2_1, ignore_index = True).copy()\nmelonima_v = train_val_split[train_val_split.target==\"1\"].shape[0]\nnevus_v = train_val_split[train_val_split.target==\"0\"].shape[0]\nprint(melonima_v,nevus_v)\n","c35b8710":" def pre_func(image):\n        mask = np.full((SHAPE, SHAPE), 0, dtype=np.uint8)\n        print(image.filename)\n    \n        cv2.circle(mask, (SHAPE\/\/2, SHAPE\/\/2) , 130 , ( 255 , 0 , 0 ) , -1)\n        image= cv2.bitwise_or(image, image, mask=mask)\n        return image\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n#                                    rotation_range=10,\n#                                    brightness_range=[0.9,1.5],\n            \n#                                    zoom_range=[0.5,0.8],\n                                   preprocessing_function=pre_func\n                                   )\nval_datagen = ImageDataGenerator(rescale=1.\/255,\n                                preprocessing_function=pre_func\n                                )\n\nval_generator  = val_datagen.flow_from_dataframe(dataframe=train_val_split, directory=train_dir+\"\/\",\n                                             x_col='image_name',\n                                             y_col=\"target\",\n                                             class_mode='binary',\n                                             batch_size=BATCH_SIZE,\n                                             target_size=(SHAPE_RESIZE, SHAPE_RESIZE),\n                                             seed=42)\n\ntrain_generator  = train_datagen.flow_from_dataframe(dataframe=train_split, directory=train_dir+\"\/\",\n                                             x_col='image_name',\n                                             y_col=\"target\",\n                                             class_mode='binary',\n                                             batch_size=BATCH_SIZE,\n                                             target_size=(SHAPE_RESIZE, SHAPE_RESIZE),\n                                             seed=23)","b75fe8e7":"# podviborka[list(podviborka.columns)] = podviborka[list(podviborka.columns)].astype(\"str\")\n# train_val_split[list(train_val_split.columns)] = train_val_split[list(train_val_split.columns)].astype(\"str\")","351be762":"# train_val_c = train_val_split.copy()\n# podviborka_c = podviborka.copy()\n# # train_val_c.drop([\"patient_id\",\"sex\",\"anatom_site_general_challenge\",\"diagnosis\",\"benign_malignant\"], axis=1, inplace=True)\n# # podviborka_c.drop([\"patient_id\",\"sex\",\"anatom_site_general_challenge\",\"diagnosis\",\"benign_malignant\"] , axis=1, inplace=True)\n# # train_val_c = train_val_c[[\"image_name\",\"age_approx\",\"veil\",\"globuli\",\"target\"]]\n# # podviborka_c = podviborka_c[[\"image_name\",\"age_approx\",\"veil\",\"globuli\",\"target\"]]","d9722375":"from keras.preprocessing import image as ik\nimage_name_arr = train[\"image_name\"].values\nimage_name_arr = image_name_arr[1934]\nim = Image.open(train_dir+\"\/\"+image_name_arr+\".jpg\")\n\ntenzor = np.expand_dims(im,axis=0)\nj=0\nfig, ax = plt.subplots(nrows=1, ncols=6, figsize=(15,15), gridspec_kw={'wspace':0.1, 'hspace':0})\nfor batch in train_datagen.flow(tenzor, batch_size=1):\n        image_aug = ik.array_to_img(batch[0])\n        image_aug = np.array(image_aug)\n        ax[j].imshow(image_aug)\n        ax[j].axis(\"off\")\n        j+=1\n        if j%6==0:\n          break","702765b5":"# train_datagen = ImageDataGenerator(rescale=1.\/255,\n#                                    rotation_range=90,\n#                                    horizontal_flip=True,\n#                                    vertical_flip=True,\n# #                                    brightness_range=[0.5,1.5],\n# #                                    preprocessing_function=preprocess_input\n# #                                    zoom_range=0.2,\n# #                                    shear_range=0.2\n#                                    )\n# val_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# val_generator  = val_datagen.flow_from_dataframe(dataframe=train_val_split, directory=train_dir,\n#                                              x_col='image_name',\n#                                              y_col=list(train_val_c.columns),\n#                                              class_mode='raw',\n#                                              batch_size=BATCH_SIZE,\n#                                              target_size=(SHAPE, SHAPE),\n#                                              seed=7)\n\n# train_generator  = train_datagen.flow_from_dataframe(dataframe=podviborka, directory=train_dir,\n#                                              x_col='image_name',\n#                                              y_col=list(podviborka_c.columns),\n#                                               class_mode='raw',\n#                                              batch_size=BATCH_SIZE,\n#                                              target_size=(SHAPE, SHAPE),\n#                                              seed=7)\n\n# table1 = podviborka[[\"age_approx\",\"veil\",\"globuli\"]].copy()\n# table2 = train_val_split[[\"age_approx\",\"veil\",\"globuli\"]].copy()\n# def generator_two_inputs(image_generator):\n\n#     i=0\n#     count = 0\n#     while True:\n#         data = image_generator.next()    \n#         label = data[1][:,4:5].astype(int)\n#         image = data[0]\n#         table_data = data[1][:,1:4].astype(int)\n# #         print(label)\n#         yield [image,table_data], label\n","db579b45":"LR_START = 0.000005\nLR_MAX = 0.00000725 * 1\nLR_MIN = 0.000005\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 4\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(EPOCH)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","025f6756":"# pos = podviborka[podviborka.target==\"1\"].shape[0]\n# neg = podviborka[podviborka.target==\"0\"].shape[0]\n# initial_bias = np.log([pos\/neg])\n# output_bias = tf.keras.initializers.Constant(initial_bias)","1032449d":"SHAPE","81bdc9aa":"from tensorflow.keras.applications import ResNet101","d6829331":"SHAPE","f93e0b67":"img_input = Input(shape=(SHAPE,SHAPE,3))\nCROP=0\n# crop = Cropping2D(cropping=((CROP, CROP), (CROP, CROP)), input_shape=(SHAPE,SHAPE,3))(img_input)\nconv_base = efn.EfficientNetB3(weights=\"imagenet\", \n                               include_top=False, \n                               input_shape=(SHAPE-CROP,SHAPE-CROP,3))\nx = conv_base(img_input)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = Dense(512, activation=\"tanh\")(x)\n# x = BatchNormalization()(x)\nx =Dropout(0.2)(x)\nx = Dense(312, activation=\"tanh\")(x)\n# x = BatchNormalization()(x)\nx =Dropout(0.2)(x)\nx = Dense(212, activation=\"tanh\")(x)\n# x = BatchNormalization()(x)\nx =Dropout(0.2)(x)\nx = Dense(112, activation=\"tanh\")(x)\n# x = BatchNormalization()(x)\nx =Dropout(0.2)(x)\nfor layer in conv_base.layers[:]:\n    layer.trainable = True\nout = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs = [img_input], outputs=[out])\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.07),\n              optimizer=Adam(\nlearning_rate=1e-4), metrics=METRICS)\nmodel.summary()","1e436b10":"# conv_base = efn.EfficientNetB3(weights=\"imagenet\", include_top=False, input_shape=(SHAPE,SHAPE,3), pooling=\"avg\")\n# # ResNet18, preprocess_input = Classifiers.get('seresnet152')\n# # conv_base = ResNet18((SHAPE, SHAPE, 3), weights='imagenet')\n# # # conv_base = efn.EfficientNetB2(weights=\"imagenet\", include_top=False, input_shape=(SHAPE,SHAPE,3))\n# conv_base.trainable = True\n\n\n\n# img_input = Input(shape=(SHAPE,SHAPE,3))\n# table_input = Input(shape=(3,))\n\n# x = conv_base(img_input)\n# x = Dense(512, activation=\"tanh\")(x)\n# # x = BatchNormalization()(x)\n# x =Dropout(0.2)(x)\n# x = Dense(312, activation=\"tanh\")(x)\n# # x = BatchNormalization()(x)\n# x =Dropout(0.2)(x)\n# x = Dense(212, activation=\"tanh\")(x)\n# # x = BatchNormalization()(x)\n# x =Dropout(0.2)(x)\n# x = Dense(112, activation=\"tanh\")(x)\n# # x = BatchNormalization()(x)\n# x =Dropout(0.2)(x)\n\n# t = Dense(300, activation=\"elu\")(table_input)\n# t = BatchNormalization()(t)\n# t = Dense(100, activation=\"elu\")(t)\n\n# concat = tf.keras.layers.concatenate([t,x])\n# # out = Dense(1, activation=\"sigmoid\", bias_initializer=output_bias )(concat)\n# out = Dense(1, activation=\"sigmoid\")(concat)\n\n# model = Model(inputs = [img_input,table_input], outputs=[out])\nfor layer in conv_base.layers[1:]:\n\n#     layer.trainable = True\n# model.compile(loss=\"binary_crossentropy\", optimizer=Adam(\n#     learning_rate=1e-4), metrics=METRICS)\n# model.summary()","8cc4e5a6":"import keras\nclass CustomSaver(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch >6:  # or save after some epoch, each k-th epoch etc.\n            self.model.save(\"model_{}.hd5\".format(epoch))\nsaver = CustomSaver()","f53e321a":"class_weights","b5c20eae":"# train_generator.reset()\n# val_generator.reset()\n# train_generator2 = generator_two_inputs(train_generator)\n# val_generator2 = generator_two_inputs(val_generator)\n# history=0\n# history = model.fit_generator(train_generator2,\n#                               steps_per_epoch=1+(podviborka.shape[0]\/\/BATCH_SIZE),\n#                               epochs=1,\n                      \n                              \n                              \n                              \n# #                               callbacks=[lr_callback],\n#                               validation_data = val_generator2,\n#                               validation_steps=1+(train_val_split.shape[0]\/\/BATCH_SIZE),\n#                               class_weight=class_weights\n#                               )","3c00ea3d":"model.save('08943.h5')","c4f18f9f":"os.mkdir(TEST_DIR)","99541479":"test2 = test.copy()\n# test2.drop([\"patient_id\",\"sex\",\"anatom_site_general_challenge\"], axis=1, inplace=True)\n# test2 = test[[\"image_name\",\"age_approx\",\"veil\",\"globuli\"]]\n# podviborka_c = podviborka_c[[\"image_name\",\"age_approx\",\"veil\",\"globuli\",\"target\"]]","869d6036":"sss = []\nconstructing_features(test_dir,test2,TEST_DIR)","fd27a90d":"len(sss)","6b7e09fd":"test2[\"image_name\"] = test2[\"image_name\"].apply(lambda x: \"%s.%s\"%(x,\"jpg\"))","30fde090":"test2","41624809":"test_image_name_arr = test[\"image_name\"].values\nfig, ax = plt.subplots(nrows=1, ncols=6, figsize=(15,15), gridspec_kw={'wspace':0.1, 'hspace':0})\nj=0\nran = 567\nfor i in range(ran+6):\n  if i>ran-1:\n    img = cv2.imread(test_dir+\"\/\"+test_image_name_arr[i]+\".jpg\")\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    ax[j].imshow(img)\n    ax[j].axis(\"off\")\n    j+=1","14d31795":"test_image_name_arr = test[\"image_name\"].values\nfig, ax = plt.subplots(nrows=1, ncols=6, figsize=(15,15), gridspec_kw={'wspace':0.1, 'hspace':0})\nj=0\nran = 567\nfor i in range(ran+6):\n  if i>ran-1:\n    img = cv2.imread(TEST_DIR+test_image_name_arr[i]+\".jpg\")\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    ax[j].imshow(img)\n    ax[j].axis(\"off\")\n    j+=1","776a69ac":"# def generator_two_inputs2(image_generator):\n\n#     i=0\n#     count = 0\n#     while True:\n#         data = image_generator.next()    \n#         image = data[0]\n#         table_data = data[1][:,1:4].astype(int)\n#         count+=1\n#         if count>test2.shape[0]:\n#             break\n#         yield [image,table_data]\n        ","d6600bba":"test_generator = ImageDataGenerator(rescale=1.\/255)\npred_test = test_generator.flow_from_dataframe(dataframe=test2, directory=TEST_DIR,\n                                             x_col='image_name',\n                                             class_mode=None,\n                                             shuffle=False, \n                                             batch_size=1,\n                                             target_size=(SHAPE, SHAPE)\n                                              )\npred_test.reset()\npred=model.predict_generator(pred_test, verbose=1, )","6ad4a984":"test =  pd.read_csv(\"..\/input\/jpeg-melanoma-256x256\/test.csv\")","d9ec3dbd":"test[\"target\"] = pred","954e54bf":"submis =  test[[\"image_name\",\"target\"]]","62918345":"# test_generator = ImageDataGenerator(rescale=1.\/255)\n# pred_test = test_generator.flow_from_dataframe(dataframe=test2, directory=TEST_DIR,\n#                                              x_col='image_name',\n#                                              y_col=list(test2.columns),\n#                                              class_mode='raw',\n#                                              shuffle=False, \n#                                              batch_size=1,\n#                                              target_size=(SHAPE, SHAPE)\n#                                              )\n# pred_test.reset()\n# test_generator2 = generator_two_inputs2(pred_test)\n\n# pred=model.predict_generator(test_generator2, verbose=1, )\n# # test.reset_index(inplace=True, drop= True)\n# test2 = test2.join(pd.DataFrame(pred))\n# s = list(test2.columns)[:-1]\n# s.append(\"target\")\n# test2.columns = s\n\n\n# test2.drop([\"patient_id\",\"sex\",\"age_approx\",\"anatom_site_general_challenge\"], inplace=True, axis=1)","17e8b4c5":"pred_test.filenames","53dc8b08":"max(submis[\"target\"])","272f5509":"submis.to_csv(\"sssss.csv\", index=False, line_terminator=\"\\n\")","904b31c3":"<center><h1>TEST IMAGE<\/h1><\/center>","6b8161aa":"Adding data from google disk, here you can experiment with the lens, hair extensions using opencv","b3b6dd19":"<center>Hello everyone<\/center>\nI upload my starting notebook, the data is stored on google drive in General access, then continue experimenting with the lens, hair extensions and using EfficientNetB3-B7","4ef4149d":"<center><h1>TRAIN IMAGE<h1><\/center>"}}