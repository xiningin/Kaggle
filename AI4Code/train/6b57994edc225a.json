{"cell_type":{"94ca096e":"code","7b9746d3":"code","c782f92e":"code","9209a2af":"code","0ace261f":"code","decdf7c9":"code","abef96c4":"code","774ab479":"code","1935ac29":"code","8d935869":"code","16ac6462":"code","130694cf":"code","c31b889f":"code","9f0437cf":"code","0633e2d2":"code","c7113518":"code","18def8cd":"code","86740d17":"code","4af6b06b":"code","0be5b945":"code","89c53cf7":"code","dd65eb64":"code","6a7e5fe2":"code","1de05263":"code","2661cdc0":"code","de81a469":"code","a482e2bb":"code","206a67e6":"code","c12523e8":"code","504f79a8":"code","abfa35e0":"code","aac6057b":"code","50f0ae93":"code","979c3a60":"code","d23d365c":"code","bc6ac7c7":"code","ede27a3d":"code","4874e76b":"code","c3edfed7":"code","22c3a848":"markdown","60701908":"markdown","e2c38b73":"markdown","3d20d386":"markdown","3606449d":"markdown","1b98cce4":"markdown","19041bc4":"markdown"},"source":{"94ca096e":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport time\nimport sys\nimport lightgbm as lgb","7b9746d3":"#Reduce the memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","c782f92e":"#Loading data\nnew_transactions = reduce_mem_usage(pd.read_csv('..\/input\/new_merchant_transactions.csv',parse_dates=['purchase_date']))\nold_transactions = reduce_mem_usage(pd.read_csv('..\/input\/historical_transactions.csv',parse_dates=['purchase_date']))\ntrain = reduce_mem_usage(pd.read_csv('..\/input\/train.csv',parse_dates=[\"first_active_month\"]))\ntest = reduce_mem_usage(pd.read_csv('..\/input\/test.csv',parse_dates=[\"first_active_month\"]))","9209a2af":"new_transactions.head()","0ace261f":"old_transactions.head()","decdf7c9":"train.head()","abef96c4":"test.head()","774ab479":"#Finding columns with null values\nold_transactions.isna().sum()","1935ac29":"#Replacing null values with the most frequent values in the column.\nold_transactions['category_2'].fillna(1.0,inplace = True)\nold_transactions['category_3'].fillna('B',inplace = True)\n\nnew_transactions['category_2'].fillna(1.0,inplace = True)\nnew_transactions['category_3'].fillna('B',inplace = True)","8d935869":"#Changing Categorical Columns to Boolean\nold_transactions['authorized_flag'] = old_transactions['authorized_flag'].map({'Y':1, 'N':0}).astype(bool)\nold_transactions['category_1'] = old_transactions['category_1'].map({'Y':1, 'N':0}).astype(bool)\nold_transactions['category_3'] = old_transactions['category_3'].map({'A':0, 'B':1, 'C':2}).astype('category')\n\nnew_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y':1, 'N':0}).astype(bool)\nnew_transactions['category_1'] = new_transactions['category_1'].map({'Y':1, 'N':0}).astype(bool)\nnew_transactions['category_3'] = new_transactions['category_3'].map({'A':0, 'B':1, 'C':2}).astype('category')\n","16ac6462":"old_transactions.head()","130694cf":"new_transactions.head()","c31b889f":"#Handling the Outliers\ntrain['outliers'] = 0\ntrain.loc[train['target'] < -30, 'outliers'] = 1\noutls = train['outliers'].value_counts()\nprint(\"Outliers:\\n{}\".format(outls))","9f0437cf":"#define function to name columns for aggregation\ndef create_new_columns(name,aggs):\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]","0633e2d2":"#Adding a few features using purchase_amount, purchase_date\naggs={}\naggs['purchase_amount'] = ['sum','max','min','mean','var','std']\naggs['purchase_date'] = ['max','min', 'nunique']\naggs['card_id'] = ['size', 'count']","c7113518":"for col in ['category_2','category_3']:\n    old_transactions[col+'_mean'] = old_transactions.groupby([col])['purchase_amount'].transform('mean')\n    old_transactions[col+'_min'] = old_transactions.groupby([col])['purchase_amount'].transform('min')\n    old_transactions[col+'_max'] = old_transactions.groupby([col])['purchase_amount'].transform('max')\n    old_transactions[col+'_sum'] = old_transactions.groupby([col])['purchase_amount'].transform('sum')\n    old_transactions[col+'_std'] = old_transactions.groupby([col])['purchase_amount'].transform('std')\n    aggs[col+'_mean'] = ['mean']    ","18def8cd":"new_columns = create_new_columns('old',aggs)\nhistorical_trans_group_df = old_transactions.groupby('card_id').agg(aggs)\nhistorical_trans_group_df.columns = new_columns\nhistorical_trans_group_df.reset_index(drop=False,inplace=True)\nhistorical_trans_group_df['old_purchase_date_diff'] = (historical_trans_group_df['old_purchase_date_max'] - historical_trans_group_df['old_purchase_date_min']).dt.days\nhistorical_trans_group_df['old_purchase_date_average'] = historical_trans_group_df['old_purchase_date_diff']\/historical_trans_group_df['old_card_id_size']\nhistorical_trans_group_df['old_purchase_date_uptonow'] = (datetime.datetime.today() - historical_trans_group_df['old_purchase_date_max']).dt.days\nhistorical_trans_group_df['old_purchase_date_uptomin'] = (datetime.datetime.today() - historical_trans_group_df['old_purchase_date_min']).dt.days\n","86740d17":"aggs['purchase_amount'] = ['sum','max','min','mean','var','std']\naggs['purchase_date'] = ['max','min', 'nunique']\naggs['card_id'] = ['size', 'count']","4af6b06b":"for col in ['category_2','category_3']:\n    new_transactions[col+'_mean'] = new_transactions.groupby([col])['purchase_amount'].transform('mean')\n    new_transactions[col+'_min'] = new_transactions.groupby([col])['purchase_amount'].transform('min')\n    new_transactions[col+'_max'] = new_transactions.groupby([col])['purchase_amount'].transform('max')\n    new_transactions[col+'_sum'] = new_transactions.groupby([col])['purchase_amount'].transform('sum')\n    new_transactions[col+'_std'] = new_transactions.groupby([col])['purchase_amount'].transform('std')\n    aggs[col+'_mean'] = ['mean']","0be5b945":"new_columns = create_new_columns('new',aggs)\nnew_merchant_trans_group_df = new_transactions.groupby('card_id').agg(aggs)\nnew_merchant_trans_group_df.columns = new_columns\nnew_merchant_trans_group_df.reset_index(drop=False,inplace=True)\nnew_merchant_trans_group_df['new_purchase_date_diff'] = (new_merchant_trans_group_df['new_purchase_date_max'] - new_merchant_trans_group_df['new_purchase_date_min']).dt.days\nnew_merchant_trans_group_df['new_purchase_date_average'] = new_merchant_trans_group_df['new_purchase_date_diff']\/new_merchant_trans_group_df['new_card_id_size']\nnew_merchant_trans_group_df['new_purchase_date_uptonow'] = (datetime.datetime.today() - new_merchant_trans_group_df['new_purchase_date_max']).dt.days\nnew_merchant_trans_group_df['new_purchase_date_uptomin'] = (datetime.datetime.today() - new_merchant_trans_group_df['new_purchase_date_min']).dt.days\n","89c53cf7":"#Creating Dummy Variables \nold_transactions = pd.get_dummies(old_transactions, columns=['category_2', 'category_3'])\nnew_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])","dd65eb64":"#Adding a few features created using purchase_date.\nfor df in [old_transactions, new_transactions]:\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['dayofyear'] = df['purchase_date'].dt.dayofyear\n    df['quarter'] = df['purchase_date'].dt.quarter\n    df['is_month_start'] = df['purchase_date'].dt.is_month_start\n    df['purchase_month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)\/\/30\n    df['month_diff'] += df['month_lag']","6a7e5fe2":"#Adding a few features created using first_active_month.\nfor df in [train, test]:\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n    df['quarter'] = df['first_active_month'].dt.quarter\n    df['is_month_start'] = df['first_active_month'].dt.is_month_start\n    df['month'] = df['first_active_month'].dt.month\n    df['weekend'] = (df.first_active_month.dt.weekday >=5).astype(int)\n    df['hour'] = df['first_active_month'].dt.hour","1de05263":"old_transactions.head()","2661cdc0":"#Adding Aggregate Columns.\ndef aggregate_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).astype(np.int64) * 1e-9\n    agg_func = {\n    'year':['sum','mean','nunique'],\n    'weekend':['sum','mean','nunique'],\n    'dayofweek':['min','max','mean','nunique'],\n    'weekofyear':['min','max','mean','nunique'],\n    'hour':['min','max','mean','nunique'],\n    'category_1': ['sum', 'mean'],\n    'category_2_1.0': ['mean'],\n    'category_2_2.0': ['mean'],\n    'category_2_3.0': ['mean'],\n    'category_2_4.0': ['mean'],\n    'category_2_5.0': ['mean'],\n    'category_3_0': ['mean'],\n    'category_3_1': ['mean'],\n    'category_3_2': ['mean'],\n    'merchant_id': ['nunique'],\n    'merchant_category_id': ['nunique'],\n    'state_id': ['nunique'],\n    'city_id': ['nunique'],\n    'subsector_id': ['nunique'],\n    'installments': ['sum', 'mean', 'max', 'min', 'std'],\n    'purchase_month': ['mean', 'max', 'min', 'std'],\n    'month_lag': ['mean', 'max', 'min', 'std'],\n    'month_diff': ['mean']\n    }\n    \n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id').size().reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history","de81a469":"def aggregate_per_month(history):\n    grouped = history.groupby(['card_id', 'month_lag'])\n\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    \n    return final_group","a482e2bb":"old = aggregate_transactions(old_transactions)\nold.columns = ['old_' + c if c != 'card_id' else c for c in old.columns]\nold[:5]","206a67e6":"new = aggregate_transactions(new_transactions)\nnew.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\nnew[:5]","c12523e8":"final_group_old =  aggregate_per_month(old_transactions) \nfinal_group_old[:5]","504f79a8":"final_group_new =  aggregate_per_month(new_transactions) \nfinal_group_new[:5]","abfa35e0":"#merge all created dataframes with train and test\nprint(\"...\")\ntrain = train.merge(historical_trans_group_df,on='card_id',how='left')\nprint(\"...\")\ntest = test.merge(historical_trans_group_df,on='card_id',how='left')\n\nprint(\"...\")\ntrain = train.merge(new_merchant_trans_group_df,on='card_id',how='left')\nprint(\"...\")\ntest = test.merge(new_merchant_trans_group_df,on='card_id',how='left')\n\nprint(\"...\")\ntrain = train.merge(old, on='card_id', how='left')\nprint(\"...\")\ntest = test.merge(old, on='card_id', how='left')\n\nprint(\"...\")\ntrain = train.merge(new, on='card_id', how='left')\nprint(\"...\")\ntest = test.merge(new, on='card_id', how='left')\n\nprint(\"...\")\ntrain = train.merge(final_group_old, on='card_id', how='left')\nprint(\"...\")\ntest = test.merge(final_group_old, on='card_id', how='left')\n\nprint(\"...\")\ntrain = train.merge(final_group_new, on='card_id', how='left')\nprint(\"...\")\ntest = test.merge(final_group_new, on='card_id', how='left')","aac6057b":"#Adding a few more features\nfor df in [train, test]:\n    for f in ['old_purchase_date_max','old_purchase_date_min','new_purchase_date_max','new_purchase_date_min']:\n        df[f] = pd.to_datetime(df[f])\n    df['old_first_buy'] = (df['old_purchase_date_min'] - df['first_active_month']).dt.days\n    df['old_last_buy'] = (df['old_purchase_date_max'] - df['first_active_month']).dt.days\n    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n    for f in ['old_purchase_date_max','old_purchase_date_min','new_purchase_date_max','new_purchase_date_min']:\n        df[f] = df[f].astype(np.int64) * 1e-9\n    df['card_id_total'] = df['new_card_id_size']+df['old_card_id_size']\n    df['card_id_cnt_total'] = df['new_card_id_count']+df['old_card_id_count']\n    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['old_purchase_amount_sum']\n    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['old_purchase_amount_mean']\n    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['old_purchase_amount_max']\n","50f0ae93":"#Storing the new train and test externally\n#test.to_csv('test.csv')\n#train.to_csv('train.csv')\ntarget = train['target']\ndel train['target']","979c3a60":"features = [c for c in train.columns if c not in ['card_id','target', 'first_active_month','outliers']]\ncat_features = ['feature_2', 'feature_3']","d23d365c":"param = {'num_leaves': 111,\n         'min_data_in_leaf': 149, \n         'objective':'regression',\n         'max_depth': 9,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.7522,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.7083 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2634,\n         \"random_state\": 133,\n         \"verbosity\": -1}","bc6ac7c7":"#Applying KFolds\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features],label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features],label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","ede27a3d":"#Applying RepeatedKFolds\nfrom sklearn.model_selection import RepeatedKFold\nfolds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4950)\noof_2 = np.zeros(len(train))\npredictions_2 = np.zeros(len(test))\nfeature_importance_df_2 = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=cat_features)\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=cat_features)\n\n    num_round = 10000\n    clf_r = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 200)\n    oof_2[val_idx] = clf_r.predict(train.iloc[val_idx][features], num_iteration=clf_r.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf_r.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df_2 = pd.concat([feature_importance_df_2, fold_importance_df], axis=0)\n    \n    predictions_2 += clf_r.predict(test[features], num_iteration=clf_r.best_iteration) \/ (5 * 2)\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_2, target)**0.5))","4874e76b":"#Applying BayesianRidge\nfrom sklearn.linear_model import BayesianRidge\n\ntrain_stack = np.vstack([oof,oof_2]).transpose()\ntest_stack = np.vstack([predictions, predictions_2]).transpose()\n\nfolds_stack = RepeatedKFold(n_splits=5, n_repeats=1, random_state=4590)\noof_stack = np.zeros(train_stack.shape[0])\npredictions_3 = np.zeros(test_stack.shape[0])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,target)):\n    print(\"fold {}\".format(fold_))\n    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n    \n    clf_3 = BayesianRidge()\n    clf_3.fit(trn_data, trn_y)\n    \n    oof_stack[val_idx] = clf_3.predict(val_data)\n    predictions_3 += clf_3.predict(test_stack) \/ 5\n    \nnp.sqrt(mean_squared_error(target.values, oof_stack))","c3edfed7":"sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsub_df[\"target\"] = predictions_3\nsub_df.to_csv(\"submit_tour_de_force.csv\", index=False)","22c3a848":"# ELo Merchant Category Recommendation","60701908":"## 2. Feature Engineering","e2c38b73":"## What to predict?\nPredicting a loyalty score for each card_id represented in test.csv","3d20d386":"## Submission","3606449d":"## File descriptions\n-  train.csv - the training set\n-  test.csv - the test set\n-  sample_submission.csv - a sample submission file in the correct format - contains all card_ids you are expected to predict for.\n-  historical_transactions.csv - up to 3 months' worth of historical transactions for each card_id\n-  merchants.csv - additional information about all merchants \/ merchant_ids in the dataset.\n-  new_merchant_transactions.csv - two months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.\n","1b98cce4":"## Training","19041bc4":"## 1. Setting up the Environment and Loading Data"}}