{"cell_type":{"de64b40b":"code","c7c7962c":"code","db9f2b6e":"code","eedfd3d8":"code","79e17b1e":"code","c73e67d1":"code","1f8a7af0":"code","77232b96":"code","2ef991d5":"code","ac7fa123":"code","d072c924":"code","d5708db8":"code","127c3c55":"code","9199a110":"code","f658e71f":"code","5f66ac48":"code","d9d81793":"code","7e29aa64":"code","af7f94bd":"code","09946f88":"code","180cf40d":"code","9e042e9c":"code","29cc4997":"code","2274463e":"code","cb57406a":"code","4cb23ea0":"code","16f5721e":"code","dd120dbc":"code","a317a959":"code","5150f076":"code","22f395a7":"code","589673ed":"code","144be10c":"code","76a59cf1":"code","1c5a0fdc":"markdown","6060c2b9":"markdown","f646fec6":"markdown","30338386":"markdown","d0f2d25b":"markdown","03af26c6":"markdown","b4e4375b":"markdown","8d773e25":"markdown"},"source":{"de64b40b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7c7962c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil \n%matplotlib inline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","db9f2b6e":"# Load the training data\ndf_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n\n# Preview the data\ndf_train.head()","eedfd3d8":"bins = np.arange(0, 12, 0.1)\nsns.displot(df_train.target, height = 10, aspect = 1, bins = bins);","79e17b1e":"df_train.dtypes","c73e67d1":"df_test.dtypes","1f8a7af0":"df_train.info()","77232b96":"df_test.info()","2ef991d5":"plt.figure(figsize= (20, 15))\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.transpose(np.tril(np.ones(df_train.corr().shape)))\nsns.heatmap(df_train.corr(), annot = True, center = 0.5, cmap = 'RdBu', mask = mask);","ac7fa123":"plt.figure(figsize= (20, 15))\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.transpose(np.tril(np.ones(df_test.corr().shape)))\nsns.heatmap(df_test.corr(), annot = True, center = 0.5, cmap = 'RdBu', mask = mask);","d072c924":"num_cols = [col for col in df_train.columns if 'cont' in col] \nnum_cols","d5708db8":"def plot(data, cols, features_type, nrows, ncols, bins='auto', target=None, figsize=None,\n         hspace=None, wspace=None, color = None):\n    '''plot all features vs target or the distribution of features'''\n    if figsize != None:\n        plt.figure(figsize = figsize)\n    for col, plot_num in zip(cols, list(range(1, len(cols)))):\n        plt.subplot(nrows, ncols, plot_num)\n        if hspace != None or wspace != None:\n            plt.subplots_adjust(hspace = hspace, wspace = wspace)\n            \n        if features_type == 'numerical':\n            if target != None:\n                plt.scatter(data[col], data[target])\n                plt.title(col)\n            else:\n                sns.histplot(data[col], bins=bins)\n                \n        if features_type == 'categorical':\n            if target != None:\n                sns.violinplot(data=data, y=col, x=target, color=color, inner='quartile');\n            else:\n                countplot_ratio(x = col, data = data, color = color)","127c3c55":"n_cols = 4\nn_rows = ceil(len(num_cols)\/n_cols)\nbins = np.arange(0, 1.3, 0.02)\nplot(data=df_train, cols=num_cols, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3, wspace=0.5, bins=bins,\n    figsize = (20, 20))","9199a110":"n_cols = 4\nn_rows = ceil(len(num_cols)\/n_cols)\nbins = np.arange(0, 1.3, 0.02)\nplot(data=df_test, cols=num_cols, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3, wspace=0.5, bins=bins,\n    figsize = (20, 20))","f658e71f":"n_cols = 4\nn_rows = ceil(len(num_cols)\/n_cols)\nplot(data=df_train, target='target', cols=num_cols, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3,\n    figsize = (15, 15))","5f66ac48":"object = [col for col in df_train.columns if 'cat' in col]\nobject","d9d81793":"def countplot_ratio(x = None, data = None, hue = None, ax = None, color = None):\n    # plot the variable\n    ax = sns.countplot(x, data = data, hue = hue, ax = ax, color = color)\n    # names of x labels\n    ax.set_xticklabels(ax.get_xticklabels())\n    # plot title\n    ax.set_title(x + \" Distribution\")\n    # total number of data which used to get the proportion\n    total = float(len(data))\n    # for loop to iterate on the patches\n    for patch in ax.patches:\n        # get the height of the patch which represents the number of observations.\n        height = patch.get_height()\n        # Put text on each patch with the proportion of the observations\n        ax.text(patch.get_x()+patch.get_width()\/2,height+4,'{:.2f}%'.format((height\/total)*100),weight = 'bold',\n                fontsize = 12,ha = 'center')","7e29aa64":"n_cols = 2\nn_rows = ceil(len(object)\/n_cols)\nbase_color = sns.color_palette(n_colors=2)[1]\nplot(data=df_train, cols=object, features_type='categorical', nrows=n_rows, ncols=n_cols,\n     hspace=0.5, figsize = (15, 20), color='0.75')","af7f94bd":"\nn_cols = 3\nn_rows = ceil(len(object)\/n_cols)\nplot(data=df_train, target='target', cols=object, features_type='categorical',\n     nrows=n_rows, ncols=n_cols, hspace=0.5, figsize = (15, 20), color='0.75')","09946f88":"y = df_train['target']\nfeatures = df_train.drop(['target'], axis=1)\n\nfeatures.head(15)","180cf40d":"# ordinal-encode categorical columns\nX = features.copy()\nX_test = df_test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object] = ordinal_encoder.fit_transform(features[object])\nX_test[object] = ordinal_encoder.transform(df_test[object])\n\n# Preview the ordinal-encoded features\nX.head(15)","9e042e9c":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state =0)","29cc4997":"model = XGBRegressor(n_estimators=1375, max_depth = 3,learning_rate=0.14, colsample_bytree= 0.5,\n                     subsample=0.99, random_state=1, reg_alpha = 25.4)\n\n# Train the model \nmodel.fit(X_train, y_train, early_stopping_rounds = 100, eval_set=[(X_valid, y_valid)], verbose=False)\npreds_valid = model.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","2274463e":"from sklearn.model_selection import GridSearchCV\ndef  Hyperparameter_tuning(params):\n        '''We use this function to get the best hyperparameters\n        Parameters\n        ----------\n        param: dict\n        A dictionary of hyperparameters names and lists of possible values of it\n        example:\n        param = { 'max_depth': [3,6,10],\n           'learning_rate': [0.01, 0.05, 0.1],\n           'n_estimators': [100, 500, 1000],\n           'colsample_bytree': [0.3, 0.7]}'''\n        params = params\n        model = XGBRegressor(n_estimators=1000, max_depth = 3,learning_rate=0.14,\n                             colsample_bytree= 0.5, subsample=0.99, random_state=1,\n                             reg_alpha = 25.4, tree_method = 'gpu_hist')\n        clf = GridSearchCV(estimator=model, \n                           param_grid=params,\n                           scoring='neg_mean_squared_error', \n                           verbose=2)\n        clf.fit(X, y)\n        print(\"Best parameters:\", clf.best_params_)\n        print(\"Lowest RMSE: \", (-clf.best_score_)**(1\/2.0))","cb57406a":"predictions = model.predict(X_test)","4cb23ea0":"sub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsub.target = predictions\n\nsub.to_csv('submission2.csv', index=False)\nsub.head()","16f5721e":"sns.heatmap(df_train.isnull(),yticklabels=False)","dd120dbc":"sns.heatmap(df_test.isnull(),yticklabels=False)","a317a959":"sns.boxplot(x = df_train[\"target\"])\nplt.show()","5150f076":"target = ['target']\nvar_categorical = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9' ]\nvar_numerical = list(set(df_train.columns) - set(var_categorical) - set(target))","22f395a7":"def label_values(ax, spacing=5):\n    total = 0\n    for rect in ax.patches:\n        total += rect.get_height()\n\n    for rect in ax.patches:\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        space = spacing\n        \n        va = 'bottom'\n        \n        if y_value < 0:\n            space *= -1\n            va = 'top'\n        label = \"{:.2f}, {:.2f}\".format(y_value, y_value\/total*100)\n        ax.annotate(\n            label,                      \n            (x_value, y_value),         \n            xytext=(0, space),          \n            textcoords=\"offset points\", \n            ha='center',                \n            va=va)","589673ed":"from tensorflow.keras import utils as np_utils\nfrom keras.utils import to_categorical\nfor column in var_categorical:\n    plt.figure(figsize=(15, 6))\n    print(column.title())\n    ax = sns.countplot(x = df_train[column])\n    label_values(ax)\n    plt.show()","144be10c":"for column in var_categorical:\n    plt.figure(figsize=(15, 6))\n    print(column.title())\n    ax = sns.boxplot(x = df_train[column], y = df_train['target'])\n    label_values(ax)\n    plt.show()","76a59cf1":"print(\"Final Submission Of 30 Days Machine Learning\")","1c5a0fdc":"# Disturbution of continous variable vs target","6060c2b9":"# Separate target from features","f646fec6":"# Reading the Dataset","30338386":"# Distrubution of categorical features","d0f2d25b":"# Prepare the data","03af26c6":"# Corelation","b4e4375b":"# Train the model","8d773e25":"# Disturbution of target variable"}}