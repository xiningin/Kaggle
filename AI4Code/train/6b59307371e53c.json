{"cell_type":{"798216f3":"code","006fb0b6":"code","46cd5318":"code","0d7cf458":"code","bb44fd84":"code","51186f42":"code","81f24ab5":"code","ca3acf83":"code","bb431e8a":"code","376dc690":"code","6c60e618":"code","68d84011":"code","560b7319":"code","9b33275d":"code","ca966844":"code","2116cb8a":"code","c8d3e305":"code","a3c2e46c":"code","e6878200":"code","eae1cbc0":"code","1bf9d688":"code","84faf5c0":"code","10a7cabc":"code","a10c4431":"code","2617c2ce":"code","61772b03":"code","364ab6ce":"code","afb9c607":"code","76730f25":"code","c592a1b0":"code","08b7d495":"code","4c26442e":"code","cc5e21f9":"code","3bb12eb8":"code","2f148c52":"code","f3f0af29":"code","b8d8df00":"code","49081582":"code","6873f42b":"code","6031f0fd":"code","4209441d":"code","335447ab":"code","803a6ec5":"code","ead8b5ef":"code","64dd770b":"code","3a997284":"code","752acfa2":"code","e5d367bb":"code","9195a54b":"code","3e71a881":"code","dacc7afa":"code","72c5a6f4":"code","9dabb11e":"code","86d31e48":"code","d51f3892":"code","19e4a981":"code","dc9eaee7":"code","e61a6ce7":"code","9b1e2fe8":"code","a85a3707":"code","2db81930":"code","e84161d7":"code","ae9cf00a":"code","2da5263b":"code","66f6322e":"code","55de58bd":"code","b8f2952b":"code","82a493d6":"code","ce892ca1":"code","42536bcf":"code","302bb1bf":"code","c472b120":"code","b2e69894":"code","abd601df":"code","15c18d36":"code","2b735608":"code","47ba269e":"code","f4a0f75d":"code","18020570":"markdown","ec18e378":"markdown","9a6dacc7":"markdown","cce616e8":"markdown","b703a511":"markdown","b6072eea":"markdown","ac1e006b":"markdown","a5c04dc3":"markdown","ad555c97":"markdown","3193a658":"markdown","4dc1ea7e":"markdown","bcfea022":"markdown","e22a40e9":"markdown","d04df444":"markdown","0de126e0":"markdown"},"source":{"798216f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","006fb0b6":"# Carregando os dados\ndf = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')\n\ndf.shape, test.shape","46cd5318":"# Juntando os dataframes\ndf_all = df.append(test)\n\ndf_all.shape","0d7cf458":"df_all.columns","bb44fd84":"df['Target'].isnull().sum()","51186f42":"df_all['Target'].value_counts()","81f24ab5":"# Verificando tamanhos e tipos\ndf_all.info()","ca3acf83":"df.dtypes.value_counts()","bb431e8a":"df_all.select_dtypes(include='object')\n","376dc690":"# Olhando a coluna dependency\ndf_all['dependency'].value_counts()","6c60e618":"# Vamos transformar 'yes' em 1 e 'no' em 0\n# na coluna dependency\nmapeamento = {'yes': 1, 'no': 0}\ndf_all['dependency'] = df_all['dependency'].replace(mapeamento).astype(float)","68d84011":"# Analisando os dados da coluna edjefa\ndf_all['edjefa'].value_counts()","560b7319":"# Analisando os dados da coluna edjefe\ndf_all['edjefe'].value_counts()","9b33275d":"# Vamos transformar 'yes' em 1 e 'no' em 0\n# nas colunas edjefa e edjefe\nmapeamento = {'yes': 1, 'no': 0}\n\ndf_all['edjefa'] = df_all['edjefa'].replace(mapeamento).astype(int)\ndf_all['edjefe'] = df_all['edjefe'].replace(mapeamento).astype(int)","ca966844":"# Visualizando do comando info\ndf_all.info()","2116cb8a":"# Verificando os valores nulos\ndf_all.isnull().sum()","c8d3e305":" # Verificando os valores de aluguel (v2a1) para os chefes\/as de familia (parentesco1 = 1)\ndf_all[df_all['parentesco1'] == 1]['v2a1'].isnull().sum()","a3c2e46c":"# Qual a cara dos dados de v18q\ndf_all['v18q'].value_counts()","e6878200":"# Prenchendo com -1 os valores nulos de v2a1\ndf_all['v2a1'].fillna(-1, inplace=True)","eae1cbc0":"# Prenchendo com 0 os valores nulos de v18q1\ndf_all['v18q1'].fillna(0, inplace=True)","1bf9d688":"# Verificando os valores nulos\ndf_all.isnull().sum().sort_values()","84faf5c0":"# Prenchendo com -1 os valores nulos de SQBmeaned, meaneduc e rez_esc\ndf_all['SQBmeaned'].fillna(-1, inplace=True)\ndf_all['meaneduc'].fillna(-1, inplace=True)\ndf_all['rez_esc'].fillna(-1, inplace=True)","10a7cabc":"df_all.describe()\n","a10c4431":"#Visualiza\u00e7\u00e3o da distribui\u00e7\u00e3o das vari\u00e1veis\ndf_all.hist(figsize=(20,15))\nplt.show()","2617c2ce":"# Separando as colunas para treinamento\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]","61772b03":"# Separar os dataframes\ntrain, test = df_all[~df_all['Target'].isnull()], df_all[df_all['Target'].isnull()]\n\ntrain.shape, test.shape","364ab6ce":"# Instanciando o random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42)","afb9c607":"# Treinando o modelo\nrf.fit(train[feats], train['Target'])","76730f25":"# Prever o Target de teste usando o modelo treinado\ntest['Target'] = rf.predict(test[feats]).astype(int)","c592a1b0":"# Vamos verificar as previs\u00f5es\ntest['Target'].value_counts(normalize=True)","08b7d495":"# Criando o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","4c26442e":"import matplotlib.pyplot as plt\n\nfig=plt.figure(figsize=(15, 20))\n\n# Avaliando a importancia de cada coluna (cada vari\u00e1vel de entrada)\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","cc5e21f9":"# Verificando a classe target nos dados de treino\ntrain['Target'].value_counts(normalize=True)","3bb12eb8":"# Limitando o treinamento as\/aos chefas\/es de familia\n\n# Coluna parentesco1\nheads = train[train['parentesco1'] == 1]","2f148c52":"# Criando, treinando, fazendo previs\u00f5es e gerando o arquivo de submiss\u00e3o com RF2\n# Dados de treinao apenas dos chefes\/chefas de familia\n\nrf2 = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42)\n\nrf2.fit(heads[feats], heads['Target'])\n\ntest['Target'] = rf2.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","f3f0af29":"# Qual o tamanho da base de treino heads?\nheads.shape","b8d8df00":"# Verificando os valores da coluna hhsize\ntrain['hhsize'].value_counts()","49081582":"# Verificando os dados da coluna tamviv\ntrain['tamviv'].value_counts()","6873f42b":"# Verificando os dados da coluna tamhog\ntrain['tamhog'].value_counts()","6031f0fd":"# Feature Engineering \/ Cria\u00e7\u00e3o de novas colunas\n\n# Rela\u00e7\u00e3o tamanho da casa \/ moradores\ndf_all['hhsize-pc'] = df_all['hhsize'] \/ df_all['tamviv']\n\n# Rela\u00e7\u00e3o qtde celulares \/ moradores\ndf_all['mobile-pc'] = df_all['qmobilephone'] \/ df_all['tamviv']\n\n# Rela\u00e7ao qtde de tablets \/ moradores\ndf_all['tablet-pc'] = df_all['v18q1'] \/ df_all['tamviv']\n\n# Rela\u00e7\u00e3o qtde de quartos \/ moradores\ndf_all['rooms-pc'] = df_all['rooms'] \/ df_all['tamviv']","4209441d":"# Separando as colunas para treinamento\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]","335447ab":"# Separar os dataframes\ntrain, test = df_all[~df_all['Target'].isnull()], df_all[df_all['Target'].isnull()]\n\ntrain.shape, test.shape","803a6ec5":"# Criando, treinando, fazendo previs\u00f5es e gerando o arquivo de submiss\u00e3o com RF3\n# Dados de treino com 4 colunas a mais\n\nrf3 = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42)\n\nrf3.fit(train[feats], train['Target'])\n\ntest['Target'] = rf3.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","ead8b5ef":"fig=plt.figure(figsize=(15, 20))\n\n# Avaliando a importancia de cada coluna (cada vari\u00e1vel de entrada)\npd.Series(rf3.feature_importances_, index=feats).sort_values().plot.barh()","64dd770b":"# Juntando as abordagens\n\n# Selecionando para treio s\u00f3 parentesco1 == 1\nheads2 = train[train['parentesco1'] == 1]","3a997284":"# Criando, treinando, fazendo previs\u00f5es e gerando o arquivo de submiss\u00e3o com RF4\n# Dados de treino apenas dos chefes\/chefas de familia e 4 colunas a mais\n\nrf4 = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42)\n\nrf4.fit(heads2[feats], heads2['Target'])\n\ntest['Target'] = rf4.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","752acfa2":"# Verificando os dados de treino e teste\ntrain.shape, test.shape","e5d367bb":"# Usando o Out-Of-Bag (oob_score)\n\n# Criando, treinando, fazendo previs\u00f5es e gerando o arquivo de submiss\u00e3o com RF3\n# Dados de treino com 4 colunas a mais + oob_score\n\nrf5 = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42, oob_score=True, class_weight='balanced')\n\nrf5.fit(train[feats], train['Target'])\n\ntest['Target'] = rf5.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)\n","9195a54b":"# Voltando as origens\n\n# Carregando os dados\ndf = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')\n\ndf.shape, test.shape","3e71a881":"# Juntando os dataframes\ndf_all = df.append(test)\n\ndf_all.shape","dacc7afa":"# Vamos transformar 'yes' em 1 e 'no' em 0\ndf_all['edjefa'] = df_all['edjefa'].replace(mapeamento).astype(int)\ndf_all['edjefe'] = df_all['edjefe'].replace(mapeamento).astype(int)\ndf_all['dependency'] = df_all['dependency'].replace(mapeamento).astype(float)","72c5a6f4":"# Preenchendo os valores nulos\ndf_all['v2a1'].fillna(-1, inplace=True)\ndf_all['v18q1'].fillna(0, inplace=True)\ndf_all['SQBmeaned'].fillna(-1, inplace=True)\ndf_all['meaneduc'].fillna(-1, inplace=True)\ndf_all['rez_esc'].fillna(-1, inplace=True)","9dabb11e":"# Separando as colunas para treinamento\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'edjefe', 'edjefa','Target']]","86d31e48":"# Separar os dataframes\ntrain, test = df_all[~df_all['Target'].isnull()], df_all[df_all['Target'].isnull()]\n\ntrain.shape, test.shape","d51f3892":"# Usando o Out-Of-Bag (oob_score)\n\n# Criando, treinando, fazendo previs\u00f5es e gerando o arquivo de submiss\u00e3o com RF6\n# oob_score + class_weight\n\nrf6 = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42, oob_score=True, class_weight='balanced')\n\nrf6.fit(train[feats], train['Target'])\n\ntest['Target'] = rf6.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","19e4a981":"# Copiando do campe\u00e3o\nrf7 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')\n\nrf7.fit(train[feats], train['Target'])\n\ntest['Target'] = rf7.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","dc9eaee7":"# Mudan\u00e7a 1 - max_features=n_features\nrf8 = RandomForestClassifier(max_depth=None, max_features=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')\n\nrf8.fit(train[feats], train['Target'])\n\ntest['Target'] = rf8.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","e61a6ce7":"# Mudan\u00e7a 2 - criterion=\"entropy\"\nrf9 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2, criterion=\"entropy\",\n                            verbose=0, class_weight='balanced')\n\nrf9.fit(train[feats], train['Target'])\n\ntest['Target'] = rf9.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","9b1e2fe8":"# Mudan\u00e7a 3 - max_leaf_nodes=10\nrf10 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700, max_leaf_nodes=10,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')\n\nrf10.fit(train[feats], train['Target'])\n\ntest['Target'] = rf10.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","a85a3707":"# Importando a biblioteca\nfrom sklearn.utils import resample\nimport scikitplot as skplt\n\n# Separando os dados de acordo com a classifica\u00e7\u00e3o\ndf_1 = df_all[df_all['Target'] == 1]\ndf_2 = df_all[df_all['Target'] == 2]\ndf_3 = df_all[df_all['Target'] == 3]\ndf_4 = df_all[df_all['Target'] == 4]\n\ndf_1.shape, df_2.shape, df_3.shape, df_4.shape","2db81930":"# Over-Sampling\ndf_1_over = resample(df_1, # vamos aumentar a classe menor\n                       replace=True, # sample com replacement\n                       n_samples=len(df_4), # igualando a maior classe\n                       random_state=42)\n\n# Over-Sampling\ndf_2_over = resample(df_2, # vamos aumentar a classe menor\n                       replace=True, # sample com replacement\n                       n_samples=len(df_4), # igualando a maior classe\n                       random_state=42)\n\n# Over-Sampling\ndf_3_over = resample(df_3, # vamos aumentar a classe menor\n                       replace=True, # sample com replacement\n                       n_samples=len(df_4), # igualando a maior classe\n                       random_state=42)\n\n\n\n# juntando os dados\ndf_over= pd.concat([df_1_over, df_2_over, df_3_over, df_4])\n\n# check new class counts\ndf_over['Target'].value_counts()","e84161d7":"# Quais colunas do dataframe s\u00e3o do tipo object\ndf_over.select_dtypes('object').head()","ae9cf00a":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# Executando o modelo com df_over\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_over, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['Target'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['Target'], preds_test)","2da5263b":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['Target'], preds_test)","66f6322e":"# Under-Sampling\ndf_2_under = resample(df_2, # vamos diminuir as classes maiores\n                       replace=False, # sample sem replacement\n                       n_samples=len(df_1), # igualando a menor classe\n                       random_state=42)\n\n# Under-Sampling\ndf_3_under = resample(df_3, # vamos aumentar a classe menor\n                       replace=False, # sample se replacement\n                       n_samples=len(df_1), # igualando a menor classe\n                       random_state=42)\n# Under-Sampling\ndf_4_under = resample(df_4, # vamos aumentar a classe menor\n                       replace=False, # sample se replacement\n                       n_samples=len(df_1), # igualando a menor classe\n                       random_state=42)\n\n\n\n# juntando os dados\ndf_under= pd.concat([df_1, df_2_under, df_3_under, df_4_under])\n\n# check new class counts\ndf_under['Target'].value_counts()","55de58bd":"# Executando o modelo com df_under\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_under, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['Target'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['Target'], preds_test)","b8f2952b":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['Target'], preds_test)","82a493d6":"# Importando a biblioteca\nimport imblearn","ce892ca1":"# Separando os dados de entrada e o target\nX, y = df_all[feats], df_all[['Target']]","42536bcf":"# Importando a biblioteca\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Fazendo o over-sampling\nros = RandomOverSampler(random_state=42)\nX_ros,y_ros= ros.fit_resample(X,y)\n\n# Verificando os dados\ny_ros['Target'].value_counts()","302bb1bf":"# Executando o modelo com imblearn over-sampling\n\n# Juntando os dados\ndf_over = pd.concat([X_ros, y_ros], axis=1)\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_over, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['Target'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['Target'], preds_test)","c472b120":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['Target'], preds_test)","b2e69894":"# Importando a biblioteca\nfrom imblearn.under_sampling import TomekLinks\n\n# Fazendo o under-sampling\ntl = TomekLinks()\nX_tl, y_tl = tl.fit_resample(X,y)\n\n# Verificando os dados\ny_tl['Target'].value_counts()","abd601df":"# Executando o modelo com Tomek-links\n\n# Juntando os dados\ndf_under = pd.concat([X_tl, y_tl], axis=1)\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_under, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['Target'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['Target'], preds_test)","15c18d36":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['Target'], preds_test)","2b735608":"# Importando a biblioteca\nfrom imblearn.over_sampling import SMOTE\n\n# Fazendo o under-sampling\nsm = SMOTE()\nX_sm, y_sm = sm.fit_resample(X,y)\n\n# Verificando os dados\ny_sm['Target'].value_counts()","47ba269e":"# Executando o modelo com SMOTE\n\n# Juntando os dados\ndf_over = pd.concat([X_sm, y_sm], axis=1)\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_over, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['Target'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['Target'], preds_test)","f4a0f75d":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['Target'], preds_test)","18020570":"Random Over-Sampling\nAumentando a classe minorit\u00e1ria","ec18e378":"# IV - Balanceamento das Classes\n","9a6dacc7":"Imblearn Tomek-links (under-sampling)","cce616e8":"Existe uma diferen\u00e7a entre propriedades de cada uma das \u00e1rvores, e as propriedades da floresta (sacola de \u00e1rvores).\nOs principais par\u00e2metros s\u00e3o n_estimators (n\u00famero de \u00e1rvores na floresta) e max_features (o tamanho do subconjunto de vari\u00e1veis que ser\u00e3o consideradas quando o modelo faz a divis\u00e3o do n\u00f3).\nA documenta\u00e7\u00e3o do scikit-learn recomenda utilizar max_features=\"sqrt\" para tarefas de classifica\u00e7\u00e3o. Uma possibilidade seria n\u00e3o utilizar um subconjunto, e sim todas as vari\u00e1veis, como \u00e9 recomendado por Geurts, Erns e Wehenkel (2006) para o modelo the RandomForestRegressor\n\nhttps:\/\/orbi.uliege.be\/bitstream\/2268\/9357\/1\/geurts-mlj-advance.pdf\n","b703a511":"N\u00famero m\u00e1ximo de folhas \u00e9 utilizado para limitar a quantidade de folhas, escolhendo apenas as melhores folhas. ","b6072eea":"# IESB - Graduacao - CIA028 - Costa Rica","ac1e006b":"# II - Aplicando o Modelo Random Forest\n","a5c04dc3":"Usando a bilioteca imbalanced-learn\nEssa biblioteca implementa diversos modelos diferentes para tratar classes desabalanceadas","ad555c97":"Imblearn SMOTE (over-sampling)","3193a658":"# III - Modifica\u00e7\u00f5es no Modelo (Prova 1)","4dc1ea7e":"Random Under-Sampling\nDiminuindo a classe majorit\u00e1ria","bcfea022":"Imblearn Random Over-Sampling","e22a40e9":"Tive problemas com essa biblioteca:\nModuleNotFoundError: No module named 'sklearn.neighbors._base'","d04df444":"O modelo RandomForestClassifier pode utilizar dois m\u00e9todos de avalia\u00e7\u00e3o da qualidade da divis\u00e3o: gini e entropia.\nGini - Mede o quanto um elemento aleatoriamente escolhido do conjunto seria incorretamente classificado. Quanto maior o valor gini, mais \"impuro\", ou seja, mais o elemento aleat\u00f3rio seria incorretamente classificado.\nEntropia - mede a \"desordem\" geral de uma segmenta\u00e7\u00e3o da \u00e1rvore. \u00c9 um m\u00e9todo mais pesado computacionalmente do que o gini, por usar logbase2 das probabilidades\n\nLaura Elena Raileanu e Kilian Stoffel estimam que s\u00f3 h\u00e1 diferen\u00e7a entre as duas medidas em 2% dos casos.\n\nhttps:\/\/www.unine.ch\/files\/live\/sites\/imi\/files\/shared\/documents\/papers\/Gini_index_fulltext.pdf\n","0de126e0":"# I - An\u00e1lise Explorat\u00f3ria e tratamento dos dados\n"}}