{"cell_type":{"adb89690":"code","987baf54":"code","ee62dbde":"code","223c9f4c":"code","e5810751":"code","29b714ef":"code","64695089":"code","6c801caf":"code","55a20039":"code","80ac1d4a":"code","fe007dcf":"code","6475e66e":"code","3475b62b":"code","8348680a":"code","04022197":"code","55fcf9cc":"code","8e209ca2":"code","c5ad89be":"code","67086fde":"code","680bc279":"code","9ac61fd4":"code","b98d3b9c":"code","0fd81399":"code","c154e6fd":"code","bd33f4c9":"code","fdf71527":"code","6b55eaa2":"code","3e72c7e4":"code","da4bf5d5":"code","6498db1c":"code","7042996d":"code","39f91dcf":"code","3f0e1ba1":"code","1964144e":"code","ade953e3":"code","99aa6ca7":"code","0e9c48dc":"code","e0e15bf7":"code","59949640":"code","3e4d06a6":"code","d3e59744":"code","f9723bae":"code","e0a694b8":"code","e288ef22":"code","f32b206f":"code","5b04ea6d":"code","42e63550":"code","bd3a89d2":"code","d5fb28f6":"markdown","cc18a45a":"markdown","b4e99360":"markdown","58d60f72":"markdown","13922716":"markdown","c04f1821":"markdown","9539045f":"markdown","273d95c3":"markdown","2283aabd":"markdown","f0d56ba3":"markdown","e9135a10":"markdown","d0586d23":"markdown","da086c30":"markdown"},"source":{"adb89690":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import clone\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\n\n\n# setting styles for plotting\nplt.rcParams['figure.figsize'] = [10, 5]\nsns.set_palette('husl')\nsns.set_style('whitegrid')\nsns.set_context('talk')","987baf54":"df = pd.read_csv('..\/input\/Melbourne_housing_FULL.csv')\n\nprint(df.shape)\ndf.head()","ee62dbde":"df.columns","223c9f4c":"# 1st things 1st - we're trying to predict value - can't do that with nan values for price\n#dropping rows where price = nan\ndf = df[np.isfinite(df.Price)]\ndf.head()","e5810751":"df.describe()","29b714ef":"# checking for outliers - 75% of homes have 4 rooms and the max is 16\ndf.loc[df['Rooms'] > 10]","64695089":"# car seems like it has some outliers ... \ndf.loc[df['Car'] > 10]","6c801caf":"df.info()","55a20039":"#checking for missing values\n\n# from https:\/\/github.com\/WillKoehrsen\/machine-learning-project-walkthrough\/blob\/master\/Machine%20Learning%20Project%20Part%201.ipynb# from  \n# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\nmissing_values_table(df).head(10)","80ac1d4a":"def drop_missing_values(df, percent_drop):\n    \"\"\"\n    Drop columns with missing values.\n    \n    Args:\n        df = dataframe\n        percent_drop = percentage of null values above which the column will be dropped\n            as decimal between 0 and 1\n    Returns:\n        df = df where columns above percent_drop are dropped.\n    \n    \"\"\"\n    to_drop = [column for column in df if (df[column].isnull().sum()\/len(df) >= percent_drop)]\n\n    print('Columns to drop: ' , (len(to_drop)))\n    # Drop features \n    df = df.drop(columns=to_drop)\n    print('Shape: ', df.shape)\n    return df","fe007dcf":"#dropping columns where >60% of values missing\ndf = drop_missing_values(df, .6)","6475e66e":"# find correlations to target = price\ncorr_matrix = df.corr().abs()\n\nprint(corr_matrix['Price'].sort_values(ascending=False).head(20))","3475b62b":"# Visualizing the correlation matrix\n# Select upper triangle of correlation matrix\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nsns.heatmap(upper)\nplt.show();","8348680a":"#dropping highly correlated features\n#code from: https:\/\/chrisalbon.com\/machine_learning\/feature_selection\/drop_highly_correlated_features\/\n\n# Find index of feature columns with correlation greater than 0.90\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\nprint('Columns to drop: ' , (len(to_drop)))\n\n# Drop features \ndf = df.drop(columns=to_drop)\nprint('train_features_df shape: ', df.shape)","04022197":"# plotting distribution of price ... \n# The long right tail indicates there are probably highly priced outliers\nsns.distplot(df.Price);","55fcf9cc":"#checking data types of columns\ndf.dtypes","8e209ca2":"# checking for categorical variables\ndf.select_dtypes('object').apply(pd.Series.nunique, axis=0)","c5ad89be":"df.shape","67086fde":"# Address has a very high percentage of unique values\n# Theoretically each home has a different address\n# Duplicated are being resold possibly\n# But bottom line I'm assuming address won't be valuable in the model so dropping the column\n\ndf = df.drop(['Address'], 1)","680bc279":"# visualizing some categorical variables relationship to price\n\nplt_data = df[['Price', 'Suburb', 'Type', 'Method', 'Date', 'CouncilArea', 'Regionname']].copy()\nplt_data = plt_data.dropna()\n\nfig, ax = plt.subplots(figsize=(20,5))\n\nfig.add_subplot(121)\nsns.boxplot(x=plt_data.Type, y=plt_data.Price)\n\nfig.add_subplot(122)\nsns.stripplot(x=plt_data.Method, y=plt_data.Price)\n\nplt.show;","9ac61fd4":"sns.boxplot(x=plt_data.Regionname, y=plt_data.Price)\nplt.xticks(rotation=90);","b98d3b9c":"# I don't think the seller should have anything to do with the home price,,,\n# but plotting just to be sure\nfig, ax = plt.subplots()\n\nplt.scatter(df.SellerG, df.Price)\nax.set_xticks([])\nplt.show();","0fd81399":"# dropping column SellerG\ndf = df.drop('SellerG', axis=1)","c154e6fd":"#Checking the Date column\nfig, ax = plt.subplots()\n\nplt.scatter(df.Date, df.Price)\nax.set_xticks([])\nplt.show();","bd33f4c9":"# Date also doesn't seem like it will be useful so dropping\ndf = df.drop('Date', axis=1)","fdf71527":"# checking categorical variables again\ndf.select_dtypes('object').apply(pd.Series.nunique, axis=0)","6b55eaa2":"#plotting postcode vs price\n#it definitely looks like certain postcodes are more popular and expensive\nplt.scatter(df.Postcode, df.Price);","3e72c7e4":"# converting postcode to categorical\n# set up bins\n# just setting up random bins by looking at the plot above\nbins = [3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000,]\ncategory = pd.cut(df.Postcode, bins)\ncategory = category.to_frame()\ndf['Postcode'] = category","da4bf5d5":"# and checking to make sure it worked\n\nsns.barplot(x=df.Postcode, y=df.Price)\nplt.xticks(rotation=90);","6498db1c":"# one hot encoding categorical variables\ndf = pd.get_dummies(df)","7042996d":"# and checking to make sure it worked\ndf.select_dtypes('object').apply(pd.Series.nunique, axis=0)","39f91dcf":"# Now finding the outliers - starting with the really expensive homes\ndf.sort_values('Price', ascending=False).head()","3f0e1ba1":"# dropping the home with the 11 million dollar price\ndf = df.drop(25635)","1964144e":"# checking for really inexpensive homes\ndf.sort_values('Price', ascending=True).head()","ade953e3":"# dropping index 4378\n#df = df.drop(4378)","99aa6ca7":"# we need to deal with missing values before modeling\nmissing_values_table(df)","0e9c48dc":"# dropping YearBuilt since over 50% of values missing\ndf = df.drop(['YearBuilt'], 1)","e0e15bf7":"# seperating our target out from the dataframe\nprice = df.Price\ndf = df.drop(['Price'], 1)","59949640":"# imputing values for remaining null values\nimputer = Imputer(strategy='mean')\n\nimputed = imputer.fit_transform(df)","3e4d06a6":"# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(imputed, price, test_size=0.25, random_state=27)","d3e59744":"# normalizing the data\nscaler = StandardScaler()\nX_test = scaler.fit_transform(X_test)\nX_train = scaler.fit_transform(X_train)","f9723bae":"from sklearn import ensemble\n\nGBR = ensemble.GradientBoostingRegressor().fit(X_train, y_train)","e0a694b8":"y_ = GBR.predict(X_test)\ny_","e288ef22":"GBR.score(X_test, y_test)","f32b206f":"# checking which are the most important features\nfeature_importance = GBR.feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[-20:-1:1]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","5b04ea6d":"GBR2 = ensemble.GradientBoostingRegressor(max_depth=5, n_estimators=500, min_samples_split=5).fit(X_train, y_train)\nGBR2.score(X_test, y_test)","42e63550":"from sklearn import metrics\nprint('MAE:',metrics.mean_absolute_error(y_test,y_))\nprint('MSE:',metrics.mean_squared_error(y_test,y_))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_)))","bd3a89d2":"# checking which are the most important features\nfeature_importance = GBR2.feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[-20:-1:1]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","d5fb28f6":"From the correlation matrix it looks like the most important variables that affect price are rooms, bedrooms, and year built.","cc18a45a":"From the max values above it looks like there are some outliers in the data.   Let's explore a little further.","b4e99360":"## Modeling - Gradient Boosting Regression","58d60f72":"  ## Exploratory Analysis and Data Cleaning","13922716":"We can see here that Rooms and Bedroom2 are highly correlated.","c04f1821":"From the variable importance plot above we can see the most important features in determining a home's selling price are rooms, landsize, type of home, and bathrooms.  Location also plays an important role in price.  For future research it might be interesting to further explore location's affect on price.","9539045f":"It looks like seller could play a role in Price, but we'll drop the column for now.","273d95c3":"From this plot we can see that a larger percentage of homes are in Southern Metropolitan and that they are generally priced higher and have the most outliers.","2283aabd":"Let's tune the model to see if we can improve the accuracy score.","f0d56ba3":"These plots tell us that h type (house) is generally priced higher than t (townhouse) which is higher than u (unit), but all values have outliers.\n\nAlso SA method seems to be priced lower than other methods so it appears that method of selling has an impact on price.","e9135a10":"# Predicting Housing Prices\n\nData Source: https:\/\/www.kaggle.com\/anthonypino\/melbourne-housing-market\n\nHere we are using supervised learning to predict housing prices.  We also are interested in determining the most important features for determining a home's price.","d0586d23":"Technically we've dealt with all the categorical variables, but what about postcode?  \nSince it is numeric it seems like its being classified as continuous, but really I think it should be categorical because a higher number isn't neccesarily better.","da086c30":"We get an accuracy score of 75% without tuning the model!"}}