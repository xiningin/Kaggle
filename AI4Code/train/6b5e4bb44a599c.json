{"cell_type":{"2ec0ed72":"code","f1ffa75e":"code","392942e1":"code","746ce658":"code","5c544d14":"code","636fdd90":"code","7c374bd9":"code","56002075":"code","10dc5eb9":"code","e00083dd":"code","f7d56815":"code","06795e7b":"code","1dcd3237":"code","e18939df":"code","77747708":"code","b1dfd6a3":"code","f4e73aad":"code","225f4c4a":"code","43e39a15":"code","927dce21":"code","ae2da94c":"code","1536ba74":"code","49bd541f":"code","6e2be50d":"code","da8aea1f":"code","fe33d3ea":"code","08b53343":"code","4a6716af":"code","0e86923d":"code","5388e415":"code","11f6d5eb":"code","95611e57":"code","fd8a265a":"markdown","8601e97e":"markdown","4102f7d7":"markdown"},"source":{"2ec0ed72":"# Import important libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport math\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom six.moves import urllib\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nkfold = model_selection.KFold(n_splits=5,shuffle = True)","f1ffa75e":"# Get the Data\n\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\nSurvived = df_train['Survived']\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_whole = pd.concat([df_train, df_test],ignore_index=True)\ndf_whole= df_whole.drop(columns=['Survived'])\ndf_whole.info()","392942e1":"# Prepare the Data\n\nprep=df_whole.copy() # Copying the original data set \nmean_Fare = (prep.groupby('Pclass')['Fare'].mean()).values # Grouping Fare by Passenger Class which will be used to fill missing\/0 values\n# Removed prep['Age'] = prep['Age'].fillna(prep['Age'].mean()) # Filling missing Age values with mean \n\nprep.loc[(prep['Fare']==0) & (prep['Pclass']==1),'Fare']  = mean_Fare[0] \nprep.loc[(prep['Fare']==0) & (prep['Pclass']==2),'Fare']  = mean_Fare[1]\nprep.loc[(prep['Fare']==0) & (prep['Pclass']==3),'Fare']  = mean_Fare[2]\nprep['Fare'] = prep['Fare'].fillna(prep['Fare'].mean())\n\n# Create Family Size Column\nprep['FamilySize'] = prep['SibSp'] + prep['Parch'] + 1\n\n# Map gender\nprep['Sex'] = prep['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n# Replace missing Embarked with most common station (S)\nprep['Embarked'] = prep['Embarked'].fillna('S')\n# Map Embarked\nprep['Embarked'] = prep['Embarked'].map( {'C': 0, 'Q': 1, 'S': 2} ).astype(int)\n# Cabin has a lot of missing values. Making it binary\nprep['Has Cabin'] = prep[['Cabin']].applymap(lambda x: 0 if pd.isnull(x) else 1)\n\n# Title Search (inspired by https:\/\/www.kaggle.com\/dmilla\/introduction-to-decision-trees-titanic-dataset)\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\nprep['Title'] = prep['Name'].apply(get_title)\n# Grouping Titles together\nprep['Title'] = prep['Title'].replace(['Lady', 'Countess', 'Don', 'Sir', 'Jonkheer', 'Dona'], 'Royal')\nprep['Title'] = prep['Title'].replace(['Capt', 'Col', 'Dr', 'Major', 'Rev', ], 'Other')\nprep['Title'] = prep['Title'].replace('Mlle', 'Miss')\nprep['Title'] = prep['Title'].replace('Ms', 'Miss')\nprep['Title'] = prep['Title'].replace('Mme', 'Mrs')\nprep['Title'] = prep['Title'].map( {'Master': 1, 'Miss': 2, 'Mr': 3, 'Mrs':4, 'Other': 5, 'Royal':6} ).astype(int)\n\n# Remove columns not be used in the model\nprep = prep.drop(columns=['Name','Ticket','Cabin', 'Parch', 'SibSp', 'PassengerId'])\nprep.info()","746ce658":"## Creating the traning dataset\ntrain = prep.iloc[0:891]\n# Add labels\ntrain['Survived'] = Survived\ntrain.info()","5c544d14":"sns.catplot(x='Sex', y='Age',hue='Survived',col = 'Embarked',data=train,kind= 'swarm')\nsns.catplot(x='Sex', y='Age',hue='Survived',col = 'Pclass',data=train,kind= 'swarm')","636fdd90":"mean_Age = (train.groupby(['Pclass', 'Survived'])['Age'].mean()).values\ntrain['Age'] = train['Age'].fillna(0)\ntrain.loc[(train['Age']==0) & (train['Pclass']==1) & (train['Survived']==0),'Age']  = mean_Age[0]\ntrain.loc[(train['Age']==0) & (train['Pclass']==1) & (train['Survived']==1),'Age']  = mean_Age[1] \ntrain.loc[(train['Age']==0) & (train['Pclass']==2) & (train['Survived']==0),'Age']  = mean_Age[2]\ntrain.loc[(train['Age']==0) & (train['Pclass']==2) & (train['Survived']==1),'Age']  = mean_Age[3]\ntrain.loc[(train['Age']==0) & (train['Pclass']==3) & (train['Survived']==0),'Age']  = mean_Age[4]\ntrain.loc[(train['Age']==0) & (train['Pclass']==3) & (train['Survived']==1),'Age']  = mean_Age[5]","7c374bd9":"sns.catplot(x='Sex', y='Age',hue='Survived',col = 'Embarked',data=train,kind= 'swarm')\nsns.catplot(x='Sex', y='Age',hue='Survived',col = 'Pclass',data=train,kind= 'swarm')","56002075":"## Plot Pearson's Correlation Matrix\ncolormap = plt.cm.viridis\nplt.figure(figsize=(14,14))\nplt.title('Pearson Correlation of Features', y=1.05, size=18)\nsns.heatmap(train.astype(float).corr(),linewidths=0.08,vmax=1, square=True, cmap=colormap, linecolor='white', annot=True)","10dc5eb9":"## Survival Rate by Title\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n# Mapping {'Master': 1, 'Miss': 2, 'Mr': 3, 'Mrs':4, 'Other': 5, 'Royal':6}","e00083dd":"## Survival Rate by Cabin\ntrain[['Has Cabin', 'Survived']].groupby(['Has Cabin'], as_index=False).agg(['mean', 'count', 'sum'])\n# Mapping {'No Cabin': 0, 'Yes Cabin': 1}","f7d56815":"## Survival Rate by Sex\ntrain[['Sex', 'Survived']].groupby(['Sex'], as_index=False).agg(['mean', 'count', 'sum'])\n# Mapping {'female': 0, 'male': 1}","06795e7b":"## Survival Rate by Passenger Class\n\nprint (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).agg(['mean', 'count', 'sum']))\n","1dcd3237":"## Roll back some features to text category for feature mapping done later\nrev_prep = prep.copy()\ny = (train['Survived']).values\nrev_prep['Sex'] = rev_prep['Sex'].map( {0:'female', 1:'male'} ).astype(str)\nrev_prep['Embarked'] = rev_prep['Embarked'].map( {0: 'C', 1: 'Q',2: 'S'} ).astype(str)\nrev_prep['Has Cabin'] = rev_prep['Has Cabin'].map( {0: 'No Cabin', 1: 'Yes Cabin'} ).astype(str)\nrev_prep['Title'] = rev_prep['Title'].map( {1: 'Master', 2: 'Miss',3: 'Mr', 4: 'Mrs', 5: 'Other', 6: 'Royal'} ).astype(str)\nrev_prep['Pclass'] = rev_prep['Pclass'].apply(str)\nrev_prep['Pclass'] = 'P' + rev_prep['Pclass']\nrev_prep.info()","e18939df":"### Creating Training and Test Data Sets\ntrain_pre = rev_prep.iloc[0:891]\ntest_pre = rev_prep.iloc[891:]\ntest_pre['Age'] = test_pre['Age'].fillna(test_pre['Age'].mean()) # Filling missing Age values with mean","77747708":"### Pipelines for converting numerical values (useless)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]\n    def get_feature_names(self):\n        return X.columns.tolist()\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"Age\", \"Fare\", \"FamilySize\"])),\n        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n    ])\nnum_pipeline.fit_transform(train_pre)","b1dfd6a3":"### Pipelines for converting categorical values\n\nOHE = OneHotEncoder(sparse = False)\nclass MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)\ncat_pipeline = Pipeline([\n        (\"select_cat\", DataFrameSelector([\"Embarked\", \"Pclass\", \"Sex\", 'Has Cabin', 'Title'])),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OHE),\n    ])\ncat_pipeline.fit_transform(train_pre)","f4e73aad":"from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])\nX =  preprocess_pipeline.fit_transform(train)","225f4c4a":"### Splitting the Data set to train and test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","43e39a15":"# Train the data and evaluate accuracy scores for commonly used classifiers\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='scale')))\nmodels.append(('RF', RandomForestClassifier()))\ncv_res = pd.DataFrame()\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=3)\n    model.fit(X_train,y_train)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    y_pred = model.predict(X_test)\n    acc_score = accuracy_score(y_test, y_pred)\n    msg = \"%s: %f %f (%f)\" % (name, cv_results.mean(),acc_score, cv_results.std())\n    print(msg)","927dce21":"\n# Randomized Search for Logistic Regression\nfrom sklearn.model_selection import RandomizedSearchCV\n\nLR = LogisticRegression()\nparam_grid_LR = {\"solver\": [\"newton-cg\", 'liblinear','sag'],\n                 'max_iter' : [1,5,10,100,500,1000]\n             }\n\n\nrnd_search_LR = RandomizedSearchCV(LR, param_grid_LR,n_iter=500, cv=kfold, scoring = 'accuracy', return_train_score = True, random_state=42)\nrnd_search_LR.fit(X_train, y_train)","ae2da94c":"## Best parameters for LR\nrnd_search_LR.best_params_","1536ba74":"cvres = rnd_search_LR.cv_results_\nfor accuracy, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(accuracy), params)","49bd541f":"## List LR features with importances\ncat_pipeline.fit_transform(train_pre)\nLR  =  LogisticRegression(**rnd_search_LR.best_params_)\nLR.fit(X_train, y_train)\nweights_LR = np.round(np.transpose(LR.coef_),3)\nattribs0 = list(OHE.categories_[0])\nattribs1 = list(OHE.categories_[1])\nattribs2 = list(OHE.categories_[2])\nattribs3 = list(OHE.categories_[3])\nattribs4 = list(OHE.categories_[4])\nattributes = [\"Age\", \"Fare\", \"FamilySize\"] + attribs0 + attribs1 + attribs2 +  attribs3 + attribs4\nsorted(zip(weights_LR, attributes), reverse=True)","6e2be50d":"## List LDA features with importances\n\nLDA = LinearDiscriminantAnalysis(solver = 'svd')\nLDA.fit(X_train,y_train)\nweights_LDA = np.transpose(LDA.coef_)\nsorted(zip(weights_LDA, attributes), reverse=True)","da8aea1f":"# Randomized Search for Random Forest Classifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nRF = RandomForestClassifier()\n\nparam_grid_RF = {'bootstrap': [True, False],\n 'max_depth': [5,10,50,150],\n 'max_features': [2,3,4,5,6],\n 'min_samples_leaf': [2, 3, 4, 5],\n 'min_samples_split': [7,8,9,10,11],\n 'n_estimators': [100,150,200,250]}\n\nrnd_search_RF = RandomizedSearchCV(RF, param_grid_RF,n_iter=500, cv=kfold, scoring = 'accuracy',random_state=42, return_train_score = True)\nrnd_search_RF.fit(X_train, y_train)","fe33d3ea":"rnd_search_RF.best_params_","08b53343":"## Best parameters for RF\n\nrnd_search_RF.best_params_","4a6716af":"feature_importance_list = rnd_search_RF.best_estimator_.feature_importances_\ndf_attr_imp = pd.DataFrame({'attribute_name': attributes, 'importance': feature_importance_list})\ndf_attr_imp.sort_values('importance', ascending=False)","0e86923d":"# Final CV scores, accuracy and confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\nfinal_models = []\nfinal_models.append(('LR', LogisticRegression(**rnd_search_LR.best_params_, random_state=42)))\nfinal_models.append(('LDA', LinearDiscriminantAnalysis(solver = 'svd')))\nfinal_models.append(('RF', RandomForestClassifier(**rnd_search_RF.best_params_)))\ncv_res = pd.DataFrame()\nresults = []\nnames = []\nfor name, model in final_models:\n    model.fit(X_train,y_train)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    y_pred = model.predict(X_test)\n    cv_score = np.round((cv_results.mean()),3)\n    acc_score = np.round(accuracy_score(y_test, y_pred),3)\n    \n    print(name, ': CV accuracy is', cv_score,'Test Accuracy is',acc_score  )\n\n    print ('\\n The confusion matrix of ',name,'is \\n',confusion_matrix( y_test,y_pred), \"\\n\" )","5388e415":"# Final LDA predictions for test data set\ntest_final = preprocess_pipeline.fit_transform(test_pre)\ny_pred_LDA = LDA.predict(test_final)\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": y_pred_LDA\n    })\nfilename = 'Titanic Predictions LDA.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","11f6d5eb":"# Final LR predictions for test data set\n\nLR  =  LogisticRegression(**rnd_search_LR.best_params_)\nLR.fit (X_train, y_train)\ny_pred_LR = LR.predict(test_final)\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": y_pred_LR\n    })\nfilename = 'Titanic Predictions LR.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","95611e57":"# Final RF predictions for test data set\nRF  =  RandomForestClassifier(**rnd_search_RF.best_params_)\nRF.fit (X_train, y_train)\ny_pred_RF = RF.predict(test_final)\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": y_pred_RF\n    })\nfilename = 'Titanic Predictions RF.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","fd8a265a":"Seems like Logistic Regression, Linear Discriminant Analysis and Random Forest have the best scores. We will proceed with optimizing these classifiers to attain the highest scores","8601e97e":"The best accuracies so far were\n\nLogistic Regression~ 79%\n\nLinear Discriminant Analysis ~ 78.5%\n\nRandom Forest: 80.4% (V3)\n\nFurther steps: Group family size into bins","4102f7d7":"Commit 6 Changes \n* Added constant random seed\n"}}