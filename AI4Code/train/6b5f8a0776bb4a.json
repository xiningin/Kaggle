{"cell_type":{"e33d0c3b":"code","d0368488":"code","56a83665":"code","fb4998e8":"code","7f21d2da":"code","e939eec2":"code","fc09b14b":"code","2c85ecbd":"code","9ba7dd41":"code","fee74f86":"code","a09cdfe7":"code","1a5d7637":"code","2bddd422":"code","cde0f539":"code","2b70eb3e":"code","473e4d89":"code","8897717f":"code","c78c3369":"code","fd63bcb1":"code","16ce5cfa":"code","6080a951":"code","6c3cebfc":"code","e745e713":"code","be03c3b4":"code","4081b0c9":"code","626b5fb7":"code","143fde24":"code","f2bb21c0":"code","aad648d2":"code","c29d0ce2":"code","96434dbd":"code","e369a374":"code","13e94de1":"code","fc9b78f4":"code","108b1fb3":"code","19345d2d":"markdown","19cfccf1":"markdown"},"source":{"e33d0c3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0368488":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","56a83665":"\ndata=pd.read_csv(\"\/kaggle\/input\/ipl-2020-player-performance\/Training.csv\")\n","fb4998e8":"data.describe(include = \"all\")","7f21d2da":"data.info()","e939eec2":"data.isnull().sum()","fc09b14b":"data.columns","2c85ecbd":"col=['Runs', 'Boundaries', 'Six', 'Fifty', 'Hundred', 'Duck',\n       'Batting_Points', 'Wickets', '4W_Haul', '5W_Haul', 'Maidens',\n       'Bowling_Points', 'Total Points']\nfor i in col:\n    print(data.boxplot(column = col))\n ","9ba7dd41":"data.boxplot(column= \"Runs\" )  \n ","fee74f86":"data.boxplot(column=\"Boundaries\") \n","a09cdfe7":"data.boxplot(column=\"Six\") \n","1a5d7637":"data.boxplot(column=\"Fifty\") \n","2bddd422":"data.boxplot(column=\"Hundred\") \n","cde0f539":"data.boxplot(column=\"Duck\") \n","2b70eb3e":"data.boxplot(column=\"Batting_Points\") \n","473e4d89":"data.boxplot(column=\"Wickets\") \n","8897717f":"data.boxplot(column=\"4W_Haul\") \n","c78c3369":"data.boxplot(column=\"5W_Haul\") \n","fd63bcb1":"data.boxplot(column=\"Maidens\")\n","16ce5cfa":"data.boxplot(column=\"Bowling_Points\")\n","6080a951":"data.boxplot(column=\"Total Points\")\n","6c3cebfc":"## check assumption of linearity\nsns.pairplot(data,x_vars=['Runs', 'Boundaries', 'Six', 'Fifty', 'Hundred', 'Duck',\n       'Batting_Points', 'Wickets', '4W_Haul', '5W_Haul', 'Maidens',\n       'Bowling_Points'],y_vars=\"Total Points\",kind='reg')\n","e745e713":"## split the data into X and Y\nprint(data.shape)    \n\nX = data.iloc[:,0:13]\nY = data.iloc[:,-1]\n","be03c3b4":"## check the normality of dependent variable\nsns.distplot(Y,hist=\"True\")\n","4081b0c9":"#Split the data into test and train\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)\n","626b5fb7":"print(X_train.shape)\nprint(Y_train.shape)\n\nprint(X_test.shape)\nprint(Y_test.shape)\n","143fde24":"## check the correletion\ncorr_X_train=X_train.corr(method=\"pearson\")\nprint(corr_X_train)  \n","f2bb21c0":"# droping column one by one which has high correltion\n\n# for X_train dataset \nX_train=X_train.drop(\"Id\",axis=1)\n\nX_train=X_train.drop([\"Batting_Points\"],axis=1)\nX_train=X_train.drop([\"Bowling_Points\"],axis=1)\nX_train=X_train.drop([\"Boundaries\"],axis=1)\n","aad648d2":"corr_X_train=X_train.corr(method=\"pearson\")\nprint(corr_X_train)  \n","c29d0ce2":"## corr_heatmap\nplt.figure(figsize=(12,12),dpi=200)\nsns.heatmap(corr_X_train,vmax=1.0,vmin=-1.0,annot=True)\n","96434dbd":"# check VIF of independent variable\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\nvif_df = pd.DataFrame()\nvif_df[\"features\"] = X_train.columns\nvif_df[\"VIF Factor\"] = [vif(X_train.values, i) for i in range(X_train.shape[1])]\nvif_df.round(2)\n","e369a374":"#for X_test remove the columns which has high correlation  \n\nX_test=X_test.drop(\"Id\",axis=1)\n\nX_test=X_test.drop([\"Batting_Points\"],axis=1)\nX_test=X_test.drop([\"Bowling_Points\"],axis=1)\nX_test=X_test.drop([\"Boundaries\"],axis=1)\n","13e94de1":"# linear regression model\nfrom sklearn.linear_model import LinearRegression\n\n#create a model object\nlm = LinearRegression()\n\nlm.fit(X_train,Y_train)\n","fc9b78f4":"# predict the Y values\ny_pred=lm.predict(X_test)\nprint(y_pred)\n","108b1fb3":"# evaluation matrix\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nr2=r2_score(Y_test,y_pred)\nprint(\"r2\",r2)\n\nrmse=np.sqrt(mean_squared_error(Y_test,y_pred))\nprint(\"rmse\",rmse)\n\nadjusted_r_squared = 1 - (1-r2)*(len(Y)-1)\/(len(Y)-X.shape[1]-1)\nprint(\"adjusted_r_squared\",adjusted_r_squared)\n","19345d2d":"> *****check the boxplot of each columns for outlier***","19cfccf1":"for better understanding check individually"}}