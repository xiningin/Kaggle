{"cell_type":{"9015c36c":"code","d1c19f7b":"code","ef3042c6":"code","b6178c7a":"code","6e7cbe91":"code","8498e07d":"code","94df23d4":"code","3506dcbd":"code","7e212471":"code","0c66b83d":"code","4cc4a663":"code","53505f2f":"code","0a668df5":"code","d66e94ba":"code","e6823ff7":"code","021b34a2":"code","00f080a5":"code","de7c0904":"code","a5976c50":"code","ae756d57":"code","eb55907f":"code","fb8441e1":"code","ed6c292e":"code","71283d47":"code","7026c2bc":"markdown","000c42e3":"markdown","49cef4cc":"markdown","6c9b5bc9":"markdown","89cdab49":"markdown","7f2a490d":"markdown","59a8a11d":"markdown","0b019c66":"markdown","3fa77beb":"markdown","43eebd0a":"markdown","7a46357f":"markdown","dc230b84":"markdown","fbda788d":"markdown","4ebddfde":"markdown","77d30d59":"markdown","997cc127":"markdown","dc260d17":"markdown","f93c9463":"markdown","85e87271":"markdown"},"source":{"9015c36c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn import datasets\nimport xgboost as xgb\nimport seaborn as sns","d1c19f7b":"def nulls_by_col(df):\n# Calculate the number and percent of null values in each column.\n    num_missing = df.isnull().sum()\n    rows = df.shape[0]\n    pct_missing = num_missing\/rows\n    cols_missing = pd.DataFrame({'num_rows_missing': num_missing, 'pct_rows_missing': pct_missing})\n    return cols_missing","ef3042c6":"def nulls_by_row(df):\n# Calculate the number of percent of null values in each row.\n    num_cols_missing = df.isnull().sum(axis=1)\n    pct_cols_missing = df.isnull().sum(axis=1)\/df.shape[1]*100\n    rows_missing = pd.DataFrame({'num_cols_missing': num_cols_missing, 'pct_cols_missing': pct_cols_missing}).reset_index().groupby(['num_cols_missing','pct_cols_missing']).count().rename(index=str, columns={'index': 'num_rows'}).reset_index()\n    return rows_missing","b6178c7a":"def df_summary(df):\n# Print information about the data including its shape, datatypes, number of values, \n# number of null values in each row and column, the number of unique rows, etc.\n    print('--- Shape: {}'.format(df.shape))\n    print('\\n--- Info')\n    display(df.info())\n    print('\\n--- Descriptions')\n    display(df.describe(include='all'))\n    print('\\n--- Nulls By Column')\n    display(nulls_by_col(df))\n    print('\\n--- Nulls By Row')\n    display(nulls_by_row(df))\n    print('\\n---Unique Rows')\n    display(df.apply(lambda x: x.nunique()))","6e7cbe91":"def get_scaled_df(df):\n# Return a dataframe that contains only numeric data so that we can scale it for XGBoost.\n# This is not necessary for this data as it is already scaled, but it is part of a \n# pre-existing function that I wrote so I am leaving it here.\n    numerics = ['int64', 'float64', 'float']\n    scaled_df = df.select_dtypes(include=numerics)\n    col = scaled_df.columns\n    scaled_df = preprocessing.scale(scaled_df)\n    scaled_df = pd.DataFrame(scaled_df, columns=col)\n    return scaled_df","8498e07d":"def xgb_rank(df,target_variable,feature_percent=80,mode='gain'):\n    '''\n    This function receives a dataframe and the target variable, and then returns \n    a sorted feature list, a sorted scaled feature list, and a dataframe. \n    \n    For the input parameters:\n        - feature_percent is the optional cut-off (default is 80 percent) for features \n        - mode is optional. The default value is 'gain' which shows the importance. \n          Another possible value for mode is 'weight.'\n     \n     For the returned:\n        - feature_list, scaled_features: lists of features, both including those that \n          satisfy the cumulative percentage limit. \n        - scaled_df: dataframe that has all features in decending order \n        - importance_df: dataframe showing all cumulative percent rankings \n    '''    \n\n    scaled_df = get_scaled_df(df) \n    xgb_params = {'max_depth': 8,'seed' : 123}\n    dtrain = xgb.DMatrix(scaled_df, target_variable, feature_names=scaled_df.columns.values)\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n    importance_dict = model.get_score(importance_type=mode)\n    sorted_importance_dict = sorted(importance_dict.items(), key=lambda kv: kv[1])\n    importance_df = pd.DataFrame.from_dict(sorted_importance_dict)\n    importance_df.columns = ['feature',mode] \n    importance_df.sort_values(mode, inplace = True) \n    importance_df['rank'] = importance_df[mode].rank(ascending = False)\n    importance_df.sort_values('rank', inplace = True) \n    importance_df.set_index('rank', inplace = True)\n    importance_df.reset_index(inplace=True) \n    importance_df[mode] = importance_df[mode].apply(lambda x: round(x, 2))\n    importance_df['cum_sum'] = round(importance_df[mode].cumsum(),2)\n    importance_df['cum_perc'] = round(100*importance_df.cum_sum\/importance_df[mode].sum(),2)\n    feature_list = []\n    scaled_features = [] \n\n    for i in range((importance_df.shape[0])): \n\n        feature_name = importance_df.iloc[i,1].replace('scaled_','')\n        scaled_name = 'scaled_' + feature_name\n        importance_df.iloc[i,1] = feature_name\n        cum_percent = importance_df.iloc[i,4]\n\n        if cum_percent > feature_percent:\n            break\n        else:\n            feature_list.append(feature_name)\n            scaled_features.append(scaled_name)\n    return feature_list, scaled_features, scaled_df, importance_df","94df23d4":"diabetes = datasets.load_diabetes() # load data\ndata = np.c_[diabetes.data, diabetes.target]\ncolumns = np.append(diabetes.feature_names, 'target')\ndf = pd.DataFrame(data, columns=columns)\nprint(type(df))\ndf.head()","3506dcbd":"df_summary(df)","7e212471":"X = df.drop(columns=['target'])\ny = df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","0c66b83d":"feature_list, scaled_features, scaled_df, importance_df = xgb_rank(X_train, y_train)","4cc4a663":"print('feature_list: ', feature_list, '\\n')\nprint('scaled_features: ', scaled_features, '\\n')\nprint('\\nscaled_df:')\ndisplay(scaled_df.head())\nprint('\\ny_train:')\ndisplay(y_train.head())\nprint('\\nimportance_df:')\ndisplay(importance_df)","53505f2f":"full_scaled_df = scaled_df.copy()\nfull_scaled_df['target'] = preprocessing.scale(y_train)\ndisplay(full_scaled_df.head())","0a668df5":"g = sns.PairGrid(full_scaled_df)\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter);","d66e94ba":"# Set the background to white so it won't show after adding the mask.\nsns.set(style=\"white\")\n\n# Compute the correlation matrix from train_df.\ncorr = full_scaled_df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 11))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.xticks(rotation=60)","e6823ff7":"X_train = X_train.drop(columns=['s2', 's4', 'age', 's1', 'sex'])\nprint('X_train.head(2):')\ndisplay(X_train.head(2))\nX_test = X_test.drop(columns=['s2', 's4', 'age', 's1', 'sex'])\nprint('X_test.head(2):')\ndisplay(X_test.head(2)) # Just to confirm we have the right features there \n                        # for when we do our test.\n# And while we're at it, let's drop those columns from X.\nX = df.drop(columns=['s2', 's4', 'age', 's1', 'sex'])\nprint('X.head(2):')\ndisplay(X.head(2)) # Again, just to confirm.","021b34a2":"plt.figure(figsize=(15,10))\nplt.tight_layout()\nsns.distplot(y_train)","00f080a5":"# There are three steps to model something with sklearn\n# 1. Set up the model\nmodel = LinearRegression()\n# 2. Use fit\nmodel.fit(X_train, y_train)\n# 3. Check the score\nprint(model.score(X_test, y_test))\n#4. Check the regression metrics\ny_pred = model.predict(X_test)","de7c0904":"#To retrieve the intercept:\nprint('The intercept is ', model.intercept_)\n#For retrieving the slope:\nprint('The slope is ', model.coef_)","a5976c50":"coeff_df = pd.DataFrame(model.coef_, X_train.columns, columns=['Coefficient'])  \ncoeff_df.sort_values(by='Coefficient', ascending=False)","ae756d57":"eval_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\neval_df_top25 = eval_df.head(25)\ndisplay(eval_df_top25)","eb55907f":"eval_df_top25.plot(kind='bar',figsize=(10,8))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","fb8441e1":"print('Mean value of the target variable is:', y_test.mean())\nprint('R-squared:', metrics.r2_score(y_test, y_pred))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('mean\/RMSE = ', np.sqrt(metrics.mean_squared_error(y_test, y_pred))\/y_test.mean())","ed6c292e":"sns.residplot(y_test, y_pred, lowess=True, color=\"g\")","71283d47":"# There are three steps to model something with sklearn\n# 1. Set up the model\nmodel = LogisticRegression(solver='newton-cg', multi_class='ovr')\n# 2. Use fit\nmodel.fit(X_train, y_train)\n# 3. Check the score\nprint('score = ', model.score(X_test, y_test))\n# 4. Check the regression metrics\ny_pred = model.predict(X_test)","7026c2bc":"#### Load the data and put the feature data and target data into a single dataframe.","000c42e3":"#### Show Seaborn's PairGrid of the scaled data to explore it.","49cef4cc":"This was an exercise in linear regression on a diabetes dataset. \n\nTen baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n\nData Set Characteristics:\n\nNumber of Instances:\n442\n\nNumber of Attributes:\nFirst 10 columns are numeric predictive values\n\n    Attribute Information:\n    - Age\n    - Sex\n    - Body mass index\n    - Average blood pressure\n    - S1 (serum measurement 1)\n    - S2 (serum measurement 2)\n    - S3 (serum measurement 3)\n    - S4 (serum measurement 4)\n    - S5 (serum measurement 5)\n    - S6 (serum measurement 6)\n\nTarget:\t\nColumn 11 is a quantitative measure of disease progression one year after baseline\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).\n\nSource URL: https:\/\/www4.stat.ncsu.edu\/~boos\/var.select\/diabetes.html\n\nFor more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \u201cLeast Angle Regression,\u201d Annals of Statistics (with discussion), 407-499. (https:\/\/web.stanford.edu\/~hastie\/Papers\/LARS\/LeastAngle_2002.pdf)\n\n\n#### Import necessary tools.","6c9b5bc9":"#### Age is the least important feature with significantly less gain.\n\n#### Let's create a dataframe that contains all of the scaled features plus the scaled target data.","89cdab49":"#### Let's list the coefficients of each feature.","7f2a490d":"#### This Residplot is a plot of the residuals after fitting our linear model. Obviously, this model has much room for improvement!\n\n#### Let's try a logistic regression model.","59a8a11d":"Looks like this error is happening for pandas>=0.23.0 because the pandas Series.base property has been deprecated:\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/core\/base.py#L779\nfrom pandas-dev\/pandas#20419 \n\nHopefully, XGBoost will fix this error.","0b019c66":"#### And let's look at the correlation of the features.","3fa77beb":"#### Now we are ready to start developing our predictive model.","43eebd0a":"#### RMSE\n\nThe RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data\u2013how close the observed data points are to the model\u2019s predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. *Lower values of RMSE indicate better fit.* RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.\n\n\nYou can see that the above value of root mean squared error is 58.501, which is 37.9% of the mean value which is 154.225. This means our algorithm was not very accurate.","7a46357f":"#### Let's look at the distribution of the target variable in our training data.","dc230b84":"#### I played with various values for solver and multi_class, but still only got a score of 0.0. ","fbda788d":"#### That's not looking as good as we'd like, but let's check out the evaluative statistics.","4ebddfde":"#### Note the correlation of the features with the target varaiable differs from the results of XGBoost:\n- bmi =  0.61\n- s5  =  0.59\n- s4  =  0.45\n- bp  =  0.44\n- s3  = -0.43\n- s6  =  0.38\n- s1  =  0.21\n- age =  0.19\n- s2  =  0.16\n- sex =  0.04\n\n```\nimportance_df (importance as ranked by XGBoost):\n    rank\tfeature\tgain\t  cum_sum\tcum_perc\n0\t1.0     s5\t    4605.48\t  4605.48\t47.10\n1\t2.0     bmi\t    1673.09\t  6278.57\t64.21\n2\t3.0     s3\t     558.55\t  6837.12\t69.92\n3\t4.0     s4\t     557.12\t  7394.24\t75.62\n4\t5.0     s6\t     521.27\t  7915.51\t80.95\n5\t6.0     bp\t     512.64\t  8428.15\t86.19\n6\t7.0     sex      417.06   8845.21\t90.46\n7\t8.0     s1\t     400.20\t  9245.41\t94.55\n8\t9.0     s2\t     375.20\t  9620.61\t98.39\n9\t10.0    age\t     157.56\t  9778.17\t100.00\n```\n\n#### The aren't completely different, but XGBoost is a boosted algorithm, so let's use it to make our decisions.\n\n\n##### Serum measurements 1 and 2 (s1 and s2) are highly positively correlated at 0.89, but s1 ranks higher in importance. Drop s2.\n##### Serum measurements 3 and 4 (s3 and s4) are highly negatively correlated at -0.75, but s3 ranks higher in importance. Drop s4.\n##### Serum measurements 2 and 4 (s2 and s4) are fairly positively correlated at 0.66, but s4 ranks higher in importance. However, we already dropped s2 and s4!\n##### Serum measurements 4 and 5 (s4 and s5) are fairly positively correlated at 0.63, but s5 ranks higher in importance. We already decided to drop s4.\n\n#### So dropping s2 and s4.\n\n```\nimportance_df:\n    rank  feature  gain\t\n0\t1.0   s5       4605.48\t\n1\t2.0   bmi      1673.09\t\n2\t3.0   s3        558.55\t\n4\t5.0   s6        521.27\t\n5\t6.0   bp        512.64\t\n6\t7.0   sex       417.06\t\n7\t8.0   s1        400.20\t\n9\t10.0  age       157.56\t\n```\n\n#### Age comes in last in importance and gives much less gain than the other features. Let's drop it. \n#### After playing for a bit, I realized our accuracy will increase if we also drop s1 and sex.\n\n```\nimportance_df:\n    rank  feature  gain\t\n0\t1.0   s5       4605.48\t\n1\t2.0   bmi      1673.09\t\n2\t3.0   s3        558.55\t\n4\t5.0   s6        521.27\t\n5\t6.0   bp        512.64\t\n6\t7.0   sex       417.06\t\n7\t8.0   s1        400.20\t\n```\n#### So we dropped s2, s4, age, s1, and sex.\n#### That will leave us with seven features.","77d30d59":"#### Now let's compare the actual output values for X_test with the predicted values. But let's also slice of the top 25 rows so we can see them in our visualization below.","997cc127":"#### Let's visualize those 25 rows so we can see how close the actual and predicted values are.","dc260d17":"#### Call the function, xgb_rank, to collect a list of features that account for 80% of the affect on the target variable in our training data as well as a dataframe of scaled training data and a datafram showing the importance of each feature in our training data.","f93c9463":"#### Print a summary of the data.","85e87271":"#### This data has no null values.\n\n#### Let's separate our data into our train and test dataframes."}}