{"cell_type":{"65b71a3a":"code","da79ca1e":"code","cdfa8ef3":"code","72767346":"code","046f032c":"code","d2e6a375":"code","08774797":"code","12b4ed15":"code","2121c4bc":"code","bb4555af":"code","06f4132f":"code","b26a5df2":"code","23401e0f":"code","0d0af34c":"code","67517fe7":"code","89ed7a71":"code","2ec0532a":"code","8ec3d980":"code","1e12cbe6":"code","92932861":"code","3eb08942":"code","7b2b8f4a":"code","ee727e16":"code","5dfcd1e0":"code","26dc95a9":"code","bb7ec9c1":"code","bca58f74":"code","413cf6f0":"code","d950f828":"code","b16a84d0":"code","46635dec":"code","85d393a6":"code","3dd2b774":"code","a010a4ed":"code","4691b964":"code","97919e7f":"code","c8d4abb3":"code","0ce8617e":"code","4cc86d20":"code","97cd2092":"code","58008242":"code","415d5480":"code","a65a9525":"code","f730e101":"code","f6d929bc":"code","6ff44afa":"code","dd3f37ec":"code","bc8c9f8f":"code","399bfede":"code","1300f379":"code","01211e42":"code","75c28724":"code","5fa36e7b":"code","5fdbe275":"code","ebb37fa7":"code","95da1296":"code","805377e9":"markdown","f9278e4d":"markdown","8e10f386":"markdown","79a4fb6e":"markdown","d7c9cea1":"markdown","9624a8dc":"markdown","13bd25d9":"markdown","365d1c64":"markdown","9eddfa40":"markdown","63467db2":"markdown","8c2befc1":"markdown","9c08fe9e":"markdown","f7f7019b":"markdown","90d174ae":"markdown","6dd864d7":"markdown","4ca32d56":"markdown","31feb1b0":"markdown","6863479c":"markdown","c4680cf0":"markdown","ae48be38":"markdown","aebba2aa":"markdown","d66c40fd":"markdown","938e4cea":"markdown","6c429cf8":"markdown","a9e88b00":"markdown","b6e76fa3":"markdown","56fb8161":"markdown","05f93bdd":"markdown","12cd4e3e":"markdown","5034d6cd":"markdown","2d190246":"markdown","92235b25":"markdown","7edac8e1":"markdown","7b46f13a":"markdown","b57c9018":"markdown","95a96f04":"markdown","49c35205":"markdown","3895d4b1":"markdown","4bc9caec":"markdown","1065d6ea":"markdown","52ba49ed":"markdown","0215374a":"markdown","08cd144b":"markdown","59f84424":"markdown","73abe81f":"markdown","0a789b04":"markdown","3aebc96a":"markdown","205fc12e":"markdown","a640be72":"markdown"},"source":{"65b71a3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da79ca1e":"import warnings\nwarnings.filterwarnings('ignore')","cdfa8ef3":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata","72767346":"data.drop('id', axis=1, inplace=True)","046f032c":"import matplotlib.pyplot as plt\nimport seaborn as sns","d2e6a375":"sns.factorplot(x='stroke', col='hypertension', kind='count', data=data)","08774797":"sns.factorplot(x='stroke', col='heart_disease', kind='count', data=data)","12b4ed15":"sns.factorplot(x='stroke', col='ever_married', kind='count', data=data)","2121c4bc":"sns.factorplot(x='stroke', col='gender', kind='count', data=data)","bb4555af":"sns.factorplot(x='stroke', col='smoking_status', kind='count', data=data)","06f4132f":"avg = data[data['stroke']==1].avg_glucose_level\nmax(avg)","b26a5df2":"data.hist()","23401e0f":"data.isnull().sum()","0d0af34c":"data.dropna(inplace=True)","67517fe7":"data.duplicated().any()","89ed7a71":"from sklearn.preprocessing import KBinsDiscretizer","2ec0532a":"age_binner = KBinsDiscretizer(n_bins=5, encode='ordinal')\nglucode_lvl_binner = KBinsDiscretizer(n_bins=4, encode='ordinal')\nbmi_binner = KBinsDiscretizer(n_bins=5, encode='ordinal')","8ec3d980":"data['age_bins'] = age_binner.fit_transform(data['age'].values.reshape(-1,1)).astype('int64')\ndata['avg_glucose_level_bins'] = glucode_lvl_binner.fit_transform(data['avg_glucose_level'].values.reshape(-1,1)).astype('int64')\ndata['weight'] = bmi_binner.fit_transform(data['bmi'].values.reshape(-1,1)).astype('int64')","1e12cbe6":"cat_cols = data.select_dtypes(object).columns","92932861":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in cat_cols:\n    data[col] = le.fit_transform(data[col])","3eb08942":"from sklearn.preprocessing import RobustScaler, StandardScaler","7b2b8f4a":"cols_to_scale = ['age', 'avg_glucose_level', 'bmi']","ee727e16":"robust = RobustScaler()\nstandard = StandardScaler()\n\ndata[cols_to_scale] = robust.fit_transform(data[cols_to_scale])\ndata[cols_to_scale] = standard.fit_transform(data[cols_to_scale])","5dfcd1e0":"corr = data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap='coolwarm', annot=True, square=True, fmt='.2f')\nplt.show()","26dc95a9":"X, y = data.drop('stroke', axis=1), data['stroke']","bb7ec9c1":"sns.countplot(y)","bca58f74":"from imblearn.over_sampling import SMOTE","413cf6f0":"smote = SMOTE()\n\nX, y = smote.fit_resample(X, y)","d950f828":"sns.countplot(y)","b16a84d0":"from sklearn.model_selection import train_test_split","46635dec":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","85d393a6":"x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.2)","3dd2b774":"from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier","a010a4ed":"from sklearn.metrics import f1_score, classification_report, mean_squared_error","4691b964":"def model_selection(x_train_, x_val, y_train_, y_val, model):\n  model = model()\n  model.fit(x_train_, y_train_)\n\n  pred = model.predict(x_val)\n\n  error = np.sqrt(mean_squared_error(y_val, pred))\n  acc = f1_score(y_val, pred)\n  report = classification_report(y_val, pred)\n  train_score = model.score(x_train_, y_train_)\n  val_score = model.score(x_val, y_val)\n\n  print('Error:', error*100)\n  print('\\n')\n  print('ACC:', acc*100)\n  print('\\n')\n  print('Classification report:', report)\n  print('\\n')\n  print('Train Score:', train_score*100)\n  print('\\n')\n  print('Val Score:', val_score*100)\n  print('\\n')\n  print('Is overfitting:', True if train_score>val_score else False)\n  print('\\n')\n  print('Overfitting by:',train_score*100-val_score*100)","97919e7f":"extratrees = model_selection(x_train_, x_val, y_train_, y_val, ExtraTreesClassifier)\nextratrees ","c8d4abb3":"gradient = model_selection(x_train_, x_val, y_train_, y_val, GradientBoostingClassifier)\ngradient","0ce8617e":"randomforest = model_selection(x_train_, x_val, y_train_, y_val, RandomForestClassifier)\nrandomforest","4cc86d20":"ada = model_selection(x_train_, x_val, y_train_, y_val, AdaBoostClassifier)\nada","97cd2092":"xgb = model_selection(x_train_, x_val, y_train_, y_val, XGBClassifier)\nxgb","58008242":"lgbm = model_selection(x_train_, x_val, y_train_, y_val, LGBMClassifier)\nlgbm","415d5480":"catboost = model_selection(x_train_, x_val, y_train_, y_val, CatBoostClassifier)\ncatboost","a65a9525":"tree = model_selection(x_train_, x_val, y_train_, y_val, DecisionTreeClassifier)\ntree","f730e101":"logistic = model_selection(x_train_, x_val, y_train_, y_val, LogisticRegression)\nlogistic","f6d929bc":"sgd = model_selection(x_train_, x_val, y_train_, y_val, SGDClassifier)\nsgd","6ff44afa":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","dd3f37ec":"model = LGBMClassifier()","bc8c9f8f":"params = {'num_leaves': sp_randint(6, 50), \n            'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n             'scale_pos_weight':[1,2,6,12],\n             'depth': sp_randint(3,10),\n             'learning_rate': sp_uniform()}","399bfede":"search = RandomizedSearchCV(model, params, n_iter=50, scoring='f1', n_jobs=-1, cv=5)\nsearch.fit(x_train, y_train)","1300f379":"print('Best Score:', search.best_score_)\nprint('\\n')\nprint('Best Params:', search.best_params_)\nprint('\\n')\nprint('Best Estimator:', search.best_estimator_)","01211e42":"model = LGBMClassifier(colsample_bytree=0.9040713748241764, depth=8,\n               learning_rate=0.887075684185765, min_child_samples=208,\n               min_child_weight=1e-05, num_leaves=35, reg_alpha=0.1,\n               reg_lambda=1, scale_pos_weight=2, subsample=0.507606910665195)\n\nmodel.fit(x_train, y_train)","75c28724":"pred = model.predict(x_test)\npred","5fa36e7b":"error = np.sqrt(mean_squared_error(y_test, pred))*100\nerror","5fdbe275":"acc = f1_score(y_test, pred)\nacc*100","ebb37fa7":"classification_report(y_test, pred)","95da1296":"over_fitting_rate = model.score(x_train, y_train)*100 - model.score(x_test, y_test)*100\nover_fitting_rate","805377e9":"# **MODEL BUILDING AND TRAINING**","f9278e4d":"**there is a lot of imbalance in the data lets fix this**","8e10f386":"**Mean Squared Error**","79a4fb6e":"**lets plot the histogram of the data**","d7c9cea1":"**OVER FITTING CHECK**","9624a8dc":"# DATA EDA","13bd25d9":"# **LOADING THE DATASET**","365d1c64":"**F1 Score**","9eddfa40":"**there are 201 missing values which is just 3% of the data and we can dop them**","63467db2":"# **MODEL SELECTION**","8c2befc1":"# SPLITTING TRAINING DATA INTO TRAINING AND VALIDATION SETS FOR MODEL SELECTION","9c08fe9e":"**Feature Correlation**","f7f7019b":"**a person who has never smoked has a higher chance of getting stroke**","90d174ae":"**robust scaler to remove outliars and standard scaler to scale the data**","6dd864d7":"**We got our optimal parameters now lets build the model using them just copy the best estimator and paste it in another cell**","4ca32d56":"**lets also see if gender effects the chances of stroke**","31feb1b0":"**first I want to look if a person has a higher chance of geeting a stroke if he has hypertension**","6863479c":"**dropping the id column**","c4680cf0":"**a person with no hyper tension has a slightly higher change of having a stroke**","ae48be38":"**scaling continuous data**","aebba2aa":"**lets look for some insights**","d66c40fd":"**even though our accuracy is excellent i am including this part just to show how hyper parameter tuning works**","938e4cea":"**Now I want to look if a person has a higher chance of geeting a stroke if he has heart disease**","6c429cf8":"**first lets check for missing values**","a9e88b00":"**binning the features**","b6e76fa3":"# METRIC CHECK","56fb8161":"Encoding categorical data","05f93bdd":"**lets see if marriage effects the chances to stroke**","12cd4e3e":"**Classification Report**","5034d6cd":"**the model is overfitting by 4% which is honestly not a lot so we can leave it there**","2d190246":"**creating all the binners**","92235b25":"# If you like the notebook please upvote it","7edac8e1":"**what is the highest glucose level recorded among the people who had stroke?**","7b46f13a":"**a person who does not have a heart disease has a slightly higher chance of getting a stroke**","b57c9018":"**females have a slightly higher chance of getting a stroke compared to male**","95a96f04":"**now our data is balanced**","49c35205":"**lets check for any duplicate values**","3895d4b1":"**I will use LGBMClassifier because it gives a good accuracy and the  error and overfitting rate is low**","4bc9caec":"# **HYPER PARAMETER TUNING**","1065d6ea":"**if you get an accuracy of like 95+ you dont need to do hyper parameter tuning because it may decrease your accuracy**","52ba49ed":"**Feature Imbalance**","0215374a":"**Feature Binning**","08cd144b":"# **PREDICTIONS**","59f84424":"# **SPLITTING DATA INTO TRAINING AND TESTING SETS**","73abe81f":"**there are no duplicate values**","0a789b04":"**stroke aka the target column has no highly correlated features but if your target column has highly correlated features then you gotta remove them or reduce their dimentions using PCA**","3aebc96a":"**and finally lets see if smoking effects the chances of stroke**","205fc12e":"**a person who is married has a higher chance of stroke**","a640be72":"# **DATA PROCESSING**"}}