{"cell_type":{"73629e12":"code","d1592390":"code","01109de1":"code","6e3d743c":"code","3fd6bc11":"code","1651e0b0":"code","6e87ed74":"code","83ded68e":"code","650104ae":"code","c4ceb7ef":"code","b18622d7":"code","727e8c21":"code","cdf19379":"code","f02f1392":"code","f711971b":"code","f44f9288":"code","6a241954":"code","0a43ca6c":"code","63059a36":"code","f8a18beb":"code","3826ca00":"markdown","118f5064":"markdown","545d31a9":"markdown","e38aa025":"markdown","6df0247a":"markdown","ce7fd357":"markdown","2f3d6665":"markdown","1e60958a":"markdown","f4aedb58":"markdown","923ded1c":"markdown","5789d65e":"markdown"},"source":{"73629e12":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow import function, GradientTape, sqrt, abs, reduce_mean, ones_like, zeros_like, convert_to_tensor,float32\nfrom tensorflow import data as tfdata\nfrom tensorflow import config as tfconfig\nfrom tensorflow import nn\nfrom tensorflow.keras import Model, Sequential, Input\nfrom tensorflow.keras.layers import GRU, LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n\nimport numpy as np\nfrom tqdm import tqdm, trange\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","d1592390":"df = pd.read_csv('..\/input\/time-series-forecasting-with-yahoo-stock-price\/yahoo_stock.csv')\ndf = df.set_index('Date').sort_index()\ndf.head()","01109de1":"seq_len = 24\nn_seq = 6\nhidden_dim = 24\ngamma = 1\n\nnoise_dim = 32\ndim = 128\nbatch_size = 128\n\nlog_step = 100\nlearning_rate = 5e-4\ntrain_steps = 5000\n\ngan_args = batch_size, learning_rate, noise_dim, 24, 2, (0, 1), dim","6e3d743c":"def preprocess(data, seq_len):\n    ori_data = data[::-1]\n    scaler = MinMaxScaler().fit(ori_data)\n    ori_data = scaler.transform(ori_data)\n    \n    temp_data = []\n    for i in range(0, len(ori_data) - seq_len):\n        _x = ori_data[i:i + seq_len]\n        temp_data.append(_x)\n        \n    idx = np.random.permutation(len(temp_data))\n    data = []\n    for i in range(len(temp_data)):\n        data.append(temp_data[idx[i]])\n        \n    return data","3fd6bc11":"stock_data = preprocess(df.values, seq_len)","1651e0b0":"def net(model, n_layers, hidden_units, output_units, net_type='GRU'):\n    if net_type=='GRU':\n        for i in range(n_layers):\n            model.add(GRU(units=hidden_units,\n                      return_sequences=True,\n                      name=f'GRU_{i + 1}'))\n    else:\n        for i in range(n_layers):\n            model.add(LSTM(units=hidden_units,\n                      return_sequences=True,\n                      name=f'LSTM_{i + 1}'))\n\n    model.add(Dense(units=output_units,\n                    activation='sigmoid',\n                    name='OUT'))\n    return model","6e87ed74":"class Generator(Model):\n    def __init__(self, hidden_dim, net_type='GRU'):\n        self.hidden_dim = hidden_dim\n        self.net_type = net_type\n\n    def build(self, input_shape):\n        model = Sequential(name='Generator')\n        model = net(model,\n                    n_layers=3,\n                    hidden_units=self.hidden_dim,\n                    output_units=self.hidden_dim,\n                    net_type=self.net_type)\n        return model","83ded68e":"class Discriminator(Model):\n    def __init__(self, hidden_dim, net_type='GRU'):\n        self.hidden_dim = hidden_dim\n        self.net_type=net_type\n\n    def build(self, input_shape):\n        model = Sequential(name='Discriminator')\n        model = net(model,\n                    n_layers=3,\n                    hidden_units=self.hidden_dim,\n                    output_units=1,\n                    net_type=self.net_type)\n        return model","650104ae":"class Recovery(Model):\n    def __init__(self, hidden_dim, n_seq):\n        self.hidden_dim=hidden_dim\n        self.n_seq=n_seq\n        return\n\n    def build(self, input_shape):\n        recovery = Sequential(name='Recovery')\n        recovery = net(recovery,\n                       n_layers=3,\n                       hidden_units=self.hidden_dim,\n                       output_units=self.n_seq)\n        return recovery","c4ceb7ef":"class Embedder(Model):\n\n    def __init__(self, hidden_dim):\n        self.hidden_dim=hidden_dim\n        return\n\n    def build(self, input_shape):\n        embedder = Sequential(name='Embedder')\n        embedder = net(embedder,\n                       n_layers=3,\n                       hidden_units=self.hidden_dim,\n                       output_units=self.hidden_dim)\n        return embedder","b18622d7":"class Supervisor(Model):\n    def __init__(self, hidden_dim):\n        self.hidden_dim=hidden_dim\n\n    def build(self, input_shape):\n        model = Sequential(name='Supervisor')\n        model = net(model,\n                    n_layers=2,\n                    hidden_units=self.hidden_dim,\n                    output_units=self.hidden_dim)\n        return model","727e8c21":"class TimeGAN():\n    def __init__(self, model_parameters, hidden_dim, seq_len, n_seq, gamma):\n        self.seq_len=seq_len\n        self.batch_size, self.lr, self.beta_1, self.beta_2, self.noise_dim, self.data_dim, self.layers_dim = model_parameters\n        self.n_seq=n_seq\n        self.hidden_dim=hidden_dim\n        self.gamma=gamma\n        self.define_gan()\n\n    def define_gan(self):\n        self.generator_aux=Generator(self.hidden_dim).build(input_shape=(self.seq_len, self.n_seq))\n        self.supervisor=Supervisor(self.hidden_dim).build(input_shape=(self.hidden_dim, self.hidden_dim))\n        self.discriminator=Discriminator(self.hidden_dim).build(input_shape=(self.hidden_dim, self.hidden_dim))\n        self.recovery = Recovery(self.hidden_dim, self.n_seq).build(input_shape=(self.hidden_dim, self.hidden_dim))\n        self.embedder = Embedder(self.hidden_dim).build(input_shape=(self.seq_len, self.n_seq))\n\n        X = Input(shape=[self.seq_len, self.n_seq], batch_size=self.batch_size, name='RealData')\n        Z = Input(shape=[self.seq_len, self.n_seq], batch_size=self.batch_size, name='RandomNoise')\n\n        # AutoEncoder\n        H = self.embedder(X)\n        X_tilde = self.recovery(H)\n        \n        self.autoencoder = Model(inputs=X, outputs=X_tilde)\n\n        # Adversarial Supervise Architecture\n        E_Hat = self.generator_aux(Z)\n        H_hat = self.supervisor(E_Hat)\n        Y_fake = self.discriminator(H_hat)\n\n        self.adversarial_supervised = Model(inputs=Z,\n                                       outputs=Y_fake,\n                                       name='AdversarialSupervised')\n\n        \n        # Adversarial architecture in latent space\n        Y_fake_e = self.discriminator(E_Hat)\n\n        self.adversarial_embedded = Model(inputs=Z,\n                                    outputs=Y_fake_e,\n                                    name='AdversarialEmbedded')\n        \n        #Synthetic data generation\n        X_hat = self.recovery(H_hat)\n        \n        self.generator = Model(inputs=Z,\n                            outputs=X_hat,\n                            name='FinalGenerator')\n\n        \n        # Final discriminator model\n        Y_real = self.discriminator(H)\n        \n        self.discriminator_model = Model(inputs=X,\n                                         outputs=Y_real,\n                                         name=\"RealDiscriminator\")\n\n        # Loss functions\n        self._mse=MeanSquaredError()\n        self._bce=BinaryCrossentropy()","cdf19379":"class TimeGAN(TimeGAN):\n    def __init__(self, model_parameters, hidden_dim, seq_len, n_seq, gamma):\n        super().__init__(model_parameters, hidden_dim, seq_len, n_seq, gamma)\n    \n    @function\n    def train_autoencoder(self, x, opt):\n        with GradientTape() as tape:\n            x_tilde = self.autoencoder(x)\n            embedding_loss_t0 = self._mse(x, x_tilde)\n            e_loss_0 = 10 * sqrt(embedding_loss_t0)\n\n        var_list = self.embedder.trainable_variables + self.recovery.trainable_variables\n        gradients = tape.gradient(e_loss_0, var_list)\n        opt.apply_gradients(zip(gradients, var_list))\n        return sqrt(embedding_loss_t0)\n\n    @function\n    def train_supervisor(self, x, opt):\n        with GradientTape() as tape:\n            h = self.embedder(x)\n            h_hat_supervised = self.supervisor(h)\n            g_loss_s = self._mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n\n        var_list = self.supervisor.trainable_variables + self.generator.trainable_variables\n        gradients = tape.gradient(g_loss_s, var_list)\n        apply_grads = [(grad, var) for (grad, var) in zip(gradients, var_list) if grad is not None]\n        opt.apply_gradients(apply_grads)\n        return g_loss_s\n\n    @function\n    def train_embedder(self,x, opt):\n        with GradientTape() as tape:\n            h = self.embedder(x)\n            h_hat_supervised = self.supervisor(h)\n            generator_loss_supervised = self._mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n\n            x_tilde = self.autoencoder(x)\n            embedding_loss_t0 = self._mse(x, x_tilde)\n            e_loss = 10 * sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n\n        var_list = self.embedder.trainable_variables + self.recovery.trainable_variables\n        gradients = tape.gradient(e_loss, var_list)\n        opt.apply_gradients(zip(gradients, var_list))\n        return sqrt(embedding_loss_t0)\n\n    def discriminator_loss(self, x, z):\n        y_real = self.discriminator_model(x)\n        discriminator_loss_real = self._bce(y_true=ones_like(y_real),\n                                            y_pred=y_real)\n\n        y_fake = self.adversarial_supervised(z)\n        discriminator_loss_fake = self._bce(y_true=zeros_like(y_fake),\n                                            y_pred=y_fake)\n\n        y_fake_e = self.adversarial_embedded(z)\n        discriminator_loss_fake_e = self._bce(y_true=zeros_like(y_fake_e),\n                                              y_pred=y_fake_e)\n        return (discriminator_loss_real +\n                discriminator_loss_fake +\n                self.gamma * discriminator_loss_fake_e)\n\n    @staticmethod\n    def calc_generator_moments_loss(y_true, y_pred):\n        y_true_mean, y_true_var = nn.moments(x=y_true, axes=[0])\n        y_pred_mean, y_pred_var = nn.moments(x=y_pred, axes=[0])\n        g_loss_mean = reduce_mean(abs(y_true_mean - y_pred_mean))\n        g_loss_var = reduce_mean(abs(sqrt(y_true_var + 1e-6) - sqrt(y_pred_var + 1e-6)))\n        return g_loss_mean + g_loss_var\n\n    @function\n    def train_generator(self, x, z, opt):\n        with GradientTape() as tape:\n            y_fake = self.adversarial_supervised(z)\n            generator_loss_unsupervised = self._bce(y_true=ones_like(y_fake),\n                                                    y_pred=y_fake)\n\n            y_fake_e = self.adversarial_embedded(z)\n            generator_loss_unsupervised_e = self._bce(y_true=ones_like(y_fake_e),\n                                                      y_pred=y_fake_e)\n            h = self.embedder(x)\n            h_hat_supervised = self.supervisor(h)\n            generator_loss_supervised = self._mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n\n            x_hat = self.generator(z)\n            generator_moment_loss = self.calc_generator_moments_loss(x, x_hat)\n\n            generator_loss = (generator_loss_unsupervised +\n                              generator_loss_unsupervised_e +\n                              100 * sqrt(generator_loss_supervised) +\n                              100 * generator_moment_loss)\n\n        var_list = self.generator_aux.trainable_variables + self.supervisor.trainable_variables\n        gradients = tape.gradient(generator_loss, var_list)\n        opt.apply_gradients(zip(gradients, var_list))\n        return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss\n\n    @function\n    def train_discriminator(self, x, z, opt):\n        with GradientTape() as tape:\n            discriminator_loss = self.discriminator_loss(x, z)\n\n        var_list = self.discriminator.trainable_variables\n        gradients = tape.gradient(discriminator_loss, var_list)\n        opt.apply_gradients(zip(gradients, var_list))\n        return discriminator_loss\n\n    def get_batch_data(self, data, n_windows):\n        data = convert_to_tensor(data, dtype=float32)\n        return iter(tfdata.Dataset.from_tensor_slices(data)\n                                .shuffle(buffer_size=n_windows)\n                                .batch(self.batch_size).repeat())\n\n    def _generate_noise(self):\n        while True:\n            yield np.random.uniform(low=0, high=1, size=(self.seq_len, self.n_seq))\n\n    def get_batch_noise(self):\n        return iter(tfdata.Dataset.from_generator(self._generate_noise, output_types=float32)\n                                .batch(self.batch_size)\n                                .repeat())\n\n    def sample(self, n_samples):\n        steps = n_samples \/\/ self.batch_size + 1\n        data = []\n        for _ in trange(steps, desc='Synthetic data generation'):\n            Z_ = next(self.get_batch_noise())\n            records = self.generator(Z_)\n            data.append(records)\n        return np.array(np.vstack(data))","f02f1392":"synth = TimeGAN(model_parameters=gan_args, hidden_dim=24, seq_len=seq_len, n_seq=n_seq, gamma=1)","f711971b":"autoencoder_opt = Adam(learning_rate=learning_rate)\nfor _ in tqdm(range(train_steps), desc='Emddeding network training'):\n    X_ = next(synth.get_batch_data(stock_data, n_windows=len(stock_data)))\n    step_e_loss_t0 = synth.train_autoencoder(X_, autoencoder_opt)","f44f9288":"supervisor_opt = Adam(learning_rate=learning_rate)\nfor _ in tqdm(range(train_steps), desc='Supervised network training'):\n    X_ = next(synth.get_batch_data(stock_data, n_windows=len(stock_data)))\n    step_g_loss_s = synth.train_supervisor(X_, supervisor_opt)","6a241954":"generator_opt = Adam(learning_rate=learning_rate)\nembedder_opt = Adam(learning_rate=learning_rate)\ndiscriminator_opt = Adam(learning_rate=learning_rate)","0a43ca6c":"step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\nfor _ in tqdm(range(train_steps), desc='Joint networks training'):\n\n    #Train the generator (k times as often as the discriminator)\n    # Here k=2\n    for _ in range(2):\n        X_ = next(synth.get_batch_data(stock_data, n_windows=len(stock_data)))\n        Z_ = next(synth.get_batch_noise())\n        \n        # Train the generator\n        step_g_loss_u, step_g_loss_s, step_g_loss_v = synth.train_generator(X_, Z_, generator_opt)\n\n        # Train the embedder\n        step_e_loss_t0 = synth.train_embedder(X_, embedder_opt)\n\n    X_ = next(synth.get_batch_data(stock_data, n_windows=len(stock_data)))\n    Z_ = next(synth.get_batch_noise())\n    step_d_loss = synth.discriminator_loss(X_, Z_)\n    \n    if step_d_loss > 0.15:\n        step_d_loss = synth.train_discriminator(X_, Z_, discriminator_opt)","63059a36":"sample_size = 250\nidx = np.random.permutation(len(stock_data))[:sample_size]\n\nreal_sample = np.asarray(stock_data)[idx]\nsynth_data = synth.sample(len(stock_data))\nsynthetic_sample = np.asarray(synth_data)[idx]\n\n#for the purpose of comparision we need the data to be 2-Dimensional. For that reason we are going to use only two componentes for both the PCA and TSNE.\nsynth_data_reduced = real_sample.reshape(-1, seq_len)\nstock_data_reduced = np.asarray(synthetic_sample).reshape(-1,seq_len)\n\nn_components = 2\npca = PCA(n_components=n_components)\ntsne = TSNE(n_components=n_components, n_iter=300)\n\n#The fit of the methods must be done only using the real sequential data\npca.fit(stock_data_reduced)\n\npca_real = pd.DataFrame(pca.transform(stock_data_reduced))\npca_synth = pd.DataFrame(pca.transform(synth_data_reduced))\n\ndata_reduced = np.concatenate((stock_data_reduced, synth_data_reduced), axis=0)\ntsne_results = pd.DataFrame(tsne.fit_transform(data_reduced))","f8a18beb":"fig = plt.figure(constrained_layout=True, figsize=(20,10))\nspec = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n\n#TSNE scatter plot\nax = fig.add_subplot(spec[0,0])\nax.set_title('PCA results',\n             fontsize=20,\n             color='red',\n             pad=10)\n\n#PCA scatter plot\nplt.scatter(pca_real.iloc[:, 0].values, pca_real.iloc[:,1].values,\n            c='black', alpha=0.2, label='Original')\nplt.scatter(pca_synth.iloc[:,0], pca_synth.iloc[:,1],\n            c='red', alpha=0.2, label='Synthetic')\nax.legend()\n\nax2 = fig.add_subplot(spec[0,1])\nax2.set_title('TSNE results',\n              fontsize=20,\n              color='red',\n              pad=10)\n\nplt.scatter(tsne_results.iloc[:sample_size, 0].values, tsne_results.iloc[:sample_size,1].values,\n            c='black', alpha=0.2, label='Original')\nplt.scatter(tsne_results.iloc[sample_size:,0], tsne_results.iloc[sample_size:,1],\n            c='red', alpha=0.2, label='Synthetic')\n\nax2.legend()\n\nfig.suptitle('Validating synthetic vs real data diversity and distributions',\n             fontsize=16,\n             color='grey')","3826ca00":"<h1 id=\"modules\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Modules\n        <a class=\"anchor-link\" href=\"#modules\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","118f5064":"<h1 id=\"parameters\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Parameters\n        <a class=\"anchor-link\" href=\"#parameters\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","545d31a9":"<h1 id=\"dataset\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","e38aa025":"<h1 id=\"definitions\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Definitions\n        <a class=\"anchor-link\" href=\"#definitions\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","6df0247a":"<h1 id=\"training\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ce7fd357":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/986308\/1665609\/1f835a26900f388d8f39e33691e46570\/dataset-cover.jpg\"\/>\n<\/div>","2f3d6665":"<h1 id=\"trainingModules\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Training Modules\n        <a class=\"anchor-link\" href=\"#trainingModules\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","1e60958a":"<h1 id=\"analyzing\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Analyzing\n        <a class=\"anchor-link\" href=\"#analyzing\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","f4aedb58":"Full credits goes to [Fabiana Clemented](https:\/\/towardsdatascience.com\/synthetic-time-series-data-a-gan-approach-869a984f2239) for this implementation.<br>\nPaper on [TimeGAN](https:\/\/papers.nips.cc\/paper\/2019\/file\/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)","923ded1c":"<h1 id=\"credits\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Credits\n        <a class=\"anchor-link\" href=\"#credits\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","5789d65e":"<h1 id=\"preprocess\" style=\"color:#34a68a; background:#5c594f; border:0.5px dotted #34a68a;\"> \n    <center>Preprocess\n        <a class=\"anchor-link\" href=\"#preprocess\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}