{"cell_type":{"5b8ed713":"code","716599d6":"code","612638c4":"code","908e3b79":"code","7a118b52":"code","732bb164":"code","2be203aa":"code","b43cb69d":"code","cbc7b50b":"code","7a688cd3":"code","f94478fb":"code","44005092":"code","55d036b5":"code","c1eaf745":"code","99bd6d2c":"code","f43ad220":"code","fcd12f45":"code","4c63cde4":"code","acf8d0cf":"code","7db2c5e5":"code","558053e7":"code","6136d210":"code","bf8630f5":"code","51b0b741":"code","c79e68ea":"code","d76f4cb3":"code","8a053613":"code","8d88a831":"code","5c37e1cb":"code","c9e33d41":"code","02736e1d":"code","d6ea3049":"code","4571b6f9":"code","8b188c81":"code","8682fe4e":"code","c9e99b9b":"code","f1f3d123":"markdown","93c866a7":"markdown","9f51d123":"markdown","df07e961":"markdown","343ea4fa":"markdown","38648af3":"markdown","8c090701":"markdown","7e559d15":"markdown","7182b92c":"markdown","6fadaa28":"markdown","11e6a958":"markdown","8c23da06":"markdown","d1034296":"markdown","2ca4f1e2":"markdown","4f63a493":"markdown","e5dbd949":"markdown","d6861ff0":"markdown","6004be3e":"markdown","0e338030":"markdown","28208884":"markdown","686399d1":"markdown","7d117275":"markdown","5046fbff":"markdown","73bab22a":"markdown","f37ef269":"markdown","baa8581e":"markdown","02222477":"markdown","e4e3a054":"markdown","8bdd87d1":"markdown","edf76923":"markdown","6356a9c5":"markdown","dc851e56":"markdown","26a3ae19":"markdown","5599d923":"markdown","7878afdd":"markdown"},"source":{"5b8ed713":"# Importing python modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgbm\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import FunctionTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('precision', 2)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nplt.style.use('ggplot')","716599d6":"data = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\nprint(\"Data loaded successfully!!\")\nprint(f\"There are {data.shape[0]} rows and {data.shape[1]} columns are present in the data.\")","612638c4":"# random sample of data\ndata.sample(5)","908e3b79":"# statistical summary\ndata.describe()","7a118b52":"# Null values\ndata.isna().mean().to_frame(name=\"% of null values\")","732bb164":"# Unique values\ndata.nunique().to_frame(name=\"# of unique values\")","2be203aa":"features = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n            'ejection_fraction', 'high_blood_pressure', 'platelets',\n            'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\n\ncontinuous_features = ['age','creatinine_phosphokinase','ejection_fraction',\n                       'platelets','serum_creatinine','serum_sodium','time']\n\ndiscrete_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n\ntarget = 'DEATH_EVENT'","b43cb69d":"fig, ax = plt.subplots(figsize=(8,5))\nsns.countplot(x=data[target], ax=ax)\nax.set_xlabel(target, fontsize=13, fontweight='bold')\nfor patch in ax.patches:\n    height = patch.get_height()\n    width = patch.get_width()\n    new_width = width * 0.4\n    patch.set_width(new_width)\n    x = patch.get_x()\n    patch.set_x(x + (width - new_width) \/ 2)\n    ax.text(x=x + width\/2, y=height, s=height, ha='center', va='bottom')\nplt.tight_layout()","cbc7b50b":"fig, axes = plt.subplots(4,2, figsize=(15,20))\naxes = np.ravel(axes)\nfor i, col in enumerate(continuous_features):\n    sns.distplot(a=data[col], ax=axes[i], bins=30, color='blue')\n    axes[i].set_title(f\" Distribution of {col}\")\nplt.tight_layout()","7a688cd3":"## source: https:\/\/stackoverflow.com\/questions\/64946868\/on-changing-the-bar-width-of-a-countplot-the-relative-position-of-the-bars-get\ndisc_data = data[discrete_features].astype('category')\n\nfig, axes = plt.subplots(3,2, figsize=(13,15))\naxes=np.ravel(axes)\nfor i, col in enumerate(discrete_features):\n    sns.countplot(x=disc_data[col], ax=axes[i])\n    axes[i].set_title(col, fontsize=13, fontweight='bold')\n    for patch, label in zip(axes[i].patches, [\"NO\", \"YES\"]):\n        height = patch.get_height()\n        width = patch.get_width()\n        new_width = width * 0.4\n        patch.set_width(new_width)\n        patch.set_label(label)\n        x = patch.get_x()\n        patch.set_x(x + (width - new_width) \/ 2)\n        axes[i].text(x=x + width\/2, y=height, s=height, ha='center', va='bottom')\n            \n    axes[i].legend(loc='lower right')\n    axes[i].margins(y=0.1)\nplt.tight_layout()\nplt.show()","f94478fb":"fig, axes = plt.subplots(4,2, figsize=(13,15))\naxes=np.ravel(axes)\n\nfor i,col in enumerate(continuous_features):\n    sns.boxplot(x = data[target].astype('category'), y = col, data=data, ax=axes[i])\n    axes[i].set_ylabel(col, fontweight='bold')\n    axes[i].set_xlabel(target, fontweight='bold')\n    axes[i].set_title(f'{col} vs target', fontsize=14)\n    \nplt.tight_layout()","44005092":"fig, axes = plt.subplots(3, 2, figsize=(15, 15))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, col in enumerate(discrete_features):\n    fltr = data[target] == 0\n    vc_a = data[fltr][col].value_counts().reset_index().rename({'index' : col, col: 'count'}, axis=1)\n\n    vc_b = data[~fltr][col].value_counts().reset_index().rename({'index' : col, col: 'count'}, axis=1)\n\n    vc_a[target] = 0\n    vc_b[target] = 1\n\n    df = pd.concat([vc_a, vc_b]).reset_index(drop = True)\n\n    sns.barplot(x = col, y = 'count', data = df , hue=target, ax=axes[i])\n    axes[i].set_title(col, fontweight='bold')\nplt.tight_layout()","55d036b5":"fig, axes = plt.subplots(3, 2, figsize=(15,15))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, c in enumerate(discrete_features):\n    df = data[[c,target]].groupby(c).mean().reset_index()\n    sns.barplot(df[c], df[target], ax=axes[i])\n    for patch in axes[i].patches:\n        height = patch.get_height()\n        width = patch.get_width()\n        new_width = width * 0.4\n        patch.set_width(new_width)\n        x = patch.get_x()\n        patch.set_x(x + (width - new_width) \/ 2)\n    axes[i].set_ylabel('mean of target', fontsize=14)\n    axes[i].set_xlabel(c, fontsize=14, fontweight='bold')\n    \nplt.tight_layout()\nplt.show()","c1eaf745":"corr_mat = data.corr()[target].sort_values(ascending=False).to_frame()\nplt.figure(figsize=(2,8))\nsns.heatmap(corr_mat, cmap='Blues', cbar=False, annot=True)\nplt.show()","99bd6d2c":"train, test = train_test_split(data, test_size=0.2, random_state=1, stratify=data[target])","f43ad220":"transformer = FunctionTransformer(np.log)\n\ntrain[continuous_features] = transformer.fit_transform(train[continuous_features])\ntest[continuous_features] = transformer.transform(test[continuous_features])","fcd12f45":"X_train = train[features]\ny_train = train[target]\n\nX_test = test[features]\ny_test = test[target]\n\nprint(\"Train set : \", train.shape)\nprint(\"Test set : \", test.shape)","4c63cde4":"selector = SelectFromModel(\n    \n    RandomForestClassifier(n_estimators = 100,\n                           random_state=1),\n    threshold='median')\n\nselector.fit(X_train, y_train)\n\nselected_feat= X_train.columns[(selector.get_support())].tolist()\nprint(\"Best features : \",selected_feat)","acf8d0cf":"importance = pd.Series(\n    selector.estimator_.feature_importances_.ravel(),\n    features).to_frame(name=\"feature importance\") \\\n.sort_values('feature importance', ascending=False)\nimportance","7db2c5e5":"X_train = X_train[selected_feat]\nX_test = X_test[selected_feat]","558053e7":"results = {\"model\":[], \"CV f1-score\":[]}","6136d210":"# Baseline model\ndef base_model(clf):\n    clf.fit(X_train, y_train)\n    train_preds = clf.predict(X_train)\n    test_preds = clf.predict(X_test)\n    print(\"Train f1 Score :\", f1_score(y_train, train_preds))\n    print(\"Test f1 Score :\", f1_score(y_test, test_preds))  ","bf8630f5":"'''### K - Fold Cross validation ###\nStep 1: Randomly divide a dataset into k groups, or \u201cfolds\u201d, of roughly equal size.\nStep 2: Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds.\nStep 3: Calculate the test F1-score on the observations in the fold that was held out.\nStep 4: Repeat this process k times, using a different set each time as the holdout set.\nStep 5: Calculate the average of the k test F1-scores to get the overall test F1-score.'''\n# Below function implements K-Fold cross validation.\n\ndef run_kfold(model, X_train, y_train, N_SPLITS = 10):\n    f1_list = []\n    oofs = np.zeros(len(X_train))\n    folds = StratifiedKFold(n_splits=N_SPLITS)\n    for i, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        \n        print(f'\\n------------- Fold {i + 1} -------------')\n        X_trn, y_trn = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n        X_val, y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        model.fit(X_trn, y_trn)\n        # Instead of directly predicting the classes we will obtain the probability of positive class.\n        preds_val = model.predict_proba(X_val)[:,1]\n        \n        fold_f1 = f1_score(y_val, preds_val.round())\n        f1_list.append(fold_f1)\n        \n        print(f'\\nf1 score for validation set is {fold_f1}') \n        \n        oofs[val_idx] = preds_val\n        \n    print(f'\\n----------------------------------')\n    mean_f1 = sum(f1_list)\/N_SPLITS\n    print(\"\\nMean validation f1 score :\", mean_f1)\n    \n    oofs_score = f1_score(y_train, oofs.round())\n    print(f'\\nF1 score for oofs is {oofs_score}')\n    return oofs, mean_f1","51b0b741":"tree = DecisionTreeClassifier(random_state=1)\nbase_model(tree)","c79e68ea":"params = {\n    'max_depth': [4, 6, 8, 10, 12, 14, 16, 20],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [5, 10, 20, 30, 40, 50],\n    'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n    'max_leaf_nodes': [8, 16, 32, 64, 128,256],\n    'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2},\n                     {0: 1, 1: 3}, {0: 1, 1: 4}]\n}\n\nclf = RandomizedSearchCV(DecisionTreeClassifier(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","d76f4cb3":"clf = DecisionTreeClassifier(random_state = 1,\n                             **search.best_params_)\noofs, mean_f1 = run_kfold(clf, X_train, y_train, N_SPLITS=5)\nresults['model'].append(\"Decision Tree\")\nresults['CV f1-score'].append(mean_f1)","8a053613":"log = LogisticRegression(random_state=1)\nbase_model(log)","8d88a831":"params = {\n    'penalty': ['l1', 'l2','elasticnet'],\n    'C':[0.0001, 0.001, 0.1, 1, 10, 100,1000],\n    'fit_intercept':[True, False],\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'class_weight':['balanced', None]\n}\n\nclf = RandomizedSearchCV(LogisticRegression(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","5c37e1cb":"clf = LogisticRegression(random_state = 1,\n                         **search.best_params_)\n\noofs, mean_f1 = run_kfold(clf, X_train, y_train, N_SPLITS=5)\n\nresults['model'].append(\"Logistic regression\")\nresults['CV f1-score'].append(mean_f1)","c9e33d41":"tree = RandomForestClassifier(random_state=1)\nbase_model(tree)","02736e1d":"params = {'bootstrap': [True, False],\n         'max_depth': [5,10, 20, 30, 50,None],\n         'max_features': ['auto', 'sqrt'],\n         'min_samples_leaf': [1, 2, 4],\n         'min_samples_split': [2, 5, 10],\n         'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2}, {0: 1, 1: 3}],\n         'n_estimators': [50, 100, 200, 300, 500]}\n\nclf = RandomizedSearchCV(RandomForestClassifier(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","d6ea3049":"clf = RandomForestClassifier(random_state = 1,\n                         **search.best_params_)\n\noofs, mean_f1 = run_kfold(clf, X_train, y_train, N_SPLITS=5)\n\nresults['model'].append(\"Random Forest\")\nresults['CV f1-score'].append(mean_f1)","4571b6f9":"pd.DataFrame(results)","8b188c81":"params = {'n_estimators': 100,\n          'min_samples_split': 5,\n          'min_samples_leaf': 4,\n          'max_features': 'auto',\n          'max_depth': 30, \n          'class_weight': \n          {0: 1, 1: 2}, \n          'bootstrap': True}\n\nfinal_model = RandomForestClassifier(random_state=1,\n                                     **params\n                                    )\nfinal_model.fit(X_train, y_train)\n\ntrain_preds = final_model.predict(X_train)\ntest_preds = final_model.predict(X_test)\n\nprint(\"Train f1 Score :\", f1_score(y_train, train_preds))\nprint(\"Test f1 Score :\", f1_score(y_test, test_preds))  ","8682fe4e":"print(classification_report(y_test, test_preds))","c9e99b9b":"cm = confusion_matrix(y_test,test_preds,normalize='true')\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, cmap='Blues', cbar=False,fmt='.2f')\nplt.show()","f1f3d123":"### K Fold - Cross validation","93c866a7":"<a id=\"section2\"><\/a>\n# Exploratory Data Analysis\n\n","9f51d123":"Observation : Seems like Many discrete features are not so helpful in predicting target.<br>\nHigh_blood_pressure, anaemia are useful","df07e961":"### Distribution of continuous features","343ea4fa":"### Discrete features Vs Target","38648af3":"### Importance of all features","8c090701":"### Log transformation","7e559d15":"### Confusion matrix","7182b92c":"<center><h1> <u>Heart failure prediction<\/u><\/h1><\/center>\n<img src=\"https:\/\/images.pexels.com\/photos\/6765583\/pexels-photo-6765583.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940\" width=\"50%\">\n<center><a href=\"https:\/\/www.pexels.com\/photo\/flower-petals-scattered-around-decorative-heart-6765583\/\">Photo by Michelle Leman from Pexels<\/a><\/center>\n\n## Contents\n- [The problem and The data](#section1)\n    - [Understanding the problem](#subsection1)\n    - [About the dataset](#subsection2)\n- [Exploratory data analysis](#section2)\n- [Feature Engineering](#section3)\n- [Feature Selection](#section4)\n- [Modeling and Evaluation](#section5)\n    - [Decision tree Classifier](#tree)\n    - [Logistic Regression](#logistic)\n    - [Random Forest Classifier](#forest)\n- [Final Model](#final)","6fadaa28":"### Correlation of features with target","11e6a958":"<a id=\"section4\"><\/a>\n# Feature Selection\n<b>Feature selection using Random forest<\/b> comes under the category of Embedded methods. Embedded methods combine the qualities of filter and wrapper methods. They are implemented by algorithms that have their own built-in feature selection methods. Some of the benefits of embedded methods are :\n- They are highly accurate.\n- They generalize better.\n- They are interpretable\n\n<a href=\"https:\/\/towardsdatascience.com\/feature-selection-using-random-forest-26d7b747597f\">Reference blog<\/a>","8c23da06":"<a id=\"forest\"><\/a>\n## Random Forest Classifier\n\n### Base model","d1034296":"<a id=\"final\"><\/a>\n# Final Model\n","2ca4f1e2":"<a id=\"section5\"><\/a>\n# Modeling and Evaluation","4f63a493":"### K Fold - Cross validation","e5dbd949":"<a id=\"tree\"><\/a>\n## Decision tree","d6861ff0":"### Target distribution","6004be3e":"### Distribution of discrete features","0e338030":"### Hyperparameter tuning","28208884":"<a id=\"section1\"><\/a>\n# The problem and The data\n<a id=\"subsection1\"><\/a>\n## Understanding the problem\n<b>Let's understand the problem that we are going to solve.<br><\/b>\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.<br>\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management.<br>\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.<br>\n<b>We can build a machine learning model for predicting mortality caused by Heart Failure using other health factors of the patient.<\/b><br>\nIn ML terminology, a <b style=\"color:green;\"> Supervised Learning Binary Classifcation problem.<\/b>\n\n<a id=\"subsection2\"><\/a>\n## About the dataset\nThis dataset contains 12 features that can be used to predict mortality by heart failure.<br>\n<b> age <\/b> : Age of the patient <br>\n<b> anaemia <\/b> : 0 = N0, 1 = YES  <br>\n<b> creatinine_phosphokinase <\/b> : measure of creatinine phosphokinase level in bloodstream <br>\n<b> diabetes <\/b> : 0 = NO, 1 = YES <br>\n<b> ejection_fraction <\/b> : The measurement of the percentage of blood leaving the heart each time it contracts. <br>\n<b> high_blood_pressure <\/b> : 0 = NO, 1 = YES <br>\n<b> platelets <\/b> : Count of platelets <br>\n<b> serum_creatinine <\/b> : serum creatinine level <br>\n<b> serum_sodium <\/b> :  measure of sodium in the body<br>\n<b> sex <\/b> :  0 = FEMALE, 1 = MALE<br>\n<b> smoking <\/b> : 0 = NO, 1 = YES <br>\n<b> time <\/b> : the time at which DEATH_EVENT happened in days. For example; if the patient died, then it tells how many days it took to happen, if the patient survives, it tells how long recovery took.<br>\n<b> DEATH_EVENT <\/b> : 0 = NO, 1 = YES (target) <br>","686399d1":"### Preprocessed data","7d117275":"### Loading the data into memory","5046fbff":"### K fold - Cross validation","73bab22a":"<b> Random Forest performing better.. Let's evaluate the results.!<\/b>","f37ef269":"### Final data","baa8581e":"### Thank you..!!\n--- &nbsp;  Ashok kumar","02222477":"### Hyperparameter tuning","e4e3a054":"### Classification report","8bdd87d1":"### Continuous features Vs Target (Box plot)","edf76923":"### Discrete features distribution w.r.t Target","6356a9c5":"<a id=\"section3\"><\/a>\n# Feature Engineering\nBefore transforming the features it is better to split the data into train and test sets\n\n### Train test split\n<b>Training : <\/b>80% of data<br>\n<b>Testing : <\/b>20% of data","dc851e56":"### Hyperparameter tuning","26a3ae19":"### Base model","5599d923":"<a id=\"logistic\"><\/a>\n## Logistic Regression\n\n### Base model","7878afdd":"### Variable Separation\nSeparating the features based on their data type."}}