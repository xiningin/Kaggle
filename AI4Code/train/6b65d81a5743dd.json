{"cell_type":{"6e34dc3a":"code","61fdc702":"code","08c6497d":"code","e9c92722":"code","146615c8":"code","32864d54":"code","8f0bcafb":"code","d76e63ae":"code","1204ec9f":"code","9d5a4396":"code","e6b755de":"code","320d5891":"code","315dc290":"code","803f20ee":"code","337b47d6":"markdown","5a8091eb":"markdown","ef5487c1":"markdown","f489e0d6":"markdown","ce314169":"markdown","5700f8aa":"markdown","2bcea94e":"markdown","7a249388":"markdown","f590a23c":"markdown","980eb9f8":"markdown","ece88ece":"markdown","0027b3f4":"markdown","18b17185":"markdown","689373f6":"markdown","53a84da9":"markdown","9ae8a8c3":"markdown","8aa84545":"markdown","0dcb6120":"markdown","e8e8f7a2":"markdown","a66bd52a":"markdown","d19393ca":"markdown"},"source":{"6e34dc3a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom imblearn.combine import SMOTETomek","61fdc702":"df = pd.read_csv('..\/input\/performance-prediction\/summary.csv')\ndf.head()","08c6497d":"df.info()","e9c92722":"df.isnull().sum()","146615c8":"df = df.fillna(0)","32864d54":"columns = df.columns.tolist()[1:]\nplt.figure(figsize=(20,20))\nsns.heatmap(df[columns].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.title('Correlations Heat Map')\nplt.show()","8f0bcafb":"bins = np.arange(10,df.GamesPlayed.max(),5)\nplt.figure(figsize=(10,7))\nplt.hist(df[df.Target == 1].GamesPlayed,alpha=0.8,bins=bins)\nplt.hist(df[df.Target == 0].GamesPlayed,alpha=0.8,bins=bins)\nplt.title('Difference in Games played by the Target')\nplt.xlabel('Number of Games Played')\nplt.ylabel('Frequency')\nplt.xticks(bins)\nplt.show()","d76e63ae":"df['efficiency'] = (df['FieldGoalsMade']+df['Rebounds']+df['Assists']+df['Steals']+df['Blocks']+df['Turnovers'])\/df['MinutesPlayed']\ndf['Participation'] = df['MinutesPlayed']\/df['GamesPlayed']","1204ec9f":"bins = np.arange(0.2,df.Participation.max(),0.1)\nplt.figure(figsize=(10,7))\nplt.hist(df[df.Target == 1]['efficiency'],alpha=0.8)\nplt.hist(df[df.Target == 0]['efficiency'],alpha=0.8)\nplt.title('Difference in Efficiency by the Target')\nplt.xlabel('Efficiency Score')\nplt.ylabel('Frequency')\nplt.xticks(bins)\nplt.show()","9d5a4396":"bins = np.arange(0.2,df.Participation.max(),0.1)\nplt.figure(figsize=(10,7))\nplt.hist(df[df.Target == 1]['Participation'],alpha=0.8)\nplt.hist(df[df.Target == 0]['Participation'],alpha=0.8)\nplt.title('Difference in Participation by the Target')\nplt.xlabel('Participation Score')\nplt.ylabel('Frequency')\nplt.xticks(bins)\nplt.show()","e6b755de":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler(feature_range=(0,1))","320d5891":"#Target\nY = df['Target'].values\n#Inputs\nX = df[['GamesPlayed', 'MinutesPlayed', 'PointsPerGame',\n       'FieldGoalsMade', 'FieldGoalsAttempt', 'FieldGoalPercent', '3PointMade',\n       '3PointAttempt', '3PointPercent', 'FreeThrowMade', 'FreeThrowAttempt',\n       'FreeThrowPercent', 'OffensiveRebounds', 'DefensiveRebounds',\n       'Rebounds', 'Assists', 'Steals', 'Blocks', 'Turnovers',\n       'efficiency', ]].values\n#Normalize our variables\nX = mms.fit_transform(X)\n#Split to training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n#Define the model\nmodel = XGBClassifier(learning_rate = 0.1,n_estimators=200, max_depth=6)\n#train the model\nmodel.fit(X_train, y_train)\n#Check training accuracy\ntrainingAccuracy =  metrics.accuracy_score(y_train,model.predict(X_train))\nprint(\"Training Accuracy: %.2f%%\" % (trainingAccuracy * 100.0))\n#Check testing accuracy\ntestingAccuracy =  metrics.accuracy_score(y_test, model.predict(X_test))\nprint(\"Testing Accuracy: %.2f%%\" % (testingAccuracy * 100.0))","315dc290":"from collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context='notebook', palette='deep')","803f20ee":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","337b47d6":"We got 72.4% Accuracy from XGBoost","5a8091eb":"* 2.1 Import libraries needed","ef5487c1":"# 1. Introduction","f489e0d6":"Today I will explore the basketball player's performance dataset, and finish the required task by the author.\n\n**Task Details:**\n> I came up with this dataset in order to know that how long a player can play based on the previous summary stats.\n> In this dataset there are 21 features describing the performance measures of each player or you can say the summary of each player.\n> Your task is to predict the target variable.\n> Target Variable:\n> 1-Whether a player's career is equal to or greater than 5 years.\n> 0-Career is shorter than 5 years\n>Expected Submission\n>You have to solve the task primarily using Notebooks\n>Evaluation\n>Use various Classification Algorithms to predict the target variable with higher accuracy score.\n","ce314169":"\nWill start with XGBoost","5700f8aa":"Let's Explore the GamesPlayed feature to see if it correlates ","2bcea94e":"Let's check the correlations of other features with a heat map","7a249388":"After some google searching we can intoduce 2 new features \n> Player Efficiency Rating : (FieldGoalsMade + Rebounds + Assists + Steals + Blocks + Turnovers)\/ MinutesPlayed\n\n> Participation : MinutesPlayed\/GamedPlayed","f590a23c":"# 2. Import Libraries and load data","980eb9f8":"Check for missing values","ece88ece":"Ok so on everage higher efficiency score correlate with more experienced players","0027b3f4":"Not much difference between the two targets when it comes to participation score","18b17185":"#### It's obvious that more game will lead to more in every other feature else so let's introduce some new features","689373f6":"We Understand from this that on average players' career who played more than 50 games are more likely to be equal to or greater than 5 years.","53a84da9":"Check The dataset structure","9ae8a8c3":"So XGBoost has the best Accuracy of 72.4%","8aa84545":"Let's try other classifiers","0dcb6120":"* 2.2 Load and Check Data","e8e8f7a2":"Now let's make some predictions.\n","a66bd52a":"We have 11 missing values in 3PointPercent,\n\nAfter checking the dataframe entries where there is missing data it appeared it's missing because it's a zero divide so we replace it with zero instead","d19393ca":"# EDA"}}