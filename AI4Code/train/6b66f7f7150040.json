{"cell_type":{"e456f352":"code","0b54e80e":"code","b176080a":"code","639a96f6":"code","d688e7c4":"code","465c1158":"code","a283fbd3":"code","0ad42470":"code","8b410826":"code","2153cf95":"code","6a0278c9":"code","dc970b1e":"code","07f66b1b":"code","be57bf6c":"code","aad72f5f":"code","2747642d":"code","3aec035e":"code","374bc746":"code","1a54ca87":"code","468890e7":"code","6e2cd3c6":"code","081e80ad":"code","5bf859a1":"code","2bd6722b":"code","011c4068":"code","e6e2dce0":"code","4c2b1f34":"code","02d827f3":"code","820ba605":"code","0f2a2f33":"code","dbfe1d76":"code","f287d1c3":"code","abb82aa2":"code","5fcb8109":"code","92466b8e":"code","499f336c":"code","008d3602":"code","05458563":"code","239a84ac":"code","c81a8d9a":"code","ca22a266":"code","5be38166":"code","d77aaedb":"code","fb3190e6":"code","3e8adbdb":"code","c3f98e9f":"code","0f601341":"code","5d4b318a":"code","bce89687":"code","280fdd62":"code","829964c5":"code","c19288cb":"code","b5775fc8":"code","51b34a69":"code","8b34ceeb":"code","bc0a8fc5":"markdown","85cf3fe8":"markdown","cef79a22":"markdown","530adb76":"markdown","d835a75d":"markdown","d5727d06":"markdown","b5084044":"markdown","1490862e":"markdown","cb3c71f7":"markdown","3bf25476":"markdown","53fa4862":"markdown","b1be3895":"markdown","fcae29b5":"markdown","82a248b6":"markdown","33f2736a":"markdown","b5fcf249":"markdown","98842900":"markdown","693260c9":"markdown","6414db19":"markdown","a9a75c99":"markdown","ab7a534f":"markdown","cf2b8407":"markdown","1415f609":"markdown","4bdb2215":"markdown","44c37e7f":"markdown","c763bbfe":"markdown","d196cb6f":"markdown","4f4ff26d":"markdown","85a54d9b":"markdown","621b70b5":"markdown","d3f26b21":"markdown","e82b1f0f":"markdown","1d2f9e1d":"markdown","f4f6b4f9":"markdown","6412943a":"markdown","dc7e1247":"markdown","4db214e2":"markdown","dc2247b7":"markdown","06137b59":"markdown","b9e4411b":"markdown","79d68882":"markdown","8f6cbd80":"markdown","6c10703e":"markdown","e0e83ae8":"markdown","eebb64a2":"markdown","e51c3ffc":"markdown","323f8458":"markdown","427036a7":"markdown","8435d4c9":"markdown","fbf52a2c":"markdown","5c091ca7":"markdown","3d951fd9":"markdown","86e62c57":"markdown","0a2703b5":"markdown","d6ef1661":"markdown","3e5b862f":"markdown","9ecec2a1":"markdown","a3c42364":"markdown","38e6b08f":"markdown","79e21d07":"markdown","7ad4e820":"markdown","74853afa":"markdown","203e0fa7":"markdown","c9b9cf57":"markdown","0b6f6bca":"markdown","faf54717":"markdown","3060571c":"markdown"},"source":{"e456f352":"import sys\nimport glob\nimport pandas as pd\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport json\nfrom pylab import rcParams\nimport numpy as np\nfrom collections import defaultdict\nimport itertools\nfrom IPython.display import display\nimport gc\nfrom tqdm import tqdm\nimport cudf\nfrom cudf import DataFrame\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import shapiro, spearmanr\nfrom scipy.cluster import hierarchy\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nimport optuna\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport warnings\n%matplotlib inline\n\n# Set pandas options\npd.set_option(\"display.max_colwidth\", None)\npd.set_option(\"display.max_columns\", None)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Threshold:\", gc.get_threshold())\nprint(\"Count:\", gc.get_count())","0b54e80e":"# 0 for training, 1 for inference\nMODE = 0\n\n# 1 if pretrained datasets exist at phase 1, 2 at phase 2, and so on. There are 6 phases in total.\nPRETRAINED = 1\n\n# Optiver data directory\nDIR = \"..\/input\/optiver-realized-volatility-prediction\/\"\n\n# csv train and test data\nCSV_FILES = ['train.csv', 'test.csv']\n\n# Parquet train and test data of order and trade books\nPARQUET_FILES = {\"Order Book\": ['book_train.parquet', 'book_test.parquet'], \n                 \"Trade Book\": ['trade_train.parquet', 'trade_test.parquet']}\n\n# Epsilon number to avoid dividing by zero error\nEPSILON = sys.float_info.epsilon\n\n# Impute values for missing items\nCORRECT_DATASET = \"yes\"\n\n# Seconds windows\nWINDOWS = [0, 120, 240, 360, 480]\n\n# Seed value to be used in the model\nSEED = 1","b176080a":"# Make dtype changes\ndef change_dtype(df, convert_dict):\n    df = df.astype(convert_dict)\n    return df\n\n# Create dtype list\ndef make_dtype_list(df, source_dtype, target_dtype):\n    try:\n        source_dtype_list = list(df.select_dtypes(include=source_dtype).columns)\n        dtype_dict = dict(zip(source_dtype_list, [target_dtype] * len(source_dtype_list)))\n    except:\n        pass\n    df = change_dtype(df, dtype_dict)\n    return df","639a96f6":"%%time\ndef load_reformat_csv(file):\n    \n    print(\"Information about\", CSV_FILES[MODE] + \" data\")\n    print('-' * 50, '\\n\\n')\n    df = cudf.read_csv(file)\n    \n    # Describe data before reformat\n    print(\"Before reformat \\n\")\n    display(df.info())\n    print(\"\\n\")\n    \n    # Make dtype changes\n    if MODE == 1:\n        df[\"target\"] = 0\n    convert_dict = {'time_id': 'int16', 'stock_id': 'int16', 'target': 'float32'}\n    df = change_dtype(df, convert_dict)\n    \n    # Describe data after reformat\n    print(\"After reformat \\n\")\n    display(df.info())\n    print(\"\\n\\n\")\n    display(df.head())\n    return df\n\nfilename = DIR + CSV_FILES[MODE]\ndf_csv = load_reformat_csv(filename)","d688e7c4":"%%time\ndef describe_parquet_data(key):\n    \n    # Retrieve the file name\n    file = PARQUET_FILES[key][MODE]\n    \n    # Column names\n    column_names = ['time_id', \"stock_id\"]\n    \n    print(file, \"from\", key, \":\")\n    print('-' * 50, '\\n')\n    \n    # Load the parquet file\n    list_order_book_file_train = glob.glob(DIR + file + \"\/*\")\n    df_parquet = cudf.read_parquet(list_order_book_file_train)\n    \n    # Get info about data\n    display(df_parquet.info())\n    print(\"\\n\")\n    \n    # Check if any nan values\n    display(df_parquet.isnull().values.any())\n    print(\"\\n\")\n    \n    # Get unique values in time_id and stock_id columns and put them in a list\n    time_ids = sorted(df_parquet['time_id'].unique().to_array().tolist())\n    stock_ids = sorted([int(file_path.split('=')[1]) for file_path in list_order_book_file_train])\n    ids =[time_ids, stock_ids]\n\n    # Find the number of records, max and min values in the column specified\n    time_id_records = len(time_ids)\n    stock_id_records = len(stock_ids)\n    records = [time_id_records,stock_id_records]\n    \n    # Output the values found\n    for i in range(2):\n        minRecord = np.min(ids[i])\n        maxRecord = np.max(ids[i])\n        print(f\"Number of {column_names[i]} records in {key}: {records[i]} \\n Minimum {column_names[i]} record: {minRecord} \\n Maximum {column_names[i]} record: {maxRecord}\")\n        print('\\n')\n        \n    # Delete parquet df\n    del df_parquet\n        \n    return time_ids, stock_ids\n\nif MODE == 0:\n\n    keys = [\"Order Book\", \"Trade Book\"]\n\n    # Get order book info and time_ids, stock_ids as a list\n    time_ids, stock_ids = describe_parquet_data(keys[0])\n\n    # Get trade book info\n    _, _ = describe_parquet_data(keys[1])","465c1158":"%%time\n\n# Forward fill data\ndef ffill(data_df, key):\n    \n    # Forward fill\n    data_df = data_df.set_index(['time_id', 'seconds_in_bucket']).to_pandas()\n    \n    if key == \"Order Book\":\n        data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n        data_df = data_df.reset_index()\n        convert_dict = {'time_id': 'int16', 'seconds_in_bucket': 'int16'}\n    else:\n        data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']))\n        # Fill nan values with 0\n        data_df = data_df.fillna(0)\n        data_df = data_df.reset_index()\n        convert_dict = {'time_id': 'int16', 'seconds_in_bucket': 'int16', 'size': 'int32', 'order_count': 'int32'}\n        \n    # Make dtype changes\n    data_df = change_dtype(data_df, convert_dict)\n\n    # Convert to a cudf dataframe\n    data_df = cudf.DataFrame.from_pandas(data_df)\n\n    return data_df\n\n\n# Load parquet data\ndef load_parquet_data(key, stockId):\n    \n    # Retrieve the file name\n    file = PARQUET_FILES[key][MODE]\n    \n    # Find the file of the stock_id and create a dataframe\n    df_parquet_data = cudf.read_parquet(DIR + file + \"\/stock_id=\" + str(stockId), npartitions=1)\n    df_parquet_data = ffill(df_parquet_data, key)\n    \n    return df_parquet_data","a283fbd3":"def add_stock_id(df, stockId):\n    \n    df['stock_id'] = stockId\n    \n    # Make dtype changes\n    convert_dict = {'stock_id': 'int16'}\n    df = change_dtype(df, convert_dict)\n    \n    # Reorder the columns\n    cols = df.columns.tolist()\n    cols = list(itertools.chain(cols[0:1], cols[-1:], cols[1:-1]))\n    df = df[cols]\n    \n    return df","0ad42470":"def calculate_density_code(x):\n    if x !=0:\n        return 1\n    else:\n        return 0","8b410826":"def calculate_density_score(df, m):\n    \n    # Calculate density score by window\n    df[\"density_score_\" + str(m)] = df[\"density_code_\" + str(m) + \"_sum\"] \/ m\n    \n    return df","2153cf95":"def shift_wap(window):\n    for a in window:\n        b = a * 1\n    return b","6a0278c9":"def calculate_avg_log_return(df, m):\n       \n    # Calculate the average log return by window and assign it to a new column\n    df[\"avg_log_return_\" + str(m)] = (df[\"log_return_\" + str(m) + \"_sum\"] ** 2) \/ m\n        \n    return df","dc970b1e":"def calculate_normal_return_by_timeid(df):\n    \n    return_list = []\n    \n    # Create a dictionary with time_id as the key and wap1 as the value\n    list_000 = df[df['seconds_in_bucket']==1][[\"time_id\", \"wap1\"]].to_pandas().to_dict('records')\n    list_599 = df[df['seconds_in_bucket']==599][[\"time_id\", \"wap1\"]].to_pandas().to_dict('records')\n    dict_000 = {x['time_id']: x['wap1'] for x in list_000}\n    dict_599 = {x['time_id']: x['wap1'] for x in list_599}\n    \n    # Calculate return\n    for time_id in time_ids:\n        # Check if time_id exists\n        if time_id in dict_000 and time_id in dict_599:\n            stock_return_by_timeid = (dict_599[time_id] \/ dict_000[time_id]) - 1\n        else:\n            stock_return_by_timeid = 0\n            print(\"missing time_id: \", time_id, \", substitute stock_return_by_timeid: \", stock_return_by_timeid)\n        return_list.append(stock_return_by_timeid)\n        \n    pdf = pd.DataFrame({'time_id': time_ids,'norm_return_by_timeid': return_list})                                                                                               \n    \n    df_temp = cudf.DataFrame.from_pandas(pdf)\n    \n    return df_temp","07f66b1b":"def calculate_realized_volatility(df, m):\n    \n    # Calculate realized volatility by window\n    df[\"realized_volatility_\" + str(m)] = np.sqrt(df[\"sqrd_log_return_\" + str(m) + \"_sum\"])\n        \n    return df","be57bf6c":"def calculate_density_scored_total_trade_vol(df, m):\n    \n    # Calculate density-scored trade volume by window\n    df[\"density_scored_total_trade_vol_\" + str(m)] = df[\"trade_volume_\" + str(m) + \"_sum\"] * df[\"density_score_\" + str(m)]\n    \n    return df","aad72f5f":"def calculate_density_scored_total_orders(df, m):\n\n    # Calculate density-scored total orders by window\n    df[\"density_scored_total_orders_\" + str(m)] = df[\"order_count_\" + str(m) + \"_sum\"] * df[\"density_score_\" + str(m)]\n    \n    return df","2747642d":"def calculate_max_min_difference(df, m):\n\n    # Calculate max-min difference by window\n    for fs in features_list:\n        df[fs + '_' + str(m) + '_max_min'] = df[fs + '_' + str(m) + '_max'] - df[fs + '_' + str(m) + '_min']\n        \n    return df","3aec035e":"def calculate_kurtosis(df_merged, df, window):\n    \n    # Calculate kurtosis by group\n    df_moment = df_merged[df_merged['seconds_in_bucket'] >= window].groupby('time_id', as_index=False, sort=True)[features_list].apply(lambda x: x.kurt(skipna = True))\n    \n    # Rename columns\n    df_moment.rename({col: col + \"_\" +  str(window) + \"_kurt\" for col in df_moment.columns.values[1:]}, axis=1, inplace=True)\n    \n    # Merge with the main dataframe\n    df = df.merge(df_moment, how=\"left\", on=[\"time_id\"])\n    \n    # Delete dataframe\n    del df_moment\n\n    return df","374bc746":"# Mathematical and statistical operations\noperations = [\"mean\", \"median\", \"max\", \"min\", \"std\"]\n\n# Features list\nfeatures_list = ['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2', 'bid1_volume', \n                 'bid2_volume', 'ask1_volume', 'ask2_volume', 'price', 'size', 'wap1', 'wap2', 'trade_volume', 'order_count', \n                 'spread_ask1_bid1', 'spread_ask2_bid2', 'spread_bid1_bid2', 'spread_ask2_ask1', 'log_return', 'high_low_measure', \n                 'volume_imbalance', 'trade_volume_imbalance', 'stock_liquidity', 'wap_liquidity', 'norm_return_by_second'\n                ]    \n\n# Data dictionary\ndata_dict = defaultdict(list)\nmoment_dict = {}\n\nfor fs in features_list:\n    data_dict[fs].extend(operations)\n    moment_dict[fs] = \"skew\"\n    \nsum_list = ['bid1_volume', 'bid2_volume', 'ask1_volume', 'ask2_volume', 'density_code', 'trade_volume', 'order_count', 'log_return', \n            'sqrd_log_return', 'stock_liquidity', 'wap_liquidity', 'volume_imbalance', 'trade_volume_imbalance'\n           ]\n\nfor fs in sum_list:\n    data_dict[fs].append(\"sum\")\n\ndisplay(data_dict)\nprint(\"\\n\")\ndisplay(moment_dict)","1a54ca87":"def create_stat_dfs(df_merged, window):\n    \n    # Create a dataframe filtered by seconds_in_bucket and grouped by time_id. Calculate aggregate measures by group.\n    df = df_merged[df_merged['seconds_in_bucket'] >= window].groupby('time_id', as_index=False, sort=True).agg(data_dict)\n    \n    # Rename columns\n    df.columns = [f\"_{window}_\".join(col) for col in df.columns.values]\n    df = df.rename(columns={f\"time_id_{window}_\": \"time_id\"})\n    \n    return df","468890e7":"def develop_initial_features():\n    \n    # Define for loop terms\n    start = 0\n    end = len(stock_ids)\n    step = 112\n    \n    for p in range(start, end, step):  \n\n        # Create an empty dataframe for features\n        df_initial_features = cudf.DataFrame()\n\n        for stock_id in tqdm(stock_ids[p:p+step]):\n\n            print('\\n')\n            print(\"processing stock_id: \", stock_id)\n\n            # Load the order book parquet data as dataframe\n            df_order = load_parquet_data(\"Order Book\", stock_id)\n\n            # Load the trade book parquet data as dataframe\n            df_trade = load_parquet_data(\"Trade Book\", stock_id)\n\n            # Merge both dataframes and sort by 'seconds_in_bucket' in ascending order\n            df_merged = df_order.merge(df_trade, how=\"left\", on=[\"time_id\", \"seconds_in_bucket\"])\n\n            # Create statistics dataframe\n            df_statistics = df_merged[[\"time_id\", \"seconds_in_bucket\"]]\n\n            # Calculate WAP1\n            df_merged['wap1'] = (df_merged['bid_price1'] * df_merged['ask_size1'] + df_merged['ask_price1'] * df_merged['bid_size1']) \/ (df_merged['bid_size1'] + df_merged['ask_size1'])\n\n            # Calculate WAP2\n            df_merged['wap2'] = (df_merged['bid_price2'] * df_merged['ask_size2'] + df_merged['ask_price2'] * df_merged['bid_size2']) \/ (df_merged['bid_size2'] + df_merged['ask_size2'])\n            \n            # Calculate order volumes\n            df_merged['bid1_volume'] = df_merged['bid_price1'] * df_merged['bid_size1']\n            df_merged['bid2_volume'] = df_merged['bid_price2'] * df_merged['bid_size2']\n            df_merged['ask1_volume'] = df_merged['ask_price1'] * df_merged['ask_size1']\n            df_merged['ask2_volume'] = df_merged['ask_price2'] * df_merged['ask_size2']\n            \n            # Calculate spreads\n            df_merged['spread_ask1_bid1'] = np.absolute(df_merged['ask_price1']-df_merged['bid_price1']) \/ ((df_merged['ask_price1'] + df_merged['bid_price1']) \/ 2)\n            df_merged['spread_ask2_bid2'] = np.absolute(df_merged['ask_price2']-df_merged['bid_price2']) \/ ((df_merged['ask_price2'] + df_merged['bid_price2']) \/ 2)\n            df_merged['spread_bid1_bid2'] = np.absolute(df_merged['bid_price1']-df_merged['bid_price2']) \/ ((df_merged['bid_price1'] + df_merged['bid_price2']) \/ 2)\n            df_merged['spread_ask2_ask1'] = np.absolute(df_merged['ask_price2']-df_merged['ask_price1']) \/ ((df_merged['ask_price2'] + df_merged['ask_price1']) \/ 2)\n\n            # Calculate high-low measure \n            df_merged['max_price'] = df_merged[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']].max(axis=1)\n            df_merged['min_price'] = df_merged[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']].min(axis=1)\n            df_temp = df_merged.groupby('time_id', as_index=False, sort=True).agg({'max_price': 'max', 'min_price': 'min'})\n            df_temp['high_low_measure'] = ((np.log(df_temp['max_price']) - np.log(df_temp['min_price'])) ** 2) \/ (4 * np.log(2))                                                                       \n            df_merged.drop(['max_price', 'min_price'], axis=1, inplace=True)\n            df_merged = df_merged.merge(df_temp[['time_id', 'high_low_measure']], how=\"left\", on=[\"time_id\"])\n\n            # Calculate volume imbalance\n            df_merged['volume_imbalance'] = np.absolute((df_merged['ask_size1'] + df_merged['ask_size2']) - (df_merged['bid_size1'] + df_merged['bid_size2']))\n\n            # Calculate trade volume imbalance\n            df_merged['trade_volume_imbalance'] = np.absolute((df_merged['bid_price1'] * df_merged['bid_size1']) - (df_merged['ask_price1'] * df_merged['ask_size1']))\n\n            # Calculate stock liquidity\n            # Calculate weighted average bid price (wabidp)\n            wabidp = (df_merged['bid_price1'] * df_merged['bid_size1'] + df_merged['bid_price2'] * df_merged['bid_size2']) \/ (df_merged['bid_size1'] + df_merged['bid_size2'])\n            # Calculate weighted average ask price (waaskp)\n            waaskp = (df_merged['ask_price1'] * df_merged['ask_size1'] + df_merged['ask_price2'] * df_merged['ask_size2']) \/ (df_merged['ask_size1'] + df_merged['ask_size2'])\n            # Find non-zero wabidp-waaskp spread\n            spread = np.absolute(waaskp - wabidp) + EPSILON\n            # Find the mean of weighted prices\n            average_price = (wabidp + waaskp) \/ 2\n            # Calculate order_book_liquidity reduced by 1000\n            df_merged['stock_liquidity'] = (average_price \/ spread) \/ 1000\n\n            # Calculate wap liquidity\n            df_merged['wap_liquidity'] = (np.absolute(df_merged['wap1'] - df_merged['wap2'])) \/ ((df_merged['wap1'] + df_merged['wap2'])\/2)\n\n            # Trade_volume (Multiply price by size in the trade book)\n            df_merged['trade_volume'] = df_merged['price'] * df_merged['size']\n\n            # Calculate density code (1 when a transaction occurs at a given second else 0)\n            df_merged['density_code'] = df_merged['order_count'].applymap(calculate_density_code)\n\n            # Sort dataframe\n            df_merged = df_merged.sort_values([\"time_id\", \"seconds_in_bucket\"], ascending=True)\n\n            # Calculate normal return by second\n            # Create a rolling window\n            rolling = df_merged.groupby('time_id')['wap1'].rolling(window=2, min_periods=2, center=False)\n            # Discard the first row that has no precedent value\n            df_temp = rolling.apply(shift_wap)\n            # Convert the first row null value to zero\n            df_temp = df_temp.fillna(0)\n            # Convert the series to a dataframe\n            df_temp = df_temp.to_frame()\n            # Extract the shifted wap1 values as an array\n            array = df_temp.wap1.values\n            # Add the shifted wap1 values to the main df\n            df_merged['wap1_shifted'] = array.tolist()\n            # Shift the wap1_shifted row values up by one\n            df_merged['wap1_shifted'] = df_merged['wap1_shifted'].shift(-1)\n            # Calculate the normal return by second\n            df_merged[\"norm_return_by_second\"] = (df_merged['wap1_shifted'] \/ df_merged['wap1']) - 1\n            # Shift the norm_return_by_second row values down by one\n            df_merged['norm_return_by_second'] = df_merged['norm_return_by_second'].shift(1)\n            # Replace -1 with 0\n            df_merged['norm_return_by_second'].replace(-1, 0, inplace=True)\n\n            # Calculate normal return by time_id (Percentage change between WAPS at second 1 and second 599)\n            df_temp = calculate_normal_return_by_timeid(df_merged[['time_id', 'seconds_in_bucket', 'wap1']])\n            df_merged = df_merged.merge(df_temp, how=\"left\", on=[\"time_id\"])\n\n            df_merged['log_return'] = np.log(df_merged['norm_return_by_second'] + 1)\n            df_merged[['norm_return_by_second', 'log_return']].fillna(0, inplace=True)\n\n            # Find the square of log_returns towards calculating the realized volatility\n            df_merged['sqrd_log_return'] =  df_merged['log_return'] ** 2\n\n            # Calculate sum, mean, median, max, min, standard deviation, and kurtosis of data series\n            df_0 = create_stat_dfs(df_merged, 0)\n            df_120 = create_stat_dfs(df_merged, 120)\n            df_240 = create_stat_dfs(df_merged, 240)\n            df_360 = create_stat_dfs(df_merged, 360)\n            df_480 = create_stat_dfs(df_merged, 480)  \n\n            functions = [calculate_avg_log_return, # Calculate average log return\n                         calculate_density_score, # Number of seconds when at least one transaction occurred, divided by the total number of seconds in a given window\n                         calculate_realized_volatility, # Calculate the realized volatility\n                         calculate_density_scored_total_trade_vol, # Calculate density-scored total trade volume (total trade volume multiplied by density score)\n                         calculate_density_scored_total_orders, # Calculate density-scored total orders (total orders multiplied by density score)\n                         calculate_max_min_difference # Calculate max-min difference of data series\n                        ]\n\n            for function in functions:\n                df_0 = function(df_0, 0)\n                df_120 = function(df_120, 120)\n                df_240 = function(df_240, 240)\n                df_360 = function(df_360, 360)\n                df_480 = function(df_480, 480)\n\n#             df_0 = calculate_kurtosis(df_merged, df_0, 0)\n#             df_120 = calculate_kurtosis(df_merged, df_120, 120)\n#             df_240 = calculate_kurtosis(df_merged, df_240, 240)\n#             df_360 = calculate_kurtosis(df_merged, df_360, 360)\n#             df_480 = calculate_kurtosis(df_merged, df_480, 480)    \n                \n            # Calculate historical average realized volatility\n            df_0['historical_avg_real_volatility'] = df_0['realized_volatility_0'].mean()\n\n            del df_temp\n\n            # Sort dataframe\n            df_merged = df_merged.sort_values([\"time_id\", \"seconds_in_bucket\"], ascending=True)\n\n            # Plot\n            if stock_id == 0:\n                print('\\n')\n\n                # Plot wap\n                X = df_merged[df_merged['time_id']==5][\"seconds_in_bucket\"].to_array()\n                Y = df_merged[df_merged['time_id']==5][\"wap1\"].to_array()\n                fig = px.line(x=X, y=Y, title='WAP of stock_id_0, time_id_5')\n                fig.show()\n                print('\\n')\n\n                # Plot log return\n                df_plot = df_merged[~df_merged['log_return'].isnull()]\n                X = df_plot[df_plot['time_id']==5][\"seconds_in_bucket\"].to_array()\n                Y = df_plot[df_plot['time_id']==5][\"log_return\"].to_array()\n                fig = px.line(x=X, y=Y, title='Log return of stock_id_0, time_id_5')\n                fig.show()\n                del df_plot\n                print('\\n')\n\n            # Delete unneccessary columns\n            delete_columns = ['seconds_in_bucket', 'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1',\n                            'bid1_volume', 'bid2_volume', 'ask1_volume', 'ask2_volume', 'bid_size2', 'ask_size2', 'wap1', 'wap2', \n                            'spread_ask1_bid1', 'spread_ask2_bid2', 'spread_bid1_bid2', 'spread_ask2_ask1', 'price', 'size', \n                            'order_count', 'trade_volume', 'norm_return_by_second', 'sqrd_log_return', 'wap1_shifted', 'density_code', \n                            'high_low_measure', 'volume_imbalance', 'trade_volume_imbalance', 'stock_liquidity', 'wap_liquidity', \n                            'log_return'\n                             ]\n            \n            df_merged.drop(delete_columns, axis=1, inplace=True)\n\n            # Group the dataframe by time_id\n            df_merged = df_merged.groupby('time_id', as_index=False, sort=True).agg({'norm_return_by_timeid': 'mean'})\n            \n            # Add stock_id\n            df_merged.insert (1, \"stock_id\", stock_id)\n\n            # Merge dataframes into one\n            df_merged = df_merged.merge(df_0, how=\"left\", on=[\"time_id\"])\n            df_merged = df_merged.merge(df_120, how=\"left\", on=[\"time_id\"])\n            df_merged = df_merged.merge(df_240, how=\"left\", on=[\"time_id\"])\n            df_merged = df_merged.merge(df_360, how=\"left\", on=[\"time_id\"])\n            df_merged = df_merged.merge(df_480, how=\"left\", on=[\"time_id\"])\n\n            # Delete unneccessary dataframes\n            del df_0\n            del df_120\n            del df_240\n            del df_360\n            del df_480\n\n            # Append the dataframe to the features dataframe\n            df_initial_features = df_initial_features.append(df_merged)     \n\n            del df_merged\n            \n        delete_columns = ['sqrd_log_return_0_sum', 'sqrd_log_return_120_sum', 'sqrd_log_return_240_sum', 'sqrd_log_return_360_sum', 'sqrd_log_return_480_sum']\n        df_initial_features.drop(delete_columns, axis=1, inplace=True)   \n\n        # Dtype checkpoint Charlie\n        df_initial_features.astype({'stock_id': 'int16', 'time_id': 'int16'})\n        df_initial_features = make_dtype_list(df_initial_features, \"float64\", \"float32\")\n        df_initial_features = make_dtype_list(df_initial_features, \"int64\", \"int32\")\n\n        # Sort dataframe\n        df_initial_features = df_initial_features.sort_values([\"time_id\", \"stock_id\"], ascending=True)     \n\n        # Store the dataframe\n        df_initial_features.to_parquet(f\".\/initial_features_{p}_to_{p+step-1}.parquet\", compression=None, index=False)\n    \n    return df_initial_features","6e2cd3c6":"%%time\n\n\nif PRETRAINED == 1:\n    df_initial_features = develop_initial_features()\n\n    print(\"\\n\")\n    display(df_initial_features)   \n    print(\"\\n\")\n    display(df_initial_features.info())\n    print(\"\\n\")\n\nprint(\"Count:\", gc.get_count())\ncollected = gc.collect()\nprint(\"Garbage collector: collected %d objects.\" % (collected))","081e80ad":"def check_clustering(df_initial_features):\n\n    # Calculate mean target by stock_id\n    df_target_mean_by_stockid = df_initial_features.groupby('stock_id', as_index=False, sort=True).agg({\"target\": \"mean\"})\n    \n    # Plot stock_id vs. target\n    print(\"\\n\")\n    x = df_temp['stock_id']\n    y = df_temp['target']\n    plt.scatter(x, y, alpha=0.7)\n    plt.show()\n    print(\"\\n\")\n    \n    return df_target_mean_by_stockid","5bf859a1":"def check_normality(df_target_mean_by_stockid):\n    \n    # Check normality of the normal_target column\n    y = df_target_mean_by_stockid['normal_target']\n    \n    # Shapiro-Silk test\n    shapiro_test = shapiro(y)\n    \n    # Probability\n    p = shapiro_test[1]\n    \n    return p\n\n\ndef scale_data(df_initial_features, df_target_mean_by_stockid):\n    \n    # Transform the target column data\n    df_target_mean_by_stockid['normal_target'] = 1 \/ np.sqrt(df_target_mean_by_stockid['target'])\n    \n    # Check normality of the transformed column\n    p = check_normality(df_target_mean_by_stockid)\n    \n    # If probability is higher than the threshold, data is normal\n    if p > 0.05:\n        \n        # Normalize the data\n        scaler = StandardScaler()\n        \n        # Round-up to create a categorical data\n        df_target_mean_by_stockid['stock_group'] = math.ceil(scaler.fit_transform(df_target_mean_by_stockid['normal_target']))\n        \n        # Merge with the main dataframe\n        df_initial_features = df_initial_features.merge(df_target_mean_by_stockid[['stock_id', 'stock_group']], how=\"left\", on=[\"stock_id\"])\n        \n        # Delete the unnecessary dataframe\n        del df_target_mean_by_stockid\n        \n        # Return the new df\n        return df_initial_features\n    \n    # If probability is lower than the threshold, data is not normal\n    else:\n        \n        # Delete the unnecessary dataframe\n        del df_target_mean_by_stockid\n        \n        # Return the old df as is\n        return df_initial_features","2bd6722b":"def calculate_total_sister_volatility(df_initial_features):\n\n    # Select necessary columns of data to calculate the sister volatility\n    selected_columns = {'time_id': \"\", \n                     'realized_volatility_0': \"sum\", \n                     'realized_volatility_120': \"sum\", \n                     'realized_volatility_240': \"sum\", \n                     'realized_volatility_360': \"sum\", \n                     'realized_volatility_480': \"sum\", \n                     'norm_return_by_timeid': \"mean\"\n                   }\n\n    # Calculate aggregate operations\n    df_temp = df_initial_features[list(selected_columns.keys())].groupby('time_id', as_index=False, sort=True).agg({k: v for k, v in selected_columns.items() if v != \"\"})\n        \n    # Rename columns\n    df_temp.rename({col: col + \"_sum\" for col in df_temp.columns.values[1:-1]}, axis=1, inplace=True)\n    df_temp.rename({\"norm_return_by_timeid\": \"norm_market_return_by_timeid\"}, axis=1, inplace=True)\n        \n    # Merge with the main dataframe                                         \n    df_initial_features = df_initial_features.merge(df_temp, how=\"left\", on=[\"time_id\"])\n    \n    # Calculate total sister volatility\n    for window in WINDOWS:\n        df_initial_features[f\"total_sister_volatility_{window}\"] = df_initial_features[f\"realized_volatility_{window}_sum\"] - \\\n                                                                            df_initial_features[f\"realized_volatility_{window}\"]\n    return df_initial_features","011c4068":"def calculate_pseudo_beta(stock_returns, market_returns, market_return_variance):\n \n    # Calculate market return - stock return covariance \n    market_stock_covariance = stock_returns.cov(market_returns)\n    \n    # Calculate pseudo-beta\n    pseudo_beta = market_stock_covariance \/ market_return_variance\n    \n    return pseudo_beta","e6e2dce0":"def calculate_correlation_matrix(df):\n\n    # Create a pivot table for each window\n    df_pivot_0 = df[['stock_id', 'time_id', 'wap1_0_mean']].pivot(index='time_id', columns='stock_id')\n    df_pivot_120 = df[['stock_id', 'time_id', 'wap1_120_mean']].pivot(index='time_id', columns='stock_id')\n    df_pivot_240 = df[['stock_id', 'time_id', 'wap1_240_mean']].pivot(index='time_id', columns='stock_id')\n    df_pivot_360 = df[['stock_id', 'time_id', 'wap1_360_mean']].pivot(index='time_id', columns='stock_id')\n    df_pivot_480 = df[['stock_id', 'time_id', 'wap1_480_mean']].pivot(index='time_id', columns='stock_id')\n    \n    # Calculate correlations for each window\n    df_corr_0 = df_pivot_0.corr()\n    df_corr_120 = df_pivot_120.corr()\n    df_corr_240 = df_pivot_240.corr()\n    df_corr_360 = df_pivot_360.corr()\n    df_corr_480 = df_pivot_480.corr()\n    \n    # Create a correlation matrix for the first window for visualization\n    corr_matrix_0 = df_corr_0.as_gpu_matrix()\n    del df_pivot_0\n    del df_pivot_120\n    del df_pivot_240\n    del df_pivot_360\n    del df_pivot_480    \n\n    # Plot correlation matrix\n    print('\\n')\n    print(\"Correlation matrix\", \"\\n\")\n    display(df_corr_0.head())\n    print('\\n')\n    plt.figure(figsize = (12,10))\n    sns.heatmap(corr_matrix_0, cmap=\"YlGnBu\")\n    plt.title(\"Correlation matrix\", fontsize=20)\n    plt.xlabel('stock_id', fontsize=15)\n    plt.ylabel('stock_id', fontsize=15)\n    plt.show()\n    print('\\n')\n    return df_corr_0, df_corr_120, df_corr_240, df_corr_360, df_corr_480\n\n\ndef calculate_stock_correlations(df_corr, number_stocks):\n\n    # Calculate stock correlations\n    stock_corrs = {}\n    for i in range(number_stocks):\n        stock = df_corr.index[i][1]\n        values = df_corr.iloc[i:i+1,i:].values.tolist()[0]\n        corr = (sum(values)-1) \/ (number_stocks-1)\n        stock_corrs[stock] = corr\n    \n    return stock_corrs","4c2b1f34":"def calculate_skewness(df):\n\n    for feat in features_list:\n        for window in WINDOWS:\n            df[f\"{feat}_{window}_skew\"] = (3 * (df[f\"{feat}_{window}_mean\"] - df[f\"{feat}_{window}_median\"])) \/ df[f\"{feat}_{window}_std\"]\n\n    return df","02d827f3":"def calculate_schwert_model(df_initial_features):\n    \n    # Calculate volatility by the model developed by Schwert\n    for window in WINDOWS:\n        df_initial_features[f\"schwert_volatility_{window}\"] = np.sqrt((math.pi \/ 2)) * np.absolute(df_initial_features[f\"log_return_{window}_mean\"] - \\\n                                                                                                   df_initial_features[\"log_return_0_mean\"])\n    \n    return df_initial_features","820ba605":"def calculate_correlations(df):\n\n    # Create a schema dataframe\n    df_stocks = load_parquet_data(\"Order Book\", 0)\n    delete_columns = ['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\n    df_stocks.drop(delete_columns, axis=1, inplace=True)\n\n    # Calculate wap1 for stocks and fill the dataframe\n    for stock_id in tqdm(stock_ids):\n        df_stock = load_parquet_data(\"Order Book\", stock_id)\n        df_stock[f\"wap1_{stock_id}\"] = (df_stock['bid_price1'] * df_stock['ask_size1'] + df_stock['ask_price1'] * df_stock['bid_size1']) \/ \\\n                            (df_stock['bid_size1'] + df_stock['ask_size1'])\n        df_stocks = df_stocks.merge(df_stock[[\"time_id\", \"seconds_in_bucket\", f\"wap1_{stock_id}\"]], how=\"left\", on=[\"time_id\", \"seconds_in_bucket\"])  \n\n    # Save the dataframe\n    df_stocks.to_parquet(\".\/wap1s.parquet\", compression=None, index=False)    \n    \n    # Check null values\n    x = df_stocks[df_stocks.isna().any(axis=1)]\n    print(\"The number of null rows: \", len(x))\n    del x\n\n    # Fill null values with row means\n    df_stocks = df_stocks.fillna(df_stocks.iloc[:, 2:].mean(axis=1))\n\n    # Calculate columnwise sum of wap1s across seconds\n    df_sum = df_stocks[[\"time_id\", \"seconds_in_bucket\"]]\n    df_sum[\"wap1_sum\"] = df_stocks.iloc[:, 2:].sum(axis=1)\n    df_stocks = df_stocks.merge(df_sum[[\"time_id\", \"seconds_in_bucket\", \"wap1_sum\"]], how=\"left\", on=[\"time_id\", \"seconds_in_bucket\"])\n    del df_sum\n\n    # The number of total stocks        \n    num_of_stocks = len(stock_ids)\n    \n    # Create a dictionary to store correlations by time_id and stock_id\n    corrs = defaultdict(list)\n    \n    # Calculate mean wap1 across seconds except one stock each time\n    for stock_id in tqdm(stock_ids):\n        \n        df_stocks[f\"wap1_sec_mean_{stock_id}\"] = (df_stocks[\"wap1_sum\"] - df_stocks[f\"wap1_{stock_id}\"]) \/ (num_of_stocks - 1)\n\n        for time_id in time_ids:\n            \n            s1 = df_stocks[df_stocks[\"time_id\"]==time_id][f\"wap1_{stock_id}\"]\n            s2 = df_stocks[df_stocks[\"time_id\"]==time_id][f\"wap1_sec_mean_{stock_id}\"]\n            corr = s1.corr(s2)\n            del s1\n            del s2\n            corrs[\"time_id\"].append(time_id)\n            corrs[\"stock_id\"].append(stock_id)\n            corrs[\"correlation\"].append(corr)            \n\n    # Delete uncessary df\n    del df_stocks\n\n    # Create a new dataframe from the correlations dictionary\n    pdf = pd.DataFrame.from_dict(corrs)                                                                                            \n    df_temp = cudf.DataFrame.from_pandas(pdf)\n    del corrs\n    display(df_temp)\n\n    # Convert dtype\n    df_temp['correlation'] = df_temp['correlation'].astype(float32)\n        \n    # Save the dataframe\n    df_temp.to_parquet(\".\/correlations.parquet\", compression=None, index=False)\n\n    # Merge with the main dataframe\n    df = df.merge(df_temp, how=\"left\", on=[\"time_id\", \"stock_id\"])\n    del df_temp\n\n    # Reorder the columns\n    new_cols = [col for col in df.columns if col != 'target'] + ['target']\n    df = df[new_cols]     \n        \n    return df","0f2a2f33":"%%time\n\n\nif PRETRAINED == 2:\n    \n    df_initial_features = cudf.DataFrame()\n\n    # List pretrained datasets\n    pretrained_files = glob.glob(\"..\/input\/optiver-data\/initial_features*\")\n\n    # Load files into dataframes\n    for pretrained_file in pretrained_files:        \n        df_temp = cudf.read_parquet(pretrained_file)\n        df_initial_features = df_initial_features.append(df_temp)\n        \n    # Fill nan values with zero, which will have no impact on the model\n    df_initial_features = df_initial_features.fillna(0)\n    \n    print(\"\\n\")\n    display(df_initial_features)   \n    print(\"\\n\")\n    display(df_initial_features.info())\n    print(\"\\n\")","dbfe1d76":"def correct_dataset(df_initial_features):\n\n    # Find the stocks with missing time_ids\n    missing_timeids = {}\n    for stock_id in tqdm(stock_ids):\n        stock_time_ids = df_initial_features[df_initial_features['stock_id']==stock_id]['time_id'].unique().to_array()\n        missing_items = [item for item in time_ids if item not in stock_time_ids]\n        if len(missing_items) > 0:\n            missing_timeids[stock_id] = missing_items\n\n    print(missing_timeids, \"\\n\")\n\n    # Add missing time_ids to the related stocks with the mean values of predictors in that time_id\n    df_temp = cudf.DataFrame()\n    columns = list(df_initial_features)\n    for key, values in missing_timeids.items():\n        for value in values:\n            df_filtered = df_initial_features.drop(\"stock_id\", axis=1) \n            new_row = df_filtered[df_filtered['time_id']==value].iloc[:,1:].mean().to_array()\n            new_row = np.insert(new_row, 0, key)\n            new_row = np.insert(new_row, 0, value)\n            pdf = pd.DataFrame([new_row], columns=columns)\n            cdf = cudf.from_pandas(pdf)\n            df_temp = df_temp.append(cdf)\n\n    df_initial_features = df_initial_features.append(df_temp)\n\n    del df_filtered\n    del df_temp\n    \n    return df_initial_features","f287d1c3":"def add_new_features(df_initial_features):\n\n    # Calculate total sister volatility (1)\n    df_initial_features = calculate_total_sister_volatility(df_initial_features)\n    \n    # Calculate stock pseudo-beta (2)\n    pseudo_betas = {}\n    market_returns = df_initial_features['norm_market_return_by_timeid']\n\n    # Calculate market normal return variance\n    market_return_variance = market_returns.var()\n    for stock_id in stock_ids:\n\n        # Get norm_return_by_timeid by selected stock\n        stock_returns = df_initial_features[df_initial_features['stock_id'] == stock_id][\"norm_return_by_timeid\"]\n        pseudo_beta = calculate_pseudo_beta(stock_returns, market_returns, market_return_variance)\n        pseudo_betas[stock_id] = pseudo_beta\n\n    # Create and merge the temporary df\n    data = {'stock_id': list(pseudo_betas.keys()), 'pseudo_beta': list(pseudo_betas.values())}\n    pdf = pd.DataFrame.from_dict(data)                                                                                            \n    df_temp = cudf.DataFrame.from_pandas(pdf)\n    df_initial_features = df_initial_features.merge(df_temp, how=\"left\", on=[\"stock_id\"])\n\n    # Calculate correlations (3)\n    # Create a new dataframe and sort it\n    df_correlations = df_initial_features[['stock_id', 'time_id', 'wap1_0_mean', 'wap1_120_mean', 'wap1_240_mean', 'wap1_360_mean', 'wap1_480_mean']]\n    df_correlations = df_correlations.sort_values([\"stock_id\", \"time_id\"], ascending=True)\n\n    # Create a correlation matrix for each window\n    df_corr_0, df_corr_120, df_corr_240, df_corr_360, df_corr_480 = calculate_correlation_matrix(df_correlations)\n    number_stocks = len(df_corr_0)\n\n    # List correlation dataframes\n    df_corrs = [df_corr_0, df_corr_120, df_corr_240, df_corr_360, df_corr_480]\n\n    # Calculate average correlation for each stock for each window\n    for df_corr in zip(df_corrs, WINDOWS):\n        stock_corrs = calculate_stock_correlations(df_corr[0], number_stocks)\n\n        # Reformat the dataframe\n        df_correlations = cudf.DataFrame({'stock_id': list(stock_corrs.keys()), f\"correlation_{df_corr[1]}\": list(stock_corrs.values())})\n\n        # Merge it with the features dataframe\n        df_initial_features = df_initial_features.merge(df_correlations, how=\"left\", on=[\"stock_id\"])\n        \n    # Calculate skewness\n    df_initial_features = calculate_skewness(df_initial_features)\n    \n    # Calculate volatility by the model developed by Schwert    \n    df_initial_features = calculate_schwert_model(df_initial_features)\n     \n    # Delete unused dfs\n    del pdf\n    del df_temp\n    del df_correlations\n\n    return df_initial_features","abb82aa2":"def develop_enhanced_features(df_initial_features):\n\n    # Delete columns\n    delete_columns = ['norm_market_return_by_timeid', 'norm_return_by_timeid', 'realized_volatility_0_sum', 'realized_volatility_120_sum', \n                      'realized_volatility_240_sum', 'realized_volatility_360_sum', 'realized_volatility_480_sum'\n                     ]\n    \n    df_initial_features.drop(delete_columns, axis=1, inplace=True)\n    \n    # Merge train.csv\n    df_initial_features = df_initial_features.merge(df_csv, how=\"left\", on=[\"stock_id\", \"time_id\"])\n\n    # Check null values\n    df_temp = df_initial_features[df_initial_features.isna().any(axis=1)]\n    print(\"The number of null rows: \", len(df_temp))\n    \n    # Filter in only non null rows\n    df_initial_features = df_initial_features.dropna(axis=0, subset=['target'])\n    df_initial_features.fillna(0, inplace=True)\n    \n    # Check for clustering\n#     df_target_mean_by_stockid = check_clustering(df_initial_features)\n    \n    # Add stock group if target data can be normalized\n#     df_initial_features = scale_data(df_initial_features, df_target_mean_by_stockid)\n    \n    # Change dtype\n    df_initial_features = df_initial_features.astype({'stock_id': 'int16', 'time_id': 'int16'})\n    df_initial_features = make_dtype_list(df_initial_features, \"float64\", \"float32\")\n    df_initial_features = make_dtype_list(df_initial_features, \"int64\", \"int32\")\n    \n    # Sort the data and assign it to the new dataframe\n    df_enhanced_features = df_initial_features.sort_values([\"time_id\", \"stock_id\"], ascending=True)\n \n    # Save the dataframe\n    df_enhanced_features.to_parquet(\".\/enhanced_features.parquet\", compression=None, index=False)\n    \n    del df_initial_features\n    del df_temp\n    \n    return df_enhanced_features","5fcb8109":"%%time\n\n\nif PRETRAINED == 1 or PRETRAINED == 2:\n    if CORRECT_DATASET == \"yes\":\n        \n        # Correct dataset\n        df_initial_features = correct_dataset(df_initial_features)\n        \n    # Add new features\n    df_initial_features = add_new_features(df_initial_features)\n    df_enhanced_features = develop_enhanced_features(df_initial_features)\n    \nprint(\"\\n\")\ndisplay(df_enhanced_features)   \nprint(\"\\n\")\ndisplay(df_enhanced_features.info())\nprint(\"\\n\")\n\nprint(\"Count:\", gc.get_count())\ncollected = gc.collect()\nprint(\"Garbage collector: collected %d objects.\" % (collected))","92466b8e":"if PRETRAINED == 3:\n    \n    # Load dataframe\n    df_enhanced_features = cudf.read_parquet(\"..\/input\/optiver-data\/enhanced_features.parquet\")","499f336c":"def get_model_for_feature_selection(start, end):\n\n    # Parameters\n    params = {\n        'objective': 'reg:squarederror',\n        'n_estimators' : 10,\n        'tree_method': 'gpu_hist',\n        'seed' : SEED\n    }\n\n    # Instantiation\n    model = xgb.XGBRegressor(**params)\n\n    # Fitting the model\n    model.fit(X_train.iloc[:, start:end], y_train)\n    \n    return model","008d3602":"def calculate_permutation_importance(start, end):\n    \n    # Get model to select features\n    model = get_model_for_feature_selection(start, end)\n\n    # Calculate permutation importance\n    result = permutation_importance(model, X_train.iloc[:, start:end], y_train.to_array(), n_repeats=10, random_state=SEED)\n    tree_importance_sorted_idx = np.argsort(model.feature_importances_)\n\n    # Store features and their scores\n    feature_names = X_train.iloc[:, start:end].columns[tree_importance_sorted_idx].tolist()\n    importance_scores = model.feature_importances_[tree_importance_sorted_idx].tolist()\n    \n    return feature_names, importance_scores","05458563":"%%time\n\n\nif PRETRAINED in list(range(1, 4)):\n\n    # Determine predictors and target\n    X = df_enhanced_features.iloc[:, 1:-1]\n    y = df_enhanced_features['target']\n\n    # Splitting\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\n    # Dictionary to store features and scores\n    features_list = []\n    \n    # Number of rounds to run permutation importance\n    number_of_features = len(X.columns)\n    number_of_rounds = int(number_of_features \/ 200)\n    step_size = math.ceil(number_of_features \/ number_of_rounds) + 1\n    \n    # Number of top features to consider in the model\n    print('Total number of features to be considered: ', number_of_features)\n    print(\"\\n\")\n\n    # Find permutation importance of features\n    for rnd in tqdm(range(0, number_of_features, step_size)):\n        start = rnd\n        end = rnd + step_size\n        feature_names, importance_scores = calculate_permutation_importance(start, end)\n        features_list.extend(zip(feature_names, importance_scores))\n    \n    # Arrange features and scores\n    feats = [x[0] for x in features_list]\n    scors = [x[1] for x in features_list]\n    features_dict = {\"features\": feats, \"scores\": scors}\n    \n    # Create and sort a dataframe\n    df_features = pd.DataFrame.from_dict(features_dict)\n    df_features = df_features[df_features['scores'] > 0.0]\n    df_features = df_features.sort_values([\"scores\"], ascending=True)\n\n    # Plot importance features\n    df = pd.DataFrame({'scores': df_features['scores'].values.tolist()}, index=df_features['features'].values.tolist())\n    df.plot.barh(figsize = (20, 60), title=\"Feature Importance\")\n    plt.show()\n    print(\"\\n\")\n    \n    # Sort importance dataframe\n    df_features = df_features.sort_values([\"scores\"], ascending=False)\n    \n    # Save importance dataframe\n    df_features.to_parquet(\".\/perm_importance_scores.parquet\", compression=None, index=False)\n    \n    display(df_features)\n\n    del feats\n    del scors\n    del df","239a84ac":"def plot_dendogram():\n    \n    # Plot dendogram\n    fig = plt.figure(figsize=(20, 30))\n\n    # Cluster analysis to handle multicollinear features\n    corr = spearmanr(X.to_pandas()).correlation\n    corr_linkage = hierarchy.ward(corr)\n    dendro = hierarchy.dendrogram(corr_linkage, labels=X.columns.tolist(),  leaf_rotation=0, leaf_font_size=10, orientation='left')\n    dendro_idx = np.arange(0, len(dendro['ivl']))\n\n    # Threshold line\n    plt.vlines(x = t_line, ymin=0, ymax=10000, color = 'r', linestyle = '-')\n    fig.tight_layout()\n    plt.show()\n    print(\"\\n\")\n    \n    # Plot correlation matrix\n    fig = plt.figure(figsize=(20, 20))\n    plt.imshow(corr[dendro['leaves'], :][:, dendro['leaves']], cmap=\"viridis\")\n    plt.xticks(ticks=dendro_idx, labels=dendro['ivl'], rotation='vertical', fontsize=7)\n    plt.yticks(ticks=dendro_idx, labels=dendro['ivl'], fontsize=7)\n    fig.tight_layout()\n    plt.show()\n    print(\"\\n\")\n    \n    return corr_linkage","c81a8d9a":"# Find clusters of features\ndef get_feature_clusters():\n    cluster_ids = hierarchy.fcluster(corr_linkage, t_line, criterion='distance')\n    cluster_id_to_feature_ids = defaultdict(list)\n    for idx, cluster_id in enumerate(cluster_ids):\n        cluster_id_to_feature_ids[cluster_id].append(idx)\n    return cluster_ids, cluster_id_to_feature_ids\n\n\n# Find the most important feature in each cluster\ndef find_feature(values):\n    cluster_features = {}\n    for value in values:\n        feat_name = X.columns[value]\n        cluster_features[feat_name] = important_features_dict[feat_name]\n    best_feature = max(cluster_features, key=cluster_features.get)\n    return best_feature\n\n\n# Create a list of selected most important features\ndef collect_final_features():\n    cluster_ids, cluster_id_to_feature_ids = get_feature_clusters()\n    decoll_features = []\n    for key, values in cluster_id_to_feature_ids.items():\n        best_feature = find_feature(values)\n        decoll_features.append(best_feature)\n    print(\"Refined Features:\", \"\\n\", \"-\" * 50)\n    display(decoll_features)\n    print(\"\\n\")\n    print(\"The number of cluster ids: \", len(set(cluster_ids)), \"\\n\")\n    print(\"The number of refined features: \", len(decoll_features), \"\\n\")\n    return decoll_features","ca22a266":"def calculate_vif(column_list, df):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = column_list\n    vif[\"VIF\"] = [variance_inflation_factor(df.to_pandas().values, i) for i in range(len(column_list))]\n    display(vif)\n    print('\\n')\n    \n    return vif","5be38166":"def reduce_features(df_source, df_1):\n    \n    # Get necessary columns\n    if isinstance(df_1, list):\n        feature_columns = df_1\n    else:\n        feature_columns = df_1['features'].tolist()\n\n    # Add stock_id, time_id, and target columns\n    feature_columns.append(\"target\")\n    feature_columns.insert (0, \"time_id\")\n    if 'stock_id' in feature_columns:\n        feature_columns.remove('stock_id')\n    feature_columns.insert (1, \"stock_id\")\n    \n    # Create new dataframe\n    df_2 = df_source[feature_columns]\n    \n    return df_2\n\n\ndef create_final_df():\n    \n    # Find the number of records in the importance features dataframe\n    number_of_records = len(df_features)\n    \n    # Find the half of that number\n    halfway = math.ceil(number_of_records\/2)\n    \n    # Filter the first half of records\n    df_temp = df_features.iloc[0:halfway, :]\n\n    # Get reduced dataframe\n    final_features = reduce_features(df_reduced_features, df_temp)\n    \n    del df_temp\n    \n    return final_features","d77aaedb":"%%time\n\n\nif PRETRAINED == 4:\n    \n    # Load the main dataframe\n    df_enhanced_features = cudf.read_parquet(\"..\/input\/optiver-data\/enhanced_features.parquet\")\n    \n    # Load permutation importance features daaframe\n    df_features = cudf.read_parquet(\"..\/input\/optiver-data\/perm_importance_scores.parquet\")\n    \nif PRETRAINED in list(range(1, 5)):    \n\n    # Extract permutation importance features and reduce the main dataframe\n    df_reduced_features = reduce_features(df_enhanced_features, df_features)\n    \n    # Get features data\n    X = df_reduced_features.iloc[:, 2:-1]\n    \n    # Plot dendogram\n    t_line = 1\n    corr_linkage = plot_dendogram()\n    \n    # Convert importance features dataframe to dictionary\n    records = df_features.to_dict(orient=\"records\")\n    important_features_dict = {rec['features']: rec['scores'] for rec in records}\n    \n    # Get multicollinearity-handled features\n    decoll_features = collect_final_features()\n    \n    # Get multicollinearity-handled features dataframe\n    df_decoll_features = reduce_features(df_reduced_features, decoll_features)\n    \n    # Get further reduced features of df_reduced\n    df_final_features = create_final_df()\n\n    # Save the dataframes\n    df_reduced_features.to_parquet(\".\/reduced_features.parquet\", compression=None, index=False)\n    df_final_features.to_parquet(\".\/final_features.parquet\", compression=None, index=False)\n    df_decoll_features.to_parquet(\".\/decoll_features.parquet\", compression=None, index=False)\n\n    del df_reduced_features\n    del df_features\n    del records\n    del important_features_dict\n    del df_enhanced_features","fb3190e6":"def load_dataframes(d):\n    \n    if PRETRAINED == 5 or PRETRAINED == 6: \n        \n        # Dataframes in a dictionary\n        dataframes = {1: \"..\/input\/optiver-data\/final_features.parquet\", \n                      2: \"..\/input\/optiver-data\/decoll_features.parquet\", \n                      3: \"..\/input\/optiver-data\/reduced_features.parquet\", \n                      4: \"..\/input\/optiver-data\/enhanced_features.parquet\"}\n    \n        # Select the dataframe\n        dataframe = dataframes[d]\n        \n        display(dataframe)\n\n    return dataframe","3e8adbdb":"# Get hyperparameters\ndef define_hyperparameters(trial):\n    \n    hyper_params = {\n                    \"task\": 'train',\n                    \"objective\": \"regression\",\n                    \"metric\": \"rmse\",\n                    \"verbosity\": -1,\n                    \"n_jobs\": -1,\n                    \"seed\": SEED,\n                    \"data_random_seed\": SEED,\n                    \"bagging_seed\": SEED,\n                    \"drop_seed\": SEED,\n                    \"num_iterations\": 100,\n                    \"device_type\": \"gpu\",\n                    'feature_fraction': 0.8,\n                    'feature_fraction_bynode': 0.8,\n                    \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),\n                    \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 1, 10, step=1),\n                    \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 1, 10, step=1),\n                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, step=0.005),\n                    \"max_depth\": trial.suggest_int(\"max_depth\", 8, 12, step=2),\n                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 500, 1000, step=100),\n                    \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 500, step=100),\n                    \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.7, 1.0, step=0.1),\n                    \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 30, 70, step=10),\n                }\n    \n    return hyper_params","c3f98e9f":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))","0f601341":"def objective(trial):\n    \n    # Get the target dataframe\n    data = load_dataframes(d)\n\n    # Predictors and target\n    X = data.iloc[:, 2:-1]\n    y = data['target']\n    \n    # Get hyperparameters\n    hyper_params = define_hyperparameters(trial)\n        \n    # Model    \n    gbm = lgb.LGBMRegressor(**hyper_params)\n    \n    # Loss scores\n    scores = []\n    \n    # Cross validation\n    k = KFold(n_splits = 5, random_state = SEED, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X)):\n        \n        # Split train, test\n        train_x, valid_x = X.iloc[trn_idx, :], X.iloc[val_idx, :]\n        train_y, valid_y = y.iloc[trn_idx], y.iloc[val_idx]\n        \n        # Train the model\n        gbm.fit(train_x.as_gpu_matrix(), train_y.to_array(), eval_set = [(valid_x.as_gpu_matrix(), valid_y.to_array())], early_stopping_rounds = 20, verbose = False)\n        \n        # Predict for training data\n        tr_preds = gbm.predict(train_x.as_gpu_matrix())\n        tr_score = rmspe(train_y.to_array(), tr_preds)\n        \n        # Predict for validation data\n        val_preds = gbm.predict(valid_x.as_gpu_matrix())\n        val_score = rmspe(valid_y.to_array(), val_preds)\n\n        # Collect scores\n        scores.append((tr_score, val_score))\n\n    print(\"\\n\")\n    print(f\"RMSPE scores: {scores}\")\n    print(\"\\n\\n\")\n    \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    # Return mean validation score\n    mean_score = scores['validation score'].mean()\n    print(\"Mean validation score: \", mean_score)\n    \n    return mean_score","5d4b318a":"%%time\n\n\n# if PRETRAINED in list(range(1, 6)):\nif PRETRAINED == 7: # skip this phase\n    \n    # Target dataframe\n    d = 1\n\n    # Run iterations\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=25, n_jobs=-1)\n\n    # Best parameters\n    optimized_hparams =  study.best_trial.params\n\n    # Display results\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print('Best trial:', optimized_hparams)\n    print('Best value:', study.best_value)\n    print(\"\\n\\n\\n\")\n\n    # Save best parameters\n    with open('.\/optimized_hparams.json', 'w') as fp:\n        json.dump(optimized_hparams, fp)\n        \n    # Optuna visualization of optimization history\n    fig = optuna.visualization.plot_optimization_history(study)\n    fig.show()\n    print('\\n')\n    \n    # Optuna visualization of slice plot\n    fig = optuna.visualization.plot_slice(study)\n    fig.show()\n    print('\\n')\n    \n    # Optuna visualization of parallel coordinate plot\n    fig = optuna.visualization.plot_parallel_coordinate(study)\n    fig.show()\n    print('\\n')","bce89687":"def declare_hyperparams():\n    \n    if PRETRAINED == 6:\n    \n        # Load json file\n        with open('..\/input\/optiver-data\/final_hparams_0.24102.json') as f:\n            optuna_optimized_params = json.load(f)\n        print(\"Hyperparameters optimized by Optuna: \", optuna_optimized_params, \"\\n\")\n    \n    used_hyperparams = {'task': 'train', \n                        'objective': 'regression',\n                        'metric': 'rmse',\n                        'verbosity': -1,\n                        'n_jobs': -1,\n                        'seed': SEED,\n                        'bagging_seed': SEED,\n                        'data_random_seed': SEED,\n                        'drop_seed': SEED,\n                        'num_iterations': 200,\n                        'device_type': 'gpu',\n                        'boosting': 'dart',\n                        'lambda_l1': 0,\n                        'lambda_l2': 7, \n                        'max_depth': 10,\n                        'num_leaves': 800,\n                        'learning_rate': 0.05,\n                        'min_data_in_leaf': 200,\n                        'bagging_fraction': 0.90,\n                        'bagging_freq': 30,\n                        'feature_fraction': 0.8,\n                        'feature_fraction_bynode': 0.8,\n                      } \n\n    # Save used hyperparameters\n    with open('.\/used_hyperparams.json', 'w') as fp:\n        json.dump(used_hyperparams, fp)\n    print(\"Selected hyperparameters for training:\", used_hyperparams, \"\\n\")\n    \n    return used_hyperparams","280fdd62":"def add_corrs(df):\n\n    # Load correlations daaframe\n    df_corrs = cudf.read_parquet(\"..\/input\/optiver-data\/correlations.parquet\")\n    \n    # Merge with the main dataframe\n    df = df.merge(df_corrs, how=\"left\", on=[\"time_id\", \"stock_id\"])\n    \n    # Reorder the columns\n    new_cols = [col for col in df.columns if col != 'target'] + ['target']\n    df = df[new_cols]\n    \n    del df_corrs\n    \n    return df","829964c5":"def split_dataset(data):\n\n    # Define features and target\n    X = data.iloc[:, 2:-1]\n    y = data['target']\n    \n    # Split the data\n    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.30, random_state = SEED)\n    \n    return X, y, train_x, valid_x, train_y, valid_y","c19288cb":"def train_and_evaluate_model():\n    \n    global valid_x, valid_y\n    \n    evals_result = {}\n    \n    # Get best hyperparameters\n    used_hyperparams = declare_hyperparams()\n    \n    # Get the target dataframe\n#     data = load_dataframes(d)\n    data = add_corrs(df_final_features)\n    \n    # Split the dataframe\n    X, y, train_x, valid_x, train_y, valid_y = split_dataset(data)\n    \n    # Find feature names\n    feature_names = X.columns.tolist()\n    \n    # Create train and validation datasets for the ML method selected\n    dtrain = lgb.Dataset(train_x.as_gpu_matrix(), label=train_y.to_array(), feature_name=feature_names)\n    dvalid = lgb.Dataset(valid_x.as_gpu_matrix(), valid_y.to_array(), reference=dtrain, feature_name=feature_names)\n    \n    # Train the model\n    booster = lgb.train(used_hyperparams, \n                        train_set=dtrain,\n                        valid_sets=dvalid,\n                        num_boost_round=300,\n                        early_stopping_rounds=30,\n                        evals_result=evals_result,\n                        verbose_eval=100)\n    \n    return booster, evals_result, feature_names","b5775fc8":"def save_and_plot_model():\n    \n    # Train and evaluate the model\n    booster, evals_result, feature_names = train_and_evaluate_model()\n    \n    # Save the best model\n    booster.save_model(f\".\/lgb_{d}.model\", num_iteration=booster.best_iteration)\n    \n    # Plot functionality\n    # Plot metric\n    lgb.plot_metric(evals_result, metric=\"rmse\", figsize=(12, 8))\n    plt.title(\"Loss metric\")\n    plt.show()\n    print('\\n')\n    \n    # Plot importance\n    height = math.ceil(len(feature_names) \/ 4)\n    lgb.plot_importance(booster, figsize=(12, height))\n    plt.title(\"Feature importance\")\n    plt.show()\n    print('\\n')\n    \n    # Minimum RMSE\n    min_rmse = min(evals_result['valid_0']['rmse'])\n    \n    print(\"Minimum RMSE for the dataframe {d}:\", min_rmse, \"\\n\")\n    \n    return booster, min_rmse","51b34a69":"%%time\n\n\nif PRETRAINED in list(range(1, 7)):\n    \n    # Target dataframe\n    d = 1\n    \n    # LGBMs\n    boosters = {}\n\n    print(f\"Summary and results for the dataframe {d}\")\n    print(\"-\" * 40)\n\n    boosters[d] = save_and_plot_model()","8b34ceeb":"%%time\n\n\nif PRETRAINED in list(range(1, 7)):\n\n    # Model\n    lgbm_model = boosters[d][0]\n\n    # Predict\n    ypred = lgbm_model.predict(valid_x.as_gpu_matrix(), num_iteration=lgbm_model.best_iteration)\n\n    # Loss\n    rmspe_loss = rmspe(valid_y.to_array(), ypred)\n\n    print(f\"Loss of the model lgbm for the dataframe {d}: {rmspe_loss}\")","bc0a8fc5":"### **Feature extraction by time_id**","85cf3fe8":"**Find multicollinearity-handled features**","cef79a22":"**Load and prepare the dataframe**","530adb76":"### **Feature engineering on stock_id**","d835a75d":"#### Find the best hyperparameters","d5727d06":"#### _density score_  \n\nNumber of seconds when at least one transaction occurred, divided by the total number of seconds in a given window","b5084044":"## **Feature Engineering**","1490862e":"#### _density-scored total trade volume_  \n\nTotal trade volume multiplied by density score","cb3c71f7":"#### Develop enhanced features","3bf25476":"#### _schwert model_","53fa4862":"#### Correct the pretrained dataset","b1be3895":"#### Load best hyperparameters","fcae29b5":"### **Feature extraction by stock_id**","82a248b6":"#### Handle multicollinear features","33f2736a":"#### _stock_id_","b5fcf249":"**Plot dendogram**","98842900":"#### _normal return by second_\n\nPercentage change of wap from previous to the current second within a particular time_id. This is not log return.","693260c9":"#### _total sister stock volatility by time_id_\n<br>\n\n![sister_stock_volatility.png](attachment:a58dd875-9375-424c-b653-c33585ce62ba.png)\n\nwhere  \n`s` represents a specific stock  \n`t` is a particular time_id  \n`sigma` is the realized volatility of stock `s` within the time_id `t`  \n`n` is the number of stocks  \n`n-1` is the number of stocks except `s`  ","6414db19":"#### _kurtosis_","a9a75c99":"#### _skewness_","ab7a534f":"**Find further reduced features in terms of permutation importance**","cf2b8407":"#### _stock correlations_","1415f609":"# **Optiver Realized Volatility Prediction**","4bdb2215":"#### _average stock log return_\n<br>\n\n![avg_sqrd_log_return.png](attachment:4e9e727a-e110-4839-a806-3fe757b97340.png)\n\nwhere    \n    `s` represents a specific stock  \n    `t` is a particular time_id  \n    `r` is the log return by second of stock `s` within the time_id `t`  \n    `n` is the number of seconds  ","44c37e7f":"#### Create a Booster instance","c763bbfe":"#### _high-low measure_  \n<br>\nCalculate `high_low_measure` from the order book for each `stock_id` and every `time_id`  \n\n![high_low_measure.png](attachment:09cfa2d2-3d2a-414f-8b7e-0c2ce05ff720.png)\n\nwhere  \n`H` is the highest price  \n`L` is the lowest price  ","d196cb6f":"#### Add correlations by seconds","4f4ff26d":"#### _realized volatility_  ","85a54d9b":"#### Check for clustering","621b70b5":"[https:\/\/www.kaggle.com\/tatudoug\/stock-embedding-ffnn-features-of-the-best-lgbm\/notebook](http:\/\/)  \n[https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/251277  ](http:\/\/)  ","d3f26b21":"#### Define hyperparameters for optimization","e82b1f0f":"#### Load and reformat the parquet data","1d2f9e1d":"#### Load enhanced features","f4f6b4f9":"#### _statistical measures of features_  ","6412943a":"#### Create dataframes with statistical measures of features","dc7e1247":"#### Tune hyperparameters with CV","4db214e2":"## **Acknowledgement**","dc2247b7":"#### Add new features","06137b59":"#### Define parameters","b9e4411b":"#### Save and plot the model","79d68882":"## **Define the infrastructure**","8f6cbd80":"#### Import libraries and set options","6c10703e":"#### Load and reformat the csv data","e0e83ae8":"#### _max-min difference_  \n\nDifference between maximum and minimum values","eebb64a2":"#### _density-scored total orders_  \n\nTotal orders multiplied by density score","e51c3ffc":"#### _density code_","323f8458":"### **Feature engineering on time_id**","427036a7":"#### Dtype changes","8435d4c9":"#### Finalize initial features","fbf52a2c":"### **Training the best model**","5c091ca7":"#### _correlations by second_","3d951fd9":"**Perform final VIF check**","86e62c57":"#### Define the loss function","0a2703b5":"#### _stock pseudo-beta_  \n\n![stock_beta.png](attachment:2e467f0e-3078-4e33-ad17-cd071cd0ee74.png)  \n \n[https:\/\/www.investopedia.com\/terms\/b\/beta.asp](http:\/\/)\n\n_Note: Since we don't know if the stocks belong to the same stock exchange, and market return on that exchange, this is not a real beta._","d6ef1661":"#### Find feature importance","3e5b862f":"#### Get the parquet data info","9ecec2a1":"#### Load initial features","a3c42364":"## **Load and reformat data**","38e6b08f":"#### Build model for feature selection","79e21d07":"## **Feature Selection**","7ad4e820":"#### Predict","74853afa":"## **Model training**","203e0fa7":"#### Check for normality and add _stock_group_","c9b9cf57":"#### _normal return by time_id_  \n\nPercentage change between wap at second=1 and wap at second=599 across time_ids. This is not log return.","0b6f6bca":"#### Load training dataframes","faf54717":"#### Develop initial features","3060571c":"#### Create the datasets"}}