{"cell_type":{"f3fe2ebb":"code","eafe0bf6":"code","f2140daf":"code","b4161790":"code","01b86ad7":"code","24270c5e":"code","52aae82f":"code","b2bb9a29":"code","82c40e21":"code","6dae50c0":"code","effc2516":"code","69a43119":"code","3f1d931b":"code","acf52b3c":"code","0d789833":"code","ef17f0f5":"code","482a5d27":"code","ef466eef":"code","1e5c71aa":"code","8f2c8d85":"code","c7b89022":"code","864cf61a":"code","ef96ed1d":"code","04a260ff":"code","6c5ef504":"code","b0193a24":"code","442be19c":"code","78799e09":"code","a40ed04e":"code","e0f893ba":"code","5f4eb8a4":"code","f6ad8a49":"code","0863bb87":"code","56e3518f":"code","8089c5e3":"code","cdf18f01":"code","9cbeaf89":"code","5110438a":"code","e4178ded":"code","86b0ddc0":"code","11e7397b":"markdown","c153371d":"markdown","8d021889":"markdown","e19d272d":"markdown","9efcb55d":"markdown","9ba1711f":"markdown","75c9959e":"markdown","691cbdb3":"markdown","e172028e":"markdown","f388fbd7":"markdown","af2582cf":"markdown","bd8961f1":"markdown","af087672":"markdown","afc586a3":"markdown","b6b12098":"markdown","db2a6cb9":"markdown","8bbde7b7":"markdown","c34944ca":"markdown","27ca15a3":"markdown","d35726ac":"markdown","ece10652":"markdown","57d64bef":"markdown","6a9fb24a":"markdown","2a4ae1a7":"markdown","2b931615":"markdown"},"source":{"f3fe2ebb":"# ********** import packages ****************\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n#import os,sys\nfrom sklearn.preprocessing import MinMaxScaler #to scale features\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix,precision_score,f1_score,classification_report\nimport seaborn as sns\nsns.set(color_codes=True) # adds a background to the graphs\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nprint('all packages are imported')","eafe0bf6":"#read the data into a DataFrame \ndata=pd.read_csv('..\/input\/parkinson-disease\/Parkinson disease.csv')\n#data.head() # get the first 5 records\ndisplay(data)  \n#data.head(10).style.background_gradient(cmap=\"RdYlBu\")\n ","f2140daf":"#fetch all columns\ndata.columns","b4161790":"data.shape","01b86ad7":"data.dtypes","24270c5e":"#check for duplicates in the dataset\ndata[data.duplicated()]","52aae82f":"print('informations about the dataset')\ndata.info()\n# ensure that there are no Null Values\nprint(\"Null Values Check\\n\")\nprint(data.isnull().sum())\nprint(\"\\n\\n NAN Values Check \\n\")\nprint(data.isna().sum())","b2bb9a29":"#distribution of status variable.\nsns.countplot(data['status'])\n\n# Add labels\nplt.title('Countplot of status')\nplt.xlabel('status')\nplt.ylabel('Patients')\nplt.show()","82c40e21":"#overview of data\ndata.describe().transpose()","6dae50c0":"#the distribution of the features individually\nprint('The measures of vocal fundamental frequency')\nfig, ax = plt.subplots(1,3,figsize=(15,6)) \nsns.distplot(data['MDVP:Flo(Hz)'],ax=ax[0]) \nsns.distplot(data['MDVP:Fo(Hz)'],ax=ax[1]) \nsns.distplot(data['MDVP:Fhi(Hz)'],ax=ax[2])","effc2516":"def distributionPlot(data):\n    fig, ax = plt.subplots(2,3,figsize=(16,8)) \n    sns.distplot(data['MDVP:Shimmer'],ax=ax[0,0],color='red') \n    sns.distplot(data['MDVP:Shimmer(dB)'],ax=ax[0,1],color='gray') \n    sns.distplot(data['Shimmer:APQ3'],ax=ax[0,2],color='pink') \n    sns.distplot(data['Shimmer:APQ5'],ax=ax[1,0],color='blue') \n    sns.distplot(data['MDVP:APQ'],ax=ax[1,1],color='green') \n    sns.distplot(data['Shimmer:DDA'],ax=ax[1,2],color='Aquamarine')\n    plt.show()\ndistributionPlot(data)","69a43119":"fig, ax = plt.subplots(1,2,figsize=(10,7)) \nsns.distplot(data['NHR'],ax=ax[0],color='cyan') \nsns.distplot(data['HNR'],ax=ax[1],color='pink')","3f1d931b":"fig, ax = plt.subplots(1,3,figsize=(10,7)) \nsns.boxplot(y='spread1',data=data, ax=ax[0],orient='v') \nsns.boxplot(y='spread2',data=data, ax=ax[1],orient='v')\nsns.boxplot(y='PPE',data=data,ax=ax[2],orient='v')","acf52b3c":"#Bivariate Analysis to study Dependencies\n#outliers\nfig,axes=plt.subplots(5,5,figsize=(15,15))\naxes=axes.flatten()\n\nfor i in range(1,len(data.columns)-1):\n    sns.boxplot(x='status',y=data.iloc[:,i],data=data,orient='v',ax=axes[i],palette = 'copper')\nplt.tight_layout()\nplt.show()","0d789833":"#Get the features and labels from the DataFrame ( 23 features and 1 label (statut))\nfeatures= data.drop(['status','name'], axis = 1)\n#features=data.loc[:,data.columns!='status','name'].values[:,1:] #Selecting data by label or by a conditional statement [<row selection>,<column selection>]\nlabels=data.loc[:,'status'].values\n#features=data.drop(['status','name'],axis=1)\n#labels=data['status']","ef17f0f5":"#Get the count of each label (0 and 1) in labels\nprint('number of parkinson people in the dataset:',labels[labels==1].shape[0])\nprint('number of Healthy people in the dataset: ',labels[labels==0].shape[0]) #The shape attribute for numpy arrays returns the dimensions of the array\n#1 : 147 , 0 : 48","482a5d27":"#Scale the features to between -1 and 1 (normalisation)\nscaler=MinMaxScaler((-1,1))\nx=scaler.fit_transform(features)\ny=labels","ef466eef":"#split the dataset into training and testing sets keeping 20% of the data for testing.\nx_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.2, random_state=2)\nprint(\"{0:.1f}% data is in training set\".format((len(x_train)\/len(data)) * 100))\nprint(\"{0:.1f}% data is in testing set\".format((len(x_test)\/len(data)) * 100))\n#overview of data\nprint(x_train.shape)\nprint(x_test.shape)","1e5c71aa":"#class_weights = \u2018balanced\u2019, the model automatically assigns the class weights inversely proportional to their respective frequencies.\n#for a binary classifier\nclass_weight = {0: sum(y_train == 1)\/len(y_train), 1: sum(y_train == 0)\/len(y_train)}\nprint(class_weight)","8f2c8d85":"# define the model\nmodel_xgboost=XGBClassifier()\n# fit the model\nmodel_xgboost.fit(x_train,y_train)","c7b89022":"# **********make prediction ********** \n\npred_xgboost=model_xgboost.predict(x_test)\n\n#****** evaluate model ******\n\n#accuracy metrix\nacc=accuracy_score(y_test,pred_xgboost)*100\nprint('the accuracy of the model',acc)\n\n# confusion matrix\nc_matrix = confusion_matrix(y_test,pred_xgboost)\nprint('Confusion matrix : \\n',c_matrix)\n\n# outcome values order in sklearn\ntn, fp, fn, tp = confusion_matrix(y_test,pred_xgboost).ravel()\nprint('Outcome values : \\n', tn, fp, fn, tp)\n\n# classification report for precision, recall f1-score and accuracy\ncl_rep = classification_report(y_test,pred_xgboost)\nprint('Classification report : \\n',cl_rep)","864cf61a":"sns.heatmap(c_matrix,annot=True,cmap='Blues')\nplt.title('The confusion matrix')\nplt.xlabel('predicted values')\nplt.ylabel('actual values')\nplt.show()","ef96ed1d":"from sklearn.neighbors import KNeighborsClassifier\n#determine the optimum value of k \ntrain_score = []\ntest_score = []\nk_vals = []\n\nfor k in range(1, 21):\n    k_vals.append(k)\n    mod_knn = KNeighborsClassifier(n_neighbors = k)\n    mod_knn.fit(x_train, y_train)\n    \n    tr_score = mod_knn.score(x_train, y_train)\n    train_score.append(tr_score)\n    \n    te_score = mod_knn.score(x_test, y_test)\n    test_score.append(te_score)\n\n# score that comes from the testing set only\n \nmax_test_score = max(test_score)\ntest_scores_ind = [i for i, v in enumerate(test_score) if v == max_test_score]\nprint('Max test score {} and k = {}'.format(max_test_score * 100, list(map(lambda n: n + 1, test_scores_ind))))\n ","04a260ff":"model_knn = KNeighborsClassifier(11)\n\nmodel_knn.fit(x_train, y_train)\nmodel_knn.score(x_test, y_test)*100","6c5ef504":"pred_knn=model_knn.predict(x_test)\n \n#accuracy metrix\ntest_acc=accuracy_score(y_test,pred_knn)*100\nprint('the accuracy of the model',test_acc)\n\nc_matrix = confusion_matrix(y_test,pred_knn)\nprint('Confusion matrix : \\n',c_matrix)\n\n# outcome values order in sklearn\ntn, fp, fn, tp = confusion_matrix(y_test,pred_knn).reshape(-1)\nprint('Outcome values : \\n', tn, fp, fn, tp)\n\n# classification report for precision, recall f1-score and accuracy\ncl_rep = classification_report(y_test,pred_knn)\nprint('Classification report : \\n',cl_rep)","b0193a24":"sns.heatmap(c_matrix, annot=True,cmap='Blues')\nplt.title('The confusion matrix')\nplt.xlabel('predicted values')\nplt.ylabel('actual values')\nplt.show()","442be19c":"from sklearn.svm import SVC\n\nmodel_svm = SVC(class_weight=class_weight)\n#fit the model\nmodel_svm.fit(x_train, y_train)","78799e09":"pred_svm = model_svm.predict(x_test)\n#accuracy metrix\ntest_acc=accuracy_score(y_test,pred_svm)*100\nprint('the accuracy of the model',test_acc)\n\nc_matrix = confusion_matrix(y_test,pred_svm)\nprint('Confusion matrix : \\n',c_matrix)\n\n# outcome values order in sklearn\ntn, fp, fn, tp = confusion_matrix(y_test,pred_svm).reshape(-1)\nprint('Outcome values : \\n', tn, fp, fn, tp)\n\n \n\n# classification report for precision, recall f1-score and accuracy\ncl_rep = classification_report(y_test,pred_svm)\nprint('Classification report : \\n',cl_rep)","a40ed04e":"#Draw Confusion Matrix \nsns.heatmap(c_matrix, annot=True,cmap='Blues')\nplt.title('The confusion matrix')\nplt.xlabel('predicted values')\nplt.ylabel('actual values')\nplt.show()","e0f893ba":"input_data=(244.99000,272.21000,239.17000,0.00451,0.00002,0.00279,0.00237,0.00837,0.01897,0.18100,0.01084,0.01121,0.01255,0.03253,0.01049,21.52800,0.522812,0.646818,-7.304500,0.171088,2.095237,0.096220)\n\n#convert input data to  numpy array\ninput_data_array =np.asarray(input_data)\n\n#reshape\ninput_data_reshape = input_data_array.reshape(1, -1)\n\n#standardize data\nsca_data =scaler.transform(input_data_reshape)","5f4eb8a4":"#prediction : KNN model\nprediction = mod_knn.predict(sca_data)\nprint(prediction)\n\ndef pred(x):\n    \n    if (prediction == 1):\n        \n           Diagnosis = 'The patient has Parkinson'\n\n    elif (prediction == 0):\n        \n           Diagnosis = 'The patient does not have Parkinson'\n    else:\n          print('error in processing')\n    \n    return Diagnosis \n        \npred(prediction)","f6ad8a49":"#prediction : SVM model\nprediction = model_svm.predict(sca_data)\nprint(prediction)\n \npred(prediction)","0863bb87":"#prediction : xgboost model\nprediction = model_xgboost.predict(sca_data)\nprint(prediction)\n\npred(prediction)","56e3518f":"print(x_train.shape)","8089c5e3":"import tensorflow  \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout \n\nprint('all packages are imported')","cdf18f01":"#build the model\n\nmodel_DNN=tensorflow.keras.models.Sequential([\n    Dense(64,input_shape=(x_train.shape[1],),activation='relu'), \n     \n    Dense(32,activation='relu'),  \n    Dropout(0.5), # regularization method\n     \n    \n    Dense(16,activation='relu'), \n    Dropout(0.5),\n     \n    \n    Dense(8,activation='relu'), \n    Dropout(0.5),\n    \n    #Output layer\n    Dense(1,activation='sigmoid')\n])\n#print(model_DNN.weights)\nmodel_DNN.summary()","9cbeaf89":"# Create an optimizer.\n#opt = tensorflow.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)  \nfrom tensorflow.keras import optimizers\n\n#set class weight to balance the loss in model.fit\nmodel_DNN.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy', tensorflow.keras.metrics.Recall()])   \ntr = model_DNN.fit(x_train, y_train, epochs=150, batch_size=64,class_weight=class_weight)","5110438a":"pred_DNN = model_DNN.predict(x_test)\n \n# evaluate the model\ntest_acc=accuracy_score(y_test,pred_DNN.round())*100\nprint('the accuracy of the model',test_acc)\n\nc_matrix = confusion_matrix(y_test,pred_DNN.round())\nprint('Confusion matrix : \\n',c_matrix)\n\n# outcome values order in sklearn\ntp, fn, fp, tn = confusion_matrix(y_test,pred_DNN.round()).reshape(-1)\nprint('Outcome values : \\n', tp, fn, fp, tn)\n\n# classification report for precision, recall f1-score and accuracy\ncl_rep = classification_report(y_test,pred_DNN.round())\nprint('Classification report : \\n',cl_rep)\n\n ","e4178ded":"# visualize with seaborn library\nsns.heatmap(c_matrix,annot=True,fmt=\"d\" ,cmap='Blues') \nplt.xlabel('predicted value')\nplt.ylabel('actual value')\nplt.show()","86b0ddc0":"model_DNN.save_weights(\"model_DNN.h5\")\nprint(\"Saved model to disk\")","11e7397b":"**Inferences :**\n\n* Total number of records in the dataset 195\n* Total number of features in the dataset 24\n* There are no null values in the dataset\n* There are no duplicates in the dataset\n* Distribution of Parkinson disease people and healthy people are not uniformly distributed","c153371d":"# **Load and prepare data**","8d021889":"The boxplot help us in identifying the difference in values with respect to the 'status' of the patient.","e19d272d":"# Build the Model","9efcb55d":"we have obtained the optimum value of k to be 11 with a score of 94.87. Now,we will fit the model","9ba1711f":"The value NHR is positively skewed.\n\nThe value HNR looks like normally distributed","75c9959e":"we have an unbalanced data,we set class weights to balance the loss in our model_DNN.fit","691cbdb3":"> There are no missing values. Now we can take a look at how many patients are actually suffering from parkinson disease (1) and how many are not (0)","e172028e":"**XGBoost algorithm (extreme gradient boosting )**\n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.","f388fbd7":"**BiVariate Analysis**","af2582cf":"The three variations have outliers.","bd8961f1":"**Univariate Analysis**","af087672":"People who have PD (status = 1) have higher levels of Noise to Harmonic ratio (NHR) but they have lower levels of HNR (Harmonics to noise ratio) than the healthy people.\n\nPeople who have PD tend to have higher jitter % (if the values goes above 0.15 we can confirm the patient is having PD )  \n\nSpread1 is normally distributed between person who have PD and who is normal. People who have spread1 between - 8.5 and -7.5 are more and they are normal. People whose spread1 is between -6.5 and -5 are having PD","afc586a3":"# feature engineering  ","b6b12098":"# Import necessary packages","db2a6cb9":"There is a positive skewness for minimum vocal fundemental frequency.\n\nThe high vocal frequency does not have any skewness.","8bbde7b7":"**SVM algorithm**","c34944ca":"# Define the problem\n\n**It is a classification problem : classify the patients into (Patient has Parkinson \/ Patient does not have Parkinson) using the attributes from their voice recordings**","27ca15a3":"There are no duplicate entries in the dataset ","d35726ac":"**DNN Model ( with set class_weights)**","ece10652":"# Data Analysis","57d64bef":"# Normalize data (data preprocessing)\nInitialize a MinMaxScaler and scale the features between -1 and 1 to normalize them.\nThe MinMaxScaler transforms features by scaling them to a given range. The fit_transform() method fits to the data and then transforms it.We don't need to scale labels","6a9fb24a":"**KNN algorithm**  ","2a4ae1a7":"This is an unbalanced dataset,it is fairly skewed towards patients with Parkinsons.","2b931615":"Make predictions and evaluate model"}}