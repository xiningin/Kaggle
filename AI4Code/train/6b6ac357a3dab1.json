{"cell_type":{"a949d7d4":"code","1df16535":"code","4fc872c2":"code","61309f08":"code","31bbe61e":"code","38cda206":"code","7ca631fd":"code","f8863784":"code","d3ebec2e":"code","b621dd8c":"code","5f93d2a2":"code","49a74760":"code","52ca07c1":"code","57a7d9fc":"code","f975b2f4":"code","febfdac2":"code","af459ee1":"code","38947787":"code","d5834102":"markdown","ead14c84":"markdown","3da311aa":"markdown","ab497b81":"markdown","1be1f9b2":"markdown","45c7d3bf":"markdown","2f1b106c":"markdown","a6771421":"markdown","f481ece8":"markdown","ff01b4e9":"markdown","3573241c":"markdown","bdf5b1f5":"markdown","8ef93039":"markdown","27924c6d":"markdown","8f29a101":"markdown","a583968b":"markdown","ca746985":"markdown","26595fc6":"markdown","cd5c1f84":"markdown","4aa50a91":"markdown","659cd6d4":"markdown"},"source":{"a949d7d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1df16535":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport math\nimport random\n\n%matplotlib inline","4fc872c2":"train_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nn_pixels = len(train_df.columns) - 1","61309f08":"BATCH_SIZE = 64\nEPOCHS = 50\nVALID_SIZE = 0.15 # percentage of training set to use as validation","31bbe61e":"# Dataset responsible for manipulating data for training as well as training tests.z\nclass DatasetMNIST(torch.utils.data.Dataset):\n    def __init__(self, file_path, \n                 transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), \n                     transforms.Normalize(mean=(0.5,), std=(0.5,))])\n                ):\n        df = pd.read_csv(file_path)\n        \n        if len(df.columns) == n_pixels:\n            # test data\n            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = None\n        else:\n            # training data\n            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = torch.from_numpy(df.iloc[:,0].values)\n            \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        else:\n            return self.transform(self.X[idx])","38cda206":"transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(0, 0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntrain_data = DatasetMNIST('..\/input\/digit-recognizer\/train.csv', transform= transform)\n\nvalid_train_data = DatasetMNIST('..\/input\/digit-recognizer\/train.csv', transform= transform)\n\ntest_data = DatasetMNIST('..\/input\/digit-recognizer\/test.csv', transform= transform)\n\n\n# Shuffling data and choosing data that will be used for training and validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(VALID_SIZE * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data,\n                                           batch_size=BATCH_SIZE,sampler=train_sampler)\n\nvalid_loader = torch.utils.data.DataLoader(dataset=valid_train_data,\n                                           batch_size=BATCH_SIZE,sampler=valid_sampler)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_data,\n                                           batch_size=BATCH_SIZE, shuffle=False)","7ca631fd":"class CNN(nn.Module):\n        def __init__(self):\n            super(CNN,self).__init__()\n\n            self.conv = nn.Sequential(\n                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(32),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(32),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.Dropout(p = 0.25),\n                \n                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.Dropout(p = 0.25),\n            )\n\n            self.fc = nn.Sequential(\n                nn.Linear(64 * 7 * 7, 512),\n                nn.BatchNorm1d(512),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p = 0.5),\n                nn.Linear(512, 10),\n            )\n            \n            #Weight Initilitation \n            for m in self.conv.children():\n                if isinstance(m, nn.Conv2d):\n                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                    m.weight.data.normal_(0, math.sqrt(2. \/ n))\n                elif isinstance(m, nn.BatchNorm2d):\n                    m.weight.data.fill_(1)\n                    m.bias.data.zero_()\n\n            for m in self.fc.children():\n                if isinstance(m, nn.Linear):\n                    nn.init.xavier_uniform_(m.weight)\n                elif isinstance(m, nn.BatchNorm1d):\n                    m.weight.data.fill_(1)\n                    m.bias.data.zero_()\n\n\n        def forward(self, x):\n            x= self.conv(x)\n            x = x.view(x.size(0), -1)\n            x = self.fc(x)\n\n            return F.log_softmax(x, dim=1)  \n\n\n\n","f8863784":"model = CNN()\nprint(model)","d3ebec2e":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","b621dd8c":"error = nn.CrossEntropyLoss()","5f93d2a2":"if torch.cuda.is_available():\n    model = model.cuda()\n    error = error.cuda()","49a74760":"\ndef fit(epoch):\n    \n    valid_loss_min = np.Inf\n    running_loss= 0\n    accuracy_train = 0\n    model.train() \n    exp_lr_scheduler.step()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n            \n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = error(output, target)\n  \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        _, top_class = output.topk(1, dim=1)\n                \n        equals = top_class == target.view(*top_class.shape)\n        \n        accuracy_train += torch.mean(equals.type(torch.FloatTensor))\n        \n    valid_loss = 0\n    accuracy_val = 0\n        \n    with torch.no_grad():\n               \n        model.eval() \n        for data, target in valid_loader:\n            data, target = Variable(data), Variable(target)\n                \n            if  torch.cuda.is_available():\n                data, target = data.cuda(), target.cuda()\n               \n            output = model(data)\n                \n\n            _, top_class = output.topk(1, dim=1)\n                \n            equals = top_class == target.view(*top_class.shape)\n                \n            loss = error(output, target)\n            valid_loss += loss.item() \n            accuracy_val += torch.mean(equals.type(torch.FloatTensor))\n                \n    model.train() \n        \n    train_losses.append(running_loss\/len(train_loader))\n    valid_losses.append(valid_loss\/len(valid_loader))\n        \n    history_accuracy_val.append(accuracy_val\/len(valid_loader))\n    history_accuracy_train.append(accuracy_train\/len(train_loader))\n        \n    network_learned = valid_loss < valid_loss_min\n\n   \n    if epoch == 1 or epoch % 5 == 0 or network_learned:\n        print(f\"Epoch: {epoch}\/{EPOCHS}.. \",\n               f\"Training Loss: {running_loss\/len(train_loader):.3f}.. \",\n                f\"Validation Loss: {valid_loss\/len(valid_loader):.3f}.. \",\n                 f\"Training Accuracy: {accuracy_train\/len(train_loader):.3f}.. \",\n                f\"Validation Accuracy: {accuracy_val\/len(valid_loader):.3f}\")\n        \n    if network_learned:\n        valid_loss_min = valid_loss\n        torch.save(model.state_dict(), 'model_mtl_mnist.pt')\n        print('Detected network improvement, saving current model')\n        ","52ca07c1":"train_losses, valid_losses = [], []\nhistory_accuracy_train,history_accuracy_val = [], []\nfor epoch in range(EPOCHS):\n    fit(epoch)","57a7d9fc":"plt.plot(train_losses, label='Training Loss')\nplt.plot(valid_losses, label='Validation Loss')\nplt.legend(frameon=False)","f975b2f4":"plt.plot(history_accuracy_train, label='Accuracy Train')\nplt.plot(history_accuracy_val, label='Accuracy Validation')\nplt.legend(frameon=False)","febfdac2":"def prediciton():\n    model.eval()\n    test_pred = torch.LongTensor()\n    for i,data in enumerate(test_loader):\n        data = Variable(data, requires_grad=True)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            \n        output = model(data)\n        \n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n        \n    return test_pred","af459ee1":"test_pred = prediciton()  \nout_df = pd.DataFrame(np.c_[np.arange(1, len(test_data)+1)[:,None], test_pred.numpy()], \n                      columns=['ImageId', 'Label'])\n\nprint(out_df)","38947787":"out_df.to_csv('submission.csv', index=False)","d5834102":"**Graficas**","ead14c84":"Definimos el metodo fit que es para entrenar el modelo y realizar las validaciones del mismo, primero iniciamos la perdida y el accuracy en 0 ,tambien indicamos al modelo que va ser entrenado con model.train() ,luego mediante un for se procede a recorrer el train loader y realizar el entrenamiento,con el optimizerse van optimizando los diferentes parametros, al mismo tiempo se calcula la perdida y eficacia del modelo.\n\nPara la validacion ocurre algo similar pero se indica que se va evaluar con model.eval() y no se optimizan parametros simplemente se calculan perdida y eficacia.","3da311aa":"# **Definicion de los Dataloaders y transformaciones para la normalizacion y data augmentation**","ab497b81":"Se llama al metodo fit para comenzar todo el proceso de entrenamiento","1be1f9b2":"# Entrenamiento del Modelo","45c7d3bf":"**Training**","2f1b106c":"Aqui iniciamos el modelo como una red convolucional y lo imprimimos para ver su estructura","a6771421":"La arquitectura la definimos en 2 partes , primero la parte convolucional que va a tener las capas convolucionales y los maxpool, luego despues de aplanar el vector tendremos la parte \"lineal\" donde tenemos 2 capas linear con el vector ya plano\nLuego se inicializan los pesos con el metodo xavier","f481ece8":"**Graficas**","ff01b4e9":"# Defincion de la Arquitectura ","3573241c":"El trainset lo pasamos auna variable llamada train_df y extraemos la cantidad de pixeles de la imagen, le restamos 1 para eliminar la columna de los labels","bdf5b1f5":"# Submission","8ef93039":"# Prediccion","27924c6d":"**Transformaciones y normalizacion del dataset**","8f29a101":"# Preprocesamiento de la data\n","a583968b":"Si cuda esta disponible el modelo lo utilizara","ca746985":"Funcion de Costo","26595fc6":"Indicamos el tama\u00f1o del Batch size, la cantidad de epochs y el porcentaje del training set que va ser usado para validar\n","cd5c1f84":"# Imports","4aa50a91":"# Test","659cd6d4":"**Learning Rate y algoritmo de optimizacion**\nEstablecemos el learning rate inicial y impplementamos el learning rate scheduler que es para ir ajustando el lr a medida que pasen los epochs"}}