{"cell_type":{"fbb691c3":"code","b1bf136b":"code","8947ea5e":"code","41dda643":"code","2ecc7845":"code","5c402930":"code","099b6e57":"code","9664609b":"code","9ea7afbb":"code","82f04ccd":"code","2320508a":"code","5f140a19":"code","a214742c":"code","2cd1f6c4":"code","4c7c2c09":"code","f47dee65":"code","78562c54":"code","91a2ca01":"code","3cf4225b":"code","d4243d15":"code","61a36ed5":"code","da028612":"code","82642947":"code","ea71d974":"code","db26b0b1":"code","6c0db556":"code","655c698e":"code","952ee410":"code","99a57a81":"code","e68cb0ca":"code","74e1c971":"code","b2762eb5":"code","7206ea5a":"code","7b5ebbfd":"code","6df178ba":"code","e803010f":"code","d05cb17c":"code","844aeb32":"code","f8f048a1":"code","7c2ea6d6":"code","64001502":"code","df863bca":"code","a981d9c1":"code","83729268":"code","eed5e049":"code","2c942c74":"code","c4e953d7":"code","3498192c":"code","ad511074":"code","db923c46":"code","25242b20":"code","f591ef81":"code","c396f9a4":"code","dd7bf1ff":"code","ce860449":"code","0ec6571b":"code","2ce8352c":"code","8e9f4e9b":"code","370ce84e":"code","87da3af2":"code","818af9ea":"code","bdc85e9a":"code","963550e4":"code","ab08416f":"code","6cd7bfb0":"code","379b3bab":"code","66a29a85":"code","518a3cb4":"code","9ea86158":"code","693618cb":"code","2639bfd3":"code","d402021c":"code","a7156fa0":"code","b69d9a49":"code","ec3c2409":"code","99cd00d8":"code","12f3ac1e":"code","49683a5a":"markdown","282ee1b1":"markdown","4d258ed9":"markdown","5c797d52":"markdown","fd239dab":"markdown","5ba22d03":"markdown","2d746467":"markdown","e78bfb69":"markdown","529479bb":"markdown","fd8a575c":"markdown","2b69f8ae":"markdown","fda6d081":"markdown","4f86b2ba":"markdown","dc34c05a":"markdown","29c5b669":"markdown","a4f68390":"markdown","7c4a87dd":"markdown","b287e45e":"markdown","9953297e":"markdown","badd500c":"markdown","6c8624ff":"markdown","5ad89ec6":"markdown","6b82e1cd":"markdown"},"source":{"fbb691c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1bf136b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8947ea5e":"train = pd.read_csv('..\/input\/titanicdataset-traincsv\/train.csv')\ntrain.head()","41dda643":"train.describe()","2ecc7845":"train.isnull().sum()","5c402930":"sns.heatmap(train.isnull(),xticklabels=True, yticklabels=False, cbar=False, cmap='viridis')","099b6e57":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', data=train)","9664609b":"train['Survived'].value_counts()","9ea7afbb":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train)","82f04ccd":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train)","2320508a":"sns.countplot(x='SibSp',data=train)","5f140a19":"sns.pairplot(train, hue='Survived')","a214742c":"sns.distplot(train['Age'])","2cd1f6c4":"sns.distplot(train['Fare'])","4c7c2c09":"sns.boxplot(x='Pclass',y='Age',data=train)","f47dee65":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\n","78562c54":"train['Age'] = train[['Age','Pclass']].apply(impute_age, axis=1)","91a2ca01":"train.isnull().sum()","3cf4225b":"sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap='viridis')","d4243d15":"train.drop('Cabin',axis=1,inplace= True)\ntrain.head()","61a36ed5":"train.isnull().sum()","da028612":"train['Age'].describe()","82642947":"fig = train.Age.hist(bins=50)\nfig.set_title('Age')\nfig.set_xlabel('Age')\nfig.set_ylabel('No of passenger')","ea71d974":"## as Age column follows Guassion Distribution, we will calculate the boundries which differntiate the outliers.\nupper_boundry = train['Age'].mean() + 3 * train['Age'].std()\nlower_boundry = train['Age'].mean() - 3 * train['Age'].std()\nprint(lower_boundry)\nprint(upper_boundry)\nprint(train['Age'].mean())","db26b0b1":"## Fare column is skewed\nfig = train.Fare.hist(bins=50)\nfig.set_title('Fare')\nfig.set_xlabel('Fare')\nfig.set_ylabel('No of passenger')","6c0db556":"train.boxplot(column='Fare')","655c698e":"train['Fare'].describe()","952ee410":"## lets compute the inter quantile range to calculate the boundries\nIQR=train.Fare.quantile(0.75)-train.Fare.quantile(0.25)\nIQR","99a57a81":"lower_bridge = train['Fare'].quantile(0.25)-(IQR*1.5)\nupper_bridge = train['Fare'].quantile(0.75)+(IQR*1.5)\nprint(lower_bridge)\nprint(upper_bridge)","e68cb0ca":"## for extreme outlier\nlower_bridge = train['Fare'].quantile(0.25)-(IQR*3)\nupper_bridge = train['Fare'].quantile(0.75)+(IQR*3)\nprint(lower_bridge)\nprint(upper_bridge)","74e1c971":"train=train.copy()","b2762eb5":"train.loc[train['Age']>=68.80,'Age']=68.80","7206ea5a":"train.loc[train['Fare']>=100.2688,'Fare']=100.2688","7b5ebbfd":"fig = train.Fare.hist(bins=50)\nfig.set_title('Fare')\nfig.set_xlabel('Fare')\nfig.set_ylabel('No of passenger')","6df178ba":"fig = train.Age.hist(bins=50)\nfig.set_title('Age')\nfig.set_xlabel('Age')\nfig.set_ylabel('No of passenger')","e803010f":"train.head()","d05cb17c":"pd.get_dummies(train['Embarked'],drop_first=True).head()","844aeb32":"sex = pd.get_dummies(train['Sex'],drop_first = True)\nembark = pd.get_dummies(train['Embarked'],drop_first = True)","f8f048a1":"train.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)\ntrain.head()","7c2ea6d6":"train = pd.concat([train,sex,embark],axis=1)","64001502":"train.head()","df863bca":"train.isnull().sum()","a981d9c1":"from sklearn.feature_selection import SelectKBest, chi2\nX = train.drop(['Survived'],axis=1)\ny = train['Survived']\nbestfeatures = SelectKBest(chi2, k=8)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Attributes','Score']","83729268":"print(featureScores.nlargest(9,'Score'))","eed5e049":"train.shape","2c942c74":"## train test split\nfrom sklearn.model_selection import train_test_split\nX = train.drop(['Survived'],axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","c4e953d7":"X_train","3498192c":"X_test","ad511074":"y_train","db923c46":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train.values)","25242b20":"X_train = scaler.transform(X_train)\nX_train","f591ef81":"X_test = scaler.transform(X_test)\nX_test","c396f9a4":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score, accuracy_score","dd7bf1ff":"from sklearn.linear_model import LogisticRegression\nlog_cls = LogisticRegression()\nlog_cls.fit(X_train,y_train)\nytrain_pred = log_cls.predict_proba(X_train)\nprint('RF train roc.auc: {}'.format(roc_auc_score(y_train,ytrain_pred[:,1])))\ny_pred = log_cls.predict(X_test)\nytest_pred = log_cls.predict_proba(X_test)\nprint('RF test roc.auc: {}'.format(roc_auc_score(y_test,ytest_pred[:,1])))\nprint('Accuracy score: {}'.format(accuracy_score(y_test,y_pred)))","ce860449":"ytrain_pred","0ec6571b":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\nmax_features = ['auto','sqrt','log2']\nmax_depth = [int(x) for x in np.linspace(10,1000,10)]\nmin_samples_split = [2,5,10,14]\nmin_samples_leaf = [1,3,5,8]\nparam = {'n_estimators':n_estimators, 'max_features':max_features, 'max_depth':max_depth, 'min_samples_split':min_samples_split,\n        'min_samples_leaf':min_samples_leaf, 'criterion':['entropy','gini']}\nprint(param)","2ce8352c":"from tpot import TPOTClassifier\ntpot_classifier = TPOTClassifier(generations=5,population_size=24,offspring_size=12,verbosity=2,early_stop=12,\n                                config_dict={'sklearn.ensemble.RandomForestClassifier': param},cv=4, scoring='accuracy')\ntpot_classifier.fit(X_train,y_train)","8e9f4e9b":"accuracy = tpot_classifier.score(X_test,y_test)\nprint(accuracy)","370ce84e":"import optuna\nimport sklearn\ndef objective(trial):\n    n_estimators = trial.suggest_int('n_estimators',100,2000,10)\n    max_depth = trial.suggest_float('max_depth',10,1000)\n    cls = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth)\n    return sklearn.model_selection.cross_val_score(cls,X_train,y_train,n_jobs=-1,cv=4).mean()\n","87da3af2":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective,n_trials=100)\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.values))\nprint(\"Best hyperparameters: {}\".format(trial.params))","818af9ea":"trial","bdc85e9a":"study.best_params","963550e4":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=1150, max_depth=681.0458391285997)\nrf.fit(X_train,y_train)","ab08416f":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ny_pred=rf.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","6cd7bfb0":"from sklearn import tree\ndt_cls = tree.DecisionTreeClassifier()\ndt_cls.fit(X_train, y_train) \nytrain_pred = dt_cls.predict_proba(X_train)\nprint('DT train roc.auc: {}'.format(roc_auc_score(y_train,ytrain_pred[:,1])))\ny_pred = dt_cls.predict(X_test)\nytest_pred = dt_cls.predict_proba(X_test)\nprint('DT test roc.auc: {}'.format(roc_auc_score(y_test,ytest_pred[:,1])))\nprint('Accuracy score: {}'.format(accuracy_score(y_test,y_pred)))","379b3bab":"from sklearn.ensemble import RandomForestClassifier\nrf_cls = RandomForestClassifier()\nrf_cls.fit(X_train,y_train)\nytrain_pred = rf_cls.predict_proba(X_train)\nprint('RF train roc.auc: {}'.format(roc_auc_score(y_train,ytrain_pred[:,1])))\ny_pred = rf_cls.predict(X_test)\nytest_pred = rf_cls.predict_proba(X_test)\nprint('RF test roc.auc: {}'.format(roc_auc_score(y_test,ytest_pred[:,1])))\nprint('Accuracy score: {}'.format(accuracy_score(y_test,y_pred)))","66a29a85":"from sklearn.ensemble import GradientBoostingClassifier\ngb_cls = GradientBoostingClassifier()\ngb_cls.fit(X_train,y_train)\nytrain_pred = gb_cls.predict_proba(X_train)\nprint('GB train roc.auc: {}'.format(roc_auc_score(y_train,ytrain_pred[:,1])))\ny_pred = gb_cls.predict(X_test)\nytest_pred = gb_cls.predict_proba(X_test)\nprint('GB test roc.auc: {}'.format(roc_auc_score(y_test,ytest_pred[:,1])))\nprint('Accuracy score: {}'.format(accuracy_score(y_test,y_pred)))","518a3cb4":"from sklearn.ensemble import AdaBoostClassifier\nada_cls = AdaBoostClassifier()\nada_cls.fit(X_train,y_train)\nytrain_pred = ada_cls.predict_proba(X_train)\nprint('GB train roc.auc: {}'.format(roc_auc_score(y_train,ytrain_pred[:,1])))\ny_pred = ada_cls.predict(X_test)\nytest_pred = ada_cls.predict_proba(X_test)\nprint('GB test roc.auc: {}'.format(roc_auc_score(y_test,ytest_pred[:,1])))\nprint('Accuracy score: {}'.format(accuracy_score(y_test,y_pred)))","9ea86158":"from sklearn.neighbors import KNeighborsClassifier\nKNN_cls = KNeighborsClassifier()\nKNN_cls.fit(X_train,y_train)\nytrain_pred = KNN_cls.predict_proba(X_train)\nprint('KNN train roc.auc: {}'.format(roc_auc_score(y_train,ytrain_pred[:,1])))\ny_pred = KNN_cls.predict(X_test)\nytest_pred = KNN_cls.predict_proba(X_test)\nprint('KNN test roc.auc: {}'.format(roc_auc_score(y_test,ytest_pred[:,1])))\nprint('Accuracy score: {}'.format(accuracy_score(y_test,y_pred)))","693618cb":"pred = []\nfor model in [log_cls,dt_cls,rf_cls,gb_cls,ada_cls,KNN_cls]:\n    pred.append(pd.Series(model.predict_proba(X_test)[:,1]))\nfinal_prediction = pd.concat(pred,axis=1).mean(axis=1)\nprint('Ensemble test roc.auc: {}',format(roc_auc_score(y_test,final_prediction)))","2639bfd3":"pd.concat(pred,axis=1)","d402021c":"final_prediction","a7156fa0":"### Calculate the roc curve\nfpr, tpr, thresholds = roc_curve(y_test,final_prediction)\nthresholds","b69d9a49":"from sklearn.metrics import accuracy_score\naccuracy = []\nfor thres in thresholds:\n    y_pred = np.where(final_prediction>thres,1,0)\n    accuracy.append(accuracy_score(y_test,y_pred, normalize=True))\n    \naccuracy = pd.concat([pd.Series(thresholds), pd.Series(accuracy)],axis=1)\naccuracy.columns = ['thresholds', 'accuracy']\naccuracy","ec3c2409":"accuracy.sort_values(by='accuracy',ascending=False, inplace=True)\naccuracy.head()","99cd00d8":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0,1],[0,1], color='darkblue')\n    plt.xlabel('False Positive rate')\n    plt.ylabel('True Positive rate')\n    plt.title('Receiver Opearting Char Curve')\n    plt.legend()\n    plt.show()","12f3ac1e":"plot_roc_curve(fpr, tpr)","49683a5a":"**importing necessary modules**","282ee1b1":"It seems that, Age and Cabin columns has none values. Roughly 20% Age data missing, which is reasonable replacable. \nLooking at the Cabin column, it looks we are just missing too much data, probabaly will drp this column.","4d258ed9":"## Feature Engineering","5c797d52":"**importing the dataset**","fd239dab":"## Model Creation","5ba22d03":"## from obove plot, it seems this is balanced data set","2d746467":"## Now will check the accuracy with Optuna","e78bfb69":"<h3>Imputing the null values with mean<h3>","529479bb":"## Ada Boost Classifier","fd8a575c":"**Building Gradient Boosting Classifier**","2b69f8ae":"## Scaling","fda6d081":"**Building Logistics Regression**","4f86b2ba":"## From above code, it assumes Age column follows Guassion \/ Normal Distribution and  the Fare columns is skewed.","dc34c05a":"## from above plot, average mean age for 1st class passenger is 37, for 2nd class is 30, for 3rd class is 25","29c5b669":"## KNN Classifier","a4f68390":"<h3>We have removed the nan values<h3>","7c4a87dd":"## Lets handle the outlier","b287e45e":"## Now we will focus on selecting the best threshold for max accuracy","9953297e":"## Lets transform the other columns to numearical value ","badd500c":"## EDA","6c8624ff":"**Building Random Forest Classifier**","5ad89ec6":"## Will apply the Genetic algorithm and will check the accuracy.","6b82e1cd":"**Building Decesion tree Classifier**"}}