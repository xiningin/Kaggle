{"cell_type":{"eadb6582":"code","12888b23":"code","f0dff638":"code","c4d684ad":"code","e8a83b44":"code","ec88d4bf":"code","003d7c40":"code","2814665f":"code","6b210a50":"code","8b29a7c5":"markdown","4be41a76":"markdown","f74f32dc":"markdown","921b463e":"markdown","c4dcd393":"markdown","015fa43b":"markdown","7608debb":"markdown","e97e54a0":"markdown","d9ee2b55":"markdown","9b2a340b":"markdown"},"source":{"eadb6582":"from __future__ import print_function, division\nimport os\nfrom os import walk\nimport sys\nimport torch\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nimport tqdm\nimport re\n\nPATH = '..\/input\/covid19-pneumonia-normal-chest-xray-pa-dataset' # Path of the dataset\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nuse_gpu = torch.cuda.is_available()\nif use_gpu:\n    print(\"Using CUDA\")\n    \nall_best_accs = []","12888b23":"available_models = [\"resnet\",\"alexnet\",\"vgg\",\"squeezenet\",\"densenet\",\"inception\"]\n\ndef set_parameter_requires_grad(model, feature_extracting): #function to freezing parameters in our model\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n            \ndef initialize_model(model_name, num_classes = 3, feature_extract = True, use_pretrained=True): # num_classes=3 \n                                                                                                #since we have 3 classes\n    \n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    \n    model_name = model_name.lower() \n    model_name.replace(\" \", \"\") # removing spaces in string\n    model_name = re.sub(r'[0-9]',\"\",model_name) # removing numbers in string\n    \n    \n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_name = \"Resnet18\"\n        model_ft = models.resnet18(pretrained=use_pretrained) # using pretrained weights\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)   # setting number of outputs to 3 (initially most models have number of \n                                                         #  outputs as 1000 since they are trained on imagenet)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_name = \"Alexnet\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG16_bn\n        \"\"\"\n        model_name = \"VGG16 with batch normalization\"\n        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_name = \"Squeezenet\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_name = \"Densenet\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has two outputs normal output and auxiliary output\n        \"\"\"\n        model_name = \"Inception v3\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n    model_ft = model_ft.to(device)\n    return model_ft, input_size","f0dff638":"# available_models = [\"resnet\",\"alexnet\",\"vgg\",\"squeezenet\",\"densenet\",\"inception\"]\nmodel_name = \"inception\"\nmodel, size = initialize_model(model_name)\nprint(\"model =\",model_name,\"\\nsize of image = \"+ str(size)) # optimal size of the image to be given input","c4d684ad":"data_dir = PATH \nbatch_size = 32\nvalid_size = 0.1 # define partition of validation size in train size\n\n# Defining transforms for the training data and testing data\ndata_transforms = transforms.Compose([transforms.RandomAffine(degrees = [0,9], shear = [0., 0.066, 0.133, 0.2]),\n                                       transforms.Resize((size,size)),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor()])\n\n\n# obtain training indices that will be used for validation\ndata = datasets.ImageFolder(data_dir , transform= data_transforms)\ndataset_sizes = {\"train\" : (len(data)*9)\/\/10, \"val\" : len(data)\/\/10}\nnum_train = len(data)\n\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\n\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,sampler=valid_sampler)\ndata_loader = {\"train\" : train_loader, \"val\" : val_loader}\nphases = [\"train\", \"val\"]\nclass_names = data.classes\n\nprint(\"The Classes are : \",end = \"\")\nprint(*class_names,sep = \", \")\n","e8a83b44":"def imshow1(image, ax=None, title=None, normalize=True):\n    #Imshow for Tensor\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0., 0., 0.])\n        std = np.array([1., 1., 1.])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\ndata_iter = iter(val_loader)\n\nimages, labels = next(data_iter)\nprint(images[0].shape)\nimshow1(images[0])","ec88d4bf":"def train_model(model, criterion, optimizer, scheduler, epochs= 3):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(1, epochs+1):\n        print('Epoch {}\/{}'.format(epoch, epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for batch, (inputs, labels) in enumerate(data_loader[phase]):\n                print(\"\\r batch {}\/{}\".format(batch+1, len(data_loader[phase])), end='', flush=True)\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    if model_name == \"inception\" and phase == \"train\": # Will only run for incepton training\n                        outputs,aux = model(inputs)\n                    else:\n                        outputs = model(inputs)\n                    \n                    _, preds = torch.max(outputs, 1)\n                    \n                    if model_name == \"inception\" and phase == \"train\": # Will only run for incepton training\n                        loss = criterion(outputs,labels) + 0.4*criterion(aux,labels)\n                    else:\n                        loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('\\n{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    all_best_accs.append(best_acc)\n    return model","003d7c40":"model = {x:[] for x in available_models}\n\nbegin_tol = time.time()\n\nfor model_name in available_models:\n    \n    # ----------------> Initialization part\n    model[model_name], size = initialize_model(model_name)\n    \n    \n    # ----------------> Data loader part\n    data_dir = PATH\n    batch_size = 32\n    valid_size = 0.1 # define partition of validation size in train size\n\n    # TODO: Define transforms for the training data and testing data\n    train_transforms = transforms.Compose([transforms.RandomAffine(degrees = [0,9], shear = [0., 0.066, 0.133, 0.2]),\n                                           transforms.Resize((size,size)),\n                                           transforms.RandomHorizontalFlip(),\n                                           transforms.ToTensor()])\n\n\n    # obtain training indices that will be used for validation\n    data = datasets.ImageFolder(data_dir , transform=train_transforms)\n    dataset_sizes = {\"train\" : (len(data)*9)\/\/10, \"val\" : len(data)\/\/10}\n    num_train = len(data)\n\n    indices = list(range(num_train))\n    np.random.shuffle(indices)\n    split = int(np.floor(valid_size * num_train))\n\n    train_idx, valid_idx = indices[split:], indices[:split]\n\n    # define samplers for obtaining training and validation batches\n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,sampler=train_sampler)\n    val_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,sampler=valid_sampler)\n    data_loader = {\"train\" : train_loader, \"val\" : val_loader}\n    modes = [\"train\", \"val\"]\n    class_names = data.classes    \n    \n    # ----------------> Training part    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model[model_name].parameters(), lr = 0.01, momentum = 0.9)\n    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.1)\n    \n    print(\"\\n\"+\"\\033[1m\" + model_name.upper() + \"\\033[0m\")\n    \n    train_model(model[model_name], criterion, optimizer, exp_lr_scheduler, epochs = 3)\n    torch.save(model[model_name].state_dict(), model_name+'_covid_xray.pt')\n\n    \n # We get the total time for training of all the models    \ntol_time = time.time()-begin_tol \nprint('Total training completed in {:.0f}m {:.0f}s'.format(tol_time \/\/ 60, tol_time % 60))\n\n# Finding the best model on the validation set\noverall_best_acc = max(all_best_accs)\nbest_model_name = available_models[all_best_accs.index(overall_best_acc)]\nprint(\"best model is \"+best_model_name.upper()+\" with accuracy of \"+str(overall_best_acc.item()*100)[:6]+\" %\")","2814665f":"all_best_accs_test = []\ndef test_model(model, criterion, optimizer, scheduler, epochs= 3):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    print('-' * 10)\n    model.eval()   # Set model to evaluate mode\n    running_loss = 0.0\n    running_corrects = 0\n\n    # Iterate over data.\n    for batch, (inputs, labels) in enumerate(data_loader):\n        print(\"\\rbatch {}\/{}\".format(batch+1, len(data_loader)), end='', flush=True)\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward\n        # track history if only in train\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n\n        # statistics\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\n#                 if batch == 3:\n#                     break\n\n        epoch_loss = running_loss \/ dataset_size\n        epoch_acc = running_corrects.double() \/ dataset_size\n        phase = \"test\"\n    print('\\n{} Loss: {:.4f} Acc: {:.4f}'.format(\n        phase, epoch_loss, epoch_acc))\n    best_acc = epoch_acc\n    print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    all_best_accs_test.append(best_acc)\n","6b210a50":"model_c = {x:[] for x in available_models}\nbegin_tol = time.time()\nfor model_name in available_models:\n    \n    model_c[model_name], size = initialize_model(model_name)\n    model_c[model_name].load_state_dict(torch.load(\".\/\"+model_name+\"_covid_xray.pt\"))\n    data_dir = PATH\n    batch_size = 32\n    valid_size = 0.1 # define partition of validation size in train size\n\n    # TODO: Define transforms for the training data and testing data\n    train_transforms = transforms.Compose([transforms.RandomAffine(degrees = [0,9], shear = [0., 0.066, 0.133, 0.2]),\n                                           transforms.Resize((size,size)),\n                                           transforms.RandomHorizontalFlip(),\n                                           transforms.ToTensor()])\n\n\n    # obtain training indices that will be used for validation\n    data = datasets.ImageFolder(data_dir , transform=train_transforms)\n    dataset_size = len(data)\n    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n\n    class_names = data.classes    \n    \n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model_c[model_name].parameters(), lr = 0.01, momentum = 0.9)\n    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.1)\n    print(\"\\n\"+\"\\033[1m\" + model_name.upper() + \"\\033[0m\")\n    test_model(model_c[model_name], criterion, optimizer, exp_lr_scheduler, epochs = 3)\n\n\ntol_time = time.time()-begin_tol\nprint('Total testing completed in {:.0f}m {:.0f}s'.format(tol_time \/\/ 60, tol_time % 60))\noverall_best_acc = max(all_best_accs_test)\nbest_model_name = available_models[all_best_accs_test.index(overall_best_acc)]\n\nprint(\"best model is \"+best_model_name.upper()+\" with accuracy of \"+str(overall_best_acc.item()*100)[:6]+\" %\")","8b29a7c5":"## Importing necessary libraries","4be41a76":"### Testing on the whole dataset\n#### We'll test each model on a loop just like we did in testing \n##### This will take about 15 mins ","f74f32dc":"## It'll take a lot of time for train training (about an hour on GPU) as we are training six models in a huge dataset, so grab your \u2615 or \ud83c\udf7f while training\n![2417841a0bb0739c81e4eea300452cf8.jpg](attachment:bd4d64f4-fa29-4e89-93f8-099a42fa1fff.jpg)\n\n## Training all the models in a loop \n### some models have different prescribed image size, so creating data loaders for each models separately \n#### I'm also saving the best state of all the models in a .pt file for each","921b463e":"## Function for training the models\n#### Note: Inception will give two outputs(normal output and auxlilary output) while training here but will give only one output while testing\/evaluating  ","c4dcd393":"## Data Augmentation and Creating the Dataset loader","015fa43b":"## Funtion to initialize model and Fine tune the pretrained models \n#### These models have 1000 classes in output layer since they are trained on imagenet. We fine tune it to three\n#### We'll be using Resnet, Alexnet, VGG16, Squeezenet, Densenet, InceptionV3 with pretrained weights. There are other models too but most of them are difficult to fine tune (some have multiple output layers,etc) ","7608debb":"##  Visualizing an image from the dataset loader","e97e54a0":"## Now testing on the whole dataset\n### Creating the funtion for testing all the models","d9ee2b55":"### Checking the above function","9b2a340b":"## That did took a while \ud83d\ude05\n![9dbfbb3d895b3db929ce30c471a6addb-min.gif](attachment:3a8a602c-2b9e-4149-ba5d-1d78ccdb9c7c.gif)\n#### Ok, lets get back\n\n#### If you want to use any model, download the .pt file from the output folder and just follow the code below:\n#### model, size = initialize_model(model_name)   #dont forget to copy the initialize_model function\n#### model.load_state_dict(torch.load(model_name+\"_covid_xray.pt\"))\n\n#### \ud83e\udd67 Voil\u00e0 the model is ready to use\n## Hope you find this useful \ud83d\ude01 \ud83d\udc4d\ud83c\udffb\n### Any comments on changes or improvements, etc would really help. \n#### Thanks in advance \ud83d\udc4d\ud83c\udffb"}}