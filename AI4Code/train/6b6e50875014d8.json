{"cell_type":{"220771cb":"code","08d69805":"code","41e03e4d":"code","8e9fabcd":"code","9dba0746":"code","f0b7b245":"code","469a9a70":"code","f4e79acb":"code","a8ea609c":"code","458c4c52":"code","895c3290":"code","d6c89df4":"code","17c50f29":"code","ed3c04dd":"code","79db2298":"code","2f92421e":"code","336ce865":"markdown","afd65fff":"markdown","d1d2d1b3":"markdown"},"source":{"220771cb":"#\u30e1\u30bf\u30c7\u30fc\u30bf\u6574\u5f62\u95a2\u6570\n#from notebook\u300cCorrections to notation errors and EDA for META\u300d\n#https:\/\/www.kaggle.com\/ryoichi0917\/corrections-to-notation-errors-and-eda-for-meta\ndef meta_define():\n    \"\"\"\n    input : none\n    output : Corrected metadata\n    \"\"\"\n    import pandas as pd\n    import os \n    os.chdir(\"\/kaggle\/input\/hah-data-science-challenge\/\")\n    df_train = pd.read_csv(\"train.csv\", index_col=False)\n    df_test = pd.read_csv(\"test.csv\", index_col=False)\n    \n    ##################################################\n    #\u4ee5\u4e0b\u8f9e\u66f8\u3084\u5909\u6570\u306e\u5b9a\u7fa9\n    #\u5404\u30c7\u30fc\u30bf\u4fee\u6b63\u7528\u306e\u8f9e\u66f8\n    bolt_dict = {\n        '\u5927':\"big\",\n        '\u5c0f':\"small\"\n    }\n\n    plate_dict = {\n        '\u5927':\"big\",\n        '\u5c0f':\"small\"\n    }\n\n    record_dict = {\n        'PC\u5185\u81d3':\"pc_built_in\",\n        'PC\u5185\u8535':\"pc_built_in\",\n        'USB1':\"usb1\", \n        'USB2':\"usb2\", \n        'USB3':\"usb3\", \n        'USB4':\"usb4\", \n        '\u30b9\u30de\u30db':\"smart_phone\",\n        '\u30b9\u30de\u30db\u306e\u30dc\u30a4\u30b9\u30ec\u30b3\u30fc\u30c0':\"smart_phone\",\n        '\u5185\u8535\u30de\u30a4\u30af':\"pc_built_in\",\n        }\n\n    distance_dict = {\n        '10cm': 0.1, \n        '10\u339d': 0.1, \n        '1M': 1.0, \n        '20cm': 0.2, \n        '20\u339d': 0.2, \n        '2M': 2.0, \n        '2m': 2.0, \n        '30cm': 0.3, \n        '30cn': 0.3, \n        '30\u339d': 0.3, \n        '3m': 3.0, \n        '40cm': 0.4, \n        '40\u339d': 0.4, \n        '50cm': 0.5, \n        '50\u339d': 0.5, \n        '5cm': 0.05,\n        '8cm': 0.08, \n        '\uff11\uff2d': 1.0   \n    }\n\n    cvt_dict = {\n        \"\u306d\u3058\" : bolt_dict, \n        '\u30d7\u30ec\u30fc\u30c8' : plate_dict, \n        '\u9332\u97f3\u65b9\u6cd5' : record_dict, \n        '\u30de\u30a4\u30af\u8ddd\u96e2' : distance_dict\n    }\n    \n    #df_train\u65e5\u672c\u8a9e\u30ab\u30e9\u30e0\u540d : ['ID', '\u306d\u3058', '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2', '\u30d5\u30a1\u30a4\u30eb', 'Target']\n    col_train = ['id', 'bolt', 'plate', 'record', 'mic_dist', 'file', 'target']\n    #df_test\u65e5\u672c\u8a9e\u30ab\u30e9\u30e0\u540d : ['ID', '\u306d\u3058', '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2', '\u30d5\u30a1\u30a4\u30eb', 'Target']\n    col_test = ['id', 'bolt', 'plate', 'record', 'mic_dist', 'file']\n    \n    tgt_col = [\"\u306d\u3058\", '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2']\n    ##################################################\n    \n    for col in tgt_col:#Target\u306f\u5909\u63db\u5bfe\u8c61\u5916\n        df_train[col] = df_train[col].map(cvt_dict[col])\n        df_test[col] = df_test[col].map(cvt_dict[col])\n        \n    df_train.columns = col_train\n    df_test.columns = col_test\n    \n    return df_train, df_test\n","08d69805":"#operation check\ndf_train, df_test = meta_define()","41e03e4d":"import numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm.notebook import tqdm\nimport librosa \n\nimport matplotlib.pyplot as plt\nimport os","8e9fabcd":"def cvt_unit_vector(x):\n    \"\"\"\n    \u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u95a2\u6570\n    extract_feature\u5185\u3067\u5229\u7528\n    \"\"\"\n    norm = np.linalg.norm(x)\n    return x\/norm\n\ndef decide_nfft(win_length):\n    \"\"\"\n    \u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u5f8c\u306e\u6700\u7d42\u7684\u306awindow_size\u3092\u6c7a\u5b9a\u3059\u308b\u95a2\u6570\n    extract_feature\u5185\u3067\u5229\u7528\n    \"\"\"\n    n = 1\n    n_fft = 0\n    while win_length > n_fft:\n        n_fft = 2**n\n        n += 1\n    return n_fft\n\ndef extract_feature(df, mode = \"max\", unit_vect=True, n_mels = 128):\n    \"\"\"\n    \u3010\u5165\u529b\u3011\n    df:\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u306a\u3069\u30e1\u30bf\u60c5\u5831\u306e\u5165\u3063\u305f\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\n    mode:\u30b9\u30da\u30af\u30c8\u30eb\u306e\u6642\u9593\u65b9\u5411\u3078\u306e\u5727\u7e2e\u65b9\u6cd5(\"max\"\u306a\u3089\u6700\u5927\u5024, \"mean\"\u306a\u3089\u5e73\u5747\u5024)\n    unit_vect:\u30d9\u30af\u30c8\u30eb\u3092\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u304b\u5426\u304b\n    n_mels:\u30e1\u30eb\u30d5\u30a3\u30eb\u30bf\u30d0\u30f3\u30af\u306e\u6570\n    \u3010\u51fa\u529b\u3011\n    \u7279\u5fb4\u91cf\u306e\u884c\u5217\uff1a\uff08shape:\u30b5\u30f3\u30d7\u30eb\u6570*n_mels\uff09\n    \"\"\"\n    \n    # librosa\u3067.wav\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u7279\u5fb4\u62bd\u51fa\u3057\u307e\u3059\u3002\n\n    X = np.zeros((len(df), n_mels))\n    filelist = [i for i in df.file.values]\n    print(len(filelist))\n\n    for i, filename in enumerate(tqdm(filelist)):\n        if \"train\" in filename:\n            filename = os.path.join(\"train\/train\", filename)\n        if \"test\" in filename:\n            filename = os.path.join(\"test\/test\", filename)\n        \n        \n        y, sr = librosa.core.load(filename,sr=None)\n        y = librosa.util.normalize(y)\n        win_length = int(sr\/8)\n        hop_length = int(win_length\/8)\n        \n        \n        fft = librosa.feature.melspectrogram(y=y,\n                                         n_fft=decide_nfft(win_length),#n_fft:\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u5f8c\u306e\u6700\u7d42\u7684\u306awindow_size\n                                         win_length=win_length,#win_length:\u5404\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u6295\u5165\u3059\u308b\u5b9f\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba\n                                         hop_length=hop_length,#hop_length:\u5404\u7a93\u306e\u30b9\u30e9\u30a4\u30c9\u91cf\n                                         power=1,\n                                         n_mels = n_mels    \n                                          )\n        fft, _ = librosa.magphase(fft, power=1)#\u632f\u5e45\u30b9\u30da\u30af\u30c8\u30eb\u306b\u5909\u63db\n        fft = librosa.amplitude_to_db(fft)#db\u5909\u63db\n        \n    \n        # \u6642\u9593\uff0an_mels\u306e\u884c\u5217\u304c\u5f97\u3089\u308c\u308b\u306e\u3067\u3001\u6642\u9593\u65b9\u5411\u306b\u5727\u7e2e\u3059\u308b\n        #\"mode:max\"\u306e\u6642\u306f\u6700\u5927\u5024\n        #\"mode:mean\"\u306e\u6642\u306f\u6700\u5c0f\u5024\n        if mode == \"max\":\n            fft = fft.max(axis=1)\n        elif mode == \"mean\":\n            fft = fft.mean(axis=1)\n            \n        if unit_vect == True:\n            fft = cvt_unit_vector(fft)\n        \n        X[i] = fft\n    return X","9dba0746":"df_normal = df_train[df_train.target==0]#\u6b63\u5e38\u30c7\u30fc\u30bf\ndf_anormal = df_train[df_train.target==1]#\u7570\u5e38\u30c7\u30fc\u30bf\ndf_unlabeled = df_train[df_train.target.isnull()==True]#\u30e9\u30d9\u30eb\u306a\u3057\u30c7\u30fc\u30bf","f0b7b245":"def normalize_and_clip(data, min_, max_):\n    \"\"\"\n    \u8f9e\u66f8\u5b66\u7fd2\u304c\u5b9f\u65bd\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u30c7\u30fc\u30bf\u3092\u6b63\u898f\u5316\u3057\u3001\u3055\u3089\u306b0~1\u306e\u9593\u306b\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u3059\u308b\n    \u3010\u5165\u529b\u3011\n    data:\u51e6\u7406\u5bfe\u8c61\u306e\u95a2\u6570\n    min_\uff1a\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u6700\u5c0f\u5024\n    max_:\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u6700\u5927\u5024\n    \"\"\"\n    data_norm = (data - min_) \/ (max_ - min_)\n    data_norm = np.clip(data_norm, 0, 1)\n    return data_norm","469a9a70":"X_normal = extract_feature(df_normal,  mode=\"max\", unit_vect=True)#[:, 2:90]\nX_normal_min = X_normal.min()\nX_normal_max = X_normal.max()\n\nX_anormal = extract_feature(df_anormal,  mode=\"max\", unit_vect=True)#[:, 2:90]\nX_unlabeled = extract_feature(df_unlabeled,  mode=\"max\", unit_vect=True)#[:, 2:90]\nX_test = extract_feature(df_test,  mode=\"max\", unit_vect=True)#[:, 2:90]\n\n\nX_normal_bet_oz = normalize_and_clip(X_normal, X_normal_min, X_normal_max)\nX_anormal_bet_oz = normalize_and_clip(X_anormal, X_normal_min, X_normal_max)\nX_unlabeled_bet_oz = normalize_and_clip(X_unlabeled, X_normal_min, X_normal_max)\nX_test_bet_oz = normalize_and_clip(X_test, X_normal_min, X_normal_max)","f4e79acb":" #\u4f5c\u6210\u3057\u305f\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316\u3057\u3066\u6bd4\u8f03\u3057\u307e\u3059\u3002\nfig, axes = plt.subplots(2,2, figsize=(10, 5), sharey=True)\naxes[0,0].plot(X_normal_bet_oz.T)\naxes[0,0].set_title(\"normal\")\n\naxes[1,0].plot(X_anormal_bet_oz.T)\naxes[1,0].set_title(\"anormal\")\n\naxes[0,1].plot(X_unlabeled_bet_oz.T)\naxes[0,1].set_title(\"unlabel\")\n\naxes[1,1].plot(X_test_bet_oz.T)\naxes[1,1].set_title(\"test\")\n\nfig.tight_layout()\nfig.show()","a8ea609c":"from sklearn.decomposition import DictionaryLearning\n\nn_components = 5 #\u7d44\u307f\u5408\u308f\u305b\u308b\u8f9e\u66f8\u306e\u6570(\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3059\u3002\u4eca\u56de\u306f\u3048\u3044\u3084\u30675\u3068\u3057\u307e\u3057\u305f\u3002)\ndict_learner = DictionaryLearning(\n    n_components=n_components, \n    fit_algorithm='cd', \n    transform_algorithm='lasso_lars', \n    random_state=42,\n    positive_code=True, #\u7dda\u578b\u7d50\u5408\u306e\u91cd\u307f\u3092\u30010\u307e\u305f\u306f\u6b63\u306e\u6570\u306b\u9650\u5b9a\u3059\u308b\u304b\n    positive_dict=True #\u8f9e\u66f8\u306e\u8981\u7d20\u30920\u307e\u305f\u306f\u6b63\u306e\u6570\u306b\u9650\u5b9a\u3059\u308b\u304b\n)\n\nX_normal_transformed = dict_learner.fit_transform(X_normal_bet_oz)\nX_anormal_transformed = dict_learner.transform(X_anormal_bet_oz)\nX_unlabeled_transformed = dict_learner.transform(X_unlabeled_bet_oz)\nX_test_transformed = dict_learner.transform(X_test_bet_oz)\n\nprint(\"\u8f9e\u66f8\u306e\u6b21\u5143\u6570:\", dict_learner.components_.shape)#\u6b21\u5143\u6570(\u8f9e\u66f8\u306e\u6570, mel\u306e\u6b21\u5143)\n\nplt.plot(dict_learner.components_.T)\nplt.title(\"dict_visual\")\nplt.show()","458c4c52":"print(\"\u7dda\u578b\u7d50\u5408\u306e\u91cd\u307f\u306e\u6b21\u5143\u6570:\", X_normal_transformed.shape)#\u6b21\u5143\u6570(\u30b5\u30f3\u30d7\u30eb\u6570, \u8f9e\u66f8\u306e\u6570)\n#\u91cd\u307f\u306e\u4f8b\nX_normal_transformed[0:3]","895c3290":"normal_pred = np.matmul(X_normal_transformed,dict_learner.components_)#\u91cd\u307f\u3068\u8f9e\u66f8\u306e\u7dda\u578b\u7d50\u5408\u3067\u3001\u30c7\u30fc\u30bf\u3092\u518d\u69cb\u6210\ndiff_normal = (normal_pred-X_normal_bet_oz)#\u518d\u69cb\u6210\u7d50\u679c\u3068\u5143\u30c7\u30fc\u30bf\u306e\u5dee\u5206\u3092\u8a08\u7b97\n\nanormal_pred = np.matmul(X_anormal_transformed,dict_learner.components_)\ndiff_anormal = (anormal_pred-X_anormal_bet_oz)\n\nunlabeled_pred = np.matmul(X_unlabeled_transformed,dict_learner.components_)\ndiff_unlabeled = (unlabeled_pred-X_unlabeled_bet_oz)\n\ntest_pred = np.matmul(X_test_transformed,dict_learner.components_)\ndiff_test = (test_pred-X_test_bet_oz)","d6c89df4":" #\u518d\u69cb\u6210\u8aa4\u5dee\u306e\u53ef\u8996\u5316\nfig, axes = plt.subplots(2,2, figsize=(10, 5), sharey=True)\naxes[0,0].plot(diff_normal.T)\naxes[0,0].set_title(\"diff_normal\")\n\naxes[1,0].plot(diff_anormal.T)\naxes[1,0].set_title(\"diff_anormal\")\n\naxes[0,1].plot(diff_unlabeled.T)\naxes[0,1].set_title(\"diff_unlabel\")\n\naxes[1,1].plot(diff_test.T)\naxes[1,1].set_title(\"diff_test\")\n\nfig.tight_layout()\nfig.show()","17c50f29":"#\u7570\u5e38\u5ea6\u306e\u8a08\u7b97\ndef anormaly_score(data, use_dim =10):\n    #\u518d\u69cb\u6210\u8aa4\u5dee\u306e\u9ad8\u3044top use_dim\u306e\u5e73\u5747\u3092\u7570\u5e38\u5ea6\u3068\u3057\u3066\u5b9a\u7fa9\u3059\u308b\n    data = np.sort(np.abs(data), axis=1)[:,::-1]\n    score = data[:,:use_dim].mean(axis=1)\n    \n    return score\n\nscore_normal = anormaly_score(diff_normal)\nscore_anormal = anormaly_score(diff_anormal)\nscore_unlabeled = anormaly_score(diff_unlabeled)\nscore_test = anormaly_score(diff_test)","ed3c04dd":"#\u4f5c\u6210\u3057\u305f\u7570\u5e38\u5ea6\u3092\u53ef\u8996\u5316\u3057\u3066\u6bd4\u8f03\u3057\u307e\u3059\u3002\n#\u4e00\u3064\u306e\u65b9\u306f\u307e\u3042\u307e\u3042\u3067\u3059\u304c\u3001\u3082\u3046\u4e00\u3064\u306f\u5b8c\u5168\u306b\u57cb\u3082\u308c\u3066\u307e\u3059...\u3002\nfig, axes = plt.subplots(2,2, figsize=(10, 5), sharex=True)\naxes[0,0].hist(score_normal)\naxes[0,0].set_title(\"normal\")\n\naxes[1,0].hist(score_anormal)\naxes[1,0].set_title(\"anormal\")\n\naxes[0,1].hist(score_unlabeled)\naxes[0,1].set_title(\"unlabel\")\n\naxes[1,1].hist(score_test)\naxes[1,1].set_title(\"test\")\n\nfig.tight_layout()\nfig.show()","79db2298":"#test score\u30920~1\u306b\u5909\u63db\u3059\u308b\nscore_test_std = (score_test - score_test.min()) \/ (score_test.max() - score_test.min())\nplt.hist(score_test_std)\nplt.show()","2f92421e":"df_sub = pd.read_csv('\/kaggle\/input\/hah-data-science-challenge\/sample_submission.csv', index_col=0)\ndf_sub['Target'] = score_test_std\ndf_sub.to_csv('\/kaggle\/working\/submission.csv')\ndf_sub.head()","336ce865":"## \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406","afd65fff":"\u8f9e\u66f8\u5b66\u7fd2\u306f\u3001\u89b3\u6e2c\u30c7\u30fc\u30bf\u3092\u300c\u8f9e\u66f8(\u57fa\u5e95\u30fb\u30a2\u30c8\u30e0\u3068\u3082)\u300d\u3068\u547c\u3070\u308c\u308b\u5178\u578b\u7684\u306a\u30d1\u30bf\u30fc\u30f3\u306e\u7dda\u578b\u7d50\u5408\u3067\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3001\u3068\u3044\u3046\u4eee\u8aac\u306b\u57fa\u3065\u3044\u3066\u304a\u308a\u307e\u3059\u3002\n\n\u305d\u306e\u305f\u3081\u3001\u6b63\u5e38\u30c7\u30fc\u30bf\u3068\u7570\u5e38\u30c7\u30fc\u30bf\u306b\u542b\u307e\u308c\u308b\u5178\u578b\u7684\u306a\u30d1\u30bf\u30fc\u30f3(\u8f9e\u66f8)\u306b\u9055\u3044\u304c\u3042\u308c\u3070\u3001\u7570\u5e38\u30c7\u30fc\u30bf\u306f\u6b63\u5e38\u30c7\u30fc\u30bf\u306b\u3088\u3063\u3066\u4f5c\u6210\u3057\u305f\u300c\u8f9e\u66f8(\u57fa\u5e95\u30fb\u30a2\u30c8\u30e0\u3068\u3082)\u300d\u306e\u7dda\u578b\u7d50\u5408\u3067\u306f\u8868\u73fe\u3067\u304d\u306a\u3044\u3053\u3068\u304c\u671f\u5f85\u3055\u308c\u3001\u7570\u5e38\u30c7\u30fc\u30bf\u3092\u7121\u7406\u77e2\u7406\u3001\u6b63\u5e38\u30c7\u30fc\u30bf\u306b\u3088\u3063\u3066\u4f5c\u6210\u3057\u305f\u300c\u8f9e\u66f8(\u57fa\u5e95\u30fb\u30a2\u30c8\u30e0\u3068\u3082)\u300d\u306e\u7dda\u578b\u7d50\u5408\u3068\u3057\u3066\u8868\u73fe\u3057\u305f\u5834\u5408\u306f\u6b8b\u5dee\u304c\u5927\u304d\u304f\u306a\u308b\u3053\u3068\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u3002(\u57fa\u672c\u7684\u306a\u8003\u3048\u65b9\u306f\u3001AutoEncoder\u7cfb\u306a\u3069\u3067\u3001\u518d\u69cb\u6210\u8aa4\u5dee\u306b\u3088\u308a\u7570\u5e38\u5224\u5b9a\u3059\u308b\u6642\u3068\u540c\u3058\u3067\u3059\u3002)","d1d2d1b3":"## \u8f9e\u66f8\u5b66\u7fd2\u306b\u3088\u308b\u7570\u5e38\u691c\u77e5\n- \u6700\u8fd1\u30b9\u30d1\u30fc\u30b9\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u4e00\u3064\u3067\u3042\u308b\u8f9e\u66f8\u5b66\u7fd2\u3092\u52c9\u5f37\u3059\u308b\u6a5f\u4f1a\u304c\u3042\u308a\u307e\u3057\u305f\u306e\u3067\u3001 \u5b9f\u969b\u306b\u9069\u7528\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n- \u4eca\u56de\u306e\u554f\u984c\u3067\u306e\u6027\u80fd\u306f\u826f\u304f\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n- \u6027\u80fd\u306f\u3055\u3063\u3071\u308a\u3067\u3057\u305f\u304c\u3001\u307f\u306a\u3055\u3093\u306b\u3054\u89a7\u9802\u304f\u3053\u3068\u3067**\u4f9b\u990a\u3057\u3066\u3044\u305f\u3060\u3051\u308c\u3070**\u3068\u5b58\u3058\u307e\u3059\u3002"}}