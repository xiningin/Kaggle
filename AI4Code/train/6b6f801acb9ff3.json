{"cell_type":{"3436a71a":"code","f82e56f7":"code","699af72d":"code","9ea25012":"code","7170f300":"code","ca23c776":"code","82eb488c":"code","44c1f6af":"code","34a42578":"code","1e51af26":"code","d3488668":"code","c3f02803":"code","f52183c3":"code","516e4b56":"code","f3c01a33":"code","37f9c339":"code","8e812dec":"code","bb38baf0":"code","5240c193":"code","051c68e4":"code","b57a2f47":"code","386ac69f":"code","912b101e":"code","ce1e888b":"code","6f0ef0dd":"code","7799e20b":"code","85225c4e":"code","fa1a5664":"code","9f307b8f":"code","1e388a74":"code","e94c76be":"code","13cbf5b6":"code","900f103f":"code","34c3758e":"code","bc9cbba8":"code","7021256f":"code","4e1d289c":"code","3b94c700":"code","49cde505":"code","c8319c17":"code","39bd17b2":"code","f7404111":"code","16646580":"code","86f5dfb5":"code","428803fd":"code","2c04ddb1":"code","f4d2c6c4":"code","f68178a5":"code","75f1b82a":"code","a008f5d0":"code","9cd1d406":"code","fc09bf1f":"code","c01edf06":"code","2457862d":"code","8631d438":"code","3272c289":"code","c55e2e12":"code","a95812d4":"code","128fd5c9":"code","64a4d031":"code","3602d653":"code","db99a29c":"code","3d409506":"code","828bc693":"code","c4efac12":"code","78043b78":"code","4267bd35":"code","6db820e2":"code","2e4f69a2":"code","3b354fb1":"code","f5107e7a":"code","5f2a7aad":"code","39b0d3d2":"code","3bf2a7ae":"markdown","1f366787":"markdown","0461afe2":"markdown","62d831b8":"markdown","7fc6d520":"markdown","e9994788":"markdown","ec43a2c2":"markdown","5ac06217":"markdown","75aed067":"markdown","800ee2bf":"markdown","40bb7435":"markdown","b88500a9":"markdown","a91dd0a3":"markdown","766dcf3c":"markdown","b2f9f1d6":"markdown","d8897d06":"markdown","4006b55e":"markdown","30958fb8":"markdown","1bf51b54":"markdown","f00b033c":"markdown","6145819b":"markdown","a932d54e":"markdown"},"source":{"3436a71a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f82e56f7":"employee_data = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nemployee_data.head()","699af72d":"# Radomly replace some of the Monthly Income values with nan for our demonstration on fixing nan values\nrandom_replace = 0.1\n\nitem_list = []\nfor item in employee_data['MonthlyRate']:\n    rng = np.random.rand()\n    if rng <= random_replace:\n        item_list.append(np.nan)\n    else:\n        item_list.append(item)\n    \n\nemployee_data['MonthlyRate'] = item_list\nemployee_data['MonthlyRate'].head()","9ea25012":"list(employee_data)","7170f300":"data_types = employee_data.dtypes\ndata_types","ca23c776":"employee_data['MonthlyRate'] = employee_data['MonthlyRate'].astype(int)","82eb488c":"len(employee_data)","44c1f6af":"employee_data.describe().iloc[0]","34a42578":"employee_data['MonthlyRate'][(employee_data['MonthlyRate']).isna()==True].head()","1e51af26":"len(employee_data['MonthlyRate'][(employee_data['MonthlyRate']).isna()==True])","d3488668":"len(employee_data['MonthlyRate'][(employee_data['MonthlyRate']).isna()==True])\/len(employee_data)*100","c3f02803":"employee_data_categorical = employee_data[['Attrition', 'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime']]\nemployee_data_categorical.head()","f52183c3":"data_types[data_types=='object'].index","516e4b56":"employee_data_categorical = employee_data[data_types[data_types=='object'].index]\nemployee_data_categorical.head()","f3c01a33":"len(employee_data_categorical['MaritalStatus'][(employee_data_categorical['MaritalStatus']).isna()==True])","37f9c339":"len(employee_data_categorical['JobRole'][(employee_data_categorical['JobRole']).isna()==True])","8e812dec":"employee_data_categorical_columns = list(employee_data_categorical)\nemployee_data_categorical_columns","bb38baf0":"employee_data_categorical_columns = list(employee_data_categorical)\n\nprint(\"Categorical Features - unique value checks\")\nfor col_id in employee_data_categorical_columns:\n    column = employee_data_categorical[col_id]\n    print(\"-----------------------------\")\n    print(col_id)\n    print(column.unique())\n","5240c193":"employee_data['JobRole'][(employee_data['JobRole']).isna()==True].head()","051c68e4":"len(employee_data['JobRole'][(employee_data['JobRole']).isna()==True])\/len(employee_data)*100","b57a2f47":"employee_data['MaritalStatus'][(employee_data['MaritalStatus']).isna()==True].head()","386ac69f":"len(employee_data['MaritalStatus'][(employee_data['MaritalStatus']).isna()==True])\/len(employee_data)*100","912b101e":"employee_data['MonthlyRate'].iloc[0:20]","ce1e888b":"employee_data_removed_1 = employee_data[employee_data['MonthlyRate'].isna()==False]\nprint(len(employee_data_removed_1))","6f0ef0dd":"employee_data_removed_2 = employee_data.dropna(subset = ['MonthlyRate'])\nprint(len(employee_data_removed_2))","7799e20b":"print(np.round(len(employee_data[employee_data['MonthlyRate'].isna()==True])\/len(employee_data),3)*100, \"%\")","85225c4e":"employee_data_removed_3 = employee_data.drop('MonthlyRate', axis=1)\nlist(employee_data_removed_3)","fa1a5664":"employee_data[['DailyRate','MonthlyRate']].head(20)","9f307b8f":"employee_data_monthlyrate_hourlyrate_div = employee_data['MonthlyRate']\/employee_data['DailyRate']\nemployee_data_monthlyrate_hourlyrate_div.head()","1e388a74":"plt.hist(employee_data_monthlyrate_hourlyrate_div, bins=20)\nplt.title(\"Histogram of the Multiplication Factor for \\n Daily to Monthly Rate\")\nplt.show()","e94c76be":"# fillna() function\nemployee_data_replaced_1 = employee_data.copy()\nemployee_data_replaced_1['MonthlyRate'].fillna(employee_data_replaced_1['DailyRate']*30, inplace=True)\nemployee_data_replaced_1[['DailyRate','MonthlyRate']].head(20)","13cbf5b6":"# np.where() function\nemployee_data_replaced_1 = employee_data.copy()\nemployee_data_replaced_1['MonthlyRate'] = np.where(employee_data_replaced_1['MonthlyRate'].isna()==True,\n                                                       employee_data_replaced_1['DailyRate']*30,\n                                                       employee_data_replaced_1['MonthlyRate'])\nemployee_data_replaced_1[['DailyRate','MonthlyRate']].head(20)","900f103f":"# fillna() function\nemployee_data_replaced_2 = employee_data.copy()\nemployee_data_replaced_2['MonthlyRate'].fillna(0, inplace=True)\nemployee_data_replaced_2[['DailyRate','MonthlyRate']].head(20)","34c3758e":"# np.where() function\nemployee_data_replaced_2 = employee_data.copy()\nemployee_data_replaced_2['MonthlyRate'] = np.where(employee_data_replaced_2['MonthlyRate'].isna()==True,\n                                                       0,\n                                                       employee_data_replaced_2['MonthlyRate'])\nemployee_data_replaced_2[['DailyRate','MonthlyRate']].head(20)","bc9cbba8":"# fillna() function\nemployee_data_replaced_3 = employee_data.copy()\nemployee_data_replaced_3['MonthlyRate'].fillna(employee_data_replaced_3['MonthlyRate'].mean(), inplace=True)\nemployee_data_replaced_3[['DailyRate','MonthlyRate']].head(20)","7021256f":"# np.where() function\nemployee_data_replaced_3 = employee_data.copy()\nemployee_data_replaced_3['MonthlyRate'] = np.where(employee_data_replaced_3['MonthlyRate'].isna()==True,\n                                                       employee_data_replaced_3['MonthlyRate'].mean(),\n                                                       employee_data_replaced_3['MonthlyRate'])\nemployee_data_replaced_3[['DailyRate','MonthlyRate']].head(20)","4e1d289c":"# fillna() function\nemployee_data_replaced_4 = employee_data.copy()\nemployee_data_replaced_4['MonthlyRate'].fillna(employee_data_replaced_4['MonthlyRate'].median(), inplace=True)\nemployee_data_replaced_4[['DailyRate','MonthlyRate']].head(20)","3b94c700":"# np.where() function\nemployee_data_replaced_4 = employee_data.copy()\nemployee_data_replaced_4['MonthlyRate'] = np.where(employee_data_replaced_4['MonthlyRate'].isna()==True,\n                                                       employee_data_replaced_4['MonthlyRate'].median(),\n                                                       employee_data_replaced_4['MonthlyRate'])\nemployee_data_replaced_4[['DailyRate','MonthlyRate']].head(20)","49cde505":"employee_data['Department'].head()","c8319c17":"employee_data['Department'].unique()","39bd17b2":"employee_data_department_monthylratemean = employee_data.groupby('Department').mean()['MonthlyRate']\nemployee_data_department_monthylratemean","f7404111":"employee_data_department_monthylratemean['Sales']","16646580":"employee_data_replaced_5 = employee_data.copy()\nemployee_data_replaced_5['MonthlyRate'] = np.where((employee_data_replaced_5['MonthlyRate'].isna()==True)&(employee_data_replaced_5['Department']=='Sales'),\n                                                    employee_data_department_monthylratemean['Sales'],\n                                            np.where((employee_data_replaced_5['MonthlyRate'].isna()==True)&(employee_data_replaced_5['Department']=='Research & Development'),\n                                                      employee_data_department_monthylratemean['Research & Development'],\n                                                np.where((employee_data_replaced_5['MonthlyRate'].isna()==True)&(employee_data_replaced_5['Department']=='Human Resources'),\n                                                         employee_data_department_monthylratemean['Human Resources'],employee_data_replaced_5['MonthlyRate'])))\n\n\nemployee_data_replaced_5[['Department','MonthlyRate']].head(20)","86f5dfb5":"# REPLACE EACH ITEM IN THE MONTHLYRATE COLUMN WITH THE AVERAGE OF THE DEPARTMENT CLASS USING A FOR LOOP\n#-----------------------------------------------------------------------------------------------------\n# Create a copy of the employee dataset\n# Compute the mean value of the MonthlyRate for each Department class\n#\n# MAIN LOOP:\n#\n# Initialise an empty list for logging results\n# for each index,row in our employee data:\n#     if the MonthlyRate value is not null --> dont change\n#     else:\n#         find the Department class for the row, extract the mean from the previously and replace with this value\n#\n#     store each value into the initialised list with the \".append()\" function\n#\n# replace the MonthlyRate column with this updated list\n#-----------------------------------------------------------------------------------------------------\n\nemployee_data_replaced_6 = employee_data.copy()\n\n# Find the mean results of the MonthlyRate column for each Department class for all known values\nemployee_data_department_monthylratemean = employee_data.groupby('Department').mean()['MonthlyRate']\n\nMonthlyRate_replacement = []\nfor n,row in employee_data_replaced_6.iterrows():\n    if pd.notnull(row['MonthlyRate']):\n        MonthlyRate_value = row['MonthlyRate']\n    else:\n        row_dep_class = row['Department']\n        row_dep_class_mean = employee_data_department_monthylratemean[row_dep_class]\n        MonthlyRate_value = row_dep_class_mean\n    \n    MonthlyRate_replacement.append(MonthlyRate_value)\n\n\nemployee_data_replaced_6['MonthlyRate'] = MonthlyRate_replacement\nemployee_data_replaced_6[['Department','MonthlyRate']].head(20)","428803fd":"list(employee_data)","2c04ddb1":"plt.scatter(employee_data['TotalWorkingYears'], employee_data['MonthlyRate'])\nplt.title(\"Comparison of Total Working Years against \\n Monthly Rate for all Employees\")\nplt.xlabel(\"Total Working Years\")\nplt.ylabel(\"Monthly Rate\")\nplt.show()","f4d2c6c4":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_ols.html\nfrom sklearn import linear_model","f68178a5":"employee_data_replaced_7 = employee_data.copy()\n\nemployee_data_7_NAN = employee_data_replaced_7[employee_data_replaced_7['MonthlyRate'].isna()==True]\nemployee_data_7 = employee_data_replaced_7[employee_data_replaced_7['MonthlyRate'].isna()==False]\n\nX = employee_data_7[['TotalWorkingYears']]\ny = employee_data_7[['MonthlyRate']]\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X, y)\n\n# The correlation coefficient\nprint('Coefficients: \\n', regr.coef_)\n\n# The intercept\nprint('Intercept: \\n', regr.intercept_)\n","75f1b82a":"plt.scatter(employee_data['TotalWorkingYears'], employee_data['MonthlyRate'])\nplt.plot(X, regr.predict(X),color='r')\nplt.title(\"Comparison of Total Working Years against \\n Monthly Rate for all Employees\")\nplt.xlabel(\"Total Working Years\")\nplt.ylabel(\"Monthly Rate\")\nplt.show()","a008f5d0":"# Make predictions using the testing set\nnan_pred = regr.predict(employee_data_7_NAN[['TotalWorkingYears']])\nnan_pred[0:10]","9cd1d406":"employee_data_replaced_7 = employee_data.copy()\n\nMonthlyRate_replacement = []\nfor n,row in employee_data_replaced_7.iterrows():\n    if pd.notnull(row['MonthlyRate']):\n        MonthlyRate_value = row['MonthlyRate']\n    else:\n        regr_pred = regr.predict([row[['TotalWorkingYears']]])\n        MonthlyRate_value = regr_pred[0][0]\n    \n    MonthlyRate_replacement.append(MonthlyRate_value)\n\n\nemployee_data_replaced_7['MonthlyRate'] = MonthlyRate_replacement\n\nemployee_data_replaced_7[['TotalWorkingYears','MonthlyRate']].head(20)","fc09bf1f":"print(np.round(len(employee_data['JobRole'][(employee_data['JobRole']).isna()==True])\/len(employee_data)*100,2),\"%\")","c01edf06":"print(np.round(len(employee_data['MaritalStatus'][(employee_data['MaritalStatus']).isna()==True])\/len(employee_data)*100,2),\"%\")","2457862d":"employee_data_removed_8 = employee_data.copy()\nemployee_data_removed_8 = employee_data_removed_8.drop('MaritalStatus',axis=1)\nlist(employee_data_removed_8)","8631d438":"employee_data_removed_8['JobRole'].mode()[0]","3272c289":"# fillna() function\nemployee_data_removed_9 = employee_data.copy()\nemployee_data_removed_9['JobRole'].fillna(employee_data_removed_9['JobRole'].mode()[0], inplace=True)\nemployee_data_removed_9[['JobRole']].head(20)","c55e2e12":"# np.where() function\nemployee_data_removed_9 = employee_data.copy()\nemployee_data_removed_9['JobRole'] = np.where(employee_data_removed_9['JobRole'].isna()==True,\n                                                       employee_data_removed_9['JobRole'].mode(),\n                                                       employee_data_removed_9['JobRole'])\nemployee_data_removed_9[['JobRole']].head(20)","a95812d4":"employee_data_clean = employee_data.copy()\n\nemployee_data_clean['MonthlyRate'].fillna(employee_data_clean['MonthlyRate'].mean(), inplace=True)\nemployee_data_clean = employee_data_clean.drop('MaritalStatus',axis=1)\nemployee_data_clean['JobRole'].fillna(employee_data_clean['JobRole'].mode()[0], inplace=True)\n\n#Final check for any null values, if false then there are none :)\nemployee_data_clean.isnull().values.any()","128fd5c9":"employee_data_clean.head(10)","64a4d031":"employee_data_clean['MonthlyRate'] = employee_data_clean['MonthlyRate'].astype(np.int64)\ndata_types_clean = employee_data_clean.dtypes\ndata_types_clean","3602d653":"import statistics \n\nx = employee_data_clean['MonthlyIncome']\n# mean and stdev\nmu = employee_data_clean['MonthlyIncome'].mean()\nsigma = statistics.stdev(employee_data_clean['MonthlyIncome'])\n\nnum_bins = 50\n\nfig, ax = plt.subplots()\n\n# the histogram of the data\nn, bins, patches = ax.hist(x, num_bins, density=1)\n\n# add a 'best fit' line\ny = ((1 \/ (np.sqrt(2 * np.pi) * sigma)) *\n     np.exp(-0.5 * (1 \/ sigma * (bins - mu))**2))\nax.plot(bins, y, 'r--')\nax.set_xlabel('MonthlyRate')\nax.set_ylabel('Probability density')\nax.set_title(r'Histogram of MonthlyRate for 50 bins')\n\n# Tweak spacing to prevent clipping of ylabel\nfig.tight_layout()\nplt.show()","db99a29c":"# compute value for outlier cutoff points\nmu + 3*sigma","3d409506":"print(np.round(len(employee_data_clean[employee_data_clean['MonthlyIncome']>=(mu + 3*sigma)])\/len(employee_data_clean)*100,5), \"%\")","828bc693":"print(np.round(len(employee_data_clean[employee_data_clean['MonthlyIncome']<=(mu - 3*sigma)])\/len(employee_data_clean)*100,5), \"%\")\n","c4efac12":"ax = sns.boxplot(x=employee_data_clean['MonthlyIncome'])\nplt.title(\"Simple Box Plot of MonthlyIncome to find Outliers\")\nplt.show()","78043b78":"employee_data_clean_outliers = employee_data_clean[employee_data_clean['MonthlyIncome']<17000]\nprint(\"Mean Monthly Income BEFORE removing outliers = \", employee_data_clean['MonthlyIncome'].mean())\nprint(\"Mean Monthly Income AFTER removing outliers = \", employee_data_clean_outliers['MonthlyIncome'].mean())","4267bd35":"plt.scatter(employee_data_clean_outliers['MonthlyIncome'],employee_data_clean_outliers['MonthlyRate'])\nplt.title(\"Comparison of Monthly Income against Monthly Rate\")\nplt.xlabel(\"Monthly Income\")\nplt.ylabel(\"Monthly Rate\")\nplt.show()","6db820e2":"plt.scatter(employee_data_clean_outliers['MonthlyIncome'],employee_data_clean_outliers['MonthlyRate'])\nplt.title(\"Comparison of Monthly Income against Monthly Rate (log scale)\")\nplt.xlabel(\"Monthly Income\")\nplt.ylabel(\"Monthly Rate (log scale)\")\n\nplt.yscale(\"log\")\n\nplt.show()","2e4f69a2":"plt.scatter(employee_data_clean_outliers['MonthlyIncome'],employee_data_clean_outliers['MonthlyRate'])\nplt.title(\"Comparison of Monthly Income (log scale) against Monthly Rate\")\nplt.xlabel(\"Monthly Income (log scale)\")\nplt.ylabel(\"Monthly Rate\")\n\nplt.xscale(\"log\")\n\nplt.show()","3b354fb1":"plt.scatter(employee_data_clean_outliers['MonthlyIncome'],employee_data_clean_outliers['MonthlyRate'])\nplt.title(\"Comparison of Monthly Income (log scale) against Monthly Rate (log scale)\")\nplt.xlabel(\"Monthly Income (log scale)\")\nplt.ylabel(\"Monthly Rate (log scale)\")\n\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n\nplt.show()","f5107e7a":"plt.hist(employee_data_clean_outliers['HourlyRate'], alpha=0.5)\nplt.hist(employee_data_clean_outliers['MonthlyIncome'], alpha=0.5)\nplt.title(\"Histogram comparison between the \\n Monthly Income and Hourly Rate\")\nplt.show()","5f2a7aad":"employee_data_clean_outliers['HourlyRate_norm'] = ((employee_data_clean_outliers['HourlyRate']-min(employee_data_clean_outliers['HourlyRate']))\/          \n                                            (max(employee_data_clean_outliers['HourlyRate'])-min(employee_data_clean_outliers['HourlyRate'])))\nemployee_data_clean_outliers['MonthlyIncome_norm'] = ((employee_data_clean_outliers['MonthlyIncome']-min(employee_data_clean_outliers['MonthlyIncome']))\/          \n                                            (max(employee_data_clean_outliers['MonthlyIncome'])-min(employee_data_clean_outliers['MonthlyIncome'])))\n","39b0d3d2":"plt.hist(employee_data_clean_outliers['HourlyRate_norm'], alpha=0.5)\nplt.hist(employee_data_clean_outliers['MonthlyIncome_norm'], alpha=0.5)\nplt.title(\"Normalised Histogram comparison between the \\n Monthly Income and Hourly Rate\")\nplt.show()","3bf2a7ae":"### Final Removed and Replaced Dataset\n\n1. MonthlyRate replaced with mean value\n2. MaritalStatus removed feature\n3. JobRole replaced with mode class","1f366787":"# Transforming Variables\n\n### Logarithm Scales for Visualisations","0461afe2":"## Change Data Types\n\nWe note that the MonthlyRate feature is the only float column and this appears to be incorrect. Therefore, we can change this to an integer column to align with the other features. \n\nHowever, we recieve and error because it appears we have nan values in this column and so need to first validate if missing\/null values exist in the dataset. \n","62d831b8":"### Visual Computation\n\nAlternatively, we can use the visual box plot outlier detection and deduce that any point above, say 17,000 are outliers.","7fc6d520":"#### 6. Use an average value for each class to replace with \u2192 e.g. mean for age group 18-24\n\nA little more complicated, we use a logical categorical feature's clases a more sophisticated indication of what the missing values should be. For example, we can use the 'Department' feature as a better indicator. \n\nTo calculate the statistics by classes, we use the 'groupby()' function.\n\nContinuing from our previous replacement method, we extend the np.where function and can nest if-else statements for a manual replacement.\n\n**Advanced:** Because this is a very manual replacement, we can utilise a loop function as before to improve the speed at which we replace values and is particularly important if we had more classes. Because this is fairly complex, I have done my best to summarise the process in the comments but you may wish to find an alternative method for achieving the same result if you are more comfortable with programming.","e9994788":"---\n\n## Conclusion\n\nIn this notebook, we have successfully demonstrated how to complete the majority of pre-processing steps in Python. This includes checking for and removing missing\/null values and outliers. The final pre-processing steps will be covered in later modules.\n\nIt is important to understand that pre-processing often requires a good understanding of the data to make decisions. Often data available publicly will already be clean but it is important to check before applying any Machine Learning or Visual Analytics. \n","ec43a2c2":"### Normlisation for Improved Comparison\n\nWe use normalisation when variables to compare are on vastly different scales. For example, HourlyRate and MonthlyIncome would otherwise be incomparable.","5ac06217":"# Import and Summarise Data","75aed067":"# Other Methods\n\n### Feature Reduction\n\n#### Manual Removal\n\nWe have covered this previously when we removed the \"MaritalStatus\" feature.\n\n#### Feature selection\n\nWill cover in Machine Learning.\n\n#### PCA merging features\n\nWill cover in Machine Learning.\n\n\n### Data Split into Train\/Test Subsets\n\nWill cover in Machine Learning.\n\n","800ee2bf":"\n![banner](https:\/\/i.imgur.com\/vmCCvUB.png \"https:\/\/www.notion.so\/sterlingdatascience\")\n## Data Science MSc - DATA101: Principles of Data Science\n# DATA101: Cleaning and Preparing Data\n\n## About\n\nThis is the code notebook for the \"Cleaning and Preparing Data\" of Sterling Osborne's Data Science MSc online course found here:\n\n\n### Contents:\n1. Missing\/Null Values\n    - Identify Missing Continuous Values\n    - Identify Missing Categorical Values\n    - Fix Missing Continuous Values\n    - Fix Missing Categorical Values\n2. Outlier Values\n    - Check for and Remove Outlier Values \n3. Transforming Variables\n    - Log Scale for Visualistions\n    - Normalization\n4. Other Methods\n    - Feature Reduction\n    - Data Split into Train\/Test Subsets\n5. Conclusion\n\nCode Notebook by Sterling Osborne\n\nOctober 2019\n\nhttps:\/\/twitter.com\/DataOsborne\n\n---\n\n\n---\n","40bb7435":"#### 4. Use a standard value to replace with \u2192 e.g. \"unknown\", works well if only a few for categorical features\n\nSimilarly, we could just replace missing values with 0.","b88500a9":"#### 1. Remove the entry \u2192 delete row\n\nWe can either subset the dataset for rows with na in the MonthlyRate column or we can use pandas pre-built function \"dropna()\" https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.dropna.html.","a91dd0a3":"#### 2. Remove the feature \u2192 delete column if lots of missing values\n\nFirst, we check how many rows in the MonthlyRate column are missing, we find this to be approximately 9.2% and is not sufficient to simply remove to column.\n\nHowever, we show how to drop a single column in case this is more significant in your case.","766dcf3c":"#### 7. Use algorithm output values to replace with \u2192 e.g. regression or decision tree predictions \n\nOur features do not correlate well in a simple linear regression so would not be advised but can show how this would work. First we use the sklearn package to fit a linear regression line against the known values for \"TotalWorkingYears\" and \"MonthlyRate\". With this, we predict what we think values should be given we know the \"TotalWorkingYears\" for each missing \"MonthlyRate\" value.\n\nThe previous \"fillna()\" and \"np.where()\" replacement functions do now work in this case and so have had to use the more complex for loop method as shown before. \n","b2f9f1d6":"#### 3. Fill in value manually \u2192 requires strong justification for chosen values and time consuming\n\nThis is not often advised but in some cases, we can deduce what the missing values should be based on a link to another column. For example, it seems logical that MonthlyRate could be linked to DailyRate.\n\nWe therefore estimate the multiplication value based on known values.\n\nWe find that this multiplication factor aligns with the fact that there are 28-31 days in a month. Therefore, we could replace missing values in the MonthlyRate with the result of $MonthlyRate = DailyRate * 30$.\n\nIn order to replace just the missing values, we use either the fillna() pandas function or the numpy function 'np.where' that works similar to the Excel 'if()' function https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.where.html.","d8897d06":"---\n\n# Missing\/Null Values\n\n## Check for Missing\/Null Values\n\n### Continuous Variables\n\nFor continuous variables we can use the describe function to return the count result for each feature and compare to the length of the dataset. We find that only the \"MonthlyRate\" column has missing\/null values.","4006b55e":"### Categorical Variables\n\nUnlike for continuous variables, we have fewer replacement choices for categorical variables. As before, we can simply remove the row or column, add an improved class label (e.g. \"unknown\") or replace with the mode.\n\nThe first thing to consider is that the MaritalStatus feature has almost 26% of its values missing. Furthermore, this may be considered \"personal information\" that has no relevancy to making predictions or may be unethical to make decisions on. Therefore, we can make a solid justification for removing this feature completely. ","30958fb8":"#### 5. Use an average value to replace with \u2192 mean, median and mode, works well if only a few for continuous features\n\nAgain, we can very easily replace with the mean and median values (mode is advised for categorical features).\n","1bf51b54":"### Categorical Variables\n\nFor categorical variables, we first select only the relevant columns. This can be done either manually as shown in the previous lecture or by applying a more systematic approach. \n\n\nMore specifically, a useful habit to utilise when programming is to extract lists of information and subset on these. For example, instead of manually sub-setting the original data just for categorical features, we can extract the list of categorical features from the '.dtypes' function and use this directly for our column selection as shown in the comparison images below. In short, we:\n\n1. Subset the '.dtpes' result only for 'object' columns\n2. Take the index of these (row names of a list) that correspond to the column headers\n3. Use this index list as the column selection for sub-setting the original data for categorical features only\n\nWith this completed, we can again check the columns manually or apply a simple loop function to systematically check each.\n\n\n**Your first loop!**\n\nUnlike with continuous variables, we do not have a simple function that will provide the information for all columns on null values needed. Instead, we need to perform a more manual check. This could mean that we go through each column one-by-one with our is null check but this is laborious and not needed. \n\nInstead, we introduce an important feature of programming: **iterative loops**. [Pythonforbeginners.com](https:\/\/www.pythonforbeginners.com\/loops\/for-while-and-nested-loops-in-python) provide a gentle introduction to loops in Python and will summarise how the \"for loop\" function works by the following:\n\n> The for loop that is used to iterate over elements of a sequence, it is often used when you have a piece of code which you want to repeat \"n\" number of time. \n> It works like this: \" for all elements in a list, do this \"\n\nFor loop summary:\n\n1. *Generate list of categorical feature column names*\n2. ***Iterate** over this list where each step is a column name*\n3. *At each step, create a variable only for this column*\n4. ***Return**:* \n    1. The column name\n    2. The list of unique values for that column\n5. *Format results with some extra print functions*\n\n\nWe confirm with this simple print loop that  \"JobRole\" and \"MaritalStatus\" are the only categorical features with missing\/null values.","f00b033c":"---\n\n# Check and Fix Outlier Values\n\n### Numerical Computation\n\nIf you have any data point that is more than 3 times the standard deviation, then those points are very likely to be anomalous or outliers.\n\nBased on this, we find that MonthlyIncome appears to have no clear outliers.","6145819b":"## Change Data Types - Re-visited\n\nNow that missing values have been fixed, we can attempt to change the MonthlyRate data type to align with the other features.\n","a932d54e":"---\n\n# Fixing Missing Values\n### Continuous Variables"}}