{"cell_type":{"b7306b9c":"code","3c5ef7d5":"code","1964ea0d":"code","499f49a1":"code","c9c60717":"code","e8343daf":"code","e4f41d63":"code","19fd24b2":"code","5eea59c0":"code","742a9591":"code","c79bf090":"code","9fe7e752":"code","83600bba":"code","c4c46a5a":"code","6d35930a":"code","66a92108":"code","1be33766":"code","71ea4f93":"code","ad442247":"code","f75cc3e1":"code","78b85c2c":"code","01c50e0b":"code","bf4622bf":"code","cfaebe1c":"code","0125ba52":"code","586284a3":"code","d8060423":"code","ef41fd3d":"code","bea576c3":"code","631c4581":"code","ccb0c6f9":"code","ed93c554":"code","3e08c27e":"code","d695a49a":"code","7835c074":"code","aa277beb":"code","46cd0e1d":"code","8ff24666":"code","2c8049c7":"code","b6cbe9b2":"code","813f4340":"code","4c157246":"code","eb95f275":"code","d476a59b":"code","15170e19":"code","2108d2ab":"code","8913190b":"code","ed86c46e":"code","5af4b5ed":"code","41f7629f":"code","09a88cfc":"code","d3729b3a":"code","bfca2e77":"code","161227fa":"code","e3672829":"code","92cba455":"code","42315e23":"code","4739c30a":"code","f3b25561":"code","5c8f8c31":"code","ec43db0b":"code","1eeb7c5e":"code","4cf61ae5":"code","06105e09":"code","3e292ec5":"code","1c95233f":"code","cb919079":"code","94a5042b":"code","709d4dc8":"code","2dde0469":"code","9129f6f7":"code","be0652ea":"code","c0779354":"code","c4b3cd62":"code","291e7751":"code","e739d7ea":"code","651cba90":"code","1dd12480":"code","ad9cd5f0":"code","bd622edf":"code","48b37c0a":"code","cacbf830":"code","576c6a4b":"code","1d3adb63":"code","4f9f7143":"code","2d6fc2c1":"code","2d0e6e72":"code","efca34f7":"code","0565c47b":"code","f20aae35":"code","ab775304":"code","e698f828":"code","ea3213b9":"code","fd775cad":"code","7fafd79a":"code","27c56880":"code","31ae9af7":"code","3d0cf575":"code","2136f4dc":"code","a9222ad4":"code","9e74c695":"code","a61dfe2f":"code","649c3f18":"code","b4cf5d68":"code","e39176a6":"code","1e7f04f6":"code","ecee2a81":"code","ee1cc40e":"code","6b26dea2":"code","8ab7b518":"code","d0930660":"code","56554f9d":"code","57c846cb":"code","4857c84e":"code","3a00e647":"markdown","a29f011b":"markdown","c6e835d4":"markdown","ab45d904":"markdown","bdf70562":"markdown","15987ebb":"markdown","0d1d7683":"markdown","13012167":"markdown","d23e15a3":"markdown","0cd5d781":"markdown","f975e9f1":"markdown","1d7c4861":"markdown","67b5d3fe":"markdown","c4ec48af":"markdown","c03feafe":"markdown","3b879fa0":"markdown","1012f118":"markdown","4fc6a2ab":"markdown","60af56a2":"markdown","d93c2551":"markdown","a92225b6":"markdown","aaef9b92":"markdown","a3536f95":"markdown","635cda3e":"markdown","6dadcee3":"markdown","02a004e0":"markdown","0689cf76":"markdown","2ac68630":"markdown","90941277":"markdown","dfbd92f4":"markdown","dcb092cd":"markdown","b4cf18e4":"markdown","f333273e":"markdown","51ed1296":"markdown","87b3a58a":"markdown","d8728332":"markdown","362f696a":"markdown","d4131cc8":"markdown","fb3513d3":"markdown","da71f2f8":"markdown","899cd75a":"markdown","d45e3548":"markdown","61756ac4":"markdown","7e4e9c91":"markdown","8deeff47":"markdown","5c75c96f":"markdown","f7e5d17f":"markdown"},"source":{"b7306b9c":"\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nimport seaborn as sns\n%matplotlib inline\nimport missingno as msno\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n# to display all columns:\npd.set_option('display.max_columns', None)\nfrom sklearn.model_selection import train_test_split, GridSearchCV","3c5ef7d5":"# Read train and test data with pd.read_csv():\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","1964ea0d":"# copy data in order to avoid any change in the original:\ntrain = train_data.copy()\ntest = test_data.copy()","499f49a1":"train.head()","c9c60717":"train.tail()","e8343daf":"test.head()","e4f41d63":"train.shape","19fd24b2":"test.shape","5eea59c0":"train.ndim","742a9591":"test.ndim","c79bf090":"train.describe().T","9fe7e752":"test.describe().T","83600bba":"train.columns","c4c46a5a":"test.columns","6d35930a":"test.dtypes","66a92108":"train.dtypes","1be33766":"train.info()\n","71ea4f93":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \ng = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","ad442247":"train[\"Age\"].describe()","f75cc3e1":"train[\"Age\"].value_counts()","78b85c2c":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","01c50e0b":"train[\"SibSp\"].value_counts()","bf4622bf":"train[\"SibSp\"].value_counts().plot.barh();","cfaebe1c":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=train);","0125ba52":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","586284a3":"train[\"Parch\"].value_counts()","d8060423":"sns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","ef41fd3d":"train[\"Fare\"].value_counts()","bea576c3":"sns.boxplot(x = train['Fare']);","631c4581":"Q1 = train['Fare'].quantile(0.25)\nQ3 = train['Fare'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_limit = Q1- 1.5*IQR\nprint(lower_limit)\n\nupper_limit = Q3 + 1.5*IQR\nupper_limit","ccb0c6f9":"train[\"Survived\"].value_counts()","ed93c554":"train[\"Survived\"].value_counts().plot.barh();","3e08c27e":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d695a49a":"train[\"Sex\"].value_counts()","7835c074":"train[\"Sex\"].value_counts().plot.barh();","aa277beb":"sns.catplot(x = \"Sex\", y = \"Age\", hue= \"Survived\",data = train);","46cd0e1d":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train)","8ff24666":"train[\"Pclass\"].value_counts()","2c8049c7":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b6cbe9b2":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train);","813f4340":"print(\"Pclass Percantage = 1  survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Pclass Percantage = 2  survived :\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Pclass Percantage = 3 survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","4c157246":"train[\"Embarked\"].value_counts()","eb95f275":"g = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","d476a59b":"g = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train,\n                   size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","15170e19":"train[\"Cabin\"].value_counts()","2108d2ab":"train.isnull().sum()","8913190b":"g = sns.factorplot(y=\"Age\",x=\"Sex\",data=train,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=train,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=train,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=train,kind=\"box\")","ed86c46e":"## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(train[\"Age\"][train[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = train[\"Age\"].median()\n    age_pred = train[\"Age\"][((train['SibSp'] == train.iloc[i][\"SibSp\"]) & (train['Parch'] == train.iloc[i][\"Parch\"]) & (train['Pclass'] == train.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        train['Age'].iloc[i] = age_pred\n    else :\n        train['Age'].iloc[i] = age_med","5af4b5ed":"index_NaN_age = list(test[\"Age\"][test[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = test[\"Age\"].median()\n    age_pred = test[\"Age\"][((test['SibSp'] == test.iloc[i][\"SibSp\"]) & (test['Parch'] == test.iloc[i][\"Parch\"]) & (test['Pclass'] == test.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        test['Age'].iloc[i] = age_pred\n    else :\n        test['Age'].iloc[i] = age_med","41f7629f":"train.isnull().sum()","09a88cfc":"\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\ntest[\"Embarked\"] = test[\"Embarked\"].fillna(\"S\")","d3729b3a":"train.isnull().sum()","bfca2e77":"test.isnull().sum()","161227fa":"test[test[\"Fare\"].isnull()]","e3672829":"test[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\").mean()","92cba455":"test[\"Fare\"] = test[\"Fare\"].fillna(12)","42315e23":"test[\"Fare\"].isnull().sum()","4739c30a":"train[\"Yeni_cabin\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"Yeni_Cabin\"] = (test[\"Cabin\"].notnull().astype('int'))\nprint(\"Percentage of Yeni_cabin = 1 who survived:\", train[\"Survived\"][train[\"Yeni_cabin\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Yeni_cabin = 0 who survived:\", train[\"Survived\"][train[\"Yeni_cabin\"] == 0].value_counts(normalize = True)[1]*100)\n\nsns.barplot(x=\"Yeni_cabin\", y=\"Survived\", data=train)\n\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)\n","f3b25561":"train.head()","5c8f8c31":"train.isnull().sum()","ec43db0b":"test.isnull().sum()","1eeb7c5e":"train = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)","4cf61ae5":"train.head()","06105e09":"# convert Sex into categorical value 0 for male and 1 for female\ntrain[\"Sex\"] = train[\"Sex\"].map({\"male\": 0, \"female\":1})\ntest[\"Sex\"] = test[\"Sex\"].map({\"male\": 0, \"female\":1})\n","3e292ec5":"train.head()","1c95233f":"train[\"unvan\"] = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest[\"unvan\"] = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)","cb919079":"train['unvan'] = train['unvan'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntrain['unvan'] = train['unvan'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntrain['unvan'] = train['unvan'].replace('Mlle', 'Miss')\ntrain['unvan'] = train['unvan'].replace('Ms', 'Miss')\ntrain['unvan'] = train['unvan'].replace('Mme', 'Mrs')","94a5042b":"test['unvan'] = test['unvan'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntest['unvan'] = test['unvan'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntest['unvan'] = test['unvan'].replace('Mlle', 'Miss')\ntest['unvan'] = test['unvan'].replace('Ms', 'Miss')\ntest['unvan'] = test['unvan'].replace('Mme', 'Mrs')","709d4dc8":"train[['unvan', 'Survived']].groupby(['unvan'], as_index=False).mean()","2dde0469":"# Map each of the unvan groups to a numerical value\n\nunvan_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 5}\n\ntrain['unvan'] = train['unvan'].map(unvan_mapping)","9129f6f7":"test['unvan'] = test['unvan'].map(unvan_mapping)","be0652ea":"train.head()","c0779354":"test.head()","c4b3cd62":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","291e7751":"train.head()","e739d7ea":"# Map Fare values into groups of numerical values:\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])","651cba90":"# Drop Fare values:\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","1dd12480":"train.head()","ad9cd5f0":"bins = [0, 5, 12, 18, 24, 35, 60, np.inf]\nmylabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = mylabels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = mylabels)","bd622edf":"# Map each Age value to a numerical value:\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)","48b37c0a":"#dropping the Age feature for now, might change:\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","cacbf830":"train.head()","576c6a4b":"# 5.Feature Engineering","1d3adb63":"### Embarked & Title\n# Convert Title and Embarked into dummy variables:\n\ntrain = pd.get_dummies(train, columns = [\"unvan\"])\ntrain = pd.get_dummies(train, columns = [\"Embarked\"], prefix=\"Em\")\ntrain.head()","4f9f7143":"test = pd.get_dummies(test, columns = [\"unvan\"])\ntest = pd.get_dummies(test, columns = [\"Embarked\"], prefix=\"Em\")\ntest.head()","2d6fc2c1":"\n# Create categorical values for Pclass:\ntrain[\"Pclass\"] = train[\"Pclass\"].astype(\"category\")\ntrain = pd.get_dummies(train, columns = [\"Pclass\"],prefix=\"Pc\")\ntest[\"Pclass\"] = test[\"Pclass\"].astype(\"category\")\ntest = pd.get_dummies(test, columns = [\"Pclass\"],prefix=\"Pc\")\ntrain.head()","2d0e6e72":"test.head()","efca34f7":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.20, random_state = 0)\ntrain","0565c47b":"x_train.shape","f20aae35":"x_test.shape","ab775304":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","e698f828":"xgb_params = {\n        'n_estimators': [200, 500],\n        'subsample': [0.6, 1.0],\n        'max_depth': [2,5,8],\n        'learning_rate': [0.1,0.01,0.02],\n        \"min_samples_split\": [2,5,10]}","ea3213b9":"xgb = GradientBoostingClassifier()\n\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)","fd775cad":"xgb_cv_model.fit(x_train, y_train)","7fafd79a":"xgb_cv_model.best_params_","27c56880":"xgb = GradientBoostingClassifier(learning_rate = xgb_cv_model.best_params_[\"learning_rate\"], \n                    max_depth = xgb_cv_model.best_params_[\"max_depth\"],\n                    min_samples_split = xgb_cv_model.best_params_[\"min_samples_split\"],\n                    n_estimators = xgb_cv_model.best_params_[\"n_estimators\"],\n                    subsample = xgb_cv_model.best_params_[\"subsample\"])","31ae9af7":"xgb_tuned =  xgb.fit(x_train,y_train)","3d0cf575":"y_pred = xgb_tuned.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","2136f4dc":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression() \nlogreg.fit(x_train, y_train) \ny_pred = logreg.predict(x_test) \nacc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2) \nprint(acc_logreg)","a9222ad4":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","9e74c695":"!pip install lightgbm","a61dfe2f":"from lightgbm import LGBMRegressor","649c3f18":"lgb_model = LGBMRegressor().fit(x_train, y_train)","b4cf5d68":"lgb_model","e39176a6":"y_pred = lgb_model.predict(x_test)","1e7f04f6":"lgbm_params = {\"learning_rate\": [0.01, 0.1, 0.5, 1],\n              \"n_estimators\": [20,40,100,200,500,1000],\n              \"max_depth\": [1,2,3,4,5,6,7,8,9,10]}","ecee2a81":"lgbm_cv_model = GridSearchCV(lgb_model, \n                             lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose =2).fit(x_train, y_train)","ee1cc40e":"lgbm_cv_model.best_params_","6b26dea2":"lgbm_tuned = LGBMRegressor(learning_rate = 0.1, \n                          max_depth = 6, \n                          n_estimators = 20).fit(x_train, y_train)","8ab7b518":"y_pred = lgbm_tuned.predict(x_test)","d0930660":"np.sqrt(mean_squared_error(y_test, y_pred))","56554f9d":"test","57c846cb":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = xgb_tuned.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","4857c84e":"output.head()","3a00e647":"## 6.3 Logistic Regression","a29f011b":"## 6.1 Spliting the train data","c6e835d4":"#### In boxplot, there are too many outlier data; we can not change all. Just repress the highest value -512","ab45d904":"#### Indeed, the third class is the most frequent for passenger coming from Southampton (S) and Queenstown (Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n\n#### At this point, i can't explain why first class has an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence.","bdf70562":"## 2.2.5 Cabin","15987ebb":"### 2.1.2 SibSp","0d1d7683":"#### It is clearly obvious that Male have less chance to survive than Female.\n\n#### So Sex, might play an important role in the prediction of the survival.\n\n#### For those who have seen the Titanic movie (1997), I am sure, we all remember this sentence during the evacuation : \"Women and children first\".","13012167":"This is my first kernel at Kaggle. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. Firstly, I will display some feature analyses then will focus on the feature engineering. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure.\n\nThis script follows three main parts:\n\n1.Feature analysis\n2.Feature engineering\n3.Modeling\n","d23e15a3":" ### 3.4 Cabin","0cd5d781":"#### It seems that very young passengers have more chance to survive. \n\n","f975e9f1":"## 1.2 Load and check data","1d7c4861":"#### It seems that passenger coming from Cherbourg (C) have more chance to survive.\n\n#### My hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).\n\n#### Let's see the Pclass distribution vs Embarked","67b5d3fe":" # 2.2 Categorical values","c4ec48af":"## 4.4 AgeGroup","c03feafe":"# 1 Introduction\n## 1.1 Load and check data\n# 2 Data Visualization\n## 2.1 Numerical values\n### 2.1.1 Age\n### 2.1.2 SibSp\n### 2.1.3 Parch\n### 2.1.4 Fare\n## 2.2 Categorical values\n### 2.2.1 Survived\n### 2.2.2 Sex\n### 2.2.3 Pclass\n### 2.2.4 Embarked\n### 2.2.5 Cabin\n# 3. Filling Missing Values\n## 3.1 Age\n## 3.2 Embarked\n## 3.3 Test \"Fare\"\n## 3.4 Cabin\n## 3.5 Ticket\n# 4.Variable Transformation\n## 4.1 Sex\n## 4.2 Name\n## 4.3 Fare\n## 4.4 AgeGroup\n# 5.Feature Engineering\n## 5.1 Embarked & unvan\n## 5.2 Pclass \n#  6.Modeling, Evaluation and Model Tuning\n## 6.1 Spliting the train data\n## 6.2 Gradient Boosting Classifier\n#  7.Deployment\n","3b879fa0":"#### It seems that passengers having a lot of siblings\/spouses have less chance to survive\n\n#### Single passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive","1012f118":"## 6.2 Gradient Boosting Classifier","4fc6a2ab":"# 6.Modeling, Evaluation and Model Tuning","60af56a2":"### 3.5 Ticket","d93c2551":"## 2.2.1 Survived","a92225b6":"# 4.Variable Transformation","aaef9b92":"### 3.2 Embarked","a3536f95":"#### The passenger survival is not the same in the 3 classes. \n#### First class passengers have more chance to survive than second class and third class passengers.","635cda3e":"### 2.1.3 Parch","6dadcee3":"## 2.2.4 Embarked","02a004e0":"# 2. Feature analysis","0689cf76":"### 3.1 Age","2ac68630":"## 5.2 Pclass \n","90941277":"# 3. Filling Missing Values\n","dfbd92f4":"* #  TITANIC RESULTS ...","dcb092cd":"# 1 Introduction","b4cf18e4":"## 4.3 Fare","f333273e":"## 2.2.3 Pclass","51ed1296":"#### But we can see that passengers with a cabin have generally more chance to survive than passengers without cabin.\n\n","87b3a58a":"## 4.1 Sex","d8728332":"## 2.1.4 Fare","362f696a":"## 2.1 Numerical values\n","d4131cc8":" ### 2.1.1 Age","fb3513d3":"# 7.Deployment","da71f2f8":"## 5.1 Embarked & unvan\n","899cd75a":"## 6.4 Random Forest","d45e3548":"## 2.2.2 Sex","61756ac4":"## 4.2 Name","7e4e9c91":"#### Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).\n\n#### Be carefull there is an important standard deviation in the survival of passengers with 3 parents\/children\n\n","8deeff47":"## 1.1 Importing Librarires ","5c75c96f":"### 3.3 Test \"Fare\"","f7e5d17f":"##### Fill Embarked nan values of dataset set with 'S' most frequent value"}}