{"cell_type":{"263257c2":"code","551b3716":"code","94f6cfd9":"code","544ec097":"code","19884842":"code","043c1a7e":"code","0b0dec08":"code","a10e8c77":"code","d68a2adc":"code","6ffd11a2":"code","b6b72d1d":"code","69da203c":"code","41416827":"code","16d39f28":"code","545488fa":"code","bc868bf3":"code","3ba35f5d":"code","7173a71b":"code","9757ab1c":"code","6df50321":"code","184ec1cb":"code","4e59bd5d":"code","d00c99a4":"code","ab506f14":"code","1a62e35b":"code","e5c45026":"code","e61ff32d":"code","86e2df89":"code","ce8a7b7e":"code","7f2bee24":"code","52e0c16d":"code","adefcb64":"code","c41c2f34":"code","ae2f59f6":"code","884d8165":"code","9d227b0c":"code","aecd09e3":"code","e735dfd6":"code","d6216430":"code","80248a6a":"code","e7f8524d":"code","149bbd95":"code","20203d67":"code","98d3ae21":"code","f3a08ee4":"code","297b3c1f":"code","769c1c6d":"code","66075df1":"code","10dc230e":"code","22b09211":"code","5a870a45":"code","630746db":"code","d6aa391a":"code","5950c106":"code","ca9d7c62":"code","e716ab8e":"code","fa6ec387":"code","eb1c6b79":"code","efa66ff0":"code","d8f359f2":"code","a753d11e":"code","539c2b31":"code","ab42d7f7":"code","1de33b40":"code","4e2eb865":"code","edc7d345":"markdown","b12d31e3":"markdown","4f63efde":"markdown","c372a96e":"markdown","3d06021f":"markdown","0b50b2eb":"markdown","651b7863":"markdown","09caf543":"markdown","a5d85e93":"markdown","7b586a59":"markdown","ad77ad0d":"markdown","99a7de6f":"markdown","4d01a63f":"markdown","8649c725":"markdown","3bda5fa9":"markdown","897571ce":"markdown","3075b3dc":"markdown"},"source":{"263257c2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (20,10)\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nimport pickle\nimport json","551b3716":"filepath = '..\/input\/bengaluru-house-price-data\/Bengaluru_House_Data.csv'    #filepath of the dataset\ndata = pd.read_csv(filepath)\ndata.head()","94f6cfd9":"data.shape","544ec097":"data.groupby('area_type')['area_type'].agg('count')    # taking the 'area_type' column as a group and counting its values","19884842":"data2 = data.drop(['area_type','society','balcony','availability'], axis='columns')     # dropping useless columns\ndata2.head()","043c1a7e":"data2.isnull().sum()     # finding total empty values in each column","0b0dec08":"data3 = data2.dropna()    # dropping 'NA' values\ndata3.isnull().sum()","a10e8c77":"data3.shape","d68a2adc":"data3['size'].unique()    # checking unique values in size column","6ffd11a2":"data3['bhk'] = data3['size'].apply(lambda x: int(x.split(' ')[0]))    # getting the number of bedrooms from size column","b6b72d1d":"data3.head()","69da203c":"data3['bhk'].unique()","41416827":"data3[data3.bhk>20]    ","16d39f28":"data3.total_sqft.unique()","545488fa":"def is_float(x):\n    '''\n    Function to convert values into float dtype.\n    '''\n    try:\n        float(x)\n    except:\n        return False\n    return True","bc868bf3":"data3[~data3['total_sqft'].apply(is_float)].head(20)    # finding values those not got converted","3ba35f5d":"def convert_sqft_to_sum(x):\n    '''\n    Function to convert those unusual format of data\n    '''\n    tokens = x.split('-')\n    if len(tokens) == 2:\n        return (float(tokens[0]) + float(tokens[1])) \/ 2\n    try:\n        return float(x)\n    except:\n        return None","7173a71b":"convert_sqft_to_sum('2100 - 2850')    # usage of the function","9757ab1c":"data4 = data3.copy()\ndata4['total_sqft'] = data4['total_sqft'].apply(convert_sqft_to_sum)\ndata4.head()","6df50321":"data4.loc[30]    # loc function is used to see data row-wise","184ec1cb":"data5 = data4.copy()    # copy function is used to copy the whole dataframe\ndata5.head()","4e59bd5d":"data5['price_per_sqft'] = data5['price'] * 100000\/data5['total_sqft']    # making a new column in the dataframe named `price_per_sqft` and see the logic to create it\ndata5.head()","d00c99a4":"len(data5.location.unique())","ab506f14":"data5.location = data5.location.apply(lambda x : x.strip())    # strip is used to remove the white spaces around the data points\n\nlocation_stats = data5.groupby('location')['location'].agg('count').sort_values(ascending=False)    # sorting the location column in descending order\nlocation_stats","1a62e35b":"len(location_stats[location_stats<=10])    # totaling the minor locations","e5c45026":"location_stats_less_than_10 = location_stats[location_stats<=10]\nlocation_stats_less_than_10","e61ff32d":"data5.location = data5.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)    # changing minor locations into `other`","86e2df89":"len(data5.location.unique())","ce8a7b7e":"data5[data5.total_sqft \/ data5.bhk < 300].head()    # checking for outliners; like a house with 1407 sq. area can't have 6 bedrooms","7f2bee24":"data5.shape","52e0c16d":"data6 = data5[~(data5.total_sqft\/data5.bhk<300)]","adefcb64":"data6.price_per_sqft.describe()","c41c2f34":"data6.shape","ae2f59f6":"def remove_pps_outliers(df):\n    '''\n    Function to clear stuff (outliers) in the price_per_sqft column so that we don't live in a hypothetical dataset. \ud83d\ude04\n    '''\n    df_out = pd.DataFrame()\n    for key, subdf in df.groupby('location'):\n        m = np.mean(subdf.price_per_sqft)\n        st = np.std(subdf.price_per_sqft)\n        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n    return df_out\n        ","884d8165":"data7 = remove_pps_outliers(data6)\ndata7.shape","9d227b0c":"data7.head()","aecd09e3":"def remove_bhk_outliers(df):\n    '''\n    Function to clear stuff (outliers) in the bhk column so that we don't live in a hypothetical dataset. \ud83d\ude04\n    '''\n    exclude_indices = np.array([])\n    for location, location_df in df.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            bhk_stats[bhk] = {\n                'mean': np.mean(bhk_df.price_per_sqft),\n                'std': np.std(bhk_df.price_per_sqft),\n                'count': bhk_df.shape[0]\n            }\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            stats = bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n    return df.drop(exclude_indices,axis='index')\n\ndata8 = remove_bhk_outliers(data7)\ndata8.shape","e735dfd6":"def plot_scatter_chart(df,location):\n    '''\n    Function that will help us to visualize the data of the different locations \n    '''\n    bhk2 = df[(df.location==location) & (df.bhk==2)]\n    bhk3 = df[(df.location==location) & (df.bhk==3)]\n    matplotlib.rcParams['figure.figsize'] = (15, 10)\n    plt.scatter(bhk2.total_sqft, bhk2.price,color='blue', label='2 BHK', s=50)\n    plt.scatter(bhk3.total_sqft, bhk3.price,marker='+',color='green', label='3 BHK', s=50)\n    plt.xlabel('Total Square Feet Area')\n    plt.ylabel('Price Per Square Feet')\n    plt.title(location)\n    plt.legend()","d6216430":"plot_scatter_chart(data7, 'Whitefield')","80248a6a":"plot_scatter_chart(data7,\"Hebbal\")","e7f8524d":"plt.hist(data8.price_per_sqft, rwidth=0.8)    # visualization the price_per_sqft column\nplt.xlabel('Price per Square Feet')\nplt.ylabel('Count')","149bbd95":"data8.bath.unique()","20203d67":"data8[data8.bath>10]    # Again some idiot outliers\ud83d\ude06","98d3ae21":"plt.hist(data8.bath, rwidth=0.8)\nplt.xlabel('Number of bathrooms')\nplt.ylabel('Count')","f3a08ee4":"data8[data8.bath > data8.bhk + 2]    # plz explain me how can someone have more bathrooms that bedrooms \ud83d\ude02","297b3c1f":"data9 = data8[data8.bath < data8.bhk + 2]\ndata9.shape","769c1c6d":"data9.sample()    # sample return a random row from the dataset","66075df1":"data10 = data9.drop(['size',\"price_per_sqft\"],axis='columns')    # removing or dropping 'size' and 'prize_per_sqft' as we don't require them any more\ndata10.head()","10dc230e":"dummies = pd.get_dummies(data10.location)     \ndummies.head()","22b09211":"data11 = pd.concat([data10, dummies.drop('other', axis='columns')], axis='columns')    # joining the dummy values again with the dataset except 'other' column\ndata11.head()","5a870a45":"data12 = data11.drop('location', axis='columns')    # dropping original location as now we have dummmies in its place.\ndata12.head()","630746db":"X = data12.drop('price', axis='columns')    # dropping price column as we don't want it in our train dataset\nX.head()","d6aa391a":"X.shape","5950c106":"y = data12.price    # taking the price column as our target to predict","ca9d7c62":"y.shape","e716ab8e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)    # splitting the the data into train and test data as 80 : 20 ratio","fa6ec387":"X_train.shape","eb1c6b79":"lr_clf = LinearRegression()     # first trying training with LinearRegression \nlr_clf.fit(X_train, y_train)\nlr_clf.score(X_test, y_test)","efa66ff0":"cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)     # ShuffleSplit is just a another type of splitting data\ncross_val_score(LinearRegression(), X, y, cv=cv)     ","d8f359f2":"def find_best_model_using_gridsearchcv(X,y):\n    '''\n    Function to try different models at once of the data with different parameters to find the best ones.\n    '''\n    algos = {\n        'linear_regression':{\n            'model': LinearRegression(),\n            'params':{\n                'normalize':[True,False]\n            }\n        },\n        'lasso':{\n            'model': Lasso(),\n            'params':{\n                'alpha' : [1,2],\n                'selection':['random','cyclic']\n            }\n        },\n        'decision_tree':{\n            'model': DecisionTreeRegressor(),\n            'params':{\n                'criterion':['mse','friedman_mse'],\n                'splitter':['best','random']\n            }\n        }\n    }\n    scores = []\n    cv = ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n    for algo_name, config in algos.items():\n        gs = GridSearchCV(config['model'],config['params'], cv=cv, return_train_score=False)     # GridSearchCV is the main focus as it helps to try out the different parameters for the different models.\n        gs.fit(X,y)\n        scores.append({\n            'model': algo_name,\n            'best_score':gs.best_score_,\n            'best_params':gs.best_params_\n        })\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])     # At last binding the results of the models with best params. into a DataFrame.","a753d11e":"find_best_model_using_gridsearchcv(X, y)","539c2b31":"def predict_price(location,sqft,bath,bhk):\n    '''\n    Function which helps to actually predict the prices.\n    '''\n    loc_index = np.where(X.columns==location)[0][0]     # np.where() function returns the indices of elements in an input array where the given condition is satisfied.\n    \n    x = np.zeros(len(X.columns))    # np.zeros() function returns a new array of given shape and type, with zeros.\n    x[0] = sqft\n    x[1] = bath\n    x[2] = bhk\n    if loc_index >= 0:\n        x[loc_index] = 1\n        \n    return lr_clf.predict([x])[0]","ab42d7f7":"print(predict_price('1st Phase JP Nagar', 1000, 2, 3).round(3),'Lakhs')","1de33b40":"with open('BHP_model.pickle','wb') as f:\n    pickle.dump(lr_clf,f)","4e2eb865":"columns = {\n    'data_columns' : [col.lower() for col in X.columns]\n}\nwith open('columns.json','w') as f:\n    f.write(json.dumps(columns))","edc7d345":"# 9. Prediction Time\nPredicting the prices using LinearRegression in Lakhs.","b12d31e3":"# 4. Data Cleaning\nIn this section, we will find outliers and try to remove them.","4f63efde":"`groupby()` function is used to split the data into groups based on some criteria\nand\n`agg()` function abbreviation of aggregate is used to define what we want to do with the grouped data.\n","c372a96e":"# 8. Training the Models\nPreciesly, we will be trying more than one model, therefore, Training the `Models`","3d06021f":"Finding rows with extraordinary values!\ud83d\ude05 (Outliers)","0b50b2eb":"# 7. Splitting Data for Training and Testing\nBefore Training the model, it is required to split the data into train and test data. For this we will use, sklearn's `train_test_split`\n","651b7863":"Cross Validation is mainly used for the comparison of different models. For each model, you may get the average generalization error on the k validation sets. Then you will be able to choose the model with the lowest average generation error as your optimal model.","09caf543":"# Real Estate Price Prediction - Bengaluru House Price Dataset\nIn this notebook we will do E.D.A. (Exploratory Data Analysis) and trainig on the `Bengaluru House Price Dataset` using sklearn libraries and matplotlib.\nSklearn is a library machine-learning and matplotlib is also a popular library for data visualization.\n\nDo Upvote and Comment below!","a5d85e93":"# 3. Data Preprcessing\nData preprocessing is a data mining technique that involves transforming raw data into an understandable format.","7b586a59":"# 1. Importing necessary Libraries","ad77ad0d":"## Let's Get Started!","99a7de6f":"# 2. Reading Data\nWe will use pandas `read_csv` function to read the data in csv file.","4d01a63f":"# Bravo! We completed the model training for Bengaluru Home Prices Dataset.\n### Please upvote this notebook if you like and find it helpful and please comment down below your views and suggestions.\n\n## Thank You!\ud83d\ude0a","8649c725":"# 10. Saving Model\nFor saving the model, we will be using `pickle` module and `json` module for saving the locations' names.","3bda5fa9":"# 6. Creating Dummies\nWe will use pandas' `get_dummies()` to create dummies variables.\nIt is used for data manipulation. It converts categorical data into dummy or indicator variables.","897571ce":"# 5. Data Visualization\nTime to visualize our data","3075b3dc":"As we can see the LinearRegression model performed the best, so we are going to use as it for prediction."}}