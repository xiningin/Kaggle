{"cell_type":{"7338a60f":"code","bef706c9":"code","7ec83626":"code","3caa9399":"code","1f64f0ff":"code","216f0ef8":"code","cefa9cc9":"code","47f3e54b":"code","91f7c2aa":"code","28ed8fbd":"code","ea8194ff":"code","f2833f91":"code","71a405ac":"code","fd1d1eff":"code","88df933c":"code","a015adba":"code","1c2a0fd6":"code","348dec86":"code","1a95148b":"code","2f0ce103":"code","db38f5b3":"code","194add08":"code","1b4c363b":"code","66f98315":"code","43845f20":"code","962d7ac1":"code","0cd415f4":"code","c03e0b62":"code","6e4374d1":"code","dba4087e":"code","394eac44":"code","936eac98":"code","350d2a1e":"code","1a135854":"code","6b4dc5fc":"code","da272778":"code","806572f1":"code","bd320bdc":"code","047cbbf9":"code","578f45e3":"code","e9be17b5":"code","ee656bc0":"code","2c11b98d":"code","3fe981d0":"markdown","32d3f22f":"markdown","f59cb71b":"markdown","db0b3fbc":"markdown","242aa74b":"markdown","12573538":"markdown","f6454c93":"markdown","473f7344":"markdown","a1ab1d8f":"markdown","1b6852da":"markdown","281e85a0":"markdown","d8bc0e58":"markdown","37252765":"markdown","80f7482c":"markdown","d2bb168d":"markdown","cd40efe0":"markdown","04b7787f":"markdown","147d466f":"markdown","4ae73cc1":"markdown"},"source":{"7338a60f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bef706c9":"import pandas as pd\nimport seaborn as sns","7ec83626":"raw_df = pd.read_csv('\/kaggle\/input\/uci-wholesale-customers-data\/Wholesale customers data.csv')","3caa9399":"raw_df.head(5)","1f64f0ff":"raw_df.drop(['Channel','Region'],axis=1,inplace=True)\nraw_df.shape","216f0ef8":"raw_df.describe()","cefa9cc9":"raw_df.loc[[100,200,300],:]","47f3e54b":"raw_df.columns","91f7c2aa":"# fresh filter\nfresh_q1 = 3127.75000\nraw_df[raw_df['Fresh'] < fresh_q1].head()","28ed8fbd":"# Frozen filter\nfrozen_q1 = 742.250000\nraw_df[raw_df.Frozen < frozen_q1].head()","ea8194ff":"# frozen q3\nfrozen_q3 = 3554.250000\nraw_df[raw_df.Frozen > frozen_q3].head(7)","f2833f91":"selected_samples = [43,12,39]\n\nsamples = pd.DataFrame(raw_df.loc[selected_samples],columns=raw_df.columns).reset_index(drop = True)\nsamples","71a405ac":"mean_data = raw_df.describe().loc['mean',:]\n\nsample_bars = samples.append(mean_data)\n\nsample_bars.index = selected_samples + ['mean']\n\nsample_bars.plot(kind='bar',figsize=(15,8))","fd1d1eff":"percentiles = raw_df.rank(pct=True)\n\npercentiles = 100 * percentiles.round(decimals=3)\n\npercentiles = percentiles.iloc[selected_samples]\n\nsns.heatmap(percentiles,vmin=1,vmax=99,annot=True)","88df933c":"raw_df.columns","a015adba":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor","1c2a0fd6":"deps_vars = list(raw_df.columns)\n\nfor var in deps_vars:\n    new_data = raw_df.drop([var],axis=1)\n    \n    new_feature = pd.DataFrame(raw_df.loc[:,var])\n    \n    X_train, X_test, y_train, y_test = train_test_split(new_data, new_feature, test_size=0.25, random_state=42)\n    \n    dtr = DecisionTreeRegressor(random_state=42)\n    \n    dtr.fit(X_train,y_train)\n    \n    score = dtr.score(X_test, y_test)\n    \n    print('R2 score for {} as dependent variable: {}'.format(var, score))","348dec86":"sns.pairplot(data=raw_df,size=5)","1a95148b":"import matplotlib.pyplot as plt\n%matplotlib inline","2f0ce103":"def plot_correlation(df,size=10):\n    corr = raw_df.corr()\n    fig, ax = plt.subplots(figsize=(size,size))\n    cax = ax.matshow(df,interpolation='nearest')\n    ax.matshow(corr)\n    fig.colorbar(cax)\n    plt.xticks(range(len(corr.columns)),corr.columns)\n    plt.yticks(range(len(corr.columns)),corr.columns)\n    \nplot_correlation(raw_df)","db38f5b3":"log_data = np.log(raw_df)\nlog_sample = np.log(samples)\nsns.pairplot(log_data)","194add08":"log_sample","1b4c363b":"plot_correlation(log_data)\nplot_correlation(log_sample)","66f98315":"np.percentile(raw_df.Milk,25)","43845f20":"import itertools\noutlier_list = []\n\nfor feature in log_data.columns:\n    Q1 = np.percentile(log_data[feature],25)\n    Q3 = np.percentile(log_data[feature],75)\n    step = 1.5 * (Q3 - Q1)\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    outlier_rows = log_data.loc[~((log_data[feature] >= Q1- step) & (log_data[feature] <= Q3 + step)),:]\n    outlier_list.append(list(outlier_rows.index))\noutliers = list(itertools.chain.from_iterable(outlier_list))\nuniq_outliers = list(set(outliers))\ndup_outliers = list(set([x for x in outliers if outliers.count(x) > 1]))\n\nprint('Outliers list:\\n', uniq_outliers)\nprint('Length of outliers list:\\n', len(uniq_outliers))\nprint('Duplicate list:\\n', dup_outliers)\nprint('Length of duplicates list:\\n', len(dup_outliers))\n\ngood_data = log_data.drop(log_data.index[dup_outliers]).reset_index(drop=True)\n\nprint('Original shape of data:\\n', raw_df.shape)\nprint('New shape of data:\\n', good_data.shape)","962d7ac1":"import matplotlib.cm as cm\nfrom sklearn.decomposition import pca\n\ndef pca_results(good_data, pca):\n\t'''\n\tCreate a DataFrame of the PCA results\n\tIncludes dimension feature weights and explained variance\n\tVisualizes the PCA results\n\t'''\n\n\t# Dimension indexing\n\tdimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n\t# PCA components\n\tcomponents = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n\tcomponents.index = dimensions\n\n\t# PCA explained variance\n\tratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n\tvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n\tvariance_ratios.index = dimensions\n\n\t# Create a bar plot visualization\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Plot the feature weights as a function of the components\n\tcomponents.plot(ax = ax, kind = 'bar');\n\tax.set_ylabel(\"Feature Weights\")\n\tax.set_xticklabels(dimensions, rotation=0)\n\n\n\t# Display the explained variance ratios\n\tfor i, ev in enumerate(pca.explained_variance_ratio_):\n\t\tax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n\n\t# Return a concatenated DataFrame\n\treturn pd.concat([variance_ratios, components], axis = 1)\n\ndef cluster_results(reduced_data, preds, centers, pca_samples):\n\t'''\n\tVisualizes the PCA-reduced cluster data in two dimensions\n\tAdds cues for cluster centers and student-selected sample data\n\t'''\n\n\tpredictions = pd.DataFrame(preds, columns = ['Cluster'])\n\tplot_data = pd.concat([predictions, reduced_data], axis = 1)\n\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# Color the points based on assigned cluster\n\tfor i, cluster in plot_data.groupby('Cluster'):   \n\t    cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i)*1.0\/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);\n\n\t# Plot centers with indicators\n\tfor i, c in enumerate(centers):\n\t    ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \\\n\t               alpha = 1, linewidth = 2, marker = 'o', s=200);\n\t    ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);\n\n\t# Plot transformed sample points \n\tax.scatter(x = pca_samples[:,0], y = pca_samples[:,1], \\\n\t           s = 150, linewidth = 4, color = 'black', marker = 'x');\n\n\t# Set plot title\n\tax.set_title(\"Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\\nTransformed Sample Data Marked by Black Cross\");\n\n\ndef channel_results(reduced_data, outliers, pca_samples):\n\t'''\n\tVisualizes the PCA-reduced cluster data in two dimensions using the full dataset\n\tData is labeled by \"Channel\" and cues added for student-selected sample data\n\t'''\n\n\t# Check that the dataset is loadable\n\ttry:\n\t    full_data = pd.read_csv(\"customers.csv\")\n\texcept:\n\t    print(\"Dataset could not be loaded. Is the file missing?\")\n\t    return False\n\n\t# Create the Channel DataFrame\n\tchannel = pd.DataFrame(full_data['Channel'], columns = ['Channel'])\n\tchannel = channel.drop(channel.index[outliers]).reset_index(drop = True)\n\tlabeled = pd.concat([reduced_data, channel], axis = 1)\n\t\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# Color the points based on assigned Channel\n\tlabels = ['Hotel\/Restaurant\/Cafe', 'Retailer']\n\tgrouped = labeled.groupby('Channel')\n\tfor i, channel in grouped:   \n\t    channel.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i-1)*1.0\/2), label = labels[i-1], s=30);\n\t    \n\t# Plot transformed sample points   \n\tfor i, sample in enumerate(pca_samples):\n\t\tax.scatter(x = sample[0], y = sample[1], \\\n\t           s = 200, linewidth = 3, color = 'black', marker = 'o', facecolors = 'none');\n\t\tax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker='$%d$'%(i), alpha = 1, s=125);\n\n\t# Set plot title\n\tax.set_title(\"PCA-Reduced Data Labeled by 'Channel'\\nTransformed Sample Data Circled\");","0cd415f4":"#!pip install renders\n#import renders as rs\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=6)\npca.fit(good_data)\npca_samples = pca.transform(good_data)\npca_results = pca_results(good_data, pca)\n\n","c03e0b62":"pca_results","6e4374d1":"type(pca_results)","dba4087e":"pca_results['Explained Variance'].cumsum()","394eac44":"pca = PCA(n_components=2)\npca.fit(good_data)\nreduced_data = pca.transform(good_data)\npca_samples = pca.transform(log_sample)\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])","936eac98":"from sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","350d2a1e":"range_n_clusters = list(range(2,11))\nprint(range_n_clusters)","1a135854":"for n_clusters in range_n_clusters:\n    clusterer = GMM(n_components=n_clusters).fit(reduced_data)\n    preds = clusterer.predict(reduced_data)\n    centers = clusterer.means_\n    sample_preds = clusterer.predict(pca_samples)\n    score = silhouette_score(reduced_data, preds, metric='mahalanobis')\n    print(\"For n_clusters = {}. The average silhouette_score is : {}\".format(n_clusters, score))","6b4dc5fc":"lowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a mixture of Gaussians with EM\n        gmm = GMM(n_components=n_components, covariance_type=cv_type)\n        gmm.fit(reduced_data)\n        bic.append(gmm.bic(reduced_data))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm","da272778":"for n_clusters in range_n_clusters:\n    clusterer = KMeans(n_clusters=n_clusters).fit(reduced_data)\n    preds = clusterer.predict(reduced_data)\n    centers = clusterer.cluster_centers_\n    sample_preds = clusterer.predict(pca_samples)\n    score = silhouette_score(reduced_data, preds, metric='euclidean')\n    print(\"For n_clusters = {}. The average silhouette_score is : {}\".format(n_clusters, score))","806572f1":"cluster_results(reduced_data, preds, centers, pca_samples)","bd320bdc":"log_centers = pca.inverse_transform(centers)\ntrue_centers = np.exp(log_centers)\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = raw_df.columns)\ntrue_centers.index = segments\ntrue_centers","047cbbf9":"true_centers - raw_df.median()","578f45e3":"for i, pred in enumerate(sample_preds):\n    print(\"Sample point\", i, \"predicted to be in Cluster\", pred)","e9be17b5":"samples","ee656bc0":"dup_outliers","2c11b98d":"channel_results(reduced_data, dup_outliers, pca_samples)","3fe981d0":"cluster visualisation","32d3f22f":"**Compare percentiles**","f59cb71b":"Now lets draw a corelation matrix.","db0b3fbc":"implimentation : PCA","242aa74b":"# Dimensionality Reduction","12573538":"we will implement the following activities on the data set:\n* Feature Scaling\n* Detecting Outliers","f6454c93":"Lets Explore few samples in depth","473f7344":"# Feature Transformation","a1ab1d8f":"Implementation two types of clustering models:\n    1. k-means\n    2. Gaussian Mix model","1b6852da":"# Data recovery","281e85a0":"**Plotting Scatter graph for 6 feature**","d8bc0e58":"clusterer = GMM(n_components=2).fit(reduced_data)\npreds = clusterer.predict(reduced_data)\ncenters = clusterer.means_\nsample_preds = clusterer.predict(pca_samples)","37252765":"# Data Preprocessing ","80f7482c":"Selecting the bottom endices after filtering the quartiles \n   * 43 : low 'Fresh' highest 'Grocery'\n   * 12 : low 'Frozen' highest 'Fresh'\n   * 39 : low 'Detergents_Paper' highest 'Fresh'","d2bb168d":"# Applying Clustering to identify Customer segments in data","cd40efe0":"yellow being the most correlated features that can be seen from the above plot.\nso we figure out the degree of CORRELATION between the items.\n* Grocery and Detergent Papers\n* Milk and Grocery\n* milk and Detergent Papers","04b7787f":"**Comparing sample means**","147d466f":"**Feature Importance**","4ae73cc1":"Detecting Outliers"}}