{"cell_type":{"2618f4d4":"code","7787b6cd":"code","8fad5d57":"code","ecde7486":"code","f506f8a9":"code","dd7bd40f":"code","8b5a7f4e":"code","7f284825":"code","a88d39ed":"code","9bf62144":"code","e75d4639":"markdown","8d056f85":"markdown","3754cd4e":"markdown","8fbaf299":"markdown","c87e376e":"markdown","f6383c7d":"markdown"},"source":{"2618f4d4":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.metrics import accuracy_score","7787b6cd":"# Define the traning data.\nX, y = make_classification(n_samples=1000, n_classes=2)\n\n# Chnage the shape of the target to 1 dimentional array.\ny = y[:, np.newaxis]\n\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\nprint(\"Shape of the target value ----------> {}\".format(y.shape))","8fad5d57":"# display the data.\ndata = pd.DataFrame(X)\ndata.head()\n","ecde7486":"# display the data.\ndata_y = pd.DataFrame(y)\ndata_y.head()","f506f8a9":"def euclidean_distance(x1,x2):\n    \"\"\"\n    \"\"\"\n    distance = 0\n    for i in range(len(x1)):\n        distance += np.square(x1[i] - x2[i])\n    return np.square(distance)","dd7bd40f":"class KNN:\n    \"\"\"\n    \"\"\"\n    def __init__(self, n_neibours=5):\n        \"\"\"\n        \"\"\"\n        self.n_neibours = n_neibours\n    \n    def most_frequent_class(self, neibours_y):\n        \"\"\"\n        \"\"\"\n        counts = np.bincount(neibours_y)\n        return counts.argmax()\n\n    def train(self, X, y):\n        \"\"\"\n        \"\"\"\n        # there is no learning nedded in KNN. Everything happening at the time of prediction.\n        self.m , self.n = X.shape\n        self.train_X = X\n        self.train_y = y\n\n    def predict(self, test_X):\n        \"\"\"\n        \"\"\"\n        # create a empty y_pred array to store the prediction.\n        y_pred = np.empty((test_X.shape[0], 1))\n        # iterate over all the test dataset.\n        for index, test_x in enumerate(test_X):\n            # Find the euclidean distance between text x sample and all train X values.\n            distance = [euclidean_distance(test_x, train_x ) for train_x in self.train_X]\n\n            # Sort the distance and get the first smallest k-neibours's index.\n            n_neibours_index = np.argsort(distance)[: self.n_neibours]\n\n            # Get the neibours corresponding train_y value.\n            n_neibours_y = np.array([self.train_y[ind][0] for ind in n_neibours_index ])\n\n            # Get the most frequent class in the n_neibours group as a prediction for that test_x sample.\n            y_pred[index] = self.most_frequent_class(n_neibours_y)\n\n        return y_pred\n","8b5a7f4e":"#define the parameters\nparam = {\n    \"n_neibours\" : 5\n}\nprint(\"=\"*100)\nknn_cla = KNN(**param)\n\n# Train the model.\nknn_cla.train(X, y) \n\n# Predict the values.\ny_pred = knn_cla.predict(X)\n\n#calculate accuracy.\nacc = np.sum(y==y_pred)\/X.shape[0]\nprint(\"=\"*100)\nprint(\"Accuracy of the prediction is {}\".format(acc))","7f284825":"from sklearn.neighbors import KNeighborsClassifier","a88d39ed":"# data is already defined, going to use the same data for comparision.\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))","9bf62144":"knn_sklearn = KNeighborsClassifier(n_neighbors=5)\nknn_sklearn.fit(X, y)\n\n# predict the value\ny_pred_sklearn = knn_sklearn.predict(X)\nacc = accuracy_score(y, y_pred_sklearn)\nprint(\"=\"*100)\nprint(\"Accuracy of the prediction is {}\".format(acc))","e75d4639":"# Conclution","8d056f85":"- 1. The accuracy of out model is very much the same as scikit-learn KNN. :)\n- 2. But our model is little bit slow and so expensive since it is calculating distance with all tarin x for each test x sample. :(","3754cd4e":"# Data Creation","8fbaf299":"# KNN using scikit-learn for comparision","c87e376e":"# Supervised Machine Learning models scratch series....\nyou can also check....\n\n- 1) Linear Regression         ---> https:\/\/www.kaggle.com\/ninjaac\/linear-regression-from-scratch\n- 2) Lasso Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch \n- 3) Ridge Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch\n- 4) ElasticNet Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/elasticnet-regression-from-scratch \n- 5) Polynomail Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch \n- 5) PolynomailRidge Regression---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch\n- 6) KNN Classifier            ---> https:\/\/www.kaggle.com\/ninjaac\/knnclassifier-from-scratch (Same Notebook you are looking now)","f6383c7d":"# K-nearest Neipours (KNN) from scratch"}}