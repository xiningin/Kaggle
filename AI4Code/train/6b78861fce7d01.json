{"cell_type":{"16cb9357":"code","4a05b873":"code","74f6dfa5":"code","cb845a20":"code","f27f5658":"code","f63da53f":"code","303a28c9":"code","6bc982e2":"code","38378e25":"code","76178419":"code","447071dc":"markdown","f87c901d":"markdown","a7fec808":"markdown","1685efda":"markdown","26022f0f":"markdown","744d6d36":"markdown","99f946a6":"markdown","9e588cb1":"markdown","ceaa76bf":"markdown","d7fefe73":"markdown"},"source":{"16cb9357":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os","4a05b873":"# Importing the dataset\nprint(os.listdir(\"..\/input\"))\n# Importing the dataset\ndf = pd.read_csv('..\/input\/Mall_Customers.csv')\ndf.head()","74f6dfa5":"print (\"The Shape of our dataset is: \" + str(df.shape))","cb845a20":"# Create new dataframe of annual income and spending score\nX_spend = df[['Annual Income (k$)','Spending Score (1-100)']]\nX_spend.head()","f27f5658":"# Using the dendrogram to find the optimal number of clusters\nimport scipy.cluster.hierarchy as hcd\ndendrogram = hcd.dendrogram(hcd.linkage(X_spend, metric='euclidean', method = 'ward'))\nplt.title('Dendrogram', size=20)\nplt.xlabel('Customers', size=15)\nplt.ylabel('Euclidean Distances', size=15)\nplt.show()","f63da53f":"# Fitting Hierarchical Clustering with 3 Clusters to the dataset\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\nX_spend['Cluster'] = hc.fit_predict(X_spend)","303a28c9":"# Examine new dataframe with cluster column\nX_spend.head()","6bc982e2":"# Define cluster colors\nhc_colors = ['green' if c == 0 else 'blue' if c == 1 else 'purple' if c == 2 else 'black' if c == 3 else 'red' for c in X_spend.Cluster]\n\n# Plot the scatter plot & clusters\nfig = plt.figure(figsize=(10, 6))\nplt.scatter(x=\"Annual Income (k$)\",y=\"Spending Score (1-100)\", data=X_spend, alpha=0.25, color = hc_colors)\nplt.xlabel(\"Annual Income (k$)\", size=15)\nplt.ylabel(\"Spending Score (1-00)\", size=15)\nplt.title(\"Clusters of Spenders (3)\", size=25)\nplt.show()","38378e25":"# Fitting Hierarchical Clustering with 5 Clusters to the dataset\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\nX_spend['Cluster'] = hc.fit_predict(X_spend)\n# Define cluster colors\nhc_colors = ['green' if c == 0 else 'blue' if c == 1 else 'purple' if c == 2 else 'black' if c == 3 else 'red' for c in X_spend.Cluster]\n\n# Plot the scatter plot & clusters\nfig = plt.figure(figsize=(10, 6))\nplt.scatter(x=\"Annual Income (k$)\",y=\"Spending Score (1-100)\", data=X_spend, alpha=0.25, color = hc_colors)\nplt.xlabel(\"Annual Income (k$)\", size=15)\nplt.ylabel(\"Spending Score (1-00)\", size=15)\nplt.title(\"Clusters of Spenders (5)\", size=25)\nplt.show()","76178419":"df['cluster'] = X_spend['Cluster']\ndf[df['cluster']==3].describe()","447071dc":"**Hierarchical Clustering Pros\/Cons**\n\n**PROS**:\n\u25cf No prior knowledge of cluster\nsize needed\n\u25cf Easy to understand\n\u25cf Easy to visualize with\ndendrogram\n\u25cf Works well on small data sets\n\n**CONS**:\n\u25cf Really slow on large data sets and high\ndimensions\n\u25cf May be difficult to identify correct\namount of clusters from dendrogram\n\u25cf Difficult to handle different sized\nclusters and weird shapes","f87c901d":"![image.png](attachment:image.png)","a7fec808":"**Summary and Speculations**\n\nIt looks like the cluster count of 5 seems a bit more reasonable.\n\nOur customers can be seen as:\n\nPeople who are earning less that are spending less, \nPeople who are earning less are spending more,\nPeople who earn an average amount and are spending an average amount,\nPeople who earn more and are spending less,\nPeople who earn more and spending more.\n\nWe can use these insights to do a deeper dive into these clusters & understand our consumers.","1685efda":"**Apply Agglomerative Clustering**\n\nn_clusters = 3","26022f0f":"**Identifying Consumer Behavior**\n\nThe dataset that we'll be working with contains 200 customer data. We are going to be focusing on their annual income vs spending behavior and our goal is to identify groups of customers","744d6d36":"**Visualizing the Clusters**","99f946a6":"Looks like the group of people who have high spending habits and also don't earn as much are the young adults (median age - 22)","9e588cb1":"**Dendogram - Finding Optimal Number of Clusters**\n\nDendrograms can help you decide\nthe optimal number of clusters for\nyour dataset by showing explicitly\nthe hierarchy of the clusters.Linking the clusters together will form a hierarchical construct known\nas a Dendrogram\n\n**How It Works**\n1. Assign K data points to K different\nclusters, where each data point\nbecomes its own cluster\n2. Then, two data points with minimum\ndistance between them are merged\ninto a single cluster\n3. Then, two clusters with minimum\nlinkage distance between them are\nmerged into a single cluster\nRepeat this process until there is only one\nsingle cluster\n\nDONE - Hierarchy of clusters formed!","ceaa76bf":"**Apply Agglomerative Clustering**\n\nn_clusters = 5\n","d7fefe73":"**Types of Hierarchical Clustering**\n\n**1. Agglomerative Clustering (bottom-up):**\n\na. Each individual data point starts as an individual cluster\nb. Merge each two closest clusters into a new combined cluster\nc. Eventually, all points are combined into a single cluster\n\n**2. Divisive Clustering (top-down):**\n\na. All points are combined into a single cluster\nb. Then the most dissimilar data points being to split off into\ndifferent clusters\nc. Keep splitting until each individual data point is its own\ncluster"}}