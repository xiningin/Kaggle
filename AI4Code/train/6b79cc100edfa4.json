{"cell_type":{"b6114643":"code","4538509b":"code","fe843bdc":"code","6acd1151":"code","6d800e99":"code","5710464d":"code","48c93efa":"code","450af331":"code","0ddb8ff0":"code","c206b36e":"code","64d5fc67":"code","eb9d56bb":"code","62e0f2f8":"code","50cf154d":"code","187a922e":"code","421fcede":"code","00194fea":"code","705817e7":"code","cbfd8ec4":"code","65c754cc":"code","d91c8d0c":"code","d2c00579":"code","6d74ddb8":"code","9bddd7f3":"code","10b1acdd":"code","dacc6d1f":"code","d5feea0b":"code","27651adf":"code","4550e557":"code","9dc41c0c":"code","452c08b6":"code","17c2d3a9":"code","9e08e936":"code","49061e4e":"code","a1e6896c":"code","5fbc2770":"code","ad6e4e87":"code","7dcacafa":"code","99137e75":"code","cf52a63c":"code","6b24d118":"code","c5064a2f":"code","5e68cd7d":"code","124eb628":"code","866d275e":"code","dcda22a8":"code","4f568a23":"code","5b08e809":"code","24259eea":"code","d4a64ac2":"code","5c9901be":"code","34ee4f8a":"code","6043cf84":"code","70525bb0":"code","2bfdafb5":"code","e99a0fdc":"code","17b6c951":"code","0d175830":"code","eb674a0f":"code","4f06cca6":"code","b6b9c3b7":"code","5b93b67a":"code","2e8daec0":"code","eed11b7e":"code","a64b42e2":"code","23579b7f":"code","5abf9df8":"code","c9ebaa66":"code","5316d849":"code","5f9d46c8":"code","4367b8cb":"code","0210c280":"code","3b7023b9":"markdown","00b09186":"markdown","799a4cb5":"markdown","bc8d97f0":"markdown","68082759":"markdown","4c63678e":"markdown","1a8eebe2":"markdown","ead61823":"markdown","7c18a644":"markdown","00a043b3":"markdown","ae2e333c":"markdown","c1f96126":"markdown","2f42d4ce":"markdown","d35de4ca":"markdown","a4a2b6c4":"markdown","ec747904":"markdown","367b72c8":"markdown","024c80e3":"markdown","470f9665":"markdown","be6b40cc":"markdown","789ccf07":"markdown","e3cdc1dd":"markdown","8bb609a4":"markdown","91d1f403":"markdown","ba84b80c":"markdown","6bd7b2e6":"markdown","39865cde":"markdown","be9c6bb7":"markdown","c851d119":"markdown","9a728274":"markdown","45181f43":"markdown","9a250cdf":"markdown","ed3b2e07":"markdown","21157d13":"markdown","2e8bfbc2":"markdown","221205f5":"markdown","8dd348c2":"markdown","7a649ee1":"markdown","f93e31da":"markdown","beaf68a8":"markdown","fe73de40":"markdown","aa0460e6":"markdown","d22c9f96":"markdown","098b1f2c":"markdown","e22aa0b0":"markdown","dd6f49a8":"markdown","106a8417":"markdown","331e00e8":"markdown","b2c065e1":"markdown","d75460ae":"markdown","1e55e83e":"markdown","9c2d57e3":"markdown","9fbc2412":"markdown","bfc85c22":"markdown","7e14d8f6":"markdown","2f009180":"markdown","d6ba36a2":"markdown","79eadd4a":"markdown","aa40e2ef":"markdown","7a262367":"markdown","db9cf672":"markdown","26415f67":"markdown","a3b6de14":"markdown","d60b914c":"markdown","88efe775":"markdown","b3c72afb":"markdown","13d5e223":"markdown","9a0d4de4":"markdown","b2be7fdd":"markdown","d34e9357":"markdown","38ee6d28":"markdown","9135ab07":"markdown","638799cf":"markdown","4ca00a6d":"markdown","ec51e3a8":"markdown","2607032a":"markdown","b565dbc8":"markdown","2633edec":"markdown","6746327b":"markdown","33592a01":"markdown","3b8935bf":"markdown","ec025e00":"markdown","f537d7de":"markdown","86dc90b2":"markdown","10a3b917":"markdown","489837cd":"markdown","44e53388":"markdown","fdec64f5":"markdown"},"source":{"b6114643":"#This librarys is to work with matrices\nimport pandas as pd \n# This librarys is to work with vectors\nimport numpy as np\n# This library is to create some graphics algorithmn\nimport seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n# import module to set some ploting parameters\nfrom matplotlib import rcParams\n# Library to work with Regular Expressions\nimport re\nimport gc\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, accuracy_score, make_scorer\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nfrom scipy import stats\n\n# This function makes the plot directly on browser\n%matplotlib inline\n\n# Seting a universal figure size \nrcParams['figure.figsize'] = 12,5","4538509b":"# Importing train dataset\ndf_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n# Importing test dataset\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\", index_col='PassengerId')","fe843bdc":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","6acd1151":"resumetable(df_train)","6d800e99":"resumetable(df_test)","5710464d":"df_train['Survived'].replace({0:'No', 1:'Yes'}, inplace=True)","48c93efa":"total = len(df_train)\nplt.figure(figsize=(12,7))\n#plt.subplot(121)\ng = sns.countplot(x='Survived', data=df_train, color='green')\ng.set_title(f\"Passengers alive or died Distribution \\nTotal Passengers: {total}\", \n            fontsize=22)\ng.set_xlabel(\"Passenger Survived?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15) \ng.set_ylim(0, total *.70)\n\nplt.show()","450af331":"#First I will look my distribuition without NaN's\n#I will create a df to look distribuition \nage_high_zero_died = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 'No')]\nage_high_zero_surv = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 'Yes')]\n\n#figure size\nplt.figure(figsize=(16,5))\n\nplt.subplot(121)\nplt.suptitle('Age Distributions', fontsize=22)\nsns.distplot(df_train[(df_train[\"Age\"] > 0)][\"Age\"], bins=24)\nplt.title(\"Distribuition of Age\",fontsize=20)\nplt.xlabel(\"Age Range\",fontsize=15)\nplt.ylabel(\"Probability\",fontsize=15)\n\nplt.subplot(122)\n\nsns.distplot(age_high_zero_surv[\"Age\"], bins=24, color='r', label='Survived')\nsns.distplot(age_high_zero_died[\"Age\"], bins=24, color='blue', label='Not Survived')\nplt.title(\"Distribution of Age by Target\",fontsize=20)\nplt.xlabel(\"Age\",fontsize=15)\nplt.ylabel(\"Probability\",fontsize=15)\nplt.legend()\n\n\nplt.show()","0ddb8ff0":"def plot_categoricals(df, col=None, cont='Age', binary=None, dodge=True):\n    tmp = pd.crosstab(df[col], df[binary], normalize='index') * 100\n    tmp = tmp.reset_index()\n\n    plt.figure(figsize=(16,12))\n\n    plt.subplot(221)\n    g= sns.countplot(x=col, data=df, order=list(tmp[col].values) , color='green')\n    g.set_title(f'{col} Distribuition', \n                fontsize=20)\n    g.set_xlabel(f'{col} Values',fontsize=17)\n    g.set_ylabel('Count Distribution', fontsize=17)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.0f}'.format(height),\n                ha=\"center\", fontsize=15) \n    g.set_ylim(0,max(sizes)*1.15)\n\n    plt.subplot(222)\n    g1= sns.countplot(x=col, data=df, order=list(tmp[col].values),\n                     hue=binary,palette=\"hls\")\n    g1.set_title(f'{col} Distribuition by {binary} ratio %', \n                fontsize=20)\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False)\n    gt.set_ylim(0,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Survived %Ratio\", fontsize=16)\n    g1.set_ylabel('Count Distribuition',fontsize=17)\n    g1.set_xlabel(f'{col} Values', fontsize=17)\n    \n    sizes = []\n    \n    for p in g1.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=10) \n    g1.set_ylim(0,max(sizes)*1.15)\n\n    plt.subplot(212)\n    g2= sns.swarmplot(x=col, y=cont, data=df, dodge=dodge, order=list(tmp[col].values),\n                     hue=\"Survived\",palette=\"hls\")\n    g2.set_title(f'{cont} Distribution by {col} and {binary}', \n                fontsize=20)\n    g2.set_ylabel(f'{cont} Distribuition',fontsize=17)\n    g2.set_xlabel(f'{col} Values', fontsize=17)\n\n\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n    plt.subplots_adjust(hspace = 0.4, top = 0.90)\n    \n    plt.show()","c206b36e":"plot_categoricals(df_train, col='Sex', cont='Age', binary='Survived')","64d5fc67":"plot_categoricals(df_train, col='Pclass', cont='Age', binary='Survived')","eb9d56bb":"(round(pd.crosstab(df_train['Survived'], [df_train['Pclass'], df_train['Sex']], \n             normalize='columns' ) * 100,2))","62e0f2f8":"plot_categoricals(df_train, col='Embarked', cont='Age', binary='Survived')\n","50cf154d":"#lets input the NA's with the highest frequency\ndf_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')","187a922e":"(round(pd.crosstab(df_train['Survived'], [df_train['Embarked'], df_train['Pclass']], \n             normalize='columns' ) * 100,2))","421fcede":"(round(pd.crosstab(df_train['Survived'], [df_train['Embarked'], df_train['Sex']], \n             normalize='columns' ) * 100,2))","00194fea":"df_train['Fare'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index()","705817e7":"df_train['Fare_log'] = np.log(df_train['Fare'] + 1)\ndf_test['Fare_log'] = np.log(df_test['Fare'] + 1)","cbfd8ec4":"# Seting the figure size\nplt.figure(figsize=(16,10))\n\n# Understanding the Fare Distribuition \nplt.subplot(221)\nsns.distplot(df_train[\"Fare\"], bins=50 )\nplt.title(\"Fare Distribuition\", fontsize=20)\nplt.xlabel(\"Fare\", fontsize=15)\nplt.ylabel(\"Density\",fontsize=15)\n\nplt.subplot(222)\nsns.distplot(df_train[\"Fare_log\"], bins=50 )\nplt.title(\"Fare LOG Distribuition\", fontsize=20)\nplt.xlabel(\"Fare (Log)\", fontsize=15)\nplt.ylabel(\"Density\",fontsize=15)\n\nplt.subplot(212)\ng1 = plt.scatter(range(df_train[df_train.Survived == 'No'].shape[0]),\n                 np.sort(df_train[df_train.Survived == 'No']['Fare'].values), \n                 label='No Survived', alpha=.5)\ng1 = plt.scatter(range(df_train[df_train.Survived == 'Yes'].shape[0]),\n                 np.sort(df_train[df_train.Survived == 'Yes']['Fare'].values), \n                 label='Survived', alpha=.5)\ng1= plt.title(\"Fare ECDF Distribution\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Fare Amount\", fontsize=15)\ng1 = plt.legend()\n\nplt.suptitle('Fare Distributions', fontsize=22)\nplt.subplots_adjust(hspace = 0.4, top = 0.90)\n\nplt.show()","65c754cc":"def ploting_cat_group(df, col):\n    plt.figure(figsize=(14,6))\n    tmp = pd.crosstab(df['Survived'], df[col], \n                      values=df['Fare'], aggfunc='mean').unstack(col).reset_index().rename(columns={0:'FareMean'})\n    g = sns.barplot(x=col, y='FareMean', hue='Survived', data=tmp)\n    g.set_xlabel(f'{col} values', fontsize=18)\n    g.set_ylabel('Fare Mean', fontsize=18)\n    g.set_title(f\"Fare Distribution by {col} \", fontsize=20)\n    \n    plt.show()","d91c8d0c":"ploting_cat_group(df_train, 'Pclass')","d2c00579":"ploting_cat_group(df_train, 'Embarked')","6d74ddb8":"ploting_cat_group(df_train, 'Sex')","9bddd7f3":"plt.figure(figsize=(14,6))\ng = sns.scatterplot(x='Age', y='Fare_log', data=df_train, hue='Survived')\ng.set_title('Fare Distribution by Age', fontsize= 22)\ng.set_xlabel('Age Distribution', fontsize=18)\ng.set_ylabel(\"Fare Log Distribution\", fontsize=18)\n\nplt.show()","10b1acdd":"df_train.groupby(['Survived', 'Pclass'])['Age'].mean().unstack('Survived').reset_index()","dacc6d1f":"df_train['Name'].unique()[:10]","d5feea0b":"# Extracting the prefix of all Passengers\ndf_train['Title'] = df_train.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\ndf_test['Title'] = df_test.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\n(df_train['Title'].value_counts(normalize=True) * 100).head(5)\n","27651adf":"#Now, I will identify the social status of each title\n\nTitle_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n}\n    \n# we map each title to correct category\ndf_train['Title'] = df_train.Title.map(Title_Dictionary)\ndf_test['Title'] = df_test.Title.map(Title_Dictionary)","4550e557":"plot_categoricals(df_train, col='Title', cont='Age', binary='Survived')","9dc41c0c":"#Let's group the median age by sex, pclass and title, to have any idea and maybe input in Age NAN's\nage_group = df_train.groupby([\"Sex\",\"Pclass\",\"Title\"])[\"Age\"]\n\n#printing the variabe that we created by median\nage_group.median().unstack('Pclass').reset_index()","452c08b6":"#inputing the values on Age Na's \n# using the groupby to transform this variables\ndf_train.loc[df_train.Age.isnull(), 'Age'] = df_train.groupby(['Sex','Pclass','Title']).Age.transform('median')\ndf_test.loc[df_train.Age.isnull(), 'Age'] = df_test.groupby(['Sex','Pclass','Title']).Age.transform('median')\n\n# printing the total of nulls in Age Feature\nprint(df_train[\"Age\"].isnull().sum())\n","17c2d3a9":"#df_train.Age = df_train.Age.fillna(-0.5)\n\n#creating the intervals that we need to cut each range of ages\ninterval = (0, 5, 12, 18, 25, 35, 60, 120) \n\n#Seting the names that we want use to the categorys\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\n# Applying the pd.cut and using the parameters that we created \ndf_train[\"Age_cat\"] = pd.cut(df_train.Age, interval, labels=cats)\ndf_test[\"Age_cat\"] = pd.cut(df_test.Age, interval, labels=cats)\n\n# Printing the new Category\ndf_train[\"Age_cat\"].unique()","9e08e936":"plot_categoricals(df_train, col='Age_cat', cont='Fare', binary='Survived')","49061e4e":"plot_categoricals(df_train, col='SibSp', cont='Age', binary='Survived')","a1e6896c":"plot_categoricals(df_train, col='Parch', cont='Age', binary='Survived', dodge=False)","5fbc2770":"#Create a new column and sum the Parch + SibSp + 1 that refers the people self\ndf_train[\"FSize\"] = df_train[\"Parch\"] + df_train[\"SibSp\"] + 1\ndf_test[\"FSize\"] = df_test[\"Parch\"] + df_test[\"SibSp\"] + 1\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', \n              5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large',\n              11: 'Large'}\n\ndf_train['FSize'] = df_train['FSize'].map(family_map)\ndf_test['FSize'] = df_test['FSize'].map(family_map)","ad6e4e87":"plot_categoricals(df_train, col='FSize', cont='Fare', binary='Survived', dodge=True)","7dcacafa":"## I saw this code in another kernel and it is very useful\n## Link: https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic\nimport string\n\ndef extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families","99137e75":"df_train['Family'] = extract_surname(df_train['Name'])\ndf_test['Family'] = extract_surname(df_test['Name'])","cf52a63c":"df_train['Ticket'].value_counts()[:10]","6b24d118":"df_train['Ticket_Frequency'] = df_train.groupby('Ticket')['Ticket'].transform('count')\ndf_test['Ticket_Frequency'] = df_test.groupby('Ticket')['Ticket'].transform('count')","c5064a2f":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]","5e68cd7d":"df_train.groupby(['Survived', 'FSize'])['Fare'].mean().unstack('FSize').reset_index()","124eb628":"#Filling the NA's with -0.5\ndf_train.Fare = df_train.Fare.fillna(-1)\ndf_test.Fare = df_test.Fare.fillna(-1)\n#intervals to categorize\nquant = (-1, 0, 12, 30, 80, 100, 200, 600)\n\n#Labels without input values\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4', 'quart_5', 'quart_6']\n\n#doing the cut in fare and puting in a new column\ndf_train[\"Fare_cat\"] = pd.cut(df_train.Fare, quant, labels=label_quants)\ndf_test[\"Fare_cat\"] = pd.cut(df_test.Fare, quant, labels=label_quants)","866d275e":"plot_categoricals(df_train, col='Fare_cat', cont='Age', binary='Survived', dodge=False)","dcda22a8":"# Excellent implementation from: \n# https:\/\/www.kaggle.com\/franjmartin21\/titanic-pipelines-k-fold-validation-hp-tuning\n\ndef cabin_extract(df):\n    return df['Cabin'].apply(lambda x: str(x)[0] if(pd.notnull(x)) else str('M'))\n\ndf_train['Cabin'] = cabin_extract(df_train)\ndf_test['Cabin'] = cabin_extract(df_test)","4f568a23":"plot_categoricals(df_train, col='Cabin', cont='Age', binary='Survived', dodge=True)","5b08e809":"pd.crosstab(df_train['Cabin'], df_train['Pclass'])","24259eea":"df_train['Cabin'] = df_train['Cabin'].replace(['A', 'B', 'C'], 'ABC')\ndf_train['Cabin'] = df_train['Cabin'].replace(['D', 'E'], 'DE')\ndf_train['Cabin'] = df_train['Cabin'].replace(['F', 'G'], 'FG')\n# Passenger in the T deck is changed to A\ndf_train.loc[df_train['Cabin'] == 'T', 'Cabin'] = 'A'\n\ndf_test['Cabin'] = df_test['Cabin'].replace(['A', 'B', 'C'], 'ABC')\ndf_test['Cabin'] = df_test['Cabin'].replace(['D', 'E'], 'DE')\ndf_test['Cabin'] = df_test['Cabin'].replace(['F', 'G'], 'FG')\ndf_test.loc[df_test['Cabin'] == 'T', 'Cabin'] = 'A'","d4a64ac2":"from pandas.api.types import CategoricalDtype \nfamily_cats = CategoricalDtype(categories=['Alone', 'Small', 'Medium', 'Large'], ordered=True)","5c9901be":"df_train.FSize = df_train.FSize.astype(family_cats)\ndf_test.FSize = df_test.FSize.astype(family_cats)","34ee4f8a":"df_train.Age_cat = df_train.Age_cat.cat.codes\ndf_train.Fare_cat = df_train.Fare_cat.cat.codes\ndf_test.Age_cat = df_test.Age_cat.cat.codes\ndf_test.Fare_cat = df_test.Fare_cat.cat.codes\ndf_train.FSize = df_train.FSize.cat.codes\ndf_test.FSize = df_test.FSize.cat.codes","6043cf84":"#Now lets drop the variable Fare, Age and ticket that is irrelevant now\ndf_train.drop([ 'Ticket', 'Name'], axis=1, inplace=True)\ndf_test.drop(['Ticket', 'Name', ], axis=1, inplace=True)\n#df_train.drop([\"Fare\", 'Ticket', 'Age', 'Cabin', 'Name', 'SibSp', 'Parch'], axis=1, inplace=True)\n#df_test.drop([\"Fare\", 'Ticket', 'Age', 'Cabin', 'Name', 'SibSp', 'Parch'], axis=1, inplace=True)","70525bb0":"df_test['Survived'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )","2bfdafb5":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['Family'] = le.fit_transform(df['Family'].astype(str))","e99a0fdc":"df = pd.get_dummies(df, columns=['Sex', 'Cabin', 'Embarked', 'Title'],\\\n                          prefix=['Sex', \"Cabin\", 'Emb', 'Title'], drop_first=True)\n\ndf_train, df_test = df[df['Survived'] != 'test'], df[df['Survived'] == 'test'].drop('Survived', axis=1)\ndel df","17b6c951":"df_train['Survived'].replace({'Yes':1, 'No':0}, inplace=True)","0d175830":"print(f'Train shape: {df_train.shape}')\nprint(f'Train shape: {df_test.shape}')","eb674a0f":"df_train.drop(['Age', 'Fare','Fare_log','Family', 'SibSp', 'Parch'], axis=1, inplace=True)\ndf_test.drop(['Age', 'Fare','Fare_log','Family', 'SibSp', 'Parch'], axis=1, inplace=True)","4f06cca6":"X_train = df_train.drop([\"Survived\",\"PassengerId\"],axis=1)\ny_train = df_train[\"Survived\"]\n\nX_test = df_test.drop([\"PassengerId\"],axis=1)","b6b9c3b7":"resumetable(X_train)","5b93b67a":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","2e8daec0":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier(n_estimators=100))]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(n_estimators=100))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, \n                                 cv= 5, scoring=scoring,\n                                 n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.show()","eed11b7e":"import time\n\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': params['max_depth'],\n        'max_features': params['max_features'],\n        'n_estimators': params['n_estimators'],\n        'min_samples_split': params['min_samples_split'],\n        'criterion': params['criterion']\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 10\n    count=1\n\n    skf = StratifiedKFold(n_splits=FOLDS, random_state=42, shuffle=True)\n\n    kf = KFold(n_splits=FOLDS, shuffle=False, random_state=42)\n\n    score_mean = 0\n    for tr_idx, val_idx in kf.split(X_train, y_train):\n        clf = RandomForestClassifier(\n            random_state=4, \n            verbose=0,  n_jobs=-1, \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(accuracy_score)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean \/ FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean \/ FOLDS)\n\nrf_space = {\n    'max_depth': hp.choice('max_depth', range(2,8)),\n    'max_features': hp.choice('max_features', range(1,X_train.shape[1])),\n    'n_estimators': hp.choice('n_estimators', range(100,500)),\n    'min_samples_split': hp.choice('min_samples_split', range(5,35)),\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n}","a64b42e2":"best = fmin(fn=objective,\n            space=rf_space,\n            algo=tpe.suggest,\n            max_evals=40, \n            # trials=trials\n           )","23579b7f":"best_params = space_eval(rf_space, best)\nbest_params","5abf9df8":"clf = RandomForestClassifier(\n        **best_params, random_state=4,\n        )\n\nclf.fit(X_train, y_train)\n\ny_preds= clf.predict(X_test)\n\nsubmission['Survived'] = y_preds.astype(int)\nsubmission.to_csv('Titanic_rf_model_pred.csv')","c9ebaa66":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                  np.unique(y_train),\n                                                  y_train)","5316d849":"def objective_logreg(params):\n    time1 = time.time()\n    params = {\n        'tol': params['tol'],\n        'C': params['C'],\n        'solver': params['solver'],\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 10\n    count=1\n\n    skf = StratifiedKFold(n_splits=FOLDS, random_state=42, shuffle=True)\n\n    kf = KFold(n_splits=FOLDS, shuffle=False, random_state=42)\n\n    score_mean = 0\n    for tr_idx, val_idx in kf.split(X_train, y_train):\n        clf = LogisticRegression(\n            random_state=4,  \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        score = make_scorer(accuracy_score)(clf, X_vl, y_vl)\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean \/ FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean \/ FOLDS)\n\nspace_logreg = {\n    'tol' : hp.uniform('tol', 0.00001, 0.001),\n    'C' : hp.uniform('C', 0.001, 2),\n    'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n}","5f9d46c8":"best = fmin(fn=objective_logreg,\n            space=space_logreg,\n            algo=tpe.suggest,\n            max_evals=45, \n            # trials=trials\n           )","4367b8cb":"best_params = space_eval(space_logreg, best)\nbest_params","0210c280":"clf = LogisticRegression(\n        **best_params, random_state=4,\n        )\n\nclf.fit(X_train, y_train)\n\ny_preds= clf.predict(X_test)\n\nsubmission['Survived'] = y_preds.astype(int)\nsubmission.to_csv('Titanic_logreg_model_pred.csv')\n\n","3b7023b9":"## Ticket feature\n- Understanding and creading new feature","00b09186":"<br>\nDescription of Fare variable<br>\n- Min: 0<br>\n- Median: 14.45<br>\n- Mean: 32.20<br>\n- Max: 512.32<br> \n- Std: 49.69<br>\n\n","799a4cb5":"Cool. We can see that only 38.38% of passengers survived. <br>\nLet's explore our other featuers and try to find some patterns","bc8d97f0":"# Preprocessing","68082759":"# Let's start exploring Age Column\nI will start by the \"simplest\" columns that are columns that don't need some transformations or have only few unique values.\nThe first objective is to:\n- Explore the features\n- Imput missing values\n- See the distribution of numerical and categorical features\n- Understand the difference between groups that survived and not","4c63678e":"# HyperOpt - Automated Bayeasian Hyperparameter serach. ","1a8eebe2":"## <font color=\"red\">If this kernel were useful for you, please <b>UPVOTE<\/b> the kernel =)<\/font>\nAlso, don't forget to give me your feedback, it's many important to me","ead61823":"# Encoding and getting Dummies of categorical features","7c18a644":"### Seting Cabin into Groups","00a043b3":"The fist class passengers has highest Fare mean, that make many sense. But we can't see a high difference between second and third class Fare mean","ae2e333c":"## Dropping unecessary features","c1f96126":"We can see that 55% of passengers are in the 3rd Class and also, is the Class where more people died. <br>\nLet's use the powerful <b>pd.crosstab<\/b> to see the distribution of Pclass by Sex and get the ratio of survivors","2f42d4ce":"## Crossing Embarked by Sex and Survived","d35de4ca":"# Objective LogReg","a4a2b6c4":"<h2>Competition Description: <\/h2>\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","ec747904":"# Let's use some features to help us fill Age NaN's ","367b72c8":"Cool. Now we can see meaningful informations about the passengers. <br>\nThe distribution of Ages and Gender by Survived can show us a interesting pattern in people who survived and who not","024c80e3":"Now we might have information enough to think about the model structure","470f9665":"### Let's fill na's in Embarked","be6b40cc":"# Fare Column","789ccf07":"<a id=\"Known\"><\/a> <br> \n# **4. Exploring the data:** \n- Different of the other Kernel, as now I am more experienced in data science, I will start by the target distribution","e3cdc1dd":"Very interesting information. The data has 18 Officer's and we can note that Officer's have the highest Age mean. <br>\nCuriously we can note that \"Master\" has a very low Age distribution. It sounds very strange to me. ","8bb609a4":"Cool. We can note that the big part of passengers paid less than USD 100. We can't see some difference between the survived or not group. <br>\nI will try cross Fare by other features and try to find some interesting patterns","91d1f403":"Now it look's better and clearly","ba84b80c":"## Fare mean by Sex","6bd7b2e6":"# End of EDA","39865cde":"# PClass\n- Other feature that I think that could be important to understand the passenger's survivors\n- Let's understand distributions of Pclass and how it is distributed in considering our target feature","be9c6bb7":"I am using the beapproachs as possible but if you think I can do anything another best way, please, let me know.","c851d119":"Ticket is a very sparse data, with many values. So, lets try associate it with Family's and feed our model\n","9a728274":"## Crossing Embarked by PClass and Survived","45181f43":"## Mapping the titles","9a250cdf":"# Names Column","ed3b2e07":"## Setting X and Y","21157d13":"We can note that all names have the titles of passengers. Let's use regex to extract titles of passengers.","2e8bfbc2":"We can see that We can infer that pooverty people had more probability to die.","221205f5":"# Modelling\n- To a better understanding of the modelling part, I will delete df and train and ","8dd348c2":"# Gender Column\nUnderstanding Gender distribution and distribution by target","7a649ee1":"<a id=\"Introduction\"><\/a> <br> \n# **1. Introduction:** \n<h3> The data have 891 entries on train dataset and 418 on test dataset<\/h3>\n- 10 columns in train_csv and 9 columns in train_test\n","f93e31da":"# Categorical features by Fare","beaf68a8":"# Ploting Title Distributions","fe73de40":"<a id=\"Librarys\"><\/a> <br> \n# **2. Importing Librarys:** ","aa0460e6":"## Fare mean by Pclass","d22c9f96":"## Fare by Age","098b1f2c":"In df test, we have missing values only on ","e22aa0b0":"## Looking quantiles of Fare ","dd6f49a8":"## Running the HyperOpt to get the best params","106a8417":"<i>*I'm from Brazil, so english is not my first language, sorry about some mistakes<\/i>","331e00e8":"## Best params ","b2c065e1":"People of C has a highest Fare mean.","d75460ae":"<h1> Welcome to my Titanic Kernel! <\/h1>\nThis kernel covers: Data Exploration and Visualization, data handling and modelling, Features preprocessing, ML pipeline, automated hyper parameter with HyperOpt and prediction of a dependent variable ('survived').  \n\nWhen I started on Data Science field, my second work on Kaggle was on titanic Dataset and now, I want to improve my work here.\n\nI will use a easy code that maybe could be useful to many people that are starting on Data Science or PyData libraries.","1e55e83e":"## Fare mean by Embarked","9c2d57e3":"# Ploting Family Size","9fbc2412":"## Summary of df train","bfc85c22":"Cool!!! We can see that 60% of passengers are traveling alone. Curiosly, we can note that these people have a highest ratio of Not survived Passengers. <br>\nThe chance to survive is highest to people with small families on the boat. \n","7e14d8f6":"## Running LogReg HyperOpt","2f009180":"# Parch\tfeature\nThe feature refers to parents \/ children aboard the Titanic\t<br>","d6ba36a2":"### Dummies","79eadd4a":"Cool!!! Exactly what I tought. We can see that in 3 class we have a smallest Age mean","aa40e2ef":"Based on informations of the boat and the confirmation of te crosstab.\n- ABC cabins are to first class\n- DE cabins are to first and second class\n- FG are majority to third class\n- M are the missing values\n- On the Boat Deck there were **6** rooms labeled as **T, U, W, X, Y, Z**, but only the **T** cabin is present in the dataset.","7a262367":"# Keep thinking about Familys\n### Extracting Sur Names \n- Taking advantage that we are dealing with family features, lets extract sur name from Name Features","db9cf672":"## Let's see the means of Fare ","26415f67":"Very cool! Based on the result of our CVLet's try model LogReg and XGBClassifier to predict who will survive or not","a3b6de14":"# Embarked Feature\n- Exploring the Distributions of the feature\n- Filling Na's values (we have only 2 missing values in this feature)","d60b914c":"If you want many other simple kernels with pythonic code <a href=\"https:\/\/www.kaggle.com\/kabure\/kernels\">CLICK HERE<\/a> <br>\n\n","88efe775":"Another interesting information. We can see that 72.4% of all passengers embarked in \"S\" (Southampton). <br>\nAlso, 47% of all died passengers is from S.","b3c72afb":"# Sibsp\tfeature\nthis feature refers to siblings \/ spouses aboard the Titanic\t\n","13d5e223":"We can see that we have we have different ratios when considering the Embarked place and Sex. It could be useful to build some features. ","9a0d4de4":"## Summary of df test","b2be7fdd":"## Ploting Fare Distribution","d34e9357":"### Let's see the extracted titles","38ee6d28":"<h3>Data Dictionary<\/h3><br>\nVariable\tDefinition\tKey<br>\n<b>survival<\/b>\tSurvival\t0 = No, 1 = Yes<br>\n<b>pclass<\/b>\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd<br>\n<b>sex<\/b>\tSex\t<br>\n<b>Age<\/b>\tAge in years\t<br>\n<b>sibsp<\/b>\t# of siblings \/ spouses aboard the Titanic\t<br>\n<b>parch<\/b>\t# of parents \/ children aboard the Titanic\t<br>\n<b>ticket<\/b>\tTicket number\t<br>\n<b>fare<\/b>\tPassenger fare\t<br>\n<b>cabin<\/b>\tCabin number\t<br>\n<b>embarked\t<\/b>Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton<br>\n<h3>Variable Notes<\/h3><br>\n<b>pclass: <\/b>A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n<b>age: <\/b>Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n<b>sibsp:<\/b> The dataset defines family relations in this way...<br>\n- <b>Sibling <\/b>= brother, sister, stepbrother, stepsister<br>\n- <b>Spouse <\/b>= husband, wife (mistresses and fianc\u00e9s were ignored)<br>\n\n<b>parch: <\/b>The dataset defines family relations in this way...<br>\n- <b>Parent<\/b> = mother, father<br>\n- <b>Child <\/b>= daughter, son, stepdaughter, stepson<br>\n\nSome children travelled only with a nanny, therefore parch=0 for them.<br>","9135ab07":"<a id=\"Model\"><\/a> <br> \n# **6. Modelling Pipeline of models to find the algo that best fit our problem ** ","638799cf":"# HyperOpt with Random Forest","4ca00a6d":"<a id=\"Known\"><\/a> <br> \n# **3. First look at the data:** \n- I will implement a function to we summary all columns in a meaningful table.","ec51e3a8":"I noted one interesting thing. <br>\nIn first and second class the female have 92%+ of survivors, and in 3rd class the ratio is 50% in female survivors<br>\nAnother interesting information is that in first class the percent of male survivors are almost 37% and in 2 and 3 class the ratio is 15.7 and 13.5 respectivelly.","2607032a":"### Calculating the class_weights","b565dbc8":"_______________________________________________\n# Predicting X_test with Logreg and HyperOpt","2633edec":"# Table of Contents:\n\n**1. [Introduction](#Introduction)** <br>\n**2. [Librarys](#Librarys)** <br>\n**3. [Knowning the data](#Known)** <br>\n**4. [Exploring some Variables](#Explorations)** <br>\n**5. [Preprocessing](#Prepocess)** <br>\n**6. [Modelling](#Model)** <br>\n**7. [Validation](#Validation)** <br>\n","6746327b":"### Encoding\n","33592a01":"Cool!! We can see very important information about all our data. <br>\nOur target is \"Survived\" column that informs if the passenger survived or not the disaster","3b8935bf":"# Stay tuned and don't forget to votesup this kernel =)","ec025e00":"Cool! We can see see the Fare distribution by Age and confirm and infer some questions. For example, the first class, probably has a highest age mean. Let's confirm that. ","f537d7de":"# Creating the Family Size feature","86dc90b2":"# Ploting Age Cat Distributions - Fare","10a3b917":"## Importing Datasets","489837cd":"Interesting! A big part of all passengers has between 20 to 40 old years. <br>\nWhen we analyze the distribution by the target we can note that youngest adults has a highest density in not survived passengers.\n\nI will continue working in Age feature but before, I will try to understand the other columns. Maybe it could work well together ","44e53388":"## Predicting the X_test with Random Forest","fdec64f5":"# Geting the Fare Log "}}