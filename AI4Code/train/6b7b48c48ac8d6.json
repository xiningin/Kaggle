{"cell_type":{"e386816a":"code","8c6f67c8":"code","2ba8c537":"code","ea746144":"code","462a446c":"code","9290a8d3":"code","78f5d2a2":"code","bbe08d81":"code","b689b06a":"code","3eddc39b":"code","f03ae0d5":"code","05079adb":"code","e47c7427":"code","d392f726":"code","ea6d243a":"code","b40bcf72":"code","b50f9088":"code","c5a384f7":"code","a566ddf6":"code","65164796":"code","6ba62ba1":"code","b5555188":"code","881ad8fe":"code","a8776f46":"code","a8ff7ce6":"code","fe4e27d6":"code","bb413448":"code","acef1572":"code","2a0ff611":"code","85328c51":"code","bb3d18b1":"code","342066ec":"code","187db756":"code","a71ee8b4":"code","16dca1b8":"code","c840d2fa":"code","e9d2fca8":"code","21c833c2":"code","5092b7c7":"code","62735d9f":"code","64f3ebcd":"code","ca02e571":"code","93d1898e":"code","77516d49":"code","0e55ac12":"code","87310013":"code","d0e32436":"markdown","3c91544e":"markdown","490e20a9":"markdown","818aae6b":"markdown","a1484dda":"markdown","436249c6":"markdown","2ae83f52":"markdown","914bae08":"markdown","79e63163":"markdown","be654c37":"markdown","e857f592":"markdown","3a939772":"markdown","44bc4b6e":"markdown"},"source":{"e386816a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score, auc, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import label_binarize, LabelEncoder\nfrom scipy import stats\nfrom itertools import cycle","8c6f67c8":"data = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndata.head()","2ba8c537":"print(f'Data length: {len(data)}')","ea746144":"#check missing data\ndata.isnull().sum()","462a446c":"#correlation matrix\nplt.figure(figsize=(10,5))\nheatmap = sns.heatmap(data.corr(), annot=True, fmt=\".1f\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\nplt.show()","9290a8d3":"#Plot pairwise relationships in data for few features  (plot size constraint)\ncols_sns = ['residual sugar', 'chlorides', 'density', 'pH', 'alcohol', 'quality']\nsns.set(style=\"ticks\")\nsns.pairplot(data[cols_sns], hue='quality')","78f5d2a2":"sns.countplot(x='quality', data=data)","bbe08d81":"# Features distribution over quality's possible values\nfig, ax = plt.subplots(4, 3, figsize=(15, 15))\nfor var, subplot in zip(data.columns, ax.flatten()):\n    if var == \"quality\":\n        continue\n    else:\n        sns.boxplot(x=data['quality'], y=data[var], data=data, ax=subplot)","b689b06a":"features, labels = data.loc[:,data.columns !='quality'], data['quality']","3eddc39b":"sns.distplot(labels, kde=True, hist=False)\nplt.title('KDE: Kernel Density Estimation')\nplt.show()","f03ae0d5":"#scaling data\nscaler = MinMaxScaler()\nX = scaler.fit_transform(features)","05079adb":"#classic split the data into train and test sets for unbalanced data isn't a good idea\nxtr, xts, ytr, yts = train_test_split(X, labels, test_size=0.3, random_state=42, shuffle = True)\n#We'll opt for stratified shuffled split for better class proportions\nsss = StratifiedShuffleSplit(test_size=0.3)\nfor train_index, test_index in sss.split(X, labels):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = labels[train_index], labels[test_index]","e47c7427":"#Checking if train and test lables have the same set of possible values\ny_train.unique(), y_test.unique()","d392f726":"logreg = LogisticRegression(multi_class='ovr', class_weight='balanced', random_state=42)","ea6d243a":"logreg.fit(X_train, y_train)","b40bcf72":"y_pred = logreg.predict(X_test)","b50f9088":"print(classification_report(y_test, y_pred))","c5a384f7":"classes_q = sorted(data.quality.unique())\nclasses_q","a566ddf6":"#For ROC curves we have to binarize lables\ny_test_bin = label_binarize(y_test, classes=classes_q)\ny_pred_bin = label_binarize(y_pred, classes=classes_q)\n#Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(classes_q)):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_bin.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","65164796":"#ROC for a specific class\nlw = 2\nplt.plot(fpr[2], tpr[2], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","6ba62ba1":"#ROC for multiclass #sklearn doc\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classes_q))]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(len(classes_q)):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= len(classes_q)\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure(figsize=(10,5))\n# plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n#          label='micro-average ROC curve (area = {0:0.2f})'\n#                ''.format(roc_auc[\"micro\"]),\n#          color='deeppink', linestyle=':', linewidth=4)\n\n# plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n#          label='macro-average ROC curve (area = {0:0.2f})'\n#                ''.format(roc_auc[\"macro\"]),\n#          color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'magenta'])\nfor i, color in zip(range(len(classes_q)), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of quality {0} (area = {1:0.2f})'\n             ''.format(classes_q[i], roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","b5555188":"#Initial dataset\ndata.head()","881ad8fe":"#Transform labels into binary ones: 'bad' and 'good'\nbins = (2, 6.5, 8)\nquality_names = ['bad', 'good']\ndata['quality'] = pd.cut(data.quality, bins=bins, labels=quality_names)\ndata.head()","a8776f46":"sns.countplot(data=data, x='quality')\nplt.title('Quality Count Plot')\nplt.show()","a8ff7ce6":"#Plot pairwise relationships in data for few features  (plot size constraint)\ncols_sns = ['residual sugar', 'chlorides', 'density', 'pH', 'alcohol', 'quality']\nsns.set(style=\"ticks\")\nsns.pairplot(data[cols_sns], hue='quality')","fe4e27d6":"# Features distribution over quality's possible values\nfig, ax = plt.subplots(4, 3, figsize=(15, 15))\nfor var, subplot in zip(data.columns, ax.flatten()):\n    if var == \"quality\":\n        continue\n    else:\n        sns.boxplot(x=data['quality'], y=data[var], data=data, ax=subplot)","bb413448":"#Encoding labels\nlabel_encoder = LabelEncoder()\ndata['quality'] = label_encoder.fit_transform(data['quality'])\ndata.head()","acef1572":"#Prepare data\nX, y = data.loc[:, data.columns != 'quality'], data['quality']\nnew_scaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n#We'll opt for stratified shuffled split again for better class proportions\nnew_sss = StratifiedShuffleSplit(test_size=0.3)\nfor train_index, test_index in new_sss.split(X, labels):\n    new_X_train, new_X_test = X[train_index], X[test_index]\n    new_y_train, new_y_test = y[train_index], y[test_index]","2a0ff611":"log_reg = LogisticRegression(random_state=42)","85328c51":"log_reg.fit(new_X_train, new_y_train)","bb3d18b1":"logreg_y_pred = log_reg.predict(new_X_test)","342066ec":"print(f'Precision Score: {precision_score(new_y_test, logreg_y_pred)}')\nprint(f'Recall Score: {recall_score(new_y_test, logreg_y_pred)}')\nprint(f'F1-Score: {f1_score(new_y_test, logreg_y_pred)}') ","187db756":"#Cross Validation for Logistic Regression\nlg = LogisticRegression()\ncv_lg_result = cross_val_score(lg, X, y, cv=5, scoring='f1_macro')","a71ee8b4":"print(f'Mean F1-Score of Cross Validation {np.mean(cv_lg_result)}')","16dca1b8":"#Grid Search \ngrid ={\"C\": [0.001,0.01,0.1,1,10,100]}\nlg_ = LogisticRegression()\nlg_cv = GridSearchCV(lg_, grid, cv=3)\nlg_cv.fit(new_X_train, new_y_train)\n\n#hyperparameters\nprint(f\"Tuned hyperparameters: {lg_cv.best_params_}\")\nprint(f\"Best score: {lg_cv.best_score_}\")","c840d2fa":"best_logreg = lg_cv.best_estimator_\nbest_logreg","e9d2fca8":"y_gs_pred = best_logreg.predict(new_X_test)\nprint(\"With Grid Search...\")\nprint(f'Precision Score: {precision_score(new_y_test, y_gs_pred)}')\nprint(f'Recall Score: {recall_score(new_y_test, y_gs_pred)}')\nprint(f'F1-Score: {f1_score(new_y_test, y_gs_pred)}') ","21c833c2":"#ROC\nfpr, tpr, thresholds = roc_curve(new_y_test, y_gs_pred)\nplt.plot([0,1], [0,1], '--k')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","5092b7c7":"#Confusion Matrix\ncm = confusion_matrix(new_y_test, y_gs_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.show()","62735d9f":"rf = RandomForestClassifier(random_state=42)","64f3ebcd":"rf.fit(new_X_train, new_y_train)","ca02e571":"y_rf_pred = rf.predict(new_X_test)","93d1898e":"print('Random Forest Performance...')\nprint(f'Precision Score: {precision_score(new_y_test, y_rf_pred)}')\nprint(f'Recall Score: {recall_score(new_y_test, y_rf_pred)}')\nprint(f'F1-Score: {f1_score(new_y_test, y_rf_pred)}') ","77516d49":"#ROC\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(new_y_test, y_rf_pred)\nplt.plot([0,1], [0,1], '--k')\nplt.plot(fpr_rf, tpr_rf)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","0e55ac12":"rf_param_grid = {\"n_estimators\": np.arange(2,50),\n                \"max_depth\": np.arange(2,50),\n                \"min_samples_split\": np.arange(2,50),\n                \"min_samples_leaf\":np.arange(2,50),\n                \"max_leaf_nodes\": np.arange(2,50)}\n\ni=1\nfor param, range_param in rf_param_grid.items():\n        rf_grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n                              param_grid={param:range_param},\n                              scoring='f1_macro')\n        rf_grid.fit(new_X_train, new_y_train)\n        df = pd.DataFrame(rf_grid.cv_results_)\n        plt.figure(figsize=(20,5))\n        plt.subplot(2,3,i)\n        plt.plot(range_param, df.mean_test_score.values)\n        plt.title(param)\n        i += 1","87310013":"#Based on the plots above we can test the following model\nrf_test = RandomForestClassifier(n_estimators=28,\n                                max_depth=14,\n                                random_state=42)\nrf_test.fit(new_X_train, new_y_train)\nyy = rf_test.predict(new_X_test)\nf1_score(new_y_test, yy)","d0e32436":"Bigger area in the top left corner means Better results. Considering the task as a binary classification one may give better results","3c91544e":"Note: We'll consider all possible qualities. In a further step we can transform it into a binary classification problem","490e20a9":"# Logistic Regression","818aae6b":"### EDA: Binary Classification Task","a1484dda":"# EDA: Exploratory Data Analysis","436249c6":"## Random Forest","2ae83f52":"We can easily notice that the results improved comparing to Logistic Regression","914bae08":"We can see that data is skewed and some qualities are more probable (5 and 6) to occur than the others.","79e63163":"# Preparing Data","be654c37":"## Logistic Regression","e857f592":"# Binary Classification","3a939772":"# Reading data","44bc4b6e":"In this part we'll try multiple algorithms."}}