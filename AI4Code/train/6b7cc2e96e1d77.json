{"cell_type":{"4b057bcd":"code","b41deb6b":"code","68ab72f8":"code","9aa2c27e":"code","6f36c30e":"code","df747baf":"code","bcd9dd71":"code","9287eb4e":"code","5fbde8f6":"code","b5c52d5a":"code","e632a1e5":"code","01ec0d05":"code","01e01fa0":"code","d48073bb":"code","16b20923":"code","3940b01a":"code","03f9b5ef":"code","01a6fa54":"code","45531d05":"code","d63c3f5e":"code","1d4ad3cb":"code","e8388ac3":"code","afbbbe39":"code","4a126ffc":"code","af5b6596":"code","af5f62ac":"code","1337ef6c":"code","caea83a2":"code","7a6fa606":"code","936224d9":"code","03197461":"code","ceefcb0d":"code","890a15d4":"code","0a3550d2":"code","fca0b8d3":"code","4e25c0d9":"markdown","e94aca1a":"markdown","aeda492a":"markdown","61bd1a74":"markdown","c0f3a91d":"markdown","024d3f3d":"markdown","8ff13d3c":"markdown","be66ddcb":"markdown","0e21ddf7":"markdown","28546e8a":"markdown","5e97f017":"markdown","f388cd51":"markdown","4e6fd905":"markdown","c324ef17":"markdown","895279b5":"markdown","388e548a":"markdown","c29c48f9":"markdown","a5a6f925":"markdown","4d1d529b":"markdown","bc241460":"markdown","711b062e":"markdown","f59e1b6b":"markdown","c405679d":"markdown","1a7048e5":"markdown"},"source":{"4b057bcd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom imblearn import under_sampling , over_sampling\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\n","b41deb6b":"# Load Data\ndata = pd.read_csv('..\/input\/lending-club-loan-data-analysis\/loan_data.csv')\ndata.head()","68ab72f8":"data.info()","9aa2c27e":"plt.figure(figsize=(12,6))\nsns.countplot(data=data, x='purpose')","6f36c30e":"data.corr()","df747baf":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr() ,cmap ='BuPu',cbar=True,annot=True,linewidths=.5)\nplt.show()","bcd9dd71":"data.corr()['not.fully.paid'].sort_values(ascending = False)","9287eb4e":"plt.figure(figsize=(12,6))\nsns.countplot(x='purpose', data=data, palette='viridis')","5fbde8f6":"plt.figure(figsize=(12,6))\nsns.countplot(x='purpose', data=data, palette='viridis', hue='not.fully.paid')","b5c52d5a":"plt.figure(figsize=(10,4))\nsns.distplot(data[data[\"not.fully.paid\"] == 0]['int.rate'], color = 'r',label='Personal Loan=0')\nsns.distplot(data[data[\"not.fully.paid\"] == 1]['int.rate'], color = 'b',label='Personal Loan=1')\nplt.legend()\nplt.title(\"int.rate Distribution\")","e632a1e5":"data.hist(bins=10 ,figsize=(16,12), color = 'Green')\nplt.show()","01ec0d05":"data.columns","01e01fa0":"data.isnull().sum()","d48073bb":"data.dtypes","16b20923":"data.nunique()","3940b01a":"data.describe()","03f9b5ef":"data['int.rate'].max()","01a6fa54":"data['installment'].max()","45531d05":"data['not.fully.paid'].value_counts().plot(kind= 'bar')","d63c3f5e":"data['not.fully.paid'].value_counts()","1d4ad3cb":"numofzero  = 100 * (data['not.fully.paid'].value_counts()[0]\/len(data))\nnumofone  = 100 * (data['not.fully.paid'].value_counts()[1]\/len(data))\n\nprint(f' The percentage of zero = {numofzero}')\nprint(f' The percentage of one = {numofone}')","e8388ac3":"datacopy =data\nno_frauds = len(datacopy[datacopy['not.fully.paid'] == 1])\nnon_fraud_indices = datacopy[datacopy[\"not.fully.paid\"] == 0].index\nrandom_indices = np.random.choice(non_fraud_indices,no_frauds, replace=False)\nfraud_indices = datacopy[datacopy[\"not.fully.paid\"] == 1].index\nunder_sample_indices = np.concatenate([fraud_indices,random_indices])\nfinal_under = datacopy.loc[under_sample_indices]\n# X_under = under_sample.drop('not.fully.paid',axis = 1)\n# y_under = under_sample['not.fully.paid']\nprint(len(final_under[final_under['not.fully.paid'] == 1]))\nprint(len(final_under[final_under['not.fully.paid'] == 0]))\n","afbbbe39":"data_lab = final_under['not.fully.paid']\ndata_lab","4a126ffc":"data_num = final_under.drop(['delinq.2yrs','purpose','not.fully.paid'],axis=1)","af5b6596":"data_cat = final_under[['purpose']]\ndata_cat.head(10)","af5f62ac":"cat_encoder = OneHotEncoder()\ndata_cat_hot = cat_encoder.fit_transform(data_cat)\ndata_cat_hot","1337ef6c":"data_cat_hot.toarray()","caea83a2":"num_pipeline = Pipeline([\n    ('std_scaler',StandardScaler())\n])\ndata_num_tr = num_pipeline.fit_transform(data_num)","7a6fa606":"num_attrs = list(data_num)\ncat_attrs = ['purpose']\n\nfullpipeline = ColumnTransformer([\n    ('num',num_pipeline,num_attrs),\n    ('cat',OneHotEncoder(),cat_attrs)\n])\n","936224d9":"data_prepared = fullpipeline.fit_transform(final_under)\ndata_prepared","03197461":"X_train, X_test, y_train, y_test = train_test_split(data_prepared,data_lab,test_size = 0.2, random_state = 0)","ceefcb0d":"\nmodels = {\n    \"LR\": LogisticRegression(),\n    \"KNN\": KNeighborsClassifier(n_neighbors=7),\n    \"RF\": RandomForestClassifier(n_estimators=150, max_depth=10),\n    \"XGB\": XGBClassifier(),\n}\n\nfor name,model in models.items():\n    print(f'Training Model {name} \\n--------------')\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    print(f'Training Accuracy: {accuracy_score(y_train, model.predict(X_train))}')\n    print(f'Testing Accuracy: {accuracy_score(y_test, y_pred)}')\n    print(f'Testing Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\n    print('-'*30)\n","890a15d4":"from imblearn.over_sampling import SMOTE\n\nmydata = pd.get_dummies(data, columns=['purpose'], drop_first=True)\nros = SMOTE(sampling_strategy=1)\nx_over = mydata.drop(['not.fully.paid'],axis =1)\ny_over = mydata['not.fully.paid']\n\n\nx_over_train, x_over_test, y_over_train, y_over_test = train_test_split(x_over, y_over, test_size = 0.2, random_state=22)\n\nx_over_train , y_over_train = ros.fit_resample(x_over_train,y_over_train)","0a3550d2":"scaler = StandardScaler()\nscaler.fit(x_over_train)\nx_over_train = scaler.transform(x_over_train)\nx_over_test = scaler.transform(x_over_test)","fca0b8d3":"for name,model in models.items():\n    print(f'Training Model {name} \\n--------------')\n    model.fit(x_over_train,y_over_train)\n    y_pred = model.predict(x_over_test)\n    print(f'Training Accuracy: {accuracy_score(y_over_train, model.predict(x_over_train))}')\n    print(f'Testing Accuracy: {accuracy_score(y_over_test, y_pred)}')\n    print(f'Testing Confusion Matrix: \\n{confusion_matrix(y_over_test, y_pred)}')\n    print('-'*30)","4e25c0d9":"## Libraries\n","e94aca1a":"#### In this section, we will look at the structure of the datasets. Firstly, we will check the features present in our data and then we will look at their data types.\n\n>  we have 13 independent variables and 1 target variable\n","aeda492a":"#### in this section we will look at the target variable , it found that The data is imbalanced , An imbalanced data can result in inaccurate \/ biased classifications of the final output. Therefore, before fitting the data into the machine learning model, we need to rebalance the data\n> there are 3 ways to handle imbalanced data \n-  under-sampling\n-  over-sampling \n-  smote     \n  \n       Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling.\n\n---\n\n       On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by using e.g. repetition, bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique)\n\nreference: https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets  \n\nVideo : https:\/\/www.youtube.com\/watch?v=JnlM4yLFNuo","61bd1a74":"## Prepare the Data for Machine Learning Algorithms","c0f3a91d":"## The model using Undersampling  \n","024d3f3d":"#### Print the data type of each variable , we can see there are 3 formats of data type : \n\n> - object: Object format means variables are categorical. Categorical variables in our dataset are: Purpose , The machine cannot deal with texts, so we will convert text columns to numbers using 'OneHotEncoder' or OrdinalEncoder\n\n\n> - int64: It represents the integer variables .\n\n>- float64: It represents the variable which have some decimal values involved. They are also numerical variables.\n\n","8ff13d3c":"### if we look at the data columns we will find that the data has one categorical column  ","be66ddcb":"#### (2) Handling Categorical attributes","0e21ddf7":"## under-sampling","28546e8a":"#### to get the number of unique values in data columns \n","5e97f017":"### (3) Transformation Pipelines \n    to handle data transformation steps that need to be executed in right order , fortunately Scikit-Learn provides the Pipeline class to help with such sequences of transformations , here is a small pipeline for the numerical attributes : \n\n    as we mentioned before the data have different scaling thus we must use Standard scaling or Min-max Scaling in this section we will use Standard , it shift and rescale the data so that they end up ranging from -1 to 1 , Some machine learning algorithms cannot handle negative variables thus we can not use standard \n\nRefernce : Hands on Machine Learning With Scikit-learn Book   \n\nPipeline Video : https:\/\/www.youtube.com\/watch?v=w9IGkBfOoic    \n \n  \nStandardization Videos :\n- Arabic :- https:\/\/www.youtube.com\/watch?v=4RmXXNxAves  \n\n- English :- https:\/\/www.youtube.com\/watch?v=mnKm3YP56PY","f388cd51":"#### Observation\n    -'int.rate' column is normally distributed. Here can see that the mean and midean is almost same .\n   \n\n","4e6fd905":"### Split the data :\n> The only way to know how well a model will generalize to new cases is to actually try it out on new cases , A better option is to split your data into two sets : the training set and test set , you train your model using the train set , and you test it using the test set .  \nScikit learn provides \"train test split\" fuction to split the dataset into multiple subsets in various ways\n\n    its common to use 80% of the data for training and hold out 20% for testing , however this depends on the size of the datasets .\n    \nRefernce : Hands on Machine Learning With Scikit-learn Book   \nVideo : https:\/\/www.youtube.com\/watch?v=fwY9Qv96DJY","c324ef17":"#### Lets plot this column using seaborn library , we will find that propose column have 7 values","895279b5":"## Getting insights about the data","388e548a":"#### Second, to filter numerical features, we can use .corr() function to select only features with high correlation to the target variable ","c29c48f9":"#### to get the five-number summary (mean , median , first second third  quatile ) + std , max , min , \n> it found that the data have different scales for instance \"int.rate\" max = .21 and \"installment\" max = 940 , Machine learning algorithms don't perform well when the input numerical attributes have very different scales , there are two common ways to get all attributes to have the same scale : min-maxScaling , Standardization .  \n  \n: to get more insights click the forward links :\n- Arabic :- https:\/\/www.youtube.com\/watch?v=4RmXXNxAves  \n- English :- https:\/\/www.youtube.com\/watch?v=mnKm3YP56PY","a5a6f925":"#### as we mentioned in previous section , machine learning algorithm cannot handle textual data , we can solve this problem using (1) One Hot Encoder (2) OrdinalEncoder  to transform categorical column \"Purpose\" to numerical column\n\n1. OrdinalEncoder :  \n             \n       In ordinal encoding, each unique category value is assigned an integer value.\n\n       For example, \u201cred\u201d is 1, \u201cgreen\u201d is 2, and \u201cblue\u201d is 3.\n\n       This is called an ordinal encoding or an integer encoding and is easily reversible. Often, integer values starting at zero are used.\n\n       For some variables, an ordinal encoding may be enough. The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n\n       It is a natural encoding for ordinal variables. For categorical variables, it imposes an ordinal relationship where no such relationship may exist. This can cause problems and a one-hot encoding may be used instead\n2. OneHotEncoder:  \n\n       For categorical variables where no ordinal relationship exists, the integer encoding may not be enough, at best, or misleading to the model at worst.\n\n       Forcing an ordinal relationship via an ordinal encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n\n       In this case, a one-hot encoding can be applied to the ordinal representation. This is where the integer encoded variable is removed and one new binary variable is added for each unique integer value in the variable\n\nRefernce : https:\/\/machinelearningmastery.com\/one-hot-encoding-for-categorical-data  \n\nVideo : https:\/\/www.youtube.com\/watch?v=6WDFfaYtN6s","4d1d529b":"#### in this section,  we will check if the data have null values \n> There are no null values","bc241460":"## The model using Oversampling - Smote  \n","711b062e":"#### Observation\n    \"revol,util\" and \"int.rate\" in moderately correlated  \n    \"installment\" and \"log.annual\" in moderately correlated  \n    \"delinq.2yrs\" is not correlate to \"not.fully.paid\"","f59e1b6b":"#### Observation\n    most loan purpose is \"debt_consolidation\"","c405679d":"### Select and Train a Model\n    in this section , we will train 3 models  \n    - LogisticRegression  \n    - KNeighborsClassifier(knn)  \n    - RandomForestClassifier  \nLogisticRegression : https:\/\/www.youtube.com\/watch?v=zM4VZR0px8E  \n\nKNN : https:\/\/www.youtube.com\/watch?v=HVXime0nQeI  \n\nRandomForestClassifier : https:\/\/www.youtube.com\/watch?v=J4Wdy0Wc_xQ","1a7048e5":"#### ColumnTransformer , \n    We have handled the categorical columns and the numerical columns separately , it would be more convenient to have a single transformer able to habdle all columns , Scikit-Learn introduced ColumnTransformer for this purpose "}}