{"cell_type":{"a5bf0354":"code","a4639ca0":"code","fa2adf7c":"code","0a891a60":"code","16bfa0d0":"code","342669b8":"code","c73ae3a4":"code","8aef8646":"code","68288ee3":"code","a52338c1":"code","821ab93d":"code","397ac669":"code","e5528d95":"code","53662158":"code","1ede677f":"code","76470aab":"code","56306828":"code","4b74ed6b":"code","f246ef8e":"code","2ee81c99":"code","daa0a2a4":"code","747d01b2":"code","a6e783e0":"code","bddbb06d":"code","6b38fdeb":"code","16cc1aed":"markdown","e4308f56":"markdown","dad43292":"markdown","0b1a0ccf":"markdown"},"source":{"a5bf0354":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a4639ca0":"data = pd.read_json('\/kaggle\/input\/quotes-dataset\/quotes.json')","fa2adf7c":"print(data.shape)\nprint(data.columns)","0a891a60":"data.head()","16bfa0d0":"quotes = np.array(data.Quote)\nprint(quotes[:10])","342669b8":"# storing all words to a words array\nall_quotes = \"\\n\\n\".join(quotes)\nall_words = sorted(set(all_quotes.split()+['\\n']))\nlen_all_words = len(all_words)\nprint(len_all_words)\nprint('\\n' in all_words)\ndel(all_quotes)","c73ae3a4":"# storing start words for later use\nstart_words = sorted(set(map(lambda x: x.split()[0], quotes)))\nlen_start_words = len(start_words)\nprint(start_words[:100])","8aef8646":"# mapping and reverse-mapping\nword2idx = {u:i for i, u in enumerate(all_words)}\nidx2word = np.array(all_words)","68288ee3":"#encoding data to be able to use it for training\nencoded_data = []\nesc_id = word2idx['\\n']\n\nfor i in quotes:\n    for j in i.split():\n        encoded_data.append(word2idx[j])\n    encoded_data.append(esc_id)\nencoded_data = np.array(encoded_data)","a52338c1":"encoded_data.shape","821ab93d":"# returns n_batches numbered quotes from quotes array\ndef get_batches(data, seq_len, n_batchs):\n    idxs = np.random.choice(len(data)-seq_len, n_batchs)\n    x = [np.array(data[idx:idx+seq_len]) for idx in idxs]\n    y = [np.array(data[idx+1:idx+seq_len+1]) for idx in idxs]\n    return (np.array(x), np.array(y))","397ac669":"#test\nx, y = get_batches(encoded_data, 100, 4)\nprint(x, y, sep = '\\n\\n')","e5528d95":"def create_model(len_all_words, embedding_size, batch_size, rnn_units):\n    return tf.keras.models.Sequential([\n        tf.keras.layers.Embedding(len_all_words, embedding_size, batch_input_shape = [batch_size, None]),\n        tf.keras.layers.LSTM(rnn_units, stateful = True, return_sequences = True),\n        tf.keras.layers.Dense(len_all_words)\n    ])\n","53662158":"model = create_model(len_all_words, 1024, 4, 1024)\nmodel.summary()","1ede677f":"pred = model(x)\nprint(x[0])\nprint(tf.squeeze(tf.random.categorical(pred[0],1)).numpy())","76470aab":"def calc_loss(a, e):\n  return tf.keras.losses.sparse_categorical_crossentropy(a, e, from_logits=True)","56306828":"LEARNING_RATE = 1E-2\nLEN_ALL_WORDS = len_all_words\nEMBEDDING_DIM = 1024\nRNN_UNITS = 1024\nBATCH_SIZE = 64\nSEQ_LEN = 100\n\nEPOCHS =1500\nWEIGHTS_PATH = '.\/mod_weights.wt'\ncheckpoint_prefix = os.path.join(WEIGHTS_PATH, \"my_ckpt\")","4b74ed6b":"optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\nmodel = create_model(LEN_ALL_WORDS, EMBEDDING_DIM, BATCH_SIZE, RNN_UNITS)","f246ef8e":"@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        y_hat = model(x)\n        loss = calc_loss(y, y_hat)\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    return loss        ","2ee81c99":"history = []\nplotter = mdl.util.PeriodicPlotter(sec=7, xlabel='Iterations', ylabel='Loss')\nfor i in tqdm(range(EPOCHS)):\n    x, y = get_batches(encoded_data, SEQ_LEN, BATCH_SIZE)\n    history.append(train_step(x, y).numpy().mean())\n    plotter.plot(history)\n    if i % 100 == 0:     \n        model.save_weights(checkpoint_prefix)","daa0a2a4":"# temp = []\n# for i in range(1000):\n#     temp.append(history[i].numpy().mean())\n# plt.plot(temp)\nplt.plot(history)","747d01b2":"model.save_weights(WEIGHTS_PATH)","a6e783e0":"model = create_model(LEN_ALL_WORDS, EMBEDDING_DIM, 1, RNN_UNITS)\nmodel.load_weights(WEIGHTS_PATH)\nmodel.build(tf.TensorShape([1, None]))","bddbb06d":"def pred_freq(model, start, length):\n  generated = []\n\n  input_ = [word2idx[start]]\n  input_ = tf.expand_dims(input_, 0)\n\n  for i in range(length):\n    pred = model(input_)\n    pred = tf.squeeze(pred, 0)\n    pred_id = tf.random.categorical(pred, 1)\n    generated.append(idx2word[pred_id[-1][0].numpy()])\n    input_ = pred_id.numpy()\n\n  return start+\" \".join(generated)\nnovel_quotes = pred_freq(model, start_words[np.random.choice(len(start_words), 1)[0]], 1000)","6b38fdeb":"print(novel_quotes)","16cc1aed":"# Model","e4308f56":"# Data Aquisision","dad43292":"# EDA and Data Preparation","0b1a0ccf":"# import stuff"}}