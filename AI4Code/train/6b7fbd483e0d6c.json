{"cell_type":{"6233e605":"code","2a379473":"code","a6d20d17":"code","962f1148":"code","a93e6e0d":"code","14567796":"code","7f0842fa":"code","6b9fb189":"code","8d7fda32":"code","3d0152a8":"code","8ce643b9":"code","8719dba7":"code","300c6080":"code","88dd95dd":"code","c4329b29":"code","d33ea08d":"code","1273d6cf":"code","3c500095":"code","2f24ad9a":"code","9e5a5878":"code","85dc5c66":"code","ec2fa785":"code","aab605aa":"code","4a63d665":"code","86d9136c":"code","13104784":"code","e9b3c084":"code","1616032e":"code","af9c8f28":"code","c2789a90":"code","cc8743d5":"code","8349f414":"code","593c2bf2":"code","23700940":"code","552ba538":"code","f242553b":"code","e5f8c98c":"code","3dd4ec46":"code","d62b7cdc":"code","6b3fd5de":"code","6c482c28":"code","a39e1cc9":"code","83e3c10f":"code","991a3098":"code","9d8cc2c7":"code","bd565c28":"code","3edad149":"code","6f8c4c5e":"code","37ade715":"code","bdb163ba":"code","0fa06f20":"code","3a0bc381":"code","f02a90bd":"code","4b422cd5":"code","4cd050dc":"code","f000200c":"code","8ab70580":"code","0e261533":"code","ab99ae3c":"code","6f1599ff":"code","c21d789a":"code","62b682a3":"code","4dbf0710":"code","03884855":"code","6de8efff":"code","5343cee8":"code","2aa9b0a9":"code","d909285d":"code","b38323b6":"code","ba0d5d04":"code","6beefcbe":"code","8e93c86a":"code","8235a0d2":"code","39c7191d":"code","48f73f80":"code","5ea19349":"code","5b0da44d":"code","0b470b54":"code","a6611697":"code","3c9e5191":"code","897757bd":"code","c6f5236d":"code","3b193806":"code","ad6fece3":"code","62bd4228":"code","3b957e80":"code","70fa29ad":"code","3182dcc8":"code","2e710d57":"code","c95f7696":"code","b49e0f3f":"code","33e27c5f":"code","49017cb6":"code","48896be0":"code","76e0cce6":"code","809978e1":"markdown","0b4c45e9":"markdown","946af97f":"markdown","87e301e9":"markdown","9115e83e":"markdown","a049240d":"markdown","7d0fa292":"markdown","b1946f0c":"markdown","f1af9b93":"markdown","8f92ef89":"markdown","e89dc0f9":"markdown","93151911":"markdown","b7f93058":"markdown","6b5f53b2":"markdown","176868dd":"markdown","caee00f3":"markdown","ef83f5d6":"markdown","034ee15c":"markdown","5e3d7a47":"markdown","faa8ff93":"markdown","7ffe29f2":"markdown","b634bc26":"markdown","10501678":"markdown","e4460f62":"markdown","d18daf5a":"markdown","4e6db633":"markdown","58b7cf09":"markdown","3d80f19d":"markdown","48e46f9e":"markdown","01cd25f2":"markdown","ca66aef1":"markdown","bbc85fc7":"markdown","b0a8a833":"markdown","d2166cb5":"markdown","27660231":"markdown","a34b49cf":"markdown","41d957f6":"markdown","6b62e897":"markdown","69bfcf9f":"markdown","f66aaae2":"markdown","be9b9f31":"markdown","eed4c467":"markdown","2dcd65f4":"markdown","8a2fc072":"markdown","5a45ffe8":"markdown","8a88d622":"markdown","e46a45b5":"markdown","f3584ea2":"markdown","67ff48c3":"markdown","7d3780fe":"markdown","89885728":"markdown","96581678":"markdown","ceeeb805":"markdown","ae070517":"markdown","8d90eae4":"markdown","f5f64acc":"markdown","348f614d":"markdown","4d6f7971":"markdown","39fcf986":"markdown","b3a0a46c":"markdown","efc4d09b":"markdown","d1ca2547":"markdown","449b8264":"markdown","b6214b21":"markdown","61886e64":"markdown","097fbbec":"markdown"},"source":{"6233e605":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix\n#importing libraries\nfrom sklearn import neighbors\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')","2a379473":"df = pd.read_csv('..\/input\/zomato.csv')\ndf.head()","a6d20d17":"df.describe()","962f1148":"# Dimensions\nprint(f'This dataset has {df.shape[0]} rows and {df.shape[1]} columns.')\nprint(f'\\nList of columns: \\n{df.columns}')","a93e6e0d":"# Null data\ndf.isnull().sum()","14567796":"df.dtypes\ndf.describe()","7f0842fa":"# Approx cost\ndf['approx_cost(for two people)'] = df['approx_cost(for two people)'].astype(str)\ndf['approx_cost(for two people)'] = df['approx_cost(for two people)'].apply(lambda x: x.replace(',', '.'))\ndf['approx_cost(for two people)'] = df['approx_cost(for two people)'].astype(float)\nprint(f'{type(df[\"approx_cost(for two people)\"][0])}')\n\n","6b9fb189":"# Transformando dados de ratings\ndf['rate_transformed'] = df['rate'].astype(str)\ndf['rate_transformed'] = df['rate_transformed'].apply(lambda x: x.split('\/')[0])\n\n# Cuidando de entradas inv\u00e1lidas\ndf['rate_transformed'] = df['rate_transformed'].apply(lambda x: x.replace('NEW', str(np.nan)))\ndf['rate_transformed'] = df['rate_transformed'].apply(lambda x: x.replace('-', str(np.nan)))\n\n# Transformando em float\ndf['rate_transformed'] = df['rate_transformed'].astype(float)\ndf.drop(['rate'], axis=1, inplace=True)\nprint(f'{type(df[\"rate_transformed\"][0])}')","8d7fda32":"df.dtypes","3d0152a8":"# Droping NA from rate_transformed\ndf_unrated = df[df['rate_transformed'].isnull()]\ndf.dropna(subset=['rate_transformed', 'approx_cost(for two people)'], inplace=True)\n\n# Drop columns\ndf.drop(['url', 'phone'], axis=1, inplace=True)\n\n# Verificando\ndf.isnull().sum()","8ce643b9":"def format_spines(ax, right_border=True):\n    \"\"\"\n    this function sets up borders from an axis and personalize colors\n    \"\"\"    \n    # Setting up colors\n    ax.spines['bottom'].set_color('#CCCCCC')\n    ax.spines['left'].set_color('#CCCCCC')\n    ax.spines['top'].set_visible(False)\n    if right_border:\n        ax.spines['right'].set_color('#CCCCCC')\n    else:\n        ax.spines['right'].set_color('#FFFFFF')\n    ax.patch.set_facecolor('#FFFFFF')","8719dba7":"import seaborn as sb\n# Value ditribution\nplt.figure(1, figsize=(18, 7))\nsb.set(style=\"whitegrid\")\nsb.countplot( x= 'rate_transformed', data=df)\nplt.title('distribution of all rates')\nplt.show()","300c6080":"df['rate_transformed'].describe()","88dd95dd":"grouped_rest = df.groupby(by='name', as_index=False).mean()\ntop_rating = grouped_rest.sort_values(by='rate_transformed', ascending=False).iloc[:10, np.r_[0, -1]]\ntop_rating","c4329b29":"# Adjusting a restaurant name\ntop_rating.iloc[1, 0] = 'Spa Cuisine'\n\n# Plotting\nfig, ax = plt.subplots(figsize=(8, 5))\nax = sns.barplot(y='name', x='rate_transformed', data=top_rating, palette='Blues_d')\nax.set_xlim([4.7, 5])\nformat_spines(ax, right_border=False)\n\nfor p in ax.patches:\n    width = p.get_width()\n    ax.text(width+0.01, p.get_y() + p.get_height() \/ 2. + 0.2, '{:1.2f}'.format(width), \n            ha=\"center\", color='grey')\n\nax.set_title('Top 10 Restaurants in Bengaluru', size=14)\nplt.show()","d33ea08d":"df['approx_cost(for two people)'].describe()","1273d6cf":"high_cost = grouped_rest.sort_values(by='approx_cost(for two people)', \n                                     ascending=False).iloc[:10, np.r_[0, -1, -2]]\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(x='name', y='approx_cost(for two people)', data=high_cost, ax=ax, palette='PuBu')\nax2 = ax.twinx()\nsns.lineplot(x='name', y='rate_transformed', data=high_cost, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nax.set_ylim(700, 980)\nax2.set_ylim([3, 5])\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = high_cost['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Higher Cost Restaurants and the Rates', size=14)\nplt.tight_layout()\nplt.show()","3c500095":"low_cost = grouped_rest.sort_values(by='approx_cost(for two people)', \n                                     ascending=True).iloc[:10, np.r_[0, -1, -2]]\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(x='name', y='approx_cost(for two people)', data=low_cost, ax=ax, palette='PuBu')\nax2 = ax.twinx()\nsns.lineplot(x='name', y='rate_transformed', data=low_cost, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nax.set_ylim([0, 2])\nax2.set_ylim([0, 5])\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = low_cost['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Lower Cost Restaurants and the Rates', size=14)\nplt.tight_layout()","2f24ad9a":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x='rate_transformed', y='approx_cost(for two people)', data=df, ax=ax)\nformat_spines(ax, right_border=False)\nax.set_title('Correlation Between Rate and Approx Cost', size=14)\nplt.show()","9e5a5878":"# Separating by Online Order and Book Table options\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\nsns.scatterplot(x='rate_transformed', y='approx_cost(for two people)', hue='online_order', \n                data=df, ax=axs[0])\nsns.scatterplot(x='rate_transformed', y='approx_cost(for two people)', hue='book_table', \n                data=df, ax=axs[1])\nformat_spines(axs[0], right_border=False)\nformat_spines(axs[1], right_border=False)\naxs[0].set_title('Cost and Rate Distribution by Online Order Option', size=14)\naxs[1].set_title('Cost and Rate Distribution by Book Table Option', size=14)\nplt.show()","85dc5c66":"# Contagem de restaurantes por oferta de Delivery\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\nsns.countplot(x='online_order', data=df, ax=axs[0], palette='Blues_d')\nformat_spines(axs[0], right_border=False)\nncount = len(df)\nfor p in axs[0].patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        axs[0].annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n                ha='center', va='bottom') # set the alignment of the text\naxs[0].set_title('Counting of Restaurants by Online Order Service', size=14)\n\n# Contagem de restaurantes por agendamento de mesa\nsns.countplot(x='book_table', data=df, ax=axs[1], palette='Blues_d')\nformat_spines(axs[1], right_border=False)\nncount = len(df)\nfor p in axs[1].patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        axs[1].annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n                ha='center', va='bottom') # set the alignment of the text\naxs[1].set_title('Counting Restaurants by Book Table Service', size=14)\n\nplt.tight_layout()\nplt.show()","ec2fa785":"x=df['online_order'].value_counts()\ncolors = ['#FEBFB3', '#E1396C']\n\ntrace=go.Pie(labels=x.index,values=x,textinfo=\"value\",\n            marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\nlayout=go.Layout(title=\"Accepting vs not accepting online orders\",width=500,height=500)\nfig=go.Figure(data=[trace],layout=layout)\npy.iplot(fig, filename='pie_chart_subplots')","aab605aa":"x=df['book_table'].value_counts()\ncolors = ['#96D38C', '#D0F9B1']\n\ntrace=go.Pie(labels=x.index,values=x,textinfo=\"value\",\n            marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\nlayout=go.Layout(title=\"Table booking\",width=500,height=500)\nfig=go.Figure(data=[trace],layout=layout)\npy.iplot(fig, filename='pie_chart_subplots')","4a63d665":"# Online order restaurants comparison\ndf_delivery = df.groupby(by='online_order').mean()\ndf_delivery","86d9136c":"# Book table restaurants comparison\ndf_delivery = df.groupby(by='book_table').mean()\ndf_delivery","13104784":"df['listed_in(type)'].value_counts()","e9b3c084":"type_rest = df.groupby(by='listed_in(type)').mean().sort_values(by='rate_transformed', ascending=False)\ntype_rest","1616032e":"rest_params = df.groupby(by='listed_in(type)', as_index=False).mean().sort_values(by='rate_transformed', \n                                                                                  ascending=False)\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=rest_params, ax=ax, palette='Blues_d')\nax2 = ax.twinx()\nsns.lineplot(x='listed_in(type)', y='rate_transformed', data=rest_params, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = rest_params['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Average Cost and Rating of Restaurants by Type', size=14)\nplt.tight_layout()","af9c8f28":"df['listed_in(city)'].value_counts()","c2789a90":"city_rest = df.groupby(by='listed_in(city)').mean().sort_values(by='rate_transformed', ascending=False)\ncity_rest","cc8743d5":"city_rest = df.groupby(by='listed_in(city)', as_index=False).mean().sort_values(by='rate_transformed', \n                                                                                  ascending=False)\nfig, ax = plt.subplots(figsize=(14, 7))\nsns.barplot(x='listed_in(city)', y='approx_cost(for two people)', data=city_rest, ax=ax, palette='Blues_d')\nax2 = ax.twinx()\nsns.lineplot(x='listed_in(city)', y='rate_transformed', data=city_rest, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,len(city_rest),1)\nys = city_rest['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Average Cost and Rating of Restaurants by City', size=14)\nplt.tight_layout()","8349f414":"# Numerical features distribution\nsns.set(style='white', palette='muted', color_codes=True)\nfig, axs = plt.subplots(1, 3, figsize=(12, 3.5))\nsns.despine(left=True)\naxs[0] = sns.distplot(df['votes'], bins=20, ax=axs[0])\naxs[1] = sns.distplot(df['approx_cost(for two people)'], bins=20, ax=axs[1], color='g')\naxs[2] = sns.distplot(df['rate_transformed'], bins=20, ax=axs[2], color='r')\n\nfig.suptitle('Numerical Feature Distribution')\nplt.setp(axs, yticks=[])\nplt.tight_layout()\nplt.show()","593c2bf2":"# Numerical features distribution\nsns.set(style='white', palette='muted', color_codes=True)\nfig, axs = plt.subplots(1, 3, figsize=(12, 3.5))\nsns.despine(left=True)\naxs[0] = sns.distplot(np.log1p(df['votes']), bins=20, ax=axs[0])\naxs[1] = sns.distplot(np.log1p(df['approx_cost(for two people)']), bins=20, ax=axs[1], color='g')\naxs[2] = sns.distplot(np.log1p(df['rate_transformed']), bins=20, ax=axs[2], color='r')\n\nfig.suptitle('Numerical Feature Log Distribution')\nplt.setp(axs, yticks=[])\nplt.tight_layout()\nplt.show()","23700940":"plt.figure(figsize=(7,7))\nRest_locations=df['location'].value_counts()[:20]\nsns.barplot(Rest_locations,Rest_locations.index,palette=\"rocket\")","552ba538":"from pandas.plotting import scatter_matrix\nattributes = ['online_order', 'book_table', 'rate_transformed', 'votes',\n       'approx_cost(for two people)']\nscatter_matrix(df[attributes], figsize=(12, 8))\nplt.show()","f242553b":"# Reading the data again to make a complete pipeline\nnew_df = pd.read_csv('..\/input\/zomato.csv')\nnew_df.head(1)","e5f8c98c":"# Filtering data\nimportant_columns = ['online_order', 'book_table', 'rate', 'votes', 'approx_cost(for two people)', 'rest_type', 'dish_liked', 'cuisines',\n                     'listed_in(type)', 'listed_in(city)']\ndata_filtered = new_df.loc[:, important_columns]\ndata_filtered.head()","3dd4ec46":"# Class for filtering data\nclass attrSelect(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        cols = ['online_order', 'book_table', 'rate', 'votes', 'approx_cost(for two people)',\n                'listed_in(type)', 'listed_in(city)']\n        \n        return X.loc[:, cols]","d62b7cdc":"# Creating a class for making some transformation\nclass transformData(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):        \n        return self\n    \n    def transform(self, X, y=None):\n        \n        # Cost column transforming\n        X['approx_cost(for two people)'] = X['approx_cost(for two people)'].astype(str)\n        X['approx_cost(for two people)'] = X['approx_cost(for two people)'].apply(lambda x: x.replace(',', '.'))\n        X['approx_cost(for two people)'] = X['approx_cost(for two people)'].astype(float)\n        \n        # Rate column transforming\n        X['rate'] = X['rate'].astype(str).apply(lambda x: x.split('\/')[0])\n        X['rate'] = X['rate'].apply(lambda x: x.replace('NEW', str(np.nan)))\n        X['rate'] = X['rate'].apply(lambda x: x.replace('-', str(np.nan)))\n        X['rate'] = X['rate'].astype(float)\n        \n        return X","6b3fd5de":"\n# Creating a class for handle null data\nclass handleNullData(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):        \n        return self\n    \n    def transform(self, X, y=None):        \n        # For now we will just drop null data. In the future we can try another option\n        return X.dropna()","6c482c28":"# Class for log transformation\nclass logTransformation(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):        \n        return self\n    \n    def transform(self, X, y=None):        \n        return np.log1p(X)","a39e1cc9":"# Splitting data\nX = new_df.copy()\nattr_selector = attrSelect()\nX_filtered = attr_selector.fit_transform(X)\n\nX_train, X_test = train_test_split(X, test_size=.20, random_state=42)\n\n","83e3c10f":"# Defining a pipeline\nnum_attribs = ['votes', 'approx_cost(for two people)']\ncat_attribs = ['online_order', 'book_table', 'listed_in(type)', 'listed_in(city)']\nall_attribs = num_attribs + cat_attribs\nX_num = X_train.loc[:, num_attribs]\nX_cat = X_train.loc[:, cat_attribs]\n\n# Common pipeline\ncommon_pipeline = Pipeline([\n    ('attr_selector', attrSelect()),\n    ('data_transformer', transformData()),\n    ('null_handler', handleNullData()),\n])\n\n# Numerical pipeline\nnum_pipeline_first_approach = Pipeline([\n    ('log_transformer', logTransformation()),\n])\n\n# Categorical pipeline\ncat_pipeline_first_approach = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False)),\n])\n\n# Full pipeline\nfull_pipeline_first_approach = ColumnTransformer([\n    ('num', num_pipeline_first_approach, num_attribs),\n    ('cat', cat_pipeline_first_approach, cat_attribs),\n])","991a3098":"# Preprocessing data\nX_prep_temp = common_pipeline.fit_transform(new_df)\nX = X_prep_temp.drop('rate', axis=1)\ny = X_prep_temp['rate']\ny_log = np.log1p(y)\n\n#X = pd.get_dummies(X).values\n#y = pd.get_dummies(y).values\n\n# Spliting and preparing data\nX_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=.20, random_state=42)\nX_train_prepared = full_pipeline_first_approach.fit_transform(X_train)\nX_test_prepared = full_pipeline_first_approach.fit_transform(X_test)\nX_train_prepared[0]\n\n#print(X_train_prepared.shape)\n#X_train_prepared = X_train_prepared.reshape((18607, 6702))\n#print(X_train_prepared.shape)\n#X_test_prepared = X_test_prepared.reshape((18542872, 1))\n#print(X_test_prepared.shape)","9d8cc2c7":"X_test_prepared","bd565c28":"\n# Functions for report\ndef create_dataset():\n    \"\"\"\n    This functions creates a dataframe to keep performance analysis\n    \"\"\"\n    attributes = ['model', 'rmse_train', 'rmse_cv', 'rmse_test', 'total_time']\n    model_performance = pd.DataFrame({})\n    for col in attributes:\n        model_performance[col] = []\n    return model_performance\n\ndef model_results(models, X_train, y_train, X_test, y_test, df_performance, cv=5, \n                  scoring='neg_mean_squared_error'):\n    for name, model in models.items():\n        t0 = time.time()\n        model.fit(X_train, y_train)\n        train_pred = model.predict(X_train)\n        train_rmse = mean_squared_error(y_train, train_pred)\n        score_train = r2_score(y_train, train_pred)  \n        train_cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n        train_cv_rmse = np.sqrt(-train_cv_scores).mean()\n        \n       # print(\"score: %.2f\" % (score_train))\n        print(\"Score:\", score_train)\n        print(\"Mean:\", score_train.mean())\n        print(\"Standard deviation:\", score_train.std())\n\n          \n        \n        test_pred = model.predict(X_test)\n        test_rmse = mean_squared_error(y_test, test_pred)\n        score_test = r2_score(y_test, test_pred)\n        t1 = time.time()\n        delta_time = t1-t0\n        model_name = model.__class__.__name__\n        # print(\"score: %.2f\" % (score_train))\n        #print(\"Score:\", score_test)\n        #print(\"Mean:\", score_test.mean())\n        #print(\"Standard deviation:\", score_test.std())\n        performances = {}\n        performances['model'] = model_name\n        performances['rmse_train'] = round(train_rmse, 4)\n        performances['rmse_cv'] = round(train_cv_rmse, 4)\n        performances['rmse_test'] = round(test_rmse, 4)\n        performances['total_time'] = round(delta_time, 3)\n        df_performance = df_performance.append(performances, ignore_index=True)\n        \n    return df_performance","3edad149":"params = {'n_neighbors':[2,3,4,5,6,7,8,9]}\nknn = neighbors.KNeighborsRegressor()\n\ndf_performance = create_dataset()\nregressors = {\n    'lin': LinearRegression(),\n    'ridge': Ridge(),\n    'lasso': Lasso(),\n    'elastic': ElasticNet(),\n    'forest': RandomForestRegressor(),\n    'KNN': GridSearchCV(knn, params, cv=5)\n}\ndf_performance = model_results(regressors, X_train_prepared, y_train, X_test_prepared, y_test, df_performance)\ndf_performance.set_index('model', inplace=True)\ncm = sns.light_palette(\"cornflowerblue\", as_cmap=True)\ndf_performance.style.background_gradient(cmap=cm)\n","6f8c4c5e":"def calc_rmse(model, X, y, cv=5, scoring='neg_mean_squared_error'):\n    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n    return np.sqrt(-scores).mean()","37ade715":"# Alpha analysis\nalphas = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3]\nlasso_scores = []\nfor a in alphas:\n    lasso_scores.append(calc_rmse(Lasso(alpha=a), X_train_prepared, y_train))\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.lineplot(alphas, lasso_scores)\nformat_spines(ax, right_border=False)\nax.set_title('Alpha - Lasso Regression')\nplt.show()","bdb163ba":"param_grid = [\n    {'n_estimators': [30, 40, 50, 75, 90], 'max_features': [10, 12, 15, 20, 25]},\n]\n\n# Criando regressor\nforest_reg = RandomForestRegressor()\n\n# Treinando e procurando a melhor combina\u00e7\u00e3o\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train_prepared, y_train)\nbest_forest_rmse = np.sqrt(-grid_search.best_score_)\nbest_forest_rmse","0fa06f20":"# Alpha analysis\nalphas = [0.001, 0.003, 0.01, 0.03, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\nridge_scores = []\nfor a in alphas:\n    ridge_scores.append(calc_rmse(Ridge(alpha=a), X_train_prepared, y_train))\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.lineplot(alphas, ridge_scores)\nformat_spines(ax, right_border=False)\nax.set_title('Alpha - Ridge Regression')\nplt.show()","3a0bc381":"single_performance = create_dataset()\nfinal_model = {\n    'best_ridge': Ridge(alpha=30)\n}\nsingle_performance = model_results(final_model, X_train_prepared, y_train, X_test_prepared, y_test, \n                                   single_performance)\nsingle_performance.set_index('model', inplace=True)\nsingle_performance","f02a90bd":"# Training a definitive model\nridge_reg = Ridge(alpha=30)\nridge_reg.fit(X_train_prepared, y_train)","4b422cd5":"# Create function\ndef chooseOnlineOrder(rnd=True):\n    if rnd:\n        return 'Yes' if np.random.randint(1, 3) == 1 else 'No'  \n    online_order_opt = 0\n    while True:\n        try:\n            online_order_opt = int(input('Online Order? \\n(1) Yes\\n(2) No\\n'))\n            if online_order_opt in (1, 2):\n                online_order = 'Yes' if online_order_opt == 1 else 'No'\n                break\n            else:\n                print('Please, input a number between 1 and 2')\n        except ValueError:\n            print('Please, input a number between 1 and 2.')\n    return online_order\n\ndef chooseBookTable(rnd=True):\n    if rnd:\n        return 'Yes' if np.random.randint(1, 3) == 1 else 'No'  \n    book_table_opt = 0\n    while True:\n        try:\n            book_table_opt = int(input('Book Table? \\n(1) Yes\\n(2) No\\n'))\n            if book_table_opt in (1, 2):\n                book_table = 'Yes' if book_table_opt == 1 else 'No'\n                break\n            else:\n                print('Please, input a number between 1 and 2')\n        except ValueError:\n            print('Please, input a number between 1 and 2.')\n    return book_table\n\ndef chooseVotes(rnd=True):\n    if rnd:\n        return int(np.random.randint(1, 1001))\n    while True:\n        try:\n            votes = int(input('Votes: '))\n            if votes < 0:\n                print('Please, insert a positive number')\n            else:\n                break\n        except ValueError:\n            print('Please, insert number.')\n    return votes\n\ndef chooseApproxCost(rnd=True):\n    if rnd:\n        return float(np.random.randint(1, 1001))\n    while True:\n        try:\n            approx_cost = int(input('Approx cost (for two people: '))\n            if approx_cost < 0:\n                print('Please, insert a positive number')\n            else:\n                break\n        except ValueError:\n            print('Please, insert a number.')\n    return approx_cost\n\ndef chooseRestType(rnd=True):\n    listed_in_select = list(X_train['listed_in(type)'].value_counts().index)\n    idx_list = np.arange(len(list(X_train['listed_in(type)'].value_counts().index)))\n    if rnd:\n        return list(zip(idx_list, listed_in_select))[np.random.randint(1, 8)-1][1]\n    print('\\nChoose one option for Listed in (type): ')\n    for idx, tipo in zip(idx_list, listed_in_select):\n        print(f'({idx+1}) {tipo}')\n    listed_in_opt = 0\n    while True:\n        try:\n            listed_in_opt = int(input())\n            if listed_in_opt in range(1, 8):\n                listed_in_type = list(zip(idx_list, listed_in_select))[listed_in_opt-1][1]\n                break\n            else:\n                print('Please, input a number between 1 and 7.')\n        except ValueError:\n            print('Please, input a number between 1 and 7.')\n    return listed_in_type\n\ndef chooseRestCity(rnd=True):\n    listed_in_select = list(X_train['listed_in(city)'].value_counts().index)\n    idx_list = np.arange(len(list(X_train['listed_in(city)'].value_counts().index)))\n    if rnd:\n        return list(zip(idx_list, listed_in_select))[np.random.randint(1, 30)-1][1]\n    print('\\nChoose one option for Listed in (city): ')\n    for idx, city in zip(idx_list, listed_in_select):\n        print(f'({idx+1}) {city}')\n    listed_in_opt = 0\n    while True:\n        try:\n            listed_in_opt = int(input())\n            if listed_in_opt in range(1, 31):\n                listed_in_city = list(zip(idx_list, listed_in_select))[listed_in_opt-1][1]\n                break\n            else:\n                print('Please, input a number between 1 and 30.')\n        except ValueError:\n            print('Please, input a number between 1 and 30.')\n    return listed_in_city","4cd050dc":"# Generating new data\ndef generateNewData(qtd_sample, cols, random=True):\n    new_data = pd.DataFrame({})\n    for col in cols:\n        new_data[col] = []\n    new_data_dict = {}\n    for i in range(qtd_sample):\n        new_data_dict['online_order'] = chooseOnlineOrder(random)\n        new_data_dict['book_table'] = chooseBookTable(random)\n        new_data_dict['votes'] = chooseVotes(random)\n        new_data_dict['approx_cost(for two people)'] = chooseApproxCost(random)\n        new_data_dict['listed_in(type)'] = chooseRestType(random)\n        new_data_dict['listed_in(city)'] = chooseRestCity(random)\n        new_data = new_data.append(new_data_dict, ignore_index=True)\n    return new_data  ","f000200c":"cols = X_train.columns\nnew_data = generateNewData(50, cols, random=True)\nnew_data.head()","8ab70580":"# Preparing new data and predicting\nnew_data_prepared = full_pipeline_first_approach.transform(new_data)\npredictions = ridge_reg.predict(new_data_prepared)\nrates = np.exp(predictions)\nnew_data['predicted_rate'] = rates\nnew_data.head()","0e261533":"rest_params = df.groupby(by='listed_in(type)', as_index=False).mean().sort_values(by='rate_transformed', \n                                                                                  ascending=False)\nfig, axs = plt.subplots(2, 1, figsize=(11, 13))\nsns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=rest_params, ax=axs[0], palette='Blues_d')\nax2 = axs[0].twinx()\nsns.lineplot(x='listed_in(type)', y='rate_transformed', data=rest_params, ax=ax2, color='crimson', sort=False)\naxs[0].tick_params(axis='x', labelrotation=90)\nformat_spines(axs[0], right_border=True)\nformat_spines(ax2, right_border=True)\naxs[0].xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = rest_params['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\n# Data generated\nrest_params = new_data.groupby(by='listed_in(type)', as_index=False).mean().sort_values(by='predicted_rate',                                                                                   ascending=False)\nsns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=rest_params, ax=axs[1], palette='Blues_d')\nax3 = axs[1].twinx()\nsns.lineplot(x='listed_in(type)', y='predicted_rate', data=rest_params, ax=ax3, color='crimson', sort=False)\naxs[1].tick_params(axis='x', labelrotation=90)\nformat_spines(axs[1], right_border=True)\nformat_spines(ax3, right_border=True)\naxs[1].xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = rest_params['predicted_rate']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\naxs[0].set_title('Average Cost and Rating of Restaurants by Type', size=14)\naxs[1].set_title('Predicted Rate of Restaurants by Type', size=14)\nplt.tight_layout()\nplt.show()","ab99ae3c":"data = X_prep_temp.copy()\nbin_edges = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]\nbin_names = [1, 2, 3, 4, 5]\ndata['rate_category'] = pd.cut(data['rate'], bin_edges, labels=bin_names)\ndata.drop('rate', axis=1, inplace=True)\ndata.head()","6f1599ff":"# Splitting data\nX = data.drop('rate_category', axis=1)\ny = data['rate_category']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)\n\nX_train_prepared = full_pipeline_first_approach.fit_transform(X_train)\nX_test_prepared = full_pipeline_first_approach.transform(X_test)","c21d789a":"y_train.value_counts()","62b682a3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_prepared, y_train)\npredictions = cross_val_predict(log_reg, X_train_prepared, y_train, cv=5)","4dbf0710":"print(classification_report(y_train, predictions))","03884855":"data = X_prep_temp.copy()\ndata['rounded_rate'] = data['rate'].apply(lambda x: round(x))\ndata.drop('rate', axis=1, inplace=True)\ndata.head()","6de8efff":"# Balance of classification target\ndata['rounded_rate'].value_counts()","5343cee8":"# Splitting data\nX = data.drop('rounded_rate', axis=1)\ny = data['rounded_rate']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)\n\nX_train_prepared = full_pipeline_first_approach.fit_transform(X_train)\nX_test_prepared = full_pipeline_first_approach.transform(X_test)\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_prepared, y_train)\npred = log_reg.predict(X_train_prepared)\nprint(classification_report(y_train, pred))","2aa9b0a9":"predictions = cross_val_predict(log_reg, X_train_prepared, y_train, cv=5)\nprint(classification_report(y_train, predictions))","d909285d":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier()\nforest_clf.fit(X_train_prepared, y_train)\npredictions = cross_val_predict(forest_clf, X_train_prepared, y_train, cv=5)\nprint(classification_report(y_train, predictions))","b38323b6":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=False)\nfrom wordcloud import WordCloud\nfrom geopy.geocoders import Nominatim\nfrom folium.plugins import HeatMap\nimport folium\nfrom tqdm import tqdm\nimport re\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.colors as mcolors\nfrom sklearn.manifold import TSNE\nfrom gensim.models import word2vec\nimport nltk\n# Any results you write to the current directory are saved as output.","ba0d5d04":"df=pd.read_csv(\"..\/input\/zomato.csv\")","6beefcbe":"all_ratings = []\n\nfor name,ratings in tqdm(zip(df['name'],df['reviews_list'])):\n    ratings = eval(ratings)\n    for score, doc in ratings:\n        if score:\n            score = score.strip(\"Rated\").strip()\n            doc = doc.strip('RATED').strip()\n            score = float(score)\n            all_ratings.append([name,score, doc])","8e93c86a":"rating_df=pd.DataFrame(all_ratings,columns=['name','rating','review'])\nrating_df['review']=rating_df['review'].apply(lambda x : re.sub('[^a-zA-Z0-9\\s]',\"\",x))","8235a0d2":"rating_df.to_csv(\"Ratings.csv\")","39c7191d":"rating_df.head()","48f73f80":"plt.figure(figsize=(7,6))\nrating=rating_df['rating'].value_counts()\nsns.barplot(x=rating.index,y=rating)\nplt.xlabel(\"Ratings\")\nplt.ylabel('count')","5ea19349":"rating_df['sent']=rating_df['rating'].apply(lambda x: 1 if int(x)>2.5 else 0)\n","5b0da44d":"stops=stopwords.words('english')\nlem=WordNetLemmatizer()\ncorpus=' '.join(lem.lemmatize(x) for x in rating_df[rating_df['sent']==1]['review'][:3000] if x not in stops)\ntokens=word_tokenize(corpus)","0b470b54":"vect=TfidfVectorizer()\nvect_fit=vect.fit(tokens)\n    ","a6611697":"id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvectorized_data=vect_fit.transform(tokens)\ngensim_corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(gensim_corpus,id2word=id_map,num_topics=5,random_state=34,passes=25)\n","3c9e5191":"counter=Counter(corpus)","897757bd":"stops=stopwords.words('english')\nlem=WordNetLemmatizer()\ncorpus=' '.join(lem.lemmatize(x) for x in rating_df[rating_df['sent']==0]['review'][:3000] if x not in stops)\ntokens=word_tokenize(corpus)","c6f5236d":"vect=TfidfVectorizer()\nvect_fit=vect.fit(tokens)\nid_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvectorized_data=vect_fit.transform(tokens)\ngensim_corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(gensim_corpus,id2word=id_map,num_topics=5,random_state=34,passes=25)","3b193806":"counter=Counter(corpus)\nout=[]\ntopics=ldamodel.show_topics(formatted=False)\nfor i,topic in topics:\n    for word,weight in topic:\n        out.append([word,i,weight,counter[word]])\n\ndataframe = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(8,6), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.3, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=8)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(dataframe.loc[dataframe.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=8, y=1.05)    \nplt.show()","ad6fece3":"stops=set(stopwords.words('english'))\nlem=WordNetLemmatizer()\ncorpus=[]\nfor review in tqdm(rating_df['review'][:10000]):\n    words=[]\n    for x in word_tokenize(review):\n        x=lem.lemmatize(x.lower())\n        if x not in stops:\n            words.append(x)\n            \n    corpus.append(words)","62bd4228":"model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)","3b957e80":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n             \n    plt.figure(figsize=(10, 10)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","70fa29ad":"postive=rating_df[rating_df['rating']>3]['review'][:2000]\nnegative=rating_df[rating_df['rating']<2.5]['review'][:2000]\n\ndef return_corpus(df):\n    corpus=[]\n    for review in df:\n        tagged=nltk.pos_tag(word_tokenize(review))\n        adj=[]\n        for x in tagged:\n            if x[1]=='JJ':\n                adj.append(x[0])\n        corpus.append(adj)\n    return corpus","3182dcc8":"corpus=return_corpus(postive)\nmodel = word2vec.Word2Vec(corpus, size=100, min_count=10,window=20, workers=4)\ntsne_plot(model)","2e710d57":"rating_df['sent']=rating_df['rating'].apply(lambda x: 1 if int(x)>2.5 else 0)","c95f7696":"max_features=3000\ntokenizer=Tokenizer(num_words=max_features,split=' ')\ntokenizer.fit_on_texts(rating_df['review'].values)\nX = tokenizer.texts_to_sequences(rating_df['review'].values)\nX = pad_sequences(X)","b49e0f3f":"embed_dim = 32\nlstm_out = 32\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n#model.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","33e27c5f":"Y = pd.get_dummies(rating_df['sent'].astype(int)).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","49017cb6":"\nbatch_size = 3200\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size)\nhistory.history","48896be0":"validation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","76e0cce6":"import json\nwith open('history.json', 'w') as f:\n    json.dump(history.history, f)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss']].plot()\nhistory_df[['acc']].plot()","809978e1":"## Data Types","0b4c45e9":"## Rating distribution","946af97f":"## Relationship Between Cost and Rate","87e301e9":"Nowadays, Starting a restaurant project in a given location requires a large investment by investors in the money and effort. This study aims to analyze the factors influencing the building of the restaurant, such as food type, location, etc., which helps investors not to leave their success rate to their conditions.\nMy Supervised Machine Learning Algorithm will use Numerical and categorized features such as ('online_order', 'book_table', 'votes', 'approx_cost(for two people)') ,to predict the rate of The Restaurants. by applying the concept of Machine Learning Such as Leanier regressor , random forest , KNN ,LSTM and more. I make prepare for the data and Analysis it to Understand the effect of every feature on the rating of the restaurants, then train the model and fine-tune it.\n","9115e83e":"## Visualizing output\n**Word Counts of Topic Keywords**\n\nWhen it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n\nLet\u2019s plot the word counts and the weights of each keyword in the same chart.\n\nYou want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I\u2019ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process.","a049240d":"# Reading Data","7d0fa292":"Let's apply the log transformation and see the results.","b1946f0c":"# Classification Task","f1af9b93":"Problem:\n    - Predictions are too high to be real;\n    - Predictions are over limit (higher than 5);\n    - Next steps: work with new approach (classification task between five class (1 to 5)","8f92ef89":"## What is the ratio b\/w restaurants that provide and do not provide table booking ?","e89dc0f9":"## Data preparation\nFor doing sentimental analysis on reviews provided bt users.We have to prepare our data in appropriate format. We will map reviews to positive and negative on the basis of the ratings provided by each user.So,we will map reviews to negative if the rating given is less than 2.5 and positive if rating is greater than 2.5","93151911":"# Predicting Restaurant Rate","b7f93058":"### Another Approach to Group Rate","6b5f53b2":"## Topic modeling for positive comments\n- As the first step we will divide comments as negative and positive on the basis on rating provided.\n- Comments with rating below 2.5 is classified as negative and greater tham 2.5 as classified as positive.","176868dd":"## How many of the restuarants do not accept online orders?","caee00f3":"# Train and test split\nWe will now encode our target variable. pd.get_dummies is used for on-hot encoding.\n33 percent of data is reserved for testing our model\n","ef83f5d6":"**Conclusion**\n\nThe rate of a restaurants isn't defined by its cost. There are expensive restaurants with a bad average rate and there are cheap restaurants with good rate.","034ee15c":"Random Forest Regressor was the best but the slowest.","5e3d7a47":"## Topic modelling\nWe will do topic modelling for postive and negative comments seperately to understand the different between the two types.\n","faa8ff93":"In this Exploratory Data Analysis we will see:\n\n* Changing data types and make some transformation;\n* Dealing with null data;\n* Top Restaurants by rating;\n* The approx cost (for two people) and rate of restaurants;\n* Correlation between cost and rate;\n* The influence of online_order and book_table attributes;\n* Cost and rate by type of restaurants;\n* Cost and rate of restaurants by localization (city)","7ffe29f2":"## Location","b634bc26":"## Lasso Regression","10501678":"That's too bad. The model didn't recognize any of 5 and 2 categories. There is few data to train and it's necessary to apply something to handle it.","e4460f62":"If you liked this kernel","d18daf5a":"## Topic modeling for negative comments","4e6db633":"## Online Order and Book Table Services","58b7cf09":"# Going Depper","3d80f19d":"## Ridge Regression","48e46f9e":"# Generating New Data","01cd25f2":"## Random Forest","ca66aef1":"## Top Restaurants by Rating","bbc85fc7":"Let's see the average cost in each case.","b0a8a833":"We have restaurants where approx cost for two people is 1. On the other hands, we have restaurants where cost is 950!","d2166cb5":"Now,\n   - we will remove stopwords\n   - Lemmatize each word\n   - Create corpus\n   - Tokenize them","27660231":"So we have a huge amount of null data in some features. Something must be done about it in the future but for now let's just keep in mind this amount and apply other data prep before threat null data.","a34b49cf":"Let's round the rates and not to apply the pandas function `cut`. Maybe it would be better.","41d957f6":"# Simple LSTM baseline for predicting review scores","6b62e897":"## Logistic Regression","69bfcf9f":"Just for curiosity, let's take a look at the null amount of data in this dataset","f66aaae2":"## Which are the foodie areas?","be9b9f31":"Now we will use word2vec to represent each word as a vector.","eed4c467":"**Conclusions:**\n\n* We can see that the rating of each restaurant isn't influenced by its approx cost.\n* There is a notable concentration of restaurants that don't have online order service and a low cost for two people. On the other hand, there is a concentration of restaurants on top of cost that offer online order service. Probably this service can increase the cost of restaurants;\n* Few restaurants offer book table service. In general, the ones who offer it have higher rate.","2dcd65f4":"Let's see the behavior of numerical features.","8a2fc072":"> Now we will  use **Termfrequency Inverse doc frequency(Tfidf)** to vectorize the tokens.","5a45ffe8":"**Conclusions:**\n\n* _Pubs and bars_ and _Drinks & nightlife_ are the restaurants with lowest cost and highest rate. Customers prefer restaurants with lower cost.\n* _Delivery_ and _Dine out_ restaurants are the ones with the lowest rate. Probably this kind of restaurant has the most demanding customers.\n* _Cafe_ restaurants are the most expensive.","8a88d622":"We will take 1500 rows to validate our model.We have choosen accuacy to be our evaluation criteria.","e46a45b5":"We can clearly observe the difference between the two types of comments\nThe words used are clearly distinguishable.\nThe words used in negative comments are clearly critisizing.\nThe word used in positive comments are clearly appreciating.","f3584ea2":"Let's create a Linear Regression model for predict a restaurant rate.","67ff48c3":"# Libraries","7d3780fe":"Final model","89885728":"To be certain of what we just said, let's see the distribution of cost and rate of restaurants.","96581678":"**Conclusions:**\n\n* Restaurants with book table service have a higher rating. Curiously, the cost of these restaurants are significantly lower.\n* Restaurants with online order service have higher approx cost (for two people). The rating of restaurantes who offer and those who don't are about to be the same;\n* Customers are more satisfied with restaurants with book table service.","ceeeb805":"## Restaurant Type","ae070517":"Highly unbalanced dataset. We can change the way we categorized the rate.","8d90eae4":"## Building our model","f5f64acc":"# EDA","348f614d":"**Conclusions:**\n\n* Restaurants in Church Street, MG Road and Brigade Road are the customer's favorites. Its costs are low and its rates are high.\n* On the other hand, restaurants in Electronic City are the ones with lowest rate.\n* Restaurants with highest cost for two people are in Kaiyan Nagar.","4d6f7971":"## Random Forest Regressor","39fcf986":"Let's do some transformation here:\n\n* **approx_cost(for two people):**\n    - Change the data type from object to float;\n    \n\n* **rate:**\n    - Let's eliminate the \"\/5\" text and change data type from object to float","b3a0a46c":"Next, we will tokenize the data and vectorize the reviews to feed it to our model.","efc4d09b":"## Numerical Feature Distribution","d1ca2547":"In this section we will visualize words used in reviews in a 2 dimensional space.\n\nFor that we will first lemmatize and tokenize each reviews and build a corpus out of it.","449b8264":"## Null Data","b6214b21":"## Cost and Rate of Restaurants","61886e64":"    - Search for more analysis on data;\n    - Verify if all attributes are relevant for creating a linear regression model;\n    - Create a sentimental analysis to be applied on customers reviews.","097fbbec":"As we saw above, our data has null data in some attributes. In our first approach, let's do a quick analysis and drop some columns that probably don't make difference on conclusions."}}