{"cell_type":{"66d8f769":"code","5a3ee4a4":"code","ae4a79ff":"code","b5925fef":"code","38e3d322":"code","da9f5d66":"code","0a835fb0":"code","06a07466":"code","c8d27e81":"code","fcc6a3c1":"code","1fadce49":"code","cf7511c9":"code","0c187912":"code","6ba58242":"code","65284aa1":"code","d5392c39":"code","b37df844":"code","9ae0e674":"code","980ab8a1":"code","8dc3b201":"code","63ffdb6a":"code","056cda83":"code","b59fbbe2":"code","9530d1f5":"markdown","3fbaefb5":"markdown","1404ebaf":"markdown","dbe5acfd":"markdown","5ef963b0":"markdown","edeaa313":"markdown","72973879":"markdown","e96155d9":"markdown","54fe1b7a":"markdown","fd100b12":"markdown","ccb524ed":"markdown"},"source":{"66d8f769":"!pip install rdflib\n\n!git clone https:\/\/github.com\/IBCNServices\/pyRDF2Vec.git\n!cd pyRDF2Vec; python3 setup.py install\n!pip install pyRDF2Vec","5a3ee4a4":"import numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm_notebook\n\nimport rdflib\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import TSNE\n\nimport sys\n\n# RDF2Vec for embeddings\nsys.path.append('pyRDF2Vec')\nsys.path.append('pyRDF2Vec\/rdf2vec')\nfrom graph import KnowledgeGraph, Vertex\nfrom walkers import RandomWalker\nfrom rdf2vec import RDF2VecTransformer","ae4a79ff":"import matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom collections import defaultdict, Counter, OrderedDict\nfrom functools import lru_cache\nimport heapq\n\nimport os\nimport itertools\nimport time\n\nimport rdflib\n\nfrom scipy.stats import entropy\n\n# The idea of using a hashing function is taken from:\n# https:\/\/github.com\/benedekrozemberczki\/graph2vec\nfrom hashlib import md5\nimport copy\n\n\nclass Vertex(object):\n    \n    def __init__(self, name, predicate=False, _from=None, _to=None):\n        self.name = name\n        self.predicate = predicate\n        self._from = _from\n        self._to = _to\n        \n    def __eq__(self, other):\n        if other is None: \n            return False\n        return self.__hash__() == other.__hash__()\n    \n    def get_name(self):\n        return self.name\n    \n    def __hash__(self):\n        if self.predicate:\n            return hash((self._from, self._to, self.name))\n        else:\n            return hash(self.name)\n\n    def __lt__(self, other):\n        if self.predicate and not other.predicate:\n            return False\n        if not self.predicate and other.predicate:\n            return True\n        if self.predicate:\n            return (self.name, self._from, self._to) < (other.name, other._from, other._to)\n        else:\n            return self.name < other.name\n\nclass Graph(object):\n    _id = 0\n\n    def __init__(self):\n        self.vertices = set()\n        self.transition_matrix = defaultdict(set)\n        self.name_to_vertex = {}\n        self.root = None\n        self._id = Graph._id\n        Graph._id += 1\n        \n    def add_vertex(self, vertex):\n        if vertex not in self.vertices:\n            self.vertices.add(vertex)            \n\n        self.name_to_vertex[vertex.name] = vertex\n\n    def add_edge(self, v1, v2):\n        self.transition_matrix[v1].add(v2)\n\n    def get_neighbors(self, vertex):\n        return self.transition_matrix[vertex]\n\n    def visualise(self):\n        nx_graph = nx.DiGraph()\n        \n        for v in self.vertices:\n            if not v.predicate:\n                name = v.name.split('\/')[-1]\n                nx_graph.add_node(name, name=name, pred=v.predicate)\n            \n        for v in self.vertices:\n            if not v.predicate:\n                v_name = v.name.split('\/')[-1]\n                # Neighbors are predicates\n                for pred in self.get_neighbors(v):\n                    pred_name = pred.name.split('\/')[-1]\n                    for obj in self.get_neighbors(pred):\n                        obj_name = obj.name.split('\/')[-1]\n                        nx_graph.add_edge(v_name, obj_name, name=pred_name)\n        \n        plt.figure(figsize=(10,10))\n        _pos = nx.circular_layout(nx_graph)\n        nx.draw_networkx_nodes(nx_graph, pos=_pos)\n        nx.draw_networkx_edges(nx_graph, pos=_pos)\n        nx.draw_networkx_labels(nx_graph, pos=_pos)\n        nx.draw_networkx_edge_labels(nx_graph, pos=_pos, \n                                     edge_labels=nx.get_edge_attributes(nx_graph, 'name'))\n        plt.show()\n\n    def extract_neighborhood(self, instance, depth=8):\n        neighborhood = Neighborhood()\n        root = self.name_to_vertex[str(instance)]\n        to_explore = { root }\n\n        for d in range(depth):\n            new_explore = set()\n            for v in list(to_explore):\n                if not v.predicate:\n                    neighborhood.depth_map[d].add(v.get_name())\n                for neighbor in self.get_neighbors(v):\n                    new_explore.add(neighbor)\n            to_explore = new_explore\n        \n        return neighborhood\n\n    @staticmethod\n    def rdflib_to_graph(rdflib_g, label_predicates=[]):\n        kg = Graph()\n        for (s, p, o) in rdflib_g:\n\n            if p not in label_predicates:\n                s = str(s)\n                p = str(p)\n                o = str(o)\n\n                if isinstance(s, rdflib.term.BNode):\n                    s_v = Vertex(str(s), wildcard=True)\n                elif isinstance(s, rdflib.term.Literal):\n                    s_v = Vertex(str(s), literal=True)\n                else:\n                    s_v = Vertex(str(s))\n                    \n                if isinstance(o, rdflib.term.BNode):\n                    o_v = Vertex(str(o), wildcard=True)\n                elif isinstance(s, rdflib.term.Literal):\n                    o_v = Vertex(str(o), literal=True)\n                else:\n                    o_v = Vertex(str(o))\n                    \n                p_v = Vertex(str(p), predicate=True, _from=s_v, _to=o_v)\n                kg.add_vertex(s_v)\n                kg.add_vertex(p_v)\n                kg.add_vertex(o_v)\n                kg.add_edge(s_v, p_v)\n                kg.add_edge(p_v, o_v)\n        return kg\n\n\nclass Neighborhood(object):\n    def __init__(self):\n        self.depth_map = defaultdict(set)\n        \n    def find_walk(self, vertex, depth):\n        return vertex in self.depth_map[depth]\n\n\nclass Walk(object):\n    def __init__(self, vertex, depth):\n        self.vertex = vertex\n        self.depth = depth\n\n    def __eq__(self, other):\n        return (hash(self.vertex) == hash(other.vertex) \n                and self.depth == other.depth)\n    \n    def __hash__(self):\n        return hash((self.vertex, self.depth))\n\n    def __lt__(self, other):\n        return (self.depth, self.vertex) < (other.depth, other.vertex)\n\n\nclass TopQueue:\n    def __init__(self, size):\n        self.size = size\n        self.data = []\n\n    def add(self, x, priority):\n        if len(self.data) == self.size:\n            heapq.heappushpop(self.data, (priority, x))\n        else:\n            heapq.heappush(self.data, (priority, x))\n\n\nclass Tree():\n    def __init__(self, walk=None, _class=None):\n        self.left = None\n        self.right = None\n        self._class = _class\n        self.walk = walk\n        \n    def evaluate(self, neighborhood):\n        if self.walk is None:\n            return self._class\n        \n        if neighborhood.find_walk(self.walk[0], self.walk[1]):\n            return self.right.evaluate(neighborhood)\n        else:\n            return self.left.evaluate(neighborhood)\n\n    @property\n    def node_count(self):\n        left_count, right_count = 0, 0\n        if self.left is not None:\n            left_count = self.left.node_count\n        if self.right is not None:\n            right_count = self.right.node_count\n        return 1 + left_count + right_count\n    \nfrom sklearn.base import ClassifierMixin, TransformerMixin, BaseEstimator\nfrom collections import Counter\nimport copy\nimport numpy as np\nimport itertools\nfrom joblib import Parallel, delayed\nfrom multiprocessing import Pool\nfrom multiprocessing.pool import ThreadPool\nfrom scipy.stats import entropy\nimport time\nimport psutil\n\ndef _calculate_igs(neighborhoods, labels, walks):\n    prior_entropy = entropy(np.unique(labels, return_counts=True)[1])\n    results = []\n    for (vertex, depth) in walks:\n        features = {0: [], 1: []}\n        for inst, label in zip(neighborhoods, labels):\n            features[int(inst.find_walk(vertex, depth))].append(label)\n\n        pos_frac = len(features[1]) \/ len(neighborhoods)\n        pos_entr = entropy(np.unique(features[1], return_counts=True)[1])\n        neg_frac = len(features[0]) \/ len(neighborhoods)\n        neg_entr = entropy(np.unique(features[0], return_counts=True)[1])\n        ig = prior_entropy - (pos_frac * pos_entr + neg_frac * neg_entr)\n\n        results.append((ig, (vertex, depth)))\n\n    return results\n\nclass MINDWALCMixin():\n    def __init__(self, path_max_depth=8, progress=None, n_jobs=1, init=True):\n        if init:\n            if n_jobs == -1:\n                n_jobs = psutil.cpu_count(logical=False)\n        self.path_max_depth = path_max_depth\n        self.progress = progress\n        self.n_jobs = n_jobs\n\n    def _generate_candidates(self, neighborhoods, sample_frac=None, \n                             useless=None):\n        \"\"\"Generates an iterable with all possible walk candidates.\"\"\"\n        # Generate a set of all possible (vertex, depth) combinations\n        walks = set()\n        for d in range(2, self.path_max_depth + 1, 2):\n            for neighborhood in neighborhoods:\n                for vertex in neighborhood.depth_map[d]:\n                    walks.add((vertex, d))\n\n        # Prune the useless ones if provided\n        if useless is not None:\n            old_len = len(walks)\n            walks = walks - useless\n\n        # Convert to list so we can sample & shuffle\n        walks = list(walks)\n\n        # Sample if sample_frac is provided\n        if sample_frac is not None:\n            walks_ix = np.random.choice(range(len(walks)), replace=False,\n                                        size=int(sample_frac * len(walks)))\n            walks = [walks[i] for i in walks_ix]\n\n        # Shuffle the walks (introduces stochastic behaviour to cut ties\n        # with similar information gains)\n        np.random.shuffle(walks)\n\n        return walks\n\n    def _feature_map(self, walk, neighborhoods, labels):\n        \"\"\"Create two lists of labels of neighborhoods for which the provided\n        walk can be found, and a list of labels of neighborhoods for which \n        the provided walk cannot be found.\"\"\"\n        features = {0: [], 1: []}\n        vertex, depth = walk\n        for i, (inst, label) in enumerate(zip(neighborhoods, labels)):\n            features[int(inst.find_walk(vertex, depth))].append(label)\n        return features\n\n\n    def _mine_walks(self, neighborhoods, labels, n_walks=1, sample_frac=None,\n                    useless=None):\n        \"\"\"Mine the top-`n_walks` walks that have maximal information gain.\"\"\"\n        walk_iterator = self._generate_candidates(neighborhoods, \n                                                  sample_frac=sample_frac, \n                                                  useless=useless)\n\n        results = _calculate_igs(neighborhoods, labels, walk_iterator)\n        print(len(results), len(walk_iterator))\n\n        if n_walks > 1:\n            top_walks = TopQueue(n_walks)\n        else:\n            max_ig, best_depth, top_walk = 0, float('inf'), None\n\n        for ig, (vertex, depth) in results:\n            if n_walks > 1:\n                top_walks.add((vertex, depth), (ig, -depth))\n            else:\n                if ig > max_ig:\n                    max_ig = ig\n                    best_depth = depth\n                    top_walk = (vertex, depth)\n                elif ig == max_ig and depth < best_depth:\n                    max_ig = ig\n                    best_depth = depth\n                    top_walk = (vertex, depth)\n\n        print(top_walks.data)\n                    \n        if n_walks > 1:\n            return top_walks.data\n        else:\n            return [(max_ig, top_walk)]\n\n    def _prune_useless(self, neighborhoods, labels):\n        \"\"\"Provide a set of walks that can either be found in all \n        neighborhoods or 1 or less neighborhoods.\"\"\"\n        useless = set()\n        walk_iterator = self._generate_candidates(neighborhoods)\n        for (vertex, depth) in walk_iterator:\n            features = self._feature_map((vertex, depth), neighborhoods, labels)\n            if len(features[1]) <= 1 or len(features[1]) == len(neighborhoods):\n                useless.add((vertex, depth))\n        return useless\n\n    def fit(self, kg, instances, labels):\n        if self.progress is not None:\n            inst_it = self.progress(instances, desc='Neighborhood extraction')\n        else:\n            inst_it = instances\n\n        d = self.path_max_depth + 1\n        self.neighborhoods = []\n        for inst in inst_it:\n            neighborhood = kg.extract_neighborhood(inst, d)\n            self.neighborhoods.append(neighborhood)\n\nclass MINDWALCTransform(BaseEstimator, TransformerMixin, MINDWALCMixin):\n    def __init__(self, path_max_depth=8, progress=None, n_jobs=1, \n                 n_features=1):\n        super().__init__(path_max_depth, progress, n_jobs)\n        self.n_features = n_features\n\n    def fit(self, kg, instances, labels):\n        if self.progress is not None:\n            inst_iterator = self.progress(instances, \n                                          desc='Extracting neighborhoods')\n        else:\n            inst_iterator = instances\n\n        neighborhoods = []\n        d = self.path_max_depth + 1\n        for inst in inst_iterator:\n            neighborhood = kg.extract_neighborhood(inst, depth=d)\n            neighborhoods.append(neighborhood)\n\n        prior_entropy = entropy(np.unique(labels, return_counts=True)[1])\n\n        cache = {}\n\n        self.walks_ = set()\n\n        if len(np.unique(labels)) > 2:\n            _classes = np.unique(labels)\n        else:\n            _classes = [labels[0]]\n\n        for _class in _classes:\n            label_map = {}\n            for lab in np.unique(labels):\n                if lab == _class:\n                    label_map[lab] = 1\n                else:\n                    label_map[lab] = 0\n\n            new_labels = list(map(lambda x: label_map[x], labels))\n\n            walks = self._mine_walks(neighborhoods, new_labels, \n                                     n_walks=self.n_features)\n\n            prev_len = len(self.walks_)\n            n_walks = min(self.n_features \/\/ len(np.unique(labels)), len(walks))\n            for _, walk in sorted(walks, key=lambda x: x[0], reverse=True):\n                if len(self.walks_) - prev_len >= n_walks:\n                    break\n\n                if walk not in self.walks_:\n                    self.walks_.add(walk)\n\n    def transform(self, kg, instances):\n        if self.progress is not None:\n            inst_iterator = self.progress(instances, \n                                          desc='Extracting neighborhoods')\n        else:\n            inst_iterator = instances\n\n        neighborhoods = []\n        d = self.path_max_depth + 1\n        for inst in inst_iterator:\n            neighborhood = kg.extract_neighborhood(inst, depth=d)\n            neighborhoods.append(neighborhood)\n\n        features = np.zeros((len(instances), self.n_features))\n        for i, neighborhood in enumerate(neighborhoods):\n            for j, (vertex, depth) in enumerate(self.walks_):\n                features[i, j] = neighborhood.find_walk(vertex, depth)\n        return features","b5925fef":"# This takes a while...\ng = rdflib.Graph()\ng.parse('\/kaggle\/input\/covid19-literature-knowledge-graph\/kg.nt', format='nt')","38e3d322":"for p1, _, p2 in g.triples((None, rdflib.URIRef(\"http:\/\/purl.org\/spar\/cito\/isCitedBy\"), None)):\n    g.add((p2, rdflib.URIRef(\"http:\/\/purl.org\/spar\/cito\/cites\"), p1))\n    g.remove((p1, rdflib.URIRef(\"http:\/\/purl.org\/spar\/cito\/isCitedBy\"), p2))","da9f5d66":"def create_kg(triples, label_predicates):\n    kg = KnowledgeGraph()\n    for (s, p, o) in tqdm_notebook(triples):\n        if p not in label_predicates:\n            s_v = Vertex(str(s))\n            o_v = Vertex(str(o))\n            p_v = Vertex(str(p), predicate=True, _from=s_v, _to=o_v)\n            kg.add_vertex(s_v)\n            kg.add_vertex(p_v)\n            kg.add_vertex(o_v)\n            kg.add_edge(s_v, p_v)\n            kg.add_edge(p_v, o_v)\n    return kg\n    \ndef rdflib_to_kg(g, label_predicates=[]):\n    \"\"\"Convert a rdflib.Graph (located at file) to our KnowledgeGraph.\"\"\"\n    import rdflib\n    label_predicates = [rdflib.term.URIRef(x) for x in label_predicates]\n    return create_kg(g, label_predicates)\n\nkg = rdflib_to_kg(g)","0a835fb0":"import urllib\nmetadata = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\ndois = metadata['doi'].dropna().apply(lambda x: 'http:\/\/dx.doi.org\/' + x.strip('doi.org').strip('http:\/\/dx.doi.org\/')).values\ndois = list(set(dois))\nprint(dois[:25])","06a07466":"papers = []\nfor doi in tqdm_notebook(dois):\n    if len(list(g.triples((rdflib.URIRef(doi), None, None)))) > 0:\n        papers.append(doi)\nprint(len(papers))","c8d27e81":"papers = np.random.choice(papers, size=10000, replace=False)","fcc6a3c1":"random_walker = RandomWalker(4, 500)\ntransformer = RDF2VecTransformer(walkers=[random_walker], sg=1, n_jobs=4)\nwalk_embeddings = transformer.fit_transform(kg, papers)","1fadce49":"# We got our embeddings, so let's clear up some more memory\ndel kg","cf7511c9":"walk_tsne = TSNE(random_state=42, perplexity=30, n_components=2)\nX_walk_tsne = walk_tsne.fit_transform(walk_embeddings)\n    \nplt.figure(figsize=(15, 15))\nplt.scatter(X_walk_tsne[:, 0], X_walk_tsne[:, 1], s=10, alpha=0.5)\nplt.show()","0c187912":"np.random.seed(42)\nrand_paper_ix = np.random.choice(range(len(papers)))\nrand_embedding = walk_embeddings[rand_paper_ix]\nrand_paper = papers[rand_paper_ix]\ncosine_sims = cosine_similarity([rand_embedding], walk_embeddings)[0]\ntop_matches = np.argsort(cosine_sims)[-10:-1]\ntop_papers = []\nfor ix in top_matches:\n    top_papers.append(papers[ix].replace('\/doi.org%2F', '\/'))\nprint('Closest neighbors of {}:\\n'.format(rand_paper))\nprint('\\t' + '\\n\\t'.join(top_papers))","6ba58242":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=20)\nkmeans.fit(walk_embeddings)","65284aa1":"cmap = plt.get_cmap('tab20')\nplt.figure(figsize=(15, 15))\nplt.scatter(X_walk_tsne[:, 0], X_walk_tsne[:, 1], color=[cmap(x\/15) for x in kmeans.labels_], s=10, alpha=0.5)\nplt.legend()\nplt.show()","d5392c39":"cluster_ix = np.where(kmeans.labels_ == sorted(Counter(kmeans.labels_).items(), key=lambda x: x[1])[0][0])[0]\n\nplt.figure(figsize=(15, 15))\nplt.scatter(X_walk_tsne[:, 0], X_walk_tsne[:, 1], color=['r' if i in cluster_ix else 'b' for i in range(len(X_walk_tsne))], s=10, alpha=0.5)\nplt.legend()\nplt.show()","b37df844":"# This datastructure is different from the one we used for RDF2Vec\nkg = Graph.rdflib_to_graph(g)","9ae0e674":"del g","980ab8a1":"pos_papers = [papers[ix] for ix in cluster_ix]\nneg_papers = [papers[ix] for ix in np.random.choice(list(set(range(len(papers))) - set(cluster_ix)), size=1000, replace=False)]\n\ntrain_entities = pos_papers + neg_papers\ntrain_labels = [1]*len(pos_papers) + [0]*len(neg_papers)","8dc3b201":"print(pos_papers[:10])","63ffdb6a":"transf = MINDWALCTransform(path_max_depth=8, n_features=100, progress=tqdm_notebook, n_jobs=1)\ntransf.fit(kg, train_entities, train_labels)","056cda83":"transf.walks_","b59fbbe2":"transformer.walks_[:50]","9530d1f5":"# Other applications\n\nThere is a lot of other possibilities now that we have our data in a Knowledge Graph:\n* We can create embeddings of all our entities: journals, authors, ...\n* We can extend our knowledge graph with more knowledge (this will sometimes be needed to create som eof the embeddings, such as inverse relations from the authors to the papers)\n* We can solve classification problems\n* ...","3fbaefb5":"# Application 2: Find nearest neighbors of paper in embedding space\n\nWe can get the nearest neighbors of a paper in the embedded space to find similar papers","1404ebaf":"# Application 3: Clustering the embeddings\n\nWe can automatically cluster the embeddings. Here, I just demonstrate the clustering with k-Means. An algorithm that can deal with custom distance metrics and automatic decision of number of clusters would probably be more ideal. I have played around with DBScan and OPTICS, but they results were poor + they were slow. I also did not really play around with the number of clusters.","dbe5acfd":"# Convert rdflib.Graph to rdf2vec.KnowledgeGraph\n\nOur python implementation uses a special Knowledge Graph datastructure. The conversion (defined here) is rather easy.","5ef963b0":"# Defining MINDWALC below here","edeaa313":"# COVID-19 Literature Knowledge Graph\n\n**In a [previous notebook](https:\/\/www.kaggle.com\/group16\/covid-19-knowledge-graph-starter), I demonstrated how we can interact with the Knowledge Graph using rdflib. In this notebook, I will use RDF2Vec ([paper](https:\/\/madoc.bib.uni-mannheim.de\/41307\/1\/Ristoski_RDF2Vec.pdf)|[code](github.com\/IBCNServices\/pyRDF2Vec)) to generate embeddings of the papers. Afterwards, I'll show some applications of these embeddings. The dataset can be downloaded [here](https:\/\/www.kaggle.com\/group16\/covid19-literature-knowledge-graph).**","72973879":"# Application 1: t-SNE plot of the embeddings\n\nWe can visually check if there are clusters of similar data\/embeddings by creating a t-SNE visualization","e96155d9":"# Application 4: Explaining clusters with MINDWALC\n\nLet's take one of the smaller clusters that we could visually identify in our t-SNE plot and see what distinguishes them from other papers. For this, we shall use MINDWALC ([paper](https:\/\/biblio.ugent.be\/publication\/8628802\/file\/8628803)|[code](https:\/\/github.com\/IBCNServices\/MINDWALC)). We will mine for (depth, vertex) combinations that maximize the information gain.","54fe1b7a":"# Install the dependencies","fd100b12":"# Filter out the COVID-19 papers from our KG & generate their embeddings","ccb524ed":"# Load the KG\n\nWe use rdflib to deserialize the RDF data, we shall convert this rdflib Graph to other datastructures further on."}}