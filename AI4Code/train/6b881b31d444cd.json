{"cell_type":{"479bce56":"code","731ea6d1":"code","65e46140":"code","65ee2d43":"code","e34f8f1b":"code","9fff2036":"code","51bf5053":"code","a3f98e74":"code","cebc7fd2":"code","748203f9":"code","c26845dd":"code","0666ec28":"code","cd21d1a4":"code","50b1f60c":"code","bb8bcd04":"code","8bca30c3":"code","fe38c746":"code","9e7024b9":"code","ff7dc9f9":"code","19dbc363":"code","c912c17d":"code","08198efe":"code","5c4a2670":"code","f1106960":"code","2a28f2f1":"code","ac3d0bd1":"code","7b7e6602":"markdown","5eef1396":"markdown","ba567605":"markdown"},"source":{"479bce56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, UpSampling2D, BatchNormalization, SpatialDropout2D, Input, MaxPool2D, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.data import Dataset, AUTOTUNE\nimport tensorflow.strings\nfrom tensorflow.keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom pathlib import Path\nimport os","731ea6d1":"PATH_TO_TRAINING = r'..\/input\/drishtigs-retina-dataset-for-onh-segmentation\/Training-20211018T055246Z-001\/Training\/'\nPATH_TO_TRAINING_TENSOR = tf.Variable(PATH_TO_TRAINING)\n\nTRAIN_GLAUCOMA_IMAGES = np.array(glob(os.path.join(PATH_TO_TRAINING, 'Images\/GLAUCOMA\/*')))\nTRAIN_NORMAL_IMAGES = np.array(glob(os.path.join(PATH_TO_TRAINING, 'Images\/NORMAL\/*')))\n\nTRAIN_GLAUCOMA_GT = np.array(glob(PATH_TO_TRAINING + '\/GT\/GALUCOMA\/*'))\nTRAIN_NORMAL_GT = np.array(glob(PATH_TO_TRAINING + '\/GT\/NORMAL\/*'))\n\nPATH_TO_TEST = r'..\/input\/drishtigs-retina-dataset-for-onh-segmentation\/Test-20211018T060000Z-001\/Test'\nTEST_GLAUCOMA_IMAGES = np.array(glob(os.path.join(PATH_TO_TEST, 'Images\/glaucoma\/*')))\nTEST_NORMAL_IMAGES = np.array(glob(os.path.join(PATH_TO_TEST, 'Images\/normal\/*')))\n\nSEED = 2\nEPSILON = 10 ** -4\nSTATELESS_RNG= tf.random.Generator.from_seed(SEED, alg='philox')\nIMAGE_SIZE = (224, 224)\nCHANNELS = 3\nSHUFFLE_BUFFER = len(TRAIN_NORMAL_IMAGES) + len(TRAIN_GLAUCOMA_IMAGES)\nBATCH_SIZE = SHUFFLE_BUFFER \/\/ 2\nearly_stopping = EarlyStopping(monitor = 'val_loss', restore_best_weights = True, patience = 5)\n","65e46140":"Path(TRAIN_GLAUCOMA_IMAGES[0]).stem\nTEST_GLAUCOMA_IMAGES","65ee2d43":"def load_image_and_gt(path, test = False):\n    '''\n    Takes a path to the image,\n    and returns the image alongside\n    its ground truth\n    '''\n    \n    print(path)\n    \n    filename = Path(path).stem\n    if test:\n        path_to_OD_softmap = os.path.join(PATH_TO_TEST, 'Test_GT', filename, 'SoftMap', filename + '_ODsegSoftmap.png')\n        path_to_cup_softmap = os.path.join(PATH_TO_TEST, 'Test_GT', filename, 'SoftMap', filename + '_cupsegSoftmap.png')    \n    else:\n        path_to_OD_softmap = os.path.join(PATH_TO_TRAINING, 'GT', filename, 'SoftMap', filename + '_ODsegSoftmap.png')\n        path_to_cup_softmap = os.path.join(PATH_TO_TRAINING, 'GT', filename, 'SoftMap', filename + '_cupsegSoftmap.png')\n    image = tf.io.read_file(path)\n    image = tf.io.decode_png(image)\n    od = tf.io.read_file(path_to_OD_softmap)\n    od = tf.io.decode_png(od)\n    cup = tf.io.read_file(path_to_cup_softmap)\n    cup = tf.io.decode_png(cup)\n    return image, od, cup\n\ndef show_image_with_masks(image, od, cup, ax, index, label):\n    '''\n    Plots the real world image\n    And the two masks\n    Also shows the image after\n    being masked with each mask\n    Requires len(ax) >= 5\n    '''\n    ax[5 * index].imshow(image)\n    ax[5 * index].set_title(label)\n    ax[5 * index + 1].imshow(od)\n    ax[5 * index + 1].set_title('Optical Disk Mask')\n    ax[5 * index + 2].imshow(od * image)\n    ax[5 * index + 2].set_title('Image with OD Mask')\n    ax[5 * index + 3].imshow(cup)\n    ax[5 * index + 3].set_title('Cup Mask')\n    ax[5 * index + 4].imshow(cup * image)\n    ax[5 * index + 4].set_title('Image with CUP Mask')\n    \ndef load_image_with_masks(path, dice = False, test = False):\n    '''\n    This is similar to the\n    load image_with_gt function\n    however uses pure tf functions\n    for the use in datasets\n    otherwise I'd have to run\n    in eagerexecution\n    '''\n    \n    filename = tf.strings.split(path, sep = '.')[-2]\n    filename = tf.strings.split(filename, sep = '\/')[-1]\n    directory_path = tf.strings.split(path, sep = 'Images')[0]\n    if test:\n        softmap_path = tf.strings.join([directory_path, 'Test_GT\/', filename, '\/SoftMap\/'])\n    else:\n        softmap_path = tf.strings.join([directory_path, 'GT\/', filename, '\/SoftMap\/'])\n    od_path = tf.strings.join([softmap_path, filename, '_ODsegSoftmap.png'])\n    cup_path = tf.strings.join([softmap_path, filename, '_cupsegSoftmap.png'])\n    image = tf.io.read_file(path)\n    image = tf.io.decode_png(image, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = image \/ 255.0\n    \n    od = tf.io.read_file(od_path)\n    od = tf.io.decode_png(od, channels = 1)\n    od = tf.image.resize(od, IMAGE_SIZE)\n    od = od \/ 255.0\n    if dice:\n        od = tf.where(od >= 0.5, 1.0, 0.0)\n    \n    cup = tf.io.read_file(cup_path)\n    cup = tf.io.decode_png(cup, channels = 1)\n    cup = tf.image.resize(cup, IMAGE_SIZE)\n    cup = cup \/ 255.0\n    if dice:\n        cup = tf.where(cup >= 0.5, 1.0, 0.0)\n    \n    return {'image' : image}, {'od' : od, 'cup' : cup}\n\ndef augment_images(image, od, cup, single_target = False):\n    \n    seeds = STATELESS_RNG.make_seeds(2)[0], STATELESS_RNG.make_seeds(2)[0]\n    \n    def augment(img, seeds):\n        \n        img = tf.image.stateless_random_flip_left_right(img, seeds[0])\n        img = tf.image.stateless_random_flip_up_down(img, seeds[1])\n        img = tf.image.resize_with_crop_or_pad(img, IMAGE_SIZE[0] + 20, IMAGE_SIZE[1] + 20)\n        img = tf.image.stateless_random_crop(img, size = (*IMAGE_SIZE, img.shape[2]), seed = seeds[0])\n        \n        return img\n    \n    image = augment(image, seeds)\n    od = augment(od, seeds)\n    cup = augment(cup, seeds)\n    \n    if single_target:\n        return image, od\n    return {'image' : image}, {'od' : od, 'cup' : cup}\n    ","e34f8f1b":"image, od, cup = load_image_and_gt(TRAIN_NORMAL_IMAGES[0])\nplt.imshow(od)\nimage.shape","9fff2036":"rows = 6\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize = (15, 15))\naxes = axes.flatten()\n\nnormal_images = np.random.choice(TRAIN_NORMAL_IMAGES, rows \/\/ 2)\nglaucoma_images = np.random.choice(TRAIN_GLAUCOMA_IMAGES, rows - (rows \/\/ 2))\n\nfor idx, file in enumerate(normal_images):\n    image, od, cup = load_image_and_gt(file)\n    show_image_with_masks(image, od, cup, axes, index = idx, label = 'Normal :' + os.path.basename(file))\n    \nfor idx, file in enumerate(glaucoma_images):\n    image, od, cup = load_image_and_gt(file)\n    show_image_with_masks(image, od, cup, axes, index = idx + rows \/\/ 2, label = 'Glaucoma :' + os.path.basename(file))\n\nplt.tight_layout()","51bf5053":"def get_avgboundary(file):\n    image = tf.io.read_file(file)\n    image = tf.io.decode_png(image)\n    \n    cup_mask = np.zeros(shape = (image.shape[0], image.shape[1]))\n    od_mask = np.zeros(shape = (image.shape[0], image.shape[1]))\n    center_mask = np.zeros(shape = (image.shape[0], image.shape[1]))\n    \n    filename = Path(file).stem\n    boundary_path = os.path.join(PATH_TO_TRAINING, 'GT', filename, 'AvgBoundary')\n    cup_file = os.path.join(boundary_path, filename + '_CupAvgBoundary.txt')\n    od_file = os.path.join(boundary_path, filename + '_ODAvgBoundary.txt')\n    center_file = os.path.join(boundary_path, filename + '_diskCenter.txt')\n    \n    count = 0\n    with open(cup_file) as cf:\n        for line in cf:\n            a, b = line.split()\n            a = int(a)\n            b = int(b)\n            cup_mask[a, b] = 255\n            count += 1\n\n    with open(od_file) as of:\n        for line in of:\n            a, b = line.split()\n            a = int(a)\n            b = int(b)\n            od_mask[a, b] = 255\n        \n    with open(center_file) as ctrf:\n        for line in ctrf:\n            a, b = line.split()\n            a = int(a)\n            b = int(b)\n            center_mask[a, b] = 255\n    return od_mask.reshape(image.shape[0], image.shape[1], 1), \\\ncup_mask.reshape(image.shape[0], image.shape[1], 1), center_mask.reshape(image.shape[0], image.shape[1], 1)\n    ","a3f98e74":"glaucoma_avgboundary_test = TRAIN_GLAUCOMA_IMAGES[3]\n\nod_mask, cup_mask, center_mask = get_avgboundary(glaucoma_avgboundary_test)\nimage, od, cup = load_image_and_gt(glaucoma_avgboundary_test)","cebc7fd2":"fig, axes = plt.subplots(2, 3, figsize = (20, 20))\naxes = axes.flatten()\n\naxes[0].imshow(image)\naxes[0].set_title('Test Image')\naxes[1].imshow(od)\naxes[1].set_title('Optical Disk Mask')\naxes[2].imshow(od_mask)\naxes[2].set_title('Optical Disk Avg Boundary')\naxes[3].imshow(cup)\naxes[3].set_title('Cup Mask')\naxes[4].imshow(cup_mask)\naxes[4].set_title('Cup Avg Boundary')\naxes[5].imshow(center_mask)\naxes[5].set_title('Center Avg Boundary')\nplt.tight_layout()","748203f9":"train_images = np.concatenate([TRAIN_NORMAL_IMAGES, TRAIN_GLAUCOMA_IMAGES])\ntest_images = np.concatenate([TEST_NORMAL_IMAGES, TEST_GLAUCOMA_IMAGES])\n\ntrain_ds_dice = Dataset.from_tensor_slices(train_images)\\\n    .shuffle(buffer_size = SHUFFLE_BUFFER)\\\n    .map(lambda x: load_image_with_masks(x, dice = True), num_parallel_calls = AUTOTUNE)\\\n    .map(lambda image, targets: augment_images(image['image'], targets['od'], targets['cup']))\\\n    .batch(batch_size = BATCH_SIZE)\\\n    .prefetch(1)\\\n    .cache()\n\ntrain_ds_no_dice = Dataset.from_tensor_slices(train_images)\\\n    .shuffle(buffer_size = SHUFFLE_BUFFER)\\\n    .map(lambda x: load_image_with_masks(x, dice = False), num_parallel_calls = AUTOTUNE)\\\n    .map(lambda image, targets: augment_images(image['image'], targets['od'], targets['cup']))\\\n    .batch(batch_size = BATCH_SIZE)\\\n    .prefetch(1)\\\n    .cache()\n\ntest_ds_dice = Dataset.from_tensor_slices(test_images)\\\n    .map(lambda x: load_image_with_masks(x, dice = True, test = True), num_parallel_calls = AUTOTUNE)\\\n    .batch(batch_size = BATCH_SIZE)\n\ntest_ds_no_dice = Dataset.from_tensor_slices(test_images)\\\n    .map(lambda x: load_image_with_masks(x, dice = False, test = True), num_parallel_calls = AUTOTUNE)\\\n    .batch(batch_size = BATCH_SIZE)","c26845dd":"train_ds_no_dice_st = Dataset.from_tensor_slices(train_images)\\\n    .shuffle(buffer_size = SHUFFLE_BUFFER)\\\n    .map(lambda x: load_image_with_masks(x, dice = False), num_parallel_calls = AUTOTUNE)\\\n    .map(lambda image, targets: augment_images(image['image'], targets['od'], targets['cup'], single_target = True))\\\n    .batch(batch_size = BATCH_SIZE)\\\n    .prefetch(1)\\\n    .cache()\n\ntest_ds_no_dice_st = Dataset.from_tensor_slices(test_images)\\\n    .map(lambda x: load_image_with_masks(x, dice = False, test = True), num_parallel_calls = AUTOTUNE)\\\n    .map(lambda image, targets: (image['image'], targets['od']))\\\n    .batch(batch_size = BATCH_SIZE)","0666ec28":"def upsampling_block(inp, skips, filters, kernels, rates):\n    '''\n    Creates a block for upsampling\n    Doubles the image dimensions\n    And adds the skip connection\n    '''   \n    activation = 'relu'\n    x = inp  \n    for f, kernel, skip, rate in zip(filters, kernels, skips, rates):\n        x = UpSampling2D()(x)\n        x = Conv2D(filters = f, kernel_size = kernel, strides = 1, activation = activation, padding = 'same')(x)\n        x = concatenate([x, skip])\n        x = BatchNormalization()(x)\n        x = SpatialDropout2D(rate = rate)(x)\n    return x\n\ndef conv_block(inp, filters, kernels, strides, rates):\n    '''\n    Builds a convulitional block\n    When the strides reach 2\n    Keeps the output of the last\n    Conv layer to be used\n    For a skip connection\n    '''\n\n    activation = 'relu'\n    x = inp\n    skips = []\n    for f, kernel, stride, rate in zip(filters, kernels, strides, rates):\n        if stride == 2:\n            skips.append(skip)\n            x = MaxPool2D()(x)\n            x = Conv2D(filters = f, kernel_size = kernel, strides = 1, activation = activation, padding = 'same')(x)\n            skip = x\n        else:\n            x = Conv2D(filters = f, kernel_size = kernel, strides = 1, activation = activation, padding = 'same')(x)\n            skip = x\n        \n        x = BatchNormalization()(x)\n        x = SpatialDropout2D(rate = rate)(x)\n    \n    return skips, x","cd21d1a4":"def res_unet(conv_filters, conv_kernels, conv_strides, conv_rates, up_filters, up_kernels, up_rates):\n    '''\n    Builds the u-net structure\n    The output is two heads\n    One for the Optical Disk\n    And another for the cup\n    '''\n    inp = Input(shape = (*IMAGE_SIZE, 3), name = 'image')\n    x = inp\n    \n    skips, x = conv_block(x, conv_filters, conv_kernels, conv_strides, conv_rates)\n    \n    skips = skips[::-1]\n    \n    od = upsampling_block(x, skips, up_filters, up_kernels, up_rates)\n    od = Conv2D(filters = 1, kernel_size = 1, strides = 1, padding = 'same', activation = 'sigmoid', name = 'od')(od)\n    \n    cup = upsampling_block(x, skips, up_filters, up_kernels, up_rates)\n    cup = Conv2D(filters = 1, kernel_size = 1, strides = 1, padding = 'same', activation = 'sigmoid', name = 'cup')(cup)\n       \n    return Model(inputs = inp, outputs = [od, cup])","50b1f60c":"def res_unet_st(conv_filters, conv_kernels, conv_strides, conv_rates, up_filters, up_kernels, up_rates):\n    '''\n    Builds the u-net structure\n    The output is two heads\n    One for the Optical Disk\n    And another for the cup\n    '''\n    inp = Input(shape = (*IMAGE_SIZE, 3), name = 'image')\n    x = inp\n    \n    skips, x = conv_block(x, conv_filters, conv_kernels, conv_strides, conv_rates)\n    \n    skips = skips[::-1]\n    \n    od = upsampling_block(x, skips, up_filters, up_kernels, up_rates)\n    od = Conv2D(filters = 1, kernel_size = 1, strides = 1, padding = 'same', activation = 'sigmoid', name = 'od')(od)\n           \n    return Model(inputs = inp, outputs = od)","bb8bcd04":"def dice(y, y_pred):\n    '''\n    Dice Loss, similar to\n    Intersection over Union\n    '''\n    epsilon = EPSILON\n    numerator = 2 * tf.reduce_sum(y * y_pred, axis = [1, 2])\n    denominator = tf.reduce_sum(y + y_pred, axis = [1, 2])\n    dice = tf.reduce_mean((numerator + epsilon)\/(denominator + epsilon))\n    return 1 - dice\n","8bca30c3":"conv_filters = [8, 16, 32, 64]\nconv_kernels = [3, 3, 3, 3]\nconv_strides = [1, 2, 2, 2]\nconv_rates = [0.8, 0.8, 0.8, 0.8]\n\nupsampling_filters = [32, 16, 8, 8]\nupsampling_kernels = [3, 3, 3, 3]\nupsampling_rates = [0.5, 0.5, 0.5, 0.5]\n\nmod1 = res_unet(conv_filters, conv_kernels, conv_strides, conv_rates, upsampling_filters, upsampling_kernels, upsampling_rates)\nmod1.compile(optimizer = 'Adam', loss = {'od' : dice, 'cup' : dice}, metrics = ['accuracy'])","fe38c746":"plot_model(mod1)","9e7024b9":"hist1 = mod1.fit(train_ds_dice, validation_data = test_ds_dice, callbacks = [early_stopping], epochs = 200)","ff7dc9f9":"mod2 = res_unet(conv_filters, conv_kernels, conv_strides, conv_rates, upsampling_filters, upsampling_kernels, upsampling_rates)\nmod2.compile(optimizer = 'Adam', loss = {'od' : 'binary_crossentropy', 'cup' : 'binary_crossentropy'}, metrics = ['accuracy'])\nhist2 = mod2.fit(train_ds_no_dice, validation_data = test_ds_no_dice, callbacks = [early_stopping], epochs = 100)","19dbc363":"def plot_loss(epochs, hist, loss):\n    '''\n    Shows the loss metrics\n    Uses a more fair comparison\n    Between Val and Train\n    By shifting the train loss\n    an epoch later\n    (Train loss is also hurt\n    by the dropout layers)\n    '''\n    plt.plot(epochs + .5, hist.history['loss'], 'r', label = 'Loss')\n    plt.plot(epochs + .5, hist.history['od_loss'], 'r.-', label = 'OD Loss')\n    plt.plot(epochs + .5, hist.history['cup_loss'], 'r+', label = 'Cup Loss')\n    plt.plot(epochs, hist.history['val_loss'], 'b', label = 'Val Loss')\n    plt.plot(epochs, hist.history['val_od_loss'], 'b.-', label = 'Val OD Loss')\n    plt.plot(epochs, hist.history['val_cup_loss'], 'b+', label = 'Val Cup Loss')\n    plt.legend()\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title(loss)\n\n\nepochs1 = np.arange(0, len(hist1.history['loss']))\nplot_loss(epochs1, hist1, 'Dice')","c912c17d":"epochs2 = np.arange(0, len(hist2.history['loss']))\nplot_loss(epochs2, hist2, 'Binary CrossEntropy')","08198efe":"to_predict = Dataset.from_tensor_slices(test_images)\\\n    .map(lambda x: load_image_with_masks(x, dice = True, test = True), num_parallel_calls = AUTOTUNE)\\\n    .map(lambda image, targets: image['image'])\\\n    .batch(batch_size = BATCH_SIZE)\n\n\npredict1 = mod1.predict(to_predict)\npredict2 = mod2.predict(to_predict)","5c4a2670":"test = next(iter(predict1))\ntest.shape","f1106960":"def plt_preds(ds, preds, super_title, binarizer_threshold = 0.5):\n    '''\n    Plots row numbers of images\n    Along with the original mask\n    And the predicted mask\n    Also binarizes the predicted\n    masks\n    '''\n    ds = next(iter(ds))\n    image, targets = ds\n    image = image['image']\n    od = targets['od']\n    cup = targets['cup']\n    preds = next(iter(preds))\n    rows = 5\n    cols = 7\n    fig, axes = plt.subplots(rows, cols, figsize = (20, 20))\n    axes = axes.flatten()\n    for i in range(rows):\n        axes[cols * i].imshow(image[i])\n        axes[cols * i].set_title('Original Image')\n        axes[cols * i + 1].imshow(od[i])\n        axes[cols * i + 1].set_title('Original OD Mask')\n        axes[cols * i + 2].imshow(cup[i])\n        axes[cols * i + 2].set_title('Original Cup Mask')\n        axes[cols * i + 3].imshow(preds[i])\n        axes[cols * i + 3].set_title('Predicted OD Mask')\n        axes[cols * i + 4].imshow(np.where(preds[i] > binarizer_threshold, 1, 0))\n        axes[cols * i + 4].set_title('Binarized OD Mask')\n        axes[cols * i + 5].imshow(preds[26 + i - 1])\n        axes[cols * i + 5].set_title('Predicted Cup Mask')\n        axes[cols * i + 6].imshow(np.where(preds[26 + i] >= binarizer_threshold, 1, 0))\n        axes[cols * i + 6].set_title('Binarized Cup Mask')\n    plt.suptitle(super_title)","2a28f2f1":"plt_preds(test_ds_dice, predict1, super_title = 'Dice Loss Trained on Binarized GT Masks', binarizer_threshold = 0.5)","ac3d0bd1":"plt_preds(test_ds_no_dice, predict2, binarizer_threshold = 0.25, super_title = 'Binary Cross Entropy Trained on Default Masks')","7b7e6602":"So the Avg Boundary files give a small ring for a stricter border for the masks. I'm going to ignore them for the time being","5eef1396":"Current status:\n\nI'm trying to make a u_net that makes the mask for both the optical disk and cup\nI'm trying to use tf datasets to do this\nSo far, based on some masks, it looks like it's treating the input image as the loss as well","ba567605":"So these images were using the Softmap folder. Let's ee what the AvgBoundary and CDR values look like."}}