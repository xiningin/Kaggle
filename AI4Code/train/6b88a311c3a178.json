{"cell_type":{"0242320e":"code","07628cce":"code","7608156d":"code","bcff495b":"code","8d03c84b":"code","21436b35":"code","fb4e15d8":"code","be7c166a":"code","de0e5345":"code","3dad3844":"code","c88a01ed":"code","6892d8a3":"code","3b5ba490":"code","d9aeb415":"code","19d84c21":"code","7c312789":"code","c3619d14":"code","739fe435":"code","4fe372b2":"code","0f6516b5":"code","783c7988":"code","b40a00dc":"code","966647d0":"code","6f3b7838":"code","0b9fff77":"code","7bb0862e":"code","e4057dde":"code","055c60d1":"code","24aa0d71":"markdown"},"source":{"0242320e":"\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport re\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport xgboost as xgb","07628cce":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\n","7608156d":"train_df.columns","bcff495b":"import missingno as msno\nmsno.matrix(train_df)\nmsno.matrix(test_df)","8d03c84b":"\n\ndef feature_plots(feature, df=train_df, labels={}):\n\n    survived_mapping = df['Survived'].map({0: 'Dead', 1: 'Survived'})\n\n    fig = px.histogram(df, x=survived_mapping, width=600, color=feature, labels=labels)\n    fig.update_layout(\n        bargap=0.2,\n        xaxis_title_text='Survived',\n        yaxis_title_text='Survival count'\n    )\n    \n    return fig\n\n","21436b35":"feature_plots('Sex')","fb4e15d8":"feature_plots('SibSp')","be7c166a":"feature_plots('Pclass')","de0e5345":"fig = px.histogram(train_df, x='Age', color='Survived', barmode='overlay',width= 600)\nfig","3dad3844":"fig = px.histogram(train_df, x=train_df['Age'], facet_row=train_df['Pclass'],facet_col=train_df['Survived'], width=700)\n\nfig","c88a01ed":"for df in combine:\n    \n    df['Title'] = df['Name'].map(lambda x: re.search(r' ([A-Za-z]+)\\.', x).group().strip().replace('.', ''))\n\n\n\ntrain_df['Title'].value_counts().index","6892d8a3":"def feature_engineering(df):\n    \n    age_bins = [0, 5.99, 11.9, 17.9, 25.9, 47.9, 61.9, 80] #Binning Age feature based on the trend in histogram \n    age_labels = [i for i in range(1, 8)]\n    title_map = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Dr': 5, 'Rev': 5, 'Col': 5,\n                 'Major': 5, 'Mlle': 5, 'Ms': 5, 'Countess': 5, 'Lady': 5, 'Capt': 5,\n                 'Jonkheer': 5, 'Don': 5, 'Sir': 5, 'Mme': 5}\n    \n    \n        \n    df['Title'] = df['Title'].map(title_map)\n    df['Title'].fillna(1,inplace=True)\n    df = pd.concat([df, pd.get_dummies(df['Title'], prefix='Title')], axis=1)\n    df = pd.concat([df, pd.get_dummies(df['Sex'], prefix='Sex')], axis=1)\n    df = df.drop(['Ticket', 'Cabin','Name', 'PassengerId'], axis=1)\n    df['Age'] = df['Age'].fillna(df.groupby('Title')['Age'].transform('median'))\n    df['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels)\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = 0\n    df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n    df['Fare'] = df['Fare'].fillna(df.groupby('Pclass')['Fare'].transform('median'))\n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n    df = pd.concat([df, pd.get_dummies(df['Pclass'], prefix='Pclass')], axis=1)\n    df = pd.concat([df, pd.get_dummies(df['Embarked'], prefix='Embarked')], axis=1)\n    df = pd.concat([df, pd.get_dummies(df['AgeGroup'], prefix='AgeGroup')], axis=1)\n    df = df.drop(['Age','Sex','Pclass', 'Embarked', 'Title', 'AgeGroup'], axis=1)\n      \n    return df\n           ","3b5ba490":"train_df= feature_engineering(train_df)\ntest_df= feature_engineering(test_df)","d9aeb415":"def scale_feature(feature):\n    result = []\n    \n    # Applying min-max scaling to the 'Parch' and 'SipSp' features\n    for df in combine:\n        feature_val = df[feature]\n        max_val = feature_val.max()\n        min_val = feature_val.min()\n        scaled_feature = (feature_val - min_val) \/ (max_val - min_val)\n        result.append(scaled_feature)\n        \n    return result\n\ntrain_df['SibSp'], test_df['SibSp'] = scale_feature('SibSp')\ntrain_df['Parch'], test_df['Parch'] = scale_feature('Parch')\ntrain_df['Fare'], test_df['Fare'] = scale_feature('Fare')\n","19d84c21":"def plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","7c312789":"\ntrain_set, test_set = train_test_split (train_df, test_size = 0.1, random_state = 42)\ndata = train_set\nvalid_data = test_set","c3619d14":"\ny = valid_data.Survived.tolist()\nvalid_data = valid_data.drop('Survived', 1)\nX = np.array(valid_data)","739fe435":"y = np.array(data.Survived.tolist())\ndata = data.drop('Survived', 1)\nX = np.array(data)","4fe372b2":"\nskf = StratifiedKFold(n_splits=5 ,shuffle = True, random_state = 42)\nfor train_index, test_index in skf.split(X, y):\n    X_train, y_train = X[train_index.astype(int)], y[train_index.astype(int)]\n    X_test, y_test = X[test_index.astype(int)], y[test_index.astype(int)]","0f6516b5":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1)\n\nxgb_cfl.fit(X_train, y_train)\ny_pred = xgb_cfl.predict(X_test)\ny_score = xgb_cfl.predict_proba(X_test)[:,1]\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='XGB Confusion matrix')\nplt.show()\nf1_score(y_test,y_pred, labels=None, pos_label=1, average= 'macro', sample_weight=None)\n#show_metrics()","783c7988":"print(xgb_cfl.get_xgb_params())","b40a00dc":"#param_grid = {\n#            'n_estimators': [700, 1000, 1200, 1300],\n#            'max_depth': [3, 4, 5],\n#            'min_child_weight': [1, 2]\n#              }\n#\n#CV_xgb_cfl = GridSearchCV(estimator = xgb_cfl, param_grid = param_grid, scoring= 'f1_macro', verbose = 2)\n#CV_xgb_cfl.fit(X_train, y_train)\n#\n#best_parameters = CV_xgb_cfl.best_params_\n#print(\"The best parameters for using this model is\", best_parameters)","966647d0":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1, max_depth= 3,\n                            n_estimators = 700, min_child_weight= 2)\n\nxgb_cfl.fit(X_train, y_train)\ny_pred = xgb_cfl.predict(X_test)\ny_score = xgb_cfl.predict_proba(X_test)[:,1]\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes = class_names, \n                      title = 'XGB Confusion matrix')\nplt.savefig('2.xgb_cfl_confusion_matrix.png')\nplt.show()\nf1_score(y_test,y_pred, labels=None, pos_label=1, average= 'macro', sample_weight=None)","6f3b7838":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","0b9fff77":"PassengerId= test.PassengerId\n\ntest_df=test_df.to_numpy()","7bb0862e":"Survived=xgb_cfl.predict(test_df)","e4057dde":"submission_df = pd.DataFrame({\n    'PassengerId': test[\"PassengerId\"],\n    'Survived': Survived\n})\n\nsubmission_df.head()","055c60d1":"submission_df.to_csv('submission.csv', index=False)","24aa0d71":"The best parameters for using this model is {'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 700}"}}