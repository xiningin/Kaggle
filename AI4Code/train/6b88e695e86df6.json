{"cell_type":{"aade55f5":"code","b1cc3793":"code","ad2b0f21":"code","9dfef39d":"code","e46ac964":"code","44142367":"code","d2390930":"code","fa1a3299":"code","5197b30a":"code","7373cdc7":"code","fdcd9b42":"code","f82f2e15":"code","607339f4":"markdown","da16910e":"markdown","a3df8103":"markdown","c389c4d0":"markdown","a258a120":"markdown","c6e99371":"markdown","903373b3":"markdown"},"source":{"aade55f5":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b1cc3793":"# Importatio  des donn\u00e9es\ndata = pd.read_csv('\/kaggle\/input\/donnes-iris\/Irs.csv')\ndata = data[:100]\ndata = data.drop(['Id'], axis=1)\ndata.head()","ad2b0f21":"x = data.iloc[:100,[0, 2]].values\ny = data.iloc[0:100, 4].values","9dfef39d":"# convertion des variables en binaire\ny = np.where(y == 'Iris-setosa', -1, 1)","e46ac964":"# s\u00e9paration des donn\u00e9es train et test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)","44142367":"# Modification d'echelle\nsc_x = StandardScaler()\nx_train_scale = sc_x.fit_transform(x_train) \nx_test_scale = sc_x.transform(x_test) ","d2390930":"# plot DES DONNEES\nplt.scatter(x[:49, 0], x[:49, 1], color='red', marker='o', label='setosa')\nplt.scatter(x[49:100, 0], x[49:100, 1], color='blue', marker='x', label='versicolor')\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.legend(loc='upper left')\nplt.grid()\nplt.show()","fa1a3299":"def gradient_descent(alpha, x_train_scale, y_train, ep=0.0001, max_iter=10000):\n    converged = False\n    iter = 0\n    m = x_train_scale.shape[0]\n    \n    w = np.random.random(x_train_scale.shape[1])\n    b = np.random.random()\n    \n    dot = []\n    x_l = []\n    y_l = []\n    for i in range(m):\n        L = float((np.dot(w, x_train_scale[i:i+1,:].reshape(2,))+b)*y_train[i:i+1]) #+b\n        if L < 0:\n            dot.append(L)\n            x_l.append(x_train_scale[i:i+1,:])\n            y_l.append(y_train[i:i+1])\n            \n    len(dot)!= 0\n    x_lo = []\n    y_lo = []        \n    loss = sum(dot)\/len(dot)\n    x_lo = np.array(x_l).reshape(len(dot),2)\n    y_lo = np.array(y_l).reshape(len(dot),1)\n    \n    while not converged:\n   # for each training sample, compute the gradient (d\/d_theta j(theta))\n        \n        grad_w = 1\/len(dot) * sum(-(x_lo*y_lo))\n        grad_b = 1\/len(dot) * float(sum(-(y_lo)))\n\n        # update the theta_temp\n        temp_w = w - alpha * grad_w\n        temp_b = b - alpha * grad_b\n        \n        # update theta\n        w = temp_w\n        b = temp_b\n\n        # loss with new weights\n        dot1 = []\n        x_l = []\n        y_l = []\n       #len(dot1) != 0 \n        for i in range(m):\n            E = float((np.dot(w, x_train_scale[i:i+1,:].reshape(2,))+b)*y_train[i:i+1]) #+b\n            if E < 0:\n                dot1.append(E)\n                x_l.append(x_train_scale[i:i+1,:])\n                y_l.append(y_train[i:i+1])\n                 \n        x_lo = []\n        y_lo = []\n        \n        loss_upd = sum(dot1)\/len(dot1)\n        x_lo = np.array(x_l).reshape(len(dot1),2)\n        y_lo = np.array(y_l).reshape(len(dot1),1)\n\n        if abs(loss-loss_upd) <= ep:\n            print('Converged, iterations: ', iter, '!!!')\n            converged = True\n    \n        loss = loss_upd   # update error \n        #x_l = x_l1\n        #y_l = y_l1\n        iter += 1  # update iter\n    \n        if iter == max_iter:\n            print('Max interactions exceeded!')\n            converged = True\n    \n    return w,b","5197b30a":"alpha = 0.01 # pas d'apprentissage\nep = 0.00005 # convergence criteria\n\n# call gredient decent, and get intercept(=theta0) and slope(=theta1)\nweights, bias = gradient_descent(alpha, x_train_scale, y_train, ep, max_iter=10000)\nprint(weights, bias)","7373cdc7":"xx = np.linspace(-3, 3)\na = -weights[0]\/weights[1]\nyy = a*xx - bias\/weights[1]\n\nplt.figure(2, figsize=(8, 6))\n\n# Plot the training points\nplt.scatter(x_train_scale[:, 0], x_train_scale[:, 1], c=y_train, cmap='bwr', edgecolor='k')    #plt.cm.Set1,\n\n\nplt.plot(xx, yy)\nplt.ylim(-3,3)\nplt.xlim(-3,3)\n\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.legend(loc='upper left')\n\nplt.grid()\nplt.show()","fdcd9b42":"m_test = x_test_scale.shape[0]\ndot_test = []\nx_l = []\ny_l = []\nfor i in range(m_test):\n    L = float((np.dot(weights, x_test_scale[i:i+1,:].reshape(2,))+bias)*y_test[i:i+1]) #+b\n    if L < 0:\n        dot_test.append(L)\n        x_l.append(x_test_scale[i:i+1,:])\n        y_l.append(y_test[i:i+1])\n\nprint('accuracy:', 1-len(dot_test)\/len(x_test_scale))","f82f2e15":"xx = np.linspace(-3, 3)\na = -weights[0]\/weights[1]\nyy = a*xx - bias\/weights[1]\n\nplt.figure(2, figsize=(8, 6))\n\n# Plot the test points\nplt.scatter(x_test_scale[:, 0], x_test_scale[:, 1], c=y_test, cmap='bwr',    #plt.cm.Set1,\n            edgecolor='k')\n\nplt.plot(xx, yy)\nplt.ylim(-3,3)\nplt.xlim(-3,3)\n\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.grid()\nplt.show()","607339f4":"### L'impl\u00e9mentation de l'algorithme de descent de gradient","da16910e":"DONNEES DE TEST\n","a3df8103":"Pour classifier les donn\u00e9es iris en 2 classes, on prend juste les 100 premiers lignes pour ne pas avoir 3 classes.","c389c4d0":"L'objectif est de classifier les donn\u00e9es en deux classes en utilisant le mod\u00e9le de perceptron sans faire appel \u00e0 des biblioth\u00e8ques de l'apprentissage automatique.","a258a120":"## Perceptron","c6e99371":"Le probl\u00e8me pos\u00e9 est une classification $x \\in \\mathbb{R}^n$\n\n\\begin{eqnarray}\n\\forall z \\in \\mathbb{R},\nf(z) = \\begin{cases} 1 & \\text{si z est positif} \\\\\n-1  & \\text{si $z$ est n\u00e9gatif (ou nul)}\\end{cases}   \n\\end{eqnarray}\n","903373b3":"Un perceptron \u00e0 $n$ entr\u00e9es ($ x_1 ,..., x_n $) et \u00e0 une seule sortie $O$ est d\u00e9fini par la donn\u00e9e de $n$ poids (ou coefficients synaptiques) ($w_1 , ... , w_n$) et un biais (ou seuil)  $b$, associ\u00e9 \u00e0 une information virtuelle $x_0 = 1$ et $w_0 = b$ par:\n\n\\begin{eqnarray}\n\\forall x \\in \\mathbb{R}^n, z  = \\sum \\limits_{i=0}^n x_{i}w_{i} = \\sum \\limits_{i=1}^n x_{i}w_{i} + w_0 = \\sum \\limits_{i=1}^n x_{i}w_{i} + b     \n\\end{eqnarray}\n\nLe r\u00f4le d'un neurone est de fournir une r\u00e9ponse $y$ entre 0 et 1 \u00e0 partir de $z$.\n\nOn d\u00e9finit une fonction $f$ : $\\mathbb{R} \\longrightarrow[0,1]$\n\n\\begin{eqnarray}\ny = f(z) = f(\\sum \\limits_{i=1}^n x_{i}w_{i} + b) \n\\end{eqnarray}"}}