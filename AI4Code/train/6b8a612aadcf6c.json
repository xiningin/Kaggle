{"cell_type":{"d7fc1d18":"code","b3013a2f":"code","9d33472c":"code","642a691f":"code","12fbd1fe":"code","db91abea":"code","5d773a25":"code","9fbf7a3a":"code","c74d2f3a":"code","141606e8":"code","889f70b1":"code","c877c476":"code","8feff3fb":"code","3a6056ef":"code","35d65ef7":"code","c7d5811c":"code","d47e2181":"code","14f968ed":"code","ae930e2d":"code","5488ebf0":"code","c9098f87":"code","b138657b":"code","36d9f3f3":"code","8cdd2f2a":"code","0862a83e":"code","a8a2fbd2":"code","b42f49b1":"code","d9dd8f41":"code","0fbfb543":"markdown","e32d49c1":"markdown","030c8424":"markdown","577c9e8a":"markdown","883961eb":"markdown","c00926d5":"markdown","7a39f4e1":"markdown","a7f904e5":"markdown","bdefaa76":"markdown","a897caff":"markdown","4af88b15":"markdown","b7b56b86":"markdown","e8f33484":"markdown","239357b9":"markdown","ac83816f":"markdown","dfbcdc89":"markdown","01ec5432":"markdown","adb29b92":"markdown"},"source":{"d7fc1d18":"import gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import LayerNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.layers import Conv1D, Flatten, Dense\nfrom tensorflow.keras.layers import Input, Dropout, Activation\n\nfrom transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel, XLMRobertaConfig","b3013a2f":"! mkdir \".\/Roberta-Base\"\n! mkdir \".\/XLM-Roberta-Base\"\n! mkdir \".\/DistilRoberta-Base\"","9d33472c":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df[\"excerpt_wordlen\"] = train_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ntrain_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\ntrain_df.set_index(\"id\", inplace=True)\nprint(f\"train_df: {train_df.shape}\\n\")\ntrain_df.head()","642a691f":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df[\"excerpt_wordlen\"] = test_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ntest_df.drop(['url_legal','license'], inplace=True, axis=1)\ntest_df.set_index(\"id\", inplace=True)\nprint(f\"test_df: {test_df.shape}\\n\")\ntest_df.head()","12fbd1fe":"Ytrain = train_df['target'].values\nYtrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\ntrain_df.drop(['target'], inplace=True, axis=1)\nprint(f\"Ytrain: {Ytrain.shape}\")","db91abea":"FOLD = 5\nNUM_SEED = 1\nVERBOSE = 1\nMINI_BATCH_SIZE = 16\nNUM_EPOCH = 20\nMAX_LEN = max(train_df['excerpt_wordlen'].max(), \n              test_df['excerpt_wordlen'].max()) + 11\n\nROBERTA_BASE = \"..\/input\/huggingface-roberta-variants\/roberta-base\/roberta-base\"\nXLM_ROBERTA_BASE = \"..\/input\/huggingface-roberta-variants\/tf-xlm-roberta-base\/tf-xlm-roberta-base\"\nDISTILROBERTA_BASE = \"..\/input\/huggingface-roberta-variants\/distilroberta-base\/distilroberta-base\"","5d773a25":"def sent_encode(texts, tokenizer):\n    input_ids = []\n    attention_mask = []\n    token_type_ids = []\n\n    for text in tqdm(texts):\n        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n                                       padding='max_length', add_special_tokens=True, \n                                       return_attention_mask=True, return_token_type_ids=True, \n                                       return_tensors='tf')\n        \n        input_ids.append(tokens['input_ids'])\n        attention_mask.append(tokens['attention_mask'])\n        token_type_ids.append(tokens['token_type_ids'])\n\n    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids)","9fbf7a3a":"def rmse_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))","c74d2f3a":"def commonlit_model(transformer_model, use_tokens_type_ids=True):\n    \n    input_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n\n    if use_tokens_type_ids:\n        embed = transformer_model(input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n    \n    else:\n        embed = transformer_model(input_id, attention_mask=attention_mask)[0]\n    \n    #x = embed[:, 0, :]\n    embed = LayerNormalization()(embed)\n    \n    x = WeightNormalization(\n            Conv1D(filters=384, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(embed)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = WeightNormalization(\n            Conv1D(filters=192, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(x)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = Flatten()(x)\n    x = Dropout(rate=0.5)(x)\n    \n    x = Dense(units=1, kernel_initializer='lecun_normal')(x)\n\n    model = Model(inputs=[input_id, attention_mask, token_type_id], outputs=x, \n                  name='CommonLit_Readability_Model')\n    return model","141606e8":"tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_BASE)","889f70b1":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","c877c476":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","8feff3fb":"config = RobertaConfig.from_pretrained(ROBERTA_BASE)\nconfig.output_hidden_states = False\n\ntransformer_model = TFRobertaModel.from_pretrained(ROBERTA_BASE, config=config)","3a6056ef":"model = commonlit_model(transformer_model)\nmodel.summary()","35d65ef7":"np.random.seed(23)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final1 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/Roberta-Base\/CLRP_Roberta_Base_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'.\/Roberta-Base\/CLRP_Roberta_Base_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final1 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final1 = y_pred_final1 \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","c7d5811c":"tokenizer = XLMRobertaTokenizer.from_pretrained(XLM_ROBERTA_BASE)","d47e2181":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","14f968ed":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","ae930e2d":"config = XLMRobertaConfig.from_pretrained(XLM_ROBERTA_BASE)\nconfig.output_hidden_states = False\n\ntransformer_model = TFXLMRobertaModel.from_pretrained(XLM_ROBERTA_BASE, config=config)","5488ebf0":"model = commonlit_model(transformer_model)\nmodel.summary()","c9098f87":"np.random.seed(29)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final2 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/XLM-Roberta-Base\/CLRP_XLMRoberta_Base_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'.\/XLM-Roberta-Base\/CLRP_XLMRoberta_Base_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final2 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final2 = y_pred_final2 \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","b138657b":"tokenizer = RobertaTokenizer.from_pretrained(DISTILROBERTA_BASE)","36d9f3f3":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","8cdd2f2a":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","0862a83e":"config = RobertaConfig.from_pretrained(DISTILROBERTA_BASE)\nconfig.output_hidden_states = False\n\ntransformer_model = TFRobertaModel.from_pretrained(DISTILROBERTA_BASE, config=config)","a8a2fbd2":"model = commonlit_model(transformer_model, use_tokens_type_ids=False)\nmodel.summary()","b42f49b1":"np.random.seed(31)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final3 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model, use_tokens_type_ids=False)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/DistilRoberta-Base\/CLRP_DistilRoberta_Base_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'.\/DistilRoberta-Base\/CLRP_DistilRoberta_Base_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final3 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final3 = y_pred_final3 \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","d9dd8f41":"y_pred_final = (y_pred_final1 + y_pred_final2 + y_pred_final3) \/ 3.0\n\nsubmit_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\".\/submission.csv\", index=False)\nsubmit_df.head()","0fbfb543":"### Generate word tokens and attention masks","e32d49c1":"## Extract target label","030c8424":"### Generate word tokens and attention masks","577c9e8a":"## Roberta-Base Model","883961eb":"## Model Hyperparameters","c00926d5":"## Import libraries","7a39f4e1":"### Fit the model with K-Fold validation","a7f904e5":"### Initialize the Albert-V2 model","bdefaa76":"## Create submission file","a897caff":"## DistilRoberta-Base Model","4af88b15":"### Initialize the DistilBert-Base model","b7b56b86":"## Load source datasets","e8f33484":"### Fit the model with K-Fold validation","239357b9":"### Generate word tokens and attention masks","ac83816f":"## XLM-Roberta-Base Model","dfbcdc89":"### Fit the model with K-Fold validation","01ec5432":"### Initialize the Bert-Base model","adb29b92":"## Helper Functions"}}