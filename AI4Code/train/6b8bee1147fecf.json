{"cell_type":{"b9a25cfb":"code","3ea7ab6e":"code","05287fa9":"code","afd3381f":"code","b5c903c6":"code","94b70258":"code","131d977a":"code","98e92009":"code","c00b9f88":"code","69437837":"code","944eb985":"code","d050b1d6":"code","607187a2":"code","7a3e451e":"code","828d4705":"code","98dafccf":"code","86070ce9":"code","912a014e":"code","056dc8ae":"code","a4b69b73":"code","adfdeb02":"code","0b6d0cca":"code","c71f20f4":"code","189db0b7":"code","c4882078":"code","7d6f53c9":"code","e4ab4df2":"code","aedd746c":"code","b9fb99fd":"code","1ba9a9f4":"code","da76a1ec":"code","53110d74":"code","210f86f4":"code","a1786ff1":"code","80677f55":"code","e2a6f686":"code","11ada45c":"markdown","0c7430ac":"markdown","2f165b68":"markdown","72e559c3":"markdown","e482281c":"markdown","f47496b4":"markdown","3495b0b7":"markdown","591539b5":"markdown","da26601c":"markdown","be8ba87e":"markdown","7d5e47dc":"markdown","4ef70994":"markdown","d3460c5f":"markdown","bb5a5d54":"markdown"},"source":{"b9a25cfb":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3ea7ab6e":"data = pd.read_csv(\"\/kaggle\/input\/deep-learning-az-ann\/Churn_Modelling.csv\")","05287fa9":"data.head()","afd3381f":"data.shape","b5c903c6":"data.info()","94b70258":"data.iloc[:,0:13].head().describe().T","131d977a":"sns.countplot(x = \"Exited\", data = data)\nplt.show()","98e92009":"fig, ax = plt.subplots(figsize=(15,10))   \nsns.heatmap(data.corr(), annot = True , fmt=\".2f\",ax=ax ,cmap = \"Blues\")\nplt.show()","c00b9f88":"sns.pairplot(data, hue=\"Exited\")\nplt.show()","69437837":"plt.boxplot(data[\"Age\"])\nplt.show()","944eb985":"plt.boxplot(data[\"Balance\"])\nplt.show()","d050b1d6":"plt.boxplot(data[\"CreditScore\"])\nplt.show()","607187a2":"sns.countplot(x = \"NumOfProducts\", data = data)\nplt.show()","7a3e451e":"data = pd.get_dummies(data, columns= [\"Geography\"])\ndata = pd.get_dummies(data, columns= [\"Gender\"])\ndata = pd.get_dummies(data, columns= [\"NumOfProducts\"])\ndata.head()","828d4705":"print(\"Max age: \", data['Age'].max())\nprint(\"Min age: \", data['Age'].min())","98dafccf":"age_cat = [0 if i < 38 else 1 if i <42  else 2 if i < 45 else 3 if i < 60 else 4 for i in data[\"Age\"]]\ndata[\"Age_Cat\"] = age_cat","86070ce9":"sns.countplot(x = \"Age_Cat\", data = data)\nplt.show()","912a014e":"g = sns.factorplot(x = \"Age_Cat\", y = \"Exited\", data = data, kind = \"bar\")\ng.set_ylabels(\"Exited\")\nplt.show()","056dc8ae":"data[\"Age_Cat\"] = data[\"Age_Cat\"].astype(\"category\")\ndata = pd.get_dummies(data, columns=[\"Age_Cat\"])\ndata.head(10)","a4b69b73":"print(\"Max age: \", data['CreditScore'].max())\nprint(\"Max age: \", data['CreditScore'].min())\nprint(\"Mean age: \", data['CreditScore'].mean())","adfdeb02":"credit_cat = [0 if i < 450 else 1 if i <670  else 2 for i in data[\"CreditScore\"]]\ndata[\"CreditScore_Cat\"] = credit_cat","0b6d0cca":"sns.countplot(x = \"CreditScore_Cat\", data = data)\nplt.show()","c71f20f4":"g = sns.factorplot(x = \"CreditScore_Cat\", y = \"Exited\", data = data, kind = \"bar\")\ng.set_ylabels(\"Exited\")\nplt.show()","189db0b7":"data[\"CreditScore_Cat\"] = data[\"CreditScore_Cat\"].astype(\"category\")\ndata = pd.get_dummies(data, columns=[\"CreditScore_Cat\"])\ndata.head(10)","c4882078":"data.drop(labels = [\"RowNumber\", \"CustomerId\",\"Surname\",\"CreditScore\",\"Age\"], axis = 1, inplace = True) \ndata.head()","7d6f53c9":"data.drop(labels = [\"Tenure\", \"HasCrCard\",\"EstimatedSalary\"], axis = 1, inplace = True)\ndata.head()","e4ab4df2":"x_data = data.drop([\"Exited\"],axis=1)\ny_data = data[\"Exited\"].values","aedd746c":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data,y_data,test_size = 0.33,random_state=42)","b9fb99fd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns.values)\nx_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns.values)","1ba9a9f4":"print(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","da76a1ec":"from keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\n\nclassifier=Sequential()\nclassifier.add(Dense(10, activation = 'relu' , input_dim = x_train.shape[1])),\nclassifier.add(Dense(8 , activation = 'relu')),\nclassifier.add(Dense(1 , activation = 'sigmoid'))\n\n\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nclassifier.fit(x_train, y_train, batch_size = 25, epochs = 100,verbose = 1)","53110d74":"# Predicting on train data\ny_pred = classifier.predict(x_train)\nscore, acc = classifier.evaluate(x_train, y_train,batch_size=10)\nprint('Train score:', score)\nprint('Train accuracy:', acc*100)","210f86f4":"y_pred = (y_pred > 0.5)*1\n\nfrom sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_train,y_pred)\n\nsns.heatmap(cf_matrix, annot=True, fmt='.0f', cmap='Blues')\nplt.show()","a1786ff1":"# Predicting on test data\ny_pred = classifier.predict(x_test)\nscore, acc = classifier.evaluate(x_test, y_test,batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc*100)","80677f55":"y_pred = (y_pred > 0.5)*1\n\ncf_matrix = confusion_matrix(y_test,y_pred)\n\nsns.heatmap(cf_matrix, annot=True, fmt='.0f', cmap='Blues')\nplt.show()","e2a6f686":"sns.heatmap(cf_matrix\/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Blues')\nplt.show()","11ada45c":"* Drop rows that have correlation value = 0.01 with Exited.","0c7430ac":"<a id='9'><\/a><br>\n## Artificial Neural Network (ANN)","2f165b68":"<a id='6'><\/a><br>\n# Modeling","72e559c3":"* No nan value.","e482281c":"* Drop non-necessary row -> RowNumber, CustomerId\n* Drop object data type -> Surname\n* Drop no longer necessary row -> CreditScore, Age","f47496b4":"<a id='7'><\/a><br>\n## Train - Test Split","3495b0b7":"* get_dummies() is used for data manipulation. It converts categorical data into dummy or indicator variables.","591539b5":"<a id='5'><\/a><br>\n### CreditScore-> Categorical CreditScore","da26601c":"<a id='1'><\/a><br>\n# Load and Check Data","be8ba87e":"<a id='8'><\/a><br>\n## Standardization","7d5e47dc":"# \u2728 Introduction\n\n![image.png](attachment:c5ced484-9f7e-4d10-8753-bd40c9252b90.png)\n\n\ud83c\udfe6 **Dataset:** A bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon.\n\n**Columns:**\n* RowNumber\n* CustomerId\n* Surname\n* CreditScore\n* Geography\n* Gender\n* Age\n* Tenure\n* Balance\n* NumOfProducts\n* HasCrCard\n* IsActiveMember\n* EstimatedSalary\n* Exited\n\n\n\ud83d\udccc **In this notebook, we will predict customer churn using Artificial Neural Network (ANN).**\n\n<font color= 'blue'>\nContent:\n\n1. [Load and Check Data](#1)\n1. [Data Visualization](#2)\n1. [Feature Engineering](#3)\n    * [Age -> Categorical Age](#4)\n    * [CreditScore -> Categorical CreditScore](#5)\n1. [Modeling](#6)\n    * [Train - Test Split](#7)\n    * [Standardization](#8)\n    * [Artificial Neural Network (ANN)](#9)","4ef70994":"<a id='4'><\/a><br>\n### Age -> Categorical Age","d3460c5f":"<a id='2'><\/a><br>\n# Data Visualization","bb5a5d54":"<a id='3'><\/a><br>\n# Feature Engineering"}}