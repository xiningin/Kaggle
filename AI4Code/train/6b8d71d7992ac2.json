{"cell_type":{"969fa5b1":"code","2091f838":"code","46e39295":"code","b554dbd6":"code","5f7ed12c":"code","1e35a0d5":"code","e117cee5":"code","81c2b641":"code","cc072984":"code","700b7727":"markdown","d8f1927e":"markdown","355fa0a6":"markdown"},"source":{"969fa5b1":"from sklearn import *\nimport numpy as np\nimport pandas as pd\nimport glob\n\ndata = {k.split('\/')[-1][:-4]:k for k in glob.glob('\/kaggle\/input\/**\/**.csv')}\ntrain = pd.read_csv(data['jigsaw-toxic-comment-train'], usecols=['id', 'comment_text', 'toxic'])\nval = pd.read_csv(data['validation'], usecols=['comment_text', 'toxic'])\ntest = pd.read_csv(data['test'], usecols=['id', 'content'])\ntest.columns = ['id', 'comment_text']\ntest['toxic'] = 0.5\n\n##Our team second best submission (non normalized submission)\nsub2 = pd.read_csv('..\/input\/finalsubmission\/submission-.9480.csv')\n\n#Our team best submission : Taking ensemble of .9479 Kernel (Ashish) and .9480 (best stable submission Normalized)\nsub4 = pd.read_csv('..\/input\/finalsubmission\/submission-.9481.csv')","2091f838":"sub2.head(5)","46e39295":"%%time\ndef f_experience(c, s):\n    it = {'memory':10,\n        'influence':0.5,\n        'inference':0.5,\n        'interest':0.9,\n        'sentiment':1e-10,\n        'harmony':0.5}\n    \n    exp = {}\n    \n    for i in range(len(c)):\n        words = set([w for w in str(c[i]).lower().split(' ')])\n        for w in words:\n            try:\n                exp[w]['influence'] = exp[w]['influence'][1:] + [s[i]] #need to normalize\n                exp[w]['inference'] += 1\n                exp[w]['interest'] = exp[w]['interest'][1:] + [(exp[w]['interest'][it['memory']-1] + (s[i] * it['interest']))\/2]\n                exp[w]['sentiment'] += s[i]\n                #exp[w]['harmony']\n            except:\n                m = [0. for m_ in range(it['memory'])]\n                exp[w] = {}\n                exp[w]['influence'] = m[1:] + [s[i]]\n                exp[w]['inference'] = 1\n                exp[w]['interest'] = m[1:] + [s[i] * it['interest'] \/ 2]\n                exp[w]['sentiment'] = s[i]\n                #exp[w]['harmony'] = 0\n                \n    for w in exp:\n        exp[w]['sentiment'] \/= exp[w]['inference'] + it['sentiment']\n        exp[w]['inference'] \/= len(c) * it['inference']\n\n    return exp\n\nexp = f_experience(train['comment_text'].values, train['toxic'].values)","b554dbd6":"%%time\ndef features(df):\n    df['len'] = df['comment_text'].map(len)\n    df['wlen'] = df['comment_text'].map(lambda x: len(str(x).split(' ')))\n    \n    df['influence_sum'] = df['comment_text'].map(lambda x: np.sum([np.mean(exp[w]['influence']) if w in exp else 0 for w in str(x).lower().split(' ')]))\n    df['influence_mean'] = df['comment_text'].map(lambda x: np.mean([np.mean(exp[w]['influence']) if w in exp else 0 for w in str(x).lower().split(' ')]))\n    \n    df['inference_sum'] = df['comment_text'].map(lambda x: np.sum([exp[w]['inference'] if w in exp else 0 for w in str(x).lower().split(' ')]))\n    df['inference_mean'] = df['comment_text'].map(lambda x: np.mean([exp[w]['inference'] if w in exp else 0 for w in str(x).lower().split(' ')]))\n    \n    df['interest_sum'] = df['comment_text'].map(lambda x: np.sum([np.mean(exp[w]['interest']) if w in exp else 0 for w in str(x).lower().split(' ')]))\n    df['interest_mean'] = df['comment_text'].map(lambda x: np.mean([np.mean(exp[w]['interest']) if w in exp else 0 for w in str(x).lower().split(' ')]))\n    \n    df['sentiment_sum'] = df['comment_text'].map(lambda x: np.sum([exp[w]['sentiment'] if w in exp else 0.5 for w in str(x).lower().split(' ')]))\n    df['sentiment_mean'] = df['comment_text'].map(lambda x: np.mean([exp[w]['sentiment'] if w in exp else 0.5 for w in str(x).lower().split(' ')]))\n    return df\n\nval = features(val)\ntest= features(test)","5f7ed12c":"col = [c for c in val if c not in ['id', 'comment_text', 'toxic']]\nx1, x2, y1, y2 = model_selection.train_test_split(val[col], val['toxic'], test_size=0.3, random_state=20)\n\nmodel = ensemble.ExtraTreesClassifier(n_estimators=1000, max_depth=7, n_jobs=-1, random_state=20)\nmodel.fit(x1, y1)\nprint(metrics.roc_auc_score(y2, model.predict_proba(x2)[:,1].clip(0.,1.)))\n\nmodel.fit(val[col], val['toxic'])\ntest['toxic'] = model.predict_proba(test[col])[:,1].clip(0.,1.)\nsub1 = test[['id', 'toxic']]","1e35a0d5":"sub1.rename(columns={'toxic':'toxic1'}, inplace=True)\nsub2.rename(columns={'toxic':'toxic2'}, inplace=True)\nsub4.rename(columns={'toxic':'toxic4'}, inplace=True)\n\nsub3 = sub1.merge(sub2,on='id').merge(sub4,on='id')","e117cee5":"sub3.head(5)","81c2b641":"sub3['toxic'] = (sub3['toxic1'] * 0.1) + (sub3['toxic4'] * 0.9) #blend 1\nsub3['toxic'] = (sub3['toxic2'] * 0.49) + (sub3['toxic'] * 0.51) #blend 2\n\nsub3[['id', 'toxic']].to_csv('submission.csv', index=False)","cc072984":"#Is it toxic :)\ntest = pd.DataFrame(['Howling with Wolf on L\u00fcgenpresse'], columns=['comment_text'])\ntest['id'] = test.index\ntest= features(test)\ntest['toxic'] = model.predict_proba(test[col])[:,1].clip(0.,1.)\ntest[['id', 'comment_text', 'toxic']].head()","700b7727":"I would like to thank my team members, for their awesome contribution in the competition.\n\n* Ashish Gupta (https:\/\/www.kaggle.com\/roydatascience)\n* Mukharbek Organokov (https:\/\/www.kaggle.com\/muhakabartay)\n* Firat Gonen (https:\/\/www.kaggle.com\/frtgnn)\n* Atharva (https:\/\/www.kaggle.com\/atharvap329)\n* Kirill Balakhonov (https:\/\/www.kaggle.com\/kirill702b)","d8f1927e":"Please note: Here I am testing jazivxt kernel (https:\/\/www.kaggle.com\/jazivxt\/howling-with-wolf-on-l-genpresse) on my best submissions.","355fa0a6":"Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results \"translate\" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages."}}