{"cell_type":{"b1b6f928":"code","7f1bcf66":"code","43711e84":"code","3c6ba72e":"code","3111358d":"code","36d0e6c9":"code","268f7bdd":"code","d94c3275":"code","5ce45ab2":"code","eb68b9a3":"code","94f1cd11":"code","c8dedc9a":"code","bcd5b1da":"code","78226d67":"code","6ed7f0f2":"code","2efd3bbc":"code","d315da6a":"code","49b8dea1":"code","54b967cc":"code","f1957e29":"code","5f526999":"code","66698de9":"code","d7575b40":"code","f54683bb":"code","c4690eca":"code","136d988c":"code","07c32baa":"code","4c15f382":"code","3c797fbe":"code","bb094bf9":"markdown","bbc6baf5":"markdown","d4008d4b":"markdown","b0ec42a7":"markdown","de107a32":"markdown","7dc1f35c":"markdown","e5dd7de7":"markdown","717f1fe2":"markdown","9ee62e4a":"markdown","cd56d609":"markdown","c0c76704":"markdown"},"source":{"b1b6f928":"#our basic libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n# basic libraries related to torch\nimport torch\nimport torch.nn as nn\nimport torchvision\n#libraries for actions on dataset\nfrom torch.utils.data import Dataset, DataLoader \nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2 # for data aurmentaion\n#optimizer funtion\nimport torch.optim as optim\n\n# tqdm is for progress bar\nfrom tqdm import tqdm  \n\nfrom torchvision.utils import save_image\nfrom PIL import Image","7f1bcf66":"device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","43711e84":"main = \"..\/input\/gan-getting-started\"\ndirs = os.listdir(main) \n#print(type(dirs))\nfor dir_1 in dirs:\n    if(\"monet\" in dir_1 and \"jpg\" in dir_1):\n        monets_dir = main + \"\/\" + dir_1\n    if(\"photo\" in dir_1 and \"jpg\" in dir_1):\n        photos_dir = main + \"\/\" + dir_1\nprint(monets_dir, photos_dir)","3c6ba72e":"# I havbe written a generic function here ,\nclass mydata(Dataset):\n    def __init__(self, img_dir_1,img_dir_2, transforms=None):\n        #as we want to use the arguments received in other functions of this class using self.\n        self.img_dir_1 = img_dir_1\n        self.img_dir_2 = img_dir_2\n        self.transforms = transforms\n        #use self if you want to use it across all the function in thsi class\n        self.images_1_names = os.listdir(img_dir_1)\n        self.images_2_names = os.listdir(img_dir_2)\n        self.len_dataset = max(len(self.images_1_names), len(self.images_2_names))\n        self.len_images_1_names = len(self.images_1_names)\n        self.len_images_2_names = len(self.images_2_names)\n        \n    def __len__(self):#whenever we call len() on any object of this class it will call this functin.\n        return self.len_dataset\n    \n    def __getitem__(self, idx): #whenever we call this a[] using object of this class it will invoke this function.\n        #no need to use self. if you done need to use that variable in any other class\n        image_1_name = self.images_1_names[idx % self.len_images_1_names]\n        image_2_name = self.images_2_names[idx % self.len_images_2_names]\n        \n        image_1_path = os.path.join(self.img_dir_1, image_1_name )\n        image_2_path = os.path.join(self.img_dir_2, image_2_name )\n        \n        images_1 = np.array(Image.open(image_1_path).convert('RGB'))#making it np.array so that just to plot images as it\n        images_2 = np.array(Image.open(image_2_path).convert('RGB'))# we dont have to use transform To Tensor\n        if self.transforms:\n            augmentations = self.transforms(image=images_1, image0=images_2)\n            images_1 = augmentations[\"image\"]\n            images_2 = augmentations[\"image0\"]\n\n        return (images_1, images_2)\n# As we can see image_1 corresponds to the images from img_dir_1","3111358d":"data = mydata(photos_dir, monets_dir)","36d0e6c9":"def plot_some(data):\n    figure = plt.figure(figsize = (18,18))\n    rows, cols = 8, 8\n    for i in range(1, rows*cols + 1):\n        index = torch.randint(low = 0, high = len(data), size = (1,)).item()\n        _,img = data[index] #getting images of zebras \n        if(i>=33):\n            img,_ = data[index]  # getting images of zebras\n        figure.add_subplot(rows, cols, i)\n        plt.imshow(img.squeeze())\n        plt.axis('off')\n    plt.title(\"Monets and Photos\")\n    plt.show()","268f7bdd":"\nplot_some(data)\n#note that these have not been applied with transform yet.","d94c3275":"batch_size = 1\nloader = DataLoader(data, batch_size = batch_size, shuffle = False, num_workers = 4, pin_memory = True)","5ce45ab2":"phot,mon = next(iter(loader))","eb68b9a3":"plt.imshow(phot.squeeze())\nplt.axis(\"off\")\nplt.title(\"Photo\")","94f1cd11":"plt.imshow(mon.squeeze())\nplt.axis(\"off\")\nplt.title(\"Monet\")","c8dedc9a":"class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias= True, padding_mode='reflect'),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.2),\n        )\n    def forward(self, x):\n        return self.conv(x)","bcd5b1da":"class Discriminator(nn.Module):\n    def __init__(self,in_channels = 3, features = [64,128,256,512]): #we will be using conv blocks for all of this\n        super().__init__()\n        self.initial = nn.Sequential(\n            nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding = 1, padding_mode='reflect'),\n            nn.LeakyReLU()\n        )\n        layers = []\n        for i in range(1, len(features)):\n            layers.append(Block(features[i-1], features[i], stride =1 if i == len(features)-1 else 2))\n        layers.append(nn.Conv2d(features[len(features)-1],1,  kernel_size = 4 , stride = 1, padding =1 , padding_mode='reflect' ))\n        self.model = nn.Sequential(*layers)\n        \n    def forward(self,x):\n        x = self.initial(x)\n        x = self.model(x)\n        sig = nn.Sigmoid()\n        return sig(x)","78226d67":"class ConvBlock(nn.Module):\n    def __init__(self,in_channels, out_channels, down =  True, use_act =  True , **kwargs):#some other keyword arguments like stride, padding etc.\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, padding_mode = \"reflect\", **kwargs)\n            if down\n            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(inplace = True) if use_act else nn.Identity()\n        )\n    def forward(self, x):\n        return self.conv(x)","6ed7f0f2":"class ResBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            ConvBlock(channels, channels, kernel_size = 3, padding = 1),\n            ConvBlock(channels, channels, use_act = False, kernel_size = 3, padding =1 ),\n        )\n    def forward(self, x):\n        return x + self.block(x) # we are not changin in output respected to x","2efd3bbc":"class Generator(nn.Module):\n    def __init__(self, img_channels,num_feature = 64,  num_residuals = 9):\n        super().__init__()\n        self.initial = nn.Sequential(\n            nn.Conv2d(img_channels, num_feature, kernel_size  = 7, stride = 1, padding = 3 , padding_mode = \"reflect\"),\n            nn.ReLU(inplace =True)\n        )\n        # down sampling\n        self.down_blocks = nn.ModuleList(\n            [\n                ConvBlock(num_feature, num_feature*2, kernel_size = 3, stride = 2, padding =1),\n                ConvBlock(num_feature*2, num_feature*4, kernel_size = 3, stride = 2 , padding = 1),   \n            ]\n        )\n        # does not much change the input       \n        self.resblocks = nn.Sequential(\n            *[ResBlock(num_feature*4) for _ in range(num_residuals)]\n        )\n        # up sampling\n        self.up_blocks = nn.ModuleList(\n        [\n           ConvBlock(num_feature*4, num_feature*2,down=False,  kernel_size = 3, stride = 2, padding =1, output_padding = 1),\n           ConvBlock(num_feature*2, num_feature, down=False, kernel_size = 3, stride = 2 , padding = 1, output_padding = 1),\n        ])\n        # converting it ot RGB\n        self.last = nn.Conv2d(num_feature*1, img_channels,kernel_size = 7, stride = 1, padding  =  3, padding_mode = \"reflect\")\n        \n    def forward(self, x):\n        x = self.initial(x)\n        for layer in self.down_blocks:\n            x = layer(x)\n        x = self.resblocks(x)\n        for layer in self.up_blocks:\n            x = layer(x)\n        return torch.tanh(self.last(x))","d315da6a":"import torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nbatch_size = 1\nlearning_rate = 1e-5\nlambda_identity = 0.5\nlambda_cycle = 10\nnum_workers = 4\nnum_epochs = 2\nload_model = False\nsave_model = True\ncheckpoint_g_photo = \"..\/input\/weights1\/g_photo.pth.tar\"\ncheckpoint_g_monet = \"..\/input\/weights1\/g_monet.pth.tar\"\ncheckpoint_d_photo = \"..\/input\/weights1\/d_photo.pth.tar\"\ncheckpoint_d_monet = \"..\/input\/weights1\/d_monet.pth.tar\"\n\ntransformer = A.Compose(\n    [\n        A.Resize(width=256, height=256),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n        ToTensorV2(),\n     ],\n    additional_targets={\"image0\": \"image\"},\n)","49b8dea1":"import copy\n\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=device)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n\n\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","54b967cc":"! mkdir new_photos\n! mkdir new_monets","f1957e29":"#photos to monet\ndef train(d_photo,d_monet, g_photo, g_monet, d_optim, g_optim, l1, mse, loader, g_scaler, d_scaler, i):\n    loop = tqdm(loader, leave = True)\n    \n    for idx, (photo, monet) in enumerate(loop):\n        #to use gpu or  cpu data should be on same device\n        monet = monet.to(device)\n        photo = photo.to(device)\n        \n        with torch.cuda.amp.autocast():\n            \n            #training d_photo to claddify a real ans fake photo(transfomrd to photo from monet)\n            real_photo = photo\n            fake_photo = g_photo(monet)\n    \n            d_photo_real = d_photo(real_photo)\n            d_photo_fake = d_photo(fake_photo.detach())\n            \n            \n            d_photo_real_loss = mse(d_photo_real, torch.ones_like(d_photo_real))\n            d_photo_fake_loss = mse(d_photo_fake, torch.zeros_like(d_photo_fake))\n            \n            d_photo_loss = d_photo_real_loss + d_photo_real_loss\n            #training d_monet to classify a real and fake monet\n            real_monet = monet\n            fake_monet = g_monet(photo)\n            \n            d_monet_real  = d_monet(real_monet)\n            d_monet_fake = d_monet(fake_monet.detach())\n            \n            d_monet_real_loss = mse(d_monet_real, torch.ones_like(d_monet_real))\n            d_monet_fake_loss = mse(d_monet_fake, torch.zeros_like(d_monet_fake))\n            \n            d_monet_loss = d_monet_fake_loss + d_monet_real_loss\n            \n            #puttinh them together\n            d_loss = (d_photo_loss + d_monet_loss)\/2\n        \n        d_optim.zero_grad()\n        d_scaler.scale(d_loss).backward()\n        d_scaler.step(d_optim)\n        d_scaler.update()\n        \n        #training the generator\n        with torch.cuda.amp.autocast():\n            #adversarial loss\n            d_photo_fake = d_photo(fake_photo)\n            d_monet_fake = d_monet(fake_monet)\n            g_photo_loss = mse(d_photo_fake, torch.ones_like(d_photo_fake))\n            g_monet_loss = mse(d_monet_fake, torch.ones_like(d_monet_fake))\n            \n            #cycle loss\n            cycle_monet = g_monet(fake_photo)\n            cycle_photo = g_photo(fake_monet)\n            \n            cycle_monet_loss = l1(real_monet, cycle_monet)\n            cycle_photo_loss = l1(real_photo, cycle_photo)\n            \n            #identity loss\n            identity_monet = g_monet(real_monet)\n            identity_photo =g_photo(real_photo)\n            identity_monet_loss = l1(real_monet, identity_monet)\n            identity_photo_loss = l1(real_photo, identity_photo)\n            \n            #let put all loss togther\n            g_loss = (\n                g_monet_loss + g_photo_loss +\n                cycle_monet_loss*lambda_cycle + cycle_photo_loss*lambda_cycle +\n                identity_monet_loss*lambda_identity + identity_photo_loss*lambda_identity \n                \n            )\n            \n        g_optim.zero_grad()\n        g_scaler.scale(g_loss).backward()\n        g_scaler.step(g_optim)\n        g_scaler.update()\n        \n        if idx%100 == 0 :\n            save_image(fake_photo*0.5 + 0.5, f\".\/new_photos\/photos_{i}_{idx}.jpg\" )\n            save_image(fake_monet*0.5 + 0.5, f\".\/new_monets\/monets_{i}_{idx}.jpg\" )","5f526999":"def main():\n    #defining d and g s\n    d_photo = Discriminator(in_channels = 3).to(device)\n    d_monet = Discriminator(in_channels = 3).to(device)\n    g_photo = Generator(img_channels = 3).to(device)\n    g_monet = Generator(img_channels = 3).to(device)\n    #defining optimizers\n    d_optim = optim.Adam( list(d_photo.parameters()) + list(d_monet.parameters()), lr = learning_rate, betas = (0.5, 0.999),  )       \n    g_optim = optim.Adam( list(g_photo.parameters()) + list(g_monet.parameters()), lr = learning_rate, betas = (0.5, 0.999),  )\n    #defining loss\n    l1 = nn.L1Loss()\n    mse = nn.MSELoss()\n    \n    data_n = mydata(photos_dir, monets_dir, transforms = transformer)\n    loader = DataLoader(data_n, batch_size = batch_size, shuffle = False, num_workers = 4, pin_memory = True)\n    \n    g_scaler = torch.cuda.amp.GradScaler()  # to run the program in float 16\n    d_scaler = torch.cuda.amp.GradScaler()\n    \n    if load_model:\n        load_checkpoint(checkpoint_g_photo, g_photo, g_optim, learning_rate,)\n        load_checkpoint(checkpoint_g_monet, g_monet, g_optim, learning_rate,)\n        load_checkpoint(checkpoint_d_photo, d_photo, d_optim, learning_rate,)\n        load_checkpoint(checkpoint_d_monet, d_monet, d_optim, learning_rate,)\n\n    for epoch in range(num_epochs):\n        train(d_photo, d_monet, g_photo, g_monet, d_optim, g_optim, l1, mse , loader, g_scaler, d_scaler, epoch)\n        \n        if save_model:\n            save_checkpoint(g_photo, g_optim, filename=checkpoint_g_photo)\n            save_checkpoint(g_monet, g_optim, filename=checkpoint_g_monet)\n            save_checkpoint(d_photo, d_optim, filename=checkpoint_d_photo)\n            save_checkpoint(d_monet, d_optim, filename=checkpoint_d_monet)\n","66698de9":"# if __name__ == \"__main__\":\n#     main()","d7575b40":"g_photo = Generator(img_channels = 3).to(device)\ng_monet = Generator(img_channels = 3).to(device)\noptimizer = g_optim = optim.Adam( list(g_photo.parameters()) + list(g_monet.parameters()), lr = learning_rate, betas = (0.5, 0.999),  )\n# checkpoint_g_monet = \"..\/input\/weights-painter\/g_monet.pth.tar\"\ncheckpoint = torch.load(checkpoint_g_monet)\ng_monet.load_state_dict(checkpoint['state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer'])\n# epoch = checkpoint['epoch']\n# loss = checkpoint['loss']","f54683bb":"print(g_monet)","c4690eca":"data_n = mydata(photos_dir, monets_dir, transforms = transformer)","136d988c":"! mkdir ..\/images","07c32baa":"cd ..\/images\n","4c15f382":"for i in range(len(data)):\n    photo_initial,_ =  data_n[i]\n    photo_initial = photo_initial.to(device)\n    photo_initial = photo_initial.unsqueeze(0)\n    monet_got = g_monet(photo_initial)\n    save_image(monet_got*0.5 + 0.5, f\"..\/images\/monets_{i}.jpg\" )","3c797fbe":"import shutil\nshutil.make_archive(\"\/kaggle\/working\/images\", 'zip', \"\/kaggle\/images\")","bb094bf9":"## Dataset","bbc6baf5":"Lets check if we ahve GPU access and assign it a variable.\nIf we have access to GPU we will have \"cuda\" assigned to variable or it will be \"cpu\" other wise","d4008d4b":"# Training Function","b0ec42a7":"# Discriminator\n\nNow discriminators has some layers repeating in same manner so I will be creating a another which will contain those layers and then we can use this class in the main generator function as many time wewant","de107a32":"As you can see we are using PyTorch so lets create a dataset which can return a pair of images.\nAs we want to create a model which can be trained to convert an **Photo to Monet** style.\n(Note: Thanks to Cycle GAN we need not worry about correspondece of these two images as PixtoPix needs images which are tightly corelated to each other from different domain, but that is not the case with Cycle GAN we just want images from two different domains.)","7dc1f35c":"## Painter - Cycle GAN","e5dd7de7":"We want convert photos to monet style, now we have given data names as photo and monet.\nData is available in tfrec format also but as our main aim is to learn Cycle GAN here Lets use use jpg format directly for simplicity.","717f1fe2":"Here I am just messing around, you can directly copy paste the dir from the kaggle input","9ee62e4a":"## DataLoader\n\nDataloader is the one who provides the data to our model in a particular manner.","cd56d609":"Lets create a class here named as mydata, which can inherit fucntions from Dataset.\nSome observations about the dataset.\n1. It is already given that images are 256*256 so we dont need to do any resizing and reshaping.\n2. As we already have a lot of data lets refrain from using any other augmentations\/transformations.\n3. We will just do one transform of making all the value to -1 to 1.","c0c76704":"# Generator\nComing to the most important part of the model , the generator."}}