{"cell_type":{"5e55317d":"code","c748412d":"code","00d2fc8a":"code","d6d82168":"code","ce69f904":"code","0df7cfc1":"code","f93ad9f5":"code","56afa146":"code","ac3ac9e6":"code","ef62aade":"code","50728b66":"code","0f561e2e":"code","9daea77c":"code","6a7976e3":"code","4bbf02b1":"code","29f1b49e":"code","8d5a6dc2":"markdown","b1ae696c":"markdown","ab4ed713":"markdown","2f319b8f":"markdown","9ed06077":"markdown","e0e5b04e":"markdown","96d8938c":"markdown","b52615cc":"markdown","ed18f5e7":"markdown","9b6f8e0f":"markdown","061642d2":"markdown","1441a79a":"markdown","33bdc10f":"markdown","42a14e94":"markdown","7115a46d":"markdown","e1bf6df5":"markdown","dce33201":"markdown","8f54a9fa":"markdown","867a464e":"markdown","98c8d2f3":"markdown","9102e311":"markdown","49ee1760":"markdown","faca43c5":"markdown","da264985":"markdown","ddc26cd0":"markdown","13859b1e":"markdown","d77748a0":"markdown","71975e38":"markdown"},"source":{"5e55317d":"import os\nimport cv2\nimport sys\nimport matplotlib.pyplot as plt \nimport numpy as np\n\nTRAIN_PATH='..\/input\/training-data-and-masks\/train\/'\ntrain_ids = next(os.walk(TRAIN_PATH))[1]\nfor n, id_ in enumerate(train_ids):\n    path = TRAIN_PATH + id_\n    img = plt.imread(path + '\/images\/' + id_ )[:,:,:3]\n    img1 = plt.imread(path + '\/masks\/' + id_ )[:,:,:3]\n    f = plt.figure()\n    f.add_subplot(1,2, 1)\n    plt.imshow(np.rot90(img,2))\n    f.add_subplot(1,2, 2)\n    plt.imshow(np.rot90(img1,2))\n    plt.show(block=True)\n","c748412d":"f, axarr","00d2fc8a":"from tqdm import tqdm\nfrom skimage.transform import resize\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\nprint('Getting and resizing training images ... ')\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n        \n# Re-sizing our training images to 128 x 128\n# Note sys.stdout prints info that can be cleared unlike print.\n# Using TQDM allows us to create progress bars\nsys.stdout.flush()\n\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = plt.imread(path + '\/images\/' + id_ )[:,:,:IMG_CHANNELS]\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img\n    mask = plt.imread(path + '\/masks\/' + id_)[:,:,:1]\n    mask=resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    Y_train[n] = mask\n\nprint('Done')","d6d82168":"plt.figure(figsize=(20,16))\nx, y = 12,4\nfor i in range(y):  \n    for j in range(x):\n        plt.subplot(y*2, x, i*2*x+j+1)\n        pos = i*5 + j*8\n        plt.imshow(X_train[pos])\n        plt.title('Image #{}'.format(pos))\n        plt.axis('off')\n        plt.subplot(y*2, x, (i*2+1)*x+j+1)\n        \n        #We display the associated mask we just generated above with the training image\n        plt.imshow(np.squeeze(Y_train[pos]))\n        plt.title('Mask #{}'.format(pos))\n        \n        plt.axis('off')\n        \nplt.show()","ce69f904":"import sys\nimport random\nimport warnings #\nimport pandas as pd\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.morphology import label\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dropout, Lambda\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf","0df7cfc1":"# Build U-Net model\n# Note we make our layers varaibles so that we can concatenate or stack\n# This is required so that we can re-create our U-Net Model\n\ninputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = Lambda(lambda x: x \/ 255) (inputs)\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\n# Note our output is effectively a mask of 128 x 128 \noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","f93ad9f5":"!pip install tensorflow-gpu==1.14.0","56afa146":"import tensorflow as tf\nprint(tf.__version__)\ntf.test.is_gpu_available() ","ac3ac9e6":"tf.compat.v1.disable_eager_execution()\nmodel_path = \"nuclei_finder_unet_1.h5\"\n# Initialize our callbacks\n\ncheckpoint = ModelCheckpoint(model_path,\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             verbose=1)\n\nearlystop = EarlyStopping(monitor = 'val_loss', \n                          min_delta = 0, \n                          patience = 5,\n                          verbose = 1,\n                          restore_best_weights = True)\n\n# Fit our model \nresults = model.fit(X_train, Y_train, validation_split=0.1,\n                    batch_size=16, epochs=10,callbacks=[earlystop, checkpoint])\n\n","ef62aade":"    plt.plot(results.history['accuracy'])\n    plt.plot(results.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    \n    # \"Loss\"\n    plt.plot(results.history['loss'])\n    plt.plot(results.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()","50728b66":"model = load_model('nuclei_finder_unet_1.h5', \n                   custom_objects={'my_iou_metric': my_iou_metric})\n\n# the first 90% was used for training\npreds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n\n\n#preds_test = model.predict(X_test, verbose=1)\n\n# Threshold predictions\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\n","0f561e2e":"# Ploting our predicted masks\nix = random.randint(0, 120)\nplt.figure(figsize=(20,20))\n\n# Our original training image\nplt.subplot(131)\nimshow(X_train[ix])\nplt.title(\"Image\")\n\n# Our original combined mask  \nplt.subplot(132)\nimshow(np.squeeze(Y_train[ix]))\nplt.title(\"Mask\")\n\n# The mask our U-Net model predicts\nplt.subplot(133)\nimshow(np.squeeze(preds_train_t[ix] > 0.5))\nplt.title(\"Predictions\")\nplt.show()","9daea77c":"import os\nimport cv2\nimport numpy as np\ndata=[]\nfor dirname, _, filenames in os.walk('..\/input\/testdata\/test'):\n    for filename in filenames:\n        img=cv2.imread(os.path.join(dirname, filename))\n        img=cv2.resize(img, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n        if img is not None:\n                data.append(img)\n        \ndata1=np.array(data)\n","6a7976e3":"preds_test = model.predict(data1, verbose=1)\n\n\n#preds_test = model.predict(X_test, verbose=1)\n\n# Threshold predictions\npreds_test_t = (preds_train > 0.5).astype(np.uint8)","4bbf02b1":"preds_test = model.predict(data1, verbose=1)\n\npreds_test_t = (preds_test > 0.5).astype(np.uint8)","29f1b49e":"for n in range(len(data)):\n    f = plt.figure()\n    f.add_subplot(1,2, 1)\n    plt.imshow(np.rot90(data[n],2))\n    f.add_subplot(1,2, 2)\n    plt.imshow(np.rot90(np.squeeze(preds_test_t[n]),2))\n    plt.show(block=True)","8d5a6dc2":"Evaluating test data","b1ae696c":"**What is encoder and decoder?**\n\nThe encoder is the first half in the architecture diagram (Figure below). It usually is a pre-trained classification network like VGG\/ResNet where you apply convolution blocks followed by a maxpool downsampling to encode the input image into feature representations at multiple different levels.\n\nThe decoder is the second half of the architecture. The goal is to semantically project the discriminative features (lower resolution) learnt by the encoder onto the pixel space (higher resolution) to get a dense classification. The decoder consists of upsampling and concatenation followed by regular convolution operations.","ab4ed713":"Let's look at images and masks after preparation","2f319b8f":"[![enter image description here][1]][1]\n\n\n  [1]: https:\/\/i.stack.imgur.com\/hMQjJ.png","9ed06077":"# Step 1: Making Masks","e0e5b04e":"**What is mask? **\n\nImage segmentation creates a pixel-wise mask for each object in the image. This technique gives us a far more granular understanding of the object(s) in the image.\n\n[![enter image description here][1]][1]\n\n  [1]: https:\/\/i.stack.imgur.com\/sXrwS.jpg","96d8938c":"**How to create masks for our train data set?**\n\nThere are many ways to make masks for a data set. You can find many Github sources. In this project I used labelbox. Labelbox is a very powerfull software for making masks. I uploudes masks for all train data. You can find them in the train folder. (Note that I don't have medical background so masks migth not be quite exact). \n","b52615cc":"# Conclusion:\n    \nIn this project I used U-net CNN for Image segmentation of brain tumor. The result is good on the test data but it can be better by improving the model. For example creating accuracy function. ","ed18f5e7":"# Trading the model","9b6f8e0f":"In the previous kernel I made a CNN model to classify if a brain has tumor or not. The accuracy of the model was really good. But in this kernel I am going to use Image segmentation by U-net CNN model to determain the brain's tumor in yes cases. ","061642d2":"# Step 2: Preparing Data ","1441a79a":"Showing the evalation result","33bdc10f":"What is encoder and decoder?","42a14e94":"First let's look at the traing data evaluation","7115a46d":"**What is u-net CNN model?**\n\nU-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 \u00d7 512 image takes less than a second on a modern GPU.","e1bf6df5":"Now let's evalute the model on the test data. First we read test images from testData folder.","dce33201":"Showing the evalution of the data on the training set","8f54a9fa":"**Why we call it U-net network? **\n\nThe reason behind why it is named U-Net is because of the shape of its architecture, which is the letter \u201cU\u201d. The architecture contains two paths. The left part of the letter \u201cU\u201d is called an encoder and the right part is called a decoder.","867a464e":"In this project tumors in yes images are masks. ","98c8d2f3":"prediction of the test data: ","9102e311":"# Evaluation of the Model","49ee1760":"**Building U-net CNN**\n","faca43c5":"# Step 3: Building U-net CNN Model","da264985":"Lets look at some masks and images:","ddc26cd0":"In the rigth photo, each item bounded in red,blue and green curves is called a mask. Image segmentation creates a pixel-wise mask for each object in the image.","13859b1e":"*** My analysis take the following steps:**\n\n1- Making Mask\n\n2- Preparing data\n\n3- Building U-net CNN model\n\n\n4- Training the model\n\n5-Evalutation Model on some training data\n\n\n6-Evaluation model on the test data\n\n","d77748a0":"# Evaluation of the model on the test data","71975e38":"#  Image segmentation for detecting brain tumor"}}