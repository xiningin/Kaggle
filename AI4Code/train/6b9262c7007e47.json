{"cell_type":{"074028f6":"code","cbd61069":"code","e4ccccbb":"code","76f0b145":"code","389fb9cd":"code","0a0023b0":"code","023e977f":"code","1800c4b5":"code","5bef1d0f":"code","f8f66acf":"code","df33d14c":"code","a7a09692":"code","82937687":"code","932c5d1e":"code","6aa3d804":"code","495443fa":"code","4b9d7eff":"code","cf39f611":"code","6ff8d0d3":"code","6e67be6d":"code","3c3348e9":"code","3c6618a2":"code","f42ca042":"code","b5724987":"code","0a706745":"code","e90a7d00":"code","61aad853":"code","d1b0edca":"code","8ba48625":"code","e224cef0":"code","bac16f01":"code","1659c6d7":"code","cf6f28c1":"code","3b60f756":"code","e0afaaf7":"code","a7987500":"code","a7aa4e98":"code","2ea56cc1":"markdown","fbfc8ed8":"markdown","c966df87":"markdown"},"source":{"074028f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cbd61069":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.data import iris_data\nfrom mlxtend.plotting import plot_decision_regions","e4ccccbb":"clf = LogisticRegression(random_state=0)\n","76f0b145":"# Loading some example data\nX, y = iris_data()\nX = X[:,[0, 2]]","389fb9cd":"gs = gridspec.GridSpec(5, 5)\nfig = plt.figure(figsize=(10, 8))","0a0023b0":"labels = ['Logistic Regression']","023e977f":"for clf, lab, grd in zip([clf],\n                         labels,\n                         itertools.product([0, 1],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y,\n                                clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()","1800c4b5":"clf.fit(X, y)\n#ax = plt.subplot(gs[grd[0], grd[1]])\nfig = plot_decision_regions(X=X, y=y,\n                                clf=clf, legend=2)\nplt.title(\"Logistic Regression\")","5bef1d0f":"from sklearn.svm import SVC\n","f8f66acf":"clf=SVC(random_state=0, probability=True)\nclf.fit(X, y)\n#ax = plt.subplot(gs[grd[0], grd[1]])\nfig = plot_decision_regions(X=X, y=y,\n                                clf=clf, legend=2)\nplt.title(\"SVM\")","df33d14c":"from mlxtend.cluster import Kmeans\n\nkm = Kmeans(k=3, \n            max_iter=50, \n            random_seed=1, \n            print_progress=3)\n\nkm.fit(X)\n\nprint('Iterations until convergence:', km.iterations_)\nprint('Final centroids:\\n', km.centroids_)","a7a09692":"y_clust = km.predict(X)\nplt.figure(figsize=(10,10))\nplt.scatter(X[y_clust == 0, 0],\n            X[y_clust == 0, 1],\n            s=50,\n            c='lightgreen',\n            marker='s',\n            label='cluster 1')\n\nplt.scatter(X[y_clust == 1,0],\n            X[y_clust == 1,1],\n            s=50,\n            c='orange',\n            marker='o',\n            label='cluster 2')\n\nplt.scatter(X[y_clust == 2,0],\n            X[y_clust == 2,1],\n            s=50,\n            c='lightblue',\n            marker='v',\n            label='cluster 3')\n\n\nplt.scatter(km.centroids_[:,0],\n            km.centroids_[:,1],\n            s=250,\n            marker='*',\n            c='red',\n            label='centroids')\n\nplt.legend(loc='lower left',\n           scatterpoints=1)\nplt.grid()\nplt.show()","82937687":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","932c5d1e":"df.columns","6aa3d804":"df=df.drop('Unnamed: 32', axis=1)","495443fa":"df.columns","4b9d7eff":"col=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","cf39f611":"#diagnosis={'M':1, 'B':0}\n#df['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","6ff8d0d3":"X=df[col]\ny=df['diagnosis']","6e67be6d":"from mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis","3c3348e9":"X = standardize(X)\n","3c6618a2":"pca = PrincipalComponentAnalysis(n_components=2)\npca.fit(X)\nX_pca = pca.transform(X)","f42ca042":"X","b5724987":"pca = PrincipalComponentAnalysis(n_components=None)\npca.fit(X)\nX_pca = pca.transform(X)","0a706745":"pca.e_vals_","e90a7d00":"pca.e_vals_normalized_\n","61aad853":"import numpy as np\n\ntot = sum(pca.e_vals_)\nvar_exp = [(i \/ tot)*100 for i in sorted(pca.e_vals_, reverse=True)]\ncum_var_exp = np.cumsum(pca.e_vals_normalized_*100)","d1b0edca":"with plt.style.context('seaborn-whitegrid'):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    plt.bar(range(30), var_exp, alpha=0.5, align='center',label='individual explained variance')\n    plt.step(range(30), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.xticks(range(30))\n    ax.set_xticklabels(np.arange(1, X.shape[1] + 1))\n    plt.legend(loc='best')\n    plt.tight_layout()","8ba48625":"df.head()","e224cef0":"df.columns","bac16f01":"col=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","1659c6d7":"X=df[col]\ny=df['diagnosis']","cf6f28c1":"pca = PrincipalComponentAnalysis(n_components=2,\n                                 solver='svd')\npca.fit(X)\nX_pca = pca.transform(X)","3b60f756":"import matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip((0, 1),\n                        ('blue', 'red')):\n        plt.scatter(X_pca[y==0],\n                    X_pca[y==1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()","e0afaaf7":"y.value_counts()","a7987500":"y[y==0].shape\n","a7aa4e98":"plt.figure(figsize=(6, 4))\n\nplt.scatter(X_pca[y==0],\n                    X_pca[y==1])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(loc='lower center')\nplt.tight_layout()\nplt.show()","2ea56cc1":"# PCA via SVD","fbfc8ed8":"# PCA on Breast Cancer","c966df87":"# MLextend"}}