{"cell_type":{"83eb44f1":"code","ef407cf9":"code","a43a4b79":"code","66611816":"code","458d0d44":"code","d45dc95f":"code","e4125008":"code","a9e64619":"code","35441eeb":"code","0deccae3":"code","ed020087":"code","29953ce4":"code","295d0384":"code","195b4050":"code","66eb1c69":"code","68817221":"code","2d391428":"code","118fe65e":"code","6bad8767":"code","fcecab3f":"code","2ccfd1dc":"code","3faf4c3a":"code","05a0c94c":"code","f075e9f9":"code","1d897f3b":"code","fb46538f":"code","4b8f332b":"code","7abdb9a6":"code","cc9a3d01":"code","6a62bfa4":"code","d7dc5a3b":"code","11665f2c":"code","8c453abc":"code","cc0404e7":"code","bdda5560":"code","085701f2":"code","6ff663d2":"code","cbff6001":"code","6552aa25":"code","ebcdc425":"code","e88e7468":"code","54498dcc":"code","7dbc9200":"code","243fffd4":"code","7f20427a":"code","3715d5bc":"code","bb18bf9a":"markdown","58e7ff6d":"markdown","1395ca5c":"markdown","083ec0b2":"markdown","fee35f87":"markdown","222db8f4":"markdown","d64110e0":"markdown","f717ad17":"markdown","a680f8a3":"markdown","9fad1f89":"markdown","230c1471":"markdown"},"source":{"83eb44f1":"# Importing libs and modules\nimport seaborn as sb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OrdinalEncoder\nimport matplotlib as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\", \n                         sep=r'\\s*,\\s*', engine='python', na_values=\"?\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\", \n                         sep=r'\\s*,\\s*', engine='python', na_values=\"?\")\n# declaring some auxiliary variables\ncolor = {\n    \"boxes\": \"DarkGreen\",\n    \"whiskers\": \"DarkOrange\",\n    \"medians\": \"DarkBlue\",\n    \"caps\": \"Gray\",\n}\ngreen_diamond = dict(markerfacecolor='g', marker='D')\n\n# see attributes and rows\ntrain_data","ef407cf9":"#search for missing data\ntrain_data.isna().any()","a43a4b79":"# Using pd.describe to find the number of non-null values and mode of the Series\ntrain_data[\"workclass\"].describe()","66611816":"train_data[\"occupation\"].describe()","458d0d44":"train_data[\"native.country\"].describe()","d45dc95f":"# 3 attributes have missing data and all of them are string-like data, separated into categories\n# The 3 attributes with missing data have more than 89% of non-missing values. \n# Thus, the missing values will","e4125008":"# Replacing missing values with most frequent data (mode)\ntrain_data['workclass'] = train_data['workclass'].fillna('Private')\ntrain_data['occupation'] = train_data['occupation'].fillna('Prof-specialty')\ntrain_data['native.country'] = train_data['native.country'].fillna('United-States')","a9e64619":"#initial treatment to drop any possible duplicate rows\ntrain_data = train_data.drop_duplicates(keep='first')\ntrain_data","35441eeb":"# calculate capital.net and drop columns\ntrain_data['capital.net'] = train_data['capital.gain']-train_data['capital.loss'] \ntrain_data = train_data.drop(columns = ['Id', 'fnlwgt', 'education', 'capital.gain', 'capital.loss'])\ntrain_data.head()","0deccae3":"train_data['age'].plot.box(color=color, flierprops=green_diamond, title='Age')","ed020087":"train_data['age'].plot.hist(bins=40, title = 'Age')","29953ce4":"train_data['education.num'].plot.box(color=color, flierprops=green_diamond, title ='Education')","295d0384":"train_data['education.num'].plot.hist(bins=16, title='Education')","195b4050":"train_data['hours.per.week'].plot.box(color=color, flierprops=green_diamond, title='Hours per week')","66eb1c69":"train_data['hours.per.week'].plot.hist(bins=50, title='Hours per week')","68817221":"train_data['capital.net'].plot.box(color=color, flierprops=green_diamond, title = 'Net Capital')","2d391428":"atributos_qualitativos = [\"workclass\",\n                          \"occupation\", \"marital.status\", \"relationship\", \"sex\", \n                          \"race\", \"native.country\"]\n\n# ajustando os dados de treino \u00e0s vari\u00e1veis categ\u00f3ricas modificadas\ntrain_data[atributos_qualitativos] = OrdinalEncoder().fit_transform(train_data[atributos_qualitativos])","118fe65e":"train_data.head()","6bad8767":"# using scalers in each attribute:\nshaped_attr = ['age']\nnormal_attr = ['education.num', 'hours.per.week']\nsparse_attr = ['capital.net']\n\ntrain_data[shaped_attr] = MinMaxScaler().fit_transform(train_data[shaped_attr])\ntrain_data[normal_attr] = StandardScaler().fit_transform(train_data[normal_attr])\ntrain_data[sparse_attr] = RobustScaler().fit_transform(train_data[sparse_attr])","fcecab3f":"y_treino = train_data['income']\nx_treino = train_data.drop(columns=['income'])","2ccfd1dc":"# this snippet is a test to find an adequate range for a number for nearest neighbors\n#snippet 1 cited in KNN model introduction\n\nmodelo_knn = KNeighborsClassifier(n_neighbors=20)\n\ncv_scores = cross_val_score(modelo_knn, x_treino.values, y_treino, cv=5)\n#print each cv score (accuracy) and average them\nprint(cv_scores)\nprint('cv_scores mean:{}'.format(np.mean(cv_scores)))","3faf4c3a":"# now, we'll do an exhaustive search over specified parameter values for an estimator \n#using hyper parameters\n# snippet 2 cited in KNN model introduction\n\nfinal_modelo_knn = KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\n\nparam_grid = {'n_neighbors': np.arange(15, 25)}\n#use gridsearch to test all values for n_neighbors\n\nknn_gscv = GridSearchCV(final_modelo_knn, param_grid, cv=5)\n#fit model to data\nknn_gscv.fit(x_treino.values, y_treino.values)","05a0c94c":"knn_gscv.best_params_","f075e9f9":"knn_gscv.best_score_","1d897f3b":"# applicating all transformations done to train data\ntest_data['workclass'] = test_data['workclass'].fillna('Private')\ntest_data['occupation'] = test_data['occupation'].fillna('Prof-specialty')\ntest_data['native.country'] = test_data['native.country'].fillna('United-States')\ntest_data = test_data.drop_duplicates(keep='first')\ntest_data['capital.net'] = test_data['capital.gain']-test_data['capital.loss'] \ntest_data = test_data.drop(columns = ['Id', 'fnlwgt', 'education', 'capital.gain', 'capital.loss'])\ntest_data[atributos_qualitativos] = OrdinalEncoder().fit_transform(test_data[atributos_qualitativos])\ntest_data[shaped_attr] = MinMaxScaler().fit_transform(test_data[shaped_attr])\ntest_data[normal_attr] = StandardScaler().fit_transform(test_data[normal_attr])\ntest_data[sparse_attr] = RobustScaler().fit_transform(test_data[sparse_attr])","fb46538f":"# this piece of code will not be used in final submission\n'''\nfinal_model_knn = KNeighborsClassifier(n_neighbors=19)\nfinal_model_knn.fit(x_treino, y_treino)\n# prediction to be realized\npredicao = final_model_knn.predict(test_data)\nkey = {'income': predicao}\nresults_knn = pd.DataFrame(key)\nresults_knn\n'''","4b8f332b":"from sklearn.svm import SVC","7abdb9a6":"# instance support vector classifier from SVM\n\n\nsvc = SVC(random_state=42, probability=True, gamma='auto')","cc9a3d01":"score_svm = cross_val_score(svc, x_treino.values, y_treino, cv =5, scoring = 'accuracy')","6a62bfa4":"print('Score para svm:', score_svm.mean())","d7dc5a3b":"from sklearn.ensemble import AdaBoostClassifier","11665f2c":"# instance random florest classifier \n\nabc = AdaBoostClassifier(n_estimators = 100, random_state=0)\n","8c453abc":"score_abc = cross_val_score(abc, x_treino.values, y_treino, cv =5, scoring = 'accuracy')","cc0404e7":"print('Score para AdaBoost:', score_abc.mean())","bdda5560":"from sklearn.linear_model import LogisticRegression","085701f2":"# instance Logistic Regression classifier\n\nlogit = LogisticRegression(solver = 'liblinear', C = 2, warm_start =  True)","6ff663d2":"score_logit = cross_val_score(logit, x_treino.values, y_treino, cv =5, scoring = 'accuracy')","cbff6001":"print('Score para Logit:', score_logit.mean())","6552aa25":"from xgboost import XGBClassifier","ebcdc425":"# instance Logistic Regression classifier\nxboost = XGBClassifier(n_estimators = 200, max_depth = 5, eval_metric='mlogloss')","e88e7468":"score_xboost = cross_val_score(xboost, x_treino.values, y_treino, cv =5, scoring = 'accuracy')","54498dcc":"print('Score para XGBoost:', score_xboost.mean())","7dbc9200":"xboost.fit(x_treino, y_treino)\n# prediction to be realized\npredicao = xboost.predict(test_data)\nkey = {'income': predicao}\nresults_xboost = pd.DataFrame(key)\nresults_xboost","243fffd4":"results_xboost['Id'] = results_xboost.index","7f20427a":"results = results_xboost[['Id', 'income']]\nresults.head()","3715d5bc":"results.to_csv('submission.csv',index = False)","bb18bf9a":"# Results and Submission\n\nAs shown above, XGBoosting classifier has achieved the highest score among our models.\n\nFrom now on, this notebook will be showing the submission of the test data results.","58e7ff6d":"# Models Dataset Adult\n\n### Adult - PMR3508\n#### Hash - PMR3508-2021-32\n#### Aluno - Renan Rosenfeld Bayer Matias","1395ca5c":"# SVM","083ec0b2":"# Extreme Gradient Boosting","fee35f87":"# Models\n\nIn the next sections, we'll be creating our 5 distinct prediction models to evaluate this problem.\n","222db8f4":"### 2. Data Understanding and cleaning\n\nNow, we want to understand our data. We'll do the following in the snippets below.\n\n1) First, any duplicate rows are dropped. \n\n2) Then, we calculate net capital for each row and drop capital.loss and capital.gain attributes. Irrelevant attributes like id, fnlwgt and education (which is redundant) are also dropped.\n\n3) Then, some boxplots and histograms are generated to understand the distribution of the quantitative data. Categorical plots could be used also.\n\n4) Lastly, the categorical data is encoded to ordinal distribution to prepare for analysis.","d64110e0":"# KNN Model\n\nLastly, we'll calculate the KNeighborsClassifier. Next, the first snippet is used to check individually a possible range to analyse a candidate number of Neighbors for the model.\n\nThe last snippet in this section do a GridSearch (more info [here](https:\/\/towardsdatascience.com\/building-a-k-nearest-neighbors-k-nn-model-with-scikit-learn-51209555453a) ) and plot the variable knn_gscv which contains the best params and values given a range of K Neighbors (that were previously chosen using snippet 1)","f717ad17":"# Logit (Logistic Regression)","a680f8a3":"# Data Prepping\n### 1. Missing Data\n\nFirst, we'll look for and handle missing data.\n\nAs can be seen, the 3 attributes that are missing data are categorical, so the missing data was replaced by the distribution mode.","9fad1f89":"# Ada Boost","230c1471":"### Data normalization\n\nHere, we want to normalize quantitative data.\n\n- For Age, we want to preserve the distribution shape, so MinMaxScaler is chosen.\n\n- For education and hours per week, we want a Gaussian distribution\n\n- And for net capital, we want a RobustScaler due to sparse distribution\n\n(great [reference](https:\/\/towardsdatascience.com\/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02) to read if reader has decent knowledge of statistics)"}}