{"cell_type":{"01fba3bc":"code","64fa7602":"code","e262d26b":"code","896fdd78":"code","7eef53e3":"code","0537e113":"code","1f6409dc":"code","16bf6fc6":"code","dbfee4f4":"code","18d1ce4a":"code","4ac7d2cb":"code","b204a1e9":"code","c39691cb":"code","900e1dea":"code","95f1d840":"code","b60d6212":"code","bc13bf91":"code","06a9b2bc":"code","d8265e6e":"code","d2601cd0":"code","d882986b":"code","d3cf6cb4":"markdown","ea9cdaf4":"markdown","9a561d1f":"markdown","e90d0551":"markdown","8ffe1600":"markdown","0fcce3b1":"markdown"},"source":{"01fba3bc":"#\/kaggle\/input\/state-farm-distracted-driver-detection\/sample_submission.csv\n#\/kaggle\/input\/state-farm-distracted-driver-detection\/driver_imgs_list.csv\n#\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/train\/c4\/img_16261.jpg\n\n","64fa7602":"import os\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageEnhance","e262d26b":"tf.__version__","896fdd78":"sample_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/sample_submission.csv\"\nimgs_list_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/driver_imgs_list.csv\"\ntrain_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/train\"","7eef53e3":"driver_imgs_list = pd.read_csv(imgs_list_path)\ndriver_imgs_list.head()","0537e113":"os.listdir(train_path)","1f6409dc":"def pair_sort(className,values):\n    for j in range(0,len(className)-1):\n        for i in range(0,len(className)-1):\n            if values[i] > values[i+1]:\n                temp =  values[i+1]\n                values[i+1] = values[i]\n                values[i] = temp\n\n                N_temp =  className[i+1]\n                className[i+1] = className[i]\n                className[i] = N_temp\n    \n    return className,values","16bf6fc6":"from matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 5), dpi=80, facecolor='w', edgecolor='k')\n\nclass_names = np.unique(driver_imgs_list['classname'])\nclass_image_list = [len(driver_imgs_list[driver_imgs_list['classname'] == current_class]) for current_class in class_names]\n\nclass_names,class_image_list=  pair_sort(class_names,class_image_list)\n\n#plt.figure()\nplt.suptitle('Number of images per Class')\nplt.bar(class_names,class_image_list,color=(0.2, 0.4, 0.6, 0.6))\nplt.show()","dbfee4f4":"from matplotlib.pyplot import figure\nsub_names = np.unique(driver_imgs_list['subject'])\nsub_image_list = [len(driver_imgs_list[driver_imgs_list['subject'] == current_sub]) for current_sub in sub_names]\nsub_names,sub_image_list=  pair_sort(sub_names,sub_image_list)\n\nfigure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\n\ny_pos = np.arange(len(sub_names))\n# Create horizontal bars\nplt.barh(y_pos, sub_image_list,color=(0.2, 0.4, 0.6, 0.6))\n \n# Create names on the y-axis\nplt.yticks(y_pos,sub_names )\nplt.suptitle('Number of images per subject')\n\n# Show graphic\nplt.show()","18d1ce4a":"img_width,img_height = (256,256)\nmodel_input_shape = (img_width,img_height,3)\nbatch_size = 16\ninput_image = (img_width,img_height)\n\ndef load_image(path):\n    read_path = train_path+\"\/\"+path\n    image = Image.open(read_path)\n    image = image.resize(input_image)\n    \n    return np.asarray(image)","4ac7d2cb":"def show_images(image_ids,class_names):\n    pixels = [load_image(path) for path in image_ids]\n    \n    num_of_images = len(image_ids)\n    \n    fig, axes = plt.subplots(\n        1, \n        num_of_images, \n        figsize=(5 * num_of_images, 5 * num_of_images),\n        \n    )\n   \n    \n    for i, image_pixels in enumerate(pixels):\n        axes[i].imshow(image_pixels)\n        axes[i].axis(\"off\")\n        axes[i].set_title(class_names[i])","b204a1e9":"sub_names_imgs = [ current_class+\"\/\"+driver_imgs_list[driver_imgs_list['classname'] == current_class]['img'].values[0] for current_class in class_names]\n\nshow_images(sub_names_imgs[:5],class_names[:5])\nshow_images(sub_names_imgs[5:],class_names[5:])","c39691cb":"train_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/train\"\ntest_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/test\"","900e1dea":"x_train = []\ny_train = []\n\nx_val = []\ny_val = []\n\n\nsplit_rate = 0.8\nfor current_class in class_names:\n    select_df = driver_imgs_list[driver_imgs_list['classname'] == current_class ]\n    image_list = select_df['img'].values\n    train_amount = int(len(image_list)*split_rate)\n    train_list = image_list[:train_amount]\n    val_list = image_list[train_amount:]\n    \n    for filename in train_list:\n        x_train.append(load_image(current_class+\"\/\"+filename))\n        y_train.append(current_class.replace('c',''))\n\n    for filename in val_list:\n        x_val.append(load_image(current_class+\"\/\"+filename))\n        y_val.append(current_class.replace('c',''))\n","95f1d840":"x_train = np.asarray(x_train)\ny_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\nx_val = np.asarray(x_val)\ny_val =tf.keras.utils.to_categorical(y_val, num_classes=10)\nprint(\"Train x Shape: \",x_train.shape)\nprint(\"Test x Shape: \",x_val.shape)\n","b60d6212":"print(\"Train y Shape: \",y_train.shape)\nprint(\"Test y Shape: \",y_val.shape)","bc13bf91":"base_model  = tf.keras.applications.resnet.ResNet50(include_top = False,\n                                                  weights = 'imagenet',\n                                                  input_shape = model_input_shape)\nbase_model.summary()","06a9b2bc":"x = base_model.output\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\noutput =tf.keras.layers.Dense(units = len(class_names),activation = tf.nn.softmax)(x)\nmodel = tf.keras.models.Model(inputs=base_model.inputs, outputs=output)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False),\n              metrics=['accuracy'])\n\nmodel.summary()","d8265e6e":"num_epochs = 50\ndef lr_schedule(epoch,lr):\n    # Learning Rate Schedule\n\n    lr = lr\n    total_epochs = num_epochs\n\n    check_1 = int(total_epochs * 0.9)\n    check_2 = int(total_epochs * 0.8)\n    check_3 = int(total_epochs * 0.6)\n    check_4 = int(total_epochs * 0.4)\n\n    if epoch > check_1:\n        lr *= 1e-4\n    elif epoch > check_2:\n        lr *= 1e-3\n    elif epoch > check_3:\n        lr *= 1e-2\n    elif epoch > check_4:\n        lr *= 1e-1\n\n    print(\"[+] Current Lr rate : {} \".format(lr))\n    return lr\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)","d2601cd0":"history = model.fit(\n      x = x_train,y=y_train,\n      validation_data=(x_val,y_val),\n      steps_per_epoch=16,\n      batch_size = 8,\n      epochs=num_epochs,\n    \n    callbacks = [lr_callback],\n      verbose=1)","d882986b":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nax[0].set_title('Training Loss')\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\n\nax[1].set_title('Validation Loss')\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])","d3cf6cb4":"## 6. Model Evaluation","ea9cdaf4":" ## 3. Split and load Train\/Validation ","9a561d1f":"## 2.Plot class images","e90d0551":"## 1.Check data distribution","8ffe1600":"## 4. Encode Labels","0fcce3b1":"## 5. Create Model\n"}}