{"cell_type":{"4e1901cf":"code","84c467a2":"code","2eb22a2b":"code","20ad169e":"code","90db7a0a":"code","afbf1888":"code","785efb07":"code","d9e05159":"code","fd00f719":"code","4f28b179":"code","1a342e35":"code","c5048ca0":"code","844255f3":"code","e511a9f6":"code","1a677e16":"code","108dc404":"code","715d1fb9":"code","131d044d":"code","af44736d":"code","8ead758b":"code","e6ebe1af":"code","d627fc4f":"code","340d6566":"code","2a74086f":"code","703bd936":"code","b606f031":"code","e943b29c":"code","68e38533":"code","84ac1ded":"code","af6d84fd":"code","57fb0c11":"code","f1c39f29":"code","da6ad43b":"code","a5897a38":"markdown","553095fb":"markdown","731bf44c":"markdown","4d30e47f":"markdown","3a5e6877":"markdown","d8d7ffd1":"markdown","a3aba143":"markdown","ed81ffc4":"markdown","7bd89e02":"markdown","56634d06":"markdown","65db8909":"markdown","f4e24f1a":"markdown","c6d9ffd2":"markdown","3a15a5ce":"markdown","cedfb644":"markdown","6d9eb0f2":"markdown","3cb064e5":"markdown","f06d861e":"markdown","2e0bf568":"markdown","e5962dca":"markdown","3be6147e":"markdown"},"source":{"4e1901cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport re\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport itertools\nfrom sklearn.metrics import classification_report","84c467a2":"train=pd.read_csv(\"\/kaggle\/input\/fakenewskdd2020\/train.csv\", sep='\\t', encoding='utf-8')\ntest = pd.read_csv(\"\/kaggle\/input\/fakenewskdd2020\/test.csv\", sep='\\t', encoding='utf-8')\nsubmission = pd.read_csv(\"\/kaggle\/input\/fakenewskdd2020\/sample_submission.csv\")\ntrain.head()","2eb22a2b":"print(f\"E\u011fitim Verisinin Boyutu = {train.shape}\")\nprint(f\"Test Verisinin Boyutu = {test.shape}\")","20ad169e":"train.info()","90db7a0a":"test.info()","afbf1888":"print(f\"E\u011fitim Veri Seti Null De\u011fer Kontrol\u00fc = \\n {train.isnull().sum()}\")\nprint(f\"E\u011fitim Veri Seti Null De\u011fer Kontrol\u00fc = \\n{test.isnull().sum()}\")","785efb07":"plt.figure(figsize=(7,7))\n\nvalue_count = train[\"label\"].value_counts()\n\n\nplt.pie(value_count, labels=value_count.index ,\n        startangle = 90 ,counterclock =False, wedgeprops = {'width' : 0.6},\n        autopct='%1.1f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 18}, \n        shadow = True,colors = sns.color_palette(\"Paired\")[7:])\nplt.text(x = -0.25, y=0,s =\"Toplam Veri :{}\".format(train.shape[0]))\nplt.title(\"Veri Setinin Da\u011f\u0131l\u0131m\u0131\",size=15)\n","d9e05159":"from wordcloud import WordCloud\n#Kelime Bulutu (wordCloud) \u0130le Veri Setinin G\u00f6rselle\u015ftirilmesi\n\ncloud = train.text.tolist()\ncloud_st = \" \".join(cloud)\nplt.figure(figsize=(10,10))\nplt.imshow(WordCloud().generate(cloud_st))\nplt.axis(\"off\")","fd00f719":"plt.figure(figsize=(10,6))\nplt.hist(train[train[\"label\"]==\"0\"][\"text\"].str.len(),label=\"Ger\u00e7ek Haber\",color=\"red\")\nplt.hist(train[train[\"label\"]==\"1\"][\"text\"].str.len(),label=\"Sahte Haber\",color=\"b\")\nplt.xlabel(\"Karakter Uzunluk\",size=15)\nplt.ylabel(\"frekans\",size=15)\nplt.title(\"Karakter Uzunluklar\u0131na G\u00f6re Sahte Haber ve Ver\u00e7ek Haberler\",size=12)\nplt.legend(loc=\"best\")\nplt.show()","4f28b179":"plt.figure(figsize=(10,6))\nplt.hist(train[train[\"label\"]==\"0\"][\"text\"].str.split().map(lambda x: len(x)),label=\"Do\u011fru Haber\",color=\"red\")\nplt.hist(train[train[\"label\"]==\"1\"][\"text\"].str.split().map(lambda x: len(x)),label=\"Sahte Haber\",color=\"b\")\n\nplt.xlabel(\"Kelime Uzunlu\u011fu\",size =15)\nplt.ylabel(\"Frekans\",size=15)\nplt.title(\"Kelime Uzunlukluklar\u0131na G\u00f6re Verilerin Da\u011f\u0131l\u0131m\u0131\")\nplt.legend(loc=\"best\")\nplt.show()","1a342e35":"a = train[\"text\"].tolist()\n#print(a)","c5048ca0":"stop_words = set(stopwords.words(\"english\"))\nlemma = WordNetLemmatizer()\n\ndef celaning_data(text):\n    text = re.sub(r'http\\S+','',text)                \n    #Url'lerin temizlenmesi                \n    text = re.sub('[^a-zA-Z]',' ',text)              \n    # \u00d6zel karakterlerin ve numarlar\u0131n temizlenmesi\n    text = str(text).lower()                          \n    #B\u00fct\u00fcn karakterlerin k\u00fc\u00e7\u00fck harfe d\u00f6n\u00fc\u015f\u00fct\u00fcr\u00fclmesi\n    text = word_tokenize(text)                        \n    #Verilerin Tokenlara ayr\u0131lmas\u0131\n    text = [i for i in text if i not in stop_words]   \n    # Etkisiz kelimelerin \u00e7\u0131kart\u0131lmas\u0131\n    text = [lemma.lemmatize(word= j, pos=\"v\") for j in text] \n    # Lemmatizasyon i\u015fleminin ger\u00e7ekle\u015ftirilmesi\n    text = [k for k in text  if len(k) >2]            \n    #karkater boyutu 2 den d\u00fc\u015f\u00fck olanlar\u0131n veri setinden at\u0131lmas\u0131 (g\u00fcr\u00fclt\u00fcl\u00fc verilerin kald\u0131r\u0131lmas\u0131)\n    text = \" \".join(text)                             \n    return text\ntrain[\"celaning_data\"] = train[\"text\"].apply(celaning_data) # Bu yap\u0131y\u0131 Test veri setinde de kullanacaks\u0131n","844255f3":"train.head(20)","e511a9f6":"gercek_haber = train[train[\"label\"] == \"0\"][\"celaning_data\"]\nsahte_haber = train[train[\"label\"] == \"1\"][\"celaning_data\"]\ndf = [gercek_haber,sahte_haber]\n\nfor i in range(2):\n    plt.figure(figsize=(10,6))\n    pd.Series(' '.join([j for j in df[i]]).split()).value_counts().head(20).plot(kind=\"bar\")\n    plt.show()","1a677e16":"def kelime_sil(text):\n    return ' '.join([i for i in text.split() if i !=\"say\"])\ntrain[\"clean\"] = train[\"celaning_data\"].apply(kelime_sil)","108dc404":"#\u0130htiyac\u0131m\u0131z olmayan kolonlar\u0131n temizlenmesi\ntrain.drop([\"text\",\"celaning_data\"],axis=1,inplace=True)\ntrain.head()","715d1fb9":"x = train[\"clean\"]  \n# Tahmin edilecek girdi verilerini x de\u011fi\u015fkenine.\ny = train [\"label\"] \n# Hedef dde\u011fi\u015fkenimiz olan \"label\" \u00f6zniteli\u011fini y de\u011fi\u015fkenine atad\u0131k.\n\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=18)\n# Modelin e\u011fitimi i\u00e7in verilerin %80'ini e\u011fitime %20 sini test verileri olarak b\u00f6lme i\u015flemi ger\u00e7ekle\u015ftirdim.\n\nprint(f\"X_train Boyutu : {X_train.shape}\")\nprint(f\"X_test Boyutu : {X_test.shape}\")\nprint(f\"y_train Boyutu : {y_train.shape}\")\nprint(f\"y_test Boyutu : {y_test.shape}\")","131d044d":"# Count Vectorize(Sayma Vekt\u00f6r\u00fc)\n\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english') \n# E\u011fitim verilerin fit edilmesi .\ncount_train = count_vectorizer.fit_transform(X_train)\n# Test verilerin fit edilmesi\ncount_test = count_vectorizer.transform(X_test)","af44736d":"#Tf-IDF Vekt\u00f6rizasyonu\n\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n#E\u011fitim Verisinin  Tf-Idf D\u00f6n\u00fc\u015ft\u00fcr\u00fclmesi\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n#Test Verilerinin Tf-Idf D\u00f6n\u00fc\u015ft\u00fcr\u00fclmesi \ntfidf_test = tfidf_vectorizer.transform(X_test)\n","8ead758b":"# Count Vectorizer metodu ile Multinomial Naive Bayes algoritmas\u0131n\u0131n uygulanmas\u0131 \nnb_classifier = MultinomialNB(alpha = 0.1)\n#MNB algoritmas\u0131n\u0131n e\u011fitilmesi\nnb_classifier.fit(count_train, y_train)\n#MNB algoritmas\u0131n\u0131n predict edilmesi\npred_nb_count = nb_classifier.predict(count_test)\n#Do\u011fruluk metri\u011finin hesaplanmas\u0131\nacc_nb_count = metrics.accuracy_score(y_test, pred_nb_count)\n#round ile elde edilen sonucun y\u00fczelik olarak yazd\u0131r\u0131lmas\u0131\nprint(f\"Count Vectorizer MNB Modelinin Accuracy Ba\u015far\u0131m\u0131 = %{round(acc_nb_count*100,2)}\")\nprint(f\" = {classification_report(y_test,pred_nb_count)}\")\n","e6ebe1af":"# MNB i\u00e7in hiperparametrenin belirlenmesi\nfor alpha in np.arange(0,1,.05):\n    nb_classifier_hp = MultinomialNB(alpha=alpha)\n    nb_classifier_hp.fit(count_train, y_train)\n    pred_hp = nb_classifier_hp.predict(count_test)\n    score = metrics.accuracy_score(y_test, pred_hp)\n    print(\"Alpha: {:.2f} Score: {:.5f}\".format(alpha, score))\n","d627fc4f":"# Alpha De\u011feri 0.90 olarak MNB modelinin uygulanmas\u0131\nnb_classifier = MultinomialNB(alpha = 0.90)\nnb_classifier.fit(count_train, y_train)\npred_nb_count = nb_classifier.predict(count_test)\ncm = metrics.confusion_matrix(y_test, pred_nb_count)\n\n# Confusion Matrisinin Olu\u015fturulmas\u0131 ve ileride kullan\u0131lacak modellere uygulanabilmesi i\u00e7in confusion matris fonksiyounun olu\u015futurlmas\u0131\ndef plot_confusion_matrix(cm, classes,\n                          #cm = e\u011fitlen modelin CF metrikleri , Classes= True,Fake olmak \u00fczere sahte haberlerin s\u0131n\u0131f de\u011ferleri\n                          normalize=False,\n                          # Grafi\u011fimizin Ba\u015fl\u0131\u011f\u0131\n                          title='Confusion matrix', \n                          #Grafi\u011fimizin Renkleri\n                          cmap=plt.cm.Blues):       \n    #Grafi\u011fin imshow metodu ile \u00e7izilmesi\n    plt.imshow(cm, cmap=cmap) \n    #Grafik Ba\u015fl\u0131\u011f\u0131\n    plt.title(title)          \n    plt.colorbar()            \n    tick_marks = np.arange(len(classes))\n    # xekseninkide \"True-False\" s\u0131n\u0131flar\u0131n\u0131n yazd\u0131r\u0131lmas\u0131\n    plt.xticks(tick_marks, classes, rotation=45)\n    # yekseninkide \"True-False\" s\u0131n\u0131flar\u0131n\u0131n yazd\u0131r\u0131lmas\u0131\n    plt.yticks(tick_marks, classes)              \n    thresh = cm.max() \/ 2                        \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 #kutucuklar\u0131n merkezine konumland\u0131rma\n                 horizontalalignment=\"center\",   \n                 #TN de\u011ferini beyaz fontta yazd\u0131rma\n                 color=\"white\" if cm[i, j] > thresh else \"black\") \n\n    plt.tight_layout()\n    plt.ylabel('Ger\u00e7ek Veriler')\n    plt.xlabel('Tahmin Verileri')\n    \n    \nplot_confusion_matrix(cm, classes=['TRUE','FAKE'], title ='Count Vectorizer \u0130le MultinoimalNB Algoritmas\u0131')","340d6566":"#TF-IDF Vekt\u00f6r\u00fc ile MNB Algoritmas\u0131n\u0131n Uygulanmas\u0131\n\nnb_classifier = MultinomialNB(alpha = 0.1)                   \n#MNB algoritmas\u0131n\u0131n de\u011fi\u015fkene atanmas\u0131\nnb_classifier.fit(tfidf_train, y_train)                      \n#Mnb algoritmas\u0131 ile verilerin e\u011fitilmesi\npred_nb_tfidf = nb_classifier.predict(tfidf_test)            \n#tahminleme Prediction i\u015fleminin ger\u00e7ekle\u015ftirilmesi\nacc_nb_tfidf = metrics.accuracy_score(y_test, pred_nb_tfidf) \n#do\u011fruluk metri\u011finin hesaplanmas\u0131\nprint(f\"Accuracu = %{round(acc_nb_tfidf*100,2)}\")  \nprint(f\" = {classification_report(y_test,pred_nb_tfidf)}\")\n#do\u011fruluk metri\u011finin yazd\u0131r\u0131lmas\u0131","2a74086f":"#TF-IDf y\u00f6ntemi ile ger\u00e7ekle\u015ftirdi\u011fimiz modelin hiper parametresini ayarlama\nfor alpha in np.arange(0,0.1,.01): \n    nb_classifier_tune = MultinomialNB(alpha=alpha)            \n    #d\u00f6ng\u00fc i\u00e7erisindeki artt\u0131r\u0131m say\u0131s\u0131na g\u00f6re modelin alpha de\u011ferinin girilmesi \n    nb_classifier_tune.fit(tfidf_train, y_train)               \n    #Modelin e\u011fitilmesi\n    pred_tune = nb_classifier_tune.predict(tfidf_test)         \n    #tahminleme prediction i\u015flemininger\u00f6ekle\u015ftirilmesi\n    score = metrics.accuracy_score(y_test, pred_tune)          \n    #do\u011fruluk metri\u011finin hesaplanmas\u0131\n    print(\"Alpha: {:.2f}  Score: {:.5f}\".format(alpha, score)) \n    #elde edilen sonucun yazd\u0131r\u0131lmas\u0131","703bd936":"nb_classifier = MultinomialNB(alpha = 0.04)\nnb_classifier.fit(tfidf_train, y_train)\npred_nb_tfidf = nb_classifier.predict(tfidf_test)\ncm2 = metrics.confusion_matrix(y_test, pred_nb_tfidf)\nprint(metrics.accuracy_score(y_test, pred_nb_tfidf))\nprint(f\" = {classification_report(y_test,pred_nb_tfidf)}\")\nplot_confusion_matrix(cm2, classes=['TRUE','FAKE'],title ='TF-IDF y\u00f6ntemi ile haz\u0131rlanan MNB Modeli')","b606f031":"# TF-IDF metodu ile say\u0131salla\u015ft\u0131r\u0131lm\u0131\u015f veriler ile Pasif Agresif S\u0131n\u0131fland\u0131r\u0131c\u0131n\u0131n uygulanmas\u0131\nlinear_classifier = PassiveAggressiveClassifier(C=0.5,random_state=5)                    \n#PA s\u0131n\u0131fland\u0131r\u0131c\u0131n\u0131n \u00e7a\u011f\u0131rlmas\u0131\nlinear_classifier=linear_classifier.fit(tfidf_train, y_train)        \n#e\u011fitim seti i\u00e7erisinde e\u011fitimin ger\u00e7ekle\u015ftrilmesi\npred_linear_tfidf = linear_classifier.predict(tfidf_test)            \n#tahminleme prediction i\u015fleminin ger\u00e7ekle\u015ftirilmesi\nacc_linear_tfidf = metrics.accuracy_score(y_test, pred_linear_tfidf) \n#Do\u011fruluk de\u011ferinin hesaplanmas\u0131\nprint(f\"Accuracy = %{round(acc_linear_tfidf*100,2)}\")                \n#Do\u011fruluk de\u011ferinin girilmesi\ncm5 = metrics.confusion_matrix(y_test, pred_linear_tfidf) \nprint(f\" = {classification_report(y_test, pred_linear_tfidf)}\")\n#Confusion matrisi metriklerinin hesaplanmas\u0131\nplot_confusion_matrix(cm5, classes=['TRUE','FAKE'], title ='TF-IDF metoduna g\u00f6re ger\u00e7ekle\u015ftirilmi\u015f PA Modeli')","e943b29c":"trigram_vectorizer = CountVectorizer(analyzer = \"word\", ngram_range=(1,3))\n\nclassifier = LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n          tol=0.0001, verbose=0, warm_start=False)","68e38533":"pipe = Pipeline([('vect', CountVectorizer(analyzer = \"word\", ngram_range=(1,3))),\n                 ('tfidf', TfidfTransformer(norm=\"l2\")),\n                 ('model', classifier)\n                  ])\n# modelin e\u011fitilmesi\nmodel = pipe.fit(X_train, y_train)\n# tahminleme Prediction i\u015fleminin ger\u00e7ekle\u015ftirilmesi\nprediction = model.predict(X_test)\n#do\u011fruluk ba\u015far\u0131m\u0131n\u0131n ekrana yaz\u0131lmas\u0131\nprint(f\"accuracy: %{round(accuracy_score(y_test, prediction)*100,2)}\")\nprint(f\"{classification_report(y_test, prediction)}\")\n#Confusion matrisini olu\u015fturma\ncm5 = metrics.confusion_matrix(y_test, prediction)\nplot_confusion_matrix(cm5, classes=['TRUE','FAKE'], title ='Logistic Regression S\u0131n\u0131fland\u0131r\u0131c\u0131s\u0131 CF Matrisi')","84ac1ded":"# Test Verisini Olu\u015fturudu\u011fumuz Veri \u00d6n \u0130\u015fleme Fonskyionundan Ge\u00e7iriyoruz.\ntest[\"text\"] = test[\"text\"].apply(celaning_data)","af6d84fd":"# Modelimizi Test Veri Setinde Tahminleme \u0130\u015flemini Ger\u00e7ekle\u015ftiriyoruz.\ntest_pred = model.predict(test[\"text\"])","57fb0c11":"#Test Veri Setine \"Label\" Adl\u0131 Hedef \u00d6zniteli\u011fimizi Olu\u015fturup \u0130\u00e7erisine Elde Etti\u011fimiz Sonucu Yaz\u0131yoruz.\ntest['label'] = test_pred\ntest.head()\n\n#Olu\u015fturdu\u011fumuz \u00d6znitelik \u0130\u00e7eriindeki De\u011ferleri Value_Count Metodu \u0130le Da\u011f\u0131l\u0131m\u0131n\u0131 G\u00f6zlemliyoruz\nprint(test[\"label\"].value_counts())\n","f1c39f29":"sub_test = test[['id','label']]","da6ad43b":"sub_test.to_csv(\"first_sub.csv\",index = False)\nfrom IPython.display import FileLink\nFileLink(r'first_sub.csv')","a5897a38":"<a id=\"2\"><\/a> <br>\n# Veri \u00d6n \u0130\u015fleme\n\nKe\u015fifsel veri analizi s\u0131ras\u0131nda elde etti\u011fimiz bilgilere dahilinde veri seti i\u00e7erisinde bulunan ;\n* URL'leri Temizleme\n* \u00d6zel Karakterlerin ve Numeric \u0130fadelerin Temizlenmesi\n* Karakterlerin K\u00fc\u00e7\u00fck Harfe \u00c7evrilmesi\n* Etkisiz Kelimelerin \u00c7\u0131kal\u0131mas\u0131 (Stop Words)\n* Lematizasyon \u0130\u015fleminin Ger\u00e7ekle\u015ftirilmesi\n* G\u00fcr\u00fclt\u00fcl\u00fc Verilerin Silinmesi \n\n\u0130\u015flemlerini ger\u00e7ekle\u015ftirece\u011fiz.\n\n","553095fb":"<a id=\"11\"><\/a> <br>\n\n## Pasif Agresif S\u0131n\u0131fland\u0131r\u0131c\u0131","731bf44c":"<a id=\"7\"><\/a> <br>\n### TF-IDF Vecktorizer","4d30e47f":"<a id=\"13\"><\/a> <br>\n\n# Test Veri Setine Se\u00e7ilen Modelin Uygulanmas\u0131","3a5e6877":"Hiperparametre se\u00e7i\u00e7inde en y\u00fcksek ba\u015far\u0131m\u0131 sa\u011flayan Alpha: Alpha: 0.04  Score: 0.78156 de\u011feridir bunu se\u00e7erek modelimi tekrar olu\u015fturup confusion matrisini olu\u015fturaca\u011f\u0131m","d8d7ffd1":"E\u011fitim Veri Setinin Boyutu : (4987,2)\n\nTest Veri Setinin Boyutu :  (1247, 2)\n\nHem e\u011fitim hem de test veri setleri i\u00e7erisinde bo\u015f(null) de\u011fer yoktur.\n","a3aba143":"Uygulam\u0131\u015f oldu\u011fum modeller aras\u0131nda en y\u00fcksek do\u011fruluk ve en y\u00fcksek F-1 Skor de\u011ferlerini veren model Logistic Regression modeli oldu\u011fu i\u00e7in test veri seti i\u00e7erisinde bu modelimle tahminleme i\u015flemini ger\u00e7ekle\u015ftirip submission'umu ger\u00e7ekle\u015ftirece\u011fim.","ed81ffc4":"<a id=\"10\"><\/a> <br>\n\n## TF-IDF Vekt\u00f6r\u00fc ile MNB Algoritmas\u0131n\u0131n Uygulanmas\u0131","7bd89e02":"<a id=\"14\"><\/a> <br>\n\n# Submission","56634d06":"<a id=\"6\"><\/a> <br>\n### Count Vectorizer","65db8909":"Veri Setine ilk bak\u0131\u015fta veri seti i\u00e7erisinde \u00f6zel karakterlerin , url'lerin ve emojilerin oldu\u011funu g\u00f6zlemliyoruz.\nVeri \u00f6ni\u015fleme safahas\u0131nda olu\u015fturaca\u011f\u0131m\u0131z modele negatif y\u00f6nde etkileyecek bu verilerin silinmesi i\u015flemini ger\u00e7ekle\u015ftirece\u011fiz.\n\n\n![1.png](attachment:1.png)","f4e24f1a":"<a id=\"9\"><\/a> <br>\n\n## Count Vectorizer ile MNB Modeli","c6d9ffd2":"<a id=\"12\"><\/a> <br>\n\n## Logistic Regression Modelinin Olu\u015fturulmas\u0131","3a15a5ce":"# Fake News Detected\n\n1. [Ke\u015fifsel Veri Analizi](#1)\n1. [Veri \u00d6n \u0130\u015fleme](#2)\n    1. [En S\u0131k Kullan\u0131lan Kelime Tespiti](#3)\n1. [E\u011fitim Test Verilerinin B\u00f6l\u00fctlenmesi](#4)\n1. [Metin Verilerinin Say\u0131sal Verilere \u00c7evrilmesi](#5)\n    1. [Count Vectorizer](#6)\n    1. [TFIDF Vecktorizer](#7)\n1. [Model Olu\u015fturma](#8)\n    1. [Count Vectorizer ile MNB Modeli](#9)\n    1. [TF-IDF Vekt\u00f6r\u00fc ile MNB Algoritmas\u0131n\u0131n Uygulanmas\u0131](#10)\n    1. [Pasif Agresif S\u0131n\u0131fland\u0131r\u0131c\u0131](#11)\n    1. [Logistic Regression Modelinin Olu\u015fturulmas\u0131](#12)\n1. [Test Veri Setine Se\u00e7ilen Modelin Uygulanmas\u0131](#13)\n1. [Submission](#14)\n\n    ","cedfb644":"Alpha: 0.90 Score: 0.77355","6d9eb0f2":"Hem Sahte haber hem de ger\u00e7ek haberde en \u00e7ok tekrar eden kelime \"say\" kelimesidir. Modelimizin daha iyi bir performansa sahip olmas\u0131 i\u00e7in \"say\" kelimesini veri setinden temizleme i\u015flemi ger\u00e7ekle\u015ftiriyoruz.","3cb064e5":"<a id=\"5\"><\/a> <br>\n# Metin Verilerinin Say\u0131sal Verilere D\u00f6n\u00fc\u015ft\u00fcr\u00fclmesi","f06d861e":"<a id=\"3\"><\/a> <br>\n## En S\u0131k Kullan\u0131lan Kelime Tespiti\n\nSahte haber ve Ger\u00e7ek haberlerin verilerine bak\u0131larak her iki veri s\u0131n\u0131f\u0131nda en \u00e7ok tekrar eden ortak kelimelerin tespit edilip veri setinden \u00e7\u0131karma i\u015flemi ger\u00e7ekle\u015ftirdik.","2e0bf568":"<a id=\"8\"><\/a> <br>\n# Model Olu\u015fturma","e5962dca":"<a id=\"4\"><\/a> <br>\n# Model Olu\u015fturma","3be6147e":"<a id=\"1\"><\/a> <br>\n# Ke\u015fifsel Veri Analizi"}}