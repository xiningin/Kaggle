{"cell_type":{"011a2ed0":"code","3fa37ffd":"code","9c3aeec0":"code","45804e1b":"code","f7d1d84e":"code","3366bade":"code","1a67e31e":"code","073f57f1":"code","6fa59684":"code","1f627594":"code","95bea970":"code","b2869e64":"code","c65b0a29":"code","298bfbaa":"code","7eb37f74":"code","d1e4e468":"code","d8873389":"code","5423b4fe":"code","ca23068f":"code","f4dd6915":"code","7f700f69":"code","5c3fcf8c":"markdown","dd267888":"markdown","2d33db75":"markdown"},"source":{"011a2ed0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n#SKLEARN\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\n#OTHER LIBRARIES\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport string\nimport re\nimport nltk\n\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint('Loading Successful.')","3fa37ffd":"test_data  = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nprint('Loading Successful.')","9c3aeec0":"train_data.head(20)","45804e1b":"\ntrain_data.shape\n","f7d1d84e":"train_data.groupby('target').count()","3366bade":"#.head(n=50) - is a great function that allows you to see 50 records not just 10\ntrain_data.groupby('keyword').size().head(n=50)","1a67e31e":"train_data.describe()","073f57f1":"train_data.info()","6fa59684":"#Not working correctly x-axis should display 0 and 1 value.\n\ntrain_values = train_data['target'].value_counts()\ntrain_values = train_values[:2,]\nbarlist = plt.bar(train_values.index, train_values.values, alpha=0.8)\nbarlist[0].set_color('r')\nplt.xlabel('value', fontsize=12)\nplt.ylabel('count', fontsize=12)\nplt.title('fake news(0)         vs         real news(1)')\nplt.show()\n\n\n","1f627594":"#Great loop to show full text of each tweet as .head() might not display full message beacuse of it lenght.\nt = train_data[\"text\"].to_list()\nfor i in range(5):\n    print('Tweet Number '+str(i+1)+': '+t[i])","95bea970":"train_data['location'].value_counts().head(n=20)\n","b2869e64":"#You need to make train_keyword a list to get number of unique values of 'keyword' or you will get total number of all records which is useless.\ntrain_keyword = list(set(train_data['keyword']))\n\nprint(len(train_keyword))","c65b0a29":"#the way you can check if there's any duplicate messages 7613-7503 = 110 . thats the number of duplicated messages \nx1 = len(train_data) #=7613\nx2 = len(set(train_data['text'])) #=7498\nnumber_of_duplicated_records = (x1 - x2) \nprint('Number of duplicated records is:',number_of_duplicated_records)","298bfbaa":"def remove_punct(text):\n    text_nopunct = ''.join([char for char in text if char not in string.punctuation])\n    return text_nopunct\n\ntrain_data['text'] = train_data['text'].apply(lambda x: remove_punct(x))\n# this is how you can drop any column. --> train_data.drop('text_clean', axis=1, inplace=True)\ntrain_data.head()\n","7eb37f74":"#function to tokenize words\ndef tokenize(text):\n    tokens = re.split('\\W+',text) #W+ means that either a word character (A-Z) or a dash(-) can go there.\n    return tokens\n\n#converting to lowercase as python is case-sensitive\ntrain_data['text'] = train_data['text'].apply(lambda x: tokenize(x.lower()))\ntrain_data.head()","d1e4e468":"stopword = nltk.corpus.stopwords.words('english') #all english stopwords\n#function to remove stopwords\ndef remove_stopwords(tokenized_list):\n    text = [word for word in tokenized_list if word not in stopword]#to remofe all stopwords\n    return text\n\ntrain_data['text'] = train_data['text'].apply(lambda x: remove_stopwords(x))\ntrain_data.head()","d8873389":"# 1.5 pre-processing (cleaning) this is how I managed to lemmatize text(uprageded stemmer)\n# u can choose stemmer or lemmatize process (lemmatize is better but longer ;)\n\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatizing(tokenized_text):\n    text = [wn.lemmatize(word) for word in tokenized_text]\n    return text\n\ntrain_data['text'] = train_data['text'].apply(lambda x: lemmatizing(x))\n\ntrain_data.head()","5423b4fe":"train_data.head(50)","ca23068f":"train_data=train_data.drop('keyword',1)\ntrain_data=train_data.drop('location',1)\n\ntest_data=test_data.drop('keyword',1)\ntest_data=test_data.drop('location',1)","f4dd6915":"train_data.head(50)","7f700f69":"#MACHINE LEARNING\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nvectorizer = CountVectorizer(analyzer='word', binary=True)\nvectorizer.fit(train_data['text'])\nX = vectorizer.transform(train_data['text']).todense()\ny = train_data['target'].values\nX.shape, y.shape","5c3fcf8c":"# **2. Pre-processing (cleaning) Data.**","dd267888":"# Trying to drop location and keyword columns.","2d33db75":"# **1.Exploration of datasets.**"}}