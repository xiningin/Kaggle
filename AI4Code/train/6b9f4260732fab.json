{"cell_type":{"a886ca52":"code","baf7a275":"code","2d7070ed":"code","f616de68":"code","da065fee":"code","663fbb81":"code","464da2bc":"code","1d3c7a19":"code","59940c40":"code","f3a8115e":"code","3528165c":"code","0b213c2f":"code","ce9998d0":"code","a47a8e42":"code","32c07545":"code","993c4954":"markdown","2d29b659":"markdown","25c85052":"markdown","6c322f60":"markdown","445a8c74":"markdown","efbc9e6c":"markdown","ae367418":"markdown","275a7b2e":"markdown"},"source":{"a886ca52":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")","baf7a275":"# read the dataset \nreview = pd.read_csv('\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\nreview.head()","2d7070ed":"from sklearn.model_selection import train_test_split\nfrom nltk import word_tokenize, sent_tokenize\nimport nltk \n\nnltk.download('punkt')\nnltk.download('wordnet')\n\n# split the data into train test set \ntrain_set, hold_out = train_test_split(review, train_size=0.7, random_state=123)\nvalid_set, test_set = train_test_split(hold_out, train_size=0.5, random_state=123)\n\nprint(\"[TOTAL]: \", review.shape)\n\n# Display general information\nfor set_, name in zip([train_set, valid_set, test_set], ['TRAIN', 'VALID', 'TEST']):\n  # shape info\n  print(f\"{name:5s} SHAPE:\", set_.shape)","f616de68":"train_set['token_counts'] = train_set['Review'].apply(lambda x: len(word_tokenize(x)))\ntrain_set['sentence_counts'] = train_set['Review'].apply(lambda x: len(sent_tokenize(x)))","da065fee":"train_set.describe()","663fbb81":"train_set.groupby('Rating').aggregate(['mean', 'std'])","464da2bc":"import math\n\n# define the embedding dimension\n# using mean + 1 std tokens \nmaxlen = math.ceil(train_set['token_counts'].mean() + train_set['token_counts'].std())\nmaxlen = int(maxlen \/ 10) * 10\nprint(f\"Defining our Tensor Shape: ({maxlen}, ?)\")","1d3c7a19":"import tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.data import Dataset\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.stem import WordNetLemmatizer\n\nclass PreprocessSequence:\n  def __init__(self, \n               tokenizer: Tokenizer, \n               stemmer: callable, \n               maxlen: int):\n    self.tokenizer = tokenizer\n    self.stemmer = stemmer\n    self.maxlen = maxlen\n\n  @property\n  def word_counts(self):\n    return len(self.tokenizer.word_index) + 1\n\n  def fit(self, X):\n    processed_tokens = []\n    tokens = map(word_tokenize, X)\n    for token in tokens:\n      processed_tokens += list(map(self.stemmer, token))\n\n    # add the tokens into keras tokenizer \n    self.tokenizer.fit_on_texts(processed_tokens)\n    return self\n\n  def transform(self, X):\n    sequences = self.tokenizer.texts_to_sequences(X)\n    return pad_sequences(sequences, maxlen=self.maxlen)\n\n# lemmatizer the word to a proper english root word\nlemmatizer = WordNetLemmatizer()\ntokenizer = Tokenizer(oov_token='UKN')\n\n# create the preprocess instance and fit the train set tokens\npreprocess = PreprocessSequence(tokenizer, lemmatizer.lemmatize, maxlen)\npreprocess.fit(train_set['Review'])\n\n# transform data\ntrain_seq = preprocess.transform(train_set['Review'])\nvalid_seq = preprocess.transform(valid_set['Review'])\ntest_seq = preprocess.transform(test_set['Review'])\n\nprint(\"Train Sequence Shape: \", train_seq.shape)\nprint(\"Valid Sequence Shape: \", valid_seq.shape)\nprint(\"Test  Sequence Shape: \", test_seq.shape)","59940c40":"from keras.layers import Embedding, Conv1D, Dense, AveragePooling1D, Flatten, Dropout, Activation, GlobalAveragePooling1D\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n\ndef build_model(emb_size=200, \n                conv_layers=[(100, 3, 'relu')],  # 100 filters, 3words window, relu  \n                dense_layers=[(200, 0.2, 'relu')], # (neurons, dropout_rate, activation)\n                optimizer='adam'):\n\n  # build model\n  model = keras.models.Sequential()\n\n  # add the embedding layer\n  model.add(Embedding(preprocess.word_counts, emb_size, input_length=maxlen, input_shape=[None]))\n\n  # add conv layers\n  for i, layer in enumerate(conv_layers):\n    model.add(Conv1D(layer[0], layer[1], strides=1, padding='valid', activation=layer[2]))\n    \n    \n    # use global pooling to flatten if last layer \n    if i + 1 == len(conv_layers):\n        model.add(GlobalAveragePooling1D())\n    else:\n        model.add(AveragePooling1D())\n\n  # add dense layer \n  for layer in dense_layers:\n    model.add(Dense(layer[0]))\n    model.add(Dropout(layer[1]))\n    model.add(Activation(layer[2]))\n\n  # output layer\n  model.add(Dense(1))\n\n  # compile model\n  model.compile(loss='mse', optimizer=optimizer, metrics=[keras.metrics.RootMeanSquaredError()])\n  return model","f3a8115e":"# create model\nmodel = build_model()\nepochs = 20\n\n# train model\nhistory = model.fit(train_seq, train_set['Rating'], epochs=epochs, \n          validation_data=(valid_seq, valid_set['Rating']), verbose=1)","3528165c":"import seaborn as sns\n\n# plot leanring curve\ndef plot_learning_curve(history):\n  train_loss = history['root_mean_squared_error']\n  val_loss = history['val_root_mean_squared_error']\n  x_axis = list(map(int, range(1, len(train_loss) + 1)))\n\n  _ = sns.lineplot(x=x_axis, y=train_loss, label='train')\n  g = sns.lineplot(x=x_axis, y=val_loss, label='validate')\n  _ = g.set(xlabel='Epochs', ylabel='rmse', xticks=x_axis)\n\nplot_learning_curve(history.history)","0b213c2f":"from sklearn.model_selection import GridSearchCV\nfrom keras.callbacks import EarlyStopping\n\n\n# 3 words 2 Conv1D \nwindow_3 = build_model(conv_layers=[(200, 3, 'relu'), (200, 3, 'relu')])\nwindow_5 = build_model(conv_layers=[(250, 5, 'relu'), (250, 5, 'relu')])\ndense_2 = build_model(dense_layers=[(200, 0.2, 'relu'), (100, 0.2, 'relu')])\ndense_22 = build_model(dense_layers=[(100, 0.5, 'relu'), (50, 0.5, 'relu')])\n\n# train model\nhistories = {}\nfor i, tune_model in enumerate([window_3, window_5, dense_2, dense_22]):\n    histories[i] = tune_model.fit(train_seq, train_set['Rating'], epochs=epochs, \n                                  validation_data=(valid_seq, valid_set['Rating']), verbose=1)","ce9998d0":"# plotting the learning curves of tuned models \nfor k, item in histories.items():\n    plot_learning_curve(item.history)","a47a8e42":"print(\"CNN (without tuning): \", np.mean(history.history['val_root_mean_squared_error']))\nfor k, item in histories.items():\n    print(\"CNN (with tuning)   : \", np.mean(item.history['val_root_mean_squared_error']))","32c07545":"# evaluate on test set \nprint(\"CNN (without tuning):\", history.model.evaluate(test_seq, test_set['Rating'])[1])\nfor k, item in histories.items():\n    print(\"CNN (with tuning)   : \", item.model.evaluate(test_seq, test_set['Rating'])[1])","993c4954":"# Model Building\nWe will train a 1D ConvNet regressor to predict the review ratings based on the sequences on words. Using CNN, the model has two advantage over using TF-IDF machine learning models.   \n\nFirstly, training an embedding layers could help to capture some semantic relationship based on surrounding words. TF-IDF could only provide some weightings relative to the corpus. \nSecondly, CNN could preserve the words' order of a sentence instead of considering a document as a bag of word. ","2d29b659":"# 1. Data Exploration","25c85052":"The topic for the NLP task is to create a model to automatically assign news category base on its headline and short description. The dataset is downloaded from Kaggle (https:\/\/www.kaggle.com\/rmisra\/news-category-dataset). Categories\/ tag assignment is one of an important NLP task as this could help companies to achieve more efficient Information Retrieval procedure. For example, ","6c322f60":"# Tuning Model","445a8c74":"The Root Mean Squared Error is about 0.8 in the validation set. This indicates that our regression on average estimate <= 1 rating error. ","efbc9e6c":"On average customers gave about 4\/5 reviews score. Review generally is not longer than 1 - 2 sentences and written in just about 100 words. One might be interested if the statistics might be different of different rating.","ae367418":"Testing on test set","275a7b2e":"# 2. Data Preparation \nFrom the descriptive analysis, we could see that reviewers generally wrote about x1 - 2 sentences. In this case, we could use word embedding only instead of sentence embedding. \nTo use word embedding, we need to first find the maximum number of words an review has to create the embedding dimension.\n\nIn terms of the prediction task, we have the flexibility to choose between classification and regression task. Although the rating only has 1 - 5 classes ratings, one could define it as regression task. Consider it as a regression task has several benefits. Firstly, we don't have to worry about imbalance classification as there are much fewer low rating reviews. Secondly, incorrect predictions might still provide some insights. For example, an incorrect regression might produce 3.5 for a 4 star rating instead of a 2."}}