{"cell_type":{"93c89e98":"code","a6f345c6":"code","608487c7":"code","098abfd0":"code","ed77543b":"code","c4424281":"code","bf130e4a":"code","fa20851a":"code","ce25edfe":"code","d7cffc70":"code","fc9e0d23":"code","6babff3d":"code","f4e60bad":"code","7cca226a":"code","ac5b06b4":"code","8604d935":"code","6607359c":"code","061e0ab8":"code","c330350d":"code","34d3c27d":"code","b13b4663":"code","251d9cec":"code","cc8f91ef":"code","5ad21ff6":"code","de92fee8":"code","bc0273f8":"markdown","d2887d27":"markdown","87180b2a":"markdown","bf218f49":"markdown","457ccee8":"markdown","144ce35c":"markdown","6f1e9086":"markdown","96c572c2":"markdown","013b9f0c":"markdown","1f153563":"markdown","44c1bcbe":"markdown","1d2ea839":"markdown","d9ab7952":"markdown","de38d889":"markdown","3c364ad6":"markdown","85d1be87":"markdown","9f1127f3":"markdown","b4613eb1":"markdown","e0c325dd":"markdown","0d1f2a8f":"markdown","139d8e52":"markdown","a01a336d":"markdown","be08311e":"markdown","642fb2ed":"markdown","210ef988":"markdown"},"source":{"93c89e98":"import numpy as np \nimport pandas as pd\n\nfull_train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nfull_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","a6f345c6":"train_data.info()\nprint(\"\\n\\n --- \\n\\n\")\ntest_data.info()","608487c7":"train_data.head()","098abfd0":"test_data.head()","ed77543b":"train_data.isnull().sum()","c4424281":"test_data.isnull().sum()","bf130e4a":"del train_data[\"Cabin\"]\ndel test_data[\"Cabin\"]\n\nprint(\"Cabin columns deleted\")","fa20851a":"# Calculate the average age\nprint(\" - Calculating average ages - \\n\\n\")\naverage_train_age = 0\naverage_test_age = 0\ncount = 0\nfor age in train_data[\"Age\"]:\n    if not pd.isnull(age):\n        count += age\naverage_train_age = count\/train_data[\"Age\"].count()\n\ncount = 0\n\nfor age in test_data[\"Age\"]:\n    if not pd.isnull(age):\n        count += age\naverage_test_age = count\/test_data[\"Age\"].count()\nprint(\"Average training-set age: %s\" % average_train_age)\nprint(\"Average test-set age: %s\" % average_test_age)\n\nprint(\"\\n\\n - Rounding up - \\n\\n\")\n\n# Rounding up (to fit the format of the data, one decimal)...\naverage_train_age = round(average_train_age, 1)\naverage_test_age = round(average_test_age, 1)\n\nprint(\"Average training-set age: %s\" % average_train_age)\nprint(\"Average test-set age: %s\" % average_test_age)\n\n# Assign it to the missing values\n\nprint(\"\\n\\n - Assigning average values - \\n\\n\")\nfor i in range(0, train_data[\"PassengerId\"].count()):\n    if pd.isnull(train_data.iloc[i][\"Age\"]):\n        train_data.at[i, \"Age\"] = average_train_age\nfor i in range(0, test_data[\"PassengerId\"].count()):\n    if pd.isnull(test_data.iloc[i][\"Age\"]):\n        test_data.at[i, \"Age\"] = average_test_age\nprint(\"Done\")","ce25edfe":"print(\"Embarked unique values count: %s\" % train_data[\"Embarked\"].nunique())\n\n# Next, let's try list those values...\nembarked_unique_vals = []\n\nfor loc in train_data[\"Embarked\"]:\n    if not loc in embarked_unique_vals:\n       embarked_unique_vals.append(loc) \nprint(\"Embarked unique values: %s\" % embarked_unique_vals)\n\n# This fits exactly the prediction we had earlier!","d7cffc70":"print(\"Training data: \\n%s\\n\\n\" % train_data[\"Embarked\"].value_counts())","fc9e0d23":"print(\"Before fix, number of null values: %s\" % train_data[\"Embarked\"].isnull().sum())\ntrain_data[\"Embarked\"].fillna(\"S\", inplace=True)  \nprint(\"After fix, number of null values: %s\" % train_data[\"Embarked\"].isnull().sum())","6babff3d":"fare_missing_value_row = 99999\nfor i in range(0, test_data[\"PassengerId\"].count()):\n    if pd.isnull(test_data.at[i, \"Fare\"]):\n        fare_missing_value_row = i\nprint(\"Person: \\n%s\" % test_data.loc[fare_missing_value_row])\nprint(\"\\n\\n - Determining class - \\n\\n\")\nprint(\"Class: %s\" % test_data.at[fare_missing_value_row, \"Pclass\"])","f4e60bad":"test_fare_average = 0 \nfor i in range(0, test_data[\"PassengerId\"].count()):\n    if not pd.isnull(test_data.at[i, \"Fare\"]) and test_data.at[i, \"Pclass\"] == 3:\n        test_fare_average += test_data.at[i, \"Fare\"]\ntest_fare_average \/= test_data[\"PassengerId\"].count()\nprint(\"Unrounded fare average: %s\" % test_fare_average)\n# Round to 4 decimals to fit the data's format\ntest_fare_average = round(test_fare_average, 4)\nprint(\"Rounded fare average: %s\" % test_fare_average)\n# Let's assign it now...\ntest_data.at[fare_missing_value_row, \"Fare\"] = test_fare_average\nprint(\"After fix, number of missing values in Fare (test data): %s\" % test_data[\"Fare\"].isnull().sum())\n","7cca226a":"train_data.isnull().sum()","ac5b06b4":"test_data.isnull().sum()","8604d935":"train_data.info()","6607359c":"test_data.info()","061e0ab8":"# Before the example, remove name as a column.\ndel train_data[\"Name\"]\ndel test_data[\"Name\"]\n\n# So, how important is the gender feature? How many people from each gender survived (at least in the training data)? Let's see.\nfemale_passengers = [train_data.iloc[i] for i in range(0, len(train_data.index)) if train_data.iloc[i][3] == \"female\"]\nmale_passengers = [train_data.iloc[i] for i in range(0, len(train_data.index)) if train_data.iloc[i][3] == \"male\"]\n\nfemale_survivors = [female_passengers[i] for i in range(0, len(female_passengers)) if female_passengers[i][1]==1]\nmale_survivors = [male_passengers[i] for i in range(0, len(male_passengers)) if male_passengers[i][1]==1]\n\nprint(\"Number of male survivors (from training data): %s\" % len(male_survivors))\nprint(\"Number of female survivors (from training data): %s\" % len(female_survivors))\n\n# Let's remember the total number of people in the training set.\n\nprint(\"Total number of passengers on the Titanic (from the training data): %s\" % len(train_data.index))\nprint(\"Total number of female passengers on the Titanic (from the training data): %s\" % len(female_passengers))\nprint(\"Total number of male passengers on the Titanic (from the training data): %s\" % len(male_passengers))\n\n# Thus... survival rate. (Rounded to 1 decimal)\nprint(\"Rounded female survival rate: %s%%\" % (100 * round(len(female_survivors) \/ len(female_passengers), 3)))\nprint(\"Rounded male survival rate: %s%%\" % (100 * round(len(male_survivors) \/ len(male_passengers), 3)))","c330350d":"# Let's drop Ticket first\ndel train_data[\"Ticket\"]\ndel test_data[\"Ticket\"]\n\n# Now let's label encode sex\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\ntrain_data[\"Sex\"] = label_encoder.fit_transform(train_data[\"Sex\"])\ntest_data[\"Sex\"] = label_encoder.transform(test_data[\"Sex\"])\n\n# Let's take a look at how important \"embarked\" is\nS_passengers = [train_data.iloc[i] for i in range(0, len(train_data.index)) if train_data.iloc[i][8] == \"S\"]\nC_passengers = [train_data.iloc[i] for i in range(0, len(train_data.index)) if train_data.iloc[i][8] == \"C\"]\nQ_passengers = [train_data.iloc[i] for i in range(0, len(train_data.index)) if train_data.iloc[i][8] == \"Q\"]\n\nS_survivors = [S_passengers[i] for i in range(0, len(S_passengers)) if S_passengers[i][1]==1]\nC_survivors = [C_passengers[i] for i in range(0, len(C_passengers)) if C_passengers[i][1]==1]\nQ_survivors = [Q_passengers[i] for i in range(0, len(Q_passengers)) if Q_passengers[i][1]==1]\n\nprint(\"Number of Southampton survivors (from training data): %s\" % len(S_survivors))\nprint(\"Number of Cherbourg survivors (from training data): %s\" % len(C_survivors))\nprint(\"Number of Queenstown survivors (from training data): %s\" % len(Q_survivors))\n\n# Let's remember the total number of people in the training set.\n\nprint(\"Total number of passengers on the Titanic (from the training data): %s\" % len(train_data.index))\nprint(\"Total number of Southampton passengers on the Titanic (from the training data): %s\" % len(S_passengers))\nprint(\"Total number of Cherbourg passengers on the Titanic (from the training data): %s\" % len(C_passengers))\nprint(\"Total number of Queenstown passengers on the Titanic (from the training data): %s\" % len(Q_passengers))\n\n# Thus... survival rate. (Rounded to 1 decimal)\nprint(\"Rounded Southampton survival rate: %s%%\" % (100 * round(len(S_survivors) \/ len(S_passengers), 3)))\nprint(\"Rounded Cherbourg survival rate: %s%%\" % (100 * round(len(C_survivors) \/ len(C_passengers), 3)))\nprint(\"Rounded Queenstown survival rate: %s%%\" % (100 * round(len(Q_survivors) \/ len(Q_passengers), 3)))","34d3c27d":"# One-hot encode the \"Embarked\" value\nfrom sklearn.preprocessing import OneHotEncoder\n\none_hot_enc = OneHotEncoder(categories='auto', handle_unknown='ignore', sparse=False) # ignore values that aren't present in the training data, and return values as a numpy array. Though the first parameter is unlikely to be needed.\n\nOH_cols_train = pd.DataFrame(one_hot_enc.fit_transform(train_data.Embarked.values.reshape(-1, 1))) \nOH_cols_test = pd.DataFrame(one_hot_enc.transform(test_data.Embarked.values.reshape(-1, 1)))\n\nOH_cols_train.index = train_data.index\nOH_cols_test.index = test_data.index\n\ndel train_data[\"Embarked\"]\ndel test_data[\"Embarked\"]\n\ntrain_data = pd.concat([train_data, OH_cols_train], axis=1)\ntest_data = pd.concat([test_data, OH_cols_test], axis=1)\n","b13b4663":"del train_data[\"PassengerId\"]\ndel test_data[\"PassengerId\"]","251d9cec":"train_data.head()","cc8f91ef":"test_data.head()","5ad21ff6":"# Seperate the target, survived, from our training data.\n\ny = train_data[\"Survived\"]\ntrain_data = train_data.drop(['Survived'], axis=1)\n\n# Import XGRegressor\nfrom xgboost import XGBRegressor\n\n# Create model and fit\nmodel = XGBRegressor()\nmodel.fit(pd.get_dummies(train_data), y)\n\n# Get predictions\npredictions = model.predict(pd.get_dummies(test_data))\n\n# Round each prediction up or down to get a whole 0, or 1.\npredictions = [round(x) for x in predictions]\nint_predictions = [int(x) for x in predictions]\n\n# Save and submit\noutput = pd.DataFrame({'PassengerId': full_test_data.PassengerId, 'Survived': int_predictions})\noutput.to_csv('my_submission.csv', index=False)","de92fee8":"output.head() #  Output first few predictions","bc0273f8":"**Missing Value Analysis:**\n\nLet's move on to checking for missing values, and understanding how we might deal with these issues. ","d2887d27":"Hmm. Embarked didn't seem to have that much of a noticeable effect. It is true though, that there is far more data for Southampton's survival chances (as there are a far more Southamtpon passengers than any other city) than there are for the other cities.\n\nEither way though, it definitely seems like a useful variable. At any rate, there's nothing stopping us from including it, and our machine learning approach may find something with it we miss.\n\nSo, to make it useable, let's one-hot encode it. ","87180b2a":"Another large challenge in a machine learning approach is figuring out how to deal with the issue of *categorical* variables, or non-numerical variables. As an example, we can't provide someone's name just as a string of characters to the machine learning algortitms we're going to apply today, but we can provide numbers. So! Let's lay out our data again, determine which variables are categorical, and how we'll deal with this.","bf218f49":"We did it! We got his class, 3. So let's average the fares (from the test data) from his *class* only. ","457ccee8":"**Handling Categorical Variables:**","144ce35c":"Great! Column 0 = C, 1 = Q, 2 = S. \n\nOneeeee final change, PassengerID won't be useful in learning, as it's just a counter for each passenger, and not actual data. This isn't technically a categorical variable, but seeing as this is our final bit of preprocessing, I've included it here. ","6f1e9086":"Great, so we see it's the same result for both. S (Southampton) is the most common embarked location. Little issue here, it seems that the data is almost completely Southampton in the training set, and only just over half Southampton for the testing. We'll talk about such things in the skewness chapter though, for now let's focus on the \"average\" value.\n\nIt's quite clear that we'll be selecting S, or Southampton, for our \"average\" value in this case. Let's going ahead and replace all NaN values for embarked with S. ","96c572c2":"# Thank you!\n\nI hope this notebook is useful in some way! This is my first published notebook, so if you have any comments at all, please feel free to leave them down in the comments section.\nThank you again!\n\nSources:\nhttps:\/\/en.wikipedia.org\/wiki\/RMS_Titanic#:~:text=After%20leaving%20Southampton%20on%2010,11%3A40%20p.m.%20ship's%20time.\nhttps:\/\/www.kaggle.com\/c\/titanic\/discussion\/4693","013b9f0c":"Let's remember a few major pieces of information:\n\nSize of the training set: 891 passengers\n\nSize of the test set: 418 passengers\n\nThus the missing values can be broken down as follows...","1f153563":"Next up is **embarked**, which is only missing in a few of the values. \n\n... Hmm. This one is a little difficult. It's a categorical variable, which we're gonna have to deal with later, but for now we have to deal with the fact that it's got a few missing values. Any sort of logical guessing is definitely not going to work... based on the information we have there's no way to logically deduce where a person embarked from. It's *very* unlikely that this feature matters at all... it's just the location someone boarded after all. But certain factors, such as more familiarity with the ship's layouts, or common sounds, might've helped a passenger survive in an odd incident. \n\nLet's d a little digging on the *embarked* feature, see if we can understand it a little better.\n\nFirst off, number of unique values.\n\n(We'll only concern ourselves with the training data, as no values are missing for embarked from the testing data). ","44c1bcbe":"**Test:**\n\nAge: 86\/418 = approx 20.6%\n\nFare: 1\/418 = approx 0.24%\n\nCabin: 327\/418 = approx 78.2%","1d2ea839":"Great! We've fixed all the missing values! Observe...","d9ab7952":"There's an issue here. The cabin value is obviously missing almost all of the time (upwards of 80% of the values are missing). **Yet!** ... the cabin value is ***extremely important***! The number itself, without an extremely in-depth knowledge of the Titanic's layout, is fairly useless for us to analyze... but the first letter indicates the *deck* that the cabin was located on, as presented by the image below.\n\n![Titanic_cutaway.png](attachment:Titanic_cutaway.png)\n\nIt is a shame then, that the cabin value seems to be extremely useful, but not really accessible en masse. Yet! *PClass* will serve our purposes here, as the class one traveled in directly corresponds to the deck one would be assigned to. To increase the accuracy of this model, using cabin values may be added as an option, but seeing as so many values are missing, we will drop the cabin column. ","de38d889":"Great, so we can see right away that Name, Sex, Ticket, and Embarked, are categorical variables. Cabin is as well, but we've already removed it from consideration.\nLet's start off easy. For an inital model, patterns in name, and survival, aren't likely to exist... so to save time and effort, we're just going to remove Name as a column.\n\nSex is definitely going to be a different story though, as it can be shown to play a much larger role in who survived, and who didn't (as women and children would be evacuated first). As just an example of that...","3c364ad6":"**Training:**\n\nAge: 177\/891 = approx 20%\n\nCabin: 687\/891 = approx 77.1%\n\nEmbarked: 2\/891 = approx 0.22%","85d1be87":"Great! So it's exactly as we predicted earlier, in the feature description. Let's see if we can get an \"average\" value... or which value, of S, C, Q, occurs the most.","9f1127f3":"# Starting Off...\nImporting the data, and necessary libraries. Welcome to my notebook!","b4613eb1":"It's only a small fix, in this case, but it's all that's needed. \n\nAlright, final issue! Let's fix the missing value for fare in the test data! Let's just average the fares, and assign that. It's a simple fix, but it's only a single missing value.\n\nBut! Let's try and do it with a little intelligence. Not only the machines should do the learning, huh? Let's figure out the person's assigned class first.","e0c325dd":"**Features of the Dataset:**\n\nSo let's start by training to view and understand the features of the dataset. We have the following features. ","0d1f2a8f":"Okay, so gender is needed. We won't one-hot encode that, as it seems to be a little too much trouble for a variable that can simply be encoded as binary, so we'll assign 1=male, 0=female. \n\nTicket has too many possible values, and many aren't very understandable. Perhaps this could be revisisted to improve model performance, but for now we're just going to drop Ticket.\n\nEmbarked might be important. It will be one-hot encoded, but, for now, let's just handle the above statements, and then .info() the data, and then in the following box we'll take a look at how much \"embarked\" had an effect.","139d8e52":"# Machine Learning (XGBoost):\n\nWe're here! Finally, time to start making predictions. We've analyzed (a little) our data, and set up our dataset for usage. The only step in between us and predictions now is the fact that we haven't seperated the target, survived, from our training data. We'll do that, and start training!","a01a336d":"# Introduction:\n\nAs said before, welcome to my notebook! Today we'll be analyzing one's likelihood for survival on the Titanic, based on features of one's travel plan. Despite the grim nature of this project, considering the tragedy it is based around, this will be an attempt at a thorough and professional look at the subject at hand.\n\nSo, let's start with the basics. What's our *question*?\n\n**Question: Based on the features in the provided dataset, can we accurately predict one's survival on the Titanic? If so, how accurately?**\n**MOREOVER: How can we do this?**\n\nSo... it's clear then that we will be using a *machine learning* approach in answering this question, as we will be attempting to build an algorithm to learn from data for this particular example. Yet we must first understand the data we are using! \n\nSo! Let's move on to some interactive data analysis, shall we?","be08311e":"# Interactive Data Analysis:\n\nThis is going to be a large and broad section, so let's start by laying out a *table of contents*:\n\n**1. Features of the datasets**\n\n**2. Missing Value Analysis**\n\n**3. Categorical Variable Handling**\n","642fb2ed":"**Passenger-Id:** A simple number to keep track of each passenger, starting at 1. It seems the split from training to testing data occurs between passenger 891-892. \n\n**Pclass:** What class the passenger was a part of, ranging from 1st to 3rd (according to research on the Titanic's classes). It should be noted that 3rd class enjoyed the least preferable locations on the ship... towards the bottom and front\/back of the ship. As such, it would be unsurprising if the third class had the highest moratlity rate, though that trend will be looked for later.\n\n**Name:** Fairly self-describing... Last name, first name. Sometimes, for women, the women's full name (and maiden name) is given in parantheses after the married name. Ex: Mrs. Alexander (Helga E Lindqvist). \n\n**Sex:** Again, self-describing... 'male' or 'female', specified as such. \n\n**Age:** Number denoting the person's age. *Surprisingly*, this number is a float, and seems to denote fractions of ages as well.\n\n**SibSp:** Detailed in the dictionary, number of siblings + spouses aboard.\n\n**Parch:** Also detailed in the dictionary, number of parents + children aboard.\n\n**Ticket:** These seem to be most likely ticket numbers for each passenger. Essentially another way of keeping track of each passenger individually.\n\n**Fare:** Amount of money payed to embark. These numbers likley directly correspond to class, and other factors (perhaps number of parents, children, siblings, spouses... where he\/she embarked from... etc.). \n\n**Cabin:** Most of these values seem to be null... As an example, in the training data, there are 891 total values (as evidenced by the passenger IDs), and only 204 cabin values. These values, where they exist, seem to consist of the form \\[Letter\\]\\[Set of three numbers\\]. There are some passengers with multiple cabins listed. \n\n**Embarked:** Single letters listing the locations in which the passenger embarked the ship from. The ship, according to research, departed from Southhampton, stopped at Cherbourg, and Queenstown (now Cobh), and then departed for New York. Q likely corresponds to Queenstown, S=Southhampton, C=Cherbourg. ","210ef988":"Let's also try and figure out how to handle the *age*, *embarked*, and *fare* missing values in both the training and testing data. Age is a little difficult as, by the cacluations done above, approximately 20% of the training data's rows are missing the age values. Yet, if we think about the time, it's likely that the elderly would be evacuated first (and, though grim, it's likely that the elderly in, say, 3rd class, would've had a hard time reaching the lifeboats). Thus, it's clear that age is an important feature... or at least one that should be included in an inital model for testing purposes. So! We must figure out how to deal with the missing values.\n\nIn this case... for an inital model, and seeing as the age category is numeric, it's likely most fair to say that we can probably get by with assigning the average age... so let's calculate and assign that."}}