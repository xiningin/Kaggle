{"cell_type":{"2d2b38ca":"code","ead7db5d":"code","bf2097fe":"code","2cf6447a":"code","a5a50176":"code","54b9c284":"code","33c2d9f4":"code","884830c6":"code","5de8f417":"code","3a24f405":"code","54aa6c38":"code","0340029f":"code","1263a6c0":"code","6027b689":"code","c75fae33":"markdown"},"source":{"2d2b38ca":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\nimport regex as re\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.backend import eval\nfrom keras.optimizers import Adam\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv1D,MaxPooling1D\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ead7db5d":"df=pd.read_csv('..\/input\/nyc-jobs.csv')\npd.set_option('display.max_columns', None)\ndf=df.drop(['Job ID', 'Agency', 'Posting Type', '# Of Positions', 'Business Title',\n       'Civil Service Title', 'Title Code No', 'Level', 'Job Category',\n       'Full-Time\/Part-Time indicator' , 'Additional Information', 'To Apply', 'Hours\/Shift', 'Work Location 1',\n       'Recruitment Contact', 'Residency Requirement', 'Posting Date',\n       'Post Until', 'Posting Updated', 'Process Date'], axis=1)\ndf=df.reset_index(drop=True)\nprint(df.columns, df.tail(7))\n","bf2097fe":"print(df.info())","2cf6447a":"#df['Preferred Skills'].dropna(inplace=True)\n#print(len(df['Preferred Skills']))\n#df[['Preferred Skills'].fillna('Unspecified', inplace=True)\nX=df['Job Description']\nohe=OneHotEncoder()\ny=df[['Salary Range From']].astype('str')\nprint(y.info())","a5a50176":"import string\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\ndef object_to_list(text):\n    '''\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Return the cleaned text as a list of words\n    '''\n    nopunc = [char for char in text if char not in string.punctuation]#removing puctuations\n    nopunc = ''.join(nopunc)\n    \n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]#removing non english words\n","54b9c284":"from sklearn.feature_extraction.text import CountVectorizer\n# next we need to vectorize our input variable (X)\n#we use the count vectoriser function and the analyser we use is the above lines of code\n# this should return a vector array\nX = CountVectorizer(analyzer=object_to_list).fit_transform(X)","33c2d9f4":"print(X[6].split())","884830c6":"from sklearn.model_selection import train_test_split\n#from sklearn.preprocessing import OneHotEncoder\n#ohe=OneHotEncoder()\n#ohe.fit_transform(X)\nx_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=0)","5de8f417":"from sklearn.ensemble  import RandomForestClassifier\nfirst_model=RandomForestClassifier()\nfirst_model.fit(x_train, y_train)\nprint(first_model.score(x_train, y_train))\n","3a24f405":"from sklearn.metrics import confusion_matrix, classification_report\npredicted=first_model.predict(x_test)\nprint(confusion_matrix(y_test, predicted))\nprint('\\n')\nprint(classification_report(y_test, predicted))#we see the precision, recall, f1-score and supprt for \n# predicted values here","54aa6c38":"def clean_document(doco):\n    punctuation = string.punctuation\n    punc_replace = ''.join([' ' for s in punctuation])\n    doco_link_clean = re.sub(r'http\\S+', '', doco)\n    doco_clean_and = re.sub(r'&\\S+', '', doco_link_clean)\n    doco_clean_at = re.sub(r'@\\S+', '', doco_clean_and)\n    doco_clean = doco_clean_at.replace('-', ' ')\n    doco_alphas = re.sub(r'\\W +', ' ', doco_clean)\n    trans_table = str.maketrans(punctuation, punc_replace)\n    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')])\n    doco_clean = doco_clean.split(' ')\n    p = re.compile(r'\\s*\\b(?=[a-z\\d]*([a-z\\d])\\1{3}|\\d+\\b)[a-z\\d]+', re.IGNORECASE)\n    doco_clean = ([p.sub(\"\", x).strip() for x in doco_clean])\n    doco_clean = [word.lower() for word in doco_clean if len(word) > 2]\n    doco_clean = ([i for i in doco_clean if i not in stop])\n#     doco_clean = [spell(word) for word in doco_clean]\n#     p = re.compile(r'\\s*\\b(?=[a-z\\d]*([a-z\\d])\\1{3}|\\d+\\b)[a-z\\d]+', re.IGNORECASE)\n    doco_clean = ([p.sub(\"\", x).strip() for x in doco_clean])\n#     doco_clean = ([spell(k) for k in doco_clean])\n    return doco_clean","0340029f":"reviews=df['Job Description']\nreview_cleans = [clean_document(doc) for doc in reviews];\nsentences = [' '.join(r) for r in review_cleans ]\nprint(reviews.shape)","1263a6c0":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)\ntext_sequences = np.array(tokenizer.texts_to_sequences(sentences))\nsequence_dict = tokenizer.word_index\nword_dict = dict((num, val) for (val, num) in sequence_dict.items())\nprint(text_sequences)","6027b689":"#print(sequence_dict)","c75fae33":"To vectorize our string input we will use sklearn CounterVectorizer. This algorith makes object data like a vector which then help as for applying machine learning algorith on it. for more information about CounterVectorizer visit:   http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html"}}