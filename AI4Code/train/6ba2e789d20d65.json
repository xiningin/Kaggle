{"cell_type":{"6a689171":"code","e2651065":"code","1c5b3462":"code","078f9bcf":"code","bc92e300":"code","51144bf0":"code","3257dd73":"code","b4aab1be":"code","b992bfee":"code","1ed30b3f":"code","fe25ef63":"code","94412578":"code","c5754695":"code","12094098":"code","880281d8":"code","73c445aa":"code","e45eaafc":"code","a353050e":"code","a8e0e4b6":"code","f7722739":"code","f172d2b7":"code","d2e619cd":"code","ab91277e":"code","b2d93b56":"code","590bd04a":"code","8d4c6c8c":"code","5959d58e":"code","27e2bb64":"code","a0ab39a9":"code","f6e90c89":"markdown","6fe495e1":"markdown","cb1bff8d":"markdown","278ab38b":"markdown","9eafa61d":"markdown","65805f7e":"markdown","2b5c98c8":"markdown","5e558ae9":"markdown","a91f2458":"markdown","825b3ddc":"markdown","687f5cbf":"markdown","5987d88f":"markdown","2af1efaf":"markdown","fd320de5":"markdown","9d177683":"markdown","dd03260d":"markdown","8c13d9df":"markdown","87df0f90":"markdown","b2475019":"markdown","d9ab45a7":"markdown","77483101":"markdown","60b27cbf":"markdown","5785f73b":"markdown","dc816d78":"markdown","6e7b2459":"markdown"},"source":{"6a689171":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","e2651065":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","1c5b3462":"import pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","078f9bcf":"pp.ProfileReport(train_df, title = 'Pandas Profiling report of \"Train\" set', html = {'style':{'full_width': True}})","bc92e300":"pp.ProfileReport(test_df, title = 'Pandas Profiling report of \"Test\" set', html = {'style':{'full_width': True}})","51144bf0":"train_df=train_df.drop(\"PassengerId\",axis=1)\ntrain_df=train_df.drop(\"Name\",axis=1)\ntrain_df=train_df.drop(\"Ticket\",axis=1)\ntrain_df=train_df.drop(\"Cabin\",axis=1)","3257dd73":"train_df.head()","b4aab1be":"test_df=test_df.drop(\"Name\",axis=1)\ntest_df=test_df.drop(\"Ticket\",axis=1)\ntest_df=test_df.drop(\"Cabin\",axis=1)","b992bfee":"test_df.head()","1ed30b3f":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\ntrain_df[\"Age\"].isnull().sum()","fe25ef63":"common_value = 'S'\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(common_value)","94412578":"test_df = test_df.fillna(test_df['Fare'].mean())","c5754695":"train_df.info()","12094098":"test_df.info()","880281d8":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df[\"Sex\"]= le.fit_transform(train_df[\"Sex\"])\nprint(train_df[\"Sex\"])","73c445aa":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntest_df[\"Sex\"]= le.fit_transform(test_df[\"Sex\"])\nprint(test_df[\"Sex\"])","e45eaafc":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df[\"Embarked\"]= le.fit_transform(train_df[\"Embarked\"])\nprint(train_df[\"Embarked\"])","a353050e":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntest_df[\"Embarked\"]= le.fit_transform(test_df[\"Embarked\"])\nprint(test_df[\"Embarked\"])","a8e0e4b6":"train_df.head()","f7722739":"test_df.head()","f172d2b7":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n''' OR\nX_train = train_df[:, 0:-1]\nY_train = train_df[:, -1]\nX_test  = test_df[:, 1:]\n'''","d2e619cd":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","ab91277e":"print(X_train)","b2d93b56":"print(Y_train)","590bd04a":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, Y_train)\nY_pred = classifier.predict(X_test)","8d4c6c8c":"''' \nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, Y_train)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, Y_train)\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, Y_train)\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, Y_train)\n\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, Y_train)\n\n'''","5959d58e":"from sklearn.metrics import accuracy_score\nclassifier.score(X_train, Y_train)\nclassifier = round(classifier.score(X_train, Y_train) * 100, 2)\nclassifier","27e2bb64":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })","a0ab39a9":"submission.to_csv('submission.csv', index=False)","f6e90c89":"# If you liked my work then please upvote, Thank you.","6fe495e1":"### 'Fare' in Test set","cb1bff8d":"## Spliting the Train & Test datasets","278ab38b":"## Dropping unnecessary columns","9eafa61d":"# <font color='blue'>Part 1 - Data Preprocessing <\/font>","65805f7e":"# <font color='blue'>Part 2 - Training the Classification model<\/font>","2b5c98c8":"# <font color='blue'>Titanic:<\/font> I Got the best results by using Kernel SVM with 84% accuracy\n---\n \n* **Part 1 - Data Preprocessing**\n   1. Importing libraries\n   2. Importing the dataset\n   3. Dataset information (Pandas Profiling)\n   4. Dropping unnecessary columns\n      - \"Train\" set\n      - \"Test\" set\n   5. Taking care of misssing data\n      - \"Age\" in Train & Test set\n      - \"Embarked\" in Train set\n      - \"Fare\" in Test set\n      - Updated info()\n   6. Encoding categorical data\n      - \"sex\" in Train & Test set\n      - \"Embarked\" in Train & Test set\n      - Updated head()\n   7. Spliting the Train & Test datasets\n   8. Feature Scaling   \n* **Part 2 - Training the Classification model**\n   1. Kernel SVM\n   2. Other algorithms\n   3. Accuracy score  \n* **Part 3 - Creating a submission.csv**","5e558ae9":"## Importing libraries","a91f2458":"### 'Embarked' in Train & Test set","825b3ddc":"## Other algorithms","687f5cbf":"## Dataset information (Pandas Profiling)","5987d88f":"## Accuracy score","2af1efaf":"### Updated info()","fd320de5":"### 'Test' set","9d177683":"## Encoding categorical data ","dd03260d":"### 'sex' in Train & Test set","8c13d9df":"## Feature Scaling","87df0f90":"### 'Age' in Train & Test set","b2475019":"## Taking care of misssing data","d9ab45a7":"### 'Train' set","77483101":"## Importing the dataset","60b27cbf":"## Kernel SVM","5785f73b":"### Updated head()","dc816d78":"### 'Embarked' in Train set","6e7b2459":"# <font color='blue'>Part 3 - Creating a submission.csv<\/font>"}}