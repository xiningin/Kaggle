{"cell_type":{"44e685e1":"code","c02c3068":"code","01f4d9a5":"code","0bddb101":"code","8d0da5d3":"code","aca80e7b":"code","16846d19":"code","40bb9ed4":"code","3496fa83":"code","469a4499":"code","3f311344":"code","5036e5b1":"code","acaaf27e":"code","8be6130b":"code","2f515e5f":"code","a146e74f":"code","76a40df3":"code","96e43596":"code","71d1f4c9":"code","59854310":"code","1962dbf3":"code","a7d89778":"code","187727c6":"code","da9d29ca":"code","8a5a26b5":"code","e3b82b2c":"code","714540ab":"code","1a9f57ca":"code","bfd98b81":"code","77b69df2":"code","51f13e0e":"code","2927c5cc":"code","acd73000":"code","8f9e2723":"code","2e6296ca":"code","b5cf0e0d":"code","e46754ae":"code","4e2db872":"code","f975f215":"code","527f7418":"code","48c62c33":"code","e12aea07":"code","c3272887":"code","304c0a2a":"code","9e7d0bb6":"code","5710cdb8":"code","0081f814":"code","604b2e96":"code","7237c1a8":"code","685e485e":"code","e1b89f1a":"code","cd0ead66":"code","c2ce39fd":"code","86d2f2f5":"code","9eb91226":"code","04beb07f":"markdown","9a90ac9f":"markdown","d243b669":"markdown","ed63b485":"markdown","0712e3ae":"markdown","2a1afa5d":"markdown","0e47b88a":"markdown","6fc033ca":"markdown","17c0e320":"markdown","33bf01df":"markdown","25592854":"markdown","fdb82bad":"markdown","42f40500":"markdown","79102e27":"markdown","c8cb5b78":"markdown","ab3aa7f8":"markdown","06b048bc":"markdown","e1333451":"markdown","198a6b14":"markdown","2d9f6006":"markdown","2fd97ed7":"markdown","74132af6":"markdown","75dea167":"markdown","d2f1e114":"markdown","a0e98ed3":"markdown","282a15bf":"markdown","5a0d62c3":"markdown","a3e2ba8a":"markdown","f10a2912":"markdown","163d653d":"markdown","0b3f7853":"markdown","8fbafb6d":"markdown","8729a91e":"markdown","1c693587":"markdown","75980fab":"markdown","e705c084":"markdown","43a1bbc0":"markdown","f15ec160":"markdown","6e75e69b":"markdown","3b14b854":"markdown","fed0e1c4":"markdown","f7a80473":"markdown","c6142a47":"markdown","c9c8b7d5":"markdown","ab17a3ee":"markdown","8750dd51":"markdown","b771b9f0":"markdown","1049b1b4":"markdown","31155c1f":"markdown","a6b22858":"markdown","eae343d7":"markdown","7f74589f":"markdown"},"source":{"44e685e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c02c3068":"cell_df=pd.read_csv(\"\/kaggle\/input\/cell_samples.csv\")","01f4d9a5":"# There are 699 records and for each record we have multiple parameters that is measured.\n# Public Source - https:\/\/s3-api.us-geo.objectstorage.s... \"\n# we are using a dataset that has a 9 predictors in each record, 699 records ","0bddb101":"cell_df.head()","8d0da5d3":"cell_df.tail()","aca80e7b":"# Let us drop the ID column as it doesnot influence the output \"class\".\ncell_df.drop('ID',axis=1,inplace=True)","16846d19":"cell_df.shape","40bb9ed4":"#Missing Or Null data points\n\ncell_df.isnull().sum()\ncell_df.isna().sum()","3496fa83":"cell_df.count()","469a4499":"cell_df.columns","3f311344":"col_names = ['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit', 'Class']\nfor x in col_names: \n    print(cell_df[x].nunique())","5036e5b1":"# Let us check whether the dataset is a balanced or imbalanced one.\ntarget_count = cell_df.Class.value_counts()\nprint('Benign:', target_count[2])\nprint('Malignant:', target_count[4])","acaaf27e":"458\/241","8be6130b":"cell_df.dtypes","2f515e5f":"# Identify the unwanted rows\n\ncell_df = cell_df[pd.to_numeric(cell_df['BareNuc'],errors='coerce').notnull()]\ncell_df['BareNuc']=cell_df['BareNuc'].astype('int')","a146e74f":"cell_df.dtypes","76a40df3":"cell_df.shape","96e43596":"cell_df.describe().transpose()","71d1f4c9":"cell_df.hist(figsize=(20,12))\nplt.show()","59854310":"cell_df.columns","1962dbf3":"# Taking out the predictors and predicted variables seperately for the further.\n\nfeature_df=cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize',\n       'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\nx=np.asarray(feature_df)\ny=np.asarray(cell_df['Class'])\n","a7d89778":"x[0:5]","187727c6":"# Divide the data as train and test dataset\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=4)\nx_train.shape","da9d29ca":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(C=100,random_state=0)\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)\ncorrect = (y_test == y_pred).sum()\nincorrect = (y_test != y_pred).sum()\naccuracy = correct \/ (correct + incorrect) * 100\n\nprint('\\nPercent Accuracy: %0.1f' %accuracy)","8a5a26b5":"prediction = pd.DataFrame()\nprediction['actual'] = y_test\nprediction['predicted'] = y_pred\nprediction['correct'] = prediction['actual'] == prediction['predicted']\n\nprint ('\\nDetailed results for first 20 tests:')\nprint (prediction.head(20))\n","e3b82b2c":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_logistic=confusion_matrix(y_test,y_pred)\nprint(c_logistic)\nAccuracy_logistic=sum(np.diag(c_logistic))\/(np.sum(c_logistic))\nAccuracy_logistic","714540ab":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","1a9f57ca":"from sklearn.naive_bayes import GaussianNB\nclassifier_naive=GaussianNB()\nclassifier_naive.fit(x_train, y_train)\ny_predict=classifier_naive.predict(x_test)","bfd98b81":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_naive=confusion_matrix(y_test,y_predict)\nprint(c_naive)\nAccuracy_naive=sum(np.diag(c_naive))\/(np.sum(c_naive))\nAccuracy_naive","77b69df2":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict))","51f13e0e":"#modelling SVM\n\nfrom sklearn import svm\nclassifier_svm=svm.SVC(kernel='linear',gamma='auto',C=1)\nclassifier_svm.fit(x_train,y_train)\ny_predict=classifier_svm.predict(x_test)","2927c5cc":"# Confusion matrix and Accuracy of our model.\n\nfrom sklearn.metrics import confusion_matrix\nc_svm=confusion_matrix(y_test,y_predict)\nprint(c_svm)\nAccuracy_svm=sum(np.diag(c_svm))\/(np.sum(c_svm))\nAccuracy_svm","acd73000":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict))","8f9e2723":"from sklearn import svm\nclassifier_svmk=svm.SVC(kernel='poly',gamma='auto',C=1)\nclassifier_svmk.fit(x_train,y_train)\ny_predict=classifier_svmk.predict(x_test)","2e6296ca":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_svmk=confusion_matrix(y_test,y_predict)\nprint(c_svmk)\nAccuracy_svmk=sum(np.diag(c_svmk))\/(np.sum(c_svmk))\nAccuracy_svmk","b5cf0e0d":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict))","e46754ae":"# modelling Knn Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# nothing but we are using euclidean distance\nclassifier_knn=KNeighborsClassifier(n_neighbors=6,metric=\"minkowski\",p=2)\nclassifier_knn.fit(x_train,y_train)\ny_predict=classifier_knn.predict(x_test)","4e2db872":"# lets see the best value of k for which the model is predicting with high accuracy.\n\nn=[]\nacc=[]\n\nfor i in range(1,27):\n    classifier_knn_trail=KNeighborsClassifier(n_neighbors=i,metric=\"minkowski\",p=2)\n    classifier_knn_trail.fit(x_train,y_train)\n    c_knn_trail=confusion_matrix(y_test,classifier_knn_trail.predict(x_test))\n    acc.append(sum(np.diag(c_knn_trail))\/(np.sum(c_knn_trail)))\n    n.append(i)\nn=np.array(n)\nacc=np.array(acc)\nplt.plot(n,acc)\nplt.show()","f975f215":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_knn=confusion_matrix(y_test,y_predict)\nprint(c_knn)\nAccuracy_knn=sum(np.diag(c_knn))\/(np.sum(c_knn))\nAccuracy_knn","527f7418":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict))","48c62c33":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)# for gini 0.948905109489051\nclassifier_tree.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_predict = classifier_tree.predict(x_test)","e12aea07":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_tree=confusion_matrix(y_test,y_predict)\nprint(c_tree)\nAccuracy_tree=sum(np.diag(c_tree))\/(np.sum(c_tree))\nAccuracy_tree","c3272887":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict))","304c0a2a":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ensemble = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_ensemble.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_predict = classifier_ensemble.predict(x_test)","9e7d0bb6":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_ensemble=confusion_matrix(y_test,y_predict)\nprint(c_ensemble)\nAccuracy_ensemble=sum(np.diag(c_ensemble))\/(np.sum(c_ensemble))\nAccuracy_ensemble","5710cdb8":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict))","0081f814":"!pip install xgboost","604b2e96":"# Fitting the XGBoost to the training set\nfrom xgboost import XGBClassifier\nclassifier_xg=XGBClassifier()\nclassifier_xg.fit(x_train,y_train)\n\n# Predicting the test results\ny_predictor= classifier_xg.predict(x_test)","7237c1a8":"# Confusion Matrix\n#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_xg=confusion_matrix(y_test,y_predictor)\nprint(c_xg)\nAccuracy_xg=sum(np.diag(c_xg))\/(np.sum(c_xg))\nAccuracy_xg","685e485e":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predictor))","e1b89f1a":"d={'Accuracy(%)' : [97.08,94.89,96.35,97.08,98.54,96.35,97.81,97.91],'Precision' : [0.96,0.94,0.95,0.96,0.98,0.96,0.97,0.97],'recall' : [0.98,0.96,0.97,0.98,0.99,0.96,0.98,0.98],'F1 Score' : [0.97,0.95,0.96,0.97,0.98,0.96,0.98,0.98]}\nModel_metrics = pd.DataFrame(d,index=['Logistic Regression','Naive Bayes','Svm-Linear','Svm-Polynomial','KNN','Decison Tree','Random Forest','XGBoost'])","cd0ead66":"Model_metrics","c2ce39fd":"y_predict=classifier_knn.predict(np.array([[1,2,2,5,3,4,6,4,8]]))\nprint(y_predict)","86d2f2f5":"y_predict=classifier_ensemble.predict(np.array([[1,2,2,5,3,4,6,4,8]]))\nprint(y_predict)","9eb91226":"y_predict=classifier_xg.predict(np.array([[1,2,2,5,3,4,6,4,8]]))\nprint(y_predict)","04beb07f":"# XGBoost","9a90ac9f":"The count of each column is 699 which suggests there are no missing values.","d243b669":"The metrics for this model are as follows\nAccuracy : 97.08 %,\nPrecision : 96%,\nRecall: 98%,\nF1 Score: 97%.","ed63b485":"My xg boost model is also predicting the same as benign.","0712e3ae":"# Dropping the unwanted columns.","2a1afa5d":"# Data Visualisation","0e47b88a":"# Naive Bayes Model","6fc033ca":"# Loading the dataset.","17c0e320":"No Missing values found in the dataset.","33bf01df":"We can see KNN performing well and then comes random forest and XGBoost which have more accuracy and F1 Score as well.","25592854":"# Importing the Packages","fdb82bad":"# About the dataset.\n\nWhen pathologists examine FNA(fine needle aspirate) tissues samples in breast cancer diagnosis,they consider the nine attributes.each of the attribute is assigned to number from 1-10 by the pathologists.the larger the number the\ngreater the likelihood of malignancy.no single measurement can be used to determine whether it is benign or malignant.","42f40500":"# Kernal SVM. Kernal taken is polynomial kernal.","79102e27":"# Decision tree","c8cb5b78":"Bare Buc is an object type which is not a numeric value. So, we cannot apply mathematical operations on this column.\nSo, Here lets simply removethe non numeric data.\nFor a particular row, and in that row a particular column( BareNuc) if values are non numeric. We could simply remove it, as synthetic data would not contribute to right decisions.","ab3aa7f8":"It is a balanced dataset. the proportion of benign and malignant is almost 2:1.","06b048bc":"# Conclusion\nBreast Cancer has become the foremost cause of death worldwide for womens. The most successful way to reduce\ncancer deaths is to detect it earlier. Many people avoid cancer screening due to the cost involved in taking numerous\ntests for diagnosis.This prediction system may provide easy and a cost effective way for screening cancer and may play\na significant role in earlier diagnosis process for different types of cancer and provide effective preventive\napproach. ","e1333451":"\u2018clump thickness\u2019 is evenly distributed to some extent. All other variables are skewed to the right.","198a6b14":"Clump thickness indicates that radius was computed by averaging the length of radial line segments from the center of\nthe nuclear mass to each of the points of the nuclear border. For cell size, perimeter was measured as the distance\naround the nuclear border which is considered to be uniform. For measuring the cell shape, area is measured by\ncounting the number of pixels in the interior of the nuclear border and adding one-half of the pixels on the perimeter.\nMarginal adhesion is measured bycombining the perimeter and area to give a measure of the compactness of the cell\nnuclei","2d9f6006":"All the variables are Categorical variables.","2fd97ed7":"The metrics for this model are as follows\nAccuracy : 97.81 %,\nPrecision : 97%,\nRecall: 98%,\nF1 Score: 98%.","74132af6":"# SVM Model","75dea167":"My random forest model is also predicting the same as benign.","d2f1e114":"# Prediction using trained model\nThe trained models is used to predict a particular case :- \u2018clump thickness\u2019 = 1, \u2018uniformity of cell size\u2019 = 2, \u2018uniformity of cell shape\u2019 = 2, \u2018marginal adhesion\u2019 = 5 , \u2018single epithelial cell size\u2019 = 3 , 'bare nuclei' = 4 , \u2018bland chromatin\u2019 = 6, \u2018normal nucleoli\u2019 = 4, \u2018mitosis\u2019 = 8.","a0e98ed3":"#classify cells to whether the samples are benign(mild state)=2 or malignant(evil state)=4","282a15bf":"The output variable \u2018class\u2019 is discrete and takes two values :- 2 (Benign) and 4 (Malignant).\nThe mean of \u2018class\u2019 is closer to 2 indicating there are more benign cases. \nThe minimum and maximum value of all input variables are 1 and 10 respectively.","5a0d62c3":"# Problem Statement\n\nBreast Cancer is one of the leading cancer developed in many countries including India.With early diagnosis 97% women can survive for more than 5 years. Statistically, the death toll due to this disease has increased drastically in last few decades. If we recognise early, we can take the necesaary action which results in less death toll. Hence,apart from medicinal solutions some Data Science solution needs to be integrated for resolving the death causing issue. ","a3e2ba8a":"# Summary","f10a2912":"# Model Evaluation Metrics","163d653d":"BareNuc variable is an object data type, so need to convert it into integer data type.","0b3f7853":"# Checking whether the dataset is a balanced or not.","8fbafb6d":"The metrics for this model are as follows\nAccuracy : 97.81 %,\nPrecision : 97%,\nRecall: 98%,\nF1 Score: 98%.","8729a91e":"The metrics for this model are as follows\nAccuracy : 96.35 %,\nPrecision : 96%,\nRecall: 96%,\nF1 Score: 96%.","1c693587":"# Random forest","75980fab":"The metrics for this model are as follows\nAccuracy : 96.35 %,\nPrecision : 95%,\nRecall: 97%,\nF1 Score: 96%.\n","e705c084":"So, BareNuc  is converted to integer data type.","43a1bbc0":"* \u2018id\u2019, \u2018clump thickness\u2019, \u2018uniformity of cell size\u2019, \u2018uniformity of cell shape\u2019, \u2018marginal adhesion\u2019, \u2018single epithelial cell size\u2019, \u2018bare nuclei\u2019, \u2018bland chromatin\u2019, \u2018normal nucleoli\u2019, \u2018mitosis\u2019 are the variables used to predict the output \u2018class\u2019.","f15ec160":"# Logistic Regression Model","6e75e69b":"The metrics for this model are as follows\nAccuracy : 97.08 %,\nPrecision : 96%,\nRecall: 98%,\nF1 Score: 97%.","3b14b854":"# Splitting the dataset.","fed0e1c4":"The metrics for this model are as follows\nAccuracy : 98.54 %,\nPrecision : 98%,\nRecall: 99%,\nF1 Score: 98%.","f7a80473":"The metrics for this model are as follows\nAccuracy : 94.89 %,\nPrecision : 94%,\nRecall: 96%,\nF1 Score: 95%.","c6142a47":"# Checking for missing values","c9c8b7d5":"# Training My Models","ab17a3ee":"Let us find the optimum value for K. Here we have taken the loop iteration till 27 as per thumb rule optimum k value is always Square root of number of records. so square root of 699 rounds of to 26. So that is why it is taken.","8750dd51":"# Objective:\nTo Build a breast cancer classifier on the dataset that can be accurately classify as benign and malignant.","b771b9f0":"The predicted value of \u2018class\u2019 is 2 which suggests it is a benign tumor.","1049b1b4":"# Checking for Categorical variables","31155c1f":"# Taking out the predictors and predicted variables seperately","a6b22858":"This is the description of the features of the dataset.\n\n1. Sample code number: id number\n2. Clump Thickness: 1 - 10\n3. Uniformity of Cell Size: 1 - 10\n4. Uniformity of Cell Shape: 1 - 10\n5. Marginal Adhesion: 1 - 10\n6. Single Epithelial Cell Size: 1 - 10\n7. Bare Nuclei: 1 - 10\n8. Bland Chromatin: 1 - 10\n9. Normal Nucleoli: 1 - 10\n10. Mitoses: 1 - 10\n11. Class: (2 for benign, 4 for malignant)","eae343d7":"The data frame is of shape (699,10) suggesting there are 699 training cases.","7f74589f":"# KNN Model"}}