{"cell_type":{"a1b118d6":"code","6cbcaa00":"code","58ae7eab":"code","45bbbc37":"code","6ee24087":"code","fdf3c4a0":"code","be61daeb":"code","0b97ae2c":"code","6e6abdd6":"code","cc396e65":"code","8dfbe8d2":"code","05f3c4d2":"code","77f3fba8":"code","a43c0b7e":"code","82025104":"code","50b01512":"code","9d5699c1":"code","444c599e":"code","d8adfbd8":"code","488dc0f5":"code","ad5e3b41":"code","050e32b2":"code","1f2b7bb1":"code","e344c8fc":"code","533b46b2":"code","594f9a0b":"code","98c9556a":"code","4fc76e28":"code","cc3755e9":"code","abd5e365":"code","8f3cc7d8":"code","19451e1d":"code","277b2c7a":"code","889d7799":"code","b1be5414":"code","93e2be05":"code","605cd9f5":"code","e03dec24":"code","b77ef5c3":"code","2ba5734f":"code","4fb55f4d":"code","e32acda5":"code","0bdbc88e":"code","4dafb8e1":"markdown","8fb7dcd2":"markdown","0656e88b":"markdown","7d271273":"markdown","6d462cc8":"markdown","b18c231a":"markdown","35a47eb8":"markdown","0f861416":"markdown","f536e15f":"markdown","befa6fdd":"markdown","f3645269":"markdown"},"source":{"a1b118d6":"!pip install textstat rich -q","6cbcaa00":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport wandb\nimport rich\nimport spacy\n\nfrom pandas import DataFrame\nfrom matplotlib.lines import Line2D\nfrom rich.console import Console\nfrom rich import print\nfrom rich.theme import Theme\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\n\nnltk.download('stopwords')","58ae7eab":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb-api\")\n\nos.environ[\"WANDB_SILENT\"] = \"true\"","45bbbc37":"! wandb login $secret_value_0","6ee24087":"sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\ndef custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\npalette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\npalette2 = sns.diverging_palette(120, 220, n=20)\ncustom_palette(palette)\n\ncustom_theme = Theme({\n    \"info\" : \"italic bold cyan\",\n    \"warning\": \"italic bold magenta\",\n    \"danger\": \"bold blue\"\n})\n\nconsole = Console(theme=custom_theme)","fdf3c4a0":"INPUT_DATA = \"..\/input\/sf-dl-movie-genre-classification\/\"","be61daeb":"train_df = pd.read_csv(INPUT_DATA+'train.csv')\ntest_df = pd.read_csv(INPUT_DATA+'test.csv')","0b97ae2c":"train_df.sample(10)","6e6abdd6":"msno.bar(train_df,color=palette[2], sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.show()","cc396e65":"text = train_df['text'].min()\nconsole.print(\"Before preprocessing: \",style=\"info\")\nconsole.print(text,style='warning')\n\ne = re.sub(\"[^a-zA-Z]\", \" \", text)\ne = e.lower()\n        \ne = nltk.word_tokenize(e)\n        \ne = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \nlemma = nltk.WordNetLemmatizer()\ne = [lemma.lemmatize(word) for word in e]\ne=\" \".join(e)\nconsole.print(\"After preprocessing: \",style=\"info\")\nconsole.print(e,style='warning')","8dfbe8d2":"#====== Preprocessing function ======\ndef preprocess(data):\n    text_processed=[]\n    for e in data['text']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        text_processed.append(e)\n        \n    return text_processed ","05f3c4d2":"#train_df[\"text_preprocessed\"] = preprocess(train_df)\n#test_df[\"text_preprocessed\"] = preprocess(test_df)","77f3fba8":"train_df = pd.read_csv('..\/input\/sf-movie-genre\/train_text_preprocessed.csv')\ntest_df = pd.read_csv('..\/input\/sf-movie-genre\/test_text_preprocessed.csv')","a43c0b7e":"# #====== Saving to csv files and creating artifacts ======\n#train_df.to_csv(\"train_text_preprocessed.csv\")\n\n# #====== Saving to csv files and creating artifacts ======\n#test_df.to_csv(\"test_text_preprocessed.csv\")\n\n\n\n#run = wandb.init(project='Movie Genre Classification', name='text_preprocessed', entity='gusevski')\n\n#artifact = wandb.Artifact('train_text_preprocessed', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n#artifact.add_file(\"train_text_preprocessed.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n#run.log_artifact(artifact)\n\n#run.finish()\n\n\n#run = wandb.init(project='Movie Genre Classification', name='text_preprocessed')\n\n#artifact = wandb.Artifact('test_text_preprocessed', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n#artifact.add_file(\"test_text_preprocessed.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n#run.log_artifact(artifact)\n\n#run.finish()","82025104":"'''\nrun = wandb.init(project='Movie Genre Classification')\nartifact = run.use_artifact('ruchi798\/commonlit\/train_text_preprocessed:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"train_text_preprocessed.csv\")\ntrain_df = pd.read_csv(path)\ntrain_df = train_df.drop(columns=[\"Unnamed: 0\"])\n\nrun = wandb.init(project='Movie Genre Classification')\nartifact = run.use_artifact('ruchi798\/commonlit\/test_text_preprocessed:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"test_text_preprocessed.csv\")\ntest_df = pd.read_csv(path)\ntest_df = test_df.drop(columns=[\"Unnamed: 0\"])\n'''","50b01512":"#====== Function to plot wandb bar chart ======\ndef plot_wb_bar(df,col1,col2,name,title): \n    run = wandb.init(project='Movie Genre Classification', job_type='image-visualization',name=name)\n    \n    dt = [[label, val] for (label, val) in zip(df[col1], df[col2])]\n    table = wandb.Table(data=dt, columns = [col1,col2])\n    wandb.log({name : wandb.plot.bar(table, col1,col2,title=title)})\n\n    run.finish()\n    \n#====== Function to plot wandb histogram ======\ndef plot_wb_hist(df,name,title):\n    run = wandb.init(project='Movie Genre Classification', job_type='image-visualization',name=name)\n\n    dt = [[x] for x in df[name]]\n    table = wandb.Table(data=dt, columns=[name])\n    wandb.log({name : wandb.plot.histogram(table, name, title=title)})\n\n    run.finish()","9d5699c1":"plot_wb_hist(train_df,\"genre\",\"Genre\")","444c599e":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","d8adfbd8":"def plot_bt(x,w,p):\n    common_words = x(train_df['text_preprocessed'], 20)\n    common_words_df = DataFrame (common_words,columns=['word','freq'])\n\n    plt.figure(figsize=(16,8))\n    sns.barplot(x='freq', y='word', data=common_words_df,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(p,20))\n    plt.title(\"Top 20 \"+ w,font='Serif')\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");\n    return common_words_df","488dc0f5":"common_words = get_top_n_words(train_df['text_preprocessed'], 20)\ncommon_words_df1 = DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 8))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 unigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words_df2 = plot_bt(get_top_n_bigram,\"bigrams\",\"ch:rot=-.5\")\ncommon_words_df3 = plot_bt(get_top_n_trigram,\"trigrams\",\"ch:start=-1, rot=-.6\")","ad5e3b41":"plot_wb_bar(common_words_df1,'word','freq',\"unigrams\",\"Top 20 unigrams\")\nplot_wb_bar(common_words_df2,'word','freq',\"bigrams\",\"Top 20 bigrams\")\nplot_wb_bar(common_words_df3,'word','freq',\"trigrams\",\"Top 20 trigrams\")","050e32b2":"# color function for the wordcloud\ndef color_wc(word=None,font_size=None,position=None, orientation=None,font_path=None, random_state=None):\n    h = int(360.0 * 150.0 \/ 255.0)\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(80, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nplt.subplots(figsize=(16,16))\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", contour_width=2, contour_color='blue',width=1500, height=750,color_func=color_wc,max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(train_df['text_preprocessed']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","1f2b7bb1":"text_props = train_df.copy()\n\ndef avg_word_len(df):\n    df = df.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return df\n\ntext_len = train_df['text'].str.len()\ntext_len_pre = train_df['text_preprocessed'].str.len()\navg_text = avg_word_len(train_df['text'])\navg_text_pre = avg_word_len(train_df['text_preprocessed'])\nlexicon_count = []\nlexicon_count_pre = []\nsentence_count = []\nfor i in range(len(train_df)):\n    lc = textstat.lexicon_count(train_df['text'][i])\n    lcp = textstat.lexicon_count(train_df['text_preprocessed'][i])\n    sc = textstat.sentence_count(train_df['text'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    sentence_count.append(sc)\n    \ntext_props['text_len'] = text_len\ntext_props['text_len_pre'] = text_len_pre\ntext_props['lexicon_count'] = lexicon_count\ntext_props['lexicon_count_pre'] = lexicon_count_pre\ntext_props['avg_text'] = avg_text\ntext_props['avg_text_pre'] = avg_text_pre\ntext_props['sentence_count'] = sentence_count\n\ndef plot_distribution(col1,col2,title1,title2):\n    fig, ax = plt.subplots(1,2,figsize=(20,10))\n    sns.kdeplot(data=text_props, x=col1,color=palette[3],label=\"text\",ax=ax[0])\n    sns.kdeplot(data=text_props, x=col2,color=palette[4],label=\"text preprocessed\",ax=ax[0])\n    ax[0].set_title(title1,font=\"Serif\")\n\n    sns.scatterplot(data=text_props,x=col1,y='genre',color= palette[3],ax=ax[1],markers='.')\n    sns.scatterplot(data=text_props,x=col2,y='genre',color= palette[4],ax=ax[1],markers='.')\n    ax[1].set_title(title2,font=\"Serif\")\n\n    plt.show()\n\ncustom_lines = [Line2D([0], [0], color=palette[3], lw=4),\n                Line2D([0], [0], color=palette[4], lw=4)]\n\nplt.figure(figsize=(20, 1))\nlegend = plt.legend(custom_lines, ['text', 'text preprocessed'],loc=\"center\")\nplt.setp(legend.texts, family='Serif')\nplt.axis('off')\nplt.show()\n\nplot_distribution(\"text_len\",\"text_len_pre\",\"Character count distribution\",\"Character count vs Genre\")\nplot_distribution(\"lexicon_count\",\"lexicon_count_pre\",\"Word count distribution\",\"Word count vs Genre\")\nplot_distribution(\"avg_text\",\"avg_text_pre\", \"Average word length distribution\",\"Average word length vs Genre\")\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.kdeplot(data=text_props, x=sentence_count,color=palette[3],label=\"text\",ax=ax[0])\nax[0].set_title(\"Sentence count distribution\",font=\"Serif\")\nax[0].set_xlabel(\"sentence_count\")\nsns.scatterplot(data=text_props,x='sentence_count',y='genre',color= palette[3],ax=ax[1],markers='.')\nax[1].set_title(\"Sentence count vs Genre\",font=\"Serif\")\nplt.show()\n\nnum_cols = ['text_len','text_len_pre','lexicon_count','lexicon_count_pre','avg_text','avg_text_pre','sentence_count','genre']\ncorr = text_props[num_cols].corr()\n\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0,\n            square=True, linewidths=.5)\nplt.title('Correlation of text properties', fontsize=15,font=\"Serif\")\nplt.show()","e344c8fc":"run = wandb.init(project='Movie Genre Classification', name='count')\n\n# maximum target\nm_t = train_df[\"genre\"].max() \n\n# minimum target\nl_t = train_df[\"genre\"].min() \n\n\n\nwandb.log({'Genre (highest value)': m_t, \n           'Genre (lowest value)': l_t,\n          })\n\nrun.finish()","533b46b2":"plot_wb_hist(text_props,\"text_len\",\"Character Count Distribution\")\nplot_wb_hist(text_props,\"lexicon_count\",\"Word Count Distribution\")\nplot_wb_hist(text_props,\"avg_text\",\"Average Word Length Distribution\")\nplot_wb_hist(text_props,\"sentence_count\",\"Sentence Count Distribution\")","594f9a0b":"text_props['pos_tags'] = text_props['text_preprocessed'].str.split().map(pos_tag)\n\ndef count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntext_props['tag_counts'] = text_props['pos_tags'].map(count_tags)","98c9556a":"set_pos = set([tag for tags in text_props['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    text_props[tag] = text_props['tag_counts'].map(lambda x: x.get(tag, 0))","4fc76e28":"pos = text_props[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,10))\nax = sns.barplot(x=pos.index, y=pos.values,palette=\"Wistia\")\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('POS tags frequency',fontsize=15,font=\"Serif\")\nplt.show()","cc3755e9":"pos_data = pd.DataFrame({'part_of_speech':pos.index, 'freq':pos.values})\nplot_wb_bar(pos_data,'part_of_speech', 'freq',\"POS\",\"POS tags frequency\")","abd5e365":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=text_props,x='NN',y='genre',color= palette[3],markers='.',label=\"noun, singular\")\nsns.scatterplot(data=text_props,x='JJ',y='genre',color= palette[4],markers='.',label=\"adjective\",)\nsns.scatterplot(data=text_props,x='VBD',y='genre',color= palette[0],markers='.',label=\"verb past tense\")\nsns.scatterplot(data=text_props,x='RB',y='genre',color= palette[1],markers='.',label=\"adverb\")\nplt.legend(title=\"POS tag\",bbox_to_anchor=(1.4, 1))\nplt.xlabel(\"POS tags count\")\nplt.title(\"POS vs genre\")\nplt.show()","8f3cc7d8":"toughest_text = text_props[text_props[\"genre\"] == text_props[\"genre\"].min()].text.values[0]\nlowest_genre = text_props[text_props[\"genre\"] == text_props[\"genre\"].min()].genre.values[0]\nnlp = spacy.load(\"en_core_web_sm\")\nsentences = sent_tokenize(toughest_text)\nword_count = lambda sentence: len(word_tokenize(sentence))\npos_text = max(sentences, key=word_count)  \n\nconsole.print(\"Genre of the toughest text: \",style=\"info\")\nconsole.print(lowest_genre,style='warning')\n\nconsole.print(\"Longest sentence of the toughest text: \",style=\"info\")","19451e1d":"doc = nlp(pos_text)\ndisplacy.render(doc, style=\"dep\")","277b2c7a":"flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\nfor i in range(len(text_props)):\n    flr = textstat.flesch_reading_ease(train_df['text'][i])\n    flkg = textstat.flesch_kincaid_grade(train_df['text'][i])\n    fs = textstat.gunning_fog(train_df['text'][i])\n    ar = textstat.automated_readability_index(train_df['text'][i])\n    cole = textstat.coleman_liau_index(train_df['text'][i])\n    lins = textstat.linsear_write_formula(train_df['text'][i])\n    ts = textstat.text_standard(train_df['text'][i])\n    \n    flesch_re.append(flr)\n    flesch_kg.append(flkg)\n    fog_scale.append(fs)\n    automated_r.append(ar)\n    coleman.append(cole)\n    linsear.append(lins)\n    text_standard.append(ts)\n    \ntext_props['flesch_re'] = flesch_re\ntext_props['flesch_kg'] = flesch_kg\ntext_props['fog_scale'] = fog_scale\ntext_props['automated_r'] = automated_r\ntext_props['coleman'] = coleman\ntext_props['linsear'] = linsear\ntext_props['text_standard'] = text_standard","889d7799":"# #====== Saving to csv files and creating artifacts ======\ntext_props.to_csv(\"text_props_readability.csv\")\n\nrun = wandb.init(project='Movie Genre Classification', name='text_props_readability')\n\nartifact = wandb.Artifact('text_props_readability', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\nartifact.add_file(\"text_props_readability.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \nrun.log_artifact(artifact)\n\nrun.finish()","b1be5414":"'''\nrun = wandb.init(project='Movie Genre Classification')\nartifact = run.use_artifact('ruchi798\/commonlit\/text_props_readability:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"text_props_readability.csv\")\ntext_props = pd.read_csv(path)\ntext_props = text_props.drop(columns=[\"Unnamed: 0\"])\n'''","93e2be05":"readability_cols = ['flesch_re','flesch_kg','fog_scale','automated_r','coleman','linsear','text_standard','genre']\n\ncorr = text_props[readability_cols].corr()\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='PuBuGn', robust=True, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.title('Correlation of readability tests', fontsize=15,font=\"Serif\")\nplt.show()","605cd9f5":"plt.figure(figsize=(10,8))\nsns.kdeplot(text_props[\"flesch_re\"],color=palette[4],shade=True)\nplt.title(\"Distribution of Flesch Reading Ease test\")\nplt.show()","e03dec24":"plot_wb_hist(text_props,\"flesch_re\",\"Flesch Reading Ease Distribution\")","b77ef5c3":"text_props.loc[text_props['flesch_re'] > 60]['flesch_re'].count() \/ len(text_props) *100","2ba5734f":"pd.set_option('display.max_colwidth', None)\nmax_text = text_props[text_props[\"genre\"] == text_props[\"genre\"].max()]['text']\nmin_text = text_props[text_props[\"genre\"] == text_props[\"genre\"].min()]['text']\n\nmax_text_f = text_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].max()]['text']\nmin_text_f = text_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].min()]['text']\n\nconsole.print(\"Highest Target\", style=\"danger\")\nconsole.print(max_text, style=\"info\")\ntext_props[text_props[\"genre\"] == text_props[\"genre\"].max()][['flesch_re','genre','text_standard']]","4fb55f4d":"console.print(\"Highest Flesch Reading Ease Score\", style=\"danger\")\nconsole.print(max_text_f, style=\"info\")\ntext_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].max()][['flesch_re','genre','text_standard']]","e32acda5":"console.print(\"Lowest Target\", style=\"danger\")\nconsole.print(min_text, style=\"warning\")\ntext_props[text_props[\"genre\"] == text_props[\"genre\"].min()][['flesch_re','genre','text_standard']]","0bdbc88e":"console.print(\"Lowest Flesch Reading Ease Score\", style=\"danger\")\nconsole.print(min_text_f, style=\"warning\")\ntext_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].min()][['flesch_re','genre','text_standard']]","4dafb8e1":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [Movie Genre Classification Project on W&B Dashboard](https:\/\/wandb.ai\/gusevski\/Movie%20Genre%20Classification) \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n> \n> - To get the API key, an account is to be created on the [website](https:\/\/wandb.ai\/site) first.\n> - Next, use secrets to use API Keys more securely \ud83e\udd2b\n## Task: Predict the genre of the film by its description and title.","8fb7dcd2":"# Pre-processing text \u2702\ufe0f\nIt's important to preprocess the **text** before we proceed further!","0656e88b":"Logging a **custom histogram** for the distribution of Flesch Reading Ease scores \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","7d271273":"# Readability tests \ud83e\uddea\n\n[textstat](https:\/\/pypi.org\/project\/textstat\/) is a library used to calculate statistics from text.\nIt came in super handy for calculating scores of various readability tests!\n\n- ```flesch_re:``` [The Flesch Reading Ease formula](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease)\n- ```flesch_kg:``` [The Flesch-Kincaid Grade Level ](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease)\n- ```fog_scale:``` [The Fog Scale (Gunning FOG Formula)](https:\/\/en.wikipedia.org\/wiki\/Gunning_fog_index)\n- ```automated_r:``` [Automated Readability Index](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index)\n- ```coleman:``` [The Coleman-Liau Index](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index)\n- ```linsear:``` [Linsear Write Formula](https:\/\/en.wikipedia.org\/wiki\/Linsear_Write)\n- ```text_standard:``` Readability Consensus based upon all the above tests","6d462cc8":"Logging **custom bar charts** for unigrams, bigrams and trigrams \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","b18c231a":"Logging the preprocessed dataset as an **artifact** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","35a47eb8":"Let's see which texts have the highest and lowest ```Target``` and ```Flesch Reading Ease Score``` \u2b07\ufe0f","0f861416":"Here I have used a viz module called [missingno](https:\/\/pypi.org\/project\/missingno\/) to visualize missing values in the training set.\n\nWe don't have any missing values in the columns of our interest, i.e., ```name```, ```genre``` and ```text```!","f536e15f":"# EDA \ud83d\udcca","befa6fdd":"Using the **saved artifact** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","f3645269":"Logging the text properties dataset as an **artifact** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n\nThis helps me to save on time since I can directly use the saved artifact for my workflow \ud83e\udd73"}}