{"cell_type":{"356bfd0e":"code","9e69fe29":"code","84a41e35":"code","13877a22":"code","ffdf744b":"code","25ca0f9f":"code","65da8683":"code","61bf94a5":"code","f71c44eb":"code","ecf3d48e":"code","c889c822":"code","778b6616":"code","cedfd868":"code","b9ae0a3d":"code","4d8a084d":"code","7102fa67":"code","962f1e5f":"code","0ad7f2cc":"code","db8a4bfe":"code","5823d4a1":"code","590fb61e":"code","dc338a9b":"code","56e3d205":"code","7fd591b1":"code","2add678d":"code","6b8ed444":"code","af627ca0":"code","8527a002":"code","8afc621b":"code","7e450038":"code","cd150190":"code","0bacd55c":"code","021256c3":"code","9d9ad15b":"code","6f212848":"code","83d9dae2":"code","e086867c":"code","fee39aed":"code","ec800fb8":"code","b43bacd8":"code","150ca60a":"code","cb693d0f":"code","fd2749be":"code","d74dee6a":"code","4027398c":"code","34865215":"code","9c04c356":"code","28cded9e":"code","0e8aa1d4":"code","9724cfca":"code","1342d6f7":"code","3cd6c492":"code","73a7eab5":"code","cc7daab2":"code","56008ce9":"code","9ab4972a":"code","fcf8812d":"code","f16909b5":"code","fd682757":"code","8715ad9b":"code","7bd301c2":"code","bb6ddbc6":"code","89175483":"code","60c230a0":"code","7b68d4f4":"code","27936c70":"code","827bc77d":"code","d1f38688":"code","7bde9d80":"code","47e48379":"code","30f7251e":"code","85485e13":"code","ad7275b9":"code","d55b914e":"code","7e522d24":"code","d12aee88":"code","a3005c4f":"code","3e5f997b":"code","550a74bd":"code","6df35e5a":"code","f19b1093":"code","9fbce1ec":"code","2343ae26":"code","9ee70848":"code","87577495":"code","5618cbb5":"code","0b030152":"code","580551f8":"code","70e5da21":"code","b214983f":"code","2937c19d":"code","44de9140":"code","67aca10d":"code","83fc39fc":"code","6c47277b":"code","8f1fa659":"code","8ab254b5":"code","fdf74f5d":"code","407c7564":"code","671d4505":"code","8553450c":"code","a4564553":"code","ea6b9b24":"code","f96a6f97":"code","de7f86b7":"code","bd509c4a":"code","7f868f2c":"code","d868820d":"code","a3a61a5f":"code","c5945982":"code","64371f1f":"code","cb9718eb":"code","2926d4d1":"code","03b72dd4":"code","220d3246":"code","8903dfa5":"code","a9ab0777":"code","c5dcca1d":"code","836d4f3f":"code","ca3624da":"code","b1d3ca51":"code","9184385a":"code","165950a8":"code","5bd8cd8a":"code","5f063906":"code","0970fd40":"code","b0bd5fe4":"code","5e116045":"code","f28c0e13":"code","ab06aead":"code","6f7f20f5":"code","82eda706":"code","20badcfc":"code","2f9215b2":"code","e5796932":"code","fbb097e7":"code","bf77c895":"code","e4db7b25":"code","0a69d4f0":"code","bd964ceb":"code","9be245cc":"code","3b31c551":"code","0c797543":"code","178f317d":"code","fcec7339":"code","1096fd08":"code","47a01ce8":"code","ed18b245":"code","db50faf3":"code","1e4ab58e":"code","1fdbe447":"markdown","f01fc432":"markdown","a57bbe09":"markdown","45d951b3":"markdown","fcafeb91":"markdown","dc0147cf":"markdown","45639e3e":"markdown","c59218b8":"markdown","d86c2ab8":"markdown","8d4e4346":"markdown","9a18579a":"markdown","cf069cb2":"markdown","33706d58":"markdown","9264d0da":"markdown","9c84f826":"markdown","d8b9f241":"markdown","b4648a9c":"markdown","f17fca9f":"markdown","4299df88":"markdown","8423931f":"markdown","c1ff6c18":"markdown","460490a8":"markdown","a5f5c18e":"markdown","22296da0":"markdown","e874bae7":"markdown","941c30eb":"markdown","d46d0da3":"markdown","2a837a35":"markdown","72b17f04":"markdown","f40eed34":"markdown","25404f9f":"markdown","ac2180d0":"markdown","a8daf27d":"markdown","9a4c1b27":"markdown","74561f6e":"markdown","4b75a998":"markdown","92733587":"markdown","1b3e4d6a":"markdown","d1322364":"markdown","e0249c43":"markdown","89e0dada":"markdown","0f74f8f2":"markdown"},"source":{"356bfd0e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9e69fe29":"pip install xlrd","84a41e35":"pip install openpyxl","13877a22":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ffdf744b":"df = pd.read_excel('\/kaggle\/input\/ecommerce-customer-churn-analysis-and-prediction\/E Commerce Dataset.xlsx', sheet_name='E Comm')\ndf.head()","25ca0f9f":"df.shape","65da8683":"df.duplicated().sum()","61bf94a5":"df.isnull().sum()","f71c44eb":"df.dtypes","ecf3d48e":"for i in df.columns:\n    if df[i].dtypes == 'object':\n        print(i)\n        print()\n        print('the values are:') \n        print(df[i].value_counts())\n        print()\n        print()","c889c822":"df1=df.copy()","778b6616":"df.drop(['CustomerID'],axis=1, inplace=True)","cedfd868":"for i in df.columns:\n    if df[i].isnull().sum() > 0:\n        print(i)\n        print('the total null values are:', df[i].isnull().sum())\n        print('the datatype is', df[i].dtypes)\n        print()","b9ae0a3d":"df['Churn'] = df['Churn'].astype('object')\ndf['CityTier'] = df['CityTier'].astype('object')","4d8a084d":"df.describe().transpose()","7102fa67":"for i in df.columns:\n    if df[i].isnull().sum() > 0:\n        df[i].fillna(df[i].median(),inplace=True)","962f1e5f":"plt.figure(figsize=(50,10))\nsns.boxplot(data=df)\nplt.title('The boxplot to study outliers')\nplt.xlabel('Variables that predict the customer churn')\nplt.ylabel('Values')","0ad7f2cc":"def remove_outlier(col):\n    sorted(col)\n    Q1,Q3=np.percentile(col,[25,75])\n    IQR=Q3-Q1\n    lr= Q1-(1.5 * IQR)\n    ur= Q3+(1.5 * IQR)\n    return lr, ur","db8a4bfe":"df.columns","5823d4a1":"for column in df.columns:\n    if df[column].dtype != 'object': \n        lr,ur=remove_outlier(df[column])\n        df[column]=np.where(df[column]>ur,ur,df[column])\n        df[column]=np.where(df[column]<lr,lr,df[column])","590fb61e":"plt.figure(figsize=(50,10))\nsns.boxplot(data=df)\nplt.title('The boxplot to study outliers')\nplt.xlabel('Variables that predict the customer churn')\nplt.ylabel('Values')","dc338a9b":"df['avg_cashbk_per_order'] = df['CashbackAmount'] \/ df['OrderCount']","56e3d205":"# Percentage of customer churn\nChurn_perc = round((df['Churn'][df['Churn']==1].count()*100\/df['Churn'][df['Churn']==0].count()),2)","7fd591b1":"print('The average customer churn is:', Churn_perc,'%')","2add678d":"cat=[]\nnum=[]\nfor i in df.columns:\n  if df[i].dtype=='object':\n    cat.append(i)\n  else:\n    num.append(i)\nprint('cat = ',cat)\nprint('num = ',num)","6b8ed444":"df[cat].describe().T","af627ca0":"for i in cat:\n    print(i)\n    print()\n    print(df[i].value_counts())\n    print()\n    print()","8527a002":"df[num].describe().T","8afc621b":"df[num].hist(figsize=(40,40))","7e450038":"fig, ax = plt.subplots(8, 2, figsize=(40, 40))\nfor i, subplot in zip(num, ax.flatten()):\n    sns.distplot(df[i], ax=subplot)","cd150190":"# defining a tuple for storing the dataframes in a containing information about the contribution each value to the customer churn.\n# this was done so that we can quickly automate making such dataframes\n\nd={}\n\n# running loop for storing the calculating and storing the values in the relevant dataframes  \nfor i in df.columns:\n    d[i] = pd.concat([df.groupby(i).Churn.sum(),df[i].value_counts(),round(df.groupby(i).Churn.sum()*100\/df[i].value_counts(),2)], axis=1)\n    d[i].reset_index(level=0,inplace=True)\n    d[i] = d[i].rename(columns = {'index':i, 'Churn':'Customers_churned', i:'Total_Customers', 0:'perc_of_total_cust'}, )","0bacd55c":"for i in df.columns:\n    print(i)\n    print(d[i])\n    print()","021256c3":"# Lets define a function drawing the graph","9d9ad15b":"def analysis_chart(variable):\n  # definig the plot for matplotlib\n  plt.figure(figsize=(20,12))\n  fig, ax = plt.subplots()\n  # defining the title\n  title1 = 'Customers Churn analysed by ' + variable\n  plt.title(title1)\n  # defining the lines for the y -axis\n  line1 = ax.plot(d[variable][variable],d[variable]['Customers_churned'], color='lightskyblue', label = 'Customers churned')\n  line2 = ax.plot(d[variable][variable],d[variable]['Total_Customers'], color='dodgerblue', label = 'Total Customers')\n  # labelling the x -axis and y-axis\n  plt.xlabel (variable)\n  plt.ylabel ('No. of customers')\n  # rotating the labels on the x-axis for better visualisation\n  for tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n  # defining another axis on the right side of the graph\n  ax2=ax.twinx()\n  # defining the line for the right side y -axis\n  line3 = ax2.plot(d[variable][variable],d[variable]['perc_of_total_cust'], color='yellowgreen', label = 'Churn as Percent of total')\n  y = 0*d[variable]['perc_of_total_cust']+20.25\n  line4 = ax2.plot(d[variable][variable], y, color='orangered', label='Average customer Churn', linestyle='dashed')\n  # labelling the right side y-axis\n  plt.ylabel ('percentage of customers churned')\n  # adding the three lines to show the legend on the right corner in a coherent place, not doing this will lead to overlapping of legends of lines belonging to left and right y axis\n  lines = line1+line2+line3+line4\n  labs = [l.get_label() for l in lines]\n  ax.legend(lines, labs, bbox_to_anchor=(1.7, 1))\n  # adding sns palette for better visualisation\n  sns.despine(ax=ax, right=True, left=True)\n  sns.despine(ax=ax2, left=True, right=False)\n","6f212848":"col = ['Tenure', 'PreferredLoginDevice', 'CityTier',\n       'WarehouseToHome', 'PreferredPaymentMode', 'Gender', 'HourSpendOnApp',\n       'NumberOfDeviceRegistered', 'PreferedOrderCat', 'SatisfactionScore',\n       'MaritalStatus', 'NumberOfAddress', 'Complain',\n       'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount',\n       'DaySinceLastOrder']","83d9dae2":"analysis_chart('Tenure')","e086867c":"analysis_chart('CityTier')","fee39aed":"analysis_chart('WarehouseToHome')","ec800fb8":"analysis_chart('PreferredPaymentMode')","b43bacd8":"analysis_chart('Gender')","150ca60a":"analysis_chart('HourSpendOnApp')","cb693d0f":"analysis_chart('NumberOfDeviceRegistered')","fd2749be":"analysis_chart('PreferedOrderCat')","d74dee6a":"analysis_chart('SatisfactionScore')","4027398c":"analysis_chart('MaritalStatus')","34865215":"analysis_chart('NumberOfAddress')","9c04c356":"analysis_chart('Complain')","28cded9e":"analysis_chart('OrderAmountHikeFromlastYear')","0e8aa1d4":"analysis_chart('CouponUsed')","9724cfca":"analysis_chart('OrderCount')","1342d6f7":"analysis_chart('DaySinceLastOrder')","3cd6c492":"# writing the loop for automating the figure generation\n# Writing the loop for one dataframe at a time since we need to have the figures in separate cells so that it becomes easy to analyse and write the conclusion there only.\nfor i in ['avg_cashbk_per_order']:\n    # definig the plot for matplotlib\n    plt.figure(figsize=(20,12))\n    fig, ax = plt.subplots()\n    # defining the title\n    title1 = 'Customers Churn analysed by ' + i\n    plt.title(title1)\n    # defining the lines for the y -axis\n    line1 = ax.scatter(d[i][i],d[i]['Customers_churned'], color='lightskyblue', label = 'Customers churned')\n    line2 = ax.scatter(d[i][i],d[i]['Total_Customers'], color='dodgerblue', label = 'Total Customers')\n    # labelling the x -axis and y-axis\n    plt.xlabel (i)\n    plt.ylabel ('No. of customers')\n    # rotating the labels on the x-axis for better visualisation\n    for tick in ax.get_xticklabels():\n      tick.set_rotation(45)\n    # defining another axis on the right side of the graph\n    ax2=ax.twinx()\n    # defining the line for the right side y -axis\n    line3 = ax2.scatter(d[i][i],d[i]['perc_of_total_cust'], color='yellowgreen', label = 'Churn as Percent of total')\n    # labelling the right side y-axis\n    plt.ylabel ('percentage of customers churned')\n    # adding sns palette for better visualisation\n    sns.despine(ax=ax, right=True, left=True)\n    sns.despine(ax=ax2, left=True, right=False)","73a7eab5":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True)","cc7daab2":"df.head()","56008ce9":"df_encoded=df.copy()\ndf_encoded.head()","9ab4972a":"df_encoded = pd.get_dummies(df_encoded,drop_first=True)","fcf8812d":"df_encoded.head(10)","f16909b5":"#importing StandardScaler from SciKit Learn\nfrom sklearn.preprocessing import StandardScaler","fd682757":"#defining a function standard scaler that will scale the the dataframe\nscaler = StandardScaler()","8715ad9b":"features = df_encoded[num]\nfeatures = scaler.fit_transform(features)","7bd301c2":"scaled_df_encoded = df_encoded.copy()","bb6ddbc6":"scaled_df_encoded[num] = features","89175483":"#checking the how does the scaled df looks like\nscaled_df_encoded","60c230a0":"scaled_df_encoded_h = scaled_df_encoded.copy()","7b68d4f4":"from scipy.cluster.hierarchy import dendrogram, linkage","27936c70":"#linking using the average linking method\nlink_method=linkage(scaled_df_encoded_h,method = 'average')","827bc77d":"# just checking what do we get in the Link_method variable\nlink_method","d1f38688":"labellist = np.array(scaled_df_encoded_h.Churn_1)\nlabellist","7bde9d80":"len(labellist)","47e48379":"dend = dendrogram(link_method,labels=labellist)","30f7251e":"# truncate mode = lastp means last 10 dendrograms to stay and rest to go.\ndend = dendrogram(link_method,\n                  labels=labellist,\n                 truncate_mode='lastp',\n                 p=10)","85485e13":"from scipy.cluster.hierarchy import fcluster","ad7275b9":"# criterion= maxclust, clusters=4 \nclusters_max = fcluster(link_method,4,criterion = 'maxclust')\nclusters_max","d55b914e":"#appending the clusters to the dataframe\nscaled_df_encoded_h['clusters_max'] = clusters_max","7e522d24":"scaled_df_encoded_h.head()","d12aee88":"df_h = df.copy()","a3005c4f":"df_h['clusters_max'] = clusters_max","3e5f997b":"aggdata_max = scaled_df_encoded_h.iloc[:,:].groupby('clusters_max').median()\naggdata_max['freq'] = scaled_df_encoded_h.clusters_max.value_counts().sort_index()\naggdata_max\n# 4 clusters in the maxcluster method","550a74bd":"aggdata_max_2 = df_h.iloc[:,:].groupby('clusters_max').mean()\naggdata_max_2['freq'] = df_h.clusters_max.value_counts().sort_index()\naggdata_max_2","6df35e5a":"scaled_df_encoded_k= scaled_df_encoded.copy()","f19b1093":"from sklearn.cluster import KMeans ","9fbce1ec":"k_means = KMeans(n_clusters = 2)","2343ae26":"k_means.fit(scaled_df_encoded_k)","9ee70848":"k_means.labels_","87577495":"k_means.inertia_","5618cbb5":"k_means = KMeans(n_clusters = 3)\nk_means.fit(scaled_df_encoded_k)\nk_means.inertia_","0b030152":"k_means = KMeans(n_clusters = 4)\nk_means.fit(scaled_df_encoded_k)\nk_means.inertia_","580551f8":"k_means = KMeans(n_clusters = 5)\nk_means.fit(scaled_df_encoded_k)\nk_means.inertia_","70e5da21":"wss =[] \nfor i in range(1,20):\n    k_means = KMeans(n_clusters=i)\n    k_means.fit(scaled_df_encoded_k)\n    wss.append(k_means.inertia_)","b214983f":"wss","2937c19d":"plt.plot(range(1,20), wss)","44de9140":"from sklearn.metrics import silhouette_samples, silhouette_score","67aca10d":"# Silhoutte score for 2 cluster\nSil_Score = []\nfor i in range(2,20):\n  k_means = KMeans(n_clusters=i)\n  k_means.fit(scaled_df_encoded_k)\n  labels = k_means.labels_\n  ss = silhouette_score(scaled_df_encoded_k,labels)\n  Sil_Score.append(ss)","83fc39fc":"Sil_Score","6c47277b":"# taking ideal number of clusters as 4\nk_means = KMeans(n_clusters = 4)\nk_means.fit(scaled_df_encoded_k)\nlabels = k_means.labels_","8f1fa659":"scaled_df_encoded_k[\"Clus_kmeans\"] = labels\nscaled_df_encoded_k.head(10)","8ab254b5":"aggdata_k = scaled_df_encoded_k.iloc[:,:].groupby('Clus_kmeans').mean()\naggdata_k['freq'] = scaled_df_encoded_k.Clus_kmeans.value_counts().sort_index()\naggdata_k","fdf74f5d":"df_k = df.copy()","407c7564":"df_k['Clus_kmeans'] = scaled_df_encoded_k[\"Clus_kmeans\"]","671d4505":"aggdata_k2 = df_k.iloc[:,:].groupby('Clus_kmeans').mean()\naggdata_k2['freq'] = df_k.Clus_kmeans.value_counts().sort_index()\naggdata_k2","8553450c":"scaled_df_encoded","a4564553":"scaled_df_encoded_h['clusters_max'] = scaled_df_encoded_h['clusters_max'].astype('object')","ea6b9b24":"scaled_df_encoded_h1 = pd.get_dummies(scaled_df_encoded_h, drop_first=True)","f96a6f97":"X=scaled_df_encoded_h1.drop(['Churn_1'],axis=1)\ny=scaled_df_encoded_h1['Churn_1']","de7f86b7":"print('Before OverSampling, the shape of X: {}'.format(X.shape)) \nprint('Before OverSampling, the shape of y: {} \\n'.format(y.shape)) \n  \nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1))) \nprint(\"Before OverSampling, counts of label '0': {}\".format(sum(y == 0)))","bd509c4a":"from imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state=33)\nX_res, y_res = sm.fit_sample(X, y.ravel())","7f868f2c":"print('After OverSampling, the shape of X: {}'.format(X_res.shape)) \nprint('After OverSampling, the shape of y: {} \\n'.format(y_res.shape)) \n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_res == 0)))","d868820d":"X_res=pd.DataFrame(X_res)\n#Renaming column name of Target variable\ny_res=pd.DataFrame(y_res)\ny_res.columns = ['Churn_1']\nscaled_df_encoded_h1_smote = pd.concat([X_res,y_res], axis=1)","a3a61a5f":"from sklearn.model_selection import train_test_split","c5945982":"X = scaled_df_encoded_h1_smote.drop(['Churn_1'],axis=1)\ny = scaled_df_encoded_h1_smote['Churn_1']\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.75, random_state=42)","64371f1f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","cb9718eb":"?LogisticRegression","2926d4d1":"model_lr = LogisticRegression()\nmodel_lr.fit(X_train, y_train)","03b72dd4":"ytrain_predict = model_lr.predict(X_train)\nytest_predict = model_lr.predict(X_test)","220d3246":"ytest_predict_prob=model_lr.predict_proba(X_test)\npd.DataFrame(ytest_predict_prob).head()","8903dfa5":"# Accuracy - Training Data\nmodel_lr.score(X_train, y_train)","a9ab0777":"from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix\n# predict probabilities\nprobs = model_lr.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","c5dcca1d":"# Accuracy - Test Data\nmodel_lr.score(X_test, y_test)","836d4f3f":"# predict probabilities\nprobs = model_lr.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","ca3624da":"#confusion matrix training Data\ncm_lr = confusion_matrix(y_train, ytrain_predict)\ncm_lr\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_lr,annot = True,fmt = 'd', cmap ='rainbow')","b1d3ca51":"print(classification_report(y_train, ytrain_predict))","9184385a":"#confusion matrix test Data\ncm_test_lr=confusion_matrix(y_test, ytest_predict)\ncm_test_lr\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_test_lr,annot = True,fmt = 'd', cmap='rainbow')","165950a8":"#Test Data Accuracy\ntest_acc=model_lr.score(X_test,y_test)\ntest_acc","5bd8cd8a":"print(classification_report(y_test, ytest_predict))","5f063906":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nLDA_model= LinearDiscriminantAnalysis()","0970fd40":"LDA_model.fit(X_train, y_train)","b0bd5fe4":"## Performance Matrix on train data set\ny_train_predict = LDA_model.predict(X_train)\nmodel_score = LDA_model.score(X_train, y_train)\nprint(model_score)\n#confusion matrix training Data\ncm_train_lda = confusion_matrix(y_train, y_train_predict)\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_train_lda, annot = True,fmt = 'd', cmap='rainbow')\nprint(metrics.classification_report(y_train, y_train_predict))","5e116045":"## Performance Matrix on test data set\ny_test_predict = LDA_model.predict(X_test)\nmodel_score = LDA_model.score(X_test, y_test)\nprint(model_score)\n#confusion matrix test Data\ncm_test_lda = confusion_matrix(y_test, y_test_predict)\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_test_lda, annot = True,fmt = 'd', cmap='rainbow')\nprint(metrics.classification_report(y_test, y_test_predict))","f28c0e13":"# predict probabilities\nprobs = LDA_model.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","ab06aead":"# predict probabilities\nprobs = LDA_model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","6f7f20f5":"from sklearn import tree\nDT_model= tree.DecisionTreeClassifier()\nDT_model.fit(X_train, y_train)","82eda706":"## Performance Matrix on train data set\ny_train_predict = DT_model.predict(X_train)\nmodel_score = DT_model.score(X_train, y_train)\nprint(model_score)\nprint(metrics.classification_report(y_train, y_train_predict))\n#confusion matrix training Data\ncm_train_dt = confusion_matrix(y_train, y_train_predict)\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_train_dt, annot = True,fmt = 'd', cmap='rainbow')","20badcfc":"## Performance Matrix on test data set\ny_test_predict = DT_model.predict(X_test)\nmodel_score = DT_model.score(X_test, y_test)\nprint(model_score)\nprint(metrics.classification_report(y_test, y_test_predict))\n#confusion matrix test Data\ncm_test_dt = confusion_matrix(y_test, y_test_predict)\nsns.set(font_scale = 1.2)\nprint('Confusion Matrix')\nsns.heatmap(cm_test_dt, annot = True,fmt = 'd', cmap='rainbow')","2f9215b2":"# predict probabilities\nprobs = DT_model.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","e5796932":"# predict probabilities\nprobs = DT_model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","fbb097e7":"from sklearn.ensemble import RandomForestClassifier\n\nRF_model=RandomForestClassifier(n_estimators=100,random_state=1)\nRF_model.fit(X_train, y_train)","bf77c895":"## Performance Matrix on train data set\ny_train_predict = RF_model.predict(X_train)\nmodel_score =RF_model.score(X_train, y_train)\nprint(model_score)\nprint(metrics.classification_report(y_train, y_train_predict))\n#confusion matrix training Data\ncm_train_rf = confusion_matrix(y_train, y_train_predict)\nprint('Confusion Matrix')\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_train_rf, annot = True,fmt = 'd', cmap='rainbow')","e4db7b25":"## Performance Matrix on test data set\ny_test_predict = RF_model.predict(X_test)\nmodel_score = RF_model.score(X_test, y_test)\nprint(model_score)\nprint(metrics.classification_report(y_test, y_test_predict))\n#confusion matrix test Data\ncm_test_rf = confusion_matrix(y_test, y_test_predict)\nsns.set(font_scale = 1.2)\nprint('Confusion Matrix')\nsns.heatmap(cm_test_rf, annot = True,fmt = 'd', cmap='rainbow')","0a69d4f0":"# predict probabilities\nprobs = RF_model.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","bd964ceb":"# predict probabilities\nprobs = RF_model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","9be245cc":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN_model=KNeighborsClassifier()\nKNN_model.fit(X_train,y_train)","3b31c551":"## Performance Matrix on train data set\ny_train_predict = KNN_model.predict(X_train)\nmodel_score = KNN_model.score(X_train, y_train)\nprint(model_score)\nprint(metrics.classification_report(y_train, y_train_predict))\n#confusion matrix training Data\ncm_train_knn = confusion_matrix(y_train, y_train_predict)\nprint('Confusion Matrix')\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_train_knn, annot = True,fmt = 'd', cmap='rainbow')","0c797543":"## Performance Matrix on test data set\ny_test_predict = KNN_model.predict(X_test)\nmodel_score = KNN_model.score(X_test, y_test)\nprint(model_score)\nprint(metrics.classification_report(y_test, y_test_predict))\n#confusion matrix test Data\ncm_test_knn = confusion_matrix(y_test, y_test_predict)\nsns.set(font_scale = 1.2)\nprint('Confusion Matrix')\nsns.heatmap(cm_test_knn, annot = True,fmt = 'd', cmap='rainbow')","178f317d":"# predict probabilities\nprobs = KNN_model.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","fcec7339":"# predict probabilities\nprobs = KNN_model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","1096fd08":"import xgboost as xgb\nXGB_model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\nXGB_model.fit(X_train, y_train)","47a01ce8":"## Performance Matrix on train data set\ny_train_predict = XGB_model.predict(X_train)\nmodel_score = XGB_model.score(X_train, y_train)\nprint(model_score)\nprint(metrics.classification_report(y_train, y_train_predict))\n#confusion matrix training Data\ncm_train_knn = confusion_matrix(y_train, y_train_predict)\nprint('Confusion Matrix')\nsns.set(font_scale = 1.2)\nsns.heatmap(cm_train_knn, annot = True,fmt = 'd', cmap='rainbow')","ed18b245":"## Performance Matrix on test data set\ny_test_predict = XGB_model.predict(X_test)\nmodel_score = XGB_model.score(X_test, y_test)\nprint(model_score)\nprint(metrics.classification_report(y_test, y_test_predict))\n#confusion matrix test Data\ncm_test_knn = confusion_matrix(y_test, y_test_predict)\nsns.set(font_scale = 1.2)\nprint('Confusion Matrix')\nsns.heatmap(cm_test_knn, annot = True,fmt = 'd', cmap='rainbow')","db50faf3":"# predict probabilities\nprobs = XGB_model.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","1e4ab58e":"# predict probabilities\nprobs = XGB_model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","1fdbe447":"## KNN Model","f01fc432":"## Random Forest","a57bbe09":"### Cluster Profiles","45d951b3":"## One hot encoding","fcafeb91":"Here we can see that there are quite a lot of outliers in almost all of the variables. Lets treat these outliers","dc0147cf":"importing libraries - dendogram and linkage","45639e3e":"# Exploratory Data Analysis\n\n# 1. The Univariate Analysis","c59218b8":"# Handling the Missing Values","d86c2ab8":"First lets go about filling the missing values by the median values -> Mean and median values are very close for all the above variables hence we will prefer filling the median values.","8d4e4346":"Also converting the Churn variable to object","9a18579a":"# Feature Engineering - Adding new variables","cf069cb2":"# One Hot coding and Scaling of the data","33706d58":"Here since we have a scaled & encoded dataframe, along with the clusters. lets use that dataframe","9264d0da":"### Creating the Dendrogram - Importing dendrogram and linkage module","9c84f826":"## Train Test Split","d8b9f241":"# EDA - Univariate Analysis - Analysing Churn by each Variable","b4648a9c":"## Linear Discriminant Analysis\n","f17fca9f":"From the preliminary Data set we can see that there are certain missing values. There are no duplicate entries.\n\nThere are 5630 rows and 20 columns.\n\nLooking at the Data types, it doesnt look like there's any anomaly in the values in the dataset. ","4299df88":"Checking for the Data Hygiene in the dataset -> Lets see the value counts of each of the categorical variables","8423931f":"# EDA - Bivariate Analysis","c1ff6c18":"## XGBoost","460490a8":"# Hierarchical Clustering","a5f5c18e":"creating the dendrogram","22296da0":"selecting the target variable or label","e874bae7":"Here we can see that we have treated all the outliers. The outliers are now replaced with their corresponding upper range or lower range values","941c30eb":"## Logistic regression","d46d0da3":"## Correcting the imbalanced data with the help of SMOTE oversampling ","2a837a35":"###Scaling the data","72b17f04":"Lets make a list of the categorical and numerical variables these will be helpful in analysing the data efficiently","f40eed34":"## Decision Tree","25404f9f":"Lets caurve out a new variable -> Average Cashback per order","ac2180d0":"### Importing the KMeans from sklearn","a8daf27d":"### importing fcluster to create clusters","9a4c1b27":"Lets first single out those variables where there are missing values:","74561f6e":"# Outlier Treatment","4b75a998":"# Analysing the Churn by each Variable - Visualising via Graphs\n\n","92733587":"We will now treat outliers. For this we will define the lower range and upper range which is going to be at a distnace of 1.5 times the Interquartile range from the respective whiskers","1b3e4d6a":"Copying the data frame into another one","d1322364":"# K means Clustering","e0249c43":"# Understanding the dataset","89e0dada":"# Building Classification models","0f74f8f2":"The variables above look fine hence there is no need to do anything from the perspective of data hygiene "}}