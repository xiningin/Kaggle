{"cell_type":{"b39dbe11":"code","199e10fa":"code","5f518d6e":"code","2443eaa5":"code","24d1acba":"code","36539f49":"code","6a0e1485":"code","16a3cdc5":"code","8d5c3d0d":"code","5b5fa72e":"code","42997729":"code","7acea985":"code","59ff94ec":"code","f6798adc":"code","f780c5c0":"code","2bfc7e12":"code","f9c40dea":"code","7cbe3ac2":"code","28f9c1fd":"code","4b2e9a3d":"code","c56125ee":"code","5103fee6":"code","ad814a75":"code","194ca17c":"code","84f3d4a7":"code","ffce13cf":"code","212bea97":"markdown","fab06d05":"markdown","4a437714":"markdown","79125c75":"markdown","dd942ea1":"markdown","96d50a6b":"markdown","556915b2":"markdown","887739f0":"markdown","61339e9d":"markdown","1a74f81d":"markdown","59103c1e":"markdown","68e70320":"markdown","0a660a40":"markdown","4b23bcd9":"markdown","40c6afa3":"markdown","e3abf1ce":"markdown","2bdfb982":"markdown","3af272cf":"markdown","906b944c":"markdown","cf8a32bb":"markdown","9a4b6b78":"markdown","077e8c2a":"markdown","aa975b25":"markdown","8e013778":"markdown","bf8fb96e":"markdown","7b495c46":"markdown","ee62aa1f":"markdown","ade918e6":"markdown","7ae17de7":"markdown","2c5ff9fc":"markdown"},"source":{"b39dbe11":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os","199e10fa":"# Set columns to most suitable type to optimize for memory usage, \n# We can also drop passenger_count since it's not used in this kernel, but I just keep it since it uses only uint8\ntypes = {'fare_amount': 'float32',\n         'passenger_count': 'uint8'}\n\n# Columns to load for training data\ncols_train = ['fare_amount', 'pickup_datetime', 'passenger_count', 'pickup_datetime']\n\n# Columns to load for test data\ncols_test = ['pickup_datetime', 'passenger_count']","5f518d6e":"%%time\ni = 0\ndf_list = [] # list to hold the batch dataframe\nchunksize = 10_000_000 # 10 million rows at one go. \nfor df_chunk in pd.read_csv('..\/input\/train.csv', usecols=cols_train, dtype=types, chunksize=chunksize):\n    \n    i = i+1\n    print(f'DataFrame Chunk {i}')\n    \n    # Neat trick from https:\/\/www.kaggle.com\/btyuhas\/bayesian-optimization-with-xgboost\n    # Slicing off unnecessary components of the datetime and specifying the date \n    # format results in a MUCH more efficient conversion to a datetime object.\n    df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n    df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    \n    df_list.append(df_chunk) ","2443eaa5":"# Merge all dataframes into one dataframe\ntrain_df = pd.concat(df_list)\n\ndel df_list\n\ntrain_df.info()","24d1acba":"# save both training and test data to feather format\nos.makedirs('tmp', exist_ok=True)\ntrain_df.to_feather('tmp\/taxi-train-no-gps.feather')\n\ntest_df = pd.read_csv('..\/input\/test.csv', parse_dates=[\"pickup_datetime\"], usecols=cols_test,\n                         infer_datetime_format=True, dtype=types)\ntest_df.to_feather('tmp\/taxi-test-no-gps.feather')","36539f49":"%%time\ntrain_df = pd.read_feather('tmp\/taxi-train-no-gps.feather')","6a0e1485":"test_df = pd.read_feather('tmp\/taxi-test-no-gps.feather')","16a3cdc5":"display(train_df.head())\ndisplay(train_df.tail())","8d5c3d0d":"display(test_df.head())\ndisplay(test_df.tail())","5b5fa72e":"# Group all training data rides by month+year combination and aggregate them by count, mean, median, min and max\nfare_grouped_df = train_df.fare_amount.groupby([train_df.pickup_datetime.dt.year,train_df.pickup_datetime.dt.month])\n\nfare_count_df = fare_grouped_df.count()\nfare_mean_df = fare_grouped_df.mean()\nfare_median_df = fare_grouped_df.median()\nfare_min_df = fare_grouped_df.min()\nfare_max_df = fare_grouped_df.max()","42997729":"# for drawing barchart, so we don't have to retype the following everytime\ndef draw_barchart(df, title):\n    fig = plt.figure(figsize=(20, 4))\n    ax = fig.add_subplot(111)\n    df.plot(kind='bar')\n    ax.set_xlabel(\"(Year, Month)\")\n    plt.xticks(rotation=60) \n    plt.title(title)\n    plt.show()","7acea985":" fare_count_df.describe()","59ff94ec":"test_group_df = test_df.pickup_datetime.groupby([test_df.pickup_datetime.dt.year,test_df.pickup_datetime.dt.month]).count()","f6798adc":"draw_barchart(fare_count_df, 'Monthly total rides of training data from January 2009 to June 2015')\ndraw_barchart(test_group_df, 'Monthly total rides of test data from January 2009 to June 2015')","f780c5c0":"fare_mean_df.describe()","2bfc7e12":"draw_barchart(fare_mean_df,'Monthly mean of fare from January 2009 to June 2015')","f9c40dea":"fare_median_df.describe()","7cbe3ac2":"draw_barchart(fare_median_df,'Monthly median of fares from January 2009 to June 2015')","28f9c1fd":"fare_min_df.describe()","4b2e9a3d":"fare_min_df.mode()","c56125ee":"# take note that we are checking the fare amount in the original dataframe, not the aggregated one\nprint(f\"Number of rides below $0.00: \\t\\t{len(train_df.fare_amount[train_df.fare_amount<0])}\")\nprint(f\"Number of rides at $0.00: \\t\\t{len(train_df.fare_amount[train_df.fare_amount==0])}\")\nprint(f\"Number of rides between $0.01 & $2.49:  {len(train_df.fare_amount[train_df.fare_amount.between(0,2.50,inclusive=False)])}\")\nprint()\nprint(f\"Number of rides below $2.50(all above): {len(train_df.fare_amount[train_df.fare_amount<2.5])}\")\nprint()\nprint(f\"Number of rides at $2.50: \\t\\t{len(train_df.fare_amount[train_df.fare_amount==2.5])}\")\nprint(f\"Number of rides of more than $2.50: \\t{len(train_df.fare_amount[train_df.fare_amount>2.5])}\")","5103fee6":"draw_barchart(fare_min_df, 'Monthly minimum of fares from January 2009 to June 2015')","ad814a75":"fare_max_df.describe()","194ca17c":"draw_barchart(fare_max_df, 'Monthly maximum fares from January 2009 to June 2015')","84f3d4a7":"sorted_max_fare = fare_max_df.sort_values(ascending=False)\ndraw_barchart(sorted_max_fare.head(30), 'Sorted monthly maximum fares')","ffce13cf":"sorted_max_fare.head(30)","212bea97":"From January 2009 until September 2012, the monthly mean fare is quite uniform. \n\nThere is a gradual increase from January until December before it dropped again on January of following year.\n\nHowever, there is a big jump from October 2012 onwards. The monthly mean fare increased by about \\$3 during this period. Probably a price increase at this point onwards.\n\nThe pattern where the monthly mean fare drops from December to January remains true during this period.","fab06d05":"As a follow up from my previous [kernel](https:\/\/www.kaggle.com\/szelee\/how-to-import-a-csv-file-of-55-million-rows), I proceed to do EDA on the entire 55 million rows of data. \n\nThe main ideas is to load the CSV without the pickup and dropoff coordinates. By selectively choosing the columns during CSV file reading, we can reduce the memory usage and loading time.\n\nI focus on the analysis of monthly taxi ride for this kernel.\n\nTLDR:\n1. The test data distribution is quite **different** from training data in terms of monthly total taxi rides. This is not my discovery, I read about it from this [kernel](https:\/\/www.kaggle.com\/akosciansky\/using-ml-for-data-exploration-feat-engineering) by @akosciansky\n1. There is a sharp increase in taxi fare from **September 2012** onward. This matches the news of [New York Taxis to Start Charging Increased Rates](https:\/\/www.nytimes.com\/2012\/09\/04\/nyregion\/new-york-taxis-to-start-charging-increased-rates.html)\n1. **\\$2.50** is the reasonable minimum cut-off point to remove outliers. \n1.  **\\$500** is the reasonable maximum cut-off point to remove outliers (further analysis on pickup and dropoff corrdinates should lower this value)\n1. Most of the outliers of both maximum and minimum monthly fare lie in the  same periods of time.","4a437714":"Here we can see the median is relatively uniform from January 2009 to September 2012\n\nNot surprisingly, as we've seen in the monthly mean fare bar chart, there is a big jump from October 2012 onwards. \n\nThe monthly median fare increased permanently by about almost \\$2.00 during this period of September 2012 to June 2015.\n\nA quick Google search verifies this:\n\nhttps:\/\/www.nytimes.com\/2012\/09\/04\/nyregion\/new-york-taxis-to-start-charging-increased-rates.html\n\nOne idea is we can create a boolean column based on the month and year to indicate whether it is before or after the price increase. This will help our model to learn better.","79125c75":"We also do the aggregate by count for test data, since we are only concern with the **count** of rides here, not the **fare**. We can then compare this with training data.","dd942ea1":"1. Remove outliers based on the findings above. See how much outliers we removed.\n1. Revisit all the monthly group and aggregates after the previous step, see if we have cleaner data.\n1. Explore other parts of datetime to see how is the fare affected by week of the year, day of the week and time of the day.\n1. Consider adding a holiday or calendar events column to see if holiday or other events affect the taxi ride count.","96d50a6b":"This has only used less than **700Mb** of memory for the entire **55 million rows** of data.\n\n(Bear in mind that we have purposely ignore pickup and dropoff coordinates for this analysis, otherwise it would consume about **1.5Gb** of memory instead)\n\nAs comparison, with eat-all-you-can loading such as:\n\n`train_df =  pd.read_csv('..\/input\/train.csv', nrows = 10_000_000)`\n\nwill result in about **610Mb** just for the first **10 million rows**, and \n\n`train_df =  pd.read_csv('..\/input\/train.csv')`\n\nwill consume more than **3Gb** of memory for entire **55 million rows**!","556915b2":"1. By selectively choosing the columns\/features during CSV file loading, we can reduce the memory usage of reading the **entire csv files of 55 million rows from 3Gb++ to less than 700Mb**.\n1. By saving the raw dataframe to feather format, we can reload the entire dataframe in subsequent session much faster from a matter of **minutes to seconds**. There is no need to read the CSV file again.\n1. Test data distribution is quite **different** from training data in terms of monthly total taxi rides. This is not my discovery, I derived from this [kernel](https:\/\/www.kaggle.com\/akosciansky\/using-ml-for-data-exploration-feat-engineering). This could affect how we create our validation data in order to maximize our test data score.\n1. There is a jump in taxi fare by about \\$2 to \\$3 from September 2012. This matches the news report of [New York Taxis to Start Charging Increased Rates](https:\/\/www.nytimes.com\/2012\/09\/04\/nyregion\/new-york-taxis-to-start-charging-increased-rates.html). We can create a boolean column to indicate whether the taxi charge is before or after the rate increase by checking the month and year.\n1. From monthly minimum fare, there is a high occurence of **\\$2.50**, which is the real minimum fare for NYC taxi ride.  Using this value is better than just simply removing rows with negative fare.\n1. From monthly maximum fare, **\\$500** is the reasonable cut-off point to remove outliers. To bring down this maximum value, we need to study the pickup and dropoff coordinates for both training and test data.\n1. The outliers of maximum and minimum fare almost fall in the same periods of time - February 2010, March 2010, August 2013, and January to June of 2015 (more apparent in February and May).","887739f0":"# Summary Findings","61339e9d":"# Load data csv format\n(This section only needs to be executed once! Subsequently, once feather format data is created, jump straight to **Load data feather format** section below which is much faster!","1a74f81d":"Luckily the chart above points shows us very clearly the few outliers that we should remove immediately.\n\nQuite similar to the monthly minimum fare chart, there are outliers in the same periods, namely February 2010, \nMarch 2010, August 2013, and January to June of 2015 (more apparent in February and May).\n\nWe can sort the maximum monthly fare for easier observation.","59103c1e":"## Monthly median fare","68e70320":"# Import libraries","0a660a40":"## Monthly ride count","4b23bcd9":"# Group and aggregate by year-month","40c6afa3":"## Monthly mean fare","e3abf1ce":"# Todo","2bdfb982":"Many minimum fare outliers fall in simply a few periods, such as February 2010, March 2010, August 2013, and January to June of 2015. \n\nRemoving all rows with fare below \\$.250 will remove those outliers.","3af272cf":"While training data barchart has a relative uniform monthly total rides, the test data monthly total rides are not spread evenly, with some random huge spikes. We might need to consider this when we create our validation set.","906b944c":"# Exploring 55 million rows of NYC Taxi Fare data with efficient memory usage","cf8a32bb":"Seem that we only need to remove the first 9 elements in the list above. We can confirm by printing out the values.","9a4b6b78":"The 25% percentile, 50% percentile (median) and 75% percentile all shows \\$2.5. Which is probably the real minimum fare.\n\nMost people just assume the minimum fare of \\$0 and drop the negative fares. We need to dig deeper.","077e8c2a":"## Monthly maximum fare","aa975b25":"# Load data feather format\n(begin from here directly once we have the feather format file)","8e013778":"## Monthly minimum fare","bf8fb96e":"Mode value is also \\$2.50","7b495c46":"This section is derived from this [kernel](https:\/\/www.kaggle.com\/akosciansky\/using-ml-for-data-exploration-feat-engineering) by @akosciansky","ee62aa1f":"There are obviously some extreme values we need to get rid of before the statistics would make any sense.\n\nThe mean value of \\$3526 for taxi ride is absurd.\n\nUnlike minimum fare, we have no way of working out the optimal value for maximum fare.\n\nObviously, this is where the dependency with pickup and dropoff coordinates matters more - from where, to where, and how far you travel determines how much is the fare, instead of the date and time when you travel.\n\nWe rely more on visualization help from barchart.","ade918e6":"From the sorted values above, the 9th element has value of \\$900. Thus we can safely remove those above **\\$500.00** as outliers at this point.\n\nThis is better than just picking maximum value by gut feeling.\n\nThe maximum values of \\$500.00 for taxi fare is still quite mind-boggling, but this is because we haven't considered the pickup-dropoff coordinates. It's most likely there are some extremely long taxi rides that we need to check.","7ae17de7":"There are 2,454 cases of negative fare, 1,380 cases of zero fares and 913 cases more than \\$0 but below \\$2.50. \n\nIn total, these three categories add up to 4,747 cases.\n\nThere are 224,309 cases at \\$2.50 in comparison.\n\nFrom above we can see above that \\$2.50 would be a more reasonable minimum fare, as can be confirmed below:\n\nhttp:\/\/www.nyc.gov\/html\/tlc\/html\/passenger\/taxicab_rate.shtml\n\nHence we should remove those below **\\$2.50** as outliers, instead of just dropping rows with negative fares.","2c5ff9fc":"Notice it took only a few seconds to load the entire dataframe."}}