{"cell_type":{"e962d5ce":"code","b4329fe6":"code","6cda47b4":"code","0c0dfd36":"code","ab820ec3":"code","a1da1100":"code","e0d8b11a":"code","e6aa19da":"code","a9d7f713":"code","9234d45a":"code","0cad557e":"code","ee1444fb":"code","258ad219":"code","7d93e037":"code","5cb10e30":"code","22816c34":"code","fa52dff6":"code","0b5569fc":"code","acf7a231":"code","4ca830b2":"code","435381ce":"code","2830779f":"code","9f87a165":"code","c3c607d5":"code","78007a53":"code","6a4fe4e8":"code","d49addb2":"code","9b923d78":"code","ec0f33c9":"code","d2acd4cb":"code","86921e20":"code","695c9cc3":"code","a3e9047c":"code","a2522e63":"code","c9c818b1":"code","b31a077c":"code","45fdc34d":"code","f8e721d1":"code","b1ac12b3":"code","5a292812":"code","0ed56562":"code","6d43bddd":"code","cd1571d3":"code","c57b97ec":"code","86f9a6ac":"code","1a76a2d8":"code","a9fe6ab5":"code","ae8c25fb":"code","adf41c68":"code","565da9d5":"code","11c0d0f4":"code","3cf45165":"code","7fd79f03":"code","6a5fa5f2":"code","8db51095":"code","7441c3d9":"code","01996785":"code","f539d22d":"code","1cde8cc2":"code","12c7dfd2":"code","3618ecfb":"code","4122636d":"code","41b913d0":"code","64134568":"code","72fcfa6c":"code","4bc02c2d":"code","9f200c51":"markdown","aac16aaa":"markdown","969a3e76":"markdown","f2a3fe32":"markdown","06013d49":"markdown","6ae64d70":"markdown","2e8dfd8e":"markdown","1b01fe0f":"markdown","20b71af7":"markdown","e42993be":"markdown","acd78042":"markdown","b789f9c2":"markdown","a5017a2f":"markdown","f49c4f02":"markdown","e270ad2f":"markdown","97d35f8a":"markdown","6712a52c":"markdown","340b1149":"markdown","88506826":"markdown","22c62149":"markdown","afd38d64":"markdown","2451f8a3":"markdown","0c45fed0":"markdown","531cecde":"markdown","4a2afff3":"markdown","9412c0e5":"markdown","f799044b":"markdown","fbe2e9f5":"markdown","f46d570c":"markdown","6f1ca683":"markdown","c40ab560":"markdown","6d6f601b":"markdown","1f0da2d7":"markdown","2e071e3a":"markdown","1d2e0d62":"markdown","d98493fd":"markdown","128e59b3":"markdown","1f2453f0":"markdown","287d5625":"markdown","18bf4cb1":"markdown","ecb7d020":"markdown","b72e2413":"markdown","ab963f6b":"markdown","f0634def":"markdown","44993e22":"markdown","bec91326":"markdown","b5953d5a":"markdown"},"source":{"e962d5ce":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nseed = 93","b4329fe6":"# Basic\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport os\nimport json\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom pandas_summary import DataFrameSummary\nfrom IPython.display import display\n\n# Sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD","6cda47b4":"PATH = '..\/input\/'","0c0dfd36":"!ls {PATH}","ab820ec3":"train_raw = pd.read_csv(f'{PATH}\/train\/train.csv', low_memory=False)\ntest_raw = pd.read_csv(f'{PATH}\/test\/test.csv', low_memory=False)","a1da1100":"train_raw.shape, test_raw.shape","e0d8b11a":"train_raw.head(3)","e6aa19da":"test_raw.head(3)","a9d7f713":"train_raw.describe().T","9234d45a":"test_raw.describe().T","0cad557e":"train_raw.isnull().sum(axis=0)","ee1444fb":"train_raw.isnull().sum(axis=1).sort_values(ascending=False).head(10)","258ad219":"train_raw['from_dataset'] = 'train'\ntest_raw['from_dataset'] = 'test'\nalldata = pd.concat([train_raw, test_raw], axis = 0)","7d93e037":"feats_counts = alldata.nunique(dropna = False)","5cb10e30":"feats_counts.sort_values()[:10]","22816c34":"alldata.fillna('NaN', inplace=True)","fa52dff6":"alldata.head(5).T","0b5569fc":"train_raw['AdoptionSpeed'].value_counts().sort_index().plot('bar');","acf7a231":"sns.set(style=\"darkgrid\")\nplt.figure(figsize=(14,6))\nax = sns.countplot(x=\"from_dataset\", data=alldata, hue='Type')\nplt.title('Number cats and dogs in train and test sets');","4ca830b2":"plt.figure(figsize=(14,6))\nplt.ylabel('Age')\nplt.plot(alldata['Age'], '.');","435381ce":"alldata['Age'].value_counts().head(20)","2830779f":"alldata['age_mod_12'] = alldata['Age'].apply(lambda x: True if (x%12)==0 else False)","9f87a165":"# dogs\ndogs = alldata.loc[alldata['Type'] == 1]\n# cats\ncats = alldata.loc[alldata['Type'] == 2]","c3c607d5":"dogs['Name'].value_counts().head(20)","78007a53":"cats['Name'].value_counts().head(20)","6a4fe4e8":"alldata['NaN_name'] = alldata['Name'].apply(lambda x: True if str(x) == 'NaN' else False)","d49addb2":"alldata['NaN_name'].value_counts()","9b923d78":"alldata[alldata['Name'].apply(lambda x: len(str(x))) < 3]['Name'].unique()","ec0f33c9":"alldata['name_len_one_or_two'] = alldata['Name'].apply(lambda x: True if len(str(x)) < 3 else False)","d2acd4cb":"alldata['name_len_one_or_two'].value_counts()","86921e20":"alldata[alldata['Name'].apply(lambda x: len(str(x))) == 3]['Name'].unique()","695c9cc3":"plt.figure(figsize=(14,6))\nplt.ylabel('Fee')\nplt.plot(alldata['Fee'], '.');","a3e9047c":"train_raw['RescuerID'].value_counts().head(15)","a2522e63":"test_raw['RescuerID'].value_counts().head(15)","c9c818b1":"top_20_rescuers = list(train_raw['RescuerID'].value_counts()[:20].index)\ntop_20_data = train_raw.loc[train_raw['RescuerID'].isin(top_20_rescuers)]","b31a077c":"plt.figure(figsize=(10,4))\ntop_20_data['AdoptionSpeed'].value_counts().sort_index().plot('bar');\nplt.title('AdoptionSpeed of the top20 rescuers');","45fdc34d":"plt.figure(figsize=(10,4))\ntrain_raw['AdoptionSpeed'].value_counts().sort_index().plot('bar');\nplt.title('Adoptionspeed in the whole training sample');","f8e721d1":"def desc_len_feature(df):\n    descs = np.stack([item for item in df.Description])\n    desc_len = [len(item) for item in descs]\n    # Add the features to the dataframe\n    df['desc_length'] = desc_len\n    \ndef rescue_count_feature(df):\n    rescuers_df = pd.DataFrame(df.RescuerID)\n    rescuer_counts = rescuers_df.apply(pd.value_counts)\n    rescuer_counts.columns = ['rescue_count']\n    rescuer_counts['RescuerID'] = rescuer_counts.index\n    df = df.merge(rescuer_counts, how='left', on='RescuerID')\n    return df\n\ndef name_len_feature(df):\n    names = np.stack([item for item in df.Name])\n    name_len = [len(item) for item in names]\n    df['name_length'] = name_len\n    return df\n","b1ac12b3":"# We have the convert the IDs into categories in order to create the features.\nalldata['RescuerID'] = alldata.RescuerID.astype('category')\nalldata['RescuerID'] = alldata.RescuerID.cat.codes\ndesc_len_feature(alldata)\nname_len_feature(alldata)\nalldata = rescue_count_feature(alldata)","5a292812":"# Kudos to https:\/\/www.kaggle.com\/artgor\/exploration-of-data-step-by-step\ndef parse_sentiment_files(datatype):\n    sentiments = {}\n    for filename in os.listdir('..\/input\/' + datatype + '_sentiment'):\n        with open('..\/input\/' + datatype + '_sentiment\/' +  filename, 'r') as f:\n            sentiment = json.load(f)\n            pet_id = filename.split('.')[0]\n            sentiments[pet_id] = {}\n            sentiments[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\n            sentiments[pet_id]['score'] = sentiment['documentSentiment']['score']\n            sentiments[pet_id]['language'] = sentiment['language']\n            \n    return sentiments\n\ndef sentiment_features(df, sentiments):\n    df['lang'] = df['PetID'].apply(lambda x: sentiments[x]['language']\n                                 if x in sentiments else 'no')\n    df['magnitude'] = df['PetID'].apply(lambda x: sentiments[x]['magnitude']\n                                       if x in sentiments else 0)\n    df['score'] = df['PetID'].apply(lambda x: sentiments[x]['magnitude']\n                                   if x in sentiments else 0)\n    return df","0ed56562":"train_sentiment = parse_sentiment_files('train')\ntest_sentiment = parse_sentiment_files('test')\nsentiment_features(alldata, train_sentiment);\nsentiment_features(alldata, test_sentiment);","6d43bddd":"alldata.head()","cd1571d3":"cols = ['Breed1']\nfor col in cols:\n    frequencies = dict(alldata[col].value_counts()\/alldata[col].shape[0])\n    alldata[col + '_frequency'] = alldata[col].apply(lambda x: frequencies[x])","c57b97ec":"headlines = [\"President trump won the election\", \"The world was shocked\",\n              \"Barcelona won the champions league\"]","86f9a6ac":"vectorizer = CountVectorizer(analyzer='word')\nX = vectorizer.fit_transform(headlines)\ncolumns = [x for x in vectorizer.get_feature_names()]\npd.DataFrame(X.todense(), columns=columns)","1a76a2d8":"count_matrix = pd.DataFrame(X.todense(), columns=columns)\ntfidf = TfidfTransformer()\ninverse_frequencies = tfidf.fit_transform(count_matrix)\npd.DataFrame(inverse_frequencies.todense(), columns=columns)","a9fe6ab5":"def tfidf_features(corpus):\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,)\n    tfv.fit(list(corpus))\n    X = tfv.transform(corpus)\n    return X\n    \ndef svd_features(df, freq_matrix, n_comps=1):\n    svd = TruncatedSVD(n_components=n_comps) #Choose 20 most relevant ones.\n    svd.fit(freq_matrix)\n    freq_matrix = svd.transform(freq_matrix)\n    freq_matrix = pd.DataFrame(freq_matrix, columns=['svd_{}'.format(i) for i in range(n_comps)])\n    df = pd.concat((df, freq_matrix), axis=1)\n    return df","ae8c25fb":"X = tfidf_features(alldata['Description']); X","adf41c68":"alldata=svd_features(alldata, X)","565da9d5":"alldata.head()","11c0d0f4":"# Store PetID for later\ntrain_pet_ids = train_raw.PetID\ntest_pet_ids = test_raw.PetID","3cf45165":"alldata = alldata.drop(['Description', 'PetID', 'Name', 'lang', 'RescuerID'], axis=1)","7fd79f03":"# Split the feature engineered dataframe back into test and train sets.\ntrain = alldata.loc[alldata['from_dataset'] == 'train']\ntest = alldata.loc[alldata['from_dataset'] == 'test']","6a5fa5f2":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(train['AdoptionSpeed'])\ny = pd.DataFrame(y, columns=['AdoptionSpeed'])","8db51095":"train = train.drop(['from_dataset', 'AdoptionSpeed'], axis=1)\ntest = test.drop(['from_dataset', 'AdoptionSpeed'], axis=1);","7441c3d9":"m = RandomForestClassifier(n_estimators=500, random_state=seed,\n                           max_features='sqrt',\n                           min_samples_leaf=25, n_jobs=-1);\nm.fit(train, y);","01996785":"def rf_feature_importance(df, m):\n    df = pd.DataFrame({'cols' : df.columns, 'imp' : m.feature_importances_}).sort_values('imp', ascending=False)\n    return df","f539d22d":"fi = rf_feature_importance(train, m)\nfi[:25].plot('cols', 'imp', kind='barh', legend=False);","1cde8cc2":"train.shape","12c7dfd2":"to_keep = fi[fi.imp>0.005].cols; len(to_keep)\ntrain = train[to_keep].copy()\ntest = test[to_keep].copy()\ntrain.shape","3618ecfb":"train.head(3)","4122636d":"m = RandomForestClassifier(n_estimators=500, random_state=seed,\n                           max_features='sqrt',\n                           min_samples_leaf=1, n_jobs=-1)\n\ntest_preds = np.zeros(test.shape[0])\nresults=[]\nn_folds = 4\ncv = StratifiedKFold(n_splits=n_folds, random_state=seed)\nfor (train_idx, valid_idx) in cv.split(train,y):\n    m.fit(train.iloc[train_idx], y.iloc[train_idx])\n    score = metrics.cohen_kappa_score(y.loc[valid_idx], m.predict(train.iloc[valid_idx]), weights='quadratic')\n    results.append(score)\n    y_test = m.predict(test)\n    test_preds += y_test.reshape(-1)\/n_folds\n    \nmean = np.mean(results)\nstd = np.std(results)","41b913d0":"print(f'Mean kappa score: {mean} with std: {std}')","64134568":"fi = rf_feature_importance(train, m)\nfi[:25].plot('cols', 'imp', kind='barh', legend=False);","72fcfa6c":"df_sub = pd.DataFrame({'PetID' : test_pet_ids})\ndf_sub['AdoptionSpeed'] = test_preds\ndf_sub['AdoptionSpeed'] = df_sub['AdoptionSpeed'].astype(int)\ndf_sub.head()","4bc02c2d":"df_sub.to_csv(f'submission.csv', index=False)","9f200c51":"### Removing features\nLet's remove all features with importance less than half a percent as having those just wastes computation.","aac16aaa":"We found no constant features, therefore we cannot remove any columns here.","969a3e76":"It is odd that the test set does not seem to contain any of the same ID:s. This tells us something about how the organizers have split the data. It is not a random split, but might be somehow related to the people who have rescued the animals.\n\nThis also means that we most likely should remove the RescuerID in the modelling part.\n\n\nWe can still use the amount of animals a person has rescued as a feature. It would make sense that it matters in the prediction in some way.","f2a3fe32":"### Bag of words features\nAs we can see the dataframe also contains text descriptions of the pets. This probably has at least some relationship with the target variable. We need to turn the description into some meaningful features that our algorithms can utilize. First we use bag of words techniques. These techniques rely on counting how many times words appear in documents.\n\nWe will use a variation of this technique called term frequency inverse document frequency (**TFIDF**) to generate some features from the descriptions.\n\nThis technique relies on counting the frequncies of words, but also solves the problem of frequent but uninformative words like \"the\" by taking the logarithm of the frequency.\n\nLet's look at an example to understand this.","06013d49":"This table reveals us something else that is strange. Age of 12, 24, 36, 48, 60, 72 are high in the listing. This does not make intuitive sense, so it could be some behaviour that could help our model. We could create a feature Age modulo 12 to capture this behaviour to make it easier for tree based models to split on this feature if it has some significance.","6ae64d70":"Here we see that there are no rows that contain two NaN values.","2e8dfd8e":"### Fee\nThe adoption fee could be interesting feature, since money always drives human behaviour.","1b01fe0f":"## Reading in the data","20b71af7":"### Removing duplicated features\nIt is pretty clear from just looking at our dataframe that we do not have any duplicated features, therefore we can skip this step here. If our dataframe was larger it would be good to check, since having two features with exactly the same values just wastes computation.\n\nFinal thing to do is to fill all the NaN values with some field so our models can use them and we can find them easier.","e42993be":"## Data cleaning\n### Null values\nLet's check which columns contain null values.","acd78042":"From this picture we can see that the train set contains more dogs and the test set contains more cats.\n\nSince our dataset is fairly small this is likely due to random chance occured while sampling the data.","b789f9c2":"### Removing constant features\nIf a feature is constant among all rows, it contains no useful information for our models, so we should just remove it.\n\nIt's convenient to do all the feature engineering to both train and test set at the same time, so we create one big dataframe here. We also create a new feature to each of the dataframes to indicate whether it was from the test or train set. This makes it easier to plot stuff.","a5017a2f":"## Building a baseline RF model\n\nRandomforest is algorithm based on the concept of bagging (bootstrap aggregating). In random forest you create multiple decision trees by randomly sampling the rows and columns of the data. This way you create multiple trees that contain random errors. The key word is random, since if your errors are truly random the expected value of random errors is 0. Then when you combine these trees you find the true relationship. If the errors are not random the model won't work well.\n","f49c4f02":"### Notes:\n    - Rescue_count seems to be very good feature, so our hypothesis was correct, that who rescued the animal matters in the adoptionspeed.\n    \n    - Age is the second most important feature. Im interested to see how the split went. Let's try tree interpreter to do that.\n    \n    - Age%12 feature had some importance, mayber there's something going on there.\n \n    - name_len_one_or_two and NaN_name did not any impact on the model. This could be due to collinearity with the name_length feature. Or maybe this fact does not matter in the prediction\n    \n    - Description based features are very high in the feature importance. We should really investigate word2vec approach in order to generate more advanced features from the descriptions.","e270ad2f":"### Frequency encodings\nIn frequency encoding we utilize the frequencies of categories. It can help tree based models deal with high cardinality categorical features more easily, if the frequency of breed has some correlation with the target variable.","97d35f8a":"Here we can see one of the problems with this approach even with moderately small dataset the word frequency matrix becomes huge. To combat this issue let's take SVD of this matrix and use only the most relevant components.\n\nAfter some hyperparameter tuning it seems that only using the first component helps in the prediction.\n\nShould investigate what could be the reason for this since it seems odd, maybe random forest cannot utilize this information very well..\n\nWe should also try word2vec approach, since it can capture relationships between words.","6712a52c":"Test set shows the same relationship with the age variable.","340b1149":"Here we can see that Name and Description fields contain null values. These null values could affect the adoptionspeeds in some way. We should explore this more in the visualization part.","88506826":"## Type\nAccording to the kaggle page Type means:\n\n    - 1 --> dog\n    - 2 --> cat","22c62149":"Here we can see that 0 is the class that has significantly less observations. This makes sense, since 0 means that the pet was adopted on the same day it was listed and this is quite unlikely by applying commonsense.\n\nAlso we can see that the most frequent class is 4, which means that the pet was not adopted in 100 days. Finding out what features drive this class is the most important thing for the analysis, since then we  can maybe save those pets.","afd38d64":"### Feature importance\nIt's good to look at Random Forest feature importance plot to get some idea of what features are important. This has the issue that when there's collinearity the feature importance can be skewed and the importances are split between the colinear features, but it still gives us some idea on what features might be important.","2451f8a3":"We can see that in both of the cases NaN is the most common name. This could be important. \n\nWe also see some common names like Kitty, No Name, Kittens appear quite frequently.\n\nWe could make a new feature isNaNname to help tree based models to make a split on this phenomena.\n\nLet's next check if there are names with just 1 or 2 characters long. These names are likely meaningless could contain some information about the adoptionspeeds.","0c45fed0":"#### Notes:\n    \n    -  Maximum age is 255, Probably people have filled out some random number as an age, when they do not know the real age.\n    \n    - There's a lot of categorical features with some number as encoding, we should find out what each these numbers means, when we examine each feature more closely.","531cecde":"It looks like that the top rescuers pets are less likely to be in category 4. This supports my hypothesis that these frequent rescuers do extra work in order for the pet to get adopted.\n\nAlso it seems that on average top rescuers animals take longer time to get adopted than in the whole sample. This could be due to the fact that people who rescue a lot of animals probably rescue animal that are more sick than people who just give their dog away because of some life situation.\n\n\n**TODO**: Continue EDA with more features based on the feedback from the Random forest feature importance.","4a2afff3":"## Submission","9412c0e5":"## RescuerID\nRescuerID is another interesting feature. My hypothesis is that there are some recuers, who constantly rescue animals and therefore use more effort in the process --> Faster adoptionSpeed","f799044b":"By plotting the age feature against its index we can see that the data is shuffled correctly. We can also see many points with unintuitive age values.\n\nAfter some googling I found that oldest dog that ever lived was ~30 years old [https:\/\/en.wikipedia.org\/wiki\/List_of_oldest_dogs] and the oldest ever cat was ~38 years old [https:\/\/en.wikipedia.org\/wiki\/List_of_oldest_cats], therefore there's many values that are just impossible.\n\nLet's see what age values are the most common ones.","fbe2e9f5":"### Size of datasets","f46d570c":"There's quite a few non sense names in here also, but also some real names. Let's keep this in mind, but not do any feature engineering yet.\n\nOne useful feature could be just length of the name, then the models could split on name_len < 4 for example.","6f1ca683":"We can see that our hypothesis was true and we can see bunch on names that are nonsense.\n\nNow let's create a new feature based on this info.","c40ab560":"# Petfinder\n\nIf you are not familiar on what this competition is about you can read the description here:\n\nhttps:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction#description\n\nThis kernel is just for my practice and I have took some ideas from other kernel. I have tried to cite them though.\n\nFor example some of the EDA stuff is inspired by this great EDA kernel https:\/\/www.kaggle.com\/artgor\/exploration-of-data-step-by-step.\n\nFeel free to comment on anything related to this kernel as I'm still really new to data science and would appreciate any comments that could help me improve my skills.\n\n\n**Note this is still work in progress**","6d6f601b":"## Building the final model\nLet's use k-fold cross validation, so we get all information out of our limited dataset.","1f0da2d7":"Let's imagine we have these three news headlines and want to generate tfidf features out of them.\n\n\nFirst thing to do is count the frequencies of the words.","2e071e3a":"## EDA\nNow that we have built some intuition about the data, it's time to find some more insights using various EDA techniques such as plotting.\n\nLet's remind ourselves what the features are.","1d2e0d62":"## Feature engineering\n\n### Basic features\nLet's generate three basic features that came to mind:\n    1. Description length\n    2. Name length\n    3. How many rescued animals a specific rescuerID has","d98493fd":"## Imports","128e59b3":"We can see that the dataset is quite small. We should think about using cross validation, if there's no time series in the data.","1f2453f0":"### Natural language API stuff\nThe organizers have kindly provided us with some scores on the descriptions, by running them through the Google NLP API. Let's use these scores as features as well.","287d5625":"**NOTE** sklearn implementation does some more complicated stuff to prevent division by zero etc. But the basic idea stays the same. We can see word \"the\" getting lower values, because it appears in all of the documents.","18bf4cb1":"## Data overview\n\nLet's try to build a basic intuition about the data first.\n\n### Response variable (AdoptionSpeed)\n- 0 - Pet was adopted the same day as it was listed\n- 1 - Pet was adopted between days 1-7 after being listed (1st week).\n- 2 - Pet was adopted between 8 and 30 days (1st month).\n- 3 - Pet was adopted between 31 and 90 days.\n- 4 - No adoption after 100 days\n\n\n### Independent variables:\n- PetID - Unique hash ID of pet profile\n- AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n- Type - Type of animal (1 = Dog, 2 = Cat)\n- Name - Name of pet (Empty if not named)\n- Age - Age of pet when listed, in months\n- Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n- Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n- Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n- Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n- Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n- Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n- MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n- FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n- Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n- Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n- Sterilized - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n- Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n- Quantity - Number of pets represented in profile\n- Fee - Adoption fee (0 = Free)\n- State - State location in Malaysia (Refer to StateLabels dictionary)\n- RescuerID - Unique hash ID of rescuer\n- VideoAmt - Total uploaded videos for this pet\n- PhotoAmt - Total uploaded photos for this pet\n- Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n\nSo the task is simple. Predict the Adoptionspeed of an animal based on these features.\n\n\nLet's then look at the dataframes to build some intuition about the data","ecb7d020":"### Age\nWe previously found out that age contains some unintuitive values.\n\nLet's investigate this feature further","b72e2413":"Let's test this same hypothesis for names that are the length of 3.","ab963f6b":"Here we can see that CountVectorizer simply counts how many times words appear in a corpus. For example word \"the\" appears in all of the documents.\n\nAs we can imagine word \"the\" has no relevance in prediciting the adoption speed, so we want to scale it down. This is done by taking the inverse document freuquency transformation. \n\nBasically we take the logarithm of the frequency, so very common words get lower values. ","f0634def":"### Summary statistics","44993e22":"## Name\nLet's see what names are the most frequent to find some patterns.","bec91326":"### AdoptionSpeed (Our response variable)\n\nLet's recap what AdoptionSpeed meant:\n- 0 - Pet was adopted the same day as it was listed\n- 1 - Pet was adopted between days 1-7 after being listed (1st week).\n- 2 - Pet was adopted between 8 and 30 days (1st month).\n- 3 - Pet was adopted between 31 and 90 days.\n- 4 - No adoption after 100 days\n","b5953d5a":"Here we can see that most pets have no fees. Also there some abnormally high fees e.g 2000, 3000.\n\n**TODO**: Examine this more closely"}}