{"cell_type":{"ae5df4f2":"code","470ecbf7":"code","2b1dcea0":"code","8e086fec":"code","7c5fecb5":"code","c57959d5":"code","30aa8c86":"code","e4a75d1c":"code","a113fe7c":"code","b7ce0aba":"code","a8ed8ebd":"code","24990de7":"code","1338a809":"code","41278648":"code","74d29198":"code","31475be2":"code","cbe42f7e":"code","84f5602f":"code","4dee05f1":"code","75fd588a":"code","1136e998":"code","f68c681c":"code","628758a0":"code","397e7867":"code","222b569d":"code","6bba5b57":"code","ed035dfc":"code","b076d5b0":"code","54cfbc87":"code","56550554":"code","efe6e74f":"code","8d81d777":"code","c57a232b":"code","bac5fcbe":"code","20bc4c65":"code","22219747":"code","98f3e19d":"code","554571b8":"code","38d06fbf":"code","9f1bc7fe":"code","51da28c3":"code","4fb23d57":"code","3d9266f2":"code","a8e20681":"code","e70d5d23":"code","7d6302dd":"code","5a238369":"code","355998b9":"code","15cb16e7":"code","547248e6":"code","e66d3503":"code","9d13916b":"code","1000f029":"code","f386998e":"code","656dca88":"code","2397f9f0":"code","9018e091":"code","d1e5522c":"code","fade2827":"code","c82e565d":"code","c603209b":"code","fd91363b":"code","e7289ffe":"code","ae6c2e42":"code","d972f0c0":"code","1601a17c":"code","192f265b":"code","b1762040":"code","a4980a14":"code","d4f2c5c5":"code","1ab7591e":"code","7afb8cc7":"code","cfbb65a3":"code","0dbc9b11":"code","7b47bd6f":"code","1e4f3c3a":"code","07e672d1":"code","66ea03f6":"code","919716bd":"markdown","e9db2302":"markdown","772c3ee0":"markdown","7256ccc7":"markdown","f13d7ae7":"markdown","66f786d2":"markdown","a02f1f69":"markdown","d5e434d5":"markdown","f9408b8c":"markdown","15f125a3":"markdown","bed7f154":"markdown","9e0ccd25":"markdown"},"source":{"ae5df4f2":"# Importing all required packages\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\n%matplotlib inline\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import StandardScaler\n\nimport statsmodels.stats.diagnostic as diag\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.stats.stattools import durbin_watson\nimport statsmodels.tsa.api as smt\nfrom statsmodels.compat import lzip\nfrom statsmodels.graphics.gofplots import qqplot_2samples\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline","470ecbf7":"## Generic Functions\n\n# Method to get Meta-Data about any dataframe passed \ndef getMetadata(dataframe) :\n    metadata_matrix = pd.DataFrame({\n                    'Datatype' : dataframe.dtypes, # data types of columns\n                    'Total_Element': dataframe.count(), # total elements in columns\n                    'Null_Count': dataframe.isnull().sum(), # total null values in columns\n                    'Null_Percentage': round(dataframe.isnull().sum()\/len(dataframe) * 100,2) ,# percentage of null values\n                    'Unique_Value': dataframe.nunique()\n                       })\n    return metadata_matrix\n\ndef getVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif\n\ndef binary_map(x):\n    return x.map({'yes': 1, \"no\": 0})\n\ndef hyper_params_selection(X_train,y_train,lm):\n    # step-1: create a cross-validation scheme\n    folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n    hyper_params = [{'n_features_to_select': list(range(len(X_train.columns)))}]\n    lm.fit(X_train, y_train)\n    rfe = RFE(lm)             \n    model_cv = GridSearchCV(estimator = rfe, \n                            param_grid = hyper_params, \n                            scoring= 'accuracy', \n                            cv = folds, \n                            verbose = 1,\n                            return_train_score=True)      \n\n    model_cv.fit(X_train, y_train)                  \n    cv_results = pd.DataFrame(model_cv.cv_results_)\n    plt.figure(figsize=(16,6))\n    plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n    plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n    plt.xlabel('number of features')\n    plt.ylabel('accuracy_score')\n    plt.title(\"Optimal Number of Features\")\n    plt.legend(['test score', 'train score'], loc='upper left')","2b1dcea0":"train_df=pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_df.head()","8e086fec":"test_df=pd.read_csv('..\/input\/titanic\/test.csv')\ntest_df.head()","7c5fecb5":"train_df.shape","c57959d5":"test_df.shape","30aa8c86":"train_metadata=getMetadata(train_df)\ntrain_metadata","e4a75d1c":"test_metadata=getMetadata(test_df)\ntest_metadata","a113fe7c":"train_df[\"Sex\"].unique()","b7ce0aba":"train_df[\"SibSp\"].unique()","a8ed8ebd":"train_df[\"Parch\"].unique()","24990de7":"train_df[\"Cabin\"].unique()","1338a809":"train_df[\"Embarked\"].unique()","41278648":"### Converting Datatype\ntrain_df[\"PassengerId\"]=train_df[\"PassengerId\"].astype(\"object\")\ntrain_df[\"Pclass\"]=train_df[\"Pclass\"].astype(\"object\")\n### Adding new fields\ntrain_df[\"Family_Members\"]=(train_df[\"SibSp\"] + train_df[\"Parch\"]).astype(\"int\")\ntrain_df[\"Fare_PP\"]=(train_df[\"Fare\"]\/(train_df[\"Family_Members\"]+1)).astype(\"float\")\ntrain_df[\"Age\"]=train_df[\"Age\"].fillna((train_df[\"Age\"].mean()))\ntrain_df[\"Embarked\"]=train_df[\"Embarked\"].fillna((train_df[\"Embarked\"].mode()[0][0]))\ntrain_df[\"Cabin\"]=train_df[\"Cabin\"].fillna(\"UNK\")\ntrain_df[\"Deck\"]=train_df[\"Cabin\"].apply(lambda x: \"UNK\" if x==\"UNK\" else x[0])\ntrain_df.drop(['Ticket','Cabin','SibSp','Parch','Name'], axis = 1, inplace = True)\nclassmeans=pd.pivot_table(data=train_df,columns=\"Pclass\",values=\"Fare\",aggfunc=np.mean,fill_value=0)\ntrain_df[\"Fare\"] = train_df[['Fare', 'Pclass']].apply(lambda x: classmeans[x['Pclass']] \n                                                   if pd.isnull(x['Fare']) else x['Fare'], axis=1)","74d29198":"train_df[\"Deck\"].value_counts()","31475be2":"train_df.head()","cbe42f7e":"train_metadata=getMetadata(train_df)\ntrain_metadata","84f5602f":"## Checking survival Ratio\nsurvival = (sum(train_df['Survived'])\/len(train_df['Survived'].index))*100\nsurvival","4dee05f1":"num_list=['Age', 'Family_Members', 'Fare', 'Fare_PP','Survived']\ncat_list=[\"Pclass\",\"Sex\",\"Embarked\",\"Deck\"]","75fd588a":"#### 2.1 Visualising numeric columns\nsns.pairplot(data=train_df,vars=num_list)\nplt.show()","1136e998":"plt.figure(figsize = (16, 10))\nsns.heatmap(train_df[num_list].corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","f68c681c":"plt.figure(figsize=(10,6))\nplt.subplot(2,2,1)\nsns.boxplot(y = 'Age', data = train_df)\nplt.subplot(2,2,2)\nsns.boxplot(y = 'Family_Members', data = train_df)\nplt.subplot(2,2,3)\nsns.boxplot(y = 'Fare', data = train_df)\nplt.subplot(2,2,4)\nsns.boxplot(y = 'Fare_PP', data = train_df)\nplt.show()","628758a0":"plt.figure(figsize=(50, 30))\nplt.subplot(3,3,1)\nsns.barplot(x = 'Pclass', y = 'Survived', data = train_df,estimator=sum)\nplt.subplot(3,3,2)\nsns.barplot(x = 'Sex', y = 'Survived', data = train_df,estimator=sum)\nplt.subplot(3,3,3)\nsns.barplot(x = 'Embarked', y = 'Survived', data = train_df,estimator=sum)\nplt.subplot(3,3,4)\nsns.barplot(x = 'Deck', y = 'Survived', data = train_df,estimator=sum)\nplt.subplot(3,3,5)\nsns.barplot(x = 'Pclass', y = 'Survived',hue=\"Deck\",data = train_df,estimator=sum)\nplt.show()","397e7867":"train_df.head()","222b569d":"train_metadata=getMetadata(train_df)\ntrain_metadata","6bba5b57":"#dummy = train_df[['Pclass','Embarked','Deck',\"Sex\"]]\n#dummy = pd.get_dummies(dummy,drop_first=True)\n#train_df = pd.concat([dummy,train_df],axis = 1)\n## dropping columns for which dummy variables were created\n#train_df.drop(['Pclass','Embarked','Deck',\"Sex\",'Fare'], axis = 1, inplace = True)\n#train_df.shape","ed035dfc":"dummy = train_df[['Pclass','Embarked',\"Sex\"]]\ndummy = pd.get_dummies(dummy,drop_first=True)\ntrain_df = pd.concat([dummy,train_df],axis = 1)\ndummy = pd.get_dummies(train_df[\"Deck\"],prefix=\"Deck\")\ndummy.drop([\"Deck_T\"],axis=1,inplace=True)\ntrain_df = pd.concat([dummy,train_df],axis = 1)\n## dropping columns for which dummy variables were created\ntrain_df.drop(['Pclass','Embarked','Deck',\"Sex\",'Fare'], axis = 1, inplace = True)\ntrain_df.shape","b076d5b0":"scaler = StandardScaler()\ntrain_df[['Age','Fare_PP']] = scaler.fit_transform(train_df[['Age','Fare_PP']])\ntrain_df.head()","54cfbc87":"train_df.head()","56550554":"X_train = train_df.drop(['PassengerId','Survived'], axis=1)\nX_train.head()\ny_train=train_df['Survived']\ny_train.head()","efe6e74f":"#scaler = StandardScaler()\n#X_train[['Age','Fare_PP']] = scaler.fit_transform(X_train[['Age','Fare_PP']])\n#X_train.head()","8d81d777":"logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","c57a232b":"hyper_params_selection(X_train,y_train,LogisticRegression())","bac5fcbe":"# final model\nn_features_optimal = 13\nlm = LogisticRegression() \nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, n_features_to_select=n_features_optimal)             \nrfe = rfe.fit(X_train, y_train)\n\n# predict prices of X_test\ny_pred = rfe.predict(X_train)\naccuracy_sc = sklearn.metrics.accuracy_score(y_train, y_pred)\nprint(accuracy_sc)","20bc4c65":"logreg = LogisticRegression()\nrfe = RFE(logreg, 13)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","22219747":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","98f3e19d":"col = X_train.columns[rfe.support_]\ncol","554571b8":"X_train_rfe = X_train[col]\nX_train_sm = sm.add_constant(X_train_rfe)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","38d06fbf":"X_train_rfe = X_train_rfe.drop([\"Deck_F\"], axis = 1)\nX_train_sm = sm.add_constant(X_train_rfe)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","9f1bc7fe":"X_train_rfe = X_train_rfe.drop([\"Deck_B\"], axis = 1)\nX_train_sm = sm.add_constant(X_train_rfe)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","51da28c3":"X_train_rfe = X_train_rfe.drop([\"Deck_D\"], axis = 1)\nX_train_sm = sm.add_constant(X_train_rfe)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","4fb23d57":"X_train_rfe = X_train_rfe.drop([\"Fare_PP\"], axis = 1)\nX_train_sm = sm.add_constant(X_train_rfe)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","3d9266f2":"X_train_rfe = X_train_rfe.drop([\"Deck_E\"], axis = 1)\nX_train_sm = sm.add_constant(X_train_rfe)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","a8e20681":"vif=getVIF(X_train_rfe)\nvif","e70d5d23":"col=X_train_rfe.columns\ncol","7d6302dd":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","5a238369":"y_train_pred_final = pd.DataFrame({'Survived':y_train.values, 'Survived_Prob':y_train_pred})\ny_train_pred_final['Pass_Id'] = y_train.index\ny_train_pred_final.head(10)","355998b9":"y_train_pred_final['predicted'] = y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.5 else 0)\n# Let's see the head\ny_train_pred_final.head()","15cb16e7":"from sklearn import metrics\nconfusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.predicted )\nprint(confusion)","547248e6":"print(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.predicted))","e66d3503":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None\nfpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Survived, y_train_pred_final.Survived_Prob, drop_intermediate = False )","9d13916b":"draw_roc(y_train_pred_final.Survived, y_train_pred_final.Survived_Prob)","1000f029":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","f386998e":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","656dca88":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","2397f9f0":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_pred_final.Survived, y_train_pred_final.predicted)","9018e091":"recall_score(y_train_pred_final.Survived, y_train_pred_final.predicted)","d1e5522c":"#test_df=pd.read_csv('test.csv')\n#test_df.head()\n### Converting Datatype\ntest_df[\"PassengerId\"]=test_df[\"PassengerId\"].astype(\"object\")\ntest_df[\"Pclass\"]=test_df[\"Pclass\"].astype(\"object\")\n### Adding new fields\ntest_df[\"Family_Members\"]=(test_df[\"SibSp\"] + test_df[\"Parch\"]).astype(\"int\")\ntest_df[\"Age\"]=test_df[\"Age\"].fillna((test_df[\"Age\"].mean()))\ntest_df[\"Embarked\"]=test_df[\"Embarked\"].fillna((test_df[\"Embarked\"].mode()[0][0]))\ntest_df[\"Cabin\"]=test_df[\"Cabin\"].fillna(\"UNK\")\ntest_df[\"Deck\"]=test_df[\"Cabin\"].apply(lambda x: \"UNK\" if x==\"UNK\" else x[0])\ntest_df.drop(['Ticket','Cabin','SibSp','Parch','Name'], axis = 1, inplace = True)\nclassmeans=pd.pivot_table(data=test_df,columns=\"Pclass\",values=\"Fare\",aggfunc=np.mean,fill_value=0)\ntest_df['Fare'] = test_df[['Fare', 'Pclass']].apply(lambda x: classmeans[x['Pclass']] \n                                                   if pd.isnull(x['Fare']) else x['Fare'], axis=1)\ntest_df[\"Fare_PP\"]=(test_df[\"Fare\"]\/(test_df[\"Family_Members\"]+1)).astype(\"float\")","fade2827":"test_df[\"Deck\"].value_counts()","c82e565d":"dummy = test_df[['Pclass','Embarked',\"Sex\"]]\ndummy = pd.get_dummies(dummy,drop_first=True)\ntest_df = pd.concat([dummy,test_df],axis = 1)\ndummy = pd.get_dummies(test_df[\"Deck\"],prefix=\"Deck\")\ntest_df = pd.concat([dummy,test_df],axis = 1)\n## dropping columns for which dummy variables were created\ntest_df.drop(['Pclass','Embarked','Deck',\"Sex\",'Fare'], axis = 1, inplace = True)\ntest_df.shape","c603209b":"test_df.head()","fd91363b":"X_test = test_df.drop(['PassengerId'], axis=1)\nX_test.head()\ny_test=test_df['PassengerId']\ny_test.head()","e7289ffe":"scaler = StandardScaler()\nX_test[['Age','Fare_PP']] = scaler.fit_transform(X_test[['Age','Fare_PP']])\nX_test.head()","ae6c2e42":"X_test_rfe=X_test[col]\nX_test_sm = sm.add_constant(X_test_rfe)\ny_test_pred = res.predict(X_test_sm)","d972f0c0":"y_test_pred[:10]","1601a17c":"y_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survival_Prob'})\ny_pred_final['Survived'] = y_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_pred_final['Survived'].value_counts()","192f265b":"y_pred_final.drop(['Survival_Prob'],axis=1,inplace=True)\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction.csv',index=False)","b1762040":"####### Using Sklearn Prediction \nn_features_optimal = 13\nlm = LogisticRegression() \nlm.fit(X_train, y_train)\nrfe = RFE(lm, n_features_to_select=n_features_optimal)             \nrfe = rfe.fit(X_train, y_train)\ny_test_pred = rfe.predict(X_test)","a4980a14":"y_test_pred[:10]","d4f2c5c5":"y_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final.head()\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived'})\n#y_pred_final['Survived'] = y_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_pred_final['Survived'].value_counts()","1ab7591e":"#y_pred_final.drop(['Survival_Prob'],axis=1,inplace=True)\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction_sklearn.csv',index=False)","7afb8cc7":"####### Using Sklearn Prediction \nlm = LogisticRegression() \nnp.random.seed(0)\ndf_train, df_test = train_test_split(train_df, train_size = 0.7, test_size = 0.3, random_state = 100)\nX_train = df_train.drop(['PassengerId','Survived'], axis=1)\ny_train=df_train['Survived']\nX_validation = df_test.drop(['PassengerId','Survived'], axis=1)\ny_validation=df_test[['PassengerId','Survived']]\nlm.fit(X_train, y_train)\nprint('Train Dataset Accuracy:',lm.score(X_train,y_train))\ny_validation_pred = lm.predict(X_validation)\ny_validation_pred_df = pd.DataFrame(y_validation_pred)\ny_validation_df = pd.DataFrame(y_validation)\ny_validation_df.reset_index(drop=True, inplace=True)\ny_validation_pred_df.reset_index(drop=True, inplace=True)\ny_validation_pred_final = pd.concat([y_validation_df, y_validation_pred_df],axis=1)\ny_validation_pred_final= y_validation_pred_final.rename(columns={ 0 : 'Predicted_Survived'})\nprint('Validation Prediction:')\nprint(y_validation_pred_final['Predicted_Survived'].value_counts())\nprint('Validation Dataset Accurary:',metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived))\nlr_train_score=lm.score(X_train,y_train)\nlr_validation_score=metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived)\n########### Actual Prediction ##########\ny_test_pred = lm.predict(X_test)\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived'})\nprint(y_pred_final['Survived'].value_counts())\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction_sklearn_lr.csv',index=False)","cfbb65a3":"##### Using svm\nfrom sklearn import svm\nlm = svm.SVC()\nnp.random.seed(0)\ndf_train, df_test = train_test_split(train_df, train_size = 0.7, test_size = 0.3, random_state = 100)\nX_train = df_train.drop(['PassengerId','Survived'], axis=1)\ny_train=df_train['Survived']\nX_validation = df_test.drop(['PassengerId','Survived'], axis=1)\ny_validation=df_test[['PassengerId','Survived']]\nlm.fit(X_train, y_train)\nprint('Train Dataset Accuracy:',lm.score(X_train,y_train))\ny_validation_pred = lm.predict(X_validation)\ny_validation_pred_df = pd.DataFrame(y_validation_pred)\ny_validation_df = pd.DataFrame(y_validation)\ny_validation_df.reset_index(drop=True, inplace=True)\ny_validation_pred_df.reset_index(drop=True, inplace=True)\ny_validation_pred_final = pd.concat([y_validation_df, y_validation_pred_df],axis=1)\ny_validation_pred_final= y_validation_pred_final.rename(columns={ 0 : 'Predicted_Survived'})\nprint('Validation Prediction:')\nprint(y_validation_pred_final['Predicted_Survived'].value_counts())\nprint('Validation Dataset Accurary:',metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived))\nsvm_train_score=lm.score(X_train,y_train)\nsvm_validation_score=metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived)\n########### Actual Prediction ##########\ny_test_pred = lm.predict(X_test)\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived'})\nprint(y_pred_final['Survived'].value_counts())\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction_sklearn_svm.csv',index=False)","0dbc9b11":"#### Using Decision Tree\nfrom sklearn import tree\nimport numpy\nlm = tree.DecisionTreeClassifier(criterion='entropy')\nnp.random.seed(0)\ndf_train, df_test = train_test_split(train_df, train_size = 0.7, test_size = 0.3, random_state = 100)\nX_train = df_train.drop(['PassengerId','Survived'], axis=1)\ny_train=df_train['Survived']\nX_validation = df_test.drop(['PassengerId','Survived'], axis=1)\ny_validation=df_test[['PassengerId','Survived']]\nlm.fit(X_train, y_train)\nprint('Train Dataset Accuracy:',lm.score(X_train,y_train))\ny_validation_pred = lm.predict(X_validation)\ny_validation_pred_df = pd.DataFrame(y_validation_pred)\ny_validation_df = pd.DataFrame(y_validation)\ny_validation_df.reset_index(drop=True, inplace=True)\ny_validation_pred_df.reset_index(drop=True, inplace=True)\ny_validation_pred_final = pd.concat([y_validation_df, y_validation_pred_df],axis=1)\ny_validation_pred_final= y_validation_pred_final.rename(columns={ 0 : 'Predicted_Survived'})\nprint('Validation Prediction:')\nprint(y_validation_pred_final['Predicted_Survived'].value_counts())\nprint('Validation Dataset Accurary:',metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived))\ndt_train_score=lm.score(X_train,y_train)\ndt_validation_score=metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived)\n########### Actual Prediction ##########\ny_test_pred = lm.predict(X_test)\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived'})\nprint(y_pred_final['Survived'].value_counts())\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction_sklearn_decision_tree.csv',index=False)","7b47bd6f":"#### Using RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nlm= RandomForestClassifier()\nnp.random.seed(0)\ndf_train, df_test = train_test_split(train_df, train_size = 0.7, test_size = 0.3, random_state = 100)\nX_train = df_train.drop(['PassengerId','Survived'], axis=1)\ny_train=df_train['Survived']\nX_validation = df_test.drop(['PassengerId','Survived'], axis=1)\ny_validation=df_test[['PassengerId','Survived']]\nlm.fit(X_train, y_train)\nprint('Train Dataset Accuracy:',lm.score(X_train,y_train))\ny_validation_pred = lm.predict(X_validation)\ny_validation_pred_df = pd.DataFrame(y_validation_pred)\ny_validation_df = pd.DataFrame(y_validation)\ny_validation_df.reset_index(drop=True, inplace=True)\ny_validation_pred_df.reset_index(drop=True, inplace=True)\ny_validation_pred_final = pd.concat([y_validation_df, y_validation_pred_df],axis=1)\ny_validation_pred_final= y_validation_pred_final.rename(columns={ 0 : 'Predicted_Survived'})\nprint('Validation Prediction:')\nprint(y_validation_pred_final['Predicted_Survived'].value_counts())\nprint('Validation Dataset Accurary:',metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived))\nrf_train_score=lm.score(X_train,y_train)\nrf_validation_score=metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived)\n########### Actual Prediction ##########\ny_test_pred = lm.predict(X_test)\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived'})\nprint(y_pred_final['Survived'].value_counts())\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction_sklearn_rfc.csv',index=False)","1e4f3c3a":"from sklearn.ensemble import GradientBoostingClassifier\n#Assumed you have, X (predictor) and Y (target) for #training data set and x_test(predictor) of test_dataset#Create Gradient Boosting Classifier object\nlm= GradientBoostingClassifier(n_estimators=100,learning_rate=1.0, max_depth=1, random_state=0)\nnp.random.seed(0)\ndf_train, df_test = train_test_split(train_df, train_size = 0.7, test_size = 0.3, random_state = 100)\nX_train = df_train.drop(['PassengerId','Survived'], axis=1)\ny_train=df_train['Survived']\nX_validation = df_test.drop(['PassengerId','Survived'], axis=1)\ny_validation=df_test[['PassengerId','Survived']]\nlm.fit(X_train, y_train)\nprint('Train Dataset Accuracy:',lm.score(X_train,y_train))\ny_validation_pred = lm.predict(X_validation)\ny_validation_pred_df = pd.DataFrame(y_validation_pred)\ny_validation_df = pd.DataFrame(y_validation)\ny_validation_df.reset_index(drop=True, inplace=True)\ny_validation_pred_df.reset_index(drop=True, inplace=True)\ny_validation_pred_final = pd.concat([y_validation_df, y_validation_pred_df],axis=1)\ny_validation_pred_final= y_validation_pred_final.rename(columns={ 0 : 'Predicted_Survived'})\nprint('Validation Prediction:')\nprint(y_validation_pred_final['Predicted_Survived'].value_counts())\nprint('Validation Dataset Accurary:',metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived))\ngbc_train_score=lm.score(X_train,y_train)\ngbc_validation_score=metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived)\n########### Actual Prediction ##########\ny_test_pred = lm.predict(X_test)\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived'})\nprint(y_pred_final['Survived'].value_counts())\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction_sklearn_gbc.csv',index=False)","07e672d1":"from sklearn.neighbors import KNeighborsClassifier\nlm=KNeighborsClassifier(n_neighbors=5)\nnp.random.seed(0)\ndf_train, df_test = train_test_split(train_df, train_size = 0.7, test_size = 0.3, random_state = 100)\nX_train = df_train.drop(['PassengerId','Survived'], axis=1)\ny_train=df_train['Survived']\nX_validation = df_test.drop(['PassengerId','Survived'], axis=1)\ny_validation=df_test[['PassengerId','Survived']]\nlm.fit(X_train, y_train)\nprint('Train Dataset Accuracy:',lm.score(X_train,y_train))\ny_validation_pred = lm.predict(X_validation)\ny_validation_pred_df = pd.DataFrame(y_validation_pred)\ny_validation_df = pd.DataFrame(y_validation)\ny_validation_df.reset_index(drop=True, inplace=True)\ny_validation_pred_df.reset_index(drop=True, inplace=True)\ny_validation_pred_final = pd.concat([y_validation_df, y_validation_pred_df],axis=1)\ny_validation_pred_final= y_validation_pred_final.rename(columns={ 0 : 'Predicted_Survived'})\nprint('Validation Prediction:')\nprint(y_validation_pred_final['Predicted_Survived'].value_counts())\nprint('Validation Dataset Accurary:',metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived))\nknn_train_score=lm.score(X_train,y_train)\nknn_validation_score=metrics.accuracy_score(y_validation_pred_final.Survived, y_validation_pred_final.Predicted_Survived)\n########### Actual Prediction ##########\ny_test_pred = lm.predict(X_test)\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived'})\nprint(y_pred_final['Survived'].value_counts())\ny_pred_final.reset_index(drop=True, inplace=True)\ny_pred_final.to_csv('prediction_sklearn_knn.csv',index=False)","66ea03f6":"models= pd.DataFrame({'Model':['Logistic Regression','SVM','Decision Tree','Random Forest','Gradient Boosting','KNN'], \n                      'Train Score':[lr_train_score,svm_train_score,dt_train_score,rf_train_score,gbc_train_score,knn_train_score],\n                      'Validation Score':[lr_validation_score,svm_validation_score,dt_validation_score,rf_validation_score,gbc_validation_score,knn_validation_score]\n                     })\nmodels[\"Train-Validation-Diff\"]=models[\"Train Score\"]-models[\"Validation Score\"]\nmodels.sort_values(by = 'Train-Validation-Diff', ascending = True, ignore_index = True)","919716bd":"#### Running first iteration","e9db2302":"#### 4. Split","772c3ee0":"####  Testing Other Algos","7256ccc7":"### 5. Model Building","f13d7ae7":"#### Selecting Hyperparamter tuning","66f786d2":"### Dummy Variable (Using One Hot Encoder Method)","a02f1f69":"#### Feature Selection Using RFE","d5e434d5":"### Step 3: Data Prep","f9408b8c":"### 2. Visualising the data","15f125a3":"### Step 1: Reading, understanding the data","bed7f154":"#### Adding new features","9e0ccd25":"#### Predicting on the test data"}}