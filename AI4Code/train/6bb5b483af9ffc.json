{"cell_type":{"0e7ea93c":"code","2f2f3961":"code","6d9a5e31":"code","33bb463d":"code","3edb0d59":"code","96618c5b":"code","5ef93c37":"code","0f4856c0":"code","ca12aba3":"code","6c3bfd89":"markdown","ba48cf0d":"markdown","b5cf0c3f":"markdown","5c529b1b":"markdown","0a3a9db6":"markdown","7398e9c4":"markdown","741bf102":"markdown","818a1442":"markdown","92cb0e44":"markdown","e2479804":"markdown","05e68ab9":"markdown"},"source":{"0e7ea93c":"# Learned from: https:\/\/github.com\/nicku33\/demo\/blob\/master\/Contextual%20Bandit%20Synthetic%20Data%20using%20LinUCB.ipynb","2f2f3961":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nA = np.array([[ 0.70558142,  0.86273594],\n       [ 0.68797076,  0.02774431],\n       [ 0.11372903,  0.38687036],\n       [ 0.43072205,  0.54141448],\n       [ 0.39529217,  0.91495635],\n       [ 0.39027663,  0.95704016],\n       [ 0.5450535,  0.02473527],\n       [ 0.12265648,  0.88966732],\n       [ 0.59852203,  0.28636077],\n       [ 0.85040799,  0.04557076],\n       [ 0.56896747,  0.89559782],\n       [ 0.85425338,  0.01838715],\n       [ 0.80782139,  0.17350079]]);\nfg = plt.boxplot(A)","6d9a5e31":"a_mu = np.mean(A, axis=0)\na_sig = np.std(A, axis=0)\na_upper_ci = a_mu + (3*a_sig)\na_upper_ci","33bb463d":"n = 5000 # number of data points\nk = 30 # number of features\nn_a = 8 # number of actions\nD = np.random.random((n, k)) - 0.5 # our data, or these are the contexts, there are n contexts, each has k features\nth = np.random.random((n_a, k)) - 0.5 # our real theta, what we will try to guess (there are 8 arms, and each has 30 features)","3edb0d59":"P = D.dot(th.T)\noptimal = np.array(P.argmax(axis=1), dtype=int)\nplt.title(\"Distribution of ideal arm choices\")\nfig = plt.hist(optimal, bins=range(0, n_a))","96618c5b":"P","5ef93c37":"import pdb\ndef set_break():\n    pdb.set_trace()","0f4856c0":"eps = 0.2\nchoices = np.zeros(n, dtype=int)\nrewards = np.zeros(n)\nexplore=np.zeros(n)\nnorms = np.zeros(n)\nb = np.zeros_like(th)\nA = np.zeros((n_a, k, k))\nfor a in range(0, n_a):\n    A[a] = np.identity(k)\nth_hat = np.zeros_like(th) # our temporary feature vectors, our best current guesses\np = np.zeros(n_a)\nalph = 0.2\n\n# LINUCB, usign disjoint model\n# This is all from Algorithm 1, p 664, \"A contextual bandit appraoch...\" Li, Langford\nfor i in range(0, n):\n    x_i = D[i] # the current context vector\n    for a in range(0, n_a):\n        A_inv = np.linalg.inv(A[a]) # we use it twice so cache it.\n        th_hat[a] = A_inv.dot(b[a]) # Line 5\n        ta = x_i.dot(A_inv).dot(x_i) # how informative is this?\n        a_upper_ci = alph * np.sqrt(ta) # upper part of variance interval\n\n        a_mean = th_hat[a].dot(x_i) # current estimate of mean\n        p[a] = a_mean + a_upper_ci\n    norms[i] = np.linalg.norm(th_hat - th, 'fro') # diagnostic, are we converging?\n    #Let's hnot be biased with tiebraks, but add in some random noise\n    p = p + (np.random.random(len(p))*0.000001)\n    choices[i] = p.argmax() # choose the highest, line 11\n    \n    # See what kind of result we get\n    rewards[i] = th[choices[i]].dot(x_i) # using actual theta to figure out reward\n    \n    #update the input vector\n    A[choices[i]] += np.outer(x_i, x_i)\n    b[choices[i]] += rewards[i]*x_i","ca12aba3":"plt.figure(1, figsize=(10, 5))\nplt.subplot(121)\nplt.plot(norms)\nplt.title(\"Frobeninus norm of estimated theta vs actual\")\n\nregret = (P.max(axis=1) - rewards)\nplt.subplot(122)\nplt.plot(regret.cumsum())\nplt.title(\"Cumulative regret\")","6c3bfd89":"# Hybrid Models\nIt is a good bet that there will be features that are common to all arms, which we need to know how to interpret. For example, if we classify articles into course categories, then information about one article should transfer to another.\nTo take these into account, we can addanother term and another set of coefficients $\\beta^T z_i$, which we update regardless of what arm we pick.","ba48cf0d":"# Adding the context Vector\nWhen we add in context, we expand the problem space significantly. Naively, we might treat all unique combinations of context as separate problems. However, it's unlikely we'll ever get enough data for that. In stead, we assume that the expected value of an arm is some linear combination of the context $x_i$. In this sense, we are performing an online regression for each possible action, conditional on the context. In this case, we only get to see results for the chosen action, so we can only improve one estimate at a time. We are trying to estimate $\\theta_a$ for each action $a$, and the expected value = $x_{i}^{T}\\theta_a$.\n\nThis is the expected value of an action $a$ from time step $0$ up to time step $T$, associated with a context $x_{i}$ at the time of recommendation ($t$ or $T$?).\n","b5cf0c3f":"# Solve this with UCB\nLet's solve this with the LinUCB algorithm.\nLinUCB uses the same highest C.I. bound principle as above, but in a multidimensional setting. We maintain, for each arm, a running mean and running covariance matrix. For each new data point, we pass the context vector $x_i$ through the covariance matrix to come up with an estimate of how much value the new information is.\n\nTo simplify let's assume that all our feature vectors are independent, and thus we have covariance matrix which is close to diagonal. Passing $x_i$ through it $x_i\\sum^{-1}x_i$ now just becomes $\\sum_{d \\in D}(x_d\\sigma_d)^2$. I.e., it's a sum of the squared variances, weighted by the magnitude of each component in our new data point.\n\n\\begin{equation*}\n    p[a] = a\\_mean + a\\_upper\\_ci\n\\end{equation*}\n\nIf the new data point has a strong signal along a dimension we know little about, the aggregate score will be high and we will be more likely to explore.\n\nUsing a tuning parameter $\\alpha$, we can adjust our explore\/exploit ratio, just like greedy-epsilon.","5c529b1b":"# The LinUCB implemenation","0a3a9db6":"# Taking this to production\n### Can we have multiple boxes serve different requests, so we have load balancing and decentralization?\nYes. Sicne the A and b variables above are only added to, we can read off the logs of each box into cumulative A and b in shared memory or offline in batch, then distribute them back periodically. The loss of accuracy as a result just leads to some extra exploration vs the single box but this only aids convergence. Upon gettin ght esum for all boxes each box would have a recent sum of pooled information for all models. Optionally each box can add to it's own A and b until the next update.\n\nSince the model is additive, we are also free to use anything that supported atomic increments ,which are primitives in Java and some data stores.\n\n### Can we train up the models off line and avoid cold start?\nI think so. In the algorithm above we choose our initial means as (0, 0, ..., 0). We can get a closer estimate by just running a regression over previous data. Even if it's just similar data (different articles and users, but there will be some features such as article topic that carry over) it's better than startin gfrom 0. It will be very biased toward whaever chosen articles before, but I'll take a biased initial estimate over a random as long as there's no chance of local minima, which our linear model guarantees.\n\nAlso, in the paper they made the point that if hyou run a test with random bandit choices, you can use this to score your own algorithm offline, probabiy useful for feature selection, since the rewards will also be randomly distributed. I put in a request with Yahoo Labs to get the original data and see.\n\n### What to use?\nI haven't used it but this seems like Storm's use case in nutshell. However, we could even just publish matrices periodically off of a batch mapreduce or Spark job. It depends on how often articles change. Storm would be more responsive, clearly.","7398e9c4":"In this case, we choose arm 2. After a while, our mean will settle down and the deviation will shrink as $1\/\\sqrt{n_a}$ and we may eventually choose 1","741bf102":"# Solving Contextual Bandit problems using LinUCB\nA contextual bandit problem is a subset of the full reinforcement learning problem. There is context vector $X_i$ attached to each choice, but no state or state transitions that can be predicted. It's assumed that the contexts are IID.\n\nThe tradeoff between exploration and exploitation is central to bandit problems. The simplest algorithm to decide whether to try out something new or go with what you yeilds the best reward is known as greedy-eepsilon algorithm, where a certain percentage is devoted to exploration.\n\nHowever, an improvement in efficient can be gained by using an estimation of variance to quantify how much we do not know about a choice. In contextual problems we have variance along each dimension of D, for each arm.\n\n# Upper Confidence Bound methods\nFor each action we may maintain an expected value, which we assume to be normally distributed, therefore we can also estimate a confidence interval based on teh data we have.\n\nBy having our algorithm choose the higherst of all the actions' confidence intervals (say 0.95), we create a smooth transition between learning and exploiting. This is the essence of a UCB method. It's easy to demonstreate in one dimension with 2 arms.","818a1442":"Each action yeilds a reward. In this case, a success probability. We get maximium rewa4d when we always choose the best action.\nSince tehre's no state, we can precompute the best actions ahead of time. Here is the distribution of optimal arms","92cb0e44":"This boxplot is just a quick visual proxy, but let's calculate mean and s.d. of each arm. However, if we take the mean + 3 standard deviations up as the number which helps us select which arm to pull, we combine expected values and uncedrtainty in one metric.","e2479804":"# Creating the synthetic data\nLet's create some idealized synthetic data, and then some random $\\theta_a$. Assume 30 features and 8 possible actions.","05e68ab9":"# Analysis\nThe two plots above show how we converge to the true linear model parameters, and how much it cost us along the way.\n"}}