{"cell_type":{"015e6585":"code","14800791":"code","fdb67c16":"code","054a3f29":"code","9b7e8e1d":"code","19c993bf":"code","9c28aec5":"code","84255712":"code","4f598704":"code","427a946f":"code","843f805d":"code","874c9da3":"code","8fefbbf3":"code","3e1806c9":"code","177121f8":"code","e10aca1c":"code","f4f711cc":"code","776aca19":"markdown","4465bfd0":"markdown","052c0f48":"markdown","635881cd":"markdown","b9f1c3dd":"markdown","db0ced7e":"markdown","a6c54f3b":"markdown","678bfbfa":"markdown","b8b7a5f8":"markdown","e7e19281":"markdown","8c6ace34":"markdown","008926f1":"markdown","ecc75780":"markdown"},"source":{"015e6585":"# importing the required packages\nimport pandas as pd \nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nimport catboost as cat\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import IterativeImputer","14800791":"# reading the data\ntrain = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\ntrain = train[~((train['energy'].isnull()) & (train['liveness'].isnull()) ) ].reset_index(drop=True)\ntrain = train[~((train['acousticness'].isnull()) & (train['instrumentalness'].isnull()) ) ].reset_index(drop=True)\ntrain = train[~((train['loudness'].isnull()) & (train['danceability'].isnull()) ) ].reset_index(drop=True)\ntrain = train[~((train['liveness'].isnull()) & (train['danceability'].isnull()) ) ].reset_index(drop=True)\nX = train.drop('song_popularity', axis=1).copy()\ny = train['song_popularity'].copy()\nX.head()","fdb67c16":"X.info()","054a3f29":"X.describe(include='all')","9b7e8e1d":"# droping the 'id' column from train and test\nX.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\n# replacing the -ve values with 0\nX.acousticness[X.acousticness<0] = 0\nX.instrumentalness[X.instrumentalness<0] = 0","19c993bf":"# pipeline to impute the null value and data transformation\npipeline = Pipeline([\n    ('impute', IterativeImputer(max_iter=10,random_state=42,add_indicator=False)),\n    ('transform_', QuantileTransformer()) \n])","9c28aec5":"kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # stratified k fold \n\nx_test = test.copy()\nx = X.copy()\n\n# dataframe to store OOF predictions\nvalid_pred_df = pd.DataFrame()\ntest_pred_df = pd.DataFrame()\ntest_pred_df['id'] = x_test.index\nvalid_pred_df['id'] = X.index\n\nx = pd.DataFrame(pipeline.fit_transform(x),columns=x.columns)\nx_test = pd.DataFrame(data=pipeline.transform(x_test),columns=x.columns)","84255712":"temp_dict = {}\ntemp_df = pd.DataFrame()\nvalid_preds,test_preds,scores = [],[],[]\n\nxgb_params = {'max_depth': 8,\n 'n_estimators': 9600,\n 'learning_rate': 0.010729802684564086,\n 'subsample': 0.2,\n 'colsample_bytree': 0.7,\n 'colsample_bylevel': 0.4,\n 'reg_lambda': 12.411155548777836,\n 'reg_alpha': 0.6685496387870691,\n 'gamma': 0.0001373432756811366}\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(x, y)):\n    x_train, y_train = x.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = x.iloc[idx_valid], y.iloc[idx_valid] \n\n    model = XGBRegressor(**xgb_params, booster= 'gbtree',\n                        objective= 'binary:logistic',\n                        eval_metric = 'auc',\n                        tree_method= 'gpu_hist',\n                        predictor=\"gpu_predictor\",\n                        random_state=0, \n                        use_label_encoder=False,\n                        )\n    \n    model.fit(x_train,y_train,\n            eval_set=[(x_valid,y_valid)],\n            early_stopping_rounds=300,\n            verbose=False)\n    \n    valid_pred = model.predict(x_valid)\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    temp_dict.update(dict(zip(x_valid.index,valid_pred)))\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n    \n    test_pred = model.predict(x_test)\n    test_preds.append(test_pred)\n    \ntemp_df = pd.DataFrame.from_dict(temp_dict,orient='index').reset_index().rename(columns = {'index':'id',0:'valid_xgb'})\nvalid_pred_df = valid_pred_df.merge(temp_df,on = 'id')\ntest_pred_df['pred_xgb'] = pd.DataFrame(np.column_stack(test_preds).mean(axis = 1))\n\nprint(f\"Overall Validation Score : {np.mean(scores)}\")","4f598704":"temp_dict = {}\ntemp_df = pd.DataFrame()\nvalid_preds,test_preds,scores = [],[],[]\n\nlgb_params = {'max_depth': 69,\n 'n_estimators': 19600,\n 'learning_rate': 0.09074604767677533,\n 'subsample': 0.9,\n 'colsample_bytree': 0.6000000000000001,\n 'reg_lambda': 10.637736685175083,\n 'reg_alpha': 12.732914587118874,\n 'boosting_type': 'gbdt',\n 'num_leaves': 10}\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(x, y)):\n    x_train, y_train = x.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = x.iloc[idx_valid], y.iloc[idx_valid] \n\n    model = lgb.LGBMRegressor(**lgb_params, \n                               metric= 'auc', \n                               random_state= 0,\n                               )  \n    \n    model.fit(x_train, y_train,\n            eval_set=[(x_valid, y_valid)],\n            early_stopping_rounds=300,\n            verbose=False\n           )\n    \n    valid_pred = model.predict(x_valid)\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    temp_dict.update(dict(zip(x_valid.index,valid_pred)))\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n    \n    test_pred = model.predict(x_test)\n    test_preds.append(test_pred)\n    \ntemp_df = pd.DataFrame.from_dict(temp_dict,orient='index').reset_index().rename(columns = {'index':'id',0:'valid_lgb'})\nvalid_pred_df = valid_pred_df.merge(temp_df,on = 'id')\ntest_pred_df['pred_lgb'] = pd.DataFrame(np.column_stack(test_preds).mean(axis = 1))\n\nprint(f\"Overall Validation Score : {np.mean(scores)}\")\n","427a946f":"\ntemp_dict = {}\ntemp_df = pd.DataFrame()\nvalid_preds,test_preds,scores = [],[],[]\n\nrf_params = {'max_depth': 54,\n 'n_estimators': 100,\n 'max_features': 'log2',\n 'max_leaf_nodes': 128,\n 'min_samples_split': 3,\n 'min_samples_leaf': 10}\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(x, y)):\n    x_train, y_train = x.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = x.iloc[idx_valid], y.iloc[idx_valid] \n\n    model = RandomForestRegressor(**rf_params, \n                                   random_state= 0,\n                                   n_jobs = -1)  \n    \n    model.fit(x_train, y_train)\n    \n    valid_pred = model.predict(x_valid)\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    temp_dict.update(dict(zip(x_valid.index,valid_pred)))\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n    \n    test_pred = model.predict(x_test)\n    test_preds.append(test_pred)\n    \ntemp_df = pd.DataFrame.from_dict(temp_dict,orient='index').reset_index().rename(columns = {'index':'id',0:'valid_rf'})\nvalid_pred_df = valid_pred_df.merge(temp_df,on = 'id')\ntest_pred_df['pred_rf'] = pd.DataFrame(np.column_stack(test_preds).mean(axis = 1))\n\nprint(f\"Overall Validation Score : {np.mean(scores)}\")","843f805d":"temp_dict = {}\ntemp_df = pd.DataFrame()\nvalid_preds,test_preds,scores = [],[],[]\n\ncat_params = {'iterations': 13254,\n 'od_wait': 1174,\n 'learning_rate': 0.010958320637930964,\n 'reg_lambda': 0.01860946566788274,\n 'subsample': 0.5,\n 'random_strength': 5.132433821132644,\n 'depth': 4,\n 'grow_policy': 'Depthwise',\n 'min_child_samples': 16,\n 'border_count': 31,\n 'bagging_temperature': 0.7740200804146857}\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(x, y)):\n    x_train, y_train = x.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = x.iloc[idx_valid], y.iloc[idx_valid]\n\n    model = cat.CatBoostRegressor(\n        random_state=0,\n        eval_metric='AUC',\n        **cat_params,\n    )\n    \n    model.fit(x_train,y_train,\n              eval_set=[(x_valid,y_valid)], \n              early_stopping_rounds=300, \n              verbose=False,\n              use_best_model=True)\n    \n    valid_pred = model.predict(x_valid)\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    temp_dict.update(dict(zip(x_valid.index,valid_pred)))\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n    \n    test_pred = model.predict(x_test)\n    test_preds.append(test_pred)\n    \ntemp_df = pd.DataFrame.from_dict(temp_dict,orient='index').reset_index().rename(columns = {'index':'id',0:'valid_cat'})\nvalid_pred_df = valid_pred_df.merge(temp_df,on = 'id')\ntest_pred_df['pred_cat'] = pd.DataFrame(np.column_stack(test_preds).mean(axis = 1))\n\nprint(f\"Overall Validation Score : {np.mean(scores)}\")","874c9da3":"# dataframe to store the OOF prediction\nvalid_pred_1_df = pd.DataFrame()\ntest_pred_1_df = pd.DataFrame()\n\nvalid_pred_1_df['id'] = valid_pred_df['id']\ntest_pred_1_df['id'] = test_pred_df['id']\n\ndf_val = valid_pred_df.drop('id',axis = 1)\ndf_test = test_pred_df.drop('id',axis = 1)\n\ndf_test.columns = df_val.columns","8fefbbf3":"temp_dict = {}\ntemp_df = pd.DataFrame()\nvalid_preds,test_preds,scores = [],[],[]\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(df_val, y)):\n    x_train, y_train = df_val.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = df_val.iloc[idx_valid], y.iloc[idx_valid]  \n\n    model = LogisticRegression()\n    \n    model.fit(x_train,y_train)\n    \n    valid_pred = model.predict_proba(x_valid)[:,1]\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    temp_dict.update(dict(zip(x_valid.index,valid_pred)))\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n    \n    test_pred = model.predict_proba(df_test)[:,1]\n    test_preds.append(test_pred)\n    \ntemp_df = pd.DataFrame.from_dict(temp_dict,orient='index').reset_index().rename(columns = {'index':'id',0:'valid_lr'})\nvalid_pred_1_df = valid_pred_1_df.merge(temp_df,on = 'id')\ntest_pred_1_df['pred_lr'] = pd.DataFrame(np.column_stack(test_preds).mean(axis = 1))\n\nprint(f\"Overall Validation Score : {np.mean(scores)}\")","3e1806c9":"temp_dict = {}\ntemp_df = pd.DataFrame()\nvalid_preds,test_preds,scores = [],[],[]\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(df_val, y)):\n    x_train, y_train = df_val.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = df_val.iloc[idx_valid], y.iloc[idx_valid]   \n\n    model = LinearRegression()\n    \n    model.fit(x_train,y_train)\n    \n    valid_pred = model.predict(x_valid)\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    temp_dict.update(dict(zip(x_valid.index,valid_pred)))\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n    \n    test_pred = model.predict(df_test)\n    test_preds.append(test_pred)\n    \ntemp_df = pd.DataFrame.from_dict(temp_dict,orient='index').reset_index().rename(columns = {'index':'id',0:'valid_lr_1'})\nvalid_pred_1_df = valid_pred_1_df.merge(temp_df,on = 'id')\ntest_pred_1_df['pred_lr_1'] = pd.DataFrame(np.column_stack(test_preds).mean(axis = 1))\n\nprint(f\"Overall Validation Score : {np.mean(scores)}\")","177121f8":"temp_dict = {}\ntemp_df = pd.DataFrame()\nvalid_preds,test_preds,scores = [],[],[]\n\ncat_params = {'iterations': 3990,\n 'od_wait': 632,\n 'learning_rate': 0.05403922778627244,\n 'reg_lambda': 0.2835239884107686,\n 'subsample': 0.8,\n 'random_strength': 3.224920718394828,\n 'depth': 7,\n 'grow_policy': 'SymmetricTree',\n 'min_child_samples': 71,\n 'border_count': 87,\n 'bagging_temperature': 0.8797092144468472}\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(df_val, y)):\n    x_train, y_train = df_val.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = df_val.iloc[idx_valid], y.iloc[idx_valid]\n\n    model = cat.CatBoostRegressor(\n        random_state=0,\n        eval_metric='AUC',\n        **cat_params,\n    )\n    \n    model.fit(x_train,y_train,\n              eval_set=[(x_valid,y_valid)], \n              early_stopping_rounds=300, \n              verbose=False,\n              use_best_model=True)\n    \n    valid_pred = model.predict(x_valid)\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    temp_dict.update(dict(zip(x_valid.index,valid_pred)))\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n    \n    test_pred = model.predict(df_test)\n    test_preds.append(test_pred)\n    \ntemp_df = pd.DataFrame.from_dict(temp_dict,orient='index').reset_index().rename(columns = {'index':'id',0:'valid_cat'})\nvalid_pred_1_df = valid_pred_1_df.merge(temp_df,on = 'id')\ntest_pred_1_df['pred_cat'] = pd.DataFrame(np.column_stack(test_preds).mean(axis = 1))\n\nprint(f\"Overall Validation Score : {np.mean(scores)}\")","e10aca1c":"valid_preds,test_preds,scores = [],[],[]\n\ndf_val_final = valid_pred_1_df.drop(['id'],axis = 1)\ndf_test_final = test_pred_1_df.drop(['id'],axis = 1)\ndf_test_final.columns = df_val_final.columns\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(df_val_final, y)):\n    x_train, y_train = df_val_final.iloc[idx_train], y.iloc[idx_train]\n    x_valid, y_valid = df_val_final.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = LinearRegression()\n    \n    model.fit(x_train,y_train)\n\n    valid_pred = model.predict(x_valid)\n    valid_preds.append(valid_pred)\n\n    fpr, tpr, _ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*25)\n\n    test_pred = model.predict(df_test_final)\n    test_preds.append(test_pred)\n    \nprint(f\"Overall Validation Score : {np.mean(scores)}\")","f4f711cc":"# saving the submission\nsubmission = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')\npredictions = np.mean(np.column_stack(test_preds),axis=1)\nsubmission['song_popularity'] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission","776aca19":"### CatBoost_0","4465bfd0":"## L0","052c0f48":"### RandomForest_0","635881cd":"# Final submission","b9f1c3dd":"## L2","db0ced7e":"### CatBoost_1","a6c54f3b":"### LightGBM_0","678bfbfa":"## L1\n\n>> uses the prediction of layer 0 to predict further\n\n>> all the hyperparameters here are differnet from layer 0 ","b8b7a5f8":"### LinearRegression_1","e7e19281":"# Stacking\n\n>> 3 layers of stacking done with 10 fold validation\n\n>> all the hypeparameters were tuned using optuna","8c6ace34":"### LinearRegression_2","008926f1":"### LogisticRegresion_1","ecc75780":"### XGBoost_0"}}