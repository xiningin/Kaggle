{"cell_type":{"4a7e4e76":"code","af83bc41":"code","1dd29faa":"code","8bb67a3e":"code","c7cc8c13":"code","e2ae1db0":"code","3383349e":"code","7d525c8f":"code","903f5bd6":"code","116b2308":"code","ebf9f795":"code","f73f76e2":"code","aa918154":"code","bbf72f21":"code","6bfd3c36":"code","2cb1e3d6":"code","9f8ecf20":"code","a11bb521":"code","4042c887":"code","ba53add8":"code","dd3968f5":"code","ebe5af1d":"code","54eb48e2":"code","82df31b4":"code","5487e812":"code","c89f9184":"code","b633c996":"code","5807cfe5":"code","7f2731e0":"code","06708ca4":"code","8f0fc22d":"code","c41ea117":"code","a3a1b442":"code","0c63609c":"code","5f36dc3d":"code","3df09da8":"code","77223027":"code","2b17c3a3":"code","e82fce51":"markdown","ca1f24f9":"markdown","b022d7c6":"markdown","33788aa7":"markdown","12bc9672":"markdown","eec652c5":"markdown","33fe61b0":"markdown","d86a3c65":"markdown","3149db77":"markdown","2362f4c8":"markdown","7b0fe7bd":"markdown","1fc83ec7":"markdown","b0e8eb12":"markdown","c73586ab":"markdown","04b746eb":"markdown","1341331f":"markdown","4d6baece":"markdown","f64af102":"markdown","255b4721":"markdown"},"source":{"4a7e4e76":"%%capture\n# install tensorflow 2.0 beta\n!pip install -q tensorflow-gpu==2.0.0-beta1\n# !pip install -q tensorflow-gpu==2.0.0-rc0 # issue https:\/\/github.com\/tensorflow\/tensorflow\/issues\/24828\n\n#install GapCV\n!pip install -q gapcv","af83bc41":"%%capture\nimport os\nimport time\nimport cv2\nimport gc\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import SGD, Adam, Optimizer\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import callbacks\n\nimport gapcv\nfrom gapcv.vision import Images\n\nfrom sklearn.utils import class_weight\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['TF_KERAS']='1'\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","1dd29faa":"print('tensorflow version: ', tf.__version__)\nprint('keras version: ', tf.keras.__version__)\nprint('gapcv version: ', gapcv.__version__)\n\nos.makedirs('model', exist_ok=True)\nprint(os.listdir('..\/input'))\nprint(os.listdir('.\/'))","8bb67a3e":"def elapsed(start):\n    \"\"\"\n    Returns elapsed time in hh:mm:ss format from start time in unix format\n    \"\"\"\n    elapsed = time.time()-start\n    return time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))","c7cc8c13":"def plot_sample(imgs_set, labels_set, img_size=(12,12), columns=4, rows=4, random=False):\n    \"\"\"\n    Plot a sample of images\n    \"\"\"\n    \n    fig=plt.figure(figsize=img_size)\n    \n    for i in range(1, columns*rows + 1):\n        \n        if random:\n            img_x = np.random.randint(0, len(imgs_set))\n        else:\n            img_x = i-1\n        \n        img = imgs_set[img_x]\n        ax = fig.add_subplot(rows, columns, i)\n        ax.set_title(str(labels_set[img_x]))\n        plt.axis('off')\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.show()","e2ae1db0":"def plot_history(history, val_1, val_2, title):\n    plt.plot(history.history[val_1])\n    plt.plot(history.history[val_2])\n\n    plt.title(title)\n    plt.ylabel(val_1)\n    plt.xlabel('epoch')\n    plt.legend([val_1, val_2], loc='upper left')\n    plt.show()","3383349e":"data_set = 'wildlife'\ndata_set_folder = 'oregon_wildlife\/oregon_wildlife'\nminibatch_size = 32\n\nif not os.path.isfile('..\/input\/{}.h5'.format(data_set)):\n    images = Images(data_set, data_set_folder, config=['resize=(128,128)', 'store', 'stream'])\n\n# stream from h5 file\nimages = Images(config=['stream'], augment=['flip=horizontal', 'edge', 'zoom=0.3', 'denoise'])\nimages.load(data_set, '..\/input')\n\n# generator\nimages.split = 0.2\nX_test, Y_test = images.test\nimages.minibatch = minibatch_size\ngap_generator = images.minibatch\n\nY_int = [y.argmax() for y in Y_test]\nclass_weights = class_weight.compute_class_weight(\n    'balanced',\n    np.unique(Y_int),\n    Y_int\n)\n\ntotal_train_images = images.count - len(X_test)\nn_classes = len(images.classes)","7d525c8f":"# dataset meta data\nprint('content:', os.listdir(\".\/\"))\nprint('time to load data set:', images.elapsed)\nprint('number of images in data set:', images.count)\nprint('classes:', images.classes)\nprint('data type:', images.dtype)","903f5bd6":"!free -m","116b2308":"model_file = '.\/model\/model.h5'\n\nearlystopping = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5\n)\n\nmodel_checkpoint = callbacks.ModelCheckpoint(\n    model_file,\n    monitor='val_accuracy',\n    save_best_only=True,\n    save_weights_only=False,\n    mode='max'\n)","ebf9f795":"def model_seq():\n    return Sequential([\n        layers.Conv2D(filters=128, kernel_size=(4, 4), activation='relu', input_shape=(128, 128, 3)),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.3),\n        layers.Conv2D(filters=64, kernel_size=(4, 4), activation='relu'),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.1),\n        layers.Conv2D(filters=32, kernel_size=(4, 4), activation='relu'),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.1),\n        layers.Conv2D(filters=32, kernel_size=(4, 4), activation='relu'),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.2),\n        layers.Flatten(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(5, activation='softmax')\n    ])","f73f76e2":"model = model_seq()\nmodel.summary()","aa918154":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","bbf72f21":"start = time.time()\n\nhistory = model.fit_generator(\n    generator=gap_generator,\n    validation_data=(X_test, Y_test),\n    epochs=100,\n    steps_per_epoch=int(total_train_images \/ minibatch_size),\n    initial_epoch=0,\n    verbose=1,\n    class_weight=class_weights,\n    callbacks=[\n        model_checkpoint\n    ]\n)","6bfd3c36":"# moved line for kernel bug\nprint('\\nElapsed time: {}'.format(elapsed(start)))","2cb1e3d6":"plot_history(history, 'accuracy', 'val_accuracy', 'Accuracy')\nplot_history(history, 'loss', 'val_loss', 'Loss')","9f8ecf20":"del model\nmodel = load_model(model_file)","a11bb521":"scores = model.evaluate(X_test, Y_test, batch_size=32)\n\nfor score, metric_name in zip(scores, model.metrics_names):\n    print(\"{} : {}\".format(metric_name, score))","4042c887":"class RAdam(Optimizer):\n    \"\"\"RAdam optimizer.\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https:\/\/arxiv.org\/abs\/1412.6980v8)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https:\/\/arxiv.org\/pdf\/1908.03265v1.pdf)\n    \"\"\"\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0.0, weight_decay=0.0, **kwargs):\n        super(RAdam, self).__init__(name='RAdam', **kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self._lr = K.variable(lr, name='lr')\n            self._iterations = K.variable(0, dtype='int64', name='iterations')\n            self._beta_1 = K.variable(beta_1, name='beta_1')\n            self._beta_2 = K.variable(beta_2, name='beta_2')\n            self._decay = K.variable(decay, name='decay')\n            self._weight_decay = K.variable(weight_decay, name='weight_decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self._iterations, 1)]\n        lr = self._lr\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self._decay * K.cast(self._iterations, K.dtype(self._decay))))\n\n        t = K.cast(self._iterations, K.floatx()) + 1\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n\n        self._weights = [self._iterations] + ms + vs\n\n        beta_1_t = K.pow(self._beta_1, t)\n        beta_2_t = K.pow(self._beta_2, t)\n\n        sma_inf = 2.0 \/ (1.0 - self._beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t \/ (1.0 - beta_2_t)\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            m_t = (self._beta_1 * m) + (1. - self._beta_1) * g\n            v_t = (self._beta_2 * v) + (1. - self._beta_2) * K.square(g)\n\n            m_hat_t = m_t \/ (1.0 - beta_1_t)\n            v_hat_t = K.sqrt(v_t \/ (1.0 - beta_2_t) + self.epsilon)\n\n            r_t = K.sqrt((sma_t - 4.0) \/ (sma_inf - 4.0) *\n                         (sma_t - 2.0) \/ (sma_inf - 2.0) *\n                         sma_inf \/ sma_t + self.epsilon)\n\n            p_t = K.switch(sma_t > 5, r_t * m_hat_t \/ (K.sqrt(v_hat_t + self.epsilon)), m_hat_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self._weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\n            'lr': float(K.get_value(self._lr)),\n            'beta_1': float(K.get_value(self._beta_1)),\n            'beta_2': float(K.get_value(self._beta_2)),\n            'decay': float(K.get_value(self._decay)),\n            'weight_decay': float(K.get_value(self._weight_decay)),\n            'epsilon': self.epsilon,\n        }\n        base_config = super(RAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","ba53add8":"del model\nmodel = model_seq()","dd3968f5":"model.compile(optimizer=RAdam(), loss='categorical_crossentropy', metrics=['accuracy'])","ebe5af1d":"start = time.time()\n\nhistory = model.fit_generator(\n    generator=gap_generator,\n    validation_data=(X_test, Y_test),\n    epochs=100,\n    steps_per_epoch=int(total_train_images \/ minibatch_size),\n    initial_epoch=0,\n    verbose=1,\n    class_weight=class_weights,\n    callbacks=[\n        model_checkpoint\n    ]\n)","54eb48e2":"# moved line for kernel bug\nprint('\\nElapsed time: {}'.format(elapsed(start)))","82df31b4":"plot_history(history, 'accuracy', 'val_accuracy', 'Accuracy')\nplot_history(history, 'loss', 'val_loss', 'Loss')","5487e812":"del model\nmodel = load_model(model_file)","c89f9184":"scores = model.evaluate(X_test, Y_test, batch_size=32)\n\nfor score, metric_name in zip(scores, model.metrics_names):\n    print(\"{} : {}\".format(metric_name, score))","b633c996":"del model\nmodel = model_seq()","5807cfe5":"model.compile(optimizer=RAdam(), loss='categorical_crossentropy', metrics=['accuracy'])","7f2731e0":"start = time.time()\n\nhistory = model.fit_generator(\n    generator=gap_generator,\n    validation_data=(X_test, Y_test),\n    epochs=200,\n    steps_per_epoch=int(total_train_images \/ minibatch_size),\n    initial_epoch=0,\n    verbose=1,\n    class_weight=class_weights,\n    callbacks=[\n        model_checkpoint\n    ]\n)","06708ca4":"# moved line for kernel bug\nprint('\\nElapsed time: {}'.format(elapsed(start)))","8f0fc22d":"plot_history(history, 'accuracy', 'val_accuracy', 'Accuracy')\nplot_history(history, 'loss', 'val_loss', 'Loss')","c41ea117":"# handle unexpected bug - ValueError: Unknown optimizer: RAdam\ndel model\nmodel = load_model(model_file)\n","a3a1b442":"scores = model.evaluate(X_test, Y_test, batch_size=32)\n\nfor score, metric_name in zip(scores, model.metrics_names):\n    print(\"{} : {}\".format(metric_name, score))","0c63609c":"!curl https:\/\/d36tnp772eyphs.cloudfront.net\/blogs\/1\/2016\/11\/17268317326_2c1525b418_k.jpg > test_image.jpg","5f36dc3d":"labels = {val:key for key, val in images.classes.items()}\nlabels","3df09da8":"%pwd\n%ls","77223027":"image2 = Images('foo', ['test_image.jpg'], [0], config=['resize=(128,128)'])\nimg = image2._data[0]","2b17c3a3":"prediction = model.predict_classes(img)\nprediction = labels[prediction[0]]\n\nplot_sample(img, ['predicted image: {}'.format(prediction)], img_size=(8, 8), columns=1, rows=1)","e82fce51":"## Keras model definition","ca1f24f9":"## Callbacks","b022d7c6":"This is interesting. Even that it seems the accuracy and val_accuracy are improving. The model only got a max accuracy of 0.87 on the validation dataset vs 0.85 on the training dataset after 100 epochs. We can ignore the time of training since the difference is 12 sec aprox. Which is not bad at all.","33788aa7":"## Keras RAdam Optimizer\nby https:\/\/github.com\/CyberZHG\/keras-radam","12bc9672":"## install tensorflow and gapcv","eec652c5":"# OREGON WILDLIFE - TENSORFLOW 2.0 + KERAS RAdam\n\nRecently was introduce a new ML optimizer. Rectified Adam (**RAdam**) comes with the promisse of improving your AI accuracy instantly. There are already good [articles](https:\/\/medium.com\/@lessw\/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b) and the [paper](https:\/\/arxiv.org\/pdf\/1908.03265v1.pdf) itself to see what it's this about. My porpuse with this notebook is just to test `RAdam` implemented on `Keras` on my current dataset [Oregon Wildlife](https:\/\/www.kaggle.com\/virtualdvid\/oregon-wildlife). I'm also going to use a previous [model](https:\/\/www.kaggle.com\/virtualdvid\/wildlife-keras-gapcv-hyperas-optimization) I got using hyperparameter optimization with `hyperas` and got good results with `Adam`. I'm wondering if `RAdam` can solve my over-fitting issues.\n\nTo be fair I'm going to test the same model with `Adam` and `Radam` with their default values and as always use [GapCv](https:\/\/www.kaggle.com\/virtualdvid\/wildlife-keras-vs-gapcv-generators-performance) for image preprocessing. In future tests I'll involve tuning their parameters to test performance and results.","33fe61b0":"### Training with Adam optimizer","d86a3c65":"#### accuracy and loss charts","3149db77":"## Training with RAdam (Rectified Adam) optimizer","2362f4c8":"## import libraries","7b0fe7bd":"We can see here how the models starts to over-fit after the epoch 22 (aprox). Reaching a max accuracy of 0.91 on the validation dataset vs 0.95 on the training dataset.","1fc83ec7":"### RAdam 200 epochs\nLet's give the chance that in order to improve accuracy. Let's train the model with the same parameters but with 200 epochs.","b0e8eb12":"## GapCV image preprocessing","c73586ab":"## get a random image and get a prediction!","04b746eb":"What a dissapoinment. The over-fitting stills there. It seems `RAdam` for this case is just delaying the over-fitting in the epoch 100 aprox.  \nThe model only got a max accuracy of 0.91 on the validation dataset vs 0.94 on the training dataset after 200 epochs (Adam got similar results but in less time).\n\nI'm not going to reject at all this new optimizer. There is tons of things to tweak and I haven't played yet with its parameters. Maybe I can get a better hyperparameter optimization where I can reduce time of training and get a better accuracy for this dataset...","1341331f":"## utils functions","4d6baece":"#### accuracy and loss charts","f64af102":"#### accuracy and loss charts","255b4721":"## What's next\n\nTo be fair I'll need to test several models with different hyperparameters for the model and the parameters in `RAmdam` and `Adam` optimizer.  \nAs I mentioned before my goal is to get a better accuracy and time of training. I'll publish the results once I have them! :)\n\nThanks for your time!"}}