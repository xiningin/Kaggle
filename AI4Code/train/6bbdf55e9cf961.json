{"cell_type":{"da8e29bb":"code","eafda565":"code","531373b8":"code","9fd96c67":"code","c85e9cb8":"code","7a66429a":"code","e38c7ae2":"code","2ef3582b":"code","15471e56":"code","cbc79607":"code","71d34d3b":"code","0ac808e4":"code","e80bb2b7":"code","0f3fc390":"code","9a6570f2":"code","b04c2882":"code","30c13c64":"code","4e5b60c5":"code","28b5eec7":"code","20e4795a":"code","25247024":"code","11a78e4a":"code","93074cc6":"code","5dd9da29":"code","de57955b":"code","a2cf2d11":"code","bf724485":"code","3c27a078":"code","7b68b4e9":"code","8f9fd65a":"code","a09eaae6":"code","e7bafb3e":"code","b2f40ee6":"code","52a81c91":"code","ef19f3fc":"code","1dc167b0":"code","bf140354":"code","0364093a":"code","1fcba810":"code","95812a9a":"code","df41ee4e":"code","4b323437":"code","4458ceba":"code","62393e39":"code","3d57fae7":"code","d51ff9a6":"code","578dc2ed":"code","2b246497":"code","5d2646d9":"markdown","358ca60e":"markdown","6121530b":"markdown"},"source":{"da8e29bb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten , Dense\n","eafda565":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","531373b8":"from google.colab import files\nuploaded = files.upload()","9fd96c67":"import io\ndf = pd.read_csv(io.BytesIO(uploaded['advertising.csv']))","c85e9cb8":"df.head()","7a66429a":"df.isnull().any()","e38c7ae2":"import seaborn as sns\nsns.heatmap(df.isnull())","2ef3582b":"#outliers\nfrom sklearn.model_selection import train_test_split\nx = df.drop(labels = ['Ad Topic Line','City','Timestamp'],axis = 1)\ny = ['Clicked on Ad']\n","15471e56":"\nx = df.drop('Country',axis = 1)","cbc79607":"len(df['Ad Topic Line'].unique())","71d34d3b":"x.head(5)","0ac808e4":"#feature standardization , however sucky the features are ewwwwww shity\n#the scales are varying much and neural network cant work easily in this so we have to preprocess the feaures\n","e80bb2b7":"x = df[['Daily Time Spent on Site','Age','Area Income','Daily Internet Usage','Male']]\ny = df['Clicked on Ad']\nfrom sklearn.preprocessing import StandardScaler\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,stratify = y)","0f3fc390":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\n","9a6570f2":"x_train","b04c2882":"y_train .to_numpy","30c13c64":"model = Sequential()\nmodel.add(Dense(x.shape[1],activation='relu',input_dim = x.shape[1]))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n\n","4e5b60c5":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","28b5eec7":"history = model.fit(x_train,y_train,batch_size=10,epochs=20,verbose=1,validation_split=0.2)","20e4795a":"y_pred = model.predict_classes(x_test)\n","25247024":"model.evaluate(x_test,y_test.to_numpy())","11a78e4a":"from sklearn.ensemble import GradientBoostingClassifier\nboost = GradientBoostingClassifier()\nboost.fit(x_train,y_train.to_numpy())\nprint(boost.score(x_test,y_test.to_numpy()))","93074cc6":"history.history","5dd9da29":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend(['train','val'])","de57955b":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['train','val'])","a2cf2d11":"!pip install mlxtend","bf724485":"from mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n","3c27a078":"mat = confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(conf_mat=mat)","7b68b4e9":"\nplot_confusion_matrix(conf_mat=mat,show_normed=True)","8f9fd65a":"df.head()","a09eaae6":"df['Timestamp']","e7bafb3e":"df['Timestamp'] = pd.to_datetime(df['Timestamp'])","b2f40ee6":"df['Timestamp']","52a81c91":"df['year'] = df['Timestamp'].dt.year\ndf['month'] = df['Timestamp'].dt.month\ndf['day'] = df['Timestamp'].dt.day\ndf['week'] = df['Timestamp'].dt.week\ndf['day_of_week'] = df['Timestamp'].dt.dayofweek\ndf['hour'] = df['Timestamp'].dt.hour\ndf['minute'] = df['Timestamp'].dt.minute","ef19f3fc":"df.head(2)","1dc167b0":"#feature important\nfeature = df[['year','month','day','week','day_of_week','hour','minute']]\ntarget = df['Clicked on Ad']\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(feature,target)\nprint(model.feature_importances_)","bf140354":"# some eda\nimport seaborn as sns\nsns.jointplot(x = df['minute'], y =df['Clicked on Ad'])","0364093a":"sns.jointplot(x = df['day_of_week'],y = df['Clicked on Ad'],kind = 'hex')","1fcba810":"df.columns","95812a9a":"x_new = df[['Daily Time Spent on Site', 'Age', 'Area Income',\n       'Daily Internet Usage',   'Male', \n         'month', 'day', 'week',\n       'day_of_week', 'hour', 'minute']]\ny_new = df['Clicked on Ad']","df41ee4e":"x_train_new,x_test_new,y_train_new,y_test_new = train_test_split(x_new,y_new,test_size = 0.2,stratify = y_new)\nx_train_new = scaler.fit_transform(x_train_new)\nx_test_new = scaler.fit_transform(x_test_new)","4b323437":"from sklearn.ensemble import GradientBoostingClassifier\nboost = GradientBoostingClassifier()\nboost.fit(x_train_new,y_train_new)\nprint(boost.score(x_test_new,y_test_new))","4458ceba":"model = Sequential()\nmodel.add(Dense(x.shape[1],activation='relu',input_dim = x.shape[1]))\nmodel.add(Dense(128,activation='relu',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(256,activation='tanh',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n\n","62393e39":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","3d57fae7":"history = model.fit(x_train,y_train,batch_size=10,epochs=30,verbose=1,validation_split=0.2)","d51ff9a6":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend(['train','val'])\n","578dc2ed":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['train','val'])","2b246497":"#handling string data","5d2646d9":"DEALING WITH TIMESTAMPS","358ca60e":"PLOTTING LEARNING CURVE","6121530b":"confusion matrix"}}