{"cell_type":{"607a157d":"code","e11229b4":"code","ac670bec":"code","ebda4824":"code","335e0d8d":"code","129b8edf":"code","faeb73d7":"code","a0672e3f":"code","63cad053":"code","f22f85f5":"code","f0ff41f9":"code","d77667a2":"code","2f723548":"code","adfe8a42":"code","b2464bc8":"code","6d99e3c2":"code","17eab00c":"code","79e8fd37":"code","9f1a7b93":"code","ad97b999":"code","745d5b7d":"code","5941d172":"code","9ee38830":"code","f220f97b":"code","a06806b0":"code","f7be0b91":"markdown","6a8d79d5":"markdown","3d181af7":"markdown","b673eb3f":"markdown","d618a67a":"markdown","b43012cb":"markdown","347fcb9c":"markdown","111cf167":"markdown","c609f646":"markdown","bff7d7de":"markdown","4fc3b853":"markdown","8826ce12":"markdown","6d311669":"markdown","6755414f":"markdown","07789ff6":"markdown","9aeb405d":"markdown","b2519f13":"markdown","e75bb1e9":"markdown","87d16234":"markdown","161daeaf":"markdown"},"source":{"607a157d":"import numpy as np\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import LabelEncoder\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\npio.templates.default = \"none\"\n\nfrom umap import UMAP\nfrom sklearn.manifold import TSNE\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e11229b4":"subm = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv', index_col='id')\ntrain = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv',  index_col='id')\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv',  index_col='id')","ac670bec":"print('Train data of shape {}'.format(train.shape))\ndisplay(train.head())\nprint('Test data of shape {}'.format(test.shape))\ndisplay(test.head())","ebda4824":"target = train.pop('target')","335e0d8d":"display(train.info())","129b8edf":"display(train.describe())","faeb73d7":"# train_data missing values\nunique_values_train = []\nfor col in train.columns:\n    c = train[col].nunique()\n    pc = np.round((100 * (c)\/len(train)), 2)            \n    dict1 ={\n        'Features' : col,\n        'unique_train (count)': c,\n        #'unique_trian (%)': '{}%'.format(pc)\n    }\n    unique_values_train.append(dict1)\nDF1 = pd.DataFrame(unique_values_train, index=None).sort_values(by='unique_train (count)',ascending=False)\n#print(DF1)\n\n\n# test_data missing values\nunique_values_test = []\nfor col in test.columns:\n    c = test[col].nunique()\n    pc = np.round((100 * (c)\/len(test)), 2)            \n    dict2 ={\n        'Features' : col,\n        'unique_test (count)': c,\n        #'unique_test (%)': '{}%'.format(pc)\n    }\n    unique_values_test.append(dict2)\nDF2 = pd.DataFrame(unique_values_test, index=None).sort_values(by='unique_test (count)',ascending=False)\n#print(DF2)\n\ndf = pd.concat([DF1, DF2], axis=1)\ndf#.head()","a0672e3f":"fig = go.Figure(data=[go.Scatter(x=DF1['Features'],\n                             y=DF1[\"unique_train (count)\"], mode= 'markers',                             \n                             name='Train', marker_color='lightseagreen'),        \n\n                go.Scatter(x=DF2['Features'],\n                             y=DF2[\"unique_test (count)\"], mode= 'markers',\n                             name='Test', marker_color='lightsalmon')])\nfig.update_traces(marker_line_color='black', marker_line_width=1.5, opacity=1)\nfig.update_layout(title_text='Unique Values In Each Feature ', \n                  #template='plotly_dark',\n                  paper_bgcolor='#f8f0ec',\n                  plot_bgcolor='#f8f0ec',\n                  width=750, height=500,\n                  xaxis_title='Features', yaxis_title='Count',\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'})\nfig.show()","63cad053":"targeT = target\nfig = px.histogram(targeT, x=\"target\", \n                   width=700, \n                   height=500,\n                   histnorm='percent',                 \n                   template=\"simple_white\"\n                   )\ncolors = ['#00a08f'] * 9 \ncolors[0] = ['#A52A2A']\ncolors[1] = ['#A52A2A']\ncolors[8] = 'salmon'\n\n\nfig.update_traces(marker_color=colors, marker_line_color='red',\n                  marker_line_width=2.5, opacity=0.5)\n\nfig.update_layout(title=\"<b>Target Class Distribution<b>\", \n                  font_family=\"San Serif\",\n                  titlefont={'size': 24},\n                  legend=dict(\n                  orientation=\"v\", y=1, yanchor=\"top\", x=1.0, xanchor=\"right\" )                 \n                 ).update_xaxes(categoryorder='total descending') # ordering the x-axis values\n\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False)\nfig.show()","f22f85f5":"import holoviews as hv\nfrom holoviews import opts, dim\nhv.extension('bokeh')","f0ff41f9":"df_train = hv.Dataset(train)\ndf_test = hv.Dataset(test)\n\nfeat = test.columns[:]\nhist1 = df_train.hist(dimension=list(feat), bins=10, adjoin=False)\nhist1.opts(opts.Histogram(alpha=0.9, width=300, height=200))\nhist1.opts(title='Train data Histograms', fontscale=1.5)","d77667a2":"hist2 = df_test.hist(dimension=list(feat), bins=10, adjoin=False)\nhist2.opts(opts.Histogram(alpha=0.9, width=300, height=200))\nhist2.opts(title='Test data Histograms', fontscale=1.5)\nhist2","2f723548":"## Install LightAutoML\n!pip install -U lightautoml","adfe8a42":"# Standard python libraries\nimport os\nimport time\nimport re\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\n# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.dataset.roles import NumericRole","b2464bc8":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 2021 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 5 * 3600 # Time in seconds for automl run\nTARGET_NAME = 'target'","6d99e3c2":"test_data = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntrain_data[TARGET_NAME] = train_data[TARGET_NAME].str.slice(start=6).astype(int) - 1\ntrain_data.head()","17eab00c":"test_data = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\ntest_data.head()","79e8fd37":"submission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\nsubmission.head()","9f1a7b93":"# selecting some features which have less that 50% zeros and make new interacting features\n\n#train\ntrain_data['50p43'] = train_data['feature_50'] + train_data['feature_43']\ntrain_data['50p54'] = train_data['feature_50'] + train_data['feature_54']\ntrain_data['50p19'] = train_data['feature_50'] + train_data['feature_19']\ntrain_data['50p12'] = train_data['feature_50'] + train_data['feature_12']\n\n# test \ntest_data['50p43'] = test_data['feature_50'] + test_data['feature_43']\ntest_data['50p54'] = test_data['feature_50'] + test_data['feature_54']\ntest_data['50p19'] = test_data['feature_50'] + test_data['feature_19']\ntest_data['50p12'] = test_data['feature_50'] + test_data['feature_12']","ad97b999":"%%time\ntask = Task('multiclass', )\n\nroles = {\n    'target': TARGET_NAME,\n    'drop': ['id'],\n}","745d5b7d":"%%time\n\nautoml = TabularAutoML(task = task,\n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       \n                       general_params ={\n                           'use_algos': [['lgb_tuned', 'cb_tuned'], ['lgb_tuned', 'cb_tuned']],\n#                            'return_all_predictions': True,\n#                            'weighted_blender_max_nonezero_coef': 0.0                                                   \n                       },                       \n#                        tuning_params = {'max_tuning_time': 1800},\n                        reader_params = {'n_jobs': N_THREADS},\n                       \n)\noof_pred = automl.fit_predict(train_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","5941d172":"%%time\ntest_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(log_loss(train_data[TARGET_NAME].values, oof_pred.data)))","9ee38830":"# Fast feature importances calculation\nfast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.barh(figsize = (10, 16), grid = True, title='Feature Importance').invert_yaxis()","f220f97b":"submission.iloc[:, 1:] = test_pred.data\nsubmission.to_csv('lightautoml_2lvl_5hrs_newFeats_1.csv', index = False)","a06806b0":"prediction_plot = pd.Series(test_pred.data.argmax(axis=1)).replace({0: \"Class_1\", 1: \"Class_2\", 2: \"Class_3\", \n                                                                   3: \"Class_4\", 4: \"Class_5\", 5: \"Class_6\",\n                                                                   6: \"Class_7\", 7: \"Class_8\", 8: \"Class_9\", \n                                                                   9: \"Class_10\"})\n\nprint(prediction_plot.value_counts())\n\nfig = plt.figure(figsize=(8,4))\nsns.countplot(prediction_plot)\nplt.show()\n","f7be0b91":"### Features Distribution (Histograms)\n* Category\/value zero dominates the features","6a8d79d5":"### Dataset size\n* Train: 200000 rows, 75 features and a target columns\n* Test: 100000 rows and 75 features columns","3d181af7":"### Set params","b673eb3f":"### Train on full data","d618a67a":"## Thank you for reading this notebook!","b43012cb":"# Introduction\n\nStarting from January this year, the kaggle competition team is offering a month-long tabulary playground competitions. This series aims to bridge between inclass competition and featured competitions with a friendly and approachable datasets. For June kaggle is offering a dataset which is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\nGoal of the competition: Predict 9 classes given 75 features.\n\nSubmissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true Class. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:\n\n$ log loss = - \\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{M} y_{ij}log(p_{ij}) $.\n\nwhere **$N$** is the number of rows in the test set, **$M$** is the number of class labels, **$log$** is the natural logarithm, **$y_{ij}$** is 1 if observation  is in class  and 0 otherwise, and **$p_{ij}$** is the predicted probability that observation **$i$** belongs to class **$j$**.\n\nNote: The submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the  function, predicted probabilities are replaced with.","347fcb9c":"# 2. Models\n\n* **version_6**: Boosted trees (lgbm, xgboost, catboost)\n* **version_7**: LightAutoML with Alexander Ryzhkov's starter code. Thanks Alex!\n> Aim is to get a hang on LightAutoML library","111cf167":"### Unique values in Features\n* Most of the features have the same number of unique values in test and trian data. Only Features **15, 28, 46, 59, 60, and 73** have different values in test and train dataset","c609f646":"### Prediction, check OOF score","bff7d7de":"### Target Distribution\n\n* Target class_6 and class_8 are the dominant target classes\n* Target class_5 and class_4 are the fewest target classes","4fc3b853":"# June 2021 TPS","8826ce12":"### Submission","6d311669":"### Additional Features (FE)\n* Note: This part is not in Alexander Ryzhkov's starter code.\n* Trying additional featured by just combining some features which have relatively lower percentage of zeros","6755414f":"### No missing values in the data ","07789ff6":"### Visualizing the predicted classes\n* class_8, class_6 and class_2 are the most predicted classes. Where are the other classes?","9aeb405d":"### Looking at the feature importance\n\n* Interesting to note that the newly created features are in the top 'rank', especially `feature 50p43`","b2519f13":"## AutoML preset usage\n### Setup task and column roles","e75bb1e9":"### Load Data","87d16234":"### Imports","161daeaf":"# 1 EDA\n\n### Imports"}}