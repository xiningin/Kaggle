{"cell_type":{"7f1e28f7":"code","3f017fac":"code","d257f576":"code","741c78b8":"code","edd548cc":"code","0dcd136c":"code","b873c825":"code","01db0875":"code","3c6de51f":"code","b0d12a35":"code","137f26e4":"code","d25137fe":"code","7dbf5ca1":"code","54dd106d":"code","d0166590":"code","c49b4ba2":"code","acf17847":"code","1297c446":"code","72e924da":"code","c88be540":"code","213e39a1":"code","7163a7bb":"code","13e0db8c":"code","68392dd5":"markdown","126d2dd7":"markdown","6df80086":"markdown"},"source":{"7f1e28f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3f017fac":"data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","d257f576":"len(data[data['target']==1]); len(data)","741c78b8":"data=data.to_numpy()","edd548cc":"x_trn,y_trn = data[:,1:-1][:4500],data[:,-1][:4500]\nx_vld,y_vld = data[:,1:-1][4500:],data[:,-1][4500:]","0dcd136c":"len(x_trn), len(x_vld)","b873c825":"x_trn[0]","01db0875":"from sklearn.feature_extraction.text import CountVectorizer\nveczr = CountVectorizer(ngram_range=(1,3))","3c6de51f":"trn_term = veczr.fit_transform(x_trn[:,2])\nvld_term = veczr.transform(x_vld[:,2])","b0d12a35":"vocab = veczr.get_feature_names(); vocab[:100], len(vocab)","137f26e4":"trn_term.shape","d25137fe":"y_trn=y_trn.astype('int64')\ny_vld=y_vld.astype('int64')","7dbf5ca1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score as f1\nm=LogisticRegression(C=1e8,dual=True,solver='liblinear',max_iter=400, random_state=1)\nm.fit(trn_term,y_trn)\npreds = m.predict(vld_term)\n(preds==y_vld).mean(), f1(y_vld,preds)","54dd106d":"m=LogisticRegression(C=10,dual=True,solver='liblinear',max_iter=400, random_state=1)\nm.fit(trn_term,y_trn)\npreds = m.predict(vld_term)\n(preds==y_vld).mean(), f1(y_vld,preds)","d0166590":"p = trn_term[y_trn==1].sum(0)+1\nq = trn_term[y_trn==0].sum(0)+1\n\nr = np.log(((p\/p.sum())\/(q\/q.sum())))","c49b4ba2":"m=LogisticRegression(C=0.1,dual=True,solver='liblinear',max_iter=300, random_state=1)\nm.fit(trn_term.multiply(r),y_trn)\npreds = m.predict(vld_term.multiply(r))\n(preds==y_vld).mean(), f1(y_vld,preds)","acf17847":"x = veczr.fit_transform(data[:,3])\ny = data[:,-1]\n","1297c446":"y","72e924da":"p = x[y==1].sum(0)+1\nq = x[y==0].sum(0)+1\np,q","c88be540":"r = np.log((p\/p.sum())\/(q\/q.sum()))\ntest = veczr.transform(test_data[\"text\"])\ntest_data[\"text\"]","213e39a1":"m=LogisticRegression(C=0.1,dual=True,solver='liblinear',max_iter=500, random_state=69)\nm.fit(x.multiply(r),y.astype('int64'))\nsubmission[\"target\"] = m.predict(test.multiply(r))\nsubmission.to_csv(\"submission1.csv\", index=False)","7163a7bb":"m=LogisticRegression(C=1e8,dual=True,solver='liblinear',max_iter=600, random_state=69)\nm.fit(x,y.astype('int64'))\nsubmission[\"target\"] = m.predict(test)\nsubmission.to_csv(\"submission2.csv\", index=False)","13e0db8c":"# kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Message\"","68392dd5":"# This Notebook is not aimed at high f1 score.\n# It's just a demonstration of how the Deterministic Naive Bayes Algorithm can be combined with Logistic regression to get decent results.","126d2dd7":"dual = True, when the data is **wider than taller.**","6df80086":"Multiplying the training data sparse matrix X by the Naive Bayes Matrix r, regularizes the data more efficiently.\nSubmission1 implements Naive Bayes whereas Submission2 is our good old Logistic Regression."}}