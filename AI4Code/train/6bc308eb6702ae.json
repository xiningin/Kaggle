{"cell_type":{"72c2755d":"code","96f4f2aa":"code","6cb239cf":"code","071d5568":"code","e8fb74d1":"code","eab52c4a":"code","8df5b985":"code","483e5e3a":"code","3da4062d":"code","7af7f2f6":"code","28469086":"code","81a3a6b5":"code","11e1ba8f":"code","8943c540":"code","7503c3b2":"code","5b8cc4f7":"code","d8641fe2":"code","8103abbf":"code","07e5af39":"code","3d02c53a":"code","ed4bd1f6":"code","5fb1a7d0":"code","6d7d40a7":"code","ec86492e":"markdown","21bb9fd5":"markdown","a665288f":"markdown","7e8f08ce":"markdown"},"source":{"72c2755d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96f4f2aa":"import numpy as np \nimport pandas as pd\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nseed = 16 \nnp.random.seed(seed)","6cb239cf":"train = pd.read_csv('..\/input\/assignment-regression-1\/train.csv')\ntest = pd.read_csv('..\/input\/assignment-regression-1\/test.csv')\nsub  = pd.read_csv('..\/input\/assignment-regression-1\/sub.csv')\n\ntrain.shape, test.shape, sub.shape","071d5568":"train.head()","e8fb74d1":"train.isnull().sum()","eab52c4a":"test.isnull().sum()","8df5b985":"train.drop(\"cloud_coverage\", axis=1, inplace=True)","483e5e3a":"test.drop(\"cloud_coverage\", axis=1, inplace=True)","3da4062d":"#Fill missing values with median \n\nfor c in train.columns :\n    \n    if train[c].isna().sum() != 0:\n        train[c] = train[c].fillna(train[c].median())\n        \n    if test[c].isna().sum() != 0:\n        test[c] = test[c].fillna(test[c].median())","7af7f2f6":"train.nunique()","28469086":"train['primary_use'].value_counts()","81a3a6b5":"fig=plt.figure(figsize=(15,10))\nx=train['primary_use']\nplt.hist(x,bins=20)","11e1ba8f":"#We need to convert strings into integres for 'primary_use'.... Google Label Encoding\nencoder = LabelEncoder()\n\ntrain['primary_use'] = encoder.fit_transform(train['primary_use'])\ntest['primary_use'] = encoder.transform(test['primary_use'])","8943c540":"\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])\n\ntrain['month'] = train['timestamp'].dt.month\ntest['month'] = test['timestamp'].dt.month \n\ntrain['day'] = train['timestamp'].dt.day\ntest['day'] = test['timestamp'].dt.day\n\ntrain['hour'] = train['timestamp'].dt.hour\ntest['hour'] = test['timestamp'].dt.hour\n","7503c3b2":"x=train.drop(['meter_reading'],axis=1)\ny=train['meter_reading']\n\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.30,random_state=seed)","5b8cc4f7":"X_test[\"primary_use\"] = X_test[\"primary_use\"].astype('category')\nX_test[\"primary_use\"]\nX_test[\"primary_use\"] = X_test[\"primary_use\"].cat.codes","d8641fe2":"X_train[\"primary_use\"] = X_train[\"primary_use\"].astype('category')\nX_train[\"primary_use\"]\nX_train[\"primary_use\"] = X_train[\"primary_use\"].cat.codes","8103abbf":"X_train.describe()","07e5af39":"X_train.nunique()","3d02c53a":"X_train.info()","ed4bd1f6":"#from sklearn.preprocessing import StandardScaler\n#sc=StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test = sc.transform(X_test)","5fb1a7d0":"# features used for traiining models\nfeats = ['building_id'] + list(train.columns[3:])\n\n\n#spliting data to training and validation\ntrain1, valid = train_test_split(train, test_size = 0.5, random_state = seed)\n\n\n\n\nmodel = DecisionTreeRegressor(random_state = seed,max_depth=5)\n\n\nmodel.fit(train1[feats], train1['meter_reading'])\ny_valid = model.predict(valid[feats])\nX_test=valid[feats]\ny_train=valid['meter_reading']\n\n\n#RMSE value for validation \nmse=mean_squared_error(valid['meter_reading'], y_valid, squared = False)\nrmse=np.sqrt(mse)\nprint(rmse)","6d7d40a7":"#predict for test set \nsub['meter_reading'] = model.predict(test[feats])\nsub.to_csv('submission_final.csv', index = False)","ec86492e":"From here we can see that buildings used for education purpose are more than the rest of them.","21bb9fd5":"There are 101 buildings and 7 primary use of buildings.","a665288f":"Building a model using Decision Tree","7e8f08ce":"Since,cloud coverage has a lot of missing values, we'll drop that column in both test and train dataframe."}}