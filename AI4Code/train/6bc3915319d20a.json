{"cell_type":{"6bf5ae05":"code","8f8ec265":"code","abe21ecb":"code","97c21e60":"code","2243ae0d":"code","7fdaca3c":"code","7b2757fd":"code","1321704d":"code","e01aebb3":"code","844ee9e2":"code","f6f435e1":"code","24c0f8e5":"code","53bdd48f":"code","8e808546":"code","75fde46c":"code","0bddcf7d":"code","1576acb2":"code","e9c8e67b":"code","751825be":"code","287c717f":"code","fcbe23b3":"code","b254786e":"code","fe2f99cc":"code","3760e491":"code","c4be0147":"code","dd779cb3":"code","1fc434a3":"code","2cf4bfc5":"code","fc49db6b":"code","e9dcaa03":"code","95baa4dd":"code","a82b6e00":"code","33852331":"code","5ee28be0":"code","01c7f205":"code","017f4e62":"code","8974ff20":"code","12bc46ff":"code","80428c09":"code","9972b2c8":"code","43ec7db1":"code","1eb988a4":"code","ee8e9837":"code","807f9447":"code","f81d7143":"code","c509d184":"code","98119b06":"code","c87ca543":"code","99629c14":"code","962dff4b":"code","d0e298da":"code","8eacff38":"code","782587ca":"code","08c2c40a":"code","ae2cf135":"markdown","6207a31e":"markdown","0fe5d9ad":"markdown","33c40d6d":"markdown","90f6897d":"markdown","e2fe2e87":"markdown","578526a3":"markdown","97ff266b":"markdown","a7cd1063":"markdown","c90ea83f":"markdown","c44df66b":"markdown"},"source":{"6bf5ae05":"# Import the required packages here\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore') #Remove warning messages within the kernel","8f8ec265":"# Import the datasets here\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","abe21ecb":"print(train.shape)\nprint(test.shape)","97c21e60":"# Append the train and test dataframes for data cleaning. \n# We also add a flag so that at a later stage, we can split the combined dataframe to train and test again\n\ntrain['test_flag'] = 0\ntest['test_flag'] = 1\ndf_combined = pd.concat([train, test], axis=0, copy=True)","2243ae0d":"# Check the % missing values in all the columns of the train set\nprint(df_combined.isnull().sum()*100\/df_combined.shape[0])\n\n# Ignore the missing values for the output class 'Survived' as they are from the test set","7fdaca3c":"# Subsetting for the list of columns which has less to no missing values\n\ndf_subset = df_combined[['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Name', 'Embarked', 'test_flag']]","7b2757fd":"# Cleaning and level modifications for the categorical features\n\nfor dataset in df_subset:\n    df_subset['Title'] = df_subset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(df_subset['Title'], df_subset['Sex'])","1321704d":"for dataset in df_subset:\n    df_subset['Title'] = df_subset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    df_subset['Title'] = df_subset['Title'].replace('Mlle', 'Miss')\n    df_subset['Title'] = df_subset['Title'].replace('Ms', 'Miss')\n    df_subset['Title'] = df_subset['Title'].replace('Mme', 'Mrs')\n    \ndf_subset[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","e01aebb3":"df_subset['Cabin'] = df_subset['Cabin'].replace(np.nan, 'U')\ndf_subset['Cabin_Class'] = df_subset['Cabin'].astype(str).str[0]","844ee9e2":"# Remove the columns here, for which we have generated our own classes\n\ndf_subset = df_subset.drop(['Ticket', 'Cabin', 'Name'], axis = 1)","f6f435e1":"median_age = df_subset[\"Age\"].median()\ndf_subset[\"Age\"].fillna(median_age, inplace=True)","24c0f8e5":"df_subset['Age'].describe()","53bdd48f":"df_subset.Age[df_subset.Age < 18] = 1\ndf_subset.Age[df_subset.Age >= 18] = 0","8e808546":"df_subset['Age'] = df_subset['Age'].astype(int) # Convert the age column to int type to be able to perform operations","75fde46c":"for dataset in df_subset:\n    df_subset['FamilySize'] = df_subset['SibSp'] + df_subset['Parch'] + 1","0bddcf7d":"df_subset = df_subset.drop(['SibSp', 'Parch'], axis = 1)","1576acb2":"df_subset.FamilySize[df_subset.FamilySize == 1] = 0\ndf_subset.FamilySize[(df_subset.FamilySize > 1) & (df_subset.FamilySize <= 4)] = 1\ndf_subset.FamilySize[(df_subset.FamilySize > 4)] = 2","e9c8e67b":"df_subset['Fare'].describe()","751825be":"df_subset['Fare'].fillna(df_subset['Fare'].dropna().median(), inplace=True)","287c717f":"df_subset.Fare[df_subset.Fare <= 7.91] = 0\ndf_subset.Fare[(df_subset.Fare > 7.91) & (df_subset.Fare <= 14.454)] = 1\ndf_subset.Fare[(df_subset.Fare > 14.454) & (df_subset.Fare <= 31)] = 2\ndf_subset.Fare[df_subset.Fare > 31] = 3\ndf_subset['Fare'] = df_subset['Fare'].astype(int) # Convert the fare column to integer type\n\ndf_subset.head()","fcbe23b3":"mode_embarked = df_subset.Embarked.dropna().mode()[0]\ndf_subset[\"Embarked\"].fillna(mode_embarked, inplace=True)","b254786e":"print(df_subset.columns)\nprint(df_subset.head(2))","fe2f99cc":"# One-hot encoding: Pclass, Sex, Age, Fare, Embarked, Title, Cabin_Class, Family Size\n# Label encoding:\n# Drop:","3760e491":"var = 'Sex'\n\nSex = df_subset[[var]]\nSex = pd.get_dummies(Sex,drop_first=True)\nSex.head()","c4be0147":"var = 'Embarked'\n\nEmbarked = df_subset[[var]]\nEmbarked = pd.get_dummies(Embarked,drop_first=True)\nEmbarked.head()","dd779cb3":"var = 'Title'\n\nTitle = df_subset[[var]]\nTitle = pd.get_dummies(Title,drop_first=True)\nTitle.head()","1fc434a3":"var = 'Cabin_Class'\n\nCabin_Class = df_subset[[var]]\nCabin_Class = pd.get_dummies(Cabin_Class,drop_first=True)\nCabin_Class.head()","2cf4bfc5":"df_subset_int = pd.concat([df_subset, Sex, Embarked, Title, Cabin_Class],axis=1)","fc49db6b":"df_subset_int.drop(['Sex', 'Embarked', 'Title', 'Cabin_Class'],axis=1,inplace=True)\ndf_subset_int.head()","e9dcaa03":"# # In this step, we use the factorize method to factorize the categorical features that we have created above.\n# # This is done for model interpretability\n\n# cols_new = ['Embarked', 'Title', 'Sex', 'Cabin_Class']\n\n# for col in cols_new:\n#     df_subset[col] = pd.factorize(df_subset[col])[0] + 1\n    \n# df_subset.head()","95baa4dd":"# We split the combined dataset to train and test set again\n\ntrain_set = df_subset_int[df_subset['test_flag']==0]\ntest_set = df_subset_int[df_subset['test_flag']==1]\n\ntrain_set['Survived'] = train_set['Survived'].astype(int) # Convert the target feature to integer","a82b6e00":"# Check if the train and test split was done properly\n\nprint(train_set.shape)\nprint(test_set.shape)","33852331":"# Remove the columns which are not required. Test Flag is not required since we split the data already, and the Survived column is redundant in the test set\n\ntest_set = test_set.drop(['Survived', 'test_flag'], axis = 1)\ntrain_set = train_set.drop(['test_flag'], axis = 1)","5ee28be0":"train_set.head(5)","01c7f205":"train_set = train_set.drop(['PassengerId'], axis = 1)","017f4e62":"print(train_set.shape)\nprint(test_set.shape)","8974ff20":"from sklearn.model_selection import train_test_split\n\nX = train_set[train_set.columns.drop('Survived')]\ny = train_set[['Survived']]","12bc46ff":"from sklearn.ensemble import ExtraTreesRegressor\nselection= ExtraTreesRegressor()\nselection.fit(X,y)","80428c09":"plt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","9972b2c8":"from sklearn.model_selection import train_test_split\n\nX = train_set[['Cabin_Class_T', 'Cabin_Class_F', 'Cabin_Class_G', 'Cabin_Class_C', \n               'Cabin_Class_D', 'Cabin_Class_B', 'Title_Mrs', 'Cabin_Class_E', 'Embarked_Q', \n               'Sex_male', 'Title_Miss', 'Age', 'Embarked_S', 'Title_Rare', 'Cabin_Class_U', \n               'Fare', 'Pclass', 'FamilySize', 'Title_Mr']]\ny = train_set[['Survived']]","43ec7db1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","1eb988a4":"X_test.head()","ee8e9837":"# Logistic Regression Model\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogistic_model = logreg.fit(X_train, y_train)\npredictions = logistic_model.predict(X_test)\n\nacc_logistic_regressions = round(logistic_model.score(X_train, y_train) * 100, 2)\nacc_logistic_regressions","807f9447":"# Decision Tree model\nfrom sklearn.tree import DecisionTreeClassifier\n\ndec_tree = DecisionTreeClassifier()\ndecision_tree = dec_tree.fit(X_train, y_train)\nprediction_tree = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","f81d7143":"# Random Forest Model\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100)\nrandom_forest = rf.fit(X_train, y_train)\nprediction_forest = random_forest.predict(X_test)\n\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","c509d184":"# Naive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngaussian = gnb.fit(X_train, y_train)\nprediction_naive = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","98119b06":"from catboost import CatBoostClassifier\nclf = CatBoostClassifier(\n    iterations=10,\n#     verbose=5,\n)\n\ncatboost = clf.fit(X_train, y_train,\n    # cat_features=cat_features,\n    eval_set=(X_test, y_test),\n)","c87ca543":"acc_catboost = round(catboost.score(X_train, y_train) * 100, 2)\nacc_catboost","99629c14":"test_feature = test_set[['Cabin_Class_T', 'Cabin_Class_F', 'Cabin_Class_G', 'Cabin_Class_C', \n               'Cabin_Class_D', 'Cabin_Class_B', 'Title_Mrs', 'Cabin_Class_E', 'Embarked_Q', \n               'Sex_male', 'Title_Miss', 'Age', 'Embarked_S', 'Title_Rare', 'Cabin_Class_U', \n               'Fare', 'Pclass', 'FamilySize', 'Title_Mr']]\ntest_id = test_set['PassengerId']\ntest_feature.head()","962dff4b":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest',\n             'Naive Bayes', 'Cat Boost'],\n    'Score': [acc_logistic_regressions, acc_decision_tree, acc_random_forest,\n             acc_gaussian, acc_catboost]})\nmodels.sort_values(by='Score', ascending=False)","d0e298da":"test_predictions = decision_tree.predict(test_feature)","8eacff38":"output = pd.DataFrame({'PassengerId' : test_id, 'Survived': test_predictions})\noutput.head()","782587ca":"output.to_csv('\/kaggle\/working\/submission.csv', index=False)","08c2c40a":"# Use this data type conversion for the output feature, if the score comes as 0 in submission\n\n# model.predict(test_data).astype(int)","ae2cf135":"### Fare\n\n* The **Fare** column has been used to create buckets of Fare class\n* Firstly, the missing values (0.07%) are replaced using Median imputation\n* The buckets are Fare <= 7.91 (0), Fare > 7.91 and Fare <= 14.454 (1), Fare > 14.454 and Fare <= 31 (2), Fare > 31 (3)\n* The Fare buckets have been created based on the percentile of data availabilities","6207a31e":"### Age\n\n* We find the median of the **Age** feature to impute the missing ages, which is roughly about 20%\n* We use the same **Age** column to create buckets of age groups. For _less than 18 (1)_ and for _greater than or equal to 18 (0)_","0fe5d9ad":"### SibSp & Parch\n\n* We use the **Siblings** and the **Parents\/Children** features combined to generate a feature called **Family Size**\n* Having created a new feature out of the above two, we drop the original features\n* We also use the **Family Size** feature to create buckets. _Family Size = 1 (0), Family Size greater than 1 and less than or equal to 4 (1), Family Size greater than 4 (2)_","33c40d6d":"### High level EDA\n\nWe take a look at the % of missing values in each of the features available in the train and test dataset.\n\n_Ignore the missing values in the 'Survived' feature, because of the concatenation the missing values are attributed from the test set_","90f6897d":"So far, catboost provided the best competition scores for me","e2fe2e87":"### Data Cleaning and missing value imputation for the columns\n\n* One of the most common approach to feature engineering has been using the **Name** feature to create a Title feature using regex\n* Using the **Cabin** feature to create classes from the initial letter of the column. The nan have been tagged as 'U' or 'Unknown' class in the data\n* We'll not use the **Ticket** feature for now, as that feature didn't make quite a lot of sense to me\n","578526a3":"It is observed that the best accuracy is obtained in the Random Forest Model, which we will use to make our submissions to the competition.","97ff266b":"### Model Implementation Process\n\nWe use the engineered features into the models to check out the scores. Below are the set of models that we use for this Kernel\n1. Logistic Regression\n2. Decision Tree\n3. Random Forest\n4. Naive Bayes","a7cd1063":"### Embarked\n\n* We do a simple mode imputation in the Embarked feature which attributes to approximately 0.15% missing","c90ea83f":"## Titanic Disaster Dataset Feature Engineering\n\nThis notebook is an attempt towards kick-starting my way around the Titanic dataset to improve my model scores based on feature engineering techniques.\nThis is not going to be an extensive EDA notebook for viewers, as there are already tons of notebooks available with great amount of EDA to understand the intricacies of the dataset from all aspects.\n* We will work with missing value imputations\n* We will also try to combine some features with each other to come up with new ones\n* We will try to improve our scores by building up on our feature engineering process","c44df66b":"### Use the section below to submit on the test set"}}