{"cell_type":{"e6a4a975":"code","61a8810a":"code","102a5178":"code","b1711f0a":"code","98b0343c":"code","b3ffbcd6":"code","8a324955":"code","918e8e94":"code","3113ce53":"code","403c230f":"code","d274e45a":"code","3bace521":"code","427abff1":"code","d407d00f":"code","098ba9ac":"code","364ea312":"code","4b54b710":"code","f21188ca":"code","351d71cc":"code","6f85c76d":"code","87bbf57e":"code","7210a187":"code","34011d69":"code","a7b1efc4":"code","0bf3cc13":"code","0d2947fc":"code","af8dfa51":"code","5ea548e3":"code","eabf4a08":"code","abb79b58":"code","d42ab1d6":"code","97bb1a62":"code","060a86b8":"code","7890e39a":"code","0f009a3d":"code","806633d8":"code","d74aed2f":"code","58bb1110":"code","58b123f7":"code","0f063225":"code","ece146f6":"code","eab45a48":"markdown","24de164e":"markdown","885faab3":"markdown","dd6a7b8e":"markdown","b4906b4f":"markdown","b0b27d21":"markdown","a1ff67a9":"markdown","4311ce68":"markdown","8ec414ab":"markdown","58ba576f":"markdown","a42249fe":"markdown","77d8de2f":"markdown","447bfbdd":"markdown","4b9ba550":"markdown","7f984164":"markdown","6507800a":"markdown"},"source":{"e6a4a975":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61a8810a":"import datetime\n\n# print time\ndef printbar():\n    nowtime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print (\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)","102a5178":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch \nfrom torch import nn \nfrom torch.utils.data import Dataset,DataLoader,TensorDataset","b1711f0a":"%matplotlib inline","98b0343c":"dftrain_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ndftest_raw = pd.read_csv('..\/input\/titanic\/test.csv')\ndftrain_raw.head(10)","b3ffbcd6":"dftrain_raw.info()","8a324955":"dftrain_raw.isnull().sum(axis=0)","918e8e94":"fig, axe = plt.subplots(dpi=150)\n\nx = np.arange(len(dftrain_raw.Survived.unique()))\naxe.bar(x, dftrain_raw.Survived.value_counts(), width = 0.4)\naxe.set_xticks(x)\naxe.set_xticklabels(dftrain_raw.Survived.unique())\n\nplt.show()","3113ce53":"fig, axe = plt.subplots(dpi=150)\n\naxe.hist(dftrain_raw.Age, color='purple', bins=20)\nplt.show()","403c230f":"fig, axe = plt.subplots(dpi=150)\nfor survived_status in dftrain_raw.Survived.unique():\n    sns.distplot(dftrain_raw.loc[dftrain_raw.Survived==survived_status].Age, hist=False, label=survived_status)","d274e45a":"pd.get_dummies(dftrain_raw.Pclass)","3bace521":"pd.isna(dftrain_raw['Age'])","427abff1":"def preprocessing(dfdata):\n    \n    dfresult = pd.DataFrame()\n    \n    #Pclass\n    dfresult['Pclass'] = dfdata['Pclass']\n    \n    #Sex\n    dfSex = pd.get_dummies(dfdata['Sex'])\n    dfresult = pd.concat([dfresult, dfSex], axis=1)\n    \n    #Age\n    dfresult['Age'] = dfdata['Age'].fillna(dfdata['Age'].median())\n    dfresult['Age_null'] = pd.isna(dfdata['Age']).astype('int32')\n    \n    #SibSp, Parch, Fare\n    dfresult['SibSp'] = dfdata['SibSp']\n    dfresult['Parch'] = dfdata['Parch']\n    dfresult['Fare'] = dfdata['Fare']\n    \n    #Carbin -- too many missing values\n    dfresult['Cabin_null'] = pd.isna(dfdata['Cabin']).astype('int32')\n    \n    #Embarked\n    dfEmbarked = pd.get_dummies(dfdata['Embarked'], dummy_na=True)\n    dfEmbarked.columns = ['Embarked_' + str(x) for x in dfEmbarked.columns]\n    dfresult = pd.concat([dfresult, dfEmbarked], axis=1)\n    \n    return(dfresult)","d407d00f":"from sklearn.model_selection import train_test_split","098ba9ac":"whole_dataset = preprocessing(dftrain_raw)\n\nxtrain, xtest, ytrain, ytest=train_test_split(whole_dataset, dftrain_raw[['Survived']], train_size=0.8, random_state=88)","364ea312":"xtrain = xtrain.values\nytrain = ytrain.values\n\nxvalid = xtest.values\nyvalid = ytest.values\n\nprint(\"xtrain.shape =\", xtrain.shape)\nprint(\"xvalid.shape =\", xvalid.shape)\n\nprint(\"ytrain.shape =\", ytrain.shape)\nprint(\"yvalid.shape =\", yvalid.shape)","4b54b710":"x_test = preprocessing(dftest_raw).values\n\nprint(\"x_test.shape =\", x_test.shape )","f21188ca":"dl_train = DataLoader(TensorDataset(torch.tensor(xtrain).float(), torch.tensor(ytrain).float()), shuffle=True, batch_size=8)\ndl_valid = DataLoader(TensorDataset(torch.tensor(xvalid).float(), torch.tensor(yvalid).float()), shuffle=True, batch_size=8)","351d71cc":"# test our dataloader\nfor features,labels in dl_train:\n    print(features,labels)\n    break","6f85c76d":"# test our dataloader\nfor features,labels in dl_valid:\n    print(features,labels)\n    break","87bbf57e":"def create_net():\n    net = nn.Sequential()\n    net.add_module(\"linear1\", nn.Linear(13, 64))\n    net.add_module(\"relu1\", nn.ReLU())\n    net.add_module(\"linear2\",nn.Linear(64,16))\n    net.add_module(\"relu2\",nn.ReLU())\n    net.add_module(\"linear3\",nn.Linear(16,1))\n    net.add_module(\"sigmoid\",nn.Sigmoid())\n    return net","7210a187":"net = create_net()\nprint (net)","34011d69":"from sklearn.metrics import accuracy_score\n\nloss_func = nn.BCELoss()\noptimizer = torch.optim.Adam(params=net.parameters(), lr=0.005)\nmetric_func = lambda y_pred, y_true: accuracy_score(y_true.data.numpy(), y_pred.data.numpy()>0.5)\nmetric_name = \"accuracy\"","a7b1efc4":"epochs = 100\nlog_step_freq = 30\n\ndfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",metric_name,\"val_loss\",\"val_\"+metric_name]) \nprint(\"Start Training...\")\nnowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\nprint(\"==========\"*8 + \"%s\"%nowtime)\n\nfor epoch in range(1, epochs+1):\n    \n    #1. Training loop --------------------------------------------------\n    net.train()\n    loss_sum = 0.0\n    metric_sum = 0.0\n    step = 1\n    \n    for step, (features, labels) in enumerate(dl_train, 1):\n        \n        # delete previous grad\n        optimizer.zero_grad()\n        \n        # loss\n        predictions = net(features)\n        loss = loss_func(predictions, labels) # loss score\n        metric = metric_func(predictions, labels) # accuracy score\n        \n        # backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        # print loss\n        loss_sum += loss.item()\n        metric_sum += metric.item()\n        \n        if step%log_step_freq == 0:\n            print((\"[step = %d] loss: %.3f, \"+metric_name+\": %.3f\") %\n                  (step, loss_sum\/step, metric_sum\/step))\n            \n    #2 Validation loop ----------------------------------------------------\n    \n    net.eval()\n    val_loss_sum = 0.0\n    val_metric_sum = 0.0\n    val_step = 1\n    \n    for val_step, (val_features, val_labels) in enumerate(dl_valid, 1):\n        # close gradient calculation\n        with torch.no_grad():\n            predictions = net(val_features)\n            val_loss = loss_func(predictions, val_labels)\n            val_metric = metric_func(predictions, val_labels)\n        \n        val_loss_sum += val_loss.item()\n        val_metric_sum += val_metric.item()\n        \n    #3 remember loss info\n    info = (epoch, loss_sum\/step, metric_sum\/step, val_loss_sum\/val_step, val_metric_sum\/val_step)\n    dfhistory.loc[epoch-1] = info\n    \n    print((\"\\nEPOCH = %d, loss = %.3f,\"+ metric_name + \\\n          \"  = %.3f, val_loss = %.3f, \"+\"val_\"+ metric_name+\" = %.3f\") \n          %info)\n    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n        \nprint('Finished Training...')","0bf3cc13":"dfhistory","0d2947fc":"def plot_metric(dfhistory, metric):\n    fig, axe = plt.subplots(dpi=150)\n    \n    train_metrics = dfhistory[metric]\n    val_metrics = dfhistory['val_'+metric]\n    \n    epochs = range(1, len(train_metrics) + 1)\n    \n    axe.plot(epochs, train_metrics, 'bo--')\n    axe.plot(epochs, val_metrics, 'ro-')\n    \n    plt.title('Training and validation '+ metric)\n    axe.set_xlabel(\"Epochs\")\n    axe.set_ylabel(metric)\n    axe.legend([\"train_\"+metric, 'val_'+metric])\n    plt.show()","af8dfa51":"plot_metric(dfhistory, 'loss')","5ea548e3":"plot_metric(dfhistory, 'accuracy')","eabf4a08":"#predict probability\ny_pred_probs = net(torch.tensor(x_test[0:10]).float()).data\ny_pred_probs","abb79b58":"y_pred_probs = net(torch.tensor(x_test).float()).data","d42ab1d6":"# predict 0 or 1\n\ny_pred = torch.where(y_pred_probs>0.5, torch.ones_like(y_pred_probs), torch.zeros_like(y_pred_probs))\n\ny_pred","97bb1a62":"submission_file = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","060a86b8":"submission_file.head()","7890e39a":"submission_file['Survived'] = y_pred.numpy()","0f009a3d":"submission_file['Survived'] = submission_file['Survived'].astype('int')","806633d8":"submission_file.head()","d74aed2f":"submission_file.to_csv(\"submission.csv\", index=False)","58bb1110":"print (net.state_dict().keys())","58b123f7":"torch.save(net.state_dict(), 'net_parameter.pkl')","0f063225":"net_clone = create_net()\nnet_clone.load_state_dict(torch.load(\"net_parameter.pkl\"))\n\nnet_clone.forward(torch.tensor(x_test[:10]).float()).data","ece146f6":"torch.save(net, 'net_model.pkl')\nnet_loaded = torch.load('net_model.pkl')\nnet_loaded(torch.tensor(x_test[0:10]).float()).data","eab45a48":"## Define our model","24de164e":"### Age","885faab3":"## Data Preprocessing\n\n* get_dummies: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html\n* is_na: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.isna.html","dd6a7b8e":"### only save parameters (recommend)","b4906b4f":"## Age and Label\n\n* density plot: https:\/\/towardsdatascience.com\/histograms-and-density-plots-in-python-f6bda88f5ac0","b0b27d21":"## EDA","a1ff67a9":"### DataLoader\n\n* https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\n* https:\/\/discuss.pytorch.org\/t\/make-a-tensordataset-and-dataloader-with-multiple-inputs-parameters\/26605\/2\n* https:\/\/towardsdatascience.com\/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#58f2\n\nif a dataset is nothing else but a couple of tensors, we can use PyTorch\u2019s TensorDataset class, which will do pretty much what we did in our custom dataset above.","4311ce68":"## Saving Model","8ec414ab":"## Use model to predict","58ba576f":"#### load saving model","a42249fe":"### Saving whole model","77d8de2f":"## training","447bfbdd":"### Train Test Split\n\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html","4b9ba550":"## check null value","7f984164":"#### check Survived distributions","6507800a":"For straightforward models, that use run-of-the-mill layers, where the output of a layer is sequentially fed as an input to the next, we can use a, er\u2026 Sequential model :-)\nIn our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression."}}