{"cell_type":{"ad33a75c":"code","0cc8b25e":"code","17ce7831":"code","e63c3eda":"code","ad0534f7":"code","bab539fb":"code","fb2cddc8":"code","bbaa186c":"code","105f40f1":"code","3f3adb7c":"code","aa62490b":"code","0b70dcdb":"code","638e4579":"code","7182784c":"markdown","dbb17d7e":"markdown","5efdf23d":"markdown","bfa33883":"markdown","53e88af6":"markdown","cd1b6881":"markdown","d3fc3dc9":"markdown","f443e1aa":"markdown","2f97d397":"markdown","e0acde3e":"markdown","889fa6b0":"markdown","83e578b8":"markdown","0de0592f":"markdown"},"source":{"ad33a75c":"import os \nos.system('pip install -U -r requirements.txt')\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm","0cc8b25e":"np.random.seed(42)\n\nx_lo = np.random.normal(15,3, (200,1))\ny_lo = (x_lo**2)\/2+1*x_lo + 1\nx_hi = np.random.normal(25,3, (200,1))\ny_hi = (x_hi**2)\/2+x_hi*1.5+2\nx    = np.concatenate((x_lo, x_hi), axis=0)\ny    = np.concatenate((y_lo, y_hi), axis=0)\nyn   = np.random.normal(3,5, y.shape) + y\n\nxs, i = np.unique(x, return_index=True, axis=0) # axis preserves the shape of returned array\nys    = yn[i]\n\nx_partition = np.where(xs<20)[0][-1]\n\nx_lo = xs[:x_partition]\ny_lo = ys[:x_partition]\nx_hi = xs[x_partition:]\ny_hi = ys[x_partition:]\n","17ce7831":"# plot to see linearity\n\nplt.plot(x_hi, y_hi, '.', label='Data')\nplt.xlabel('Features')\nplt.ylabel('Output')\nplt.legend()\nplt.grid()\n","e63c3eda":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nX       = x_hi.reshape(-1,1)\ny       = y_hi\nlin_reg.fit(X, y)\n\ntheta    = [lin_reg.coef_[0], lin_reg.intercept_]\nprint('Theta: ', theta)\n\nx_new         = np.arange(20, 36)\nx_to_predict  = x_new.reshape(-1, 1)\ny_predicted   = lin_reg.predict(x_to_predict)\n\nplt.plot(x_new, y_predicted,'--r', label='Linear fit Scikit Learn')\nplt.plot(x_hi, y_hi, '.', label='Data')\nplt.legend()\nplt.grid()","ad0534f7":"X        = np.c_[x_hi, np.ones((len(x_hi),1))]\ny        = y_hi\n\ntheta    = np.linalg.pinv(X)@y\n\nx_new         = np.arange(20, 36)\nx_to_predict  = np.c_[x_new, np.ones((len(x_new),1))]\ny_predicted   = x_to_predict@theta\n\nplt.plot(x_new, y_predicted,'--r', label='Linear fit Matrix Inversion')\nplt.plot(x_hi, y_hi, '.', label='Data')\nplt.grid()\nplt.legend()\nplt.show()\n","bab539fb":"plt.plot(xs, ys, '.',label='Data')\nplt.legend()\nplt.grid()\n","fb2cddc8":"x_transformed = np.log(xs);\ny_transformed = np.log(ys);\n\n# Plot transformed (new) data\nfig = plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(x_transformed, y_transformed, '.', label='Transformed Data')\nplt.legend()\nplt.grid()\n\nX         = np.c_[x_transformed, np.ones((len(x_transformed),1))]\ntheta     = np.linalg.pinv(X)@y_transformed\n\nx_to_predict               = X\ny_predicted_transformed    = x_to_predict@theta\n\nplt.subplot(1,2,1)\nplt.plot(x_transformed, y_predicted_transformed, '--r',label='Linear fit on transformed data')\nplt.legend()\n\n# Consequently, \n\ny_predicted_original  = np.exp(theta[1])*xs**theta[0]\nplt.subplot(1,2,2)\nplt.plot(xs, ys, '.', label='original data')\nplt.plot(xs, y_predicted_original, '--r', label='Linear Parameter model fit')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n# Or using sklearn\nX        = x_transformed.reshape(-1,1)\ny        = y_transformed\nlin_reg  = LinearRegression()\nlin_reg.fit(X,y)\n\ny_predicted = lin_reg.predict(X)\n\nplt.plot(X, y, '.', label='original data')\nplt.plot(X, y_predicted, '--r', label='Scikit learn fit')\nplt.legend()\nplt.grid()\nplt.show()","bbaa186c":"plt.plot(xs, ys, '.', label='Data')\n\nX           = xs.reshape(-1,1)\ny           = ys\nlin_reg     = LinearRegression()\n\nlin_reg.fit(X,y)\ny_predicted = lin_reg.predict(X)\n\nmodel_error   = ys - y_predicted\nde_mean_ys    = ys - np.mean(ys)\nr_squared     = np.round(1 - (model_error.T@model_error)\/(de_mean_ys.T@de_mean_ys),2)\n\nplt.plot(xs, y_predicted, '--r',label=('Linear fit, R^2: ' + str(r_squared)))\nplt.grid()\nplt.legend()\nplt.show()","105f40f1":"plt.hist(xs,20, ec='white')\nplt.plot([20, 20], [0, 50], '--k')\nplt.grid()\nplt.show()\n\n# Knowing xs is distributed in a certain way we can fit different models to parts of xs. Here we divide xs < 20 and xs > 20 and corresponding ys\n\nplt.plot(x_lo, y_lo, '.', label='Lo Data') \nplt.plot(x_hi, y_hi, '.', label='Hi Data')\n\nX_lo        = x_lo.reshape(-1,1)\nlin_reg_lo  = LinearRegression()\nlin_reg_lo.fit(X_lo, y_lo)\n\nX_hi        = x_hi.reshape(-1,1)\nlin_reg_hi  = LinearRegression()\nlin_reg_hi.fit(X_hi, y_hi)\n\nmodel_error_lo   = y_lo - lin_reg_lo.predict(X_lo)\nde_mean_y_lo     = y_lo - np.mean(y_lo);\nr_squared_lo     = 1 - (model_error_lo.T@model_error_lo)\/(de_mean_y_lo.T@de_mean_y_lo)\n\nmodel_error_hi   = y_hi - lin_reg_hi.predict(X_hi)\nde_mean_y_hi     = y_hi - np.mean(y_hi);\nr_squared_hi     = 1 - (model_error_hi.T@model_error_hi)\/(de_mean_y_hi.T@de_mean_y_hi)\n\nx_new            = np.arange(min(x_lo), max(x_lo))\nx_to_predict     = x_new.reshape(-1,1)\ny_predicted      = lin_reg_lo.predict(x_to_predict)\nplt.plot(x_to_predict, y_predicted, '--r', label='Linear fit lo Data')\n\nx_new            = np.arange(min(x_hi), max(x_hi))\nx_to_predict     = x_new.reshape(-1,1)\ny_predicted      = lin_reg_hi.predict(x_to_predict)\nplt.plot(x_to_predict, y_predicted, '--b', label='Linear fit hi Data')\nplt.legend()\nplt.grid()\nplt.show()\n","3f3adb7c":"m = len(x_transformed); # number of training examples\n\nX = np.c_[x_transformed, np.ones(x_transformed.shape)]\ny = y_transformed","aa62490b":"def compute_cost(X, y, theta):\n    y    = y.ravel()\n    m    = len(y)\n    j    = 0\n    hx   = X@theta\n    j    = 1\/(2*m)*((hx-y).T@(hx-y))\n    return j\n\n\ndef gradient_descent(X, y, theta, alpha, num_iterations):\n    y = y.ravel()\n    m = len(y) # number of training examples\n    j_history = np.zeros((num_iterations, 1))\n\n    for iter in range(num_iterations):\n        j_history[iter] = compute_cost(X, y, theta) # Save the cost J in every iteration\n        hx              = X@theta\n        theta           = theta - alpha*1\/m*(hx-y)@X\n    return theta, j_history\n\n\ndef visualise_cost_function(X, y, theta=[]):\n    theta0_vals = np.linspace(-10,10, 100)\n    theta1_vals = np.linspace(-10,10,100)\n    \n    # Initialise j_vals to a matrix of 0's\n    j_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n    \n    # fill out j_vals\n    for i, theta0 in enumerate(theta0_vals):\n        for j, theta1 in enumerate(theta1_vals):\n            t = np.array([theta0, theta1])\n            j_vals[i,j] = compute_cost(X,y,t)\n            \n            \n    j_vals = np.transpose(j_vals)\n    \n    # Surface plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.plot_surface(theta0_vals, theta1_vals, j_vals, cmap=cm.jet)\n    plt.xlabel('theta0')\n    plt.ylabel('theta1')\n    plt.show()\n    \n    if np.any(theta):\n        # Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n        plt.contour(theta0_vals, theta1_vals, j_vals, levels=np.logspace(-2, 3, 20))\n        plt.xlabel('theta0')\n        plt.ylabel('theta1')\n        plt.plot(theta[0], theta[1], 'rx')\n    \n","0b70dcdb":"# Compute and display initial cost with theta all zeros. Answer should be almost 14\n\ncompute_cost(X, y, [0, 0])\n\nvisualise_cost_function(X, y, theta)","638e4579":"# Run gradient descent:\ntheta = np.array([0, 0]) # initialize fitting parameters\niterations = 1500\nalpha = 0.005 # Learning rate (not too small not too big)\n\n# Compute theta\ntheta, j_theta = gradient_descent(X, y, theta, alpha, iterations)\n\nplt.plot(np.arange(iterations), j_theta, '.')\nplt.title('Convergence check')\nplt.xlabel('Number of iterations')\nplt.ylabel('Cost function value')\nplt.grid()\nplt.show()\n\nx_new        =  x_transformed\nx_to_predict =  np.c_[x_new, np.ones(x_new.shape)]\ny_predicted  =  x_to_predict@theta\nplt.plot(x_transformed, y_transformed, '.', label='data')\nplt.plot(x_new, y_predicted,'--k', label='Linear fit Gradient Descent')\nplt.legend()\nplt.grid()\nplt.show()","7182784c":"#### Matrix inversion (The logic behind)  \nLet's assume, <br\/>\n\n$ y = [x, 1]*[\\theta_{0}, \\theta_{1}]'$\n\nor \n\n$ Y = X*\\Theta \\hspace{0.5cm} remember \\hspace{0.5cm} X = [x, 1] ; \\hspace{0.5cm} \\Theta = [\\theta_{0}, \\theta_{1}]'$\n\nbut, X is not a square matrix. In such situation mathematically we can only find pseudo-inverse.\n\n$ pseudo inverse = inv(X*X'), \\hspace{0.5cm} X*X' $ is a square symmetric matrix  \nor, $ pseudo inverse = pinv(X) $\n\nso   $ \\hspace{0.5 cm} \\Theta = pinv(X)*Y $","dbb17d7e":"#### Gradient Descent\nKeep in mind that the cost $J(\\theta)$ is parameterized by the vector $\\theta$, not $X$ and $y$. That is, we minimize the value of $J(\\theta)$ by changing the values of the vector $\\theta$, not by changing $X$ or $y$.\n\nA good way to verify that gradient descent is working correctly is to look at the value of $J$ and check that it is decreasing with each step.  \nThe function **gradient_descent** calls **compute_cost** on every iteration and prints the cost. If you have implemented **gradient_descent** and **compute_cost** correctly, your value of  should never increase, and should converge to a steady value by the end of the algorithm.","5efdf23d":"#### Computing the cost $J(\\theta) $\n It is helpful to monitor the convergence at each iteration by computing the cost, which is mean squared error of predicted outputs. \n\nAlso, we can check the convexity of cost function.","bfa33883":"## Topics for future ?\n#### Linear Regression (Supervised):\n* Influence of learning rate and using different rates in gradient descent\n* Feature Normalization\n* Regularisation\n* Model Performance metrics\n* Stochastic Gradient descent\n* Using a cost function whose derivative function is not known  \n\n#### Logistic Regression (Supervised)\n#### Decision trees\n#### Principal component analysis (Unsupervised)\n","53e88af6":"### Linear Parameter Model\nSuch models are also called linear parameter models. The key is model is linear in its parameters not necessarily in it's input vs output relationship.","cd1b6881":"## Linear regression","d3fc3dc9":"# Machine learning\n\n## Types \n#### Supervised: \nGiven a set of data the task is to learn the relationship between the input x and output y such that, when given a novel input x* the predicted y* is accurate. Accuracy is measure by some Loss function e.g. MSE (mean squared errors)\nRegression (linear\/logistic), SVM, K-nearest neighbours etc. \n\n#### Unsupervised:\nGiven a set of data we aim to find a plausible compact description of data. An objective is used to quantify tge accuracy of the description e.g. retained variance.\nIn essence we are interested in modelling the distribution of x.\nPrincipal component analysis, K means clustering etc.\n\n#### Anomaly Detection\n#### Online (sequential) learning\n#### Semi supervised learning\n#### Self supervised learning (turing learning)","f443e1aa":"#### Measure of Accuracy  \n\n$ R^2: $ Goodness of fit \n\nWhat % of variance in data is explained by the model  \n\ni.e. 100% - (Residual variance in the data)\/(Variance of data)  \n\nor $$ R^2 = \\frac{1 - [(y_1^* - y_1)^2 + (y_2^*-y_2)^2 + ...]}{var(y)} $$\n\nThis means if $R^2$ is 0 then y cannot be explained by knowing x.","2f97d397":"## Generate data","e0acde3e":"## Bayesian Reasoning\nThe above approach to machine learning is empirical.\nBayesian reasoning dictates not to discard the prior knowledge available rather start from it. (especially useful for higher dimensional data and allows to make use of prior subject matter knowledge)\n\nThere are 2 main approaches using which parameters can be fit using Bayesian reasoning:  \n* **Discriminative**: In essence we identify the boundary that separates classes or find the linear regression function.  \n* **Generative**: In essence we start with how x itself is distributed for different classes in y  and build from there.","889fa6b0":"## Gradient Descent\nThe objective of linear regression is to minimize the cost function which is MSE (mean squared error). \n$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$\n\nThis is a convex function in $ \\theta $, which is very good to achieve global minima.\n*m* is number of examples (data points) and the hypothesis $ h_{\\theta}(x) $ is given by the linear model\n\n$$ h_{\\theta}(x) = \\theta^{T}x = \\theta_{0} + \\theta_{1}x_{1}  $$\n\nThe parameters of your model are the $\\theta$ values. These are the values you need to adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update  \n\n$$ \\theta_{j} := \\theta_{j} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}  $$  (simultaneously update $\\theta_{j}$ for all *j* (features))\n\nWith each step of gradient descent, your parameters $\\theta_{j}$ come closer to the optimal values (by step size $\\alpha$) that will achieve the lowest cost $J(\\theta)$.\n\n**Also very important that x's and even y's are scaled to be between 0 and 1 for proper convergence**","83e578b8":"#### fit a polynomial  $ y = \\theta_{0}x + \\theta_{1} $\n","0de0592f":"Let's assume\n\n$ y = \\alpha_{0}*(x^{\\alpha_{1}}) $\n\n$ log(y) = \\alpha_{1} * log(x) + log(\\alpha_{0}) $\n\nor   $ y_{transformed}  = \\theta_{0} * x_{transformed} + \\theta_{1} \\hspace{0.5cm}, where \\hspace{0.5cm} \\theta_{0} = \\alpha_{1},\\hspace{0.5cm} \\theta_{1} = log(\\alpha_{0}) $\n\nalternately $ Y = X* \\Theta  \\hspace{0.5cm}, where \\hspace{0.5cm} Y = log(y),\\hspace{0.5cm} X = [log(x), 1] $ \n\nand we know that $ \\Theta = pinv(X)*Y $ , \nNote numerical inversion methods ensure optimum fit in a way"}}