{"cell_type":{"efba6ac5":"code","815ddcd3":"code","1116f489":"code","cea27bdd":"code","3e010ea2":"code","9d363b43":"code","7a2aad15":"code","b8e7ac24":"code","9b47b313":"markdown"},"source":{"efba6ac5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","815ddcd3":"!pip install bs4","1116f489":"from glob import glob\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch\nfrom bs4 import BeautifulSoup\nimport cv2\n\n\nimgs_dir = list(sorted(glob('..\/input\/face-mask-detection\/images\/*.png')))\nlabels_dir = list(sorted(glob(\"..\/input\/face-mask-detection\/annotations\/*.xml\")))\n\nclass dataset(Dataset) :\n    def __init__(self, imgs, labels) :\n        self.imgs = imgs\n        self.labels = labels\n        self.transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n        self.device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n        \n    def __len__(self) :\n        return len(self.imgs)\n    \n    def __getitem__(self, index) :\n        x = cv2.imread(self.imgs[index])\n        x = self.transform(x).to(self.device)\n        \n        y = dict()\n        with open(self.labels[index]) as f :\n            data = f.read()\n            soup = BeautifulSoup(data, 'xml')\n            data = soup.find_all('object')\n            \n            box = []\n            label = []\n            for obj in data :\n                xmin = int(obj.find('xmin').text)\n                ymin = int(obj.find('ymin').text)\n                xmax = int(obj.find('xmax').text)\n                ymax = int(obj.find('ymax').text)\n                \n                label_ = 0\n                if obj.find('name').text == 'with_mask' :\n                    label_ = 1\n                elif obj.find('name').text == 'mask_weared_incorrect' :\n                    label_ = 2\n                \n                box.append([xmin, ymin, xmax, ymax])\n                label.append(label_)\n                \n            box = torch.FloatTensor(box)\n            label = torch.IntTensor(label)\n            \n            y['image_id'] = torch.FloatTensor([index]).to(device)\n            y[\"boxes\"] = box.to(device)\n            y[\"labels\"] = torch.as_tensor(label, dtype=torch.int64)\n            \n        return x, y\n    \ndef collate_fn(batch) : return tuple(zip(*batch))\ntrain_data = dataset(imgs_dir, labels_dir)\ntrain_data = DataLoader(train_data, batch_size = 1, shuffle = True,\n                       collate_fn = collate_fn)","cea27bdd":"#\ud559\uc2b5\ub41c \ubaa8\ub378\ub85c \uc0ac\uc6a9 : \ud559\uc2b5 \uc2dc\uac04 \uc544\ub07c\uc790.\n#load pre-trained model\nimport torch.nn as nn\nimport torch.functional as F\nimport torchvision.models as models\n\ndef get_model(output_shape) :\n    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, output_shape)\n    \n    return model\n\ndevice = torch.device('cuda' if torch.cuda.is_available else 'cpu')\nmodel = get_model(3).to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr = 0.001, momentum = 0.9, weight_decay = 0.0001)","3e010ea2":"#\ud559\uc2b5\n#train\nfrom tqdm.notebook import tqdm\n\nmodel.train()\nnum_epoch = 50\n\nfor epoch in range(num_epoch) :\n    epoch_loss = 0\n    \n    for imgs, annotations in tqdm(train_data):\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        \n        predict = model(imgs, annotations)\n        losses = sum(loss for loss in predict.values())        \n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step() \n        epoch_loss += losses\n        \n    print(epoch+1, '\/', num_epoch, ' : {:.5f}'.format(epoch_loss))\n    if epoch_loss < 0.1 :\n        print('early stop')\n        break","9d363b43":"#out of memory \ub54c\ubb38\uc5d0 \ud559\uc2b5\ud55c \uac70 \uc783\uc744\uae4c\ubd10 \uc77c\ub2e8 \uc800\uc7a5\n#because of out of memory, save the model first\ntorch.save(model.state_dict(), 'model.pt')","7a2aad15":"#load model\nmodel = get_model(3).to(device)\nmodel.load_state_dict(torch.load('model.pt'))","b8e7ac24":"#\uacb0\uacfc \ud655\uc778\n#get result\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#\ubc30\uce58 \uc0ac\uc774\uc988 \uc870\uc815\ud574\uc11c \uc7ac\uc2e4\ud589 => out of memory \ubc29\uc9c0\n#Resize the batch-size to prevent out of memory\nvalid_data = dataset(imgs_dir, labels_dir)\nvalid_data = DataLoader(valid_data, batch_size = 4, shuffle = True,\n                       collate_fn = collate_fn)\n\ndef plot_img(img, predict, annotation) :\n    fig, ax = plt.subplots(1, 2)\n    img = img.cpu().data\n    \n    ax[0].imshow(img.permute(1, 2, 0)) #rgb, w, h => w, h, rgb\n    ax[1].imshow(img.permute(1, 2, 0))\n    ax[0].set_title(\"real\")\n    ax[1].set_title(\"predict\")\n    \n    for box in annotation[\"boxes\"] :\n        xmin, ymin, xmax, ymax = box\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n        ax[0].add_patch(rect)\n        \n    for box in predict[\"boxes\"] :\n        xmin, ymin, xmax, ymax = box\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n        ax[1].add_patch(rect)\n    plt.show()\n\nmodel.eval()\n\nwith torch.no_grad() :\n    for imgs, annotations in valid_data:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        \n        preds = model(imgs)\n        \n        for i in range(3) :\n            plot_img(imgs[i], preds[i], annotations[i])\n            #plot_img(imgs[i], annotations[i])\n        \n        break","9b47b313":"#### This code was created referring to the code below.\n#### https:\/\/www.kaggle.com\/daniel601\/pytorch-fasterrcnn"}}