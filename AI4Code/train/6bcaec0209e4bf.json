{"cell_type":{"d185e40d":"code","ad2850e6":"code","c76f1a11":"code","c643d416":"code","b68eb1db":"code","2fda555b":"code","f953f1b4":"code","a4fcaa18":"code","2929f245":"code","b5f4196a":"code","48f4150a":"code","74cc6f45":"code","7c4cb999":"markdown","6ccc7fb5":"markdown"},"source":{"d185e40d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad2850e6":"import pandas as pd\ntrain_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ntest_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')","c76f1a11":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor","c643d416":"X = train_data.drop(['claim'], axis=1)\ny = train_data.claim","b68eb1db":"to_beDropped=['f43', 'f49', 'f66', 'f72','f74', 'f88', 'f94','f101','f115']\nX = X.drop(to_beDropped, axis=1)","2fda555b":"X.head()","f953f1b4":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)","a4fcaa18":"imputer=SimpleImputer(strategy='median')\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', imputer, list(set(X_train.columns)-set(to_beDropped)))\n    ])","2929f245":"model= XGBRegressor(\n    n_estimators=10000,\n    learning_rate=0.03,\n    subsample=0.8,\n    colsample_bytree=0.1,\n    max_depth=3,\n    booster='gbtree',\n    reg_lambda=0.0008,\n    reg_alpha=24,\n    random_state=42\n)","b5f4196a":"my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Transform the data\nmy_pipeline.fit(X_train, y_train) \npreds = model.predict(X_valid)","48f4150a":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\npreds = model.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\nprint(mean_squared_error(y_valid, preds, squared=False))","74cc6f45":"predictions = my_pipeline.predict(test_data)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test_data.index,\n                       'claim': predictions})\noutput.to_csv('submission.csv', index=False)","7c4cb999":"**Based on the notebook 'TabularPlaygroundSeriesDataAnalysis', we are going to drop for the moment some comlumns**","6ccc7fb5":"**impute missing values**"}}