{"cell_type":{"45245608":"code","502a3d91":"code","d9ac5144":"code","a7b1bb23":"code","771c9bb7":"code","6fd2058f":"code","c324d3b5":"code","65f7c7c0":"markdown","5e459f15":"markdown"},"source":{"45245608":"!pip install kaggle-environments","502a3d91":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom kaggle_environments import evaluate, make, utils, agent\n\nDISCOUNT = 0.99\n\nDRAW = 0.3\nWIN = 1\nLOSE = -1\nERROR = -5\n\nEPSILON = 1  # not a constant, going to be decayed\nEPSILON_DECAY = 0.9993\nMIN_EPSILON = 0.001\n\n\nclass ConnectX:\n    def __init__(self, pair=[None, \"random\"], config={\"rows\": 6, \"columns\": 7, \"inarow\": 4}):\n        self.env = make(\"connectx\", config, debug=True)\n        self.config = config\n        self.set_pair(pair)\n        \n    def set_pair(self, pair):\n        self.pair = pair\n    \n    def switch_pair(self):\n        self.pair = self.pair[::-1]\n    \n    def change_reward(self, reward, done):\n        if done:\n            if reward is None: #Error - column has already filled\n                reward = ERROR\n            elif reward == 1:\n                reward = WIN\n            elif reward == -1:\n                reward = LOSE\n            elif reward == 0:\n                reward = DRAW\n        else:\n            reward = -1\/(self.config['rows'] * self.config['columns'])\n        return reward\n    \n    def play_game(self, model, render=False):\n        global EPSILON\n        trainer = self.env.train(self.pair)\n        observation = trainer.reset()\n        steps = []\n\n        while not self.env.done:\n            current_state = get_board(observation, self.config) \n            current_qs = model.predict(current_state[None,:])[0]\n            if np.random.random() > EPSILON:\n                action = int(np.argmax(current_qs))\n            else:\n                action = np.random.randint(0, self.config[\"columns\"] - 1)    \n            observation, reward, done, info = trainer.step(action)\n            reward = self.change_reward(reward, done)\n            \n            steps.append((current_state, current_qs, action, reward, done))\n            \n        if render:\n            self.env.render(mode=\"ipython\", width=500, height=500)\n            \n        # Decay epsilon\n        if EPSILON > MIN_EPSILON:\n            EPSILON *= EPSILON_DECAY\n            EPSILON = max(MIN_EPSILON, EPSILON)\n            \n        return (reward, steps)","d9ac5144":"class DQN:\n    def __init__(self):\n        initializer = tf.initializers.VarianceScaling(scale=2.0)\n        layers = [\n            \n# Model for Connect 2 \n            \n#             Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=initializer, \n#                    kernel_regularizer='l2', input_shape=(3,3,1)),\n#             BatchNormalization(),\n#             Flatten(),\n#             Dense(100, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n#             BatchNormalization(),\n#             Dropout(0.1),\n#             Dense(3)\n    \n            \n# Model for Connect 3\n            \n            Conv2D(64, 3, activation='relu', kernel_initializer=initializer, \n                   kernel_regularizer='l2', input_shape=(4,5,1)),\n            BatchNormalization(),\n            Flatten(),\n            Dense(100, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(20, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(5)\n\n            \n# Model for Connect 4 \n\n#             Conv2D(64, 3, activation='relu', kernel_initializer=initializer, input_shape=(6,7,1)),\n#             Conv2D(64, 3, activation='relu', kernel_initializer=initializer),\n#             Flatten(),\n#             Dense(50, activation='relu', kernel_initializer=initializer),\n#             Dropout(0.1),\n#             Dense(7)\n        ]\n        model = tf.keras.Sequential(layers)\n        # clipvalue and clipnorm needs to prevent gradient explosion\n        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001, clipvalue = 1., clipnorm = 1.), metrics=['accuracy'])\n        \n        self.model = model\n        \n    def train(self, steps, epochs = 10):\n        X = []\n        y = []\n        \n        er_before = 0\n        for index in range(len(steps)-1, -1 , -1):\n            current_state, current_qs, action, reward, done = steps[index]\n\n            # If not a terminal state, get new q from future states, otherwise set it to 0\n            # almost like with Q Learning, but we use just part of equation here\n            if not done:\n                max_future_q = np.max(steps[index + 1][1])\n                new_q = reward + DISCOUNT * max_future_q\n            else:\n                new_q = reward\n\n            # Update Q value for given state\n            current_qs[action] = new_q\n            current_qs[current_qs!=action] -= reward\n\n            # And append to our training data\n            X.append(current_state)\n            y.append(current_qs)\n            \n            # Add symetrical state\n            X.append(current_state[:, ::-1, :])\n            y.append(current_qs[::-1])\n            \n            \n        self.model.fit(np.array(X), np.array(y), batch_size=64, epochs = epochs)\n    \n    def predict(self, X):\n        return self.model.predict(X)\n    \n\nclass Trainer:\n    def __init__(self, game, model):\n        self.game = game\n        self.model = model\n    \n    def _print_statistics(self, rewards):\n        rewards = np.array(rewards)\n        print(\"Wins:\", (rewards == WIN).sum())\n        print(\"Loses:\", (rewards == LOSE).sum())\n        print(\"Errors:\", (rewards == ERROR).sum())\n\n    def train(self, num_of_games = 1000, every = 100, epochs = 50, switch=True):\n        games = []\n        rewards = []\n        \n        for i in range(1, num_of_games + 1):\n            reward, steps = self.game.play_game(self.model)\n            games = games + steps\n            rewards.append(reward)\n            \n            if i % every == 0:\n                if switch:\n                    self.game.switch_pair()\n                self._print_statistics(rewards)\n                self.model.train(games, epochs)\n                games = []\n                rewards = []\n        \n        if len(games) > 200:\n            self._print_statistics(rewards)\n            self.model.train(games, epochs)\n            \n            \ndef get_board(observation, configuration):\n    rows = configuration['rows']\n    columns = configuration['columns']\n\n    board = np.array(observation['board']).reshape((rows,columns,1))\n    new_board = np.zeros_like(board)\n\n    mark = observation[\"mark\"]\n    new_board[board == mark] = 1\n    new_board[(board != mark) & (board != 0)] = 2\n    return new_board \/ 2\n\n\ndef agent(observation, configuration):\n    current_state = get_board(observation, configuration)\n    current_qs = model.predict(current_state[None,:])[0]\n    return int(np.argmax(current_qs))","a7b1bb23":"model = DQN()\ngame = ConnectX(config={\"rows\": 4, \"columns\": 5, \"inarow\": 3})\ntrainer = Trainer(game, model)","771c9bb7":"# test\ngame.set_pair([None, agent])\nt = game.play_game(model, render=True)","6fd2058f":"EPSILON = 1.0\ngame.set_pair(['random', None])\ntrainer.train(num_of_games=10000, every=100, epochs=15, switch=True)","c324d3b5":"game.set_pair([None, 'negamax'])\ntrainer.train(num_of_games=10000, every=100, epochs=15, switch=True)","65f7c7c0":"# Conclusion  \nAs you can see, dueling double DQN doesn't solve Connect 3 game. And I don't have idea why. I also tried dueling double DQN and PPO algorithm with different model architectures, but get the same result.  \n\nLeave comment and write, where I can have a mistake. \n\nMy other notebooks:  \n1) [Usual DQN](https:\/\/www.kaggle.com\/masurte\/deep-q-learning)   \n2) [Dueling double DQN](https:\/\/www.kaggle.com\/masurte\/dueling-double-dqn)    \n3) [PPO](https:\/\/www.kaggle.com\/masurte\/ppo-algorithm)  ","5e459f15":"# Deep Q-Network"}}