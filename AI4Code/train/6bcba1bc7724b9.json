{"cell_type":{"c3d9c10c":"code","180f07b3":"code","b3a86c14":"code","587981c4":"code","1afa0c97":"code","d9d5ece0":"code","02b539fb":"code","4d88844d":"code","287921fb":"code","ed0ce074":"code","7cd0be7e":"code","53600e0e":"code","3313b207":"markdown","243a824e":"markdown"},"source":{"c3d9c10c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","180f07b3":"data = pd.read_csv(\"..\/input\/body-performance-data\/bodyPerformance.csv\")\ndata","b3a86c14":"data.gender = [1 if each == 'M' else 0 for each in data.gender]\ny = data.gender.values\nx = data.drop([\"gender\",\"class\"],axis=1)\ny","587981c4":"# normalization\nx = (x - np.min(x))\/(np.max(x) - np.min(x)).values\nx","1afa0c97":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \", x_train.shape)\nprint(\"x_test: \", x_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)","d9d5ece0":"def init_wb(dimension):\n    w = np.full((dimension, 1), 0.01)\n    b = 0.0\n    return w, b","02b539fb":"def sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))\n    return y_head","4d88844d":"def forward_backward(w, b, x_train, y_train):\n    # forward propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    \n    # backward propagation\n    weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    grad = {\"weight\": weight, \"bias\": bias}\n    \n    return cost, grad","287921fb":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate, iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(iterarion):\n        cost, grad = forward_backward(w, b, x_train, y_train)\n        # lets update\n        w = w - learning_rate * grad[\"weight\"]\n        b = b - learning_rate * grad[\"bias\"]\n        if i % 100 == 0:\n            cost_list.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index, cost_list)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, grad, cost_list","ed0ce074":"# prediction\ndef predict(w, b, x_test):\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    y_predict = np.zeros((1, x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0\n        else:\n            y_predict[0,i] = 1\n            \n    return y_predict","7cd0be7e":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, iteration):\n    dimension = x_train.shape[0]\n    w, b = init_wb(dimension)\n    parameters, grad, cost_list = update(w, b, x_train, y_train, learning_rate, iteration)\n    \n    prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    \n    print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(prediction_test - y_test)) * 100))","53600e0e":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.1, iteration = 3000) ","3313b207":"Now, we will define the functions. First one is initializing weights and bias.","243a824e":"As we can see, there are 'gender' and 'class' columns. In this dataset we won't use 'class' column. Besides, we will use 'gender' column as as label so that I split it from features."}}