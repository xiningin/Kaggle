{"cell_type":{"16f0ecc0":"code","37b5b5d4":"code","e14c61f4":"code","d651142d":"code","81da2042":"code","e1c36d16":"code","760ad34d":"code","abad5ba3":"code","fac06220":"code","96ec2d91":"code","0cb78534":"code","0ca98d49":"code","bfc9b0dc":"code","a1da16dd":"code","5e868b54":"code","7256da0c":"code","5db27a42":"markdown","c649358b":"markdown","4dc5888d":"markdown","a1c985c6":"markdown","b5ab0a78":"markdown","f980b886":"markdown"},"source":{"16f0ecc0":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict\nimport string\nimport tensorflow as tf\nimport re\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel","37b5b5d4":"train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv',index_col='id')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv',index_col='id')\ny=train['target']\ntrain.drop(['location','keyword'],inplace=True,axis=1)\ntest.drop(['location','keyword'],inplace=True,axis=1)","e14c61f4":"#Use regex to clean the data\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text \n\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ntrain['text']=train['text'].apply(lambda x : remove_url(x))\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\ntrain['text']=train['text'].apply(lambda x : remove_emoji(x))\ntrain['text']=train['text'].apply(lambda x : decontraction(x))\ntrain['text']=train['text'].apply(lambda x : seperate_alphanumeric(x))\ntrain['text']=train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n\ntest['text']=test['text'].apply(lambda x : remove_url(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))\ntest['text']=test['text'].apply(lambda x : remove_emoji(x))\ntest['text']=test['text'].apply(lambda x : decontraction(x))\ntest['text']=test['text'].apply(lambda x : seperate_alphanumeric(x))\ntest['text']=test['text'].apply(lambda x : unique_char(cont_rep_char,x))","d651142d":"onehot_encoder = OneHotEncoder(sparse=False)\ny = (np.asarray(y)).reshape(-1,1)\nY = onehot_encoder.fit_transform(y)\n\nX_train, X_val, y_train, y_val = train_test_split(train.text,Y, random_state=10, test_size=0.2, shuffle=True)","81da2042":"print(X_train.shape,X_val.shape,y_train.shape,y_val.shape)","e1c36d16":"model_checkpoint = \"distilbert-base-uncased\"\nbatch_size = 16","760ad34d":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","abad5ba3":"tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")","fac06220":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","96ec2d91":"X_train_t = regular_encode(list(X_train), tokenizer, maxlen=512)\nX_val_t = regular_encode(list(X_val), tokenizer, maxlen=512)","0cb78534":"AUTO = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_t, y_train))\n    .repeat()\n    .shuffle(1995)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val_t, y_val))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","0ca98d49":"def build_model(transformer, max_len=160):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(2, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","bfc9b0dc":"transformer_layer = TFAutoModel.from_pretrained(model_checkpoint)\nmodel_base = build_model(transformer_layer, max_len=512)\nmodel_base.summary()","a1da16dd":"n_steps = X_train.shape[0] \/\/ batch_size\nmodel_base.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=3)","5e868b54":"X_test = regular_encode(list(test.text), tokenizer, maxlen=512)\ntest1 = (tf.data.Dataset.from_tensor_slices(X_test).batch(batch_size))\npred = model_base.predict(test1,verbose = 0)\npred = np.argmax(pred,axis=-1)\npred = pred.astype('int16')\npred[:5]","7256da0c":"res=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv',index_col=None)  \nres['target'] = pred\nres.to_csv('submission.csv',index=False)","5db27a42":"# Preparing the Dataset ","c649358b":"# Importing Libraries","4dc5888d":"# Preparing the Model","a1c985c6":"# Training","b5ab0a78":"# Predicting on new tweets","f980b886":"# Prepare the Tokens"}}