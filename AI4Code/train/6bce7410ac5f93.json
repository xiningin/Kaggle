{"cell_type":{"ecdfd070":"code","bcdcb44c":"code","2ad0e19c":"code","60b5c3b8":"code","da9da2bc":"code","b412ef13":"code","6bf38bef":"code","2eac5a9e":"code","d466107b":"code","c84fecd6":"code","02c10918":"code","7ce9f139":"code","7d3cdbf9":"code","4c4df1c9":"code","011fd8d0":"code","97078bbd":"code","9335a177":"code","28233b48":"code","41d8d69a":"code","3bb60f98":"code","7c7a7357":"code","f61c721a":"code","297ef4d2":"code","151d8d31":"code","65948d2e":"code","3347eb3d":"code","dfc29296":"code","1fb1c756":"code","e0bcfeba":"code","c5b0d9c9":"code","becd7833":"code","5989171f":"code","3554e613":"code","7396c7bb":"code","53ffdf11":"code","bf46cd18":"code","445cf469":"code","be5b5f2b":"code","aea015cd":"code","d31139ea":"code","7674a631":"code","6ed3c29e":"code","e9a7cb0b":"code","f099a902":"code","1cc72f4a":"code","1033c374":"code","f572fe1d":"code","bbba1685":"code","f155dcac":"code","4f59e773":"markdown","6414f8a5":"markdown","3cc86b91":"markdown","02a2441f":"markdown","cdf5f73f":"markdown","844a3371":"markdown","e53e9e04":"markdown","6bb26824":"markdown","200942c8":"markdown","e25f955b":"markdown","df58d138":"markdown","9e774d37":"markdown","6cf7696b":"markdown","d3b10fa8":"markdown","067be832":"markdown","86bbf0ec":"markdown","c0722709":"markdown","1be31204":"markdown"},"source":{"ecdfd070":"from sklearn import model_selection","bcdcb44c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nDIR = '\/kaggle\/input'","2ad0e19c":"!unzip \/kaggle\/input\/quora-question-pairs\/train.csv.zip","60b5c3b8":"!ls","da9da2bc":"df = pd.read_csv(\"train.csv\")\ndf['kfold'] = -1\n\ndf = df.sample(frac=1.,random_state=2021).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=False)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y = df.is_duplicate.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold","b412ef13":"df.dropna(inplace=True)","6bf38bef":"df.question1.isna().sum(), df.question2.isna().sum(), df.question1.isnull().sum(), df.question2.isnull().sum()","2eac5a9e":"df.to_csv(\"train_folds.csv\", index=False)","d466107b":"df_fold = pd.read_csv(\"train_folds.csv\")","c84fecd6":"def sent_len(input_str: str):\n    input_str = str(input_str)\n    return len(input_str.strip().split(\" \"))","02c10918":"df_fold[\"question1_len\"] = list(map(sent_len, df_fold.question1.values.tolist()))","7ce9f139":"df_fold[\"question2_len\"] = list(map(sent_len, df_fold.question2.values.tolist()))","7d3cdbf9":"df_fold.head()","4c4df1c9":"df_fold.question1_len.plot.hist(bins=20);","011fd8d0":"df_fold.question2_len.plot.hist(bins=20);","97078bbd":"import tensorflow_hub as hub","9335a177":"embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")","28233b48":"embeddings = embed([\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"I am a sentence for which I would like to get its embedding\"])\n\n","41d8d69a":"import torch","3bb60f98":"#Reproducing same results\nSEED = 2021\n\n#Torch\ntorch.manual_seed(SEED)\n\n#Cuda algorithms\ntorch.backends.cudnn.deterministic = True  ","7c7a7357":"import torch.nn as nn","f61c721a":"BATCH_SIZE = 256","297ef4d2":"from torch.utils.data import DataLoader, Dataset","151d8d31":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","65948d2e":"class QuoraTrainData(Dataset):\n    def __init__(self, df: pd.DataFrame):\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        q1 = self.df.iloc[idx].question1\n        q2 = self.df.iloc[idx].question2\n        label = self.df.iloc[idx].is_duplicate\n        \n        return {\"q1\": q1, \"q2\": q2, \"label\": label}","3347eb3d":"\nFOLD_MAPPPING = {\n    0: [1, 2, 3, 4],\n    1: [0, 2, 3, 4],\n    2: [0, 1, 3, 4],\n    3: [0, 1, 2, 4],\n    4: [0, 1, 2, 3]\n}","dfc29296":"FOLD = 0","1fb1c756":"train_df = df_fold[df_fold.kfold.isin(FOLD_MAPPPING.get(FOLD))].reset_index(drop=True)\nvalid_df = df_fold[df_fold.kfold==FOLD].reset_index(drop=True)","e0bcfeba":"train_df.shape, valid_df.shape","c5b0d9c9":"valid_df.head()","becd7833":"train_dataset = QuoraTrainData(train_df)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)","5989171f":"valid_dataset = QuoraTrainData(valid_df)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)","3554e613":"# 5th example\ntrain_dataset.__getitem__(5)","7396c7bb":"valid_dataset.__getitem__(5)","53ffdf11":"train_iter = iter(train_loader)\nres = train_iter.next()","bf46cd18":"class IsDuplicate(nn.Module):\n    def __init__(self, output_dim: int, emb_dim: int, hid_dim=512):\n        \"\"\"Simple MultiLayerPerceptron\n            Linear model\n        \"\"\"\n        super().__init__()\n        #dense layer\n        self.fc1 = nn.Linear(emb_dim * 2, hid_dim)\n        \n        self.fc2 = nn.Linear(hid_dim, output_dim)\n        \n        #activation function\n        self.act = nn.Sigmoid()\n        \n    def forward(self, text1:[str], text2:[str]):\n        \"\"\"\n        text1: list of strings from question1, len: batch_size\n        text2: list of strings from question2, len: batch_size\n        \"\"\"\n        \n        emb1 = embed(text1)\n        e1 = torch.from_numpy(emb1.numpy()).to(device)\n        # e1.size()\n        \n        emb2 = embed(text2)\n        e2 = torch.from_numpy(emb2.numpy()).to(device)\n        # e2.size()\n        \n        hidden = torch.cat((e1, e2), dim = 1)\n        \n        #hidden = [batch size, hid dim * num directions]\n        dense_outputs1=self.fc1(hidden)\n        dense_outputs2=self.fc2(dense_outputs1)\n\n        #Final activation function\n        outputs=self.act(dense_outputs2)\n        \n        return outputs","445cf469":"class IsDuplicateAdv(nn.Module):\n    def __init__(self, output_dim: int, emb_dim: int, hid_dim=512):\n        \"\"\"Non Linear model\n        \"\"\"\n        super().__init__()\n        #dense layer\n        \n        self.batchnorm1 = nn.BatchNorm1d(emb_dim * 2)\n        self.dropout = nn.Dropout(p=0.2)\n        self.nonlinear = nn.PReLU()\n        \n        self.fc1 = nn.Linear(emb_dim * 2, hid_dim)\n        self.batchnorm2 = nn.BatchNorm1d(hid_dim)\n        self.fc2 = nn.Linear(hid_dim, output_dim)\n        \n        #activation function\n        self.act = nn.Sigmoid()\n        \n    def forward(self, text1:[str], text2:[str]):\n        \"\"\"\n        text1: list of strings from question1, len: batch_size\n        text2: list of strings from question2, len: batch_size\n        \"\"\"\n        \n        emb1 = embed(text1)\n        e1 = torch.from_numpy(emb1.numpy()).to(device)\n        \n        emb2 = embed(text2)\n        e2 = torch.from_numpy(emb2.numpy()).to(device)\n        \n        # merged\n        x = torch.cat((e1, e2), dim = 1)\n        x = self.batchnorm1(x)\n        \n        \n        x=self.fc1(x)\n        x = self.nonlinear(x)\n        x = self.dropout(x)\n        x = self.batchnorm2(x)\n        \n        x=self.fc2(x)\n\n        #Final activation function\n        outputs=self.act(x)\n        \n        return outputs","be5b5f2b":"model = IsDuplicateAdv(output_dim=2, emb_dim=512).to(device)\n\nprob = model(text1= res['q1'], text2 = res['q2'])","aea015cd":"print(model)","d31139ea":"from prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        table.add_row([name, param])\n        total_params+=param\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\n    \ncount_parameters(model)","7674a631":"import torch.optim as optim\n\n#define optimizer and loss\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.BCELoss()","6ed3c29e":"#define metric\ndef binary_accuracy(preds, y):\n    #round predictions to the closest integer\n    rounded_preds = torch.argmax(preds, dim=1)\n    \n    correct = (rounded_preds == y).float() \n    acc = correct.sum() \/ len(correct)\n    return acc\n    \n#push to cuda if available\nmodel = model.to(device)\ncriterion = criterion.to(device)","e9a7cb0b":"def train(model, train_data_loader, optimizer, criterion):\n    \n    #initialize every epoch \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    #set the model in training phase\n    model.train()  \n    \n    n_batch = len(train_data_loader)\n    for i, batch in enumerate(train_data_loader):\n        \n        #resets the gradients after every batch\n        optimizer.zero_grad()   \n        \n        #retrieve text and no. of words\n        q1, q2, label = batch['q1'], batch['q2'], batch['label'] \n        \n        label = label.to(device)\n        #convert to 1D tensor\n        predictions = model(q1, q2)\n        \n        #print(predictions.dtype)\n        #print(label.float().dtype)\n        #compute the loss\n        loss = criterion(predictions[:,1], label.float())        \n        \n        #compute the binary accuracy\n        acc = binary_accuracy(predictions, label.float())   \n        \n        #backpropage the loss and compute the gradients\n        loss.backward()       \n        \n        #update the weights\n        optimizer.step()      \n        \n        #loss and accuracy\n        batch_loss = loss.item()\n        batch_acc = acc.item() \n        epoch_loss += batch_loss  \n        epoch_acc +=  batch_acc  \n        if i % 100 == 0:\n            print(f\"\\t\\t\\t > trn batch_no: {i}\/{n_batch}, batch_loss: {np.round(batch_loss, 4)}, batch_acc: {np.round(batch_acc, 4)}\")\n        \n    return epoch_loss \/ len(train_data_loader), epoch_acc \/ len(train_data_loader)","f099a902":"def evaluate(model, valid_data_loader, criterion):\n    \n    #initialize every epoch\n    epoch_loss = 0\n    epoch_acc = 0\n\n    #deactivating dropout layers\n    model.eval()\n    \n    #deactivates autograd\n    n_batch = len(valid_data_loader)\n    with torch.no_grad():\n    \n        for i, batch in enumerate(valid_data_loader):\n        \n            #retrieve question pair and labels\n            q1, q2, label = batch['q1'], batch['q2'], batch['label']\n            label = label.to(device)\n            #convert to 1d tensor\n            predictions = model(q1, q2)\n            \n            #compute loss and accuracy\n            \n            loss = criterion(predictions[:,1], label.float())\n            acc = binary_accuracy(predictions, label.float())\n            \n            #loss and accuracy\n            batch_loss = loss.item()\n            batch_acc = acc.item() \n            epoch_loss += batch_loss  \n            epoch_acc +=  batch_acc \n            if i % 50 == 0:\n                print(f\"\\t\\t\\t > val batch_no: {i}\/{n_batch}, batch_loss: {np.round(batch_loss,4)}, batch_acc: {np.round(batch_acc, 4)}\")\n            \n        \n    return epoch_loss \/ len(valid_data_loader), epoch_acc \/ len(valid_data_loader)\n","1cc72f4a":"N_EPOCHS = 5\nbest_valid_loss = float('inf')\n\nhistory = {\n    \"train_loss\": [],\n    \"train_acc\": [],\n    \"valid_loss\": [],\n    \"valid_acc\": []\n}\n\nfor epoch in range(N_EPOCHS):\n     \n    #train the model\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    \n    #evaluate the model\n    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n    \n    train_loss = np.round(train_loss,4)\n    train_acc = np.round(train_acc, 4)\n    valid_loss = np.round(valid_loss, 4)\n    valid_acc = np.round(valid_acc, 4)\n    \n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"valid_loss\"].append(valid_loss)\n    history[\"valid_acc\"].append(valid_acc)\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    print(f'Epoch: {epoch+1}\/{N_EPOCHS} \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","1033c374":"history","f572fe1d":"%matplotlib inline\nfrom matplotlib import pyplot as plt","bbba1685":"plt.plot(history[\"train_loss\"], label=\"train\")\nplt.plot(history[\"valid_loss\"], label=\"val\")\nplt.title(\"Loss vs Epoch\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid(alpha=0.3)\nplt.legend()\nplt.show()","f155dcac":"plt.plot(history[\"train_acc\"], label=\"train\")\nplt.plot(history[\"valid_acc\"], label=\"val\")\nplt.title(\"Accuracy vs Epochs\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.grid(alpha=0.3)\nplt.legend()\nplt.show()","4f59e773":"## Evaluate","6414f8a5":"# Fix`nan` in `train`","3cc86b91":"# Design Train Dataloader","02a2441f":"## Check train dataset","cdf5f73f":"## Model parameters","844a3371":"## Train","e53e9e04":"## Check train dataloader","6bb26824":"# `BCEWithLogitLoss()` more stable than `Sigmoid()` + `BCELoss()`. Why?\n\n> `Sigmoid()` + `BCELoss()` = `BCEWithLogitLoss()`\n\n\n\n- [Ans](https:\/\/discuss.pytorch.org\/t\/bce-loss-vs-cross-entropy\/97437\/2)\n\n","200942c8":"# Load Universal Sentence Encoder","e25f955b":"# Quora Duplicate Questions Detection\n","df58d138":"## Adding deeper non-linear model\n\n- The original architecture idea came from [here](https:\/\/www.linkedin.com\/pulse\/duplicate-quora-question-abhishek-thakur\/). But the original architecture is heavily simplified to the below structure with the use of transfer learning using `Universal Sentence Encoder`\n\n<center>\n<img src='https:\/\/raw.githubusercontent.com\/msank00\/Kaggle_202101_Quora_Duplicate_Questions\/main\/images\/NN_Architecture.jpg' width='400'>    \n<\/center>","9e774d37":"# Check sentence length distribution","6cf7696b":"# Plot training performance","d3b10fa8":"## Simple multilayer perceptron - no nonlinearity ","067be832":"# Design Model","86bbf0ec":"## Test model","c0722709":"## Train Loop","1be31204":"## Ensure reproducibility"}}