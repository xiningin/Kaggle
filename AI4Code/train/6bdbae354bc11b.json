{"cell_type":{"a0e17415":"code","577ff2bd":"code","78dc5a57":"code","7cb6ac08":"code","75cfb1e3":"code","fb8e7728":"code","84131e2e":"code","ee0df78e":"code","780f9bbc":"code","c31383eb":"code","952c0862":"code","657d2384":"code","f6cfd2eb":"code","7cebc03c":"code","37b61c10":"code","8cd14d93":"code","056dd57e":"code","43e09fb8":"code","e188749d":"code","dd589a0a":"code","2244e5a7":"code","78291006":"code","55852b65":"code","dc75fb42":"code","c24ef3e3":"code","c3b34537":"code","3048da0f":"code","cf22fb5a":"code","27804eb5":"code","f1d00284":"code","27849725":"code","befbe45d":"code","b19c0706":"code","7b9a25ff":"markdown","16bfb6df":"markdown","eb0ff179":"markdown","54e20c7d":"markdown","0b0c4da0":"markdown","dc744305":"markdown","5089c5c2":"markdown","c33222df":"markdown","31bd44f8":"markdown","e9a31891":"markdown","69851e35":"markdown","e8001a2c":"markdown","5f183ad3":"markdown","7c4570c1":"markdown","44a2bf05":"markdown","3c77753f":"markdown","83d632db":"markdown","c674cd51":"markdown","2a5eadf5":"markdown","82fd8c8c":"markdown","edeb4935":"markdown","493f18a8":"markdown","abbb32c0":"markdown","b46270b7":"markdown","c5a579d0":"markdown"},"source":{"a0e17415":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as px\npx.init_notebook_mode(connected=True)\npx.offline.init_notebook_mode(connected=True)\nimport plotly.express as px\nfrom urllib.request import urlopen  \nimport os.path as osp\nimport os\nimport logging\nimport zipfile\nfrom glob import glob\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier,  AdaBoostClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import mean_squared_error \n  \nlogging.getLogger().setLevel('INFO')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","577ff2bd":"def download_file(url_str, path):\n    url = urlopen(url_str)\n    output = open(path, 'wb')       \n    output.write(url.read())\n    output.close()  \n    \ndef extract_file(archive_path, target_dir):\n    zip_file = zipfile.ZipFile(archive_path, 'r')\n    zip_file.extractall(target_dir)\n    zip_file.close()","78dc5a57":"BASE_URL = 'http:\/\/tennis-data.co.uk'\nDATA_DIR = \"tennis_data\"\nATP_DIR = '.\/{}\/ATP'.format(DATA_DIR)\nWTA_DIR = '.\/{}\/WTA'.format(DATA_DIR)\n\nATP_URLS = [BASE_URL + \"\/%i\/%i.zip\" % (i,i) for i in range(2000,2019)]\nWTA_URLS = [BASE_URL + \"\/%iw\/%i.zip\" % (i,i) for i in range(2007,2019)]\n\nos.makedirs(osp.join(ATP_DIR, 'archives'), exist_ok=True)\nos.makedirs(osp.join(WTA_DIR, 'archives'), exist_ok=True)\n\nfor files, directory in ((ATP_URLS, ATP_DIR), (WTA_URLS, WTA_DIR)):\n    for dl_path in files:\n        logging.info(\"downloading & extracting file %s\", dl_path)\n        archive_path = osp.join(directory, 'archives', osp.basename(dl_path))\n        download_file(dl_path, archive_path)\n        extract_file(archive_path, directory)\n    \nATP_FILES = sorted(glob(\"%s\/*.xls*\" % ATP_DIR))\nWTA_FILES = sorted(glob(\"%s\/*.xls*\" % WTA_DIR))\n\ndf_atp = pd.concat([pd.read_excel(f) for f in ATP_FILES], ignore_index=True)\ndf_wta = pd.concat([pd.read_excel(f) for f in WTA_FILES], ignore_index=True)\n\nlogging.info(\"%i matches ATP in df_atp\", df_atp.shape[0])\nlogging.info(\"%i matches WTA in df_wta\", df_wta.shape[0])","7cb6ac08":"df_atp['Lsets'].replace('`1', 1,inplace = True)","75cfb1e3":"dist_client_dfs=df_atp.groupby([\"Winner\"])[\"Winner\"].count().reset_index(name=\"count\")\ndist_client_dfs.sort_values(by=['count'], ascending=False, inplace=True)\ndist_client_dfs=dist_client_dfs.nlargest(10, ['count'])\nprint(dist_client_dfs.head(3))\nfig = px.bar(dist_client_dfs, x=\"Winner\", y=\"count\", orientation='v',text=\"count\")\nfig.show()","fb8e7728":"df_atp['Wsets'].fillna(0,inplace=True)\ndf_atp['Lsets'].fillna(0,inplace=True)","84131e2e":"df_Federer_Winner = df_atp[(df_atp.Winner == \"Federer R.\")] \ndf_Federer_Loser = df_atp[ (df_atp.Loser == \"Federer R.\")]\nprint(\"Total number of sets Roger Federer won: \",df_Federer_Winner['Wsets'].sum() + df_Federer_Loser['Lsets'].sum())","ee0df78e":"df_Federer_Loser_1617 = df_atp[(df_atp.Loser == \"Federer R.\") & ((df_atp[\"Date\"].dt.year == 2016) | \n                                                                 (df_atp[\"Date\"].dt.year == 2017))]\ndf_Federer_Winner_1617 = df_atp[(df_atp.Winner == \"Federer R.\") & ((df_atp[\"Date\"].dt.year == 2016) | \n                                                                   (df_atp[\"Date\"].dt.year == 2017))]\nprint(\"Number of sets Roger Federer won in 2016 and 2017: \",df_Federer_Winner_1617['Wsets'].sum() + \n                                                              df_Federer_Loser_1617['Lsets'].sum())\n","780f9bbc":"def previous_w_percentage(player,date, df_atp):\n    minimum_played_games = 2\n    df_previous  = df_atp[df_atp[\"Date\"] < date]\n    previous_wins = df_previous[df_previous[\"Winner\"] == player].shape[0]\n    previous_losses = df_previous[df_previous[\"Loser\"] == player].shape[0]\n    \n    if  minimum_played_games > (previous_wins + previous_losses):\n        return 0\n    return previous_wins \/ (previous_wins + previous_losses)","c31383eb":"df_atp[\"winner_previous_win_percentage\"] = df_atp.apply(\n    lambda row: previous_w_percentage(\n        row[\"Winner\"],\n        row[\"Date\"],\n        df_atp\n    ),\n    axis=1\n)","952c0862":"df_atp.to_csv(\"atp_previous_win_percentage.csv\")","657d2384":"grandslams = df_atp[['Date','Tournament','Series', 'Round', 'Winner']]\ngrandslams = grandslams[(grandslams.Series == 'Grand Slam') & (grandslams.Round == 'The Final')]\ngrandslams['Titles'] = grandslams.groupby('Winner').cumcount().astype(int) + 1\n","f6cfd2eb":"winners = grandslams.groupby('Winner')['Tournament'].count()\nwinners = winners.reset_index()\nwinners = winners.sort_values(['Tournament'], ascending=False)\n# winners","7cebc03c":"fig = px.bar(winners, x=\"Winner\", y=\"Tournament\", orientation='v',text=\"Tournament\",title='Grand Slams won since 2000')\nfig.show()","37b61c10":"winners_grandslam = grandslams.groupby(['Winner', 'Tournament']).count()\nwinners_grandslam = winners_grandslam.reset_index()\n# winners_grandslam","8cd14d93":"fig = px.bar(winners_grandslam, x=\"Winner\", y=\"Titles\", color=\"Tournament\", barmode=\"group\")\nfig.show()","056dd57e":"type_surface = df_atp[['Surface', 'Winner', 'Loser']]\n\ntype_surface_w = type_surface[['Surface', 'Winner']]\ntype_surface_l = type_surface[['Surface', 'Loser']]\ntype_surface_w.columns = ['Surface', 'Player']\ntype_surface_l.columns = ['Surface', 'Player']\n\ntype_surface_w['idx'] = range(1, len(type_surface_w) + 1)\ntype_surface_l['idx'] = range(1, len(type_surface_l) + 1)\n\ntype_surface_w = type_surface_w.groupby(['Surface', 'Player']).count()\ntype_surface_w = type_surface_w.reset_index()\ntype_surface_w.columns = ['Surface', 'Player', 'Won']\n\ntype_surface_l = type_surface_l.groupby(['Surface', 'Player']).count()\ntype_surface_l = type_surface_l.reset_index()\ntype_surface_l.columns = ['Surface', 'Player', 'Lost']\n\ntype_surface = pd.merge(type_surface_w, type_surface_l, on=['Surface', 'Player'])\n\ntype_surface['total_play'] = type_surface['Won'] + type_surface['Lost']\n\ntype_surface['perc_win'] = round(type_surface['Won'] \/ type_surface['total_play'],4)*100\n\ntype_surface = type_surface[type_surface.total_play > 50]\n\n# type_surface.sort_values(by='perc_win', ascending=False).head(30)","43e09fb8":"hard = type_surface[type_surface.Surface == 'Hard'].sort_values(by='perc_win', ascending = False).head(10)\nfig1 = px.bar(hard, x='Player', y='perc_win',\n             hover_data=['Won', 'Lost'], color='perc_win',\n             labels={'perc_win':'Win Percentage'}, height=400,title='Best players on Hard surface')\nfig1.show()","e188749d":"grass = type_surface[type_surface.Surface == 'Grass'].sort_values(by='perc_win', ascending = False).head(10)\nfig1 = px.bar(grass, x='Player', y='perc_win',\n             hover_data=['Won', 'Lost'], color='perc_win',\n             labels={'perc_win':'Win Percentage'}, height=400,title='Best players on Grass surface')\nfig1.show()","dd589a0a":"clay = type_surface[type_surface.Surface == 'Clay'].sort_values(by='perc_win', ascending = False).head(10)\nfig = px.bar(clay, x='Player', y='perc_win',\n             hover_data=['Won', 'Lost'], color='perc_win',\n             labels={'perc_win':'Win Percentage'}, height=400,title='Best players on Clay surface')\nfig.show()","2244e5a7":"df_atp[\"Winner_position\"] = df_atp.apply(lambda row: 1 if row[\"Winner\"] > row[\"Loser\"] else 0, axis=1)\ndf_atp[[\"Winner\", \"Loser\", \"Winner_position\"]].head(5)","78291006":"print(df_atp[df_atp[\"Winner_position\"] == 1].shape[0])\nprint(df_atp[df_atp[\"Winner_position\"] == 0].shape[0])","55852b65":"df_atp_X = df_atp.loc[:, ['AvgL', 'AvgW', 'B&WL', 'B&WW', 'B365L', 'B365W', 'CBL', 'CBW', 'EXL', 'EXW', 'GBL', 'GBW', \\\n    'IWL', 'IWW','LBL', 'LBW', 'LRank', 'MaxL', 'MaxW', 'PSL', 'PSW', \\\n    'SBL', 'SBW', 'SJL', 'SJW', 'UBL', 'UBW', 'WRank','Best of', \\\n    \"Date\",'ATP','Series','Court','Surface', 'Winner_position']]","dc75fb42":"df_atp_X.iloc[:, :-4] = df_atp_X.apply(pd.to_numeric, errors='coerce') \n\ndf_atp_X[\"WRank\"] = df_atp_X[\"WRank\"].fillna(df_atp_X[\"WRank\"].max())\ndf_atp_X[\"LRank\"] = df_atp_X[\"LRank\"].fillna(df_atp_X[\"LRank\"].max())\n\n\ncols_1=[\"AvgL\", \"AvgW\", \"B&WL\", \"B&WW\", \"B365L\", \"B365W\", \"CBL\", \"CBW\", \"EXL\", \"EXW\", \"GBL\", \"GBW\", \"IWL\", \"IWW\",\n    \"LBL\", \"LBW\", \"MaxL\", \"MaxW\", \"PSL\", \"PSW\", \"SBL\", \"SBW\", \"SJL\", \"SJW\", \"UBL\", \"UBW\"]\ndf_atp_X[cols_1]=df_atp_X[cols_1].fillna(1.0)\n\n# df_atp_X.isnull().sum()","c24ef3e3":"df_atp_X[\"P1Rank\"] = df_atp_X.apply(lambda row: row[\"WRank\"] if row[\"Winner_position\"] == 1 else row[\"LRank\"], axis=1)\ndf_atp_X[\"P0Rank\"] = df_atp_X.apply(lambda row: row[\"WRank\"] if row[\"Winner_position\"] == 0 else row[\"LRank\"], axis=1)\ndf_atp_X=df_atp_X.drop(\"WRank\", axis=1)\ndf_atp_X=df_atp_X.drop(\"LRank\", axis=1)\nfor cols in ( ('AvgL', 'AvgW'), ('B&WL', 'B&WW'), ('B365L', 'B365W'), ('CBL', 'CBW'), ('EXL', 'EXW'), ('GBL', 'GBW'), \\\n    ('IWL', 'IWW'),('LBL', 'LBW'), ('MaxL', 'MaxW'), ('PSL', 'PSW'), \\\n    ('SBL', 'SBW'), ('SJL', 'SJW'), ('UBL', 'UBW')):\n    suffix=cols[1][:-1]\n    df_atp_X[\"P1\"+suffix] = df_atp_X.apply(lambda row: row[cols[1]] if row[\"Winner_position\"] == 1 else row[cols[0]], axis=1)\n    df_atp_X[\"P0\"+suffix] = df_atp_X.apply(lambda row: row[cols[1]] if row[\"Winner_position\"] == 0 else row[cols[0]], axis=1)\n    df_atp_X=df_atp_X.drop(cols[0], axis=1)\n    df_atp_X=df_atp_X.drop(cols[1], axis=1) ","c3b34537":"column_names_for_onehot = df_atp_X.columns[3:6]\nencoded_atp_df = pd.get_dummies(df_atp_X, columns=column_names_for_onehot, drop_first=True)\n","3048da0f":"encoded_atp_df['Date'] = pd.to_datetime(encoded_atp_df['Date'], format = '%Y-%m-%dT', errors = 'coerce')\nencoded_atp_df['Date_year'] = encoded_atp_df['Date'].dt.year\nencoded_atp_df['Date_month'] = encoded_atp_df['Date'].dt.month\nencoded_atp_df['Date_week'] = encoded_atp_df['Date'].dt.week\nencoded_atp_df['Date_day'] = encoded_atp_df['Date'].dt.day\nencoded_atp_df=encoded_atp_df.drop(\"Date\", axis=1)","cf22fb5a":"f,ax = plt.subplots(figsize=(25, 25))\nsns.heatmap(encoded_atp_df.drop(\"Winner_position\", axis=1).corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n\ncorr_matrix = encoded_atp_df.drop(\"Winner_position\", axis=1).corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find features with correlation greater than 0.8\nto_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\nprint(\"Features to be dropped: \",to_drop)\n# Drop features \nencoded_atp_df.drop(to_drop, axis=1, inplace=True)\n# encoded_atp_df.head()","27804eb5":"from pandas.plotting import scatter_matrix\nencoded_atp_df.hist()\nplt.gcf().set_size_inches(25, 25)\nsns.set(color_codes=True)\nencoded_atp_df = encoded_atp_df.loc[:,encoded_atp_df.apply(pd.Series.nunique) != 1]","f1d00284":"year_to_predict = 2017\n\ndf_train = encoded_atp_df.iloc[df_atp[df_atp[\"Date\"].dt.year != year_to_predict].index]\ndf_test = encoded_atp_df.iloc[df_atp[df_atp[\"Date\"].dt.year == year_to_predict].index]\n\nX_train = df_train.drop([\"Winner_position\"], axis=1)\ny_train = df_train[\"Winner_position\"]\n\nX_test = df_test.drop([\"Winner_position\"], axis=1)\ny_test = df_test[\"Winner_position\"]\n\nprint(\"Training Set Shape: \",X_train.shape,  y_train.shape)\nprint(\"Test Set Shape:     \",X_test.shape,  y_test.shape)","27849725":"sc = StandardScaler()  \nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)","befbe45d":"# number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 5, stop = 40, num = 5)]\n# number of features at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(4, 32, num = 6)]\nmax_depth.append(None)\ncriterion=['entropy','gini']\n# Method of selecting samples for training each tree\nbootstrap = [True,False]\n# create random grid\nrandom_grid = {\n 'n_estimators': n_estimators,\n 'max_features': max_features,\n 'max_depth': max_depth,\n 'criterion': criterion,\n 'bootstrap': bootstrap  \n }\n# Random search of parameters\nrfc_random = RandomizedSearchCV(RandomForestClassifier(), param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2,\n                                random_state=42, n_jobs = -1)\n# Fit the model\nrfc_random.fit(X_train_scaled, y_train)\n# print results\nprint(\"Best Parameters for Random Forest: \",rfc_random.best_params_)","b19c0706":"names_of_classifier = [\"Random Forest\",\"Decision Tree\",\"Linear SVM\",\"K-Nearest Neighbors\",  \"SVM-RBF\", \"AdaBoost\"]\n\nclassifier = [\n    RandomForestClassifier(n_estimators= 31, criterion=\"gini\", bootstrap = False, max_depth=4, max_features = 'auto',class_weight=\"balanced\"),\n    DecisionTreeClassifier(max_depth=9),\n    SVC(kernel=\"linear\", C=0.03),\n    KNeighborsClassifier(6, n_jobs=-1),\n    SVC(gamma=3, C=1),\n    AdaBoostClassifier()]\n\nfor name, classifier in zip(names_of_classifier, classifier):\n    classifier.fit(X_train_scaled,y_train)\n    \n    y_predict=classifier.predict(X_test_scaled)\n    y_Train_predict=classifier.predict(X_train_scaled)\n    print(\"Classifier: \",name)\n    print(\"\\nAccuracy for Test Set: \",accuracy_score(y_test, y_predict))\n    print( \"Mean Squared Error for Test Set: \",round(mean_squared_error(y_test,y_predict), 3))\n    print(\"Confusion matrix for Test Set \\n\",confusion_matrix(y_test,y_predict))\n    print(classification_report(y_test,y_predict))\n    fpr, tpr, thresholds= metrics.roc_curve(y_test,y_predict)\n    auc = metrics.roc_auc_score(y_test,y_predict, average='macro', sample_weight=None)\n    print(\"ROC Curve for for Test Set \\n\")\n    sns.set_style('darkgrid')\n    sns.lineplot(fpr,tpr,color ='blue')\n    plt.show()\n    \n    \n    print(\"\\nAccuracy for Train Set: \",accuracy_score(y_train, y_Train_predict))\n    print( \"Mean Squared Error for Train Set: \",round(mean_squared_error(y_train,y_Train_predict), 3))\n    print(\"Confusion matrix for Train Set \\n\",confusion_matrix(y_train,y_Train_predict))\n    print(classification_report(y_train,y_Train_predict))\n    fpr_train, tpr_train, thresholds_train= metrics.roc_curve(y_train,y_Train_predict)\n    auc_train = metrics.roc_auc_score(y_train,y_Train_predict, average='macro', sample_weight=None)\n    print(\"ROC Curve for for Train Set \\n\")\n    sns.set_style('darkgrid')\n    sns.lineplot(fpr_train,tpr_train,color ='red')\n    plt.show()\n    \n    print(\"--------------------------------------xxx--------------------------------------\\n\\n\")\n    ","7b9a25ff":"- So we were having 2 columns called... Winner and looser. So we named them player 1 and player 2. And for output variable (Winner_position) we have put the \n    1. '**0**' if Player 1\u00a0wins  \n    2. '**1**' if Player 2 wins\n- But in this case, Player 1 will be the winner in every case. Output variable is highly baised. In order to remove the biasness, I come up with logic that we will call player 1 and player 2 according to alphabetical order of their names and fill the output variable accordingly.\u00a0\n- Shuffled others variables accordingly. Because they were according to winner and loser and\u00a0 we have to rearrange according to Player 1 and Player 2.","16bfb6df":"\n####  **_Warning_**  \n- This result might take time to compute 'winner_previous_win_percentage' for 52000+ entries. CSV of dataframe containing 'winner_previous_win_percentage' is attached with notebook named as 'atp_previous_win_percentage.csv'","eb0ff179":"## Helpers","54e20c7d":"## Classification Model - To predict outcome of each ATP match for 2017","0b0c4da0":"### 3. How many sets did the player 'Federer R.' win during the years 2016 and 2017 ? ","dc744305":"### 4. For each match, what is the percentage of victories of the winner in the past ? ","5089c5c2":"#### Assigning Rank, Points, Betting odds as per Winner_position of Player.","c33222df":"#### Encoding for categorical values","31bd44f8":"#### Imputing Wsets and Lsets with 0s for those matches whose result was 'Walkover' or 'Retired' having value 'NaNs'","e9a31891":"### Feature Scaling using StandardScaler","69851e35":"### Data split for training and testing","e8001a2c":"#### Date Feature engineering","5f183ad3":"## Download the dataset","7c4570c1":"# ATP Tennis Match Prediction\nThe website [ATP Match Data since 2000](http:\/\/tennis-data.co.uk\/alldata.php) gathers outcomes of both *WTA (Women Tennis Association)* and *ATP (Association of Tennis Professionals - men only)* tennis games over several years. A short description of each variable can be found here : [Data Description](http:\/\/www.tennis-data.co.uk\/notes.txt)\n\n# Objective: \nTo predict the outcome for each ATP tennis game during the year 2017","44a2bf05":"### Best player on each surface (Hard, Grass, Clay)","3c77753f":"#### Correlation Matrix: To check features with high corelation with eachother and keep only one among them.]\n- I have used threshold of 0.8 so the pair of features crossing the threeshold will be removed.","83d632db":"#### Removing Features\nDropping features which are irrelevant for prediction model. Features related to current match (W1, W2, W3,W4, W5, L1, L2, L3, L4, L5, Wsets, Lsets, Round, Comments, etc..) are not required. Also, features like Location, Tournament and ATP describes the same information. Hence only ATP is selected among them.   ","c674cd51":"### Feature Engineering and Data preprocessing\n* Feature Engineering\n    - Dropping features which are irrelevant for prediction model.\n    - Correlation Matrix: To check multi-collinearity amongst the independent variables  \n    - Improper Distribution\/ Constant Feature: To check if variables exhibited an unequal distribution\n","2a5eadf5":"### 2.  How many sets did the player 'Federer R.' win in total ? ","82fd8c8c":"### Hyper parameter tuning for Random Forest","edeb4935":"#### Constant Feature - Improper Distribution of variable to check if contains single value throughout the dataset.","493f18a8":"### 1. Who are the three ATP players with the most wins ?","abbb32c0":"### Multiple Classification Model for prediction","b46270b7":"* Data Preprocessing\n    - Imputing **_WRank_** and **_LRank_** for players with the maximum rank value in each column\n    - For odds, we replace **_NaNs_** by a value of **_1_** (i.e probability=50%)\n    - Rearranging player specific data like Rank, Points, Betting odds as per new Output label (Winner_position '0' or '1')","c5a579d0":"### Players with maximum Grandslams title"}}