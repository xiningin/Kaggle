{"cell_type":{"9bb7e875":"code","38aaad64":"code","c5357068":"code","e7e80981":"code","48e0b4fd":"code","ee137414":"code","53251eb4":"code","e7dddbbb":"code","0b2a2b0d":"code","df5067cf":"code","bb413180":"code","688a1d34":"code","6bc2ab80":"code","caebf407":"code","9e58d177":"markdown","85002f97":"markdown","23fbac5d":"markdown","3029438f":"markdown","cac33c78":"markdown","23a872fb":"markdown","2c240961":"markdown","b97ecc71":"markdown","6750cca1":"markdown","d4c3c144":"markdown","8c4b8b02":"markdown","263c2e98":"markdown","9c3f8249":"markdown","ea429bec":"markdown","fa54661e":"markdown","4f1fe134":"markdown"},"source":{"9bb7e875":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n","38aaad64":"tf.__version__","c5357068":"from keras.datasets.mnist import load_data\n\n(x_train, y_train), (x_test, y_test) = load_data()\n\nprint('# X_train shape\\t: ', x_train.shape)\nprint('# y_train shape\\t: ', y_train.shape)\nprint('# X_test shape\\t: ', x_test.shape)\nprint('# y_test shape\\t: ', y_test.shape)","e7e80981":"X_train = np.true_divide(x_train, 255).reshape(-1, x_train.shape[1] * x_train.shape[2])\nX_test = np.true_divide(x_test, 255).reshape(-1, x_test.shape[1] * x_test.shape[2])\n","48e0b4fd":"def show(data: np.ndarray, y: np.ndarray, label: np.ndarray = None, n: np.integer = 4, figsize: tuple = None, dpi: np.integer = 100, factor: np.integer = 2, reverse: bool = False) -> None:\n\n    # rows and columns dims\n    def shape(n):\n        n_rows = 1\n        n_cols = n\n        b = []\n        i = 0\n        for k in range(n\/\/2, 1, -1):\n            if n\/k == n\/\/k:\n                b.append(np.abs(k**2-n))\n\n                n_rows = k\n                n_cols = n\/\/k\n                if(i > 0 and b[i-1] < b[i]):\n                    return n_rows, n_cols\n                i += 1\n        return n_rows, n_cols\n\n    flag = False\n    annot = y\n    color = '#fff'\n    nrows, ncols = shape(n)\n    if reverse:\n        nrows, ncols = ncols, nrows\n    figsize = figsize or (ncols*factor, nrows*factor)\n    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi=dpi, subplot_kw=dict(\n        xticks=[], yticks=[]), gridspec_kw=dict(hspace=0.2, wspace=0.2))\n    fig.set_facecolor('#000')\n    if label is not None:\n        flag = True\n        annot = label\n    for i, axi in enumerate(ax.flat):\n        if (flag and label[i] != y[i]):\n            color = 'red'\n        elif(flag):\n            color = 'green'\n        axi.imshow(data[i], cmap='binary_r', interpolation='nearest')\n        axi.set_axis_off()\n        axi.text(0.5, 0.5, str(annot[i]), color=color, size=26)\n","ee137414":"show(X_train.reshape(-1, 28, 28), y_train, n=26, reverse=0)\n","53251eb4":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(X_train)","e7dddbbb":"def variance_ratio(obj, gain):\n    gain = np.round(gain, 2)\n    x_gain = np.arange(1, 785)\n    y_gain = np.cumsum(obj.explained_variance_ratio_)\n    for i, x in enumerate(x_gain):\n        if (np.isclose(gain, y_gain[i], rtol=1e-4)):\n            # print(gain, y_gain[i], i)\n            break\n    plt.figure(figsize=(8, 5), dpi=100)\n    plt.plot(x_gain, y_gain)\n    plt.title('Information gain : {0}'. format(gain), size=26)\n    plt.axvline(i + 1, alpha=0.3, ls='-', ymin=0, ymax=0.97)\n    print('n_components = ', i + 1)\n","0b2a2b0d":"variance_ratio(pca, 0.978)","df5067cf":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\ndef score(y_pred, y):\n  plt.figure(figsize=(12, 5), dpi=100)\n  cm = confusion_matrix(y_pred, y_test)\n  sns.heatmap(cm, square=True, cbar=False, annot=True, fmt='d', vmax=100)\n  plt.xlabel('True label')\n  plt.ylabel('Predicted label')\n  print(classification_report(y_pred, y_test))","bb413180":"from sklearn.svm import SVC\n\nsvc = SVC(C=26)\npca = PCA(260, svd_solver='randomized')\nsvc.fit(pca.fit_transform(X_train), y_train)\ny_pred = svc.predict(pca.transform(X_test))","688a1d34":"score(y_pred, y_test)","6bc2ab80":"show(data=(X_test).reshape(-1, 28, 28), y=y_test, label=y_pred, n=26, reverse=0, dpi=100)","caebf407":"show(data=(X_test).reshape(-1, 28, 28)[40:], y=y_test[40:], label=y_pred[40:], n=260, reverse=0, dpi=100)","9e58d177":"# Loading the datasets","85002f97":"# MNIST Digits Classification using SVC and PCA dimensionality reduction ","23fbac5d":"# Plotting function(helper function)","3029438f":"# Image data Scalling and reshaping","cac33c78":"# Feature engineering using PCA","23a872fb":"# Importing the standard libs","2c240961":"# creating a custom scoring function","b97ecc71":"# Classify the Digits using Support Vector Classification","6750cca1":"### Here we get only 1 incorrect prediction from the above result out of 260 sets","d4c3c144":"# plotting the pca cummulative explained variance ratio curves","8c4b8b02":"### From above we get the optimal reduced n_component = 260, with most information preserved almost 0.98","263c2e98":"## Checking the tf .version","9c3f8249":"### get get all the predictions correctly from the first 26 digits from the test set","ea429bec":"# Now checking for digits for large sets","fa54661e":"# Checking the result visually","4f1fe134":"## visualizing the digits from training set"}}