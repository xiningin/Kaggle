{"cell_type":{"2f53c7f1":"code","f8495bf0":"code","1d39b6fb":"code","1b3b9518":"code","8b304788":"code","aa57a0d7":"code","dbe69bcd":"code","6a9e5220":"code","48e687f5":"code","1c442dac":"code","6a5cdcc8":"code","c87cb935":"code","b2cff52c":"code","d5ab7ef5":"code","b0776a3b":"code","6e89f129":"code","fc0607ed":"code","50eb19b0":"code","add9b5e3":"code","e9806543":"code","b71905af":"code","914d94ff":"code","f40e740b":"code","bf39db8c":"code","c0acc318":"code","9d238657":"code","467fe8e0":"code","aee0a88d":"code","2b17b8bb":"code","2b457fae":"code","e164d4f9":"code","55e452d7":"code","4612f325":"code","a3abda31":"code","23a2db91":"code","6dd57a7f":"code","99e9ba04":"code","be87837f":"code","ad9fe46d":"code","29be189a":"code","8ddb6362":"code","9c32832b":"code","f0ec8c90":"code","4cfc6e81":"code","2bfc8876":"code","4ae0205b":"code","762498d8":"code","767a1432":"code","9e6af4af":"code","03df2ec3":"code","335e6a57":"code","bf8994a0":"markdown","99a41759":"markdown","d80a9c12":"markdown","4a900a38":"markdown","c5a7c3b1":"markdown","16434e2c":"markdown","ded6bb37":"markdown","03aa1b17":"markdown","75ede5cf":"markdown","c747cf0d":"markdown","e8cad642":"markdown","890ccac4":"markdown","20ab1e06":"markdown","520add03":"markdown","83390b35":"markdown","2bd75db1":"markdown","6b6cfcf6":"markdown","6aae1195":"markdown","79eec1c6":"markdown","dcd9ce40":"markdown","b63db1e6":"markdown","fa0ff7f3":"markdown","2fc8bda3":"markdown","d34850a7":"markdown","26f3cdcb":"markdown"},"source":{"2f53c7f1":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","f8495bf0":"df = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\") # reading dataset","1d39b6fb":"df.head() # display first five rows","1b3b9518":"df.shape # the shape of our dataset","8b304788":"df.dtypes","aa57a0d7":"df.hist(figsize = (20,10), layout = (3,4))\nplt.show()","dbe69bcd":"df.isnull().sum() # total number of NaN values in all columns","6a9e5220":"# Replace NaN values with medians of those columns\ndf['ph'] = df['ph'].fillna(df['ph'].median())\ndf['Sulfate'] = df['Sulfate'].fillna(df['Sulfate'].median())\ndf['Trihalomethanes'] = df['Trihalomethanes'].fillna(df['Trihalomethanes'].median())","48e687f5":"df.isnull().sum()","1c442dac":"values = df['Potability'].value_counts().to_list()\nlabels = df['Potability'].value_counts().index.to_list()\nplt.pie(values, labels = labels, autopct = \"%1.1f%%\", explode = [0.05, 0.05], shadow = True, startangle = 120)\nplt.show()","6a5cdcc8":"fig = plt.figure()\nfig.suptitle(\"Distribution Plots\", fontsize = 25)\nfig.subplots_adjust(wspace = 0.2, hspace = 0.3)\nfor i,x in enumerate(df.columns):\n    ax = fig.add_subplot(4,3,i+1)\n    fig.set_figheight(20)\n    fig.set_figwidth(20)\n    sns.distplot(df[x], hist = False, color = 'violet', kde_kws = {'shade': True})\nplt.show()","c87cb935":"df['Solids'] = np.power(df['Solids'], 1\/2)","b2cff52c":"sns.distplot(df['Solids'], hist = False, color = 'violet', kde_kws = {'shade' : True})","d5ab7ef5":"fig = plt.figure()\nfig.suptitle(\"Violin Plots\", fontsize = 25)\nfig.subplots_adjust(wspace = 0.2, hspace = 0.3)\nfor i,x in enumerate(df.columns):\n    ax = fig.add_subplot(4,3,i+1)\n    fig.set_figheight(20)\n    fig.set_figwidth(20)\n    sns.violinplot(x = df['Potability'], y = df[x])\nplt.show()","b0776a3b":"sns.pairplot(df, hue = \"Potability\")","6e89f129":"plt.figure(figsize = (10,10))\nsns.heatmap(df.corr(), annot = True, cmap = \"RdYlGn\")","fc0607ed":"from sklearn.model_selection import train_test_split\nX = df.drop('Potability', axis = 1)\ny = df['Potability']\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.3)\n# This will create a training set which consists of 70% of the original dataset and testing set contains 30% data","50eb19b0":"xtrain.shape, ytrain.shape, xtest.shape, ytest.shape","add9b5e3":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nscale.fit(xtrain, ytrain)\nxtrain_scaled = scale.transform(xtrain)\nxtest_scaled = scale.transform(xtest)","e9806543":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(xtrain_scaled, ytrain)","b71905af":"yhat_logreg = logreg.predict(xtest_scaled)","914d94ff":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint(accuracy_score(yhat_logreg, ytest), confusion_matrix(yhat_logreg, ytest),\n      classification_report(yhat_logreg, ytest), sep = '\\n\\n')","f40e740b":"logreg_score = accuracy_score(yhat_logreg, ytest)","bf39db8c":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(xtrain_scaled, ytrain)\nyhat_knn = knn.predict(xtest_scaled)","c0acc318":"print(accuracy_score(yhat_knn, ytest), confusion_matrix(yhat_knn, ytest),\n      classification_report(yhat_knn, ytest), sep = '\\n\\n')","9d238657":"train_score = []\ntest_score = []\nfor n in range(2,20,2):\n    knn = KNeighborsClassifier(n_neighbors = n)\n    knn.fit(xtrain_scaled, ytrain)\n    train_score.append(knn.score(xtrain_scaled, ytrain))\n    test_score.append(knn.score(xtest_scaled, ytest))\nplt.plot(train_score, color = 'r', label = 'train score')\nplt.plot(test_score,color = 'g', label = 'test_score')\nplt.legend()","467fe8e0":"knn = KNeighborsClassifier(n_neighbors = 7)\nknn.fit(xtrain_scaled, ytrain)\nyhat_knn = knn.predict(xtest_scaled)","aee0a88d":"print(accuracy_score(yhat_logreg, ytest), confusion_matrix(yhat_logreg, ytest),\n      classification_report(yhat_logreg, ytest), sep = '\\n\\n')","2b17b8bb":"knn_score = accuracy_score(yhat_knn, ytest)","2b457fae":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(xtrain_scaled, ytrain)","e164d4f9":"yhat_dt = dt.predict(xtest_scaled)","55e452d7":"print(accuracy_score(yhat_dt, ytest), confusion_matrix(yhat_dt, ytest),\n      classification_report(yhat_dt, ytest),sep = '\\n\\n')","4612f325":"depths = range(2,25)\ntrain_score = []\ntest_score = []\nfor max_depth in depths:\n    dtc = DecisionTreeClassifier(max_depth = max_depth)\n    dtc.fit(xtrain_scaled, ytrain)\n    train_score.append(dtc.score(xtrain_scaled, ytrain))\n    test_score.append(dtc.score(xtest_scaled, ytest))\nplt.plot(train_score)\nplt.plot(test_score)","a3abda31":"from sklearn.model_selection import RandomizedSearchCV, KFold # KFold for creating cross validation sets\ndtc_grid = {'max_depth' : range(2,30),\n           'min_samples_split' : range(2, 100, 7),\n           'min_samples_leaf' : range(2, 100, 7)}\ndtc = DecisionTreeClassifier()\ndtc_rcv = RandomizedSearchCV(dtc, param_distributions = dtc_grid, cv = KFold(n_splits = 10), scoring = 'accuracy')","23a2db91":"dtc_rcv.fit(xtrain_scaled, ytrain)","6dd57a7f":"dtc_rcv.best_score_","99e9ba04":"dtc_rcv.best_params_","be87837f":"yhat_dtc = dtc_rcv.predict(xtest_scaled)","ad9fe46d":"print(accuracy_score(yhat_dtc, ytest), confusion_matrix(yhat_dtc, ytest),\n      classification_report(yhat_dtc, ytest), sep = '\\n\\n\\n')","29be189a":"dtc_score = accuracy_score(yhat_dtc, ytest)","8ddb6362":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(xtrain_scaled, ytrain)\nyhat_svc = svc.predict(xtest_scaled)","9c32832b":"print(accuracy_score(yhat_svc, ytest), confusion_matrix(yhat_svc, ytest),\n      classification_report(yhat_svc, ytest), sep = '\\n\\n')","f0ec8c90":"svc_score = accuracy_score(yhat_svc, ytest)","4cfc6e81":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(xtrain_scaled, ytrain)\nrfc.score(xtest_scaled, ytest)","2bfc8876":"random_grid = {'n_estimators' : range(100,1000,100),\n              'max_depth' : range(2,50,2),\n              'min_samples_split' : range(2,100,7),\n              'min_samples_leaf' : range(2,100,7)}\nrfc_rcv = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, \n                             cv = KFold(n_splits = 5),scoring = 'accuracy', verbose = 2)","4ae0205b":"rfc_rcv.fit(xtrain_scaled, ytrain)","762498d8":"rfc_rcv.best_params_","767a1432":"yhat_rfc = rfc_rcv.predict(xtest_scaled)\nprint(accuracy_score(yhat_rfc, ytest), confusion_matrix(yhat_rfc, ytest),\n      classification_report(yhat_rfc, ytest), sep = '\\n\\n')","9e6af4af":"rfc_score = accuracy_score(yhat_rfc, ytest)","03df2ec3":"scores = [logreg_score, dtc_score, svc_score, rfc_score, knn_score]\nscores_df = pd.DataFrame(scores, index = ['Logistic Regression', 'Decision Tree', 'Support Vector Classifier', 'Random Forest', 'K-nearest neighbors'])\nscores_df.rename(columns = {0 : 'Scores'}, inplace = True)","335e6a57":"scores_df.plot(kind = 'barh')","bf8994a0":"This has been the best predictor so far as it is classifying the non-drinkable water well and has a higher overall accuracy.","99a41759":"**Checking just the accuracy of our model will not be important in this case. It is more important that we classify the water that is not drinkable properly, because if our model classifies non drinkable water as drinkable it will be very problematic.**","d80a9c12":"### Random Forest Classifier","4a900a38":"There are even more hyperparameters which we can tune to make the model better so instead of tuning them one by one we will use hyperparameter tuning using RandomizedSearchCV","c5a7c3b1":"### Splitting data into train and test sets","16434e2c":"# Data visualization","ded6bb37":"# Data Preprocessing","03aa1b17":"In some of the plots we can see that for potable water the distribution curve is more spread out (higher standard deviation), so we can observe from these plots that lesser values of sulfate, hardness and solids means more chance of water being potable.\n\nAlso in the pH plots we see that potable water has a higher peak, which means that most values of pH for drinkable water lie between 6 and 8.","75ede5cf":"### Support Vector Classifier","c747cf0d":"# Model Selection","e8cad642":"### Decision Tree","890ccac4":"The logistic regression is not classifying properly so we will try another model","20ab1e06":"# Handling missing values","520add03":"So choosing the Support Vector Classifier or the Random Forest Classifier would be the best choice for our case.","83390b35":"### Standardizing the data","2bd75db1":"Since the solids graph is a little bit skewed , we will apply a transformation to fix it.\n","6b6cfcf6":"### K-nearest nieghbours","6aae1195":"### Logistic Regression Model","79eec1c6":"So these are the best hyperparameters for our dataset, we can see that classification was a bit better on this model but it is still not ideal.\n","dcd9ce40":"From the above **pairplot** and **correlation heatmap** we see that almost all of the columns have pearson correlation values less than **0.1** or greater than **-0.15** with one another, which is good for us as we do not have to deal with **Multicollinearity**","b63db1e6":"The decision tree model is a bit better , even though the accuracy score is lesser than the logreg model, it is atleast acknowledging both the classes.\nIn order to make this model better we will try tuning some hyperparameters like max_depth","fa0ff7f3":"# EDA","2fc8bda3":"KNN model is better than logistic regression model, but it still does not make correct prediction about the class 1.","d34850a7":"Here 1 means water is potable(safe for human consumption) and 0 means water is not potable","26f3cdcb":"Lets see if changing some hyperparameters makes a postive change to our model."}}