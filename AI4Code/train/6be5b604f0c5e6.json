{"cell_type":{"4f0bce09":"code","157e149e":"code","83483851":"code","cfe4301c":"code","17a6251d":"code","8faab1bb":"code","d10349f0":"code","e091717f":"code","fc07c4be":"code","844d9f9f":"code","5d1554bb":"code","dde21a53":"code","ccb6afdb":"code","8a071264":"code","8fd3b0be":"code","c3ca16f9":"code","939d1bdf":"code","4d480c10":"code","20aa5228":"code","6b8c9016":"code","f42adefb":"code","5ccbbe37":"code","517aefd7":"code","34539596":"code","3a82c7be":"code","a1edd500":"code","7cbda252":"code","cdb5cb03":"code","588b9f88":"code","bce208fb":"code","b71683fb":"code","9ab76676":"code","8eace08d":"code","cae5f42a":"code","f7a4c4fb":"code","2212fcc1":"code","4f01177a":"code","13efcc07":"code","b7abb629":"code","d2618fdf":"code","7fbc12c6":"code","3cd1500d":"code","108b0b96":"code","1c1f0ce9":"code","c32334a3":"code","adf3fb4c":"code","99b8c094":"code","7405e364":"code","102178bb":"code","a19b11f3":"code","f6146370":"code","e8a5bd27":"code","2c86f4a1":"code","2f287f51":"code","3ef00bd4":"code","0f1b40b5":"code","9e1c49ed":"code","6d861a48":"code","7f0446ed":"code","b2eb937b":"code","f45f00e1":"code","61c9251b":"code","823e08e0":"code","a9d5a660":"code","b6fdce92":"code","9bbc38c9":"code","bc2e1eff":"markdown","dd77cace":"markdown","c3a1f956":"markdown","11da7814":"markdown","a34bc1b6":"markdown","c41dc90a":"markdown","a99d13ca":"markdown","84ecd812":"markdown","a9ed2cff":"markdown","95ab292c":"markdown","5850b27b":"markdown","34d0f487":"markdown","e3da19bd":"markdown","bb579247":"markdown","b68ee779":"markdown","df417bfa":"markdown","5788a2bf":"markdown","43fdb59a":"markdown"},"source":{"4f0bce09":"#!pip install scikit-learn-intelex","157e149e":"#from sklearnex import patch_sklearn\n#patch_sklearn()","83483851":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\nimport lightgbm as lgb \n#import xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport dateutil.easter as easter\nimport optuna\nimport math","cfe4301c":"LINEAR_DATE_AUG = False\n\nOPTUNA = False\nNUM_TRIALS = 200\nSEED = 5\n\n#Holidays\nHOLIDAYS = False     \nNEXT_HOLIDAY = False  \n\nSEASONS = False\nWEATHER = False \n\nLAG_FEATURES = True\n\nPOST_PROCESSING = False\nMODEL_TYPE = \"Catboost\"\n\nVAL_SPLIT = \"2017-12-31\" #\"2018-05-31\"\n","17a6251d":"EPOCHS = 10000     \nEARLY_STOPPING = 30\n\n###########Come back to this   ---Doesnt work in CATBOOST (need to apply it)\nSCALER_NAME = \"MinMax\"  #None MinMax  \nSCALER = MinMaxScaler() ","8faab1bb":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col = 0)\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\nsub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)\n\ntest_index = test_df.index\ntrain_index = train_df.index\n\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\nif HOLIDAYS:\n    holidays = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\",usecols = [\"Date\",\"Country\",\"Name\"]                      )\n    holidays.rename(columns = {\"Date\":\"date\",\"Country\":\"country\",\"Name\":\"holiday\"},inplace= True)\n    holidays[\"holiday\"]= 1\n    holidays[\"holiday\"]= holidays[\"holiday\"].astype(\"int32\")\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])","d10349f0":"df_weather = pd.read_csv('..\/input\/finland-norway-and-sweden-weather-data-20152019\/nordics_weather.csv', parse_dates=['date'])","e091717f":"#Make date\ntrain_df[\"date\"] = pd.to_datetime(train_df[\"date\"])\ntest_df[\"date\"] = pd.to_datetime(test_df[\"date\"])","fc07c4be":"train_df.head()","844d9f9f":"if SEASONS:\n    print(\"Adding Seasons \")\n    seasons = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1]\n\n    month_to_season = dict(zip(range(1,13), seasons))\n\n    train_df[\"season\"] = train_df[\"date\"].dt.month.map(month_to_season)\n    test_df[\"season\"] = test_df[\"date\"].dt.month.map(month_to_season)","5d1554bb":"if WEATHER:\n    w_feats = ['country', 'date', 'tavg','precipitation']\n    #w_feats = ['country', 'date', 'precipitation', 'snow_depth', 'tavg', 'tmax','tmin']\n    print(\"Adding weather\")\n    train_df = train_df.merge(df_weather[w_feats], on=['date', 'country'], how='left')\n    train_df.index = train_index \n    test_df = test_df.merge(df_weather[w_feats], on=['date', 'country'], how='left')\n    test_df.index = test_index ","dde21a53":"def public_hols(df):\n    df = pd.merge(df, holidays, how='left', on=['date', 'country'])\n    df.fillna(value = 0,inplace=True)\n    \n    return df\n\nif HOLIDAYS:\n    train_df = public_hols(train_df)\n    test_df = public_hols(test_df)","ccb6afdb":"def get_gdp(row):\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]","8a071264":"'''def engineer(df):\n    #get GDP from file \n    gdp_exponent = 1.2121103201489674 # see https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model for an explanation\n    def get_gdp(row):\n        \"\"\"Return the GDP based on row.country and row.date.year\"\"\"\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country] ** gdp_exponent\n    \n    df[\"day\"] = df[\"date\"].dt.day\n    df['week'] = df.date.dt.week \n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"year\"] = df[\"date\"].dt.year\n    \n    #play around with if Tree model \n    #df['dayofyear'] = df['date'].dt.dayofyear                ### This can cause noise \n    df['inverse_dayofyear'] = 365 - df['date'].dt.dayofyear    \n    #df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"] = df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"]+1     #Leap year in 2016   \n    #df['daysinmonth'] = df['date'].dt.days_in_month          ## Bad for all except Lightgbm\n\n    df['quarter'] = df.date.dt.quarter\n    df['weekday'] = df.date.dt.weekday\n    df['day_of_week'] = df.date.dt.dayofweek  \n    df['day_of_year'] = df.date.dt.dayofyear  \n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_month'] = df.date.dt.days_in_month  \n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    df['is_friday'] = np.where((df['weekday'] == 4), 1, 0)\n    \n    if LINEAR_DATE_AUG:\n        for country in ['Finland', 'Norway']:\n            df[country] = df.country == country\n        df['KaggleRama'] = df.store == 'KaggleRama'\n        for product in ['Kaggle Mug', 'Kaggle Sticker','Kaggle Hat']:#Kaggle Hat\n            df[product] = df['product'] == product\n        df.drop([\"country\",\"store\",\"product\"], axis =1, inplace = True)\n\n        # Seasonal variations (Fourier series)\n        dayofyear = df.date.dt.dayofyear\n        for k in range(1, 3):\n            new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n            new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n            new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n            new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n            new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n            new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    return df'''","8fd3b0be":"'''def engineer_more(df):\n\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)})],\n                       axis=1)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) \n                                      for d in range(1, 13)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10)) + list(range(17, 25))})],\n                       axis=1)\n    \n    # June\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) \n                                      for d in list(range(6, 14))})],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-5, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(0, 10))})],\n                       axis=1)\n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n        \n    return new_df\n\ntrain = engineer_more(train_df)\ntrain['num_sold'] = train.num_sold.astype(np.float32)\ntest = engineer_more(test_df)\ntest.index = test_index\n\n\nfeatures = test.columns\nprint(list(features))'''","c3ca16f9":"# Feature engineering\n\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv',\n                    index_col='year')\ngdp_exponent = 1.2121103201489674 # see https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model for an explanation\ndef get_gdp(row):\n    \"\"\"Return the GDP based on row.country and row.date.year\"\"\"\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country] ** gdp_exponent\n\nle_dict = {feature: LabelEncoder().fit(train_df[feature]) for feature in ['country', 'product', 'store']}\n\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    new_df = pd.DataFrame({'gdp': df.apply(get_gdp, axis=1),\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    new_df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), 'dayofyear'] += 1 # fix for leap years\n    \n    for feature in ['country', 'product', 'store']:\n        new_df[feature] = le_dict[feature].transform(df[feature])\n        \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df['days_from_easter'] = (df.date - easter_date).dt.days.clip(-3, 59)\n    new_df.loc[new_df['days_from_easter'].isin(range(12, 39)), 'days_from_easter'] = 12 # reduce overfitting\n    #new_df.loc[new_df['days_from_easter'] == 59, 'days_from_easter'] = -3\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    return new_df\n\ntrain = engineer(train_df)\ntrain['date'] = train_df.date # used in GroupKFold\ntrain['num_sold'] = train_df.num_sold.astype(np.float32)\ntrain['target'] = train['num_sold'] \/ train['gdp']\ntest = engineer(test_df)\ntest['date'] = test_df.date ","939d1bdf":"#NB catboost can encode within model\ncategorical_feats = [\n    \"country\",\"store\",\"product\",\n                     #\"quarter\", \n                     #\"weekend_ind\"\n                    ]","4d480c10":"def next_Holiday(country, date):\n\n    countryHolidays = holidays[holidays[\"country\"] ==country][\"date\"]\n    try:\n        nextDate = min([day for day in countryHolidays if day >= date])\n    except ValueError:\n        #no next holiday\n        nextDate = date\n    return (nextDate - date).days\n\nif NEXT_HOLIDAY:\n    print(\"Applying Next Holiday\")\n    train['daysTillHoliday'] = train.apply(lambda x: next_Holiday(x['country'], x['date']), axis=1)\n    test['daysTillHoliday'] = test.apply(lambda x: next_Holiday(x['country'], x['date']), axis=1)","20aa5228":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","6b8c9016":"def scale_data(X_train, X_test= None, test=None,):\n     \n    scaler= SCALER\n    \n    #this can be X or X_train \n    X_train_s = scaler.fit_transform(X_train)\n\n    if X_test is None: #full train \n        test_s = scaler.transform(test)\n        return X_train_s, test_s\n    \n    else: # validation \n        X_test_s = scaler.transform(X_test)\n    \n    return   X_train_s , X_test_s","f42adefb":"def create_lag(DAYS,df):\n    df[f\"shift{DAYS}\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(DAYS,fill_value = 0)\n    return df","5ccbbe37":"def rolling_mean_std(roll_window, df):\n    shift_days=1\n    col_name = 'rolling_'+str(shift_days)+'_'+str(roll_window)   \n    df[col_name+\"_mean\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(shift_days).rolling(roll_window).mean()\n    df[col_name+\"_std\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(shift_days).rolling(roll_window).std()\n    df[col_name+\"_median\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(shift_days).rolling(roll_window).median()\n    #df[col_name+\"_max\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(shift_days).rolling(roll_window).max()\n    #df[col_name+\"_min\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(shift_days).rolling(roll_window).min()\n    #df[col_name+\"diff\"] = df[col_name+\"_max\"] - df[col_name+\"_min\"]\n    \n#     df[col_name+\"_mean\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(0).rolling(7).mean()    \n#     df[col_name+\"_std\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(0).rolling(7).std()\n#     df[col_name+\"_median\"] = df.groupby([\"store\",\"product\",\"country\"])[\"target\"].shift(0).rolling(7).median()\n    return df.fillna(0,inplace = True)","517aefd7":"def day_roll(df,day_shift,roll_window):\n    #Shift values and rolling mean\n    for day in days_shift:\n        create_lag(day,df)\n    \n    rolling_mean_std(roll_window,df)\n\n    return df","34539596":"#days_shift = [1, 2,3,4,5,7,14,30, 60]\ndays_shift = [i for i in range(1,30)]\nroll_window = 7","3a82c7be":"if LAG_FEATURES:\n    print(\"Running Lag features\")\n    train = day_roll(train,days_shift,roll_window)","a1edd500":"# Catboost does encoding in training \ntrain = pd.get_dummies(train,columns= categorical_feats)\ntest = pd.get_dummies(test,columns= categorical_feats)","7cbda252":"#train.index = train[\"date\"]\n#train.drop(\"date\",axis=1,inplace=True)\n#test.index = test[\"date\"]\n#test.drop(\"date\",axis=1,inplace=True)","cdb5cb03":"features = train.columns \nfeatures = features.drop([\"num_sold\",\"date\",\"target\",\"gdp\"])\n\nfeatures_base = test.columns\nfeatures_base = features_base.drop([\"date\",\"gdp\"])\n\nprint([i for i in features])","588b9f88":"train[[\"store\",\"product\",\"country\"]]= train_df[[\"store\",\"product\",\"country\"]]\ntest[[\"store\",\"product\",\"country\"]]= test_df[[\"store\",\"product\",\"country\"]].values","bce208fb":"X = train[features]\ny= train[\"target\"]","b71683fb":"X_train = train[train[\"date\"]<=VAL_SPLIT][features]\nX_test = train[train[\"date\"]>VAL_SPLIT][features]\ny_train= train[train[\"date\"]<=VAL_SPLIT][\"target\"]\ny_test= train[train[\"date\"]>VAL_SPLIT][\"target\"]","9ab76676":"params = {\n'loss_function': 'Poisson',\n'eval_metric': 'Poisson',\n'learning_rate': 0.04082853714917523,\n'l2_leaf_reg': 0.11616644663434757,\n'depth': 3,\n'boosting_type': 'Plain',\n'bootstrap_type': 'Bernoulli',\n'min_data_in_leaf': 16,\n'one_hot_max_size': 9,\n'subsample': 0.9756940817316571,\n'od_type': 'Iter',       # type of overfitting detector\n'od_wait': 40,\n'has_time': True         # use the order of the data (ts), do not permute\n}\n\n'''# NEw Optuna AWS \nparams = {'loss_function': 'Tweedie:variance_power=1.2337418466853298',\n 'eval_metric': 'Tweedie:variance_power=1.1443753953439761',\n 'learning_rate': 0.06799537783216487,\n 'l2_leaf_reg': 0.35748081310296065,\n 'depth': 5,\n 'boosting_type': 'Plain',\n 'bootstrap_type': 'Bernoulli',\n 'min_data_in_leaf': 8,\n 'subsample': 0.31307811702026744 \n}'''\n","8eace08d":"'''params0 = {'objective': 'regression', # Manual optimization\n           'force_row_wise': True,\n           'max_bin': 400, # need more bins than days in a year\n           'verbosity': -1,\n           'seed': 1,\n           'bagging_seed': 3,\n           'feature_fraction_seed': 2,\n           'learning_rate': 0.018,\n           'lambda_l1': 0,\n           'lambda_l2': 1e-2,\n           'num_leaves': 18,\n           'feature_fraction': 0.710344827586207,\n           'bagging_fraction': 0.47931034482758617,\n           'bagging_freq': 3,\n           'min_child_samples': 20}\n\nmodel = lgb.LGBMRegressor(**params0, n_estimators = 2000)\nmodel.fit(X_train[features_base],np.log1p(y_train), eval_set= [(X_test[features_base],np.log1p(y_test))])\nval_preds = np.expm1 (model.predict(X_test))\nsmape = SMAPE(y_test,val_preds)'''","cae5f42a":"def fit_model(X_train,y_train, seed_num, params,X_test= None,y_test = None, test_df= None):\n    model = CatBoostRegressor(**params,\n                      iterations=EPOCHS,\n                  #cat_features=categorical_feats, \n                      random_seed=seed_num\n                 )\n\n    \n    if test_df is None:     \n        scaler = SCALER\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n        model.fit(\n            X_train, np.log1p(y_train),\n            verbose=0\n            )\n\n        val_preds = np.expm1 ( model.predict(X_test))\n        smape = SMAPE(y_test,val_preds)\n        print(\" SMAPE:\", smape)\n        \n        return val_preds,  smape, model\n        \n    else: \n        scaler = SCALER\n        X_train = scaler.fit_transform(X_train)\n        test_df = scaler.transform(test_df)\n\n        model.fit(\n            X_train, np.log1p(y_train),\n            verbose=0\n            )\n        \n        test_preds = np.expm1 ( model.predict(test_df))\n\n        return test_preds","f7a4c4fb":"val_preds, smape, model_train = fit_model(X_train,y_train, 0, params,X_test,y_test)\nval_preds = val_preds*train[train[\"date\"]>VAL_SPLIT][\"gdp\"]","2212fcc1":"feature_importance = pd.DataFrame({'feature_importance': model_train.get_feature_importance(), 'feature_names': X_train.columns}).sort_values(by=['feature_importance'],\n                                                                                                                        ascending=False)\nplt.figure(figsize=(25,10))\nplt.bar(x = feature_importance['feature_names'], height=feature_importance[\"feature_importance\"])\nplt.xticks(rotation = 90)\nplt.show()","4f01177a":"res_base = train[train[\"date\"]>VAL_SPLIT][\"num_sold\"] - val_preds \n\nplt.figure(figsize=(25,10))\nsns.lineplot(x = train.loc[X_test.index,\"date\"], y = res_base,ci = None)\nplt.axhline(0, c=\"r\")\n#sns.lineplot(x = train.loc[X_test.index,\"date\"], y = train[\"num_sold\"],ci = None )\nplt.show()","13efcc07":"plt.figure(figsize=(25,10))\nsns.scatterplot(x =train[train[\"date\"]>VAL_SPLIT][\"num_sold\"], y = val_preds)\nplt.show()","b7abb629":"# Fit on Full Train data\ntest_preds = fit_model(X[features_base],y, 0, params, test_df= test[features_base])\n\nsub_base = sub.copy(deep= True)\nsub_base[\"num_sold\"]  = test_preds * test[\"gdp\"]\nsub_base[\"num_sold\"] = sub_base[\"num_sold\"].round()\nsub_base.to_csv(\"sub_base.csv\")\nsub_base","d2618fdf":"plt.figure(figsize=(25,10))\n\nsns.lineplot(x = test[\"date\"], y = sub_base[\"num_sold\"])\nsns.lineplot(x = train[\"date\"], y = train[\"num_sold\"])\nplt.title(\"Full base model predictions \")\nplt.show()","7fbc12c6":"def seed_CV(X_train,y_train, params,X_test=None, y_test=None, test_df = None):\n    \n    if test_df is not None:\n        seed_preds = np.zeros((SEED, len(test_df)))\n    else:\n        smape_score = []\n        seed_preds = np.zeros((SEED, len(y_test)))\n    \n    for i in range(SEED):\n        print(f\"##### SEED {i} #######\")\n        \n        if test_df is None:\n            val_preds, smape, model_train = fit_model(X_train,y_train, i, params,X_test,y_test)\n            smape_score.append(smape)\n            seed_preds[i] = val_preds\n\n        else: \n            test_preds = fit_model(X_train, y_train, i,params, test_df= test_df)\n            seed_preds[i] = test_preds            \n    \n    if test_df is None:\n        print(\"\\nFinal SMAPE:\",np.mean(smape_score))\n        return np.mean(smape_score), np.mean(seed_preds,axis =0)\n        \n    return np.mean(seed_preds,axis =0)","3cd1500d":"smape , seed_train_preds  = seed_CV(X_train, y_train, params, X_test, y_test , test_df =None)","108b0b96":"# Fit on Full Train data - NB !!!! this excludes lag and shift - therefore less accurate than CV above\nseed_test_preds  = seed_CV(X[features_base], y, params , test_df =test[features_base])\nseed_test_preds","1c1f0ce9":"sub_base_seed = sub.copy(deep= True)\nsub_base_seed[\"num_sold\"]  = seed_test_preds* test[\"gdp\"]\nsub_base_seed[\"num_sold\"] = sub_base_seed[\"num_sold\"].round()\nsub_base_seed.to_csv(\"sub_base_seed.csv\")\nsub_base_seed","c32334a3":"from sklearn.model_selection import GroupKFold","adf3fb4c":"'''cv = GroupKFold(n_splits=4)\n\nsmape_score = []\nfor idx, (train_idx,val_idx) in enumerate (cv.split(train, groups=train.date.dt.year)):\n    X_train , X_test = train.iloc[train_idx] ,train.iloc[val_idx]\n    y_train , y_test = train.iloc[train_idx][\"target\"] , train.iloc[val_idx]['target']\n\n    print(f\"\\n Training: {X_train.date.dt.year.unique()}, Predicting: {X_test.date.dt.year.unique()}\" )\n\n    smape , seed_train_preds  = seed_CV(X_train[features], y_train, params, X_test[features], y_test , test_df =None)\n    \n    smape_score.append(smape)\nprint(\"\\nFinal Smape\", np.mean(smape_score))'''","99b8c094":"'''plt.figure(figsize=(25,10))\nsns.lineplot(x= X_test[\"date\"], y = seed_train_preds* test[\"gdp\"],ci = None)\nsns.lineplot(x= X_test[\"date\"], y = y_test, ci = None)'''","7405e364":"#all_df [  (all_df[\"date\"]>= start_date -delta) & (all_df[\"date\"]< start_date+delta) ][[\"store\",\"country\",\"product\",\"date\",'rolling_0_7_std']]","102178bb":"FREQUENCY = 1 #days to predict --each  loop \nstart_date = min(test[\"date\"]) \nend_date = max(test[\"date\"])","a19b11f3":"def multi_step_recursive(start_date, end_date, freq, sub, train_i, test_i):\n    delta = pd.DateOffset(days = 1)\n\n    all_df = pd.concat([train_i.assign(ds=\"a\"),test_i.assign(ds=\"b\")],axis =0)\n    \n    #Shift values and rolling \n    if LAG_FEATURES:\n        day_roll(all_df,days_shift,7)\n\n        \n    while start_date <= end_date:\n\n        #Select slice to predict\n        test_split = all_df [  (all_df[\"date\"]>= start_date ) & (all_df[\"date\"]< start_date+delta) ][features]\n        \n        #display(test_split)\n        \n        \n        X = all_df[ all_df[\"date\"]< start_date]\n        y = all_df[ all_df[\"date\"]< start_date][\"target\"]\n        \n        test_preds = fit_model(X[features],y, 0, params, test_df= test_split)\n        \n        #Add predicted-test data to X and predicted-target to y\n        all_df.loc[test_split.index, \"target\"]  = test_preds\n        \n        sub.loc[test_split.index , \"num_sold\"] = test_preds\n    \n        #Shift values and rolling \n        if LAG_FEATURES:\n            day_roll(all_df,days_shift,7)\n\n        #update start date\n        start_date += delta\n        \n    return sub  ,smape","f6146370":"sub_recursive, seed_smape  = multi_step_recursive(start_date, end_date, FREQUENCY, sub.copy(deep=True), train, test)\nsub_recursive","e8a5bd27":"sub_recursive[\"num_sold\"] =sub_recursive[\"num_sold\"]* test[\"gdp\"]\nsub_recursive[\"num_sold\"] =sub_recursive[\"num_sold\"].round()\nsub_recursive.to_csv(\"sub_recursive.csv\")\nsub_recursive","2c86f4a1":"plt.figure(figsize=(20,10))\n\n#sns.lineplot(data= train[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data = sub_base,x = test[\"date\"] , y = \"num_sold\", label =\"Base prediction\" ,ci=None)\nsns.lineplot(data = sub_base_seed,x = test[\"date\"] , y = \"num_sold\", label =\"Seed prediction\" ,ci=None)\nsns.lineplot(data = sub_recursive,x = test[\"date\"] , y = \"num_sold\", label =\"Recursive prediction\" ,ci=None)","2f287f51":"def split_models(split_on, sub_df, train ,test):    \n\n    split_smape=0\n    \n    # split training on product\/ store\/ country\n    for split in train[split_on].unique():\n        print(f\"\\nPredicting for {split_on} {split}\")\n\n        train_split= train[train[split_on] ==split]\n        test_split =test[test[split_on] ==split]\n        \n        #train on Full dataset\n        final_predictions , smape = multi_step_recursive(start_date, end_date, FREQUENCY, sub_df.loc[test_split.index], train_split, test_split, feature_list= features)\n        split_smape += smape\/train[split_on].nunique()\n        \n        sub_df.loc[test_split.index,\"num_sold\"] = final_predictions[\"num_sold\"]\n\n    print(f\"\\n Final mean smape:\",split_smape)\n    \n    return split_smape, sub_df, model","3ef00bd4":"# store_smape, sub_store, model = split_models(\"store\", sub.copy(deep=True), train ,test)\n# sub_store","0f1b40b5":"# product_smape, sub_product, model = split_models(\"product\", sub.copy(deep=True), train ,test)","9e1c49ed":"# sub_product","6d861a48":"# country_smape, sub_country, model = split_models(\"country\", sub.copy(deep=True), train ,test)","7f0446ed":"import itertools\nall_splits = list(itertools.product(['KaggleMart', 'KaggleRama'],['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker'],['Finland', 'Norway', 'Sweden']))","b2eb937b":"def split_models_ALL(split_on, sub_df):    \n\n    split_smape=0\n    train[\"preds\"]= 0\n    split_dict = {}\n\n    # split training on product\/ store\/ country\n    for split in split_on:\n        print(f\"\\nPredicting for store: {split[0]}, product: {split[1]}, country: {split[2]} \")\n\n        train_split= train[ (train[\"store\"] == split[0]) & (train[\"product\"] == split[1]) & (train[\"country\"] == split[2])]\n        test_split =test[ (test[\"store\"] == split[0]) & (test[\"product\"] == split[1]) & (test[\"country\"] == split[2])]\n\n        X_train = train_split.iloc[train_split.index<=VAL_SPLIT,:].drop(\"num_sold\", axis=1)\n        X_test = train_split.iloc[train_split.index>VAL_SPLIT,:].drop(\"num_sold\", axis=1)\n        y_train= train_split.iloc[train_split.index<=VAL_SPLIT,:][\"num_sold\"]\n        y_test= train_split.iloc[train_split.index>VAL_SPLIT,:][\"num_sold\"]\n\n\n        #run model for each split type\n        smape, test_preds, train_preds  = seed_CV(X_train,y_train, X_test, y_test, test_split, params)\n        print(f\"\\n split smape:\",smape)\n\n        split_smape += smape\/len(all_splits)\n        sub_df.loc[test_split.index,\"num_sold\"] = test_preds\n        split_dict[split] = smape\n        \n\n    print(f\"\\n final all_split smape:\",split_smape)\n    \n    return split_smape, sub_df , split_dict","f45f00e1":"#sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)\n#all_smape, all_sub, split_dict = split_models_ALL(all_splits, sub.copy(deep=True))","61c9251b":"'''print(\"  store ,       product ,       country:  score\")\nprint(\"--------------------------\")\nfor i in split_dict.items():\n    print(i[0],\":\",'{:.2f}'.format(i[1]))'''","823e08e0":"# print(\"One Shot Smape :\",seed_smape )\n# print(\"Store SMAPE  :\",store_smape )\n# print(\"Product SMAPE :\",product_smape )\n# print(\"Country SMAPE :\",country_smape )","a9d5a660":"'''plt.figure(figsize=(20,10))\n\n#sns.lineplot(data= train[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data = sub_base,x = test[\"date\"] , y = \"num_sold\", label =\"Base prediction\" ,ci=None)\nsns.lineplot(data = sub_store,x = test[\"date\"] , y = \"num_sold\", label =\"Store split prediction\" ,ci=None)\nsns.lineplot(data = sub_recursive,x = test[\"date\"] , y = \"num_sold\", label =\"Recursive split prediction\" ,ci=None)\nsns.lineplot(data = sub_product, x= test[\"date\"] , y = \"num_sold\", label =\"Product split prediction\" ,ci=None)\nsns.lineplot(data = sub_country, x= test[\"date\"] , y = \"num_sold\", label =\"Country split prediction\" ,ci=None)\n#sns.lineplot(data = all_sub, x= test[\"date\"] , y = \"num_sold\", label =\"All  split prediction\" ,ci=None)\n\nplt.show()'''","b6fdce92":"'''plt.figure(figsize=(25,10))\n\nsns.lineplot(data= train[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data =all_sub, x= test1[\"date\"], y = \"num_sold\",label=\"Final Predictions\" ,ci=None) \nplt.title(\"Actual and Predicted Sales\")\n\nplt.show()'''","9bbc38c9":"# sub_recursive.to_csv(\"sub_recursive.csv\")\n\n# sub_country.to_csv(\"submission_country.csv\")\n# sub_product.to_csv(\"submission_product.csv\")\n# sub_store.to_csv(\"submission_store.csv\")\n","bc2e1eff":"## Residual A","dd77cace":"# Libraries\n\nSpeed up sklearn as per https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298241","c3a1f956":"# Models ","11da7814":"# Load Data","a34bc1b6":"### Note: \n* Increase variance over time - log  \/ box cox transformation could help ","c41dc90a":"# Functions ","a99d13ca":"# All Together - store.product & country Split\nThis is 18 Models, 1 for each variation of store, product and country","84ecd812":"# Shift Days ","a9ed2cff":"# Group Cross Validation \nAs per [ambrosm](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart#Re-training,-inference-and-submission)","95ab292c":"# Set Features ","5850b27b":"# Split and Scale","34d0f487":"# Submission ","e3da19bd":"# Multi Step by Store\/Product \/ Country ","bb579247":"### One HotEncoder and date index ","b68ee779":"# Training Visualization","df417bfa":"All values are in April and Dec which aligns to Christmas and Easter \nAlso mostly Kaggle Ramma and sales of hats","5788a2bf":"## Random Seeding\nVariation of [adamwurdits](https:\/\/www.kaggle.com\/adamwurdits\/tps-01-2022-catboost-w-optuna-seed-averaging) ","43fdb59a":"# Multi-Step Recursive Seed"}}