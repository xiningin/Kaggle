{"cell_type":{"e007a74e":"code","40ca5296":"code","cdbbac33":"code","8f4b3702":"code","aa6d5e61":"code","af41f7a2":"code","36f20e71":"code","dd5c8ad9":"code","35f93b03":"code","1f373820":"markdown","4fe1a320":"markdown","fb59c4d9":"markdown","9f644a7d":"markdown","aeff7263":"markdown","69078646":"markdown","cdfbd807":"markdown","9082a64c":"markdown","e77f4639":"markdown","c69e6107":"markdown","7a49b10a":"markdown","b3e6dedb":"markdown","26deb5eb":"markdown","dbcf28a5":"markdown","be9b8d82":"markdown","7cf4d1e9":"markdown","9a0a8504":"markdown","262b5759":"markdown","23f8e3f1":"markdown","a9cb75ce":"markdown","890a675f":"markdown","70d277a2":"markdown","b1a01b44":"markdown"},"source":{"e007a74e":"#Numpy is used so that we can deal with array's, which are necessary for any linear algebra\n# that takes place \"under-the-hood\" for any of these algorithms.\nimport numpy as np\n\n#Pandas is used so that we can create dataframes, which is particularly useful when\n# reading or writing from a CSV.\nimport pandas as pd\n\n#Matplotlib is used to generate graphs in just a few lines of code.\nimport matplotlib.pyplot as plt\n\n#Import the classes we need to test linear, ridge, and lasso to compare\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV\n\n#Need these for selecting the best model\nfrom sklearn.model_selection import cross_val_score\n\n#These will be our main evaluation metrics \nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\n\n#import CatBoost\nfrom catboost import CatBoostClassifier, FeaturesData","40ca5296":"#read the data from csv\ndataset = pd.read_csv('..\/input\/churn-modeling\/Churn_Modelling.csv')\n\n#take a look at our dataset.  head() gives the first 5 lines. \ndataset.head()","cdbbac33":"#Grab X, ignoring row number, customer ID, and surname\nX = dataset.iloc[:, 3:13]\n\n#grab the output variable\ny = dataset.iloc[:, 13]\n\n#take a look\nX[0:10]","8f4b3702":"#create the indexes \ncategorical_features_indices = np.array([1,2])","aa6d5e61":"#split the datasets, leaving 20% for testing.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","af41f7a2":"#Create the Model\nmodel = CatBoostClassifier(iterations = 50000, \n                                learning_rate=0.20,\n                                depth=8,\n                                loss_function='Logloss',\n                                subsample = 0.8,\n                                custom_loss = ['AUC'] )\n\n\n#Fit the Model passing in our categorical Feature indexes.\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices)","36f20e71":"# Predicting the Test set results\ny_pred = model.predict(X_test)","dd5c8ad9":"cm = confusion_matrix(y_test, y_pred)\ncm","35f93b03":"Cat_preds1 = model.predict_proba(X_test)\nCat_class1 = model.predict(X_test)\nCat_score1 = roc_auc_score(y_test, Cat_preds1[:,1])\nprint(\"ACCURACY: {:.4f}\".format(Cat_score1))\n\n","1f373820":"This meanas that we got 1479 correct when the outcome was expected to be Did not Exit, and 218 correct when the outcome should have been \"Exited\".\n\nWe made 187 mistakes by predicting \"Exited\" when it should have been \"Did Not Exit\".\n\nWe made 116 mistakes by predicting \"Did not Exit\" when it should have been \"Exit\".\n\nWe can get a better idea of accuracy by using cross validation. ","4fe1a320":"> # Enough to be Dangeous: CatBoost\n\n> ### This is the 8th notebook of my **\"Enough to be Dangeous\"** notebook series\n\nSee the other notebooks here:\n\n[Simple Linear regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangeous-simple-linear-regression)\n\n[Multiple Linear Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-multiple-linear-regression)\n\n[Polynomial Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-polynomial-regression)\n\n[Decision Tree](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-decision-tree-regression)\n\n[Random Forest](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-random-forest-regression)\n\n[Lasso and Ridge Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-lasso-and-ridge-regression)\n\n[XGBoost](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-xgboost-regression)","fb59c4d9":"Here we can see that we obtained an 85.02% accuracy with just a few lines of code! \n","9f644a7d":"Next, it calculates the value in each new row by looking at previous examples with the same class, and counting the number of positive labels, then performing a calculation.\n\n![image.png](attachment:image.png)","aeff7263":"In other models, categorical variables are handled through \u201cOneHotEncoding\u201d which creates additional columns to capture the information.\n\n![image.png](attachment:image.png)","69078646":"We can now split our dataset into training and test sets.","cdfbd807":"## Conceptual Overview\n\nCatBoost, short for Category Boosting, is an algorithm that is based on decision trees and gradient boosting like XGBoost, but with ***even better performance!***\n\n![image.png](attachment:image.png)","9082a64c":"This captures additional valuable information, avoids ***\u201csparsity\u201d***, and speeds up computation.\n\n![image.png](attachment:image.png)","e77f4639":"We can see that we have some categorical and continuous variables here, and also a few columns we won't need.\n\nWe will NOT have to do any OneHotEncoding here, but I will drop a few of the identifier columns as they are redundant and unnecessary.","c69e6107":"To evaluate the results, we can get an idea with a confusion matrix first. \n\nThe diagonals tell us how much we got right and wrong, and the type of error.  ","7a49b10a":"Overall, CatBoost is an extremely fast, accurate, and innovative algorithm, yet somehow it is not as widely used as its predecessors like XGBoost.  So if you haven\u2019t already tried it, go implement it ASAP!","b3e6dedb":"> ## This notebook is separated into two parts:\n\n### **1) Conceptual Overview:**  I will introduce the topic in 200 words or less.\n\n### **2) Implementation:**  I will implement the algorithm in as few lines as possible.","26deb5eb":"## Implementation\n\nIn this section I will implement the code in its **simplest verison** so that it is understandable if you are brand new to machine learning. \n\nBelow we will predict salary based on the current role someone is in, and then make some predictions on start ups.  I will also include side by side examples showing how it performs compared to decision trees. \n\nThe first step is to start with \"imports\".","dbcf28a5":"We will need to tell the model where the categorical features are.","be9b8d82":"Now we can apply this to predict the results.","7cf4d1e9":"Next we will need to load our data.\n\nTo see the power of CatBoost, we need a more complex dataset with some categorical variables, so I will use a dataset on customer retention that has many predictor variables.  ","9a0a8504":"CatBoost does especially well with data containing ***\u201ccategorical variables.\u201d***\n\n![image.png](attachment:image.png)","262b5759":"CatBoost also ***automatically handles hyperparameter tuning*** and can even run on GPU\u2019s resulting in incredible speedups.\n\n![image.png](attachment:image.png)","23f8e3f1":"Next we do need to tell the model where the categorical features are so it can process them differently. ","a9cb75ce":"For each, it assigns a ***default value*** (\"prior\") for each class to the first few examples.\n\n![image.png](attachment:image.png)","890a675f":"To avoid overfitting, CatBoost builds new models at each step (n), by shuffling the rows and looking at n^2 previous examples.\n\n![image.png](attachment:image.png)","70d277a2":"Alternatively, CatBoost starts by shuffling the data, creating ***\u201cpermutations\u201d***.\n\n![image.png](attachment:image.png)","b1a01b44":"Then the model proceeds by building \u201csymmetric binary trees\u201d for each permutation of the data.\n\n![image.png](attachment:image.png)"}}