{"cell_type":{"faebca12":"code","fa523350":"code","45f50a30":"code","a46cccd8":"code","5921a603":"code","a9e6cb68":"code","0c66176f":"code","5fffce51":"code","dc54c93f":"code","8ed71da9":"code","9e7fc393":"code","7e235de1":"code","5286a411":"code","a3f8bd71":"code","613a9647":"code","6eae5fff":"code","254fc418":"code","3731744f":"code","0688e3a3":"code","076f0985":"code","fc1ab1a5":"code","89ede3b8":"code","f7698286":"code","1b802505":"code","c1c5b965":"code","79bbe40d":"code","06073a21":"code","eab781ad":"code","d23a0150":"code","501309c9":"code","424b549c":"code","ff702da3":"code","061815de":"code","9f57939d":"code","002927b5":"code","f02b6e82":"code","7b538abc":"code","7a5b3c10":"code","dc45b5c0":"code","ef396a28":"code","ef14b192":"code","3f74ad85":"code","f9dea513":"code","2daa513d":"markdown","02fac20f":"markdown","c32ffa17":"markdown","ce163d35":"markdown","aecfa0db":"markdown","3243c040":"markdown","45f8f826":"markdown"},"source":{"faebca12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', None)\n\nimport random\nimport re\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams.update({\n    'axes.labelsize': 18\n})\n\n%matplotlib inline","fa523350":"def read_csv(path):\n    return pd.read_csv(path)\n\ndef get_questions_by_org(cities_df):\n    gb = cities_df.groupby(['Organization', 'Year Reported to CDP'])\n    \n    data = gb.agg({\n        'Question Number': 'nunique'\n    }).reset_index()\n    \n    return data\n\ndef get_org_with_missing_bars(questions_by_year, organizations, columns=None):\n    \n    arr1 = []\n    arr2 = []\n    arr3 = []\n\n    for org in organizations:\n        years_missing = list(set([2018, 2019, 2020]) - set(questions_by_year[questions_by_year['Organization'] == org]['Year Reported to CDP'].values))\n        arr2.extend(years_missing)\n        arr1.extend([org] * len(years_missing))\n        arr3.extend([0] * len(years_missing))\n        \n    return pd.DataFrame(zip(arr1, arr2, arr3), columns=columns)","45f50a30":"def stacked_bar_plot(data, labels, legends, title, colors, **rcParams):\n    def autolabel(current_rects, sum_widths, xpos='center', color='white'):\n        for i, rect in enumerate(current_rects):\n            width = int(rect.get_width())\n            yloc = rect.get_y() + rect.get_height() \/ 2\n            \n            if width == 0:\n                continue\n            ax.annotate('{}'.format(width), xy=(width \/ 2 + sum_widths[i], yloc), color=color, weight='bold', \n                        size=10, ha='center', va='center')\n            \n    def get_sum_bars(index, bars):\n        sum_bars = np.zeros(len(bars[index]))\n        prev_bars = bars[0:index]\n        for vects in prev_bars:\n            for i, elem in enumerate(vects):\n                sum_bars[i] += elem\n                \n        return sum_bars\n            \n    rects = []\n        \n    if rcParams and 'figsize' in rcParams:\n        fig, ax = plt.subplots(figsize = rcParams['figsize'])\n    else:\n        fig, ax = plt.subplots(figsize = (12, 8))\n        \n    y_pos = np.arange(len(labels))\n    \n    for k, bar in enumerate(bars):\n            \n        rect1 = ax.barh(y_pos, bar, left=get_sum_bars(k, bars), color=colors[k], edgecolor='yellow')\n        rects.insert(k, rect1)\n\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(labels)\n        \n    \n    for i, rect in enumerate(rects):\n        sum_widths = np.zeros(len(rect))\n        for r in rects[0:i]:\n            for i, j in enumerate(r):\n                sum_widths[i] += j.get_width()\n            \n        autolabel(rect, sum_widths)\n\n    plt.title(title, fontsize=13)\n    plt.legend(legends)\n    plt.show()\n    \ndef autolabel(current_rects, sum_widths, xpos='center', color='white'):\n    for i, rect in enumerate(current_rects):\n        width = int(rect.get_width())\n        yloc = rect.get_y() + rect.get_height() \/ 2\n        ax.annotate('{}'.format(width), xy=(width, yloc), color=color, weight='bold', size=13, verticalalignment='center')\n    \n    \ndef display_bar_plot(bars, labels, title):\n    \n    fig, ax = plt.subplots(figsize = (10, 7))\n        \n    y_pos = np.arange(len(bars))\n    rects = ax.barh(y_pos, bars, color='#0504aa', alpha=0.7, edgecolor='blue')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(labels)\n    \n    autolabel(rects, [0] * len(rects))\n    plt.title(title, fontsize=14)\n    plt.show()\n    ","a46cccd8":"def create_ngrams(response, n):\n    tokens = response.split()\n    \n    ngrams = list()\n    for i in range(0, len(tokens) - 1):\n        ngrams.append(\" \".join(tokens[i:i + n]))\n        \n    return ngrams","5921a603":"SOURCE_PATH = \"..\/input\/cdp-unlocking-climate-solutions\/\"\nSUPPLE_PATH = \"..\/input\/cdp-unlocking-climate-solutions\/Supplementary Data\/\"\n\nCOLORS = ['#0504aa', '#34495E', '#A15BF0']","a9e6cb68":"!ls -l '\/kaggle\/input\/cdp-unlocking-climate-solutions\/Corporations\/Corporations Disclosing\/Water Security\/'","0c66176f":"# cities response\n\ncities_2018 = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Responses\/2018_Full_Cities_Dataset.csv\")\ncities_2019 = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Responses\/2019_Full_Cities_Dataset.csv\")\ncities_2020 = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Responses\/2020_Full_Cities_Dataset.csv\")\n\n# cities disclosing\n\ncities_dis_2018 = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Disclosing\/2018_Cities_Disclosing_to_CDP.csv\")\ncities_dis_2019 = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Disclosing\/2019_Cities_Disclosing_to_CDP.csv\")\ncities_dis_2020 = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Disclosing\/2020_Cities_Disclosing_to_CDP.csv\")\n\n# corporations response\n\ncr_ws_2018 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Responses\/Water Security\/2018_Full_Water_Security_Dataset.csv\")\ncr_ws_2019 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Responses\/Water Security\/2019_Full_Water_Security_Dataset.csv\")\ncr_ws_2020 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Responses\/Water Security\/2020_Full_Water_Security_Dataset.csv\")\n\ncr_cc_2018 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Responses\/Climate Change\/2018_Full_Climate_Change_Dataset.csv\")\ncr_cc_2019 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Responses\/Climate Change\/2019_Full_Climate_Change_Dataset.csv\")\ncr_cc_2020 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Responses\/Climate Change\/2020_Full_Climate_Change_Dataset.csv\")\n\n# Corporations Disclosing water security\n\ncr_dis_ws_2018 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Disclosing\/Water Security\/2018_Corporates_Disclosing_to_CDP_Water_Security.csv\")\ncr_dis_ws_2019 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Disclosing\/Water Security\/2019_Corporates_Disclosing_to_CDP_Water_Security.csv\")\ncr_dis_ws_2020 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Disclosing\/Water Security\/2020_Corporates_Disclosing_to_CDP_Water_Security.csv\")\n\n# Corporations Disclosing climate change\n\ncr_dis_cc_2018 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Disclosing\/Climate Change\/2018_Corporates_Disclosing_to_CDP_Climate_Change.csv\")\ncr_dis_cc_2019 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Disclosing\/Climate Change\/2019_Corporates_Disclosing_to_CDP_Climate_Change.csv\")\ncr_dis_cc_2020 = read_csv(f\"{SOURCE_PATH}\/Corporations\/Corporations Disclosing\/Climate Change\/2020_Corporates_Disclosing_to_CDP_Climate_Change.csv\")\n","5fffce51":"supp_cdp_rec = pd.read_excel(f\"{SUPPLE_PATH}\/Recommendations from CDP\/CDP_recommendations_for_supplementary_datasets_to_include.xlsx\")\n                             \nsupp_cdp_qs = pd.read_excel(f\"{SUPPLE_PATH}\/Recommendations from CDP\/CDP_recommendations_for_questions_to_focus_on.xlsx\")\n\nus_cities_mappings = read_csv(f\"{SUPPLE_PATH}\/Simple Maps US Cities Data\/uscities.csv\")\n\ncorp_locations = read_csv(f\"{SUPPLE_PATH}\/Locations of Corporations\/NA_HQ_public_data.csv\")\n\ncdc_VI_County = read_csv(f\"{SUPPLE_PATH}\/CDC Social Vulnerability Index 2018\/SVI2018_US_COUNTY.csv\")\n\ncdc_VI = read_csv(f\"{SUPPLE_PATH}\/CDC Social Vulnerability Index 2018\/SVI2018_US.csv\")\n\n# data dictionary\ndd = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Responses\/Full_Cities_Response_Data_Dictionary.csv\")\n\ndd_cities_dis = read_csv(f\"{SOURCE_PATH}\/Cities\/Cities Disclosing\/Cities_Disclosing_to_CDP_Data_Dictionary.csv\")","dc54c93f":"STOPS = stopwords.words('english')","8ed71da9":"def normalize_text(sentence):\n    \n    # 1. Split into sentences\n    sentences = sent_tokenize(sentence)\n    sentences = list(map(str.lower, sentences))\n    \n    return sentences","9e7fc393":"cities_df = pd.concat([cities_2018, cities_2019, cities_2020])                  # Cities Response\n\ncities_dis_df = pd.concat([cities_dis_2018, cities_dis_2019, cities_dis_2020])  # Cities Disclosure\n\ncr_ws_df = pd.concat([cr_ws_2018, cr_ws_2019, cr_ws_2020])                      # Corporations Response Water Security\n\ncr_cc_df = pd.concat([cr_cc_2018, cr_cc_2019, cr_cc_2020])                      # Corporations Response Climate Change\n\ncr_dis_ws_df = pd.concat([cr_dis_ws_2018, cr_dis_ws_2019, cr_dis_ws_2020])      # Corporations Disclosure Water Supply\n\ncr_dis_cc_df = pd.concat([cr_dis_cc_2018, cr_dis_cc_2019, cr_dis_cc_2020])      # Corporations Disclosure Climate Change","7e235de1":"cities_df_merged = cities_df.merge(cities_dis_df, on=['CDP Region', 'Country', 'Account Number', 'Organization', 'Year Reported to CDP'], how='left', suffixes=('', '_dis'))","5286a411":"sections_df = cities_df.groupby('Parent Section').agg({\n    'Section': 'unique'\n}).reset_index()","a3f8bd71":"temp = cities_df[cities_df['Question Number'] == '0.4']\n\nres = temp.groupby('Response Answer').agg({\n    'Organization': 'nunique',\n    'CDP Region': 'unique'\n}).reset_index()\n\nres['CDP Region'] = res['CDP Region'].apply(lambda x: \",\".join(x))","613a9647":"N = 10\nd = res.loc[random.sample(list(res.sort_values(by=['Organization'], ascending=[0]).index.ravel()[:N]), N)]\nbars = []\nbars.append(list(d['Organization'].values))\n\nstacked_bar_plot(bars, labels=d['Response Answer'], legends=['All'], title='# Organizations with currency', colors=COLORS)\n\ndel d\ndel N\ndel bars","6eae5fff":"def join_by_column_name(cities_df, used_cols):\n    grouped = cities_df.groupby('Parent Section')\n    \n    df = grouped.get_group('Introduction')\n    \n    df1 = df[df['Column Name'] == 'Current population']\n    df2 = df[df['Column Name'] == 'Projected population']\n    df3 = df[df['Column Name'] == 'Current population year']\n    df4 = df[df['Column Name'] == 'Projected population year']\n    \n    merged_df = df1[used_cols].merge(df2[used_cols], on=['Year Reported to CDP', 'Account Number'], suffixes=('_cur', '_proj'))\n    merged_df = merged_df.merge(df3[used_cols], on=['Year Reported to CDP', 'Account Number'])\n    merged_df = merged_df.merge(df4[used_cols], on=['Year Reported to CDP', 'Account Number'])\n    \n    merged_df = merged_df[filter_columns(merged_df.columns, used_cols)]\n    \n    merged_df.rename(columns={\n        'Response Answer_cur': merged_df['Column Name_cur'].iloc[0],\n        'Response Answer_proj': merged_df['Column Name_proj'].iloc[0],\n        'Response Answer_x': merged_df['Column Name_x'].iloc[0],\n        'Response Answer_y': merged_df['Column Name_y'].iloc[0]\n    }, inplace=True)\n    \n    merged_df.drop(columns=['Column Name_cur', 'Column Name_proj', 'Column Name_x', 'Column Name_y'], inplace=True)\n    \n    return merged_df\n\ndef filter_columns(all_cols, used_cols):\n    return [i for i in all_cols if i.split('_')[0] in used_cols]","254fc418":"cities_df = cities_df.sort_values(by=['Year Reported to CDP', 'Account Number', 'Question Number', 'Column Number', 'Row Number'])","3731744f":"grouped = cities_df.groupby(['Year Reported to CDP', 'Account Number', 'Question Number'])\n\ngrouped_sect = cities_df.groupby(['Parent Section'])","0688e3a3":"merged_df = join_by_column_name(cities_df, ['Year Reported to CDP', 'Account Number', 'Column Name', 'Response Answer'])\nmerged_df = merged_df.dropna()\n\nmerged_df = merged_df[~merged_df['Current population year'].isin(['216', '7', '19', '217'])]\nmerged_df['Current population'] = merged_df['Current population'].astype(float)\nmerged_df['Projected population'] = merged_df['Projected population'].astype(float)\n\ncity_names = cities_df.loc[cities_df['Account Number'].isin(merged_df['Account Number'])][['Account Number', 'Organization', 'Country']].drop_duplicates()\nmerged_df = merged_df.merge(city_names, on='Account Number', how='inner')","076f0985":"merged_df['pop_diff'] = (merged_df['Projected population'] - merged_df['Current population'])\n\nmerged_df['years'] = merged_df['Projected population year'].astype(int) - merged_df['Current population year'].astype(int)\n\nmerged_df['unit_diff'] = merged_df['pop_diff'] \/ merged_df['years']                                                                                                                   \n                                                                                                                      ","fc1ab1a5":"N = 10\nd = merged_df[(merged_df['Year Reported to CDP'] == 2020) & (merged_df['pop_diff'] == 0)][:N]\nbars = []\nbars.append(list(d['pop_diff'].values))\n\nstacked_bar_plot(bars, labels=d['Organization'].str.cat(d['Country'], sep=\"\\n\"), legends=['2020'], title='Population with no increase', colors=COLORS, **dict(figsize=(7, 5)))\n\ndel d\ndel N\ndel bars","89ede3b8":"merged_df[:2]","f7698286":"N = 10\nd = merged_df.loc[random.sample(list(merged_df[(merged_df['unit_diff'] > 0) & (merged_df['Year Reported to CDP'] == 2020)]['pop_diff'].sort_values()[::-1].index.ravel()[:10]), 10)]\n\nbars = []\nbars.append(list(d['unit_diff'].values))\n\nstacked_bar_plot(bars, labels=d['Organization'].str.cat(d['Country'], sep=\"\\n\"), legends=['2020'], title='Population with no increase', colors=COLORS, **dict(figsize=(10, 7)))\n\ndel d\ndel N\ndel bars","1b802505":"cities_df[cities_df['Parent Section'].isin(['Climate Hazards', 'Climate Hazards & Vulnerability', 'Climate Hazards and Vulnerability'])]\\\n[['Question Number', 'Question Name']].drop_duplicates()","c1c5b965":"cities_df[(cities_df['Question Number'] == '2.0')]['Question Name'].unique()","79bbe40d":"q2_df = cities_df[(cities_df['Question Number'] == '2.0') & ~(cities_df['Response Answer'].isnull())]","06073a21":"res = q2_df.groupby('Account Number')['Response Answer'].size().reset_index(name='size')\n\nfor ans in q2_df['Response Answer'].unique():\n    res[ans] = 0","eab781ad":"def count_words(answers, key):\n    return np.sum([1 for i in answers if i == key])","d23a0150":"for ans in q2_df['Response Answer'].unique():\n    res[ans] = res.apply(lambda x: count_words(q2_df.groupby('Account Number').get_group(x['Account Number'])['Response Answer'], ans), axis=1)\n    \nfor ans in q2_df['Response Answer'].unique():\n    res[ans] = res[ans] \/ res['size']","501309c9":"values = []\nfor ans in q2_df['Response Answer'].unique():\n    values.append(np.sum(res[ans] > 0) \/ len(res['Account Number']) * 100)\n\nvalues = pd.Series(values, index=list(q2_df['Response Answer'].unique()))","424b549c":"q2_mappings = dict()\n\nfor i in range(len(res)):\n    q2_mappings[res.loc[i, 'Account Number']] = []\n    [q2_mappings[res.loc[i, 'Account Number']].append(ans) for ans in q2_df['Response Answer'].unique() if res.loc[i, ans] > 0]\n    \nfor k, v in q2_mappings.items():\n    q2_mappings[k] = \"-\".join(v)","ff702da3":"bars = []\nd = pd.Series(q2_mappings.values()).value_counts()\nbars.append(d[d > 10])\n\nstacked_bar_plot(bars, labels=d[d>10].index, legends=['2018 + 2019 + 2020'], title='Cities with Risk Assessment actions', colors=COLORS, **dict(figsize=(8, 7)))\n\ndel d","061815de":"q2_df = cities_df[(cities_df['Question Number'].isin(['2.0a', '2.0b', '2.0c', '2.0d']))]","9f57939d":"q2_df[q2_df['Question Number'] == '2.0a']['Question Name'].unique()","002927b5":"grouped = q2_df[(q2_df['Question Number'] == '2.0a') & (q2_df['Column Name'] == 'Primary methodology')].groupby('Response Answer').agg({\n    'Organization': 'nunique'\n}).reset_index()\n\ngrouped = grouped.sort_values(by=['Organization'], ascending=[0])","f02b6e82":"bars = []\nd = grouped[grouped['Organization'] > 1]\nbars.append(d['Organization'])\n\nstacked_bar_plot(bars, labels=d['Response Answer'], legends=['2018 + 2019 + 2020'], title='Primary methods', colors=COLORS, **dict(figsize=(8, 9)))\n\ndel d","7b538abc":"df = q2_df[(q2_df['Question Number'] == '2.0a') & ~(q2_df['Column Name'] == 'Primary methodology')]","7a5b3c10":"cities_df[(cities_df['Question Number'] == '2.1')]['Question Name'].unique()\n","dc45b5c0":"keywords = ['heat', 'rain', 'economic', 'environment', 'forest', 'hazards']","ef396a28":"df = cities_df[(cities_df['Question Number'] == '2.1')]","ef14b192":"bars = []\nd = cities_df[(cities_df['Question Name'] == 'Does your city have an update \/ revision process for the climate risk or vulnerability assessment?')]\\\n.groupby('Response Answer').apply(len)\nbars.append(d)\n\nstacked_bar_plot(bars, labels=d.index, legends=['2018 + 2019 + 2020'], title='Cities with climate revision plan', colors=COLORS, **dict(figsize=(7, 5)))\n\ndel d","3f74ad85":"list(df['Response Answer'].drop_duplicates())","f9dea513":"# vectorizer = CountVectorizer()\n\n# X = vectorizer.fit_transform(q2_df['Response Answer'].dropna())","2daa513d":"#### 3. Currencies across organization","02fac20f":"#### 4. Introduction","c32ffa17":"### 5.1 Risk Assessment actions","ce163d35":"#### 2. Join data","aecfa0db":"#### 5. Climate Hazards","3243c040":"### Primary methodology","45f8f826":"#### 1. Read all files"}}