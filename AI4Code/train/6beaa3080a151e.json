{"cell_type":{"20e0dc80":"code","80b753b0":"code","0ba14c6a":"code","1cbc5125":"code","5cfc95bc":"code","7f7592e7":"code","e9df6670":"code","80fd40d8":"code","e6f7a8d6":"code","ad5d0345":"code","89af2531":"code","c8d6dade":"code","ddc059d2":"code","9df1ef46":"code","aee7024b":"code","e8b3eb1f":"code","b9542c27":"code","a91cf5f6":"code","67c35e72":"code","9ace95ba":"code","5364738b":"code","5f2ee5e2":"code","236bbe53":"code","afba142b":"code","76b130dd":"code","7196ce92":"code","95fe702e":"code","f908c915":"code","516ddfcc":"code","635cc20e":"markdown","513cb149":"markdown","f60b9f11":"markdown","619f8759":"markdown","c1ca126c":"markdown","14b1f47f":"markdown","c74501de":"markdown","cfd5fcb3":"markdown","43930363":"markdown","b6348f80":"markdown","abe4edab":"markdown","eb63f809":"markdown","593e8c7a":"markdown","47a58e52":"markdown","910c0013":"markdown","17950ced":"markdown"},"source":{"20e0dc80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn import svm\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,roc_auc_score, precision_score, roc_curve, recall_score,\\\n                             classification_report, f1_score, precision_recall_fscore_support)\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.ensemble as ensemble\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras import regularizers\nimport seaborn as sns","80b753b0":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\nprint(data.shape)\ndata.head()","0ba14c6a":"data.describe()","1cbc5125":"data.isna().sum().max()","5cfc95bc":"plt.subplots(figsize = (7,5))\ncount_classes = pd.value_counts(data['Class'],sort=True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title('Fraud class histogram', fontsize = 13)\nplt.xlabel('Class', fontsize = 13)\nplt.xticks(rotation=0)\nplt.ylabel('Frequency', fontsize = 15)\nplt.show()","7f7592e7":"data.hist(figsize=(20,20))\nplt.show()","e9df6670":"X_train = data.iloc[:,1:-2]","80fd40d8":"clf = IsolationForest(contamination = 0.03,n_estimators = 100, max_samples = 0.6, max_features = 0.6,random_state = 42)\nclf.fit(X_train)","e6f7a8d6":"y_pred_1IF = pd.Series(clf.predict(X_train))\ny_pred_1IF.replace(1,0,inplace = True)\ny_pred_1IF.replace(-1,1,inplace = True)\ncross_table = pd.crosstab(data.Class, columns = y_pred_1IF)\ncross_table","ad5d0345":"OCSVM = svm.OneClassSVM(nu = 0.03,kernel = 'rbf')\nOCSVM.fit(X_train)","89af2531":"y_pred_2OCSVM = pd.Series(OCSVM.predict(X_train))\ny_pred_2OCSVM.replace(1,0,inplace = True)\ny_pred_2OCSVM.replace(-1,1,inplace = True)\ncross_table = pd.crosstab(data.Class, columns = y_pred_2OCSVM)\ncross_table","c8d6dade":"input_dim = X_train.shape[1]\nencoding_dim = 14\ninput_layer = Input(shape = (input_dim, ))\n\nencoder = Dense(14, activation = 'tanh', activity_regularizer = regularizers.l1(10e-5))(input_layer)\nencoder = Dense(input_dim,activation = 'relu')(encoder)\nautoencoder = Model(inputs = input_layer, outputs = encoder)\n\nnb_epoch = 10\nbatch_size = 32\nautoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\nautoencoder.summary()","ddc059d2":"x_train = np.array(X_train)\nhistory = autoencoder.fit(x_train,x_train,epochs = nb_epoch,batch_size = batch_size,shuffle=True,validation_data = (x_train,x_train),verbose = 1).history","9df1ef46":"y_pred_AT = autoencoder.predict(x_train)\n# Restoring Error\nmse = np.mean(np.power(x_train - y_pred_AT, 2), axis = 1)\nerror_df = pd.DataFrame({'reconstruction_error':mse,\n                         'true_class':data.Class})","aee7024b":"fpr, tpr, thresholds = roc_curve(error_df.true_class,error_df.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\nplt.subplots(figsize = (7,5))\nplt.plot(fpr, tpr, label = 'AUC = {:.4f}'.format(roc_auc))\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate',fontsize=13)\nplt.xlabel('False Positive Rate',fontsize=13)\nplt.title('Receiver Operating Characteristic',fontsize=13)\nplt.show()","e8b3eb1f":"i = np.arange(len(tpr))\ncop = pd.DataFrame(\n    {'fpr' : pd.Series(fpr, index = i),\n     'tpr' : pd.Series(tpr, index = i),\n     '1-fpr' :pd.Series(1-fpr, index = i),\n     'tf' : pd.Series(tpr - (1-fpr), index = i),\n     'thresholds' : pd.Series(thresholds, index = i)})","b9542c27":"fig = plt.figure(figsize = (7,5))\nax1 = fig.add_subplot(111)\nax1.plot(cop['thresholds'], cop['tpr'], 'blue')\nax1.set_ylabel('TPR(blue)',fontsize = 13)\nax1.set_title('Threshold determination curve', fontsize = 13)\n\nax2 = ax1.twinx()\nax2.plot(cop['thresholds'], cop['1-fpr'], 'red')\nax2.set_xlim([0,4])\nax2.set_ylabel('TNR=1-fpr(red)',fontsize = 13)\nax2.set_xlabel('thresholds',fontsize = 13)\nplt.show()","a91cf5f6":"# look up the smallest value\ncop.loc[(cop.tf-0).abs().argsort()[:1]]","67c35e72":"# Establish confusion matrix\nthreshold = 3\ny_pred_3AR = np.array([1 if e>threshold else 0 for e in error_df.reconstruction_error.values])\ncross_table = pd.crosstab(data.Class, columns = y_pred_3AR)\ncross_table","9ace95ba":"three_score = pd.DataFrame([y_pred_1IF,y_pred_2OCSVM,y_pred_3AR]).T\ny_infer = three_score.apply(lambda x: x.mode(), axis = 1)\ndata['Class_infer'] = y_infer\ncross_table = pd.crosstab(data.Class, columns = data.Class_infer)\ncross_table","5364738b":"label = {\n    (0,0):0,\n    (1,1):1,\n    (0,1):1,\n    (1,0):0\n}\ndata['Class_new'] = data[['Class','Class_infer']].apply(lambda x:label[(x[0],x[1])], axis = 1)\ncount_classes = pd.value_counts(data['Class_new'], sort = True).sort_index()\ncount_classes","5f2ee5e2":"data = data.drop(['Time','Class','Class_infer'], axis = 1)\nX = np.array(data.loc[:,:'V28'])\ny = np.array(data['Class_new'])\nsess = StratifiedShuffleSplit(n_splits = 5,test_size=0.4,random_state=0)\nfor train_index,test_index in sess.split(X,y):\n    X_train,X_test = X[train_index], X[test_index]\n    y_train,y_test = y[train_index], y[test_index]\nprint('train_size: %s' %len(y_train),\n     'test_size: %s' %len(y_test))","236bbe53":"plt.figure(figsize = (7,5))\ncount_classes = pd.value_counts(y_train, sort = True)\ncount_classes.plot(kind = 'bar')\nplt.title('The histogram of fraud class in trainingdata', fontsize = 13)\nplt.xlabel('Class', fontsize = 13)\nplt.ylabel('Frequency', fontsize = 13)\nplt.xticks(rotation=0)\nplt.show() ","afba142b":"ros = RandomOverSampler(random_state = 0)\nsos = SMOTE(random_state=0)\nkos = SMOTETomek(random_state=0)\n\nx_ros, y_ros = ros.fit_sample(X_train, y_train)\nx_sos, y_sos = sos.fit_sample(X_train, y_train)\nx_kos, y_kos = kos.fit_sample(X_train, y_train)\nprint('ros: {}, sos: {}, kos:{}'.format(len(y_ros),len(y_sos),len(y_kos)))","76b130dd":"y_ros.sum(), y_sos.sum(), y_kos.sum()","7196ce92":"clf = DecisionTreeClassifier(criterion = 'gini', random_state=1234)\nparam_grid = {'max_depth':[3, 4, 5, 6], 'max_leaf_nodes':[4, 6, 8, 10, 12]}\ncv = GridSearchCV(clf, param_grid  = param_grid, scoring = 'f1')","95fe702e":"data = [[X_train, y_train],\n        [x_ros, y_ros],\n        [x_sos, y_sos],\n        [x_kos, y_kos]]\n\nfor features, labels in data:\n    cv.fit(features, labels)\n    predict_test = cv.predict(X_test)\n    \n    print('auc:{:.3f}'.format(roc_auc_score(y_test, predict_test)),\n          'recall:{:.3f}'.format(recall_score(y_test, predict_test)),\n          'precision:{:.3f}'.format(precision_score(y_test, predict_test)))","f908c915":"train_data = x_ros\ntrain_target = y_ros\ntest_target = y_test\ntest_data = X_test","516ddfcc":"lr = LogisticRegression(C = 0.1,penalty = 'l1',solver='liblinear')\nlr.fit(train_data,train_target)\ntest_est = lr.predict(test_data)\nprint('Logistic Regression accuracy:')\nprint(classification_report(test_target,test_est))\nfpr_test, tpr_test, th_test = roc_curve(test_target,test_est)\nprint('Logistic Regression AUC:{:.4f}'.format(auc(fpr_test,tpr_test)))","635cc20e":"There are 7670 were be marked as \"1\", and the rest were be marked as \"0\".\n## 3. Modeling\n### 3.1 Stratified Sampling","513cb149":"## 1. EDA\n**data resource**: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n- This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'.","f60b9f11":"- 419 out of 492 outliers were detected and 73 were not detected. Because the parameter contamination is set to 0.03, which is considered that the proportion of outliers in the data is 3%, there have 8126 normal value were detected as outliers.","619f8759":"401 out of 492 outliers were detected and 91 were not detected","c1ca126c":"## 2. Outlier Handling\n### 2.1 IsolationForest","14b1f47f":"### 3.3 LR Model","c74501de":"- 419 out of 492 outliers were detected and 73 were not detected in One-class SVM method, which is same as IsolationForest method","cfd5fcb3":"According to the majority voting results of the three methods, there were 417 outliers were detected. Next, create a new column \"class_new \"in data to record whether this data is outlier.","43930363":"The decision tree model will be established to predict under the training set directly divided and the data set obtained by the three oversampling methods, and choose the dataset with better effect into out subsequent model.","b6348f80":"### 2.2 One-class SVM","abe4edab":"We can know from threshold determination curve,tpr and 1-fpr curves intersect at the threshold of 0.85. However when threshold is 0.85, the probability of normal samples detected as outliers is too high, so we choose 3 as threshold.","eb63f809":"### 3.2 Over Sampling\nThe number of normal data is much more than outliers, we need to process the unbalanced dataset before modeling.","593e8c7a":"We know that the closer the ROC curve is to the upper left corner, the better the effect is, that is to say, the smaller the error rate (FPR) and the larger the effect (TPR). By balancing the two, we think that the point where TPR - (1-fpr) is close to 0 is the best cut point.","47a58e52":"The methods we used are: random oversampling, smote oversampling and comprehensive oversampling.","910c0013":"### 2.4 Fusion of the above three methods\n\n- We integrate the results of the above three methods by using the principle of majority voting to determine the outliers. That is, when two or more methods detect that one piece of data is an outlier, that piece of data is an outlier, otherwise it is a normal value.","17950ced":"### 2.3 autoencoder"}}