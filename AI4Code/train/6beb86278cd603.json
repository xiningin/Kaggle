{"cell_type":{"a0d4584b":"code","98a1d753":"code","a237ecf4":"code","6dfbaee2":"code","cd5162e7":"code","91010830":"code","729ce51e":"code","a955429a":"code","7f4ce21a":"code","480ee941":"code","0e4211d7":"code","166cd8f3":"code","e47ff89a":"code","97a3e62d":"markdown","7e86280b":"markdown","21f59681":"markdown","d814e42e":"markdown","b00fe598":"markdown"},"source":{"a0d4584b":"import os\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn')","98a1d753":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a237ecf4":"########## Import data and preprocess\ndf = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","6dfbaee2":"# drop bad data\ndf.dropna()\ndf.isnull().sum()\n\n# clean unwanted data\ndf.drop('id',axis=1,inplace=True)\ndf.drop('Unnamed: 32',axis=1,inplace=True)","cd5162e7":"# Convert Malignant diagnosis into 1 and Benign into 0\ndf['diagnosis'].astype(str)\ndf['diagnosis'] = [1 if val == 'M' else 0 for val in df['diagnosis'].values ]\n########## Select data and labels\nx = df.iloc[:,2:len(df.columns)]\ny = df['diagnosis'].values\n########## Data train test split\n(x_train,x_test,y_train,y_test) = train_test_split(x, y, test_size=0.3)\n########## Data scaling and normalization\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train) #fit is done for train set after split to prevent leak\nx_test = scaler.transform(x_test) # Using the same fit as in train ","91010830":"########## Initialize containers\nSVM1AccTrain = []\nSVM1AccTest = []\nSVM1Precision = []\nSVM1Recall = []\n\n########## SVM1 with linear kernel\nfor i in range (0, 20):\n    clf = SVC(kernel='linear', C=10**(10))\n    clf.fit(x_train, y_train)\n\n    y_test_pred = clf.predict(x_test)\n    y_train_pred = clf.predict(x_train)\n    \n    SVM1AccTrain.append(accuracy_score(y_train, y_train_pred))\n    SVM1AccTest.append(accuracy_score(y_test, y_test_pred))\n    SVM1Precision.append(precision_score(y_test, y_test_pred))\n    SVM1Recall.append(recall_score(y_test, y_test_pred))","729ce51e":"########## Print average stats of SVM1\nprint ('''\n        Average Statistics for SVM1\n        \\n SVM1AccTrainAvg = {}\n        \\n SVM1AccTestAvg = {}\n        \\n SVM1PrecisionAvg = {}\n        \\n SVM1RecallAvg = {}\n        '''.format(\n        np.mean(SVM1AccTrain),\n        np.mean(SVM1AccTest),\n        np.mean(SVM1Precision),\n        np.mean(SVM1Recall)\n        ))","a955429a":"########## Initialize containers\nSVM2AccTrain = []\nSVM2AccTest = []\nSVM2Precision = []\nSVM2Recall = []\n\n########## SVM2 with RBF kernel\nfor i in range (0, 20):\n    clf = SVC(kernel='rbf', C=10**(10))\n    clf.fit(x_train, y_train)\n\n    y_test_pred = clf.predict(x_test)\n    y_train_pred = clf.predict(x_train)\n    \n    SVM2AccTrain.append(accuracy_score(y_train, y_train_pred))\n    SVM2AccTest.append(accuracy_score(y_test, y_test_pred))\n    SVM2Precision.append(precision_score(y_test, y_test_pred))\n    SVM2Recall.append(recall_score(y_test, y_test_pred))","7f4ce21a":"########## Print average stats of SVM2\nprint ('''\n        Average Statistics for SVM2\n        \\n SVM2AccTrainAvg = {}\n        \\n SVM2AccTestAvg = {}\n        \\n SVM2PrecisionAvg = {}\n        \\n SVM2RecallAvg = {}\n        '''.format(\n        np.mean(SVM2AccTrain),\n        np.mean(SVM2AccTest),\n        np.mean(SVM2Precision),\n        np.mean(SVM2Recall)\n        ))","480ee941":"CList = [10**(n) for n in range (-30,30)]","0e4211d7":"########## Initialize containers\nSVM3MeanAccTrainList = []\nSVM3MeanAccTestList = []\nSVM3MeanPrecisionList = []\nSVM3MeanRecallList = []\n\n########## SVM3 RBF kernel with regularization (soft margin)\nfor CVal_i, CVal in enumerate(CList):\n    SVM3AccTrain = []\n    SVM3AccTest = []\n    SVM3Precision = []\n    SVM3Recall = []\n    for i in range(0, 20):\n        clf = SVC(kernel='linear', C=CVal)\n        clf.fit(x_train, y_train)\n\n        y_test_pred = clf.predict(x_test)\n        y_train_pred = clf.predict(x_train)\n\n        SVM3AccTrain.append(accuracy_score(y_train, y_train_pred))\n        SVM3AccTest.append(accuracy_score(y_test, y_test_pred))\n        SVM3Precision.append(precision_score(y_test, y_test_pred))\n        SVM3Recall.append(recall_score(y_test, y_test_pred))\n        \n    SVM3MeanAccTrainList.append(np.mean(SVM3AccTrain))\n    SVM3MeanAccTestList.append(np.mean(SVM3AccTest))\n    SVM3MeanPrecisionList.append(np.mean(SVM3Precision))\n    SVM3MeanRecallList.append(np.mean(SVM3Recall))","166cd8f3":"bestAccIndex = SVM3MeanAccTestList.index(max(SVM3MeanAccTestList))\nprint ('''\n             Statistics for SVM3\n        \\n bestSVM3AccTrainAvg = {}\n        \\n bestSVM3AccTestAvg = {}\n        \\n bestSVM3PrecisionAvg = {}\n        \\n bestSVM3RecallAvg = {}\n        \\n\n        \\n bestCParameter = {}\n        '''.format(\n        SVM3MeanAccTrainList[bestAccIndex],\n        SVM3MeanAccTestList[bestAccIndex],\n        SVM3MeanPrecisionList[bestAccIndex],\n        SVM3MeanRecallList[bestAccIndex],\n        CList[bestAccIndex] #index of CList and SVM3MeanAccTestList corresponds\n        ))","e47ff89a":"########## Plot grapth of average test accuracy over tree depth \nfig = plt.figure()\nplt.title(\"Average test accuracy with different regularization C\")\nplt.xlabel(\"value of regularization C\")\nplt.ylabel(\"average test accuracy\")\nplt.xscale('log')\nplt.plot(CList, SVM3MeanAccTestList)\n\nnow = datetime.now()\ndt_string = now.strftime(\"%d-%m-%Y-%H:%M:%S\")\nplt.savefig('avgTestAccSVM3Norm_{}'.format(dt_string), format='png')\nplt.show()","97a3e62d":"## SVM1 with linear kernel","7e86280b":"In SVM3, the C parameter is varied from 1e-50 to 1e50. After plotting and comparing the resulting average test accuracy over 20 trials, it is concluded that SVM3 works best with a relatively small C parameter of 0.01, which means that it is a soft margin classifier.","21f59681":"## SVM3 RBF kernel with regularization (soft margin)","d814e42e":"This notebook explores different approaches in classifying breast cancer using different types of SVM i.e using linear kernel, radial basis function, and margin adjustment.","b00fe598":"## SVM2 with RBF kernel"}}