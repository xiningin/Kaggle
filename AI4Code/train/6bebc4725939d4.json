{"cell_type":{"0634a1fd":"code","fec1a3d1":"code","7b7074ad":"code","45b5262f":"code","fb9f1ed7":"code","039a4748":"code","284e7652":"code","778f73a5":"code","439d9bc9":"code","e84c4d57":"code","8e6cdd74":"code","8aa60473":"code","80ce91e1":"code","76ec4dc6":"code","67c978d1":"code","daefc580":"code","7f2f6366":"code","df8c8451":"code","b94ab400":"code","42c1dd39":"code","3d9a9b0e":"code","e79b6de6":"code","e451db8f":"code","6ec634fe":"code","1fcbce43":"code","54ff78c1":"code","152a7063":"code","f5f66bff":"code","38eef69e":"code","ebe403b4":"code","89450969":"code","b13f80a5":"code","b48de9c9":"code","be4ea098":"code","cb1eaa25":"code","c1357822":"code","c3649209":"code","ab0fee50":"code","1e08f647":"code","16d0b3e1":"markdown","a87ded7a":"markdown","aa02c83d":"markdown","f43275bf":"markdown","eab10da6":"markdown","ad11dc13":"markdown","f4dd2bea":"markdown","41033fba":"markdown","531cf791":"markdown","d7ed1a43":"markdown","29a3b942":"markdown","56b7ab63":"markdown","5b8c0a07":"markdown","29de77e4":"markdown","fd1543a4":"markdown","99a637c5":"markdown","9392e503":"markdown","61b90c89":"markdown","474ff192":"markdown","dfe1ad3f":"markdown","bf596e27":"markdown","de11de79":"markdown","e709a605":"markdown"},"source":{"0634a1fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fec1a3d1":"df = pd.read_csv('..\/input\/data.csv')","7b7074ad":"df.head()","45b5262f":"df.info()","fb9f1ed7":"#there are a 33 columns and 569 entries","039a4748":"y = df.diagnosis\nlist = ['Unnamed: 32','id','diagnosis']\nX = df.drop(list,axis=1) # drop unnamed: 32 column\n","284e7652":"df.head(2)","778f73a5":"#import libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","439d9bc9":"sns.countplot(df['diagnosis'],label='Count')\n","e84c4d57":"B,M = y.value_counts()\nprint('Number of Benign:',B)\nprint('Number of Malignant:',M)","8e6cdd74":"#lets draw correlation graph , we use correlation because it use to remove multi colinearity it means the column are depending on each other so we should avoid it because it use same column twice.","8aa60473":"corr = df[df.columns[1:11]].corr()\nplt.figure(figsize=(14,14))\nsns.heatmap(corr,cbar ='True',annot=True,linewidths=.5,fmt='.1f',cmap='coolwarm')\n\n#corr = data[features_mean].corr() # .corr is used for find corelation\n#plt.figure(figsize=(14,14))\n#sns.heatmap(corr, cbar = 'True',square = True, annot='True', fmt= '.2f',annot_kws={'size': 15},\n #          xticklabels= features_mean, yticklabels= features_mean,\n  #         cmap= 'coolwarm') # for more on heatmap","80ce91e1":"X.head()","76ec4dc6":"#import the libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier","67c978d1":"X = StandardScaler().fit_transform(X.values)","daefc580":"X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    train_size=0.8,\n                                                    random_state=42)","7f2f6366":"from sklearn.neighbors import KNeighborsClassifier","df8c8451":"knn = KNeighborsClassifier(n_neighbors=5,\n                           p=2, metric='minkowski')","b94ab400":"knn.fit(X_train, y_train)","42c1dd39":"from sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","3d9a9b0e":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))        ","e79b6de6":"print_score(knn, X_train, y_train, X_test, y_test, train=True)","e451db8f":"print_score(knn, X_train, y_train, X_test, y_test, train=False)","6ec634fe":"from sklearn.svm import SVC #import library","1fcbce43":"model = SVC()","54ff78c1":"model.fit(X_train,y_train)  #fit the model","152a7063":"y_pred = model.predict(X_test)  #prediction","f5f66bff":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))","38eef69e":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train,y_train)\ny_pred = linear_svc.predict(X_test)\n","ebe403b4":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))","89450969":"from sklearn.model_selection import train_test_split, GridSearchCV ","b13f80a5":"# C -> controls the cost of the misclassification on the training data.\n# A large C value gives you the low bias and high variance\n# Lower C value gives you the high bias and the lower variance.\n# gamma is a free parameter in radial basis function.\n# Higher gamma value leads to Higher bias and lower variance value.\nparam_grid = {'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}","b48de9c9":"grid = GridSearchCV(SVC(),param_grid,verbose=3) # put the verbose = 3","be4ea098":"grid.fit(X_train,y_train)  #assignment: scalling the X_strain","cb1eaa25":"grid.best_params_  #we find best parameter","c1357822":"grid.best_estimator_  #find best estimator","c3649209":"grid_predictions = grid.predict(X_test)","ab0fee50":"print(confusion_matrix(y_test, grid_predictions))           #does not identify six value correctly ","1e08f647":"print(classification_report(y_test, grid_predictions))","16d0b3e1":"**Visualization**","a87ded7a":"there are 357 'Benign',and 212 'Malign' are present.","aa02c83d":"linear svc gives worst result as compare to svc.","f43275bf":" float have 31 column, int have 64 and object has only 1 column","eab10da6":"**Data Pre-processing\n**","ad11dc13":"to chose the feature by using heatmap","f4dd2bea":"In this dataset firstly fing the relationship between 'no of Benign',and 'no of malign' by using countplot\n","41033fba":"1.    This analysis hs been done using KNN and SVM With detailed explanation.\n","531cf791":"# 3)Linear SVC ","d7ed1a43":"our model fit in test  database as 94% accuracy.\n\nand 68(class 0) has malignant, and 40(class 1) has benign. only 6 value that not converted any class.","29a3b942":"barplot easily says that,there are more number of 'Benign' is present,but we dont know how much? lets find it","56b7ab63":"1. Lets Start.","5b8c0a07":" In this problem we have use 30 columns and we have to predict the stage of breast cancer M for Malignant and B for Benign.\n\n","29de77e4":"Now we can see Unnamed:32 have 0 non null object it means the all values are null in this column so we cannot use this column for our analysis*\n\nThere is an id,that can not use for KNN.","fd1543a4":"> Attribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n-3-32.Ten real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 \/ area - 1.0)\n\ng). concavity (severity of concave portions of the contour)\n\nh). concave points (number of concave portions of the contour)\n\ni). symmetry\n\nj). fractal dimension (\"coastline approximation\" - 1)\n\n5 here 3- 32 are divided into three parts first is Mean (3-13), Stranded Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension)\n\nHere Mean means the means of the all cells, standard Error of all cell and worst means the worst cell\n","99a637c5":"# 4)GridSearchCV","9392e503":"now we convert 71 to malignant and 41 to benign.","61b90c89":"# 1) KNN","474ff192":"**our task is convertion of all value to any class, now we apply SVM**","dfe1ad3f":"model has 98% accuracy.","bf596e27":"# 2) Support Vector Machin","de11de79":"our model fit in train database as 98% accuracy.\n\nand 286(class 0) has malignant, and 160(class 1) has benign. only 9 value that not converted any class.","e709a605":"our model fit in test and train database as 97% accuracy.\n\nand 70(class 0) has malignant, and 41(class 1) has benign. only 3 value that not converted any class."}}