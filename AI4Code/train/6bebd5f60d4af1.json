{"cell_type":{"aae556a5":"code","5f082a75":"code","78b6a880":"code","e2b92575":"code","0b54cf79":"code","0a3d81f6":"code","4fe7a67b":"code","529a4c9a":"code","b244e16f":"code","58d5fb97":"markdown","92c74d0c":"markdown","263aa7cb":"markdown"},"source":{"aae556a5":"# brian2 is a simulator for spiking neural networks. It allows to simulate neuraldinamics \n# using custom differential equations, but it is very computationally intensive.\n# https:\/\/brian2.readthedocs.io\/en\/stable\/resources\/tutorials\/index.html\n\n!pip install brian2","5f082a75":"from keras.datasets import mnist\nfrom brian2 import *\nimport brian2.numpy_ as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","78b6a880":"(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# simplified classification (0 1 and 8)\nX_train = X_train[(y_train == 1) | (y_train == 0) | (y_train == 8)]\ny_train = y_train[(y_train == 1) | (y_train == 0) | (y_train == 8)]\nX_test = X_test[(y_test == 1) | (y_test == 0) | (y_test == 8)]\ny_test = y_test[(y_test == 1) | (y_test == 0) | (y_test == 8)]\n\n# pixel intensity to Hz (255 becoms ~63Hz)\nX_train = X_train \/ 4 \nX_test = X_test \/ 4\n\nX_train.shape, X_test.shape","e2b92575":"plt.figure(figsize=(16,8))\nfor img in range(32):\n    plt.subplot(4,8,1+img)\n    plt.title(y_train[img])\n    plt.imshow(X_train[img])\n    plt.axis('off')","0b54cf79":"n_input = 28*28 # input layer\nn_e = 100 # e - excitatory\nn_i = n_e # i - inhibitory\n\nv_rest_e = -60.*mV # v - membrane potential\nv_reset_e = -65.*mV\nv_thresh_e = -52.*mV\n\nv_rest_i = -60.*mV\nv_reset_i = -45.*mV\nv_thresh_i = -40.*mV\n\ntaupre = 20*ms\ntaupost = taupre\ngmax = .05 #.01\ndApre = .01\ndApost = -dApre * taupre \/ taupost * 1.05\ndApost *= gmax \ndApre *= gmax \n\n# Apre and Apost - presynaptic and postsynaptic traces, lr - learning rate\nstdp='''w : 1\n    lr : 1 (shared)\n    dApre\/dt = -Apre \/ taupre : 1 (event-driven)\n    dApost\/dt = -Apost \/ taupost : 1 (event-driven)'''\npre='''ge += w\n    Apre += dApre\n    w = clip(w + lr*Apost, 0, gmax)'''\npost='''Apost += dApost\n    w = clip(w + lr*Apre, 0, gmax)'''\n\nclass Model():\n    \n    def __init__(self, debug=False):\n        app = {}\n                \n        # input images as rate encoded Poisson generators\n        app['PG'] = PoissonGroup(n_input, rates=np.zeros(n_input)*Hz, name='PG')\n        \n        # excitatory group\n        neuron_e = '''\n            dv\/dt = (ge*(0*mV-v) + gi*(-100*mV-v) + (v_rest_e-v)) \/ (100*ms) : volt\n            dge\/dt = -ge \/ (5*ms) : 1\n            dgi\/dt = -gi \/ (10*ms) : 1\n            '''\n        app['EG'] = NeuronGroup(n_e, neuron_e, threshold='v>v_thresh_e', refractory=5*ms, reset='v=v_reset_e', method='euler', name='EG')\n        app['EG'].v = v_rest_e - 20.*mV\n        \n        if (debug):\n            app['ESP'] = SpikeMonitor(app['EG'], name='ESP')\n            app['ESM'] = StateMonitor(app['EG'], ['v'], record=True, name='ESM')\n            app['ERM'] = PopulationRateMonitor(app['EG'], name='ERM')\n        \n        # ibhibitory group\n        neuron_i = '''\n            dv\/dt = (ge*(0*mV-v) + (v_rest_i-v)) \/ (10*ms) : volt\n            dge\/dt = -ge \/ (5*ms) : 1\n            '''\n        app['IG'] = NeuronGroup(n_i, neuron_i, threshold='v>v_thresh_i', refractory=2*ms, reset='v=v_reset_i', method='euler', name='IG')\n        app['IG'].v = v_rest_i - 20.*mV\n\n        if (debug):\n            app['ISP'] = SpikeMonitor(app['IG'], name='ISP')\n            app['ISM'] = StateMonitor(app['IG'], ['v'], record=True, name='ISM')\n            app['IRM'] = PopulationRateMonitor(app['IG'], name='IRM')\n        \n        # poisson generators one-to-all excitatory neurons with plastic connections \n        app['S1'] = Synapses(app['PG'], app['EG'], stdp, on_pre=pre, on_post=post, method='euler', name='S1')\n        app['S1'].connect()\n        app['S1'].w = 'rand()*gmax' # random weights initialisation\n        app['S1'].lr = 1 # enable stdp        \n        \n        if (debug):\n            # some synapses\n            app['S1M'] = StateMonitor(app['S1'], ['w', 'Apre', 'Apost'], record=app['S1'][380,:4], name='S1M') \n        \n        # excitatory neurons one-to-one inhibitory neurons\n        app['S2'] = Synapses(app['EG'], app['IG'], 'w : 1', on_pre='ge += w', name='S2')\n        app['S2'].connect(j='i')\n        app['S2'].delay = 'rand()*10*ms'\n        app['S2'].w = 3 # very strong fixed weights to ensure corresponding inhibitory neuron will always fire\n\n        # inhibitory neurons one-to-all-except-one excitatory neurons\n        app['S3'] = Synapses(app['IG'], app['EG'], 'w : 1', on_pre='gi += w', name='S3')\n        app['S3'].connect(condition='i!=j')\n        app['S3'].delay = 'rand()*5*ms'\n        app['S3'].w = .03 # weights are selected in such a way as to maintain a balance between excitation and ibhibition\n        \n        self.net = Network(app.values())\n        self.net.run(0*second)\n        \n    def __getitem__(self, key):\n        return self.net[key]\n    \n    def train(self, X, epoch=1):        \n        self.net['S1'].lr = 1 # stdp on\n        \n        for ep in range(epoch):\n            for idx in range(len(X)):\n                # active mode\n                self.net['PG'].rates = X[idx].ravel()*Hz\n                self.net.run(0.35*second)\n\n                # passive mode\n                self.net['PG'].rates = np.zeros(n_input)*Hz\n                self.net.run(0.15*second)\n        \n    def evaluate(self, X):       \n        self.net['S1'].lr = 0  # stdp off\n        \n        features = []\n        for idx in range(len(X)):\n            # rate monitor to count spikes\n            mon = SpikeMonitor(self.net['EG'], name='RM')\n            self.net.add(mon)\n            \n            # active mode\n            self.net['PG'].rates = X[idx].ravel()*Hz\n            self.net.run(0.35*second)\n            \n            # spikes per neuron foreach image\n            features.append(np.array(mon.count, dtype=int8))\n            \n            # passive mode\n            self.net['PG'].rates = np.zeros(n_input)*Hz\n            self.net.run(0.15*second)\n            \n            self.net.remove(self.net['RM'])\n            \n        return features","0a3d81f6":"def plot_w(S1M):\n    plt.rcParams[\"figure.figsize\"] = (20,10)\n    subplot(311)\n    plot(S1M.t\/ms, S1M.w.T\/gmax)\n    ylabel('w \/ wmax')\n    subplot(312)\n    plot(S1M.t\/ms, S1M.Apre.T)\n    ylabel('apre')\n    subplot(313)\n    plot(S1M.t\/ms, S1M.Apost.T)\n    ylabel('apost')\n    tight_layout()\n    show();\n    \ndef plot_v(ESM, ISM, neuron=13):\n    plt.rcParams[\"figure.figsize\"] = (20,6)\n    cnt = -50000 # tail\n    plot(ESM.t[cnt:]\/ms, ESM.v[neuron][cnt:]\/mV, label='exc', color='r')\n    plot(ISM.t[cnt:]\/ms, ISM.v[neuron][cnt:]\/mV, label='inh', color='b')\n    plt.axhline(y=v_thresh_e\/mV, color='pink', label='v_thresh_e')\n    plt.axhline(y=v_thresh_i\/mV, color='silver', label='v_thresh_i')\n    legend()\n    ylabel('v')\n    show();\n    \ndef plot_rates(ERM, IRM):\n    plt.rcParams[\"figure.figsize\"] = (20,6)\n    plot(ERM.t\/ms, ERM.smooth_rate(window='flat', width=0.1*ms)*Hz, color='r')\n    plot(IRM.t\/ms, IRM.smooth_rate(window='flat', width=0.1*ms)*Hz, color='b')\n    ylabel('Rate')\n    show();\n    \ndef plot_spikes(ESP, ISP):\n    plt.rcParams[\"figure.figsize\"] = (20,6)\n    plot(ESP.t\/ms, ESP.i, '.r')\n    plot(ISP.t\/ms, ISP.i, '.b')\n    ylabel('Neuron index')\n    show();\n\ndef test0(train_items=30):\n    '''\n    STDP visualisation\n    '''\n    seed(0)\n    \n    model = Model(debug=True)\n    model.train(X_train[:train_items], epoch=1)\n    \n    plot_w(model['S1M'])\n    plot_v(model['ESM'], model['ISM'])\n    plot_rates(model['ERM'], model['IRM'])\n    plot_spikes(model['ESP'], model['ISP'])\n    \ntest0()","4fe7a67b":"def test1(train_items=5000, assign_items=1000, eval_items=1000):\n    '''\n    Feed train set to SNN with STDP\n    Freeze STDP\n    Feed train set to SNN again and collect generated features\n    Train RandomForest on the top of these features and labels provided\n    Feed test set to SNN and collect new features\n    Predict labels with RandomForest and calculate accuacy score\n    '''\n    seed(0)\n    \n    model = Model()\n    model.train(X_train[:train_items], epoch=1)\n    model.net.store('train', 'train.b2')\n    #model.net.restore('train', '.\/train.b2')\n    \n    f_train = model.evaluate(X_train[:assign_items])\n    clf = RandomForestClassifier(max_depth=4, random_state=0)\n    clf.fit(f_train, y_train[:assign_items])\n    print(clf.score(f_train, y_train[:assign_items]))\n\n    f_test = model.evaluate(X_test[:eval_items])\n    y_pred = clf.predict(f_test)\n    print(accuracy_score(y_pred, y_test[:eval_items]))\n\n    cm = confusion_matrix(y_pred, y_test[:eval_items])\n    print(cm)\n    \ntest1()","529a4c9a":"def test2(train_items=5000, assign_items=1000, eval_items=1000):\n    '''\n    Freeze STDP at start\n    Feed train set to SNN and collect generated features\n    Train RandomForest on the top of these features and labels provided\n    Feed test set to SNN and collect new features\n    Predict labels with RandomForest and calculate accuacy score\n    '''\n    seed(0)\n    \n    model = Model()\n        \n    f_train = model.evaluate(X_train[:assign_items])\n    clf = RandomForestClassifier(max_depth=4, random_state=0)\n    clf.fit(f_train, y_train[:assign_items])\n    print(clf.score(f_train, y_train[:assign_items]))\n\n    f_test = model.evaluate(X_test[:eval_items])\n    y_pred = clf.predict(f_test)\n    print(accuracy_score(y_pred, y_test[:eval_items]))\n\n    cm = confusion_matrix(y_pred, y_test[:eval_items])\n    print(cm)\n    \ntest2()","b244e16f":"def test3(train_items=5000, eval_items=1000):\n    '''\n    Train and evaluate RandomForest without SNN\n    '''\n    seed(0)\n    \n    clf = RandomForestClassifier(max_depth=4, random_state=0)\n    \n    train_features = X_train[:train_items].reshape(-1,28*28)\n    clf.fit(train_features, y_train[:train_items])\n    print(clf.score(train_features, y_train[:train_items]))\n    \n    test_features = X_test[:eval_items].reshape(-1,28*28)\n    y_pred = clf.predict(test_features)\n    print(accuracy_score(y_pred, y_test[:eval_items]))\n\n    cm = confusion_matrix(y_pred, y_test[:eval_items])\n    print(cm)\n    \ntest3()","58d5fb97":"## Experiment design and observed results\n\n1. Feed train set to SNN described above\n2. Freeze STDP\n3. Feed train set to SNN again and collect generated features (spike counts foreach excitatory neuron during single image representation)\n4. Train RandomForest on the top of these features and labels provided\n5. Feed test set to SNN and collect new features\n6. Predict labels with RandomForest and calculate accuacy score\n\nIt reaches 0.9 accuracy on the test set with only 5k training examples and 3 classes. In order to understand whether local learning affects the result, the experiment was repeated but without training SNN with STDP. Second experiment scored 0.74, which clearly shows the efficiency of local training.","92c74d0c":"## Spiking neural network architecture\n\nTo model neuron dynamics leaky integrate-and-fire model is used. Synapses are modeled by conductance changes, i.e., synapses increase their conductance instantaneously by the synaptic weight when a presynaptic spike arrives at the synapse, otherwise the conductance is decaying exponentially.\n\nThe network consists of two layers. The first layer is the input layer, containing 28 \u00d7 28 neurons (one neuron per image pixel), and the second layer is the processing layer, containing 100 excitatory neurons and 100 inhibitory neurons. Each input is a Poisson spike-train, which is fed to the excitatory neurons of the second layer for the 350 ms following by 150 ms resting period. The rates of each neuron are proportional (devided by 4) to the intensity of the corresponding pixel in the example image.\n\nThe excitatory neurons of the second layer are connected in a one-to-one fashion to inhibitory neurons, i.e., each spike in an excitatory neuron will trigger a spike in its corresponding inhibitory neuron. Each of the inhibitory neurons is connected to all excitatory ones, except for the one from which it receives a connection. This connectivity provides lateral inhibition and leads to competition among excitatory neurons. All synapses from input neurons to excitatory neurons are learned using STDP. The weights of the remaining synapses are fixed.\n\n![image.png](attachment:ad6a31c8-9ff4-4e0c-b688-532c1cb8bb43.png)\n\nTo understand this part I recomend [https:\/\/www.coursera.org\/learn\/synapses\/](https:\/\/www.coursera.org\/learn\/synapses\/)","263aa7cb":"# MNIST - Spiking neural network with spike timing dependent plasticity\n\nThe purpose of this work is to demonstrate local learning rules (STDP) whit the MNIST handwritten digit classification problem. The main ideas are taken from the article [Unsupervised learning of digit recognition using spike-timing-dependent plasticity](https:\/\/www.frontiersin.org\/articles\/10.3389\/fncom.2015.00099\/full), but the experiment was reconstructed with significant changes from the original work."}}