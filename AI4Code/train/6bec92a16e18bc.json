{"cell_type":{"cc98ba73":"code","224eeb76":"code","0f6fc9d1":"code","06ef8a1c":"code","99ae06ca":"code","b73bc4eb":"code","0cf4717f":"code","e236fb8c":"code","01586a4b":"code","2870ae28":"code","7b473d0e":"code","fa6921b0":"code","235af554":"code","054b47f6":"code","143c41c4":"code","e40ba19c":"code","9eed986c":"markdown","5e7a2457":"markdown","21419bcb":"markdown","13473f96":"markdown","b4d6b937":"markdown","5295be12":"markdown","17cbd20a":"markdown","1f8bbbe4":"markdown"},"source":{"cc98ba73":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nimport sklearn.preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns","224eeb76":"X_train = pd.read_csv(\"..\/input\/career-con-2019\/X_train.csv\")\nX_test  = pd.read_csv(\"..\/input\/career-con-2019\/X_test.csv\")\ny_train = pd.read_csv(\"..\/input\/career-con-2019\/y_train.csv\")\ny_test73 = pd.read_csv(\"..\/input\/robots-best-submission\/mybest0.73.csv\")\n\n\nle = sklearn.preprocessing.LabelEncoder().fit(y_train['surface'])\nle.classes_","0f6fc9d1":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    \"\"\"Function refactored to accept np.array as inputs for faster processing\"\"\"\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = np.arctan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2[t2>+1.0] = +1.0\n    t2[t2<-1.0] = -1.0\n    Y = np.arcsin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = np.arctan2(t3, t4)\n\n    return X, Y, Z\n\n# conversion for train and test\nfor df in X_train, X_test:\n    X, Y, Z = quaternion_to_euler(df.orientation_X, df.orientation_Y,df.orientation_Z, df.orientation_W)\n    df['X'] = X\n    df['Y'] = Y\n    df['Z'] = Z","06ef8a1c":"###################################################################\n##########  Hyper Parameters\n###################################################################\n\n# columns to build features on\nCOLUMNS = [ 'X', 'Y', 'Z', \n           'angular_velocity_X',    'angular_velocity_Y',    'angular_velocity_Z', \n           'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z' ]\n\n# NN parameters\nEPOCHS=100\nBS=32\nINIT_LR=0.001\nDROPOUT=0.1\n","99ae06ca":"X_test.series_id\ndf_features_train = pd.DataFrame()\ndf_features_test  = pd.DataFrame()\n\ndef add_columns(df_in, df_out, op):\n    for column in COLUMNS:\n        df_out[f\"{column}_{op}\"] = df_in[column]\n\nfor (df_data, df_features) in [(X_train, df_features_train), (X_test, df_features_test)]:\n    grp = df_data.groupby('series_id')[COLUMNS]\n    df_op = grp.mean()\n    add_columns(df_op, df_features, \"mean\")    \n    df_op = grp.median()\n    add_columns(df_op, df_features, \"median\")\n    df_op = grp.std()\n#    add_columns(df_op, df_features, \"std\")\n#    df_op = grp.quantile(0.25)\n    add_columns(df_op, df_features, \"q25\")\n    df_op = grp.quantile(0.80)\n    add_columns(df_op, df_features, \"q75\")\ndf_features_test.head()","b73bc4eb":"f,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(df_features_train.iloc[:,1:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, cmap='jet')\nplt.title('TRAIN')\n","0cf4717f":"f,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(df_features_test.iloc[:,1:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, cmap='jet')\nplt.title('TEST')","e236fb8c":"from sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=777)\nX, y = df_features_train.values, le.transform(y_train.surface.values)\nscaler.fit(X)\nX = scaler.transform(X)\nfor idx_tr, idx_cv in sss.split(X, y):\n    model = SVC(kernel='rbf', gamma=0.08)\n    model.fit(X[idx_tr], y[idx_tr])\n\npred_tr = model.predict(X[idx_tr])\npred_cv = model.predict(X[idx_cv])\n\nprint(\"Accuracy on TRAIN set =\", accuracy_score(y[idx_tr], pred_tr))\nprint(\"Accuracy on CV    set =\", accuracy_score(y[idx_cv], pred_cv))\n\npred_test = model.predict(scaler.transform(df_features_test.values))\nprint(\"Test73 CMP =\", accuracy_score(y_test73.surface.values, le.inverse_transform(pred_test)))","01586a4b":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion(y0, y1, title):\n    cnf_matrix = confusion_matrix(y0, y1)\n    cnf_matrix_norm = cnf_matrix.astype('float') \/ cnf_matrix.sum(axis=1)[:, np.newaxis]\n    df_cm = pd.DataFrame(cnf_matrix_norm, index=le.classes_, columns=le.classes_)\n\n    plt.figure(figsize=(20, 7))\n    ax = plt.axes()\n    ax.set_title(title, fontsize='18')\n    sns.heatmap(df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=ax)\n    plt.show()\n    \nplot_confusion(y[idx_tr], pred_tr,'Train set for SVC model')\nplot_confusion(y[idx_cv], pred_cv,'CV set for SVC model')\n","2870ae28":"\nimport keras\nfrom keras.layers import *\nfrom keras.optimizers import Adam\nfrom keras.callbacks import *\nfrom keras.models import Model, Sequential\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.utils import plot_model\nfrom keras.utils.np_utils import to_categorical\nfrom keras.regularizers import l1, l2\n\ndef build_NN(width, classes, init_lr=INIT_LR, epochs=EPOCHS):\n    l2reg = l2(0.001)\n    \n    # initialize the model\n    model = Sequential()\n    inputShape = (width,)\n\n    \n    # first (and only) set of FC => RELU layers\n    model.add(Flatten())\n    model.add(Dropout(rate=DROPOUT, input_shape=inputShape))\n    model.add(Dense(width, activation='linear'))#, kernel_regularizer=l2reg))\n    model.add(Activation(\"relu\"))\n\n    model.add(Dense(width, activation='linear'))#, kernel_regularizer=l2reg))\n    model.add(Activation(\"relu\"))\n\n    # softmax classifier\n    model.add(Dense(classes))\n    model.add(Activation(\"softmax\"))\n\n    opt   = Adam(lr=init_lr, decay=init_lr \/ epochs)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"categorical_accuracy\"])\n\n    # return the constructed network architecture\n    return model\n\ndef kreshape(X):\n    return np.reshape(X, np.concatenate([X.shape, [1]]))\n\nmodel = build_NN(X.shape[1], len(le.classes_))\n\n#history = model.fit(kreshape(X[idx_tr]), to_categorical(y[idx_tr]), \n#              epochs=EPOCHS, batch_size=BS, verbose=0,\n#              validation_data=[kreshape(X[idx_cv]), to_categorical(y[idx_cv])])","7b473d0e":"from sklearn.model_selection import StratifiedKFold\n\ndef k_folds(X, y, X_test, k=5):\n    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=777).split(X, y))\n    y_test = np.zeros((X_test.shape[0], len(COLUMNS)))\n    y_gcv  = np.zeros((X.shape[0], len(COLUMNS)))\n    histories = []\n    \n    for i, (train_idx, val_idx) in  enumerate(folds):\n        print(f\"Fold {i+1}\")\n        model = build_NN(X.shape[1], len(le.classes_))\n\n        try:\n            history = model.fit(kreshape(X[train_idx]), to_categorical(y[train_idx]),  \n                                validation_data=[kreshape(X[val_idx]), to_categorical(y[val_idx])], \n                                batch_size=BS, epochs=EPOCHS, verbose=0)\n        except Exception as e:\n            model.summary()\n            raise e\n        histories.append(history)\n        \n        pred_cv = model.predict(kreshape(X[val_idx]))\n        y_gcv[val_idx]   += pred_cv \n        score  = accuracy_score(np.argmax(pred_cv, axis=1), y[val_idx])\n        print(f'Scored {score:.3f} on validation data')\n        \n        y_test += model.predict(kreshape(X_test))\n        \n    return histories, model, np.argmax(y_test, axis=1), np.argmax(y_gcv, axis=1)\n\nhistories, kmodel, pred_test_kfold, pred_gcv= k_folds(X, y, scaler.transform(df_features_test.values) )\n#history = model.fit(kreshape(X[idx_tr]), to_categorical(y[idx_tr]), \n#              epochs=EPOCHS, batch_size=BS, verbose=0,\n#              validation_data=[kreshape(X[idx_cv]), to_categorical(y[idx_cv])])","fa6921b0":"pred_train  = np.argmax(kmodel.predict(kreshape(X)), axis=1)\nscore_train = accuracy_score(pred_train, y)\nprint(f'Global score {score_train:.3f} on TRAIN data')\n\nscore_gcv = accuracy_score(pred_gcv, y)\nprint(f'Global Kfold {score_gcv:.3f} on CV data')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,4))\nfor history in histories:\n    history_pd = pd.DataFrame.from_dict(history.history)\n    history_pd[['loss', 'val_loss']].plot(kind='line', ax=ax1 )\n    history_pd[['categorical_accuracy', 'val_categorical_accuracy']].plot(kind='line', ax=ax2)\nax1.set_yscale('log')\n\nplot_confusion(y, pred_train,'Full Train set for NN')\nplot_confusion(y, pred_train,'KFold CV set for NN')\n","235af554":"le.classes_","054b47f6":"ntest = 3816\nLB_ratio = {\n    'carpet':0.065,\n    'concrete':0.165,\n    'fine_concrete': 0.095,\n    'hard_tiles': 0.065,\n    'hard_tiles_large_space': 0.105,\n    'soft_pvc':   0.174,   #  values slightly tweaked\n    'soft_tiles': 0.2321, #  to get nexamples total\n    'tiled': 0.035,\n    'wood': 0.065,\n}\nLB = np.concatenate( [ [surf]*int(LB_ratio[surf]*ntest) for surf in LB_ratio] )\nprint(LB.size)\n\nfig = plt.figure(figsize=(14,4))\ndf_count=pd.DataFrame({'TRAIN':y_train.surface, \n                       'CV':le.inverse_transform(pred_gcv),\n                       'TEST':le.inverse_transform(pred_test_kfold[0:3810]),\n                       'TEST73':y_test73.surface[0:3810],\n                       'LB':LB[0:3810]})\nsns.countplot(x='value', hue='variable', data=pd.melt(df_count), order=le.classes_, )\nplt.title(\"KFold CV prediction + GLobal TRAIN predictions\")\n\n\nprint(\"Test73 CMP =\", accuracy_score(y_test73.surface.values, le.inverse_transform(pred_test_kfold)))\n\nprint(\"Estimated max accuracy for TEST set from LB hack:\",\n     1.0 - np.sum([ np.abs(LB_ratio[surf] - \n                           np.sum(pred_test_kfold==le.transform([surf]))\/len(pred_test_kfold))\n                    for surf in LB_ratio ]))\n","143c41c4":"df = pd.read_csv(\"..\/input\/career-con-2019\/sample_submission.csv\")\ndf['surface'] = le.inverse_transform(pred_test_kfold)\ndf.to_csv('submission.csv', index=False)\ndf.head()","e40ba19c":"y_test73.head()","9eed986c":"## Conversion quaternions -> Euler angles\n\nReusing code snipet from kernel [#1 Smart Robots. Complete Notebook \ud83e\udd16 [0.73]](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73) and improve it with numpy vectorization support\n\n![](https:\/\/flowvisioncfd.com\/webhelp\/fven_30905\/object_prop_ship__e_zoom41.png)\nImage credit: flowvisioncfd.com (idea by Nanashi)","5e7a2457":"## Build features from time domain data","21419bcb":"**Note**:\n\nbellow I've added\n* The global train score form NN model applyied in full training set\n* The global CV score which is a KFold by-product\n* The 0.73 values from Nanashi demonstrating how a 0.73 classifier behaves\n* the pretty cool [Public leaderboard distribution](https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/84760) hack values for comparison.","13473f96":"## Run K-Fold","b4d6b937":"## Learing metrics","5295be12":"## NN (better) model definition","17cbd20a":"## Features engineering with NN \ud83d\ude05\n\nHi there!\n\nIn this kernel I followed the now classic path defined by [Nanashi](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73) and tested an approach based on a SVM classifier (bad) and a second (better) on a simple 2 layers keras Neural Network.\n\nThe NN approach using small batches should be robust (in theory) to group-vs-surface detection as pointed out by Markus in [The Missing Link...](https:\/\/www.kaggle.com\/friedchips\/the-missing-link)  (just go see this one if you have not already).  \n\nThe NN setting:\n* Stochastic gradient descent and shuffled training set\n* K-Fold\n* Dropout layers which have shown an astounding improvement in getting the Train and Validation set accuracy much more consistent\n\nThe network model provide `0.77` accuracy on the K-Fold global CV set. **But** When I submit this one I get  a frustrating  `0.34` score on test set: generalization Failed \ud83d\ude05 !\n\nFurther inspection of the values from the LeaderBord hack scores are consistent with this behaviour.\nShowing that the NN model fail to detect the expected number of surfaces in each category.\nHence this model probably detect the group correctly, but not the surface.\n\nAs we are here to learn I make this version public, to push the discussion further.\n","1f8bbbe4":"## Train SVC (bad) model"}}