{"cell_type":{"41dad336":"code","606fe9ad":"code","2089f155":"code","b49bd0b2":"code","03c3e34e":"code","97bcbbb0":"code","9d3c2489":"code","153b2eb9":"code","40c56ce9":"code","ad221697":"code","262e6c7a":"code","d6957b5a":"code","55ed8b3c":"code","cbee7f52":"code","0f262439":"markdown","3fd0993c":"markdown","8d9e59c3":"markdown","7f286a90":"markdown"},"source":{"41dad336":"\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nfrom torch import nn,optim\nfrom torchvision import models,transforms\n","606fe9ad":"vgg=models.vgg19(pretrained=True).features\n\n# Freeze the Features of VGG\nfor param in vgg.parameters():\n    param.requires_grad_(False)","2089f155":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg.to(device)","b49bd0b2":"def load_image(img_path,max_size=400,shape=None):\n    \n    image=Image.open(img_path).convert('RGB')\n    \n    if max(image.size)>max_size:\n        size=max_size\n    else:\n        size=max(image.size)\n    \n    if shape is not None:\n        size=shape\n    \n    in_transform=transforms.Compose([transforms.Resize(size),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize((0.485,0.456,0.406),(0.299,0.224,0.225))])\n    image=in_transform(image)[:3,:,:].unsqueeze(0)\n    \n    return image","03c3e34e":"import os\n!ls ..\/input\/","97bcbbb0":"content=load_image(\"..\/input\/IainM_untitled2copy1.jpg\").to('cuda')\nstyle=load_image('..\/input\/candy.jpg',shape=content.shape[-2:]).to('cuda')","9d3c2489":"def im_convert(tensor):\n    image=tensor.to(\"cpu\").clone().detach()\n    image=image.numpy().squeeze()\n    image=image.transpose(1,2,0)\n    image=image*np.array((0.229,0.224,0.225))+np.array((0.485,0.456,0.406))\n    image=image.clip(0,1)\n    \n    return image\n","153b2eb9":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nax1.imshow(im_convert(content))\nax2.imshow(im_convert(style))","40c56ce9":"def get_features(image,model,layers=None):\n    \n    if layers is None:\n        layers = {'0': 'conv1_1',\n                  '5': 'conv2_1', \n                  '10': 'conv3_1', \n                  '19': 'conv4_1',\n                  '21': 'conv4_2',  \n                  '28': 'conv5_1'}\n        \n    features = {}\n    x = image\n    for name, layer in model._modules.items():\n        x = layer(x)\n        if name in layers:\n            features[layers[name]] = x\n            \n    return features","ad221697":"def gram_matrix(tensor):\n    \n    _,d,h,w=tensor.size()\n    \n    tensor=tensor.view(d,w*h)       # Tensor *transpose Tensor \n    \n    return torch.mm(tensor,tensor.t())","262e6c7a":"content_features=get_features(content,vgg)     #Extract Features \n\nstyle_features=get_features(style,vgg)\n\nstyle_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n\ntarget = content.clone().requires_grad_(True).to(\"cuda\")\n","d6957b5a":"style_weights = {'conv1_1': 1.,\n                 'conv2_1': 0.75,\n                 'conv3_1': 0.2,\n                 'conv4_1': 0.2,\n                 'conv5_1': 0.2}\n\ncontent_weight = 1  # alpha\nstyle_weight = 1e6  # beta\n","55ed8b3c":"show_every = 1000\n\n# iteration hyperparameters\noptimizer = optim.Adam([target], lr=0.003)\nsteps = 5000  # decide how many iterations to update your image (5000)\n\nfor ii in range(1, steps+1):\n    \n    # get the features from your target image\n    target_features = get_features(target, vgg)\n    \n    # the content loss\n    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n    \n    # the style loss\n    # initialize the style loss to 0\n    style_loss = 0\n    # then add to it for each layer's gram matrix loss\n    for layer in style_weights:\n        # get the \"target\" style representation for the layer\n        target_feature = target_features[layer]\n        target_gram = gram_matrix(target_feature)\n        _, d, h, w = target_feature.shape\n        # get the \"style\" style representation\n        style_gram = style_grams[layer]\n        # the style loss for one layer, weighted appropriately\n        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n        # add to the style loss\n        style_loss += layer_style_loss \/ (d * h * w)\n        \n    # calculate the *total* loss\n    total_loss = content_weight * content_loss + style_weight * style_loss\n    \n    # update your target image\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    # display intermediate images and print the loss\n    if  ii % show_every == 0:\n        print('Total loss: ', total_loss.item())\n        plt.imshow(im_convert(target))\n        plt.show()","cbee7f52":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\nax1.imshow(im_convert(content))\nax2.imshow(im_convert(target))","0f262439":"### Load pretrained VGG Model ","3fd0993c":"### Import important Packages","8d9e59c3":"### Run GPU and Pass VGG to Cuda","7f286a90":"# Neural Style Transfer\n\n###  Is an optimization technique used to take three images, a content image, a stylereference image (such as an artwork by a famous painter), and the input image you want to style and blend them together such that the input image is transformed to look like the content image, but \u201cpainted\u201d in the style image.\n   ### Using VGG19 model"}}