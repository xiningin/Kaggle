{"cell_type":{"b7966461":"code","00b7e567":"code","829a4386":"code","b4879899":"code","7ee4dd67":"code","7e4421b8":"code","2dd6c0f5":"code","23617fe6":"code","968fdf01":"code","ce2ece32":"code","ae349f71":"code","d0d6a8fc":"code","d8d4eb9b":"code","273d4eb0":"code","dd4a0df4":"code","993cc2cc":"markdown"},"source":{"b7966461":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","00b7e567":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","829a4386":"df = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv', encoding='latin-1')\ndf = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)","b4879899":"df.head()","7ee4dd67":"from wordcloud import WordCloud, STOPWORDS\nimport PIL\nimport itertools\nimport matplotlib.pyplot as plt\n\nraw_str=df[df['v1']=='spam']['v2']\nraw_str=' '.join(raw_str)\nwordcloud = WordCloud(max_words=800,margin=0,stopwords=STOPWORDS, background_color='white',collocations=False).generate(raw_str)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","7e4421b8":"ham_str=\" \".join(df[df['v1']=='ham']['v2'])\nwordcloud = WordCloud( max_words=1000,margin=0,stopwords=STOPWORDS,background_color='white').generate(ham_str)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","2dd6c0f5":"df = df.replace(['ham','spam'],[0, 1])","23617fe6":"X = df.iloc[:, 1].values \ny = df.v1.values","968fdf01":"def process(x):\n    processed_msg = []\n \n    for i in range(0, len(x)):\n        l = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))', ' ',str(x[i]))\n        l=re.sub(r'[^a-zA-Z]',' ',l)\n        l=re.sub(r'\\s+',' ',l)\n        l=l.lower()\n\n        processed_msg.append(l)\n    return processed_msg","ce2ece32":"A=process(X)","ae349f71":"from sklearn.model_selection import train_test_split  \nX_train, X_test, y_train, y_test = train_test_split(A, y, test_size=0.2, random_state=0)","d0d6a8fc":"tfidfconverter = TfidfVectorizer(max_features=3000, min_df=4, max_df=0.9, stop_words=stopwords.words('english'))  \na = tfidfconverter.fit_transform(X_train).toarray()\nXtest = tfidfconverter.transform(X_test).toarray()","d8d4eb9b":"from sklearn.linear_model import LogisticRegression\nlogmodel=LogisticRegression()\nlogmodel.fit(a, y_train)","273d4eb0":"predictions = logmodel.predict(Xtest)","dd4a0df4":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions))  \nprint(classification_report(y_test,predictions))  \nprint(accuracy_score(y_test, predictions)) #0.9659192825112107","993cc2cc":"Applying Logistic Regression"}}