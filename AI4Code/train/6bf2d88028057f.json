{"cell_type":{"bf35509f":"code","faf9c80b":"code","53f44a80":"code","ed869f0c":"code","cbf2fcee":"code","e642db8d":"code","671d9c2d":"code","91f6d21b":"code","ba994bfe":"code","fbef7314":"code","d5733aed":"code","f497d752":"code","29a08bbd":"code","b43dca09":"code","d7f0ab09":"code","107d9449":"code","c18707f5":"code","aff76ad5":"code","a3f07f74":"code","9f772a9a":"code","1639ff42":"code","f136427f":"code","aedf09b9":"code","fde02dba":"code","11987286":"code","f99884ea":"code","e112f2ea":"code","ee5048ec":"code","521c7f5b":"code","8cc23550":"code","02726624":"code","707fad90":"code","2c05f243":"code","b793ac4d":"code","30fe833e":"code","90f16117":"code","4de1c751":"code","0f0c2bad":"code","5e01d094":"code","67e69581":"code","f8b77b0d":"code","d35678bc":"code","a6130e7c":"code","e85c59e9":"code","9cfc8e10":"code","e6dc56e2":"code","f35be037":"code","07343163":"code","ac93a692":"code","b3e92c5e":"code","158619ce":"code","eda05a3e":"code","b7dc9785":"code","38d3fd09":"code","0b69dba0":"code","a8ecb3f7":"code","eab48312":"code","a0361c87":"code","ce8929fc":"code","7a5780ae":"code","29497c99":"code","8739c16e":"code","8bca5586":"code","8aea9599":"code","3f84b827":"code","6da5728e":"code","7c14998a":"code","aec416ab":"code","d7803a4f":"code","25fa5a73":"code","64f2bbca":"code","19d01784":"code","8dd2602f":"code","11f44dbe":"markdown","84b6d943":"markdown","33600383":"markdown","d95a7d7c":"markdown","b6afe975":"markdown","d53ea2cf":"markdown","1ffb2bd2":"markdown","214d2aa2":"markdown","ce911edf":"markdown","1a7f15ad":"markdown","f1c2c68d":"markdown","06e25d4e":"markdown","6fdea6a4":"markdown","d5b526e6":"markdown","139fd055":"markdown","a04cb5a0":"markdown","0a14f87e":"markdown","d89e6753":"markdown","cd84806c":"markdown","78126b1a":"markdown","24eada2a":"markdown","c161d220":"markdown","72342b02":"markdown","13231fea":"markdown","c4f2fd26":"markdown","0663bd9d":"markdown","2cd94faa":"markdown","11acbd81":"markdown","849bdf65":"markdown","300101f3":"markdown","14066712":"markdown","3b6262ef":"markdown","44cf64e2":"markdown","c2e97cb0":"markdown","a3b11700":"markdown","bf3ed4da":"markdown","8cabd4d3":"markdown","cbad333e":"markdown","0e19a0c7":"markdown","016af7a1":"markdown","6607ebfc":"markdown","7557b7da":"markdown","62621565":"markdown","0db8df18":"markdown","87c068bb":"markdown","d10680da":"markdown","5389359d":"markdown","ca16687f":"markdown","74368371":"markdown","141e987e":"markdown","7a817591":"markdown","78343052":"markdown","f04c5af2":"markdown","e80a54a4":"markdown","13c97960":"markdown","2e8005cc":"markdown","c70cb651":"markdown","3689deb4":"markdown","e9084973":"markdown","25d87dcd":"markdown","0eefca98":"markdown","c19f4a80":"markdown","26496000":"markdown","99756f4c":"markdown","87a7785a":"markdown","0f96250e":"markdown","9741f476":"markdown"},"source":{"bf35509f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\n#Showing full path of datasets\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n#Disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')","faf9c80b":"df = pd.read_csv(\"\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")","53f44a80":"df.head()","ed869f0c":"#Number of rows and columns in our dataset\ndf.shape","cbf2fcee":"#The 24 columns \ndf.columns","e642db8d":"#As mentioned in the dataset description , \n#we should exclude the variable Risk-MM when training a binary classification model.\n#Not excluding it will leak the answers to your model and reduce its predictability.\n\ndf.drop(['RISK_MM'],axis=1,inplace=True)","671d9c2d":"#Basic Information of dataset\n\ndf.info()","91f6d21b":"#Before looking at the description of the data\n#We can see that there are few columns with very less data\n#Evaporation,Sunshine,Cloud9am,Cloud3pm\n#It is better to remove these four columns as it will affect our prediction even if we\n#fill the na values...\n\n#Date and Location is also not required\n#As we are predicting rain in australia and not when and where in australia\n\n\ndrop_cols = ['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date','Location']\n\ndf.drop(columns=drop_cols,inplace=True,axis=1)","ba994bfe":"df.info()","fbef7314":"#Basic description of our data\n#Numerical features first\ndf.describe()","d5733aed":"#Including Categorical features with include object\ndf.describe(include='object')","f497d752":"#Now including all the features\ndf.describe(include='all')","29a08bbd":"#Our dataset consists of 142193 rows and the count for many features is less than 142193.\n#This shows presence of Null values.\n#Let's look at the null values..\n\ndf.isna().sum()","b43dca09":"df.skew()","d7f0ab09":"#Filling missing values\n\n#We can see that there are outliers in our data\n#So the best way to fill the na values in our numerical features is with median\n#Because median deals the best with outliers\n\n#Let's separate numerical and categorical\n#data type of numerical features is equal to float64\n#With the help of following list comprehension we separate the numerical features...\n\nnum = [col for col in df.columns if df[col].dtype==\"float64\"]\n\nfor col in num:\n    df[col].fillna(df[col].median(),inplace=True)\n    \ncat = [col for col in df.columns if df[col].dtype==\"O\"]\nfor col in cat:\n    df[col].fillna(df[col].mode()[0],inplace=True)","107d9449":"#Check missing values\ndf.isna().sum()","c18707f5":"df.corr().style.background_gradient(cmap=\"Reds\")","aff76ad5":"#With the use of heatmap\ncorr = df.corr()\n\nfig = plt.figure(figsize=(12,12))\nsns.heatmap(corr,annot=True,fmt=\".1f\",linewidths=\"0.1\")","a3f07f74":"print(\"Numerical features :: {}\\n\".format(num))\nprint(\"No of Numerical features :: {}\".format(len(num)))","9f772a9a":"plt.figure(figsize=(15,15))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\ncolors = ['Red','Blue','Green','Cyan',\n         'Red','Blue','Green','Cyan',\n         'Red','Blue','Green','Cyan']\nj=0\nfor col in num:\n    plt.subplot(3,4,i)\n    a1 = sns.distplot(df[col],color=colors[j])\n    i+=1\n    j+=1","1639ff42":"plt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\nfor col in num:\n    plt.subplot(6,2,i)\n    a1 = sns.boxplot(data=df,x=\"RainTomorrow\",y=col)\n    i+=1","f136427f":"#Create a loop that finds the outliers in train and test  and removes it\nfeatures_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']\n\nfor col in features_to_examine:\n    IQR = df[col].quantile(0.75) - df[col].quantile(0.25) \n    Lower_Bound = df[col].quantile(0.25) - (IQR*3)\n    Upper_Bound = df[col].quantile(0.75) + (IQR*3)\n    \n    print(\"The outliers in {} feature are values <<< {} and >>> {}\".format(col,Lower_Bound,Upper_Bound))\n    \n    minimum = df[col].min()\n    maximum = df[col].max()\n    print(\"The minimum value in {} is {} and maximum value is {}\".format(col,minimum,maximum))\n    \n    if maximum>Upper_Bound:\n          print(\"The outliers for {} are value greater than {}\\n\".format(col,Upper_Bound))\n    elif minimum<Lower_Bound:\n          print(\"The outliers for {} are value smaller than {}\\n\".format(col,Lower_Bound))","aedf09b9":"plt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\nfor col in num:\n    plt.subplot(6,2,i)\n    a1 = sns.barplot(data=df,x=\"RainTomorrow\",y=col)\n    i+=1","fde02dba":"plt.figure(figsize=(15,5))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\nfeatures_list = [\"MaxTemp\",\"Temp9am\",\"Temp3pm\"]\nfor feature in features_list:\n    plt.subplot(1,3,i)\n    sns.scatterplot(data=df,x=\"MinTemp\",y=feature,hue=\"RainTomorrow\")\n    i+=1","11987286":"plt.figure(figsize=(15,8))\nplt.subplots_adjust(hspace=0.5)\n\nplt.subplot(3,2,1)\nsns.scatterplot(data=df,x=\"WindSpeed9am\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,2)\nsns.scatterplot(data=df,x=\"WindSpeed3pm\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,3)\nsns.scatterplot(data=df,x=\"Humidity9am\",y=\"Humidity3pm\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,4)\nsns.scatterplot(data=df,x=\"Temp9am\",y=\"Temp3pm\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,5)\nsns.scatterplot(data=df,x=\"MaxTemp\",y=\"Temp9am\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,6)\nsns.scatterplot(data=df,x=\"Humidity3pm\",y=\"Temp3pm\",hue=\"RainTomorrow\")","f99884ea":"cat","e112f2ea":"df['WindGustDir'].value_counts()","ee5048ec":"fig = plt.figure(figsize=(15,5))\nsns.countplot(data=df,x=\"WindGustDir\",hue=\"RainTomorrow\");","521c7f5b":"df['WindDir9am'].value_counts()","8cc23550":"fig = plt.figure(figsize=(15,5))\nsns.countplot(data=df,x=\"WindDir9am\",hue=\"RainTomorrow\");","02726624":"df['WindDir3pm'].value_counts()","707fad90":"fig = plt.figure(figsize=(15,5))\nsns.countplot(data=df,x=\"WindDir3pm\",hue=\"RainTomorrow\");","2c05f243":"df['RainTomorrow'].value_counts()","b793ac4d":"sns.countplot(data=df,x=\"RainTomorrow\")","30fe833e":"from sklearn.model_selection import train_test_split as tts\ny=df[['RainTomorrow']]\nX=df.drop(['RainTomorrow'],axis=1)\n\nX_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=0)","90f16117":"X_train","4de1c751":"X_test","0f0c2bad":"#We'll plot these four as subplots \n\nplt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\nfeatures_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']\ni=1\nfor col in features_to_examine:\n    plt.subplot(6,2,i)\n    fig = df[col].hist(bins=10)\n    fig.set_xlabel(col)\n    fig.set_ylabel('RainTomorrow')\n    i+=1","5e01d094":"def remove_outliers(df,col,Lower_Bound,Upper_Bound):    \n    minimum = df[col].min()\n    maximum = df[col].max()\n    \n    if maximum>Upper_Bound:\n        return np.where(df[col]>Upper_Bound,Upper_Bound,df[col])\n          \n    elif minimum<Lower_Bound:\n        return np.where(df[col]<Lower_Bound,Lower_Bound,df[col])","67e69581":"for df1 in [X_train,X_test]:\n    df1['Rainfall'] = remove_outliers(df1,'Rainfall',-1.799,2.4)\n    df1['WindGustSpeed'] = remove_outliers(df1,'WindGustSpeed',-14.0,91.0)\n    df1['WindSpeed9am'] = remove_outliers(df1,'WindSpeed9am',-29.0,55.0)\n    df1['WindSpeed3pm'] = remove_outliers(df1,'WindSpeed3pm',-20.0,57.0)","f8b77b0d":"#If we look at their boxplots we can see that the outliers are now capped...\nplt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\nfeatures_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']\ni=1\nfor col in features_to_examine:\n    plt.subplot(6,2,i)\n    fig = sns.boxplot(data=X_train,y=col)\n    fig.set_xlabel(col)\n    fig.set_ylabel('RainTomorrow')\n    i+=1","d35678bc":"#Describe helps us understand more about the mean and max values\n\nX_train[features_to_examine].describe()","a6130e7c":"X_test[features_to_examine].describe()","e85c59e9":"#Our next step is to encode all the categorical variables.\n#first we will convert our target variable\n\nfor df2 in [y_train,y_test]:\n    df2['RainTomorrow'] = df2['RainTomorrow'].replace({\"Yes\":1,\n                                                    \"No\":0})\n\n","9cfc8e10":"import category_encoders as ce\n\nencoder = ce.BinaryEncoder(cols=['RainToday'])\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","e6dc56e2":"#Now we will make our training dataset\n\nX_train = pd.concat([X_train[num],X_train[['RainToday_0','RainToday_1']],\n                    pd.get_dummies(X_train['WindGustDir']),\n                    pd.get_dummies(X_train['WindDir9am']),\n                    pd.get_dummies(X_train['WindDir3pm'])],axis=1)\n","f35be037":"X_train.head()","07343163":"#Same for testing set\n\nX_test = pd.concat([X_test[num],X_test[['RainToday_0','RainToday_1']],\n                    pd.get_dummies(X_test['WindGustDir']),\n                    pd.get_dummies(X_test['WindDir9am']),\n                    pd.get_dummies(X_test['WindDir3pm'])],axis=1)","ac93a692":"X_test.head()","b3e92c5e":"#our training and testing set is ready for our model\n#But ,before that we need to bring all the features to same scale with feature scaling\n#For this we will use MinMaxScaler\n#As there our negative values in our dataset and MinMaxScaler scales our data in range -1 to 1.\n\ncols = X_train.columns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","158619ce":"X_train = pd.DataFrame(X_train,columns=cols)\nX_test = pd.DataFrame(X_test,columns=cols)","eda05a3e":"from sklearn.linear_model import LogisticRegression\n\n# instantiate the model\nlogreg = LogisticRegression(solver='liblinear', random_state=0)\n\n\n# fit the model\nlogreg.fit(X_train, y_train)","b7dc9785":"#Prediction on Xtest\n\ny_pred_test = logreg.predict(X_test)\n\ny_pred_test","38d3fd09":"#using predict_proba gives the probability value for the target feature\n\nlogreg.predict_proba(X_test)","0b69dba0":"#probability of getting no rain (0)\n\nlogreg.predict_proba(X_test)[:,0]","a8ecb3f7":"#probability of getting rain (1)\n\nlogreg.predict_proba(X_test)[:,1]","eab48312":"#Check accuracy with accuracy_score\n\nfrom sklearn.metrics import accuracy_score\n\npredict_test = accuracy_score(y_test,y_pred_test)\n\nprint(\"Accuracy of model on test set :: {}\".format(predict_test))","a0361c87":"#Creating confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test, y_pred_test)\nprint(confusion_matrix)","ce8929fc":"#Classification report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_test))","7a5780ae":"#Comparing train and test accuracy\n\ny_pred_train = logreg.predict(X_train)\ny_pred_train","29497c99":"#Check accuracy of our model with train set\n\npredict_train = accuracy_score(y_train,y_pred_train)\nprint(\"Accuracy of our model on train set :: {}\".format(predict_train))","8739c16e":"#Overall Accuracy\n\nprint(\"Accuracy of our model :: {}\".format(logreg.score(X_test,y_test)))","8bca5586":"#Let's try to improve the accuracy of our model\n\n#Let's try different C values\n\n#Now what is C","8aea9599":"#C=100\n\n# instantiate the model\nlogreg100 = LogisticRegression(solver='liblinear',C=100, random_state=0)\n\n\n# fit the model\nlogreg100.fit(X_train, y_train)\n\n#Prediction on Xtest\n\ny_pred_test = logreg100.predict(X_test)\n\ny_pred_test","3f84b827":"predict_test = accuracy_score(y_test,y_pred_test)\n\nprint(\"Accuracy of model on test set :: {}\".format(predict_test))","6da5728e":"#Overall Accuracy\n\nprint(\"Accuracy of our model :: {}\".format(logreg100.score(X_test,y_test)))","7c14998a":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test, y_pred_test)\nprint(confusion_matrix)","aec416ab":"#Classification report\nprint(classification_report(y_test, y_pred_test))","d7803a4f":"#Let's increase the regularization strength\n\n#C=0.01\n\n# instantiate the model\nlogreg001 = LogisticRegression(solver='liblinear',C=0.01, random_state=0)\n\n\n# fit the model\nlogreg001.fit(X_train, y_train)\n\n#Prediction on Xtest\n\ny_pred_test = logreg001.predict(X_test)\n\ny_pred_test","25fa5a73":"predict_test = accuracy_score(y_test,y_pred_test)\n\nprint(\"Accuracy of model on test set :: {}\".format(predict_test))","64f2bbca":"#Overall Accuracy\n\nprint(\"Accuracy of our model :: {}\".format(logreg001.score(X_test,y_test)))","19d01784":"# store the predicted probabilities for class 1 - Probability of rain\n\ny_pred1 = logreg100.predict_proba(X_test)[:, 1]\ny_pred0 = logreg100.predict_proba(X_test)[:, 0]","8dd2602f":"# plot histogram of predicted probabilities\n\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n\n# plot histogram with 10 bins\nplt.hist(y_pred1, bins = 10)\nplt.hist(y_pred0, bins = 10)\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities')\n\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n#Set legend\nplt.legend('upper left' , labels = ['Rain','No Rain'])\n\n# set the title\nplt.xlabel('Predicted probabilities')\nplt.ylabel('Frequency')","11f44dbe":"# Import Dataset","84b6d943":"#### Let's see the probability of raining with the help of histogram","33600383":"# Import Libraries","d95a7d7c":"#### This kernel predicts whether it will rain tomorrow.","b6afe975":"# Rain in Australia","d53ea2cf":"# Heatmap","1ffb2bd2":"### 84% accuracy is good but we can still improve it","214d2aa2":"## Head","ce911edf":"#### We can see a decrease in our model accuracy with C=0.01","1a7f15ad":"**Finally ,after removing outliers,encoding the categorical variables and scaling**\n\n**Our training and testing sets are ready**","f1c2c68d":"### WindGustDir","06e25d4e":"#### I'm a newbie in ML,if you like it, please upvote :)","6fdea6a4":"#### We can see a slight increase in our model with C=100","d5b526e6":"# Boxplot","139fd055":"## Columns","a04cb5a0":"## Shape","0a14f87e":"# Correlation","d89e6753":"#### First we will remove any outliers present in our data","cd84806c":"## Study the numerical features","78126b1a":"### We can see somewhat same score for both out training and testing datasets using this model","24eada2a":"# Model Training","c161d220":"**35,886 Correct predictions**\n\n**6,792 Incorrect predictions**","72342b02":"**Features that are less correlated with other features**\n\n* Rainfall\n* Pressure9am\n* Pressure3pm","13231fea":"**With the help of skewness values and above box plots,Rainfall,WindGustSpeed,WindSpeed9am and WindSpeed3pm**\n**may contain outliers**","c4f2fd26":"### PLEASE GIVE A UPVOTE IF YOU LIKE THIS KERNEL :)","0663bd9d":"#### Splitting the data into **training** and **testing** sets","2cd94faa":"# Barplots","11acbd81":"## Info","849bdf65":"#### Data after dropping columns","300101f3":"### The result is telling us that we have 31308+4554 correct predictions and 5068+1728 incorrect predictions.","14066712":"Encode **RainToday** variable","3b6262ef":"## Null Values","44cf64e2":"**35,862 correct predictions.**\n\n**6796 Incorrect predictions.**","c2e97cb0":"* Rainfall and Evaporation as seen with skewness value (9.88) and (3.74) are right skewed as seen above.\n* Cloud9am and Cloud3pm behave as categorical features.","a3b11700":"# Feature Engineering","bf3ed4da":"#### Drop Risk_MM column","8cabd4d3":"* Now let's reduce the regularization strength","cbad333e":"#### We can see increase in correct predictions and decrease in incorrect predictions","0e19a0c7":"# Find the outliers","016af7a1":"#### C is inverse of regularization strength.","6607ebfc":"* There are no missing values present now and we can start our analysis.","7557b7da":"#### We have found the outliers in Rainfall,WindGustSpeed,WindSpeed9am and WindSpeed3pm.\n**We Will cap these outliers now**","62621565":"### WindDir3pm","0db8df18":"**Now let's have a look at the correlated variables from the above histogram**","87c068bb":"**We can clearly see right skewed histograms in all the four**","d10680da":"* The above histogram is highly right skewed for rain\n* Highly left skewed for no rain \n* There is less chance that it will rain tomorrow as most of the predicted probabilities are near to zero.\n* Higher chance as probabilities are close to 1.\n","5389359d":"* The count is different for all the features\n* We can see difference in mean and max is huge in many features.","ca16687f":"* The features with skewness values near zero may follow gaussian distribution.\n* Rainfall is strongly right skewed (9.88)\n* We'll have a look at the distributions for further clarity.","74368371":"* Here too there are null values in our categorical data","141e987e":"## Our aim is to predict whether it will rain or not tomorrow in australia.","7a817591":"#### Higher values of C correspond to less regularization","78343052":"# Scatterplots","f04c5af2":"# Categorical Features","e80a54a4":"### WindDir9am","13c97960":"* Except Date,Location and our target feature,\n* All the other features have null values\n* We'll deal with these later in the notebook..","2e8005cc":"# Distributions of each numerical feature","c70cb651":"\n**Let's have a look at their histograms.**","3689deb4":"# Feature Scaling","e9084973":"## Target Feature","25d87dcd":"***We'll try to cap these outliers that will help us in predictions later.***","0eefca98":"* Later we'll have a look at the scatterplots of these correlated features.....","c19f4a80":"# Encode categorical variables","26496000":"# Logistic Regression","99756f4c":"## Skewness","87a7785a":"#### By default , C is equal to 1","0f96250e":"**Correlated features**\n\n* MinTemp -- MaxTemp (0.7)\n* MinTemp -- Temp3pm (0.7)\n* MaxTemp -- Temp9am (0.9)\n* WindGustSpeed -- WindSpeed9am (0.6)\n* WindGustSpeed -- WindSpeed3pm (0.7)\n* Humidity9am -- Humidity3pm (0.7)\n* Humidity3pm -- Temp3pm (-0.6)\n* Temp9am -- MinTemp (0.9)\n* Temp9am -- Temp3pm","9741f476":"Conclusion :-\n    \n    * Our model predicts that there's a higher chance of not raining tomorrow in australia as seen in the above histogram.\n    \n    * Accuracy of our model is 84%."}}