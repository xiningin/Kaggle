{"cell_type":{"9c9f5596":"code","75f4f888":"code","0716c4e9":"code","151c3a2c":"code","9388cb25":"code","2f3badd5":"code","67ee4b7a":"code","0c16af88":"code","7e0385df":"code","5654e25e":"code","50e8faec":"code","bcba095d":"code","b62d910e":"code","87177713":"code","f93e2fec":"markdown","bb9439b9":"markdown","a7e91e6a":"markdown","00629eb7":"markdown","3b374425":"markdown","252a86e5":"markdown","91cbaced":"markdown"},"source":{"9c9f5596":"# Transform Video to .npy Format\nimport os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom time import time\nfrom pathlib import Path\nfrom random import shuffle","75f4f888":"def getOpticalFlow(video):\n    \"\"\"Calculate dense optical flow of input video\n    Args:\n        video: the input video with shape of [frames,height,width,channel]. dtype=np.array\n    Returns:\n        flows_x: the optical flow at x-axis, with the shape of [frames,height,width,channel]\n        flows_y: the optical flow at y-axis, with the shape of [frames,height,width,channel]\n    \"\"\"\n    # initialize the list of optical flows\n    gray_video = []\n    for i in range(len(video)):\n        img = cv2.cvtColor(video[i], cv2.COLOR_RGB2GRAY)\n        gray_video.append(np.reshape(img,(224,224,1)))\n\n    flows = []\n    for i in range(0,len(video)-1):\n        # calculate optical flow between each pair of frames\n        flow = cv2.calcOpticalFlowFarneback(gray_video[i], gray_video[i+1], None, 0.5, 3, 15, 3, 5, 1.2, cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n        # subtract the mean in order to eliminate the movement of camera\n        flow[..., 0] -= np.mean(flow[..., 0])\n        flow[..., 1] -= np.mean(flow[..., 1])\n        # normalize each component in optical flow\n        flow[..., 0] = cv2.normalize(flow[..., 0],None,0,255,cv2.NORM_MINMAX)\n        flow[..., 1] = cv2.normalize(flow[..., 1],None,0,255,cv2.NORM_MINMAX)\n        # Add into list \n        flows.append(flow)\n        \n    # Padding the last frame as empty array\n    flows.append(np.zeros((224,224,2)))\n      \n    return np.array(flows, dtype=np.float32)","0716c4e9":"def Video2Npy(file_path, resize=(224,224)):\n    \"\"\"Load video and tansfer it into .npy format\n    Args:\n        file_path: the path of video file\n        resize: the target resolution of output video\n    Returns:\n        frames: gray-scale video\n        flows: magnitude video of optical flows \n    \"\"\"\n    # Load video\n    cap = cv2.VideoCapture(file_path)\n    # Get number of frames\n    len_frames = int(cap.get(7))\n    # Extract frames from video\n    try:\n        frames = []\n        for i in range(len_frames-1):\n            _, frame = cap.read()\n            frame = cv2.resize(frame,resize, interpolation=cv2.INTER_AREA)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = np.reshape(frame, (224,224,3))\n            frames.append(frame)   \n    except:\n        print(\"Error: \", file_path, len_frames,i)\n    finally:\n        frames = np.array(frames)\n        cap.release()\n            \n    # Get the optical flow of video\n    flows = getOpticalFlow(frames)\n    \n    result = np.zeros((len(flows),224,224,5))\n    result[...,:3] = frames\n    result[...,3:] = flows\n    \n    return result","151c3a2c":"def Save2Npy(videos, save_dir):\n    \"\"\"Transfer all the videos and save them into specified directory\n    Args:\n        videos: list of target videos with pathlib Path.\n        save_dir: destination folder of output .npy files\n    \"\"\"\n    \n    \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    # List the files\n    \n    for path in tqdm(videos):\n        # Split video name\n        name = path.stem\n        # Get dest \n        save_path = Path(save_dir) \/ (name + '.npy')\n        # os.path.join(save_dir, name + '.npy')\n        # Load and preprocess video\n        data = Video2Npy(file_path=path.as_posix(), resize=(224,224))\n        data = np.uint8(data)\n        # Save as .npy file\n        np.save(save_path.as_posix(), data)\n    \n    return None","9388cb25":"source_path = '..\/input\/violence\/Peliculas'\ntarget_path = '.\/'\n\neach_video = '.\/train\/fights'\neach_video = '.\/train\/noFights'\n\nfight_videos =list(Path('..\/input\/violence\/Peliculas\/fights').rglob('*.avi'))\nnofight_videos = list(Path('..\/input\/violence\/Peliculas\/noFights').rglob('*.mpg'))\n\nprint(len(fight_videos))\nprint(len(nofight_videos))\n\nn_fights = len(fight_videos)\ntrain_size = int(0.8 * n_fights)\n\ntrain_fights = fight_videos[:train_size]\ntrain_nofights = nofight_videos[:train_size]\nval_fights = fight_videos[train_size:]\nval_nofights = nofight_videos[train_size:]\n\nSave2Npy(train_fights, target_path+'\/train\/fight')\nSave2Npy(train_nofights, target_path+'\/train\/nofight')\nSave2Npy(val_fights, target_path+'\/val\/fights')\nSave2Npy(val_nofights, target_path+'\/val\/nofights')","2f3badd5":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","67ee4b7a":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (Input, Dense, Flatten, Conv3D, MaxPooling3D, Dropout, Multiply, Input, Lambda,\n                                     BatchNormalization)\n\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Lambda","0c16af88":"from tensorflow.keras.utils import Sequence, to_categorical\n\nclass DataGenerator(Sequence):\n    \"\"\"Data Generator inherited from keras.utils.Sequence\n    Args: \n        directory: the path of data set, and each sub-folder will be assigned to one class\n        batch_size: the number of data points in each batch\n        shuffle: whether to shuffle the data per epoch\n    Note:\n        If you want to load file with other data format, please fix the method of \"load_data\" as you want\n    \"\"\"\n    def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n        # Initialize the params\n        self.batch_size = batch_size\n        self.directory = directory\n        self.shuffle = shuffle\n        self.data_aug = data_augmentation\n        # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n        self.X_path, self.Y_dict = self.search_data() \n        # Print basic statistics information\n        self.print_stats()\n        return None\n    \n    def __len__(self):\n        # calculate the iterations of each epoch\n        steps_per_epoch = np.ceil(len(self.X_path) \/ float(self.batch_size))\n        return int(steps_per_epoch)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Get the data of each batch\n        \n        \"\"\"\n        # get the indexs of each batch\n        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # using batch_indexs to get path of current batch\n        batch_path = [self.X_path[k] for k in batch_indexs]\n        # get batch data\n        batch_x, batch_y = self.data_generation(batch_path)\n        return batch_x, batch_y\n\n    def on_epoch_end(self):\n        \"\"\"\n        shuffle the data at each end of epoch\n        \n        \"\"\"\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n        \n    def search_data(self):\n        \"\"\"\n        1 - Lists subfolders.\n        2 - takes all the names of npy files in th paths\n        3 - make the one-hot encoded labels.\n        \"\"\"\n        X_path = []\n        Y_dict = {}\n        # list all kinds of sub-folders\n        self.dirs = sorted(os.listdir(self.directory))\n        one_hots = to_categorical(range(len(self.dirs)))\n        for i,folder in enumerate(self.dirs):\n            folder_path = os.path.join(self.directory,folder)\n            for file in os.listdir(folder_path):\n                file_path = os.path.join(folder_path,file)\n                # append the each file path, and keep its label  \n                X_path.append(file_path)\n                Y_dict[file_path] = one_hots[i]\n        return X_path, Y_dict\n    \n    def print_stats(self):\n        \"\"\"\n        calculate basic information\n        \n        \"\"\"\n        self.n_files = len(self.X_path)\n        self.n_classes = len(self.dirs)\n        self.indexes = np.arange(len(self.X_path))\n        np.random.shuffle(self.indexes)\n        # Output states\n        print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.n_classes))\n        for i,label in enumerate(self.dirs):\n            print('%10s : '%(label),i)\n        return None\n    \n    def data_generation(self, batch_path):\n        \"\"\"\"\n        load data into memory, you can change the np.load to any method you want\n        \n        \"\"\"\n        batch_x = [self.load_data(x) for x in batch_path]\n        batch_y = [self.Y_dict[x] for x in batch_path]\n        # transfer the data format and take one-hot coding for labels\n        batch_x = np.array(batch_x)\n        batch_y = np.array(batch_y)\n        return batch_x, batch_y\n    \n    def normalize(self, data):\n        mean = np.mean(data)\n        std = np.std(data)\n        return (data-mean) \/ std\n    \n    def load_data(self, path):\n        # load the processed .npy files which have 5 channels (1-3 for RGB, 4-5 for optical flows)\n        data = np.load(path, mmap_mode='r')\n        data = np.float32(data)\n        # sampling 64 frames uniformly from the entire video\n        data = self.uniform_sampling(video=data, target_frames=64)\n        # whether to utilize the data augmentation\n        if  self.data_aug:\n            data[...,:3] = self.color_jitter(data[...,:3])\n            data = self.random_flip(data, prob=0.5)\n        # normalize rgb images and optical flows, respectively\n        data[...,:3] = self.normalize(data[...,:3])\n        data[...,3:] = self.normalize(data[...,3:])\n        return data\n    \n    def uniform_sampling(self, video, target_frames=64):\n        \"\"\"\n        1 - Count the number of frames.\n        2 - calculate the interval = num_frames\/target_frames\n        3 - Take frames from video after skipping interval frames in between.\n        \n        \"\"\"\n        # get total frames of input video and calculate sampling interval \n        len_frames = int(len(video))\n        interval = int(np.ceil(len_frames\/target_frames))\n        # init empty list for sampled video and \n        sampled_video = []\n        for i in range(0,len_frames,interval):\n            sampled_video.append(video[i])     \n        # calculate numer of padded frames and fix it \n        num_pad = target_frames - len(sampled_video)\n        padding = []\n        if num_pad>0:\n            for i in range(-num_pad,0):\n                try: \n                    padding.append(video[i])\n                except:\n                    padding.append(video[0])\n            sampled_video += padding     \n        # get sampled video\n        return np.array(sampled_video, dtype=np.float32)\n    \n    def random_flip(self, video, prob):\n        \"\"\"\n        # Augmentation\n        flips all the video frames\n        \"\"\"\n        s = np.random.rand()\n        if s < prob:\n            video = np.flip(m=video, axis=2)\n        return video\n    \n    def random_clip(self, video, target_frames=64):\n        \"\"\"\n        # Augmentation\n        randomly clips some part of video.\n        \"\"\"\n        start_point = np.random.randint(len(video)-target_frames)\n        return video[start_point:start_point+target_frames]\n    \n    def dynamic_crop(self, video):\n        \"\"\"\n        # Augmentation\n        \n        \"\"\"\n        # extract layer of optical flow from video\n        opt_flows = video[...,3]\n        # sum of optical flow magnitude of individual frame\n        magnitude = np.sum(opt_flows, axis=0)\n        # filter slight noise by threshold \n        thresh = np.mean(magnitude)\n        magnitude[magnitude<thresh] = 0\n        # calculate center of gravity of magnitude map and adding 0.001 to avoid empty value\n        x_pdf = np.sum(magnitude, axis=1) + 0.001\n        y_pdf = np.sum(magnitude, axis=0) + 0.001\n        # normalize PDF of x and y so that the sum of probs = 1\n        x_pdf \/= np.sum(x_pdf)\n        y_pdf \/= np.sum(y_pdf)\n        # randomly choose some candidates for x and y \n        x_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=x_pdf)\n        y_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=y_pdf)\n        # get the mean of x and y coordinates for better robustness\n        x = int(np.mean(x_points))\n        y = int(np.mean(y_points))\n        # avoid to beyond boundaries of array\n        x = max(56,min(x,167))\n        y = max(56,min(y,167))\n        # get cropped video \n        return video[:,x-56:x+56,y-56:y+56,:]  \n    \n    def color_jitter(self,video):\n        \"\"\"\n        # Augmentation\n        \n        \"\"\"\n        # range of s-component: 0-1\n        # range of v component: 0-255\n        s_jitter = np.random.uniform(-0.2,0.2)\n        v_jitter = np.random.uniform(-30,30)\n        for i in range(len(video)):\n            hsv = cv2.cvtColor(video[i], cv2.COLOR_RGB2HSV)\n            s = hsv[...,1] + s_jitter\n            v = hsv[...,2] + v_jitter\n            s[s<0] = 0\n            s[s>1] = 1\n            v[v<0] = 0\n            v[v>255] = 255\n            hsv[...,1] = s\n            hsv[...,2] = v\n            video[i] = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n        return video","7e0385df":"\n# extract the rgb images \ndef get_rgb(input_x):\n    rgb = input_x[...,:3]\n    return rgb\n\n# extract the optical flows\ndef get_opt(input_x):\n    opt= input_x[...,3:5]\n    return opt","5654e25e":"inputs = Input(shape=(64,224,224,5))\n\nrgb = Lambda(get_rgb,output_shape=None)(inputs)\nopt = Lambda(get_opt,output_shape=None)(inputs)\n\n##################################################### RGB channel\nrgb = Conv3D(\n    16, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    16, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\nrgb = Conv3D(\n    16, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    16, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\nrgb = Conv3D(\n    32, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    32, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\nrgb = Conv3D(\n    32, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    32, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\n##################################################### Optical Flow channel\nopt = Conv3D(\n    16, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(opt)\nopt = Conv3D(\n    16, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(opt)\nopt = MaxPooling3D(pool_size=(1,2,2))(opt)\n\nopt = Conv3D(\n    16, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(opt)\nopt = Conv3D(\n    16, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(opt)\nopt = MaxPooling3D(pool_size=(1,2,2))(opt)\n\nopt = Conv3D(\n    32, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(opt)\nopt = Conv3D(\n    32, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(opt)\nopt = MaxPooling3D(pool_size=(1,2,2))(opt)\n\nopt = Conv3D(\n    32, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='sigmoid', padding='same')(opt)\nopt = Conv3D(\n    32, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='sigmoid', padding='same')(opt)\nopt = MaxPooling3D(pool_size=(1,2,2))(opt)\n\n\n##################################################### Fusion and Pooling\nx = Multiply()([rgb,opt])\nx = MaxPooling3D(pool_size=(8,1,1))(x)\n\n##################################################### Merging Block\nx = Conv3D(\n    64, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = Conv3D(\n    64, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = MaxPooling3D(pool_size=(2,2,2))(x)\n\nx = Conv3D(\n    64, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = Conv3D(\n    64, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = MaxPooling3D(pool_size=(2,2,2))(x)\n\nx = Conv3D(\n    128, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = Conv3D(\n    128, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = MaxPooling3D(pool_size=(2,3,3))(x)\n\n##################################################### FC Layers\nx = Flatten()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\n\n# Build the model\npred = Dense(2, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=pred)\nmodel.summary()","50e8faec":"from keras.optimizers import Adam, SGD\n\nsgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])","bcba095d":"import keras.backend as K\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import ModelCheckpoint, CSVLogger\nimport keras\n\ndef scheduler(epoch):\n    if epoch % 8 == 0 and epoch != 0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr * 0.85)\n    return K.get_value(model.optimizer.lr)\n\nreduce_lr = LearningRateScheduler(scheduler)\n\nclass MyCbk(keras.callbacks.Callback):\n\n    def __init__(self, model):\n         self.model_to_save = model\n\n    def on_epoch_end(self, epoch, logs=None):\n        self.model_to_save.save('Logs\/model_at_epoch_%d.h5' % (epoch+1))\n\ncheck_point = MyCbk(model)\n\n\nfilename = 'Logs\/ours_log.csv'\n\ncsv_logger = CSVLogger(filename, separator=',', append=True)\n\ncallbacks_list = [check_point, csv_logger, reduce_lr]\n","b62d910e":"!mkdir Logs\n!touch Logs\/outs_log.csv","87177713":"num_epochs  = 40\nnum_workers = 6\nbatch_size  = 3\n\ntrain_generator = DataGenerator(directory='.\/train', \n                                batch_size=batch_size, \n                                data_augmentation=True)\n\nval_generator = DataGenerator(directory='.\/val',\n                              batch_size=batch_size, \n                              data_augmentation=False)\n\nhist = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    callbacks=callbacks_list,\n    verbose=1,\n    epochs=num_epochs,\n    workers=num_workers,\n    max_queue_size=4,\n    steps_per_epoch=len(train_generator),\n    validation_steps=len(val_generator))","f93e2fec":"# Model Compiling","bb9439b9":"# The model Architecture:\n\nThe network consists of 2 parallel nets:\n\n- RGB input\n- Optical flow input\n\nthe output of these networks are then multiplied to keep the RGB features which there is movement in sequence of frames. (take a look at figure 5 - cropping strategy)\n\n![Screenshot%202021-03-17%2010:38:54.png](attachment:Screenshot%202021-03-17%2010:38:54.png)\n\n\n# Cropping strategy:\n\n![Screenshot%202021-03-17%2010:39:23.png](attachment:Screenshot%202021-03-17%2010:39:23.png)\n","a7e91e6a":"# Preprocessing","00629eb7":"# Introduction\n\nThis code belongs to authors of paper RWF 2000.\nCheck the link below for more information.\n\n\nSrc: https:\/\/github.com\/mchengny\/RWF2000-Video-Database-for-Violence-Detection\n\nImage from original repository:\n\n![RWF2000](https:\/\/github.com\/mchengny\/RWF2000-Video-Database-for-Violence-Detection\/raw\/master\/Images\/blocked.gif)\n\nTo cite:\n>\"Ming Cheng, Kunjing Cai, and Ming Li. \"RWF-2000: An Open Large Scale Video Database for Violence Detection.\" arXiv preprint arXiv:1911.05913 (2019).\"\n","3b374425":"# Set Callbacks\n- Learning Rate Scheduler","252a86e5":"# Build Data Loader\n\n> Based on this article: \n> https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly\n\nOnly rewrite these functions:\n\n- def \\__len__(self):\n- def \\__getitem__(self, index):\n- def on_epoch_end(self):","91cbaced":"# Build Model"}}