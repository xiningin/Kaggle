{"cell_type":{"2e8302be":"code","3e240fb6":"code","d39040b6":"code","5ee67312":"code","2222061a":"code","309976a2":"code","e0c272fd":"code","d020fd86":"markdown","22fa5491":"markdown","f8d95366":"markdown","569d4527":"markdown","c3188cb2":"markdown","f94f165a":"markdown","9c095e47":"markdown","4c9f8f5d":"markdown","03ebbaab":"markdown"},"source":{"2e8302be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport requests\nimport warnings\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nimport datetime\nimport shap  # package used to calculate Shap values\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e240fb6":"df_g=pd.read_csv(\"\/kaggle\/input\/stock-market-google-facebook-twitter\/google_data.csv\")\ndf_f=pd.read_csv(\"\/kaggle\/input\/stock-market-google-facebook-twitter\/facebook_data.csv\")\ndf_t=pd.read_csv(\"\/kaggle\/input\/stock-market-google-facebook-twitter\/twtr_data.csv\")\ndf_g[\"date\"]=pd.to_datetime(df_g[\"date\"])\ndf_f[\"date\"]=pd.to_datetime(df_f[\"date\"])\ndf_t[\"date\"]=pd.to_datetime(df_t[\"date\"])","d39040b6":"dataset=df_g  # which dataset we use on model","5ee67312":"def calculate_vif_(X, thresh=5.0):\n    variables = list(range(X.shape[1]))\n    dropped = True\n    while dropped:\n        dropped = False\n        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n               for ix in range(X.iloc[:, variables].shape[1])]\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n                  '\\' at index: ' + str(maxloc))\n            del variables[maxloc]\n            dropped = True\n    return X.iloc[:, variables]\n\n\n\ndef RSI(df, n):\n    close = df['close']\n    delta = close.diff()\n    delta = delta[1:]\n    pricesUp = delta.copy()\n    pricesDown = delta.copy()\n    pricesUp[pricesUp < 0] = 0\n    pricesDown[pricesDown > 0] = 0\n    rollUp = pricesUp.rolling(n).mean()\n    rollDown = pricesDown.abs().rolling(n).mean()\n    rs = rollUp \/ rollDown\n    rsi = 100.0 - (100.0 \/ (1.0 + rs))\n    return rsi\n\n\ndef stochastic(df, k, d):\n    df = df.copy()\n    low_min  = df['low'].rolling(window=k).min()\n    high_max = df['high'].rolling( window=k).max()\n    df['stoch_k'] = 100 * (df['close'] - low_min)\/(high_max - low_min)\n    df['stoch_d'] = df['stoch_k'].rolling(window=d).mean()\n    return df","2222061a":"dataset.loc[(dataset[\"close\"]>dataset[\"close\"].shift(-1)), \"target\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"close\"].shift(-1)), \"target\"] =  0\ndataset[\"target\"]=dataset[\"target\"].fillna(0)\ndataset[\"target\"]=dataset[\"target\"].astype(int)\n\n\ndataset.drop([\"exchange\",\"symbol\",\"adj_volume\",\"adj_open\",\"adj_close\",\"adj_low\",\"adj_high\"],axis=1,inplace=True)\n\ndataset=dataset.sort_values(by=[\"date\"],ascending=True)\ndataset['SMA_5'] = dataset['close'].rolling(5).mean()\ndataset['SMA_7'] = dataset['close'].rolling(7).mean()\ndataset['SMA_10'] = dataset['close'].rolling(10).mean()\ndataset['SMA_12'] = dataset['close'].rolling(12).mean()\ndataset['SMA_20'] = dataset['close'].rolling(20).mean()\ndataset['SMA_21'] = dataset['close'].rolling(21).mean()\ndataset['SMA_23'] = dataset['close'].rolling(23).mean()\ndataset['SMA_26'] = dataset['close'].rolling(26).mean()\ndataset['SMA_50'] = dataset['close'].rolling(50).mean()\ndataset['SMA_200'] = dataset['close'].rolling(200).mean()\ndataset['SMA_233'] = dataset['close'].rolling(233).mean()\ndataset[\"rstd\"] = dataset['close'].rolling(20).std()\ndataset['BB_LOWER'] = dataset['SMA_20']-(2*dataset[\"rstd\"])\ndataset['BB_UPPER'] = dataset['SMA_20']+(2*dataset[\"rstd\"])\ndataset['RSI_14'] = RSI(dataset,14)\ndataset['RSI_9'] = RSI(dataset,9)\ndataset['RSI_20'] = RSI(dataset,20)\ndataset[\"MACD\"] = dataset['SMA_12']-dataset['SMA_26']\ndataset[\"MACD_signal\"] = dataset['MACD'].rolling(9).mean()\ndataset=stochastic(dataset, k=14, d=3)\n\ndataset[\"Typical_Price\"]=(dataset[\"high\"]+dataset[\"low\"]+dataset[\"close\"])\/3\ndataset[\"Row_Money_Flow\"]=dataset[\"Typical_Price\"]*dataset[\"volume\"]\ndataset[\"Money_Flow_Ratio\"]=dataset[\"Row_Money_Flow\"].rolling(14).max()\/dataset[\"Row_Money_Flow\"].rolling(14).min()\ndataset[\"MFI_num\"]=100 - 100 \/ (1 + dataset[\"Money_Flow_Ratio\"])\ndataset.loc[(dataset[\"MFI_num\"]<=30), \"MFI_AL\"] =  1\ndataset.loc[(dataset[\"MFI_num\"]>30), \"MFI_AL\"] =  0\n\ndataset=dataset.sort_values(by=[\"date\"],ascending=False)\n\ndataset[\"close_diff\"]=dataset[\"close\"]-dataset[\"close\"].shift(-1)\ndataset[\"close_perc\"]=(dataset[\"close\"]-dataset[\"close\"].shift(-1))\/dataset[\"close\"].shift(-1)\ndataset[\"close_open_diff\"]=dataset[\"close\"]-dataset[\"open\"]\ndataset[\"close_open_perc\"]=(dataset[\"close\"]-dataset[\"open\"])\/dataset[\"open\"]\ndataset.loc[(dataset[\"close\"]>dataset[\"close\"].shift(-1)), \"MOM1\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"close\"].shift(-1)), \"MOM1\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"close\"].shift(-2)), \"MOM2\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"close\"].shift(-2)), \"MOM2\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"close\"].shift(-3)), \"MOM3\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"close\"].shift(-3)), \"MOM3\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"close\"].shift(-4)), \"MOM4\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"close\"].shift(-4)), \"MOM4\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"close\"].shift(-5)), \"MOM5\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"close\"].shift(-5)), \"MOM5\"] =  0\ndataset[\"MOM_TOP5\"]=dataset[\"MOM1\"]+dataset[\"MOM2\"]+dataset[\"MOM3\"]+dataset[\"MOM4\"]+dataset[\"MOM5\"]\ndataset.loc[(dataset[\"SMA_5\"]>dataset[\"SMA_10\"]), \"SMA_5_10_COMP\"] =  1\ndataset.loc[(dataset[\"SMA_5\"]<=dataset[\"SMA_10\"]), \"SMA_5_10_COMP\"] =  0\ndataset.loc[(dataset[\"SMA_10\"]>dataset[\"SMA_20\"]), \"SMA_10_20_COMP\"] =  1\ndataset.loc[(dataset[\"SMA_10\"]<=dataset[\"SMA_20\"]), \"SMA_10_20_COMP\"] =  0\ndataset.loc[(dataset[\"SMA_20\"]>dataset[\"SMA_26\"]), \"SMA_20_26_COMP\"] =  1\ndataset.loc[(dataset[\"SMA_20\"]<=dataset[\"SMA_26\"]), \"SMA_20_26_COMP\"] =  0\ndataset.loc[(dataset[\"SMA_20\"]>dataset[\"SMA_50\"]), \"SMA_20_50_COMP\"] =  1\ndataset.loc[(dataset[\"SMA_20\"]<=dataset[\"SMA_50\"]), \"SMA_20_50_COMP\"] =  0\ndataset.loc[(dataset[\"SMA_50\"]>dataset[\"SMA_200\"]), \"SMA_50_200_COMP\"] =  1\ndataset.loc[(dataset[\"SMA_50\"]<=dataset[\"SMA_200\"]), \"SMA_50_200_COMP\"] =  0\ndataset.loc[(dataset[\"RSI_9\"]<=30), \"RSI_9_AL\"] =  1\ndataset.loc[(dataset[\"RSI_9\"]>30), \"RSI_9_AL\"] =  0\ndataset.loc[(dataset[\"RSI_14\"]<=30), \"RSI_14_AL\"] =  1\ndataset.loc[(dataset[\"RSI_14\"]>30), \"RSI_14_AL\"] =  0\ndataset.loc[(dataset[\"RSI_20\"]<=30), \"RSI_20_AL\"] =  1\ndataset.loc[(dataset[\"RSI_20\"]>30), \"RSI_20_AL\"] =  0\ndataset.loc[(dataset[\"MACD_signal\"]<=dataset[\"MACD\"]), \"MACD_signal_comp_al\"] =  1\ndataset.loc[(dataset[\"MACD_signal\"]>dataset[\"MACD\"]), \"MACD_signal_comp_al\"] =  0\n\n\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_5\"]), \"CLOS_SMA5_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_5\"]), \"CLOS_SMA5_comp_al\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_20\"]), \"CLOS_SMA20_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_20\"]), \"CLOS_SMA20_comp_al\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_21\"]), \"CLOS_SMA21_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_21\"]), \"CLOS_SMA21_comp_al\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_23\"]), \"CLOS_SMA23_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_23\"]), \"CLOS_SMA23_comp_al\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_26\"]), \"CLOS_SMA26_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_26\"]), \"CLOS_SMA26_comp_al\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_50\"]), \"CLOS_SMA50_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_50\"]), \"CLOS_SMA50_comp_al\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_200\"]), \"CLOS_SMA200_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_200\"]), \"CLOS_SMA200_comp_al\"] =  1\ndataset.loc[(dataset[\"close\"]<=dataset[\"SMA_233\"]), \"CLOS_SMA200_comp_al\"] =  0\ndataset.loc[(dataset[\"close\"]>dataset[\"SMA_233\"]), \"CLOS_SMA233_comp_al\"] =  1\n\ndataset.loc[(dataset[\"BB_LOWER\"]>=dataset[\"close\"]), \"BB_al\"] =  1\ndataset.loc[(dataset[\"BB_LOWER\"]<dataset[\"close\"]), \"BB_al\"] =  0 \ndataset[\"open_close_ratio\"] = dataset['open']\/dataset['close']\ndataset[\"high_low_diff\"]=dataset[\"high\"]-dataset[\"low\"]\ndataset[\"high_low_ratio\"]=(dataset[\"high\"]-dataset[\"low\"])\/dataset[\"low\"]\ndataset[\"high_low_ratio1\"]=(dataset[\"high\"].shift(-1)-dataset[\"low\"].shift(-1))\/dataset[\"low\"].shift(-1)\ndataset[\"high_high1_ratio\"]=(dataset[\"high\"]-dataset[\"high\"].shift(-1))\/dataset[\"high\"].shift(-1)\ndataset[\"high_high2_ratio\"]=(dataset[\"high\"]-dataset[\"high\"].shift(-2))\/dataset[\"high\"].shift(-2)\ndataset[\"high_high3_ratio\"]=(dataset[\"high\"]-dataset[\"high\"].shift(-3))\/dataset[\"high\"].shift(-3)\ndataset[\"high_close_ratio\"]=(dataset[\"high\"]-dataset[\"close\"])\/dataset[\"close\"]\ndataset.loc[(dataset[\"RSI_14\"]<=25) & (dataset['SMA_23']>dataset['SMA_26']), \"MACD_RSI_SIGNAL\"] =  1\ndataset.loc[(dataset[\"RSI_14\"]>25) | (dataset['SMA_23']<=dataset['SMA_26']), \"MACD_RSI_SIGNAL\"] =  0\ndataset=dataset.dropna()\ndataset.sort_values(by=\"date\",ascending=True)\ncolumns=dataset.columns.drop(\"target\")\ndataset[columns]=dataset[columns].shift(-1)\ndataset=dataset.dropna()\ndataset=dataset.drop([\"date\"],axis=1)\n","309976a2":"\nr_s=42  \ndataset=dataset.dropna()\ndataset.columns[dataset.isnull().any()]\n\ntrain_x=dataset.drop(\"target\",axis=1)\n#correlation(train_x, 0.95)\ntrain_y=dataset[\"target\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.20, random_state=r_s)\n\n\nxgb=GradientBoostingClassifier(random_state=42)\nxgb=xgb.fit(X_train,y_train)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(X_val)\n\nshap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame([X_val.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)\nimportant_feature=importance_df[\"column_name\"].head(1)\n\n#-----------------------------------------------------------------------------------------------------------   \n\nmy_regressors=[ \n               GradientBoostingClassifier(random_state=r_s),\n               GradientBoostingClassifier(learning_rate=0.05, n_estimators=120,max_depth=9, min_samples_split=1200,\n                                            min_samples_leaf=60, subsample=0.85, max_features=7,random_state=r_s),\n               GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=9, min_samples_split=1200,\n                                            min_samples_leaf=60, subsample=0.85,max_features=7,random_state=r_s),\n               GradientBoostingClassifier(learning_rate=0.005, n_estimators=1200,max_depth=9, min_samples_split=1200,\n                                            min_samples_leaf=60, subsample=0.85, max_features=7,warm_start=True,random_state=r_s),\n               GradientBoostingClassifier(learning_rate=0.005, n_estimators=1500,max_depth=9, min_samples_split=1200, \n                                            min_samples_leaf=60, subsample=0.85, max_features=7,warm_start=True,random_state=r_s),\n               GradientBoostingClassifier(learning_rate=100, n_estimators=500,max_depth=5,random_state=r_s),\n               GradientBoostingClassifier(learning_rate=0.01, n_estimators=250,max_depth=7,random_state=r_s),\n               GradientBoostingClassifier(learning_rate=0.1, n_estimators=5,max_depth=3,random_state=r_s),\n               RandomForestClassifier(random_state=r_s),\n               RandomForestClassifier(n_estimators=200,random_state=r_s),\n               RandomForestClassifier(n_estimators=200,min_samples_leaf=3,max_features=0.5,random_state=r_s),\n               RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n                                        min_impurity_decrease=0.0, min_impurity_split=None,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n                                        oob_score=False, random_state=r_s, verbose=0,\n                                        warm_start=False),\n               RandomForestClassifier(n_estimators=50,max_features=\"sqrt\",random_state=r_s),\n               RandomForestClassifier(bootstrap=True,max_depth=50,n_estimators=25,max_features=5,random_state=r_s),\n               AdaBoostClassifier(random_state=r_s),\n               ExtraTreesClassifier(random_state=r_s),\n               LGBMClassifier(random_state=r_s), \n               LGBMClassifier(min_child_weight=1e-05,reg_alpha=1,reg_lambda=100,scale_pos_weight=2,random_state=r_s),       \n               CatBoostClassifier(eval_metric='Accuracy',logging_level='Silent',random_state=r_s), \n               CatBoostClassifier(border_count=32,depth=3,iterations=250,l2_leaf_reg=3,learning_rate=0.03,eval_metric='Accuracy',logging_level='Silent',random_state=r_s),         \n               XGBClassifier(random_state=r_s),\n               XGBClassifier(scale_pos_weight=1,learning_rate=0.01,colsample_bytree = 0.4,subsample = 0.8,\n                                objective='binary:logistic',n_estimators=1000,reg_alpha = 0.3,max_depth=4,gamma=10,random_state=r_s),\n               XGBClassifier(learning_rate =0.01,n_estimators=5000,max_depth=4,min_child_weight=6,gamma=0,subsample=0.8,\n                                colsample_bytree=0.8,reg_alpha=0.005,objective= 'binary:logistic',nthread=4,\n                                scale_pos_weight=1,seed=27,random_state=r_s),\n              XGBClassifier(max_depth=3,min_child_weight=10,gamma=5,subsample=0.6,\n                                colsample_bytree=0.8,random_state=r_s),\n             XGBClassifier(learning_rate =10,gamma=1.5,subsample=1.0,\n                                colsample_bytree=0.6,random_state=r_s),\n               #MLPClassifier(random_state=r_s),\n               KNeighborsClassifier(3),\n               SVC(kernel=\"linear\", C=0.025,random_state=r_s),\n               SVC(gamma=2, C=1,random_state=r_s),\n               GaussianProcessClassifier(1.0 * RBF(1.0),random_state=r_s),\n               DecisionTreeClassifier(random_state=r_s),\n               DecisionTreeClassifier(criterion=\"entropy\",max_depth=4,random_state=r_s),\n               GaussianNB(),\n               QuadraticDiscriminantAnalysis(),\n               LogisticRegression(random_state=r_s)\n              ]\n\nregressors=[]\n\nfor my_regressor in my_regressors:\n    regressors.append(my_regressor)\n\n\nscores_val=[]\nscores_train=[]\nMAE=[]\nMSE=[]\nRMSE=[]\n\n\n\nfor model in regressors:\n    pipe=Pipeline([(\"scaler\",RobustScaler()),(\"regressor\",model)])\n    scores_val.append(pipe.fit(X_train,y_train).score(X_val,y_val))\n    scores_train.append(pipe.fit(X_train,y_train).score(X_train,y_train))\n    y_pred=pipe.predict(X_val)\n    scores_val.append(pipe.fit(X_train,y_train).score(X_val,y_val))\n    scores_train.append(pipe.fit(X_train,y_train).score(X_train,y_train))\n    y_pred=pipe.predict(X_val)\n    MAE.append(mean_absolute_error(y_val,y_pred))\n    MSE.append(mean_squared_error(y_val,y_pred))\n    RMSE.append(np.sqrt(mean_squared_error(y_val,y_pred)))\n\n\n\nresults=zip(scores_val,scores_train,MAE,MSE,RMSE)\nresults=list(results)\nresults_score_val=[item[0] for item in results]\nresults_score_train=[item[1] for item in results]\nresults_MAE=[item[2] for item in results]\nresults_MSE=[item[3] for item in results]\nresults_RMSE=[item[4] for item in results]\n\n\ndf_results=pd.DataFrame({\"Algorithm\":regressors,\"Training Score\":results_score_train,\"Validation Score\":results_score_val,\"MAE\":results_MAE,\"MSE\":results_MSE,\"RMSE\":results_RMSE})\nbest_models=df_results.sort_values(by=\"Validation Score\",ascending=False)\nbest_model=best_models.iloc[0]\n#print(best_model)\nbest_model_name=best_models.iloc[0][0]\npipe_best=Pipeline([(\"scaler\",RobustScaler()),(\"regressor\",best_model_name)])\nbest_model_learn=pipe_best.fit(X_train,y_train)\ny_pred_best=best_model_learn.predict(X_val)\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report\")\nprint(classification_report(y_val, y_pred_best)) \n","e0c272fd":"cf_matrix=confusion_matrix(y_val, y_pred_best)\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.show()","d020fd86":"### Stock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit. The efficient-market hypothesis suggests that stock prices reflect all currently available information and any price changes that are not based on newly revealed information thus are inherently unpredictable. Others disagree and those with this viewpoint possess myriad methods and technologies which purportedly allow them to gain future price information.","22fa5491":"Let's create Pandas tables","f8d95366":"<img src=\"https:\/\/www.webtekno.com\/images\/editor\/default\/0002\/48\/a5c0386ffcd7114174d86c924aff21ae90cba989.jpeg\" width=600 height=200>","569d4527":"### **Technical analysis**\n<br>\nTechnical analysts or chartists are not concerned with any of the company's fundamentals. They seek to determine the future price of a stock based solely on the trends of the past price (a form of time series analysis). Numerous patterns are employed such as the head and shoulders or cup and saucer. Alongside the patterns, techniques are used such as the exponential moving average (EMA), oscillators, support and resistance levels or momentum and volume indicators. Candle stick patterns, believed to have been first developed by Japanese rice merchants, are nowadays widely used by technical analysts. Technical analysis is rather used for short-term strategies, than the long-term ones. And therefore, it is far more prevalent in commodities and forex markets where traders focus on short-term price movements. There are some basic assumptions used in this analysis, first being that everything significant about a company is already priced into the stock, other being that the price moves in trends and lastly that history (of prices) tends to repeat itself which is mainly because of the market psychology.\n","c3188cb2":"### Prediction Methods\n> Prediction methodologies fall into three broad categories which can (and often do) overlap. They are fundamental analysis, technical analysis (charting) and technological methods.\n\n","f94f165a":"# On this Notebook we use Technical analysis and Machine learning methods","9c095e47":"### Create functions that we need","4c9f8f5d":"### First we need to add most popular indicator to our Dataset\n","03ebbaab":"# IS IT POSSIBLE TO FORECAST OR IT'S JUST A DREAM"}}