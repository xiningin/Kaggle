{"cell_type":{"97205a00":"code","deef4590":"code","1e31dffc":"code","bf2bfa6d":"code","21e7c46c":"code","73cc6c2d":"code","53d5db47":"code","11fc5fca":"code","9eebaf02":"code","f6add1ea":"code","ca2954e9":"code","f4680bf2":"code","d66f6a74":"code","e21094c8":"code","18f1b430":"code","6de7ab9b":"code","651564f1":"code","ca35b0be":"code","1ade23a6":"code","22d3e626":"code","98fe94d1":"code","68fe128e":"markdown","7d4613bb":"markdown","021cdea3":"markdown","19961197":"markdown","9d871281":"markdown","786ecc7f":"markdown","c1efed5a":"markdown","8b046531":"markdown","e58c54bf":"markdown"},"source":{"97205a00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","deef4590":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = pd.read_csv('\/kaggle\/input\/netflix-shows\/netflix_titles.csv')","1e31dffc":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import KFold","bf2bfa6d":"data.describe()","21e7c46c":"data.isnull().sum()","73cc6c2d":"data.head()","53d5db47":"bar, ax = plt.subplots(figsize = (12,12))\nplt.pie(data['type'].value_counts(), labels = data['type'].value_counts().index, autopct=\"%.1f%%\")\nplt.title('Sebaran Data Film dan Tv Show di Netflix', size=20)","11fc5fca":"bar, ax = plt.subplots(figsize = (10,10))\nsns.barplot(x = data['release_year'].value_counts().index[:5], y = data['release_year'].value_counts()[:5])\nplt.xlabel('Tahun')\nplt.ylabel('Banyaknya')\nplt.title('Banyaknya Rilis Dalam Tahun')","9eebaf02":"movie_data = data[data['type'] == 'Movie']\ntv_show_data = data[data['type'] == 'TV Show']\n# bar,ax = plt.subplots(1,2,figsize=(10,10))\ntemp = data[['type', 'release_year']]\ntemp = temp.value_counts().to_frame()\ntemp.reset_index(level=[0,1], inplace=True)\ntemp = temp.rename(columns = {0:'count'})\ntemp = pd.concat([temp[temp['type'] == 'Movie'][:5], temp[temp['type']== 'TV Show'][:5]])","f6add1ea":"# ax, bar = plt.subplots(figsize = (10,10))\nsns.catplot(x = 'release_year', y = 'count', hue = 'type', data = temp, kind = 'point')\nplt.xlabel('Tahun Rilis')\nplt.ylabel('Banyaknya')\nplt.title('Banyaknya Rilis pertahun dalam film dan Tv Show', size=14)","ca2954e9":"temp = list()\nclean_data = data.dropna()\nclean_data.reset_index(inplace=True)\nfor ind, element in clean_data.iterrows():\n    type_show = element['release_year']\n    for cast in str(element['listed_in']).split(','):\n        temp.append([type_show, cast])\ncast_data = pd.DataFrame(temp, columns= ['release_year', 'cast'])\ncast_data","f4680bf2":"cast = cast_data.value_counts().to_frame()\ncast.reset_index(level=[0,1], inplace=True)\ncast = cast.rename(columns = {0:'count'})\n\n\nyears = [2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]\nyear_data = list()\nfor year in years:\n    temp1 = cast[cast['release_year'] == year].iloc[0,:]\n    temp2 = cast[cast['release_year'] == year].iloc[1,:]\n    year_data.append(list(temp1))\n    year_data.append(list(temp2))\n    \nyear = pd.DataFrame(year_data, columns=('years', 'genre', 'count'))\nbar, ax = plt.subplots(figsize=(10,10))\nsns.barplot(x = 'years', y ='count', hue='genre', data = year)\nplt.xlabel('Tahun')\nplt.ylabel('Banyaknya')\nplt.title('Genre Paling Populer dalam 10 Tahun', size=20)","d66f6a74":"def get_data():\n    movies = pd.read_csv('..\/input\/netflix-shows\/netflix_titles.csv')\n    \n    # Replace NaN values with empty string\n    movies.dropna(inplace=True)\n    \n    # Uniformisation of labels with same signification\n    movies['rating'].replace(to_replace='PG-13', value='TV-PG', inplace=True)\n    movies['rating'].replace(to_replace='PG', value='TV-PG', inplace=True)\n    movies['rating'].replace(to_replace='TV-Y7-FV', value='TV-Y7', inplace=True)\n    movies['rating'].replace(to_replace='G', value='TV-G', inplace=True)\n    movies['rating'].replace(to_replace='NC-17', value='R', inplace=True)\n    # Drop rows that don't have any labels (NR = UR = Unrated)\n    movies.drop(movies[movies['rating']=='NR'].index, inplace=True)\n    movies.drop(movies[movies['rating']==''].index, inplace=True)\n    movies.drop(movies[movies['rating']=='UR'].index, inplace=True)\n    #Drop useless labels\n    movies.drop(movies[movies['rating']=='TV-G'].index, inplace=True)\n    movies.drop(movies[movies['rating']=='TV-Y'].index, inplace=True)\n    movies.drop(movies[movies['rating']=='TV-Y7'].index, inplace=True)\n\n\n    #Drop useless colmns\n    movies = movies[[\"type\", \"title\", \"description\", \"director\", \"cast\", 'rating', \"listed_in\", \"country\"]]\n    \n    return movies","e21094c8":"movies = get_data()\n\n#Plot label repartition\nfig = plt.figure(figsize=(8,6))\nmovies.groupby('rating').title.count().plot.bar(ylim=0)\nplt.show()","18f1b430":"#Convert labels to int and create a new column\nmovies['category_id'] = movies['rating'].factorize()[0]\n#Get unique values\ncategory_id_df = movies[[\"rating\", \"category_id\"]].drop_duplicates().sort_values('category_id')\n#Dicts associating rating and its int value\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'rating']].values)\nlabels = movies.category_id\n\n#Group all the columns together\nmovies ['txt'] = movies['type'] + \" \" + movies['title'] + \" \" + movies['description'] + \" \" + movies['listed_in'] \nmovies['txt'] = movies['txt'] + \" \" + movies['director'] + \" \" + movies['cast'] + \" \" + movies[\"country\"]\nmovies = movies[['rating', 'txt']]\n\n#Cross val\nmskf = KFold(n_splits = 15, random_state=0, shuffle = True)\nacc = []\n\nfor train_index, test_index in mskf.split(movies['txt']) :\n    X_train, X_test = movies['txt'].iloc[train_index], movies['txt'].iloc[test_index]\n    y_train, y_test = movies['rating'].iloc[train_index], movies['rating'].iloc[test_index]\n    \n    #Preprocess\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(X_train)\n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n    #Fit\n    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n    \n    #score\n    acc.append(clf.score(count_vect.transform(X_test.values), y_test.values))\n    \nplot_confusion_matrix(clf, count_vect.transform(X_test), np.array(y_test.values))\nplt.show()","6de7ab9b":"print(\"Accuracy with Naive Bayes: \" + str(np.mean(acc) * 100) + \"%\")","651564f1":"movies = get_data().drop(['rating'], axis = 1)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n\nfor rating, category_id in sorted(category_to_id.items()):\n    print(\"######### '{}':\".format(rating))\n    for col in movies.columns :\n        features = tfidf.fit_transform(movies[col]).toarray()\n        features_chi2 = chi2(features, labels == category_id)\n        mean = np.mean(features_chi2[0])\n        print(\"    '{}':\".format(col))\n        print(\"        {}\".format(mean*100))","ca35b0be":"movies = get_data()\n#Group all the columns together\nmovies ['txt'] = movies['director'] + \" \" + movies['listed_in'] + \" \" + movies['type'] + \" \" + movies['country'] + \" \" + movies['cast'] \nmovies = movies[['rating', 'txt']]\n\n\n#Cross val\nmskf = KFold(n_splits = 15, random_state=0, shuffle = True)\nacc = []\n\nfor train_index, test_index in mskf.split(movies['txt']) :\n    X_train, X_test = movies['txt'].iloc[train_index], movies['txt'].iloc[test_index]\n    y_train, y_test = movies['rating'].iloc[train_index], movies['rating'].iloc[test_index]\n    \n    #Preprocess\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(X_train)\n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n    \n    #Fit\n    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n    \n    #score\n    acc.append(clf.score(count_vect.transform(X_test.values), y_test.values))\n    \nplot_confusion_matrix(clf, count_vect.transform(X_test), np.array(y_test.values))\nplt.show()\n\nprint(\"Accuracy with Naive Bayes on the whole dataset: \" + str(clf.score(count_vect.transform(movies['txt'].values), movies['rating'].values) * 100) + \"%\")\n    ","1ade23a6":"from nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize  \nimport nltk\nimport re","22d3e626":"data['director'] = data['director'].fillna('')\ndata['cast'] = data['cast'].fillna('')\ndata['text'] = data['title'] + ' '+data['director'] + ' '+ data['cast']+ ' ' +data['listed_in'] + ' '+data['description']\n\ndef preprocess(text):\n    text = re.sub('[^A-z]', ' ', text)\n    stop_words = set(stopwords.words('english'))  \n    word_tokens = word_tokenize(text)  \n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    \n    filtered_sentence = []  \n    for w in word_tokens:  \n        if w not in stop_words:  \n            filtered_sentence.append(lemmatizer.lemmatize(w))\n    filtered = ' '.join([x for x in filtered_sentence])\n    return filtered.lower().strip()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ntext_features = vectorizer.fit_transform(data['text'])\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity_matrix = cosine_similarity(text_features)    ","98fe94d1":"def get_recommendation(movie_name):\n        movie_index = data[data['title'] == movie_name].index\n        movie_similarity = similarity_matrix[movie_index]\n        movie_data = pd.DataFrame({'cosine_similarity':movie_similarity[0], 'index':np.arange(6234)})\n        movie_data = movie_data.sort_values(by = 'cosine_similarity', ascending = False)\n        topn=10\n        movie_ids = movie_data['index'][1:topn]\n        recommendation_movies = list()\n        for temp in movie_ids:\n            movie = data['title'][temp]\n            recommendation_movies.append(movie)\n        return  recommendation_movies\n        \nget_recommendation('Transformers: Robots in Disguise')","68fe128e":"# Konten Berdasarkan Rekomendasi Sistem\n**Menggunakan Cosine Similarity**","7d4613bb":"# Sebaran Data Film dan TV Show Di Netflix","021cdea3":"# Input","19961197":"**Mencoba meningkatkan akurasi terutama mencoba memprediksi beberapa film 'R'. Untuk ini akan melihat korelasi antara kolom dan label menggunakan chi2 independency test**","9d871281":"# Predisksi Rating\n**Menggunakan Algoritma Naive Bayes**","786ecc7f":"# Banyaknya Rilis Dalam Tahun","c1efed5a":"# Prediksi Menggunakan Algoritma Naive Bayes","8b046531":"# Genre Paling Populer dalam 10 Tahun","e58c54bf":"# Eksploratory Data Analysis (EDA) \n**Pendekatan Untuk Menganalisis Set Data**"}}