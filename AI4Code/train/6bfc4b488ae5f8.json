{"cell_type":{"7226bf71":"code","e460818a":"code","3702c487":"code","84d342d6":"code","18e60384":"code","2c098742":"code","54c5b0af":"code","00e24de9":"code","bbfad703":"code","69743ffb":"code","e0733aa4":"code","d10bf88a":"code","28bc8ed8":"code","33223597":"code","b4af5451":"code","4bd2cbdd":"code","7214f75d":"code","688eb302":"code","eaa180ac":"code","ed63f783":"code","265e28f4":"code","5be1b242":"code","e6d35944":"code","61e279ce":"code","2638b9c9":"code","5c3c80fa":"code","857e6049":"code","2835f30e":"code","0528ec76":"code","d197b2b0":"code","cb7b72ed":"code","4ad52285":"code","9586def8":"code","421bca4d":"code","127d2fe2":"code","a35c86db":"code","38f6a0ae":"code","09af8cbd":"code","0afdd2fc":"code","44344ae8":"code","20af9864":"code","139e22f3":"code","9a862917":"code","96703dd2":"code","e12a0647":"code","09519904":"code","60be9953":"code","84cd2987":"code","fe8ab407":"code","cfc8a965":"code","d0521cb2":"code","c81d9167":"code","a63afd05":"code","2c3f2243":"code","2e677960":"code","995b7e55":"code","556bd0cb":"code","00aaef87":"code","7eb28cd4":"code","f6a32131":"code","68cf2776":"code","a0facee0":"code","e9b7a894":"code","eab99710":"code","8b9bae3b":"code","ccb2875a":"code","a8302337":"code","2a790970":"code","4cd3dfbb":"code","9f4942bb":"code","8b8e5e80":"code","270d22bb":"code","e21b26fb":"code","5c6b7a18":"code","041a126d":"code","62de993f":"code","56815363":"code","466e8477":"code","5fbbb476":"code","da2f5f02":"code","3ee87f6a":"code","8154d425":"code","98963090":"code","aeae736c":"code","7ee8dbca":"code","5f2aa8f4":"code","9921d150":"code","90514d9c":"code","8fd2e0f7":"code","1ab516f2":"code","302bf350":"code","4e6a145a":"code","02407150":"code","a3d0bddb":"code","cac81d2c":"code","ed48cfaa":"code","25e5d72c":"code","fef37825":"code","e2d59e35":"code","e0315069":"code","7a885b0c":"code","8d99e311":"code","3853e9e0":"code","79a7d775":"code","ba3b6b43":"code","fd7778a3":"markdown","aada2bdb":"markdown","59403af7":"markdown","7247ee18":"markdown","9d7a6636":"markdown","12bc6afb":"markdown","76272d40":"markdown","0c79b072":"markdown","fdeb1ec9":"markdown","c1a56f13":"markdown","89f2cfc5":"markdown","81eac0c2":"markdown","82cccb79":"markdown","65d4fffe":"markdown","807b44bd":"markdown","0a75ec3b":"markdown","3b8ddbce":"markdown","ee907154":"markdown","ce4fb513":"markdown","bb359e87":"markdown","1c5db6e3":"markdown","d3d9534c":"markdown","42b9e161":"markdown","23e441f0":"markdown","4cf5f061":"markdown","4bd08fc1":"markdown","07b74f64":"markdown","820b05a6":"markdown","15214aea":"markdown","f7a21b79":"markdown","246aa0ae":"markdown","81363232":"markdown","99640e60":"markdown","4f75cbda":"markdown","2a1a2975":"markdown"},"source":{"7226bf71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e460818a":"import numpy as np\nimport pandas as pd\n\nfrom collections import Counter\n#from collections import deque\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom wordcloud import WordCloud\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport seaborn as sns\n\nimport ast                               # ast.literal_eval() to reformat strings into dictionaries\nfrom urllib.request import urlopen\nfrom PIL import Image                    # display jpg files\n\nimport nltk\n#nltk.download('stopwords')\n#nltk.download('punkt')\n#from nltk.corpus import stopwords\n#from nltk.tokenize import word_tokenize\n#from nltk.stem import PorterStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n#from sklearn.model_selection import GridSearchCV\n#from sklearn.model_selection import RandomizedSearchCV\nfrom catboost import CatBoostRegressor","3702c487":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.shape, test.shape","84d342d6":"train.dtypes","18e60384":"# sanity check: target column\nx = list(train.columns.values)\ny = list(test.columns.values)\n[item for item in x if item not in y]","2c098742":"train.head(2)","54c5b0af":"fig = plt.figure(figsize=(15, 5))\nplt.title(\"Distribution of NAN values\")\ntrain.isna().sum().sort_values(ascending = True).plot(kind = 'barh')","00e24de9":"# reformat strings into dictionaries\n# ast.literal_eval() use instad of eval() inspired from https:\/\/www.kaggle.com\/gravix\/gradient-in-a-box\ndef refmt_str2dict(df, cols):\n    for col in cols:\n        df[col] = df[col].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies', 'production_countries', \n                'spoken_languages', 'Keywords', 'cast', 'crew']\n\ntrain = refmt_str2dict(train, dict_columns)","bbfad703":"# path found here: https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation\nTMDB_path = 'https:\/\/image.tmdb.org\/t\/p\/w600_and_h900_bestv2\/'","69743ffb":"nrow = 3\nfig = plt.figure(figsize=(20, nrow*5))\nk = 0\nfor i in np.random.randint(train.shape[0], size=nrow*7):\n    ax = fig.add_subplot(nrow, 7, k+1, xticks=[], yticks=[])\n    img = Image.open(urlopen(TMDB_path + train['poster_path'][i]))\n    plt.imshow(img)\n    ax.set_title(f\"{train['title'][i][0:22]}\")\n    k += 1","e0733aa4":"def display_posters(movies, nrow=1):\n    fig = plt.figure(figsize=(20, nrow*5))\n    max_plot = nrow*7\n    if len(movies) <= max_plot:\n        max_movies = len(movies)\n    else:\n        max_movies = max_plot\n    for i in range(max_movies):\n        ax = fig.add_subplot(nrow, 7, i+1, xticks=[], yticks=[])\n        img = Image.open(urlopen(TMDB_path + movies['poster_path'][i]))\n        plt.imshow(img)\n        ax.set_title(f\"{movies['title'][i][0:22]}\")","d10bf88a":"# correct 'release_date' year\ndef fix_date(x):\n    yr = x.split('\/')[2]\n    if int(yr) <= 19:\n        return x[:-2] + '20' + yr\n    else:\n        return x[:-2] + '19' + yr\n\ntrain['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntrain['release_date'] = pd.to_datetime(train['release_date'])","28bc8ed8":"def process_date(df):\n    date_attrs = ['year', 'month', 'day', 'weekday', 'weekofyear', 'quarter']\n    for attr in date_attrs:\n        new_col = 'release_date_' + attr\n        df[new_col] = getattr(df['release_date'].dt, attr).astype(int)\n    return df\n\ntrain = process_date(train)","33223597":"oldies = train[train['release_date_year'] < 1930].reset_index()\ndisplay_posters(oldies)","b4af5451":"def WordCloud_fromDict(dict_name, key='name'):\n    list_dict = list(train[dict_name].apply(lambda x: [i[key] for i in x] if x != {} else []).values)\n#    list2txt = ' '.join([i for j in list_dict for i in j])\n    list2txt = ' '.join(['_'.join(i.split(' ')) for j in list_dict for i in j])\n    wordcloud = WordCloud(max_font_size = None, background_color = 'black', collocations = False,\n                      width = 1200, height = 1000).generate(list2txt)\n    return wordcloud","4bd2cbdd":"fig = plt.figure(figsize = (20, 10))\nax = fig.add_subplot(1,2,1)\nax.imshow(WordCloud_fromDict('genres'))\nax.set_title('GENRES')\nax.axis('off')\nax = fig.add_subplot(1,2,2)\nax.imshow(WordCloud_fromDict('Keywords'))\nax.set_title('KEYWORDS')\nax.axis('off')\nplt.show()","7214f75d":"list_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ngenres = set([y for x in list_genres for y in x])\nkeywords = set([y for x in list_keywords for y in x])\nlen(genres), len(keywords)","688eb302":"timeTravelMovies = train[train['Keywords'].apply(lambda x: 'time travel' in [i['name'] for i in x])].reset_index()\ndisplay_posters(timeTravelMovies)","eaa180ac":"nrow = 2\n# checking lead cast 'cast_id'] == 1\nlist_cast_wURL = list(train['cast'].apply(\n    lambda x: [(i['name'], i['profile_path']) for i in x if i['cast_id'] == 1] if x != {} else []).values)\ntop_cast = Counter([y for x in list_cast_wURL for y in x]).most_common(7*nrow)\n\nfig = plt.figure(figsize=(20, nrow*5))\nk = 0\nfor i in top_cast:\n    ax = fig.add_subplot(nrow, 7, k+1, xticks=[], yticks=[])\n    img = Image.open(urlopen(TMDB_path + i[0][1]))\n    plt.imshow(img)\n    ax.set_title(i[0][0][0:22])\n    k += 1","ed63f783":"BillMurrayMovies = train[train['cast'].apply(lambda x: 'Bill Murray' in [i['name'] for i in x])].reset_index()\ndisplay_posters(BillMurrayMovies)","265e28f4":"NolanMovies = train[train['crew'].apply(lambda x: 'Christopher Nolan' in [i['name'] for i in x if\\\n                                                                          i['job'] == 'Director'])].reset_index()\ndisplay_posters(NolanMovies)","5be1b242":"list_crew = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ncast = set([y for x in list_cast_wURL for y in x])\ncrew = set([y for x in list_crew for y in x])\nlen(cast), len(crew)","e6d35944":"fig = plt.figure(figsize = (20, 10))\nax = fig.add_subplot(1,2,1)\nax.imshow(WordCloud_fromDict('production_companies'))\nax.set_title('PRODUCTION COMPANIES')\nax.axis('off')\nax = fig.add_subplot(1,2,2)\nax.imshow(WordCloud_fromDict('production_countries'))\nax.set_title('PRODUCTION COUNTRIES')\nax.axis('off')\nplt.show()","61e279ce":"list_prodCompanies = list(train['production_companies'].apply(\n    lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_prodCountries = list(train['production_countries'].apply(\n    lambda x: [i['name'] for i in x] if x != {} else []).values)\nprodCompanies = set([y for x in list_prodCompanies for y in x])\nprodCountries = set([y for x in list_prodCountries for y in x])\nlen(prodCompanies), len(prodCountries)","2638b9c9":"AmblinMovies = train[train['production_companies'].apply(\n    lambda x: 'Amblin Entertainment' in [i['name'] for i in x])].reset_index()\ndisplay_posters(AmblinMovies)","5c3c80fa":"train[['budget', 'revenue', 'runtime', 'popularity']].describe()","857e6049":"plt.figure(figsize=(20,8))\nplt.subplot(121)\nplt.hist(np.log1p(train['budget']), bins = 50)   #some zero values in 'budget'\nplt.title('budget_rescaled')\nplt.subplot(122)\nplt.hist(np.log1p(train['revenue']), bins = 50)\nplt.title('revenue_rescaled')\nplt.subplots_adjust(hspace=0.5)\nplt.show()","2835f30e":"yr_release = train['release_date_year'].value_counts().sort_index()\nyr_budget = train.groupby(['release_date_year'])['budget'].mean()\nyr_revenue = train.groupby(['release_date_year'])['revenue'].mean()\nyr_popularity = train.groupby(['release_date_year'])['popularity'].mean()\n\nplt.figure(figsize=(20,8))\nplt.subplot(311)\nplt.title('Movie count')\nplt.plot(yr_release.index, yr_release.values)\nplt.subplot(312)\nplt.plot(yr_budget.index, yr_budget.values)\nplt.plot(yr_revenue.index, yr_revenue.values)\nplt.title('$')\nplt.subplot(313)\nplt.plot(yr_popularity.index, yr_popularity.values)\nplt.title('Popularity')\nplt.subplots_adjust(hspace=0.3)\nplt.show()","0528ec76":"OneMovie = train[train['title'] == 'The Terminator']\nkeywords = list(OneMovie['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)[0]\nkeywords_str = ' '.join(keywords)\ntext_merged = list(OneMovie['overview'])[0] + ' | ' + list(OneMovie['tagline'])[0] + ' | ' + keywords_str\ntext_merged","d197b2b0":"# The following will not be used, as removing stopwords, getting stem, etc. will be done via TfidfVectorizer...\n#stop_words = set(stopwords.words('english'))\n#tokens = word_tokenize(text_merged)\n#tokens_cleaned = [w for w in tokens if not w in stop_words]\n#print(' '.join(tokens_cleaned))\n\n#ps = PorterStemmer()\n#tokens_stem = [ps.stem(w) for w in tokens_cleaned]\n#tokens_stem_nopunct = [w.lower() for w in tokens_stem if w.isalpha()]\n#print(' '.join(tokens_stem_nopunct))\n\n#bagofwords = list(set(tokens_stem_nopunct))\n#','.join(bagofwords)\n\ntext_merged2train = []\nfor id in train['id']:\n    mov = train[train['id'] == id]\n    keywords = list(mov['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)[0]\n    keywords_str = ' '.join(keywords)\n    if pd.isnull(list(mov['overview'])[0]):\n        overview = ''\n    else:\n        overview = list(mov['overview'])[0]\n    if pd.isnull(list(mov['tagline'])[0]):\n        tagline = ''\n    else:\n        tagline = list(mov['tagline'])[0]\n\n#    text_merged = overview + ' | ' + tagline + ' | ' + keywords_str\n    text_merged = keywords_str\n\n    text_merged2train.append(text_merged)\n#    tokens = word_tokenize(text_merged)\n#    tokens_cleaned = [w for w in tokens if not w in stop_words]\n#    tokens_stem = [ps.stem(w) for w in tokens_cleaned]\n#    tokens_stem_nopunct = [w.lower() for w in tokens_stem if w.isalpha()]\n#    bagofwords = list(set(tokens_stem_nopunct))\n#    text_merged2train.append(bagofwords)\n\ntrain['text_merged'] = text_merged2train","cb7b72ed":"#check\n#list(train[train['title'] == 'The Terminator']['text_merged'])","4ad52285":"train['belongs_to_collection'].head(5)","9586def8":"train['belongs_to_collection'].apply(lambda x: 1 if x != {} else 0).value_counts()","421bca4d":"train['belongs2coll_yn'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\ntrain['homepage'].head(5)","127d2fe2":"train['homepage'].apply(lambda x: 1 if pd.isnull(x) == False else 0).value_counts()","a35c86db":"train['homepage_yn'] = train['homepage'].apply(lambda x: 1 if pd.isnull(x) == False else 0)","38f6a0ae":"train['imdb_id'].head(5)","09af8cbd":"' '.join(set(train['original_language'])), len(set(train['original_language']))","0afdd2fc":"train['original_title'][0:5], train['title'][0:5]","44344ae8":"list_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlanguages = set([y for x in list_languages for y in x])\nlen(languages), Counter([i for j in list_languages for i in j]).most_common(5)","20af9864":"train['status'].apply(lambda x: 1 if x == 'Released' else 0).value_counts(), set(train['status'])","139e22f3":"# strings to dictionaries\ntest = refmt_str2dict(test, dict_columns)","9a862917":"# One release date missing for the test set, for movie 'Jails, Hospitals & Hip-Hop'\n# date '5\/1\/00' retrieved from https:\/\/www.imdb.com\/title\/tt0210130\/\n# but I don't want to use any external data wo will use dummy date\ntest.at[pd.isnull(test['release_date']), 'release_date'] = '1\/1\/11'\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = pd.to_datetime(test['release_date'])\n\n# create 'year', 'month', 'day', 'weekday', 'weekofyear', 'quarter' features\ntest = process_date(test)","96703dd2":"text_merged2test = []\nfor id in test['id']:\n    mov = test[test['id'] == id]\n    keywords = list(mov['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)[0]\n    keywords_str = ' '.join(keywords)\n    if pd.isnull(list(mov['overview'])[0]):\n        overview = ''\n    else:\n        overview = list(mov['overview'])[0]\n    if pd.isnull(list(mov['tagline'])[0]):\n        tagline = ''\n    else:\n        tagline = list(mov['tagline'])[0]\n\n#    text_merged = overview + ' | ' + tagline + ' | ' + keywords_str\n    text_merged = keywords_str\n\n    text_merged2test.append(text_merged)\n#    tokens = word_tokenize(text_merged)\n#    tokens_cleaned = [w for w in tokens if not w in stop_words]\n#    tokens_stem = [ps.stem(w) for w in tokens_cleaned]\n#    tokens_stem_nopunct = [w.lower() for w in tokens_stem if w.isalpha()]\n#    bagofwords = list(set(tokens_stem_nopunct))\n#    text_merged2test.append(bagofwords)\n    \ntest['text_merged'] = text_merged2test","e12a0647":"#list(test[test['title'] == 'Transcendence']['text_merged'])","09519904":"test['belongs2coll_yn'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\ntest['homepage_yn'] = test['homepage'].apply(lambda x: 1 if pd.isnull(x) == False else 0)","60be9953":"yr_release_test = test['release_date_year'].value_counts().sort_index()\nyr_budget_test = test.groupby(['release_date_year'])['budget'].mean()\n\nplt.figure(figsize=(20,8))\nplt.subplot(211)\nplt.title('Movie count')\nplt.plot(yr_release.index, yr_release.values)\nplt.plot(yr_release_test.index, yr_release_test.values)\nplt.subplot(212)\nplt.plot(yr_budget.index, yr_budget.values)\nplt.plot(yr_budget_test.index, yr_budget_test.values)\nplt.title('$')\nplt.subplots_adjust(hspace=0.3)\nplt.show()","84cd2987":"test[test['release_date_year'] == 1927]\n#https:\/\/en.wikipedia.org\/wiki\/List_of_most_expensive_films\n#Metropolis, the 1927 German film directed by Fritz Lang, often erroneously reported as having cost\n#$200 million at the value of modern money. Metropolis cost $1.2\u20131.3 million at the time of its\n#production, which would be about $12 million at 2009 prices","fe8ab407":"nrow = 2\n# checking lead cast 'cast_id'] == 1\nlist_cast_INtest = list(test['cast'].apply(\n    lambda x: [(i['name'], i['profile_path']) for i in x if i['cast_id'] == 1] if x != {} else []).values)\ntop_cast_INtest = Counter([y for x in list_cast_INtest for y in x]).most_common(7*nrow)\n\nfig = plt.figure(figsize=(20, nrow*5))\nk = 0\nfor i in top_cast_INtest:\n    ax = fig.add_subplot(nrow, 7, k+1, xticks=[], yticks=[])\n    img = Image.open(urlopen(TMDB_path + i[0][1]))\n    plt.imshow(img)\n    ax.set_title(i[0][0][0:22])\n    k += 1","cfc8a965":"cols2drop = ['id','belongs_to_collection', 'homepage', 'imdb_id', 'original_title', 'overview',\n            'poster_path', 'release_date', 'spoken_languages', 'status','tagline', 'Keywords']\ntrain = train.drop(cols2drop, axis=1)\ntest = test.drop(cols2drop, axis=1)\n\n#features that remain:\ntrain.columns, test.columns","d0521cb2":"df4corr = train[['budget', 'popularity', 'runtime', 'release_date_year', 'release_date_month', 'release_date_day', \\\n                   'release_date_weekday', 'release_date_weekofyear', 'release_date_quarter', 'revenue']]","c81d9167":"correlation = df4corr.corr()\nplt.figure(figsize=(12, 12))  \nsns.heatmap(correlation, annot=True, square=True, cmap='coolwarm')","a63afd05":"_, axes = plt.subplots(2, 4, figsize=(20, 8))\nsns.scatterplot(x = 'budget', y = 'revenue', data = train, marker=\"+\", ax=axes[0,0])\nsns.scatterplot(x = np.log1p(train['budget']), y = np.log1p(train['revenue']), marker=\"+\", ax=axes[0,1])\nsns.scatterplot(x = 'runtime', y = 'revenue', data = train, marker=\"+\", ax=axes[0,2])\n#sns.scatterplot(x = train['budget']\/train['runtime'], y = train['revenue'], marker=\"+\", ax=axes[0,2])\nsns.scatterplot(x = 'popularity', y = 'revenue', data = train, marker=\"+\", ax=axes[0,3])\n#sns.scatterplot(x = train['budget']\/train['popularity'], y = train['revenue'], marker=\"+\", ax=axes[0,3])\n#sns.scatterplot(x = np.where(train['popularity'] < 40, train['popularity'], 40), y = train['revenue'], \\\n#               marker=\"+\", ax=axes[0,3])\nsns.scatterplot(x = 'release_date_year', y = 'revenue', data = train, marker=\"+\", ax=axes[1,0])\nsns.stripplot(x = 'release_date_weekday', y = 'revenue', data = train, ax=axes[1,1])\nsns.stripplot(x = 'release_date_month', y = 'revenue', data = train, ax=axes[1,2])","2c3f2243":"train['budget_yn'] = train['budget'].apply(lambda x: 0 if x == 0 else 1)\ntrain['budget_per_year'] = train['budget']\/train['release_date_year']\n#train['budget_perRuntime'] = train['budget']\/train['runtime']\n#train['popularity_clipped'] = np.where(train['popularity'] < 40, train['popularity'], 40)","2e677960":"_, axes = plt.subplots(1, 3, figsize=(20, 8))\nsns.stripplot(x = 'belongs2coll_yn', y = 'revenue', data = train, ax=axes[0])\nsns.stripplot(x = 'homepage_yn', y = 'revenue', data = train, ax=axes[1])\nsns.stripplot(x = 'budget_yn', y = 'revenue', data = train, ax=axes[2])","995b7e55":"train['n_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_production_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_production_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)","556bd0cb":"_, axes = plt.subplots(2, 3, figsize=(20, 8))\nsns.stripplot(x = 'n_genres', y = 'revenue', data = train, ax=axes[0,0])\nsns.stripplot(x = 'n_production_companies', y = 'revenue', data = train, ax=axes[0,1])\nsns.stripplot(x = 'n_production_countries', y = 'revenue', data = train, ax=axes[0,2])\nsns.regplot(x = 'n_cast', y = 'revenue', data = train, marker='+', ax=axes[1,0])\nsns.regplot(x = 'n_crew', y = 'revenue', data = train, marker='+', ax=axes[1,1])","00aaef87":"train['genres_collapsed'] = train['genres'].apply(lambda x: ' '.\\\n                                                  join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in genres:\n    train['genre_' + g] =  train['genres_collapsed'].apply(lambda x: 1 if g in x else 0)\n\nkeys_genre = ['genre_' + g for g in genres]","7eb28cd4":"n_prodCompanies = 30\nCounter([i for j in list_prodCompanies for i in j]).most_common(n_prodCompanies)","f6a32131":"train['production_companies_collapsed'] = train['production_companies'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_prodCompanies = [m[0] for m in Counter([i for j in list_prodCompanies for i in j]).most_common(n_prodCompanies)]\nfor comp in top_prodCompanies:\n    train['production_company_' + comp] = train['production_companies_collapsed'].\\\n            apply(lambda x: 1 if comp in x else 0)\n    \nkeys_production_company = ['production_company_' + comp for comp in top_prodCompanies]","68cf2776":"n_prodCountries = 30\nCounter([i for j in list_prodCountries for i in j]).most_common(n_prodCountries)","a0facee0":"train['production_countries_collapsed'] = train['production_countries'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_prodCountries = [m[0] for m in Counter([i for j in list_prodCountries for i in j]).most_common(n_prodCountries)]\nfor comp in top_prodCountries:\n    train['production_country_' + comp] = train['production_countries_collapsed'].\\\n            apply(lambda x: 1 if comp in x else 0)\n\nkeys_production_country = ['production_country_' + comp for comp in top_prodCountries]","e9b7a894":"#train['EnglishLead_yn'] = train['original_language'].apply(lambda x: 1 if x == 'en' else 0)\ntop_languages = [m[0] for m in Counter(train['original_language']).most_common(5)]\nfor l in top_languages:\n    train['language_' + l] = train['original_language'].\\\n            apply(lambda x: 1 if l in x else 0)\n\nkeys_language = ['language_' + l for l in top_languages]","eab99710":"# use lead actor for top_cast\nn_lead = 50\nlist_lead = list(train['cast'].apply(lambda x: [i['name'] for i in x if i['cast_id'] == 1] if x != {} else []).values)\nCounter([i for j in list_lead for i in j]).most_common(n_lead)","8b9bae3b":"# different top leads in training and test sets - used combined set\nfullset = pd.concat([train, test])\n\nlist_lead = list(fullset['cast'].apply(lambda x: [i['name'] for i in x if i['cast_id'] == 1] if x != {} else []).values)\nCounter([i for j in list_lead for i in j]).most_common(n_lead)","ccb2875a":"# find top_lead even if not lead in movie\ntrain['cast_collapsed'] = train['cast'].apply(lambda x: ' '.\\\n                                              join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_lead = [m[0] for m in Counter([i for j in list_lead for i in j]).most_common(n_lead)]\nfor lead in top_lead:\n    train['cast_' + lead] = train['cast_collapsed'].apply(lambda x: 1 if lead in x else 0)\n    \nkeys_cast = ['cast_' + lead for lead in top_lead]","a8302337":"n_directors = 50\nlist_directors = list(train['crew'].apply(\n    lambda x: [i['name'] for i in x if i['job'] == 'Director'] if x != {} else []).values)\nCounter([i for j in list_directors for i in j]).most_common(n_directors)","2a790970":"#same as cast - use combined set\nlist_directors = list(fullset['crew'].apply(\n    lambda x: [i['name'] for i in x if i['job'] == 'Director'] if x != {} else []).values)\nCounter([i for j in list_directors for i in j]).most_common(n_directors)","4cd3dfbb":"train['directors_collapsed'] = train['crew'].apply(lambda x: ' '.\\\n        join(sorted([i['name'] for i in x if i['job'] == 'Director'])) if x != {} else '')\ntop_directors = [m[0] for m in Counter([i for j in list_directors for i in j]).most_common(n_directors)]\nfor d in top_directors:\n    train['director_' + d] = train['directors_collapsed'].apply(lambda x: 1 if d in x else 0)\n    \nkeys_director = ['director_' + d for d in top_directors]","9f4942bb":"# deprecated: was used when text transformation was already performed on 'text_merged'\n#tokens_train = [word for l in train['text_merged'] for word in l]\n#tokens_test = [word for l in test['text_merged'] for word in l]\n#tokens = tokens_train + tokens_test\n#lexicon = set(tokens)\n#len(lexicon)\n\n#lexicon10 = [m[0] for m in Counter(tokens).most_common(10)]\n#lexicon10","8b8e5e80":"vectorizer = TfidfVectorizer(\n            analyzer = 'word',\n            stop_words = 'english',\n            ngram_range = (1, 2),\n            min_df = 10,\n            sublinear_tf = True)\n\noverview_transf = vectorizer.fit_transform(train['text_merged'])\noverview_transf","270d22bb":"lexicon = list(vectorizer.vocabulary_.keys())\n#lexicon","e21b26fb":"linreg = LinearRegression()\nlinreg.fit(overview_transf, train['revenue'])","5c6b7a18":"top = 100   #500: 2.06288, 100 : 2.01548, 50: 2.01763 (test set result)\nnegcorrTop = np.argsort(linreg.coef_)[0:top]\nposcorrTop = np.argsort(linreg.coef_)[len(linreg.coef_)-top:]","041a126d":"keywords_newPos = [lexicon[i] for i in poscorrTop]\nkeywords_newNeg = [lexicon[i] for i in negcorrTop]\nkeywords_new = keywords_newPos + keywords_newNeg\nkeywords_new[0:10]","62de993f":"for k in keywords_new:\n    train['txt_' + k] =  train['text_merged'].apply(lambda x: 1 if k in x else 0)\n\nkeys_txt = ['txt_' + s for s in keywords_new]","56815363":"test['budget_yn'] = test['budget'].apply(lambda x: 0 if x == 0 else 1)\ntest['budget_per_year'] = test['budget']\/test['release_date_year']\n\ntest['n_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_production_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_production_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\n\n#test['EnglishLead_yn'] = test['original_language'].apply(lambda x: 1 if x == 'en' else 0)\nfor l in top_languages:\n    test['language_' + l] = test['original_language'].\\\n            apply(lambda x: 1 if l in x else 0)\n\ntest['genres_collapsed'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in genres:\n    test['genre_' + g] =  test['genres_collapsed'].apply(lambda x: 1 if g in x else 0)\n\ntest['production_companies_collapsed'] = test['production_companies'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\nfor comp in top_prodCompanies:\n    test['production_company_' + comp] = test['production_companies_collapsed'].\\\n              apply(lambda x: 1 if comp in x else 0)\n\ntest['production_countries_collapsed'] = test['production_countries'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\nfor comp in top_prodCountries:\n    test['production_country_' + comp] = test['production_countries_collapsed'].\\\n            apply(lambda x: 1 if comp in x else 0)\n\ntest['cast_collapsed'] = test['cast'].apply(lambda x: ' '.\\\n                                              join(sorted([i['name'] for i in x])) if x != {} else '')\nfor lead in top_lead:\n    test['cast_' + lead] = test['cast_collapsed'].apply(lambda x: 1 if lead in x else 0)\n\ntest['directors_collapsed'] = test['crew'].apply(lambda x: ' '.\\\n        join(sorted([i['name'] for i in x if i['job'] == 'Director'])) if x != {} else '')\nfor d in top_directors:\n    test['director_' + d] = test['directors_collapsed'].apply(lambda x: 1 if d in x else 0)\n    \nfor k in keywords_new:\n    test['txt_' + k] =  test['text_merged'].apply(lambda x: 1 if k in x else 0)","466e8477":"cols2drop = ['genres', 'production_companies', 'production_countries', 'genres_collapsed',\n             'production_companies_collapsed', 'production_countries_collapsed', 'cast_collapsed', \n             'directors_collapsed', 'n_cast', 'n_crew']\ntrain = train.drop(cols2drop, axis=1)\ntest = test.drop(cols2drop, axis=1)","5fbbb476":"def RMSE(y_obs, y_pred):\n    n = len(y_obs)\n    rmse = np.sqrt( 1\/n*np.sum((y_pred-y_obs)**2) )\n    return rmse\n\ndef RMSLE(y_obs, y_pred):\n    n = len(y_obs)\n    rmsle = np.sqrt( 1\/n*np.sum((np.log(y_pred)-np.log(y_obs))**2) )\n    return rmsle\n\ndef LinearRegression(X, y):\n    intercept_term = np.ones(shape = y.shape)\n    X = np.concatenate((intercept_term, X), 1)\n    #closed-form solution:\n    coeffs = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    return coeffs","da2f5f02":"x = train['budget'].values\n#x = train['budget_per_year'].values\nX = np.reshape(x, (len(x),1))     #len(x) samples, 1 dimension\ny = train['revenue'].values\ny = np.reshape(y, (len(y),1))\nnp.shape(X), np.shape(y)","3ee87f6a":"model_baseline_coeffs = LinearRegression(X, y)\nmodel_baseline_pred_train = model_baseline_coeffs[0] + model_baseline_coeffs[1]*X\nmodel_baseline_coeffs","8154d425":"plt.figure(figsize=(10,5))\nplt.title('Linear regression (1d)')\nplt.scatter(X, y)\nplt.plot(X, model_baseline_pred_train, c = 'black')","98963090":"#x=budget: 2.6656031635747532\n#x=budget_per_date_year: 2.661467020660518\nRMSLE(y, model_baseline_pred_train)","aeae736c":"train_selected = train[['budget', 'popularity', 'runtime',\n                        'release_date_year', 'release_date_weekday', 'release_date_month',\n                        'budget_yn', 'belongs2coll_yn', 'homepage_yn',\n                        'n_genres', 'n_production_companies', 'n_production_countries', \n                        'revenue']+keys_genre+keys_cast+keys_director+keys_production_company+\n                       keys_production_country+keys_language]\n\n#train_selected = train_selected.replace([np.inf, -np.inf], np.nan)\ntrain_selected = train_selected.dropna(axis = 0)\n\nX = train_selected.drop(['revenue'], axis=1)\ny = train_selected['revenue']","7ee8dbca":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n\nmodel_RF = RandomForestRegressor(n_estimators = 100,\n                                 max_depth = 20)\nmodel_RF.fit(X_train, y_train)\nmodel_RF_pred_valset = model_RF.predict(X_test)\nRMSLE(y_test, model_RF_pred_valset)","5f2aa8f4":"model_RF.fit(X, y)\nmodel_RF_pred_train = model_RF.predict(X)\nRMSLE(y, model_RF_pred_train)","9921d150":"X_train, X_test, y_train, y_test = train_test_split(X, np.log1p(y), test_size=0.4)\n\nmodel_CatBoost = CatBoostRegressor(silent=True)\n\nmodel_CatBoost.fit(X_train, y_train)\nmodel_CatBoost_pred_valset = model_CatBoost.predict(X_test)\nRMSE(y_test, model_CatBoost_pred_valset)","90514d9c":"model_CatBoost.fit(X, np.log1p(y))\nmodel_CatBoost_pred_train = model_CatBoost.predict(X)\nRMSE(np.log1p(y), model_CatBoost_pred_train)","8fd2e0f7":"X_train, X_test, y_train, y_test = train_test_split(X, np.log1p(y), test_size=0.4)\n\nmodel_gboost = GradientBoostingRegressor()\n\nmodel_gboost.fit(X_train, y_train)\nmodel_gboost_pred_valset = model_gboost.predict(X_test)\nRMSE(y_test, model_gboost_pred_valset)","1ab516f2":"model_gboost.fit(X, np.log1p(y))\nmodel_gboost_pred_train = model_gboost.predict(X)\nRMSE(np.log1p(y), model_gboost_pred_train)","302bf350":"model_ensemble_pred_train = (model_CatBoost_pred_train+model_gboost_pred_train)\/2\nRMSE(np.log1p(y), model_ensemble_pred_train)","4e6a145a":"x = test['budget'].values\nX = np.reshape(x, (len(x),1))     #len(x) samples, 1 dimension\nnp.shape(X)","02407150":"model_baseline_pred_test = model_baseline_coeffs[0] + model_baseline_coeffs[1]*X","a3d0bddb":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['revenue'] = model_baseline_pred_test\nsubmission.to_csv('submission_baseline.csv', index = False)","cac81d2c":"test_features = test[['budget', 'popularity', 'runtime',\n                        'release_date_year', 'release_date_weekday', 'release_date_month',\n                        'budget_yn', 'belongs2coll_yn', 'homepage_yn',\n                        'n_genres', 'n_production_companies', 'n_production_countries']+\n                     keys_genre+keys_cast+keys_director+keys_production_company+keys_production_country+\n                    keys_language]\n\nX = test_features\ntest_features.columns[test_features.isna().any()].tolist()","ed48cfaa":"test_features['runtime'][test_features['runtime'].isna() == True]","25e5d72c":"test_features['runtime'][test_features['runtime'].isna() == True] = test_features['runtime'].median()","fef37825":"model_RF_pred_test = model_RF.predict(X)","e2d59e35":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['revenue'] = model_RF_pred_test\nsubmission.to_csv('submission_RF.csv', index = False)","e0315069":"model_CatBoost_pred_test = model_CatBoost.predict(X)\nmodel_CatBoost_pred_test = np.expm1(model_CatBoost_pred_test)","7a885b0c":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['revenue'] = model_CatBoost_pred_test\nsubmission.to_csv('submission_CatBoost.csv', index = False)","8d99e311":"model_gboost_pred_test = model_gboost.predict(X)\nmodel_gboost_pred_test = np.expm1(model_gboost_pred_test)","3853e9e0":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['revenue'] = model_gboost_pred_test\nsubmission.to_csv('submission_gboost.csv', index = False)","79a7d775":"model_ensemble_pred_test = (model_CatBoost_pred_test + model_gboost_pred_test)\/2","ba3b6b43":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['revenue'] = model_ensemble_pred_test\nsubmission.to_csv('submission_ensemble.csv', index = False)","fd7778a3":"# In Parameter Space, No One Can Hear You Scream\n\nI will try to keep it compact for readibility and visibility. I will also not use any external data and avoid too much parameter tuning.\n\nFor EDA, I took a lot of inspiration from Kernels Grandmaster Andrew Lukyanenko: https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation\n\n_Latest updates:_\n* Baseline model (linear regression revenue=f(budget)): **2.67065**\n* Random Forest model (no textual data, maxdepth tuning): **2.56128** (baseline model does almost as well!)\n* CatBoost vanilla model (no textual data, no tuning): **1.97972**","aada2bdb":"### b. Trees","59403af7":"#### Loose ends: 'belongs_to_collection', 'homepage', 'imdb_id', 'original_language', 'original_title', 'spoken_languages', 'status'","7247ee18":"---\n\n#### Other submissions (no improvement)\n\nGradient Boosting vanilla model without textual data.","9d7a6636":"Let's apply the same procedure for the full training set:","12bc6afb":"#### 'crew'","76272d40":"A priori a stimulating problem:\n- we only have 3000 samples for training while we need to test for c. 4400 test samples;\n- we must predict revenue (regression problem) based on a bunch of feature types:\n    - numerical data;\n    - categorical data (e.g. genre, keywords, cast);\n    - unstructured data (text strings);\n- we must deal with columns with many NAN values (but not in the case of numericals).\n\nThis sounds fun! Now, what movies are we talking about??\n\n### b. Data Cleansing & Preliminary Feature Definition (on training set)\n\nSome of the objects are string versions of dictionaries. We must reformat them before extracting their content.","0c79b072":"#### CatBoost\n\nvanilla model","fdeb1ec9":"#### 'title' & 'poster_path'","c1a56f13":"#### boolean","89f2cfc5":"#### 'production_companies' & 'production_countries'","81eac0c2":"#### All numericals: 'budget', 'popularity', 'runtime', 'revenue'","82cccb79":"#### Textual data","65d4fffe":"#### Gradient Boosting\n\nvanilla model","807b44bd":"### b. Apply to test set","0a75ec3b":"Let's get a better feeling of the data and possibilities. I feel like searching for:\n- The oldest movies in the training set (to check 'release_date');\n- Time travel movies (to check 'Keywords', 'genres');\n- Movies with Bill Murray (to check 'cast');\n- Movies by Christopher Nolan (to check 'crew');\n- Movies by Amblin Entertainment (to check 'production_companies', 'production_countries')\n\nand work from there\n\n#### 'release_date'","3b8ddbce":"### a. Baseline Model (Linear Regression)\n\nOur baseline model is a linear regressor between some numericals (budget, etc.) & revenue. No need to define a validation set since we will clearly not overfit here.\n\nWe will use the closed-form solution, which is the cleanest approach for such simple case\n\ny = b + WX","ee907154":"#### Categoricals","ce4fb513":"#### Random Forest","bb359e87":"#### Third submission\n\nCatBoost vanilla model without textual data.<br>\nYields **1.99677** (ranked 68 as of 27 Feb 2019) to be compared to training set result of 1.74703.<br>\nYields **1.98573** (ranked 73 as of 5 Mar 2019) - production countries, 5 top languages added compared to previous one.","1c5db6e3":"After testing, top keywords lead to less overfitting than top keywords+tagline+overview.","d3d9534c":"### c. Submission\n\n#### First submission\n\nBaseline model based on simplest linear regression<br>\nYields a score of **2.67065** (ranked 186 as of 24 Feb 2019) to be compared to training set result of 2.66560.","42b9e161":"#### Drop columns that won't be used","23e441f0":"### d. Training set \/ Test set Balance\n\n#### Create new features in test set (as in training set)","4cf5f061":"#### 'Keywords', 'genres'","4bd08fc1":"#### Ensembling CatBoost & Gradient Boosting","07b74f64":"#### Second submission\n\nSimple Random Forest model without textual data.<br>\nYields score of **2.56128** (ranked 180 as of 27 Feb 2019) to be compared to training set result of 2.19076.","820b05a6":"Ensembling CatBoost & Gradient Boosting. Yields 2.02841","15214aea":"###  c. To Recap (training set only)\n\nHave we checked all data columns?\n\n**id**                         : Irrelevant<br>\n**belongs_to_collection**      : Turned into boolean 'belongs2coll_yn'<br>\n**budget**                     : Numerical<br>\n**genres**                     : 20 genres -> feature engineering (one-hot categoricals)<br>\n**homepage**                   : Turned into boolean 'homepage_yn'<br>\n**imdb_id**                    : Irrelevant (will not use IMDB data)<br>\n**original_language**          : 36 languages -> feature engineering<br>\n**original_title**             : Irrelevant (only difference with 'title' is language, which is already in 'original_language')<br>\n**overview**                   : Cleaned & merged with other text in new feature 'text_merged'<br>\n**popularity**                 : Numerical<br>\n**poster_path**                : Just for visualization<br>\n**production_companies**       : 3695 companies -> feature engineering<br>\n**production_countries**       : 74 countries -> feature engineering<br>\n**release_date**               : Cleaned & turned into 'year', 'month', 'day', 'weekday', 'weekofyear', 'quarter'<br>\n**runtime**                    : Numerical<br>\n**spoken_languages**           : 56 languages -> feature engineering<br>\n**status**                     : Irrelevant (only 4 'Rumored' instead of 'Released' out of 3000 samples)<br>\n**tagline**                    : Cleaned & merged with other text in new feature 'text_merged'<br>\n**title**                      : Not (yet) included in new feature 'text_merged'<br>\n**Keywords**                   : Cleaned & merged with other text in new feature 'text_merged'<br>\n**cast**                       : 38723 names -> feature engineering (incl. 'character', 'gender'...)<br>\n**crew**                       : 38554 names -> feature engineering (incl. 'department', 'gender', 'job'...)<br>\n**revenue**                    : TARGET<br>\n\nNew columns: **release_date_xxx**, **text_merged**, **belongs2coll_yn**, **homepage_yn**<br>\nBefore moving to feature engineering, let's check the consistency between training set and test set.","f7a21b79":"## III. Modelling","246aa0ae":"## II. Feature Engineering\n\n### a. Feature Selection Based on Correlations\n\n#### Numericals","81363232":"#### Compare distributions","99640e60":"#### 'cast'","4f75cbda":"#### Unstructured textual data: 'overview' & 'tagline' (& 'keywords' again)\n\nSince we have few samples, let's keep it simple and only see those unstructured data as bags-of-words. Let's play first with one movie only; let's make it The Terminator. We will merge all text columns into one since some words are redundant and keywords do not seem always that relevant compared to the overview\/tagline.","2a1a2975":"## I. Exploratory Data Analysis (EDA)\n\n### a. Sneak peak into the data"}}