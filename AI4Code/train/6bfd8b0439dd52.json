{"cell_type":{"a307710a":"code","e81d3ce7":"code","c533c542":"code","dc8e3e9c":"code","c83b537f":"code","11ceafe5":"code","8f913b19":"code","60d56047":"code","ef831aec":"code","af1eab7e":"code","01072f8d":"code","aff93e49":"code","e01cd4f9":"code","e0beaebe":"code","b821f957":"code","a87e75ae":"code","83c43802":"code","6b58bc67":"code","f329f8e1":"code","3a3c53bb":"code","2d0a43fb":"code","e7b13379":"code","303708e0":"markdown","5e6363ef":"markdown","8664a3e3":"markdown","bf15bf8d":"markdown"},"source":{"a307710a":"pip install transformers==4.5.0","e81d3ce7":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel","c533c542":"!nvidia-smi","dc8e3e9c":"torch.manual_seed(42)","c83b537f":"#tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI\/gpt-neo-1.3B\", bos_token='<|startoftext|>',\n                                         # eos_token='<|endoftext|>', pad_token='<|pad|>')\n#model = GPTNeoForCausalLM.from_pretrained(\"Martian\/Neo-GPT-Title-Generation-Electric-Car\").cuda()\n\ntokenizer = GPT2Tokenizer.from_pretrained('mrm8488\/GPT-2-finetuned-common_gen', bos_token='<|startoftext|>',\n                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\nmodel = GPT2LMHeadModel.from_pretrained('mrm8488\/GPT-2-finetuned-common_gen').cuda()\nmodel.resize_token_embeddings(len(tokenizer))\n","11ceafe5":"descriptions = pd.read_csv(\"..\/input\/poetry-foundation-poems\/PoetryFoundationData.csv\")","8f913b19":"descriptions[\"Title\"] = [x.replace(\"\\r\\r\\n\",\" \") for x in descriptions[\"Title\"]]","60d56047":"descriptions = descriptions[\"Title\"]","ef831aec":"descriptions","af1eab7e":"max_length = max([len(tokenizer.encode(description)) for description in descriptions])","01072f8d":"max_length","aff93e49":"class NetflixDataset(Dataset):\n    def __init__(self, txt_list, tokenizer, max_length):\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n        for txt in txt_list:\n            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n                                       max_length=max_length, padding=\"max_length\")\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx]","e01cd4f9":"dataset = NetflixDataset(descriptions, tokenizer, max_length=max_length)\ntrain_size = int(0.9 * len(dataset))\ntrain_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n","e0beaebe":"import gc\ngc.collect()","b821f957":"torch.cuda.empty_cache()","a87e75ae":"training_args = TrainingArguments(output_dir='.\/results', num_train_epochs=10, logging_steps=100, save_steps=1000,\n                                  per_device_train_batch_size=64, per_device_eval_batch_size=1,\n                                  warmup_steps=10, weight_decay=0.05, logging_dir='.\/logs', report_to = 'none')\n","83c43802":"Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                                              'attention_mask': torch.stack([f[1] for f in data]),\n                                                              'labels': torch.stack([f[0] for f in data])}).train()","6b58bc67":"modela = torch.load(\".\/model.pth\")","f329f8e1":"generated = tokenizer(\"<|startoftext|> \", return_tensors=\"pt\").input_ids.cuda()","3a3c53bb":"sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n                                max_length=50, top_p=0.95, temperature=1, num_return_sequences=2000)\n\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","2d0a43fb":"torch.save(model, 'model.pt')","e7b13379":"pd.options.display.max_colwidth = 1000\ndescriptions.sample(10)","303708e0":"### Loading GPT2-Medium Model from \ud83e\udd17 Model Hub ","5e6363ef":"### GPT Generated Description","8664a3e3":"**Code Borrowed from this medium post - https:\/\/medium.com\/geekculture\/fine-tune-eleutherai-gpt-neo-to-generate-netflix-movie-descriptions-in-only-47-lines-of-code-40c9b4c32475** ","bf15bf8d":"### Original Description (Random)"}}