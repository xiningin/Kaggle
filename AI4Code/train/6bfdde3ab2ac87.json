{"cell_type":{"3101c28e":"code","9326dfc2":"code","1d5c1fba":"code","2b0a391a":"code","14d2fafa":"code","c9024e2f":"code","50c87a1b":"code","3e91771f":"code","1820a813":"code","2bf7db1b":"markdown","d767f482":"markdown","6c7c223c":"markdown"},"source":{"3101c28e":"import sys\nsys.path.append('..\/input\/rapids-kaggle-utils')\n\nimport pandas as pd\nimport numpy as np\nimport cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\nimport cu_utils.transform as cutran\n\nimport gc\nfrom joblib import Parallel, delayed","9326dfc2":"PATH = \"..\/input\/optiver-realized-volatility-prediction\/\"\norder_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\ntrade_training = glob.glob(f'{PATH}\/trade_train.parquet\/*\/*')\nprint('size of order_book_training' ,len(order_book_training))\nprint('size of trade_training' ,len(trade_training))","1d5c1fba":"data_df = cudf.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\/c439ef22282f412ba39e9137a3fdabac.parquet')\noffsets = data_df.groupby(['time_id'], as_index=False).agg({'seconds_in_bucket':'min'}).reset_index(drop=True)\noffsets.columns = ['time_id', 'offset']\ndata_df = cudf.merge(data_df, offsets, on = ['time_id'], how = 'left')\ndata_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n# MultiIndex.from_product uses pandas in the background\n# That's why we need to transform the data into pd dataframe\ndata_df = data_df.set_index(['time_id', 'seconds_in_bucket'])\ncolumns = [col for col in data_df.columns.values]\ndata_df = data_df.reindex(cudf.MultiIndex.from_product([data_df.to_pandas().index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), columns=columns).fillna(method='ffill')\ndata_df = cudf.DataFrame(data_df.reset_index())","2b0a391a":"data_df","14d2fafa":"data_df = cudf.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\/c439ef22282f412ba39e9137a3fdabac.parquet')\noffsets = data_df.groupby(['time_id'], as_index=False).agg({'seconds_in_bucket':'min'}).reset_index(drop=True)\noffsets.columns = ['time_id', 'offset']\ndata_df = cudf.merge(data_df, offsets, on = ['time_id'], how = 'left')\ndata_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n# MultiIndex.from_product uses pandas in the background\n# That's why we need to transform the data into pd dataframe\ndata_df = data_df.set_index(['time_id', 'seconds_in_bucket'])\ncolumns = [col for col in data_df.columns.values]\nindices = cudf.MultiIndex.from_product([data_df.to_pandas().index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket'])\n\ndata_df = cudf.DataFrame().set_index(indices).join(data_df, how=\"left\").fillna(method='ffill').reset_index(drop=True)","c9024e2f":"data_df","50c87a1b":"def fix_offsets_ffill(data_df):\n    \n    data_df = cudf.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\/c439ef22282f412ba39e9137a3fdabac.parquet')\n    offsets = data_df.groupby(['time_id'], as_index=False).agg({'seconds_in_bucket':'min'}).reset_index(drop=True)\n    offsets.columns = ['time_id', 'offset']\n    data_df = cudf.merge(data_df, offsets, on = ['time_id'], how = 'left')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    # MultiIndex.from_product uses pandas in the background\n    # That's why we need to transform the data into pd dataframe\n    data_df = data_df.set_index(['time_id', 'seconds_in_bucket'])\n    columns = [col for col in data_df.columns.values]\n    indices = cudf.MultiIndex.from_product([data_df.to_pandas().index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket'])\n    \n    data_df = cudf.DataFrame().set_index(indices).join(data_df, how=\"left\").fillna(method='ffill').reset_index(drop=True)\n    \n    return data_df\n\n\ndef preprocess_book(book_path):\n    \n    def rel_vol_fe(df, null_val=-9999):\n    \n        # compute wap\n        for n in range(1, 3):\n            p1 = df[f\"bid_price{n}\"]\n            p2 = df[f\"ask_price{n}\"]\n            s1 = df[f\"bid_size{n}\"]\n            s2 = df[f\"ask_size{n}\"]\n            df[\"WAP\"] = (p1*s2 + p2*s1) \/ (s1 + s2)\n\n\n            df[\"log_wap\"] = df[\"WAP\"].log()\n            df[\"log_wap_shifted\"] = (df[[\"time_id\", \"log_wap\"]].groupby(\"time_id\")\n                                 .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                                incols={\"log_wap\": 'x'},\n                                                outcols=dict(y_out=cp.float32),\n                                                tpb=32)[\"y_out\"])\n            df = df[df[\"log_wap_shifted\"] != null_val]\n\n            df[\"diff_log_wap\"] = df[\"log_wap\"] - df[\"log_wap_shifted\"]\n            df[f\"diff_log_wap{n}\"] = df[\"diff_log_wap\"]**2\n\n\n    \n        # Summary statistics for different 'diff_log_wap'\n        sum_df = df.groupby(\"time_id\").agg({\"diff_log_wap1\": {\"sum\", \"mean\", \"std\", \"median\", \"max\", \"min\"}, \n                                        \"diff_log_wap2\": {\"sum\", \"mean\", \"std\", \"median\", \"max\", \"min\"}}\n                                      ).reset_index()\n    \n        # Create wanted features for training\n        def f(x):\n            if x[1] == \"\":\n                return x[0]\n            return x[0] + \"_\" + x[1]\n    \n        sum_df.columns = [f(x) for x in sum_df.columns]\n        sum_df[\"volatility1\"] = (sum_df[\"diff_log_wap1_sum\"])**0.5\n        sum_df[\"volatility2\"] = (sum_df[\"diff_log_wap2_sum\"])**0.5\n        sum_df[\"vol1_mean\"] = sum_df[\"diff_log_wap1_mean\"].fillna(0).values\n        sum_df[\"vol2_mean\"] = sum_df[\"diff_log_wap2_mean\"].fillna(0).values\n        sum_df[\"vol1_std\"] = sum_df[\"diff_log_wap1_std\"].fillna(0).values\n        sum_df[\"vol2_std\"] = sum_df[\"diff_log_wap2_std\"].fillna(0).values\n        sum_df[\"vol1_median\"] = sum_df[\"diff_log_wap1_median\"].fillna(0).values\n        sum_df[\"vol2_median\"] = sum_df[\"diff_log_wap2_median\"].fillna(0).values\n        sum_df[\"vol1_max\"] = sum_df[\"diff_log_wap1_max\"].fillna(0).values\n        sum_df[\"vol2_max\"] = sum_df[\"diff_log_wap2_max\"].fillna(0).values\n        sum_df[\"vol1_min\"] = sum_df[\"diff_log_wap1_min\"].fillna(0).values\n        sum_df[\"vol2_min\"] = sum_df[\"diff_log_wap2_min\"].fillna(0).values\n        sum_df[\"volatility_rate\"] = (sum_df[\"volatility1\"] \/ sum_df[\"volatility2\"]).fillna(0)\n        sum_df[\"mean_volatility_rate\"] = (sum_df[\"vol1_mean\"] \/ sum_df[\"vol2_mean\"]).fillna(0)\n        sum_df[\"std_volatility_rate\"] = (sum_df[\"vol1_std\"] \/ sum_df[\"vol2_std\"]).fillna(0)\n        sum_df[\"median_volatility_rate\"] = (sum_df[\"vol1_median\"] \/ sum_df[\"vol2_median\"]).fillna(0)\n        sum_df[\"max_volatility_rate\"] = (sum_df[\"vol1_max\"] \/ sum_df[\"vol2_max\"]).fillna(0)\n    \n        return sum_df[[\"time_id\", \"volatility1\", \"volatility2\", \n                   \"volatility_rate\", \"vol1_std\", \"vol2_std\",\n                   \"vol1_mean\", \"vol2_mean\", \"vol1_median\", \"vol2_median\",\n                   \"vol1_max\", \"vol2_max\", \"vol1_min\", \"vol2_min\",\n                   \"mean_volatility_rate\", \"std_volatility_rate\",\n                   \"median_volatility_rate\", \"max_volatility_rate\"]]\n\n    def spread_fe(df):\n    \n        # Bid ask spread\n        df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)\n                                    \/ df[['bid_price1', 'bid_price2']].max(axis = 1) - 1)                               \n    \n        # different spreads\n        df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n        df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n        df['v_spread_b'] = df['bid_price1'] - df['bid_price2']\n        df['v_spread_a'] = df['ask_price1'] - df['ask_price2']\n        \n        # Summary statistics for different spread\n        spread_df = df.groupby(\"time_id\", as_indes=False).agg({\"h_spread_l1\": { \"mean\", \"std\", \"median\", \"max\", \"min\"}, \n                                        \"h_spread_l2\": { \"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"v_spread_b\": {\"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"v_spread_a\": {\"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"bas\": {\"mean\"},\n                                       }).reset_index()\n    \n    \n        # Create wanted features for training\n        def f(x):\n            if x[1] == \"\":\n                return x[0]\n            return x[0] + \"_\" + x[1]\n    \n        spread_df.columns = [f(x) for x in spread_df.columns]\n\n        return spread_df\n    \n    # Stats for book data\n    book = cudf.read_parquet(book_path)\n    stock_id = int(book_path.split(\"=\")[1].split(\"\/\")[0])\n    book = fix_offsets_ffill(book)\n    rel_vol_data = rel_vol_fe(book)\n    spread_data = spread_fe(book)\n    \n    transbook = cudf.merge(rel_vol_data,\n                           spread_data,\n                           on = ['time_id'], how = 'left')\n    transbook['stock_id'] = stock_id\n    \n    return transbook\n\n\ndef preprocess_trade(trade_path):\n    \n    def trade_fe(trade_df, null_val=-9999):\n    \n        trade_df[\"log_wap_shifted\"] = (trade_df[[\"time_id\", \"price\"]].groupby(\"time_id\")\n                                 .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=-9999),\n                                                incols={\"price\": 'x'},\n                                                outcols=dict(y_out=cp.float32),\n                                                tpb=32)[\"y_out\"])\n        trade_df = trade_df[trade_df[\"log_wap_shifted\"] != -9999]\n    \n        trade_df[\"diff_log_wap\"] = trade_df[\"price\"] - trade_df[\"log_wap_shifted\"]\n        trade_df[f\"diff_log_wap\"] = trade_df[\"diff_log_wap\"]**2\n    \n        trade_features = trade_df.groupby('time_id', as_index=False).agg({'diff_log_wap':{'sum', 'mean', 'std'},\n                                                 'order_count':{'mean'},\n                                                 'size':{'sum'}\n                                                })\n        # Create wanted features for training\n        def f(x):\n            if x[1] == \"\":\n                return x[0]\n            return x[0] + \"_\" + x[1]\n        \n        trade_features.columns = [f(x) for x in trade_features.columns]\n    \n        trade_features['trade_volatility'] = (trade_features['diff_log_wap_sum']**0.5)\n        trade_features['trade_vol_mean'] = trade_features['diff_log_wap_mean'].fillna(0).values\n        trade_features['trade_vol_std'] = trade_features['diff_log_wap_std'].fillna(0).values\n        \n        return trade_features[[col for col in trade_features.columns if col not in ['diff_log_wap_sum', 'diff_log_wap_std', 'diff_log_wap_mean', ]]]\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_stat = trade_fe(trade_df)\n    \n    return trade_stat\n\n\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = PATH + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = PATH + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = PATH + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = PATH + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = cudf.merge(preprocess_book(file_path_book),preprocess_trade(file_path_trade),on='time_id',how='left')\n     \n        return df_tmp\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n            delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n            )\n\n    df =  cudf.concat(df,ignore_index = True)\n    return df","3e91771f":"%%time\ntrain = cudf.read_csv(PATH + 'train.csv')\ntrain_ids = train.stock_id.unique().to_array()\ntrain_process = preprocessor(list_stock_ids= train_ids, is_train = True)","1820a813":"df_train = train.merge(train_process, on = ['time_id', 'stock_id'], how = 'left').fillna(0)\ndf_train.to_csv(\".\/dtrain.csv\", index=False)\ndf_train.head()","2bf7db1b":"# Steps\/Code to reproduce bug","d767f482":"# Workaround not working","6c7c223c":"This solution was given by @aerdem4."}}