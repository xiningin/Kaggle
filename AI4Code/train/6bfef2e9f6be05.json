{"cell_type":{"559aea18":"code","61b537fe":"code","1009fb90":"code","ae1a87ad":"code","b3dc1550":"code","658ead67":"code","4e67a1b1":"code","e20cfe1d":"code","c19e64e1":"code","2698c344":"code","3bed4fe0":"code","1b9425e0":"code","d6d5f692":"markdown","4a85c390":"markdown","748dd08e":"markdown","087fdaec":"markdown","6c6b3221":"markdown","e7c7fb7d":"markdown","a7d90f55":"markdown","cf008610":"markdown","977600a4":"markdown","5006b182":"markdown"},"source":{"559aea18":"import numpy as np\nimport pandas as pd\nimport lightgbm\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport pickle\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold\nimport dateutil.easter as easter","61b537fe":"original_train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv', parse_dates=['date'])\noriginal_test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv', parse_dates=['date'])\ngdp_df = pd.read_csv('..\/input\/tps-2022-1-gdp\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')","1009fb90":"gdp_df.index=gdp_df['year']\ngdp_df.drop('year',axis=1,inplace=True)","ae1a87ad":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200","b3dc1550":"def get_gdp(row):\n    \"\"\"Return the GDP based on row.country and row.date.year\"\"\"\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]\n\nle_dict = {feature: LabelEncoder().fit(original_train_df[feature]) for feature in ['country', 'product', 'store']}\n\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    new_df = pd.DataFrame({'gdp': df.apply(get_gdp, axis=1),\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    new_df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), 'dayofyear'] += 1 # fix for leap years\n    \n    for feature in ['country', 'product', 'store']:\n        new_df[feature] = le_dict[feature].transform(df[feature])\n        \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df['days_from_easter'] = (df.date - easter_date).dt.days.clip(-5, 65)\n    \n    # Last Sunday of May (Mother's Day)\n    sun_may_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-5-31')),\n                                         2016: pd.Timestamp(('2016-5-29')),\n                                         2017: pd.Timestamp(('2017-5-28')),\n                                         2018: pd.Timestamp(('2018-5-27')),\n                                         2019: pd.Timestamp(('2019-5-26'))})\n    #new_df['days_from_sun_may'] = (df.date - sun_may_date).dt.days.clip(-1, 9)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date # used in GroupKFold\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntrain_df['target'] = np.log(train_df['num_sold'] \/ train_df['gdp'])\ntest_df = engineer(original_test_df)\n\nfeatures = test_df.columns.difference(['gdp'])","658ead67":"lgb_params = {\n        'objective': 'regression',\n        'force_row_wise': True,\n        'verbosity': -1,\n        'seed': 1,\n        'learning_rate': 0.03,\n        'lambda_l1': 5e-05,\n        'lambda_l2': 1e-06,\n        'num_leaves': 20,\n        'feature_fraction': 0.6,\n        'bagging_fraction': 0.43,\n        'bagging_freq': 5,\n        'min_child_samples': 17,\n        }                        \n\ncat_params = {\n        'eval_metric': 'SMAPE', \n        'use_best_model': True,\n        'learning_rate': 0.04421730001498909,\n        'depth': 6,\n        'l2_leaf_reg': 0.24960109471113703,\n        'random_strength': 2.1314060037536735,\n        'grow_policy': 'SymmetricTree',\n        'max_bin': 406,\n        'min_data_in_leaf': 77,\n        'bootstrap_type': 'Bayesian',\n        'bagging_temperature': 0.7392707417524894}\n\nxgb_params = {\n        'tree_method': 'hist',\n        'grow_policy' : 'lossguide',\n        'learning_rate': 0.03399878704233446,\n        'max_depth': 5,\n        'reg_alpha': 0.7814373604498039,\n        'reg_lambda': 0.00018093104956619317,\n        'max_delta_step': 2,\n        'min_child_weight': 14,\n        'colsample_bytree': 0.6489299778623602,\n        'subsample': 0.6033298718112065,\n        'max_leaves': 187,  \n        }","4e67a1b1":"import warnings\nwarnings.filterwarnings('ignore')\n\ndef training_prediction() :\n    \n    run = 5 # for seeds blending\n    test_pred_list = []\n    cat_test_pred_list = []\n    all_score_list = []\n    lgb_score_list = []\n    cat_score_list = []\n    xgb_score_list = []\n\n    xgb_test = xgb.DMatrix(test_df[features])\n\n    kf = GroupKFold(n_splits=4)\n\n    for i in range(run):\n\n        lgb_params['seed'] = i\n        cat_params['random_seed'] = i\n        xgb_params['seed'] = i\n\n        print(25*'-',\"RUN\",i,25*'-')\n\n        for fold, (train_idx, val_idx) in enumerate(\n            kf.split(train_df,\n            groups = train_df.date.dt.year)):\n\n            X_tr = train_df.iloc[train_idx]\n            X_va = train_df.iloc[val_idx]\n\n            # Preprocess the train data\n            X_tr_f = X_tr[features]\n            y_tr = X_tr.target.values\n\n            lgb_data_tr = lightgbm.Dataset(\n                        X_tr[features],\n                        label = y_tr,\n                        categorical_feature = ['country',\n                                             'product',\n                                             'store'])\n\n            xgb_data_tr = xgb.DMatrix(\n                        X_tr[features],\n                        label=y_tr)\n\n            # Preprocess the validation data\n            X_va_f = X_va[features]\n            y_va = X_va.target.values\n\n            lgb_data_va = lightgbm.Dataset(\n                        X_va[features], \n                        label = y_va)\n\n            xgb_data_va = xgb.DMatrix(\n                        X_va[features],\n                        label = y_va)\n            evallist = [(xgb_data_va, 'eval'), \n                        (xgb_data_tr, 'train')]\n\n            # Training  \n            lgb_model = lightgbm.train(\n                        lgb_params,\n                        lgb_data_tr,\n                        num_boost_round=2000,\n                        categorical_feature =['country',\n                                             'product',\n                                             'store'])\n\n            cat_model = CatBoostRegressor(**cat_params) \n            cat_model.fit(\n                        X_tr_f,\n                        y_tr,eval_set =[( X_va_f,y_va)],\n                        verbose = 0,\n                        early_stopping_rounds = 200)\n\n            xgb_model = xgb.train(\n                        xgb_params, \n                        xgb_data_tr,\n                        num_boost_round=2000, \n                        evals = evallist,\n                        verbose_eval = 0,\n                        early_stopping_rounds = 200)\n\n            # Predictions\n            lgb_y_va_pred = np.exp(lgb_model.predict(X_va_f)) * X_va['gdp']\n            test_pred_list.append(np.exp(lgb_model.predict(test_df[features])) * test_df['gdp'].values)\n\n            cat_y_va_pred = np.exp(cat_model.predict(X_va_f)) * X_va['gdp']\n            test_pred_list.append(np.exp(cat_model.predict(test_df[features])) * test_df['gdp'].values)\n            cat_test_pred_list.append(np.exp(cat_model.predict(test_df[features])) * test_df['gdp'].values)\n\n            xgb_y_va_pred = np.exp(xgb_model.predict(xgb_data_va)) * X_va['gdp']\n            test_pred_list.append(np.exp(xgb_model.predict(xgb_test)) * test_df['gdp'].values)\n\n            del  xgb_data_tr, xgb_data_va, lgb_data_va\n\n            # Score list for each algo\n            lgb_smape = np.round(np.mean(smape_loss(X_va.num_sold, lgb_y_va_pred)),4)\n            lgb_score_list.append(lgb_smape)\n\n            cat_smape = np.round(np.mean(smape_loss(X_va.num_sold, cat_y_va_pred)),4)\n            cat_score_list.append(cat_smape)\n\n            xgb_smape = np.round(np.mean(smape_loss(X_va.num_sold, xgb_y_va_pred)),4)\n            xgb_score_list.append(xgb_smape)\n\n            # list for total average score (mean)\n            all_score_list += lgb_score_list + cat_score_list + xgb_score_list\n\n        print('RUN', i,\"Cumulative Average SMAPE\", np.round(sum(all_score_list) \/ len(all_score_list),4))\n    print(40*'*')   \n    print(\"TOTAL Average SMAPE   :\", np.round(sum(all_score_list) \/ len(all_score_list),4))\n    print(40*'*') \n    print(\"\\nOf which :\")\n    print(\"LGB Average SMAPE   :\", np.round(sum(lgb_score_list) \/ len(lgb_score_list),4))\n    print(\"CAT Average SMAPE   :\", np.round(sum(cat_score_list) \/ len(cat_score_list),4))\n    print(\"XGB Average SMAPE   :\", np.round(sum(xgb_score_list) \/ len(xgb_score_list),4),'\\n\\n')\n\n\n    # Training scores visualization\n    plt.figure(figsize = (12,7))\n    plt.plot(lgb_score_list,label ='LightGBM')\n    plt.plot(cat_score_list,label ='CatBoost')\n    plt.plot(xgb_score_list,label ='XgBoost')\n\n    for i in range(run-1):\n        plt.axvline(x = (i+1) * 4, \n                    label = 'RUN' +str(i+1),\n                    linewidth = 2, \n                    color ='black',\n                    linestyle = 'dotted')\n    plt.axvline(x = 19, \n                    label ='RUN' +'4',\n                    linewidth = 2, \n                    color ='black',\n                    linestyle = 'dotted')\n\n    plt.ylabel('SMAPE')\n    plt.xlabel('RUN x FOLDS')\n    plt.title('Comparison between runs and Algo',fontsize = 20)\n    plt.legend()\n    plt.show()\n    \n    return lgb_score_list, cat_score_list, xgb_score_list, all_score_list, test_pred_list, cat_test_pred_list","e20cfe1d":"lgb_score_list, cat_score_list, xgb_score_list, all_score_list, test_pred_list, cat_test_pred_list = training_prediction()","c19e64e1":"#https:\/\/www.kaggle.com\/andrej0marinchenko\/tps-jan-2022-automated-ensembling :\npseudo = pd.read_csv('..\/input\/best-submission\/submission_best.csv')\npseudo_test = original_test_df.copy()\npseudo_test['num_sold'] = pseudo['num_sold']\npseudo_df = engineer(pseudo_test)\npseudo_df['date'] = pseudo_test.date \npseudo_df['num_sold'] = pseudo_test.num_sold.astype(np.float32)\npseudo_df['target'] = np.log(pseudo_df['num_sold'] \/ pseudo_df['gdp'])\n\ntrain_df = pd.concat([train_df,pseudo_df],axis=0)\ntrain_df = train_df.reset_index(drop = True)","2698c344":"p_lgb_score_list, p_cat_score_list, p_xgb_score_list, p_all_score_list, p_test_pred_list, p_cat_test_pred_list = training_prediction()","3bed4fe0":"sub = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\nsub['num_sold'] = sum(p_test_pred_list) \/ len(p_test_pred_list)\nsub.to_csv('submission_cat_lgb_xgb.csv', index = False)\npd.read_csv('submission_cat_lgb_xgb.csv').head(2)","1b9425e0":"sub['num_sold'] = sum(p_cat_test_pred_list) \/ len(p_cat_test_pred_list)\nsub.to_csv('submission_cat.csv', index = False)\npd.read_csv('submission_cat.csv').head(2)","d6d5f692":"<h2> Feature engineering","4a85c390":"<h2> Parameters from Optuna tunings","748dd08e":"<h2> Training and predictions","087fdaec":"<h4> We can see that SEED does not provide very different results between each run.\nCatboost provides the best result and xgboost the worse, some work has to be done for xgb tuning ...","6c6b3221":"<h4> We can see that algo are crushed by pseudo labels...no benefits","e7c7fb7d":"<h2> First training","a7d90f55":"<h2> Submissions","cf008610":"<h3>Our teacher (Ambrosm) gave us an homework after his exciting lesson :-)\n    \n    \nhttps:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart\n    \nPlease upvote, I would like my degree  :-)","977600a4":"<h2> Second training with pseudo labelling","5006b182":"<h2> Exactly the same Features engineering from AmbrosM"}}