{"cell_type":{"44da7a79":"code","dc978f02":"code","6625df3f":"code","ebdd71f2":"code","9a59cd75":"code","6805a274":"code","0d150482":"code","fc77ee16":"code","ada1f713":"code","82b72867":"code","8d904562":"code","eda113fb":"code","cae67b43":"code","1b97346a":"code","3535d367":"code","a0238544":"markdown","2d3ff35f":"markdown","a92ba7e8":"markdown","bd1e9679":"markdown","62bbd8d6":"markdown","8f30ade7":"markdown","6f8457cb":"markdown","addefea6":"markdown","c08ea404":"markdown","6511677d":"markdown","a188142f":"markdown","e7d038bf":"markdown","30d2a622":"markdown","e087c41e":"markdown","e6ada4a0":"markdown","0011e23c":"markdown","4072d2e8":"markdown","756d1c5b":"markdown"},"source":{"44da7a79":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport scipy.stats.distributions as scp\nfrom scipy.optimize import curve_fit\nfrom scipy.special import factorial\nfrom scipy.stats import poisson\nimport sklearn.preprocessing as skl\nimport seaborn as sns\nimport warnings\nimport scipy.stats as st\nimport statsmodels as sm\n\nVariables = ['video_id', 'views', 'likes', 'dislikes', 'comment_count']\n        \nus_yt = pd.read_csv('..\/input\/youtube-new\/USvideos.csv', usecols=Variables) #USA\nca_yt = pd.read_csv('..\/input\/youtube-new\/CAvideos.csv', usecols=Variables) #Canada\nde_yt = pd.read_csv('..\/input\/youtube-new\/DEvideos.csv', usecols=Variables) #Germany\nfr_yt = pd.read_csv('..\/input\/youtube-new\/FRvideos.csv', usecols=Variables) #France\ngb_yt = pd.read_csv('..\/input\/youtube-new\/GBvideos.csv', usecols=Variables) #Great Brittain\nin_yt = pd.read_csv('..\/input\/youtube-new\/INvideos.csv', usecols=Variables) #India\njp_yt = pd.read_csv('..\/input\/youtube-new\/JPvideos.csv', usecols=Variables) #Japan\nkr_yt = pd.read_csv('..\/input\/youtube-new\/KRvideos.csv', usecols=Variables) #South Korea\nmx_yt = pd.read_csv('..\/input\/youtube-new\/MXvideos.csv', usecols=Variables) #Mexico\nru_yt = pd.read_csv('..\/input\/youtube-new\/RUvideos.csv', usecols=Variables) #Russia\n\nN = (us_yt.dtypes == 'int64')\nNumeric = list(N[N].index)\n\nus_yt['Country'] = \"US\"\nca_yt['Country'] = \"CA\"\nde_yt['Country'] = \"DE\"\nfr_yt['Country'] = \"FR\"\ngb_yt['Country'] = \"GB\"\nin_yt['Country'] = \"IN\"\njp_yt['Country'] = \"JP\"\nkr_yt['Country'] = \"KR\"\nmx_yt['Country'] = \"MX\"\nru_yt['Country'] = \"RU\"\n\n#us_yt_sc = us_yt\n#ca_yt_sc = ca_yt\n#de_yt_sc = de_yt\n#fr_yt_sc = fr_yt\n#gb_yt_sc = gb_yt\n#in_yt_sc = in_yt\n#jp_yt_sc = jp_yt\n#kr_yt_sc = kr_yt\n#mx_yt_sc = mx_yt\n#ru_yt_sc = ru_yt\n\n# Center the data before computation of RV coefficients\n#us_yt_sc[Numeric] = skl.scale(us_yt[Numeric], axis=0, with_mean=True)\n#ca_yt_sc[Numeric] = skl.scale(ca_yt[Numeric], axis=0, with_mean=True)\n#de_yt_sc[Numeric] = skl.scale(de_yt[Numeric], axis=0, with_mean=True)\n#fr_yt_sc[Numeric] = skl.scale(fr_yt[Numeric], axis=0, with_mean=True)\n#gb_yt_sc[Numeric] = skl.scale(gb_yt[Numeric], axis=0, with_mean=True)\n#in_yt_sc[Numeric] = skl.scale(in_yt[Numeric], axis=0, with_mean=True)\n#jp_yt_sc[Numeric] = skl.scale(jp_yt[Numeric], axis=0, with_mean=True)\n#kr_yt_sc[Numeric] = skl.scale(kr_yt[Numeric], axis=0, with_mean=True)\n#mx_yt_sc[Numeric] = skl.scale(mx_yt[Numeric], axis=0, with_mean=True)\n#ru_yt_sc[Numeric] = skl.scale(ru_yt[Numeric], axis=0, with_mean=True)\n \ndf = pd.concat([us_yt, ca_yt, de_yt,fr_yt,gb_yt,in_yt,jp_yt,kr_yt,mx_yt,ru_yt] )\ndf.reset_index\ndf.head()","dc978f02":"# Function for limiting extreme values\ndef ExtremeValues (Variable,DataSet,Quantile):\n    DefinedCapped = DataSet[Variable].quantile(q=Quantile)\n    DataSet.loc[DataSet[Variable] >= DefinedCapped, Variable] = DefinedCapped\n    return DataSet","6625df3f":"df_cap = ExtremeValues ('views',df,0.9)\ndf_cap = ExtremeValues ('likes',df,0.9)\ndf_cap = ExtremeValues ('dislikes',df,0.9)\ndf_cap = ExtremeValues ('comment_count',df,0.9)","ebdd71f2":"n_bins = 100\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\nax0, ax1, ax2, ax3 = axes.flatten()\n\nax0.hist(df_cap['views'], n_bins, density=True, color = 'skyblue')\nax0.set_title(\"Views\")\n\nax1.hist(df_cap['likes'], n_bins, density=True, color = 'yellow')\nax1.set_title(\"Likes\")\n\nax2.hist(df_cap['dislikes'], n_bins, density=True, color = 'purple')\nax2.set_title(\"Dislikes\")\n\nax3.hist(df_cap['comment_count'], n_bins, density=True, color = 'green')\nax3.set_title(\"comment_count\")\n\nplt.show()","9a59cd75":"mean1, var1  = scp.norm.fit(df_cap['views'])\nx1 = np.linspace(0,2000000,1000000)\nx1_fit = scp.norm.pdf(x1, mean1, var1)\n\nplt.hist(df_cap['views'], density=True, color = 'skyblue')\nplt.plot(x1,x1_fit,'r-')","6805a274":"df_views_lim = df.loc[df['views'] < 250000, 'views']\nmean2, var2  = scp.norm.fit(df_views_lim)\nx2 = np.linspace(0,250000,100000)\nx2_fit = scp.norm.pdf(x2, mean2, var2)\n\nplt.hist(df_views_lim, density=True, color = 'skyblue')\nplt.plot(x2,x2_fit,'r-')","0d150482":"# Distributions to check \nAllDistributions = [ st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n        st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n        st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n        st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n        st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n        st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n        st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n        st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n        st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy]\n\nChosenDistributions = [ st.alpha,st.beta,st.burr,st.cauchy,st.chi2,st.expon,st.gamma,st.gompertz,st.gumbel_r,st.laplace,\n        st.levy,st.logistic,st.maxwell,st.norm,st.rayleigh,st.uniform]\n\nChosenDistributionsNames = [ st.alpha,st.beta,st.burr,st.cauchy,st.chi2,st.expon,st.gamma,st.gompertz,st.gumbel_r,st.laplace,\n        st.levy,st.logistic,st.maxwell,st.norm,st.rayleigh,st.uniform]","fc77ee16":"fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\n\nfor distribution, ax in zip(ChosenDistributions, axes.flat):\n    params = distribution.fit(df_views_lim)\n    arg = params[:-2]\n    loc = params[-2]\n    scale = params[-1]\n    x = np.linspace(0,250000,len(df_views_lim))\n    pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n    title_correct = (str(distribution).replace(\"<scipy.stats._continuous_distns.\",\"\")).split(\"_gen\",1)[0]\n    ax.hist(df_views_lim, bins=n_bins , density=True, color = 'skyblue')\n    ax.plot(x,pdf,'r-')\n    ax.set_title(title_correct)","ada1f713":"for i in range(0,len(ChosenDistributionsNames)):\n    ChosenDistributionsNames[i] = (str(ChosenDistributionsNames[i]).replace(\"<scipy.stats._continuous_distns.\",\"\")).split(\"_gen\",1)[0]\n\nPerf = pd.DataFrame([0.0000] * 16,columns=['SSE'],index=ChosenDistributionsNames)\nPerf['Scale']=[0.0000] * 16\nPerf['Location']=[0.0000] * 16\nPerf['NoOtherParameters']=[0.0000] * 16","82b72867":"for distribution, i in zip(ChosenDistributions, range(0,len(ChosenDistributions))):   \n    \n    y, x = np.histogram(df_views_lim, bins=n_bins, density=True)\n    x = (x + np.roll(x, -1))[:-1] \/ 2.0\n    \n    params = distribution.fit(df_views_lim)\n    \n    arg = params[:-2]\n    loc = params[-2]\n    scale = params[-1]\n    \n    pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n    Perf.SSE[i] = np.sum(np.power(y - pdf, 2.0)) * np.power(10, 10.0)\n    Perf.Scale[i] = scale\n    Perf.Location[i] = loc\n    Perf.NoOtherParameters[i] = len(arg)","8d904562":"def color_negative_red(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for negative\n    strings, black otherwise.\n    \"\"\"\n    color = 'red' if val < 0 else 'black'\n    return 'color: %s' % color\ndef highlight_max(s):\n    '''\n    highlight the maximum in a Series yellow.\n    '''\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]\ndef highlight_min(s):\n    '''\n    highlight the minimum in a Series yellow.\n    '''\n    is_min = s == s.min()\n    return ['background-color: yellow' if v else '' for v in is_min]","eda113fb":"Perf.sort_values(by=\"SSE\").style.applymap(color_negative_red).apply(highlight_max).format({'SSE': \"{:.6f}\",'Scale': \"{:.0f}\",'Location': \"{:.0f}\",'NoOtherParameters': \"{:.0f}\"})","cae67b43":"y, x = np.histogram(df_views_lim, bins=n_bins, density=True)\nx = (x + np.roll(x, -1))[:-1] \/ 2.0\n\npdf_L = st.levy.pdf(x,Perf.Location[10],Perf.Scale[10])\npdf_E = st.expon.pdf(x, Perf.Location[5], Perf.Scale[5])\n\npdf_fin = (pdf_E + pdf_L)\/2\n\nScore = np.sum(np.power(y - pdf_fin, 2.0)) * np.power(10, 10.0)\n\nprint(format(Score))","1b97346a":"pdf_L = st.levy.pdf(x,Perf.Location[10],Perf.Scale[10])\npdf_E = st.expon.pdf(x, Perf.Location[5], Perf.Scale[5])\n\nMixWeights = pd.DataFrame([99.00]*11,columns=['SSE'], index=range(0,11))\nMixList = [\"\"]*11\n\nfor i in range(0,11):\n    pdf_fin  =  pdf_E * i\/10 + pdf_L * (1-i\/10)\n    MixWeights.SSE[i]= np.sum(np.power(y - pdf_fin, 2.0)) * np.power(10, 10.0)\n    MixList[i]=\"Exponential: \" + str(round(i\/10,2)) + \" Levy: \" + str(round(1-i\/10,2))\n\nMixWeights=MixWeights.style.apply(highlight_min)\nMixWeights","3535d367":"pdf_fin  =  pdf_E * 6\/10 + pdf_L * (1-4\/10)\n\nfig, axes = plt.subplots(figsize=(10, 10))\naxes.hist(df_views_lim, bins=n_bins , density=True, color = 'skyblue')\naxes.plot(x,pdf_E,'g-',label='Exponential')\naxes.plot(x,pdf_L,'y-',label='Levy')\naxes.plot(x,pdf_fin,'r-',label='Mixture')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes.set_title('Mixture of exponential and Levy distributions')","a0238544":"Already this first mix leads us to huge improvement.","2d3ff35f":"The piece of the data between 0 and 250.000 views. Normal distribution fitted:","a92ba7e8":"I start with the mixture of Exponential and Levy as they take high positions in the rank and they are simple parametrically-wise.","bd1e9679":"# Parametric distributions fitting","62bbd8d6":"I start from the simplest method of non-parametric data analysis, namely histogram. This method relies on bucketing of our data and consequently plotting them.","8f30ade7":"Alright, normal distribution will not help here. However, we will keep Gaussian one as a benchmark. Let's explore other distributions. For this first we have list of all distributions which are available for 'fir' function and I have to admit it is really comprehensive set of distributions.\n\nFor time being and example reasons we will just choose 16 in arbitrary way (popular ones).","6f8457cb":"Fitting normal distribution to the 'views' column with values capped on quantile 0.90.","addefea6":"Alright, weight 0.6 for exponential distribution and effectively 0.4 for Levy give us the best score, which is an improvement by ~40% by solo distribution. This method shows how one can easily improve fit by application of mixture on the top of whole x axis.\n\nThere are another interesting topics which can be explored further in here:\n* full 'General Mixture Model' procedure, in which we fit model by iterative adjustment of parameters for different distributions\n* local analysis of distributions mix","c08ea404":"Ok, ready scaled data.","6511677d":"# Intro to distribution analysis\nI explore the topics of parametric and non-parametric distributions by use of big YouTube videos data. I aim to cover following topics:\n* empirical distribution analysis\n* parametric distributions fitting\n* mixture distributions analysis","a188142f":"# Empirical distribution analysis","e7d038bf":"We apply some steps which I describe granularly in [my second notebook on this topic](https:\/\/www.kaggle.com\/jjmewtw\/yt-pearson-spearman-distance-corr-rv-coef). In shortcut:\n* load all the data per country-level\n* identify numeric variables\n* scale and center the data\n* merge all the data to build world data set","30d2a622":"# Data preparation","e087c41e":"Alright, the best results I receive for exponentail distribution, what is a bit surprisingbecause it is very simple one-parametrical structure. In general inferior towards Weibull\/ Burr.\n\nIt would be good to have a possiblity to profit from not only distribution but from mix of them, right? The next part will investigate this possiblity as I will focus on mixture of distributions analysis.","e6ada4a0":"This looks interesting. Isn't it? We can easily change distributions and compare their behaviour on our truncated data.\n\nBut which distribution is the best? We can say that 'laplace', 'levy; and 'logistic' seem to make sense while 'uniform' (of course), 'chi2' and 'beta' don't really fit. However, it is really hard to figure out the winner of the competition. For this I need to define the error function and just to compare all of them.","0011e23c":"# Mixture of distributions analysis","4072d2e8":"Mixture of probabilistic distribution is one of unsupervised methods to build ultimate pdf by weighing input pdf's. This is quite pwoerful method allowing for profitting from different distributions at once. Good theoretical explanation is [here](https:\/\/pomegranate.readthedocs.io\/en\/latest\/GeneralMixtureModel.html) .","756d1c5b":"**Remark.** Colors have meaning:\n* skyblue - views\n* yellow - likes\n* purple - dislikes\n* green - comment count\n* red - fitted distribution"}}