{"cell_type":{"1027a78f":"code","0bec6c62":"code","859a0c4b":"code","422f9c68":"code","1cd325ed":"code","058fe827":"code","30e39efd":"code","08190492":"code","d9641383":"code","51cd05ae":"code","4c110d89":"code","bbc01a5c":"code","1e591e82":"code","8f102d37":"code","8dca14a8":"code","a89b22a0":"code","7ad9dfa0":"code","e7f46290":"code","0562c5ad":"code","9f3cc64f":"code","40bd206a":"code","d8ece824":"code","5578acb8":"code","659460e1":"code","abd41eee":"code","06b91b5e":"code","605238b3":"code","178f61b4":"code","45c9cca9":"code","5c57c910":"code","7ed50168":"code","19578086":"code","0ca9dfa3":"code","58c1cd56":"code","bba54405":"code","d79ff315":"code","4279e474":"code","c8712855":"code","fa6ff13a":"code","728cf04f":"code","df4144cc":"code","1fb5e8d3":"code","35c470ef":"code","8be1981a":"code","3ac4806e":"code","d374d709":"code","d8225521":"code","ec73d784":"code","fb4e2758":"code","78af40cb":"code","40a420e9":"code","8adaf978":"code","e25a18f6":"code","b93e9997":"code","e21ad293":"code","e7f6ae8f":"code","c8852ee7":"code","35067a7b":"code","872c2a5d":"code","5d65fec5":"code","497a0d9a":"code","1e5503db":"code","fc8567b6":"code","2071df13":"code","bc4d6a77":"code","1e2408f4":"code","53f0a749":"code","4f27e95e":"code","e8030729":"code","82a2431d":"code","6c76ccdc":"code","dce06837":"code","7f949e20":"code","92211f5d":"code","51d4742b":"code","22f7f051":"code","8c818420":"code","1d9b641a":"code","414a9aac":"code","9eba7cf5":"code","d39790ea":"code","86557e1f":"code","ac3102c8":"code","9cfd1158":"code","83839c83":"markdown","c9995fad":"markdown","54e792e5":"markdown","2845f021":"markdown","b511481c":"markdown","191c9e0d":"markdown","1cad06f9":"markdown","932169cc":"markdown","94c5fd68":"markdown","ca3c2395":"markdown","9c622cff":"markdown","f6f74127":"markdown","a935b532":"markdown","8389a4d0":"markdown","b5f54da7":"markdown","bc8d74a5":"markdown","de232212":"markdown","5cfd8bb0":"markdown"},"source":{"1027a78f":"import socket\nimport sys\n\nis_kaggle = True\nif not is_kaggle:\n    from my_utils import get_notebook_path\n    get_notebook_path().split('\/')[-1].split('.')[0]\n    NB = get_notebook_path().split('\/')[-1].split('.')[0]\n    HOST = socket.gethostname()\n    import mlflow\n    from logzero import logger\n    DELIMITER = '\\\\'\n\nelse:\n    NB = 'exp0093'\n    HOST = 'DESKTOP-M3SEAIN'\n    sys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\n    DELIMITER = '\/'\ndescription = '256'\n\nprint(f'{HOST}_{NB}')\n\nNB, HOST","0bec6c62":"#from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nfrom sklearn.model_selection import StratifiedKFold, KFold\nif not is_kaggle:\n    from sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport random\nimport os\nimport gc\nimport shutil\nimport timm\n\n\nimport torch.optim as optim\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as albu\n\nimport pickle\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nif is_kaggle:\n    DATA_DIR = Path('..\/input\/petfinder-pawpularity-score')\n    OUTPUT_DIR = Path('.\/')\n    CP_DIR = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0093'.lower().replace('-', '').replace('_', '-'))\nelse:\n    ROOT_DIR = Path('..\/..\/')\n    DATA_DIR = ROOT_DIR \/ 'data'\n    OUTPUT_DIR = Path('..\/') \/ 'output'\n    CP_DIR = OUTPUT_DIR \/ 'checkpoint'\n\ndef to_pickle(filename, obj):\n    with open(filename, mode='wb') as f:\n        pickle.dump(obj, f)\n        \ndef unpickle(filename):\n    with open(filename, mode='rb') as fo:\n        p = pickle.load(fo)\n    return p ","859a0c4b":"\nclass Config:\n    N_LABEL = 1\n    N_FOLD = 5\n    RANDOM_SATE = 42\n    LR = 1.0e-05\n    MAX_LR = 1e-5\n    PATIENCE = 18\n    EPOCH = 10\n    BATCH_SIZE = 32\n    SKIP_EVALUATE_NUM = 0\n    BACK_BONE = 'swin_large_patch4_window7_224'\n    RUN_FOLD_COUNT = 10\n    IMG_SIZE=224\n    T_MAX=20\n    ETA_MIN=3.0e-7\n    SCHEDULER_GAMMA=1.0\n    ACCUMULATION_STEMP=1 \n    \nif is_kaggle:\n    Config.BATCH_SIZE = 24\n    \ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=Config.RANDOM_SATE)","422f9c68":"glob.glob('..\/input\/*')","1cd325ed":"test_img_map = {}\nfor img_path in tqdm(glob.glob(str(DATA_DIR \/  'test\/*'))):\n    test_img_map[img_path.split(DELIMITER)[-1].split('.')[0]] = img_path","058fe827":"test_df = pd.read_csv(DATA_DIR \/ 'test.csv')","30e39efd":"from fastai.vision.all import *","08190492":"SEED = 46\nBATCH_SIZE = 16\nset_seed(SEED, reproducible=True)\ndataset_path = Path('..\/input\/petfinder-pawpularity-score\/')\ndataset_path.ls()","d9641383":"if not os.path.exists('\/root\/.cache\/torch\/hub\/checkpoints\/'):\n    os.makedirs('\/root\/.cache\/torch\/hub\/checkpoints\/')\n!cp '..\/input\/swin-transformer\/swin_large_patch4_window7_224_22kto1k.pth' '\/root\/.cache\/torch\/hub\/checkpoints\/swin_large_patch4_window7_224_22kto1k.pth'","51cd05ae":"set_seed(SEED, reproducible=True)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.use_deterministic_algorithms = True","4c110d89":"def petfinder_rmse(input,target):\n    return 100*torch.sqrt(F.mse_loss(F.sigmoid(input.flatten()), target))","bbc01a5c":"test_df = pd.read_csv(DATA_DIR \/ 'test.csv')\n\ntest_path_list = []\nfor f in glob.glob('..\/input\/petfinder-pawpularity-score\/test\/*'):\n    test_path_list.append(\n        {\n            'Id':f.split('\/')[-1].split('.')[0],\n            'path':f\n        }\n    )\ntest_df = test_df.merge(pd.DataFrame(test_path_list), how='left', on='Id')\ntest_df['norm_score'] = -1\ntest_df","1e591e82":"%%time\nall_preds = []\n\nfor i in range(10):\n    \n    print(f'===={i}====')\n    \n    path = '..\/input\/johannyjm-1790651\/'\n        \n    learn = load_learner(path + f'model_fold_{i}.pkl', cpu=False)\n        \n    dls = ImageDataLoaders.from_df(test_df, #pass in train DataFrame\n                               valid_pct=0.2, #80-20 train-validation random split\n                               seed=SEED, #seed\n                               fn_col='path', #filename\/path is in the second column of the DataFrame\n                               label_col='norm_score', #label is in the first column of the DataFrame\n                               y_block=RegressionBlock, #The type of target\n                               bs=BATCH_SIZE, #pass in batch size\n                               num_workers=4,\n                               item_tfms=Resize(224), #pass in item_tfms\n                               batch_tfms=setup_aug_tfms([Brightness(), Contrast(), Hue(), Saturation()])) \n    \n    test_dl = dls.test_dl(test_df)\n    \n    preds, _ = learn.tta(dl=test_dl, n=5, beta=0)\n    \n    all_preds.append(preds)\n    \n    del learn, test_dl\n\n    torch.cuda.empty_cache()\n\n    gc.collect()","8f102d37":"y_johannyjm_1790651 = np.mean(np.stack(all_preds), axis=0).reshape(-1) * 100\ny_johannyjm_1790651","8dca14a8":"%%time\nfast_ai_004_all_preds = []\n\nfor i in range(5):\n    \n    print(f'===={i}====')\n    \n    \n    \n    path = '..\/input\/fastai-004\/'\n        \n    learn = load_learner(path + f'fastai_fastai_0004_{i}.pkl', cpu=False)\n    \n        \n    dls = ImageDataLoaders.from_df(test_df, #pass in train DataFrame\n                               valid_pct=0.2, #80-20 train-validation random split\n                               seed=SEED, #seed\n                               fn_col='path', #filename\/path is in the second column of the DataFrame\n                               label_col='norm_score', #label is in the first column of the DataFrame\n                               y_block=RegressionBlock, #The type of target\n                               bs=BATCH_SIZE, #pass in batch size\n                               num_workers=4,\n                               item_tfms=Resize(224), #pass in item_tfms\n                               batch_tfms=setup_aug_tfms([Brightness(), Contrast(), Hue(), Saturation()])) \n    \n    test_dl = dls.test_dl(test_df)\n    \n    preds, _ = learn.tta(dl=test_dl, n=10, beta=0)\n    \n    fast_ai_004_all_preds.append(preds)\n    \n    del learn, test_dl\n\n    torch.cuda.empty_cache()\n\n    gc.collect()","a89b22a0":"fast_ai_004_preds = np.mean(np.stack(fast_ai_004_all_preds), axis=0).reshape(-1) * 100\nfast_ai_004_preds","7ad9dfa0":"\nclass Config:\n    N_LABEL = 1\n    N_FOLD = 5\n    RANDOM_SATE = 42\n    LR = 1.0e-05\n    MAX_LR = 1e-5\n    PATIENCE = 18\n    EPOCH = 10\n    BATCH_SIZE = 32\n    SKIP_EVALUATE_NUM = 0\n    BACK_BONE = 'swin_large_patch4_window7_224'\n    RUN_FOLD_COUNT = 10\n    IMG_SIZE=224\n    T_MAX=20\n    ETA_MIN=3.0e-7\n    SCHEDULER_GAMMA=1.0\n    ACCUMULATION_STEMP=1 \nseed_everything(seed=Config.RANDOM_SATE)","e7f46290":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","0562c5ad":"def get_test_augmentation(IM_SZ=384):\n    train_transform = [\n        albu.Resize(height=IM_SZ, width=IM_SZ),\n        albu.Normalize(),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_fllsize_augmentation(IM_SZ=256):\n    train_transform = [\n        albu.LongestMaxSize(max_size=IM_SZ, always_apply=False),\n        albu.PadIfNeeded(always_apply=True, min_height=IM_SZ, min_width=IM_SZ, border_mode=2),\n        albu.Normalize(),\n    ]\n    return albu.Compose(train_transform)","9f3cc64f":"class PetfinderDataSet_base(Dataset):\n    def __init__(self, df, img_map, transforms, data_type=None):\n        self.df = df\n        self.img_map = img_map\n        self.data_type = data_type\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if self.data_type in ['train', 'valid']:\n            pawpularity = self.df.iloc[idx]['Pawpularity']\n            object_id = self.df.iloc[idx]['Id']\n            img = self.img_map[object_id]\n        else:\n            pawpularity = -1\n            object_id = self.df.iloc[idx]['Id']\n            file_path = self.img_map[object_id]\n            img = cv2.imread(file_path)\n            \n        height = img.shape[0]\n        width = img.shape[1]\n            \n        if height > width:\n            center = int(height\/2)\n            half = int(width\/2)\n            img = img[center - half:center + half, :]\n\n        elif width > height:\n            center = int(width\/2)\n            half = int(height\/2)\n            img = img[:, center - half:center + half]\n        \n        augmented = self.transforms(image=img)\n        img = augmented['image']\n        img = np.moveaxis(img, 2, 0)\n\n        return img, pawpularity \/ 100","40bd206a":"class PetfinderDataSet_fllsize(Dataset):\n    # default fllsize_transforms --> get_fllsize_augmentation(IM_SZ=256)\n    def __init__(self, df, img_map, transforms, data_type=None, fllsize_transforms=get_fllsize_augmentation(IM_SZ=256)):\n        self.df = df\n        self.img_map = img_map\n        self.data_type = data_type\n        self.transforms = transforms\n        self.fllsize_transforms = fllsize_transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if self.data_type in ['train', 'valid']:\n            pawpularity = self.df.iloc[idx]['Pawpularity']\n            object_id = self.df.iloc[idx]['Id']\n            img = self.img_map[object_id]\n        else:\n            pawpularity = -1\n            object_id = self.df.iloc[idx]['Id']\n            file_path = self.img_map[object_id]\n            img = cv2.imread(file_path)\n            \n        full_augmented = self.fllsize_transforms(image=img)\n        full_img = full_augmented['image']\n        full_img = np.moveaxis(full_img, 2, 0)\n            \n        height = img.shape[0]\n        width = img.shape[1]\n            \n        if height > width:\n            center = int(height\/2)\n            half = int(width\/2)\n            img = img[center - half:center + half, :]\n\n        elif width > height:\n            center = int(width\/2)\n            half = int(height\/2)\n            img = img[:, center - half:center + half]\n        \n        augmented = self.transforms(image=img)\n        img = augmented['image']\n        img = np.moveaxis(img, 2, 0)\n\n        return img, full_img, pawpularity","d8ece824":"class PetfinderNet(nn.Module):\n    # for 0080, c0007, 0097, 0101, Age\n    def __init__(self, model_name):\n        super(PetfinderNet, self).__init__()\n\n        self.base_model = timm.create_model(model_name, num_classes=0, pretrained=False, in_chans=3)\n        num_features = self.base_model.num_features\n\n        self.cls = nn.Sequential(\n            nn.Linear(num_features, int(num_features \/ 2)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features \/ 2), int(num_features \/ 4)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features \/ 4), Config.N_LABEL)\n        )\n\n    def forward(self, x):\n        x = self.base_model(x)\n        x = self.cls(x)\n        return x","5578acb8":"class PetfinderDoubleBaseNet(nn.Module):\n    # for 0030, 0095\n    def __init__(self):\n        super(PetfinderDoubleBaseNet, self).__init__()\n\n        self.base_model1 = timm.create_model('tf_efficientnet_b1_ns', num_classes=0, pretrained=False, in_chans=3)\n        self.base_model2 = timm.create_model('tf_efficientnet_b1_ns', num_classes=0, pretrained=False, in_chans=3)\n        num_features = self.base_model1.num_features\n\n        self.cls = nn.Sequential(\n            nn.Linear(num_features * 2, int(num_features)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features), int(num_features \/ 2)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features \/ 2), Config.N_LABEL)\n        )\n\n    def forward(self, x1, x2):\n        x1 = self.base_model1(x1)\n        x2 = self.base_model1(x2)\n        \n        x = torch.cat([x1, x2], dim=1)\n        x = self.cls(x)\n        return x","659460e1":"class PetfinderRealDoubleBaseNet(nn.Module):\n    # 0149\n    def __init__(self):\n        super(PetfinderRealDoubleBaseNet, self).__init__()\n\n        self.base_model1 = timm.create_model('tf_efficientnet_b0_ns', num_classes=0, pretrained=False, in_chans=3)\n        self.base_model2 = timm.create_model('tf_efficientnet_b0_ns', num_classes=0, pretrained=False, in_chans=3)\n        num_features = self.base_model1.num_features\n\n        self.cls = nn.Sequential(\n            nn.Linear(num_features * 2, int(num_features)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features), int(num_features \/ 2)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features \/ 2), Config.N_LABEL)\n        )\n\n    def forward(self, x1, x2):\n        x1 = self.base_model1(x1)\n        x2 = self.base_model2(x2)\n        \n        x = torch.cat([x1, x2], dim=1)\n        x = self.cls(x)\n        return x","abd41eee":"class PetfinderMSENet(nn.Module):\n    # for 0087\n    def __init__(self, fold):\n        super(PetfinderMSENet, self).__init__()\n        \n        model = PetfinderNet('swin_large_patch4_window12_384')\n        \n        self.base_model = model.base_model\n        num_features = self.base_model.num_features\n\n        self.cls = nn.Sequential(\n            nn.Linear(num_features, int(num_features \/ 2)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features \/ 2), int(num_features \/ 4)),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(int(num_features \/ 4), 1)\n        )\n\n    def forward(self, x):\n        x = self.base_model(x)\n        x = self.cls(x)\n        return x","06b91b5e":"CP_DIR = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0080'.lower().replace('-', '').replace('_', '-'))","605238b3":"y_preds_0080 = np.zeros(len(test_df))\n\ntest_dataset = PetfinderDataSet_base(test_df, test_img_map, get_test_augmentation(IM_SZ=384), data_type='test')\ntestloader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, num_workers=4)\n\nfor fold in range(10):\n\n    net = PetfinderNet('swin_large_patch4_window12_384')\n    net.to(device)\n    print(CP_DIR \/ f'checkpoint_DESKTOP-M3SEAIN_exp0080_{fold}.pt')\n    net.load_state_dict(torch.load(CP_DIR \/ f'checkpoint_DESKTOP-M3SEAIN_exp0080_{fold}.pt'))\n\n    fold_preds = []\n    for i, (inputs, pawpularities) in tqdm(enumerate(testloader), total=len(testloader)):\n        net.eval()\n\n        with torch.no_grad():\n            \n            inputs, pawpularities = inputs.to(device).float(), pawpularities.to(device).reshape(-1, 1).float()\n            outputs = net(inputs)\n            outputs_np = outputs.sigmoid().to('cpu').detach().numpy().copy().reshape(-1) * 100\n            \n            print(outputs_np)\n            \n            fold_preds.append(outputs_np)\n    \n    y_preds_0080 += np.hstack(fold_preds).reshape(-1) \/ 10","178f61b4":"CP_DIR = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0158'.lower().replace('-', '').replace('_', '-'))","45c9cca9":"y_preds_0158 = np.zeros(len(test_df))\n\ntest_dataset = PetfinderDataSet_base(test_df, test_img_map, get_test_augmentation(IM_SZ=384), data_type='test')\ntestloader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, num_workers=4)\n\nfor fold in range(8):\n\n    net = PetfinderNet('swin_large_patch4_window12_384')\n    net.to(device)\n    print(CP_DIR \/ f'checkpoint_DESKTOP-M3SEAIN_exp0158_{fold}.pt')\n    net.load_state_dict(torch.load(CP_DIR \/ f'checkpoint_DESKTOP-M3SEAIN_exp0158_{fold}.pt'))\n\n    fold_preds = []\n    for i, (inputs, pawpularities) in tqdm(enumerate(testloader), total=len(testloader)):\n        net.eval()\n\n        with torch.no_grad():\n            \n            inputs, pawpularities = inputs.to(device).float(), pawpularities.to(device).reshape(-1, 1).float()\n            outputs = net(inputs)\n            outputs_np = outputs.sigmoid().to('cpu').detach().numpy().copy().reshape(-1) * 100\n            \n            print(outputs_np)\n            \n            fold_preds.append(outputs_np)\n    \n    y_preds_0158 += np.hstack(fold_preds).reshape(-1) \/ 8","5c57c910":"CP_DIR1 = Path('..\/input') \/ Path('colab3_0007'.lower().replace('-', '').replace('_', '-'))\nCP_DIR2 = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0030'.lower().replace('-', '').replace('_', '-'))","7ed50168":"y_preds_c0007 = np.zeros(len(test_df))\n\ntest_dataset1 = PetfinderDataSet_base(test_df, test_img_map, get_test_augmentation(IM_SZ=384), data_type='test')\ntestloader1 = DataLoader(test_dataset1, batch_size=Config.BATCH_SIZE, num_workers=4)\n\nfor fold in range(4):\n\n    net1 = PetfinderNet('vit_large_r50_s32_384')\n    net1.to(device)\n    net1.load_state_dict(torch.load(CP_DIR1 \/ f'checkpoint_colab3_pet_exp0007_{fold}.pt'))\n\n    fold_preds1 = []\n    for i, (inputs1, _) in tqdm(enumerate(testloader1), total=len(testloader1)):\n        net1.eval()\n\n        with torch.no_grad():\n            \n            inputs1 = inputs1.to(device).float()\n            \n            outputs1 = net1(inputs1)\n            outputs_np1 = outputs1.sigmoid().to('cpu').detach().numpy().copy().reshape(-1) * 100\n\n            print(outputs_np1)\n            \n            fold_preds1.append(outputs_np1)\n    \n    y_preds_c0007 += np.hstack(fold_preds1).reshape(-1) \/ 4","19578086":"CP_DIR1 = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0087'.lower().replace('-', '').replace('_', '-'))\n#CP_DIR2 = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0095'.lower().replace('-', '').replace('_', '-'))\nCP_DIR3 = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0097'.lower().replace('-', '').replace('_', '-'))\nCP_DIR4 = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0101'.lower().replace('-', '').replace('_', '-'))\nCP_DIR5 = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0126'.lower().replace('-', '').replace('_', '-'))\nCP_DIR6 = Path('..\/input') \/ Path('DESKTOP-M3SEAIN_exp0149'.lower().replace('-', '').replace('_', '-'))","0ca9dfa3":"y_preds_0087 = np.zeros(len(test_df))\n#y_preds_0095 = np.zeros(len(test_df))\ny_preds_0097 = np.zeros(len(test_df))\ny_preds_0101 = np.zeros(len(test_df))\ny_preds_0126 = np.zeros(len(test_df))\ny_preds_0149 = np.zeros(len(test_df))\n\ntest_dataset_384 = PetfinderDataSet_base(test_df, test_img_map, get_test_augmentation(IM_SZ=384), data_type='test')\ntestloader_384 = DataLoader(test_dataset_384, batch_size=Config.BATCH_SIZE\/\/2, num_workers=4)\n\ntest_dataset_fll_256 = PetfinderDataSet_fllsize(test_df, test_img_map, get_test_augmentation(IM_SZ=256), data_type='test', fllsize_transforms=get_fllsize_augmentation(IM_SZ=256))\ntestloader_fll_256 = DataLoader(test_dataset_fll_256, batch_size=Config.BATCH_SIZE\/\/2, num_workers=4)\n\ntest_dataset_224 = PetfinderDataSet_base(test_df, test_img_map, get_test_augmentation(IM_SZ=224), data_type='test')\ntestloader_224 = DataLoader(test_dataset_224, batch_size=Config.BATCH_SIZE\/\/2, num_workers=4)\n\ntest_dataset_fll_300 = PetfinderDataSet_fllsize(test_df, test_img_map, get_test_augmentation(IM_SZ=300), data_type='test', fllsize_transforms=get_fllsize_augmentation(IM_SZ=300))\ntestloader_fll_300 = DataLoader(test_dataset_fll_300, batch_size=Config.BATCH_SIZE\/\/2, num_workers=4)\n\nfor fold in range(5):\n\n    #net1 = PetfinderMSENet(fold)\n    #net1.to(device)\n    #net1.load_state_dict(torch.load(CP_DIR1 \/ f'checkpoint_DESKTOP-M3SEAIN_exp0087_{fold}.pt'))\n\n    #net2 = PetfinderDoubleBaseNet()\n    #net2.to(device)\n    #net2.load_state_dict(torch.load(CP_DIR2 \/ f'checkpoint_DESKTOP-M3SEAIN_exp0095_{fold}.pt'))\n  \n    #net3 = PetfinderNet('resnet18d')\n    #net3.to(device)\n    #net3.load_state_dict(torch.load(CP_DIR3 \/ f'checkpoint_DESKTOP-M3SEAIN_exp0097_{fold}.pt'))\n\n    net4 = PetfinderNet('swin_large_patch4_window12_384')\n    net4.to(device)\n    net4.load_state_dict(torch.load(CP_DIR4 \/ f'checkpoint_DESKTOP-M3SEAIN_exp0101_{fold}.pt'))\n    \n    net5 = PetfinderNet('densenet121')\n    net5.to(device)\n    net5.load_state_dict(torch.load(CP_DIR5 \/ f'checkpoint_DESKTOP-M3SEAIN_exp0126_{fold}.pt'))\n    \n    net6 = PetfinderRealDoubleBaseNet()\n    net6.to(device)\n    net6.load_state_dict(torch.load(CP_DIR6 \/ f'checkpoint_DESKTOP-M3SEAIN_exp0149_{fold}.pt'))\n    \n    fold_preds1 = []\n    #fold_preds2 = []\n    fold_preds3 = []\n    fold_preds4 = []\n    fold_preds5 = []\n    fold_preds6 = []\n    #for i, ((inputs384, _), (inputs256, full_img256, _), (inputs224, _), (inputs300, full_img300, _)) in tqdm(enumerate(zip(testloader_384, testloader_fll_256, testloader_224, testloader_fll_300)), total=len(testloader_384)):\n    for i, ((inputs384, _), (inputs224, _), (inputs300, full_img300, _)) in tqdm(enumerate(zip(testloader_384, testloader_224, testloader_fll_300)), total=len(testloader_384)):\n        #net1.eval()\n        #net2.eval()\n        #net3.eval()\n        net4.eval()\n        net5.eval()\n        net6.eval()\n\n        with torch.no_grad():\n            \n            inputs384 = inputs384.to(device).float()\n            \n            #outputs1 = net1(inputs384)\n            #outputs_np1 = outputs1.to('cpu').detach().numpy().copy().reshape(-1)\n            \n            #inputs256, full_img256 = inputs256.to(device).float(), full_img256.to(device).float()\n            #outputs2 = net2(inputs256, full_img256)\n            #outputs_np2 = outputs2.to('cpu').detach().numpy().copy().reshape(-1)\n         \n            inputs224 = inputs224.to(device).float()\n            #outputs3 = net3(inputs224)\n            #outputs_np3 = outputs3.sigmoid().to('cpu').detach().numpy().copy().reshape(-1) * 100    \n    \n            outputs4 = net4(inputs384)\n            outputs_np4 = outputs4.sigmoid().to('cpu').detach().numpy().copy().reshape(-1) * 100\n    \n            outputs5 = net5(inputs224)\n            outputs_np5 = outputs5.sigmoid().to('cpu').detach().numpy().copy().reshape(-1) * 100\n    \n            inputs300, full_img300 = inputs300.to(device).float(), full_img300.to(device).float()\n            outputs6 = net6(inputs300, full_img300)\n            outputs_np6 = outputs6.to('cpu').detach().numpy().copy().reshape(-1)\n    \n            #fold_preds1.append(outputs_np1)\n            #fold_preds2.append(outputs_np2)\n            #fold_preds3.append(outputs_np3)\n            fold_preds4.append(outputs_np4)\n            fold_preds5.append(outputs_np5)\n            fold_preds6.append(outputs_np6)\n    \n    #y_preds_0087 += np.hstack(fold_preds1).reshape(-1) \/ 5\n    #y_preds_0095 += np.hstack(fold_preds2).reshape(-1) \/ 5\n    #y_preds_0097 += np.hstack(fold_preds3).reshape(-1) \/ 5\n    y_preds_0101 += np.hstack(fold_preds4).reshape(-1) \/ 5\n    y_preds_0126 += np.hstack(fold_preds5).reshape(-1) \/ 5\n    y_preds_0149 += np.hstack(fold_preds6).reshape(-1) \/ 5","58c1cd56":"base_model = timm.create_model('efficientnet_b0', num_classes=0, pretrained=False, in_chans=3)\nbase_model.load_state_dict(torch.load('..\/input\/efficientnet-b0\/efficientnet_b0.pt'))\nbase_model = base_model.to(device)","bba54405":"org_test_img_paths = glob.glob(str(DATA_DIR \/  'test\/*'))\n#pre_train_img_paths = glob.glob('..\/input\/prepetfinder\/pre_petfinder\/train_images\/*')\npre_img_paths = glob.glob('..\/input\/prepetfinder\/pre_petfinder\/train_images\/*') + glob.glob('..\/input\/prepetfinder\/pre_petfinder_test\/test_images\/*')","d79ff315":"def get_augmentation():\n    train_transform = [\n        albu.Resize(height=224, width=224)\n    ]\n    return albu.Compose(train_transform)\n\nclass DataSet(Dataset):\n    def __init__(self, image_paths):\n        self.image_paths = image_paths\n        self.transforms = get_augmentation()\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = self.transforms(image=img)['image']\n        img = np.moveaxis(img, 2, 0)\n\n        return img\n","4279e474":"#pre_train_vectors = np.load('..\/input\/pet2-sim-tensor-calc-challenge\/pre_train_vectors.npy')\npre_vectors = np.load('..\/input\/pre-pet-vectors\/pre_pet_vectors.npy')","c8712855":"dataset = DataSet(org_test_img_paths)\nloader = DataLoader(dataset, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n\noutputs = []\nfor img in tqdm(loader, total=len(loader)):\n    img = img.to(device).float()\n    output = base_model(img)\n    outputs_np = output.to('cpu').detach().numpy().copy()\n    outputs.append(outputs_np)\n\norg_test_vectors = np.vstack(outputs)","fa6ff13a":"def cos_sim_matrix(matrix1, matrix2):\n    \"\"\"\n    item-feature \u884c\u5217\u304c\u4e0e\u3048\u3089\u308c\u305f\u969b\u306b\n    item \u9593\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u884c\u5217\u3092\u6c42\u3081\u308b\u95a2\u6570\n    \"\"\"\n    # d = matrix @ matrix.T  # item-vector \u540c\u58eb\u306e\u5185\u7a4d\u3092\u8981\u7d20\u3068\u3059\u308b\u884c\u5217\n    # matrix1 : M * V, matrix2 : V * N\n    d = matrix1 @ matrix2\n\n    # \u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u306e\u5206\u6bcd\u306b\u5165\u308c\u308b\u305f\u3081\u306e\u3001\u5404 item-vector \u306e\u5927\u304d\u3055\u306e\u5e73\u65b9\u6839\n    # V \u306e\u6b21\u5143\u3067 sum : (M * V)\n    norm1 = (matrix1 * matrix1).sum(axis=1, keepdims=True) ** .5\n    # V \u306e\u6b21\u5143\u3067 sum : (V * N)\n    norm2 = (matrix2 * matrix2).sum(axis=0, keepdims=True) ** .5\n\n    # \u305d\u308c\u305e\u308c\u306e item \u306e\u5927\u304d\u3055\u306e\u5e73\u65b9\u6839\u3067\u5272\u3063\u3066\u3044\u308b\uff08\u306a\u3093\u3060\u304b\u30b9\u30de\u30fc\u30c8\uff01\uff09\n    return d \/ norm1 \/ norm2","728cf04f":"%%time\n\n# \u884c\u5217\u6f14\u7b97\u3067 test_img_num * pre_train_img_num \u306e\u985e\u4f3c\u5ea6\u884c\u5217\u3092\u7372\u5f97\u3059\u308b\nd = cos_sim_matrix(org_test_vectors, pre_vectors.T)\nprint(d.shape)\nprint(d)\n\n# argmax \u3067 test_img \u3068\u6700\u3082\u985e\u4f3c\u3057\u3066\u3044\u308b pre_train_img \u304c,\n# max \u3067\u305d\u306e\u985e\u4f3c\u5ea6\u3092\u7372\u5f97\u3067\u304d\u308b\nprint(np.argmax(d, 1), np.max(d, 1))\n\n# \u2191 \u3053\u308c\u3089\u3092\u5bfe\u5fdc\u3055\u305b\u305f\u8f9e\u66f8\u306b\u3057\u3066\u5b8c\u6210\nimg_pair = [\n    (org_test, pre_img_paths[sim_argmax], sim_max) for org_test, sim_argmax, sim_max in zip(org_test_img_paths, np.argmax(d, 1), np.max(d, 1))\n]\n\nimg_pair","df4144cc":"import albumentations as A\nimport cv2\nimport gc\nimport os\nimport math\nimport random\nimport time\nimport warnings\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch.optim import Adam, SGD\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom pathlib import Path\nfrom typing import List\nfrom PIL import Image\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfrom tqdm import tqdm","1fb5e8d3":"class CFG:\n    ######################\n    # Globals #\n    ######################\n    seed = 71\n    epochs = 20\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    LR = 1e-4\n    train_bs = 16\n    valid_bs = 32\n    train_root = '..\/input\/petfinder-pawpularity-score\/train\/'\n    test_root = '..\/input\/petfinder-pawpularity-score\/test\/'\n    in_chans = 3\n    ID_COL = 'Id'\n    TARGET_COL = 'Pawpularity'\n    TARGET_DIM = 1\n    FEATURE_COLS = [\n        'Subject Focus', 'Eyes', 'Face', 'Near', 'Action',\n        'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', \n        'Info', 'Blur',\n    ]","35c470ef":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","8be1981a":"class Pet2Dataset:\n    def __init__(self, X, y=None, Meta_features=None, transforms=None):\n        self.X = X\n        self.y = y\n        self.Meta_features = Meta_features\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, item):\n        path = CFG.test_root + self.X[item] + '.jpg'\n        features = cv2.imread(path)\n        features = cv2.cvtColor(features, cv2.COLOR_BGR2RGB)\n        features = self.transforms['valid'](image=features)['image']\n        features = np.transpose(features, (2, 0, 1)).astype(np.float32)\n\n        return {\n            'x': torch.tensor(features, dtype=torch.float32),\n            'meta': torch.tensor(self.Meta_features[item], dtype=torch.float32),\n        }","3ac4806e":"class Pet2Model(nn.Module):\n    def __init__(self, model_name):\n        super(Pet2Model, self).__init__()    \n        \n        # Model Encoder\n        self.model = timm.create_model(model_name, pretrained=False, num_classes=0, in_chans=CFG.in_chans)\n        self.model.head = nn.Linear(self.model.num_features, 128)\n        self.dense = nn.Linear(128, CFG.TARGET_DIM)\n\n    def forward(self, features):\n        x = self.model(features)\n        output = self.dense(x)\n        return output.squeeze(-1)","d374d709":"class Pet2CNNModel(nn.Module):\n    def __init__(self, model_name):\n        super(Pet2CNNModel, self).__init__()    \n        \n        # Model Encoder\n        self.model = timm.create_model(model_name, pretrained=False, num_classes=0, in_chans=CFG.in_chans)\n        self.model.fc = nn.Linear(self.model.num_features, 128)\n        self.dense = nn.Linear(128, CFG.TARGET_DIM)\n\n    def forward(self, features):\n        x = self.model(features)\n        output = self.dense(x)\n        return output.squeeze(-1)","d8225521":"class Pet2VitCNNModel(nn.Module):\n    def __init__(self, model_name):\n        super(Pet2VitCNNModel, self).__init__()    \n        \n        # Model Encoder\n        self.model = timm.create_model(model_name, pretrained=False, num_classes=0, in_chans=CFG.in_chans)\n        self.model.head = nn.Linear(self.model.num_features, 128)\n        self.dense = nn.Linear(128, CFG.TARGET_DIM)\n\n    def forward(self, features):\n        x = self.model(features)\n        output = self.dense(x)\n        return output.squeeze(-1)","ec73d784":"def make_preds(model_dict1, model_dict2, model_dict3, model_dict4, model_dict5):    \n    df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\n    y_pred1 = []\n    y_pred2 = []\n    y_pred3 = []\n    y_pred4 = []\n    y_pred5 = []\n    for fold in CFG.folds:\n        model1 = Pet2Model(model_dict1['MODEL_NAME'])\n        model1.to(device)\n        model1.load_state_dict(torch.load(f'..\/input\/kaerururu-petfinder2-{model_dict1[\"EXP_ID\"]}\/fold-{fold}.bin'))\n        model1.eval()\n        \n        model2 = Pet2Model(model_dict2['MODEL_NAME'])\n        model2.to(device)\n        model2.load_state_dict(torch.load(f'..\/input\/kaerururu-petfinder2-{model_dict2[\"EXP_ID\"]}\/fold-{fold}.bin'))\n        model2.eval()\n        \n        model3 = Pet2Model(model_dict3['MODEL_NAME'])\n        model3.to(device)\n        model3.load_state_dict(torch.load(f'..\/input\/kaerururu-petfinder2-{model_dict3[\"EXP_ID\"]}\/fold-{fold}.bin'))\n        model3.eval()\n        \n        model4 = Pet2CNNModel(model_dict4['MODEL_NAME'])\n        model4.to(device)\n        model4.load_state_dict(torch.load(f'..\/input\/kaerururu-petfinder2-{model_dict4[\"EXP_ID\"]}\/fold-{fold}.bin'))\n        model4.eval()\n        \n        model5 = Pet2VitCNNModel(model_dict5['MODEL_NAME'])\n        model5.to(device)\n        model5.load_state_dict(torch.load(f'..\/input\/kaerururu-petfinder2-{model_dict5[\"EXP_ID\"]}\/fold-{fold}.bin'))\n        model5.eval()\n        \n        dataset1 = Pet2Dataset(X=df[CFG.ID_COL].values, y=None, Meta_features=df[CFG.FEATURE_COLS].values, transforms=model_dict1['TRANSFORM'])\n        data_loader1 = torch.utils.data.DataLoader(\n            dataset1, batch_size=CFG.valid_bs\/\/4, num_workers=0, pin_memory=True, shuffle=False\n        )\n        \n        dataset2 = Pet2Dataset(X=df[CFG.ID_COL].values, y=None, Meta_features=df[CFG.FEATURE_COLS].values, transforms=model_dict2['TRANSFORM'])\n        data_loader2 = torch.utils.data.DataLoader(\n            dataset2, batch_size=CFG.valid_bs\/\/4, num_workers=0, pin_memory=True, shuffle=False\n        )\n        \n        del dataset1, dataset2; gc.collect()\n\n        final_output1 = []\n        final_output2 = []\n        final_output3 = []\n        final_output4 = []\n        final_output5 = []\n        for b_idx, (data1, data2) in tqdm(enumerate(zip(data_loader1, data_loader2))):\n            with torch.no_grad():\n                inputs384 = data1['x'].to(device)\n                inputs224 = data2['x'].to(device)\n                                          \n                output1 = model1(inputs384)\n                output2 = model2(inputs224)\n                output3 = model3(inputs224)\n                output4 = model4(inputs384)\n                output5 = model5(inputs384)\n                \n                output1 = torch.sigmoid(output1) * 100.\n                output1 = output1.detach().cpu().numpy().tolist()\n                final_output1.extend(output1)\n                \n                output2 = torch.sigmoid(output2) * 100.\n                output2 = output2.detach().cpu().numpy().tolist()\n                final_output2.extend(output2)\n                \n                output3 = torch.sigmoid(output3) * 100.\n                output3 = output3.detach().cpu().numpy().tolist()\n                final_output3.extend(output3)\n                \n                output4 = torch.sigmoid(output4) * 100.\n                output4 = output4.detach().cpu().numpy().tolist()\n                final_output4.extend(output4)\n                \n                output5 = torch.sigmoid(output5) * 100.\n                output5 = output5.detach().cpu().numpy().tolist()\n                final_output5.extend(output5)\n                \n        y_pred1.append(np.array(final_output1))\n        y_pred2.append(np.array(final_output2))\n        y_pred3.append(np.array(final_output3))\n        y_pred4.append(np.array(final_output4))\n        y_pred5.append(np.array(final_output5))\n        torch.cuda.empty_cache()\n        \n    y_pred1 = np.mean(y_pred1, 0)\n    y_pred2 = np.mean(y_pred2, 0)\n    y_pred3 = np.mean(y_pred3, 0)\n    y_pred4 = np.mean(y_pred4, 0)\n    y_pred5 = np.mean(y_pred5, 0)\n    \n    del model1, model2, model3, model4, model5; gc.collect()\n    del final_output1, final_output2, final_output3, final_output4, final_output5; gc.collect()\n    return y_pred1, y_pred2, y_pred3, y_pred4, y_pred5","fb4e2758":"# environment\nset_seed(CFG.seed)\n\nmodel_dict1 = {\n    'EXP_ID' : '042', \n    'MODEL_NAME' : 'swin_large_patch4_window12_384',\n    'TRANSFORM' : {\n        'valid' : A.Compose([\n            A.Resize(384, 384, p=1),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0,),\n        ], p=1.0),    \n    }\n}\n\nmodel_dict2 = {\n    'EXP_ID' : '044', \n    'MODEL_NAME' : 'swin_large_patch4_window7_224',\n    'TRANSFORM' : {\n        'valid' : A.Compose([\n            A.Resize(224, 224, p=1),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0,),\n        ], p=1.0),    \n    }\n}\n\nmodel_dict3 = {\n    'EXP_ID' : '055', \n    'MODEL_NAME' : 'swin_large_patch4_window7_224',\n    'TRANSFORM' : {\n        'valid' : A.Compose([\n            A.Resize(224, 224, p=1),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0,),\n        ], p=1.0),    \n    }\n}\n\nmodel_dict4 = {\n    'EXP_ID' : '084', \n    'MODEL_NAME' : 'resnext101_32x8d',\n    'TRANSFORM' : {\n        'valid' : A.Compose([\n            A.Resize(384, 384, p=1),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0,),\n        ], p=1.0),    \n    }\n}\n\nmodel_dict5 = {\n    'EXP_ID' : '062', \n    'MODEL_NAME' : 'vit_base_r50_s16_384',\n    'TRANSFORM' : {\n        'valid' : A.Compose([\n            A.Resize(384, 384, p=1),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0,),\n        ], p=1.0),    \n    }\n}\n\nsub = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\npreds_042, preds_044, preds_055, preds_084, preds_062 = make_preds(model_dict1, model_dict2, model_dict3, model_dict4, model_dict5)","78af40cb":"pre_sim_df = pd.DataFrame(img_pair, columns=['org_path', 'pre_path', 'sim'])\npre_sim_df","40a420e9":"#pre_train_df = pd.read_csv('..\/input\/prepetfinder\/pre_petfinder\/train\/train.csv')\n# pre_df = pd.concat([pd.read_csv('..\/input\/prepetfinder\/pre_petfinder\/train\/train.csv'), pd.read_csv('..\/input\/prepetfinder\/pre_petfinder_test\/test\/test.csv')])\n\npre_df = pd.read_csv('..\/input\/pp-stacking-avg-v019-nlp-feature\/prepetfinder_add_emoji.csv')\npre_df.head(2)","8adaf978":"pre_sim_list = []\nfor idx, (org_path, pre_path, sim) in tqdm(enumerate(zip(pre_sim_df['org_path'].values, pre_sim_df['pre_path'].values, pre_sim_df['sim'].values))): \n    org_id = org_path.split('\/')[-1].split('.')[0]\n    pre_id = pre_path.split('\/')[-1].split('-')[0]\n    \n    adoptionSpeed = pre_df[pre_df['PetID'] == pre_id].iloc[0]['AdoptionSpeed']\n    \n    pre_sim_list.append(\n        {\n            'org_path':org_path,\n            'pre_path':pre_path,\n            'sim':sim,\n            'org_id':org_id,\n            'pre_id':pre_id,\n            'adoptionSpeed':adoptionSpeed\n            \n        }\n    )\npwup_speed_df = pd.DataFrame(pre_sim_list)\npwup_speed_df","e25a18f6":"test_df = test_df.merge(pwup_speed_df[['org_id', 'adoptionSpeed', 'sim', 'pre_id']], how='left', left_on='Id', right_on='org_id').reset_index()\ndisplay(test_df)\n# test_df = test_df.merge(pd.concat([pd.read_csv('..\/input\/prepetfinder\/pre_petfinder\/train\/train.csv'), pd.read_csv('..\/input\/prepetfinder\/pre_petfinder_test\/test\/test.csv')]), how='left', left_on='pre_id', right_on='PetID')\ntest_df = test_df.merge(pd.read_csv('..\/input\/pp-stacking-avg-v019-nlp-feature\/prepetfinder_add_emoji.csv'), how='left', left_on='pre_id', right_on='PetID')\n\ntest_df['Breed_205'] = (test_df['Breed1'] == 205).astype(int)\ntest_df['Breed_307'] = (test_df['Breed1'] == 307).astype(int)\n# test_df['Breed_266'] = (test_df['Breed1'] == 266).astype(int)\ntest_df['Breed_265'] = (test_df['Breed1'] == 265).astype(int)\n\n# test_df['Breed_299'] = (test_df['Breed1'] == 299).astype(int)\ntest_df['Breed_264'] = (test_df['Breed1'] == 264).astype(int)\n# test_df['Breed_292'] = (test_df['Breed1'] == 292).astype(int)\n\ntest_df['Breed_285'] = (test_df['Breed1'] == 285).astype(int)\ntest_df['Breed_141'] = (test_df['Breed1'] == 141).astype(int)\ntest_df['Breed_218'] = (test_df['Breed1'] == 218).astype(int)\ntest_df['Breed_205'] = (test_df['Breed1'] == 205).astype(int)\n\n# test_df['Breed_179'] = (test_df['Breed1'] == 179).astype(int)\ntest_df['Breed_103'] = (test_df['Breed1'] == 103).astype(int)\n\ntest_df['Breed_266_265_299'] = test_df['Breed1'].map(lambda x: 1 if x in [266, 265, 299] else 0)\ntest_df['Breed_292_179'] = test_df['Breed1'].map(lambda x: 1 if x in [292, 179] else 0)\n\ntest_df['RescuerID_1'] = (test_df['RescuerID'] == 'fa90fa5b1ee11c86938398b60abc32cb').astype(int)\ntest_df['RescuerID_2'] = (test_df['RescuerID'] == 'aa66486163b6cbc25ea62a34b11c9b91').astype(int)\ntest_df['RescuerID_3'] = (test_df['RescuerID'] == 'c00756f2bdd8fa88fc9f07a8309f7d5d').astype(int)\ntest_df['RescuerID_4'] = (test_df['RescuerID'] == 'b53c34474d9e24574bcec6a3d3306a0d').astype(int)\ntest_df['RescuerID_5'] = (test_df['RescuerID'] == 'b770bac0ca797cf1433c48a35d30c4cb').astype(int)\n\ntest_df['Name_word_len'] = test_df['Name'].fillna('').apply(lambda x : len(x.split(' ')))\ntest_df['Description_len'] = test_df['Description'].fillna('').apply(lambda x : len(x.split(' ')))\n","b93e9997":"# st_models = unpickle('..\/input\/fork-of-pp-stacking-avg-v016-reduce-pp-features2\/pp_models_stacking.pkl')\nst_models = unpickle('..\/input\/defense-pp-stacking-avg-v034\/pp_models_stacking.pkl')","e21ad293":"test_df['y_preds_0126'] = y_preds_0126\ntest_df['y_preds_0101'] = y_preds_0101\n#test_df['y_preds_0095'] = y_preds_0095\ntest_df['y_preds_0080'] = y_preds_0080\ntest_df['y_preds_0158'] = y_preds_0158\ntest_df['y_preds_c0007'] = y_preds_c0007\ntest_df['y_johannyjm_1790651'] = y_johannyjm_1790651\ntest_df['preds_042'] = preds_042\ntest_df['preds_044'] = preds_044\ntest_df['preds_055'] = preds_055\n# test_df['preds_060'] = preds_060\ntest_df['preds_084'] = preds_084\n# test_df['preds_085'] = preds_085\ntest_df['preds_062'] = preds_062\n# test_df['preds_074'] = preds_074\ntest_df['y_preds_0149'] = y_preds_0149\n\ntest_df['fast_ai_004_preds'] = fast_ai_004_preds","e7f6ae8f":"#test_df['sim'] = 1\n#test_df['adoptionSpeed'] = 0","c8852ee7":"pp_stacking_dfs = []\n\nfor c, model_as in enumerate(st_models):\n    \n    pred_stack = model_as[0].predict(test_df[['y_preds_0158', 'y_preds_0149', 'y_preds_0080', 'y_preds_c0007', 'y_johannyjm_1790651', \n                                              'preds_042', 'preds_044', 'preds_055', 'fast_ai_004_preds', \n                                              'preds_084', 'preds_062', 'y_preds_0126', \n                                              # 'preds_074'\n                                             ]])\n    \n    test_df['Pawpularity'] = pred_stack\n\n    under_threshold_df = test_df[test_df['sim'] < 0.65].copy()\n    over_threshold_df = test_df[test_df['sim'] >= 0.65].copy()\n    \n    if len(over_threshold_df) > 0:\n        over_threshold_df['Pawpularity'] = model_as[2].predict(over_threshold_df[['Pawpularity', 'Breed_307', 'Breed_266_265_299', 'Breed_292_179', 'Breed_285', 'Breed_103', 'Age', 'RescuerID_3', 'RescuerID_5', 'Description_len', 'PhotoAmt', 'Name_word_len', 'is_emoji']])\n        print('over_threshold_df')\n        \n    over_threshold_df_as_over0 = over_threshold_df[over_threshold_df['adoptionSpeed'] >= 0].copy()\n    over_threshold_df_other = over_threshold_df[~(over_threshold_df['adoptionSpeed'].isin([i for i in range(5)]))].copy()\n        \n    if len(over_threshold_df_as_over0) > 0:\n        over_threshold_df_as_over0['Pawpularity'] = model_as[1].predict(over_threshold_df_as_over0[['Pawpularity', 'Breed_205', 'Breed_307', 'Breed_266_265_299', 'Age', 'adoptionSpeed']])\n        print('over_threshold_df_as_over0')\n        \n    test_adjust_df = pd.concat([under_threshold_df, over_threshold_df_as_over0, over_threshold_df_other]).sort_values('index')\n\n    pp_stacking_dfs.append(test_adjust_df.copy())","35067a7b":"pp_stacking_df = pd.concat(pp_stacking_dfs)\npp_stacking_df.head(2)","872c2a5d":"# avg_models = unpickle('..\/input\/fork-of-pp-stacking-avg-v016-reduce-pp-features2\/pp_models_avg.pkl')\navg_models = unpickle('..\/input\/defense-pp-stacking-avg-v034\/pp_models_avg.pkl')","5d65fec5":"pp_avg_dfs = []\n\nfor c, model_as in enumerate(avg_models):\n    \n    pred_avg =  test_df[['y_preds_0158', 'y_preds_0149', 'y_preds_0080', 'y_preds_c0007', 'y_johannyjm_1790651', \n                         'preds_042', \n                         'preds_044', # 'preds_085',\n                         'preds_055', 'fast_ai_004_preds', \n                         'preds_084', 'preds_062', 'y_preds_0126', \n                         # 'preds_074'\n                        ]].mean(axis=1)\n    \n    test_df['Pawpularity'] = pred_avg\n\n    under_threshold_df = test_df[test_df['sim'] < 0.65].copy()\n    over_threshold_df = test_df[test_df['sim'] >= 0.65].copy()\n    \n    if len(over_threshold_df) > 0:\n        over_threshold_df['Pawpularity'] = model_as[2].predict(over_threshold_df[['Pawpularity', 'Breed_307', 'Breed_266_265_299', 'Breed_292_179', 'Breed_285', 'Breed_103', 'Age', 'RescuerID_3', 'RescuerID_5', 'Description_len', 'PhotoAmt', 'Name_word_len', 'is_emoji']])\n        print('over_threshold_df')\n    \n    over_threshold_df_as_over0 = over_threshold_df[over_threshold_df['adoptionSpeed'] >= 0].copy()\n    over_threshold_df_other = over_threshold_df[~(over_threshold_df['adoptionSpeed'].isin([i for i in range(5)]))].copy()\n    \n    if len(over_threshold_df_as_over0) > 0:\n        over_threshold_df_as_over0['Pawpularity'] = model_as[1].predict(over_threshold_df_as_over0[['Pawpularity', 'Breed_205', 'Breed_307', 'Breed_266_265_299', 'Age', 'adoptionSpeed']])\n        print('over_threshold_df_as_over0')\n        \n    test_adjust_df = pd.concat([under_threshold_df, over_threshold_df_as_over0, over_threshold_df_other]).sort_values('index')\n\n    pp_avg_dfs.append(test_adjust_df.copy())","497a0d9a":"pp_avg_df = pd.concat(pp_avg_dfs)\npp_avg_df.head(2)","1e5503db":"pp_avg_mean_df = pp_avg_df.groupby('Id')['Pawpularity'].mean().to_frame()","fc8567b6":"pp_stacking_mean_df = pp_stacking_df.groupby('Id')['Pawpularity'].mean().to_frame()","2071df13":"pp_df = pd.concat([pp_avg_mean_df, pp_stacking_mean_df])\npp_df","bc4d6a77":"pp_df = pp_df.groupby('Id')['Pawpularity'].mean().to_frame().reset_index()\npp_df","1e2408f4":"# y_preds_0101","53f0a749":"y_preds_0158","4f27e95e":"y_preds_c0007","e8030729":"# y_preds_0030","82a2431d":"#y_preds_0095","6c76ccdc":"y_johannyjm_1790651","dce06837":"preds_042","7f949e20":"preds_044","92211f5d":"preds_055","51d4742b":"preds_084 # preds_060","22f7f051":"# preds_085","8c818420":"fast_ai_004_preds","1d9b641a":"preds_062","414a9aac":"y_preds_0126","9eba7cf5":"# preds_074","d39790ea":"y_preds_0149","86557e1f":"y_preds_0158","ac3102c8":"y_preds_0080","9cfd1158":"submission_df = pp_df[['Id', 'Pawpularity']]\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df","83839c83":"## yuki DataAugmentations","c9995fad":"## Average","54e792e5":"## 5 fold models","2845f021":"## kaeru","b511481c":"### 42, 44","191c9e0d":"## yuki Models","1cad06f9":"## 4 fold models","932169cc":"## 0087, 0095, 0097, 0101, 0126, 0149","94c5fd68":"## Stacking","ca3c2395":"## 8fold models","9c622cff":"## \u524d\u56de\u30b3\u30f3\u30da\u3068\u540c\u3058\u753b\u50cf\u62bd\u51fa","f6f74127":"## johannyjm_1790651","a935b532":"## c0007, 0030","8389a4d0":"## fast_ai_0004\n","b5f54da7":"## 10fold models\n### 0080","bc8d74a5":"## 0158","de232212":"### \u985e\u4f3c\u5ea6\u7b97\u51fa","5cfd8bb0":"## yuki Datasets"}}