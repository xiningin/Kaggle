{"cell_type":{"bae42d89":"code","113c7375":"code","bd461888":"code","1b3c93d0":"code","114ee01c":"code","35d4611d":"code","88791e14":"code","b42f04b1":"code","53187e78":"code","f66b1d56":"code","9a51f927":"code","5cd7e075":"code","50545eab":"code","27ae5625":"code","ca5b9e42":"code","86906543":"code","0a623199":"code","6874e111":"code","44369ead":"code","d3918a7a":"code","6cf5c341":"code","e01de0d7":"code","cdef28d9":"code","89794a3f":"code","bc535084":"code","0d7611fe":"code","d31627af":"code","8f0f689d":"code","e5eddf9c":"code","fe2009fb":"code","dcf9fa29":"code","d3be0d21":"code","cb833722":"code","e0b6aaca":"code","73ea99d2":"code","4172bae3":"code","e5b5e7ca":"code","65c5031b":"code","6f3c8d80":"code","0ea1db8a":"code","9086ce3b":"code","1e4ab1d0":"code","d3ac3098":"code","6cc2dd74":"code","437edf86":"code","22a5ef8b":"code","8b1872d0":"code","d3702a3e":"code","596c797d":"code","86ece499":"code","b55980c4":"code","37fe5b39":"code","ae4521dc":"code","42f67249":"code","1cf990be":"code","599c6a0a":"code","540ed45c":"code","cb400baa":"code","4fe7700f":"code","76bc1040":"code","8d3ba8b4":"code","ce58202f":"code","66c0737f":"code","1a0c9e74":"code","3906896d":"markdown","b4a67194":"markdown","dd32fca3":"markdown","9e540872":"markdown","d5290408":"markdown","8b78dde5":"markdown","90b87cb8":"markdown","765cdea7":"markdown","65fb885f":"markdown"},"source":{"bae42d89":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","113c7375":"train = pd.read_csv('..\/input\/train.csv')\ntrain.head()","bd461888":"test = pd.read_csv('..\/input\/test.csv')\ntest.head()","1b3c93d0":"train.info()","114ee01c":"from IPython.display import IFrame\nIFrame('https:\/\/public.tableau.com\/profile\/nitin2520#!\/vizhome\/FindingOutliersHouseRegression\/Sheet1?publish=yes', width=1000, height=925)","35d4611d":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\ntrain.reset_index(drop=True, inplace=True)\ntrain.info()","88791e14":"sns.heatmap(train.isnull(),yticklabels=False, cmap = 'RdYlGn',cbar=False)","b42f04b1":"# for LotFrontage it seems good to fill null values using mean\ntrain['LotFrontage'].fillna(train['LotFrontage'].mean(), inplace = True)\ntest['LotFrontage'].fillna(test['LotFrontage'].mean(), inplace = True)","53187e78":"# those not having MasVnrType we can take them as None Type which is also the mode\n# Having None MasVnrType will also have 0 area which is also the mode so filling both of them by mode\ntrain['MasVnrType'].fillna(train['MasVnrType'].mode()[0], inplace = True)\ntrain['MasVnrArea'].fillna(train['MasVnrArea'].mode()[0], inplace = True)\ntest['MasVnrType'].fillna(test['MasVnrType'].mode()[0], inplace = True)\ntest['MasVnrArea'].fillna(test['MasVnrArea'].mode()[0], inplace = True)","f66b1d56":"print('Pearson: ')\ntrain[['BsmtFinType1', 'BsmtFinSF1']].corr(method='pearson')","9a51f927":"# Tableau visualization sheet 2  and sheet 3 needed","5cd7e075":"train['BsmtFinType1'].fillna('Unf', inplace = True)\ntrain['BsmtFinType2'].fillna('Unf', inplace = True)\ntest['BsmtFinType1'].fillna('Unf', inplace = True)\ntest['BsmtFinType2'].fillna('Unf', inplace = True)","50545eab":"# sheet 4","27ae5625":"# all those points having NA in BsmtQual lies on BsmtFullBath as 0 and in that the maximum nuber of time TA occurs so we will fill this \n# with the mode which is TA in this the same is with the test set\ntrain['BsmtQual'].fillna('TA', inplace = True)\ntest['BsmtQual'].fillna('TA', inplace = True)","ca5b9e42":"# Similarly the BsmtCond also depends according to BsmtFullBath on 0 so we will also fill null values with TA\ntrain['BsmtCond'].fillna('TA', inplace = True)\ntest['BsmtCond'].fillna('TA', inplace = True)","86906543":"# Similarly we will fill BsmtExposure with No\ntrain['BsmtExposure'].fillna('No', inplace = True)\ntest['BsmtExposure'].fillna('No', inplace = True)","0a623199":"# all those data having fireplace 0 have FireplaceQu as null so we will fill those with new value as No\ntrain['FireplaceQu'].fillna('No', inplace = True)\ntest['FireplaceQu'].fillna('No', inplace = True)","6874e111":"# We will drop these features as thse have maximum as null values\ntrain.drop(['MiscFeature', 'Fence', 'PoolQC', 'Alley'], axis = 1, inplace = True)\ntest.drop(['MiscFeature', 'Fence', 'PoolQC', 'Alley'], axis = 1, inplace = True)","44369ead":"train.info()\ntest.info()","d3918a7a":"# As can be easily seen in graph there is only one data point which has bot garagecars and garagearea value as null\n# So as there are also 76 other records in which both are 0 so will fill this record with also 0\ntest['GarageCars'].fillna( 0 , inplace = True)\ntest['GarageArea'].fillna( 0 , inplace = True)","6cf5c341":"# sheet 7\n# In Train Set all records having GarageQual as NA has GarageCars as 0 so will replace all NA with new value as NO\n# -----------\/\n# while in Test Set there are all record having GarageQual as NA has GarageCars also 0 but there is 1 record which have GarageCars as 1\n# So of that special record we will fill with 'TA' while of others with new value as 'NO'\ntrain['GarageQual'].fillna('No', inplace = True)\ntest['GarageQual'].fillna('No', inplace = True)\n","e01de0d7":"def compute_garageQual(cols):\n    cars = cols[0]\n    qual = cols[1]\n    \n    if str(qual) == 'No' :\n        if cars == 1 :\n            return 'TA'\n        else:\n            return 'No'\n    else:\n        return qual","cdef28d9":"# replacing No with TA for particular datapoint\ntest['GarageQual'] = test[['GarageCars', 'GarageQual']].apply(compute_garageQual, axis = 1)\ntest.info()","89794a3f":"#  Sheet 8\n# In GargageType where values are null all have GarageCars as 0 so will create a new value as No\ntrain['GarageType'].fillna('No', inplace = True)\ntest['GarageType'].fillna('No', inplace = True)\n# train.info()\n# test.info()","bc535084":"# Sheet 9 and Sheet 2\n# The case co GarageYrBlt is same to GarageQual \n# Similarly to that there is 1 point which has GarageCars as 1 and GarageYrBlt as null \n#  We will fill taht special value with the median as expressed in the graph\ntrain['GarageYrBlt'].fillna( 0, inplace = True)\ntest['GarageYrBlt'].fillna( 0, inplace = True)\n# train.info()","0d7611fe":"def compute_yrblt(cols):\n    cars = cols[0]\n    yrblt = cols[1]\n    \n    if yrblt == 0 :\n        if cars == 1 :\n            return 1956.5\n        else:\n            return 0\n    else:\n        return yrblt","d31627af":"# Replacing in test set with median for special case\ntest['GarageYrBlt'] = test[['GarageCars', 'GarageYrBlt']].apply(compute_yrblt, axis = 1)\n# test.info()","8f0f689d":"#  Sheet 3\n# Similarly is the case with GaregeCond\ntrain['GarageCond'].fillna( 'No', inplace = True)\ntest['GarageCond'].fillna( 'No', inplace = True)","e5eddf9c":"def compute_garagecond(cols):\n    cars = cols[0]\n    cond = cols[1]\n    \n    if str(cond) == 'No' :\n        if cars == 1 :\n            return 'TA'\n        else:\n            return 'No'\n    else:\n        return cond","fe2009fb":"# Replacing the special Value with 'TA'\ntest['GarageCond'] = test[['GarageCars', 'GarageCond']].apply(compute_garagecond, axis = 1)\n# test.info()","dcf9fa29":"# The Same is the case with GarageFinish foe special Value\ntrain['GarageFinish'].fillna( 'No', inplace = True)\ntest['GarageFinish'].fillna( 'No', inplace = True)","d3be0d21":"def compute_garagefinish(cols):\n    cars = cols[0]\n    finish = cols[1]\n    \n    if str(finish) == 'No' :\n        if cars == 1 :\n            return 'RFn'\n        else:\n            return 'No'\n    else:\n        return finish","cb833722":"# Replacing with special value\ntest['GarageFinish'] = test[['GarageCars', 'GarageFinish']].apply(compute_garagefinish, axis = 1)\n# train.info()\n# test.info()","e0b6aaca":"# There is 1 null value left in the training set of Electrical Column so dropping the row\n# As dropping single row will not affect our model\ntrain.dropna(inplace = True)\n# train.info()\n# test.info()","73ea99d2":"# Comparing on Different MS Sub Classes MSZoning as RL is always Maximum so we will fill its null values with Rl\ntest['MSZoning'].fillna( 'RL', inplace = True)\n# test.info()","4172bae3":"# Replacing the utilities with mode as its optimum for filling those\ntest['Utilities'].fillna( test['Utilities'].mode()[0], inplace = True)\n# test.info()","e5b5e7ca":"# Replacing null values of Exterior1st and 2nd with mode\ntest['Exterior1st'].fillna( test['Exterior1st'].mode()[0], inplace = True)\ntest['Exterior2nd'].fillna( test['Exterior2nd'].mode()[0], inplace = True)\n# test.info()","65c5031b":"# Replacing null values of SF1, SF2, SF with zero\ntest['BsmtFinSF1'].fillna( 0, inplace = True)\ntest['BsmtFinSF2'].fillna( 0, inplace = True)\ntest['BsmtUnfSF'].fillna( 0, inplace = True)\ntest['TotalBsmtSF'] = test['BsmtFinSF1'] + test['BsmtFinSF2'] + test['BsmtUnfSF']\n# test.info()","6f3c8d80":"# Those whose BsmtFullBath and HalfBath are not given it would be best to fill them with 0\ntest['BsmtFullBath'].fillna( 0, inplace = True)\ntest['BsmtHalfBath'].fillna( 0, inplace = True)\n# test.info()","0ea1db8a":"# replacing functional with mode\ntest['Functional'].fillna( test['Functional'].mode()[0], inplace = True)\n# test.info()","9086ce3b":"# sheet 8 of train\n# As when we compare kitchenqual with kitchenabvGr always 'TA' are the most so replacing them with 'TA'\ntest['KitchenQual'].fillna( 'TA', inplace = True)\n# test.info()","1e4ab1d0":"# Thwrw is 1 null in saletype and the sale condition of that is normal \n# so will replace it with 'WD'\ntest['SaleType'].fillna( 'WD', inplace = True)\n# train.info()\n# test.info()","d3ac3098":"train.drop(['Id'], axis = 1, inplace = True)\ntest.drop(['Id'], axis = 1, inplace = True)","6cc2dd74":"#  just copyiing to a new dataframe\ntrain2 = train.copy()\ntest2 = test.copy()","437edf86":"Y = train2['SalePrice'].values\ntrain2.drop(['SalePrice'], axis = 1, inplace = True)\ntrain2.info()","22a5ef8b":"#Acquiring mu and sigma for normal distribution plot of 'SalePrice'\nfrom scipy import stats\nfrom scipy.stats import norm,skew\n(mu, sigma) = norm.fit(Y)\n\n#Plotting distribution plot of 'SalePrice' and trying to fit the normal distribution corve on that\nplt.figure(figsize=(8,8))\nax = sns.distplot(train['SalePrice'] , fit=norm);\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","8b1872d0":"fig, ax = plt.subplots(figsize=(8,8))\n\nsns.distplot(Y, kde=False,color = 'green', fit=stats.lognorm)\n\nax.set_title(\"Log Normal\",fontsize=24)\nplt.show()","d3702a3e":"Y","596c797d":"y","86ece499":"# Applying the log transformation now\ny = np.log(Y)","b55980c4":"plt.figure(figsize=(8,8))\nax = sns.distplot(y , fit=stats.norm);\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Normal Distribution'],\n            loc='best')\nplt.show()","37fe5b39":"import category_encoders as ce\nohe = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\ntrain2 = ohe.fit_transform(train2)\ntest2 = ohe.transform(test2)\ntrain2.info()","ae4521dc":"train_final = train2.values\ntest_final = test2.values","42f67249":"from sklearn.preprocessing import MinMaxScaler\nSC = MinMaxScaler(feature_range = (0,1))\ntrain_final = SC.fit_transform(train_final)\ntest_final = SC.transform(test_final)","1cf990be":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_final, \n                                                    y, test_size=0.2, \n                                                    random_state=42)","599c6a0a":"from sklearn.svm import SVR\nregressor = SVR(kernel = 'poly', degree = 4, epsilon = 0.01, gamma = 0.5 )\nregressor.fit(X_train, y_train)","540ed45c":"y_pred = (regressor.predict(X_test))\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(\"R2 score : %.2f\" % r2_score(y_test,y_pred))\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test,y_pred))","cb400baa":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(X_train)\nX_test = poly_reg.transform(X_test)","4fe7700f":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression(n_jobs = -1)\nlin_reg.fit(X_poly,y_train)","76bc1040":"y_pred = (lin_reg.predict(X_test))\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(\"R2 score : %.2f\" % r2_score(y_test,y_pred))\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test,y_pred))\n","8d3ba8b4":"from sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error","ce58202f":"params = {'n_estimators': 2000, 'max_depth': 6, 'min_samples_split': 30, 'min_samples_leaf': 1, 'max_features': 50,\n          'learning_rate': 0.01, 'loss': 'huber', 'subsample': 0.8 , 'validation_fraction': 0.01}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_train, y_train)\n#mse = mean_squared_error(y_test, clf.predict(X_test))\n#print(\"MSE: %.4f\" % mse)","66c0737f":"# y_pred = clf.predict(X_test)\n# from sklearn.metrics import mean_squared_error, r2_score\n# print(\"R2 score : %.2f\" % r2_score(y_test,y_pred))\n# print(\"Mean squared error: %.2f\" % mean_squared_error(y_test,y_pred))\n","1a0c9e74":"y_pred = regressor.predict(test_final)\n#from sklearn.metrics import confusion_matrix\n#cm = confusion_matrix(y_test, y_pred)\n#cm\ny_pred = np.exp(y_pred)\ny_pred = list(y_pred)\nprint(len(y_pred))\n\n\ntest4 = pd.read_csv('..\/input\/test.csv')\n#test4.head()\npassengerid = list(test4['Id'])\ndictionary = {'Id':passengerid, 'SalePrice':y_pred}\ndf = pd.DataFrame(dictionary)\ndf.head()\ndf.to_csv('gradient.csv',index = False)","3906896d":"Now most of the training data is cleaned but still the test data have many null values which need to be cleared.\nSo now we will also visualize the test data.","b4a67194":"**Now All the training set is cleaned buut there are few test set columns still left now dealing with them**","dd32fca3":"It can be clearly seen that the above distribution is right skewed","9e540872":"Let us first deal with the outliers in the dataset..\nAccording to the dataset GrLivArea seems to be best to plot against sale price to find outliers.","d5290408":"It is very clear that the two points on the bottom right are the outliers of the data. But it is not always good to remove outliers, removing many of them can decrease the quality of the model. But here are only two so it's better to remove them","8b78dde5":"as we can see clearly in the graph that all the points whose BsmtFinType is NA have BsmtFinSF1 as 0 so we will fill all those by Unf the same is the case with BsmtFinType2","90b87cb8":"**Importing essential libraries**","765cdea7":"Target Variable Sale Price Checking Skewness and Improving it with transformation","65fb885f":"**Checking for Null values**"}}