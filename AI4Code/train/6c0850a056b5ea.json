{"cell_type":{"ae9aa5e5":"code","d11e2aa6":"code","925a4277":"code","c11bff61":"code","5bd97905":"code","3e383be5":"code","4ef93f33":"code","18ca97be":"code","8ed24e1a":"code","57d31f6f":"code","9391b26d":"code","c682aa20":"code","56b77026":"code","2e2c5d2f":"code","9f695dea":"code","4d585873":"code","0f0d89e8":"code","8905fed6":"code","09fd7787":"code","e2c10cad":"code","3065b7ab":"code","8dcd344b":"code","5ce71221":"code","fd38ccb3":"code","a33e6069":"markdown","4d0301e7":"markdown","1e2964dc":"markdown","bb9cd8a1":"markdown","4007e8fb":"markdown","04a4b5bd":"markdown","727d1ea3":"markdown","51a23755":"markdown","7dbed027":"markdown","bdda0b03":"markdown","29d9e5ae":"markdown","0bd9c400":"markdown","43ca1534":"markdown","7c335492":"markdown","bfce173b":"markdown","a5ba06f3":"markdown","917c7b0e":"markdown","c5b5a1e7":"markdown","0cfc2516":"markdown","ccadfef9":"markdown","cb8b4116":"markdown","99a98179":"markdown","f5b5a33e":"markdown","767fbe06":"markdown","ad9052bf":"markdown","b9fcaffa":"markdown","619d1bd2":"markdown"},"source":{"ae9aa5e5":"!pip install praat-textgrids","d11e2aa6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory","925a4277":"import textgrids\n\nFRAME_DURATION = 30 # 30 msec\nOVERLAP_RATE = 0 # frames don't overlap\n\ndef readFile(path):\n    '''\n    Read the file and return the list of SPEECH\/NONSPEECH labels for each frame\n    '''\n        \n    labeled_list  = []\n    grid = textgrids.TextGrid(path)\n\n    for interval in grid['silences']:\n        label = int(interval.text)\n\n        dur = interval.dur\n        dur_msec = dur * 1000 # sec -> msec\n        num_frames = int(round(dur_msec \/30)) # the audio is divided into 30 msec frames\n        print(dur_msec)\n        for i in range(num_frames):\n            \n            labeled_list.append(label)\n\n    return labeled_list","c11bff61":"!pip install librosa\nimport librosa\n\nroot ='\/Female\/TIMIT\/SA2'\nannotation_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Female\/TMIT\/SI2220.TextGrid\"\naudio_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/Female\/TMIT\/SI2220.wav\"\n# read annotaion\nlabel_list = readFile(annotation_path)\n\n# read wav file\ndata, fs = librosa.load(audio_path)","5bd97905":"# define time axis\nNs = len(data)  # number of sample\nTs = 1 \/ fs  # sampling period\nt = np.arange(Ns) * 1000 * Ts  # time axis\n\nshift = 1 - OVERLAP_RATE\nframe_length = int(np.floor(FRAME_DURATION * fs \/ 1000)) # frame length in sample\nframe_shift = round(frame_length * shift)# frame shift in sample","3e383be5":"import matplotlib.pyplot as plt\n\nfigure = plt.Figure(figsize=(10, 7), dpi=85)\nplt.plot(t, data)\n\nfor i, frame_labeled in enumerate(label_list):\n    idx = i * frame_shift\n    if (frame_labeled == 1):\n        plt.axvspan(xmin= t[idx], xmax=t[idx + frame_length-1], ymin=-1000, ymax=1000, alpha=0.4, zorder=-100, facecolor='g', label='Speech')\n\nplt.title(\"Ground truth labels\")\nplt.legend(['Signal', 'Speech'])\nplt.show()","4ef93f33":"print(len(label_list))\nprint(len(t))\nprint(len(data))","18ca97be":"!pip install python_speech_features","8ed24e1a":"import os # Working with directories\nimport python_speech_features # For exctracting features for deep learning\nfrom tqdm import tqdm # Progress meter\nfrom sklearn import model_selection, preprocessing, metrics # Preparation data\nfrom keras.models import Model # Keras's models\nfrom keras.layers import Input, LSTM, Dense, TimeDistributed, Bidirectional # Keras's layers","57d31f6f":"# Function for reading labels from .TextGrig file:\ndef readLabels(path, sample_rate):\n    '''\n    Read the file and return the list of SPEECH\/NONSPEECH labels for each frame\n    '''\n        \n    labeled_list  = []\n    grid = textgrids.TextGrid(path)\n\n    for interval in grid['silences']:\n        label = int(interval.text)\n\n        dur = interval.dur\n        dur_samples = int(np.round(dur * sample_rate)) # sec -> num of samples\n        \n        for i in range(dur_samples):\n            labeled_list.append(label)\n\n    return labeled_list","9391b26d":"# Function for getting all files in directories and sub-directories with definite extension:\ndef getFiles(path, extension):\n    list_paths = list()\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if(file.endswith(extension)):\n                list_paths.append(os.path.join(root, file))\n    return list_paths","c682aa20":"annotation_path = '\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/'\nannotation_extension = '.TextGrid'\naudio_path = '\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/'\naudio_extension = '.wav'\n\nannotation_files = getFiles(path=annotation_path, extension=annotation_extension)\naudio_files = getFiles(path=audio_path, extension=audio_extension)\nannotation_files = sorted(annotation_files)\naudio_files = sorted(audio_files)\n\nprint('Number of files:', len(audio_files))","56b77026":"# Set params for model:\npreemphasis_coef = 0.97 # Coefficient for pre-processing filter\nframe_length = 0.025 # Window length in sec\nframe_step = 0.01 # Length of step in sec\nwindow_function = np.hamming\nnum_nfft = 512 # Point for FFT\nnum_features = 32 # Number of Mel filters","2e2c5d2f":"# Extraction features for each file:\nfor i in tqdm(range(len(audio_files))):\n    sig, sample_rate = librosa.load(audio_files[i])\n    markers = readLabels(path=annotation_files[i], sample_rate=sample_rate)\n    \n    # Extract features:\n    features_fbank, feature_energy  = python_speech_features.base.fbank(signal=sig, samplerate=sample_rate, winlen=frame_length, winstep=frame_step, nfilt=num_features, \n                                                                        nfft=num_nfft, lowfreq=0, highfreq=None, preemph=preemphasis_coef, winfunc=window_function)\n    \n    # Logfbank and log energy:\n    features_logfbank = np.log(features_fbank)\n    feature_logenergy = np.log(feature_energy)\n    \n    # Merge logfbank and log energy:\n    features = np.hstack((feature_logenergy.reshape(feature_logenergy.shape[0], 1), features_logfbank))\n    \n    # Reshape labels for each group of features:\n    markers_of_frames = python_speech_features.sigproc.framesig(sig=markers, frame_len=frame_length * sample_rate, frame_step=frame_step * sample_rate, \n                                                                winfunc=np.ones)\n    \n    # For every frame calc label:\n    marker_per_frame = np.zeros(markers_of_frames.shape[0])\n    marker_per_frame = np.array([1 if np.sum(markers_of_frames[j], axis=0) > markers_of_frames.shape[0] \/ 2 else 0 for j in range(markers_of_frames.shape[0])])\n    \n    # Create massive for stacking features in first step:\n    if i == 0:\n        dataset_tmp = np.zeros((1, num_features + 2))\n        \n    # Check indices of features and labels:\n    restrictive_index = np.min([features.shape[0], marker_per_frame.shape[0]], axis=0)\n    features_tmp = features[:restrictive_index]\n    marker_per_frame_tmp = marker_per_frame[:restrictive_index]\n    \n    # Merge label and franes and all frames in dataset:\n    dataset_tmp = np.vstack((dataset_tmp, np.hstack((marker_per_frame_tmp.reshape(marker_per_frame_tmp.shape[0], 1), features_tmp))))\n\n# Delete row consist of zeros:\ndataset = dataset_tmp[1:]","9f695dea":"# Split dataset on train and test:\nX = dataset[:, 1:]\ny = dataset[:, 0]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, shuffle=True, random_state=1)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n# Scale data:\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Reshape datasets to 1 x 300 x num_features:\n# So, each group need to consist of 300 frames:\nn_frames = 300\nX_train_reshaped = X_train[:int(X_train.shape[0] \/ n_frames) * n_frames]\nX_train_reshaped = X_train_reshaped.reshape(int(X_train_reshaped.shape[0] \/ n_frames), n_frames, X_train_reshaped.shape[1])\nX_test_reshaped = X_test[:int(X_test.shape[0] \/ n_frames) * n_frames]\nX_test_reshaped = X_test_reshaped.reshape(int(X_test_reshaped.shape[0] \/ n_frames), n_frames, X_test_reshaped.shape[1])","4d585873":"# Encoding label:\ny_train = pd.get_dummies(y_train)\ny_test = pd.get_dummies(y_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\ny_train_reshaped = y_train[:int(y_train.shape[0] \/ n_frames) * n_frames]\ny_train_reshaped = y_train_reshaped.reshape(int(y_train_reshaped.shape[0] \/ n_frames), n_frames, y_train_reshaped.shape[1])\ny_test_reshaped = y_test[:int(y_test.shape[0] \/ n_frames) * n_frames]\ny_test_reshaped = y_test_reshaped.reshape(int(y_test_reshaped.shape[0] \/ n_frames), n_frames, y_test_reshaped.shape[1])","0f0d89e8":"input_layer = Input(shape=(n_frames, num_features + 1))\nBLSTM_1_layer = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\nBLSTM_2_layer = Bidirectional(LSTM(128, return_sequences=True))(BLSTM_1_layer)\noutput_layer = TimeDistributed(Dense(2, activation='sigmoid'))(BLSTM_2_layer)\nmodel = Model(inputs=input_layer, outputs=output_layer)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","8905fed6":"model.fit(X_train_reshaped, y_train_reshaped, validation_data=(X_test_reshaped, y_test_reshaped), epochs=25, batch_size=64)","09fd7787":"annotation_file = \"..\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Female\/TMIT\/SA1.TextGrid\"\naudio_file = \"..\/input\/speech-activity-detection-datasets\/Data\/Audio\/Female\/TMIT\/SA1.wav\"\n\n# Load samples:\ninput_signal, fs = librosa.load(audio_file)\n\n\n# Load labels:\ntruth_labels = readLabels(path=annotation_file, sample_rate=fs)\n\n# Extract features:\nfeatures_fbank_valid, feature_energy_valid  = python_speech_features.base.fbank(signal=input_signal, samplerate=fs, winlen=frame_length, winstep=frame_step, \n                                                                                nfilt=num_features, nfft=num_nfft, lowfreq=0, highfreq=None, preemph=preemphasis_coef, \n                                                                                winfunc=window_function)\n\n# Logfbank and log energy:\nfeatures_logfbank_valid = np.log(features_fbank_valid)\nfeature_logenergy_valid = np.log(feature_energy_valid)\n\n# Merge logfbank and log energy:\nfeatures_valid = np.hstack((feature_logenergy_valid.reshape(feature_logenergy_valid.shape[0], 1), features_logfbank_valid))\n\n# Reshape labels for each group of features:\nmarkers_of_frames_truth = python_speech_features.sigproc.framesig(sig=truth_labels, frame_len=frame_length * fs, frame_step=frame_step * fs, winfunc=np.ones)\n\n# For every frame calc label:\nmarker_per_frame_truth = np.zeros(markers_of_frames_truth.shape[0])\nmarker_per_frame_truth = np.array([1 if np.sum(markers_of_frames_truth[j], axis=0) > markers_of_frames_truth.shape[0] \/ 2 else 0 \n                                   for j in range(markers_of_frames_truth.shape[0])])\n\n# Check indices of features and labels:\nrestrictive_index = np.min([features_valid.shape[0], markers_of_frames_truth.shape[0]], axis=0)\nfeatures_valid_tmp = features_valid[:restrictive_index]\nmarker_per_frame_truth_tmp = marker_per_frame_truth[:restrictive_index]","e2c10cad":"# Split label and features:\nX_valid = features_valid_tmp;\ny_truth = marker_per_frame_truth_tmp\n\n# Scaling:\nX_valid = scaler.transform(X_valid)\n\n# Reshape datasets to 1 x 300 x num_features:\nif X_valid.shape[0] < n_frames:\n    added_frames = np.zeros((n_frames - X_valid.shape[0], X_valid.shape[1]))\n    add_labels = np.zeros(n_frames - X_valid.shape[0])\n    X_valid = np.vstack((X_valid, added_frames))\n    y_truth = np.hstack((y_truth, add_labels))\nX_valid_reshaped = X_valid[:int(X_valid.shape[0] \/ n_frames) * n_frames]\nX_valid_reshaped = X_valid_reshaped.reshape(int(X_valid_reshaped.shape[0] \/ n_frames), n_frames, X_valid_reshaped.shape[1])","3065b7ab":"prediction = model.predict(X_valid_reshaped)\n\npredicted_label = np.zeros(prediction.shape[0] * prediction.shape[1])\npredicted_proba = np.zeros(prediction.shape[0] * prediction.shape[1])\nind = 0\nfor i in range(prediction.shape[0]):\n    for j in range(prediction.shape[1]):\n        if prediction[i][j][0] >= prediction[i][j][1]:\n            predicted_label[ind] = 0\n            predicted_proba[ind] = prediction[i][j][0]\n        else:\n            predicted_label[ind] = 1\n            predicted_proba[ind] = prediction[i][j][1]\n        ind = ind + 1\n        \npredicted_label","8dcd344b":"# Plot AUC-ROC Curve:\npoint_start = [0, 1]\npoint_end = [0, 1]\nfpr, tpr, _ = metrics.roc_curve(y_truth, predicted_proba, pos_label=1)\nroc_display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr).plot(marker='.')\nplt.plot(point_start, point_end, '--')","5ce71221":"label_timeseries = np.zeros(input_signal.shape[0])\nbegin = int(0)\nend = int(frame_length * fs)\nshift_step = int(frame_step * fs)\nfor i in range(predicted_label.shape[0]):\n    label_timeseries[begin:end] = predicted_label[i]\n    begin = begin + shift_step\n    end = end + shift_step","fd38ccb3":"# define time axis\nNs = len(input_signal)  # number of sample\nTs = 1 \/ fs  # sampling period\nt = np.arange(Ns) * Ts  # time axis in seconds\nnorm_coef = 1.1 * np.max(input_signal)\n\nedge_ind = np.min([input_signal.shape[0], len(truth_labels)])\n\nplt.figure(figsize=(20, 9), dpi=85)\nplt.plot(t[:edge_ind], input_signal[:edge_ind])\nplt.plot(t[:edge_ind], np.array(truth_labels[:edge_ind]) * norm_coef)\nplt.plot(t[:edge_ind], label_timeseries[:edge_ind] * norm_coef)\n\nplt.title(\"Ground truth labels\")\nplt.legend(['Signal', 'Speech', 'Predicted'])\nplt.show()","a33e6069":"# Example of usage ","4d0301e7":"### Fit model:","1e2964dc":"### Define the function extracting the ground truth labels","bb9cd8a1":"Let's redefine funtion that has already been implemented above","4007e8fb":"Let's will get prediction on choosen audio file and will compare with truth labels.","04a4b5bd":"## Load a file","727d1ea3":"### Make prediction:","51a23755":"Algorithm based on a article \"ViVoVAD: a Voice Activity Detection Tool based on Recurrent Neural Networks\"\n\nIn preprocessing part we need extract Mel filter bank energies from signal as a features. Features are extracted every 10 ms using a 25 ms window. We will use 32 Mel log energies and the log energy of frame.","7dbed027":"### Build model:","bdda0b03":"### Load all files:","29d9e5ae":"### Preprocessing input data (extraction features):","0bd9c400":"The green parts indicates the frames where an human speech is detected.","43ca1534":"# Voice Activity Detection:","7c335492":"### Convert labels from frames into timeseries: ","bfce173b":"## Preparation:","a5ba06f3":"### Plot graphics:","917c7b0e":"### Preparing the variable","c5b5a1e7":"## Build and training model:","0cfc2516":"We will use BLSTM layers stacked with linear layer on output. In order to reduce the delay of the dependencies, training and evaluation is perfomed with limited length sequences of 300 frames (3 seconds).\n\n![](https:\/\/miro.medium.com\/max\/577\/1*Ousw1xzJlZ07xwS__nrzsQ.png)","ccadfef9":"### Libraries:","cb8b4116":"### Install library","99a98179":"### Preparation data:","f5b5a33e":"### Load data from kaggle","767fbe06":"It's important for this kind of tasks to perform short time analysis on the signal, so it needs to assign the lables (SPEECH\/NONSPEECH) to very little portions of the signal. I decide to split the data into portion of 30 milliseconds.","ad9052bf":"## Processing files:","b9fcaffa":"## Plot signal","619d1bd2":"## Validation model:"}}