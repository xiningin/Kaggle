{"cell_type":{"1bcc1ed7":"code","79190b8f":"code","e92c2585":"code","51b5f912":"code","7105935f":"code","d9638b7b":"code","c12c9e86":"code","ed5dc2be":"code","1e9252a9":"code","ab2c16d8":"code","36e4f106":"code","23f48746":"code","084f7e91":"code","18745421":"code","19f99877":"code","ba906ba7":"code","379a7ca6":"code","e368798a":"code","016e2864":"code","15ba7a2d":"code","10787f43":"code","10011772":"code","04810634":"code","8a4bb084":"code","c37b3549":"code","1b7a531b":"code","8a278304":"code","a4cef46f":"markdown","723be406":"markdown"},"source":{"1bcc1ed7":"import warnings\nwarnings.filterwarnings('ignore')\n%config IPCompleter.greedy=True","79190b8f":"import numpy as np                          # linear algebra\nimport os                                   # used for loading the data\nfrom sklearn.metrics import confusion_matrix# confusion matrix to carry out error analysis\nimport seaborn as sn                        # heatmap\nfrom sklearn.utils import shuffle           # shuffle the data\nimport matplotlib.pyplot as plt             # 2D plotting library\nimport cv2                                  # image processing library\nimport tensorflow as tf                     # best library ever","e92c2585":"import os\nprint(os.listdir(\"\/kaggle\/input\/inceptionv3\"))","51b5f912":"# Here's our 10 categories that we have to classify.\nclass_names = ['gallina', 'cane', 'cavallo', 'gatto', 'mucca', 'farfalla', 'elefante', 'ragno', 'scoiattolo', 'pecora']\nclass_names_label = {'gallina': 0,\n                     'cane' : 1,\n                     'cavallo' : 2,\n                     'gatto' : 3,\n                     'mucca' : 4,\n                     'farfalla' : 5,\n                     'elefante': 6,\n                     'ragno': 7,\n                     'scoiattolo': 8,\n                     'pecora': 9\n                    }\nnum_classes = len(class_names)\nimg_size = 128\nnum_channel = 3\nanimal_path = '\/kaggle\/input\/animals10\/animals\/raw-img\/'","7105935f":"def resize_to_square(image, size):\n    h, w, c = image.shape\n    ratio = size \/ max(h, w)\n    resized_image = cv2.resize(image, (int(w*ratio), int(h*ratio)), cv2.INTER_AREA)\n    return resized_image","d9638b7b":"def padding(image, min_height, min_width):\n    h, w, c = image.shape\n\n    if h < min_height:\n        h_pad_top = int((min_height - h) \/ 2.0)\n        h_pad_bottom = min_height - h - h_pad_top\n    else:\n        h_pad_top = 0\n        h_pad_bottom = 0\n\n    if w < min_width:\n        w_pad_left = int((min_width - w) \/ 2.0)\n        w_pad_right = min_width - w - w_pad_left\n    else:\n        w_pad_left = 0\n        w_pad_right = 0\n        \n    return cv2.copyMakeBorder(image, h_pad_top, h_pad_bottom, w_pad_left, w_pad_right, cv2.BORDER_CONSTANT, value=(0,0,0))","c12c9e86":"def load_data(directory):\n    \"\"\"\n        Load the data:\n            - 14,034 images to train the network.\n            - 10,000 images to evaluate how accurately the network learned to classify images.\n    \"\"\"\n    output = []\n    images = []\n    labels = []\n    file_names = []\n    for folder in os.listdir(directory):\n        curr_label = class_names_label[folder]\n        for file in os.listdir(directory + \"\/\" + folder):\n            img_path = directory + \"\/\" + folder + \"\/\" + file\n            curr_img = cv2.imread(img_path)\n            curr_img = resize_to_square(curr_img, img_size)\n            curr_img = padding(curr_img, img_size, img_size)\n            images.append(curr_img)\n            labels.append(curr_label)\n            file_names.append(file)\n    images, labels, file_names = shuffle(images, labels, file_names, random_state=817328462)     ### Shuffle the data !!!\n    images = np.array(images, dtype = 'float32') ### Our images\n    labels = np.array(labels, dtype = 'int32')   ### From 0 to num_classes-1!\n\n    return images, labels, file_names","ed5dc2be":"images, labels, file_names = load_data(animal_path)","1e9252a9":"x_data = images\ny_data = labels","ab2c16d8":"print(x_data.shape)\nprint(y_data.shape)","36e4f106":"from sklearn.preprocessing import LabelEncoder\n\nLabelencoder = LabelEncoder()\ny_data = Labelencoder.fit_transform(y_data)\nnp.unique(y_data)","23f48746":"from keras.utils import to_categorical\nnum_classes = 10\nY = to_categorical(y_data,num_classes)\nX = x_data","084f7e91":"X_train = X[:20000]\ny_train = Y[:20000]\nX_temp = X[20001:]\ny_temp = Y[20001:]\n\nX_test = X_temp[:5000]\ny_test = y_temp[:5000]\nX_val = X_temp[5001:]\ny_val = y_temp[5001:]","18745421":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(X_val.shape)\nprint(y_val.shape)","19f99877":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Flatten\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.optimizers import SGD","ba906ba7":"TRAINING_NUMBER = len(y_train)\nBATCH_SIZE = 128\nEPOCHS = 10\nLEARNING_RATE = 0.005\nMOMENTUM = 0.9\nDECAY = 1e-6","379a7ca6":"from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nimage_input = Input(shape=(img_size,img_size,num_channel))\nvgg_mod = VGG16(input_tensor=image_input, include_top=False,weights='imagenet')\nvgg_mod.summary()","e368798a":"vgg_mod.trainable = False\n\nfor layer in vgg_mod.layers:\n    if layer.name == 'block5_conv1':\n        layer.trainable = True\n    if layer.name == 'block4_conv1':\n        layer.trainable = True    \n    else:\n        layer.trainable = False\n\nvgg_mod.summary()","016e2864":"add_model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=vgg_mod.output_shape[1:]), # the nn will learn the good filter to use\n    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\nmodel_vgg16 = Model(inputs=vgg_mod.input, outputs=add_model(vgg_mod.output))\nmodel_vgg16.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'],)\nmodel_vgg16.summary()","15ba7a2d":"from keras.callbacks import ReduceLROnPlateau\n\ncallbacks_list = [ReduceLROnPlateau(monitor='loss',factor=0.2,patience=3)]","10787f43":"hist_1=model_vgg16.fit(X_train,y_train,\n                       batch_size=BATCH_SIZE,\n                       epochs=EPOCHS, \n                       validation_data=(X_val, y_val), \n                       callbacks = callbacks_list, \n                       verbose=1)","10011772":"model_vgg16.save_weights(\"model_vgg16.h5\")\n#loaded_model.load_weights(\"model_vgg16.h5\"","04810634":"plt.plot(hist_1.history['loss'])\nplt.plot(hist_1.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'val'])\nplt.show()","8a4bb084":"plt.plot(hist_1.history['accuracy'])\nplt.plot(hist_1.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'val'])\nplt.show()","c37b3549":"#Testing result\nscore = model_vgg16.evaluate(X_test, y_test, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","1b7a531b":"#from keras.preprocessing.image import ImageDataGenerator\n#from keras.callbacks import ModelCheckpoint\n\n#train_datagen = ImageDataGenerator(\n#        rotation_range=30, \n#        width_shift_range=0.1,\n#        height_shift_range=0.1, \n#        horizontal_flip=True)\n#train_datagen.fit(X_train)","8a278304":"#history_vgg16 = model_vgg16.fit_generator(\n#    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n#    steps_per_epoch=X_train.shape[0] \/\/ BATCH_SIZE,\n#    epochs=EPOCHS,\n#    validation_data=(X_val, y_val),\n#    callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc', save_best_only=True)]\n#)","a4cef46f":"**Train with VGG16 models**","723be406":"**Train with Available models**"}}