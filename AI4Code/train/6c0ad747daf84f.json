{"cell_type":{"9d8bf455":"code","c47057e2":"code","cb0f2407":"code","2f1a829e":"code","eac5b205":"code","3ca55c10":"code","67616f94":"code","af5c7085":"code","30833972":"code","856edb63":"code","abb5100c":"code","d9dc3404":"code","6d4b6157":"code","cb5a0a5f":"code","190dec6c":"code","1d3cac22":"code","5a18ea15":"code","c530a9e0":"code","b0ce3ada":"code","873cf02d":"code","ec8baaed":"code","774b46f9":"code","5cb7399a":"code","62908fc3":"code","843a5800":"code","29850f4e":"code","f90df917":"code","abec6af9":"code","1c1019f3":"code","5a46d2ab":"code","089ae284":"code","47b64924":"code","d98d5c28":"code","eee88b4c":"code","fe492800":"code","f116b6fa":"code","0ad021c9":"code","8e3a9cd5":"code","9f9f38fa":"code","59e27204":"code","2d31bc1b":"code","fe9a4cd8":"code","6d21903f":"code","4e4d4b65":"code","2de1d28d":"code","2274515c":"markdown","99168577":"markdown","022ab341":"markdown","136a8865":"markdown","c898312c":"markdown","4a114433":"markdown","45f1b3e4":"markdown","b692cbb0":"markdown","d6835102":"markdown","387ce385":"markdown","6eb3a08c":"markdown","cb8f1787":"markdown","b6e8af00":"markdown","3a041cc0":"markdown","2199f44f":"markdown","abedaf93":"markdown","bd065e88":"markdown","365f81bc":"markdown","938749f4":"markdown","d50c6cde":"markdown","19b049dd":"markdown","c53f300e":"markdown","ec1abfaf":"markdown","a91c82b3":"markdown","2e7aba88":"markdown","e8a570b4":"markdown","826195f2":"markdown","00430705":"markdown","a0c44aaf":"markdown","470460f7":"markdown","9ba670f0":"markdown","24c7861e":"markdown","7d907b8e":"markdown","130de947":"markdown","91aafbfd":"markdown","7e8aab29":"markdown","41c2cc63":"markdown","614df9a8":"markdown","136c4a5d":"markdown","9993ae9f":"markdown","048c2585":"markdown","5c7763e3":"markdown","4a52177a":"markdown","6d85dbdd":"markdown","e89f23f9":"markdown","3b3a00f6":"markdown","a2d88e2a":"markdown","b4f1b909":"markdown","e29c0d41":"markdown"},"source":{"9d8bf455":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier as rf_clf\nfrom sklearn.model_selection import RandomizedSearchCV as randomCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier","c47057e2":"hcv_df=pd.read_csv(r'..\/input\/hcv-data-data-set\/hcvdat0.csv')\nhcv_df","cb0f2407":"hcv_df_cp=hcv_df.copy()\nnp.unique(np.ravel(hcv_df[[\"Category\"]]))","2f1a829e":"hcv_df.loc[:,[\"Category\"]]=hcv_df.loc[:,[\"Category\"]].replace(\n    {'0=Blood Donor':0,\n    '0s=suspect Blood Donor':1,\n    '1=Hepatitis':2,\n    '2=Fibrosis':3,\n    '3=Cirrhosis':4},regex=True)\n\nhcv_df.loc[:,[\"Sex\"]]=hcv_df.loc[:,[\"Sex\"]].replace(\n    {'m':0,\n    'f':1},regex=True)","eac5b205":"hcv_df.describe()","3ca55c10":"hcv_df.isna().sum()","67616f94":"hcv_replace_val=\\\nhcv_df.loc[:,[ 'ALB','ALP', 'ALT','CHOL','PROT',\"Category\"]].groupby(\"Category\").agg([np.median])\nhcv_replace_val","af5c7085":"\nfor i in [ 'ALB','ALP', 'ALT','CHOL','PROT']:\n    for j in range(0,int(np.max(hcv_df[[\"Category\"]])+1)):\n        hcv_df.loc[(hcv_df[i].isna()==True) & (hcv_df[\"Category\"]==j),[i]]=\\\n        hcv_df.loc[(hcv_df[i].isna()==True) & (hcv_df[\"Category\"]==j),[i]].\\\n        replace(np.nan,int(hcv_replace_val.loc[:,[i]].iloc[j]))\n","30833972":"hcv_df[[ 'ALB','ALP', 'ALT','CHOL','PROT']].describe()","856edb63":"hcv_df.hist(figsize=(10,10))\nplt.plot()","abb5100c":"hcv_df.drop(\"Unnamed: 0\",axis=1,inplace=True)","d9dc3404":"cls_wg=dict(hcv_df[\"Category\"].value_counts())\nprint(cls_wg)","6d4b6157":"corr = hcv_df.corr()\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask,cmap=cmap, vmax=.9, center=0, square=True, linewidths=.5, annot=True,cbar_kws={\"shrink\": .5})\nplt.show()","cb5a0a5f":"hcv_X=hcv_df.copy()\nhcv_X.drop([\"Category\"],axis=1,inplace=True)\n\nhcv_Y=hcv_df[[\"Category\"]].copy()\n\nhcv_train_X,hcv_test_X,hcv_train_Y,hcv_test_Y=\\\ntrain_test_split(hcv_X,hcv_Y,test_size=0.20,random_state=48)","190dec6c":"print(hcv_train_X.shape)\nprint(hcv_test_X.shape)","1d3cac22":"def rf_classifier(min_sample_split_in,min_sample_leaf_in,no_trees,max_features_in,score_criteria):\n    rf_grid={\"min_samples_split\":min_sample_split_in,\"min_samples_leaf\":min_sample_leaf_in,\n            \"n_estimators\":no_trees,\"max_features\":max_features_in}\n    clf = rf_clf(max_depth=3, random_state=48,criterion=\"gini\")\n    rf_clf_cv = randomCV(clf, rf_grid, random_state=48,scoring=score_criteria,cv=5,return_train_score=True)\n    return rf_clf_cv","5a18ea15":"min_split=np.arange(2,40,5)\nmin_leaf=np.arange(2,40,10)\nn_trees=np.arange(100,350,50)\nmax_features=np.arange(3,11,1)\n\nrf_clf_model=rf_classifier(min_sample_split_in=min_split,min_sample_leaf_in=min_leaf,max_features_in=max_features,\n                           no_trees=n_trees,score_criteria=\"f1_weighted\")","c530a9e0":"rf_clf_model.fit(hcv_train_X,np.ravel(hcv_train_Y))","b0ce3ada":"print(\"Best set of parameters:\",rf_clf_model.best_params_)\nprint(\"Weighted F1 score for best set of parameters:\",rf_clf_model.best_score_)","873cf02d":"rf_clf_best=rf_clf(n_estimators=150, min_samples_split=7, min_samples_leaf=2, max_features=9,max_depth=3, random_state=48,\n                   criterion=\"gini\")\nrf_clf_best.fit(hcv_train_X,np.ravel(hcv_train_Y))","ec8baaed":"rf_train_y=rf_clf_best.predict(hcv_train_X)\nrf_test_y=rf_clf_best.predict(hcv_test_X)","774b46f9":"def cf_mat(data_Y_actual,data_Y_pred,title,f1_average):\n    cm_grid=confusion_matrix(data_Y_actual,data_Y_pred)\n    cm_grid_display=ConfusionMatrixDisplay(confusion_matrix=cm_grid)\n    cm_grid_display.plot()\n    plt.title(title)\n    plt.show()\n    print(\"Average F1 score for all classes:\",f1_score(data_Y_actual,data_Y_pred,average=f1_average).mean())","5cb7399a":"cf_mat(hcv_train_Y,rf_train_y,title=\"Confusion Matrix Based on Train HCV Data\",f1_average=\"weighted\")","62908fc3":"cf_mat(hcv_test_Y,rf_test_y,title=\"Confusion Matrix Based on Test HCV Data\",f1_average=\"weighted\")","843a5800":"feature_impt=dict(zip(list(hcv_train_X.columns),list(rf_clf_best.feature_importances_)))\nfeature_impt=dict(sorted(feature_impt.items(), key=lambda item: item[1],reverse=True))\nfeature_impt","29850f4e":"final_feature_list=list(feature_impt.keys())","f90df917":"def knn_classifier(neighbors,leaf_size_in,score_criteria):\n    knn_grid={\"n_neighbors\":neighbors,\"leaf_size\":leaf_size_in}\n    clf = KNeighborsClassifier(weights=\"distance\",algorithm=\"auto\",metric=\"minkowski\",n_jobs=-1)\n    knn_clf_cv = randomCV(clf, knn_grid, random_state=48,scoring=score_criteria,cv=5,return_train_score=True)\n    return knn_clf_cv","abec6af9":"neighbors_knn_in=np.arange(2,30,2)\nleaf_knn_in=np.arange(1,40,2)\n\nknn_cv_model=knn_classifier(neighbors=neighbors_knn_in,leaf_size_in=leaf_knn_in,score_criteria=\"f1_weighted\")\n","1c1019f3":"knn_cv_model.fit(hcv_train_X,np.ravel(hcv_train_Y))","5a46d2ab":"print(\"Best set of parameters:\",knn_cv_model.best_params_)\nprint(\"Weighted F1 score for best set of parameters:\",knn_cv_model.best_score_)","089ae284":"knn_clf=KNeighborsClassifier(n_neighbors= 4, leaf_size=25,\n                             weights=\"distance\",algorithm=\"auto\",metric=\"minkowski\",n_jobs=-1)\nknn_clf.fit(hcv_train_X,np.ravel(hcv_train_Y))\nknn_train_Y=knn_clf.predict(hcv_train_X)\nknn_test_Y=knn_clf.predict(hcv_test_X)","47b64924":"cf_mat(hcv_train_Y,knn_train_Y,title=\"Confusion Matrix Based on Train HCV Data\",f1_average=\"weighted\")","d98d5c28":"cf_mat(hcv_test_Y,knn_test_Y,title=\"Confusion Matrix Based on Test HCV Data\",f1_average=\"weighted\")","eee88b4c":"f1_train_list=[]\nf1_test_list=[]\n\nfor i in range(1,len(final_feature_list)+1):\n    knn_clf_red_temp=KNeighborsClassifier(n_neighbors= 4, leaf_size=25,\n                             weights=\"distance\",algorithm=\"auto\",metric=\"minkowski\",n_jobs=-1)\n    knn_clf_red_temp.fit(hcv_train_X.loc[:,final_feature_list[:i]],np.ravel(hcv_train_Y))\n    knn_train_y_red_temp=knn_clf_red_temp.predict(hcv_train_X.loc[:,final_feature_list[:i]])\n    knn_test_y_red_temp=knn_clf_red_temp.predict(hcv_test_X.loc[:,final_feature_list[:i]])\n    f1_train_temp=f1_score(hcv_train_Y,knn_train_y_red_temp,average=\"weighted\")\n    f1_test_temp=f1_score(hcv_test_Y,knn_test_y_red_temp,average=\"weighted\")\n    f1_train_list.append(f1_train_temp)\n    f1_test_list.append(f1_test_temp)","fe492800":"#f1_index=np.arange(1,6,1)\nf1_feature_sel=pd.DataFrame(zip(f1_train_list,f1_test_list))\nf1_feature_sel.columns=[\"F1_train\",\"F1_test\"]\nf1_feature_sel.index=f1_feature_sel.index+1","f116b6fa":"sns.lineplot(data=f1_feature_sel)\\\n.set_title(\"F1 Score Based on Train and Test Datasets\\n Using Different Number of Features\")\nplt.show()","0ad021c9":"knn_clf_red=KNeighborsClassifier(n_neighbors= 4, leaf_size=25,\n                             weights=\"distance\",algorithm=\"auto\",metric=\"minkowski\",n_jobs=-1)\nknn_clf_red.fit(hcv_train_X.loc[:,final_feature_list[:4]],np.ravel(hcv_train_Y))\nknn_train_y_red=knn_clf_red.predict(hcv_train_X.loc[:,final_feature_list[:4]])\nknn_test_y_red=knn_clf_red.predict(hcv_test_X.loc[:,final_feature_list[:4]])\ncf_mat(hcv_test_Y,knn_test_y_red,title=\"Confusion Matrix Based on Test HCV Data\",f1_average=\"weighted\")","8e3a9cd5":"hcv_train=hcv_train_X.loc[:,final_feature_list[:4]].copy()\nhcv_train[[\"Pred Categories\"]]=knn_train_y_red","9f9f38fa":"hcv_train.groupby(\"Pred Categories\").agg([np.median,np.mean,np.std,])","59e27204":"hcv_test=hcv_test_X.loc[:,final_feature_list[:4]].copy()\nhcv_test[[\"Pred Categories\"]]=knn_test_y_red","2d31bc1b":"hcv_test.groupby(\"Pred Categories\").agg([np.median,np.mean,np.std,])","fe9a4cd8":"fig,ax=plt.subplots(2,2,figsize=(10,10))\nsns.scatterplot(data=hcv_train, x=\"AST\", y=\"Pred Categories\",ax=ax[0][0])\nsns.scatterplot(data=hcv_train, x=\"ALP\", y=\"Pred Categories\",ax=ax[0][1])\nsns.scatterplot(data=hcv_train, x=\"ALT\", y=\"Pred Categories\",ax=ax[1][0])\nsns.scatterplot(data=hcv_train, x=\"CHE\", y=\"Pred Categories\",ax=ax[1][1])\nplt.show()","6d21903f":"fig,ax=plt.subplots(2,2,figsize=(10,10))\nsns.scatterplot(data=hcv_test, x=\"AST\", y=\"Pred Categories\",ax=ax[0][0])\nsns.scatterplot(data=hcv_test, x=\"ALP\", y=\"Pred Categories\",ax=ax[0][1])\nsns.scatterplot(data=hcv_test, x=\"ALT\", y=\"Pred Categories\",ax=ax[1][0])\nsns.scatterplot(data=hcv_test, x=\"CHE\", y=\"Pred Categories\",ax=ax[1][1])\nplt.show()","4e4d4b65":"std_scale=StandardScaler()\nhcv_train_X_s=pd.DataFrame(std_scale.fit_transform(hcv_train_X))\nhcv_test_X_s=pd.DataFrame(std_scale.fit_transform(hcv_test_X))\nhcv_train_X_s.columns=hcv_train_X.columns\nhcv_test_X_s.columns=hcv_test_X.columns","2de1d28d":"knn_clf_red.fit(hcv_train_X_s.loc[:,final_feature_list[:4]],np.ravel(hcv_train_Y))\nknn_train_y_red_s=knn_clf_red.predict(hcv_train_X_s.loc[:,final_feature_list[:4]])\nknn_test_y_red_s=knn_clf_red.predict(hcv_test_X_s.loc[:,final_feature_list[:4]])\ncf_mat(hcv_test_Y,knn_test_y_red_s,title=\"Confusion Matrix Based on Test HCV Scaled Data\",\n       f1_average=\"weighted\")","2274515c":"# Hyperparameter Tuning for K-Nearest Neighbors Classifier Using RandomCV","99168577":"# Feature Selection Using Random Forest Classifier","022ab341":"# Data Split for Train and Test Sets","136a8865":"Using the median values tabulated, the missing values are replaced with the median values based on categories.","c898312c":"This section will load the data into Jupyter notebook and display the dataframe. ","4a114433":"The features selected are stored in a list for later use.","45f1b3e4":"Looking at the test set, KNN classfied 3 subjects as blood donor despite they are awaiting confirmation of the status as blood donor or having hepatitis or cirrhosis diseases. \n\nFurthermore, the big difference in weighted F1 score between train and test sets indicates that KNN has a serious overfitting problem compared to random forest classifier. ","b692cbb0":"When comes to test set, the model able to clearly separate subjects with Fibrosis and Cirrhosis from blood donors.","d6835102":"The function above is to plot a confusion matrix with F1 score at the bottom of the plot.","387ce385":"# KNN Classifer Model Fitting Using Forward Selection to Determine Maximum Number of Features","6eb3a08c":"The function above is to create a KNN model with a randomised search cross validation process to find the best set of hyperparameters for KNN. The hyperparameters that set in KNN are number of neighbors and number of samples in each leaf.","cb8f1787":"Based on the confusion matrices above, weighted F1 score for train is around 95% while for test is 89%. This indicates that the model might be overfitting as it is not well in predicting classes using new data. However, the random forest classifier is used to determine the weights for each feature, so the overfitting problem can be ignored. ","b6e8af00":"Looking at the graph above, the first 4 features are sufficient for KNN to do classification despite the huge gap of weighted F1 score between train and test sets indicates the model is overfitting. ","3a041cc0":"# Library Loading","2199f44f":"The hyperparameters that set in random CV search are minimum sample for splitting, minimum size for each leaf, number of trees and maximum number of features used. As there are some classes with low counts, minimum size for each leaf and each split begin with 2. \n\nDue to imbalance class size, weighted f1 score is used as it includes the effect of imbalance class size by calculating f1 score based on the proportion for each class.  ","abedaf93":"Based on the dataframe above, it has 615 instances with 14 columns.","bd065e88":"This section will look into data whether there are any missing values or inconsistent data types. ","365f81bc":"Based on CV result of the KNN classifier, it has a weighted F1 score of 91%, which is considered quite good, and its performance is slightly weaker than the random forest classifier. The best KNN uses 4 neighbors to determine the class for each data point with and 25 instances in each tree to speed up KNN process.","938749f4":"There are 15 subjects being classified as blood donors despite they are pending for the confirmation of the status as blood donor or have blood-transmitted diseases such as Hepatitis and Fibrosis. ","d50c6cde":"Looking at the histograms, 0 class has the largest count out of 5 classes. ID do not carry any meaning as it is used to identify the subjects, so ID column is dropped. ALP, ALT, AST, BIL, CREA and GGT indicate that the distribution is skewed to the right as they have a long right tailed and some extremely large values. ","19b049dd":"# Data Exploration","c53f300e":"Looking at the scatter plots above for train datasets, those predicted that have Hepatitis, Fibrosis and Cirrhosis have higher AST compared to those that are blood donor or considered as future blood donor. Those predicted that have Cirrhosis have lower CHE.\n","ec1abfaf":"This section will do simple data visualisation such as histogram and correlation matrix to understand the data in terms of distribution and relationship between variables. ","a91c82b3":"Looking at the counts for data values with NA, there are quite a few in ALP and CHOL while 1 instance with NA for ALB, ALT and PROT. Therefore, median value for the variables with NA values for each category will be used.","2e7aba88":"This section is to find out how many features to be used in KNN using the best hyperparameter set in the previous section for classification using forward selection. The dataframe for X is arranged based on feature importance with the first variable as the variable with the highest weight in feature importance followed by variables with lower importance in a decreasing order.","e8a570b4":"This section is to refit the random forest classifier using the best parameter set and do feature selection based on the weight importance calculated for each feature using the classifier. ","826195f2":"Based on the confusion matrix and average weighted F1 score above, data standardisation does not improve the model performance of KNN but more instances are being misclassified. \n\nTherefore, data standardisation might not improve the model performance as some information in the data might be lost after data standardisation unless the model performs badly at the beginning even after feature selection.","00430705":"# Hyperparameter Tuning for Random Forest Classifier Using RandomCV","a0c44aaf":"Looking at the scatter plots above for test dataset, they show similar patterns as the train dataset except that no instances are predicted to have Hepatitis. However, in actual data, there are 5 instances with Hepatitis.  ","470460f7":"Using reduced set of features, 1 subject with Cirrhosis that previously classified as blood donor is correctly classified as having Cirrhosis. Furthermore, weighted F1 score is slightly increased by approximately 1%. ","9ba670f0":"The NA values replacement in ALP and CHOL using median values causes the mean values for ALP and CHOL to be slightly decrease. ","24c7861e":"# KNN Using Standardised Data","7d907b8e":"This section will split data into 2 sets: train set for model fitting and test set for model validation. ","130de947":"# Findings Based on KNN After Feature Selection","91aafbfd":"This section is to explore using K-Nearest Neighbours (KNN) classifier for classification. Before fitting the model, random CV search is used to find the best set of hyperparameters for KNN to fit the data.KNN's weight will be based on the distance, which means that the weight will be larger if the distance of 1 data point to another is smaller compared to another datapoint, and the distance is measured using Minkowski as it is the most commonly used distance metric.","7e8aab29":"Looking at the tables above based on train dataset and test dataset:\n1. subjects with Cirrhosis and Fibrosis have higher value in AST in terms of mean and median compared to other groups\n2. subjects with Cirrhosis and the status of suspect blood donor have higher value in ALP in terms of mean and median compared to other groups\n3. subjects with Cirrhosis have lower value in ALT in terms of mean and median compared to other groups\n4. subjects with Cirrhosis and the status of suspect blood donor have lower value in CHE in terms of mean and median compared to other groups\n5. subjects with Fibrosis have higher value in ALT in terms of mean and median compared to other groups\n6. subjects with Hepatitis have lower value in ALP in terms of mean and median compared to other groups\n\nTherefore, subjects with Cirrhosis tend to be more likely to have high values in AST and ALP but low values in CHE and ALT while subjects with Fibrosis tend to be more likely to have high values in AST and ALT. Subjects with Hepatitis tend to be more likely to have low value in ALP. ","41c2cc63":"# Data Loading","614df9a8":"Looking at the feature importance list above, AST, ALP, ALT, CHE and ALB are the top 5 features compared to others as others have weights less than 6%. The features with weights less than 6% might be insignificant to distinguish the subjects whether they are suitable blood donors. ","136c4a5d":"To prevent overfitting, the random forest classifier is restricted to a maximum depth of 3. The criterion for splitting is based on gini impurity as it is more suitable to deal with a categorical variable with high cardinality. For this case, category variable is considered as high cardinality as it has 5 unique values. ","9993ae9f":"To make it easier to deal with categorical data, each categorical value is converted into a number, so that it is easier to fit into statistical model.","048c2585":"The model performance of KNN after feature selection using random forest classifier has improved, especially when using test dataset. This indicates that KNN can generalised well after the feature selection. ","5c7763e3":"The data is from UCI Machine Learning Depository HCV Dataset. https:\/\/archive.ics.uci.edu\/ml\/datasets\/HCV+data\n\nThis notebook is to do classification using K-Nearest Neighbours and use random forest classifier to do feature selection. ","4a52177a":"# Data Cleansing","6d85dbdd":"This section will use random search cross validation in SKlearn to find the best set of hyperparameters for fitting the data into random forest classifier. The random forest classifier can be used as classifier, but also feature selection as it can calculate the weights for each feature by measuring how frequent and how accurate for each feature to be used in differentiating the instances into different classes using information gain or gini impurity. ","e89f23f9":"This section will use standardised data, especially for independent variables, to determine whether data standardisation can improve the model performance of KNN even further after feature selection. ","3b3a00f6":"Based on CV result of the random forest classifier, it has a weighted F1 score of 91% which is considered quite good. The best random tree classifier for the data needs to have 150 trees with a minimum sample of 7 and 2 for splitting and at each leaf respectively and 9 features. ","a2d88e2a":"Based on the correlation matrix above, there are a few pairs of variables are correlated like AST with GGT, ALB and PROT and CHOL with CHE. But, the correlation values are at a level where they are still acceptable.","b4f1b909":"Looking at the counts for each classes, class 0 (Blood donor) has the highest count followed by class 4 (Cirrhosis), class 2 (Hepatitis) and class 3 (Fibrosis). Class 1 (Suspect blood donor) is the class with the least count. ","e29c0d41":"Train dataset has 492 instances while test dataset has 123 instances. "}}