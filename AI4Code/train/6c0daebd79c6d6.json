{"cell_type":{"7811e30f":"code","af34388f":"code","7d80f743":"code","cef9ed96":"code","0c0c0b73":"code","664dd1df":"code","de18399d":"code","d840a134":"code","57d77bcc":"code","fbb30976":"code","aa8fd649":"code","f17b40cb":"code","e2537c01":"code","50908ce0":"code","61a575c9":"code","8ce386a5":"code","069b7397":"code","aa213d34":"code","d49fd013":"code","c7ccfcf4":"code","57295cb9":"code","89106b32":"code","ac26b28f":"code","81f2708c":"code","04b001f5":"code","5b53761e":"code","595d01b4":"code","5b9801a3":"code","a64d44fc":"code","f53159cf":"code","3044cb9a":"code","d73451f8":"code","cd2011c7":"code","c78d0a00":"code","613351be":"code","5a4904a9":"code","047400ed":"code","ccff7a6a":"code","3849bbe7":"code","e070fbc7":"code","1f8ecded":"code","e2278d98":"code","3b3b2cd6":"code","1a66926c":"code","ffaaf03d":"code","cca41632":"code","ab88530f":"code","c209a8d8":"code","bfb2671e":"code","6b75e08c":"code","f481d835":"code","f30525ee":"code","a0c620d4":"code","fc46d1a3":"code","c3cf5d39":"code","40c65d53":"code","2c64c377":"code","5aa71d0e":"code","b80c87a3":"code","0a41b9ce":"code","6f79597c":"code","59812408":"code","d5b9b42e":"code","e41cc0cb":"code","3598d917":"code","2a47c0ae":"code","524d7fbe":"code","a03f7554":"code","28b3c017":"code","9b2bb854":"code","0eb8c113":"code","7bd9475f":"code","14235202":"code","cb15493a":"code","99a8fbd4":"code","67b01807":"code","71e84c61":"code","e3a97959":"code","1a245e8a":"code","4210eb7c":"code","5eea5c66":"code","dbc54636":"code","9619ec1d":"code","a6b0f167":"code","e6a956b7":"code","dc9dc2c7":"code","6fa59e73":"code","6692f98b":"code","a1555e38":"code","8fa13d14":"code","7e520bb8":"code","e4f6aed9":"code","1ee77366":"code","e484a2df":"code","fe99796b":"code","5c554a53":"code","4ad32248":"code","0a8f26a7":"code","5f011f0d":"code","dfde66d9":"code","29cfa161":"code","f348ca62":"code","13d50263":"code","14152fdd":"code","5ca344a9":"code","510b3d2c":"code","166b528c":"code","dadc5b05":"code","7f31f84e":"code","31d24cca":"code","ab0f1ca8":"code","33714a5d":"code","3eb362dc":"code","2d2b64b6":"code","3b07f807":"code","fd44a449":"code","411d28b7":"code","29c2c421":"code","766d6dd0":"code","0b726036":"code","1fa43da0":"code","51772ba0":"code","6c3e96de":"code","7af01831":"code","312d5eb7":"code","2ff88ebf":"code","3dfcb1db":"code","1a659257":"code","80de2fc8":"code","e8190270":"code","3ce66966":"code","70b27895":"code","7c29cc7d":"code","6bb89adb":"code","9d5984e2":"code","3f79cd6a":"code","0f570813":"code","b22dae58":"code","e72c2f1f":"code","53d2be59":"code","f775af70":"code","a406351c":"code","0bea66fb":"code","64847b4e":"code","c21f7bd5":"code","5376cb43":"code","09f065e9":"code","4f93f924":"code","65b92a85":"code","74d4e8e4":"code","030bf396":"code","3a01996b":"code","ee1a1fe8":"code","a427e28d":"code","fa4a277c":"code","73e67148":"code","d63ea466":"code","8afb7427":"code","7bef0ef7":"code","569abc87":"code","f7bf9dcb":"code","897719cf":"code","c5be532d":"code","ce224571":"code","2e94c3e4":"code","52e7920b":"code","0633bea1":"code","e4a896b5":"code","52d6d79f":"code","2a38fc2f":"code","ef11698f":"code","dc003236":"code","07d12d34":"code","527a3704":"code","4bc09a1a":"code","d489126b":"code","511bdfab":"code","c2f9c929":"code","cb89318e":"code","471ae378":"code","095f7e29":"code","47fb2e73":"code","ae3c04e5":"code","7fcf98f0":"code","2c2460c8":"code","be7f33bf":"code","ca4f6045":"code","3fb3eb3b":"code","0722519d":"code","ca579c8f":"code","a7d560fe":"code","55387117":"code","d01c60d1":"code","183c21a5":"code","a95e04bb":"code","1b42c7d6":"markdown","5624d033":"markdown","e98409ae":"markdown","fd7cc125":"markdown","dcf5c6b7":"markdown","e0e8843d":"markdown","24e0db3d":"markdown","ac7a545c":"markdown","20fa8978":"markdown","e0dc7ead":"markdown","81e2cb58":"markdown","d15a8466":"markdown","a7503393":"markdown","73911405":"markdown","fa800d32":"markdown","688cf634":"markdown","2423fc40":"markdown","16ace299":"markdown","63dc2a27":"markdown","6079b654":"markdown","40867b6e":"markdown","9bcbf4b2":"markdown"},"source":{"7811e30f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\n\nimport statsmodels.api as sm","af34388f":"df = pd.read_csv('..\/input\/Leads.csv')\ndf.head() ","7d80f743":"df.shape","cef9ed96":"df.columns","0c0c0b73":"df.info()","664dd1df":"df.describe()","de18399d":"### We can see that there are some outliers present in a few columns like: Total Visits, Total time spent on website","d840a134":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","57d77bcc":"conversion = (sum(df['Converted'])\/len(df['Converted'].index))*100","fbb30976":"conversion","aa8fd649":"df.describe(include='all')","f17b40cb":"print(df.Country.unique())\nprint(\"-\"*100)\nprint(df['City'].unique())\nprint(\"-\"*100)","e2537c01":"sns.countplot(df.Country)\nxticks(rotation = 90)","50908ce0":"sns.countplot(df.Country)\nxticks(rotation = 90)","61a575c9":"## More than 90% of the value is India so we can safely remove this column\n\ndf = df.drop('Country',axis=1)","8ce386a5":"sns.countplot(df.City)\nxticks(rotation = 90)","069b7397":"## Select values and null values for City can be imputed as Mumbai\n\ndf['City'] = df['City'].fillna(df['City'].mode()[0])\ndf['City'] = df['City'].replace(\"Select\", \"Mumbai\")","aa213d34":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","d49fd013":"sns.countplot(x = \"City\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# Majority Conversion is also from Mumbai","c7ccfcf4":"sns.countplot(df['Specialization'])\nxticks(rotation = 90)","57295cb9":"## Specialization has almost 1750 select values and apart from this 15% null values\n## What we can do about this create a category of others for Students or misc.\n\ndf['Specialization'] = df['Specialization'].replace(np.nan, 'Others')\ndf['Specialization'] = df['Specialization'].replace(\"Select\", \"Others\")","89106b32":"sns.countplot(x = \"Specialization\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# Majority Conversion Data is from Others","ac26b28f":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","81f2708c":"sns.countplot(df['How did you hear about X Education'])\nxticks(rotation = 90)","04b001f5":"## Column \"How did you hear about X Education\" column can be dropped as most of the values are select and 23% are null values apart from select\n\ndf = df.drop('How did you hear about X Education',axis=1)","5b53761e":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","595d01b4":"sns.countplot(df['What is your current occupation'])\nxticks(rotation = 90)","5b9801a3":"## Most of the values are unemployed so null values can be imputed as unemployed\n\ndf['What is your current occupation'] = df['What is your current occupation'].replace(np.nan, 'Unemployed')","a64d44fc":"sns.countplot(x = \"What is your current occupation\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# Max Conversion is from Unemployed, but the ratio of being converted is better in Working Professionals","f53159cf":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","3044cb9a":"sns.countplot(df['What matters most to you in choosing a course'])\nxticks(rotation = 90)","d73451f8":"# Since majority data (more than 80% is for Better Career Prospect, if we impute this then it will be more than 90%) so it is safe to drop this column\n\ndf = df.drop('What matters most to you in choosing a course', axis=1)","cd2011c7":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","c78d0a00":"sns.countplot(df['Tags'])\nxticks(rotation = 90)","613351be":"# Since most frequent option is Will Revert after reading the email, we can impute the mode in this case:\n\ndf['Tags'] = df['Tags'].fillna(df['Tags'].mode()[0])","5a4904a9":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Tags\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n# We can see that conversion rate of the above imputed option is highest","047400ed":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","ccff7a6a":"sns.countplot(df['Lead Quality'])\nxticks(rotation = 90)","3849bbe7":"# Lead Quality seems an important parameter as per the business so instead of dropping this we can impute the values to  not sure since whoever was filling the form did not mention explicitly\n\ndf['Lead Quality'] = df['Lead Quality'].replace(np.nan, 'Not Sure')","e070fbc7":"sns.countplot(x = \"Lead Quality\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# Low, High in relevance have a better converion ratio but Might be have the most conversions ","1f8ecded":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","e2278d98":"sns.countplot(df['Lead Profile'])\nxticks(rotation = 90)","3b3b2cd6":"# Lead Profile already has 4000 select values and then 29% null values which makes this column useless, so its safe to drop it\n\ndf = df.drop('Lead Profile',axis=1)","1a66926c":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","ffaaf03d":"sns.countplot(df['Asymmetrique Activity Index'])\nxticks(rotation = 90)","cca41632":"# For this column we can impute the null values to 02.Medium\n\ndf['Asymmetrique Activity Index'] = df['Asymmetrique Activity Index'].fillna(df['Asymmetrique Activity Index'].mode()[0])","ab88530f":"sns.countplot(x = \"Asymmetrique Activity Index\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","c209a8d8":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","bfb2671e":"sns.countplot(df['Asymmetrique Profile Index'])\nxticks(rotation = 90)","6b75e08c":"sns.countplot(x = \"Asymmetrique Profile Index\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","f481d835":"# We can not deduce any analysis from this column and since the null values are high we can safely drop this column\n\ndf = df.drop('Asymmetrique Profile Index',axis=1)","f30525ee":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","a0c620d4":"sns.countplot(df['Asymmetrique Activity Score'])\nxticks(rotation = 90)","fc46d1a3":"# Values are too close to be imputed in this case and the number of null values is not small, we can drop this column\n\ndf = df.drop('Asymmetrique Activity Score',axis=1)","c3cf5d39":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","40c65d53":"sns.countplot(df['Asymmetrique Profile Score'])\nxticks(rotation = 90)","2c64c377":"# Values are too close to be imputed in this case and the number of null values is not small, we can drop this column\n\ndf = df.drop('Asymmetrique Profile Score',axis=1)","5aa71d0e":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","b80c87a3":"## For Lead Source, Last Activity, Page Views per Visit, Total visits we can drop \n##those rows which contain null values since the number of missing values is less\n\ndf = df.dropna()","0a41b9ce":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","6f79597c":"df.shape","59812408":"sns.countplot(x = \"Lead Source\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","d5b9b42e":"## We can club the values which do not have a considerable impact on Lead Source\n\ndf['Lead Source'] = df['Lead Source'].replace(['Pay per Click Ads','bing','blog','Social Media','WeLearn','Click2call','Live Chat','welearnblog_Home',\n                                               'youtubechannel','testone','Press_Release','NC_EDM'], 'Others')\n\ndf.loc[(df['Lead Source'] == 'google'),'Lead Source'] = 'Google'","e41cc0cb":"sns.countplot(x = \"Lead Source\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","3598d917":"sns.countplot(x = \"Do Not Email\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# We can see that There is a very small amount of conversion that has happened when this value is Yes","2a47c0ae":"sns.countplot(x = \"Do Not Call\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# We can see that There is no value of conversion for Yes value of Do not call","524d7fbe":"sns.countplot(x = \"Do Not Call\", data = df)\nxticks(rotation = 90)","a03f7554":"# Also we can see that there is no value of Yes in this case so we can drop this column since it does not add any value to the data.\n\ndf = df.drop('Do Not Call',axis=1)","28b3c017":"# Checking the percentage of missing values\nround(100*(df.isnull().sum()\/len(df.index)), 2)","9b2bb854":"sns.boxplot(x = \"TotalVisits\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n","0eb8c113":"df['TotalVisits'].describe(percentiles=[0.05,.25, .5, .75, .90, .95, .99])","7bd9475f":"# We can see that there are some outliers present so we can treat these outliers before proceeding further\n\npercentiles = df['TotalVisits'].quantile([0.05,0.95]).values\ndf['TotalVisits'][df['TotalVisits'] <= percentiles[0]] = percentiles[0]\ndf['TotalVisits'][df['TotalVisits'] >= percentiles[1]] = percentiles[1]","14235202":"sns.countplot(x = \"TotalVisits\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# This data could mean that when a user visits the website often the ratio of conversion gets better","cb15493a":"sns.boxplot(x = \"Total Time Spent on Website\", data = df)\nxticks(rotation = 90)\n","99a8fbd4":"sns.boxplot(x = \"Page Views Per Visit\",data = df)\nxticks(rotation = 90)","67b01807":"## A number of outliers are also present in this case so we can remove these\n\npercentiles = df['Page Views Per Visit'].quantile([0.05,0.95]).values\ndf['Page Views Per Visit'][df['Page Views Per Visit'] <= percentiles[0]] = percentiles[0]\ndf['Page Views Per Visit'][df['Page Views Per Visit'] >= percentiles[1]] = percentiles[1]","71e84c61":"sns.boxplot(x = \"Page Views Per Visit\",data = df)\nxticks(rotation = 90)","e3a97959":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Last Activity\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","1a245e8a":"# We can club calues which have no or very less data\n\ndf['Last Activity'] = df['Last Activity'].replace(['View in browser link Clicked','Visited Booth in Tradeshow',\n                                                   'Approached upfront','Resubscribed to emails','Email Received','Email Marked Spam'], 'Others')","4210eb7c":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Last Activity\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","5eea5c66":"sns.countplot(x = \"Specialization\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","dbc54636":"# We can club calues which have no or very less data\n\ndf['Specialization'] = df['Specialization'].replace(['Services Excellence','Retail Management',\n                                                   'Hospitality Management','Rural and Agrbusiness','E-Business'], 'Others')","9619ec1d":"sns.countplot(x = \"Specialization\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","a6b0f167":"sns.countplot(x = \"What is your current occupation\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# From here we can see that most of the people are unemployed and have a good conversion rate, but in case of \n## working professionsals, we can see that the number of conversions is more than not converted","e6a956b7":"sns.countplot(x = \"Search\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","dc9dc2c7":"# Since there are no values for Yes we can delete this column as well. As it will not add up to the model\n\ndf = df.drop('Search',axis=1)","6fa59e73":"sns.countplot(x = \"Magazine\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","6692f98b":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\ndf = df.drop('Magazine',axis=1)","a1555e38":"sns.countplot(x = \"Newspaper Article\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","8fa13d14":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\ndf = df.drop('Newspaper Article',axis=1)","7e520bb8":"sns.countplot(x = \"X Education Forums\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","e4f6aed9":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\ndf = df.drop('X Education Forums',axis=1)","1ee77366":"sns.countplot(x = \"Newspaper\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","e484a2df":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\ndf = df.drop('Newspaper',axis=1)","fe99796b":"sns.countplot(x = \"Digital Advertisement\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","5c554a53":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\ndf = df.drop('Digital Advertisement',axis=1)","4ad32248":"sns.countplot(x = \"Through Recommendations\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","0a8f26a7":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\ndf = df.drop('Through Recommendations',axis=1)","5f011f0d":"sns.countplot(x = \"Receive More Updates About Our Courses\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","dfde66d9":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\ndf = df.drop('Receive More Updates About Our Courses',axis=1)","29cfa161":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Tags\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","f348ca62":"# We can club calues which have no or very less data\n\ndf['Tags'] = df['Tags'].replace(['Still Thinking','Lost to Others',\n                                                   'Shall take in the next coming month','Lateral student','Interested in Next batch','in touch with EINS','In confusion whether part time or DLP', 'Recognition issue (DEC approval)','Want to take admission but has financial problems','University not recognized','opp hangup','number not provided'], 'Others')","13d50263":"plt.figure(figsize=(10,5))\nsns.countplot(x = \"Tags\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","14152fdd":"sns.countplot(x = \"Lead Quality\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","5ca344a9":"sns.countplot(x = \"Update me on Supply Chain Content\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","510b3d2c":"# We can drop this variable since there is no data for no\n\ndf = df.drop('Update me on Supply Chain Content',axis=1)","166b528c":"sns.countplot(x = \"Get updates on DM Content\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","dadc5b05":"# We can drop this variable since there is no data for Get updates on DM Content\n\ndf = df.drop('Get updates on DM Content',axis=1)","7f31f84e":"sns.countplot(x = \"City\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# Most of the conversions happened from Mumbai","31d24cca":"sns.countplot(x = \"Asymmetrique Activity Index\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# Medium activity has most conversions","ab0f1ca8":"sns.countplot(x = \"I agree to pay the amount through cheque\", hue = \"Converted\", data = df)\nxticks(rotation = 90)","33714a5d":"# We can drop this variable since there is no data for I agree to pay through cheque\n\ndf = df.drop('I agree to pay the amount through cheque',axis=1)","3eb362dc":"sns.countplot(x = \"A free copy of Mastering The Interview\", hue = \"Converted\", data = df)\nxticks(rotation = 90)\n\n# Highest conversion with value no","2d2b64b6":"df.shape","3b07f807":"df.columns","fd44a449":"df.head()","411d28b7":"# List of variables to map which are in the form of Yes\/No\n\nvarlist =  ['Do Not Email', 'A free copy of Mastering The Interview']\n\n# Defining the map function\ndef mapping(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\ndf[varlist] = df[varlist].apply(mapping)","29c2c421":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df[['Lead Origin', 'Lead Source',\n        'Last Activity', 'Specialization', 'What is your current occupation', 'Tags',\n       'Lead Quality', 'City', 'Asymmetrique Activity Index',\n       'A free copy of Mastering The Interview', 'Last Notable Activity']], drop_first=True)\ndummy1.head()","766d6dd0":"# Adding the results to the original dataset\ndf = pd.concat([df, dummy1], axis=1)\ndf.head()","0b726036":"df = df.drop(['Lead Origin', 'Lead Source',\n        'Last Activity', 'Specialization', 'What is your current occupation', 'Tags',\n       'Lead Quality', 'City', 'Asymmetrique Activity Index',\n       'A free copy of Mastering The Interview', 'Last Notable Activity'],axis=1)","1fa43da0":"df.head()","51772ba0":"df = df.drop('Lead Number',axis=1)\n# Dropping Lead number since both prospect id and Lead Number are unique we can keep just one column","6c3e96de":"df.head()","7af01831":"X = df.drop(['Prospect ID','Converted'], axis=1)","312d5eb7":"X.head()","2ff88ebf":"y = df['Converted']\ny.head()","3dfcb1db":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","1a659257":"scaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","80de2fc8":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(X_train.corr(),annot = True)\nplt.show()","e8190270":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","3ce66966":"logreg = LogisticRegression()","70b27895":"rfe = RFE(logreg, 15)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","7c29cc7d":"rfe.support_","6bb89adb":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","9d5984e2":"col = X_train.columns[rfe.support_]","3f79cd6a":"X_train.columns[~rfe.support_]","0f570813":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","b22dae58":"col = col.drop('Tags_invalid number', 1)","e72c2f1f":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","53d2be59":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","f775af70":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","a406351c":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","0bea66fb":"# Creating a column \"predicted\" will be 1 if prob is > than 0.5\ny_train_pred_final['predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","64847b4e":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","c21f7bd5":"# Overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","5376cb43":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","09f065e9":"TP = confusion[1,1] \nTN = confusion[0,0] \nFP = confusion[0,1] \nFN = confusion[1,0] ","4f93f924":"# Sensitivity\nTP \/ float(TP+FN)","65b92a85":"# Specificity\nTN \/ float(TN+FP)","74d4e8e4":"# False postive rate\nprint(FP\/ float(TN+FP))","030bf396":"# Positive Predictive Value \nprint (TP \/ float(TP+FP))","3a01996b":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","ee1a1fe8":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","a427e28d":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_prob, drop_intermediate = False )","fa4a277c":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","73e67148":"# Different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","d63ea466":"cutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","8afb7427":"# Plot for 'accuracy','sensitivity','specificity'against Probability\ncutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\nplt.show()","7bef0ef7":"### We can clearly see that 0.2 comes out to be an optimal cut off point in this case\n\ny_train_pred_final['final_predicted'] = y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.2 else 0)\n\ny_train_pred_final.head()","569abc87":"y_train_pred_final['Lead Score'] = y_train_pred_final.Converted_prob.map( lambda x: round(x*100))\n\ny_train_pred_final.head()","f7bf9dcb":"# Overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\n\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2\n\nTP = confusion2[1,1]\nTN = confusion2[0,0] \nFP = confusion2[0,1] \nFN = confusion2[1,0] ","897719cf":"# Sensitivity\nTP \/ float(TP+FN)","c5be532d":"# Specificity\nTN \/ float(TN+FP)","ce224571":"# False Postive Rate\nprint(FP\/ float(TN+FP))","2e94c3e4":"# Positive predictive value \nprint (TP \/ float(TP+FP))","52e7920b":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","0633bea1":"# Confusion matrix\n\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nconfusion","e4a896b5":"# Precision\nTP \/ TP + FP\n\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","52d6d79f":"# Recall\nTP \/ TP + FN\n\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","2a38fc2f":"precision_score(y_train_pred_final.Converted , y_train_pred_final.predicted)","ef11698f":"recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)","dc003236":"y_train_pred_final.Converted, y_train_pred_final.predicted","07d12d34":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","527a3704":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","4bc09a1a":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","d489126b":"X_test = X_test[col]\nX_test.head()","511bdfab":"X_test_sm = sm.add_constant(X_test)","c2f9c929":"y_test_pred = res.predict(X_test_sm)","cb89318e":"y_test_pred[:10]","471ae378":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","095f7e29":"# Let's see the head\ny_pred_1.head()","47fb2e73":"y_test_df = pd.DataFrame(y_test)","ae3c04e5":"# Prospect ID to index\ny_test_df['Prospect ID'] = y_test_df.index","7fcf98f0":"y_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","2c2460c8":"y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","be7f33bf":"y_pred_final.head()","ca4f6045":"y_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_prob'})","3fb3eb3b":"y_pred_final.head()","0722519d":"y_pred_final['final_predicted'] = y_pred_final.Converted_prob.map(lambda x: 1 if x > 0.2 else 0)","ca579c8f":"y_pred_final.head()","a7d560fe":"# Overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","55387117":"confusion_matrix1 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion_matrix1","d01c60d1":"TP = confusion2[1,1] \nTN = confusion2[0,0] \nFP = confusion2[0,1] \nFN = confusion2[1,0]","183c21a5":"# Sensitivity\nTP \/ float(TP+FN)","a95e04bb":"# Specificity\nTN \/ float(TN+FP)","1b42c7d6":"### Assigning Lead Score\n\nWe are done with the major part of the analysis and prediction now we can finally create a Lead Score column and assign their values","5624d033":"### Univariate Analysis","e98409ae":"### Plotting the ROC Curve","fd7cc125":"### Importing all the relevant libraries","dcf5c6b7":"### Missing Values Treatment","e0e8843d":"### Precision and Recall\n","24e0db3d":"### Checking VIF Values for different features","ac7a545c":"### Performing Test-Train Split","20fa8978":"##### VIF value for all the features is below the threshold","e0dc7ead":"### Checking for null values","81e2cb58":"### First Model Using all Columns","d15a8466":"### Dummy Variable Creation","a7503393":"The dataset has a conversion rate of 38.53%","73911405":"#### Prediction for Test Dataset","fa800d32":"### Feature Scaling","688cf634":"### Optimal Cut off Point","2423fc40":"# Lead Scoring Case Study\n\n### Problem Statement\n\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. \n\nAs you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\n\n### Goals of the Case Study\n\nThere are quite a few goals for this case study.\n\n- Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\n- There are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations.","16ace299":"### Missing Values Treatment","63dc2a27":"### Conversion Ratio:","6079b654":"#### Now only relevant columns are selected and there are no null values or outliers in the data, we can proceed with Dummy variable creation","40867b6e":"### Feature selection using RFE","9bcbf4b2":"### Metrics apart from accuracy"}}