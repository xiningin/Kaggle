{"cell_type":{"c77a7a22":"code","72ba9283":"code","83f43f32":"code","73e4654b":"code","6c246e52":"code","aca89d82":"code","cc1eb7de":"code","e93701ea":"code","3cb3f0d2":"code","0730e30d":"code","a55b2978":"code","d9486c09":"code","11b46f91":"code","2d566a2f":"code","67142b98":"code","8c7d5bde":"code","c875ccec":"code","ef87f226":"code","d5255c30":"code","91fdb719":"code","911f3c0e":"code","44b11b38":"code","f1095009":"code","8bafe121":"code","0500a077":"code","c5a6d1e4":"code","459269d3":"code","01860899":"code","a7b7d3b2":"code","dd3aab07":"code","e22cd7a9":"code","b9255def":"code","b8dd2333":"code","9aa66c47":"code","83d387d8":"code","a1b99c9c":"code","3a8f3115":"code","3772c4bb":"code","fb87a2a1":"code","c7b60e9b":"code","788f6467":"code","f708e6ae":"code","e1d46db4":"code","9aa39975":"code","1fe44904":"code","31792d1a":"code","d5e04778":"code","321c41e8":"markdown","1266095a":"markdown","fb380f7b":"markdown","880d0668":"markdown","2e63cb58":"markdown","470cab21":"markdown","37464ed7":"markdown","f429201f":"markdown","098e508e":"markdown","b0d7433a":"markdown","27983c61":"markdown","454dd5aa":"markdown","ee482cf9":"markdown","5a3682bf":"markdown","09650d09":"markdown","d6b1bf27":"markdown","5c3cd74c":"markdown","e9a03c50":"markdown","3bc91672":"markdown","bff04d2d":"markdown","53753d60":"markdown","db8f2126":"markdown","0892def6":"markdown","c6a4236e":"markdown","c3649302":"markdown","42fca67f":"markdown","4376c6cb":"markdown","f38d67c2":"markdown","b61ba0fa":"markdown","dab4790e":"markdown","a7f07e00":"markdown","7dbc2918":"markdown","0c1e7ad7":"markdown","fd2618ba":"markdown","1a6c8544":"markdown","16c9e657":"markdown","fad72b86":"markdown","4fce4fa1":"markdown","b5ec8e8d":"markdown","dcf35597":"markdown"},"source":{"c77a7a22":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","72ba9283":"employee_survey = pd.read_csv(\"..\/input\/hr-analytics-case-study\/employee_survey_data.csv\")\nmanager_survey = pd.read_csv(\"..\/input\/hr-analytics-case-study\/manager_survey_data.csv\")\ngeneral_data = pd.read_csv(\"..\/input\/hr-analytics-case-study\/general_data.csv\")","83f43f32":"print(employee_survey.columns)\nprint(manager_survey.columns)\nprint(general_data.columns)","73e4654b":"from functools import reduce\ndf_list = [employee_survey, manager_survey, general_data]\nemp_df = reduce(lambda left,right: pd.merge(left,right,how='inner',on='EmployeeID'), df_list)\nemp_df.columns","6c246e52":"emp_df.shape","aca89d82":"emp_df.info()","cc1eb7de":"emp_df.describe()","e93701ea":"print(emp_df['Over18'].unique())\nprint(emp_df['EmployeeCount'].unique())\nprint(emp_df['StandardHours'].unique())","3cb3f0d2":"# This function takes the dataframe and list of features to be dropped\n# returns the updated dataframe\n\ndef drop_features(df, feat_list):\n    for col in feat_list:\n        if col in df.columns:\n            df.drop(col, axis=1, inplace=True)\n            print(f\"{col} is dropped\")\n        else:\n            print(f\"{col} is already dropped\")","0730e30d":"drop_features(emp_df, ['EmployeeID', 'EmployeeCount', 'Over18', 'StandardHours'])","a55b2978":"print(\"The categorical columns and their index-\")\nfor col in emp_df.columns:\n    if emp_df[col].dtype == 'object':\n        print(col, emp_df.columns.get_loc(col))","d9486c09":"def show_percentage_of_people_left(column_name):\n    df = emp_df.groupby(column_name)['Attrition'].describe()\n    df['percentage of people left'] = (1 - (df['freq']\/df['count']))*100\n    print(df)\n    print('===============================')","11b46f91":"for col in ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']:\n    show_percentage_of_people_left(col)","2d566a2f":"sns.countplot(x='Attrition', hue='BusinessTravel', data=emp_df)","67142b98":"sns.countplot(x='Attrition', hue='Department', data=emp_df)","8c7d5bde":"sns.countplot(x='Attrition', hue='EducationField', data=emp_df)","c875ccec":"sns.countplot(x='Attrition', hue='Gender', data=emp_df)","ef87f226":"sns.countplot(x='Attrition', hue='JobRole', data=emp_df)","d5255c30":"sns.countplot(x='Attrition', hue='MaritalStatus', data=emp_df)","91fdb719":"emp_df['Single'] = pd.get_dummies(emp_df[\"MaritalStatus\"])['Single']","911f3c0e":"pd.set_option('mode.chained_assignment', None)\n\nemp_df['RD'] = np.zeros(emp_df.shape[0])\nemp_df['LT_RS_SE'] = np.zeros(emp_df.shape[0])\n\nfor row_num in range(0, emp_df.shape[0]):\n    if emp_df['JobRole'][row_num] == 'Research Director':\n        emp_df['RD'][row_num] = 1\n    if emp_df['JobRole'][row_num] in ['Laboratory Technician', 'Research Scientist', 'Sales Executive']:\n        emp_df['LT_RS_SE'][row_num] = 1","44b11b38":"emp_df['Male'] = pd.get_dummies(emp_df[\"Gender\"])[\"Male\"]\n\n# In EducationField\n# HR : Avg Attrition Rate 40 %\n# Others : Avg Attrition Rate 14 %\nemp_df[\"EducationField_HR\"] = pd.get_dummies(emp_df[\"EducationField\"], prefix='EducationField')[\"EducationField_Human Resources\"]\n\n# In Department\n# HR : Avg Attrition Rate 30 %\n# Others : Avg Attrition Rate 15 %\n\nemp_df[\"Department_HR\"] = pd.get_dummies(emp_df[\"Department\"], prefix='Department')[\"Department_Human Resources\"]\n\nemp_df[\"Travel_Frequently\"] = pd.get_dummies(emp_df[\"BusinessTravel\"])[\"Travel_Frequently\"]\nemp_df[\"Travel_Rarely\"] = pd.get_dummies(emp_df[\"BusinessTravel\"])[\"Travel_Rarely\"]\n\nemp_df[\"Attrition_Yes\"] = pd.get_dummies(emp_df[\"Attrition\"], prefix='Attrition')[\"Attrition_Yes\"]","f1095009":"drop_features(emp_df, ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Attrition'])","8bafe121":"emp_df.shape","0500a077":"emp_df.info()","c5a6d1e4":"plt.figure(figsize=(24,10))\nsns.heatmap(emp_df.corr(), annot=True)","459269d3":"drop_features(emp_df, [\"DistanceFromHome\", \"StockOptionLevel\", \"PercentSalaryHike\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\", \"TotalWorkingYears\"])","01860899":"emp_df.shape","a7b7d3b2":"emp_df.columns","dd3aab07":"sns.jointplot(emp_df['Age'], emp_df['NumCompaniesWorked'], data=emp_df, kind='kde')","e22cd7a9":"sns.jointplot(emp_df['Age'], emp_df['NumCompaniesWorked'], data=emp_df, kind='hex')","b9255def":"sns.jointplot(emp_df['JobLevel'], emp_df['NumCompaniesWorked'], data=emp_df, kind='kde')","b8dd2333":"sns.countplot(emp_df['EnvironmentSatisfaction'], hue=emp_df['Attrition_Yes'], data=emp_df)","9aa66c47":"sns.countplot(emp_df['JobSatisfaction'], hue=emp_df['Attrition_Yes'], data=emp_df)","83d387d8":"sns.countplot(emp_df['WorkLifeBalance'], hue=emp_df['Attrition_Yes'], data=emp_df)","a1b99c9c":"plt.figure(figsize=(18,10))\nsns.countplot(emp_df['Age'], hue=emp_df['Attrition_Yes'], data=emp_df)","3a8f3115":"X = emp_df.iloc[:, :-1].values\ny = emp_df.iloc[:, -1].values","3772c4bb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=47)","fb87a2a1":"plt.figure(figsize=(24,10))\nsns.heatmap(emp_df.isnull())","c7b60e9b":"for col in ['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'NumCompaniesWorked']:\n  print(\"Column : \", col)\n  print(\"Mean : \", emp_df[col].mean())\n  print(\"Mode : \", emp_df[col].mode())\n  print(\"Unique values : \", emp_df[col].unique())\n  print(\"Index : \", emp_df.columns.get_loc(col))","788f6467":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer_const = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=3)\n\n# for 'EnvironmentSatisfaction', 'WorkLifeBalance', 'NumCompaniesWorked'\n\nfor col in [0, 2, 9]:\n    imputer.fit(X_train[:, col].reshape(X_train[:, col].shape[0], 1))\n    X_train[:, col] = imputer.transform(X_train[:, col].reshape(X_train[:, col].shape[0], 1))[:, 0]\n    X_test[:, col] = imputer.transform(X_test[:, col].reshape(X_test[:, col].shape[0], 1))[:, 0]\n\n# for 'JobSatisfaction'\ncol = 1\nimputer_const.fit(X_train[:, col].reshape(X_train[:, col].shape[0], 1))\nX_train[:, col] = imputer_const.transform(X_train[:, col].reshape(X_train[:, col].shape[0], 1))[:, 0]\nX_test[:, col] = imputer_const.transform(X_test[:, col].reshape(X_test[:, col].shape[0], 1))[:, 0]","f708e6ae":"for i in [0, 1, 2, 9]:\n    array_sum = np.sum(X_train[:,i])\n    array_has_nan = np.isnan(array_sum)\n    print(array_has_nan)\n    \n    array_sum = np.sum(X_test[:,i])\n    array_has_nan = np.isnan(array_sum)\n    print(array_has_nan)","e1d46db4":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","9aa39975":"from sklearn.metrics import accuracy_score, confusion_matrix","1fe44904":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\n\nmodel.fit(X_train_scaled, y_train)\n\ny_pred = model.predict(X_test_scaled)\nprint(accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","31792d1a":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","d5e04778":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","321c41e8":"# Scaling the data","1266095a":"**Observation** : Human Resource has quite high attrition rate than others","fb380f7b":"# Importing Modules","880d0668":"Depending on the data available for the employees and attrition information (whether the employee has left the company on previous year or not), we need to be able to predict the probability of an employee to stay in the company.\nAlso this data should help us to reduce attrition rate focusing on the right factors.","2e63cb58":"**Encoding \"JobRole\"**\n\n```\n\nLet's divide this into 3 categories-\n1st:\nResearch Director           240      2  No  183                     23.75\n2nd:\nLaboratory Technician       777      2  No  651                   16.2162\nResearch Scientist          876      2  No  717                   18.1507\nSales Executive             978      2  No  813                   16.8712\n3rd:\nHealthcare Representative   393      2  No  336                   14.5038\nHuman Resources             156      2  No  135                   13.4615\nManager                     306      2  No  264                   13.7255\nManufacturing Director      435      2  No  387                   11.0345\nSales Representative        249      2  No  213                   14.4578\n\nColumn Representation-\nRD    LT_RS_SE  \n1     0        - means 1st category (Attrition rate 23%)\n0     1        - means 2nd category (avg. Attrition rate 17%)\n0     0        - means 3rd category (avg. Attrition rate 13%)\n\n```\nUsing the first line of code to avoid the warning as below\n\nC:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nRef link:\n\nhttps:\/\/www.dataquest.io\/blog\/settingwithcopywarning\/\n\nHere it is safe to ignore this error as we do want to update our original dataframe.\n","470cab21":"# Logistic Regression","37464ed7":" **Observation** : Human Resource Department has quite higher attrition rate than others\n","f429201f":"**Imputing missing values with SimpleImputer**\n\n- SimpleImputer takes 2-D numpy array\n```\nX_train[:, 0].reshape(X_train[:, 0].shape[0], 1) is a 2-D array made from X_train[:, 0] which is a 1-D array\n```\n- shape of the 2-D array is (n, 1) and shape of 1-D array is (n,)\n- array.shape[0] = no. of rows\n- array.shape[1] = no. of cols, this gives error for 1-D array\n- Missing value imputation is done after splitting the data in training and test set.\n- Note that fit is done only once with training set, not on complete set to avoid data leakage\n- training and test set both are transformed with the same value(for eg. mean) calculated by fit() method applied on training set\n\n","098e508e":"**Observations :**\n- \"DistanceFromHome\" and \"StockOptionLevel\" are barely correlated with Attrition, hence can be removed\n- \"PercentageSalaryHike\" is highly correlated with \"PerformanceRating\", \"PercentageSalaryHike\" can be removed\n- \"YearsAtCompany\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\", \"TotalWorkingYears\" are highly correlated\n- Removing \"YearsSinceLastPromotion\", \"YearsWithCurrManager\", \"TotalWorkingYears\"\n- Keeping both HR columns (education field and department) as other educational fields\/dept.s won't be considered otherwise\n- Keeping both travel_frequently and travel_rarely, as we will lose non_travel data otherwise","b0d7433a":"# Source of this dataset\n","27983c61":"**Encoding \"MaritalStatus\"**\n\nWe will encode this column \"MaritalStatus\" in a new column named \"Single\"\n\n- Value : 1 means Single, Attrition rate 25% \n- Value : 0 means Married\/Divorced, Attrition rate ~11% on average","454dd5aa":"# Visualizing Categorical Data","ee482cf9":"# Drop Unnecessary Features (Part 1)","5a3682bf":"**Note** :\n\n- It is important to encode all the categorical columns before starting to train the classification models. Otherwise it throws error similar to \"ValueError: could not convert string to float: b\"\n- Reference Link - https:\/\/stackoverflow.com\/questions\/38108832\/passing-categorical-data-to-sklearn-decision-tree#:~:text=question%20is%20misleading.-,As%20it%20stands%2C%20sklearn%20decision%20trees%20do%20not%20handle%20categorical,()%20will%20treat%20as%20numeric.\n\n- This was observed for Logistic Regression, Decision Tree and Random Forest","09650d09":"# Conclusion","d6b1bf27":"# Study Correlation of the features","5c3cd74c":"# Decision Tree Classification\n","e9a03c50":"- Scaling the data is required for Logistic Regression. \n- Decision Tree Classifier and Random Forest do not need Scaled data.\n- Scaling is done only on the independant columns.","3bc91672":"We will merge the 3 datasets on the common column \"EmployeeID\" and work on a single dataset.","bff04d2d":"# Visualizing Numerical Data","53753d60":"**Observation :** Most employees have worked in 1-2 companies, aged between 28-35 (roughly).","db8f2126":"https:\/\/www.kaggle.com\/vjchoudhary7\/hr-analytics-case-study\/","0892def6":"Let's have a quick look at the data we are going to analyze.","c6a4236e":"Our target column is \"Attrition\"\n\nLet's check out all the labeled columns in the dataframe\n- We will list the categorical features\n- get their position\/column index\n- see how target column \"Attrition\" is related with them","c3649302":"Now that all our categorical columns as encoded, we can remove the original columns.","42fca67f":"# Drop Unnecessary Features (Part 3)","4376c6cb":"# Random Forest Classification","f38d67c2":"# Drop Unnecessary Features (Part 2)","b61ba0fa":"By inspecting the features quickly, we can see that we can get rid of the following features for the mentioned reasons\n\n*   EmployeeID - is a unique ID, Attrition rate does not depend on this\n*   Over18 - has a single value for all columns (i.e. Y)\n*   EmployeeCount - has a single value for all columns (i.e. 1)\n*   StandardHours - has a single value for all columns (i.e. 8)\n\n","dab4790e":"# Loading the data","a7f07e00":"# Splitting the data for training and testing","7dbc2918":"Verifying if missing values are correctlty filled up.","0c1e7ad7":"**Observation** : Employees with poor work-life balance are more likely to leave","fd2618ba":"# Encoding Categorical Data","1a6c8544":"# Handling Missing Values","16c9e657":"- We got 84.5 % accuracy with our Logistic Regression Model\n- Decision Tree Classifier predicted result with 98.8% accuracy.\n- We have been able to achieve maximum of 99.4% accuracy with Random Forest Classifier!\n\n","fad72b86":"**Importing Modules for Performance Evaluation** ","4fce4fa1":"**Comment **: \n- We will impute missing values of 'EnvironmentSatisfaction', 'WorkLifeBalance', 'NumCompaniesWorked' with most frequent values \n- We will impute missing values of 'JobSatisfaction' with const. 3\n\n- This features are not continuous, hence chose whole number(mode) than fraction(mean)","b5ec8e8d":"# About the data\n","dcf35597":"Let's visualize the categorical data in plots as well.\n\n- For working with the categorical data, we need to encode them.\nOnehotencoding will increase the number of features dramatically \n(for e.g. for the column 'JobRole', it will add 8 columns!)\n\n- Understanding the trend of Attrition depending on the various labels would help us to bind similar labels together \nand thus reduce the column numbers.\n\n- We will also reduce a column after the encoding to avoid dummy variable trap"}}