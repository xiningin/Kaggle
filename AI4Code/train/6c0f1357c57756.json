{"cell_type":{"8034264e":"code","8700b0b0":"markdown"},"source":{"8034264e":"#!\/usr\/bin\/python3\n# coding=utf-8\n#===========================================================================\n# This is a minimal script to perform a regression on the kaggle \n# 'House Prices' data set using CatBoost\n# Carl McBride Ellis (21.V.2020)\n#===========================================================================\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\n\n#===========================================================================\n# read in the data from your local directory\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#===========================================================================\n# my selection of features of interest (\"ay, there's the rub\", Shakespeare)\n#===========================================================================\nfeatures = ['OverallQual' , 'GrLivArea' , 'TotalBsmtSF' , 'BsmtFinSF1' ,\n            '2ndFlrSF'    , 'GarageArea', '1stFlrSF'    , 'YearBuilt'  ]\n\n#===========================================================================\n#===========================================================================\nX_train       = train_data[features]\ny_train       = train_data[\"SalePrice\"]\nfinal_X_test  = test_data[features]\n\n#===========================================================================\n# essential preprocessing: imputation; substitute any 'NaN' with mean value\n#===========================================================================\nX_train      = X_train.fillna(X_train.mean())\nfinal_X_test = final_X_test.fillna(final_X_test.mean())\n\n#===========================================================================\n# perform the regression \n#===========================================================================\nfrom catboost import CatBoostRegressor\nregressor = CatBoostRegressor(loss_function='RMSE', verbose=False)\nregressor.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict the prices for the test data\n#===========================================================================\npredictions = regressor.predict(final_X_test)\n\n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"Id\":test_data.Id, \"SalePrice\":predictions})\noutput.to_csv('submission.csv', index=False)","8700b0b0":"This is *yet another* CatBoost scirpt, and yet another of my *minimalist* scripts. (I write them for my own amusement, but if they turn out to be useful to anybody, then all the better!) For feature selection I have used the top eight features obtained from my scikit-learn [recursive feature elimination](https:\/\/www.kaggle.com\/carlmcbrideellis\/recursive-feature-elimination-hp-v1) script. This script produces virtually the same score as that obtained in the aforementioned script, *i.e.* `0.153xx` (Note: that time I was using the scikit-learn Random Forest Regressor routine).\n###### External link:\n* [CatBoost: a high-performance open source library for gradient boosting on decision trees](https:\/\/catboost.ai\/)"}}