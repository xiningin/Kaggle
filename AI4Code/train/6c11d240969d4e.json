{"cell_type":{"c8bc20d2":"code","8fcf3893":"code","fe736215":"code","6f3f6053":"code","032fe92b":"code","564eeb37":"code","e5058ad8":"code","e49e5924":"code","6e2ea95c":"code","20bccd1c":"code","9a633b2d":"code","91aa736a":"code","d8f5dceb":"code","9e48c92e":"code","385865b1":"code","a33766fa":"code","3c62d30d":"code","40cce0d0":"code","83a1d943":"code","824c1499":"code","8417b7c8":"code","95d10dc3":"markdown","ae186cf0":"markdown","dd7113d8":"markdown","e5857aed":"markdown","f51e37ab":"markdown"},"source":{"c8bc20d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os","8fcf3893":"!ls \/kaggle\/input\/devanagari-character-set\/","fe736215":"data_df = pd.read_csv('\/kaggle\/input\/devanagari-character-set\/data.csv')\nprint(data_df.shape)\nprint('2000 examples of each character')\nprint(data_df.head())","6f3f6053":"X_cols = data_df.columns.tolist()\nX_cols.remove('character')\n\nencoder = LabelEncoder()\nY = encoder.fit_transform(data_df.character.values)\nX = data_df[X_cols].astype(np.uint16).values\nX = X\/255\nX = X.reshape((-1,32,32,1))","032fe92b":"plt.imshow(X[100,:,:,0])","564eeb37":"from sklearn.model_selection import train_test_split\nX_train,X_test, Y_train,Y_test = train_test_split(X,Y,stratify=Y,test_size=0.2)","e5058ad8":"from keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPool2D,Dropout,LeakyReLU,Reshape,GaussianNoise\n\ndef get_model(l_relu = 0.3,dropout=0.2):\n    model = Sequential()\n    model.add(GaussianNoise(0.2,input_shape=(32,32,1)))\n    model.add(Conv2D(32,3,padding='same',input_shape=(32,32,1)))\n    model.add(LeakyReLU(l_relu))\n    model.add(MaxPool2D(2))\n\n    model.add(Conv2D(64,3,padding='same'))\n    model.add(LeakyReLU(l_relu))\n    model.add(MaxPool2D(2))\n\n    model.add(Conv2D(128,3,padding='same'))\n    model.add(LeakyReLU(l_relu))\n    model.add(MaxPool2D(2))\n\n    model.add(Conv2D(128,4,padding='valid'))\n    model.add(LeakyReLU(l_relu))\n    model.add(Dropout(dropout))\n    model.add(Conv2D(46,1,activation='softmax'))\n    model.add(Reshape((-1,)))\n    return model\n","e49e5924":"model = get_model()\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","6e2ea95c":"model.summary()","20bccd1c":"print(X_test.shape,Y_test.shape)\nprint(X_train.shape,Y_train.shape)","9a633b2d":"from keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2)\n\ndatagen.fit(X_train)\n\nmodel_file = 'model.pkl'\nckp = ModelCheckpoint(model_file,save_best_only=True)\nbatch_size=32\nepochs=100\nmodel.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n                    validation_data=(X_test,Y_test),\n                    steps_per_epoch=len(X_train) \/ 32, \n                    epochs=epochs,\n                    callbacks=[ckp])\n\nmodel.load_weights(model_file)","91aa736a":"pred_train_probab = model.predict(X_train)\npred_train_Y = np.argmax(pred_train_probab,axis=1)","d8f5dceb":"from sklearn.metrics import confusion_matrix\nconf = confusion_matrix(Y_train,pred_train_Y)\n\nmax_i = None\nmax_j = None\nmax_err = None\n\nfor i in range(conf.shape[0]):\n    for j in range(conf.shape[1]):\n        if i ==j:\n            continue\n        if conf[i,j] >5:\n            labels = encoder.inverse_transform([i,j])\n            if max_err is None or max_err < conf[i,j]:\n                max_i = i\n                max_j = j\n                max_err = conf[i,j]\n            print(labels[0],': perceived as :', labels[1], conf[i,j],'many times')\n\n","9e48c92e":"labels = encoder.inverse_transform([max_i,max_j])\nprint(f'\"{labels[0]}\" perceived as \"{labels[1]}\"', conf[max_i,max_j],'times')\n_,ax =plt.subplots(ncols=2)\n\ni_idx = int(labels[0].split('_')[1])\nj_idx = int(labels[1].split('_')[1])\nax[0].imshow(X[(i_idx-1)*2000+ 1,:,:,0])\nax[0].set_title('Actual')\n\nax[1].imshow(X[(j_idx-1)*2000+ 1,:,:,0])\n_= ax[1].set_title('Predicted as')","385865b1":"def get_df(Y_act,Y_pred_probab):\n    Y_pred = np.argmax(Y_pred_probab,axis=1)\n    Y_pred_val = np.max(Y_pred_probab,axis=1)\n    correct_pred = Y_act == Y_pred\n    df = pd.DataFrame(np.vstack([Y_act,Y_pred_val,correct_pred]).T,columns=['Y_act','Y_pred_val','correct'])\n    return df","a33766fa":"def plot_probablity_histograms(df,data_type):\n    fig,ax = plt.subplots(ncols=2)\n    fig.suptitle('Probability histogram for correct and wrong predictions for:'+data_type)\n    df[df.correct ==1].Y_pred_val.hist(ax=ax[0])\n    df[df.correct ==0].Y_pred_val.hist(ax=ax[1])\n    ax[0].set_title('Correct Predictions')\n    ax[1].set_title('Wrong Predictions')","3c62d30d":"ts_df = get_df(Y_test,model.predict(X_test))\nplot_probablity_histograms(ts_df,'Test')\n","40cce0d0":"tr_df = get_df(Y_train,pred_train_probab)\nplot_probablity_histograms(tr_df,'Train')\n","83a1d943":"def scatter_plt(df,data_type):\n    X = df['Y_act']\n    Y = df['Y_pred_val']\n    color = ['g' if c ==1 else 'r' for c in df['correct'].values]\n    _,ax= plt.subplots(figsize=(20,8))\n    ax.scatter(X,Y,color=color)\n    ax.set_title(data_type + ':Probablities obtained for different classes. Red is incorrect, Green is correct')","824c1499":"scatter_plt(tr_df,'TRAIN')","8417b7c8":"scatter_plt(ts_df,'TEST')","95d10dc3":"## Determining the probablity thresholds for correct prediction.","ae186cf0":"## Objective\nHere I've created a fully convolutional neural network for devanagri font detection. Data contains numbers and consonants. Purpose is to use this in text dectection when text is aligned.","dd7113d8":"## Confusion matrix","e5857aed":"## Inspect for each class, what is the probablity distribution for correct and incorrect prediction\nIdeally, for correct prediction, probablity should be close to 1. For incorrect prediction, probablity should be lower.","f51e37ab":"## Showing most frequent misclassification"}}