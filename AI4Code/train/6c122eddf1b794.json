{"cell_type":{"8ebe4099":"code","ec02fc9b":"code","296367f3":"code","27846053":"code","576970f7":"code","e9e958e7":"code","288d7b2a":"code","c46e62ab":"code","02f61c63":"code","1ed2e699":"code","8b596e0b":"code","fac0d254":"code","644cc05c":"code","d76ef154":"code","85f61c9e":"code","65fe10d1":"code","3f011592":"code","79890bdd":"code","639d51f9":"code","4ea0645e":"code","38d29f62":"code","399a93af":"code","b4f1a109":"code","55b354d1":"code","9ef530dd":"code","3dcdf2b5":"code","5823e344":"code","37b2ee06":"code","a5ba4cb5":"code","31767ea0":"code","6e2195d9":"code","b0798766":"code","8801ca6e":"code","118e4036":"code","a396d1ba":"code","823fa269":"code","1511d05b":"code","b18504eb":"code","bd1ff8a9":"code","413b0f00":"markdown","70f86df9":"markdown","010c56a8":"markdown","55ddfc3a":"markdown","9ef6ffc8":"markdown","94a7a28c":"markdown","1fb8759d":"markdown","4146a437":"markdown","9ac710de":"markdown","a7969e5e":"markdown","093f781f":"markdown","e0ab5e3b":"markdown","8065a101":"markdown","7ab5cd30":"markdown","a120b972":"markdown","2b45b666":"markdown","ffc6f565":"markdown","d013b156":"markdown"},"source":{"8ebe4099":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","ec02fc9b":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","296367f3":"train.head()","27846053":"test.head()","576970f7":"train.columns","e9e958e7":"train.describe()","288d7b2a":"train['SalePrice'].describe()","c46e62ab":"sns.distplot(train['SalePrice'])\nplt.show();","02f61c63":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","1ed2e699":"sns.scatterplot(x = 'TotalBsmtSF', y='SalePrice', data = train)\nplt.show()","8b596e0b":"sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train)\nplt.show()","fac0d254":"train = train.drop(train[(train['TotalBsmtSF']>5000) & (train['SalePrice']<30000)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","644cc05c":"sns.scatterplot(x = 'TotalBsmtSF', y='SalePrice', data = train)\nplt.show()","d76ef154":"sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train)\nplt.show()","85f61c9e":"sns.scatterplot(x = 'YearBuilt', y = 'SalePrice', data = train)\nplt.show()","65fe10d1":"correlation = train.corr()\nfig, axes = plt.subplots(figsize=(15, 12))\nsns.heatmap(correlation, vmax=.8);","3f011592":"k = 10 #number of variables for heatmap\ncols = correlation.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.2)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","79890bdd":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)","639d51f9":"train.isnull().sum().sum()","4ea0645e":"# pivot table for the missing values\ntotal = train.isnull().sum().sort_values(ascending=False)\npercentage = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percentage], axis=1, keys=['Total', 'Percentage'])\nmissing_data.head(20)","38d29f62":"train = train.drop((missing_data[missing_data['Total'] > 1]).index, 1)\ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)","399a93af":"train.isnull().sum().sum()","b4f1a109":"from scipy.stats import norm\nfrom scipy import stats","55b354d1":"sns.distplot(train['SalePrice'], fit=norm);","9ef530dd":"fig = plt.figure()\nprob = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","3dcdf2b5":"train['SalePrice'] = np.log1p(train['SalePrice'])\nsns.distplot(train['SalePrice'], fit=norm);","5823e344":"fig = plt.figure()\nprob = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","37b2ee06":"train = pd.get_dummies(train)","a5ba4cb5":"train","31767ea0":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error, make_scorer, accuracy_score\nfrom math import sqrt","6e2195d9":"scaler = StandardScaler()\nX = train.drop('SalePrice', axis=1)\ny = train[['SalePrice']]\nX_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=42)\n\nprint (X_train.shape)\nprint (X_test.shape)\nprint (y_train.shape)\nprint (y_test.shape)","b0798766":"l_r = LinearRegression()\nl_r.fit(X_train, y_train)\ny_train_pred = l_r.predict(X_train)\ny_test_pred = l_r.predict(X_test)\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\nrmse_train = np.sqrt(-cross_val_score(l_r, X_train, y_train, scoring = scorer, cv=10))\nrmse_test = np.sqrt(-cross_val_score(l_r, X_test, y_test, scoring = scorer, cv=10))\nprint ('Mean RMSE for training set is',rmse_train.mean())\nprint ('Mean RMSE for the test set is',rmse_test.mean())","8801ca6e":"plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regresion\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","118e4036":"plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regresion\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","a396d1ba":"regr_cv = RidgeCV(alphas=[0.1, 1.0, 8 ,9 ,10.0 ,11 ,12 ,15, 20, 25, 30, 35, 40, 50])\nmodel_cv = regr_cv.fit(X_train, y_train)\nprint ('Best Alpha is', model_cv.alpha_)\ny_train_pred = model_cv.predict(X_train)\ny_test_pred = model_cv.predict(X_test)\nrmse_train = np.sqrt(-cross_val_score(model_cv, X_train, y_train, scoring = scorer, cv=10))\nrmse_test = np.sqrt(-cross_val_score(model_cv, X_test, y_test, scoring = scorer, cv=10))\nprint ('Mean RMSE for training set is',rmse_train.mean())\nprint ('Mean RMSE for the test set is',rmse_test.mean())","823fa269":"plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regresion\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","1511d05b":"plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regresion\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","b18504eb":"test","bd1ff8a9":"# test = pd.get_dummies(test)\n# test.head()\n# predicted_prices = model_cv.predict(test)\n# my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n# my_submission.to_csv('submission.csv', index=False)","413b0f00":"now lets do some scatterplots between theese features","70f86df9":"###### Correlation Matrix","010c56a8":"The SalePrice column is right skewed and Linear model love normal distributed so so we will make it normally distributed","55ddfc3a":"###### Outliers","9ef6ffc8":"### Missing values and Feature Engineering","94a7a28c":"### Predicting test and submiting","1fb8759d":"Now lets check again","4146a437":"Now lets see how is the correlation going with the most important features","9ac710de":"Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.","a7969e5e":"Ok, these are much missing values so it would ber better to remove all columns with missing values as these features are categorical features so it would meaningless if we replaced the missing values with the mean, and I will keep the electrical as it's just 1 column and it  has a little high correlation ","093f781f":"As we can see our target table the SalePrice has little skew to the right , Lets know how exactly is the Skewness","e0ab5e3b":"ok some columns have high correlation between out target value, so we will make a zoomed Heatmap for more Details","8065a101":"## Machine Learning Models","7ab5cd30":"### 2- Ridge Regression","a120b972":"Well it's obvious that these 2 features has strong corelation with sale price but there is some outliers in TotalBsmtSf in around 6000 and in GrLivArea in around 4000 and 5000 ,It would be better if we remove them","2b45b666":"Altough this don't tell us so much but I think that newer house has higher price","ffc6f565":"- The Values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distributionThe values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution that data is considered to be normal if skewness is between \u20102 to +2 and kurtosis is between \u20107 to +7","d013b156":"### 1- Normal Linear Regression"}}