{"cell_type":{"01eb496b":"code","76ecdec0":"code","3aff8c9b":"code","948910b2":"code","c5f04229":"code","31e6f109":"code","8ac036e3":"code","55127c05":"code","a7470aab":"code","2c7129ab":"code","a3732d14":"code","aa42014a":"code","f6a90b0b":"code","8b2ee2aa":"code","c50ffc79":"code","c78ccf86":"code","1d0b4b0f":"code","c8895aff":"code","7e08ee32":"code","4862797d":"code","6be5113a":"code","42446cd7":"code","0aca69cf":"code","654e0c0f":"code","0fe8ef4b":"code","14fc0273":"code","a458a202":"code","91355b9a":"code","a29d2850":"markdown","a072bc45":"markdown","9c3aa139":"markdown","26ee7738":"markdown","8a034e25":"markdown","aad1791b":"markdown","defafbf0":"markdown","0add794c":"markdown","fde9de7f":"markdown"},"source":{"01eb496b":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport sklearn ","76ecdec0":"df = pd.read_csv('..\/input\/heartfailure\/heart.csv')\ndf.head()","3aff8c9b":"numerical_features = df.select_dtypes(include=np.number)\nnumerical_features = numerical_features.drop('HeartDisease',axis=1).columns.tolist()\ncategorial_features = df.select_dtypes(include=object).columns.tolist()\n\ndf['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'M' else 0)\npositives = df[df['HeartDisease'] == 1]\npositives = positives.drop('HeartDisease',axis=1)\npositives.describe()","948910b2":"print(\"Correlations of the target value with the features:\")\ndf.corr()['HeartDisease']","c5f04229":"sns.heatmap(df.corr())","31e6f109":"df[categorial_features].describe()","8ac036e3":"negatives = df[df['HeartDisease'] == 0]\nnegatives = negatives.drop('HeartDisease',axis=1)\nnegatives.describe()","55127c05":"df.describe()","a7470aab":"print(f\"Percentage of males in the dataset is: {sum(df['Sex'])\/df.shape[0]}\")\nprint(f\"Percentag of positive cases: {len(df[df['HeartDisease']==1])\/df.shape[0]}.\")\nelder = df[df['Age'] > 60]\nprint(f\"Percentage of positive cases among elder people: {sum(elder['HeartDisease'])\/elder.shape[0]}%\")\nelder.groupby(df['HeartDisease']).size()","2c7129ab":"sns.boxplot(x='ChestPainType', y='Oldpeak', data=df, hue='HeartDisease')","a3732d14":"sns.boxplot(x='ST_Slope', y='Oldpeak',data=df,hue='HeartDisease')","aa42014a":"sns.boxplot(x='HeartDisease', y='Age',data=df)","f6a90b0b":"sns.boxplot(x='HeartDisease', y='RestingBP',data=df)","8b2ee2aa":"fig,axs =plt.subplots(6,3,figsize=(20,25))\nfor i,col in enumerate(numerical_features):\n    axs[i,0].grid(True)\n    axs[i,1].grid(True)\n    axs[i,2].grid(True)\n\n    axs[i,0].set_title(f'Distribution of {col} among positive patients.')\n    axs[i,0].hist(positives[col])\n    axs[i,1].set_title(f'Distribution of {col} among negative patients.')\n    axs[i,1].hist(negatives[col])\n    axs[i,2].set_title(f'Distribution of {col} among patients.')\n    axs[i,2].hist(positives[col])","c50ffc79":"#plt.scatter(df['RestingBP'],df['FastingBS'])\npositives.plot(kind='scatter',x='RestingBP',y='Oldpeak', title='Accumulation of Oldpeak and RestingBP values for positive patients.')\nnegatives.plot(kind='scatter',x='RestingBP', y='Oldpeak', title='Accumulation of Oldpeak and RestingBP values for negative patients.')","c78ccf86":"sns.scatterplot(x='RestingBP',y='Oldpeak',hue='HeartDisease',data=df)","1d0b4b0f":"sns.scatterplot(x='MaxHR',y='Oldpeak',hue='HeartDisease',data=df)","c8895aff":"from scipy.stats import skew \n\nskews =np.array([[skew(data[col]) for col in numerical_features] for data in [positives,negatives]])\nskews_statistic = pd.DataFrame(data=skews.T, columns=['Positive patients','Negative patients'],index=numerical_features)\nskews_statistic","7e08ee32":"from scipy.stats import kurtosis \n\nkurts = np.array([[kurtosis(data[col]) for col in numerical_features] for data in [positives,negatives]])\nkurtosis_statistic = pd.DataFrame(data=kurts.T, columns=['Positive patients','Negative patients'],index=numerical_features)\nkurtosis_statistic","4862797d":"from sklearn.model_selection import train_test_split\n\ny = df['HeartDisease']\nX = df.drop('HeartDisease',axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\ncategorial_features.remove('Sex')\nnumerical_features.append('Sex')","6be5113a":"from sklearn.pipeline import Pipeline \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import f1_score,accuracy_score\n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.neighbors import KNeighborsClassifier\n\ncategorial_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('scalar',StandardScaler(), numerical_features),\n        ('cat', categorial_transformer, categorial_features)\n    ])\n\nmodels = {'Logistic Regression': LogisticRegression(),\n          'Decision Tree': DecisionTreeClassifier(),\n          'Gaussian Naive Bayes': GaussianNB(), \n          'K-Nearest neighbors': KNeighborsClassifier()}\nclassifiers = []\nmetrics = {'Accuracy':[],'F1':[]}\n\nfor name,model in models.items():\n    clf = Pipeline(steps=[('preprocessor',preprocessor),\n                          (name, model)])\n    clf.fit(X_train,y_train)\n    classifiers.append(clf)\n    metrics['F1'] = f1_score(y_test,clf.predict(X_test)) \n    metrics['Accuracy'] = accuracy_score(y_test, clf.predict(X_test))","42446cd7":"from sklearn.metrics import RocCurveDisplay, plot_roc_curve\n\nfig,ax = plt.subplots()\nfor i,clf in enumerate(classifiers):\n    roc = RocCurveDisplay.from_estimator(clf, X_test,y_test, ax=ax,name=list(models.keys())[i])\n\nplt.plot([0, 1], [0, 1], color='red',lw=2, linestyle='--')\nplt.title('Benchmark of baseline models.')\nplt.show()","0aca69cf":"report = pd.DataFrame(data=metrics,index=list(models.keys()))\nreport","654e0c0f":"from sklearn.metrics import ConfusionMatrixDisplay\n\nfig,axes= plt.subplots(nrows=1,ncols=4,figsize=(10,8))\nfor clf,ax in zip(classifiers,axes.flatten()):\n    display = ConfusionMatrixDisplay.from_predictions(y_test,clf.predict(X_test),cmap='Blues',ax=ax,colorbar=False)\n    ax.set_title(list(dict(clf.named_steps).keys())[1])\n\nplt.tight_layout()\nplt.show()","0fe8ef4b":"lr = classifiers[0]\ncoefs = pd.DataFrame(lr.named_steps['Logistic Regression'].coef_[0], \n                     columns=['Coefficients'],\n                     index=lr[:-1].get_feature_names_out())","14fc0273":"train_preprocessed = pd.DataFrame(\n    lr.named_steps['preprocessor'].transform(X_train),columns=lr[:-1].get_feature_names_out()\n)\n\ncoefs = pd.DataFrame(\n    data=lr.named_steps[\"Logistic Regression\"].coef_[0] * train_preprocessed.std(axis=0),\n    columns=[\"Coefficient importance\"],\n    index=lr[:-1].get_feature_names_out(),\n)\n\ncoefs.plot(kind=\"barh\", figsize=(12, 7))\nplt.title(\"Logistic Regression with normalized coefficients.\")\nplt.axvline(x=0, color=\".5\")\nplt.subplots_adjust(left=0.3)","a458a202":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\n\nclf = Pipeline(steps=[('preprocessor',preprocessor),\n                      ('classifier', LogisticRegression())])\nparams = {\n    'classifier__C':np.linspace(0,5,num=20)\n}\n\n\"\"\"\nclf = Pipeline(steps=[('preprocessor',preprocessor),\n                      ('classifier', ElasticNet())])\n\nparams = {\n    'classifier__l1_ratio':np.linspace(0,1,num=20)\n}\n\"\"\"\n\ncv = GridSearchCV(clf,params)\ncv.fit(X,y)","91355b9a":"plt.plot(list(params.values())[0],cv.cv_results_['mean_test_score'])\nplt.title('Logistic Regression performance with respect to l2 regularization.')","a29d2850":"# Boxplots for categorial data:","a072bc45":"# Observations:\n-  We've learned throughout the analysis of the importance of features as MaxHR and Oldpeak which exhibit high correlation to the target value, wether it be positive or negative correlation, and small variance. To back this observation we note Kurtosis of below 3 for MaxHR indicating it's less likely to produce outliers and a reliable feature. \n- Oldpeak on the other hand suffers from Kurtosis > 3 hintng for possibility of outliers in data. The same problem appears in analysis of skewness; The distribution of oldpeak has positive skewness in both positive and negative patient; meaning it's high value is not indicative of being positive nor negative.","9c3aa139":"# Distributions of numerical features among positives and negative patients:","26ee7738":"## We note a few corrections regarding our observaions:\n- The average age of *whole* patients is 53, with 50 for healthy patients meaning there is no justification to argue for the diease to be common around this age.\n-  The dataset consists of 78% males! There is a huge imbalance in gender among our dataset and unless we use data balance methods we cannot research that avenue. \n-  Regarding positive\/false recordings we have 55% positive cases; somewhat balanced data in that regard. \n-  We notice positive patients tend to have higher Oldpeak by looking at the average values of positives, false and the whole data. \n","8a034e25":"After benchmarking ML models we see that Logistic Regression performs the best on test data, although KNN has more steep ROC curve indicating for maximizing TPR and minimizing FPR. Considering we do a binary classification task it is of no surprise how good LR does. Now we will try and improve our LR model by introducting regularization to it; but first we shall analyze the model's coefficient to try and learn more about the data.","aad1791b":"NOTE: The notebook was written on my local computer with different version of sklearn which apparently breaks part of the blocks due to incompability. ","defafbf0":"# Analysis of third and fourth moments data: Skewness and Kurtosis","0add794c":"# Observations: \n- Average age of positive patients is 55 \n- The common gender in positive patients is male with low variance suggesting it's mostly common on males. \n- Oldpeak has small variance among positive patients too, suggesting correlation between Oldpeak value and being positive. \n- Features with correlationto HeartDisease: MaxHR with negative correlation indicating to look for small features, Oldpeak and FastingBS with positive correlation. \n- The categorial features all have low number of unique values; keep in mind for one-hot encoding for later model preprocessing.","fde9de7f":"# Models benchmark performance \n\n We will train multiple basic machine learning models to evaluate their performance and have as a baseline for later more engineered models."}}