{"cell_type":{"40e2c13f":"code","7ffd6d3e":"code","fa592629":"code","d8a7aee9":"code","42fc5965":"code","cd64e3c0":"code","c6cb3f8b":"code","1336caef":"code","87d678da":"code","35c25656":"code","6c27703e":"code","aa7ece7e":"code","7f2f8d05":"code","749ec963":"code","c3a58fb4":"code","50697780":"code","0e5cf27b":"code","34d1d1dd":"code","189ee9a7":"code","312003ee":"markdown","5ae62d75":"markdown","a5b91ad1":"markdown","86ddcfac":"markdown","0a3d3960":"markdown","2c322dd6":"markdown","3985f957":"markdown","58d2db4c":"markdown","913bd288":"markdown","3d13c35b":"markdown"},"source":{"40e2c13f":"import pandas as pd\nimport numpy as np \nimport os\nimport string\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","7ffd6d3e":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\n\nprint(train.shape)\nprint(test.shape)","fa592629":"Q1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\ntrain  = train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]","d8a7aee9":"target = train['target']\ntrain_id = train['id']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)","42fc5965":"def description(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.iloc[0].values\n    summary['Second Value'] = df.iloc[1].values\n    summary['Third Value'] = df.iloc[2].values\n    return summary\ndescription(train)","cd64e3c0":"# dictionary to map the feature\nbin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\n\n# Maping the category values in our dict\ntrain['bin_3'] = train['bin_3'].map(bin_dict)\ntrain['bin_4'] = train['bin_4'].map(bin_dict)\ntest['bin_3'] = test['bin_3'].map(bin_dict)\ntest['bin_4'] = test['bin_4'].map(bin_dict)","c6cb3f8b":"dummies = pd.concat([train, test], axis=0, sort=False)\nprint(f'Shape before dummy transformation: {dummies.shape}')\ndummies = pd.get_dummies(dummies, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\\\n                          prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\nprint(f'Shape after dummy transformation: {dummies.shape}')\ntrain, test = dummies.iloc[:train.shape[0], :], dummies.iloc[train.shape[0]:, :]\ndel dummies\nprint(train.shape)\nprint(test.shape)","1336caef":"# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)\ndef transformingOrdinalFeatures(df, ord_1, ord_2, ord_3, ord_4):    \n    df.ord_1 = df.ord_1.astype(ord_1)\n    df.ord_2 = df.ord_2.astype(ord_2)\n    df.ord_3 = df.ord_3.astype(ord_3)\n    df.ord_4 = df.ord_4.astype(ord_4)\n\n    # Geting the codes of ordinal categoy's \n    df.ord_1 = df.ord_1.cat.codes\n    df.ord_2 = df.ord_2.cat.codes\n    df.ord_3 = df.ord_3.cat.codes\n    df.ord_4 = df.ord_4.cat.codes\n    \n    return df\ntrain = transformingOrdinalFeatures(train, ord_1, ord_2, ord_3, ord_4)\ntest = transformingOrdinalFeatures(test, ord_1, ord_2, ord_3, ord_4)\n\nprint(train.shape)\nprint(test.shape)","87d678da":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n    return df\n\ntrain = date_cyc_enc(train, 'day', 7)\ntest = date_cyc_enc(test, 'day', 7) \n\ntrain = date_cyc_enc(train, 'month', 12)\ntest = date_cyc_enc(test, 'month', 12)\n\nprint(train.shape)\nprint(test.shape)","35c25656":"import string\n\n# Then encode 'ord_5' using ACSII values\n\n# Option 1: Add up the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_add'] = train['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\ntest['ord_5_oe_add'] = test['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\n# Option 2: Join the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_join'] = train['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\ntest['ord_5_oe_join'] = test['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n\n# Option 3: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately\ntrain['ord_5_oe1'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\ntest['ord_5_oe1'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\n\ntrain['ord_5_oe2'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\ntest['ord_5_oe2'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\n\nfor col in ['ord_5_oe1', 'ord_5_oe2', 'ord_5_oe_add', 'ord_5_oe_join']:\n    train[col]= train[col].astype('float64')\n    test[col]= test[col].astype('float64')\n\nprint(train.shape)\nprint(test.shape)","6c27703e":"from sklearn.preprocessing import LabelEncoder\n\nhigh_card_feats = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor col in high_card_feats:    \n    train[f'hash_{col}'] = train[col].apply( lambda x: hash(str(x)) % 5000 )\n    test[f'hash_{col}'] = test[col].apply( lambda x: hash(str(x)) % 5000 )\n                                               \n    if train[col].dtype == 'object' or test[col].dtype == 'object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[col].values) + list(test[col].values))\n        train[f'le_{col}'] = lbl.transform(list(train[col].values))\n        test[f'le_{col}'] = lbl.transform(list(test[col].values))\n\nprint(train.shape)\nprint(test.shape)","aa7ece7e":"col = high_card_feats + ['ord_5','day', 'month']\n                    # + ['hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9']\n                    # + ['le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9']\ntrain.drop(col, axis=1, inplace=True)\ntest.drop(col, axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)","7f2f8d05":"def get_optimized_column(column):\n    if not np.issubdtype(column.dtypes, np.number):\n        return column\n    integers = [np.int8, np.int16, np.int32, np.int64]\n    floats = [np.float16, np.float32, np.float64]\n    max = column.max()\n    relevant_types = integers if np.issubdtype(column.dtypes, np.integer) else floats\n    for dtype in relevant_types:\n        if dtype(max) == max:\n            return column.astype(dtype)\n    return column\n\n\ndef reduce_mem_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for column in df.columns:\n        df[column] = get_optimized_column(df[column]) \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\ndescription(train)","749ec963":"from catboost import CatBoostClassifier, Pool # https:\/\/github.com\/catboost\/tutorials\/blob\/master\/python_tutorial.ipynb\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.model_selection import train_test_split\n\ndef runCatBoost(train_pool, validate_pool, params):\n    print('Train CatBoost model')    \n    model = CatBoostClassifier(**params)\n    model.fit(train_pool, eval_set=validate_pool, plot=True)\n    return model\n    \ndef predict(model, pool):\n    print('Predict')\n    pred_test_y = model.predict_proba(X_test)[:, 1]\n    return pred_test_y\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train, target,\n    test_size = 0.3,\n    random_state = 42\n)\ncat_features = ['bin_0','bin_1','bin_2','bin_3','bin_4','ord_0','ord_1','ord_2','ord_3','ord_4']\ntrain_pool = Pool(train, target, cat_features = cat_features)\nvalidate_pool = Pool(X_val, y_val, cat_features = cat_features)\ntest_pool = Pool(data=test, cat_features = cat_features)","c3a58fb4":"\nparams = {\n    'iterations': 15000,\n    'learning_rate': 0.1,\n    'eval_metric': 'AUC',\n    'random_seed': 42,\n    'task_type':'GPU',\n    'l2_leaf_reg': 10,\n    'logging_level': 'Silent',\n    'use_best_model': True,\n    'custom_loss': ['AUC'], \n    'metric_period': 50,\n    'bagging_temperature' : 0.2,\n    'l2_leaf_reg': 10\n    }\nmodel1 = runCatBoost(train_pool, validate_pool, params)","50697780":"params.update({\n    'random_seed': 42,\n    'learning_rate': 0.05,\n    'l2_leaf_reg': 5,\n    'depth': 4\n})\n#model2 = runCatBoost(train_pool, validate_pool, params)","0e5cf27b":"params.update({\n    'random_seed': 44,\n    'learning_rate': 0.025,\n    'l2_leaf_reg': 3,\n    'depth': 7\n})\n#model3 = runCatBoost(train_pool, validate_pool, params)","34d1d1dd":"params.update({\n    'iterations': 15000,\n    'random_seed': 45,\n    'learning_rate': 0.0125,\n    'l2_leaf_reg': 10,\n    'depth': 5\n})\n#model4 = runCatBoost(train_pool, validate_pool, params)","189ee9a7":"def submit(model, pool, i):\n    print('Predict ' + str(i))\n    results = model.predict_proba(pool)[:, 1]\n    submission = pd.DataFrame({'id': test_id, 'target': results})\n    submission.to_csv('submission' + str(i) + '.csv', index=False)\nsubmit(model1, test_pool,1)\n#submit(model2, test_pool,2)\n#submit(model3, test_pool,3)\n#submit(model4, test_pool,4)","312003ee":"### Load Data","5ae62d75":"### Encoding High Cardinality Features","a5b91ad1":"### Encoding Date features","86ddcfac":"### Subsets","0a3d3960":"### Importing required libraries","2c322dd6":"# Model Building","3985f957":"### Make submission","58d2db4c":"#  Features Transformation","913bd288":"### Variable Description","3d13c35b":"## Remove the Outliers"}}