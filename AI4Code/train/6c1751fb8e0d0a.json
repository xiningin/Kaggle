{"cell_type":{"b3b19f93":"code","7d602b4a":"code","afc3f34b":"code","8770bc71":"code","51ed921e":"code","c0e47a2d":"code","55117556":"code","1901878c":"code","8fb80d13":"code","5ea14cf4":"code","5e2678f0":"code","e689e9be":"code","e5eabc07":"code","6ac23ce1":"code","3cef15d6":"code","bd29ae41":"code","f6cb3173":"code","4fa05f56":"code","2d4db87d":"code","ed42d79f":"markdown"},"source":{"b3b19f93":"# Importing libraries\n\nimport math\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport re\nimport unidecode\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nimport tensorflow as tf\nfrom transformers import DistilBertTokenizerFast, TFDistilBertModel\nfrom transformers import Trainer, TrainingArguments\nfrom tokenizers import BertWordPieceTokenizer","7d602b4a":"# Defining constants\n\nMax_length = 512\nmodel_name = \"..\/input\/distilbertbaseuncased\"\nBatch_size = 8\nAUTO = tf.data.experimental.AUTOTUNE\n\ntrain_prev_comp = \"..\/input\/toxic-comment\/jigsaw-toxic-comment-train.csv\"\ntest_cur_comp = \"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\"\n\n\ndef seed_everything():\n    np.random.seed(123)\n    random.seed(123)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(123)\n\nseed_everything()","afc3f34b":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:,1]\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n    \n    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","8770bc71":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(length = maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size]\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","51ed921e":"# Function for cleaning comments\n\ndef clean_data(sent):\n    sent = sent.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n    soup = BeautifulSoup(sent, \"html.parser\")\n    sent = soup.get_text(separator=\" \")\n    remove_https = re.sub(r'http\\S+', '', sent)\n    sent = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n    sent = unidecode.unidecode(sent)\n    sent = sent.lower()\n    sent = re.sub(r\"[^a-zA-Z0-9:$-,()%.?!]+\", ' ', sent) \n    sent = re.sub(r\"[:$-,()%.?!]+\", ' ',sent)\n    stoplist = stopwords.words(\"english\")\n    sent = [word for word in word_tokenize(sent) if word not in stoplist]\n    sent = \" \".join(sent)\n    \n    return sent","c0e47a2d":"# Reading train file from previous competition\n\ndf = pd.read_csv(train_prev_comp)\n\n\ndf[\"y\"] = (df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(axis=1) > 0).astype(int)\ndf.drop([\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"], axis=1, inplace = True)\ndf.head()","55117556":"# Seeing that dataset is imbalanced\n\ndf[\"y\"].value_counts()","1901878c":"# Balacing dataset\n\nX = np.array(df[\"comment_text\"].values)\nX = X.reshape(-1,1)\ny = np.array(df[\"y\"].values)\nrus = RandomUnderSampler(random_state=0)\nx, y = rus.fit_resample(X, y)\n\nx = x.flatten()\ndf = pd.DataFrame()\ndf[\"text\"] = x\ndf[\"target\"] = y\n\n\n# Now its balanced\n\ndf[\"target\"].value_counts()","8fb80d13":"# Creating column clean_text for cleaned comments\n\ndf[\"text\"] = df[\"text\"].map(clean_data)\n\nx = list(df[\"text\"])\ny = list(df[\"target\"])\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2)","5ea14cf4":"# Initializing Tokenizer\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\ntokenizer.save_pretrained('.')\n\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","5e2678f0":"# Creating encoding for train and validation\ntrain_encodings = fast_encode(xtrain, fast_tokenizer, maxlen = Max_length)\nval_encodings = fast_encode(xtest, fast_tokenizer, maxlen = Max_length)","e689e9be":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_encodings, ytrain))\n    .repeat()\n    .shuffle(1024)\n    .batch(Batch_size)\n    .prefetch(AUTO)\n)\n\nval_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_encodings, ytest))\n    .repeat()\n    .shuffle(1024)\n    .batch(Batch_size)\n    .prefetch(AUTO)\n)","e5eabc07":"transformer_layer = TFDistilBertModel.from_pretrained(model_name)\n\nmodel = build_model(transformer_layer, max_len = Max_length)\nmodel.summary()","6ac23ce1":"n_steps = len(xtrain)\n\npredictor = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=val_dataset,\n    epochs=3\n)","3cef15d6":"# Initializing Bert Tokenizer and Model\ntokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n\nmodel = RobertaForSequenceClassification.from_pretrained(model_name).to(\"cuda\")","bd29ae41":"# Function to get predicitions\ndef get_prediction(text):\n    \n    text = clean_data(text)\n    inputs = tokenizer(text, truncation = True, padding = True, max_length = Max_length, return_tensors = \"pt\").to(\"cuda\")\n    output = model(**inputs)\n    probs = output[0].softmax(1)\n    return probs[:,1].item()","f6cb3173":"# Reading given test dataset \n# Storing predicted values in score column\n\ntest = pd.read_csv(file_path)\n\ntest[\"score\"] = test[\"text\"].map(get_prediction)","4fa05f56":"# Making submission file\n\nfinal = pd.DataFrame()\nfinal[\"comment_id\"] = test[\"comment_id\"]\nfinal[\"score\"] = test[\"score\"]\nfinal.to_csv(\"submission.csv\", index=False)","2d4db87d":"final.head()","ed42d79f":"### I have first trained DistilBert seperately and then used the trained model to predict the toxicity.\n\n### The notebook for DistilBert is [here](https:\/\/www.kaggle.com\/devkhant24\/distilbert-for-jigsaw-comment) and the trained model can be found [here](https:\/\/www.kaggle.com\/devkhant24\/distilbert-jigsaw-comments)."}}