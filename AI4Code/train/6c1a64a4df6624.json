{"cell_type":{"fabfd610":"code","78a6a8da":"code","d2351c61":"code","fd3dc367":"code","42590ccd":"code","6ce717c7":"code","07d09045":"code","98f84277":"code","9300a7a5":"code","0721645a":"code","9eae88bc":"code","1b832c54":"code","6e9f67a6":"code","2aa76826":"code","94fe8c83":"code","914d99f3":"code","e413f101":"code","817ca211":"code","912d601f":"code","10ebf4e8":"code","9410c108":"code","e8b06373":"markdown","19256841":"markdown","65c3bee4":"markdown","aaa28708":"markdown","3cae4277":"markdown","86015122":"markdown","60451c58":"markdown","e100f9c2":"markdown","09411de2":"markdown","9f41c1eb":"markdown","c2ed8b5e":"markdown","06eb53af":"markdown","32b0b3a5":"markdown","7676109c":"markdown","4d9b053b":"markdown","53e06946":"markdown","ee1f90f2":"markdown","988a445e":"markdown","ff2a7280":"markdown","ee6e6d44":"markdown","7db2ff06":"markdown","112690b5":"markdown","985acffe":"markdown","32147999":"markdown","479a2eaf":"markdown","4160df7f":"markdown","ce492a7c":"markdown","4cd38fba":"markdown","46a332a6":"markdown","302bbb6c":"markdown","91e5566f":"markdown","3b00833b":"markdown","3aaabf4c":"markdown","38ee40ae":"markdown","588361f4":"markdown","5660a920":"markdown","dffa12ad":"markdown","4bcc3651":"markdown","68717069":"markdown","50da7ec6":"markdown","2b9de3ea":"markdown"},"source":{"fabfd610":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Comment this if the data visualisations doesn't work on your side\n%matplotlib inline\n\nplt.style.use('bmh')","78a6a8da":"df = pd.read_csv('..\/input\/train.csv')\ndf.head()","d2351c61":"df.info()","fd3dc367":"# df.count() does not include NaN values\ndf2 = df[[column for column in df if df[column].count() \/ len(df) >= 0.3]]\ndel df2['Id']\nprint(\"List of dropped columns:\", end=\" \")\nfor c in df.columns:\n    if c not in df2.columns:\n        print(c, end=\", \")\nprint('\\n')\ndf = df2","42590ccd":"print(df['SalePrice'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(df['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4});","6ce717c7":"list(set(df.dtypes.tolist()))","07d09045":"df_num = df.select_dtypes(include = ['float64', 'int64'])\ndf_num.head()","98f84277":"df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations","9300a7a5":"df_num_corr = df_num.corr()['SalePrice'][:-1] # -1 because the latest row is SalePrice\ngolden_features_list = df_num_corr[abs(df_num_corr) > 0.5].sort_values(ascending=False)\nprint(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list))","0721645a":"for i in range(0, len(df_num.columns), 5):\n    sns.pairplot(data=df_num,\n                x_vars=df_num.columns[i:i+5],\n                y_vars=['SalePrice'])","9eae88bc":"import operator\n\nindividual_features_df = []\nfor i in range(0, len(df_num.columns) - 1): # -1 because the last column is SalePrice\n    tmpDf = df_num[[df_num.columns[i], 'SalePrice']]\n    tmpDf = tmpDf[tmpDf[df_num.columns[i]] != 0]\n    individual_features_df.append(tmpDf)\n\nall_correlations = {feature.columns[0]: feature.corr()['SalePrice'][0] for feature in individual_features_df}\nall_correlations = sorted(all_correlations.items(), key=operator.itemgetter(1))\nfor (key, value) in all_correlations:\n    print(\"{:>15}: {:>15}\".format(key, value))","1b832c54":"golden_features_list = [key for key, value in all_correlations if abs(value) >= 0.5]\nprint(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list))","6e9f67a6":"corr = df_num.drop('SalePrice', axis=1).corr() # We already examined SalePrice correlations\nplt.figure(figsize=(12, 10))\n\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","2aa76826":"quantitative_features_list = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF',\n    '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n    'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'SalePrice']\ndf_quantitative_values = df[quantitative_features_list]\ndf_quantitative_values.head()","94fe8c83":"features_to_analyse = [x for x in quantitative_features_list if x in golden_features_list]\nfeatures_to_analyse.append('SalePrice')\nfeatures_to_analyse","914d99f3":"fig, ax = plt.subplots(round(len(features_to_analyse) \/ 3), 3, figsize = (18, 12))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(features_to_analyse) - 1:\n        sns.regplot(x=features_to_analyse[i],y='SalePrice', data=df[features_to_analyse], ax=ax)","e413f101":"# quantitative_features_list[:-1] as the last column is SalePrice and we want to keep it\ncategorical_features = [a for a in quantitative_features_list[:-1] + df.columns.tolist() if (a not in quantitative_features_list[:-1]) or (a not in df.columns.tolist())]\ndf_categ = df[categorical_features]\ndf_categ.head()","817ca211":"df_not_num = df_categ.select_dtypes(include = ['O'])\nprint('There is {} non numerical features including:\\n{}'.format(len(df_not_num.columns), df_not_num.columns.tolist()))","912d601f":"plt.figure(figsize = (10, 6))\nax = sns.boxplot(x='BsmtExposure', y='SalePrice', data=df_categ)\nplt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")\nplt.xticks(rotation=45)","10ebf4e8":"plt.figure(figsize = (12, 6))\nax = sns.boxplot(x='SaleCondition', y='SalePrice', data=df_categ)\nplt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")\nplt.xticks(rotation=45)","9410c108":"fig, axes = plt.subplots(round(len(df_not_num.columns) \/ 3), 3, figsize=(12, 30))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_not_num.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(x=df_not_num.columns[i], alpha=0.7, data=df_not_num, ax=ax)\n\nfig.tight_layout()","e8b06373":"A lot of features seems to be correlated between each other but some of them such as `YearBuild`\/`GarageYrBlt` may just indicate a price inflation over the years. As for `1stFlrSF`\/`TotalBsmtSF`, it is normal that the more the 1st floor is large (considering many houses have only 1 floor), the more the total basement will be large.\n\nNow for the ones which are less obvious we can see that:\n- There is a strong negative correlation between `BsmtUnfSF` (Unfinished square feet of basement area) and `BsmtFinSF2` (Type 2 finished square feet). There is a definition of unfinished square feet [here](http:\/\/www.homeadvisor.com\/r\/calculating-square-footage\/) but as for a house of \"Type 2\", I can't tell what it really is.\n- `HalfBath`\/`2ndFlrSF` is interesting and may indicate that people gives an importance of not having to rush downstairs in case of urgently having to go to the bathroom (I'll consider that when I'll buy myself a house uh...)\n\nThere is of course a lot more to discover but I can't really explain the rest of the features except the most obvious ones.","19256841":"<font color='chocolate'>We can conclude that, by essence, some of those features may be combined between each other in order to reduce the number of features (`1stFlrSF`\/`TotalBsmtSF`, `GarageCars`\/`GarageArea`) and others indicates that people expect multiples features to be packaged together.<\/font>","65c3bee4":"We can see that features such as `TotalBsmtSF`, `1stFlrSF`, `GrLivArea` have a big spread but I cannot tell what insights this information gives us","aaa28708":"<font color='chocolate'>We can see that some categories are predominant for some features such as `Utilities`, `Heating`, `GarageCond`, `Functional`... These features may not be relevant for our predictive model<\/font>","3cae4277":"Notes: \n\n- There may be some patterns I wasn't able to identify due to my lack of expertise\n- Some values such as `GarageCars` -> `SalePrice` or `Fireplaces` -> `SalePrice` shows a particular pattern with verticals lines roughly meaning that they are discrete variables with a short range but I don't know if they need some sort of \"special treatment\".","86015122":"Note: Apparently using the log function could also do the job but I have no experience with it","60451c58":"This is the very first data analysis I do on my own. Please take the informations on this notebook with a grain of salt. I'm open to all improvements (even rewording), don't hesitate to leave me a comment or upvote if you found it useful. If I'm completely wrong somewhere or if my findings makes no sense don't hesitate to leave me a comment.\n\nThis work was influenced by some kernels of the same competition as well as the [Stanford: Statistical reasoning MOOC](https:\/\/lagunita.stanford.edu\/courses\/OLI\/StatReasoning\/Open\/info)\n\nThe purpose of this EDA is to find insights which will serve us later in another notebook for Data cleaning\/preparation\/transformation which will ultimately be used into a machine learning algorithm.\nWe will proceed as follow:\n\n<img src=\"http:\/\/sharpsightlabs.com\/wp-content\/uploads\/2016\/05\/1_data-analysis-for-ML_how-we-use-dataAnalysis_2016-05-16.png\" \/>\n\n[Source](http:\/\/sharpsightlabs.com\/blog\/data-analysis-machine-learning-example-1\/)\n\nWhere each steps (Data exploration, Data cleaning, Model building, Presenting results) will belongs to 1 notebook.\nI will write down a lot of details in this notebook (even some which may seems obvious by nature), as a beginner it's important for me to do so.","e100f9c2":"For the preparations lets first import the necessary libraries and load the files needed for our EDA","09411de2":"Some of the features of our dataset are categorical. To separate the categorical from quantitative features lets refer ourselves to the `data_description.txt` file. According to this file we end up with the folowing columns:","9f41c1eb":"# Exploratory data analysis (EDA)","c2ed8b5e":"<font color='chocolate'>Features such as `1stFlrSF`, `TotalBsmtSF`, `LotFrontage`, `GrLiveArea`... seems to share a similar distribution to the one we have with `SalePrice`. Lets see if we can find new clues later.<\/font>","06eb53af":"Perfect, we now have a list of strongly correlated values but this list is incomplete as we know that correlation is affected by outliers. So we could proceed as follow:\n\n- Plot the numerical features and see which ones have very few or explainable outliers\n- Remove the outliers from these features and see which one can have a good correlation without their outliers\n    \nBtw, correlation by itself does not always explain the relationship between data so ploting them could even lead us to new insights and in the same manner, check that our correlated values have a linear relationship to the `SalePrice`. \n\nFor example, relationships such as curvilinear relationship cannot be guessed just by looking at the correlation value so lets take the features we excluded from our correlation table and plot them to see if they show some kind of pattern.","32b0b3a5":"Very interesting! We found another strongly correlated value by cleaning up the data a bit. Now our `golden_features_list` var looks like this:","7676109c":"## Feature to feature relationship","4d9b053b":"<font color='chocolate'>With this information we can see that the prices are skewed right and some outliers lies above ~500,000. We will eventually want to get rid of the them to get a normal distribution of the independent variable (`SalePrice`) for machine learning.<\/font>","53e06946":"We will base this part of the exploration on the [C -> Q chapter of the Standford MOOC](https:\/\/lagunita.stanford.edu\/courses\/OLI\/StatReasoning\/Open\/courseware\/eda_er\/_m3_case_I\/)\n\n\nLets get all the categorical features of our dataset and see if we can find some insight in them.\nInstead of opening back our `data_description.txt` file and checking which data are categorical, lets just remove `quantitative_features_list` from our entire dataframe.","ee1f90f2":"<font color='chocolate'>Looking at these features we can see that a lot of them are of the type `Object(O)`. In our data transformation notebook we could use [Pandas categorical functions](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/categorical.html) (equivalent to R's factor) to shape our data in a way that would be interpretable for our machine learning algorithm. `ExterQual` for instace could be transformed to an ordered categorical object.<\/font>","988a445e":"To do so lets first list all the types of our data from our dataset and take only the numerical ones:","ff2a7280":"## C -> Q (Categorical to Quantitative relationship)","ee6e6d44":"Still, we have a lot of features to analyse here so let's take the *strongly correlated quantitative* features from this dataset and analyse them one by one","7db2ff06":"And don't forget the non-numerical features","112690b5":"Now lets take a look at how the housing price is distributed","985acffe":"Now lets plot them all:","32147999":"And finally lets look at their distribution","479a2eaf":"So now lets remove these `0` values and repeat the process of finding correlated values: ","4160df7f":"<font color='chocolate'> Note: If we take the features we just removed and look at their description in the `data_description.txt` file we can deduct that these features may not be present on all houses (which explains the `NaN` values). In our next Data preparation\/cleaning notebook we could tranform them into categorical dummy values.<\/font>","ce492a7c":"From these informations we can already see that some features won't be relevant in our exploratory analysis as there are too much missing values (such as `Alley` and `PoolQC`). Plus there is so much features to analyse that it may be better to concentrate on the ones which can give us real insights. Let's just remove `Id` and the features with 30% or less `NaN` values.","4cd38fba":"Let's now examine the quantitative features of our dataframe and how they relate to the `SalePrice` which is also quantitative (hence the relation Q -> Q). I will conduct this analysis with the help of the [Q -> Q chapter of the Standford MOOC](https:\/\/lagunita.stanford.edu\/courses\/OLI\/StatReasoning\/Open\/courseware\/eda_er\/_m5_case_III\/)","46a332a6":"## Q -> Q (Quantitative to Quantitative relationship)","302bbb6c":"Trying to plot all the numerical features in a seaborn pairplot will take us too much time and will be hard to interpret. We can try to see if some variables are linked between each other and then explain their relation with common sense.","91e5566f":"<font color='chocolate'>By looking at correlation between numerical values we discovered 11 features which have a strong relationship to a house price. Besides correlation we didn't find any notable pattern on the datas which are not correlated.<\/font>","3b00833b":"## Preparations","3aaabf4c":"Let's look at their distribution.","38ee40ae":"We can clearly identify some relationships. Most of them seems to have a linear relationship with the `SalePrice` and if we look closely at the data we can see that a lot of data points are located on `x = 0` which may indicate the absence of such feature in the house.\n\nTake `OpenPorchSF`, I doubt that all houses have a porch (mine doesn't for instance but I don't lose hope that one day... yeah one day...).","588361f4":"## Numerical data distribution","5660a920":"### Conclusion","dffa12ad":"#### Correlation","4bcc3651":"For this part lets look at the distribution of all of the features by ploting them","68717069":"Now we'll try to find which features are strongly correlated with `SalePrice`. We'll store them in a var called `golden_features_list`. We'll reuse our `df_num` dataset to do so.","50da7ec6":"<font color='chocolate'>We found strongly correlated predictors with `SalePrice`. Later with feature engineering we may add dummy values where value of a given feature > 0 would be 1 (precense of such feature) and 0 would be 0. \n<br \/>For `2ndFlrSF` for example, we could create a dummy value for its precense or non-precense and finally sum it up to `1stFlrSF`.<\/font>","2b9de3ea":"Now lets plot some of them"}}