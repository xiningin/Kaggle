{"cell_type":{"28cc3a2a":"code","623f9310":"code","8f024988":"code","cc46c08f":"code","f6b0ec02":"code","16af522b":"code","a172bcf4":"code","142b15b8":"code","5e1472a0":"code","8e85af27":"code","7bb93b96":"code","739fddc4":"code","1becf5f0":"code","df82736b":"code","9213466e":"code","62b9820f":"code","315895e5":"code","aec01fe8":"code","e5131c94":"code","7975944a":"code","c090141b":"code","3ddf738a":"code","136e7409":"code","735b6f24":"code","f93ded35":"code","b4458edd":"code","4383c0a8":"code","cf571ddf":"code","6c770b1a":"code","7b7cc050":"code","df6beb5c":"code","188950e3":"code","d79b7838":"code","059eefdc":"code","579250a9":"markdown","dc726827":"markdown","957a0320":"markdown","ffdf2914":"markdown","0f5c3711":"markdown","33fefe26":"markdown","9a1839f4":"markdown","0e756935":"markdown","43aaead7":"markdown","b62226f2":"markdown","c2e7840c":"markdown","c5de28c3":"markdown","a21811fd":"markdown","3f032e20":"markdown","c1cd8b80":"markdown","e372ba0e":"markdown","3dda88e5":"markdown","5445403e":"markdown"},"source":{"28cc3a2a":"import pandas as pd\ndf_wine = pd.read_csv(\"..\/input\/winedata\/wine.csv\")\ndf_wine.columns = ['Class label', 'Alcohol','Malic acid', 'Ash','Alcalinity of ash', 'Magnesium','Total phenols', 'Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity', 'Hue','OD280\/OD315 of diluted wines','Proline']","623f9310":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nX,y = df_wine.iloc[:,1:].values, df_wine.iloc[:,0].values\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)\nsc=StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n\n","8f024988":"#finding the eigen values and eigen vector using numpy\nimport numpy as np\ncov_mat = np.cov(X_train_std.T)\neigen_vals, eigen_vacs = np.linalg.eig(cov_mat)\n#Although the numpy.linalg.eig function was designed to decompose nonsymmetric square matrices, you may  nd that it returns complex eigenvalues in certain cases.\n#A related function, numpy.linalg.eigh, has been implemented to decompose Hermetian matrices, which is a numerically more stable approach to work with symmetric matrices such as the covariance matrix; numpy.linalg.eigh always returns real eigenvalues.\neigen_vals\neigen_vacs[:,0]","cc46c08f":"#Using the NumPy cumsum function, we can then calculate the cumulative sum of explained variances, which we will plot via matplotlib's step function:\ntot = sum(eigen_vals)\nvar_exp = [(i\/tot) for i in sorted(eigen_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nimport matplotlib.pyplot as plt\nplt.bar(range(1,14), var_exp, alpha=0.5, align='center', label='individual explained variance')\nplt.step(range(1,14), cum_var_exp, where = 'mid', label='cumulative align center')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.show()","f6b0ec02":"eigen_pairs = [(np.abs(eigen_vals[i] ), eigen_vacs[:,i]) for i in range(len(eigen_vals))]\neigen_pairs.sort(reverse=True)\neigen_pairs","16af522b":"w = np.hstack((eigen_pairs[0][1][:,np.newaxis],\n              eigen_pairs[1][1][:, np.newaxis]))\nprint('Matrix W:\\n',w)","a172bcf4":"X_train_std[0].dot(w)","142b15b8":"X_train_pca = X_train_std.dot(w)","5e1472a0":"colors = ['r', 'b','g']\nmarkers = ['s', 'x','o']\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor l, c, m in zip(np.unique(y_train), colors, markers):\n    ax1.scatter(X_train_pca[y_train == l, 0], X_train_pca[y_train == l,1], c=c, label=l, marker=m)\nplt.xlabel('PC 1')\nplt.ylabel('PC 2')\nplt.legend(loc = 'lower left')\nplt.show()","8e85af27":"from matplotlib.colors import ListedColormap\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl)","7bb93b96":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nlr = LogisticRegression()\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\nlr.fit(X_train_pca,y_train)\nplot_decision_regions(X_train_pca,y_train,classifier=lr)\nplt.show()","739fddc4":"plot_decision_regions(X_test_pca,y_test,classifier=lr)\nplt.show()","1becf5f0":"pca = PCA(n_components = None)\npca.fit_transform(X_train_std)\npca.explained_variance_ratio_","df82736b":"np.set_printoptions(precision=4)\nmean_vecs = []\nfor label in range(1,4):\n    mean_vecs.append(np.mean(X_train_std[y_train == label], axis=0))\n    print(mean_vecs[label-1])","9213466e":"d = 13 # number of feature\nS_W = np.zeros((d,d))\nfor label,mv in zip(range(1,4),mean_vecs):\n    class_scatter = np.zeros((d,d))\n    for row in X_train[y_train == label]:\n        row,mv = row.reshape(d,1), mv.reshape(d,1)\n        class_scatter += (row-mv).dot((row-mv).T)\n    S_W += class_scatter\nprint('Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))","62b9820f":"print('Class label distribution: %s' % np.bincount(y_train)[1:])","315895e5":"d = 13 # number of feature\nS_W = np.zeros((d,d))\nfor label,mv in zip(range(1,4),mean_vecs):\n    class_scatter = np.cov(X_train_std[y_train == label].T)\n    S_W += class_scatter\nprint('Scaled Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))","aec01fe8":"mean_overall = np.mean(X_train_std,axis=0)\nd=13# number of feature\nS_B = np.zeros((d,d))\nfor i,mean_vec in enumerate(mean_vecs):\n    n = X_train[y_train == i+1, :].shape[0]\n    mean_vec = mean_vec.reshape(d,1)\n    mean_overall = mean_overall.reshape(d,1)\n    \nS_B +=n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\nprint('Between-class scatter matrix: %sx%s' % (S_B.shape[0], S_B.shape[1]))\n","e5131c94":"eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n#sorting eigen values in descending order\neigen_pairs = [np.abs(eigen_vals[i], eigen_vecs[:,i]) for i in range(len(eigen_vals))]\neigen_pairs = sorted(eigen_pairs,key=lambda k:k[0], reverse = True)\nfor eigen_vals in eigen_pairs:\n    print(eigen_vals[0])\n","7975944a":"# plot the linear discriminants by decreasing eigenvalues\n# tot = sum(eigen_vals.real)\n# discr = [(i\/tot) for i in sorted(eigen_vals.real, reverse = True)]\n# cum_discr = np.cumsum(discr)\n# plt.bar(range(1,14), discr, alpha = 0.5,align='center', label=\"individual label\")\n# plt.step(range(1,14), cum_discr,where='mid', label=\"cumulative Label\")\n# plt.show()\ntot = sum(eigen_vals.real)\ndiscr = [(i \/ tot) for i in sorted(eigen_vals.real, reverse=True)]\ncum_discr = np.cumsum(discr)\nplt.bar(range(1, 14), discr, alpha=0.5, align='center',label='individual \"discriminability\"')\nplt.step(range(1, 14), cum_discr, where='mid',label='cumulative \"discriminability\"')\nplt.ylabel('\"discriminability\" ratio')\nplt.xlabel('Linear Discriminants')\nplt.ylim([-0.1, 1.1])\nplt.legend(loc='best')\nplt.show()\n#skipping the above implementation as it was getting a bit confusing.","c090141b":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components = 2)\nX_train_lda = lda.fit_transform(X_train_std, y_train)\nlr = LogisticRegression()\nlr.fit(X_train_lda, y_train)\nplot_decision_regions(X_train_lda, y_train, classifier = lr)\nplt.show()","3ddf738a":"X_test_lda = lda.transform(X_test_std)\nplot_decision_regions(X_test_lda, y_test, classifier = lr)\nplt.show()","136e7409":"from scipy.spatial.distance import pdist, squareform\nfrom scipy import exp\nfrom scipy.linalg import eigh\nimport numpy as np\n","735b6f24":"def rbf_kernel_pca(X, gamma, n_components):\n    sq_dists = pdist(X, 'sqeuclidean')\n    mat_sq_dists = squareform(sq_dists)\n    K = exp(-gamma * mat_sq_dists)\n    \n    # Center the kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N,N)) \/ N\n    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n    \n    # Obtaining eigenpairs from the centered kernel matrix\n    # numpy.eigh returns them in sorted order\n    eigvals, eigvecs = eigh(K)\n    \n    # Collect the top k eigenvectors (projected samples)\n    X_pc = np.column_stack((eigvecs[:, -i] for i in range(1, n_components + 1)))\n    return X_pc","f93ded35":"from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100,random_state=123)\nplt.scatter(X[y==0,0],X[y==0,1], color='red', marker='^', alpha=0.5)\nplt.scatter(X[y==1,0],X[y==1,1],color='blue',marker='o',alpha=0.5)\nplt.show()","b4458edd":"scikit_pca = PCA(n_components = 2)\nX_spca = scikit_pca.fit_transform(X)\nfig,ax = plt.subplots(nrows=1, ncols=2, figsize=(7,3))\nax[0].scatter(X_spca[y==0,0], X_spca[y==0,1], color='red', marker='^', alpha=0.5)\nax[0].scatter(X_spca[y==1,0], X_spca[y==1,1], color='blue',marker='o',alpha=0.5)\nax[1].scatter(X_spca[y==0,0], np.zeros((50,1)) + 0.02, color='red',marker='^',alpha=0.5)\nax[1].scatter(X_spca[y==1,0], np.zeros((50,1)) - 0.02, color='blue',marker='o',alpha=0.5)\nplt.show()","4383c0a8":"from matplotlib.ticker import FormatStrFormatter\nX_rkp = rbf_kernel_pca(X, gamma=15, n_components=2)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(7,3))\nax[0].scatter(X_rkp[y==0,0], X_rkp[y==0,1], color='red', marker='^', alpha=0.5)\nax[0].scatter(X_rkp[y==1,0], X_rkp[y==1,1], color='blue',marker='o',alpha=0.5)\nax[1].scatter(X_rkp[y==0,0], np.zeros((50,1)) - 0.02, color = 'red', marker='^', alpha=0.5)\nax[1].scatter(X_rkp[y==1,0], np.zeros((50,1)) + 0.02, color = 'blue', marker='o', alpha=0.5)\nplt.show()","cf571ddf":"from sklearn.datasets import make_circles\nX,y = make_circles(n_samples = 1000, random_state = 123, noise=0.1, factor=0.2)\nplt.scatter(X[y==0,0], X[y==0,1],color='red', marker='^', alpha=0.5)\nplt.scatter(X[y==1,0], X[y==1,1], color='blue', marker='o', alpha=0.5)\nplt.show()","6c770b1a":"scikit_pca = PCA(n_components=2)\nX_spca = scikit_pca.fit_transform(X)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(7,3))\nax[0].scatter(X[y==0,0], X_spca[y==0,1], color='red',marker='^', alpha=0.5)\nax[0].scatter(X[y==1,0], X_spca[y==1,1], color='blue', marker='o', alpha=0.5)\n\nax[1].scatter(X[y==0,0], np.zeros((500,1)) + 0.02, color='red',marker='^', alpha=0.5)\nax[1].scatter(X[y==1,0], np.zeros((500,1)) - 0.02, color='blue', marker='o', alpha=0.5)\nplt.show()","7b7cc050":"X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(7,3))\nax[0].scatter(X_kpca[y==0,0], X_kpca[y==0,1], color='red', marker='^', alpha=0.5)\nax[0].scatter(X_kpca[y==1,0], X_kpca[y==1,1], color='blue',marker='o',alpha=0.5)\nax[1].scatter(X_kpca[y==0,0], np.zeros((500,1)) - 0.02, color = 'red', marker='^', alpha=0.5)\nax[1].scatter(X_kpca[y==1,0], np.zeros((500,1)) + 0.02, color = 'blue', marker='o', alpha=0.5)\nplt.show()","df6beb5c":"from sklearn.decomposition import KernelPCA\nX, y = make_moons(n_samples = 1000, random_state=123)\nscikit_kpca = KernelPCA(n_components = 2, kernel='rbf', gamma=15 )\nX_sci = scikit_kpca.fit_transform(X)\n","188950e3":"plt.scatter(X_sci[y==0,0], X_sci[y==0,1], color='red', marker='^', alpha=0.5)\nplt.scatter(X_sci[y==1,0], X_sci[y==1,1], color='blue', marker='o', alpha=0.5)\nplt.show()","d79b7838":"X,y = make_circles(n_samples = 1000, random_state = 123, noise=0.1, factor=0.2)\nscikit_circle = KernelPCA(n_components=2, kernel='rbf', gamma=15)\nX_cir = scikit_circle.fit_transform(X)\n","059eefdc":"plt.scatter(X_cir[y==0,0], X_cir[y==0,1], color='red', marker='^', alpha=0.5)\nplt.scatter(X_cir[y==1,0], X_cir[y==1,1], color='blue', marker='o', alpha=0.5)\nplt.show()","579250a9":"### Example 1 \u2013 separating half-moon shapes","dc726827":"## Selecting linear discriminants for the new feature subspace","957a0320":"Similarly, we can transform the entire 124\u00d713-dimensional training dataset onto the two principal components by calculating the matrix dot product:\nX\u2032= XW","ffdf2914":"# Kernel principal component analysis in Scikit learn","0f5c3711":"The variance explained ratio of an eigenvalue \u03bbj is simply the fraction of an eigenvalue \u03bbj and the total sum of the eigenvalues:\n\u03bbj \/\u2211 \u03bbj","33fefe26":"within class scatter martix\nSW =\u2211c Si\n    i=1\n    \nThis is calculated by summing up the individual scatter matrices Si of each individual class i :\n\nSi = \u2211c(x\u2212m)(x\u2212m)^T","9a1839f4":"# Principal Component Analysis in Scikit learn\nAlthough the verbose approach in the previous subsection helped us to follow the inner workings of PCA, we will now discuss how to use the PCA class implemented in scikit-learn. PCA is another one of scikit-learn's transformer classes, where we  rst  t the model using the training data before we transform both the training data and the test data using the same model parameters. Now, let's use the PCA from scikit- learn on the Wine training dataset, classify the transformed samples via logistic regression, and visualize the decision regions via the plot_decision_region function that we de ned in Chapter 2, Training Machine Learning Algorithms\nfor Classi cation:","0e756935":"# Implementing LDA via SCIKIT Learn","43aaead7":"If we are interested in the explained variance ratios of the different principal components, we can simply initialize the PCA class with the n_components parameter set to None, so all principal components are kept and the explained variance ratio can then be accessed via the explained_variance_ratio_ attribute:","b62226f2":"### computing between class square matrix","c2e7840c":"visualization the above PCA model","c5de28c3":"Note that we set n_components=None when we initialized the PCA class so that it would return all principal components in sorted order instead of performing a dimensionality reduction.","a21811fd":"Now, let's try out our kernel PCA function rbf_kernel_pca, which we implemented in the previous subsection:","3f032e20":"The assumption that we are making when we are computing the scatter matrices is that the class labels in the training set are uniformly distributed. However, if we print the number of class labels, we see that this assumption is violated:","c1cd8b80":"let's summarize the key steps of the LDA approach:\n1. Standardize the d -dimensional dataset ( d is the number of features).\n2. For each class, compute the d -dimensional mean vector.\n3. Construct the between-class scatter matrix SB and the within-class scatter matrix Sw .\n4. Compute the eigenvectors and corresponding eigenvalues of the matrix S\u22121S .\n5. Choose the k eigenvectors that correspond to the k largest eigenvalues to construct a d \u00d7 k -dimensional transformation matrix W ; the eigenvectors are the columns of this matrix.\n6. Project the samples onto the new feature subspace using the transformation matrix W .\n\nThe assumptions that we make when we are using LDA are that the features are normally distributed and independent of each other.\nAlso, the LDA algorithm assumes that the covariance matrices for the individual classes are identical. However, even if we violate those assumptions to a certain extent, LDA may still work reasonably well in dimensionality reduction and classi cation tasks","e372ba0e":"# Implementing a kernel principal component analysis in Python","3dda88e5":" By executing the preceding code, we have created a 13\u00d72-dimensional projection matrix W from the top two eigenvectors. Using the projection matrix, we can now transform a sample x (represented as 1\u00d713-dimensional row vector) onto the PCA subspace obtaining x\u2032 , a now two-dimensional sample vector consisting of two new features:x\u2032= xW","5445403e":"## Computing the Scatter Matrices"}}