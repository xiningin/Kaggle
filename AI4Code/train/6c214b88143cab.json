{"cell_type":{"59c448ae":"code","1691b585":"code","8fde68e5":"code","b809075f":"code","c1d8a74a":"code","e285e101":"code","59b8f17f":"code","39d42510":"code","610bb138":"code","8a218871":"code","95f9691e":"code","a6428086":"code","2d792aa0":"code","4509c48a":"code","8b9da6ec":"code","c9a0d7d9":"code","b7f9090c":"code","15009243":"code","d3537b76":"code","708e7a5e":"code","74f8a8e3":"code","4ab52f8b":"code","61f335d2":"code","53192ce2":"code","a862f646":"code","f53559eb":"code","cb22c1c3":"markdown","bfedbb8b":"markdown","1e8bd08c":"markdown","3b92d1c6":"markdown","c1a59761":"markdown","600a98e9":"markdown","3377975d":"markdown","805ce04e":"markdown","69fe9e5f":"markdown","284740ae":"markdown","5d139508":"markdown","eebd6cf0":"markdown","e8d9cbc4":"markdown","953efdd7":"markdown"},"source":{"59c448ae":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nimport optuna\n\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1691b585":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\", low_memory=False)#, nrows=10000)\n# train[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\", low_memory=False)\n# test[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","8fde68e5":"test.info(memory_usage=\"deep\")","b809075f":"train.head(10)","c1d8a74a":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","e285e101":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","59b8f17f":"train.describe().T","39d42510":"train.isna().sum().sum(), test.isna().sum().sum()","610bb138":"train[\"loss\"].value_counts()","8a218871":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(train[\"loss\"].value_counts().sort_index().index,\n              train[\"loss\"].value_counts().sort_index().values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"Loss (target) distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Loss (target) value\", fontsize=14, labelpad=10)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"loss\"].value_counts().sort_index().values\/(len(train)\/100)],\n                 padding=5, fontsize=10, rotation=90)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","95f9691e":"df = pd.concat([train.drop([\"id\", \"loss\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","a6428086":"train.nunique().sort_values().head()","2d792aa0":"# Plot dataframe\ndf = train.drop(\"id\", axis=1).corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","4509c48a":"df[(df[\"loss\"]>-0.001) & (df[\"loss\"]<0.001)][\"loss\"]","8b9da6ec":"columns = train.drop([\"id\", \"loss\"], axis=1).columns.values\n\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"loss\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n#plt.suptitle(\"Features vs loss\", y=0.99)\nplt.show();","c9a0d7d9":"# Calculating edges of target bins to be used for stratified split\ntarget_bin_edges = np.histogram_bin_edges(train[\"loss\"], bins=10)\ntarget_bin_edges[0] = -np.inf\ntarget_bin_edges[-1] = np.inf\ntarget_bins = pd.cut(train[\"loss\"], target_bin_edges, labels=np.arange(10))\ntarget_bins.value_counts()","b7f9090c":"# Scaling data\nx_scaler = StandardScaler()\nX = pd.DataFrame(x_scaler.fit_transform(train.drop([\"id\", \"loss\"], axis=1)), columns=train.drop([\"id\", \"loss\"], axis=1).columns)\nX_test = pd.DataFrame(x_scaler.transform(test.drop(\"id\", axis=1)), columns=test.drop([\"id\"], axis=1).columns)\n\ny = train[\"loss\"].copy()","15009243":"X.describe()","d3537b76":"X_test.describe()","708e7a5e":"y.min(), y.max()","74f8a8e3":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n    \n        \n    #A set of hyperparameters to optimize by optuna\n    xgb_params = {\n                 \"n_estimators\": trial.suggest_categorical('n_estimators', [40000]),\n                 \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 1.0, step=0.01),\n                 \"subsample\": trial.suggest_float('subsample', 0.5, 1, step=0.01),\n                 \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.1, 1, step=0.01),\n                 \"max_depth\": trial.suggest_int(\"max_depth\", 1, 16),\n                 \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n                 \"tree_method\": trial.suggest_categorical('tree_method', [\"gpu_hist\"]),\n                 \"reg_lambda\": trial.suggest_float('reg_lambda', 0.2, 100, step=0.1),\n                 \"reg_alpha\": trial.suggest_float('reg_alpha', 0.1, 50, step=0.1),\n                 \"random_state\": trial.suggest_categorical('random_state', [42]),\n                 \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n#                  \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 10, 30),\n                    }\n\n    # Model loading and training\n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    print(f\"Number of boosting rounds: {model.best_iteration}\")\n    oof = model.predict(X_valid)\n    oof[oof<0] = 0\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","4ab52f8b":"# %%time\n# # Splitting data into train and valid folds using target bins for stratification\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n# for train_idx, valid_idx in split.split(X, target_bins):\n#     X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n#     y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n# # Setting optuna verbosity to show only warning messages\n# # If the line is uncommeted each iteration results will be shown\n# # optuna.logging.set_verbosity(optuna.logging.WARNING)\n# time_limit = 3600 * 4\n# study = optuna.create_study(direction='minimize')\n# study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n#                                                     y_train, y_valid),\n# #                n_trials = 100,\n#                timeout=time_limit\n#               )\n\n# # Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","61f335d2":"# Hyperparameters optimized by Optuna\n\nxgb_params = {'n_estimators': 40000,\n              'learning_rate': 0.01,\n              'subsample': 0.72,\n              'colsample_bytree': 0.66,\n              'max_depth': 6,\n              'booster': 'gbtree',\n              'tree_method': 'gpu_hist',\n              'reg_lambda': 68.5,\n              'reg_alpha': 21.5,\n              'random_state': 42,\n              'n_jobs': 4}","53192ce2":"%%time\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, target_bins)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    preds += model.predict(X_test) \/ splits\n    model_fi += model.feature_importances_\n    oof_preds[valid_idx] = model.predict(X_valid)\n    oof_preds[oof_preds < 0] = 0\n#     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n#         print(f\"Trees: {model.tree_count_}\")\n    total_mean_rmse += fold_rmse \/ splits\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")    ","a862f646":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = X.columns\ndf[\"Importance\"] = model_fi \/ model_fi.sum()\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x, df[\"Importance\"], height=height,\n                color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","f53559eb":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"loss\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","cb22c1c3":"# **EDA**","bfedbb8b":"## **Submission**","1e8bd08c":"As you can see, f1 feature has the smallest amount of unique values - 289. So I don't think any feature should be treated as categorical.\n\nLet's look at feature correlation.","3b92d1c6":"## **Data import**","c1a59761":"Let's check feature values distribution in the both datasets.","600a98e9":"The datasets are pretty well balanced.","3377975d":"# **Hyperparameters optimization**","805ce04e":"# **Model training**","69fe9e5f":"There are no missing value in the both datasets.\n\nLet's check target distribution.","284740ae":"The code below is commented in order to save runtime.","5d139508":"Let's visualize each feature vs loss.","eebd6cf0":"# **Data preprocessing**","e8d9cbc4":"As you can see, the correlation is between ~0.03 and ~0.03 which is pretty small. So the features are weakly correlated. \n\nThere are some features with relatively low correlation with target value even comparing with other features:","953efdd7":"## **Feature importances**"}}