{"cell_type":{"507cee30":"code","95a35eac":"code","b4c8e9a0":"code","7e299389":"code","c62ae17a":"code","ca89b8ce":"code","010c8743":"code","4144182e":"markdown","f491a3cf":"markdown","f4dd9fa6":"markdown","3a084779":"markdown","92d8c53f":"markdown"},"source":{"507cee30":"!pip install -qq plotly_express","95a35eac":"%matplotlib inline\nfrom sklearn.datasets import make_blobs, make_moons, make_swiss_roll\nimport plotly_express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nblob_data = make_blobs(n_samples=500, \n                                   cluster_std=2.0,\n                                   centers=4,\n                                   random_state = 2019)[0]\nmoon_data = make_moons(n_samples=500, \n                                   random_state = 2019)[0]\nswiss_data_1 = make_swiss_roll(n_samples=100, noise=0.2, random_state=2019)[0]\nswiss_data_2 = -0.5*make_swiss_roll(n_samples=100, noise=0.1, random_state=2020)[0]\nswiss_data = np.concatenate([swiss_data_1, swiss_data_2], 0)[:, [0, 2]]\ntest_pts = pd.DataFrame(swiss_data, columns=['x', 'y'])\nplt.plot(test_pts.x, test_pts.y, '.')\ntest_pts.sample(5)","b4c8e9a0":"from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=4, random_state = 2018)\nn_grp = km.fit_predict(test_pts)","7e299389":"plt.scatter(test_pts.x, test_pts.y, c = n_grp)\ngrp_pts = test_pts.copy()\ngrp_pts['group'] = n_grp\ngrp_pts.groupby(['group']).apply(lambda x: x.sample(2))","c62ae17a":"def calculate_iter(in_pts, iter_count=1):\n    c_pts = in_pts.copy()\n    c_pts['iter_count'] = iter_count\n    init_clusters = np.array([[-10, -10], [-5, -10], [-10, -5], [-5, -10]])\n    init_clusters = c_pts[['x', 'y']].values[:4]\n    temp_km = KMeans(n_clusters=4, \n                     init=init_clusters,\n                     random_state=2019, \n                     n_init=1, \n                     max_iter=iter_count if iter_count>0 else 1)\n    if iter_count>0:\n        c_pts['group'] = temp_km.fit_predict(c_pts[['x', 'y']])\n    else:\n        temp_km.fit(init_clusters)\n        c_pts['group'] = temp_km.predict(c_pts[['x', 'y']])\n        \n    c_pts['group'] = c_pts['group'].map(str)\n    # calculate the group centers\n    grp_center = c_pts.groupby('group').agg('mean').reset_index()\n    c_pts['point_type'] = 'points'\n    out_df = pd.concat([c_pts.assign(point_type='points'), \n                      grp_center.assign(point_type='centers')], sort=False)\n    out_df['index'] = range(out_df.shape[0])\n    return out_df","ca89b8ce":"iter_df = pd.concat([calculate_iter(test_pts, i) for i in range(0, 30)]).reset_index(drop=True)\niter_df.head(5)","010c8743":"iter_df['point_size'] = iter_df['point_type'].map(lambda x: 10 if x=='centers' else 1)\npx.scatter(iter_df, \n           x='x', y='y', \n           symbol='point_type', \n           animation_frame='iter_count', \n           animation_group='index',\n           size='point_size',\n           symbol_sequence=['cross', 'circle', 'diamond', 'square', 'x'],\n           color='group')","4144182e":"# K-Means Clustering \/ Classification (Unsupervised)\n\n- Automatic clustering of multidimensional data into groups based on a distance metric\n- Fast and scalable to petabytes of data (Google, Facebook, Twitter, etc. use it regularly to classify customers, advertisements, queries)\n- __Input__ = feature vectors, distance metric, number of groups\n- __Output__ = a classification for each feature vector to a group","f491a3cf":"## We can now show one iteration at a time\nHere we use the `max_iter` argument to show how the cluster centers and clusters change with each iteration","f4dd9fa6":"# Toy Problem\n- Distance metric \n$$ D_{ij}=||\\vec{v}_i-\\vec{v}_j|| $$\n\n- Group Count ($N=2$)\n\n","3a084779":"## Show the groups","92d8c53f":"## Create and fit the K-Means"}}