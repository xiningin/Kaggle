{"cell_type":{"b35afa6e":"code","edefb495":"code","5539466e":"code","0ce7f134":"code","0f8e9d2e":"code","2cb258b1":"code","a3ba63f7":"code","14cea269":"code","cd1a7153":"code","f1b495b7":"code","2d2c186f":"code","c5142d0d":"code","594c57cb":"code","10b2c796":"code","9a2db0fd":"code","323f766a":"markdown","e90e91df":"markdown","f1cbad62":"markdown","c675ff1a":"markdown"},"source":{"b35afa6e":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout,Dense,Activation,Conv2D,MaxPooling2D,Flatten, BatchNormalization, MaxPool2D","edefb495":"train_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/train\/'\nvalid_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/val\/'\ntest_dir =  '..\/input\/chest-xray-pneumonia\/chest_xray\/test\/'\n\nBATCH_SIZE = 128\nIMAGE_SIZE = 128\nEPOCHS = 50","5539466e":"train_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        shear_range=0.0,\n        zoom_range=0.0,\n        horizontal_flip=True)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n                                                    batch_size=BATCH_SIZE,\n                                                    class_mode='binary',\n                                                    color_mode = 'grayscale')\nvalid_generator = train_datagen.flow_from_directory(valid_dir,\n                                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n                                                    batch_size=BATCH_SIZE,\n                                                    class_mode='binary',\n                                                    color_mode = 'grayscale')\ntest_generator = train_datagen.flow_from_directory(test_dir,\n                                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n                                                    batch_size=BATCH_SIZE,\n                                                    class_mode='binary',\n                                                   color_mode = 'grayscale')\n\nX, Y = train_generator.__getitem__(0)\nrows = int(np.sqrt(X.shape[0]))\ncols = int(np.sqrt(X.shape[0]))\n\nfig = plt.figure(figsize=(18,12))\nfor i in range(1, rows*cols+1):\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.squeeze(X[i-1]), cmap='gray')\n    plt.title(Y[i-1], fontsize=16)\n    plt.axis(False)\n    fig.add_subplot","0ce7f134":"import cv2, os\n\nlabels = [\"NORMAL\", \"PNEUMONIA\"] # each folder has two sub folder name \"PNEUMONIA\", \"NORMAL\"\n\ndef get_data_train(data_dir):\n    data = []\n    for label in labels:\n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                new_array = cv2.resize(img_array, (IMAGE_SIZE, IMAGE_SIZE))\n                data.append([new_array, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)\n\ntrain = get_data_train(train_dir)\ntest = get_data_train(test_dir)\nval = get_data_train(valid_dir)\n\n\nX_train = []\ny_train = []\n\nX_val = []\ny_val = []\n\nX_test = []\ny_test = []\n\nfor feature, label in train:\n    X_train.append(feature)\n    y_train.append(label)\n\nfor feature, label in test:\n    X_test.append(feature)\n    y_test.append(label)\n    \nfor feature, label in val:\n    X_val.append(feature)\n    y_val.append(label)\n    \nX_train = np.array(X_train) \/ 255\nX_val = np.array(X_val) \/ 255\nX_test = np.array(X_test) \/ 255\n\n\nX_train = X_train.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\ny_train = np.array(y_train)\n\nX_val = X_val.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\ny_val = np.array(y_val)\n\nX_test = X_test.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\ny_test = np.array(y_test)","0f8e9d2e":"print('Number of Train Images: ', X_train.shape[0])\nprint('Number of Valid Images: ', X_val.shape[0])\nprint('Number of Test Images: ', X_test.shape[0])","2cb258b1":"model = Sequential([Conv2D(32, (3, 3), padding=\"same\", activation = 'relu', input_shape = X_train.shape[1:]), \n                    MaxPooling2D(2,2), \n                    Dropout(0.25), \n                   \n                    Conv2D(64, (3, 3), padding=\"same\", activation = 'relu'),\n                    MaxPooling2D(2,2), \n                    Dropout(0.25), \n                    \n                    Conv2D(128, (3, 3), padding=\"same\", activation = 'relu'),\n                    MaxPooling2D(2,2), \n                    Dropout(0.25),\n                    \n                    Flatten(), \n                    Dense(128, activation=\"relu\"), \n                    Dense(1, activation = 'sigmoid') \n                   ])\nmodel.summary()","a3ba63f7":"def build_lrfn(lr_start=1e-4, lr_max=1e-3, \n               lr_min=0, lr_rampup_epochs=16, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = LearningRateScheduler(lrfn, verbose=True)\n\n# Usually monitor='val_accuracy' should be tracked here. \n# Since the training set is smaller let keep it limited to accuracy\n\ncheckpoint = ModelCheckpoint(\n    filepath='best_weights.hdf5',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","14cea269":"model.compile(optimizer='adam', \n              loss = 'binary_crossentropy',  \n              metrics = ['accuracy'])","cd1a7153":"history = model.fit(X_train, y_train, \n                    epochs=20, \n                    validation_data=(X_val, y_val), \n                    callbacks = [lr_schedule, checkpoint])","f1b495b7":"model.evaluate(X_test, y_test)","2d2c186f":"def visualize_training(history, lw = 3):\n    plt.figure(figsize=(10,10))\n    plt.subplot(2,1,1)\n    plt.plot(history.history['accuracy'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_accuracy'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Accuracy Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.legend(fontsize = 'x-large')\n    \n\n    plt.subplot(2,1,2)\n    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Loss Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(fontsize = 'x-large')\n    plt.grid(True)\n    plt.show()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(history.history['lr'], label = 'lr', marker = '*',linewidth = lw)\n    plt.title('Learning Rate')\n    plt.xlabel('Epochs')\n    plt.ylabel('Learning Rate')\n    plt.grid(True)\n    plt.show()","c5142d0d":"visualize_training(history)","594c57cb":"import seaborn as sns\npreds_prob = model.predict(X_test)\nsns.distplot(preds_prob)","10b2c796":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\npreds = model.predict(X_test)\npreds = np.squeeze((1*(preds > 0.5)))\norig_test_labels = y_test\n\n# Get the confusion matrix\ncm  = confusion_matrix(orig_test_labels, preds)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","9a2db0fd":"# Calculate Precision and Recall\ntn, fp, fn, tp = cm.ravel()\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\nprint(\"Recall of the model is {:.2f}\".format(recall))\nprint(\"Precision of the model is {:.2f}\".format(precision))","323f766a":"history = model.fit_generator(train_generator,\n                              validation_data=test_generator,\n                              steps_per_epoch=len(train_generator), \n                              validation_steps=len(test_generator),\n                              epochs = 2,\n                              callbacks=[checkpoint],\n                              verbose = 1)","e90e91df":"We can just have a look at the probabiliy distribution between **[0, 1]** to understand visual correctness of the model. ","f1cbad62":"# Prvious model\n\n```Python\nmodel  = Sequential([\n    Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = X.shape[-3:]),\n    BatchNormalization(),\n    \n    MaxPool2D((2,2) , strides = 2 , padding = 'valid'),\n    Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n    Dropout(0.2),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'valid'),\n    Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'valid'),\n    Flatten(),\n    Dense(units = 512 , activation = 'relu'),\n    Dropout(0.3),\n    Dense( 1 , activation = 'sigmoid')])\n\nmodel.summary()\n```","c675ff1a":"### Accuracy is not a good metric for unbalanced data\nWhen a particular problem includes an imbalanced dataset, then accuracy isn't a good metric to look for. For example, if your dataset contains 95 negatives and 5 positives, having a model with 95% accuracy doesn't make sense at all. The classifier might label every example as negative and still achieve 95% accuracy. Hence, we need to look for alternative metrics. Precision and Recall are really good metrics for such kind of problems.\n\nWe will get the confusion matrix from our predictions and see what is the recall and precision of our model."}}