{"cell_type":{"05cf444b":"code","95b7fb00":"code","8f331efb":"code","6641d668":"code","31468a86":"code","016bef26":"code","d698888e":"code","352908f9":"code","2255065e":"code","ce7a53e3":"code","a55fe458":"code","e0a49e49":"code","3dc5f1b0":"code","8cd083cd":"code","15bf00c8":"code","f7b999ce":"code","013788de":"code","f6e50c61":"code","f971d619":"markdown","d71f2446":"markdown","5a9d51d5":"markdown","684904da":"markdown","d9d4a64d":"markdown","dc42f8e4":"markdown"},"source":{"05cf444b":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as tfs\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport pickle\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import LeaveOneOut, cross_val_score","95b7fb00":"def show_images(imgs, size=(8, 8)):\n    fig, ax = plt.subplots(figsize=(size[0]*2, size[1]*2))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    nmax = size[0] * size[1]\n    ax.imshow(\n        make_grid(imgs[:nmax], nrow=size[0]).permute(1, 2, 0)\n    )","8f331efb":"def show_history(history):\n    plt.figure(figsize=(20, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1, EPOCHS+1), history[0]['generator'], label='generator')\n    plt.plot(range(1, EPOCHS+1), history[0]['discriminator'], label='discriminator')\n    plt.legend(loc='lower right')\n    plt.title('BCE Loss')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(range(1, EPOCHS+1), history[1]['real'], label='real')\n    plt.plot(range(1, EPOCHS+1), history[1]['fake'], label='fake')\n    plt.legend(loc='upper left')\n    plt.title('Accuracy')\n\n    plt.show()","6641d668":"def generate(model, size):\n    imgs = torch.Tensor()\n    model.eval()\n    with torch.no_grad():\n        while size > 0:\n            latent = torch.randn(\n                min(size, BATCH_SIZE), LATENT_SIZE, 1, 1, device=DEVICE\n            )\n            imgs = torch.cat([imgs, model(latent).cpu()])\n            size = size - BATCH_SIZE\n    return imgs","31468a86":"DATA_DIR = '\/kaggle\/input\/flickr-faces-70k-thumbnails-128x128\/'\nDATA_OUT = '\/kaggle\/input\/gan-flickr-faces-output\/'\n#NORMS = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\nBATCH_SIZE = 128\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","016bef26":"dataset = ImageFolder(DATA_DIR, transform=tfs.Compose([\n    tfs.ToTensor(),\n#    tfs.Normalize(*NORMS),\n]))\n\ndataloader = DataLoader(\n    dataset, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n)","d698888e":"test_imgs, _ = next(iter(dataloader))\nshow_images(test_imgs)","352908f9":"discriminator = nn.Sequential(\n    # in: 3 x 128 x 128\n    nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(32),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 32 x 64 x 64\n\n    nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 64 x 32 x 32\n\n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 128 x 16 x 16\n\n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 256 x 8 x 8\n\n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 512 x 4 x 4\n\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    # out: 1 x 1 x 1\n\n    nn.Flatten(),\n    nn.Sigmoid()\n\n).to(DEVICE)","2255065e":"LATENT_SIZE = 128","ce7a53e3":"generator = nn.Sequential(\n    # in: latent_size x 1 x 1\n\n    nn.ConvTranspose2d(LATENT_SIZE, 512, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # nn.Dropout(0.5),\n    # out: 1024 x 4 x 4\n\n    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # nn.Dropout(0.5),\n    # out: 512 x 8 x 8\n\n    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # nn.Dropout(0.5),\n    # out: 256 x 16 x 16\n\n    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # nn.Dropout(0.5),\n    # out: 128 x 32 x 32\n\n    nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(32),\n    nn.ReLU(True),\n    # nn.Dropout(0.5),\n    # out: 64 x 64 x 64\n\n    # nn.ConvTranspose2d(64, 32, kernel_size=1, stride=1, padding=0, bias=False),\n    # nn.BatchNorm2d(32),\n    # nn.ReLU(True),\n    \n    nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh()\n    # out: 3 x 128 x 128\n\n).to(DEVICE)    ","a55fe458":"LR = 0.0001\n\nmodel = {\n    'discriminator': discriminator,\n    'generator': generator\n}\n\ncriterion = {\n    'discriminator': nn.BCELoss(),\n    'generator': nn.BCELoss()\n}\n\noptimizer = {\n    'discriminator': Adam(\n        model['discriminator'].parameters(), lr=LR, betas=(0.5, 0.999)\n    ),\n    'generator': Adam(\n        model['generator'].parameters(), lr=LR, betas=(0.5, 0.999)\n    )\n}","e0a49e49":"def fit(model, criterion, optimizer, epochs, save_results = False):\n    # Losses & scores\n    losses = {\n        'discriminator': [],\n        'generator': []\n    }\n    scores = {\n        'real': [],\n        'fake': []\n    }\n\n    for epoch in range(epochs):\n        loss_per_epoch = {\n            'discriminator': [],\n            'generator': []\n        }\n        score_per_epoch = {\n            'real': [],\n            'fake': []\n        }\n        \n        for real_images, _ in tqdm(dataloader):\n            real_images = real_images.to(DEVICE)\n\n            # Train discriminator\n\n            # Clear discriminator gradients\n            optimizer['discriminator'].zero_grad()\n            \n            # Pass real images through discriminator\n            real_preds = model['discriminator'](real_images)\n            real_targets = torch.ones(real_images.size(0), 1, device=DEVICE)\n            real_loss = criterion['discriminator'](real_preds, real_targets)\n            real_score = torch.mean(real_preds).item()\n            \n            # Generate fake images\n            latent = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1, device=DEVICE)\n            fake_images = model['generator'](latent)\n\n            # Pass fake images through discriminator\n            fake_targets = torch.zeros(BATCH_SIZE, 1, device=DEVICE)\n            fake_preds = model['discriminator'](fake_images)\n            fake_loss = criterion['discriminator'](fake_preds, fake_targets)\n            fake_score = torch.mean(fake_preds).item()\n\n            score_per_epoch['real'].append(real_score)\n            score_per_epoch['fake'].append(fake_score)\n\n            # Update discriminator weights\n            loss_d = real_loss + fake_loss\n            loss_d.backward()\n            optimizer['discriminator'].step()\n            loss_per_epoch['discriminator'].append(loss_d.item())\n\n            # Train generator\n            \n            model['generator'].train()\n            \n            # Clear generator gradients\n            optimizer['generator'].zero_grad()\n            \n            # Generate fake images\n            latent = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1, device=DEVICE)\n            fake_images = model['generator'](latent)\n            \n            # Try to fool the discriminator\n            preds = model['discriminator'](fake_images)\n            targets = torch.ones(BATCH_SIZE, 1, device=DEVICE)\n            loss_g = criterion['generator'](preds, targets)\n            \n            # Update generator weights\n            loss_g.backward()\n            optimizer['generator'].step()\n            loss_per_epoch['generator'].append(loss_g.item())\n            \n        # Record losses & scores\n        losses['generator'].append(np.mean(loss_per_epoch['generator']))\n        losses['discriminator'].append(np.mean(loss_per_epoch['discriminator']))\n        scores['real'].append(np.mean(score_per_epoch['real']))\n        scores['fake'].append(np.mean(score_per_epoch['fake']))\n        \n        # Log losses & scores (last batch)\n        print(\n            'Epoch [{}\/{}], gen_loss: {:.4f}, dis_loss: {:.4f}, '\n            'real_score: {:.4f}, fake_score: {:.4f}'.format(\n                epoch+1, epochs, \n                losses['generator'][-1], losses['discriminator'][-1],\n                scores['real'][-1], scores['fake'][-1]\n            )\n        )\n        \n        # Save results\n        if save_results and ((epoch+1) in [1, 3, 8, 20, 50]):\n            imgs = generate(model['generator'], 1000)\n            with open(f'imgs_epoch_{epoch+1}.pkl', 'wb') as handle:\n                pickle.dump(imgs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            torch.save(\n                model['generator'].state_dict(),\n                f'gen_epoch_{epoch+1}.weights'\n            )\n            torch.save(\n                model['discriminator'].state_dict(),\n                f'dis_epoch_{epoch+1}.weights'\n            )\n    \n    return losses, scores","3dc5f1b0":"EPOCHS = 50\nhistory = fit(model, criterion, optimizer, EPOCHS, True)","8cd083cd":"# with open('history.pkl', 'wb') as handle:\n#     pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n# with open(DATA_OUT + 'history.pkl', 'rb') as handle:\n#     history = pickle.load(handle)\nshow_history(history)","15bf00c8":"N_IMAGES = 16\nshow_images(generate(model['generator'], N_IMAGES), size=(4, 4))","f7b999ce":"N_IMAGES = 1000\n\nreal_imgs = torch.Tensor()\nfor imgs, _ in dataloader:\n    real_imgs = torch.cat([real_imgs, imgs])\n    if real_imgs.shape[0] >= N_IMAGES:\n        break","013788de":"# t-SNE presentation\nts_data = {}\nfor epoch in [1, 3, 8, 20, 50]:\n    with open(f'imgs_epoch_{epoch}.pkl', 'rb') as handle:\n        fake_imgs = pickle.load(handle)\n    \n    imgs_flatten = torch.cat([\n        real_imgs.flatten(start_dim=1)[:N_IMAGES, :],\n        fake_imgs.flatten(start_dim=1)[:N_IMAGES, :]\n    ])\n    \n    ts = TSNE(random_state=42, verbose=0).fit_transform(imgs_flatten)\n\n    fig, ax = plt.subplots(figsize=(12, 12))\n    plt.title(f'Epoch {epoch}')\n    plt.scatter(ts[:N_IMAGES, 0], ts[:N_IMAGES, 1], label='real')\n    plt.scatter(ts[N_IMAGES:, 0], ts[N_IMAGES:, 1], label='fake')\n    plt.legend()\n    \n    ts_data[epoch] = ts","f6e50c61":"labels = torch.cat([torch.ones(N_IMAGES), torch.zeros(N_IMAGES)])\n\n# build model\nmodel = KNeighborsClassifier(n_neighbors=1)\n\n# define cross-validation method to use\ncv = LeaveOneOut()\n\nfor epoch in [1, 3, 8, 20, 50]:\n    # use LOOCV to evaluate model\n    accuracies = cross_val_score(\n        model, ts_data[epoch], labels, scoring='accuracy', cv=cv, n_jobs=-1\n    )\n    print(f'Epoch {epoch} mean accuracy: {accuracies.mean()}')","f971d619":"# 3. Sample","d71f2446":"# 2. Model","5a9d51d5":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0438\u0445 \u0438 \u0444\u0435\u0439\u043a\u043e\u0432\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043f\u043e\u0441\u043b\u0435 t-SNE \u043f\u043e\u0441\u043b\u0435 1, 3, 8, 20 \u0438 50 \u044d\u043f\u043e\u0445\u0438","684904da":"\u0427\u0435\u043c \u0431\u043b\u0438\u0436\u0435 \u0444\u043e\u0440\u043c\u0430 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u043c\u0443, \u0442\u0435\u043c \u043b\u0443\u0447\u0448\u0435 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f. \u041f\u0440\u0438 \u0438\u0434\u0435\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043e\u0431\u043b\u0430\u043a\u0430 \u0442\u043e\u0447\u0435\u043a \u0431\u0443\u0434\u0443\u0442 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0442\u044c, \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u0438\u0445 \u043d\u0430 \u0434\u0432\u0430 \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f, accuracy \u0440\u0430\u0432\u043d\u043e 0.5\n\n# 4. Leave-one-out-1-NN classifier accuracy\n\n\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0430\u043a\u043a\u0443\u0440\u0430\u0441\u0438 \u0434\u043e\u043b\u0436\u043d\u043e \u0441\u0442\u0440\u0435\u043c\u0438\u0442\u044c\u0441\u044f \u043e\u0442 1 (\u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e, \u043b\u0435\u0433\u043a\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043d\u0430 2 \u043a\u043b\u0430\u0441\u0441\u0430) \u043a 0.5 (\u0441\u043e\u0441\u0435\u0434\u043e\u043c \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u043a\u0430\u043a \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435, \u0442\u0430\u043a \u0438 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435).","d9d4a64d":"# 5. Visualization\n\u0421\u0434\u0435\u043b\u0430\u043b \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e, \u0442\u0430\u043a \u043f\u0440\u043e\u0449\u0435 \u043f\u043e\u043d\u044f\u0442\u044c accuracy \u043f\u043e\u0441\u043b\u0435 knn","dc42f8e4":"# 1. Load data"}}