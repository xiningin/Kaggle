{"cell_type":{"b7aec019":"code","028a1739":"code","9eb8c1ca":"code","9b664d1b":"code","b932d24a":"code","825037f0":"code","3dc975a0":"code","ad70bb81":"code","a0c87232":"code","6523ec76":"code","5a487e7d":"code","4c3bd6b5":"code","6e68ee70":"code","dac96941":"code","750f5efd":"markdown","c18d090f":"markdown","d5452595":"markdown","5c66b4bb":"markdown","fa868e34":"markdown","7c329c71":"markdown"},"source":{"b7aec019":"!pip install \/kaggle\/input\/iterative-stratification016py3noneanywhl\/iterative_stratification-0.1.6-py3-none-any.whl","028a1739":"import matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KernelDensity\nfrom matplotlib.pyplot import figure\nfrom sklearn.decomposition import PCA\nimport fastai\nfrom fastai.tabular.all import *\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","9eb8c1ca":"train_feat = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_feat = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_targ_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","9b664d1b":"# the data columns corresponding to gene expression data g0 to g772\nsorted_rows = train_feat.iloc[:,4:776].values.copy()\nsorted_rows.sort()\n\nexamples = []\nfor i in range(1000):\n    vals = np.random.randn(772)\n    vals = (vals - np.median(vals)) \/ (1.4826 * np.abs(vals - np.median(vals)).mean())\n    vals.sort()\n    examples.append(vals)\nvals = np.mean(np.array(examples), axis=0)\nfigure(num=None, figsize=(15,8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.xlim(0, 772)\nplt.ylim(-4, 4)\nvals[np.abs(vals) > 2]\n\nmu = np.mean(sorted_rows, axis=0)\nstds = np.std(sorted_rows, axis=0)\n\nplt.subplot(121)\nplt.plot(range(0, mu.shape[0]), mu, color='b')\nplt.plot(vals, c='k')\nplt.fill_between(range(0, 772), mu-stds, mu+stds, linestyle='-', color='b', alpha=0.3)\nplt.plot(vals - mu, color='r')\nplt.legend(['Gaussian noise (gn)', 'Sorted average gene expression values (sg)','Difference between gn and sg'])","b932d24a":"def compute_cell_pca(X_train, X_test, nc):\n    cell_train = X_train.filter(like='c-')\n    cell_test = X_test.filter(like='c-')\n    cell_data = cell_train.append(cell_test, ignore_index=True)\n    pca = PCA(n_components=nc, whiten=True)\n    pca.fit(cell_data.values)\n    return pca\n\ndef preprocess_wpca(data, pca, n):\n    # Split the input into gene expression, cell viability, and metadata.\n    extra_data = data[['sig_id', 'cp_time', 'cp_dose']] \n    gene_data = data.filter(like='g-')\n    \n    cell_data = data.filter(like='c-')\n    cell_df = pd.DataFrame(\n        pca.transform(cell_data.values),\n        index=cell_data.index,\n        columns=[f'cc-{num}' for num in range(pca.transform(cell_data.values).shape[1])]\n    )\n    \n    # Select n highest and lowest gene indices.\n    sorted_gene_indices = np.argsort(gene_data.values, axis=1, kind='stable')\n    relevant_gene_indices = np.concatenate((sorted_gene_indices[:,:n], sorted_gene_indices[:,-n:]), axis=1)\n    relevant_gene_values = np.take_along_axis(gene_data.values, relevant_gene_indices, axis=1)\n    gene_index_df = pd.DataFrame(relevant_gene_indices, index=data.index, columns=[f'gi-{num}' for num in range(2*n)])\n    gene_value_df = pd.DataFrame(relevant_gene_values, index=data.index, columns=[f'gv-{num}' for num in range(2*n)])\n    \n    return pd.concat([extra_data, gene_index_df, gene_value_df, cell_df], axis=1)","825037f0":"npca = 66\nn = 4\nepsilon = 0.001\npca = compute_cell_pca(train_feat, test_feat, npca)\n\n# Ignore the control vehicle rows, since the labels are always zero.\nfiltered_train_feat = train_feat[train_feat['cp_type'] == 'trt_cp']\n\ntrain_df = preprocess_wpca(filtered_train_feat, pca, n)\\\n    .merge(train_targ_scored, on='sig_id')\\\n    .drop(columns=['sig_id'])\n\n# Extract the categorical, continuous, and target variables\ncat_names = ['cp_time', 'cp_dose'] + list(train_df.filter(like='gi-').columns) # ignore 'cp_type', since it's constant for the training data\ncont_names = list(train_df.filter(like='gv-').columns) + list(train_df.filter(like='c-').columns)\ny_names = list(train_targ_scored.drop(columns='sig_id').columns)\n\n# Apply label smoothing\ntrain_df[y_names] = train_df[y_names].mask(train_df[y_names] == 0, epsilon)\ntrain_df[y_names] = train_df[y_names].mask(train_df[y_names] == 1, 1-epsilon)","3dc975a0":"train_df.head(3)","ad70bb81":"class MyTabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, emb_szs, rep_emb_szs, rep_emb_reps, n_cont, out_sz, layers, ps=None, embed_p=0.,\n                 y_range=None, use_bn=True, bn_final=False, bn_cont=True, act_cls=nn.ReLU(inplace=True)):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.rep_embed = Embedding(*rep_emb_szs)\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds) + rep_emb_reps * self.rep_embed.embedding_dim\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        \n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            r = torch.flatten(self.rep_embed(x_cat[:,len(self.embeds):]), start_dim=-2)\n            r = self.emb_drop(r)\n            \n            x = torch.cat(x + [r], 1)\n            #x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)","a0c87232":"# Embedding sizes for cp_type, cp_time, cp_dose\nemb_szs = [(4, 3), (3, 3)]\nembedding_size = 10\n\n# Embedding size for gene expression indices\nnum_genes = train_feat.filter(like='g-').columns.size + 1\nrep_emb_szs = (num_genes, embedding_size)\n\n# Size of hidden layers\nlayers = [288, 448, 352, 192]","6523ec76":"def random_seed(seed_value, use_cuda=False):\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    random.seed(seed_value) # Python\n    if use_cuda:\n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","5a487e7d":"random_seed(42)\n\nmskf = MultilabelStratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n\npredictions = []  \npredictions_tabnet = []\n\nfor train_idx, valid_idx in mskf.split(train_df[cat_names + cont_names], train_df[y_names]):\n    splits = (L(list(train_idx)), L(list(valid_idx)))\n    \n    # Create dataloader\n    to = TabularPandas(train_df, procs = Categorify,\n                       cat_names = cat_names,\n                       cont_names = cont_names,\n                       y_names = y_names,\n                       y_block = MultiCategoryBlock(encoded=True, vocab=y_names),\n                       splits = splits)\n    dls = to.dataloaders(bs=96)\n    \n    model = MyTabularModel(\n        emb_szs,\n        rep_emb_szs,\n        len(cat_names) - len(emb_szs),\n        len(cont_names),\n        len(y_names),\n        layers,\n        embed_p=0.5\n    )\n    mylearn = TabularLearner(dls, model, y_range=(epsilon,1-epsilon))\n    \n    # Find a good learning rate\n    lr_min, lr_steep = mylearn.lr_find(show_plot=False, suggestions=True)\n    lr = (lr_min + lr_steep) \/ 2\n\n    # Fit the model\n    mylearn.fit_one_cycle(10, max_lr=lr, cbs=SaveModelCallback(monitor='valid_loss'))\n    \n    # Get predictions using the tabular model\n    test_df = preprocess_wpca(test_feat, pca, n)\n    test_dl = mylearn.dls.test_dl(test_df)\n    \n    test_preds, _ = mylearn.get_preds(dl=test_dl)\n    predictions.append(test_preds)\n","4c3bd6b5":"res = torch.mean(torch.stack(predictions), axis=0)\nmerged = pd.DataFrame(res, columns=y_names)\nmerged = pd.concat([test_feat[['sig_id']], merged], axis=1)","6e68ee70":"# Set control entries to zero manually\ndf_sigId = test_feat[test_feat['cp_type']=='ctl_vehicle'][['sig_id']].reset_index(drop=True)\ninds = merged.index[merged['sig_id'].isin(df_sigId.sig_id)].tolist()\nmerged.iloc[inds, 1:] = 0 ","dac96941":"merged.to_csv('submission.csv', index=False)","750f5efd":"One of the difficulties with the data is that the classes are highly imbalanced. Therefore, in splitting data into training and validation sets, we can have all the active MoA in the training, or in the validation set. In order to have an even split in terms of number of active and non-active MoAs, we use the scikit-MultilabelStratifiedKFold where we compute the validation set error for K folds.\n\nThe final predictions are then the average over the predictions for each fold.","c18d090f":"### Create submission","d5452595":"## Processing and Model Structure \n**Gene Expression Data**\n- Based on the noise level in gene expression data, we first try to train a network based on the most significant gene expression values (n). \n- We first sort the gene expression data and take the n largest and smallest values, as well as the corresponding gene **indices** (relevant_gene_indices, relevant_gene_values).\n- We find a concrete value for n through hyperparameter tuning. We used the [hyperopt][1] library for all hyperparameter tuning. In TODO, we provide the script that finds the best parameter settings.\n- Small notes: As the control compounds have always zero active MoAs, we exclude them from training. \n\n**Cell Viability Data** \n\nWe used principal component analysis (PCA) as a preprocessing step for the cell viability data. For this, we use the train and test cell viability data and apply PCA and project back the training cell data onto the learned components (these are known as cc columns). The number of components (nc) are another hyperparameter that we optimize over.\n\n[1]: <https:\/\/github.com\/hyperopt\/hyperopt> \"Hyperparameter Tuning\"","5c66b4bb":"## fast.ai Tabular model construction \n\nThe `train_df` dataframe contains the `gi` columns which correspond to the indices of the lowest and highest sorted gene values and their corresponding gene values `gv`. We treat the indices as <em>categorical variables<\/em> and use a learned embedding to incorparate them into our model. The idea of treating some continous variables as categorical variables is explained in the fastai webpage [fast-ai][1]. We therefore have the indices, cp_time, cp_dose as categorical variables and the gene values and the (principal components of the) cell data as continous variables. We want to learn embeddings for the categorical variables, but crucially, we want to use the same embedding for each of the gene indices to avoid an explosion of trainable parameters. (This was mentioned in some of the posts as to how to modify the tabular model to have the same embeddings at different layers. I hope it will be useful for some of you :-) ). \n\nIf we use the fastai.tabular model directly, the model learns a different vector for the same index in the columns `gi-0` to `gi-n`, whereas we probably want the embedding vector for index 531 to be the same for all the columns as it corresponds to the same gene expression value. For this, we copy the fastai tabular model and make some modifications. The original tabular model is found here [tabular-model][2]. \n\nThe additional parameters in the modified version of the TabularModel (we call it `MyTabularModel`) are: \n\n    1. rep_emb_szs: the tuple which is the size of the repeating embedding (number of genes, embedding size of interest) \n    2. rep_emb_reps: number of times that we want to repeat the embedding (i.e. we want this to be repeated 2n times, which is the number of signifcant gene values).\n\nTo handle the repeated embedding, we construct `self.rep_embed` by passing `rep_embed_szs` as input (`self.rep_embed = Embedding(*rep_emb_szs)`). The number of embeddings is then updated to be the sum of the embedding sizes for `cp_dose` and `cp_time` and the repeated embeddings (`n_emb = sum(e.embedding_dim for e in self.embeds) + rep_emb_reps * self.rep_embed.embedding_dim`). Apart from this, the code is a straight copy of the fastai tabular model and consists of an additional layer of dropout on the embeddings, batch normalization for the continuous variables followed by a number of `LinBnDrop` layers as usual.\n\nThe second place we need to add modifications is in the forward function, where we keep things simple by assuming that the repeated embedding dimensions are always the last `rep_emb_reps` columns the `x_cat` batch. Since in our case the non-repeated embeddings are small, we don't bother to apply dropout to the non-repeated embeddings (as this should allow us to use larger values for `embed_p` without suddenly forgetting the duration of a sample for no good reason).\n\nIn detail, after the second line of `forward`, the list `x` contains the embeddings for the categorical variables `cp_dose` and `cp_time`, which is a list of tensors with shape `batchSize x 3`. The variable `r` contains the flattened embeddings of the gene indices. This is a tensor with shape `batchSize x 80`. The flattening is necessary, since the result of applying `self.rep_embed` is a tensor with shape `batchSize x 8 x 10` and we want to concatenate all embeddings into a single tensor of shape `batchSize x 86`. That's exactly what we do in the fifth line, where we store the concatenated embeddings in `x`.\n\n[1]: <https:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/> \"fastai\"\n[2]: <https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/tabular\/model.py#L28> \"tabularModel\"","fa868e34":"In this notebook we present an approach based on learning embeddings for gene expression data mainly through using the fastai library and explaining code snippets throughout the post. ***Find blog posts about other aspects of the competition in [machine-learnink][3]. There will be more updates on the webpage soon.***\n\n**Initial idea**\n \n  According to some posts in the discussion forums ([post][1] and [preprocessing][2]), the informative gene expression values are either signifcantly negative or postive. This means that a whole lot of other gene expression data in between might contain little amount of information or just be noise.\n \n  I first compare the difference between gene expression values with random noise from a Gaussian distribution. The random signal in the plot below is generated from a Gaussian distribution based on the description provided in the discussions in this [post][1]. In addition, we average over a sample of 1000 different generated random signals.\n    \n[1]: <https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/184005#1034211> \"discussionPost\"\n[2]: <https:\/\/clue.io\/connectopedia\/glossary#Q> \"Preprocessing\"\n[3]: <https:\/\/machine-learnink.com\/> \"blogPost\"","7c329c71":"## Fit the model \n   **Parameter initialization**"}}