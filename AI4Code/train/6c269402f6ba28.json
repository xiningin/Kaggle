{"cell_type":{"c1b318d0":"code","660f45a7":"code","c30d172e":"code","663a7636":"code","0cfb4e97":"code","b9a44e05":"code","6b81c1f9":"code","000e4427":"code","58489db7":"code","d7734325":"code","253adbc7":"code","bb944ac8":"code","048805c3":"code","60b12aac":"code","b64aea37":"code","5684c762":"code","53b73175":"code","de890b79":"code","0a49c9d6":"code","b3473a60":"code","01061123":"code","4a58b9de":"code","9a1ba9a0":"code","3cfa79ef":"code","777ed39f":"code","79b90845":"code","0d15aa05":"code","64c6fc46":"code","74308d49":"code","7e622064":"code","a3ab409d":"code","ca08b8b7":"code","16396c33":"code","352f8786":"code","c5f01b14":"code","ab8ab6f7":"code","14c624bd":"code","58c168df":"code","6dca139f":"code","554a4f0b":"code","4e035c1b":"code","5f491001":"code","ce998fc2":"code","3c6999af":"code","d0fb3e77":"code","2860a5ba":"code","9f72d678":"code","9eeb0f6d":"code","100515a2":"code","8704eeff":"code","e7dabb40":"code","e0c9a0d1":"code","dd87999e":"code","1b9714c9":"code","26bc765e":"code","5e3d5cee":"code","93a6b480":"code","777c9362":"code","3554560f":"markdown","ffbde90b":"markdown","7f84408a":"markdown","226c73a9":"markdown","b8d96056":"markdown","64f33954":"markdown","10219ccd":"markdown","1c052fa5":"markdown","dc2b5fe1":"markdown","d8a64a9d":"markdown","6c96bdf7":"markdown","9a05b5c1":"markdown","180407cc":"markdown","abd336fa":"markdown","90f29d69":"markdown","f5abfd3d":"markdown","a5f5441c":"markdown","75c7b8c7":"markdown","983a971e":"markdown","23a1a9e7":"markdown","d977ff0f":"markdown","055c4ed9":"markdown","7d3ac27d":"markdown","29c43769":"markdown","18c589f1":"markdown","02286159":"markdown","d88b1bcb":"markdown","a2955198":"markdown","f7528643":"markdown"},"source":{"c1b318d0":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfrom pathlib import Path\n\nimport plotly.offline as plty\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nfrom wordcloud import WordCloud\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\nfrom colorama import Fore, Back, Style, init\n\nfrom tqdm import tqdm\ntqdm.pandas()","660f45a7":"!pip install -q pyspellchecker","c30d172e":"PATH = Path('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/')\n\ndata = pd.read_csv(PATH \/ 'IMDB Dataset.csv')","663a7636":"data.info()","0cfb4e97":"data.head(5)","b9a44e05":"data['sentiment'] = data['sentiment'].map({'positive' : 0, 'negative' : 1})","6b81c1f9":"from spellchecker import SpellChecker\nimport spacy\nimport string, re\n\nspell = SpellChecker()\nPUNCT_TO_REMOVE = string.punctuation\n\ndef remove_punctuation(text):    \n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\n\ndef correct_spellings(text): \n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)","000e4427":"def clean_text(text):\n    \n    # Lower Casing\n    text = text.lower()\n    \n    # Remove url\n    text = remove_urls(text) \n    \n    # Remove html tags\n    text = remove_html(text)\n    \n    # Removing @tags\n    text = re.sub('@\\w*','',text)\n    \n    # Removing Punctuations\n    text = remove_punctuation(text)\n    \n    # Removing new lines\n    text = re.sub('\\\\n',' ',text)\n    \n    # Correct spellings\n    #text = correct_spellings(text)      \n    \n    return text","58489db7":"data[\"clean_text\"] = data[\"review\"].progress_apply(lambda text: clean_text(text))","d7734325":"data[['review','clean_text']].head(10)","253adbc7":"string = ' '.join(data['clean_text'])\n\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(string.lower())\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text='Common words in comments')","bb944ac8":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef polarity(text):\n    if type(text) == str:\n        return SIA.polarity_scores(text)\n    else:\n        return 1000\n    \nSIA = SentimentIntensityAnalyzer()\ndata[\"polarity\"] = data[\"clean_text\"].progress_apply(polarity)","048805c3":"import plotly.graph_objects as go\n\nneg_pol = [pols['neg'] for pols in data[\"polarity\"] if type(pols) is dict]\nneg_pol = list(filter((0.0).__ne__, neg_pol))\n\nfig = go.Figure(go.Histogram(x=neg_pol, marker=dict(\n            color='red')\n    ))\n\nfig.update_layout(xaxis_title=\"Negativity sentiment\", title_text=\"Negativity sentiment\", template=\"simple_white\")\nfig.show()","60b12aac":"toxic = [x['neg'] for x in data.query(\"sentiment == 1\")['polarity'] if type(x) == dict]\nnon_toxic = [x['neg'] for x in data.query(\"sentiment == 0\")['polarity'] if type(x) == dict]\n\nfig = ff.create_distplot(hist_data=[toxic, non_toxic],\n                         group_labels=[\"Negative\", \"Positive\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Negativity of Bad vs Good tweets\", xaxis_title=\"Negativity\", template=\"simple_white\")\nfig.show()","b64aea37":"pos_pol = [pols['pos'] for pols in data[\"polarity\"] if type(pols) is dict]\npos_pol = list(filter((0.0).__ne__, pos_pol))\n\nfig = go.Figure(go.Histogram(x=pos_pol, marker=dict(\n            color='seagreen')\n    ))\n\nfig.update_layout(xaxis_title=\"Positivity sentiment\", title_text=\"Positivity sentiment\", template=\"simple_white\")\nfig.show()","5684c762":"neu_pol = [pols['neu'] for pols in data[\"polarity\"] if type(pols) is dict]\nneu_pol = list(filter((1.0).__ne__, neu_pol))\n\nfig = go.Figure(go.Histogram(x=neu_pol, marker=dict(\n            color='darkorange')\n    ))\n\nfig.update_layout(xaxis_title=\"Neutral sentiment\", title_text=\"Neutral sentiment\", template=\"simple_white\")\nfig.show()","53b73175":"total_comments = data['sentiment'].count()\nneg = data['sentiment'].value_counts().loc[1]\nSentiment = ['Negative','Positive']\ncount = [neg, total_comments-neg]\n\nfig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}]])\nfig.add_trace(go.Bar(x=Sentiment,y=count,text=count, marker_color=['#D9636B', '#64D9D1']),\n             row=1, col=1)\nfig.add_trace(go.Pie(labels=Sentiment, values=count, domain=dict(x=[0.5, 1.0]), marker_colors=['#D9636B', '#64D9D1']), \n              row=1, col=2)\n\nfig.update_layout(height=600, width=800, title_text=\"Negative vs Postive vs Neutral\", template='plotly_white')\n\nfig.show()","de890b79":"string = ' '.join(data.query('sentiment == 1')['clean_text'])\n\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(string.lower())\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text='Negative Reviews')","0a49c9d6":"string = ' '.join(data.query('sentiment == 0')['clean_text'])\n\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(string.lower())\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text='Positive reviews')","b3473a60":"import transformers\nimport tensorflow as tf\nfrom tokenizers import BertWordPieceTokenizer\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *","01061123":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\nsave_path = '\/kaggle\/working\/distilbert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)","4a58b9de":"from sklearn.model_selection import train_test_split\n\nX, y = data['clean_text'].values,data['sentiment'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=111)\n\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.125, \n                                                stratify=y_test, random_state=111)","9a1ba9a0":"X_train_tokens = tokenizer(list(X_train), padding='max_length', truncation=True, return_tensors=\"tf\")\nX_train_token_ids = X_train_tokens['input_ids']\n\nX_test_tokens = tokenizer(list(X_test), padding='max_length', truncation=True, return_tensors=\"tf\")\nX_test_token_ids = X_test_tokens['input_ids']\n\nX_val_tokens = tokenizer(list(X_val), padding='max_length', truncation=True, return_tensors=\"tf\")\nX_val_token_ids = X_val_tokens['input_ids']","3cfa79ef":"print(X_test_token_ids.shape)\nprint(X_train_token_ids.shape)","777ed39f":"# Embedding:\ntransformer = transformers.TFDistilBertModel.\\\n    from_pretrained('distilbert-base-uncased')\nembed = transformer.weights[0].numpy()\n\nprint('Vocab : ', np.shape(embed)[0], 'Hidden States\/Embed vector size :', np.shape(embed)[1])","79b90845":"MAX_LEN = 512\nBATCH_SIZE = 12\nSTEPS_PER_EPOCH = X_train_token_ids.shape[0] \/\/ BATCH_SIZE","0d15aa05":"reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',  \n                                    factor=0.3, patience=2, \n                                    verbose=1, mode='auto', \n                                    epsilon=0.0001, cooldown=1, min_lr=0.000001)","64c6fc46":"def visualize_model_preds(y_pred, indices=[0, 1, 2, 3]):\n\n    for idx, i in enumerate(indices):\n        if y_test[i] == 0:\n            label = \"Non-toxic\"\n            color = f'{Fore.GREEN}'\n            symbol = '\\u2714'\n        else:\n            label = \"Toxic\"\n            color = f'{Fore.RED}'\n            symbol = '\\u2716'\n\n        print('{}{} {}'.format(color, str(idx+1) + \". \" + label, symbol))\n        print(f'{Style.RESET_ALL}')\n\n        print(X_test[idx]); print(\"\")\n        fig = go.Figure()\n        if y_test[i] == 1:\n            yl = [1 - y_pred[i], y_pred[i]]\n            \n        else:\n            yl = [1 - y_pred[i], y_pred[i]]\n\n        fig.add_trace(go.Bar(x=['Positive', 'Negative'], y=yl, marker=dict(color=[\"seagreen\", \"indianred\"])))\n        fig.update_traces(name=X_test[idx])\n        fig.update_layout(xaxis_title=\"Labels\", yaxis_title=\"Probability\", template=\"plotly_white\", title_text=\"Predictions for validation comment #{}\".format(idx+1))\n        fig.show()\n        \ndef plot_cm(y_true, y_pred, title, figsize=(7,6)):\n    y_pred = y_pred.round().astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","74308d49":"def vanilla_model():\n\n    input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n\n\n    embedding = Embedding(np.shape(embed)[0], np.shape(embed)[1],\n                              input_length=MAX_LEN, weights=[embed],\n                              trainable=False)(input_word_ids)\n\n    x = K.sum(embedding, axis=2)\n    x = Dense(512, activation='relu')(x)\n    out = Dense(1, activation='sigmoid')(x)\n    \n\n    model = Model(inputs=input_word_ids, outputs=out)\n\n    model.compile(Adam(lr=0.001), \n                      loss='binary_crossentropy', \n                      metrics=['accuracy'])\n\n    return model","7e622064":"van_model = vanilla_model()\n\ntrain_history = van_model.fit(\n    X_train_token_ids, y_train,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=(X_val_token_ids, y_val),\n    epochs=7,\n    callbacks = [reduceLROnPlat]\n)","a3ab409d":"van_model.evaluate(X_test_token_ids, y_test)\n\ny_pred = van_model.predict(X_test_token_ids)","ca08b8b7":"y_preds = y_pred > 0.5\ny_preds = np.where(y_preds == True, 1, 0)","16396c33":"plot_cm(y_test, y_preds, 'Threshold=0.5')","352f8786":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_preds))","c5f01b14":"from sklearn.metrics import roc_curve, precision_recall_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred.reshape(y_test.shape[0]))\ngmeans = np.sqrt(tpr * (1-fpr))\n\n# locate the index of the largest g-mean\nix = np.argmax(gmeans)\n\nprint('Best Threshold=%f, G-mean=%.3f' % (thresholds[ix], gmeans[ix]))","ab8ab6f7":"y_preds = y_pred > thresholds[ix]\ny_preds = np.where(y_preds == True, 1, 0)","14c624bd":"plot_cm(y_test, y_preds, f'Threshold = {thresholds[ix]}')","58c168df":"def build_cnn_model(max_len=MAX_LEN):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    \n    embedding = Embedding(np.shape(embed)[0], np.shape(embed)[1],\n                          input_length=max_len, weights=[embed],\n                          trainable=False)(input_word_ids)\n    \n    embedding = SpatialDropout1D(0.3)(embedding)\n    conv_1 = Conv1D(64, 2)(embedding)\n    conv_2 = Conv1D(64, 3)(embedding)\n    conv_3 = Conv1D(64, 4)(embedding)\n    conv_4 = Conv1D(64, 5)(embedding)\n    \n    maxpool_1 = GlobalAveragePooling1D()(conv_1)\n    maxpool_2 = GlobalAveragePooling1D()(conv_2)\n    maxpool_3 = GlobalAveragePooling1D()(conv_3)\n    maxpool_4 = GlobalAveragePooling1D()(conv_4)\n    conc = concatenate([maxpool_1, maxpool_2, maxpool_3, maxpool_4], axis=1)\n\n    conc = Dense(64, activation='relu')(conc)\n    conc = Dense(1, activation='sigmoid')(conc)\n    \n    model = Model(inputs=input_word_ids, outputs=conc)\n    \n    model.compile(Adam(lr=0.01), \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    return model","6dca139f":"cnn_model = build_cnn_model()\n\ntrain_history = cnn_model.fit(\n    X_train_token_ids, y_train,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=(X_val_token_ids, y_val),\n    epochs=10,\n    callbacks = [reduceLROnPlat]\n)","554a4f0b":"class AttentionWeightedAverage(Layer):\n\n    def __init__(self, return_attention=False, **kwargs):\n        self.init = initializers.get('uniform')\n        self.supports_masking = True\n        self.return_attention = return_attention\n        super(AttentionWeightedAverage, self).__init__(** kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(ndim=3)]\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[2], 1),\n                                 name='{}_W'.format(self.name),\n                                 initializer=self.init)\n        super(AttentionWeightedAverage, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        logits = K.dot(x, self.W)\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai \/ (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return (input_shape[0], output_len)\n\n    def compute_mask(self, input, input_mask=None):\n        if isinstance(input_mask, list):\n            return [None] * len(input_mask)\n        else:\n            return None","4e035c1b":"def build_lstm_model(max_len=MAX_LEN):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    \n    embed = transformer.weights[0].numpy()\n    embedding = Embedding(np.shape(embed)[0], np.shape(embed)[1],\n                          input_length=max_len, weights=[embed],\n                          trainable=False)(input_word_ids)\n    \n    embedding = SpatialDropout1D(0.3)(embedding)\n    lstm_1 = LSTM(128, return_sequences=True)(embedding)\n    lstm_2 = LSTM(128, return_sequences=True)(lstm_1)\n    \n    attention = AttentionWeightedAverage()(lstm_2)\n    conc = Dense(64, activation='relu')(attention)\n    conc = Dense(1, activation='sigmoid')(conc)\n    \n    model = Model(inputs=input_word_ids, outputs=conc)\n    \n    model.compile(Adam(lr=0.01), \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    return model","5f491001":"lstm_model = build_lstm_model()\n\ntrain_history = lstm_model.fit(\n    X_train_token_ids, y_train,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=(X_val_token_ids, y_val),\n    epochs=6,\n    callbacks = [reduceLROnPlat]\n)","ce998fc2":"bert_transformer = transformers.TFDistilBertModel.\\\n    from_pretrained('distilbert-base-uncased')","3c6999af":"y_train = y_train.astype('int32')\ny_test = y_test.astype('int32')\ny_val = y_val.astype('int32')","d0fb3e77":"def build_bert_model(maxlen=MAX_LEN):\n    input_ids = Input(shape=(maxlen,), dtype=tf.int32, name=\"input_word_ids\")\n    embeddings = bert_transformer(input_ids)[0]\n\n    cls_token = embeddings[:, 0, :]\n    x = Dense(maxlen, activation=\"relu\")(cls_token)\n    x = Dropout(0.5)(x)\n    out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=input_ids, outputs=out)\n\n    model.compile(Adam(learning_rate=1.5e-5), \n                      loss='binary_crossentropy', \n                      metrics=['accuracy'])\n    \n    return model","2860a5ba":"model = build_bert_model()\n\ntrain_history = model.fit(\n    X_train_token_ids, y_train,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=(X_val_token_ids, y_val),\n    epochs=2\n)","9f72d678":"y_pred = model.predict(X_test_token_ids)","9eeb0f6d":"y_pred = y_pred.reshape(y_pred.shape[0])","100515a2":"y_pred[:10]","8704eeff":"visualize_model_preds(y_pred)","e7dabb40":"y_preds = y_pred > 0.5\ny_preds = np.where(y_preds == True, 1, 0)","e0c9a0d1":"old_accuracy = accuracy_score(y_preds, y_test)\nold_accuracy","dd87999e":"plot_cm(y_test, y_preds, 'Threshold=0.5')","1b9714c9":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\ngmeans = np.sqrt(tpr * (1-fpr))\n\n# locate the index of the largest g-mean\nix = np.argmax(gmeans)\n\nprint('Best Threshold=%f, G-mean=%.3f' % (thresholds[ix], gmeans[ix]))","26bc765e":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,5))\nplt.plot([0,1], [0,1], linestyle='--', label='No Skill') \n\nplt.plot(fpr, tpr, marker='.', label='Logistic') \nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best') \n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\n# show the plot\nplt.show()","5e3d5cee":"y_preds = (y_pred > thresholds[ix])\ny_preds = np.where(y_preds == True, 1, 0)","93a6b480":"new_accuracy = accuracy_score(y_preds, y_test)\n\n\nprint(f'Percent increase in accuracy after threshold moving = {((new_accuracy - old_accuracy)\/old_accuracy) * 100}')","777c9362":"plot_cm(y_test, y_pred, 'Optimal Threshold')","3554560f":"From the above plot, we can see that the neutrality sentiment distribution has a strong leftward (negative) skew, which is in constrast to the negativity and positivity sentiment distributions. This indicates that the comments tend to be very neutral and unbiased in general. This also suggests that most comments are not highly opinionated and polarizing, meaning that most comments are non-toxic.","ffbde90b":"Most of the texts are Neutral by Looking at the low postivity & negativity, Let's look at netral values to confirm this","7f84408a":"### Probability Threshold Moving : Finding the best Threshold","226c73a9":"### Classifying with probabilty threshold = 0.5","b8d96056":"### Callback","64f33954":"### Helper functions","10219ccd":"## Modelling","1c052fa5":"We can clearly see that Neg comments have a significantly greater negative sentiment than Postive comments (on average). The probability density of negativity peaks at around 0 for non-toxic comments, while the negativity for toxic comments at 0.12 . This suggests that a comment is very likely to be non-toxic if it has a negativity of 0.","dc2b5fe1":"The models using CNN, LSTM & Vanilla network did not perform good at all, these models could not learn the Dataset and performed somewhat same.","d8a64a9d":"Great improvement in the Positve class using Probabilty Threshold moving , but vice versa for Negative class","6c96bdf7":"### Poor performance from CNN model as well","9a05b5c1":"### 1. Vanilla neural network","180407cc":"#### Evaluate & predict","abd336fa":"From the above plot, we can see that negative sentiment has a strong rightward (positive) skew, indicating that negativity is usually on the lower side. This suggests that most comments are not toxic or negative. In fact, the most common negativity value is around 0.14. Virtually no comments have a negativity greater than 0.8.","90f29d69":"Very Poor Performance acheived from Vanilla network with all the positive classes missclassified.\n\nWe will be adjusting the probabilty threshold using G-Means","f5abfd3d":"### 4. Bert Transformer","a5f5441c":"### Text Preprocessing","75c7b8c7":"### 2. Modelling with CNN","983a971e":"### Encoding","23a1a9e7":"### Bert Tokenizer","d977ff0f":"### Finding the optimal threshold using ROC-AUC","055c4ed9":"We have to search a range of threshold values in order to find the best threshold.\nIn some cases, the optimal threshold can be calculated directly. Tuning or shifting the decision threshold in order to accommodate the broader requirements of the classification problem is generally referred to as threshold-moving, threshold-tuning, or simply thresholding.","7d3ac27d":"### Classifying using optimal threshold","29c43769":"### 3. LSTM + Attention Layer","18c589f1":"### Sentiment Analysis","02286159":"### Accuracy","d88b1bcb":"### Mapping Sentiments to Numbers","a2955198":"#### ROC curve","f7528643":"#### Loading the DistilBert Transformer & creating word embeddings"}}