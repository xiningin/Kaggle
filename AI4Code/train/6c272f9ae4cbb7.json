{"cell_type":{"dd26b9ff":"code","1d104a2d":"code","26acde8c":"code","e4eb96c5":"code","5dac21a6":"code","0a1886d3":"code","239b893a":"code","23d3c406":"code","ea28f876":"code","9277aaec":"code","3d3f4fc9":"code","7f147f69":"code","bf7ee6fb":"code","415f8490":"code","f48729f6":"code","46ec6991":"code","ee1e7093":"code","199338d9":"code","5fbe3115":"code","69218792":"code","58856a4e":"code","9417d890":"code","7239f655":"code","327351c0":"code","01b12ff1":"code","5271bd15":"code","e9939b00":"code","1e8ad13c":"code","a55cec6b":"code","8e23a2c5":"code","757be5bc":"markdown","16af13d5":"markdown","1cfb5b84":"markdown","1f74e90b":"markdown","d83ac0de":"markdown","a775f159":"markdown","9d9da897":"markdown","b1c7a3d1":"markdown","de40c357":"markdown","0768e456":"markdown","5a08127a":"markdown","28904ae4":"markdown","90cb2b3b":"markdown","319aafce":"markdown","89d03fc3":"markdown","c56ad38b":"markdown","915ca9e5":"markdown","48d6f2d4":"markdown","be04700f":"markdown"},"source":{"dd26b9ff":"!apt install -y ffmpeg #needed by librosa to function correctly\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.display import Audio # used for playing audio samples\nimport tensorflow as tf # tensorflow for all the ML magic\nimport librosa # for audio signal processing\nimport librosa.display # for displaying waveforms\nfrom sklearn.preprocessing import StandardScaler\nimport sys\n\n%matplotlib inline\nimport matplotlib.pyplot as plt # for plotting graphs\nimport seaborn as sns # for prettier graphs\nsns.set(style=\"dark\")","1d104a2d":"AUDIO_INPUT_FILE = '..\/input\/audio-conversation\/conversation.wav'","26acde8c":"y, sr = librosa.load(AUDIO_INPUT_FILE)","e4eb96c5":"print(type(y), type(sr))\nprint(y.shape, sr)","5dac21a6":"# calculated duration based on array length and sample rate\nprint(y.shape[0] \/ sr * 1.0)\n\n# native duration value directly from the file\nprint(librosa.get_duration(filename=AUDIO_INPUT_FILE))","0a1886d3":"Audio(data=y, rate=sr)","239b893a":"plt.figure(figsize=(22, 4))\nlibrosa.display.waveplot(y, sr, color='#7B1FA2', alpha=0.8)\nplt.title('Lost in Translation - \"Does it get easier?\"')\nplt.show()","23d3c406":"X = librosa.stft(y)\nXdb = librosa.amplitude_to_db(X)","ea28f876":"plt.figure(figsize=(27, 5))\nlibrosa.display.specshow(Xdb, sr = sr, x_axis='time', y_axis='linear', cmap=\"magma\")\nplt.colorbar(format='%+2.0f dB')\nplt.show()","9277aaec":"plt.figure(figsize=(27, 5))\nlibrosa.display.specshow(Xdb, sr = sr, x_axis='time', y_axis='log', cmap=\"magma\")\nplt.colorbar(format='%+2.0f dB')\nplt.show()","3d3f4fc9":"division_per_second = 8\n\nchunk_time = 1. \/ division_per_second\nchunk_size = sr \/\/ division_per_second\nprint(chunk_size, chunk_time)","7f147f69":"left_over = y.shape[0] % chunk_size\nnum_of_chunks = y[:-left_over].shape[0]\/chunk_size\n\ny_split = np.split(y[:-left_over], num_of_chunks)\nlen(y_split)","bf7ee6fb":"feature_vector_mfcc = np.array([ librosa.feature.mfcc(y=chunk, sr=sr, n_mfcc = 40) for chunk in y_split ])\nfeature_vector_mfcc.shape","415f8490":"feature_vector_spec_rolloff = np.array([librosa.feature.spectral_rolloff(y=chunk, sr=sr) for chunk in y_split])\nfeature_vector_spec_rolloff.shape","f48729f6":"feature_vector_spec_centroid = np.array([librosa.feature.spectral_centroid(y=chunk, sr=sr) for chunk in y_split])\nfeature_vector_spec_centroid.shape","46ec6991":"feature_vector_zcr = np.array([librosa.feature.zero_crossing_rate(y=chunk) for chunk in y_split])\nfeature_vector_zcr.shape","ee1e7093":"feature_vector_mfcc_mean = np.mean(feature_vector_mfcc, axis = 2)\nfeature_vector_mfcc_mean.shape","199338d9":"feature_vector_spec_rolloff_mean = np.mean(feature_vector_spec_rolloff, axis = 2)\nfeature_vector_spec_rolloff_mean.shape","5fbe3115":"feature_vector_spec_centroid_mean = np.mean(feature_vector_spec_centroid, axis = 2)\nfeature_vector_spec_centroid_mean.shape","69218792":"feature_vector_zcr_mean = np.mean(feature_vector_zcr, axis = 2)\nfeature_vector_zcr_mean.shape","58856a4e":"feature_matrix = np.hstack((\n    feature_vector_mfcc_mean,\n    feature_vector_spec_rolloff_mean,\n    feature_vector_spec_centroid_mean,\n    feature_vector_zcr_mean,\n))\nfeature_matrix.shape","9417d890":"scaler = StandardScaler()\n\nnormalized_feature_matrix = scaler.fit_transform(feature_matrix)\nnormalized_feature_matrix = feature_matrix\nnormalized_feature_matrix.shape","7239f655":"feature_matrix_df = pd.DataFrame(normalized_feature_matrix)\nfeature_matrix_df.columns = [\n               \"MFCC-1\", \"MFCC-2\", \"MFCC-3\", \"MFCC-4\", \n               \"MFCC-5\", \"MFCC-6\", \"MFCC-7\", \"MFCC-8\", \n               \"MFCC-9\", \"MFCC-10\", \"MFCC-11\", \"MFCC-12\", \n               \"MFCC-13\", \"MFCC-14\", \"MFCC-15\", \"MFCC-16\", \n               \"MFCC-17\", \"MFCC-18\", \"MFCC-19\", \"MFCC-20\",\n               \"MFCC-21\", \"MFCC-22\", \"MFCC-23\", \"MFCC-24\", \n               \"MFCC-25\", \"MFCC-26\", \"MFCC-27\", \"MFCC-28\", \n               \"MFCC-29\", \"MFCC-30\", \"MFCC-31\", \"MFCC-32\", \n               \"MFCC-33\", \"MFCC-34\", \"MFCC-35\", \"MFCC-36\", \n               \"MFCC-37\", \"MFCC-38\", \"MFCC-39\", \"MFCC-40\",\n               \"Spec Roll-off\", \n               \"Spec Centroid\", \n               \"ZCR\"\n              ]\n\n# take a peak in the data\nfeature_matrix_df.head()","327351c0":"# Number of cluster we wish to divide the data into( user tunable )\nk = 3\n\n# Max number of allowed iterations for the algorithm( user tunable )\nepochs = 10000","01b12ff1":"# return an np-array of k points based on k-means++\ndef selectCentroids(k, dummy_data):\n    centroids = [] \n    \n    # pick the first centroid at random\n    centroids.append(dummy_data[np.random.randint( \n            dummy_data.shape[0]), :]) \n    \n    # compute remaining k - 1 centroids \n    for centroid_index in range(k - 1):  \n        # points from nearest centroid \n        distance_array = [] \n        \n        # iterate over the data points for each centroid\n        # to find the distance from nearest chosen centroids\n        for i in range(dummy_data.shape[0]): \n            point = dummy_data[i, :]\n            distance = sys.maxsize \n              \n            ## compute distance of 'point' from each of the previously \n            ## selected centroid and store the minimum distance \n            for j in range(len(centroids)): \n                temp_distance = np.linalg.norm(point - centroids[j]) \n                distance = min(distance, temp_distance) \n            distance_array.append(distance) \n              \n        ## select data point with maximum distance as our next centroid \n        distance_array = np.array(distance_array) \n        next_centroid = dummy_data[np.argmax(distance_array), :]\n        centroids.append(next_centroid) \n    return np.array(centroids)\n\n# utility to assign centroids to data points\ndef assignCentroids(X, C):  \n    expanded_vectors = tf.expand_dims(X, 0)\n    expanded_centroids = tf.expand_dims(C, 1)\n    distance = tf.math.reduce_sum( tf.math.square( tf.math.subtract( expanded_vectors, expanded_centroids ) ), axis=2 )\n    return tf.math.argmin(distance, 0)\n                                              \n# utility to recalculate centroids\ndef reCalculateCentroids(X, X_labels):\n    sums = tf.math.unsorted_segment_sum( X, X_labels, k )\n    counts = tf.math.unsorted_segment_sum( tf.ones_like( X ), X_labels, k  )\n    return tf.math.divide( sums, counts )","5271bd15":"data = feature_matrix_df\n\nX = tf.Variable(data.values, name=\"X\")\nX_labels = tf.Variable(tf.zeros(shape=(X.shape[0], 1)),name=\"C_lables\")\nC = tf.Variable(selectCentroids(k, normalized_feature_matrix), name=\"C\")\n\nfor epoch in range( epochs ):\n    X_labels =  assignCentroids( X, C )\n    C = reCalculateCentroids( X, X_labels )","e9939b00":"X_labels","1e8ad13c":"def frame_condencer( X_labels ):\n    condenced_frame_vectors = []\n    current_label = -1\n    \n    for label in X_labels:\n        if current_label != label:\n            condenced_frame_vectors.append((label, 1))\n        else:\n            condenced_frame_vectors[-1][1] += 1\n    return condenced_frame_vectors\n\ndef rect_generator( condenced_frame_vectors, plt, time = 0.25 ):\n    rect_height = plt.ylim()[1] - plt.ylim()[0]\n    rect_range = plt.xlim()[1]\n    \n    color_map = {\n        0:\"r\",\n        1:\"b\",\n        2:\"g\",\n        3:\"y\"\n    }\n    \n    rect_list = []\n    current_count = 0\n    for vector in condenced_frame_vectors:\n        rect_list.append( patches.Rectangle(\n            (current_count * time,plt.ylim()[0]),\n            vector[1] * time, \n            rect_height, \n            linewidth=1, \n            edgecolor='none', \n            facecolor=color_map[vector[0]], \n            alpha=0.5\n        ))\n        current_count += vector[1]\n    return rect_list","a55cec6b":"import matplotlib.patches as patches\n\nplt.figure(figsize=(32, 8))\nlibrosa.display.waveplot(y, sr, color='#7B1FA2', alpha=0.8)\nrect = patches.Rectangle((0,0),5,0.75,linewidth=1,edgecolor='none',facecolor='r', alpha=0.5)\nax = plt.gca()\n\ncondenced_frame_vectors = frame_condencer(X_labels.numpy().tolist())\nrect_list = rect_generator(condenced_frame_vectors, plt, time = chunk_time)\n\nfor rect in rect_list:\n    ax.add_patch(rect)\nplt.title('Lost in Translation - \"Does it get easier?\" - Speaker diarization')\nplt.show()","8e23a2c5":"Audio(data=y, rate=sr)","757be5bc":"## 4.d Using mean for simplification\nThe features for which we will use average instead of the extended values are:\n- MFCC\n- Spectral Roll-Off\n- Spectral Centroid\n- Zero Crossing rate","16af13d5":"# 2. Defining file path variables","1cfb5b84":"# 6. Final output","1f74e90b":"## 4.c Extracting features for each chunk\nWe will extract the following feature for each chunk\n- MFCC\n- Spectral Roll-Off\n- Spectral Centroid\n- Zero Crossing Rate","d83ac0de":"# 3. Playing with the audio file\nWe will now try to extract some features and visualize the data using some well known signal processing techniques","a775f159":"## 4.b Creating an array of chunks from audio time series\nEach chunk will end up with **5512** samples, and we now have **280** such chunks that will roughly cover the whole time line of the clip","9d9da897":"# 1. Installing and Importing all the necessary libraries\nWe will use the following:\n- numpy\n- pandas\n- IPython (Audio)\n- TensorFlow","b1c7a3d1":"## 4.a Creating chunks\nHow many samples do we need in each chunk if we want our chunks to be 0.25 sec long?  \n**chunk_size** = **sameple_rate** \/ **4**  \n> Note: We will ignore the left over frames (132 in our case) for now, which are anyway not worth much","de40c357":"## 3.a Load audio file\nThis will give me the sample *data* (**y**) and the *sample rate* (**sr**) of the file.","0768e456":"## 5.c Driver function","5a08127a":"## 5.a Hyper-parameters","28904ae4":"## 3.d This is the frequency vs time plot of the same clip\nLet's try linear and log scale and see which one works best","90cb2b3b":"## 3.b We can play the audio using the features thus extracted","319aafce":"# 4. Extracting relevant features from the clip\nThere are two things that we'll have to do to prepare our dataset:\n- Create eqaul length chunks from the whole clip\n- Apply feature extraction on each of those chunks","89d03fc3":"# 5. Preparing our model\nAs we don't have a pre made K-means implementation in tensorflow, let's try to implement our own","c56ad38b":"## 5.b K-means++ utility functions","915ca9e5":"## 4.e Prepare the final matrix\nNow we will horizontally stack the vectors to form a final feature matrix where the columns are the features and the rows are the individual frames.","48d6f2d4":"## 3.c This is the waveform of our clip (Amplitude vs Time)","be04700f":"# Important links\n### Music oriented stuff\nhttps:\/\/www.youtube.com\/watch?v=_FatxGN3vAM - Understanding spectrogram  \nhttps:\/\/musicinformationretrieval.com\/magnitude_scaling.html - Log vs Linear scale in spectrogram  \nhttps:\/\/musicinformationretrieval.com\/stft.html - Understanding STFT and its need \n  \nhttps:\/\/www.youtube.com\/watch?v=6JeyiM0YNo4 - Slow motion violine string  \nhttps:\/\/www.youtube.com\/watch?v=XPbLYD9KFAo - Understanding overtone  \nhttps:\/\/www.youtube.com\/watch?v=i_0DXxNeaQ0 - Understanding what sound actually is \n  \nhttps:\/\/librosa.github.io\/librosa\/glossary - Helpful set of definitions  \nhttps:\/\/towardsdatascience.com\/music-genre-classification-with-python-c714d032f0d8 - article on music information retrieval\nhttps:\/\/stats.stackexchange.com\/questions\/133656\/how-to-understand-the-drawbacks-of-k-means?newreg=de1f000164564890b61dae124ab93403 - When K-means will perform poorly\n\nhttps:\/\/www.youtube.com\/watch?time_continue=782&v=pjxGPZQeeO4&feature=emb_logo - Why K-means is poor choice for audio data\n\nhttps:\/\/towardsdatascience.com\/how-i-understood-what-features-to-consider-while-training-audio-files-eedfb6e9002b - Description on different music features\n\n### Maths oriented stuff\nhttps:\/\/archive.org\/details\/SpectrogramCepstrumAndMel-frequency_636522 - Understanding Cepstrum and MFCC  \nhttps:\/\/haythamfayek.com\/2016\/04\/21\/speech-processing-for-machine-learning.html - A step by step guide to sound signal processing ( From sound signal to MFCC )\nhttp:\/\/practicalcryptography.com\/miscellaneous\/machine-learning\/guide-mel-frequency-cepstral-coefficients-mfccs\/ - A better guide to MFCC ( better explained math )"}}