{"cell_type":{"033f4906":"code","7a00be6e":"code","59a67199":"code","66d6f2ef":"code","9beb4555":"code","b72c4c10":"code","54127bcb":"code","f28f260d":"code","30721efd":"code","4a386b3c":"code","5ab026be":"code","12ad7bd3":"code","2b762f31":"code","b1d94f5f":"code","23717cd9":"code","707ffcbd":"code","a761bbcb":"code","e1c54140":"code","b729ecad":"code","d9225fe3":"code","91edd643":"code","e261e2ae":"code","49c1fbb3":"code","e5a8c546":"code","25788bde":"code","a776e34e":"code","1f0bd9e4":"code","19b6b336":"code","c602b7e2":"code","daad8f6f":"code","bd644742":"code","7df67e0c":"code","ca28d943":"code","d1e804c4":"code","93067071":"code","76c3ce1b":"markdown","96330353":"markdown","7cbfde8c":"markdown","59ac5ef6":"markdown","4731e383":"markdown","67963b94":"markdown","cd37c95a":"markdown","002e2bf5":"markdown","b9502733":"markdown","74010add":"markdown","abd3a53a":"markdown","2ac0a288":"markdown","e13cb4ef":"markdown","3cbad385":"markdown","96de13f2":"markdown","b0c3b886":"markdown","aefc946b":"markdown","205cc5c5":"markdown","6fe47d4c":"markdown","e864092b":"markdown","f4e21cb7":"markdown","da48c8df":"markdown","74d1d159":"markdown","f83abe1f":"markdown","f48b5ca7":"markdown","0785c437":"markdown","66f42de5":"markdown","07a95695":"markdown","71c96fb8":"markdown","1368192d":"markdown","f52fabe8":"markdown","f8743fc3":"markdown","caa7f430":"markdown","f33d03fc":"markdown","f0aa6f12":"markdown"},"source":{"033f4906":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7a00be6e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('seaborn-deep')\nfrom sklearn.metrics import confusion_matrix\nimport nltk","59a67199":"df = pd.read_csv('\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')\ndf.head()","66d6f2ef":"df.info()","9beb4555":"df.groupby('Category').describe()","b72c4c10":"df['Length'] = df['Message'].apply(len)\ndf.head()","54127bcb":"explode = (0.1,0)  \nfig1, ax1 = plt.subplots(figsize=(12,7))\nax1.pie(df['Category'].value_counts(), explode=explode,labels=['ham','spam'], autopct='%1.1f%%',\n        shadow=True)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.legend()\nplt.show()","f28f260d":"plt.figure(figsize=(10,6))\ndf['Length'].plot.hist(bins = 150)","30721efd":"df['Length'].describe()","4a386b3c":"df[df['Length'] == 910]['Message'].iloc[0]","5ab026be":"import string\nfrom nltk.corpus import stopwords","12ad7bd3":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","2b762f31":"df['Message'].head(10).apply(text_process)","b1d94f5f":"from sklearn.feature_extraction.text import CountVectorizer ","23717cd9":"bow_transformer = CountVectorizer(analyzer=text_process).fit(df['Message'])","707ffcbd":"print(len(bow_transformer.vocabulary_))","a761bbcb":"message4 = df['Message'][3]\nprint(message4)","e1c54140":"bow4 = bow_transformer.transform([message4])\nprint(bow4)\nprint(bow4.shape)","b729ecad":"print(bow_transformer.get_feature_names()[4066])\nprint(bow_transformer.get_feature_names()[9551])","d9225fe3":"messages_bow = bow_transformer.transform(df['Message'])","91edd643":"print('Shape of Sparse Matrix: ', messages_bow.shape)\nprint('Amount of Non-Zero occurences: ', messages_bow.nnz)","e261e2ae":"sparsity = (100.0 * messages_bow.nnz \/ (messages_bow.shape[0] * messages_bow.shape[1]))\nprint('sparsity: {}'.format((sparsity)))","49c1fbb3":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer().fit(messages_bow)\ntfidf4 = tfidf_transformer.transform(messages_bow)\nprint(tfidf4)","e5a8c546":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy',random_state=0)\nclassifier.fit(tfidf4, df['Category'])","25788bde":"print('predicted:', classifier.predict(tfidf4)[0])\nprint('expected:', df.Category[3])","a776e34e":"all_predictions = classifier.predict(messages_bow)\nprint(all_predictions)","1f0bd9e4":"from sklearn.metrics import classification_report\nprint (classification_report(df['Category'], all_predictions))","19b6b336":"from sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(df['Category'], all_predictions))","c602b7e2":"from sklearn.model_selection import train_test_split\n\nmsg_train, msg_test, label_train, label_test = \\\ntrain_test_split(df['Message'], df['Category'], test_size=0.2)\n\nprint(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))","daad8f6f":"from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', RandomForestClassifier()),  # train on TF-IDF vectors w\/ SVM\n])","bd644742":"pipeline.fit(msg_train,label_train)","7df67e0c":"predictions = pipeline.predict(msg_test)","ca28d943":"from sklearn.metrics import confusion_matrix,classification_report\ncm = confusion_matrix(label_test,predictions)\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"BuPu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","d1e804c4":"print(classification_report(predictions,label_test))","93067071":"from sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(predictions,label_test))","76c3ce1b":"In the above evaluation, we evaluated accuracy on the same data we used for training. You should never actually evaluate on the same dataset you train on! the proper way is to split the data into a training set and test set,","96330353":"Print total number of vocab words","7cbfde8c":"**Vectorization**","59ac5ef6":"Let's use groupby to use describe by Category, this way we can begin to think about the features that separate ham and spam!","4731e383":"Let's see which ones appear twice in our dataset","67963b94":"# **Text Cleaning**","cd37c95a":"One of the text has 910 characters, let's use masking to find this message:","002e2bf5":"Now let's see its vector representation","b9502733":"![tenor.gif](attachment:tenor.gif)","74010add":"# **Training a Random Forest model**","abd3a53a":"Let's run our model again and then predict the test set. We will create and use a pipeline for this purpose","2ac0a288":"**Creating a Data Pipeline**","e13cb4ef":"# **Making Confusion Matrix**","3cbad385":"# **let\u2019s get our environment ready with the libraries we\u2019ll need and then import the data!**","96de13f2":"Now let's transform the entire DataFrame of messages and create sparse matrix","b0c3b886":"# **TF-IDF**","aefc946b":"**Train Test Split**","205cc5c5":"Now let's compute term weighting and do normalisation with TF-IDF","6fe47d4c":"Check to make sure its working","e864092b":"Let's see the percentage of ham and spam in our dataset","f4e21cb7":"![tUt3jGI-min.gif](attachment:tUt3jGI-min.gif)","da48c8df":"Let's create the function to remove all punctuation, remove all stopwords and returns a list of the cleaned text ","74d1d159":"Confusion Matrix is going to contain the correct predictions that our model made on the set as well as the incorrect predictions.","f83abe1f":"Let's create classification report","f48b5ca7":"Let's try classifying our single random message and checking how we do:","0785c437":"Let\u2019s clean the text for the messages in our dataset with NLP.","66f42de5":"Now we have the messages as lists and we need to convert each of those messages into a vector that SciKit Learn's algorithm models can work with.","07a95695":"# **Exploratory Data Analysis**","71c96fb8":"Let's take one text message and get its bag-of-words counts as a vector, putting to use our new bow_transformer","1368192d":"Let's make a new column to detect how long the text messages are","f52fabe8":"**Model Evaluation**","f8743fc3":"Create classification report","caa7f430":"Let's check out the accuracy of our model in entire dataset","f33d03fc":"# Please Upvote ! If you find this notebook is useful!\n# Many Thanks","f0aa6f12":"**Check out the Data**"}}