{"cell_type":{"f426966f":"code","134e83e4":"code","5a0b6137":"code","97de3da1":"code","830a84b7":"code","4b1e2d6e":"code","6c46f351":"code","7626e3aa":"code","4a084e07":"code","fa291006":"code","4486cd79":"code","273154c4":"code","8a6f76cc":"code","90470142":"code","347a2107":"code","310e3f86":"code","230f0d51":"code","b7885420":"code","ef690d95":"code","deef7f2f":"code","74b7144b":"code","7b0b1b38":"code","e3e00513":"code","205f7c54":"code","df6c038d":"code","503c6c26":"code","d38139e3":"code","72e05fc6":"code","0be7f7d3":"code","41356eb4":"code","0bbc1323":"code","b9f93ec4":"code","3ce16984":"code","364cc4fc":"code","2c154589":"code","677dc2c1":"code","8f0d7df0":"code","39562849":"code","c4f9a175":"code","fbed2e3f":"code","cb9d6e51":"code","68650750":"code","b7264c0f":"code","d59570ce":"code","387704e5":"code","e070e6c4":"code","93f26110":"code","880a321c":"code","9d5dbcbc":"code","065226b3":"code","76584f1c":"code","fc4120d6":"code","a5bdf477":"code","371594e5":"code","f113a044":"code","3292304b":"code","c8522dd0":"code","61340b7e":"code","af5eef2b":"code","2d063c07":"code","ca1f621a":"code","41ef5e82":"code","0356d10b":"markdown","40dbafd0":"markdown","c8e7de06":"markdown","6c8fe883":"markdown","9aaeb1f7":"markdown","c1b7ff05":"markdown","50e2de65":"markdown","543c0578":"markdown","f56fe622":"markdown","a5201565":"markdown","b6cf9250":"markdown","841c1ec7":"markdown","e48bff58":"markdown","525eb301":"markdown","8b0b0165":"markdown","ed37816a":"markdown","30d982c5":"markdown","e5a29e82":"markdown","f15dac15":"markdown","3677da39":"markdown","39141cd8":"markdown","eb756c51":"markdown","8150f537":"markdown","81aa59e2":"markdown","76c6293d":"markdown","109bca78":"markdown","9d035af8":"markdown","709f40a0":"markdown","2965f60e":"markdown","79b87406":"markdown","28a7198d":"markdown","31077554":"markdown","bd8edad5":"markdown","f92e3841":"markdown","039f0573":"markdown","b037390d":"markdown","1f0081d9":"markdown","bb2207ef":"markdown","3686887a":"markdown","4149ba59":"markdown","23b16f9c":"markdown","971e5caf":"markdown","164a439b":"markdown","34b9ae5e":"markdown","04b061d5":"markdown"},"source":{"f426966f":"from os.path import join, isfile\nfrom os import path, scandir, listdir\n\n# standard\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport geopandas as gp\nfrom shapely.geometry import Point\n\nimport gc","134e83e4":"def list_all_files(location='..\/input\/', pattern=None, recursive=True):\n    \"\"\"\n    This function returns a list of files at a given location (including subfolders)\n    \n    - location: path to the directory to be searched\n    - pattern: part of the file name to be searched (ex. pattern='.csv' would return all the csv files)\n    - recursive: boolean, if True the function calls itself for every subdirectory it finds\n    \"\"\"\n    subdirectories= [f.path for f in scandir(location) if f.is_dir()]\n    files = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n    if recursive:\n        for directory in subdirectories:\n            files.extend(list_all_files(directory))\n    if pattern:\n        files = [f for f in files if pattern in f]\n    return files","5a0b6137":"list_all_files()[11:16]  # printing only 5 files for simplicity","97de3da1":"def _get_topics(topics_list):\n    for topic in topics_list:\n        topics_list[topics_list.index(topic)] = topic.split('_')[-1]\n    return topics_list\n\n\ndef check_topics(dept_num, base_topics, test_topics):\n    \"\"\"\n    This function checks that a department has all the topics (education, poverty, etc.)\n    that are present in the other departments.\n    \n    - dept_num: string identifying the department\n    - base_topics: list of topics that the other departments have (if empty, it is created)\n    - test_topics: topics found for the given department\n    \n    If there are new topics, the function updates base_topics and returns it\n    \"\"\"\n    test_topics = _get_topics(test_topics)\n    if len(base_topics) < 1:\n        base_topics = test_topics  # the first time just create the list\n    # check if something is missing\n    mis_topics = [top for top in base_topics if top not in test_topics]\n    if len(mis_topics) > 0:\n        print(f\"Department {dept_num} does not have data about the following topics:\")\n        print(mis_topics)\n    # check if something is new\n    new_topics = [top for top in test_topics if top not in base_topics]\n    if len(new_topics) > 0:\n        print(f\"Department {dept_num} has data about the following new topics:\")\n        print(new_topics)\n        print(\"The departments previously checked do not have these data\")\n        # updating the base_topics\n        base_topics = list(set(base_topics + test_topics))\n    return base_topics\n\n\ndef check_ids(base_ids, data):\n    \"\"\"\n    This function checks that, across the topics, the id's are consistent\n    \"\"\"\n    tmp_ids = data['GEO.id2'].unique()\n    if len(tmp_ids) != data.shape[0]:\n        print(f\"In {file} inconsistent id's\")\n    if len(base_ids) < 1: # the first time it creates the \"base\" of id's\n        base_ids = tmp_ids\n    if set(tmp_ids) != set(base_ids):\n        print(f\"In {file} inconsistent id's with the other files\")\n    return base_ids\n\n\ndef data_quality(location='..\/input\/data-science-for-good\/cpe-data\/'):\n    \"\"\"\n    This is the main function, it checks every department at the given location,\n    assuming that every department is in a separate directory.\n    \n    It checks for:\n    - presence of police related data\n    - consistency of the ACS files (and relative metadata)\n    - presence of all the necessary shapefiles\n    \"\"\"\n    # Get the list of the departments\n    dept_list = [d.path for d in scandir(location) if d.is_dir()]\n    topics = []  # needed to check if we have all\n    \n    # loop over departments\n    for dept in dept_list:\n        dept_num = dept.split('_')[1]\n        print(\"_\"*40)\n        print(f'Checking department {dept_num}')\n        \n        # Check if we have some kind of data about crime or police-------------\n        crime_files = list_all_files(dept, pattern='.csv', recursive=False)\n        if len(crime_files) < 1:\n            print(f\"Department {dept_num} does not have data about police interventions\")\n        else:\n            print(\"Department {} has {} file(s) about police interventions\".format(dept_num, \n                                                                                   len(crime_files)))\n            \n        # Check the ACS data consistency -------------------------------------------\n        data_path = dept + '\/' + dept_num + '_ACS_data\/'\n        # Check if we have all the topics (poverty, education, etc)\n        temp_topics = [d.path for d in scandir(data_path) if d.is_dir()]\n        topics = check_topics(dept_num, topics, temp_topics)\n        \n        # Check if the data have consistent id's and columns\n        files = list_all_files(data_path, pattern='_with_ann.csv')\n        ids = []  # needed to check if we have all\n        for file in files:\n            data = pd.read_csv(file, skiprows=[1], low_memory=False, nrows=3)  # nrows is for speed\n            meta = file.replace('_with_ann.csv', '_metadata.csv')\n            metadata = pd.read_csv(meta, header=None, names=['key', 'description'])\n            if not data.columns.all() in list(metadata['key']):\n                print(\"In {} inconsistent metadata\".format(file))\n            ids = check_ids(ids, data)\n        \n        # Check the Shapefiles consistency ------------------------------------------\n        data_path = dept + '\/' + dept_num + '_Shapefiles\/'\n        extensions = ['.shp', '.shx', '.dbf', '.prj']\n        for ext in extensions:\n            files = list_all_files(data_path, pattern=ext)\n            if len(files) < 1:\n                print(\"Department {} does not have the {} file\".format(dept_num, \n                                                                       ext))\n            if len(files) > 1:\n                print(\"Department {} has {} files with extension {}\".format(dept_num, \n                                                                            len(files), \n                                                                            ext))\n        print(\"\\n\")\n    print(\"Done!\")","830a84b7":"data_quality()","4b1e2d6e":"def import_topic(path, tolerance=0.7):\n    \"\"\"\n    Imports the file at a given location,\n    Coerces the values to be numerical in order to easily spot the missing values\n    Drops every column with enough missing values, the threshold is set by the parameter tolerance\n    \n    It returns 2 DataFrames: one with the data, one with the metadata.\n    \"\"\"\n    # find the file with the ACS data and load it\n    datafile = list_all_files(path, pattern='_with_ann.csv')[0]\n    data = pd.read_csv(datafile, skiprows=[1], low_memory=False)\n    # take out the ids momentarily\n    ids = data[[col for col in data.columns if 'GEO' in col]]\n    rest = data[[col for col in data.columns if 'GEO' not in col]]\n    # convert to numeric and force na's if necessary\n    rest = rest.apply(pd.to_numeric, errors='coerce')\n    # put data together again\n    data = ids.join(rest)\n    print(f'Shape: {data.shape}')\n    cols = data.columns\n    nrows = data.shape[0]\n    removed = 0\n    for col in cols:\n        mis = data[col].isnull().sum() \/ nrows\n        if mis > tolerance:\n            removed += 1\n            del data[col]\n    if removed > 0:\n        print(\"Removed {} columns because more than {}% of the values are missing\".format(removed, \n                                                                                      tolerance*100))\n        print(f\"New shape: {data.shape}\")\n    meta = datafile.replace('_with_ann.csv', '_metadata.csv')\n    metadata = pd.read_csv(meta, header=None, names=['key', 'description'])\n    return data, metadata\n\n\ndef import_dept(location):\n    \"\"\"\n    Imports all the police files, the ACS, the shapefiles at a given location\n    \n    It returns a dictionary of DataFrames\n    \"\"\"\n    dept_num = location.split('_')[1]\n    print(f'Importing department {dept_num}')\n    print('\\n')\n    data_list = {}\n    # Police data ------------------------\n    print(\"Importing police data...\")\n    crime_files = list_all_files(location, pattern='.csv', recursive=False)\n    crm_count = 1\n    for crm in crime_files:\n        crm_name = \"police_\" + str(crm_count)\n        data_list[crm_name] = pd.read_csv(crm, skiprows=[1], low_memory=False)\n        print(\"File {}, shape: {}\".format(crm_count,\n                                         data_list[crm_name].shape))\n        crm_count += 1\n    # ACS -------\n    data_path = location + '\/' + dept_num + '_ACS_data\/'\n    topics = listdir(data_path)\n    for topic in topics:\n        topic_name = topic.split('_')[-1]\n        print(f'Importing {topic_name}...')\n        data, meta = import_topic(data_path + topic, tolerance=0.3)  # I am being more strict than the default\n        data_list[topic_name] = data\n        data_list[topic_name + '_meta'] = meta    \n    # Shapefiles -----\n    print(\"Importing shapefile(s)...\")\n    data_path = location + '\/' + dept_num + '_Shapefiles\/'\n    shapes = list_all_files(data_path, pattern='.shp')\n    shapes = [shp for shp in shapes if shp.endswith('.shp')]\n    shp_count = 1\n    for shp in shapes:\n        shp_name = 'shapefile_' + str(shp_count)\n        data_list[shp_name] = gp.read_file(shp)\n        print(\"File {}, shape: {}\".format(shp_count,\n                                         data_list[shp_name].shape))\n        shp_count += 1\n    gc.collect() # in case some of the files were really big\n    return data_list","6c46f351":"dept_list = [d.path for d in scandir('..\/input\/data-science-for-good\/cpe-data\/') if d.is_dir()]\ndept_list","7626e3aa":"dept = import_dept(dept_list[9])","4a084e07":"dept.keys()","fa291006":"poverty = dept['poverty']\npoverty.head(2)","4486cd79":"pov_meta = dept['poverty_meta']\n# selecting only the columns that survived the import\npov_meta = pov_meta.loc[pov_meta.key.isin(list(poverty.columns))]  \n\nfor i in range(0,5):  # to have a full view use range(0,200)\n    name = 'EST_VC' + str(i).zfill(2)  # focusing on the estimates\n    pov_est = pov_meta[pov_meta.key.str.contains(name)].copy()\n    desc = pov_est.description.values\n    if len(desc) > 0 :\n        print(name)\n        print(desc)\n        print(\"\\n\")","273154c4":"poverty_list = {'HC01_EST_VC01' : 'p_total_est',  # these are just those we know the poverty level of\n                'HC01_MOE_VC01' : 'p_total_moe',\n                'HC02_EST_VC01' : 'p_below_pov_est',\n                'HC02_MOE_VC01' : 'p_below_pov_moe',\n                'HC03_EST_VC01' : 'p_below_pov_perc_est',\n                'HC03_MOE_VC01' : 'p_below_pov_perc_moe',\n                'HC02_EST_VC14' : 'p_males_below_pov_est',\n                'HC02_MOE_VC14' : 'p_males_below_pov_moe',\n                'HC03_EST_VC14' : 'p_males_below_pov_perc_est',\n                'HC03_MOE_VC14' : 'p_males_below_pov_perc_moe',\n                'HC02_EST_VC15' : 'p_females_below_pov_est',\n                'HC02_MOE_VC15' : 'p_females_below_pov_moe',\n                'HC03_EST_VC15' : 'p_females_below_pov_perc_est',\n                'HC03_MOE_VC15' : 'p_females_below_pov_perc_moe',\n                'HC02_EST_VC18' : 'p_white_below_pov_est',\n                'HC02_MOE_VC18' : 'p_white_below_pov_moe',\n                'HC03_EST_VC18' : 'p_white_below_pov_perc_est',\n                'HC03_MOE_VC18' : 'p_white_below_pov_perc_moe',\n                'HC02_EST_VC19' : 'p_black_below_pov_est',\n                'HC02_MOE_VC19' : 'p_black_below_pov_moe',\n                'HC03_EST_VC19' : 'p_black_below_pov_perc_est',\n                'HC03_MOE_VC19' : 'p_black_below_pov_perc_moe',\n                'HC02_EST_VC20' : 'p_native_below_pov_est',\n                'HC02_MOE_VC20' : 'p_native_below_pov_moe',\n                'HC03_EST_VC20' : 'p_native_below_pov_perc_est',\n                'HC03_MOE_VC20' : 'p_native_below_pov_perc_moe',\n                'HC02_EST_VC21' : 'p_asian_below_pov_est',\n                'HC02_MOE_VC21' : 'p_asian_below_pov_moe',\n                'HC03_EST_VC21' : 'p_asian_below_pov_perc_est',\n                'HC03_MOE_VC21' : 'p_asian_below_pov_perc_moe',\n                'HC02_EST_VC22' : 'p_islander_below_pov_est',\n                'HC02_MOE_VC22' : 'p_islander_below_pov_moe',\n                'HC03_EST_VC22' : 'p_islander_below_pov_perc_est',\n                'HC03_MOE_VC22' : 'p_islander_below_pov_perc_moe',\n                'HC02_EST_VC23' : 'p_other_race_below_pov_est',\n                'HC02_MOE_VC23' : 'p_other_race_below_pov_moe',\n                'HC03_EST_VC23' : 'p_other_race_below_pov_perc_est',\n                'HC03_MOE_VC23' : 'p_other_race_below_pov_perc_moe',\n                'HC02_EST_VC26' : 'p_hispanic_below_pov_est',\n                'HC02_MOE_VC26' : 'p_hispanic_below_pov_moe',\n                'HC03_EST_VC26' : 'p_hispanic_below_pov_perc_est',\n                'HC03_MOE_VC26' : 'p_hispanic_below_pov_perc_moe'\n                }\n\n\nrace_list = {'HC01_VC03': 'total_population',\n             'HC01_VC04': 'total_males',\n             'HC01_VC05': 'total_females',\n             'HC01_VC49': 'total_white',\n             'HC03_VC49': 'perc_white',\n             'HC01_VC50': 'total_black',\n             'HC03_VC50': 'perc_black',\n             'HC01_VC51': 'total_native',  # sorry for not including the individual tribes\n             'HC03_VC51': 'perc_native',\n             'HC01_VC56': 'total_asian',\n             'HC03_VC56': 'perc_asian', # sorry for not aknowledging that Asia is a huge place\n             'HC01_VC64': 'total_islander',\n             'HC03_VC64': 'perc_islander',\n             'HC01_VC69': 'total_other_race',\n             'HC03_VC69': 'perc_other_race',\n             'HC01_VC88': 'total_hispanic',\n             'HC03_VC88': 'perc_hispanic'\n            }\n\n\nhousing_list = {'HC01_EST_VC01': 'h_total_houses_est',\n                'HC01_MOE_VC01': 'h_total_houses_moe',\n                'HC02_EST_VC01': 'h_owner_houses_est',\n                'HC02_MOE_VC01': 'h_owner_houses_moe',\n                'HC03_EST_VC01': 'h_rented_houses_est',\n                'HC03_MOE_VC01': 'h_rented_houses_moe',\n                'HC04_EST_VC04': 'h_total_houses_white_est',\n                'HC04_MOE_VC04': 'h_total_houses_white_moe',\n                'HC02_EST_VC04': 'h_owner_houses_white_est',\n                'HC02_MOE_VC04': 'h_owner_houses_white_moe',\n                'HC03_EST_VC04': 'h_rented_houses_white_est',\n                'HC03_MOE_VC04': 'h_rented_houses_white_moe',\n                'HC05_EST_VC05': 'h_total_houses_black_est',\n                'HC05_MOE_VC05': 'h_total_houses_black_moe',\n                'HC02_EST_VC05': 'h_owner_houses_black_est',\n                'HC02_MOE_VC05': 'h_owner_houses_black_moe',\n                'HC03_EST_VC05': 'h_rented_houses_black_est',\n                'HC03_MOE_VC05': 'h_rented_houses_black_moe',\n                'HC06_EST_VC06': 'h_total_houses_native_est',\n                'HC06_MOE_VC06': 'h_total_houses_native_moe',\n                'HC02_EST_VC06': 'h_owner_houses_native_est',\n                'HC02_MOE_VC06': 'h_owner_houses_native_moe',\n                'HC03_EST_VC06': 'h_rented_houses_native_est',\n                'HC03_MOE_VC06': 'h_rented_houses_native_moe',\n                'HC07_EST_VC07': 'h_total_houses_asian_est',\n                'HC07_MOE_VC07': 'h_total_houses_asian_moe',\n                'HC02_EST_VC07': 'h_owner_houses_asian_est',\n                'HC02_MOE_VC07': 'h_owner_houses_asian_moe',\n                'HC03_EST_VC07': 'h_rented_houses_asian_est',\n                'HC03_MOE_VC07': 'h_rented_houses_asian_moe',\n                'HC08_EST_VC08': 'h_total_houses_islander_est',\n                'HC08_MOE_VC08': 'h_total_houses_islander_moe',\n                'HC02_EST_VC08': 'h_owner_houses_islander_est',\n                'HC02_MOE_VC08': 'h_owner_houses_islander_moe',\n                'HC03_EST_VC08': 'h_rented_houses_islander_est',\n                'HC03_MOE_VC08': 'h_rented_houses_islander_moe',\n                'HC09_EST_VC09': 'h_total_houses_other_race_est',\n                'HC09_MOE_VC09': 'h_total_houses_other_race_moe',\n                'HC02_EST_VC09': 'h_owner_houses_other_race_est',\n                'HC02_MOE_VC09': 'h_owner_houses_other_race_moe',\n                'HC03_EST_VC09': 'h_rented_houses_other_race_est',\n                'HC03_MOE_VC09': 'h_rented_houses_other_race_moe',\n                'HC12_EST_VC12': 'h_total_houses_hispanic_est',\n                'HC12_MOE_VC12': 'h_total_houses_hispanic_moe',\n                'HC02_EST_VC12': 'h_owner_houses_hispanic_est',\n                'HC02_MOE_VC12': 'h_owner_houses_hispanic_moe',\n                'HC03_EST_VC12': 'h_rented_houses_hispanic_est',\n                'HC03_MOE_VC12': 'h_rented_houses_hispanic_moe'\n               }\n\n\nincome_list = {'HC01_EST_VC02': 'i_total_income_est',\n               'HC01_MOE_VC02': 'i_total_income_moe',\n               'HC02_EST_VC02': 'i_median_income_est',\n               'HC02_MOE_VC02': 'i_median_income_moe',\n               'HC01_EST_VC04': 'i_total_income_white_est',\n               'HC01_MOE_VC04': 'i_total_income_white_moe',\n               'HC02_EST_VC04': 'i_median_income_white_est',\n               'HC02_MOE_VC04': 'i_median_income_white_moe',\n               'HC01_EST_VC05': 'i_total_income_black_est',\n               'HC01_MOE_VC05': 'i_total_income_black_moe',\n               'HC02_EST_VC05': 'i_median_income_black_est',\n               'HC02_MOE_VC05': 'i_median_income_black_moe',\n               'HC01_EST_VC06': 'i_total_income_native_est',\n               'HC01_MOE_VC06': 'i_total_income_native_moe',\n               'HC02_EST_VC06': 'i_median_income_native_est',\n               'HC02_MOE_VC06': 'i_median_income_native_moe',\n               'HC01_EST_VC07': 'i_total_income_asian_est',\n               'HC01_MOE_VC07': 'i_total_income_asian_moe',\n               'HC02_EST_VC07': 'i_median_income_asian_est',\n               'HC02_MOE_VC07': 'i_median_income_asian_moe',\n               'HC01_EST_VC08': 'i_total_income_islander_est',\n               'HC01_MOE_VC08': 'i_total_income_islander_moe',\n               'HC02_EST_VC08': 'i_median_income_islander_est',\n               'HC02_MOE_VC08': 'i_median_income_islander_moe',\n               'HC01_EST_VC09': 'i_total_income_other_race_est',\n               'HC01_MOE_VC09': 'i_total_income_other_race_moe',\n               'HC02_EST_VC09': 'i_median_income_other_race_est',\n               'HC02_MOE_VC09': 'i_median_income_other_race_moe',\n               'HC01_EST_VC12': 'i_total_income_hispanic_est',\n               'HC01_MOE_VC12': 'i_total_income_hispanic_moe',\n               'HC02_EST_VC12': 'i_median_income_hispanic_est',\n               'HC02_MOE_VC12': 'i_median_income_hispanic_moe'\n              }\n\n\nemployment_list = {'HC04_EST_VC01': 'e_unempl_rate_est',\n                   'HC04_MOE_VC01': 'e_unempl_rate_moe',\n                   'HC04_EST_VC15': 'e_unempl_rate_white_est',\n                   'HC04_MOE_VC15': 'e_unempl_rate_white_moe',\n                   'HC04_EST_VC16': 'e_unempl_rate_black_est',\n                   'HC04_MOE_VC16': 'e_unempl_rate_black_moe',\n                   'HC04_EST_VC17': 'e_unempl_rate_native_est',\n                   'HC04_MOE_VC17': 'e_unempl_rate_native_moe',\n                   'HC04_EST_VC18': 'e_unempl_rate_asian_est',\n                   'HC04_MOE_VC18': 'e_unempl_rate_asian_moe',\n                   'HC04_EST_VC19': 'e_unempl_rate_islander_est',\n                   'HC04_MOE_VC19': 'e_unempl_rate_islander_moe',\n                   'HC04_EST_VC20': 'e_unempl_rate_other_race_est',\n                   'HC04_MOE_VC20': 'e_unempl_rate_other_race_moe',\n                   'HC04_EST_VC23': 'e_unempl_rate_hispanic_est',\n                   'HC04_MOE_VC23': 'e_unempl_rate_hispanic_moe',\n                   'HC04_EST_VC28': 'e_unempl_rate_males_est',\n                   'HC04_MOE_VC28': 'e_unempl_rate_males_moe',\n                   'HC04_EST_VC29': 'e_unempl_rate_females_est',\n                   'HC04_MOE_VC29': 'e_unempl_rate_females_moe'\n                  }\n\n\neducation_list = {'HC02_EST_VC42': 'ed_perc_hs_white_est',\n                  'HC04_EST_VC42': 'ed_perc_hs_white_male_est',\n                  'HC06_EST_VC42': 'ed_perc_hs_white_female_est',\n                  'HC02_EST_VC43': 'ed_perc_ba_white_est',\n                  'HC04_EST_VC43': 'ed_perc_ba_white_male_est',\n                  'HC06_EST_VC43': 'ed_perc_ba_white_female_est',\n                  'HC02_EST_VC46': 'ed_perc_hs_black_est',\n                  'HC04_EST_VC46': 'ed_perc_hs_black_male_est',\n                  'HC06_EST_VC46': 'ed_perc_hs_black_female_est',\n                  'HC02_EST_VC47': 'ed_perc_ba_black_est',\n                  'HC04_EST_VC47': 'ed_perc_ba_black_male_est',\n                  'HC06_EST_VC47': 'ed_perc_ba_black_female_est',\n                  'HC02_EST_VC50': 'ed_perc_hs_native_est',\n                  'HC04_EST_VC50': 'ed_perc_hs_native_male_est',\n                  'HC06_EST_VC50': 'ed_perc_hs_native_female_est',\n                  'HC02_EST_VC51': 'ed_perc_ba_native_est',\n                  'HC04_EST_VC51': 'ed_perc_ba_native_male_est',\n                  'HC06_EST_VC51': 'ed_perc_ba_native_female_est',\n                  'HC02_EST_VC54': 'ed_perc_hs_asian_est',\n                  'HC04_EST_VC54': 'ed_perc_hs_asian_male_est',\n                  'HC06_EST_VC54': 'ed_perc_hs_asian_female_est',\n                  'HC02_EST_VC55': 'ed_perc_ba_asian_est',\n                  'HC04_EST_VC55': 'ed_perc_ba_asian_male_est',\n                  'HC06_EST_VC55': 'ed_perc_ba_asian_female_est',\n                  'HC02_EST_VC58': 'ed_perc_hs_islander_est',\n                  'HC04_EST_VC58': 'ed_perc_hs_islander_male_est',\n                  'HC06_EST_VC58': 'ed_perc_hs_islander_female_est',\n                  'HC02_EST_VC59': 'ed_perc_ba_islander_est',\n                  'HC04_EST_VC59': 'ed_perc_ba_islander_male_est',\n                  'HC06_EST_VC59': 'ed_perc_ba_islander_female_est',\n                  'HC02_EST_VC62': 'ed_perc_hs_other_race_est',\n                  'HC04_EST_VC62': 'ed_perc_hs_other_race_male_est',\n                  'HC06_EST_VC62': 'ed_perc_hs_other_race_female_est',\n                  'HC02_EST_VC63': 'ed_perc_ba_other_race_est',\n                  'HC04_EST_VC63': 'ed_perc_ba_other_race_male_est',\n                  'HC06_EST_VC63': 'ed_perc_ba_other_race_female_est',\n                  'HC02_EST_VC70': 'ed_perc_hs_hispanic_est',\n                  'HC04_EST_VC70': 'ed_perc_hs_hispanic_male_est',\n                  'HC06_EST_VC70': 'ed_perc_hs_hispanic_female_est',\n                  'HC02_EST_VC71': 'ed_perc_ba_hispanic_est',\n                  'HC04_EST_VC71': 'ed_perc_ba_hispanic_male_est',\n                  'HC06_EST_VC71': 'ed_perc_ba_hispanic_female_est'\n                 }","8a6f76cc":"def _add_ACS_column(data, column, output):\n    try:\n        to_add = data[['GEO.id', 'GEO.id2', 'GEO.display-label', column]].copy()\n        output = pd.merge(output, to_add, on=['GEO.id', 'GEO.id2', 'GEO.display-label'])\n    except KeyError:\n        pass\n    return output\n\n\ndef _add_ACS_topic(data, output, col_list):\n    data = data.rename(columns=col_list)\n    col_list = list(col_list.values())\n    for col in col_list:\n        output = _add_ACS_column(data, col, output)\n    return output\n\n\ndef prepare_ACS(dept):\n    \"\"\"\n    This function merges together the chosen columns for all the topics in the census data\n    \"\"\"\n    topics = [topic for topic in list(dept.keys()) if '_meta' not in topic \n              and 'police' not in topic and 'shapefile' not in topic \n              and 'education-attainment-over-25' not in topic]  # it is redundant\n    \n    switcher = {  # this is because python is cool by I still miss a switch statement\n        'poverty': poverty_list,\n        'poverty-status': poverty_list,\n        'owner-occupied-housing': housing_list,\n        'race-sex-age': race_list,\n        'race-age-sex': race_list,\n        'income': income_list,\n        'education-attainment': education_list,\n        'employment': employment_list\n        }\n    \n    output = dept['education-attainment'][['GEO.id', 'GEO.id2', 'GEO.display-label']].copy()\n    size = 0\n    \n    for topic in topics:\n        col_list = switcher.get(topic)\n        size += len(col_list.keys())\n        output = _add_ACS_topic(dept[topic], output, col_list)\n        \n    print(f\"Expected size of the output: {size} columns\")\n    print(f\"Available data: {output.shape}\")\n    return output","90470142":"df = prepare_ACS(dept) # it was imported above\ndf.head(3)","347a2107":"def _wavg(data, column, weight):\n    return np.average(data[column], weights=data[weight])\n\n\ndef _print_stats(data, col_list, total='total_population'):\n    try:\n        tmp = data[[total] + col_list].fillna(0)\n        for col in col_list:\n            print('{}: {}'.format(col, round(_wavg(tmp, col, total),3)))\n    except KeyError:\n        print('Total population unavailable, the means are not weighted')\n        tmp = data[col_list].fillna(0)\n        for col in col_list:\n            print('{}:{}'.format(col, round(tmp[col].mean(),3)))\n\n            \ndef _print_perc(data, col_list):\n    for col in col_list:\n        min_perc = data[col].min()\n        med_perc = data[col].median()\n        max_perc = data[col].max()\n        print(col)\n        print(f'\\t Min: {min_perc}')\n        print(f'\\t Median: {med_perc}')\n        print(f'\\t Max: {max_perc}')\n\n\ndef overview_ACS(data):\n    tot_pop = data[[col for col in data.columns if col.startswith('total_')]].sum()\n    tot_pop = round(tot_pop \/ tot_pop[0] * 100, 2)\n    print(tot_pop)\n    print(\"_\"*40)\n\n    race_perc = [col for col in data.columns if col.startswith('perc_')]\n    _print_perc(data, race_perc)\n\n\ndef unemployment_ACS(data):\n    unemp_cols = [col for col in data.columns if 'e_unemp' in col and '_est' in col]\n    _print_stats(data, unemp_cols)\n            \n\ndef poverty_ACS(data):\n    pov_cols = [col for col in data.columns if 'below_pov_perc_est' in col]\n    _print_stats(data, pov_cols)\n    print(\"_\"*40)\n    _print_perc(data, pov_cols)\n\n\ndef income_ACS(data):\n    inc_cols = [col for col in data.columns if 'median_income' in col and '_est' in col]\n    mean_inc = round(data[inc_cols].mean(),1)\n    max_inc = round(data[inc_cols].max(), 1)\n    min_inc = round(data[inc_cols].min(), 1)\n    print('Mean of medians: ' + '-'*10)\n    print(mean_inc)\n    print('Max of medians: ' + '-'*10)\n    print(max_inc)\n    print('Min of medians: ' + '-'*10)\n    print(min_inc)\n    \n    \ndef education_ACS(data):\n    ed_cols = [col for col in data.columns if 'ed_perc_' in col and\n               'male' not in col and 'female' not in col]\n    _print_perc(data, ed_cols)\n        \n\ndef summarize_ACS(data):\n    print(\"Population overview (estimated totals)\")\n    overview_ACS(data)\n    print('\\n')\n    \n    print(\"Unemployment rate (weighted averages)\")\n    unemployment_ACS(data)\n    print('\\n')\n    \n    print('Below poverty level (weighted averages)')\n    poverty_ACS(data)\n    print('\\n')\n    \n    print('Median income (means and ranges)')\n    income_ACS(data)\n    print('\\n')\n    \n    print('Education (estimated percentages)')\n    education_ACS(data)\n    print('\\n')","310e3f86":"summarize_ACS(df)","230f0d51":"def _drop_columns(feats, additional=None):\n    \"\"\"\n    This function takes a list of features and removes DETAILS and ID.\n    The user can provide an additional list to remove more features\n    \"\"\"\n    to_drop = ['DETAILS', 'ID']\n    if additional:\n        to_drop = to_drop + additional\n    feats = [feat for feat in feats if feat not in to_drop]\n    return feats       \n\n\ndef _get_columns(data):\n    \"\"\"\n    This helper finds the columns regarding subjects and officers.\n    The prefix SUBJECT_ and OFFICER_ are removed.\n    It returns a list of columns regarding subjects, one regarding officers\n    and one with their intersection\n    \"\"\"\n    subj = [col.replace('SUBJECT_', '') for col in data.columns if 'SUBJECT' in col]\n    off = [col.replace('OFFICER_', '') for col in data.columns if 'OFFICER' in col]\n    conf = list(set(subj).intersection(off))\n    conf = _drop_columns(conf)\n    return subj, off, conf\n\n\ndef subj_v_off(data, conf):\n    \"\"\"\n    This function takes the data and a list of columns describing both subjects and columns\n    Accordingly to the nature of the columns, it produces side by side plots and prints some \n    descriptive statistics (count, crosstabs)\n    \"\"\"\n    num = len(conf)\n    # 2 plots side by side for each category\n    fig, ax = plt.subplots(num,2, figsize=(15,5*num))\n    i = 0\n    for feat in conf:\n        off = 'OFFICER_' + feat\n        subj = 'SUBJECT_' + feat\n        print(feat)\n        if feat in ['GENDER', 'RACE', 'HOSPITALIZATION', 'INJURY', 'INJURY_TYPE']:\n            print('Officers: ' + '-'*40)\n            print(data[off].value_counts(dropna=False, normalize=True).head(10))\n            \n            print(\"Subjects: \" + '-'*40)\n            print(data[subj].value_counts(dropna=False, normalize=True).head(10))\n            \n            if (len(data[subj].unique()) > 10 or len(data[off].unique()) > 5):\n                print(\"Too many unique values, crosstab not printed\")\n            else:\n                print(\"Crosstab: \" + '-'*40)\n                print(pd.crosstab(data[subj], data[off], \n                                  dropna=False, margins=True))\n                print(pd.crosstab(data[subj], data[off], \n                                  dropna=False, normalize=True, margins=True))\n            print(\"_\"*40)\n            print(\"\\n\")\n            if num == 1: # dirty escape for poor usage of subplots\n                sns.countplot(x=off, data=data, ax=ax[0], \n                              order=data[off].value_counts().iloc[:5].index) # plot only top 5 \n                sns.countplot(x=subj, data=data, ax=ax[1], \n                              order=data[subj].value_counts().iloc[:5].index)\n            else:\n                sns.countplot(x=off, data=data, ax=ax[i][0], \n                              order=data[off].value_counts().iloc[:5].index)\n                sns.countplot(x=subj, data=data, ax=ax[i][1], \n                              order=data[subj].value_counts().iloc[:5].index)\n                i = i + 1\n                \n        elif feat in ['AGE']:\n            print('Officers: ' + '-'*40)\n            print(f\"\\t- mean: {data[off].mean()}\")\n            print(f\"\\t- median: {data[off].median()}\")\n            print(f\"\\t- range: {data[off].min()}--{data[off].max()}\")\n            print(f\"\\t- std: {data[off].std()}\")\n            \n            print('Subjects: ' + '-'*40)\n            print(f\"\\t- mean: {data[subj].mean()}\")\n            print(f\"\\t- median: {data[subj].median()}\")\n            print(f\"\\t- range: {data[subj].min()}--{data[subj].max()}\")\n            print(f\"\\t- std: {data[subj].std()}\")\n            print(\"_\"*40)\n            print(\"\\n\")\n            if num == 1:\n                sns.distplot(data[off].dropna(), bins = 30, ax=ax[0])\n                sns.distplot(data[subj].dropna(), bins = 30, ax=ax[1])\n            else:\n                sns.distplot(data[off].dropna(), bins = 30, ax=ax[i][0])\n                sns.distplot(data[subj].dropna(), bins = 30, ax=ax[i][1])\n                i = i + 1\n\n\ndef _cross_cat_cont(data, cont, cat, title=None):\n    \"\"\"\n    This function plots a histogram of a continuous variable by segmenting it\n    according to a categorical variable\n    \"\"\"\n    g = sns.FacetGrid(data, hue=cat, height= 5)\n    g.map(plt.hist, cont, alpha= 0.3, bins=30)\n    g.add_legend()\n    if title:\n        plt.title(title)\n        \n\ndef _experience_segm(data, segment, col=None):\n    \"\"\"\n    This function plots the officers year on force, segmented by 2 categories (if provided)\n    \"\"\"\n    g = sns.FacetGrid(data, col=col, hue=segment, height= 5)\n    g.map(plt.hist, 'OFFICER_YEARS_ON_FORCE', alpha= 0.3, bins=30)\n    g.add_legend()\n    \n\ndef individuals(data, feats, role='SUBJECT'):\n    \"\"\"\n    Prints and plots a summary of the features regarding subjects and officers\n    The output depends on what is available\n    \"\"\"\n    condition = all(x in feats for x in ['AGE', 'RACE'])\n    if condition:\n        _cross_cat_cont(data, role + '_AGE', role + '_RACE')\n    \n    condition = all(x in feats for x in ['AGE', 'GENDER'])\n    if condition:\n        _cross_cat_cont(data, role + '_AGE', role + '_GENDER')\n    \n    condition = all(x in feats for x in ['RACE', 'WAS_ARRESTED'])\n    if condition:\n        print(pd.crosstab(data[role + '_RACE'], data[role + '_WAS_ARRESTED'], \n                                  dropna=False, normalize='index', margins=True))\n        print(\"_\"*40)\n        print('\\n')\n        \n    condition = all(x in feats for x in ['RACE', 'INJURY'])\n    if condition:\n        print(pd.crosstab(data[role + '_RACE'], data[role + '_INJURY'], \n                                  dropna=False, normalize='index', margins=True))\n        print(\"_\"*40)\n        print('\\n')\n        \n    condition = all(x in feats for x in ['RACE', 'HOSPITALIZATION'])\n    if condition:\n        print(pd.crosstab(data[role + '_RACE'], data[role + '_HOSPITALIZATION'], \n                                  dropna=False, normalize='index', margins=True))\n        print(\"_\"*40)\n        print('\\n')\n        \n    condition = all(x in feats for x in ['YEARS_ON_FORCE', 'INJURY'])\n    if condition:\n        _cross_cat_cont(data, role + '_YEARS_ON_FORCE', role + '_INJURY')\n    \n    condition = all(x in feats for x in ['YEARS_ON_FORCE'])\n    if condition:\n        try:\n            _experience_segm(data, 'SUBJECT_RACE', col='SUBJECT_INJURY')\n        except KeyError:\n            _cross_cat_cont(data, role + '_YEARS_ON_FORCE', 'SUBJECT_RACE')\n        try:\n            _experience_segm(data, 'SUBJECT_GENDER', col='SUBJECT_INJURY')\n        except KeyError:\n            _cross_cat_cont(data, role + '_YEARS_ON_FORCE', 'SUBJECT_GENDER')\n        \n            \ndef explore_police(data):\n    \"\"\"\n    Wrapper for the functions above, calls the appropriate function given \n    what is available\n    \"\"\"\n    subj, off, conf = _get_columns(data)\n    if len(subj) > 0:\n        try:\n            individuals(data, subj, 'SUBJECT')\n        except Exception as e:\n            print(\"Something went wrong in exploring the subjects\")\n            print(e)\n            pass\n    if len(off) > 0:\n        try:\n            individuals(data, off, 'OFFICER')\n        except Exception as e:\n            print(\"Something went wrong in exploring the officers\")\n            print(e)\n            pass\n    if len(conf) > 0:\n        try:\n            subj_v_off(data, conf)\n        except Exception as e:\n            print(\"Something went wrong in comparing subjects and officers\")\n            print(e)\n            pass\n    print(f\"Subject related variables found: {subj}\")\n    print(f\"Officer related variables found: {off}\")","b7885420":"pol_df = dept['police_1'].copy()\nexplore_police(pol_df)","ef690d95":"col_list = ['SUBJECT_RACE', 'SUBJECT_GENDER', 'SUBJECT_INJURY', 'OFFICER_INJURY', \n            'SUBJECT_WAS_ARRESTED', 'SUBJECT_HOSPITALIZATION']\n\n\ndef _summary_cleanup(data, distr_col):\n    \"\"\"\n    Keep only the columns selected above (plus the district)\n    \"\"\"\n    feats = [distr_col] + [col for col in data.columns if col in col_list]\n    return data[feats]\n\n\ndef police_by_distr(data, distr_col):\n    \"\"\"\n    The police data are reduced to the one selected above\n    and summarized according to the distr_col column.\n    \n    Returns a datafram with the aggregated data.\n    \"\"\"\n    data = _summary_cleanup(data, distr_col)\n    \n    try:\n        tot_df = data[[distr_col, data.columns[-1]]].groupby(distr_col, as_index=False).count()\n        tot_df.columns = [distr_col, 'total_records']\n    except ValueError:\n        print(\"Insufficient data to aggregate\")\n        return data.head()\n    sum_cols = [col for col in data.columns if\n                ('RACE' not in col) and ('GENDER' not in col)]\n    sum_df = data[sum_cols].groupby(distr_col, as_index=False).agg('sum')\n    summary = pd.merge(tot_df, sum_df)\n    \n    if 'SUBJECT_RACE' in data.columns:\n        race = data.groupby([distr_col, 'SUBJECT_RACE']).size().unstack().reset_index().fillna(0)\n        summary = pd.merge(summary, race, on=distr_col)\n        \n    if 'SUBJECT_GENDER' in data.columns:\n        gender = data.groupby([distr_col, 'SUBJECT_GENDER']).size().unstack().reset_index().fillna(0)\n        summary = pd.merge(summary, gender, on=distr_col)\n    \n    return summary","deef7f2f":"conversion = {'Yes': 1, 'No': 0}\n\npol_df.SUBJECT_INJURY = pol_df.SUBJECT_INJURY.map(conversion)\npol_df.OFFICER_INJURY = pol_df.OFFICER_INJURY.map(conversion)\npol_df.SUBJECT_WAS_ARRESTED = pol_df.SUBJECT_WAS_ARRESTED.map(conversion)","74b7144b":"police_by_distr(pol_df, 'LOCATION_DISTRICT')","7b0b1b38":"shapes = list_all_files(location='..\/input\/data-science-for-good\/cpe-data\/',pattern='.shp')\nshapes = [file for file in shapes if file.endswith('.shp')]\nshapes","e3e00513":"for file in shapes[:3]: # printing them all would be horrible to see\n    print(file)\n    sh = gp.read_file(file)\n    print(sh.columns)\n    print(\"\\n\")","205f7c54":"dept['shapefile_1']","df6c038d":"dept['shapefile_1'].plot(column='Name')","503c6c26":"ref_id = gp.read_file('..\/input\/texas-shape\/cb_2015_48_tract_500k.shp')\nref_id.to_crs({'init': 'epsg:32118'},inplace=True) # because meters are better\nref_id = ref_id[['GEOID', 'geometry']].copy()\nref_id.rename(columns={'GEOID' : 'GEO.id2'}, inplace=True)\nref_id['GEO.id2'] = pd.to_numeric(ref_id['GEO.id2'])\nref_id.head()","d38139e3":"ref_id.plot()","72e05fc6":"dallas_sh = ref_id[ref_id['GEO.id2'].isin(df['GEO.id2'])]\ndallas_sh.plot()","0be7f7d3":"pol_sh = pol_df.dropna(subset = ['LOCATION_LONGITUDE','LOCATION_LATITUDE']).copy()\npol_sh['geometry'] = list(zip(pol_sh['LOCATION_LONGITUDE'],pol_sh['LOCATION_LATITUDE']))\npol_sh['geometry'] = pol_sh['geometry'].apply(Point)\npol_sh = gp.GeoDataFrame(pol_sh, geometry = 'geometry',\n                         crs ={'init': 'epsg:4326'} )\npol_sh.to_crs({'init': 'epsg:32118'},inplace = True)\n\npol_sh = pol_sh.dropna(subset = ['geometry'])\n\npol_sh.plot()","41356eb4":"def find_geoid(city, pol):\n    geoids = []\n    for point in pol.geometry:\n        try:\n            geoids.append(city[city.geometry.contains(point)]['GEO.id2'].values[0])\n        except IndexError:\n            geoids.append(np.nan)\n    return geoids","0bbc1323":"pol_sh['GEO.id2'] = find_geoid(dallas_sh, pol_sh)\npol_sh.head()","b9f93ec4":"dept = import_dept(dept_list[0])\ndept.keys()","3ce16984":"pol_df = dept['police_1'].copy()\npol_df.rename(columns={'SUBJECT_RACT' : 'SUBJECT_RACE'}, inplace=True)\nexplore_police(pol_df)","364cc4fc":"pol_df.columns","2c154589":"pol_df.CHARGE.value_counts().head()","677dc2c1":"pol_df[pol_df.CHARGE == 'Resisting Law Enforcement (M)'].SUBJECT_GENDER.value_counts()","8f0d7df0":"pol_df['LOCATION_DISTRICT'].value_counts().head(8)","39562849":"pol_df['REASON_FOR_FORCE'].value_counts()","c4f9a175":"pd.crosstab(pol_df['SUBJECT_RACE'], pol_df['REASON_FOR_FORCE'])","fbed2e3f":"pd.crosstab(pol_df['SUBJECT_RACE'], pol_df['REASON_FOR_FORCE'], normalize='index', margins=True)","cb9d6e51":"types_code = ['Physical', 'Lethal', 'Less Lethal']  # the order is sadly crucial or everything is lethal\n\npol_df.TYPE_OF_FORCE_USED = pol_df.TYPE_OF_FORCE_USED.fillna('Other')\n\nfor code in types_code:\n    pol_df.loc[pol_df.TYPE_OF_FORCE_USED.str.contains(code), 'TOF_code'] = code\n\npol_df.TOF_code = pol_df.TOF_code.fillna('Other')\n\npol_df.TOF_code.value_counts()","68650750":"pd.crosstab(pol_df['SUBJECT_RACE'], pol_df['TOF_code'])","b7264c0f":"pd.crosstab(pol_df['SUBJECT_RACE'], pol_df['TOF_code'], normalize='columns', margins=True)  \n# warning, the normalization is across races this time","d59570ce":"pd.crosstab(pol_df['REASON_FOR_FORCE'], pol_df['TOF_code'], margins=True)","387704e5":"pol_df[pol_df.TOF_code == 'Lethal'][['SUBJECT_DETAILS', \"TYPE_OF_FORCE_USED\", \"REASON_FOR_FORCE\", \"CHARGE\",\n                                     \"SUBJECT_RACE\", 'OFFICER_RACE', 'SUBJECT_AGE', 'OFFICER_YEARS_ON_FORCE']]","e070e6c4":"df_ACS = prepare_ACS(dept)\nsummarize_ACS(df_ACS)","93f26110":"ref_id = gp.read_file('..\/input\/indiana-shape\/cb_2015_18_tract_500k.shp')\nref_id.to_crs({'init': 'epsg:32118'},inplace=True) # because meters are better\nref_id = ref_id[['GEOID', 'geometry']].copy()\nref_id.rename(columns={'GEOID' : 'GEO.id2'}, inplace=True)\nref_id.head()","880a321c":"ref_id.plot()","9d5dbcbc":"ref_id['GEO.id2'] = pd.to_numeric(ref_id['GEO.id2'])\nacs = pd.merge(df_ACS, ref_id, on='GEO.id2', how='left')","065226b3":"sh = dept['shapefile_1'].copy()\nsh.plot(column='DISTRICT')","76584f1c":"sh.to_crs({'init': 'epsg:32118'},inplace=True)\nsh.plot(column='DISTRICT')","fc4120d6":"sh = sh[['DISTRICT', 'geometry']].copy()\n\ninter_shape = []\nfor index, crim in sh.iterrows():\n    for index2, popu in acs.iterrows():\n        if crim['geometry'].intersects(popu['geometry']):\n            inter_shape.append({'geometry': crim['geometry'].intersection(popu['geometry']),\n                         'district': crim['DISTRICT'],\n                         'GEO.id2' : popu['GEO.id2'],\n                         'area':crim['geometry'].intersection(popu['geometry']).area})\n            \ninter_shape = gp.GeoDataFrame(inter_shape,columns=['geometry', 'district', 'GEO.id2','area'])\n\ninter_shape.head()","a5bdf477":"tmp = inter_shape[['GEO.id2', 'district', 'area']].groupby(['GEO.id2', 'district'], as_index=False).sum()\ntmp_2 = inter_shape[['GEO.id2', 'area']].groupby(['GEO.id2'], as_index=False).sum()\ninter_shape = pd.merge(tmp, tmp_2, on='GEO.id2')\ninter_shape['fraction'] = inter_shape['area_x'] \/ inter_shape['area_y']\ndel inter_shape['area_x']\ndel inter_shape['area_y']\ndel tmp\ndel tmp_2\ninter_shape.head()","371594e5":"# and finally merge census and shapefile so that we have the districts\nacs_merged = pd.merge(df_ACS, inter_shape)\nacs_merged.head()","f113a044":"renaming = {'East District': 'East',\n           'Southwest District': 'Southwest',\n           'Northwest District': 'Northwest',\n           'North District': 'North',\n           'Southeast District': 'Southeast',\n           'Downtown  District': 'Downtown'}\n\npol_df.LOCATION_DISTRICT = pol_df.LOCATION_DISTRICT.map(renaming).fillna('Excluded')\n\npol_df.LOCATION_DISTRICT.value_counts(dropna=False)","3292304b":"# recall again the police_by_district aggregation\npol = police_by_distr(pol_df, \"LOCATION_DISTRICT\")\npol","c8522dd0":"def acs_by_district(data):\n    data = data[[col for col in data.columns if 'GEO' not in col \n                 and 'geometry' not in col]].copy()\n    \n    # applying the fraction to the merged dataframe\n    fraction = data['fraction']\n    del data['fraction']\n    cols = [col for col in data.columns if 'district' not in col]\n    data[cols] = data[cols].multiply(fraction, axis=\"index\")\n    \n    sel = ['district'] + [col for col in data.columns if '_est' in col \n                          or col.startswith('total_')]\n    \n    # grouping the totals\n    tot_cols = [col for col in sel if 'perc' not in col\n               and 'rate' not in col and 'median' not in col]\n    totals = data[tot_cols].groupby('district', as_index=False).sum()\n    \n    # grouping the proportions\n    try:\n        prp = [col for col in sel if 'perc' in col\n                                   or 'rate' in col]\n        prop_cols = ['district'] + prp\n        props = data[prop_cols].copy()\n        # make them proportions\n        props[prp] = props[prp].multiply(0.01, axis='index')\n        # groupby with weighted average\n        wm = lambda x: np.average(x, weights=data.loc[x.index, \"total_population\"])\n        props = props.groupby('district', as_index=False).agg(wm)    \n    except KeyError:\n        print(\"Total population unavailable, percentages and rates can't be summarized\")\n        \n    # mergin together\n    summary = pd.merge(totals, props, on='district')\n    \n    return summary","61340b7e":"acs_agg = acs_by_district(acs_merged)\nacs_agg","af5eef2b":"pol.rename(columns={'LOCATION_DISTRICT': 'district'}, inplace=True) # To merge easily\ntot_agg = pd.merge(acs_agg, pol, on='district')\ntot_agg","2d063c07":"max_rec = tot_agg.total_records.max()\nmin_rec = tot_agg.total_records.min()\ntot_rec = tot_agg.total_records.sum()\nprint(tot_rec, max_rec, min_rec)","ca1f621a":"tot_agg['records_pp'] = tot_agg.total_records \/ tot_agg.total_population\ntot_agg[['district', 'total_records', 'total_population', 'records_pp']]","41ef5e82":"tot_agg['pol_white_perc'] = tot_agg.White \/ tot_agg.total_records # percentages of the total police records, by race\ntot_agg['pol_black_perc'] = tot_agg.Black \/ tot_agg.total_records\ntot_agg['pol_hispanic_perc'] = tot_agg.Hispanic \/ tot_agg.total_records\n\ntot_agg['records_pp_white'] = tot_agg.White \/ tot_agg.total_white  # crime rate by race\ntot_agg['records_pp_black'] = tot_agg.Black \/ tot_agg.total_black\ntot_agg['records_pp_hispanic'] = tot_agg.Hispanic \/ tot_agg.total_hispanic\n\ntot_agg[['district', 'records_pp', 'total_white', 'total_black', 'total_hispanic', \n         'pol_white_perc', 'pol_black_perc', 'pol_hispanic_perc', 'records_pp_white', 'records_pp_black', 'records_pp_hispanic']]","0356d10b":"It would be interesting to see how other socio-demographic factors play a role in having different crime rates in different districts (*Why Downtown has such a high crime rate?*, etc). \n\nAnother interesting question regards the different distribution of the population (the southern districts are mostly white citizens, while black citizens live in the northern ones)\n\nI will reserve these studies to a future kernel since I don't have time to do so before the end of the competition.\n\n\n# Conclusions and next steps\n\nThis kernel has great margins of improvements and customization. The cases here presented (Dallas for the exposition of the base functions and Indianapolis for the case study) were chosen fairly randomly and I am particularly happy about being able to change an index on the top of the page and get similar results without too much work. All that being said, choices were made along the way and were biased by my curiosity, interest, and ability of execution. Therefore I see this as a first step and I really hope it will inspire someone to make it better or to perform a more accurate analysis.\n\nOne thing that I take away from this kernel is the following. Summary statistics and aggregations are necessary to peek into the message contained in very large datasets. However, it has to be kept in mind that the story they tell is always an approximation of a complex situation. **Different levels of granularity can tell different stories**. Pretty much in the same way people say *Africa is poor* or *Europe is rich*, completely ignoring that Africa is a very big place with very different stories to tell.\n\nI say that because we often observe that the percentages of crimes by race are not proportional to the underlying population and any statistical test of goodness of fit will tell that no, that sample is not randomly extracted from the population. When this happens, the counter-argument is sometimes related to different crime rates and socioeconomic indicators, etc. All of this is true but they are all a simplification of a very complex argument, ignoring relations of cause and effect while focusing on correlations.\n\nOn the other side, the response of trained officials should be guaranteed by a democratic State and, if we observe that it is not uniform if the color of the skin of a suspect is different, then we have a problem. Here, we have seen that the police was behaving fairly uniformly in any use of force, except for the lethal use of force. This is my definition of injustice.\n\nHowever, for the sake of completeness, even this result is partial. Not only a table does not contain the nuances of a situation but also does not contain all the counterfactuals. In other words, we don't know from this data how many times an officer could have used lethal force but didn't as well as we don't know what would have happened if the lethal force was not used. In my vision, a complete analysis would require that as well, or it will risk being biased in searching for a bias.\n\n\nThank you for reading this far, please let the feedback come, it is a very important topic that requires the attention of everyone.","40dbafd0":"The CHARGE column can be very interesting.","c8e7de06":"While at a first look there is not any statistically significant difference, it is hard to not notice that of the 18 lethal uses of force, 13 had a black person as a victim. We, unfortunately, do not have further information about the situation that led to these outcomes but we can say that, **while in any other type of force used roughly half of the subjects were black, in the case of lethal use of force 72% is black**. \n\nThe situation starts looking even worse if we then look at the reason for force, which is mostly *Fleeing*. ","6c8fe883":"Did I mention that dictionaries are awesome? \n\nLet's have a look at one of the files","9aaeb1f7":"As we see, not everything we wanted is then available. We have to live with that, I guess.\n\nThe next step is to quickly explore what we have created. This is done by the following set of functions (wrapped by `summarize_ACS()`).","c1b7ff05":"No, it requires more study.\n\nWe can see that geographically we have expected districts and other values that will probably require recoding","50e2de65":"Now we have all the shapes of Texas","543c0578":"Yep, that looks like Indiana. To merge it with our census data we need a small correction but after that we can give a polygon to every geographical Id","f56fe622":"We have achieved our goal: put police and census data together at some level of granularity when we don't know were exactly the accidents took place. Let's explore the final result.","a5201565":"This result will be useful once that we will do the same kind of aggregation on the census data. For now, we can only see that there are districts with more records than others, that the subject injury rate is more or less constant across the departments while the officer's one has some interesting cases where it is much lower. \n\nIn some districts it may look like the race is playing a role but, since we don't have any information about the population of those districts yet, it is a merely descriptive result.\n\nNow, how do we link this result to the census data explored above? While we have some geographical Id on the census data, we don't have a way to link it to a specific location. \n\nThe strategy is to use shapefiles associated with census data, which can be found at https:\/\/www.census.gov\/geo\/maps-data\/data\/cbf\/cbf_tracts.html, and use it to link it to the police data. We can do so in various ways:\n\n* If we have the address, one can use the Google API to get the coordinates (if not the district name directly)\n* If we have the coordinates, one can automatically check if they fall inside a specific location (labeled with the geographical Id of the census data)\n* Or we can use the provided shapefiles, intersect them with the census one, get the district names and aggregate to finally put all our data in one table.\n\nIn this particular case, we have coordinates and address, but it is fairly an anomaly. In the case study below we will explore how to implement the third option with yet another attempt to automating the process.\n\nBut for now we move on to the third source of data.\n\n# Shapefiles\n\nSome preliminary exploration of the shapefiles comes with the quality check and the import. Thus we already know that some department have all we need: 3 mandatory files (`['.shp', '.shx', '.dbf']`) and the equally useful `'.prj'`. The first three are necessary to make the gelocalization api of choice work but the last one is the one that tells us the coordinate system of the shapefile, without it we will have difficulties in plotting something on top of our map.\n\nLet's see what columns we have in these shapefiles with a simple routine similar to the one above (yes, the function `list_all_files` is something I like as much as the python dictionaries)","b6cf9250":"We can even plot it.","841c1ec7":"In seconds we know that in this department 60% of the population is white, but there are places where the percentage of black people is 98% and the Hispanic ethnicity is very present. We also see that black people have a higher unemployment rate and more of them are below the poverty level. At last, the access to higher education looks unevenly distributed across races.\n\nNaturally, this is a very superficial analysis but in my mind this is what you do the first time you open a file. Moreover, it is not hard to extend this quick report with more accurate analysis with automatic tests of statistical significance and graphs. All that being said, the main (and, let's face it, the only) merit of this approach is that it is very flexible across the departments and a simple loop on what I called `dept_list` can give you insights about what department is really lacking information (its output would be much shorter).\n\n# Police data\n\nThe main data source for this problem is coming from this data and it is time to adopt the same approach, aimed towards speed and flexibility, to have a first look ","e48bff58":"Now, so far we have prepared the census data so that we have a district and we made sure that the police data are aggregated by the same districts. Before finally put everything together, we need to aggregate the census data as well. In doing so, we have to take into account what fraction of a GEO.Id falls into a given district. This is done by the following function","525eb301":"As we see both from the log messages of the import and from this table, the census data are very wide tables.\n\nI personally struggled a lot to find out what information I could take away from DataFrames with so many non-descriptive columns. Luckily we have the metadata and I made up this simply routine to find out what is in there.","8b0b0165":"Since in this particular district the injuries are coded with Yes\/No, I need to do a little of processing first. ","ed37816a":"And if we plot it we see that is really the State of Indiana","30d982c5":"The merit of this function is that it spots further inconsistencies (if it fails, something is odd) and makes everything available very quickly.\n\nLet's see what departments are available once more and import a random one to show how the function works. Re-running the next cell with a different department can give you more information regarding how flexibles are the methods here presented (and, of course, what are their limitations).","e5a29e82":"With a few lines of code we know what the columns mean. For example:\n\n* HC01 is about Total numbers\n* HC02 is about Totals below the poverty level\n* HC03 is about the percentage below the poverty level.\n\nThen we know that each VC code is corresponding to a different segment of the population (either by race, gender, education, age, etc.).\n\nIt is an overwhelming feeling the one of having to look into so many different segments. That's why I propose the following selection\n\n# ACS summary\n\nThis section is aimed to create one DataFrame with all the information that I found relevant by reading their description. In doing so, it also gives a quick overview of the ACS data.\n\nThe proposed columns are (and it is very easy to add or remove some of them):","f15dac15":"Before moving on, there are some columns that got my curiosity and were not included in the summary. I want to spend some time to have a look at differences in reasons for the use of force and type of use of force across different races","3677da39":"Not as many car crashes as I was hoping for (weird sentence, I know). I wish I could say more about these cases, but I have no data to do so.\n\nLet's move on to the census data, we can use the functions to put them together and summarize according to the selection above.","39141cd8":"As mentioned before, the police data for the city of Dallas, TX contains longitude and latitude for almost every event. If we are interested in knowing if different districts behave differently, we could overlap these coordinates with the shapefile provided by the organizers. There is no way I can do it better than Pavan Kumar Kulkarni and I won't try to top that excellent kernel that I invite you to read (https:\/\/www.kaggle.com\/pavankumarkulkarni\/measure-of-justice-comprehensive-framework).\n\nWhat I want to do instead is to combine it with the census data. Having the coordinates, the combination of the two sources will be as granular as possible, i.e. any other case will require a higher level of aggregation (with the approximation that comes with that).\n\nAs an external source, I put the shapes provided by the census website.\n","eb756c51":"We see that we have always the column `geometry`, which contains the polygons or the points for the shape, something about districts or zones (can be called `name`, `district`, `precint`, or variations of this). In particular, the random one we have already imported si one of the simplest among the available.","8150f537":"This can include car crashes while attempting an escape. So let's have a look at the not recoded data","81aa59e2":"And now we have all we wanted: geographic id's on the police data, ready to be merged with the census data for a more in-depth analysis. Moreover, a further overlap with the district shapefile is still possible (and it will be done in the case study below)\n\nAs said before, this is a very benevolent case because the coordinates are not always provided. To see what happens in a more common case, let's have a look at Indianapolis for our case study.\n\n# Case study - Marion County, Indiana\n\nHere I pick a department that can give us enough to have a taste of most of the functionalities here presented, while still having some obstacles that will give us the chance of seeing how much effort is required to overcome them.\n\nLet's start with the import.","76c6293d":"* The population is mostly white, with a 63% that is fairly in line with the country rate. For this kind of things there is always some ambiguity regarding the Hispanic ethnicity and I still look for a solid solution.\n* The maximum and minimum in the race percentages by location reveals that there are places where racial diversity is not achieved at all. This indicates that any aggregation at the city level is limited since it misses the diverse situation across this department.\n* The unemployment rate is not uniformly distributed, having a higher rate for black people (and males).\n* Not surprisingly given the previous result, the poverty rate for black people is higher. On the other side, females have a higher poverty rate regardless of being more employed (although it is very close to the average)\n* The Hispanic ethnicity seems to be more vulnerable economically, this is an interesting result because it goes across races (being an ethnicity). Unfortunately, my selection is limiting the investigation and we would need to go back to the imported file to know more.\n* For everyone but white people, there is at least one location with 100% poverty rate. This can be due to numerically low representatives in some locations but, looking at the medians, we see that Blacks and Hispanics are more vulnerable in this sense.\n* The median incomes might indicate more disparity among black people, which reach a much lower minimum and the slightly higher maximum.\n* Education-wise, white and black people seem to have a similar behavior up to the high school but for the higher education we observe some disparity. It is too little to say if this determines or is determined by some of the results summarized above.\n* The Hispanic ethnicity seems to have a bigger problem already at the high school level.\n\nNow, we have seen that we need to summarize these data at a different granular level. We thus need the census shapefiles for Indiana and the appropriate census track. Here it is provided as an external source.","109bca78":"Next, we saw during this competition (and every other competition or realistic situation) that the folder structure of the data was somewhat clear but can present some minor annoying peculiarity. Therefore, the next set of functions scans through the input files and check that some consistency requirements are matched. This step was crucial to quickly find small and big criticalities and has the merit of being fairly scalable for being a 0.0.1 version.\n\nThe checks implemented are:\n\n* Has every department the same set of topics (education, poverty, etc.)?\n* Are the geographic Id's consistent across the census data? \n* Do we have police data and shapefiles in good order?\n\nA serious error handling procedure is still missing, but this is good enough to give us a quick overview of what is available and what might require more attention.","9d035af8":"Let's quickly summarize this first result:\n\n* The arrest rate looks fairly constant across races. There are a few exceptions but, looking at the count plot for the subject race, those races are very little represented in our data.\n* Injury and hospitalization rates are again skewed towards the subjects, which makes sense due to the fact that officers are trained to first protect themselves. As before, white subjects have a higher injury rate than the average and this is also observed in the hospitalization rate. We don't have information about the injury types from this summary and it will require manual inspection.\n* There are some errors in the ages of subjects and officers because 2 years old officers would be silly (although maybe appropriate to chase down 2 years old subjects, I guess)\n* The officers are overwhelmingly white, while the subjects are mostly black.\n* Gender-wise, males dominate both the categories.\n* The age patterns reveals that the distribution is slightly more skewed towards the younger ages for black subjects and for female subjects.\n* The years of experience of the officers do not seem to matter in terms of how subjects of different races are treated or in terms of injuries.\n\nThis summary does not cover everything is available (although it could with some more development of the methods), so let's see what else is available.","709f40a0":"We see immediately that Dept_37-00027 is missing its projection file (as we know already thanks to other popular kernels) and that some departments have slightly different ways of calling some of the topics (poverty vs poverty-status, etc.). Before the cleaning done by the organizers, other inconsistencies were found by this function.\n\n# Import a department\n\nOnce that we know how the folder structure is, we need to have all the available data in one convenient structure. I chose a dictionary of DataFrames to be that structure because dictionaries are awesome.\n\nAn issue that can be quickly spotted is that the ACS files have a lot of columns and often they are missing the entries. On top of that, the missing entries are not a simple *NaN* but rather some symbol.  I wanted to take care of that after I spent so much time to check what the individual column codes meant to then find out it was little to no information in them.\n\nTherefore, the import process is also a generic cleaning of the data with missing values. As of now, every column with more than 30% of missing entries is not imported. In the future, there can be a method to change this parameter.","2965f60e":"You can tell I love the dictionaries, can't you?\n\nMoving on, the next function (with 2 helpers), is preparing a unique DataFrame. The reason this is done (effectively) column by column is that not every column is necessarily present in the imported data. We thus sacrify some speed for more flexibility.","79b87406":"We can then get all the shapes of Dallas","28a7198d":"Again, this is very basic and superficial but within seconds we can extract every available information (among the ones that were preselected) about the use of force in this department. We see that the majority of the subjects were black in a population that is predominantly white (as represented by the race of the officers, fairly in line with the 60% we know from the census data). We can see that the police officer is a male-dominated profession. \n\nWe also see that most of the times there is no injury (either for the subjects or the officers) but when the injury occurs is most likely to happen to the subject rather than to the officer and that white subjects have an injury rate above average. It would be interesting to investigate the nature of this cases.\n\nThis is the spirit that led me to the creation of these summaries: simplify the overwhelming quantity and variety of data in order to start a more serious investigation. This is why **it would be a mistake to stop here and come to any sort of conclusion**.\n\nMost of the files containing data regarding police actions also have a column that regards a district. For reasons that will be clear later, it is interesting to summarize the police data by district. So it is time for a new function. I will focus on summarizing race, gender, injuries, and consequences because the nature of the entries makes them easy to pass through this process automatically","31077554":"We need another step to have what we want. We need to find if each Geo.Id falls inside a district completely or it is shared. Some simple (an inelegant) table manipulation gives the answer","bd8edad5":"There are problems with packages and dependencies when I try to run my code on Kaggle. Thus, I propose here a dirty workaround, something that works but it is slow and  disgraceful","f92e3841":"Which makes sense because Downtown is reasonably an area with more police control.","039f0573":"I wonder why there are 2 Resisting Law Enforcement columns, maybe gender?","b037390d":"# Structure of the input data and health check\n\nThe first function is simply a helper to speed up every exploration of the data folder. ","1f0081d9":"We see that in some cases we drop quite a lot of columns, there is not much we can do about it.\n\nRunning `explore_police` now leads to catching some exceptions and the reason is that there is a typo in the columns. I fix it before moving forward","bb2207ef":"The LOCATION_DISTRICT of the police file has some different names and I suspect that the one that looked somewhat weird can be all categorized as Excluded. The next cell takes care of that","3686887a":"The Center for Policing Equity has the mission of helping law enforcement professionals to better protect and serve the citizens of their cities. The path that was chosen is to use data to find and document situations where the disparity in treatment can compromise the mission of *protect and serve*.\n\nThe data sources available to accomplish such mission are:\n* census data\n* policing data about use of force or other police-related actions\n* maps\n\nThe challenge is to effectively combine these sources in order to make easy for a field expert to discover and document critical situations. The primary objective is to combine police data with census data and, as we will see, the shapefiles are a way of achieving this. In this notebook, I will present some descriptive statistics about census data and police data in order to display how I think the automation process should be. It is important to remember that any conclusion that didn't pass through several levels of skepticism has to be considered incorrect. For example, knowing that the majority of arrests involves a specific race means nothing without knowing how the underlying population is composed. Even then, the available data can be enough to prove that there is a significant difference from one segment of the population to the other but they are not telling us yet the reason behind this difference. Therefore, I strongly invite the reader to keep a skeptic eye towards what follows and please let me know if there is something methodologically wrong or other silly mistakes.\n\nA quick look at the available data reveals that, for how much effort it was spent to standardize the input of such analysis, it is not obvious to immediately get what data is available and what isn't. The United States of America is a place with wildly diverse situations and the data describing different places represent this diversity not only in the content, but also in the format.\n\nIn other words, given a department and a suitable amount of time, it is not difficult to generate a report with the available data. However, the work has to be repeated almost from scratch once that we want to analyze the next department. On the other hand, investing time and resources in maintaining a healthy database can be a frustrating job because the data are coming from many different sources.\n\nTherefore, the solution proposed here aims to make easier to spot a problem in the data (thus before spending precious hours starting an analysis that can not be finished), have a quick overview of the relevant available information, and thus pave the way to combine and better explore a given department. It would foolish of me thinking that the proposed exploration is in any way satisfactory for an expert, simply because I do not have that expertise in this field. Here I want to propose a process that can facilitate the work of an expert, not to replace it. For this reason, every function was written with the intent of being customizable (either due to other preferences, different needs, or changes in the data). \n\nThe notebook can be summarized as follows:\n\n* Check the quality of the data.\n* Import every piece of information available for a department\n* Get an overview of the census data\n* Get an overview of the police data\n* A simple way of combining these two sources of data\n* A case study to show how to combine census and police data and example analysis.\n\nThe 2 cities used here are **Dallas** and **Indianapolis**. The former used just to introduce the various functions, the latter will also be subject of an analysis.\n\nThe code written for this kernel will be soon available at my github: https:\/\/github.com\/lucabasa\/cpe-automate","4149ba59":"We now use the provided shapefiles, which looks like this","23b16f9c":"We can't observe any noticeable difference between the reason of force across the races. The main reason is always Resisting Arrest, followed by Non-Compliant and Combative Suspect.\n\nTo see the type of force used, we need to do some processing","971e5caf":"With the police data, we can find all the locations we need","164a439b":"We immediately see that the projection is different, but luckily we can simply change it","34b9ae5e":"We are finally ready to put everything together","04b061d5":"What I want to do is to overlap this shapefile with the other one so that I know in which district each geographic Id falls in and in which proportion."}}