{"cell_type":{"f7bf1c32":"code","7f932c3c":"code","e81fe17a":"code","93df64b8":"code","5cfcd05c":"code","3fd7b7f4":"code","d5873377":"code","a14551b0":"code","299cde20":"code","bf383afe":"code","c659bb2c":"code","5b14cda4":"code","1052a16d":"code","db67708e":"code","04b6f5e2":"code","c6595ec6":"markdown","28f8ef00":"markdown","78952229":"markdown","59a6ace1":"markdown","66778dde":"markdown","ce3b6901":"markdown","1a499a5a":"markdown","bd111aa3":"markdown","b6627a7d":"markdown","f3b9863b":"markdown","365c968d":"markdown","0656180c":"markdown","3f09cf53":"markdown","0023d64d":"markdown","246720e1":"markdown","fdc96c1c":"markdown","0b8cdcc8":"markdown","1892d5db":"markdown","6485487e":"markdown"},"source":{"f7bf1c32":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport itertools\nimport h2o\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\n%matplotlib inline","7f932c3c":"h2o.init()","e81fe17a":"data_df = h2o.import_file(\"..\/input\/data.csv\", destination_frame=\"data_df\")","93df64b8":"data_df.describe()","5cfcd05c":"data_df.describe(1)","3fd7b7f4":"df_group=data_df.group_by(\"diagnosis\").count()\ndf_group.get_frame()","d5873377":"features = [f for f in data_df.columns if f not in ['id', 'diagnosis', 'C33']]\n\ni = 0\nt0 = data_df[data_df['diagnosis'] == 'M'].as_data_frame()\nt1 = data_df[data_df['diagnosis'] == 'B'].as_data_frame()\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(6,5,figsize=(16,24))\n\nfor feature in features:\n    i += 1\n    plt.subplot(6,5,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Malignant\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Benign\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();\n    ","a14551b0":"plt.figure(figsize=(16,16))\ncorr = data_df[features].cor().as_data_frame()\ncorr.index = features\nsns.heatmap(corr, annot = True, cmap='YlGnBu', linecolor=\"white\", vmin=-1, vmax=1, cbar_kws={\"orientation\": \"horizontal\"})\nplt.title(\"Correlation Heatmap for the features (excluding id, C33 & diagnosis)\", fontsize=14)\nplt.show()","299cde20":"train_df, valid_df, test_df = data_df.split_frame(ratios=[0.6,0.2], seed=2018)\ntarget = \"diagnosis\"\ntrain_df[target] = train_df[target].asfactor()\nvalid_df[target] = valid_df[target].asfactor()\ntest_df[target] = test_df[target].asfactor()\nprint(\"Number of rows in train, valid and test set : \", train_df.shape[0], valid_df.shape[0], test_df.shape[0])","bf383afe":"# define the predictor list - it will be the same as the features analyzed previously\npredictors = features\n# initialize the H2O GBM \ngbm = H2OGradientBoostingEstimator()\n# train with the initialized model\ngbm.train(x=predictors, y=target, training_frame=train_df)","c659bb2c":"gbm.summary()","5b14cda4":"print(gbm.model_performance(train_df))","1052a16d":"print(gbm.model_performance(valid_df))","db67708e":"gbm.varimp_plot()","04b6f5e2":"pred_val = list(gbm.predict(test_df[predictors])[0])\ntrue_val = list(test_df[target])","c6595ec6":"# <a id=\"3\">Read the data<\/a>  \n\nFor reading the data, we will use also H2O. First, we will initialize H2O.\n\n## Initialize H2O\n\nH2O will first try to connect to an existing instance. If none available, will start one. Then informations about this engine are printed.  At the end connection to the H2O server is attempted and reported.","28f8ef00":"## <a id=\"62\">Train  GBM<\/a> \n\nWe will use a GBM model.","78952229":"We can see that the AUC is 1 for the train set and Gini coeff is 1 as well. LogLoss is 0.01.\n\nLet's see the model performance for the validation set.","59a6ace1":"# <a id=\"4\">Check the data<\/a>  \n\n\nWe use also H2O function **describe** to check the data. ","66778dde":"More information are presented: the H2O cluster uptime, timezone, version, version age, cluster name, hardware resources allocated ( number of nodes, memory, cores), the connection url, H2O API extensions exposed and the Python version used.\n\n## Import the data\n\nWe already initialized the H2O engine, now we will use H2O to import the data.","ce3b6901":"# <a id=\"61\">Split the data<\/a> \n\nLet's start by spliting the data in train, validation and test sets. We will use 60%, 20% and 20% splits.","1a499a5a":"Some of the features show good separation in terms of density plots for the subset with **malignant (M)** diagnosis and the subset with **benign (B)** diagnosis, for example:   \n\n* radius_mean;  \n* texture_mean;  \n* perimeter_mean;  \n* area_mean;  \n* radius_worst;  \n* texture_worst;  \n* perimeter_worst;  \n* area_worst;  \nSome features show perfect identity of the density plots grouped by diagnosis, as following:  \n* compactness_se;  \n* concavity_se;  \n* concave_points_se;  \n* simmetry_se;  \n* smoothness_se;  \n\nLet's represent the correlation between the features, excluding id, C33 and diagnosis:  \n","bd111aa3":"We can see that the AUC is 0.9987 for validation set and Gini coeff is 0.997. LogLoss is 0.05.\n\nConfusion matrix show that only one value in the validation set was wrongly predicted.  \n\nWith such good results in the validation set, we will not need to further tune the model.  We can now try and predict the test set values.\n\nLet's also show the variable importance plot for the model.","b6627a7d":"\n## <a id=\"63\">Model evaluation<\/a> \n\n\nLet's inspect the model already trained. We can print the summary:","f3b9863b":"Some of the features are strongly correlated , as following:  \n\n* radius_mean with perimeter_mean;  \n* radius_mean with texture_mean;  \n* perimeter_worst with radius_worst;  \n* perimeter_worst with area_worst;  \n* area_se with perimeter_se;  \n","365c968d":"# <a id=\"5\">Explore the data<\/a>  \n\nWe will use another functions from H2O to explore the data.\n\nLet's start by showing the distribution of features, grouped by **diagnosis**, which is the **target** value.\n\nWe start by looking how many cases are with **diagnosis** of each type (malignant (M) or benign (B)).","0656180c":"# <a id=\"8\">References<\/a>\n\n[1] Breast Cancer Wisconsin (Diagnostic) Data Set, https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data  \n[2] SRK, Getting started with H2O,  https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-h2o","3f09cf53":"This shows that we used 50 trees, 50 internal trees. It is also showing the min and max tree depth (4,5), the min and max number of leaves (7,14) and the mean values for tree depth and number of leaves.\n\nWe can also inspect the model further, looking to other informations.\n\nLet's see the model performance for the train set.","0023d64d":"There are 569 rows and 33 columns in the data. For each column, the following informations are shown:  \n\n+ type;  \n+ min;  \n+ mean;  \n+ max;  \n+ standard deviation (sigma);  \n+ number of zeros (zero);  \n+ number of missing values (missing);  \n+ a certain number of selected values (first 10);  \n\nNotes: Calling **describe()** function this way is equivalent with calling **summary()**.   \nWe can call describe with a parameter different from 0. In this case, more information about the type of chunk compression data and frame distribution, besides the data description, is given.\n","246720e1":"# <a id=\"1\">Introduction<\/a>  \n\n## The dataset\n\nThe **Breast Cancer (Wisconsin) Diagnosis dataset** <a href='#8'>[1]<\/a> contains the diagnosis and a set of 30  features describing the characteristics of the cell nuclei present in the digitized image of a of a fine needle aspirate (FNA) of a breast mass.\nTen real-valued features are computed for each cell nucleus:  \n+ **radius** (mean of distances from center to points on the perimeter);  \n+ **texture** (standard deviation of gray-scale values);  \n+ **perimeter**;  \n+ **area**;  \n+ **smoothness** (local variation in radius lengths);  \n+ **compactness** (perimeter^2 \/ area - 1.0);  \n+ **concavity** (severity of concave portions of the contour);  \n+ **concave points** (number of concave portions of the contour);  \n+ **symmetry**;  \n+ **fractal dimension** (\"coastline approximation\" - 1).\n\nThe **mean**, standard error (**SE**) and \"**worst**\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.\n\n\n## H2O  \n\nH2O is a Java-based software for data modeling and general computing. Primary purpose of H2O is as a distributed (many machines), parallel (many CPUs), in memory (several hundred GBs Xmx) processing engine. It has both Python and R interfaces <a href='#8'>[2]<\/a>.  \n\n## Analysis\n\nWe will analyze the features to understand the predictive value for diagnosis. We will then create models using two different algorithms and use the models to predict the diagnosis.\n\n","fdc96c1c":"<h1><center><font size=\"6\">Breast Cancer Diagnosis Prediction using H2O<\/font><\/center><\/h1>\n\n\n<img src=\"https:\/\/kaggle2.blob.core.windows.net\/datasets-images\/180\/384\/3da2510581f9d3b902307ff8d06fe327\/dataset-card.jpg\" width=\"400\"><\/img>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load packages<\/a>  \n- <a href='#3'>Read the data<\/a>  \n- <a href='#4'>Check the data<\/a>  \n- <a href='#5'>Data exploration<\/a>\n- <a href='#6'>Predictive model<\/a>  \n    - <a href='#61'>Split the data<\/a> \n    - <a href='#62'>Train  GBM<\/a>   \n    - <a href='#63'>Model evaluation<\/a>  \n    - <a href='#64'>Prediction<\/a>     \n- <a href='#8'>References<\/a>\n","0b8cdcc8":"# <a id=\"6\">Predictive model<\/a>   \n\n","1892d5db":"The most important features are perimeter_worst, concave_points_mean, radius_worst, concave_points_worst.\n\nLet's now use the model for prediction.\n\n## <a id=\"64\">Predict<\/a>   \n\nThe prediction ","6485487e":"# <a id=\"2\">Load packages<\/a>  \n\nWe load the packages we will use in the analysis.\n"}}