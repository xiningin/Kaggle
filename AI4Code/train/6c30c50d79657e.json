{"cell_type":{"b5a820df":"code","a20edab9":"code","f91e8656":"code","e85a5127":"code","22710429":"code","ea61c651":"code","c0223203":"code","dc9b5033":"code","89ba2d5c":"markdown","cc067127":"markdown","b1ac176c":"markdown","97ded0c3":"markdown","50437642":"markdown","94cb717d":"markdown","5d64bc21":"markdown"},"source":{"b5a820df":"# Import Desired Libraries \nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n# Read file\/path for training data\niowa_file_path = '..\/input\/train.csv'\nhome_data = pd.read_csv(iowa_file_path)\n# Read file\/path for predictions\ntest_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)\n\n# Create target object and call it y\ny = home_data.SalePrice\n\n# Create X Data (only select numeric data for now)\nX = home_data.drop('SalePrice', axis=1)\ntest_X = test_data\n\n#Save off Orig X's before changing them\nX_orig = X\ntest_X_orig = test_X\n\n# TODO: Look into moving get dummies and col with missing into pipeline\nX= pd.get_dummies(X) \ntest_X = pd.get_dummies(test_X)\n\n# Create new columns indicating which rows\/cols are going to be imputed\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()]\nimpute_X = X.copy()\nimpute_test_X = test_X.copy()\nfor col in cols_with_missing:\n    impute_X[col+'_was_missing']= impute_X[col].isnull()\n    impute_test_X[col+'_was_missing']= impute_test_X[col].isnull()\nX = impute_X\ntest_X = impute_test_X\n\n# Create Dataframe to store model CV results\ncvResultsdf = pd.DataFrame(columns=['Model Name','CV Mean Score','Model'])\n\n# Define constants\/variables to use\nseed_Num = 8 # aka Random State\nscoring_type = 'neg_mean_absolute_error'\ncv_folds = 3","a20edab9":"# Tuned Decision Tree\nestimators = [('Imputer', SimpleImputer()), ('model', DecisionTreeRegressor(random_state=seed_Num))]\npipe = Pipeline(estimators)\n# Set Parameters for tuning\nparameters = {'model__max_depth':[5, 10, 50], \n              'model__max_leaf_nodes':[10, 100, 300]}\n# Create Grid CV to test out parameters with CV\ndecTrReg = GridSearchCV(pipe, cv = cv_folds,n_jobs = -1,\n                   param_grid = parameters, \n                   scoring= scoring_type)\ndecTrReg.fit(X,y)\nbest_score = -decTrReg.best_score_\ncvResultsdf.loc[len(cvResultsdf)] = ['Tuned Decision Tree', best_score, decTrReg]\nprint(\"CV Mean MAE for Tuned Decision Tree: {:,.0f}\".format(best_score))\n","f91e8656":"# Tuned Random Forest\nestimators = [('Imputer', SimpleImputer()), ('model', RandomForestRegressor(random_state=seed_Num))]\npipe = Pipeline(estimators)\n# Set Parameters for tuning\nparameters = {'model__n_estimators':[10, 100], \n              'model__max_leaf_nodes':[10, 100]}\n# Create Grid CV to test out parameters with CV\nrandForestReg = GridSearchCV(pipe, cv = 4, n_jobs = -1,\n                   param_grid=parameters, \n                   scoring= scoring_type)\nrandForestReg.fit(X,y)\nbest_score = -randForestReg.best_score_\ncvResultsdf.loc[len(cvResultsdf)] = ['Random Forest', best_score, randForestReg]\nprint(\"CV Mean MAE for Tuned Random Forest Model: {:,.0f}\".format(best_score))\n","e85a5127":"# Out of Box XGBR Model\nestimators = [('Imputer', SimpleImputer()), ('model', XGBRegressor(random_state=seed_Num))]\npipe = Pipeline(estimators)\npipe.fit(X,y)\nscores = -cross_val_score(pipe, X, y, cv=cv_folds, scoring=scoring_type)\nscore = scores.mean()\ncvResultsdf.loc[len(cvResultsdf)] = ['Out of Box XGBR Model', score, pipe]\nprint(\"CV Mean MAE for Out of Box XGBRegressor Model: {:,.0f}\".format(score))","22710429":"# Tuned XGboost\nestimators = [('Imputer', SimpleImputer()), ('model', XGBRegressor(random_state=seed_Num))]\npipe = Pipeline(estimators)\n# Set Parameters for tuning\nparameters = {'model__max_depth':range(2,10), \n                'model__min_child_weight':range(1,6)}\n# Create Grid CV to test out parameters with CV\nxgbReg = GridSearchCV(pipe, cv = 4,n_jobs = -1,\n                   param_grid=parameters, \n                   scoring= scoring_type)\nxgbReg.fit(X,y)\nbest_score = -xgbReg.best_score_\ncvResultsdf.loc[len(cvResultsdf)] = ['Tuned XGBR Model', best_score, xgbReg]\nprint(\"CV Mean MAE for Tuned XGBRegressor Model: {:,.0f}\".format(best_score))","ea61c651":"cvResultsdf.sort_values(by=['CV Mean Score'], inplace=True)\nprint(cvResultsdf[['Model Name','CV Mean Score']])\n# Get Model with lowest CV Mean Score\nselected_model = cvResultsdf.loc[cvResultsdf['CV Mean Score'].idxmin()].Model","c0223203":"# Check out permutation importance\n#perm = PermutationImportance(selected_model, random_state=8).fit(X, y)\n#eli5.show_weights(perm, feature_names = X.columns.tolist())","dc9b5033":"# Make predictions\n_, final_test_X = X.align(test_X,join='left',axis=1)\ntest_preds = selected_model.predict(final_test_X)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\n\noutput.to_csv('submission.csv', index=False)","89ba2d5c":"# Make Predictions\nNow, we wll fit the selected model to all of the data. Then we will apply the fitted model to make predictions based on the test data. This will be our submission and viola! We are done.","cc067127":"## Data\n\nAs mentioned before, the data is mainly concerned with housing attributes for some houses in Iowa. \n\nThis section loads in the data and the appropiate libraries and functions.\n\nAs a first step, we are just going to use attributes\/features that are numerical and then use imputing to fill in any missing\/na values.","b1ac176c":"Here, we see that the Tuned XGBRegressor Model had the lower CV Mean Score. This will be our selected model.\n","97ded0c3":"Even though it might not be a 'home' run Kernel, I hope you enjoyed my Kernel! It is slowly evolving as I experiment and tinker with it, but I also want to keep it somewhat short a simple. \n\nIf you enjoyed the Kernel, please like it (aka Upvote). Also, let me know if you have any comments, concerns, feedback, or questions.\n\n## Sources:\n[1] I have mainly used the Kaggle Learn **[Machine Learning Modules](https:\/\/www.kaggle.com\/learn\/machine-learning)** to help with starting this Kernel. \n\n[2] Of course, **[Google](https:\/\/www.google.com\/)** has been indispensable as a search engine for my various bugs\/concerns\/questions. It has led me to sites like **[Stack Overflow](https:\/\/stackoverflow.com\/)**\n\n[3] **[Sklearn Site](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)** is also helpful with the various sklearn details and nuances, like GridSearchCV\n\n[4] For some preprocessing, I have also looked at **[Vijay Narayanan's Kernel](https:\/\/www.kaggle.com\/pnvijay\/exercise-machine-learning-competitions)** from this competition.\n","50437642":"# Compare Models and Select a Model\nHere, we will compare the error of the models (lower error = better).","94cb717d":"## Models \nThis section will go over the models selected. Each model will be a pipeline of a simple imputer and then the model itself. We will also use the mean score from a 5-fold cross validation of mean absolute error to compare the models.","5d64bc21":"# Introduction\n\nThis my notebook\/Kernel that is  derived from the **[Kaggle Machine Learning module](https:\/\/www.kaggle.com\/learn\/machine-learning)**. This was to help me get familiar with a Kaggle competition and creating\/submitting predictions for a competition. \n\nI want to get familar with this since I believe machine learning competitions are a way to improve my data science skills and measure my progress. \n\nWith that being said, this Kernel looks at a dataset with housing attributes and is mainly concerned with trying to predict housing prices. \n\nIf you do end up gleaming anything useful from this Kernel, please upvote (Kaggle's version of like?) and also feel free to give feedback. If you end up using some of this code, please do not forget to reference this Kernel and me. (Also, a comment with a link to your Kernel would be cool too.)\n\nAnyway, there is no salad in this Kernel if you were expecting some free salad. I apologize, but lettuce turnip the beet and get on to the rest of Kernel!\n"}}