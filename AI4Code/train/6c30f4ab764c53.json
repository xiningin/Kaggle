{"cell_type":{"8ca65a15":"code","8653adc5":"code","1a85cb34":"code","55b25560":"code","1cbce516":"code","fa95b9d4":"code","9382545c":"code","0445df54":"code","c9b48df6":"code","3cd3f267":"code","62f14f0e":"code","6945c262":"code","7f25311a":"code","00360648":"code","42fc471e":"code","c62a8f24":"code","4de4523d":"code","9b3b76bd":"code","ad3a67c4":"code","6b965df9":"code","8bec99ae":"code","092edea3":"code","0cd70563":"code","eb187103":"code","262fff1d":"code","fa594202":"code","f3f52f43":"code","08fb2c8f":"code","1d6307ef":"code","969962f1":"code","fb194c06":"markdown","d23e038f":"markdown","f4f68d17":"markdown","1c961f90":"markdown","334cf20f":"markdown","64e00870":"markdown","5a8db073":"markdown"},"source":{"8ca65a15":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8653adc5":"#getting train and test data\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_data.head()","1a85cb34":"#dropping columns which will not have any impact on survival\ntrain_data=train_data.drop(['Name','Ticket','Fare','Cabin','Embarked'],axis=1)\ntrain_data.head()","55b25560":"#find null values in dataset\ntrain_data.isnull().sum()","1cbce516":"#fill null values\ntrain_data.Age = train_data.Age.fillna(train_data.Age.mean())\ntrain_data.isnull().sum()","fa95b9d4":"#survival count plot and percentage survival w.r.t gender\nsns.countplot(x='Survived',hue='Sex',data=train_data)\n\nmen=train_data.loc[train_data.Sex=='male']['Survived']\nrate_men=sum(men)\/len(men) * 100\nwomen=train_data.loc[train_data.Sex=='female']['Survived']\nrate_women=sum(women)\/len(women) * 100\nprint('% of men survivors is',round(rate_men,2), 'and % of women survivors is',round(rate_women,2))","9382545c":"#survival count plot and percentage survival w.r.t passenger class\nsns.countplot(x='Survived',hue='Pclass',data=train_data)","0445df54":"#survival count plot and percentage survival w.r.t age\n#scale age to a smaller range first\ntrain_data.loc[ train_data['Age'] <= 10, 'Age'] = 1\ntrain_data.loc[(train_data['Age'] > 10) & (train_data['Age'] <= 20), 'Age'] = 2\ntrain_data.loc[(train_data['Age'] > 20) & (train_data['Age'] <= 30), 'Age'] = 3\ntrain_data.loc[(train_data['Age'] > 30) & (train_data['Age'] <= 40), 'Age'] = 4\ntrain_data.loc[(train_data['Age'] > 40) & (train_data['Age'] <= 50), 'Age'] = 5\ntrain_data.loc[ train_data['Age'] > 50, 'Age'] = 6\n","c9b48df6":"sns.countplot(x='Survived',hue='Age',data=train_data)","3cd3f267":"#assigning independent and dependent variables\nx = train_data.drop(['Survived'],axis=1)\ny = train_data['Survived']\n\nx = x.drop(['PassengerId'],axis=1) #not related to survival","62f14f0e":"#encoding text to numbers\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_x = LabelEncoder()\nx['Sex'] = labelencoder_x.fit_transform(x['Sex'])\nx.head()\n","6945c262":"from sklearn.metrics import classification_report,accuracy_score,roc_auc_score,matthews_corrcoef,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split","7f25311a":"#80-20 train-test data split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=0)","00360648":"#linear regression\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","42fc471e":"#Decision tree classifier\nmodel2 = DecisionTreeClassifier()\nmodel2.fit(x_train, y_train)\ny_pred = model2.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","c62a8f24":"#random forest classifier\nmodel3 = RandomForestClassifier()\nmodel3.fit(x_train, y_train)\ny_pred = model3.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","4de4523d":"#Naive Bayes classifier\nmodel4 = GaussianNB()\nmodel4.fit(x_train, y_train)\ny_pred = model4.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","9b3b76bd":"#SVM classifier\nmodel5 = svm.SVC()\nmodel5.fit(x_train, y_train)\ny_pred = model5.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","ad3a67c4":"#KNN classifier\nmodel6 = KNeighborsClassifier()\nmodel6.fit(x_train, y_train)\ny_pred = model6.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","6b965df9":"#SGD Classifier\nmodel7 = SGDClassifier()\nmodel7.fit(x_train, y_train)\ny_pred = model7.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","8bec99ae":"#Perceptron classifier\nmodel8 = Perceptron()\nmodel8.fit(x_train, y_train)\ny_pred = model8.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","092edea3":"#adaboost classifier\nmodel9 = AdaBoostClassifier()\nmodel9.fit(x_train, y_train)\ny_pred = model9.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","0cd70563":"#XGBoost Classifier\nmodel10 = XGBClassifier(scale_pos_weight=4)\nmodel10.fit(x_train, y_train)\ny_pred = model10.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint('roc_auc =', roc_auc_score(y_test,y_pred))\nprint('mcc coeff: ',matthews_corrcoef(y_test, y_pred))","eb187103":"#prediction on test dataset using best model\ntest=test_data.drop(['Name','Ticket','Fare','Cabin','Embarked'],axis=1)","262fff1d":"#null values\ntest.isnull().sum()","fa594202":"#fill null values\ntest.Age = test.Age.fillna(test.Age.mean())\ntest.isnull().sum()","f3f52f43":"#scaling age\ntest.loc[ test['Age'] <= 10, 'Age'] = 1\ntest.loc[(test['Age'] > 10) & (test['Age'] <= 20), 'Age'] = 2\ntest.loc[(test['Age'] > 20) & (test['Age'] <= 30), 'Age'] = 3\ntest.loc[(test['Age'] > 30) & (test['Age'] <= 40), 'Age'] = 4\ntest.loc[(test['Age'] > 40) & (test['Age'] <= 50), 'Age'] = 5\ntest.loc[test['Age'] > 50, 'Age'] = 6","08fb2c8f":"#encoding text to numbers\nlabelencoder_test = LabelEncoder()\ntest['Sex'] = labelencoder_test.fit_transform(test['Sex'])\ntest.head()","1d6307ef":"testpred = test.drop(['PassengerId'],axis=1)\nfinal_prediction = model5.predict(testpred)\ntest = test.drop(['Pclass','Age','Sex','SibSp','Parch'],axis=1)\ntest['Survived'] = final_prediction\ntest.to_csv('Submission_new.csv', index = False)","969962f1":"test.head()","fb194c06":"**Survival Analysis Using Machine Learning Algorithms**\nClassification of survivors of Titanic accident performed using machine learning algorithms. \n\nAlgorithms tested: Logistic Regression, Decision Tree, Random Forest, Naive Bayes, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Stochastic Gradient Descent (SGD), Perceptron, Adaboost, XGBoost.\nAccuracy and ROC AUC achieved: 81% & 0.79\n\nAlso, exploratory data analysis (EDA) was performed to analyse the dependence of survival on sex, age and passenger class.","d23e038f":"More women survived than men.","f4f68d17":"People having age between 20 to 30 survived more compared to people belonging to other age groups.","1c961f90":"First class passengers survived more followed by second class passengers.","334cf20f":"**Exploring classification efficiency of various machine learning algorithms**","64e00870":"The best performing algorithm for classification is SVM (model5), with 81% accuracy 0.79 ROC AUC score.","5a8db073":" **EDA to know what sort of people survived**"}}