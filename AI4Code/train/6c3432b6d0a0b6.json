{"cell_type":{"ad118256":"code","b6f1db09":"code","9cc02451":"code","b12b77d5":"code","33b0286e":"code","de976f0d":"code","9903447c":"code","df50b0e0":"code","7a7928b8":"code","79c4fb46":"code","05a18ac3":"code","7daf4c78":"code","57c3c8c6":"code","c70c60c9":"code","2c600717":"code","4b687a29":"code","0f5e2cb8":"code","b4efc0ea":"code","3a81900f":"code","0079ecc4":"code","b49e9016":"code","2232b0be":"code","d7ad9687":"code","60697de4":"code","93461945":"code","76c787a2":"code","3312f6db":"code","83f89f70":"code","9fbf3788":"code","0b97b03c":"code","e47c67af":"code","cb382853":"markdown","de20c9c9":"markdown","d3a9eeb4":"markdown","2b5b700c":"markdown","f8e1eb96":"markdown","06b7d33c":"markdown","8a9e3f5e":"markdown","d7af4319":"markdown","bbf20837":"markdown","2b88f8d5":"markdown","707bbc97":"markdown","421e8d47":"markdown"},"source":{"ad118256":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6f1db09":"#installing intel optimizations for scikit-learn\n!pip install -U scikit-learn scikit-learn-intelex >> z_pip.log\n!pip install delayed","9cc02451":"from sklearnex import patch_sklearn\npatch_sklearn()","b12b77d5":"# importing relevant modules and classes\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder, RobustScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","33b0286e":"# Loading Raw data\n#pseudo_df = pd.read_csv('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv')\n#train_df_inter = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","de976f0d":"#train_df = pd.concat([train_df_inter,pseudo_df],ignore_index=True)\nif 'pseudo_df' in globals():\n    del pseudo_df\nif 'train_df_inter' in globals():\n    del train_df_inter","9903447c":"train_df.head(10)","df50b0e0":"train_df.describe().T","7a7928b8":"## Creating Relevant features list\nsoil_features = [x for x in train_df.columns if x.__contains__('Soil')]\nwild_features = [x for x in train_df.columns if x.__contains__('Wild')]\ndistance_features = [x for x in train_df.columns if x.__contains__('Horizontal') or x.__contains__('Vertical')]\nhillshade_features = [x for x in train_df.columns if x.__contains__('Hill')]\nangle_features = ['Aspect', 'Slope']\nnumerical_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']","79c4fb46":"train_df['Cover_Type'].value_counts()","05a18ac3":"def plot_distribution_by_category(train_df):\n    fig, axis = plt.subplots(nrows=5,ncols=2,figsize=(25,25))\n    axis = axis.flatten()\n    ax_i = 0\n    \n    for i,X in enumerate(numerical_features):\n        plt.xlim(min(train_df[X]),max(train_df[X]))\n        sns.kdeplot(x=X,\n                    hue='Cover_Type',\n                    fill=True,\n                    common_norm=False,\n                    data=train_df[train_df['Cover_Type'] != 5].sample(frac=0.4),ax=axis[i])","7daf4c78":"plot_distribution_by_category(train_df[train_df['Cover_Type'] != 5])","57c3c8c6":"def plot_soil_count_by_cover_type(train_df):\n    fig, axis = plt.subplots(nrows=3,ncols=2,figsize=(25,25))\n    axis = axis.flatten()\n    ax_i = 0\n    \n    for soil_id in range(1,8):\n        if soil_id  == 5:\n            continue\n        soil_count_df = pd.DataFrame()\n        soil_count_df['soil_id'] = range(1,41)\n        soil_count_df['count'] = train_df[train_df['Cover_Type'] == soil_id][soil_features].sum(axis=0).tolist()   \n        sns.barplot(data=soil_count_df,x='soil_id',y='count',ax=axis[ax_i])\n        axis[ax_i].set_xticks([ 1,  6, 11, 16, 21, 26, 31, 36])\n        axis[ax_i].set_title('Cover_Type ' + str(soil_id))\n        ax_i = ax_i + 1","c70c60c9":"plot_soil_count_by_cover_type(train_df)","2c600717":"def plot_wild_count_by_cover_type(train_df):\n    fig, axis = plt.subplots(nrows=3,ncols=2,figsize=(25,25))\n    axis = axis.flatten()\n    ax_i = 0\n    \n    for wild_id in range(1,8):\n        if wild_id == 5:\n            continue\n        wild_count_df = pd.DataFrame()\n        wild_count_df['wild_id'] = range(1,5)\n        wild_count_df['count'] = train_df[train_df['Cover_Type'] == wild_id][wild_features].sum(axis=0).tolist()   \n        sns.barplot(data=wild_count_df,x='wild_id',y='count',ax=axis[ax_i])\n        axis[ax_i].set_title('Cover_Type ' + str(wild_id))\n        ax_i = ax_i + 1","4b687a29":"plot_wild_count_by_cover_type(train_df)","0f5e2cb8":"if 'train_df_modified' in globals():\n    del train_df_modified\nif 'test_df_modified' in globals():\n    del test_df_modified\n\ntrain_df_modified = train_df.copy(deep=True)\ntest_df_modified = test_df.copy(deep=True)","b4efc0ea":"train_df_modified[distance_features] = train_df_modified[distance_features].clip(lower=0)\ntrain_df_modified[hillshade_features] = train_df_modified[hillshade_features].clip(lower=0,upper=255)\ntrain_df_modified['Aspect'][train_df_modified['Aspect'] < 0] += 360\ntrain_df_modified['Aspect'][train_df_modified['Aspect'] >= 360] -= 360\ntrain_df_modified['Slope'][train_df_modified['Slope'] < 0] += 360\ntrain_df_modified['Slope'][train_df_modified['Slope'] >= 360] -= 360\n\n\ntest_df_modified[distance_features] = test_df_modified[distance_features].clip(lower=0)\ntest_df_modified[hillshade_features] = test_df_modified[hillshade_features].clip(lower=0,upper=255)\ntest_df_modified['Aspect'][test_df_modified['Aspect'] < 0] += 360\ntest_df_modified['Aspect'][test_df_modified['Aspect'] >= 360] -= 360\ntest_df_modified['Slope'][test_df_modified['Slope'] < 0] += 360\ntest_df_modified['Slope'][test_df_modified['Slope'] >= 360] -= 360\n\n\ntrain_df_modified['mhtn_hydr_dist'] = np.abs(train_df_modified['Horizontal_Distance_To_Hydrology']) + np.abs(train_df_modified['Vertical_Distance_To_Hydrology'])\ntest_df_modified['mhtn_hydr_dist'] = np.abs(test_df_modified['Horizontal_Distance_To_Hydrology']) + np.abs(test_df_modified['Vertical_Distance_To_Hydrology'])\n\ntrain_df_modified['eucd_hydr_dist'] = np.sqrt((train_df_modified['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                        (train_df_modified['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\ntest_df_modified['eucd_hydr_dist'] = np.sqrt((test_df_modified['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                        (test_df_modified['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\n\n\ntrain_df_modified['mhtn_hydr_dist'] = np.abs(train_df_modified['Horizontal_Distance_To_Hydrology']) + np.abs(train_df_modified['Vertical_Distance_To_Hydrology'])\ntest_df_modified['mhtn_hydr_dist'] = np.abs(test_df_modified['Horizontal_Distance_To_Hydrology']) + np.abs(test_df_modified['Vertical_Distance_To_Hydrology'])\n\ntrain_df_modified['eucd_hydr_dist'] = np.sqrt((train_df_modified['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                        (train_df_modified['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\ntest_df_modified['eucd_hydr_dist'] = np.sqrt((test_df_modified['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                        (test_df_modified['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\n\n\ntrain_df_modified['soil_count'] = train_df_modified[soil_features].sum(axis=1)\ntrain_df_modified['wild_count'] = train_df_modified[wild_features].sum(axis=1)\n\ntest_df_modified['soil_count'] = test_df_modified[soil_features].sum(axis=1)\ntest_df_modified['wild_count'] = test_df_modified[wild_features].sum(axis=1)\n\ntrain_df_modified['avg_hillshade_index'] = train_df_modified[hillshade_features].mean(axis=1)\ntest_df_modified['avg_hillshade_index'] = test_df_modified[hillshade_features].mean(axis=1)\n\nif 'Soil_Type7' in train_df_modified.columns and 'Soil_Type15' in train_df_modified.columns and 'Id' in train_df_modified.columns:\n    train_df_modified = train_df_modified.drop(columns=['Soil_Type7','Soil_Type15','Id'])\nif 'Soil_Type7' in test_df_modified.columns and 'Soil_Type15' in test_df_modified.columns and 'Id' in test_df_modified.columns:\n    test_df_modified = test_df_modified.drop(columns=['Soil_Type7','Soil_Type15','Id'])\n","3a81900f":"if 'train_df' in globals():\n    del train_df\nif 'test_df' in globals():\n    del test_df","0079ecc4":"## The following function has been leveraged from an existing notebook\ndef reduce_mem_usage(df, verbose=True):\n    \"\"\"\n    Reduce memory usage by downcasting features.\n    \n    Args:\n        df (pd.DataFrame): DataFrame with features.\n        verbose (bool): Determines verbosity of output.\n    Returns:\n        df (pd.DataFrame): DataFrame with reduces memory usage, due to smaller datatypes.\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b49e9016":"train_df_red = reduce_mem_usage(train_df_modified)\ntest_df_red = reduce_mem_usage(test_df_modified)","2232b0be":"if 'train_df_modified' in globals():\n    del train_df_modified\nif 'test_df_modified' in globals():\n    del test_df_modified","d7ad9687":"scaler = RobustScaler()\nencoder = LabelEncoder() #Relabeling from 0 to 5","60697de4":"X = train_df_red[train_df_red['Cover_Type'] != 5].drop(columns=['Cover_Type'])\ny = train_df_red[train_df_red['Cover_Type'] != 5]['Cover_Type']\ntest_X = test_df_red\n\nX = scaler.fit_transform(X)\ntest_X = scaler.transform(test_X)\ny = encoder.fit_transform(y)","93461945":"model = CatBoostClassifier(iterations=5000,\n                          task_type=\"GPU\",\n                          devices=\"0:1\",\n                          verbose=False)\nmodel.fit(X,y)","76c787a2":"xgb_params = {\n    'objective': 'multi:softmax',\n    'eval_metric': 'mlogloss', \n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    }\n\nmodel2 = XGBClassifier(**xgb_params)\nmodel2.fit(X,y)\n\n# VC = VotingClassifier(estimators = [('xgb',XGBClassifier(**xgb_params)),\n#                                     ('lgbm_gdbt',LGBMClassifier(n_jobs=4)),\n#                                     ('lgbm_dart',LGBMClassifier(boosting_type='dart',n_jobs=4)),\n#                                     ('lgbm_goss',LGBMClassifier(boosting_type='goss',n_jobs=4))],\n#                      voting='soft',flatten_transform=True,verbose=True)","3312f6db":"# VC.fit(X,y)","83f89f70":"pred = np.argmax(model.predict_proba(test_X)*0.5 + model2.predict_proba(test_X)*0.5,axis=1)","9fbf3788":"pred_y = encoder.inverse_transform(pred) # reversing back to original labels","0b97b03c":"sample_submission = pd.DataFrame(data=range(4000000,5000000),columns=['Id'])\nsample_submission['Cover_Type'] = pd.DataFrame(data=pred_y,columns=['1'])['1']\nsample_submission","e47c67af":"sample_submission.to_csv(path_or_buf='.\/submission.csv',index=False)","cb382853":"#### Memory Usage Reduction","de20c9c9":"Let us also visually examine the distributin of wilderness across forest type in a similar manner.","d3a9eeb4":"#### Printing first 10 rows \nData definitions here https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data","2b5b700c":"#### Exploratory Data Analysis","f8e1eb96":"From the above graph we can clearly see that Elevation is an important feature giving great distinction between forst cover types.","06b7d33c":"##### 1. Distribution of target variable","8a9e3f5e":"##### Summary of columns in the data","d7af4319":"###### Cover Type 5 has only one record, hence we would be dropping the row.","bbf20837":"#### Feature Engineering\n###### 1. Clipping of -ve values from distance based metrics.\n###### 2. Clip the values of angle and hillshade metrics in 0-255 and 0-360\n###### 3. Introduce count of soil_type present and wild_area present as features\n###### 4. Add l1 and l2 distance metrics using vertical and horizontal distances\n###### 5. Add avg hillshade index","2b88f8d5":"#### Visual Examination of relation between variables  \nLet us explore how the numerical values are distrbuted wrt the forest cover types. Since cover type 5 has only 1 observation, we would be dropping this for visual inspection.","707bbc97":"##### We can draw the following observations :-\n1. Soil_TypeXX and Wilderness_AreaX are categorical faetures while others are numerical features.  \n2. Soil_Type7 and Soil_Type15 are all 0. Hence they must be dropped while creating the model.  \n3. The categorical variables are sparse.\n4. Some values exist outside of their supposed range.  \n    1. The distance values must not be -ve, but they are.  \n    2. Slope and Ascent are angles and must be  betwen 0 and 360 but we see outliers.  \n    3. Hill shade indices must be between 0 and 255 but we observe values lying outside. \n\nThe 'anaomolies' could have been introduced while creation of systhetic data. The values can be rectified with some feature engineering.","421e8d47":"Let us now examine the distribution of categorical variables by cover type. We are plotting bar graph of count of categories for each forest type. Starting with soild features."}}