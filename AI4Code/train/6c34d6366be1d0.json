{"cell_type":{"84896bb9":"code","2c4309f0":"code","933d6e3e":"code","b8d676ca":"code","1a60b3c5":"code","2e1d5a0d":"code","fc21a8de":"code","8a7eb10d":"code","0b76ce68":"code","97a890bc":"code","6ea37738":"code","ea489279":"code","7133b176":"code","8f85da20":"code","94846e46":"code","2b022a5d":"code","5eef7903":"code","2a3b0846":"code","b8ff33ed":"code","3b5599f7":"code","f812101b":"code","716ba0dc":"code","09c4ca46":"markdown","fd9a1669":"markdown","1c4934fc":"markdown","ee51785f":"markdown","5a060b13":"markdown","bac67466":"markdown","54b8bef6":"markdown","821dffda":"markdown","3ccdd707":"markdown","cf145ee4":"markdown","8a47cb6b":"markdown","eee54b18":"markdown","47332229":"markdown","1564fc17":"markdown","c279f7c4":"markdown"},"source":{"84896bb9":"#Get our imports in\n#For data handling\nimport numpy as np\nimport pandas as pd\n\n#Splitting data\nfrom sklearn.model_selection import train_test_split\n\n#We'll be using keras in order to do the computing\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport kerastuner as kt\nimport IPython\n\n#Balancig the data\nfrom imblearn.over_sampling import SMOTE\n\n#Imports for EDA\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2c4309f0":"#Read in our data and take a look at it\nfp = \"..\/input\/creditcardfraud\/creditcard.csv\"\ndata = pd.read_csv(fp)\ndc = data.copy()\ndata.describe()","933d6e3e":"#Shows distribution of the class\nsns.countplot(data['Class'])","b8d676ca":"#Creates boxplots for all the data columns\nfor col in data.columns:\n    plt.title(f'Boxplot for {col}')\n    plt.boxplot(data[col])\n    plt.show()","1a60b3c5":"#Drop the time column\ndata.drop('Time',inplace=True,axis=1)","2e1d5a0d":"#Setup a function to drop the outliers\ndef remove_outliars(data):\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    \n    data_clean = data[~((data < (Q1-1.5*IQR)) | (data > (Q3+1.5*IQR))).any(axis=1)]\n    \n    return data_clean","fc21a8de":"#Get the columns used for predicting\ncols=data.columns.drop('Class')\n#Apply the function to those columns\ndata[cols] = remove_outliars(data[cols])\n#The function replaced the outliers with NA, so we drop those\ndata.dropna(inplace=True)","8a7eb10d":"#Make sure we removed something\nremoved = dc.shape[0]-data.shape[0]\nprint(f'Outliers removed: {removed}')","0b76ce68":"sm = SMOTE(k_neighbors=4,random_state=2)\nsm_data,sm_target = sm.fit_resample(data[cols],data['Class'])","97a890bc":"#Split the data into training and testing data\nX_train,X_val,y_train,y_val = train_test_split(sm_data,sm_target,test_size=0.25,random_state=2)","6ea37738":"def build_model_deep(hp=None,\n                     data = data[cols],\n                     manual=False,units1=0,units2=0,units3=0,units4=0):\n    \"\"\"\n    Builds a keras sequential model.\n    Has 5 dense layers, with 2 dropout.\n    Output is sigmoid layer\n    \"\"\"\n    input_shape=[data.columns.shape[0]]\n    #We'll start by building the model\n    model = keras.Sequential()\n    #Variables for tuning\n    min_value = 20\n    max_value = 150\n    rate = 0.4\n\n    #Hyperparameter tuning for the units of the dense layers\n    if manual == True:\n        hp_units = units1\n    else:\n        hp_units = hp.Int('units',min_value=min_value,max_value=max_value,step=10)\n    model.add(layers.Dense(hp_units,input_shape=input_shape, activation='relu'))\n    if manual == True:\n        hp_units2 = units2\n    else:\n        hp_units2 = hp.Int('units2',min_value=min_value,max_value=max_value,step=10)\n    model.add(layers.Dense(hp_units2,activation='relu'))\n    #Use dropout to minimize overfitting\n    model.add(layers.Dropout(rate=rate))\n    if manual == True:\n        hp_units3 = units3\n    else:\n        hp_units3 = hp.Int('units3',min_value=min_value,max_value=max_value,step=10)\n    model.add(layers.Dense(hp_units3,activation='relu'))\n    if manual == True:\n        hp_units4 = units4\n    else:\n        hp_units4 = hp.Int('units4',min_value=min_value,max_value=max_value,step=10)\n    model.add(layers.Dense(hp_units4,activation='relu'))\n    model.add(layers.Dropout(rate=rate))\n    model.add(layers.Dense(2,activation='softmax'))\n    \n    #Setup the learning rate for adam\n    hp_lr = 0.001\n    #Compile the model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=hp_lr),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model","ea489279":"#Setup early stopping to combat overfitting\nearly_stopping = EarlyStopping(\n    patience=15,\n    min_delta=0.001,\n    restore_best_weights=True)\n\n#Clears the output during hp tuning\nclass ClearTrainingOutput(keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)","7133b176":"#Setup the tuner for the deep model\n#tuner_deep = kt.Hyperband(build_model_deep,\n#                    objective='accuracy',\n#                    factor=2,\n#                    max_epochs=25\n#                    )","8f85da20":"\n#Init search for optimal parameters\n#tuner_deep.search(X_train,y_train,\n#            epochs=25,\n#            batch_size = 100,\n#            validation_data=(X_val,y_val),\n#            callbacks=[early_stopping,ClearTrainingOutput()])","94846e46":"#tuner_deep.results_summary()","2b022a5d":"#Fetch the best parameters from tuning\n#parameters_deep = tuner_deep.get_best_hyperparameters()[0]\n#Then build a model with them\n#with tpu_strategy.scope():\n#    deep_model = tuner_deep.hypermodel.build(parameters_deep)","5eef7903":"#Setup TPUs to speed up training\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n    deep_model = build_model_deep(manual=True,units1=40,units2=30,units3=100,units4=80)","2a3b0846":"#Train the model\ndeep_hist = deep_model.fit(\n    X_train,y_train,\n    validation_data=(X_val,y_val),\n    batch_size=64 * tpu_strategy.num_replicas_in_sync,\n    epochs=150,\n    callbacks=[early_stopping]\n    )","b8ff33ed":"#Fetch the history to see how the training progressed\ndeep_hdf = pd.DataFrame(deep_hist.history)\ndeep_hdf.loc[:,['loss','val_loss']].plot()","3b5599f7":"#Make predictions on test data\ndeep_preds= deep_model.predict_classes(X_val)\ndeep_preds = deep_preds.astype(int)","f812101b":"deep_preds","716ba0dc":"#Plot a confusion matrix to see how our predictions match the data\ndeep_cm = confusion_matrix(y_val,deep_preds,normalize=None)\ndeep_disp = ConfusionMatrixDisplay(confusion_matrix = deep_cm)\ndeep_disp.plot()","09c4ca46":"The median for time is so large it won't do any good for the model, so we can drop that column.\nWe also see a massive amount of outliers, so we'll also drop those.","fd9a1669":"I've commented out the hyperparameter parts of this notebook, because they take a long time to run.\nThe values they outputted last have been fitted into the models using the manual clause","1c4934fc":"Since we're dealing with data that is very unbalanced, we'll use SMOTE to balance out the data by creating synthetic samples. \\\nFirst check out the rest of the spread","ee51785f":"Now we've removed outliers.","5a060b13":"# Setup and Imports","bac67466":"# Data prep and model creation","54b8bef6":"# Model training","821dffda":"We use a TPU to make our training times shorter","3ccdd707":"Using a depth focused deep learning model we will attempt to predict what credit card transaction is fraudulent.","cf145ee4":"# EDA\n\nTake a look at the distribution first.","8a47cb6b":"# Model prediction","eee54b18":"Values are normalized, so no need for additional normalizing.","47332229":"We'll use SMOTE to balance our data.\nWe'll apply it on the whole dataset","1564fc17":"The next piece is modified from Stack to remove outliers.","c279f7c4":"We setup a function to create the model, so that we can utilize hyperparameter tuning"}}