{"cell_type":{"b0605070":"code","c336c3f6":"code","ef3ecd35":"code","cd6b44fb":"code","51a08ffc":"code","f3ee6466":"code","7ab8ac76":"code","572631be":"code","7103f0d5":"code","9361fc18":"code","0166c9f3":"code","4db5a724":"code","3b42a601":"code","92d1877b":"code","d4a7e7fa":"code","aa18d8a0":"code","90cd9f3d":"code","0366f6e8":"code","557286a3":"code","e38e89bf":"code","8bae81b1":"code","c7d870dd":"code","f64bc2d8":"code","338c19a5":"code","d0d11c68":"code","be6d94c3":"code","0f76c188":"code","168d0f21":"code","9bd3807b":"code","8bd6d681":"code","d07f8f68":"code","a457f614":"code","06d2b525":"code","cce323a8":"code","2095559e":"code","b5e52dd4":"code","19659051":"code","e7269fbf":"code","2f9ef0be":"code","0188b5ad":"code","e8fc86f4":"code","11a8dd7c":"code","28a1ed5c":"code","eee6c45a":"code","43e2089e":"code","336099dd":"code","78e03d75":"code","a559ee20":"code","13c4d68c":"code","37d12d61":"code","a07ec449":"code","07e1cd0b":"code","4f4657cc":"code","0b7aa530":"code","3df78997":"code","1adf3c7a":"code","ee67c751":"code","378e3f5d":"code","495795db":"code","2ab33b2b":"code","46b4e442":"code","d6fbb1b4":"code","cbd47545":"code","416378b8":"code","f57e3e64":"code","fa986c0d":"code","d24bd250":"code","a0430041":"code","08ca80c3":"code","aadfc371":"code","1eea9e5e":"code","a237d3b6":"code","20d01725":"code","0e321c1f":"code","0637d5d5":"code","04e22c81":"code","46ed2efa":"code","8ee11371":"code","d5cd8cf1":"code","79ce22bb":"code","8a698295":"code","4fa3169a":"code","313338cd":"code","84d436ab":"code","5ec75ee4":"code","9a40a670":"code","a7e951be":"code","fee6ce42":"code","e5218eed":"code","1b6fc408":"code","20968c93":"code","ca72f3c7":"code","9a1d1ed3":"code","14c06d0c":"code","d250e6db":"code","0458f401":"code","478d8cd6":"code","4f358157":"code","3a508249":"code","64b8fc37":"code","84c9adef":"code","394c911f":"code","1bebe8b2":"code","b352752f":"code","2be6a040":"code","23bedd27":"code","8c8a260f":"code","1e9c5a50":"code","39b67b20":"code","6167e1d9":"code","5135ea6a":"code","c3c8e515":"code","dc905199":"code","bb92b856":"code","fc62d768":"code","74605dca":"code","b8912dca":"code","9dfb1164":"code","c6efe939":"code","4ff1106f":"code","84243eae":"code","5b29650f":"code","74999c9c":"code","308acad1":"code","3c4ed285":"code","d946c30e":"code","020f9dbe":"code","729543fc":"code","61f32dc7":"code","0156191b":"code","1fdeaacf":"code","fa0405a8":"code","c21cdd39":"code","073840fc":"code","9e3c21d2":"code","4080580a":"code","34cb5549":"code","042f667a":"code","84092fb2":"markdown","bb0b8c88":"markdown","b1a79048":"markdown","57d66241":"markdown","fc662c2a":"markdown","6251a08d":"markdown","5c56402d":"markdown","4026d130":"markdown","1b7ccbb6":"markdown","647e7264":"markdown","91530ed4":"markdown","a0e1c40a":"markdown","d81b24c0":"markdown","bb2301c1":"markdown","8981f270":"markdown","998ab9de":"markdown","766db42c":"markdown","d209cf1d":"markdown","52d72750":"markdown","31461d09":"markdown","35e35c21":"markdown","b2061608":"markdown","8d49bf40":"markdown","b62055ab":"markdown","bbb2c309":"markdown","7e6b9516":"markdown","897c1d7f":"markdown","3bbe204e":"markdown","7260c73e":"markdown","68c86284":"markdown","8f25d8b5":"markdown","83910a5c":"markdown","6d8e5837":"markdown","7e623a4c":"markdown","0e9c42f4":"markdown","60c0ec74":"markdown","7aa954a0":"markdown","9241409a":"markdown","7660df7b":"markdown","895082d0":"markdown","3fefd3c3":"markdown","71142436":"markdown","976b9110":"markdown","f02d9e73":"markdown","ef1384b7":"markdown","4e3a6855":"markdown","2738f35d":"markdown","35bc14d9":"markdown","ac6a014b":"markdown","6ccb82fe":"markdown","660ada60":"markdown","d4d4f6da":"markdown","03ec4915":"markdown","313e77ea":"markdown","308c198f":"markdown","1ce7181d":"markdown","6ff2611e":"markdown","87c5a330":"markdown","d01667f1":"markdown","9944d3a3":"markdown","a1e58015":"markdown","c1b185ff":"markdown","ca28c0eb":"markdown","5e6eaf8d":"markdown","889ce45b":"markdown","112dc4f0":"markdown","ebbe09aa":"markdown","73d11d7f":"markdown","d723e08c":"markdown","3f0442f6":"markdown","80400179":"markdown","0233b37b":"markdown","70632952":"markdown","85e9c0ec":"markdown","7c12ebc4":"markdown","c4845bbd":"markdown","2038d41f":"markdown","34da5122":"markdown","12dbe397":"markdown","a7e583bb":"markdown","3b67d31d":"markdown","f2ee44cf":"markdown","8eca6bd6":"markdown","a9226729":"markdown","350c6d25":"markdown","c2607804":"markdown","dfd45bd6":"markdown","efccae34":"markdown","f04f6d4f":"markdown","ea22c2d7":"markdown","fe3d43f3":"markdown","cfc05438":"markdown","47590ada":"markdown","9e71d2fd":"markdown","f81b644c":"markdown","6d0cf2a0":"markdown","3e80f6ea":"markdown","f1f7d526":"markdown","16db6f52":"markdown","0ee59d0b":"markdown","e84c8bce":"markdown","c279ecd9":"markdown","761a89e2":"markdown","f9a87822":"markdown","79fbb51e":"markdown","24191089":"markdown","d81744fe":"markdown","cfa06798":"markdown","7ecca16a":"markdown","78d1e820":"markdown","57efb020":"markdown","4fa290e8":"markdown","f2e67bb5":"markdown","a9eed655":"markdown","16858511":"markdown","adee9b43":"markdown","8aa6dda4":"markdown","3b098b7b":"markdown","6c4e0b6d":"markdown","22456f9a":"markdown","b796e832":"markdown","7f81c7f6":"markdown","b57ccf69":"markdown","57a7d31b":"markdown","fdf0f399":"markdown","b9cd0134":"markdown","5fcad3a5":"markdown","9131891b":"markdown","65019bea":"markdown","b5fdd39a":"markdown","200b6370":"markdown","33e0a5c5":"markdown","283629ac":"markdown","18729c01":"markdown","c03ad85c":"markdown","2ec7d427":"markdown","3bdc277d":"markdown","7a06ef07":"markdown","8e063f2f":"markdown","724c48df":"markdown","ac742691":"markdown","0396e042":"markdown","49b1ca79":"markdown","9877f11e":"markdown","69f83e13":"markdown"},"source":{"b0605070":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.display import Image\nimport warnings\nimport time\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n%matplotlib inline","c336c3f6":"train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=\"Id\")","ef3ecd35":"train_df.head()","cd6b44fb":"print(\"Number of Training Examples: {}\".format(train_df.shape[0]))\nprint(\"Number of Test Examples: {}\".format(test_df.shape[0]))\nprint(\"\\nTraing X shape: {}\".format(train_df.shape))\nprint(\"Training y shape: {}\".format(train_df[\"SalePrice\"].shape[0]))\nprint(\"\\nTest X shape: {}\".format(test_df.shape))\nprint(\"\\nColumns in Data: {}\".format(train_df.columns))","51a08ffc":"### Let's see what features there are gaps and what percentage they make up\n\ntrain_df_missing = pd.DataFrame(train_df.isnull().sum().loc[train_df.isnull().sum() != 0],\\\n                                columns=[\"Missing train data\"])\ntest_df_missing = pd.DataFrame(test_df.isnull().sum().loc[test_df.isnull().sum() != 0],\\\n                                columns=[\"Missing test data\"])\npercent_train_missing = pd.DataFrame(round(train_df.isnull().sum().loc[train_df.isnull().sum() != 0]\/train_df.shape[0]*100, 1),\\\n                                    columns=[\"Percentage of missing Train\"])\npercent_test_missing = pd.DataFrame(round(test_df.isnull().sum().loc[test_df.isnull().sum() != 0]\/test_df.shape[0]*100, 1),\\\n                                    columns=[\"Percentage of missing Test\"])\ntype_miss_train = pd.DataFrame([train_df[x].dtype for x in train_df.isnull().sum().index[train_df.isnull().sum() != 0]],\\\n                              columns=[\"Type of feat Train\"],\\\n                               index=[x for x in train_df.isnull().sum().index[train_df.isnull().sum() != 0]])\ntype_miss_test = pd.DataFrame([test_df[x].dtype for x in test_df.isnull().sum().index[test_df.isnull().sum() != 0]],\\\n                              columns=[\"Type of feat Test\"],\\\n                               index=[x for x in test_df.isnull().sum().index[test_df.isnull().sum() != 0]])\n\npd.concat([train_df_missing, percent_train_missing, type_miss_train, test_df_missing, percent_test_missing, type_miss_test]\\\n          ,axis=1)","f3ee6466":"# Features for which gaps are removed\nfeature_for_delete_nan = [\"GrLivArea\", \"YrSold\", \"BldgType\", \"LotFrontage\"]\n\n# LotFrontage - Linear feet of street connected to property\n# GrLivArea - Above grade (ground) living area square feet\n# YrSold - Year Sold\n# BldgType - Type of dwelling","7ab8ac76":"### Removing gapped columns\ntrain_df_drop = train_df[feature_for_delete_nan].dropna()\n\n### Dimension of data after removal of gaps\ntrain_df_drop.shape","572631be":"def plot_hist_diff(new_df, old_df, columns):\n    \"\"\"Difference between distributions before and after removing gaps\"\"\"\n    \n    for i, col in enumerate(columns):\n        plt.figure(figsize=[30,10])\n        plt.subplot(2,2,i+1)\n        plt.title(\"Field - {}\".format(col))\n        old_df[col].hist(bins=50, density=True, color=\"g\")\n        new_df[col].hist(bins=50, density=True, color=\"b\", alpha=0.5)\n        plt.show()","7103f0d5":"plot_hist_diff(train_df_drop, train_df, feature_for_delete_nan)","9361fc18":"Image(\"..\/input\/img-hsprice\/normal_dist.png\", width=700, height=250)","0166c9f3":"Image(\"..\/input\/img-hsprice\/center_skewed.png\", width=400, height=250)","4db5a724":"from sklearn.impute import SimpleImputer","3b42a601":"def research_impute_numeric_column(data, num_column, const_value=None):\n    \n    \"\"\"Function for plotting distributions with filled values\"\"\"\n    \n    strategy_params = [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n    strategy_params_names = [\"Mean\", \"Median\", \"Mode\"]\n    strategy_params_names.append(\"Constant={}\".format(str(const_value)))\n\n    original_data = data[num_column].values\n    new_df = pd.DataFrame(data=original_data, columns=[\"Original data\"])\n    \n    for i in range(len(strategy_params)):\n        strategy = strategy_params[i]\n        col_name = strategy_params_names[i]\n        if (strategy!='constant') or (strategy == 'constant' and const_value!=None):\n            if strategy == \"constant\":\n                imputer = SimpleImputer(strategy=strategy, fill_value=const_value)\n                temp_data = imputer.fit_transform(data[num_column].values.reshape(-1,1))\n            else:\n                imputer = SimpleImputer(strategy=strategy, fill_value=np.nan)\n                temp_data = imputer.fit_transform(data[num_column].values.reshape(-1,1))\n        \n            new_df[col_name] = temp_data\n    \n    sns.kdeplot(data=new_df)","92d1877b":"### Distributions with filled values\nresearch_impute_numeric_column(train_df, \"GarageYrBlt\")","d4a7e7fa":"research_impute_numeric_column(train_df, \"LotFrontage\")","aa18d8a0":"IQR = train_df[\"LotFrontage\"].quantile(0.75) - train_df[\"LotFrontage\"].quantile(0.25)\nextreme_value = train_df[\"LotFrontage\"].quantile(0.75) + 1.5*IQR\nresearch_impute_numeric_column(train_df, \"LotFrontage\", extreme_value)","90cd9f3d":"IQR = train_df[\"GarageYrBlt\"].quantile(0.75) - train_df[\"GarageYrBlt\"].quantile(0.25)\nextreme_values = train_df[\"GarageYrBlt\"].quantile(0.75) + 1.5*IQR\nresearch_impute_numeric_column(train_df, \"GarageYrBlt\", extreme_values)","0366f6e8":"from sklearn.impute import KNNImputer","557286a3":"### Let's select all numeric features, except for the target - SalePrice\nnum_features = train_df.select_dtypes(np.number).columns.drop(\"SalePrice\")\nnum_features","e38e89bf":"### Numerical features with gaps\ntrain_df[num_features].isnull().sum()","8bae81b1":"### Apply KNNImputer \n\nknnimpute_df = train_df[num_features].copy()\nknnimputer = KNNImputer(n_neighbors=5,\n                       weights=\"distance\",\n                       add_indicator=None)\nknnimpute_imputed = knnimputer.fit_transform(knnimpute_df)\nknnimpute_imputed_df = pd.DataFrame(data=knnimpute_imputed, columns=knnimpute_df.columns)","c7d870dd":"### All gaps are filled\nknnimpute_imputed_df.isnull().sum()","f64bc2d8":"### Distribution before and after KNNImputer\nLotFront_df = pd.DataFrame(data=train_df[\"LotFrontage\"].values, columns=[\"original\"])\nLotFront_df[\"KNN_5\"] = knnimpute_imputed_df[\"LotFrontage\"]\n\nsns.kdeplot(data=LotFront_df);","338c19a5":"GYrBlt_df = pd.DataFrame(train_df[\"GarageYrBlt\"].values, columns=[\"original\"])\nGYrBlt_df[\"KNN_5\"] = knnimpute_imputed_df[\"GarageYrBlt\"]\n\nsns.kdeplot(data=GYrBlt_df);","d0d11c68":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","be6d94c3":"pipe = Pipeline(steps=[(\"imputer\", KNNImputer(add_indicator=False)),\n                      (\"scaler\", StandardScaler()),\n                      (\"regressor\", Lasso(max_iter=2000))])","0f76c188":"params_grid = {\"imputer__n_neighbors\": np.arange(1,11,2),\n              \"imputer__weights\": [\"uniform\", \"distance\"],\n              \"regressor__alpha\": np.arange(50,200,25)}","168d0f21":"grid_search = GridSearchCV(pipe, param_grid=params_grid, cv=5, n_jobs=-1, scoring=\"r2\")\ngrid_search.fit(knnimpute_df, train_df[\"SalePrice\"].values.ravel());","9bd3807b":"### Best params and best score\ngrid_search.best_params_, grid_search.best_score_","8bd6d681":"### Train KNNImputer with better hyperparameters\nknnimputer2 = KNNImputer(n_neighbors=1, weights=\"uniform\")\nknnimpute_imputed2 = knnimputer2.fit_transform(knnimpute_df)\nknnimpute_imputed2_df = pd.DataFrame(data=knnimpute_imputed2, columns=knnimpute_df.columns)","d07f8f68":"### Let's look at the distributions\nLotFront_df[\"KNN_1\"] = knnimpute_imputed2_df[\"LotFrontage\"]\n\nsns.kdeplot(data=LotFront_df);","a457f614":"GYrBlt_df[\"KNN_1\"] = knnimpute_imputed2_df[\"GarageYrBlt\"]\n\nsns.kdeplot(data=GYrBlt_df);","06d2b525":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor","cce323a8":"Imputer_miss_Forest = IterativeImputer(estimator=RandomForestRegressor(random_state=42),\\\n                                       random_state=42, max_iter=15)","2095559e":"%%time\n### Train the model only on numeric features\n\nmissForest_df = train_df[num_features].copy()\nmissForest_imputed = Imputer_miss_Forest.fit_transform(missForest_df)\nmissForest_imputed_df = pd.DataFrame(missForest_imputed, columns=missForest_df.columns)","b5e52dd4":"### Let's look at the distributions\nLotFront_df[\"MissForest\"] = missForest_imputed_df[\"LotFrontage\"]\n\nsns.kdeplot(data=LotFront_df);","19659051":"GYrBlt_df[\"MissForest\"] = missForest_imputed_df[\"GarageYrBlt\"]\n\nsns.kdeplot(data=GYrBlt_df);","e7269fbf":"train_data = train_df.drop([\"SalePrice\"], axis=1).copy()\ntest_data = test_df.copy()","2f9ef0be":"class Imputer:\n    \n    def __init__(self, train_set, test_set, features):\n        \n        self.train_data = train_set[features].copy()\n        self.test_data = test_set[features].copy()\n        self.imputer = SimpleImputer(fill_value=np.nan)\n        \n    def new_df(self):\n        \n        \"\"\"Function for creating DataFrame with columns and indices like train and test datasets\"\"\"\n        \n        train_df_empty = pd.DataFrame(columns=self.train_data.columns, index=self.train_data.index)\n        test_df_empty = pd.DataFrame(columns=self.test_data.columns, index=self.test_data.index)\n        \n        return train_df_empty, test_df_empty\n    \n    def couple_datasets(self, train_data, test_data):\n        \n        \"\"\"Function for combining datasets into a tuple\"\"\"\n        \n        couple = (train_data, test_data)\n        \n        return couple\n    \n    def Mean_Imputer(self):\n        \n        \"\"\"Function for filling gaps with MeanImputer method\"\"\"\n        \n        train_mean_data, test_mean_data = self.new_df()\n        \n        imputer = self.imputer.set_params(strategy=\"mean\")\n        \n        ### We go over the features, and fill in the gaps for each feature in which they are available\n        for col in num_features:\n\n            train_mean_data[col] = imputer.fit_transform(self.train_data[col].values.reshape(-1,1))\n            test_mean_data[col] = imputer.transform(self.test_data[col].values.reshape(-1,1))\n            \n        print(\"Mean Imputer ended.\")\n        \n        ### Combining the filled datasets into a tuple\n        filled_mean_data = self.couple_datasets(train_mean_data, test_mean_data)\n                \n        return filled_mean_data\n        \n    def Median_Imputer(self):\n        \n        \"\"\"Function for filling gaps with MedianImputer method\"\"\"\n        \n        train_median_data, test_median_data = self.new_df()\n        \n        imputer = self.imputer.set_params(strategy=\"median\")\n        \n        ### We go over the features, and fill in the gaps for each feature in which they are available\n        for col in num_features:\n            \n            train_median_data[col] = imputer.fit_transform(self.train_data[col].values.reshape(-1,1))\n            test_median_data[col] = imputer.transform(self.test_data[col].values.reshape(-1,1))\n            \n        print(\"Median Imputer ended.\")\n        \n        ### Combining the filled datasets into a tuple\n        filled_median_data = self.couple_datasets(train_median_data, test_median_data)\n                \n        return filled_median_data\n    \n    def Mode_Imputer(self):\n        \n        \"\"\"Function for filling gaps with ModeImputer method\"\"\"\n        \n        train_mode_data, test_mode_data = self.new_df()\n        \n        imputer = self.imputer.set_params(strategy=\"most_frequent\")\n        \n        ### We go over the features, and fill in the gaps for each feature in which they are available\n        for col in num_features:\n\n            train_mode_data[col] = imputer.fit_transform(self.train_data[col].values.reshape(-1,1))\n            test_mode_data[col] = imputer.transform(self.test_data[col].values.reshape(-1,1))\n            \n        print(\"Mode Imputer ended.\")\n        \n        ### Combining the filled datasets into a tuple\n        filled_mode_data = self.couple_datasets(train_mode_data, test_mode_data)\n                \n        return filled_mode_data\n    \n    def KNN_Imputer(self):\n        \n        \"\"\"Function for filling gaps with KNNImputer method\"\"\"\n        \n        knnimputer = KNNImputer(n_neighbors=1,\n                               weights=\"uniform\",\n                               add_indicator=None)\n        \n        ### Train the KNNImputer and fill in the gaps\n        knnimpute_train_values = knnimputer.fit_transform(self.train_data)\n        knnimpute_test_values = knnimputer.transform(self.test_data)\n        \n        ### Writing the filled values to the DataFrame\n        train_knn_data = pd.DataFrame(data=knnimpute_train_values,\\\n                                     columns=self.train_data.columns, index=self.train_data.index)\n        test_knn_data = pd.DataFrame(data=knnimpute_test_values,\\\n                                    columns=self.test_data.columns, index=self.test_data.index)\n        \n        print(\"KNN Imputer ended.\")\n        \n        ### Combining the filled datasets into a tuple\n        filled_knn_data = self.couple_datasets(train_knn_data, test_knn_data)\n        \n        return filled_knn_data\n    \n    def MICE(self):\n        \n        \"\"\"Function for filling gaps with MICE method\"\"\"\n        \n        Imputer_miss_Forest = IterativeImputer(estimator=RandomForestRegressor(random_state=42),\\\n                                       random_state=42, max_iter=15)\n        \n        \n        ### Train MissForest and fill in the gaps\n        missForest_train_values = Imputer_miss_Forest.fit_transform(self.train_data)\n        missForest_test_values = Imputer_miss_Forest.transform(self.test_data)\n        \n        ### Writing the filled values \u200b\u200bto the DataFrame\n        train_mice_data = pd.DataFrame(data=missForest_train_values,\\\n                                      columns=self.train_data.columns, index=self.train_data.index)\n        test_mice_data = pd.DataFrame(data=missForest_test_values,\\\n                                     columns=self.test_data.columns, index=self.test_data.index)\n        \n        print(\"MICE Imputer ended.\")\n        \n        ### Combining the filled datasets into a tuple\n        filled_mice_data = self.couple_datasets(train_mice_data, test_mice_data)\n        \n        return filled_mice_data\n    \n    def filled_dict(self):\n        \n        \"\"\"Function for creating a dictionary with data filled with various methods\"\"\"\n        \n        filled_mean_data = self.Mean_Imputer()\n        filled_median_data = self.Median_Imputer()\n        filled_mode_data = self.Mode_Imputer()\n        filled_knn_data = self.KNN_Imputer()\n        filled_mice_data = self.MICE()\n        \n        \n        \n        filled_data_dict = {\"MeanImputer\": filled_mean_data, \"MedianImputer\": filled_median_data,\\\n                             \"ModeImputer\": filled_mode_data, \"KNNImputer\": filled_knn_data,\\\n                             \"MICE\": filled_mice_data}\n        \n        return filled_data_dict","0188b5ad":"%%time\nimputer = Imputer(train_data, test_data, num_features)\nfilled_data_dict = imputer.filled_dict()","e8fc86f4":"### Dictionary keys\nfilled_data_dict.keys()","11a8dd7c":"### Check for gaps\nfor data_name, data in filled_data_dict.items():\n    x_train, x_test = data\n    print(\"Missing values in x_train {} : {}\".format(data_name,\\\n                                    x_train.isnull().sum().loc[x_train.isnull().sum() != 0]))\n    print(\"Missing values in x_test {} : {}\".format(data_name,\\\n                                    x_test.isnull().sum().loc[x_test.isnull().sum() != 0]))","28a1ed5c":"train_data[\"PoolQC\"] = train_df[\"PoolQC\"].fillna(\"Null\")\ntrain_data[\"Fence\"] = train_df[\"Fence\"].fillna(\"Null\")\ntrain_data[\"GarageCond\"] = train_df[\"GarageCond\"].fillna(\"Null\")\ntrain_data[\"GarageQual\"] = train_df[\"GarageQual\"].fillna(\"Null\")\ntrain_data[\"GarageFinish\"] = train_df[\"GarageFinish\"].fillna(\"Null\")\ntrain_data[\"GarageType\"] = train_df[\"GarageType\"].fillna(\"Null\")\ntrain_data[\"FireplaceQu\"] = train_df[\"FireplaceQu\"].fillna(\"Null\")\ntrain_data[\"BsmtFinType1\"] = train_df[\"BsmtFinType1\"].fillna(\"Null\")\ntrain_data[\"BsmtFinType2\"] = train_df[\"BsmtFinType2\"].fillna(\"Null\")\ntrain_data[\"BsmtExposure\"] = train_df[\"BsmtExposure\"].fillna(\"Null\")\ntrain_data[\"BsmtQual\"] = train_df[\"BsmtQual\"].fillna(\"Null\")\ntrain_data[\"BsmtCond\"] = train_df[\"BsmtCond\"].fillna(\"Null\")\ntrain_data[\"Alley\"] = train_df[\"Alley\"].fillna(\"Null\")\ntrain_data[\"MiscFeature\"] = train_df[\"MiscFeature\"].fillna(\"Null\")","eee6c45a":"test_data[\"PoolQC\"] = test_df[\"PoolQC\"].fillna(\"Null\")\ntest_data[\"Fence\"] = test_df[\"Fence\"].fillna(\"Null\")\ntest_data[\"GarageCond\"] = test_df[\"GarageCond\"].fillna(\"Null\")\ntest_data[\"GarageQual\"] = test_df[\"GarageQual\"].fillna(\"Null\")\ntest_data[\"GarageFinish\"] = test_df[\"GarageFinish\"].fillna(\"Null\")\ntest_data[\"GarageType\"] = test_df[\"GarageType\"].fillna(\"Null\")\ntest_data[\"FireplaceQu\"] = test_df[\"FireplaceQu\"].fillna(\"Null\")\ntest_data[\"BsmtFinType1\"] = test_df[\"BsmtFinType1\"].fillna(\"Null\")\ntest_data[\"BsmtFinType2\"] = test_df[\"BsmtFinType2\"].fillna(\"Null\")\ntest_data[\"BsmtExposure\"] = test_df[\"BsmtExposure\"].fillna(\"Null\")\ntest_data[\"BsmtQual\"] = test_df[\"BsmtQual\"].fillna(\"Null\")\ntest_data[\"BsmtCond\"] = test_df[\"BsmtCond\"].fillna(\"Null\")\ntest_data[\"Alley\"] = test_df[\"Alley\"].fillna(\"Null\")\ntest_data[\"MiscFeature\"] = test_df[\"MiscFeature\"].fillna(\"Null\")\ntest_data[\"BsmtExposure\"] = test_df[\"BsmtExposure\"].fillna(\"Null\")","43e2089e":"### Remaining categorical features for train_data\ntrain_data.select_dtypes(\"object\").isnull().sum().loc[train_data.select_dtypes(\"object\").isnull().sum() != 0]","336099dd":"### Remaining categorical features for test_data\ntest_data.select_dtypes(\"object\").isnull().sum().loc[test_data.select_dtypes(\"object\").isnull().sum() != 0]","78e03d75":"obj_features = train_df.select_dtypes(\"object\").columns\n\ntrain_data[obj_features] = train_data[obj_features].apply(lambda x: x.fillna(x.value_counts().index[0]))\ntest_data[obj_features] = test_data[obj_features].apply(lambda x: x.fillna(x.value_counts().index[0]))","a559ee20":"### check for gaps\ntrain_data.select_dtypes(\"object\").isnull().sum().loc[train_data.select_dtypes(\"object\").isnull().sum() != 0]","13c4d68c":"test_data.select_dtypes(\"object\").isnull().sum().loc[test_data.select_dtypes(\"object\").isnull().sum() != 0]","37d12d61":"from sklearn.preprocessing import OneHotEncoder","a07ec449":"def encoding(data, features):\n    \"\"\"Function for encoding categorical features using the OneHotEncoder method\"\"\"\n    \n    encoder = OneHotEncoder()\n    \n    encoded_df = pd.DataFrame(index=data.index)\n    \n    for col in features:\n        \n        encoded_values = encoder.fit_transform(data[col].values.reshape(-1,1)).toarray()\n        unique = data[col].unique()\n        columns = [\"{}_{}\".format(col,u) for u in unique]\n        \n        temp_df = pd.DataFrame(encoded_values, columns=columns, index=data.index)\n        encoded_df = pd.concat([encoded_df, temp_df], axis=1)\n        \n    return encoded_df","07e1cd0b":"### Let's see what happens when using the OneHotEncoder method\nencoded_df = encoding(train_data, [\"Alley\", \"Street\"])\nencoded_df.head()","4f4657cc":"### Let's encode train and test samples\ntrain_enc_data = encoding(train_data, obj_features)\ntest_enc_data = encoding(test_data, obj_features)","0b7aa530":"train_enc_data.shape, test_enc_data.shape","3df78997":"split_value = train_data.shape[0]\n\nall_data = pd.concat([train_data, test_data], axis=0)\n\nall_enc_data = encoding(all_data, obj_features)","1adf3c7a":"all_enc_data.shape","ee67c751":"### Divide the samples\ntrain_enc_data = all_enc_data[:split_value]\ntest_enc_data = all_enc_data[split_value:]","378e3f5d":"train_enc_data.shape, test_enc_data.shape","495795db":"### Let's combine numeric and coded categorical features into a common dataset\n\nfor data_name, data in filled_data_dict.items():\n    x_train, x_test = data\n    \n    x_train = pd.concat([x_train, train_enc_data], axis=1)\n    x_test = pd.concat([x_test, test_enc_data], axis=1)\n    \n    filled_data_dict[data_name] = (x_train, x_test)","2ab33b2b":"### Distribution before normalization\ny_data = train_df[\"SalePrice\"]\nsns.distplot(y_data);","46b4e442":"from scipy.stats import boxcox\nfrom scipy.stats import probplot","d6fbb1b4":"### Distribution after normalization\ny_data, lamb = boxcox(y_data)\nplt.subplot(211)\nprobplot(y_data, plot=plt)\nplt.subplot(212)\nsns.distplot(y_data);","cbd47545":"### Converting data to pandas DataFrame\ny_data = pd.DataFrame(y_data, columns=[\"SalePrice\"], index=train_df.index)\ny_data.head()","416378b8":"from sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.model_selection import cross_val_score","f57e3e64":"class MetricLogger:\n    \n    def __init__(self):\n        self.df = pd.DataFrame({\"metric\": pd.Series([], dtype=\"str\"),\n                               \"model\": pd.Series([], dtype=\"str\"),\n                               \"score\": pd.Series([], dtype=\"float64\")})\n        \n    def add_data(self, metric, model, score):\n        \n        \"\"\"Adding new data\"\"\"\n        \n        data = [{\"metric\": metric, \"model\": model, \"score\": score}]\n        self.df = self.df.append(data, ignore_index=True)\n        \n    def get_data_for_metric(self, model, ascending=True):\n        \n        \"\"\"Shaping data filtered by model\"\"\"\n        \n        data = self.df[self.df[\"model\"] == model].sort_values(by=\"score\", ascending=ascending)\n        \n        return data[\"metric\"].values, data[\"score\"].values\n    \n    def plot(self, str_head, model):\n        \n        \"\"\"Plot output\"\"\"\n        \n        array_labels, array_values = self.get_data_for_metric(model)\n        \n        plt.figure(figsize=[12,5])\n        plt.subplot(111)\n        pos = np.arange(len(array_values))\n        plt.barh(pos, array_values, tick_label=array_labels, align=\"center\")\n        plt.title(str_head)\n        for a,b in zip(pos, array_values):\n            plt.text(0.001, a-0.1, str(round(b, 4)), color=\"white\")","fa986c0d":"### Dict of models for learning\nmodels_dict = {\"KNN\": KNeighborsRegressor(),\n              \"tree\": DecisionTreeRegressor(random_state=42),\n               \"Lasso\": Lasso(random_state=42, max_iter=2000),\n               \"Ridge\": Ridge(random_state=42, max_iter=2000),\n              \"SVR\": SVR(),\n              \"RF\": RandomForestRegressor(random_state=42),\n              \"GB\": GradientBoostingRegressor(random_state=42, n_estimators=1000, learning_rate=0.05, \n                                              loss='huber')}","d24bd250":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n### Let's create a model quality metric - RMSE.\nrmse = make_scorer(mean_squared_error, squared=False)","a0430041":"def test_models(models_dict, data_dict, y_train):\n    \"\"\"test models on cross validation\"\"\"\n    logger = MetricLogger()\n    \n    for model_name, model in models_dict.items():\n        \n        print(model_name+\": beginning\")\n        \n        for data_name, data in data_dict.items():\n\n            X_train, X_test = data\n            \n            cv = cross_val_score(model, X_train, y_train, cv=5, n_jobs=-1, scoring=rmse)\n\n            score = cv.mean()\n    \n            logger.add_data(metric=data_name, model=model_name, score=score)\n        \n    print(\"Finished.\")\n            \n    return logger","08ca80c3":"%%time\nlogger = test_models(models_dict, filled_data_dict, y_train=y_data.values.ravel())","aadfc371":"for model_name, model in models_dict.items():\n    logger.plot(\"Model: \" + model_name, model_name)","1eea9e5e":"def to_df(data, num_features, test=False):\n    \"\"\"A function that will convert our scaled data to a DataFrame\"\"\"\n    train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\n    test_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=\"Id\")\n    \n    if test:\n        df = pd.DataFrame(data, columns=test_df[num_features].columns, index=test_df.index)\n    else:\n        df = pd.DataFrame(data, columns=train_df[num_features].columns, index=train_df.index)\n        \n    return df","a237d3b6":"def draw_scaler_kde(train, train_scaled, test, test_scaled, label1, label2, label3, label4, columns):\n    \"\"\"Function for plotting the density distribution before and after scaling\"\"\"\n    \n    plt.figure(figsize=[20,8])\n    \n    # Train before scaling\n    plt.subplot(221)\n    sns.kdeplot(data=train[columns])\n    plt.title(label1)\n    \n    # Train after scaling\n    plt.subplot(222)\n    sns.kdeplot(data=train_scaled[columns])\n    plt.title(label2)\n    \n    # Test before scaling\n    plt.subplot(223)\n    sns.kdeplot(data=test[columns])\n    plt.title(label3)\n    \n    # Test after scaling\n    plt.subplot(224)\n    sns.kdeplot(data=test_scaled[columns])\n    plt.title(label4)\n    \n    plt.show()","20d01725":"from sklearn.preprocessing import StandardScaler","0e321c1f":"SS = StandardScaler()\n\n### We train on the train data and transform the test data\ntrain_SS = SS.fit_transform(train_df[num_features])\ntest_SS = SS.transform(test_df[num_features])\ntrain_SS_df = to_df(train_SS, num_features)\ntest_SS_df = to_df(test_SS, num_features, test=True)","0637d5d5":"train_SS_df.describe()","04e22c81":"test_SS_df.describe()","46ed2efa":"### Let's look at the density distributions\ndraw_scaler_kde(train_df, train_SS_df, test_df, test_SS_df, label1=\"Train before Scale\",\n               label2=\"Train After Scale\", label3=\"Test Before Scale\", label4=\"Test After Scale\",\n                columns=[\"LotFrontage\",\"LotArea\",\"GarageArea\"])","8ee11371":"from sklearn.preprocessing import MinMaxScaler","d5cd8cf1":"MMS = MinMaxScaler()\n\n### We train on the train data and transform the test data\ntrain_MMS = MMS.fit_transform(train_df[num_features])\ntest_MMS = MMS.transform(test_df[num_features])\ntrain_MMS_df = to_df(train_MMS, num_features)\ntest_MMS_df = to_df(test_MMS, num_features, test=True)","79ce22bb":"### Let's look at the density distributions\ndraw_scaler_kde(train_df, train_MMS_df, test_df, test_MMS_df, label1=\"Train before Scale\",\n               label2=\"Train After Scale\", label3=\"Test Before Scale\", label4=\"Test After Scale\",\n                columns=[\"LotFrontage\",\"LotArea\",\"GarageArea\"])","8a698295":"from sklearn.preprocessing import RobustScaler","4fa3169a":"RS = RobustScaler()\n### We train on the train data and transform the test data\ntrain_RS = RS.fit_transform(train_df[num_features])\ntest_RS = RS.transform(test_df[num_features])\ntrain_RS_df = to_df(train_RS, num_features)\ntest_RS_df = to_df(test_RS, num_features, test=True)","313338cd":"### Let's look at the density distributions\ndraw_scaler_kde(train_df, train_RS_df, test_df, test_RS_df, label1=\"Train before Scale\",\n               label2=\"Train After Scale\", label3=\"Test Before Scale\", label4=\"Test After Scale\",\n                columns=[\"LotFrontage\",\"LotArea\",\"GarageArea\"])","84d436ab":"from sklearn.preprocessing import MaxAbsScaler","5ec75ee4":"MAS = MaxAbsScaler()\n### We train on the train data and transform the test data\ntrain_MAS = MAS.fit_transform(train_df[num_features])\ntest_MAS = MAS.transform(test_df[num_features])\ntrain_MAS_df = to_df(train_MAS, num_features)\ntest_MAS_df = to_df(test_MAS, num_features, test=True)","9a40a670":"### Let's look at the density distributions\ndraw_scaler_kde(train_df, train_MAS_df, test_df, test_MAS_df, label1=\"Train before Scale\",\n               label2=\"Train After Scale\", label3=\"Test Before Scale\", label4=\"Test After Scale\",\n                columns=[\"LotFrontage\",\"LotArea\",\"GarageArea\"])","a7e951be":"class Scaler:\n    \n    def __init__(self, data_dict, features):\n        \n        self.data_dict = data_dict.copy()\n        self.features = features\n    \n    def to_df(self, data, test_flag=False):\n        \n        \"\"\"Function converting scaled data to DataFrame\"\"\"\n        \n        train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\n        test_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=\"Id\")\n\n        if test_flag:\n            \n            df = pd.DataFrame(data, columns=test_df[self.features].columns, index=test_df.index)\n        else:\n            df = pd.DataFrame(data, columns=train_df[self.features].columns, index=train_df.index)\n\n        return df\n    \n    \n    def concat_data(self, data, test_flag=False):\n        \n        \"\"\"Function for combining scaled features and features that were not scaled\"\"\"\n        \n        if test_flag:\n            \n            ### First DataFrame of the test data dictionary\n            first_df_data = self.data_dict[list(self.data_dict.keys())[0]][1]\n            \n        else:\n            \n            ### First DataFrame of the train data dictionary\n            first_df_data = self.data_dict[list(self.data_dict.keys())[0]][0]\n\n        ### Remaining features from DataFrame\n        other_features = first_df_data.drop(self.features, axis=1).columns\n\n        ### Combine datasets to get a dataset from all features\n        concat_data = pd.concat([data, first_df_data[other_features]], axis=1)\n        \n        return concat_data\n    \n        \n    def Standard_Scaler(self):\n        \n        \"\"\"Function for scaling by the StandardScaler method\"\"\"\n        \n        SS_data_dict = dict()\n        \n        for name_data, data in self.data_dict.items():\n            \n            SS = StandardScaler()\n            \n            x_train, x_test = data\n            \n            ### Scaling and transforming scaled data into a DataFrame\n            temp1 = self.to_df(SS.fit_transform(x_train[self.features]))\n            temp2 = self.to_df(SS.transform(x_test[self.features]), test_flag=True)\n            \n            ### Add to each dataset features that did not participate in scaling\n            train_scaled = self.concat_data(temp1)\n            test_scaled = self.concat_data(temp2, test_flag=True)\n            \n            ### Write a tuple of datasets into the dictionary\n            SS_data_dict[name_data + \"_SS\"] = (train_scaled, test_scaled)\n            \n        return SS_data_dict\n    \n    \n    def MinMax_Scaler(self):\n        \n        \"\"\"Function for scaling by the MinMaxScaler method\"\"\"\n        \n        MMS_data_dict = dict()\n        \n        for name_data, data in self.data_dict.items():\n            \n            MMS = MinMaxScaler()\n            \n            x_train, x_test = data\n            \n            ### Scaling and transforming scaled data into a DataFrame\n            temp1 = self.to_df(MMS.fit_transform(x_train[self.features]))\n            temp2 = self.to_df(MMS.transform(x_test[self.features]), test_flag=True)\n            \n            ### Add to each dataset features that did not participate in scaling\n            train_scaled = self.concat_data(temp1)\n            test_scaled = self.concat_data(temp2, test_flag=True)\n            \n            ### Write a tuple of datasets into the dictionary\n            MMS_data_dict[name_data + \"_MMS\"] = (train_scaled, test_scaled)\n            \n        return MMS_data_dict\n    \n    \n    def Robust_Scaler(self):\n        \n        \"\"\"Function for scaling by the RobustScaler method\"\"\"\n        \n        RS_data_dict = dict()\n        \n        for name_data, data in self.data_dict.items():\n            \n            RS = RobustScaler()\n            \n            x_train, x_test = data\n            \n            ### Scaling and transforming scaled data into a DataFrame\n            temp1 = self.to_df(RS.fit_transform(x_train[self.features]))\n            temp2 = self.to_df(RS.transform(x_test[self.features]), test_flag=True)\n            \n            ### Add to each dataset features that did not participate in scaling\n            train_scaled = self.concat_data(temp1)\n            test_scaled = self.concat_data(temp2, test_flag=True)\n            \n            ### Write a tuple of datasets into the dictionary\n            RS_data_dict[name_data + \"_RS\"] = (train_scaled, test_scaled)\n            \n        return RS_data_dict\n    \n    \n    def MaxAbs_Scaler(self):\n        \n        \"\"\"Function for scaling by the MaxAbsScaler method\"\"\"\n        \n        MAS_data_dict = dict()\n        \n        for name_data, data in self.data_dict.items():\n            \n            MAS = MaxAbsScaler()\n            \n            x_train, x_test = data\n            \n            ### Scaling and transforming scaled data into a DataFrame\n            temp1 = self.to_df(MAS.fit_transform(x_train[self.features]))\n            temp2 = self.to_df(MAS.transform(x_test[self.features]), test_flag=True)\n            \n            ### Add to each dataset features that did not participate in scaling\n            train_scaled = self.concat_data(temp1)\n            test_scaled = self.concat_data(temp2, test_flag=True)\n            \n            ### Write a tuple of datasets into the dictionary\n            MAS_data_dict[name_data + \"_MAS\"] = (train_scaled, test_scaled)\n            \n        return MAS_data_dict\n    \n    \n    def scaled_dict(self):\n        \n        \"\"\"Function to return a dictionary of scaled data\"\"\"\n        \n        scaled_dict = dict()\n        \n        SS_data_dict = self.Standard_Scaler();\n        MMS_data_dict = self.MinMax_Scaler();\n        RS_data_dict = self.Robust_Scaler();\n        MAS_data_dict = self.MaxAbs_Scaler();\n        \n        scaled_dict = {**SS_data_dict, **MMS_data_dict, **RS_data_dict, **MAS_data_dict}\n        \n        return scaled_dict","fee6ce42":"%%time\n\n### Get a dictionary of scaled data\nscaler = Scaler(data_dict=filled_data_dict, features=num_features)\nfilled_scaled_data_dict = scaler.scaled_dict()","e5218eed":"### Dictionary keys\nfilled_scaled_data_dict.keys()","1b6fc408":"%%time\nlogger = test_models(models_dict, filled_scaled_data_dict, y_train=y_data.values.ravel())","20968c93":"for model_name, model in models_dict.items():\n    logger.plot(\"Model: \" + model_name, model_name)","ca72f3c7":"def diagnostic_plots(df, feature, title):\n    \n    \"\"\"Function for diagnostic plots\"\"\"\n    \n    fig, ax = plt.subplots(figsize=(15,10))\n    \n    # hist\n    plt.subplot(2, 2, 1)\n    df[feature].hist(bins=30)\n    plt.title(\"Histogram\")\n    \n    ## Q-Q plot\n    plt.subplot(2, 2, 2)\n    probplot(df[feature], plot=plt)\n    plt.title(\"Q-Q plot\")\n    \n    # violinplot\n    plt.subplot(2, 2, 3)\n    sns.violinplot(x=df[feature])\n    plt.title(\"ViolinPlot\")\n    \n    # boxplot\n    plt.subplot(2, 2, 4)\n    sns.boxplot(x=df[feature])\n    plt.title(\"BoxPlot\")\n    fig.suptitle(title)\n    plt.show()","9a1d1ed3":"diagnostic_plots(train_df, \"LotFrontage\", \"LotFrontage - Original\")","14c06d0c":"diagnostic_plots(train_df, \"GrLivArea\", \"GrLivArea - Original\")","d250e6db":"### Normal distribution\nImage(\"..\/input\/img-hsprice\/normal_dist.png\", width=700, height=250)","0458f401":"### Skewed distribution\nImage(\"..\/input\/img-hsprice\/center_skewed.png\", width=400, height=150)","478d8cd6":"Image(\"..\/input\/img-hsprice\/out_sigma.png\", width=700, height=150)","4f358157":"Image(\"..\/input\/img-hsprice\/out_quantile.png\", width=700, height=150)","3a508249":"Image(\"..\/input\/img-hsprice\/out_skewed.png\", width=400, height=150)","64b8fc37":"# Upper and lower outliers limit calculation type\nfrom enum import Enum\nclass OutlierBoundaryType(Enum):\n    SIGMA = 1\n    QUANTILE = 2\n    IRQ = 3","84c9adef":"def get_outlier_boundaries(df, feature, outlier_boundary_type: OutlierBoundaryType):\n    \n    \"\"\"Function for calculating the upper and lower outliers limits\"\"\"\n    \n    if outlier_boundary_type == OutlierBoundaryType.SIGMA:\n        \n        K1 = 3\n        lower_boundary = df[feature].mean() - (K1 * df[feature].std())\n        upper_boundary = df[feature].mean() + (K1 * df[feature].std())\n\n    elif outlier_boundary_type == OutlierBoundaryType.QUANTILE:\n        \n        lower_boundary = df[feature].quantile(0.05)\n        upper_boundary = df[feature].quantile(0.95)\n\n    elif outlier_boundary_type == OutlierBoundaryType.IRQ:\n        \n        K2 = 1.5\n        IQR = df[feature].quantile(0.75) - df[feature].quantile(0.25)\n        lower_boundary = df[feature].quantile(0.25) - (K2 * IQR)\n        upper_boundary = df[feature].quantile(0.75) + (K2 * IQR)\n\n    else:\n        raise NameError('Unknown Outlier Boundary Type')\n        \n    return lower_boundary, upper_boundary  ","394c911f":"### List of features that we will check\nfeatures_list = [\"LotFrontage\", \"GrLivArea\"]\n\nfor feature in features_list:\n       \n    ### diagnostic_plots before removing outliers\n    title = 'Method-{}, Lines-{}'.format(feature+\" Original\", train_df[feature].shape[0])\n    diagnostic_plots(train_df, feature, title)\n    \n    for obt in OutlierBoundaryType:\n    \n        ### diagnostic_plots after removing outliers\n        # Calculation of the upper and lower bounds of the distribution\n        lower_boundary, upper_boundary = get_outlier_boundaries(train_df, feature, obt)\n\n        # Flags to remove outliers\n        outliers_temp = np.where(train_df[feature] > upper_boundary, True, \n                                np.where(train_df[feature] < lower_boundary, True, False))\n\n        # Flag based data deletion\n        data_trimmed = train_df.loc[~(outliers_temp), ] \n\n        title = 'Field-{}, Method-{}, Lines-{}'.format(feature+\" Afted delete outlier\", obt, data_trimmed.shape[0])\n        diagnostic_plots(data_trimmed, feature, title)","1bebe8b2":"### List of features that we will check\nfeatures_list = [\"LotFrontage\", \"GrLivArea\"]\n\nfor feature in features_list:\n       \n    ### diagnostic_plots before replacing outliers\n    title = 'Field-{}'.format(feature+\" Original\")\n    diagnostic_plots(train_df, feature, title)\n        \n    for obt in OutlierBoundaryType:\n        \n        ### diagnostic_plots after replacing outliers\n        # Calculation of the upper and lower bounds of the distribution\n        lower_boundary, upper_boundary = get_outlier_boundaries(train_df, feature, obt)\n        \n        # Data change\n        train_df[feature] = np.where(train_df[feature] > upper_boundary, upper_boundary, \n                                 np.where(train_df[feature] < lower_boundary, lower_boundary, train_df[feature]))\n        title = 'Field-{}, Method-{}'.format(feature, obt)\n        diagnostic_plots(train_df, feature, title)","b352752f":"IQR = y_data[\"SalePrice\"].quantile(0.75) - y_data[\"SalePrice\"].quantile(0.25)\nlower_boundary = y_data[\"SalePrice\"].quantile(0.25) - (2 * IQR)\nupper_boundary = y_data[\"SalePrice\"].quantile(0.75) + (2 * IQR)","2be6a040":"### lower bounds\nlower_boundary","23bedd27":"### upper bounds\nupper_boundary","8c8a260f":"### Find the indices of outliers in the target feature\n\nupper_indexes = list(y_data[\"SalePrice\"].loc[y_data[\"SalePrice\"] < lower_boundary].index)\nlower_indexes = list(y_data[\"SalePrice\"].loc[y_data[\"SalePrice\"] > upper_boundary].index)\n\noutlier_indexes = upper_indexes + lower_indexes","1e9c5a50":"### Outlier indices\noutlier_indexes","39b67b20":"### Before outlier removal\n\nplt.subplot(211)\nprobplot(y_data[\"SalePrice\"], plot=plt)\nplt.subplot(212)\nsns.distplot(y_data);","6167e1d9":"### After outlier removal\noutliers_temp = np.where(y_data[\"SalePrice\"] > upper_boundary, True, \n                        np.where(y_data[\"SalePrice\"] < lower_boundary, True, False))\ny_trimmed = y_data.loc[~(outliers_temp), ]","5135ea6a":"plt.subplot(211)\nprobplot(y_trimmed[\"SalePrice\"], plot=plt)\nplt.subplot(212)\nsns.distplot(y_trimmed);","c3c8e515":"y_data = y_trimmed","dc905199":"for name_data, data in filled_scaled_data_dict.items():\n    x_train, x_test = data\n    x_train.drop(outlier_indexes, axis=0)\n    filled_scaled_data_dict[name_data] = (x_train.drop(outlier_indexes, axis=0), x_test)","bb92b856":"### Dimension of the training dataset after removing outliers\nfilled_scaled_data_dict[\"KNNImputer_MAS\"][0].shape","fc62d768":"%%time\nlogger = test_models(models_dict, filled_scaled_data_dict, y_train=y_data.values.ravel())","74605dca":"for model_name, model in models_dict.items():\n    logger.plot(\"Model: \" + model_name, model_name)","b8912dca":"Image(\"..\/input\/img-hsprice\/filter_method.png\")","9dfb1164":"sns.heatmap(train_df.iloc[:,:20].corr(), annot=True, fmt='.3f')","c6efe939":"def corr_treshold(data, treshold):\n    \n    corr_data = data.corr().abs().unstack()\n    corr_data = corr_data[corr_data >= treshold]\n    corr_data = corr_data[corr_data < 1]\n    \n    corr_df = pd.DataFrame(corr_data).reset_index()\n    corr_df.columns = ['feature1', 'feature2', 'corr']\n    corr_df = corr_df.drop_duplicates('corr')\n    \n    return corr_df","4ff1106f":"### Features with a correlation threshold> = 0.5\ncorr_treshold(train_df.iloc[:,:20], 0.5)","84243eae":"Image(\"..\/input\/img-hsprice\/wrapper_method.png\", width=700, height=100)","5b29650f":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS","74999c9c":"def feature_selector(models_dict, X_train, y_train):\n    \"\"\"Function for selecting the best combination of features for models\"\"\"\n\n    ### Dictionary for writing the best combinations of features\n    best_features = {}\n    \n    ### Training models on the SFS algorithm\n    for name_model, model in models_dict.items():\n\n        sfs = SFS(model, \n                   k_features=(3, 10), \n                   forward=True, \n                   floating=False, \n                   scoring=rmse,\n                   cv=5)\n\n        sfs = sfs.fit(X_train, y_train)\n\n        best_features[name_model] = sfs.k_feature_names_\n        \n        print('model {}, score (accuracy: {}):'.format(name_model, sfs.k_score_))\n        print('selected features: {}\\n'.format(sfs.k_feature_names_))\n    \n    \n    return best_features","308acad1":"### An example of how SequentialFeatureSelector works\nfeature_selector({\"KNN\": KNeighborsRegressor(), \"tree\": DecisionTreeRegressor(),\n                \"Lasso\": Lasso()}, filled_scaled_data_dict[\"MeanImputer_SS\"][0].iloc[:,:11], y_data)","3c4ed285":"Image(\"..\/input\/img-hsprice\/embedded_method.png\", width=700, height=100)","d946c30e":"from sklearn.feature_selection import SelectFromModel","020f9dbe":"### Using L1 regularization\nsvr = SVR(kernel='linear')\nsvr.fit(filled_scaled_data_dict[\"KNNImputer_SS\"][0].iloc[:,:10], y_data.values.ravel())\n### Regression coefficients\nsvr.coef_","729543fc":"### Features with the False flag are excluded\nsfm = SelectFromModel(svr)\nsfm.fit(filled_scaled_data_dict[\"KNNImputer_SS\"][0].iloc[:,:10], y_data.values.ravel())\nsfm.get_support()","61f32dc7":"params_dict = {\"KNN\": {\"n_neighbors\": np.arange(2,10)},\n                \"tree\": {\"max_depth\": np.arange(2,8),\\\n                         \"min_samples_split\": np.arange(2,4),\\\n                         \"min_samples_leaf\": np.arange(1,4)},\n               \"Lasso\": {\"alpha\": [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]},\n               \"Ridge\": {\"alpha\": [0.005, 0.01, 0.1, 1, 5, 10, 15]},\n              \"SVR\": {\"C\": [0.001,0.005,0.01,0.05,0.1,0.4,0.8,1],\\\n                        \"kernel\": [\"linear\", \"poly\", \"rbf\"]},\n               \"RF\": {\"n_estimators\": np.arange(100,200,50), \"max_depth\": np.arange(2,4),\\\n                       \"min_samples_split\": np.arange(2,4),\n                      \"min_samples_leaf\": np.arange(2,4)},\n              \"GB\": {\"n_estimators\": [1000], \"max_depth\": np.arange(2,3),\n                    \"min_samples_leaf\": np.arange(1,3), \"min_samples_split\": np.arange(2,4)}}","0156191b":"def test_models_GSCV(models_dict, data_dict, y_train, params_dict):\n    \n    \"\"\"Hyperparameter tuning function\"\"\"\n    \n    logger = MetricLogger()\n    \n    ### A dictionary for recording the values of the best parameters for each model\n    best_params_dict = {}\n    \n    for model_name, model in models_dict.items():\n        \n        print(\"\\n-------------------\")\n        print(model_name+\": beginning\")\n        print(\"-------------------\\n\")\n        params = params_dict[model_name]\n        gscv = GridSearchCV(model, params, cv=5, n_jobs=-1, scoring=rmse)\n        \n        for data_name, data in data_dict.items():\n\n            X_train, X_test = data\n            print(data_name)\n            gscv.fit(X_train,y_train)\n            score = gscv.best_score_\n            best_params = gscv.best_params_\n            \n            best_params_dict[model_name] = best_params\n            logger.add_data(metric=data_name, model=model_name, score=score)\n    \n    print(\"Finished.\")\n    return logger, best_params_dict","1fdeaacf":"%%time\nlogger, best_params_dict = test_models_GSCV(models_dict, filled_scaled_data_dict, y_data.values.ravel(), params_dict)","fa0405a8":"for model_name, model in models_dict.items():\n    logger.plot(\"Model: \" + model_name, model_name)","c21cdd39":"### best hyperparameters for each model\nbest_params_dict","073840fc":"### A dictionary of the best optimization combinations for each model\ndata_dict_for_models = {}\ndata_dict_for_models[\"KNN\"] = filled_scaled_data_dict[\"MICE_SS\"]\ndata_dict_for_models[\"tree\"] = filled_scaled_data_dict[\"MeanImputer_SS\"]\ndata_dict_for_models[\"Lasso\"] = filled_scaled_data_dict[\"ModeImputer_SS\"]\ndata_dict_for_models[\"Ridge\"] = filled_scaled_data_dict[\"ModeImputer_MAS\"]\ndata_dict_for_models[\"SVR\"] = filled_scaled_data_dict[\"MeanImputer_RS\"]\ndata_dict_for_models[\"RF\"] = filled_scaled_data_dict[\"MeanImputer_RS\"]\ndata_dict_for_models[\"GB\"] = filled_scaled_data_dict[\"MICE_MMS\"]","9e3c21d2":"### Model dictionary with tuned hyperparameters\nbest_models_dict = {\n                    \"KNN\": KNeighborsRegressor(n_neighbors=2),\n                    \"tree\": DecisionTreeRegressor(random_state=42, max_depth=2, min_samples_leaf=1,\\\n                                                 min_samples_split=2),\n                    \"Lasso\": Lasso(random_state=42, max_iter=2000, alpha=0.0008),\n                    \"Ridge\": Ridge(random_state=42, max_iter=2000, alpha=0.005),\n                    \"SVR\": SVR(C=0.001, kernel='rbf'),\n                    \"RF\": RandomForestRegressor(random_state=42, max_depth=2, min_samples_leaf=2,\n                                               min_samples_split=2, n_estimators=150),\n                    \"GB\": GradientBoostingRegressor(random_state=42, n_estimators=1000, learning_rate=0.05, \n                                              max_depth=5, min_samples_leaf=2, min_samples_split=2, \n                                              loss='huber')\n}","4080580a":"from scipy.special import inv_boxcox","34cb5549":"def submission(best_models_dict, data_dict_for_models, y_data):\n    for name_model, model in best_models_dict.items():\n\n        x_train, x_test = data_dict_for_models[name_model]\n\n        model.fit(x_train, y_data.values.ravel())\n        predicted = model.predict(x_test)\n        \n        submit = pd.DataFrame({\"Id\": test_df.index, \"SalePrice\": inv_boxcox(predicted, lamb)})\n        submit.to_csv('Submission_{}.csv'.format(name_model), index=False)\n        \n        print(\"Submission_{}.csv ended.\\n\".format(name_model))","042f667a":"### Result in the leaderboard\nImage(\"..\/input\/img-hsprice\/house_price_leaderbord.png\")","84092fb2":"In this dataset, there are no highly correlated features among the features we have selected. \nTherefore, as an example, we will call highly correlated features which have a threshold >= 0.5.","bb0b8c88":"**Assumption:** if the gaps are NOT randomly distributed and we want to separate the missing values \u200b\u200bfrom the rest of the values.","b1a79048":"**Let's remove the outlier indices from the entire set of our training datasets**","57d66241":"The outlier detection task is the task of highlighting the elements at the edges of the histogram.","fc662c2a":"**Why are feature selection methods used?**\n- Simple models (with fewer features) are easier to interpret.\n- Models learn worse on datasets with a large number of features.\n- Reduced training and prediction times.\n- Reducing the risk of overfitting.","6251a08d":"**We will not deal with the selection of features for our models, because in our case it will be too resource-intensive.**","5c56402d":"The method is implemented using the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\">StandardScaler<\/a> class.","4026d130":"<center>$x'=\\dfrac{x - \\mu(x)}{\\sigma(x)}$<\/center>","1b7ccbb6":"**The main steps of the method:**\n 1. Arbitrary imputation is made for all columns (for example, for numeric columns - the average value)..\n 2. For the first column, empty values are put back.\n 3. Missing values in the first column are predicted by a machine learning model based on the rest of the values.\n 4. The predicted values are substituted in the first column.\n 5. The steps above are performed in a loop for all columns..\n 6. The above actions are performed a specified number of times. It is assumed that at each iteration, we gradually improve the values that correspond to the gaps.","647e7264":"**Method properties:**\n- The average may vary.\n- The standard deviation may vary.\n- The shape of the original distribution can vary.\n- Maximum and minimum values in the range [0; 1].\n- The outliers are saved.","91530ed4":"In this case, other features do affect when filling.\n\nThe idea is that features can depend on each other and such dependencies should be used when filling in the gaps.\n\nIn this case, we solve a separate machine learning problem, considering the missing feature as target (y), and the remaining features as initial (X).\n\nVarious machine learning methods can be used to solve the problem. In practice, the nearest neighbors method is most often used.\n\nAlso, the problem is that almost all features can contain gaps, and to fill them, other features must first be imposed using known methods. (there is a semblance of cyclic links).","a0e1c40a":"**Method properties:**\n- The average value is reduced to 0.\n- The standard deviation is reduced to 1.\n- The shape of the original distribution is preserved.\n- The maximum and minimum values may vary.\n- The outliers are saved","d81b24c0":"- Elimination involves the removal or replacement of outliers.\n- For can replace the upper and lower bounds found.\n- The outliers found can be considered \"pseudo-missing values\" and used for their gap handling replacement methods.","bb2301c1":"## 8.2 Eliminate outliers","8981f270":"**Disadvantages:**\n- Violation of the parameters of the original distribution.\n- Since the values \u200b\u200bat the edge of the distribution are actually anomalies (outliers), this approach may overlap with anomaly search algorithms.","998ab9de":"**Removal of outliers**","766db42c":"\nHere we will select the best optimization combinations for each of the models, train each model on the best combination of hyperparameters. And let's look at the results that the models will get on Kaggle.","d209cf1d":"We will train the model from \"the box\", we will select hyperparameters on GridSearchCV at the end.","52d72750":"**Do the features need to be scaled before or after dividing into training and test datasets?**\n\nThe traditional approach is that the data is divided into training and test samples **BEFORE** scaling. Scaling parameters (eg mean, variance) are taken from the training sample only and then applied to the test sample.","31461d09":"## 9.2 Training of models","35e35c21":"The **KNNImputer** class is used.","b2061608":"Machine learning methods (both supervised and unsupervised) that **DEPEND** on feature scaling:\n- Nearest Neighbors Method\n- Linear regression\n- Logistic regression\n- Support vector machine (SVM)\n- Some clustering algorithms (K-means)\n- Principal Component Analysis (PCA)","8d49bf40":"**For numeric features:**\n- Filling value at the center of a distribution\n- Filling with a constant. Useful in case of \"non-random\" distribution of gaps.","b62055ab":"<center>$x'=\\dfrac{x - min(x)}{max(x)-min(x)}$<\/center>","bbb2c309":"For skewed distributions, these indicators differ:","7e6b9516":"**Method properties:**\n- The median is reduced to 0.\n- The standard deviation may vary.\n- The shape of the original distribution can vary.\n- The maximum and minimum values \u200b\u200bmay vary.\n- The outliers are eliminated.","897c1d7f":"To solve the problem, the <a href=\"https:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/\">Sequential Feature Selector<\/a> class (with the constructor parameter forward = True) from the MLxtend library is used.","3bbe204e":"### 3.2 Filling missing values for numeric characteristics with the \"tail of the distribution\"","7260c73e":"**Using the Three Sigma Rule**","68c86284":"- The methods select the most \"suitable\" features without using machine learning models based on the statistical characteristics of the sample (correlation, etc.)\n- The least expensive in terms of computing resources.\n- They may be inferior to other methods in terms of the quality of feature selection, because they do not take into account the relationship between features.\n- Well suited for initial feature filtering.","8f25d8b5":"## 9.1 Removal of outliers","83910a5c":"### Example. SVR","6d8e5837":"### 4.2 MICE (multivariate Imputation of Chained Equations)","7e623a4c":"### 4.1 KNNImputer","0e9c42f4":"# 7. Training of models. Part 2","60c0ec74":"### Sequential forward selection","7aa954a0":"## 6.1 Scaling based on Z-score","9241409a":"**Let's create a set of datasets where numeric features are filled with indicators of distribution centers - median, mode and mean. To see which method performs better on cross_validation for different training models.**","7660df7b":"The interquartile range (IQR) is the difference between the third quartile and the first quartile:","895082d0":"The method is implemented using the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MaxAbsScaler.html\">MaxAbsScaler<\/a> class.","3fefd3c3":"The **KNNImpute**r only works with numeric features.","71142436":"# In this notebook, only data processing methods will be shown. Conclusions for each block will not be provided, make them yourself","976b9110":"**Why is it necessary to scale features?**\n\nMany machine learning algorithms are designed in such a way that features with a lower amplitude are \"penalized\" compared to features with a larger amplitude, and have less impact on the model building process.","f02d9e73":"### Approach in case of skewed distribution","ef1384b7":"**Let's look at the distribution histogram and Q-Q plot before and after removing outliers**","4e3a6855":"### 5.1.3 Converting the target feature to \"normal\" form","2738f35d":"## 6.4 Scaling by maximum value","35bc14d9":"## 6.3 Scaling by median","ac6a014b":"To solve the problem, the <a href=\"https:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/ExhaustiveFeatureSelector\/\">ExhaustiveFeatureSelector<\/a> class from the MLxtend library is used.","6ccb82fe":"**Method properties:**\n- The average is NOT CENTERED. It is assumed that it is already centered based on the nature of the data.\n- The standard deviation is not scaled.\n- The shape of the original distribution can vary.\n- Maximum and minimum values in the range [-1; 1].","660ada60":"**Removing constant and pseudo-constant (almost constant) features**","d4d4f6da":"**When is it recommended to use?**\n- If the gaps are randomly distributed,\n- In the ideal case, gaps are no more than 5% of the sample.","03ec4915":"As we can see, the sample sizes do not match, therefore, the training sample contains sub-features of categorical features that are not in the test sample. To solve this problem, let's combine the training and test samples before coding, and then split them back.","313e77ea":"Since the **Random Forest** method is used for imputation, it is also possible to tune hyperparameters.","308c198f":"**For categorical features:**\n- Filling with a mode.\n- Entering a separate category value for missing values.","1ce7181d":"## 7.1 Datasets scaling","6ff2611e":"## 7.2 Training of models","87c5a330":"**Let's look at the quality of the models when removing outliers**","d01667f1":"# 2. Remove missing values","9944d3a3":"## 10.2 Wrapper methods","a1e58015":"### Let's encode categorical features using the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html\">OneHotEncoder<\/a> method.","c1b185ff":"**Outlier processing should be done ONLY IN THE TRAINING SAMPLE to improve the accuracy of model building.**","ca28c0eb":"\nOr a distribution similar to normal.","5e6eaf8d":"## 2.1 Let's look at the distributions when removing all gaps for several features.","889ce45b":"**How to calculate the \"tail of the distribution\"?**\n\nIf the distribution of feature data $\ud835\udc53$ resembles normal:\n<center>$extreme value = mean(f) + 3 \u22c5 std(f)$<\/center>  \n\nFor skewed distribution:\n<center>$IQR = Q3 \u2212 Q1$<\/center>\n\n$IQR$ - interquartile range\n<center>$extreme  value = Q3 + K \u22c5 IQR$<\/center>\n\n                             \n                             \nThe $ K $ value is usually chosen to be $ 1.5 $. But for extreme outliers, choose $ K $ = $ 3 $","112dc4f0":"<center>$IQR=Q_3(x)-Q_1(x)$<\/center>","ebbe09aa":"Scaling is a change in the measurement range of a feature in order to improve the quality of model building.","73d11d7f":"- They use machine learning models for feature selection.\n- Subsets of features are formed.\n- A separate machine learning model is built for each subset of features.\n- For a specific machine learning model (which is used for evaluation), an optimal set of features is most often generated. (But this set does not generalize to all models.)\n- Very expensive in terms of computing resources.\n- Can lead to overfitting of models (especially in the case of small samples).","d723e08c":"<center>$outlier<mean(x)-3\t\n\\cdot std(x)$<\/center>\n<center>$outlier>mean(x)+3\t\n\\cdot std(x)$<\/center>\n  \nwhere $\\sigma=std(x)$","3f0442f6":"### 5.1.1 Filling in numerical features:","80400179":"- For each combination, a model is built and a quality metric is assessed.\n- The best model is selected based on the metric.\n- Compared to the previous two approaches, this approach is the most resource-demanding.","0233b37b":"<center>$outlier<x.quantile(5\\%)$<\/center>\n<center>$outlier>x.quantile(95\\%)$<\/center>","70632952":"### Approach in case of a normal distribution","85e9c0ec":"By definition of a distribution histogram, outliers are the values \u200b\u200bat the edges of the histogram (very large or very small compared to the entire sample).","7c12ebc4":"# 9 Training of models. Part 3","c4845bbd":"### Exhaustive feature selection","2038d41f":"In the case of a normal distribution, the mathematical expectation, median and mode coincide:","34da5122":"- If a feature contains the same (constant) values, then it cannot contribute to the construction of the model.\n- If a feature contains almost all of the same (constant) values, then most likely it is of little use in building a model. (At the same time, you need to be careful, since this feature can be an indicator of one of the classes in the case of classification).\n- You can use the \"unique()\" function to find such features.\n- But it is more convenient to use variance:\n- A constant feature has zero variance.\n- A pseudo-constant list has a very small variance value.\n- It is possible to use the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html\">VarianceThreshold<\/a> class from the Sklearn library.","12dbe397":"<center>$x'=\\dfrac{x-median(x)}{IQR}$<\/center>\n  \nwhere\n  \n<center>$IQR=Q3(x)-Q1(x)$<\/center>\n  \n$IQR$\u2212 is the difference between the 1st and 3rd quartiles.","a7e583bb":"# 6 Feature scaling","3b67d31d":"Machine learning techniques that **DO NOT DEPEND** on feature scaling:\n- Decision trees and other algorithms based on them\n    - Random Forest\n    - Gradient Boosting\n    \nIn the algorithm for constructing a decision tree, a single metric space is not built for all features. A set of branches is constructed for individual features, the scale of the features does not matter.","f2ee44cf":"**Let's create a dataset set with all fillings (MeanImputer, MedianImputer, ...) and scales:**","8eca6bd6":"# 10 Feature Selection","a9226729":"**Let's fill in the categorical features with a mode where the missing values are less than 1%.**","350c6d25":"# 8. Outliers treatment","c2607804":"\n## 5.1 Preparing data for training models","dfd45bd6":"<center>$x'=\\dfrac{x}{max(| x |)}$<\/center>","efccae34":"### Sequential backward selection","f04f6d4f":"### Correlation-based methods","ea22c2d7":"### 5.1.2 Filling in categorical features:","fe3d43f3":"Then:\n<center> $ outlier <Q_1 (x) -K \\cdot IQR $ <\/center>\n<center> $ outlier> Q_3 (x) + K \\cdot IQR $ <\/center>\n\nThe $K$ value is usually chosen equal to 1.5.\n \nFor extreme outliers, $K$ is chosen equal to 3.","cfc05438":"**Using the interquartile range**","47590ada":"**When is it recommended to use?**\n- If there is too much of the missing data and there is a danger of disturbing the distribution of the original data when filling in the gaps. It is recommended to delete the entire attribute (column).\n- If the dataset is large and there are relatively few missing data, then it is recommended to delete lines containing gaps in the data. (By \"a little\" is meant about 5% of the sample).","9e71d2fd":"- It is desirable that the features correlate well with the target feature.\n- It is important that the features do not correlate with each other.","f81b644c":"The method is implemented using the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html\">RobustScaler<\/a> class.","6d0cf2a0":"**Selection of hyperparameters**","3e80f6ea":"**In our case, we will try only one method - using the interquartile range with $\ud835\udc3e = 2$.**","f1f7d526":"**Using 5% and 95% quantiles**","16db6f52":"The method is implemented using the  <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html\">MinMaxScaler<\/a> class.","0ee59d0b":"The <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#select-from-model\">SelectFromModel<\/a> class can be used to select the most important features from the model.","e84c8bce":"## 5.2 Training models with different filling options","c279ecd9":"The task of feature selection is to select features that are most useful for further model building.","761a89e2":"# 1. Getting familiar with data","f9a87822":"## 10.3 Embedded methods","79fbb51e":"### 3.1 Filling in missing values of numeric features with median, mode and mean","24191089":"**Why is it necessary to eliminate outliers?** Outliers have a negative impact on model building.","d81744fe":"**What models allow you to assess the importance of features?**\n- Linear models. Linear regression in a regression problem and logistic regression in a classification problem, SVR.\n- Decision tree and ensemble models based on it.","cfa06798":"**Three groups of feature selection methods:**\n- Filter methods.\n- Wrapper methods.\n- Embedded methods.","7ecca16a":"where $ x $ is a feature, $ \\mu (x) = mean (x) $ is the mean, $ \\sigma (x) = std (x) $ is the standard deviation.","78d1e820":"# 4. Filling in values for several feature","57efb020":"**Replacing outliers**","4fa290e8":"Let's look at a MissForest implementation using IterativeImputer.","f2e67bb5":"## 1.1 Missing values","a9eed655":"# 5. Training of models. Part 1","16858511":"**Wrapping methods include three kinds of algorithms:**\n- Sequential forward selection is based on the gradual addition of new features to the model.\n- Sequential backward selection is based on the gradual removal of features from the model.\n- Exhaustive feature selection tests all possible combinations of features.\n\nTo solve the problem of feature selection using wrapping methods, the <a href=\"https:\/\/rasbt.github.io\/mlxtend\/\">MLxtend<\/a> library can be used.","adee9b43":"In this case, other features do not affect when filling.","8aa6dda4":"**The last step is to select the best combinations of hyperparameters for each of the models.**","3b098b7b":"**For numeric features:**","6c4e0b6d":"## 10.1 Filter methods","22456f9a":"### The simplest filtering methods","b796e832":"**Why are there outliers?**\n- Measurement errors\n- Technical errors in data formatting","7f81c7f6":"**Filling in values or imputing** is filling in missing values with their statistical evaluation.","b57ccf69":"**Assumption:** gaps are randomly distributed","57a7d31b":"**Advantages:**\n- Ease of implementation.\n- Separates missing values \u200b\u200bfrom other values.","fdf0f399":"## 8.1 Outliers detection","b9cd0134":"In the **MICE** method, we will use Random Forest as a machine learning model.","5fcad3a5":"**Advantages:**\n- Easy to implement.\n- If the gaps are randomly distributed, the distribution parameters of the original data are saved.","9131891b":"We will optimize the score: **\"r2\"** to maintain the greatest variance between the original and new data.","65019bea":"**Outlier** is a selection item that differs significantly from the rest of the selection items.","b5fdd39a":"To solve the problem, the <a href=\"https:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/\">Sequential Feature Selector<\/a> class (with the constructor parameter forward = False) from the MLxtend library is used.","200b6370":"For centering the distribution, a combination with other methods is possible, for example with the **StandardScaler.**","33e0a5c5":"**Let's fill in the categorical features where the nan value is the null.**","283629ac":"# 3. Filling in values for one feature","18729c01":"**Which value of the center of distribution is better to use?**\n- If the distribution is unimodal, then it is better to use the mode, otherwise the mathematical expectation or the median.\n- But the median is more robust to outliers in the data.","c03ad85c":"**The main tasks of outliers processing:**\n-  Outlier detection.\n- Elimination (removal or replacement) of outliers (depending on the requirements of the task).","2ec7d427":"For conversion, we will use the **Box-Cox** method.","3bdc277d":"## 6.2 MinMax scaling","7a06ef07":"### Filling value at the center of a distribution and constant","8e063f2f":"**Disadvantages:**\n- If the gaps are not randomly distributed, then significant data can be deleted.","724c48df":"Since the KNN model is used for imputation, it becomes necessary to select hyperparameters. In this case, it is necessary to create a complete machine learning pipeline and optimize the parameters of the entire pipeline.","ac742691":"# 11. Training of models. Part 4","0396e042":"- They are a hybrid of filtering and wrapping methods.\n- Feature selection is carried out based on an assessment of the importance of features in the process of constructing a model.\n- Compared to wrapping methods, they are less expensive in terms of computational resources.","49b1ca79":"# 12 Kaggle submit","9877f11e":"This is necessary so that the weights in our linear models are not prohibitively large, and the model learns well. For non-linear methods (DesicionTree, RandomForest, ...), this is optional.","69f83e13":"**Let's find outliers in the target feature and remove these indices from the entire training dataset.**"}}