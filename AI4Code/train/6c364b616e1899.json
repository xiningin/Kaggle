{"cell_type":{"65918d39":"code","339157a2":"code","6e42aa99":"code","6f06ee06":"code","70614328":"code","fa1585c5":"code","8634d670":"markdown","7b3acc2f":"markdown","392872ed":"markdown","617aa03a":"markdown","796fbd40":"markdown","4e139277":"markdown","8f4e70d1":"markdown"},"source":{"65918d39":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import RepeatedKFold, GridSearchCV\n\n# Path to the files, set seed\nPATH = '..\/input\/tabular-playground-series-jul-2021\/'\nSEED = 999\nN_FOLDS = 3\nN_REPEATS = 5\nTARGET_VARS = ['target_carbon_monoxide',\n               'target_benzene',\n               'target_nitrogen_oxides']\n\nnp.random.seed(SEED)\n\n# Load CSV files\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\nsubm = pd.read_csv(PATH + 'sample_submission.csv', index_col='date_time')","339157a2":"def sin_cos_encoding(df, dt, feat_name, max_val):\n    # Encode variable using sin and cos\n    df['sin_' + feat_name] = np.sin(2 * np.pi * (dt\/max_val))\n    df['cos_' + feat_name] = np.cos(2 * np.pi * (dt\/max_val))\n    return None\n\ndef extract_dt_feats(df):\n    # Extract month and hour\n    date_enc = pd.to_datetime(df.date_time)\n    month = date_enc.dt.month\n    hour = date_enc.dt.hour\n    # Add features, compute and add is_weekend\n    sin_cos_encoding(df, month, 'month', 12)\n    sin_cos_encoding(df, hour, 'hour', 23)\n    df['is_weekend'] = date_enc.dt.day_name().isin(['Saturday', 'Sunday'])*1\n    return df\n\n# Expand features from train and test\nx_train = extract_dt_feats(train.copy())\nx_test = extract_dt_feats(test.copy())\n\n# Visualize relationship between is_weekend and targets\nsns.pairplot(x_train, hue='is_weekend', vars=TARGET_VARS, corner=True,\n            plot_kws={'alpha':.1})","6e42aa99":"# Log-transform target vars\nx_train[TARGET_VARS] = np.log(x_train[TARGET_VARS] + 1)\n\n# Plot again, in log-scale\nsns.pairplot(x_train, hue='is_weekend', vars=TARGET_VARS, corner=True,\n            plot_kws={'alpha':.1})\n\n# Split train X and Y, drop date_time from train and test\ny_train = pd.concat([x_train.pop(target) for target in TARGET_VARS], axis=1)\nx_train.drop(columns='date_time', inplace=True)\nx_test.drop(columns='date_time', inplace=True)","6f06ee06":"%%time\n# Define hyperparameter and CV parameter values\npars = {'estimator__learning_rate': [.01, .05, .1],\n        'estimator__max_depth': [3, 5, 10],\n        'estimator__subsample': [.5, .75, 1.],\n        'estimator__n_estimators': [500]}\ncv_pars = RepeatedKFold(n_splits=N_FOLDS, n_repeats=N_REPEATS)\n\n# Build and initialize CV\ncv_model = MultiOutputRegressor(GradientBoostingRegressor())\ncrossval = GridSearchCV(cv_model, pars, scoring='neg_mean_squared_error', cv=cv_pars, n_jobs=-1)\ncrossval.fit(x_train, y_train)\n\n# Visualize CV error\nerror = np.vstack([crossval.cv_results_['split{}_test_score'.format(str(i))] for i in range(N_FOLDS*N_REPEATS)])\nplt.figure(figsize=(16, 4))\nplt.boxplot(error); plt.ylabel('neg_MSE')","70614328":"# Final model using optimal cross-validation parameters\nopt_pars = crossval.best_params_\n\nmodel = MultiOutputRegressor(GradientBoostingRegressor(learning_rate=opt_pars['estimator__learning_rate'],\n                                                       max_depth=opt_pars['estimator__max_depth'],\n                                                       subsample=opt_pars['estimator__subsample'],\n                                                       n_estimators=opt_pars['estimator__n_estimators']))\nmodel.fit(x_train, y_train)\n\nprint('The optimal hyperparameter values are:\\n', opt_pars)","fa1585c5":"# Get predictions\npreds = model.predict(x_test)\n# Recover original units\ninv_preds = np.exp(preds) - 1\n\n# Write to submission table, export\nsubm.iloc[:, :] = inv_preds\nsubm.to_csv(\"submission.csv\")","8634d670":"# Test set predictions \u270d\ufe0f\n\nFinally, we predict on the testing set and write our predictions for all three emission targets. To revert the log-transformation that is embedded into the model, we simply take $e^{\\hat{y}} - 1$.","7b3acc2f":"# Feature engineering \ud83d\udd28\n\nThe training predictor set is of size 7111 x 9, whereas the target set is 7111 x 3. The test predictor set is of size 2247 x 9. Given the relative paucity of predictors in the dataset we will make the best of it by investigating ways of engineering the underlying predictors. In the present analysis I consider the following:\n\n* Capture periodicity over month and hour using $sin$ and $cos$ encodings. If we had picked a categorical encoding instead, the model would be oblivious to the fact January and December are consecutive months. The same holds for hour of the day. As for weekdays, since there are only seven values this particular encoding might degrade model performance\n\n* Identify weekends using a single binary feature. One might expect weekends to associate with lower emissions.\n\nTo extract these features we will process both train and test data using a custom utility defined underneath, `extract_datetime_feats`.","392872ed":"<img src=\"https:\/\/mk0eeborgicuypctuf7e.kinstacdn.com\/wp-content\/uploads\/2017\/02\/Industrial-Emissions-10-web-1024x680.jpg\" width=\"800\">\n    \n# Introduction \ud83e\uddbe\n\nThis notebook addresses the prediction of emission values from three key pollutants - carbon monoxide ($CO$), benzene ($C_6H_6$) and nitrogen oxides ($NO_X$) - using sensor readouts as well as date and time at measurement, relative and absolute humidity and temperature. We will employ some feature engineering to encode a few temporal components, conduct cross-validation (CV) and fit a Gradient Boosting Regression (GBR) model. This dataset is part of the Tabular Playground Series - July 2021 competition.\n\nIt came to my attention that most top entries in this competition exploit some leaked test data using pseudo-labeling, thereby cutting down error by a substantial margin. I am not particularly fond of leveraging leaked information and as such **no external data is used in this analysis**.\n\nWe will go over feature engineering, CV and model building using the optimal CV hyperparameter values. To start things off we will load some important utilities, set random seed and a bunch of other constants and load the CSV files.","617aa03a":"And with this we come to the end \ud83d\ude0e If you found this notebook informative and entertaining, please comment and upvote!","796fbd40":"With this simple feature engineering step we increased the number of available features from 9 to 14, although `date_time` will not be used. The above scatterplots, which depict the bivariate distributions of the three target variables - seemingly intercorrelated - suggest that indeed weekends associate with lower emissions. Also, the three target variables appear to be left-skewed and might therefore be better modeled following a log-transformation.\n\nWe may also note just over a hundred odd measurements of high $CO$ and $NO_X$ but comparatively low $C_6H_6$. These look like outliers but their exclusion did hurt model performance in a previous iteration. \n\nOverall, I propose log-transforming the three target variables from `x_train`, moving them to a separate dataframe,`y_train` and finally dropping `date_time` from both `x_train` and `x_test`.","4e139277":"Upon log-transformation, the correlation among the three emission targets is more evident. There are [interesting frameworks](https:\/\/www.datacamp.com\/community\/tutorials\/gflasso-R) that induce parameter sharing for genuine multitask regression, and take advantage of such covariance among target variables. For simplicity we will not use multitask regression but rather model each response separately, using `MultiOutputRegressor`.\n\n# Repeated *k*-Fold Cross-Validation \u23f3\n\nIn order to select appropriate hyperparameters for the GBR model, 5x repeated three-fold cross-validation (CV) will be employed first. We will experiment with different values of `learning_rate`, `max_depth` and `subsample`.","8f4e70d1":"# Fit model \ud83e\udde0\n\nNext, we take the optimal hyperparameters to fit the GBR model - in fact three models, as explained above - to the entire training set. These are contained in `crossval.best_params_`."}}