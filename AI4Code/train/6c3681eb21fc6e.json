{"cell_type":{"14d4088e":"code","424a75ce":"code","d07bc4b3":"code","0dba7928":"code","8e2e5837":"code","4c850e1d":"code","e529399a":"code","6982cec3":"code","8c42cc37":"code","3b79e6c4":"code","d259a951":"code","9bc0ca2a":"code","a75a08ef":"code","c5b13ab4":"code","b0d5ea7e":"code","438276e6":"code","22a8702f":"code","18d8edba":"code","68b2fe8a":"code","855add95":"code","b4b5cc43":"code","44b7d840":"code","20887999":"code","e19a9c2c":"code","a59410cb":"code","495ce8cb":"code","3b67c1b4":"code","2cd1ddde":"code","ec035e5f":"code","5ee0c64d":"code","f4845bad":"code","5cc6f9fa":"code","632c76eb":"code","bfb59421":"code","13ab3b7c":"code","2c1639e7":"code","df22d527":"code","8e9f17f2":"code","39a8d350":"code","7a69317f":"code","91970fbc":"code","089172c4":"code","a7d1142f":"code","683509d0":"code","8d69f75b":"code","22ae6d1c":"code","ea61e38d":"code","15e1e165":"code","87404f49":"code","a5e2c325":"code","28f564b0":"markdown","b11b0311":"markdown","54c0ac96":"markdown","447213d9":"markdown","87a6cbd9":"markdown","80c5c8dc":"markdown"},"source":{"14d4088e":"import os\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","424a75ce":"seed_value= 0\n\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\nimport random\nrandom.seed(seed_value)\n\nimport numpy as np\nnp.random.seed(seed_value)\n\nimport tensorflow as tf\ntf.random.set_seed(seed_value)\n","d07bc4b3":"train_feature = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\nX_train = train_feature.drop('sig_id', axis = 1)","0dba7928":"train_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nY_train = train_targets_scored.drop('sig_id', axis = 1)","8e2e5837":"Y_train","4c850e1d":"combine = pd.concat([X_train, Y_train], axis=1, sort=False)\n","e529399a":"combine","6982cec3":"def get_conditional_prob(feature, observation):\n    '''\n    Function for returning a resulting dataframe\n    '''\n    return (combine[[feature, observation]].groupby([feature], as_index=False).\n             mean().\n             sort_values(by=observation, ascending=False))\n\nx_col = X_train.columns\ny_col = Y_train.columns\n\nprint(\"I have an assumsion that if cp_type is ctl_vehicle then y will be all zero\")\nctl_vehicle_is_zero = True\nfor y in y_col:\n    \n    temp = get_conditional_prob(feature=x_col[0], observation=y)\n    if temp[temp['cp_type'] == 'ctl_vehicle'][y][0] > 0:\n        # If y result > 0 was found\n        ctl_vehicle_is_zero = False\n        print(temp)\n    # print(temp[temp['cp_type'] == 'ctl_vehicle'][y][0])\n\nif ctl_vehicle_is_zero:\n    print(\"That's true\")\nelse:\n    print(\"Just Misunderstood\")","8c42cc37":"cp_time_to_MOA = pd.DataFrame()\nimport scipy.stats as stats\n\n\nfor y in y_col:\n    temp = get_conditional_prob(feature=x_col[1], observation=y)\n    temp_2 = temp.T.rename(columns=temp.T.iloc[0]).iloc[1:]\n    cp_time_to_MOA = pd.concat([cp_time_to_MOA, temp_2])\ncp_time_to_MOA","3b79e6c4":"stats.ttest_rel(cp_time_to_MOA[24],cp_time_to_MOA[48])","d259a951":"stats.ttest_rel(cp_time_to_MOA[24],cp_time_to_MOA[72])","9bc0ca2a":"stats.ttest_rel(cp_time_to_MOA[48],cp_time_to_MOA[72])","a75a08ef":"stats.f_oneway(cp_time_to_MOA[24], cp_time_to_MOA[48], cp_time_to_MOA[72])","c5b13ab4":"cp_dose_to_MOA = pd.DataFrame()\nimport scipy.stats as stats\n\n# stats.f_oneway(cp_time_to_MOA[24], cp_time_to_MOA[48], cp_time_to_MOA[72])\nfor y in y_col:\n    temp = get_conditional_prob(feature=x_col[2], observation=y)\n    temp_2 = temp.T.rename(columns=temp.T.iloc[0]).iloc[1:]\n    cp_dose_to_MOA = pd.concat([cp_dose_to_MOA, temp_2])\ncp_dose_to_MOA","b0d5ea7e":"stats.ttest_rel(cp_dose_to_MOA['D1'],cp_dose_to_MOA['D2'])","438276e6":"stats.ttest_rel([10,10],[1,1])","22a8702f":"cp_time_to_MOA[24].values","18d8edba":"X_train[X_train.columns[3:]]","68b2fe8a":"y_col = Y_train.columns[1]\ncombine.loc[Y_train[Y_train[y_col]==1].index]","855add95":"all_zero = True\nfor yc in Y_train.columns:\n    temp = (Y_train[yc] == 0)\n    if type(all_zero) is bool :\n        all_zero = temp\n    else:\n        all_zero = all_zero & temp\n        ","b4b5cc43":"X_train_wraggled = X_train[X_train['cp_type'] != 'ctl_vehicle'][X_train.columns[3:]]","44b7d840":"Y_train_wraggled = Y_train.loc[X_train_wraggled.index]","20887999":"X_train_wraggled = X_train_wraggled.loc[Y_train_wraggled.index]","e19a9c2c":"X_train_wraggled_g = X_train_wraggled[X_train_wraggled.columns[pd.Series(X_train_wraggled.columns).str.startswith('g')]] \n\nX_train_wraggled_c = X_train_wraggled[X_train_wraggled.columns[pd.Series(X_train_wraggled.columns).str.startswith('c')]] ","a59410cb":"print(X_train_wraggled.shape[1])\nprint(X_train_wraggled_g.shape[1])\nprint(X_train_wraggled_c.shape[1])","495ce8cb":"Y_train_wraggled[Y_train_wraggled.sum(axis=1)== 1]","3b67c1b4":"os.listdir('..\/input\/moa-lstm')","2cd1ddde":"X_train_wraggled_c","ec035e5f":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n              \ndef calculating_class_weights(y_true):\n    from sklearn.utils.class_weight import compute_class_weight\n    number_dim = np.shape(y_true)[1]\n    weights = np.empty([number_dim, 2])\n    for i in range(number_dim):\n        weights[i] = compute_class_weight('balanced', [0,1], y_true.iloc[:, i])\n    return weights\n\ndef get_weighted_loss(weights):\n    def weighted_loss(y_true, y_pred):\n        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n    return weighted_loss\n\nimport tensorflow as tf\n\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)\n\nclass_weights = calculating_class_weights(Y_train_wraggled)","5ee0c64d":"class_weights_2 = np.copy(class_weights)\nclass_weights[:,0] = 1\nclass_weights[:,1] = 1.5","f4845bad":"class_weights","5cc6f9fa":"# Fully connect\nimport keras\nfrom keras import backend as K\nfrom numpy import loadtxt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.layers import Input, Concatenate, concatenate, BatchNormalization\nfrom keras.models import Model\nfrom keras.layers import Dense, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import LSTM, Reshape\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.callbacks import TensorBoard\nfrom tensorflow_addons.layers import WeightNormalization\nfrom functools import partial\nimport tensorflow as tf\n\nmodel_name = \"conv\"\nmodel_name = \"parallel_conv\"\nmodel_name = \"LSTM\"\n\nSPLIT_RATIO = 0.75\n# model_name = 'g_conv'\n# model_name = 'c_lstm'\n#Load pretrained model\nfrom keras.models import load_model\n\n\n\n#Create model function\ndef getModel(model_name):\n    if model_name == 'pretrained':\n        model = load_model('..\/input\/moa-lstm\/best_weights (1).hdf5')\n        toBeCompiled = False\n    elif model_name == 'conv':\n        InputLayer = Input(shape=(X_train_wraggled.shape[1], 1))\n        ConvLayer = Conv1D(filters=20,\n                           kernel_size=10,\n                           padding='valid',\n                           activation='relu',\n                           strides=1)(InputLayer)\n        PoolingLayer = GlobalMaxPooling1D()(ConvLayer)\n        OutputLayer = Dense(206, activation='sigmoid')(PoolingLayer)\n        model = Model(inputs=InputLayer, outputs=OutputLayer)\n        toBeCompiled = True\n        \n    elif model_name == 'parallel_conv':\n        InputLayer_g = Input(shape=(X_train_wraggled_g.shape[1], 1))\n        InputLayer_c = Input(shape=(X_train_wraggled_c.shape[1], 1))\n        \n        \n        ConvLayer_g = Conv1D(filters=1200,\n                           kernel_size=50,\n                           padding='valid',\n                           activation='relu',\n                           strides=1)(InputLayer_g)\n        PoolingLayer_g = GlobalMaxPooling1D()(ConvLayer_g)\n        PoolingLayer_g = BatchNormalization()(PoolingLayer_g)\n        PoolingLayer_g = Dropout(0.4)(PoolingLayer_g)\n        \n        ConvLayer_c = Conv1D(filters=1200,\n                           kernel_size=50,\n                           padding='valid',\n                           activation='relu',\n                           strides=1)(InputLayer_c)\n        PoolingLayer_c = GlobalMaxPooling1D()(ConvLayer_c)\n        PoolingLayer_c = BatchNormalization()(PoolingLayer_c)\n        PoolingLayer_c = Dropout(0.4)(PoolingLayer_c)\n        \n        merged = concatenate([PoolingLayer_g, PoolingLayer_c], axis=1)\n        merged = BatchNormalization()(merged)\n        merged = Reshape((2400,1))(merged)\n        \n        merged = WeightNormalization(LSTM(2000))(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='relu'))(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        OutputLayer = WeightNormalization(Dense(206, activation='sigmoid'))(merged)\n        model = Model(inputs=[InputLayer_g,InputLayer_c], outputs=OutputLayer)\n        toBeCompiled = True\n        \n    elif model_name == 'LSTM':\n        InputLayer_g = Input(shape=(X_train_wraggled_g.shape[1], 1))\n        InputLayer_c = Input(shape=(X_train_wraggled_c.shape[1], 1))\n        \n        LSTM_g = LSTM(X_train_wraggled_g.shape[1])(InputLayer_g)\n        LSTM_g = BatchNormalization()(LSTM_g)\n        LSTM_c = LSTM(X_train_wraggled_c.shape[1])(InputLayer_c)\n        LSTM_c = BatchNormalization()(LSTM_c)\n        merged = concatenate([LSTM_g, LSTM_c], axis=1)\n        merged = BatchNormalization()(merged)\n#         merged = Reshape((400,1))(merged)\n        \n#         merged = WeightNormalization(LSTM(400, dropout = 0.4))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.2)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n        merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.4)(merged)\n        \n        OutputLayer = WeightNormalization(Dense(206, activation='sigmoid'))(merged)\n        model = Model(inputs=[InputLayer_g,InputLayer_c], outputs=OutputLayer)\n        toBeCompiled = True\n    return toBeCompiled,model\n\ntoBeCompiled,model = getModel(model_name)\nMETRICS = [\n        'accuracy',\n        \"binary_crossentropy\",\n        f1_m,\n        keras.metrics.TruePositives(name='tp'),\n        keras.metrics.FalsePositives(name='fp'),\n        keras.metrics.TrueNegatives(name='tn'),\n        keras.metrics.FalseNegatives(name='fn'), \n        keras.metrics.Precision(name='precision'),\n        keras.metrics.Recall(name='recall'),\n        keras.metrics.AUC(name='auc'),\n    ]\nif toBeCompiled:\n    model.compile(loss=get_weighted_loss(class_weights),\n                  #loss=\"binary_crossentropy\",\n                  optimizer='adam',\n                  metrics=METRICS)\n\n\n\nif model_name in ['parallel_conv','LSTM']:\n    #Set Checkpoint\n    filepath=\"best_weights.hdf5\"\n    Monitor = 'val_binary_crossentropy'\n    #Monitor = 'binary_crossentropy'\n    checkpoint = ModelCheckpoint(filepath, monitor=Monitor, verbose=1, save_best_only=True, mode='min')\n    early_stop =EarlyStopping(monitor=Monitor, mode = 'min', patience=30)\n    \n    validation = (\n                      [\n                        X_train_wraggled_g.iloc[int(len(X_train_wraggled_g)*SPLIT_RATIO):].values.astype('float32'),\n                        X_train_wraggled_c.iloc[int(len(X_train_wraggled_c)*SPLIT_RATIO):].values.astype('float32')\n                    ],\n                      Y_train_wraggled.iloc[int(len(X_train_wraggled)*SPLIT_RATIO):].values.astype('float32')\n                  )\n    # validation = None\n    \n    model.summary()\n    model.fit([\n                X_train_wraggled_g.iloc[0:int(len(X_train_wraggled_g)*SPLIT_RATIO)].values.astype('float32'),\n                X_train_wraggled_c.iloc[0:int(len(X_train_wraggled_c)*SPLIT_RATIO)].values.astype('float32')\n                ],\n              Y_train_wraggled.iloc[0:int(len(X_train_wraggled)*SPLIT_RATIO)].values.astype('float32'),\n              epochs=600,\n        \n              validation_data= validation,\n              # batch_size=200,\n                shuffle = True,\n              callbacks = [checkpoint,early_stop]\n             )\n    \n","632c76eb":"Y_train_wraggled.str.str","bfb59421":"# Y_predicts = (best_model.predict([X_train_wraggled_g, X_train_wraggled_c]))","13ab3b7c":"# Y_predicts_df = pd.DataFrame(Y_predicts)\n# Y_predicts_df.columns = Y_train_wraggled.columns\n# Y_predicts_df","2c1639e7":"# Y_predicts_df[Y_train_wraggled[]]","df22d527":"# Y_train_wraggled.index = Y_predicts_df.index","8e9f17f2":"# Y_predicts_df[(Y_train_wraggled['5-alpha_reductase_inhibitor']==0) & (Y_predicts_df['5-alpha_reductase_inhibitor'] > 0.005)].iloc[0].idxmax()","39a8d350":"# Y_train_wraggled.loc[1024].idxmax()","7a69317f":"# min(Y_predicts_df[(Y_train_wraggled['5-alpha_reductase_inhibitor']==1)]['5-alpha_reductase_inhibitor'])","91970fbc":"test_feature = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n","089172c4":"from keras.models import load_model\ntry:\n    best_model = load_model('best_weights.hdf5', custom_objects={'weighted_loss': get_weighted_loss(class_weights), 'f1_m':f1_m})\nexcept OSError:\n    best_model = model","a7d1142f":"default_result = {}\nfor y_c in Y_train_wraggled.columns:\n    default_result[y_c] = 0","683509d0":"# predicts = []\n\nX_test = test_feature\n\nX_test_wraggled = X_test[X_test.columns[4:]]\n\nX_test_wraggled_g = X_test_wraggled[X_test_wraggled.columns[pd.Series(X_test_wraggled.columns).str.startswith('g')]] \n\nX_test_wraggled_c = X_test_wraggled[X_test_wraggled.columns[pd.Series(X_test_wraggled.columns).str.startswith('c')]] \n\nif model_name in ['conv']:\n    result = model.predict(X_test_wraggled.values)\nelif model_name in ['parallel_conv', 'LSTM']:\n    result = model.predict([X_test_wraggled_g.values, X_test_wraggled_c.values])\nelif model_name in ['g_conv']:\n    result = model.predict([X_test_wraggled_g.values])\nelif model_name in ['c_lstm']:\n    result = model.predict([X_test_wraggled_c.values])","8d69f75b":"predicts_df = pd.DataFrame(result)","22ae6d1c":"predicts_df[test_feature['cp_type'] == \"ctl_vehicle\"] = 0","ea61e38d":"predicts_df.columns = Y_train_wraggled.columns","15e1e165":"predicts_df['sig_id']=test_feature['sig_id']\n","87404f49":"predicts_df = predicts_df[['sig_id']+list(Y_train_wraggled.columns)]\npredicts_df","a5e2c325":"predicts = pd.DataFrame(predicts_df)\npredicts.to_csv('submission.csv', index=False)\npredicts","28f564b0":"# Import module","b11b0311":"# Next step","54c0ac96":"***Then, cp_time is very siginificant***","447213d9":"# Post processing","87a6cbd9":"## T-testing for cp_dose","80c5c8dc":"## Check significant of cp_time"}}