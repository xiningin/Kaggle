{"cell_type":{"1b497705":"code","a87dc5ac":"code","91d5f417":"code","813c0462":"code","870b8568":"code","c85de2ae":"code","be0e78f9":"code","a971c56e":"code","75d6d1c6":"code","0edbac96":"code","a25c781e":"code","8d48fc60":"code","a1537071":"code","d057be21":"code","dc800bff":"code","acf3bcff":"code","e489a76f":"code","cbc9f23c":"code","ebbde4bd":"code","37f4299a":"code","081d9784":"code","47a9d7c7":"code","51e6c937":"code","6c260653":"code","cf9ac84f":"code","e44f76d8":"code","00b39ddd":"code","bbfae304":"code","601911ea":"code","697631c6":"code","3f29943e":"code","9339fd32":"code","3ae792ad":"code","e9d78bd5":"code","fae02cdc":"code","57bfde15":"code","9d13c951":"code","4c363865":"code","ede29ace":"code","74fced0b":"code","95e2bdd0":"code","adca08fd":"code","9f7d6dff":"code","6d46536d":"markdown","92bf63de":"markdown","5ed9fac0":"markdown","7574e9f3":"markdown","86182ff3":"markdown","8d33badc":"markdown"},"source":{"1b497705":"from glob import glob","a87dc5ac":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom skimage.io import imread\nfrom skimage.color import rgb2grey\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import KMeans\n\n#https:\/\/medium.com\/spinor\/a-straightforward-introduction-to-image-thresholding-using-python-f1c085f02d5e\n#https:\/\/datacarpentry.org\/image-processing\/07-thresholding\/\n#https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_imgproc\/py_thresholding\/py_thresholding.html\n#https:\/\/campus.datacamp.com\/courses\/introduction-to-data-visualization-in-python\/analyzing-time-series-and-images?ex=13\n#https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_imgproc\/py_morphological_ops\/py_morphological_ops.html\n\nfrom skimage.filters import rank, threshold_otsu\nfrom skimage.morphology import closing, square, disk\nfrom skimage import exposure as hist, data, img_as_float\nfrom skimage.segmentation import chan_vese\nfrom skimage.feature import canny\nfrom skimage.color import rgb2gray\nfrom scipy import ndimage as ndi ","91d5f417":"mal_images = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant\/*')[:5]\nben_images = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/benign\/*')[:5]","813c0462":"len(mal_images)","870b8568":"def binary(image):\n    return image > threshold_otsu(image)\n\ndef equalize(image):\n    return hist.equalize_hist(image)\n\n#https:\/\/homepages.inf.ed.ac.uk\/rbf\/HIPR2\/median.htm\ndef mean_filter(image, raio_disk):\n    return rank.mean_percentile(image, selem = disk(raio_disk))\n\ndef preenche_bords(image):\n    return ndi.binary_fill_holes(image)\n\n#https:\/\/www.unioviedo.es\/compnum\/labs\/PYTHON\/intro_image.html\n\ndef load_images(paths):\n    tmp = []\n    for path in paths:\n        tmp.append(imread(path))\n    return tmp\n    \ndef plot_any(arr, title = ''):\n    plt.figure(figsize = (15, 25))\n    for i in range(len(arr)):\n        plt.subplot(1,len(arr),i + 1)\n        plt.title(title)\n        plt.imshow(arr[i]);\n\n        \ndef plot_camadas(img):\n    plt.figure(figsize = (15, 25))\n    for i in range(3):\n        plt.subplot(1, 3, i + 1)\n        plt.imshow(img[:,:,i], cmap = 'gray');\n        \ndef d2Kmeans(img, k):\n    return KMeans(n_jobs=-1, \n                  random_state=1, \n                  n_clusters = k, \n                  init='k-means++'\n    ).fit(img.reshape((-1,1))).labels_.reshape(img.shape)\n\ndef merge_segmented_mask_ROI(uri_img, img_kluster):\n    new_img = uri_img.copy()\n    for ch in range(3):\n        new_img[:,:, ch] *= img_kluster\n    return new_img\n\n\ndef elbow(img, k):\n    hist = []\n    for kclusters in  range(1, k):\n        Km = KMeans(n_jobs=-1, random_state=1, n_clusters = kclusters, init='k-means++').fit(img.reshape((-1,1)))  \n        hist.append(Km.inertia_)\n        \n    plt.figure(figsize = (15, 8))\n    plt.grid()\n    plt.plot(range(1, k), hist, 'o-')\n    plt.ylabel('Soma das dist\u00e2ncias quadradas')\n    plt.xlabel('k clusters')\n    plt.title('Elbow')\n    plt.show();\n    \n    ","c85de2ae":"mal = load_images(mal_images)\nben = load_images(ben_images)","be0e78f9":"plot_any(mal, 'Maligna')\nplot_any(ben, 'Benigma')\n","a971c56e":"img_selected = mal[1]","75d6d1c6":"elbow(img_selected, 6)","0edbac96":"k_klusters = 2","a25c781e":"result_gray = d2Kmeans(rgb2grey(img_selected), k_klusters)\nresult_img = d2Kmeans(img_selected, k_klusters)","8d48fc60":"klusters_gray = [result_gray == i for i in range(k_klusters)]\nplot_any(klusters_gray)","a1537071":"def select_cluster_index(clusters):\n    minx = clusters[0].mean()\n    index = 0\n    for i in clusters:\n        if i.mean() < minx:\n            minx = i.mean()\n            index += 1\n    return index","d057be21":"index_kluster = select_cluster_index(klusters_gray)\nprint(index_kluster)\nselecionado = klusters_gray[index_kluster]","dc800bff":" for ch in range(3):\n    img_k = []\n    for K in range(k_klusters):\n         img_k.append(result_img[:, :, ch] == K)\n    plot_any(img_k)","acf3bcff":"clusters = [(result_img[:,:,1] == K) for K in range(k_klusters)]","e489a76f":"clusters","cbc9f23c":"new_img = merge_segmented_mask_ROI(img_selected, selecionado)","ebbde4bd":"plot_any([new_img])","37f4299a":"image_mean_filter = mean_filter(selecionado, 20)\ntest_binary = binary(image_mean_filter)","081d9784":"plot_any([selecionado, image_mean_filter, test_binary])","47a9d7c7":"final_result = merge_segmented_mask_ROI(img_selected ,test_binary)","51e6c937":"plot_any([test_binary, new_img, final_result])","6c260653":" max_mean = 0\nimg_gray = rgb2gray(final_result)\n img_bin  = binary(img_gray)\nx, y = img_bin.shape\n\n limits_before = []\nfor i in range(x):\n    for j in range(y):\n        if  img_bin[i, j]:\n            limits_before.append(j)\n            \nstop_before = ( sum(limits_before) \/\/ len(limits_before) ) \/\/ 2\nimg_copy = img_bin.copy()\nfor i in range(x):\n    for j in range(stop_before):\n        img_copy[i, j] = 0\n        limits_after = []\nfor i in range(x):\n     for j in range(y - 1, 0, -1):\n        if  img_copy[i, j]:\n            limits_after.append(j)\n            \nstop_after = sum(limits_after) \/\/ len(limits_after) + min(limits_after)\nfor i in range(x):\n    for j in range(stop_after, y):\n        img_copy[i, j] = 0\n\nmean_result = mean_filter(img_copy, 15)\nmean_result = binary(mean_result)\nfinal_result = merge_segmented_mask_ROI(img_selected , mean_result)\n\n\n plot_any([mean_result, final_result])","cf9ac84f":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport cv2\nimg = cv2.imread(mal_images[1]) \nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nr, g, b = cv2.split(img)\nr = r.flatten()\ng = g.flatten()\nb = b.flatten()\n#plotting \nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(r, g, b)\nplt.show()","e44f76d8":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt","00b39ddd":"original_image = cv2.imread(mal_images[1])\noriginal_image\n","bbfae304":"img=cv2.cvtColor(original_image,cv2.COLOR_BGR2RGB)","601911ea":"vectorized = img.reshape((-1,3))","697631c6":"vectorized = np.float32(vectorized)","3f29943e":"criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)","9339fd32":"K = 2\nattempts=10\nret,label,center=cv2.kmeans(vectorized,K,None,criteria,attempts,cv2.KMEANS_PP_CENTERS)","3ae792ad":"center = np.uint8(center)","e9d78bd5":"res = center[label.flatten()]\nresult_image = res.reshape((img.shape))","fae02cdc":"figure_size = 15\nplt.figure(figsize=(figure_size,figure_size))\nplt.subplot(1,2,1),plt.imshow(img)\nplt.title('Original Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(1,2,2),plt.imshow(res)\nplt.title('Segmented Image when K = %i' % K), plt.xticks([]), plt.yticks([])\nplt.show()","57bfde15":"import sys\nimport sklearn\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n","9d13c951":"import numpy as np\nimport cv2\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MiniBatchKMeans\n\nimage = cv2.imread(mal_images[2])\n(h1, w1) = image.shape[:2]\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n\nimage = image.reshape((image.shape[0] * image.shape[1], 3))\n\nclt = KMeans(n_clusters = 2)\n\nlabels = clt.fit_predict(image)\nquant = clt.cluster_centers_.astype(\"uint8\")[labels]\n\n#reshape the feature vectors to images\nquant = quant.reshape((h1, w1, 3))\nimage = image.reshape((h1, w1, 3))\n\n# convert from L*a*b* to RGB\nquant = cv2.cvtColor(quant, cv2.COLOR_LAB2BGR)\nimage = cv2.cvtColor(image, cv2.COLOR_LAB2BGR)\n\n\n\nplt.imshow(quant)\n\n\n","4c363865":"plt.imshow(image)","ede29ace":"mal_images = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant\/*')\nben_images = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/benign\/*')","74fced0b":"len(mal_images)","95e2bdd0":"#Reading Image\nimport pandas as pd\ntrain_data =[]\nfor img in  mal_images :\n    train_data.append((img,0)) \n    \nfor img in ben_images : \n    train_data.append((img,1)) \n\n    \ntrain_data = pd.DataFrame(train_data, columns = ['image','label'] , index = None)\ntrain_data['image'][0] \ntrain_data","adca08fd":"mal_images = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant\/*')\n\n","9f7d6dff":"#https:\/\/iq.opengenus.org\/basics-of-machine-learning-image-classification-techniques\/\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant'))\n\n# Any results you write to the current directory are saved as output.\n\n#import os\nimport cv2\nW=256\npath = r\"..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant\"\nfor filename in os.listdir(path):\n    if filename.endswith('.jpg'):\n        print(filename)\n        oriimg = cv2.imread(path+'\/'+filename)\n        height, width, depth = oriimg.shape\n        imgScale = W\/width\n        newX,newY = oriimg.shape[1]*imgScale, oriimg.shape[0]*imgScale\n        newimg = cv2.resize(oriimg,(int(newX),int(newY)))\n        newimg = cv2.cvtColor(oriimg,cv2.COLOR_BGR2GRAY)\n        cv2.imwrite(str(path+'\/'+filename+\"-resized\"),newimg)        \n        print('Image saved')","6d46536d":"1. K Means Clustering for Imagery Analysis\n\nwe will use a K-means algorithm to perform image classification. Clustering isn\u2019t limited to the consumer information and population sciences, it can be used for imagery analysis as well. Leveraging Scikit-learn and the MNIST dataset, we will investigate the use of K-means clustering for computer vision.","92bf63de":"Image segmentation ","5ed9fac0":"# 1. RUN THE CODE IN GOOGLE COLAB ! ","7574e9f3":"Even though the glob API is very simple, the module packs a lot of power. It is useful in any situation where your program needs to look for a list of files on the filesystem with names matching a pattern. If you need a list of filenames that all have a certain extension, prefix, or any common string in the middle, use glob instead of writing code to scan the directory contents yourself.\n\nThe pattern rules for glob are not regular expressions. Instead, they follow standard Unix path expansion rules. There are only a few special characters: two different wild-cards, and character ranges are supported. The patterns rules are applied to segments of the filename (stopping at the path separator, \/). Paths in the pattern can be relative or absolute. Shell variable names and tilde (~) are not expanded.\n\nExample Data\u00b6\n\nImage segmentation is the process of partitioning an image into multiple different regions (or segments). The goal is to change the representation of the image into an easier and more meaningful image.\n\nIt is an important step in image processing, as real world images doesn't always contain only one object that we wanna classify. For instance, for self driving cars, the image would contain the road, cars, pedestrians, etc. So we may need to use segmentation here to separate objects and analyze each object individually (i.e image classification) to check what it is.\n\nK-Means clustering is unsupervised machine learning algorithm that aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean. A cluster refers to a collection of data points aggregated together because of certain similarities. For image segmentation, clusters here are different image colors.\n\n\nImage segmentation is an essential topic in an image processing framework. It is the process to classify an image into different groups. There are many different methods, and k-means is one of the most popular methods.","86182ff3":"ML ","8d33badc":"Structure of an Image Classification Task\nImage Preprocessing - The aim of this process is to improve the image data(features) by suppressing unwanted distortions and enhancement of some important image features so that our Computer Vision models can benefit from this improved data to work on.\nDetection of an object - Detection refers to the localization of an object which means the segmentation of the image and identifying the position of the object of interest.\nFeature extraction and Training- This is a crucial step wherein statistical or deep learning methods are used to identify the most interesting patterns of the image, features that might be unique to a particular class and that will, later on, help the model to differentiate between different classes. This process where the model learns the features from the dataset is called model training.\nClassification of the object - This step categorizes detected objects into predefined classes by using a suitable classification technique that compares the image patterns with the target patterns."}}