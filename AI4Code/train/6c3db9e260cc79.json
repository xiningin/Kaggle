{"cell_type":{"6865f5d9":"code","6d7bd708":"code","f97c8dd3":"code","860a10e9":"code","dc9dc6f8":"code","619ea750":"code","fa69c659":"code","e915daad":"code","6cd386df":"code","9a02921d":"code","cbdea7b5":"code","dcc397a8":"code","f008406a":"code","682fa355":"code","faeeed7c":"code","644bda03":"code","11db8538":"code","171f693e":"code","309a8631":"code","21708259":"code","1f488fc5":"code","c1d887c4":"code","6e457bf5":"code","1f903dc3":"code","3c6b8be7":"code","1433f8e1":"code","9284c0b6":"code","8246df44":"code","4140abf1":"code","1b341109":"code","96098d62":"code","899a3124":"code","b760bdd7":"code","689a1dc0":"code","d1e4b98c":"code","b5a9e093":"code","88366334":"code","f66762c7":"code","b10d0c51":"code","255699ac":"markdown","44844069":"markdown","52a16c7b":"markdown","51bf1ca6":"markdown","411dcad7":"markdown","9e13b8a2":"markdown","41d206a4":"markdown","79f685aa":"markdown","220c2c21":"markdown","f152e9e5":"markdown","3145c7ff":"markdown","8d76b17a":"markdown","c13f16fd":"markdown","f66a3c31":"markdown","c4ec6b8b":"markdown","459c2f9d":"markdown","a2a7a2fa":"markdown","b8d1755e":"markdown","5ec82854":"markdown","75fc91e3":"markdown","147e092f":"markdown","9e52c683":"markdown","6f114136":"markdown","588e7819":"markdown","7c1852f3":"markdown","5263bf7b":"markdown","416075e8":"markdown","ac0d7e98":"markdown","ace5114e":"markdown","a1ed7408":"markdown","335bda41":"markdown","75c1eeee":"markdown","7c4e4cba":"markdown","a3b5a837":"markdown","adca4972":"markdown","3c0a019e":"markdown"},"source":{"6865f5d9":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# create test set\nfrom sklearn.model_selection import train_test_split\n\n# to keep track of training time\nimport datetime\n\n# standard scaler\nfrom sklearn.preprocessing import StandardScaler\n\n# logistic regression model\nfrom sklearn.linear_model import LogisticRegression\n\n# metrics used for evaluation\nfrom sklearn.metrics import f1_score, matthews_corrcoef\n\n# visualizations\nfrom yellowbrick.classifier import ClassPredictionError, ConfusionMatrix\nfrom sklearn.metrics import plot_roc_curve\n\n# KNN imputation\nfrom sklearn.impute import KNNImputer\n\n# normalizer\nfrom sklearn.preprocessing import Normalizer\n\n# variance threshold\nfrom sklearn.feature_selection import VarianceThreshold\n\n# RFECV\nfrom sklearn.model_selection import StratifiedKFold\nfrom yellowbrick.model_selection import RFECV\n\n# random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# evaluation metric\nfrom sklearn.metrics import matthews_corrcoef, make_scorer","6d7bd708":"uci_secom = pd.read_csv('..\/input\/uci-semcom\/uci-secom.csv')\nprint(f'Whoa! There are {uci_secom.shape[0]} records with {uci_secom.shape[1]} features!')\nn_features0 = uci_secom.shape[1]","f97c8dd3":"uni_target = uci_secom[['Pass\/Fail']]\nuni_data = uci_secom.drop(['Pass\/Fail'], axis=1)","860a10e9":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('xxgp1Ehbuh4')","dc9dc6f8":"X_train, X_test, y_train, y_test = train_test_split(uni_data, uni_target, test_size=0.25, random_state=42, stratify=uni_target)","619ea750":"# convert to pandas dataframe\nX_train = pd.DataFrame(X_train, columns=uni_data.columns)\nX_test = pd.DataFrame(X_test, columns=uni_data.columns)\ny_train = pd.DataFrame(y_train, columns=uni_target.columns)\ny_test = pd.DataFrame(y_test, columns=uni_target.columns)","fa69c659":"y_train = y_train.replace(to_replace=[-1, 1], value=[1, 0])\ny_test = y_test.replace(to_replace=[-1, 1], value=[1, 0])","e915daad":"type_dct = {str(k): len(list(v)) for k, v in X_train.groupby(X_train.dtypes, axis=1)}\ntype_dct","6cd386df":"X_train.select_dtypes(include=['object'])","9a02921d":"X_train_notime = X_train.drop(['Time'], axis=1)\nX_test_notime = X_test.drop(['Time'], axis=1)","cbdea7b5":"np.count_nonzero(X_train_notime.isna().sum())","dcc397a8":"X_train_notime.isna().sum().sort_values(ascending=False).head(10)","f008406a":"def evaluate(train_df, test_df, train_target, test_target):\n    \n    # first, we should scale our data so logistic regression works better\n    scaler = StandardScaler()\n    scaler.fit(train_df)\n    train_std = pd.DataFrame(scaler.transform(train_df), columns=train_df.columns)\n    test_std = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n    \n    # training the model\n    logreg = LogisticRegression(random_state = 42, class_weight='balanced', C=200, dual=False, solver='liblinear')\n    start_time = datetime.datetime.now()\n    logreg.fit(train_std, train_target.values.ravel())\n    elapsed = datetime.datetime.now() - start_time\n    time = int(elapsed.total_seconds()*1000)\n    \n    # evaluation and scoring\n    y_pred = logreg.predict(test_std)\n    y_true = test_target.values.ravel()\n    f1score = f1_score(y_true, y_pred, average='micro')\n    mccscore=matthews_corrcoef(y_true, y_pred)\n    \n    # visualizations\n    cre = ClassPredictionError(logreg, classes=['fail', 'pass'])\n    cre.score(test_std, y_true)\n    cre.show()\n    cm = ConfusionMatrix(logreg, classes=['fail', 'pass'])\n    cm.score(test_std, y_true)\n    cm.show()\n    rocauc = plot_roc_curve(logreg, test_std, y_true)\n    plt.show()\n    \n    return time, f1score, mccscore","682fa355":"# impute missing values and save it as a temporary dataset.\nimputer = KNNImputer()\nimputer.fit(X_train_notime)\nimputed_train = pd.DataFrame(imputer.transform(X_train_notime), columns = X_train_notime.columns)\nimputed_test = pd.DataFrame(imputer.transform(X_test_notime), columns = X_test_notime.columns)\n\n# lists to record time and scores\nf1scores = []\nmccscores = []\ntimes = []\n\ntime, f1score, mccscore = evaluate(train_df = imputed_train, test_df = imputed_test, train_target=y_train, test_target=y_test)\nprint(f' Training time: {time}ms\\n F1 Score: {f1score}\\n MCC Score: {mccscore}')\nf1scores.append(f1score)\nmccscores.append(mccscore)\ntimes.append(time)","faeeed7c":"def percentna(dataframe, threshold):\n    columns = dataframe.columns[(dataframe.isna().sum()\/dataframe.shape[1])>threshold]\n    return columns.tolist()","644bda03":"na_columns = percentna(X_train, 0.5)\nX_train_nona = X_train_notime.drop(na_columns, axis=1)\nX_test_nona = X_test_notime.drop(na_columns, axis=1)\nn_features1 = X_train_nona.shape[1]\nprint(f'After removing {len(na_columns)} features, there are {n_features1} features left.')","11db8538":"# install the package\n! pip install missingno \nimport missingno as msno\nmsno.matrix(X_train_notime[na_columns])","171f693e":"imputer = KNNImputer()\nimputer.fit(X_train_nona)","309a8631":"X_train_imp = pd.DataFrame(imputer.transform(X_train_nona), columns = X_train_nona.columns)\nX_test_imp = pd.DataFrame(imputer.transform(X_test_nona), columns = X_test_nona.columns)","21708259":"time, f1score, mccscore = evaluate(train_df = X_train_imp, test_df = X_test_imp, train_target=y_train, test_target=y_test)\nprint(f' Training time: {time}ms\\n F1 Score: {f1score}\\n MCC Score: {mccscore}')\nf1scores.append(f1score)\nmccscores.append(mccscore)\ntimes.append(time)","1f488fc5":"normalizer = Normalizer()\nnormalizer.fit(X_train_imp)","c1d887c4":"X_train_nrm = pd.DataFrame(normalizer.transform(X_train_imp), columns = X_train_imp.columns)\nX_test_nrm = pd.DataFrame(normalizer.transform(X_test_imp), columns = X_test_imp.columns)","6e457bf5":"selector = VarianceThreshold()\nselector.fit(X_train_nrm)","1f903dc3":"mask = selector.get_support()\ncolumns = X_train_nrm.columns\nselected_cols = columns[mask]\nn_features2 = len(selected_cols)\nprint(f'remaining features: {n_features2}')","3c6b8be7":"X_train_var = pd.DataFrame(selector.transform(X_train_imp), columns = selected_cols)\nX_test_var = pd.DataFrame(selector.transform(X_test_imp), columns = selected_cols)","1433f8e1":"time, f1score, mccscore = evaluate(train_df = X_train_var, test_df = X_test_var, train_target=y_train, test_target=y_test)\nprint(f' Training time: {time}ms\\n F1 Score: {f1score}\\n MCC Score: {mccscore}')\nf1scores.append(f1score)\nmccscores.append(mccscore)\ntimes.append(time)","9284c0b6":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","8246df44":"corr_features = correlation(X_train_var, 0.95)\nX_train_corr = X_train_var.drop(corr_features, axis=1)\nX_test_corr = X_test_var.drop(corr_features, axis=1)\nn_features3 = X_train_corr.shape[1]\nprint(f'After removing {len(corr_features)} features, there are {n_features3} features left.')","4140abf1":"time, f1score, mccscore = evaluate(train_df = X_train_corr, test_df = X_test_corr, train_target=y_train, test_target=y_test)\nprint(f' Training time: {time}ms\\n F1 Score: {f1score}\\n MCC Score: {mccscore}')\nf1scores.append(f1score)\nmccscores.append(mccscore)\ntimes.append(time)","1b341109":"def corrwith_target(dataframe, target, threshold):\n    cor = dataframe.corr()\n    #Correlation with output variable\n    cor_target = abs(cor[target])\n    #Selecting non correlated features\n    relevant_features = cor_target[cor_target<threshold]\n    return relevant_features.index.tolist()[:-1]","96098d62":"# in order to find the correlation with target, I have to add target as a column to X_train_corr\ndummy_train = X_train_corr.copy()\ndummy_train['target'] = y_train","899a3124":"corrwith_cols = corrwith_target(dummy_train, 'target', 0.05)\nX_train_corw = X_train_corr.drop(corrwith_cols, axis=1)\nX_test_corw = X_test_corr.drop(corrwith_cols, axis=1)\nn_features4 = X_train_corw.shape[1]\nprint(f'After removing {len(corrwith_cols)} features, there are {n_features4} features left.')","b760bdd7":"time, f1score, mccscore = evaluate(train_df = X_train_corw, test_df = X_test_corw, train_target=y_train, test_target=y_test)\nprint(f' Training time: {time}ms\\n F1 Score: {f1score}\\n MCC Score: {mccscore}')\nf1scores.append(f1score)\nmccscores.append(mccscore)\ntimes.append(time)","689a1dc0":"scaler = StandardScaler()\nscaler.fit(X_train_corw)","d1e4b98c":"X_train_std = pd.DataFrame(scaler.transform(X_train_corw), columns=X_train_corw.columns)\nX_test_std = pd.DataFrame(scaler.transform(X_test_corw), columns=X_test_corw.columns)","b5a9e093":"mcc_scorer = make_scorer(matthews_corrcoef)\nrfecv = RFECV(estimator=LogisticRegression(random_state = 42, class_weight='balanced', C=200, dual=False, solver='liblinear'),\n              cv=StratifiedKFold(2),\n              scoring =  mcc_scorer)\nrfecv.fit(X_train_std, y_train.values.ravel())\nrfecv.show() ","88366334":"mask = rfecv.get_support()\ncolumns = X_train_corw.columns\nselected_cols = columns[mask]\nn_features5 = len(selected_cols)\nX_train_rfe = pd.DataFrame(rfecv.transform(X_train_corw), columns = selected_cols)\nX_test_rfe = pd.DataFrame(rfecv.transform(X_test_corw), columns = selected_cols)","f66762c7":"time, f1score, mccscore = evaluate(train_df = X_train_rfe, test_df = X_test_rfe, train_target=y_train, test_target=y_test)\nprint(f' Training time: {time}ms\\n F1 Score: {f1score}\\n MCC Score: {mccscore}')\nf1scores.append(f1score)\nmccscores.append(mccscore)\ntimes.append(time)","b10d0c51":"fig, (ax0, ax1) = plt.subplots(2, 1)\nax0.plot(times, label='Training time')\nax0.set(ylabel='Training Time (ms)')\nax1.plot(f1scores, label='F1 Score', c='r')\nax1.plot(mccscores, label='MCC Score', c='darkgreen')\nax1.set(ylabel='Score')\nax1.set(xlabel='Feature selection step')\nax1.legend()\nax0.legend()\nfig.show()","255699ac":"Let's see how many columns it will keep for future use:","44844069":"# How did we do?\nAlright, let's see what did work and what didn't.\n\nthe following plot shows our models' scores compared with training time. ","52a16c7b":"again, we evaluate performance of logisitc regression with our recent changes in train and test data:","51bf1ca6":"# Evaluation model and metrics\nWe are going to remove some columns in every step, which leads to loss of information. so, it's important to see how our models performs so that we can get a sense of whether we are going in the right direction or not. \n\nI'm going to use a simple logistic regressor as my classifier. \n\nAs I said earlier, **since this dataset is highly imbalanced, we must not use accuracy as our evaluation metric.** \n\n![DO NOT USE ACCURACY ON IMBALANCED DATASET](https:\/\/i.imgflip.com\/5md78a.jpg)\n\nInstead, I'm using these two metrics:\n1. **F1 Score:** F1 is a suitable measure of models tested with imbalance datasets.\n2. **Matthews Correlation Coefficient (MCC):** MCC is relatively insensitive to class size imbalance. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n\nSince I'm going to repeat the evaluation after each step of feature selection, I create a function that not only gives me the scores, but also plots some beautiful visualizations! \ud83d\ude01","411dcad7":"hmmmm... this is the **time** feature. although I'm not a domain expert, I don't think time can have an important effect of whether a test fails or not. so, let's just drop this column. \ud83e\udd37","9e13b8a2":"as you can see, although the scores (specially, F1 score) have declined during feature selection, our training time has dropped drastically.\n\nin scenarios where we have a lot of data, like in real world compaines, this is acceptable to lose a little accuracy and getting a huge boost in speed instead. ","41d206a4":"Evaluation time!","79f685aa":"# 2- Ammount of variation\nThis technique eliminates features that have a variation below a specified threshold. When a characteristic doesn't vary much within itself, it has low predictive potential, according to the theory.\n\nIn order to use this technique, it's better if we first normalize our data using scikit learn's `Normalizer`. the difference between `normalizer` and `standardScaler` is that Normalizer does not remove the mean and scale by deviation but scales the whole row to unit norm. \n\n**yet another time, to reduce test data leakage, I'm fitting scaler on training data and then transform both train and test sets.**","220c2c21":"Let's see how logistic regression performs with this new data:","f152e9e5":"Now, let's see how a logistic regressor performs on the current dataset. note that I had to impute the missing values because scikit learn's models can't handle missing values. but this imputed dataset is temporary and we won't use it in next steps.","3145c7ff":"So, 462 columns contain missing values. let's see the highest ammounts of missing values in a column:","8d76b17a":"Okay, let's run our evaluation function now:","c13f16fd":"wow! we started with approximately 600 features and now, we ended up with 17 - 34 times less that what we began with!","f66a3c31":"# Re-labeling the Target values\ntruth be told, current target values confuse me. \ud83d\ude05 according to dataset's description:\n> -1 corresponds to a pass and 1 corresponds to a fail and the data time stamp is for that specific test point.\n\nI want to change it so that each failure is is encoded as 0 while 1 corresponds to a pass:","c4ec6b8b":"![a claw crane machine](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/2d\/A_Claw_Crane_game_machine_containing_unicorn_plushes_in_Trouville%2C_France%2C_Sept_2011.jpg)\n# Feature Selection: From \u223c600 to 17 features!\n**WE ARE ALL CURSED!** there is an ancient and powerful curse, called ***the curse of dimensionality*** which haunts our models, and even our brains! (and any other machine that can learn and recognize patterns) \n\nWhat does the curse of dimensionality mean, in practice?\n> For a given sample size, there is a maximum number of features above which the performance of our classifier will degrade rather than improve\n> In most cases, the additional information that is lost by discarding some features is (more than) compensated by a more accurate mapping in the lower-dimensional space\n\n![curse of dimensionality illustrated](https:\/\/www.visiondummy.com\/wp-content\/uploads\/2014\/04\/dimensionality_vs_performance.png)\n\nAs I previously stated, the curse of dimensionality also has an impact on _our_ capacity to comprehend our surroundings. Humans have an extraordinary capacity to discern patterns and clusters in 1D, 2D and 3D, but these capabilities break down for \ud835\udc37 \u2265 4.\n\nThis curse is why we need dimensionality reduction methods so our models keep working. Fortunately, the scientific community's clever witches and wizards have already developed amazing techniques for decreasing the dimensions. \n\nTwo approaches are available to reduce dimensionality:\n1. **Feature extraction:** creating a subset of new features by combinations of the existing features (example: PCA)\n2. **Feature selection:** choosing a subset of all the features (example: Recursive Feature Elimination)\n\nIn this notebook, I'm going to try several feature selection methods used for dimensionality reduction, most of which are introduced in [this amazing video by data school](https:\/\/www.youtube.com\/watch?v=YaKMeAlHgqQ). _I'm not going to use feature extraction methods such as PCA._\n\n\u2753 Why not use the more general feature extraction methods, and simply project a high-dimensional feature vector onto a low-dimensional space?\n\n\u2757 Feature subset selection is necessary in a number of situations:\n- Features may be expensive to obtain: You evaluate a large number of features (sensors) in the test bed and select only a few for the final implementation. (***This is exactly the case for this dataset!***)\n- You may want to extract meaningful rules from your classifier: When you project, the measurement units of your features (length, weight, etc.) are lost\n- Features may not be numeric, a typical situation in machine learning\n\nso, we are going to use these Dimensionality reduction techniques:\n\n1. Percent Missing Values\n2. Ammount of Variation\n3. Pairwise Correlation\n4. Correlation with Target\n5. Recursive feature elimination","459c2f9d":"so, almost all of our data is numerical. there is one column which has `object` type, let's see what this column contains:","a2a7a2fa":"# Load the dataset","b8d1755e":"Now, we can remove columns with very low variance; which are the ones that have almost constant values in every row. we can use the estimator with a low threshold like 0. a variance of 0 means that corresponding column only contains a constant value, which is the same for each row. in other words, it's useless.\n\n![zero variance = useless feature](https:\/\/i.imgflip.com\/5m9my5.jpg)","5ec82854":"# Count the missing values\ndataset's description says it contains missing values. let's see how many missing values are there in our dataset:","75fc91e3":"as you see, ammount of whitespaces in these plots are high. which means there are too many missing values in these columns. \n\n# Impute Missing values\n\nalright, now that we have dealt with these columns, we can impute the remaining data. I'm going to use knn imputer. here is how it works according to scikit learn's documentation:\n> Each sample\u2019s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.\n\n**again, note that I fit the imputer on training set and then transform both training and test sets, in order to minimize data leakage from test set**","147e092f":"# Initialization\n# Load the libraries","9e52c683":"# See no evil! \ud83d\ude48\nbefore everything else, we should make our test set. In his brilliant paper, ['How to avoid machine learning pitfalls: a guide for academic researchers'](https:\/\/arxiv.org\/abs\/2108.02497), Michael A. Lones explains why creating a test set should be our first action:\n> It\u2019s essential to have data that you can use to measure how well your model generalises. A common problem is allowing information about this data to leak into the configuration, training or selection of models. When this happens, the data no longer provides a reliable measure of generality, and this is a common reason why published ML models often fail to generalise to real world data. There are a number of ways that information can leak from a test set. Some of these seem quite innocuous. For instance, during data preparation, using information about the means and ranges of variables within the whole data set to carry out variable scaling - in order to prevent information leakage, this kind of thing should only be done with the training data. [...] The best thing you can do to prevent these issues is to partition off a subset of your data right at the start of your project, and only use this independent test set once to measure the generality of a single model at the end of the project.\n\n<div class=\"alert alert-danger\" role=\"alert\">\n  BTW, I've made a video on this paper, feel free to watch it right here! \ud83d\udc47\n<\/div>","6f114136":"Alright, now we are going to reduce the number of features by removing columns that contain less data compared to others. note that by removing columns, we are going to lose some information. so, it is possible that our evaluation metrics become worse. on the other hand, hopefully our training time will decrease. ","588e7819":"# Feature Selection\nLet's start pruning this messy dataset! \u2702\ufe0f\ud83c\udf43","7c1852f3":"Wow! some of our columns are almost empty! we will take care of this in a moment... before that, let's set up our evaluation model and metrics. ","5263bf7b":"Amazing! now let's transform our train and test dataset. note that I'm not using `X_train_nrm` because I want to use the original data for now. so, I'll transform `X_train_imp` and `X_test_imp`.","416075e8":"# 3- Pairwise Correlation\n\nThe correlation coefficient has values between -1 to 1\n- A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n- A value closer to 1 implies stronger positive correlation\n- A value closer to -1 implies stronger negative correlation\n\nIf two independent features (independent = not target!) have a high absolute correlation, the information they offer for our ML model is basically the same.\n\n![highly correlated features are roughly, same features](https:\/\/i.imgflip.com\/5m9tvw.jpg)\n\ncorrelated features in general don't improve models, so you can drop one of them, because it's redundant.\n\nin order to find the columns we have to drop, I use the following function which is originally written by [Krish C Naik](https:\/\/github.com\/krishnaik06\/Complete-Feature-Selection\/blob\/master\/2-Feature%20Selection-%20Correlation.ipynb).","ac0d7e98":"# 5- Recursive feature elimination\nThis is the last step. in this step, we are going to use Recursive feature elimination with cross-validation to find the optimum number of features from the remaining 42 features. you can read more about this method [here](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#rfe). simply put, RFE starts with all of the features and then eliminates features one by one (based on their importance). it will stop when a certain number of features are left, or elimination of features no longer help the model.","ace5114e":"Since according to dataset's description, target values are highly umbalanced, so I split it in a stratified fashion.","a1ed7408":"Now I'm going to use this function to discover columns with more than 50% of their data missing. **note that in order to find these columns, I only use train set, and will drop the same columns from test set as well.**","335bda41":"# Conclusion\nCongrats! We successfully selected only a handful of features from about 600! and we lost only a little bit of information, while boosted our training speed. \ud83e\udd73\n\nLet's keep learning!\n\n<div class=\"alert alert-danger\" role=\"alert\" style=\"text-align:center;\">\n    I hope you enjoyed this tutorial. If you did, please consider subscribing to <b><a href=\"https:\/\/www.youtube.com\/channel\/UC34Gj0-vHuBiTNEYlP7wczg\">my YouTube Channel \u25b6<\/a><\/b>\n<\/div>\n\n<center><h2><span style=\"font-family:cursive;\"> Also, please Upvode! \ud83d\ude1c <\/span><\/h2><\/center>","75c1eeee":"# 4- Correlation with Target\nUnlike the pairwise correlation in which lower correlation is desirable, we would like our features to have high correlation with the target. if a feature has low correlation with target, it means that it is not a helpful feature for predicting the target, and hence, should be removed. The following function will calculate correlation of each feature with the target, and then returns the columns that have a correlation less than chosen threshold.","7c4e4cba":"# Let's take a look at our data!\nThis dataset has too many features- almost 600 features!\nLet's see what these features contain.\n\nthe following code will show us the type of features:","a3b5a837":"# 1- Percent Missing Values\nIf most of the data in a column are missing values, that column is actually useless. imputing a column that has more than 50% missing values won't help much either. so, let's just get rid of those columns. \n\nto do so, I define a function that takes a dataframe and a threshold, and returns the columns that must be removed. then, we can drop these columns on both train and test set.  ","adca4972":"Now let's check out our evaluation model again:","3c0a019e":"Before we move on, let's take a look at the columns we just dropped. I'm going to use `missingno` python package to visualize missing values.   "}}