{"cell_type":{"a951f1f6":"code","6e4cc640":"code","b83d8ff7":"code","5fca59b4":"code","c79d2223":"code","0f80835a":"code","a1d6c795":"code","40ccded0":"code","037f4b86":"code","217ae546":"code","4e997f99":"code","b8d1213b":"code","37d3341c":"code","5c199119":"code","d8f23b12":"code","66055470":"code","6adcd154":"code","fdae784f":"code","744e4a12":"code","816659a2":"code","257014b4":"code","84467253":"code","1e64a81c":"code","f6ed15ec":"code","83179276":"code","7b60058b":"code","c9eeedb0":"code","6e6bd26f":"code","10305944":"code","9e9d6a51":"code","67b09554":"code","1b5003b3":"code","2fc0a9f3":"code","28544cda":"code","d23167b7":"code","0ee10b52":"code","d6a137ec":"code","4b382224":"code","f414aa57":"code","dfd8e082":"code","f54fc993":"code","cde12128":"code","56e9b557":"code","790256f6":"code","0a31fe03":"code","dfe636a6":"markdown","a83df9d5":"markdown","34b41594":"markdown","efd31b2e":"markdown","21a8ba9a":"markdown","15bdb8cd":"markdown","38a12025":"markdown","fee25819":"markdown","d0cbb49f":"markdown","dffbb377":"markdown","4d2e9e9c":"markdown","b157c207":"markdown","d0362d69":"markdown","6dc17ee7":"markdown","d0c82468":"markdown","27826922":"markdown","eaf3fc75":"markdown","4c3d0318":"markdown","753c9522":"markdown","f744dac2":"markdown","45fab094":"markdown","e69e89b6":"markdown","dff4b0c2":"markdown","ce6aeb9e":"markdown","ddb93be8":"markdown","95a187c1":"markdown"},"source":{"a951f1f6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn import model_selection\nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import RidgeCV, LassoCV,ElasticNetCV","6e4cc640":"df=pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")","b83d8ff7":"df.head()","5fca59b4":"df.info()","c79d2223":"# The unique observation numbers in variables.\ndf.nunique()","0f80835a":"# The dataset was examined. It consists of 322 observation units and 20 variables.\ndf.shape","a1d6c795":"df.describe().T","40ccded0":"df.isnull().values.any()","037f4b86":"df.isnull().sum()","217ae546":"# Visualization\ncorr = df.corr()\nplt.figure(figsize=(18,10))\nsns.heatmap(corr, annot=True)\nplt.show()","4e997f99":"more_cat_cols = [col for col in df.columns if len(df[col].unique()) < 10]\nprint('Number of Categorical Variables : ', len(more_cat_cols))\nprint(more_cat_cols)","b8d1213b":"# League: A factor with levels A and N indicating player\u2019s league at the end of 1986 \ndf[\"League\"].value_counts()","37d3341c":"fig1, ax1 = plt.subplots()\nax1.pie(df[\"League\"].value_counts(),labels=['A','N'],autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')\nplt.show()","5c199119":"#  A factor with levels A and N indicating player\u2019s league at the beginning of 1987\ndf[\"NewLeague\"].value_counts()","d8f23b12":"fig1, ax1 = plt.subplots()\nax1.pie(df[\"NewLeague\"].value_counts(),labels=['A','N'],autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')\nplt.show()","66055470":"# Division: A factor with levels E and W indicating player\u2019s division at the end of 1986.\ndf[\"Division\"].value_counts()","6adcd154":"fig1, ax1 = plt.subplots()\nax1.pie(df[\"Division\"].value_counts(),labels=['W','E'],autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')\nplt.show()","fdae784f":"num_cols = [col for col in df.columns if df[col].dtypes != 'O' and col not in \"Id\"]\nprint('Number of Numerical Variables: ', len(num_cols))\nnum_cols","744e4a12":"def hist_for_nums(data, num_cols):\n    col_counter = 0\n    data = data.copy()\n    for col in num_cols:\n        data[col].hist(bins=30)\n        plt.xlabel(col)\n        plt.title(col)\n        plt.show()\n        col_counter += 1\n    print(col_counter, \"variables have been plotted\")\nhist_for_nums(df, num_cols)","816659a2":"df.Salary.describe().T","257014b4":"df['Salary'].median()","84467253":"# The values of Skewness and Kurtosis indicate that there is no a normal distribution.\nprint(\"Skewness: %f\" % df['Salary'].skew())\nprint(\"Kurtosis: %f\" % df['Salary'].kurt())","1e64a81c":"sns.distplot(df.Salary);","f6ed15ec":"# Visualization of the Salary variable with the boxplot method. There are some outlier values observed on the graphic.\nsns.boxplot(x = df[\"Salary\"]);","83179276":"Q1 = df.Salary.quantile(0.25)\nQ3 = df.Salary.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"Salary\"] > upper,\"Salary\"] = upper","7b60058b":"sns.boxplot(x = df[\"Salary\"]);","c9eeedb0":"df.Salary.describe().T","6e6bd26f":"df.isnull().sum()","10305944":"# Replacing missing data with its mean\ndf['Salary'].fillna(df['Salary'].mean(), inplace=True)","9e9d6a51":"df.isnull().sum()","67b09554":"sns.distplot(df.Salary);","1b5003b3":"# # Applying the log1p function log(1+x) to Salary\ndf[\"Salary\"] = np.log1p(df[\"Salary\"])","2fc0a9f3":"# Check the new distribution after log transformation\nsns.distplot(df.Salary);","28544cda":"# One hot encoding was applied to categorical variables (two or more).\ndf = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)","d23167b7":"df.head()","0ee10b52":"y=df[\"Salary\"]\nX=df.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20,random_state=46)","d6a137ec":"reg_model = LinearRegression()\nreg_model.fit(X_train, y_train)\ny_pred=reg_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","4b382224":"ridge_model=Ridge().fit(X_train,y_train)\ny_pred= ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","f414aa57":"alpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\nridreg_cv = RidgeCV(alphas = alpha, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nridreg_cv.fit(X_train, y_train)\nridreg_cv.alpha_\n\nridreg_tuned = Ridge(alpha = ridreg_cv.alpha_).fit(X_train,y_train)\ny_pred = ridreg_tuned.predict(X_test)\ndf_ridge_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_ridge_tuned_rmse","dfd8e082":"lasso_model = Lasso().fit(X_train, y_train)\ny_pred=lasso_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","f54fc993":"alpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\nlasso_cv = LassoCV(alphas = alpha, cv = 10, normalize = True)\nlasso_cv.fit(X_train, y_train)\nlasso_cv.alpha_\n\nlasso_tuned = Lasso(alpha = lasso_cv.alpha_).fit(X_train,y_train)\ny_pred = lasso_tuned.predict(X_test)\ndf_lasso_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_lasso_tuned_rmse","cde12128":"enet_model = ElasticNet().fit(X_train, y_train)\ny_pred = enet_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","56e9b557":"enet_params = {\"l1_ratio\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n              \"alpha\":[0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]}\n\nenet_model = ElasticNet().fit(X_train,y_train)\nenet_cv = GridSearchCV(enet_model, enet_params, cv = 10).fit(X, y)\nenet_cv.best_params_\n\nenet_tuned = ElasticNet(**enet_cv.best_params_).fit(X_train,y_train)\ny_pred = enet_tuned.predict(X_test)\ndf_enet_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_enet_tuned_rmse","790256f6":"# Evaluate each model\nmodels = [('LinearRegression', LinearRegression()),\n          ('Ridge', Ridge()),\n          ('Lasso', Lasso()),\n          ('ElasticNet', ElasticNet())]\nresults = []\nnames = []\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    result = np.sqrt(mean_squared_error(y_test, y_pred))\n    results.append(result)\n    names.append(name)\n    msg = \"%s: %f\" % (name, result)\n    print(msg)","0a31fe03":"# Result of Final Models\nresult_df = pd.DataFrame({\"RMSE\":[np.sqrt(mean_squared_error(y_test, y_pred)), df_ridge_tuned_rmse, df_lasso_tuned_rmse, df_enet_tuned_rmse]})\nresult_df.index= [\"LinearRegression\", \"Ridge\",\"Lasso\",\"ElasticNet\"]\nresult_df                        ","dfe636a6":"![image.png](attachment:image.png)\nSource: https:\/\/www.chegg.com\/","a83df9d5":"# Missing Values","34b41594":"**Visualization of numeric variables**","efd31b2e":"**LOG TRANSFORMATION**\n* The distribution of the dependent variable (Salary) is not a normal. \nLog transformation is a good choose.","21a8ba9a":"# Ridge Regression","15bdb8cd":"The dataset consists of 322 samples about major league players. All of the variables in the dataset are listed below;\n\n**Dependent Variable:**\n* Salary: 1987 annual salary on opening day in thousands of dollars\n\n\n**Independent Variable:**\n1. AtBat: Number of times at bat in 1986 \n1. Hits: Number of hits in 1986 \n1. HmRun: Number of home runs in 1986 \n1. Runs: Number of runs in 1986 \n1. RBI: Number of runs batted in in 1986 \n1. Walks: Number of walks in 1986 \n1. Years: Number of years in the major leagues \n1. CAtBat: Number of times at bat during his career \n1. CHits: Number of hits during his career \n1. CHmRun: Number of home runs during his career \n1. CRuns: Number of runs during his career\n1. CRBI: Number of runs batted in during his career \n1. CWalks: Number of walks during his career \n1. League: A factor with levels A and N indicating player\u2019s league at the end of 1986 \n1. Division: A factor with levels E and W indicating player\u2019s division at the end of 1986 \n1. NewLeague: A factor with levels A and N indicating player\u2019s league at the beginning of 1987\n1. PutOuts: Number of put outs in 1986 \n1. Assists: Number of assists in 1986 \n1. Errors: Number of errors in 1986","38a12025":"# 6. ONE-HOT ENCODING","fee25819":"# Lasso Regression","d0cbb49f":"# 5. ANALYSIS of DEPENDENT VARIABLE (TARGET ANALYSIS)","dffbb377":"**The Dataset**\n* The dataset was taken from the StatLib library. It was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The related  statistics were obtained from The 1987 Baseball Encyclopedia Update (Collier Books, Macmillan Publishing Company, New York).","4d2e9e9c":"**Objective**\n* The aim of this study is to create a regression model that predicts a baseball player\u2019s Salary on the basis of the dataset.","b157c207":"As seen above, there are three categorical variables in the dataset.","d0362d69":"# 1. IMPORT SOME NECESSARY LIBRARIES","6dc17ee7":"# Elastic Net Regression","d0c82468":"# 7. MODELING","27826922":"**Model Tuning and Final Model**","eaf3fc75":"* I split the data set: 80% of the data is train, and 20% of it is test data.","4c3d0318":"# 3. EXPLORATORY DATA ANALYSIS ON CATEGORICAL DATA","753c9522":"# Load Data","f744dac2":"**Model Tuning and Final Model**","45fab094":"**Model Tuning and Final Model**","e69e89b6":"# 2. UNDERSTANDING THE DATA-SET","dff4b0c2":"For categorical columns (string columns), the missing values can be filled with mode. For numerical columns, the missing values can be filled with with mean.","ce6aeb9e":"# Linear Regression","ddb93be8":"The root-mean-square error (RMSE) indicates a measure of the differences between actual values and predicted values observed. \nThe lowest RMSE was obtained from Lasso Regression (0.6109841011459226)","95a187c1":"# 4. ANALYSIS of NUMERICAL VARIABLE"}}