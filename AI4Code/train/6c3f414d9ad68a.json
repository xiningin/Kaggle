{"cell_type":{"282f4dab":"code","f8ec49f3":"code","2da8da75":"code","ca01ed08":"code","89132876":"code","aa980f88":"code","8fda16b6":"code","ba069df6":"code","2908a366":"code","a70047b0":"code","e7d0d84b":"code","7610ff50":"code","14eb9674":"code","0be0e79a":"code","d209c1c0":"code","48305f6d":"code","1d57f8f4":"code","07f169c6":"code","519adb76":"code","bbefddb0":"code","843d678b":"code","16f14037":"code","8b510af4":"code","8b2ac50f":"code","2312ccee":"code","3b4a4cea":"code","6d824819":"code","86a8ae4f":"code","51021cfc":"code","d8fc350f":"code","01104d2b":"code","19d75b8f":"code","8e5884b5":"code","b991c4b1":"code","62f2052c":"code","58a312c3":"code","eb32fa74":"code","94f289ae":"code","dc770ab6":"code","eec5cb4f":"code","d156e3e8":"code","b0471e7d":"code","6dafc195":"code","e9131bd4":"code","2a8c862e":"code","c705aac5":"code","1f237d42":"code","325752f9":"code","8b0d4cc6":"code","41a2239d":"code","1949564b":"code","a4132441":"code","35bf7213":"code","55329f1f":"code","c3b91af8":"code","5798a447":"code","59b0c758":"code","5f542665":"code","7a7b0b69":"code","171f35e9":"code","b3175d52":"code","4410828d":"code","30f26e57":"code","ca316359":"code","a0a53a4f":"code","e8e6a2c4":"code","c311bfb7":"code","cee8c5d7":"code","fa20aaf3":"code","8eae03e6":"code","4da8e3e2":"code","bb4dfde4":"code","117a4b95":"markdown","930cdb37":"markdown","487e0b17":"markdown","fe54fb0f":"markdown","006c12fa":"markdown","b2119113":"markdown","a083ad38":"markdown","fc2e036f":"markdown","3a77cbad":"markdown","3f8a243f":"markdown","642b43cc":"markdown","a2d7477b":"markdown","afdc20f9":"markdown","45d71153":"markdown","db2185b0":"markdown","950a7594":"markdown","cd5890f7":"markdown","06edb281":"markdown","3b4ac8f4":"markdown","c99bc273":"markdown","6c555bc3":"markdown","526fe791":"markdown","f1c32d3f":"markdown","26aebd64":"markdown","c6dcea47":"markdown","c5eacc2b":"markdown","74b34688":"markdown","91eece29":"markdown","efec3c80":"markdown","789dfca1":"markdown","a08d82e5":"markdown","000d6f8c":"markdown","983369bd":"markdown","93404af9":"markdown","c5e24dbf":"markdown","9836eb1c":"markdown","0cb340a0":"markdown","a7e037c5":"markdown","9350fa6a":"markdown","875e2ed9":"markdown","ba1e1eb9":"markdown","24259cb9":"markdown","3050eae1":"markdown","e5d7fa8f":"markdown","0fa0d812":"markdown","3b00845c":"markdown","32d38de6":"markdown","6f618b7c":"markdown","4e4a469f":"markdown","7dfd274f":"markdown","93b6c79d":"markdown","273852f1":"markdown","c4815ad3":"markdown","5ebd5160":"markdown","2abeb93c":"markdown","da0dc97b":"markdown","f10e7319":"markdown","5c683869":"markdown"},"source":{"282f4dab":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler","f8ec49f3":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n#concatenate the datasets\nframe = ([df_train , df_test])\ndf_titanic = pd.concat(frame)","2da8da75":"df_submission.head()","ca01ed08":"men_survive = df_train.loc[df_train.Sex == 'male']['Survived']\nrate_men = sum(men_survive)\/len(men_survive)\nprint('The survival rate for men is:', round(rate_men, 2))\n\nwomen_survive = df_train.loc[df_train.Sex == 'female']['Survived']\nrate_women = sum(women_survive)\/len(women_survive)\nprint('The survival rate for men is:', round(rate_women, 2))","89132876":"df_train.head(3)","aa980f88":"df_test.head(3)","8fda16b6":"print('There are', len(df_train), 'entries and ', len(df_train.columns), 'attributes in the Train dataset')\nprint('There are', len(df_test), 'entries and ', len(df_test.columns), 'attributes in the Test dataset')\nprint('There are', len(df_titanic), 'entries and ', len(df_titanic.columns), 'attributes in the Titanic dataset')","ba069df6":"df_titanic.dtypes","2908a366":"print('Train dataset missing values')\ndisplay(df_titanic.iloc[:891].isnull().sum())\nprint('*' * 40)\nprint('Test dataset missing values')\ndisplay(df_titanic.iloc[891:].isnull().sum())","a70047b0":"df = df_titanic['Name']\ndf_titanic[df.isin(df[df.duplicated()])]","e7d0d84b":"title = df_titanic['Name'].str.split(', ', expand = True)[1].str.split('.', expand = True)[0]\nprint('The titles are:')\ndisplay(title.unique())\nprint('*' * 40)\nprint('There are', title.nunique(), 'unique titles')","7610ff50":"Title_dict = {\n    \"Capt\": \"Rare\",\n    \"Col\": \"Rare\",\n    \"Major\": \"Rare\",\n    \"Jonkheer\": \"Rare\",\n    \"Don\": \"Rare\",\n    \"Dona\": \"Rare\",\n    \"Sir\" : \"Rare\",\n    \"Dr\": \"Rare\",\n    \"Rev\": \"Rare\",\n    \"the Countess\":\"Rare\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Rare\"\n}","14eb9674":"df_titanic['Title'] = title\ndf_titanic['Title'] = df_titanic.Title.map(Title_dict)","0be0e79a":"df_titanic['Title'].value_counts()","d209c1c0":"df_titanic['Title'].isnull().sum()","48305f6d":"#one hot encoding on titles\ntitles_ohe = pd.get_dummies(df_titanic['Title'], prefix = 'Title')\n#concatenate the one hot encoding dataframe with the titanic dataframe\ndf_titanic = pd.concat([df_titanic, titles_ohe], axis = 1)\ndf_titanic.head(3)","1d57f8f4":"fig = plt.subplots(figsize = (10,7))\nsns.histplot(data = df_titanic['Age'], kde = True)\nplt.show()","07f169c6":"corr = df_titanic[['Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Survived']].corr().abs()\ncorr.reset_index().iloc[0]","519adb76":"age = df_titanic.groupby(['Pclass', 'Sex'])['Age'].median()\nprint('Average age per Pclass and sex')\nage = age.reset_index()\nage","bbefddb0":"def fill_age(row):\n    condition = ((age['Pclass'] == row['Pclass']) & \n                 (age['Sex'] == row['Sex']))\n    return age[condition]['Age'].values[0]","843d678b":"df_titanic['Age'] = df_titanic.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis = 1)\ndf_titanic['Age'].isnull().sum()","16f14037":"fig = plt.subplots(figsize = (10,7))\nsns.boxplot(data = df_titanic, x = 'Age')\nplt.show()\ndisplay(df_titanic['Age'].describe())","8b510af4":"#calculate the interquartile\nq25, q50, q75 = np.percentile(df_titanic['Age'], [25, 50, 75])\niqr = q75 - q25\n#calculate the min and max limits to be considered an outlier\nmin = q25 - 1.5 * iqr\nmax = q75 + 1.5 * iqr\n#identify the points\n[x for x in df_titanic['Age'] if x > max];\nprint('The maximum limit is:', max)\nprint(min, q25, q50, q75, max)","8b2ac50f":"df_titanic['Age_map'] = pd.cut(df_titanic['Age'].astype(int), bins = [0, 12, 20, 40, 120], labels = ['Kid', 'Teen', 'Adult', 'Senior'])\ndf_titanic['Age_map'].value_counts().sort_index()","2312ccee":"#one hot encoding on ages\nage_ohe = pd.get_dummies(df_titanic['Age_map'], prefix = 'Age')\n#concatenate the one hot encoding dataframe with the titanic dataframe\ndf_titanic = pd.concat([df_titanic, age_ohe], axis = 1)\ndf_titanic.head(3)","3b4a4cea":"#create the new feature Family\n#add 1 for the current passenger\ndf_titanic['Family'] = df_titanic['SibSp'] + df_titanic['Parch'] + 1\ndf_titanic.head()","6d824819":"#replace the missing values by U\ndf_titanic.Cabin.fillna('U', inplace = True)","86a8ae4f":"#create a new column with only the letters in it\ndf_titanic['LocCabin'] = df_titanic['Cabin'].astype('str').str[0]\ndf_titanic['LocCabin'].value_counts()","51021cfc":"cabin_surv = df_titanic[['LocCabin', 'Survived']].groupby('LocCabin').mean()\ncabin_surv = cabin_surv.reset_index()\ncabin_surv","d8fc350f":"plt.figure(figsize = (10, 7))\nsns.barplot(data = cabin_surv, x = 'LocCabin', y = 'Survived')\nplt.show()","01104d2b":"df_titanic.groupby(['LocCabin', 'Pclass']).agg({'Pclass': 'count', 'Survived': 'mean'})","19d75b8f":"#replace the cabin letter by its category\ndf_titanic['LocCabin'] = df_titanic['LocCabin'].replace(['A', 'B', 'C'], 'ABC')\ndf_titanic['LocCabin'] = df_titanic['LocCabin'].replace(['D', 'E'], 'DE')\ndf_titanic['LocCabin'] = df_titanic['LocCabin'].replace(['F', 'G'], 'FG')\ndf_titanic['LocCabin'] = df_titanic['LocCabin'].replace(['T'], 'U')\n\ndf_titanic['LocCabin'].value_counts()","8e5884b5":"#one hot encoding on cabin\ncabin_ohe = pd.get_dummies(df_titanic['LocCabin'], prefix = 'Cabin')\n#concatenate the one hot encoding dataframe with the titanic dataframe\ndf_titanic = pd.concat([df_titanic, cabin_ohe], axis = 1)\ndf_titanic.head(3)","b991c4b1":"df_titanic.loc[df_titanic['Fare'].isnull()]","62f2052c":"#comute the median of passengers similar to the one with the fare value missing\nfare = df_titanic.loc[(df_titanic['Pclass'] == 3) &\n              (df_titanic['Embarked'] == 'S') &\n              (df_titanic['Family'] == 1)]['Fare'].median()\n\n#fill the null with the median\ndf_titanic.loc[df_titanic['Fare'].isnull(), 'Fare'] = fare","58a312c3":"fig = plt.subplots(figsize = (10,7))\nsns.histplot(data = df_titanic['Fare'], kde = True)\nplt.show()","eb32fa74":"fig = plt.subplots(figsize = (10,7))\nsns.boxplot(data = df_titanic, x = 'Fare')\nplt.show()","94f289ae":"df_titanic['Fare'].describe()","dc770ab6":"#calculate the interquartile\nq25, q75 = np.percentile(df_titanic['Fare'], [25, 75])\niqr = q75 - q25\n#calculate the min and max limits to be considered an outlier\nmax = q75 + 1.5 * iqr\n#identify the points\n[x for x in df_titanic['Fare'] if x > max];\nprint('The maximum limit is:', max)\nprint(q25, q50, q75, max)","eec5cb4f":"df_titanic['Fare_map'] = pd.cut(df_titanic['Fare'], bins = [0, 7.895, 26.0, 31.275, 66.34], labels = ['Cheap', 'Median', 'Average', 'Expensive'])\ndf_titanic['Fare_map'].value_counts().sort_index()","d156e3e8":"#one hot encoding on titles\nfare_ohe = pd.get_dummies(df_titanic['Fare_map'], prefix = 'Fare')\n#concatenate the one hot encoding dataframe with the titanic dataframe\ndf_titanic = pd.concat([df_titanic, fare_ohe], axis = 1)\ndf_titanic.head(3)","b0471e7d":"#one hot encoding\npclass_ohe = pd.get_dummies(df_titanic['Pclass'], prefix = 'Pclass')\n#concatenate the one hot encoding dataframe with the titanic dataframe\ndf_titanic = pd.concat([df_titanic, pclass_ohe], axis = 1)\ndf_titanic.head()","6dafc195":"#one hot encoding\nsex_ohe = pd.get_dummies(df_titanic['Sex'], prefix = 'Sex')\n#concatenate the one hot encoding dataframe with the titanic dataframe\ndf_titanic = pd.concat([df_titanic, sex_ohe], axis = 1)\ndf_titanic.head()","e9131bd4":"df_titanic.loc[df_titanic['Embarked'].isnull()]","2a8c862e":"embarked = df_titanic.groupby('Embarked')['Embarked'].count()\nembarked","c705aac5":"df_titanic.Embarked.fillna('S', inplace = True)","1f237d42":"#one-hot encoding\nembarked_ohe = pd.get_dummies(df_titanic['Embarked'], prefix = 'Embarked')\n#concatenate df_titanic and embarked_ohe\ndf_titanic = pd.concat([df_titanic, embarked_ohe], axis = 1)\ndf_titanic.head()","325752f9":"#df_titanic['Ticket_freq'] = df_titanic.groupby('Ticket')['Ticket'].transform('count')\n#df_titanic['Ticket_freq'].value_counts()","8b0d4cc6":"df_titanic['Age_class'] = df_titanic['Age'] * df_titanic['Pclass']","41a2239d":"df_titanic['Fare_person'] = df_titanic['Fare'] \/ df_titanic['Family']\ndf_titanic['Fare_person'] = df_titanic['Fare_person'].astype('int')","1949564b":"df_titanic.head(3)","a4132441":"df_titanic.info()","35bf7213":"df_titanic.drop([ 'Pclass', 'Age', 'Age_map', 'Sex', 'Fare', 'Fare_map', 'Name', 'Ticket','LocCabin', 'Cabin', 'Embarked', 'Title'], axis = 1, inplace = True)\ndf_titanic.info()","55329f1f":"df_titanic.head()","c3b91af8":"num_feat = list(['SibSp', 'Parch', 'Family', 'Age_class', 'Fare_person'])\nnum_feat","5798a447":"df_titanic_SS = pd.DataFrame(data = df_titanic)\ndf_titanic_SS[num_feat] = StandardScaler().fit_transform(df_titanic_SS[num_feat])","59b0c758":"df_titanic_SS.head()","5f542665":"df_train, df_test = df_titanic_SS.loc[:890], df_titanic_SS[891:].drop(['Survived'], axis = 1)","7a7b0b69":"#sns.heatmap(df_train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\n#fig=plt.gcf()\n#fig.set_size_inches(20,12)\n#plt.show()","171f35e9":"X = df_train.drop(['Survived', 'PassengerId'], axis = 1)\ny = df_train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b3175d52":"log_reg =  LogisticRegression(C = 1, solver = 'liblinear')\nlog_reg.fit(X_train, y_train)\n\nprint('----------------------Logistic Regression----------------------')\nprint()\n#accuracy\nprint('Training accuracy is', log_reg.score(X_train, y_train))\nprint('Testing accuracy is', log_reg.score(X_test, y_test))\nprint('*' * 40)\n\n#cross validation\nscores_accuracy = cross_val_score(log_reg, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy for the logistic regression model is', scores_accuracy.mean())\nprint('*' * 40)\n\n#confusion matrix\ny_predicted = log_reg.predict(X_test)\nconfusion = confusion_matrix(y_test, y_predicted)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('*' * 40)\n\n#Basic evaluation\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, y_predicted)))\nprint(classification_report(y_test, y_predicted, target_names=['0', '1']))","4410828d":"rfc =  RandomForestClassifier(n_estimators = 500)\nrfc.fit(X_train, y_train)\n\nprint('----------------------Random Forest Classifier----------------------')\nprint()\n#accuracy\nprint('Training accuracy is', rfc.score(X_train, y_train))\nprint('Testing accuracy is', rfc.score(X_test, y_test))\nprint('*' * 40)\n\n#cross validation\nscores_accuracy = cross_val_score(rfc, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy for the random forest model is', scores_accuracy.mean())\nprint('*' * 40)\n\n#confusion matrix\ny_predicted2 = rfc.predict(X_test)\nconfusion = confusion_matrix(y_test, y_predicted2)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('*' * 40)\n\n#Basic evaluation\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted2)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_predicted2)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_predicted2)))\nprint('F1: {:.2f}'.format(f1_score(y_test, y_predicted2)))\nprint(classification_report(y_test, y_predicted2, target_names=['0', '1']))","30f26e57":"knn =  KNeighborsClassifier(n_neighbors = 7)\nknn.fit(X_train, y_train)\n\nprint('----------------------KNN----------------------')\nprint()\n#accuracy\nprint('Training accuracy is', knn.score(X_train, y_train))\nprint('Testing accuracy is', knn.score(X_test, y_test))\nprint('*' * 40)\n\n#cross validation\nscores_accuracy = cross_val_score(knn, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy for the knn model is', scores_accuracy.mean())\nprint('*' * 40)\n\n#confusion matrix\ny_predicted3 = knn.predict(X_test)\nconfusion = confusion_matrix(y_test, y_predicted3)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('*' * 40)\n\n#Basic evaluation\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted3)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_predicted3)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_predicted3)))\nprint('F1: {:.2f}'.format(f1_score(y_test, y_predicted3)))\nprint(classification_report(y_test, y_predicted3, target_names=['0', '1']))","ca316359":"svc =  SVC()\nsvc.fit(X_train, y_train)\n\nprint('----------------------SVM classifier----------------------')\nprint()\n#accuracy\nprint('Training accuracy is', svc.score(X_train, y_train))\nprint('Testing accuracy is', svc.score(X_test, y_test))\nprint('*' * 40)\n\n#cross validation\nscores_accuracy = cross_val_score(svc, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy for the svc model is', scores_accuracy.mean())\nprint('*' * 40)\n\n#confusion matrix\ny_predicted4 = svc.predict(X_test)\nconfusion = confusion_matrix(y_test, y_predicted4)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('*' * 40)\n\n#Basic evaluation\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted4)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_predicted4)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_predicted4)))\nprint('F1: {:.2f}'.format(f1_score(y_test, y_predicted4)))\nprint(classification_report(y_test, y_predicted4, target_names=['0', '1']))","a0a53a4f":"models_comp = pd.DataFrame({\n   'Model': ['Logistic Regression', 'Random Forest', 'KNN', 'SVM'],\n    'Accuracy score': [log_reg.score(X_test, y_test), rfc.score(X_test, y_test),\n                      knn.score(X_test, y_test), svc.score(X_test, y_test)]\n})\ndisplay(models_comp.sort_values(by = 'Accuracy score', ascending = False))","e8e6a2c4":"rfc_imp =  RandomForestClassifier(criterion = 'gini',\n                              n_estimators = 1750, \n                              max_depth = 7,  \n                              min_samples_split = 6,\n                              min_samples_leaf = 6,\n                              max_features = 'auto',\n                              oob_score = True,\n                              random_state = 42,\n                              n_jobs = -1,\n                              verbose = 1)\nrfc_imp.fit(X_train, y_train)\n\nprint('----------------------Random Forest Classifier----------------------')\nprint()\n#accuracy\nprint('Training accuracy is', rfc_imp.score(X_train, y_train))\nprint('Testing accuracy is', rfc_imp.score(X_test, y_test))\nprint('*' * 40)\n\n#cross validation\nscores_accuracy = cross_val_score(rfc_imp, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy for the random forest model is', scores_accuracy.mean())\nprint('Standard deviation for the random forest model is', scores_accuracy.std())\nprint('*' * 40)","c311bfb7":"#confusion matrix\ny_predicted_imp = rfc_imp.predict(X_test)\nconfusion = confusion_matrix(y_test, y_predicted_imp)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('*' * 40)\n\n#Basic evaluation\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted_imp)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_predicted_imp)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_predicted_imp)))\nprint('F1: {:.2f}'.format(f1_score(y_test, y_predicted_imp)))\nprint(classification_report(y_test, y_predicted2, target_names=['0', '1']))","cee8c5d7":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n#define hyperparameters\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l1', 'l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\n#define grid search\ngrid = dict(solver = solvers, penalty = penalty , C = c_values)\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\ngrid_search = GridSearchCV(estimator = log_reg, param_grid = grid, n_jobs = -1, cv = cv, scoring = 'accuracy', error_score = 0)\ngrid_result = grid_search.fit(X, y)\n\n#get the results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","fa20aaf3":"log_reg_imp =  LogisticRegression(C = 1, penalty = 'l1', solver = 'liblinear')\nlog_reg_imp.fit(X_train, y_train)\n\nprint('----------------------Logistic Regression----------------------')\nprint()\n#accuracy\nprint('Training accuracy is', log_reg_imp.score(X_train, y_train))\nprint('Testing accuracy is', log_reg_imp.score(X_test, y_test))\nprint('*' * 40)\n\n#cross validation\nscores_accuracy = cross_val_score(log_reg_imp, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy for the logistic regression model is', scores_accuracy.mean())\nprint('*' * 40)\n\n#confusion matrix\ny_predicted_imp = log_reg_imp.predict(X_test)\nconfusion = confusion_matrix(y_test, y_predicted)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('*' * 40)\n\n#Basic evaluation\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted_imp)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_predicted_imp)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_predicted_imp)))\nprint('F1: {:.2f}'.format(f1_score(y_test, y_predicted_imp)))\nprint(classification_report(y_test, y_predicted, target_names=['0', '1']))","8eae03e6":"X_test_1 = df_test.drop('PassengerId', axis = 1)\npredictions = rfc_imp.predict(X_test_1)\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': predictions.astype('int')})\noutput.to_csv('my_submission_rfc.csv', index=False)\nprint(\"Your submission was successfully saved!\")","4da8e3e2":"output.head(10)","bb4dfde4":"output.describe()","117a4b95":"#### SibSp and Parch\nThe idea is to create 1 new feature which is the sum of the SibSp and Parch feature","930cdb37":"Changing the prediction model doesn't seem to affect that much the accuracy score.\n<br>\nMoreover, my accuracy score for lr is 0.835, but when I submit to Kaggle it gives me a score of 0.77033.\n<br>\n<br>\nThe key is to work on the features.\n<br>\nThe age and fare feature have a lot of outliers. Dealing with them can be key.\n<br>\n------------------------------------------------------------------------------\n- version 1 & version 3 score are 0 on Kaggle because my Survived attribute in the submission was a float type and not int type.\n- version 2, score of 0.76076 (top 84%)\n- version 3, score of 0.77033 (top 64%)\n","487e0b17":"Detect the outliers. All Age above 59.75 years old is considered an outlier. Deleting the outlier is not a good idea. Firts, it will reduce the training set samples, and second, the outliers are important for the prediction model.\n<br>\nSince we have outliers, I will not perform a normalized or standardized feature engineering, but rather a binning.","fe54fb0f":"#### Logistic Regression","006c12fa":"### Discover the datasets","b2119113":"#### Reshape all numerical values with StandardScaler()","a083ad38":"#### On Logistic Regression ","fc2e036f":"Let's get rid of all the unwanted column","3a77cbad":"<b>Fare per person<\/b>","3f8a243f":"Check for duplicated names in the titanic dataset. \n<br>\nAs we can see, we have 2 duplicated names. However, after a closer look, their age is different, and their ticket number is different. It seems that they are 2 different people.\n<br>\n<br>\nNo duplicates, we're good to go.","642b43cc":"#### Embarked\nFirst, we need to process the 2 missing values.\n<br>\nBy far the port of embarkation the most represented is Southempton. I'll replace the 2 missing values with this port.\n<br>\n<br>\nI'll perform a one-hot encoding on this one too\n","a2d7477b":"## Set up the environment","afdc20f9":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n#define hyperparameters\nn_estimators = [100, 500, 1000, 1500, 2000]\ncriterion = ['gini', 'entropy']\nmax_depth = [4, 5, 6, 7, 8, 9, 10]\nmin_samples_split = [2, 3, 4, 5, 6, 7]\nmin_samples_leaf = [1, 2, 3, 4, 5, 6, 7]\nmax_features = ['auto', 'sqrt', 'log2']\n\n#define grid search\ngrid = dict(n_estimators = n_estimators, criterion = criterion ,\n            max_depth = max_depth, min_samples_split = min_samples_split,\n           min_samples_leaf = min_samples_leaf, max_features = max_features )\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\ngrid_search = GridSearchCV(estimator = rfc, param_grid = grid, n_jobs = -1, cv = cv, scoring = 'accuracy', error_score = 0)\ngrid_result = grid_search.fit(X, y)\n\n#get the results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","45d71153":"Create a one-hot encoding for each category","db2185b0":"## Check the cleaned dataframe","950a7594":"#### Cabin\nFirst, we need to do something with the cabin feature. There are 1014 missing values out of 1309, which is huge.\n<br> \nEven if the cabin number is an important factor for the evacuation of the titanic and having a higher survival chance, there's simply too many missing values.\n<br>\nReplacing the NaN values by a category will simply be noise for the prediction model.\n<br>\nA solution could to retrieve the cabin number one by one online, or from other dataset. We won't do that here.\n<br>\n------------------------------------------------------------------------------\nThat's what I thought but no. The survival rate depends on the position of the cabin. So, I can't neglect this feature.","cd5890f7":"Detect the outliers. All Fare above 66.3 is considered an outlier. Same conclusion as for the Age feature. I will perform a binning on the Fare feature too. ","06edb281":"One-hot encoding","3b4ac8f4":"#### Create more feature","c99bc273":"Let's see how the the cabin are represented -> i.e which Pclass has which cabin. \n<br>\n<br>\nAs we can see, ABC only have 1st class. So, we can group Cabin A, B, and C in one category. ---> ABC\n<br>\nD and E have similar class and survival rate distribution. So, we can group them together. ---> DE\n<br>\nT has one value, can't figure out where this cabin is on the boat, I decided to put it with the Unknown cabins.  ---> U\n<br>\nFinally, there's no real similitude between F and G. But G has only 5 passengers. F and G will be the last category ---> FG","6c555bc3":"#### Ticket\nWe can see that there are 11 distinct class of tickets. So, I'm going to replace the ticket by their category from 1 to 11.","526fe791":"How do the training set and testing set look like?\n<br>\nThe training dataset is used to build our ML model. And the testing dataset to test the ML model.","f1c32d3f":"<b>Age * Pclass<\/b>","26aebd64":"#### Age\nIdea: \n<ol>\n    <li>Get the median age per sex and replace the missing values by this new value.<\/li>\n    <li>Each Pclass (1, 2, and 3) has a different median age value. Can get the median age per Pclass, then per gender, and replace the missing values by this new value.<\/li>\n    <li>We can even narrow down the age median --> depending on the Pclass, Sex, and Title of the passenger<\/li>\n<\/ol>\nThe plot below show a positive skew. Using the mean might give us a biased reuslt. That is why the median is a better approach.","c6dcea47":"#### Split the model with the feature selected","c5eacc2b":"Let's see how the attribute age statistically behave with a boxplot.\n<br>\nWe can see that the attribute Age has some outliers above 60 years old. So a min_max scaling is not the best idea since it's sensitive to outliers.","74b34688":"## Process the data\n### Cleaning and feature engineering","91eece29":"#### Random Forest Classifier","efec3c80":"#### KNN","789dfca1":"<b>HyperParameter Tuning<\/b>\n<br>\nIt's too long to run so I'll put a markdown for this code cell.","a08d82e5":"Create new column with the combined title","000d6f8c":"I will create 5 category of fares corresponding to 0, q25, q50, q75, max","983369bd":"## Submission ","93404af9":"Let's see how the attribute age statistically behave with a boxplot.\n<br>\nSame conclusion as for the Age boxplot. A lot of outliers, so I won't use the min_max scaling.","c5e24dbf":"## Comparison model","9836eb1c":"How's the age correlated to other features.\n<br>\nPclass has the highest correlation with the age.\n<br>\nI'll compute the age median for the Pclass AND the sex.\nI tried with the title category. But for the Master title, it gives an age median of 6 (male pclass 1) and 2 (male pclass 2) and 6 (male pclass 3). So I won't use the Title category. ","0cb340a0":"Check for missing values. \n<br>\nNo missing value.","a7e037c5":"## Prediction\n### Machine Learning prediction using a logistic regression model","9350fa6a":"Divide the df_titanic data frame in one testing set and one training set","875e2ed9":"Check the missing values in both dataset.\n<br>\nIn both dataset, <i>Age<\/i> and <i>Cabin<\/i> are the attributes with the missing data.\n<br>\nIn the test dataset we have 418 missing Survived value. Which is totally normally.","ba1e1eb9":"Perform a one-hot encoding","24259cb9":"<b>HyperParameter Tunig<\/b>","3050eae1":"The idea is to do a one-hot encoding with the title. But we have 18 unique titles, with the majority appearing less than 10 times. \n<br>\nHowever, some titles have the same meaning in different culture or country.\n<br>\nEg: Don(spanish), Sir(english), and Jonkheer(french) have the same meaning: royalty or nobility. We can then gather this in one category.\n<br>\nDue to the small amount of certain title, I will gather them under the 'Rare' title","e5d7fa8f":"#### SVM classifier","0fa0d812":"## Model improvement","3b00845c":"How many attributes and entries we have in each dataset?\n<br>\nTrain dataset -> 11 features and 1 target (survived)\n<br>\nTest dataset -> 11 features","32d38de6":"#### Sex\nPerform a one-hot encoding","6f618b7c":"Is this really the case? Let's see the % of survivors, male vs female:","4e4a469f":"## Objective\n##### What sorts of people were more likely to survive?\n<br>\nThis is a classification problem --> Binary class\n<br>\nTarget : survived\n<ul>\n    <li>0 = No<\/li>\n    <li>1 = Yes<\/li>\n<\/ul>","7dfd274f":"#### On Random Forest Classifier ","93b6c79d":"#### Pclass\nI'll do a one-hot encoding of the Pclass column","273852f1":"That's how the submission must look like:\n<br>\nThe sumbission set assume that only female passengers survived and all male passengers died.","c4815ad3":"I will separate the age in 4 categories: kids, teenagers, adults and seniors","5ebd5160":"#### Fare\nThere's just one missing value. A solution is to do the same as the Age missing values. But here, only 1 value is missing so I could find a similar passengers, compute the median value, and take this value for the missing one.\n<br>\nFirst let's see the missing value.","2abeb93c":"#### Title\nI'll first extract the title out of the name. And then do a one-hot encoding on the title column.","da0dc97b":"Correlation in the train dataframe","f10e7319":"Drop the column <i>Name<\/i> (not needed anymore) and do a one-hot encoding of the titles using get_dummies","5c683869":"Get the survival rate per cabin location.\nHas we can see, the survival rate depend a lot on the cabin location. We need to carefuly clean this feature so the model can perform as we want."}}