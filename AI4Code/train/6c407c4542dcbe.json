{"cell_type":{"1df68f3a":"code","4989701e":"code","6f17ff42":"code","ac99ace3":"code","08dc67d6":"code","64761395":"code","bf802b6a":"code","2f7ccd82":"code","453357ba":"code","25592893":"code","bd6b79e4":"code","3b42dad8":"code","e890c149":"code","0e10643c":"code","ce101866":"code","1e9c4362":"code","382ddaf6":"code","a288a414":"code","1f47acee":"code","bc4f6188":"code","1a7a8ba6":"code","d829652e":"code","a4d38a0c":"code","f670c461":"code","8b32abfc":"code","8acd99c1":"code","1923773d":"code","772d9410":"code","b550513a":"code","394fd60d":"code","19f94a23":"code","144420c3":"code","f03e1a15":"code","035d4bdb":"code","c68de082":"code","fe267175":"code","40603ab8":"code","fbbf2826":"code","16d1caff":"code","a94a8f6a":"code","a53364eb":"code","0c378fc1":"code","f7e6c215":"code","ed31b9a8":"code","7fde434d":"code","81472c5c":"code","4e080b57":"code","422a8594":"code","ffff9061":"code","395e1d8f":"code","98c03b30":"code","17fba758":"markdown","0d6d9baa":"markdown","4716db9a":"markdown","6c10b5fe":"markdown","98fa8216":"markdown","3788b19a":"markdown","d89aa538":"markdown","cd68e28e":"markdown","a2951223":"markdown","a92c12d6":"markdown","633bc2cb":"markdown","cb803b41":"markdown","a5615833":"markdown","daa34692":"markdown","cd55384a":"markdown","24fa0f3c":"markdown","bc46446c":"markdown","4537c3b9":"markdown","04cef2b9":"markdown","f0c58ebb":"markdown","675b3a59":"markdown","9ad83231":"markdown","ef9e4cf4":"markdown","04f22091":"markdown","1c8945aa":"markdown","061533a3":"markdown","4133a5f9":"markdown","5e8b7121":"markdown","d5d0a764":"markdown","35a925b9":"markdown","e070a143":"markdown","a63b9684":"markdown","041a335e":"markdown","53999efd":"markdown","038e8277":"markdown","1566e566":"markdown","ea5960a6":"markdown","0e1c2f5a":"markdown","fe692e91":"markdown","c076a128":"markdown","289af250":"markdown"},"source":{"1df68f3a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import make_column_transformer\nimport category_encoders as ce\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns    \nplt.style.use('seaborn')\n\nsns.set(font_scale=2.5) \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-apr-2021\/')","4989701e":"train = pd.read_csv(input_path \/ 'train.csv', index_col='PassengerId')\ndisplay(train.head());\ntest = pd.read_csv(input_path \/ 'test.csv', index_col='PassengerId')\n#display(test.head());\nsample_submission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='PassengerId')\ndisplay(sample_submission)","6f17ff42":"from pandas_profiling import ProfileReport\nprofile_train = ProfileReport(train, title='Pandas Train Profiling Report', html={'style':{'full_width':True}})\nprofile_train","ac99ace3":"col_to_drop=['Name','Ticket', 'Fare']","08dc67d6":"f, ax = plt.subplots(1,2,figsize=(18,8), gridspec_kw={'wspace':1})\nsns.countplot(x='Pclass', hue='Survived',data=train, ax=ax[0])\nax[0].set_title('Pclass and Survived counts')\n#ax[0].set_yticks(range(0,110,10))\nsns.countplot(x='Sex', hue='Survived',data=train, ax=ax[1])\nax[1].set_title('Sex and Survived counts')\nplt.show()\ndisplay(pd.crosstab(train['Pclass'],train['Survived']).apply(lambda r: r\/r.sum(), axis=1))\ndisplay(pd.crosstab(train['Sex'],train['Survived']).apply(lambda r: r\/r.sum(), axis=1))\ndisplay(pd.crosstab(train['Pclass'],[train['Sex'], train['Survived']]\n            ,margins=True, margins_name='Total', normalize='columns'));\n\n#sns.distplot(train,x='ParCh', hue='Survived',ax=ax[2])","64761395":"f, ax = plt.subplots(1,2,figsize=(18,8), gridspec_kw={'wspace':1})\nsns.countplot( x='Parch', hue='Survived', data= train, ax=ax[0])\nsns.countplot( x='SibSp', hue='Survived', data= train, ax=ax[1])\nplt.show()\ndisplay(pd.crosstab(train['Parch'],train['Survived']).apply(lambda r: r\/r.sum(), axis=1))\ndisplay(pd.crosstab(train['SibSp'],train['Survived']).apply(lambda r: r\/r.sum(), axis=1));","bf802b6a":"col_to_drop= col_to_drop+['Parch', 'SibSp']","2f7ccd82":"fig, ax = plt.subplots(1,2,figsize=(18,5), gridspec_kw={'wspace':0.5})\nsns.kdeplot(data=train, x=\"Age\", ax=ax[0])\nsns.kdeplot(train[train['Survived']==1]['Age'],ax=ax[1])\nsns.kdeplot(train[train['Survived']==0]['Age'],ax=ax[1])\nplt.legend(['Survived', 'Not Survived'])\nplt.show()","453357ba":"#Check the age distribution for Pclass and Sex\n#frequency shows the number of observation divided by the bin width\nf, ax= plt.subplots(1,2,figsize=(18,8), gridspec_kw={'wspace':0.5})\nsns.histplot(data=train,x='Age',  hue='Pclass', stat='frequency',binwidth=10, multiple=\"layer\", palette='Set1',  ax=ax[0])\nax[0].set_title('Pclass and Age')\n\nsns.histplot(data=train,x='Age',  hue='Sex', stat='frequency',binwidth=10, multiple=\"layer\", ax=ax[1])\nax[1].set_title('Sex and Age');\n\n","25592893":"plt.figure(figsize=(7,8))\nsns.countplot(data=train,x='Pclass',  hue='Sex')\nplt.title('Pclass and Sex')\n\np_counts=train.groupby(['Pclass','Sex','Survived']).agg({'Survived':'count'})","bd6b79e4":"f, ax= plt.subplots(1,2,figsize=(18,12), gridspec_kw={'wspace':0.5})\n#sns.violinplot(data=train, x='Pclass', y='Age', hue='Survived', palette=\"muted\",split=True, ax=ax[0])\nsns.boxplot(data=train, x='Pclass', y='Age', palette=\"muted\", ax=ax[0])\nax[0].set_title('Pclass and Age')\nax[0].set_yticks(range(0,110,10))\nax[0].legend([],[], frameon=False)\n\n#sns.violinplot(data=train, x='Sex', y='Age', hue='Survived', palette=\"muted\",split=True, ax=ax[1])\nsns.boxplot(data=train, x='Sex', y='Age', ax=ax[1])\nax[1].set_title('Sex and Age')\nax[1].set_yticks(range(0,110,10))\n#plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0, title='Survive')\nhandles, labels = ax[0].get_legend_handles_labels()\nplt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0, title='Survive',handles=handles[:], labels=['No', 'Yes'])","3b42dad8":"#train.loc[(train.Pclass==1) | (train.Sex=='male')]\np_counts[\"pct\"]= p_counts.groupby(level=[0,1]).apply(lambda x:\n                                                 100 * x \/ float(x.sum()))\n\np_counts.columns = [col + '_l0' for col in p_counts.columns.values]\np_counts = p_counts.reset_index()\np_counts","e890c149":"f, ax= plt.subplots(1,2,figsize=(18,12), gridspec_kw={'wspace':0.5})\n#sns.violinplot(data=train, x='Pclass', y='Age', hue='Survived', palette=\"muted\",split=True, ax=ax[0])\nsns.boxplot(data=train, x='Pclass', y='Age', hue='Survived', palette=\"muted\", ax=ax[0])\nax[0].set_title('Pclass and Age')\nax[0].set_yticks(range(0,110,10))\nax[0].legend([],[], frameon=False)\n\n#sns.violinplot(data=train, x='Sex', y='Age', hue='Survived', palette=\"muted\",split=True, ax=ax[1])\nsns.boxplot(data=train, x='Sex', y='Age', hue='Survived', ax=ax[1])\nax[1].set_title('Sex and Age')\nax[1].set_yticks(range(0,110,10))\n#plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0, title='Survive')\nhandles, labels = ax[0].get_legend_handles_labels()\nplt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0, title='Survive',handles=handles[:], labels=['No', 'Yes'])\n","0e10643c":"#train['Age'].fillna(train.groupby(['Pclass','Sex'])['Age'].transform('mean'))","ce101866":"train['Age'] = train.groupby(['Pclass','Sex'])['Age'].apply(lambda x: x.fillna(x.mode().iloc[0]))\ntest['Age'] = test.groupby(['Pclass','Sex'])['Age'].apply(lambda x: x.fillna(x.mode().iloc[0]))","1e9c4362":"f, ax= plt.subplots(2,2, figsize=(18,20))\nsns.countplot(data=train, x='Embarked',ax=ax[0,0])\nax[0,0].set_title('1- Embarked counts')\nsns.countplot(data=train, x='Embarked', hue='Survived',ax=ax[0,1])\nax[0,1].set_title('2- Embarked vs Survived')\nsns.countplot(data=train, x='Embarked', hue='Pclass',ax=ax[1,0])\nax[1,0].set_title('3- Embarked vs Pclass')\nsns.countplot(data=train, x='Embarked', hue='Sex',ax=ax[1,1])\nax[1,1].set_title('4- Embarked vs Sex')\n\nplt.subplots_adjust(wspace=0.5, hspace=0.4)\nplt.show()","382ddaf6":"sns.countplot(data=train.loc[train.Survived==0], x ='Cabin')","a288a414":"cabin_na= train.loc[pd.isnull(train['Cabin'])]\ncabin_na['Embarked'].value_counts()","1f47acee":"## I create two lists for the features that need to be dropped one with Embarked and the second one without.\n## I will train my first moe with a Data Set that doesn't include this category.\ncol_to_drop=col_to_drop+['Embarked', 'Cabin']","bc4f6188":"## Removing the features that we don't use in the model.\ntrain_dropped=train.drop(col_to_drop, axis=1)\ntest_dropped=test.drop(col_to_drop, axis=1)","1a7a8ba6":"# Encoding OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\ntarget= train_dropped.pop('Survived')\nct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(drop='first'), ['Pclass', 'Sex']),('scaler', MinMaxScaler(), ['Age'])], remainder='passthrough')\n#ct.fit(train_dropped1)\ntrain_encoded=ct.fit_transform(train_dropped)\ntest_encoded=ct.transform(test_dropped)\n\n##Split\n\nX_train, X_val, y_train, y_val = train_test_split(train_encoded, target, train_size=0.70) ","d829652e":"X_train.shape","a4d38a0c":"#### Scores\n\ndef print_score(classifier, X_val, y_val):\n    ##Print the predictions. \n    # classifier: the trained classifier\n    # X_val: the test inputs\n    # y_val: the test labels\n    \n    y_pred = classifier.predict(X_val) \n    score_accuracy= accuracy_score(y_val, y_pred.T)\n    y_prob=classifier.predict_proba(X_val)[:,1] # This grabs the probability that the classifier\n                                                # assign to the positive class. \n                                                \n    score_auc=roc_auc_score(y_val,y_prob)\n\n    print('The accuracy scores:')\n    print(f'{score_accuracy:0.5f}') \n    print('The auc score:')\n    print(f'{score_auc:0.5f}')\n\ndef visualize_binary_predictions(y_prob,y_pred, y_val):\n    plt.figure(figsize=(8,4))\n    plt.hist(y_prob[np.where(y_val == 0)], alpha=0.5, label='Not Surv')\n    plt.hist(y_prob[np.where(y_val == 1)], alpha=0.5, label='Surv')\n    plt.legend()\n    plt.show()\n\n\n\n    result=pd.DataFrame({'y_pred':y_pred, 'y_val':y_val.values} )\n\n\n\n    plt.figure(figsize=(8,4))\n    sns.countplot(x=result.y_pred, hue= result.y_val)\n    #plt.legend()\n    plt.show()\n    \n####Hyperparameter search\ndef hyperparameter_summary(gridresults, name=None):\n    ## hyperparameter_summary is a method to create a table with the results of hyperparameters tuning.\n    \n    params=list(gridresults['params'])#gridresults['params'] we are passing a list of dictionaries\n                                      #We use list() because we make a copy of the values. So that the oriinal is not modified\n    para_scores=[]\n    for i in range(len(params)):\n        d = dict(params[i])\n        d.update({'Score':gridresults['mean_test_score'][i] ,\n                 'Rank':gridresults['rank_test_score'][i]})\n        para_scores.append(d)\n    \n    df = pd.DataFrame(para_scores)\n    df1 = df.style.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n    df1.set_properties(**{'text-align': 'center'})\n    \n    if name!=None:\n        df1.set_caption('Table with the results of the hyperparameter search on %s' %name)\n    \n    return df1\n","f670c461":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nparameters = {\n    'penalty':['l2'],\n    'C':[0.0001,0.001,0.01,0.1,1.0]\n  }\n\n#lr.fit(X_train, y_train)\n\ngrid_lr=GridSearchCV(lr, cv=5, param_grid=parameters ,n_jobs=-1, scoring='accuracy')\n\ngrid_lr.fit(X_train,y_train)\n\nprint(grid_lr.best_params_)\n\ny_pred = grid_lr.predict(X_val) # This grabs the positive class prediction\nscore_accuracy= accuracy_score(y_val, y_pred)\n\ny_prob=grid_lr.predict_proba(X_val)[:,1]\nscore_auc=roc_auc_score(y_val,y_prob)\n\nprint('The accuracy scores:')\nprint(f'{score_accuracy:0.5f}') \nprint('The auc score:')\nprint(f'{score_auc:0.5f}')\n\nhyperparameter_summary(grid_lr.cv_results_)","8b32abfc":"y_pred = grid_lr.predict(X_val)\ny_prob = grid_lr.predict_proba(X_val)[:,1]\nvisualize_binary_predictions(y_prob, y_pred, y_val)\npd.crosstab(y_pred, y_val, rownames=['Predictions']).apply(lambda r: r\/r.sum(), axis=1)#style.background_gradient(cmap='cool')","8acd99c1":"wrong_predictions = train_dropped.loc[y_val.loc[(y_val!=y_pred)]] \nwrong_predictions['Survived'] = y_val.loc[(y_val!=y_pred)].values\nwrong_predictions","1923773d":"wrong_predictions.Survived.value_counts() \/ wrong_predictions.Survived.size","772d9410":"wrong_predictions.Pclass.value_counts() \/ wrong_predictions.Pclass.size","b550513a":"lr = LogisticRegression()\nlr.set_params(**grid_lr.best_params_)\nlr.fit(train_encoded, target)\ny_pred_LR = lr.predict(test_encoded) \nsub_LR = pd.DataFrame({\"Survived\": y_pred_LR} , index= test.index)\nsub_LR.to_csv(\"submission_LogistRegression.csv\")","394fd60d":"sub_LR","19f94a23":"## Choose classifier\nclf = RandomForestClassifier()\npipeline=make_pipeline(clf)\nparameters = {\n  'randomforestclassifier__n_estimators':[100,200,300],\n  'randomforestclassifier__max_depth': [5,7,11,15]\n  }\n\ngrid_RF=GridSearchCV(pipeline, cv=5, n_jobs=-1, param_grid=parameters ,scoring='accuracy')\n\ngrid_RF.fit(X_train,y_train)\n\nprint(grid_RF.best_params_)\n\ny_pred = grid_RF.predict(X_val) # This grabs the positive class prediction\nscore_accuracy= accuracy_score(y_val, y_pred)\n\ny_prob = grid_RF.predict_proba(X_val)[:, 1] # This grabs the positive class prediction\nscore_auc = roc_auc_score(y_val, y_prob)\n\n\n\nprint('The accuracy scores:')\nprint(f'{score_accuracy:0.5f}') \nprint('The auc score:')\nprint(f'{score_auc:0.5f}')\n\nhyperparameter_summary(grid_RF.cv_results_)","144420c3":"clf_RF = RandomForestClassifier()\npipeline=make_pipeline(clf_RF)\npipeline.set_params(**grid_RF.best_params_)\npipeline.fit(X_train, y_train)\ny_pred = grid_RF.predict(X_val)\ny_prob = grid_RF.predict_proba(X_val)[:,1]\nvisualize_binary_predictions(y_prob, y_pred, y_val)\npd.crosstab(y_pred, y_val, rownames=['Predictions']).apply(lambda r: r\/r.sum(), axis=1)#.style.background_gradient(cmap='cool')","f03e1a15":"clf_RF = RandomForestClassifier()\npipeline=make_pipeline(clf_RF)\npipeline.set_params(**grid_RF.best_params_)\npipeline.fit(train_encoded, target)\ny_pred_RF = pipeline.predict(test_encoded) \nsub_RF = pd.DataFrame({\"Survived\": y_pred_RF} , index= test.index)\nsub_RF.to_csv(\"submission_RandomForest.csv\")\n\n","035d4bdb":"## Choose classifier\n\nfrom xgboost import XGBClassifier\n\n\nclf = XGBClassifier(use_label_encoder=False)\npipeline=make_pipeline(clf)\nparameters = {\n  'xgbclassifier__n_estimators':[50,100,150,200],\n  'xgbclassifier__learning_rate': [0.01,0.001,0.0001],\n  'xgbclassifier__max_depth' : [2, 4, 6, 8],\n}\n\ngrid_XGB=GridSearchCV(pipeline, cv=5, n_jobs=-1, param_grid=parameters ,scoring='accuracy')\n\ngrid_XGB.fit(X_train,y_train)\n\nprint(grid_XGB.best_params_)\n\ny_pred = grid_XGB.predict(X_val) # This grabs the positive class prediction\nscore_accuracy= accuracy_score(y_val, y_pred)\n\ny_prob = grid_XGB.predict_proba(X_val)[:, 1] # This grabs the positive class prediction\nscore_auc = roc_auc_score(y_val, y_prob)\n\nprint('The accuracy scores:')\nprint(f'{score_accuracy:0.5f}') \nprint('The auc score:')\nprint(f'{score_auc:0.5f}')\n\nhyperparameter_summary(grid_XGB.cv_results_)","c68de082":"y_pred = grid_XGB.predict(X_val)\ny_prob = grid_XGB.predict_proba(X_val)[:,1]\nvisualize_binary_predictions(y_prob, y_pred, y_val)\npd.crosstab(y_pred, y_val, rownames=['Predictions']).apply(lambda r: r\/r.sum(), axis=1)","fe267175":"clf_XGB = XGBClassifier(use_label_encoder=False)\npipeline=make_pipeline(clf_XGB)\npipeline.set_params(**grid_XGB.best_params_)\npipeline.fit(train_encoded, target)\ny_pred_XGB = pipeline.predict(test_encoded) \nsub_XGB = pd.DataFrame({\"Survived\": y_pred_XGB} , index= test.index)\nsub_XGB.to_csv(\"submission_XGB.csv\")","40603ab8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom numpy.random import seed\nseed(1)\ntf.random.set_seed(89)\n#set_random_seed(2)\n\n\ninput_shape = [X_train.shape[1]]\n\nmodel = keras.Sequential(\n[layers.Dense(units=4, activation = 'relu', input_shape=input_shape),\n layers.Dropout(0.25),\n layers.Dense(units=1, activation= 'sigmoid')])\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=['binary_accuracy'])\n    \n\nearly_stopping=callbacks.EarlyStopping(min_delta=0.01,\n                                   patience=20,\n                                   restore_best_weights=True)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data = (X_val, y_val),\n    batch_size=1000,\n    epochs=60,\n    callbacks=[early_stopping]\n)\n\nhistory_df= pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","fbbf2826":"y_prob = model.predict(X_val) # This grabs the positive class prediction\ny_pred = np.copy(y_prob) # This grabs the positive class prediction\n#\ny_pred[y_pred <= 0.5] = 0.\ny_pred[y_pred > 0.5] = 1.\nscore_accuracy= accuracy_score(y_val, y_pred)\n\n\nscore_auc = roc_auc_score(y_val, y_prob)\n\n\n\nprint('The accuracy scores:')\nprint(f'{score_accuracy:0.5f}') \nprint('The auc score:')\nprint(f'{score_auc:0.5f}')\n","16d1caff":"plt.figure(figsize=(8,4))\nplt.hist(y_prob[np.where(y_val == 0)], alpha=0.5, label='Not Surv')\nplt.hist(y_prob[np.where(y_val == 1)], alpha=0.5, label='Surv')\nplt.legend()\nplt.show()\n\n\n\nresult=pd.DataFrame({'y_pred':y_pred[:,0], 'y_val':y_val.values} )\n\n\n\nplt.figure(figsize=(8,4))\nsns.countplot(x=result.y_pred, hue= result.y_val)\n#plt.legend()\nplt.show()\npd.crosstab(result['y_pred'], result['y_val']).apply(lambda r: r\/r.sum(), axis=1)#.style.background_gradient(cmap='cool')","a94a8f6a":"\nhistory = model.fit(\n    train_encoded, target,\n    batch_size=1000,\n    epochs=60,\n    callbacks=[early_stopping]\n)\n\n\ny_prob = model.predict(test_encoded) \ny_pred_NN = np.copy(y_prob) # This grabs the positive class prediction\n#\n\n","a53364eb":"y_pred_NN[y_pred_NN <= 0.5] = 0.\ny_pred_NN[y_pred_NN > 0.5] = 1.\n","0c378fc1":"sub_NN = pd.DataFrame({\"Survived\":y_pred_NN.T[0]} , index= test.index)\nsub_NN.to_csv(\"submission_NN.csv\")\n","f7e6c215":"col_to_drop_1=['Name','Ticket', 'Fare','Parch', 'SibSp', 'Cabin']\n\n## Removing the features that we don't use in the model.\ntrain_dropped=train.drop(col_to_drop_1, axis=1)\ntest_dropped=test.drop(col_to_drop_1, axis=1)\n\ntrain_dropped.Embarked=train_dropped.Embarked.fillna('U')\ntest_dropped.Embarked=test_dropped.Embarked.fillna('U')\n\ntarget= train_dropped.pop('Survived')\nct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(drop='first'), ['Pclass', 'Sex', 'Embarked']),('scaler', MinMaxScaler(), ['Age'])], remainder='passthrough')\n#ct.fit(train_dropped1)\ntrain_encoded=ct.fit_transform(train_dropped)\ntest_encoded=ct.transform(test_dropped)\n\n##Split\n\nX_train, X_val, y_train, y_val = train_test_split(train_encoded, target, train_size=0.70) \n\nlr = LogisticRegression()\nparameters = {\n    'penalty':['l2'],\n    'C':[0.0001,0.001,0.01,0.1,1.0]\n  }\n\n#lr.fit(X_train, y_train)\n\ngrid_lr=GridSearchCV(lr, cv=5, param_grid=parameters ,n_jobs=-1, scoring='accuracy')\n\ngrid_lr.fit(X_train,y_train)\n\nprint(grid_lr.best_params_)\n\ny_pred = grid_lr.predict(X_val) # This grabs the positive class prediction\nscore_accuracy= accuracy_score(y_val, y_pred)\n\ny_prob=grid_lr.predict_proba(X_val)[:,1]\nscore_auc=roc_auc_score(y_val,y_prob)\n\nprint('The accuracy scores:')\nprint(f'{score_accuracy:0.5f}') \nprint('The auc score:')\nprint(f'{score_auc:0.5f}')\n\nhyperparameter_summary(grid_lr.cv_results_)","ed31b9a8":"wrong_predictions = train_dropped.loc[y_val.loc[(y_val!=y_pred)]] \nwrong_predictions['Survived'] = y_val.loc[(y_val!=y_pred)].values\nwrong_predictions\n\nwrong_predictions.Pclass.value_counts() \/ wrong_predictions.Pclass.size","7fde434d":"sns.boxplot(data=wrong_predictions, x='Sex', y='Age', hue='Survived')","81472c5c":"train2 = pd.read_csv(input_path \/ 'train.csv', index_col='PassengerId')\ndisplay(train.head());","4e080b57":"col_to_drop_1=['Name','Ticket', 'Fare','Parch', 'SibSp', 'Cabin']\n\n## Removing the features that we don't use in the model.\ntrain_dropped=train.drop(col_to_drop_1, axis=1)\ntrain_dropped=train_dropped.dropna(subset=['Age'])\ntest_dropped=test.drop(col_to_drop_1, axis=1)\n\ntrain_dropped.Embarked=train_dropped.Embarked.fillna('U')\ntest_dropped.Embarked=test_dropped.Embarked.fillna('U')\n\ntarget= train_dropped.pop('Survived')\nct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(drop='first'), ['Pclass', 'Sex', 'Embarked']),('scaler', MinMaxScaler(), ['Age'])], remainder='passthrough')\n#ct.fit(train_dropped1)\ntrain_encoded=ct.fit_transform(train_dropped)\ntest_encoded=ct.transform(test_dropped)\n\n##Split\n\nX_train, X_val, y_train, y_val = train_test_split(train_encoded, target, train_size=0.70) \n\nlr = LogisticRegression()\nparameters = {\n    'penalty':['l2'],\n    'C':[0.0001,0.001,0.01,0.1,1.0]\n  }\n\n#lr.fit(X_train, y_train)\n\ngrid_lr=GridSearchCV(lr, cv=5, param_grid=parameters ,n_jobs=-1, scoring='accuracy')\n\ngrid_lr.fit(X_train,y_train)\n\nprint(grid_lr.best_params_)\n\ny_pred = grid_lr.predict(X_val) # This grabs the positive class prediction\nscore_accuracy= accuracy_score(y_val, y_pred)\n\ny_prob=grid_lr.predict_proba(X_val)[:,1]\nscore_auc=roc_auc_score(y_val,y_prob)\n\nprint('The accuracy scores:')\nprint(f'{score_accuracy:0.5f}') \nprint('The auc score:')\nprint(f'{score_auc:0.5f}')\n\nhyperparameter_summary(grid_lr.cv_results_)","422a8594":"wrong_predictions = train.loc[y_val.loc[(y_val!=y_pred)]] \nwrong_predictions['Survived'] = y_val.loc[(y_val!=y_pred)].values\nwrong_predictions\n\ndisplay(wrong_predictions.Cabin.value_counts())# \/ wrong_predictions.Cabin.size\nprint(pd.isnull(wrong_predictions.Cabin).sum())\ndisplay(wrong_predictions.Pclass.value_counts())\nwrong_predictions.shape\n#sns.boxplot(data=wrong_predictions.loc[wrong_predictions['Pclass']==1], x='Survived', y='Age')","ffff9061":"y_pred = grid_lr.predict(X_val)\ny_prob = grid_lr.predict_proba(X_val)[:,1]\nvisualize_binary_predictions(y_prob, y_pred, y_val)\ndisplay(pd.crosstab(y_pred, y_val, rownames=['Predictions']).apply(lambda r: r\/r.sum(), axis=1))#style.background_gradient(cmap='cool')\n\n\nlr = LogisticRegression()\nlr.set_params(**grid_lr.best_params_)\nlr.fit(train_encoded, target)\ny_pred_LR = lr.predict(test_encoded) \nsub_LR = pd.DataFrame({\"Survived\": y_pred_LR} , index= test.index)\nsub_LR.to_csv(\"submission_LogistRegression_withEmbarked.csv\")","395e1d8f":"train2 = pd.read_csv(input_path \/ 'train.csv', index_col='PassengerId')\ndisplay(train.head());","98c03b30":"col_to_drop_2=['Name','Ticket','Parch', 'SibSp', 'Cabin']\n\n## Removing the features that we don't use in the model, and the na in Age from the training set\ntrain_dropped=train2.drop(col_to_drop_2, axis=1)\ntest_dropped=test.drop(col_to_drop_2, axis=1)\n\ntrain_dropped.Embarked=train_dropped.Embarked.fillna('U')\ntest_dropped.Embarked=test_dropped.Embarked.fillna('U')\n\ntrain_dropped['Fare'] = train2['Fare'].fillna(train_dropped.groupby(['Pclass','Embarked'])['Fare'].transform('mean'))\ntest_dropped['Fare'] = test['Fare'].fillna(test_dropped.groupby(['Pclass','Embarked'])['Fare'].transform('mean'))\ntarget= train_dropped.pop('Survived')\nct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(drop='first'), ['Pclass', 'Sex', 'Embarked']),('scaler', MinMaxScaler(), ['Age', 'Fare'])], remainder='passthrough')\n#ct.fit(train_dropped1)\ntrain_encoded=ct.fit_transform(train_dropped)\ntest_encoded=ct.transform(test_dropped)\n\n##Split\n\nX_train, X_val, y_train, y_val = train_test_split(train_encoded, target, train_size=0.70) \n\nlr = LogisticRegression()\nparameters = {\n    'penalty':['l2'],\n    'C':[0.0001,0.001,0.01,0.1,1.0]\n  }\n\n#lr.fit(X_train, y_train)\n\ngrid_lr=GridSearchCV(lr, cv=5, param_grid=parameters ,n_jobs=-1, scoring='accuracy')\n\ngrid_lr.fit(X_train,y_train)\n\nprint(grid_lr.best_params_)\n\ny_pred = grid_lr.predict(X_val) # This grabs the positive class prediction\nscore_accuracy= accuracy_score(y_val, y_pred)\n\ny_prob=grid_lr.predict_proba(X_val)[:,1]\nscore_auc=roc_auc_score(y_val,y_prob)\n\nprint('The accuracy scores:')\nprint(f'{score_accuracy:0.5f}') \nprint('The auc score:')\nprint(f'{score_auc:0.5f}')\n\nhyperparameter_summary(grid_lr.cv_results_)","17fba758":"## **1.1 Profile Report** ","0d6d9baa":"## **4.2 Visualization of the predictions**","4716db9a":"# Load data","6c10b5fe":"## **3.3 Submit predictions**","98fa8216":"<font size='4'> Let's build a NN model. <br>\nBefore doing that, I need to scale the age column to use it together with the NeuralNetwork. <br>\nI will keep this section short and I will not make a hyperparameter search for the NN. <\/font>","3788b19a":"# **7. Improving the accuracy score**","d89aa538":"<font size=\"4\">In the third class the number of males is much larger than the number of females. This is not true for the higher classes, first and second. For these two the difference is not as significant as for the third class and the number of females seems to be slightly higher. <\/font>\n","cd68e28e":"# **5. XGBoost Classifier**","a2951223":"## **3.2 Visualization of the predictions**","a92c12d6":"<font size=\"4\"> These two figures shows the incidence of the passanger age and Pclass(or Sex) on the probability of Surviving. \nIn the first figure, the mean age of the survivor and the not survivor  very similar to the mean age of the class they belong (around 50 for the first class, 40 for the second, and 30 for the third).\nThis can also be another indication that the Age is not a very strong predictive feature <\/font>","633bc2cb":"## **4.1 Building and training the model**","cb803b41":"<font size = '4'> My first attempt to improve the score is to include some of the features I have previously excluded. For example, I try first to include Embarked. Embarked has some missing value. We can create a new class for this value, however,   <\/font>","a5615833":"### **1.4.3 Imputation of missing values**","daa34692":"## **5.2 Visualization of the predictions**","cd55384a":"<font size = '4'> As expected 'Fare' doesn't seem to add anything to the model. <\/font> ","24fa0f3c":"## **1.6 Cabin** \n\n<font size=\"4\"> In a first approach I try to not use this feature to train the model because there are too many missing values for this category. <br>\nIn the attempt to increase the score of the accuracy in the last section of the notebook I try to include some of the features that I excluded in the first place.<\/font>","bc46446c":"# **4. Random Forest Classifier**","4537c3b9":"<font size=\"4\"> To  use the age feature we need to take care of the missing values. We know there are missing values from the profile report at the beginning of the EDA. <br> From the previous plot we see that the different classes show a different mean age. So for the missing values we can use the mean value for the different Pclasses. We also separate the impuation according to the different gender using the mean value for the specific class and gender. \n<\/font>\n****","04cef2b9":"## **5.1 Building and training the model**","f0c58ebb":"<font size=\"4\"> In the first table are summarized the survival rates for the different classes. If a passanger belongs to the first class has a 60% of Survival. The percentage here showed does not take into account the Sex of the passanger.\nIn the second table the survival rate according to the sex of the passanger. Here, it is clear that being a woman increased the chances of surviving. In this case, the class of the [assanger is not taken into account.\nIn the final table we broke down the first one: we show the rate of survival according to the class differentiating man from women. Almost 60% of the man who did not survive are from the third class. While the majority of the man surviving belongs to the first class. The difference in survival rate between classes is visible for women too. However, in this case the difference in rate is smaller. <\/font>  ","675b3a59":"### **1.4.2 Age and Pclass\/Sex vs Survived**","9ad83231":"<font size='4'> The variable ParCh and SibSp do not seem to have a very strong predictive power. In fact, not having 0 sibblings or parents doesn't signficantly improve the chances of surviving.<\\font>","ef9e4cf4":"<font size=\"4\"> I remove the feature that couldn't have a reasonable effect on the dependent variable: ['Name', 'Ticket', 'Fare']. 'Fare' is obviously related to other variables like Pclass and Embarked \nI will show in the following the exploratory analysis to see how the features influence the the independent variable.<\\font>","04f22091":"## **1.5 Embarked** ","1c8945aa":"<font size=\"4\"> From figure 1 we understand that the larger portion of passanger embarkes on the ship from S. From figure 2 we noticed that the frequency of survivor embarked from S is low. This observation make us think that passangers from Southampton were damned :P. However, figure 3 and 4 shows that a large portion of the third class embarked from S and that also the majority of male embarked from there. So, the embarked information doesn't seem to add extra information. <br>  Ideally, we could train models with and without this feature. But, I will train models only without this feature. <\/font>","061533a3":"# **2. Preprocessing of the data**\n\n<font size =\"4\">Before starting with the development of the model, let's prepare the data base for the training.\nWe split the training set in two groups training and validation. All the data sets (the train, the validation, and the test) should not include the columns we have decided to exclude.<br>\nAlso ery important for someof the models we will use we need to encode the categorical features that have anon numerical values.<\/font>\n","4133a5f9":"## **3.1 Building and training the model**","5e8b7121":"## **1.2 Pclass and Sex**","d5d0a764":"<font size = \"4\">From the graph on the right it is visible a change in the probability of surviving at the age of 40. This is quite curious, if somebody thinks that younger people should have higher chances to survive. But, probably the age distribution changes across other categories which also we have seen have a strong influence on the prediction (Sex and Pclass). So for example, the older people are mostly in the Pclass which has we have seen have higher chances to survive.<\/font>","35a925b9":"<font size = '4'> I will try now adding the feature 'Fare'. My reason to exclude it in first place was that how much you pay the ticket should not have a direct influence on the survival rate. The ticket price is related to the passanger class already included in the model and maybe on the embarked station which now we have added. <\/font> ","e070a143":"## **5.3 Submit XGB predictions**\n","a63b9684":"## **1.3 Parch and SibSp**","041a335e":"# **1. Exploratory Data Analysis**","53999efd":"# **6. Neural Network**","038e8277":"## **4.3 Submit Random Forest predictions**\n\n<font size='4'> We first train the model on the entire training data set we the best parameter obtain with the GridSearchCV. <br>\nAfter, we make the predictions on the test and we submit it.<\/font> ","1566e566":"<font size=\"4\"> This might explain the change in survivor with the age.\nThe third class has a very low mean age compared to the other classes. However, third class passanger have a lower chance of surviving. Furthermore the higher percentage of male in the third class make the chances of this group of young passanger even lower. <\/font>","ea5960a6":"## **1.4 Age**","0e1c2f5a":"# **3. Logistic Regression**\n\nUsing GridSearchCV we will train the model using a cross validation method and also we select the best hyperparameters for each model with a grid search alghoritm.\nThe classifier needs to predic if the passanger, given certain features, will survive or not the catastrophe. I think for this problem it is better to train the model optimizing the accuracy. We care that the total of true positive (TP) and true negative (TN) over the toatl positive and negative outcomes is as higher as possible. ","fe692e91":"<font size = '4'> The accuracy increase in particular 3% more of the predicted positive are correct. <\/font>","c076a128":"<font size= '4'> The wrong predictions are equally distributed between positive and negative labels <\/font>","289af250":"<font size = '4'> The wrong predictions seems to corresponds to the passangers for who we imputed the Age.\nSpecifically, the wrong predictions corresponds to male of the 1st and 3rd class. <br> \nI will  try to train the model using a dataset where the na are removed from the training set, instead of being imputed. I will leave the imputation in the test dataset.<\/font>"}}