{"cell_type":{"494f54b4":"code","cee9fda9":"code","d0a6f567":"code","21997b53":"code","c95ad6df":"code","fc67a271":"code","90c356fb":"code","bdd83fd7":"code","026c847e":"code","3c5e8b43":"code","79aeda96":"code","2d7ec096":"code","9cfee4c1":"code","9e492e33":"code","4373b3c8":"code","75961c35":"code","c9566b93":"code","b88d64ff":"code","a5b6b030":"code","339999bd":"code","aead69a8":"code","d8082831":"code","6d991fc7":"markdown","d738e68d":"markdown","77a851ed":"markdown","ef80a544":"markdown","64d2afde":"markdown","69a18c62":"markdown","bb827831":"markdown","3c144e94":"markdown","e91ecff3":"markdown","518f928b":"markdown","4be02f5c":"markdown","2b99c959":"markdown"},"source":{"494f54b4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import linear_model, dummy, metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import *\n\nimport time\nfrom inspect import signature\n","cee9fda9":"from google.colab import files\nuploaded = files.upload()","d0a6f567":"# # Data Exploration\ndf_test = pd.read_csv('bank-test.csv')\ndf_train = pd.read_csv('bank-train.csv')\n\ndf_test.head()\ndf_list = [df_train,df_test]\nprint(df_train.columns)\nfor df in df_list:\n  # df.replace('unknown', np.nan, inplace=True)\n  df.replace({'YES': 1, 'NO': 0}, inplace=True)\n\n  # due to irrelevancy\n  # df.drop(['pdays','contact','poutcome','previous'],inplace=True, axis=1)\n  # due to multicollinearity or no association\n  # df.drop(['euribor3m','nr.employed'],inplace=True, axis=1)\n  # due to impracticality\n  df.drop(['duration','default'],inplace=True, axis=1)\n  # df.drop(['month','day_of_week'],inplace=True, axis=1)\n  \n  # df.dropna(inplace=True, axis=0)\n  \n# print(df_train.info())\nprint(df_train.y.value_counts()\/len(df_train.y))\n\nprint(list(df_train.columns))\n# for feature in list(df_train.columns.values):\n#   print('\\n'+str(feature)+'\\n{}'.format(df_train[str(feature)].value_counts()))\n\n","21997b53":"# plot corr map\ncm = df_train.corr()\nplt.figure(figsize=(10,10))\ndef r_squared(x):\n  return x**2\n\ncm2 = cm.apply(r_squared)\ncmap = sns.diverging_palette(240, 10, sep=20, as_cmap=True)\nsns.heatmap(cm2,vmin=-1,vmax=1,fmt='.3f',cmap=cmap,square=True,annot=True)","c95ad6df":"# plt.figure(figsize=(20,5))\n\n\n# ax1 = plt.add_subplot(411)\n# month_order = ['jan','feb','mar','apr','may','jun','jul','aug','sept','oct','nov','dec']\n# day_order = ['mon','tue','wed','thu','fri']\n# sns.catplot(x='month', hue='y', kind=\"count\", order=month_order,data=df_train)\n\n# ax2 = plt.subplot(412)\n# sns.catplot(x='day_of_week', hue='y', kind=\"count\", order=day_order,data=df_train)\n\n\n# edu_order = ['illiterate','basic.4y','basic.6y','basic.9y','high.school','professional.course','university.degree']\n# plt.subplot(4,1,3)\n# sns.catplot(x='education', hue='y', kind=\"count\", order=edu_order, data=df_train)\n# plt.show()\n\n# plt.subplot(4,1,4)\n# table=pd.crosstab(df_train.month,df_train.y)\n# table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n\nsns.distplot(df_train['euribor3m'])\n# sns.scatterplot(x='euribor3m',y='y',data=df_train)\nprint(df.loc[df[\"euribor3m\"] < 3, 'y'].mean())\nprint(df.loc[df[\"euribor3m\"] > 3, 'y'].mean())\n\n# plt.show()\n","fc67a271":"# encoding vars\ny = df_train.y\ndf_train.drop('y',inplace=True,axis=1)\n# df_train.drop('id',inplace=True,axis=1)\n# df_test.drop('id',inplace=True,axis=1)\n\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n# from sklearn.impute import SimpleImputer\n\n# imp = SimpleImputer(missing_values = np.nan, strategy='most_frequent')\n# df_train = imp.fit_transform(df_train)\n\nfor df in df_list:\n#   df.loc[df[\"job\"] == 'unknown',\"job\"] = df[\"job\"].value_counts().index[0]\n#   df.loc[df[\"marital\"] == 'unknown',\"marital\"] = df[\"marital\"].value_counts().index[0]\n#   df.loc[df.education == 'unknown',\"education\"] = df.education.value_counts().index[0]\n#   df.loc[df.housing == 'unknown',\"housing\"] = df.housing.value_counts().index[0]\n#   df.loc[df.loan == 'unknown',\"loan\"] = df.loan.value_counts().index[0]\n\n  edu_order\n  edu_dict = dict(zip(edu_order, range(0,7)))\n  df['education'] = df['education'].map(edu_dict).fillna(6,inplace=True)  \n\n#   df.loc[df[\"euribor3m\"] < 3, 'euribor3m'] = 0\n#   df.loc[df[\"euribor3m\"] > 3, 'euribor3m'] = 1\n  \n  \nX = pd.get_dummies(df_train.iloc[:,1:-1], drop_first=False, dummy_na=False)       # exclues id & y\nX_sub = pd.get_dummies(df_test.iloc[:,1:-1], drop_first=False, dummy_na=False)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(X_test.shape)\nprint(X_sub.shape)\n\nX_train, y_train = resample(method=None)\n\n# checking dimensions\na = list(X.columns.values)\nb = list(X_sub.columns.values)\nfor i in a:\n  if i not in b:\n    print(i)","90c356fb":"def resample(method=None):\n  if method == 'oversample':\n    from imblearn.over_sampling import SMOTE\n\n    os = SMOTE(random_state=42)\n    columns = X_train.columns\n    os_data_X,os_data_y=os.fit_resample(X_train, y_train)\n    os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n    os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])\n    return os_data_x, os_data_y\n          \n  elif method == 'undersample':\n    from imblearn.under_sampling import RandomUnderSampler\n  \n    us = RandomUnderSampler(random_state=42)\n    columns = X_train.columns\n    us_data_X,us_data_y=us.fit_resample(X_train, y_train)\n    u_data_X = pd.DataFrame(data=us_data_X,columns=columns )\n    us_data_y= pd.DataFrame(data=us_data_y,columns=['y'])\n    return us_data_x, us_data_y\n\n  elif method == 'combination':\n    from imblearn.combine import SMOTEENN\n          \n    cs = SMOTEENN(random_state=42)\n    columns = X_train.columns\n    cs_data_X,cs_data_y=os.fit_resample(X_train, y_train)\n    cs_data_X = pd.DataFrame(data=cs_data_X,columns=columns )\n    cs_data_y= pd.DataFrame(data=cs_data_y,columns=['y'])\n    return cs_data_x, cs_data_y\n  \n  else:\n    return X_train, y_train\n\n          \n#   bp = sns.countplot(x=cs_data_y.values.ravel())\n#   plt.title(\"Class Distribution of Train Dataset\")\n#   plt.show()","bdd83fd7":"from sklearn.svm import SVC\n\nstart = time.time()\nsvm = SVC(kernel=\"rbf\", random_state=42)\nsvm.fit(X_train,y_train)\nend = time.time()\n\nprint(start-end)\n","026c847e":"y_hat = svm.predict(X_test)\n# y_pred_proba = svm.predict_proba(X_test)\n\nprint(classification_report(y_test, y_hat, digits=4))\nprint(confusion_matrix(y_test, y_hat))\n# metrics(y_hat,y_pred_proba, plot=True)\n","3c5e8b43":"from sklearn.neural_network import MLPClassifier\nnn = MLPClassifier(activation= 'tanh', alpha=0.05, solver='adam', random_state=42)\nnn.fit(X_train,np.ravel(y_train))\n\n# parameter_space = {\n#     'activation': ['tanh', 'relu'],\n#     'solver': ['sgd', 'adam'],\n#     'alpha': [0.0001, 0.05],\n# }\n\n# start = time.time()\n\n# from sklearn.model_selection import GridSearchCV\n# clf = GridSearchCV(nn, parameter_space, n_jobs=-1, cv=3)\n# clf.fit(X_train, y_train)\n\n# end = time.time()\n# print(end - start)\n\n\n","79aeda96":"# print(clf.best_params_,clf.best_score_)\n\ny_hat = nn.predict(X_test)\ny_pred_proba = nn.predict_proba(X_test)\n\nmetrics(y_hat,y_pred_proba, plot=True)\n\n\n# y_hat = clf.predict(X_test)\n# y_pred_proba = clf.predict_proba(X_test)\n\n# metrics(y_hat,y_pred_proba, plot=False)\n\n\n# means = clf.cv_results_['mean_test_score']\n# stds = clf.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n#     print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n# print('Best parameters found:\\n', clf.best_params_)\n\n\n","2d7ec096":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","9cfee4c1":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n\nmodel = Sequential()\n# model.add(Dense(10, input_dim=14, activation='tanh'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n\nx_train_keras = np.array(X_train)\ny_train_keras = np.array(y_train)\n#print(x_train_keras.shape)\ny_train_keras = y_train_keras.reshape(y_train_keras.shape[0], 1)\n\nmodel.fit(np.array(x_train_keras), np.array(y_train_keras), epochs=10, batch_size=128, shuffle=True)\n\ny_hat  = model.predict_proba(X_test)\ny_pred = (y_hat>0.5)\n\ny_hat.describe()\n\nmetrics(y_pred,y_hat, plot=True)","9e492e33":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\n","4373b3c8":"y_hat = lr.predict(X_test)\ny_pred_proba = lr.predict_proba(X_test)\n\nmetrics(y_hat,y_pred_proba, plot=False)\n\n\n","75961c35":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=21)\nrf.fit(X_train,y_train)\n\n","c9566b93":"important_features = pd.DataFrame({'Importance': rf.feature_importances_}, index = X_train.columns).sort_values('Importance', ascending = False)\nimportant_features = pd.DataFrame({\"features\" : important_features.index})\n\nimp_fea_array = important_features.values[:10]\nimp_fea_array = imp_fea_array.ravel()\nprint(imp_fea_array)\n\ny_hat = rf.predict(X_test)\ny_pred_proba = rf.predict_proba(X_test)\n\n\nmetrics(y_hat,y_pred_proba, plot=True)\n\n","b88d64ff":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(random_state=21)\ngb.fit(X_train,y_train)\n\n# parameter_space = {\n#     'n_estimators': range(80,160,10),\n#     'max_depth': range(5,16,3),\n#     'criterion':['friedman_mse']\n# }\n\n\n# start = time.time()\n\n# from sklearn.model_selection import GridSearchCV\n# clf = GridSearchCV(gb, param_grid = parameter_space, scoring='f1_weighted', n_jobs=-1, iid=False, cv=3)\n# clf.fit(X_train, y_train)\n\n# end = time.time()\n# print(end - start)","a5b6b030":"#clf.grid_scores_, clf.best_params_, clf.best_score_\n\n","339999bd":"y_hat = gb.predict(X_test)\ny_pred_proba = gb.predict_proba(X_test)\n\nmetrics(y_hat,y_pred_proba)","aead69a8":"def metrics(y_hat, y_pred_proba, plot=True):\n  report(y_hat)\n  if plot:\n    plot_cm(y_hat)\n  report_ROC(y_pred_proba,plot=plot)\n  report_PRC(y_pred_proba,plot=plot)\n  \ndef plot_cm(y_hat):\n  cm = confusion_matrix(y_test, y_hat)\n  fig, ax = plt.subplots(figsize = (7,7))\n  sns.heatmap(pd.DataFrame(cm.T), annot=True, annot_kws={\"size\": 15}, vmin=0, vmax=2000, cmap=\"Purples\", fmt='.0f', linewidths=1, linecolor=\"white\", cbar=False,\n             xticklabels=[\"0\",\"1\"], yticklabels=[\"0\",\"1\"])\n  plt.ylabel(\"Predicted\", fontsize=15)\n  plt.xlabel(\"Actual\", fontsize=15)\n  ax.set_xticklabels([\"0\",\"1\"], fontsize=13)\n  ax.set_yticklabels([\"0\",\"1\"], fontsize=13)\n  plt.title(\"Confusion Matrix\", fontsize=15)\n  plt.show()\n  \ndef report(y_hat):\n  report = classification_report(y_test,y_hat,digits=4,output_dict=True)\n  \n  print(classification_report(y_test,y_hat,digits=4))\n  print()\n  print(\"Accuracy = {0:0.3f}\".format(report[\"accuracy\"]))\n  print(\"Precision = {0:0.3f}\".format(report[\"1\"][\"precision\"]))\n  print(\"Specificity = {0:0.3f}\".format(report[\"0\"][\"recall\"]))\n  print(\"Sensitivity = {0:0.3f}\".format(report[\"1\"][\"recall\"]))\n  print(\"F1-score = {0:0.3f}\".format(report[\"1\"][\"f1-score\"]))\n  \ndef report_ROC(y_pred_proba,plot=True):\n  fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:,1])\n  \n  if plot:\n    fig, ax = plt.subplots(figsize = (10,7))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.plot(fpr, tpr)\n    plt.fill_between(fpr, tpr, alpha=0.2, color='b')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve: AUC={0:0.3f}'.format(roc_auc_score(y_test,y_pred_proba[:,1])))\n    plt.show()\n  \n  print('AUROC={0:0.3f}'.format(roc_auc_score(y_test,y_pred_proba[:,1])))\n  \ndef report_PRC(y_pred_proba, plot=True):\n  precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba[:,1])\n  average_precision = average_precision_score(y_test, y_pred_proba[:,1])\n\n  if plot:\n    #its a step function so plotting is different \n    fig, ax = plt.subplots(figsize = (10,7))\n    step_kwargs = ({'step': 'post'}\n                   if 'step' in signature(plt.fill_between).parameters\n                   else {})\n    plt.step(recall, precision, color='orange', alpha=1,\n             where='post')\n    plt.fill_between(recall, precision, alpha=0.2, color='orange', **step_kwargs)\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title('Precision-Recall curve: Average Precision={0:0.3f}'.format(average_precision))\n    plt.show()\n    \n  print('Average Precision={0:0.3f}'.format(average_precision))\n","d8082831":"gb.fit(X,y)\ny_sub_hat = gb.predict(X_sub)\n\nsubmission = pd.concat([df_test.id, pd.Series(y_sub_hat)], axis = 1)\nsubmission.columns = ['id', 'Predicted']\nsubmission.to_csv('submission2.csv', index=False)\n\nfrom google.colab import files\nfiles.download(\"submission2.csv\")","6d991fc7":"## Helper Functions","d738e68d":"##Neural Network (keras)","77a851ed":"## Preprocessing Pt. 1","ef80a544":"## SVM\n","64d2afde":"## Neural Network (sklearn)","69a18c62":"## Visualizations","bb827831":"## Preprocessing Pt. 2","3c144e94":"##Gradient Boost","e91ecff3":"## Imports & Setup","518f928b":"##LogReg","4be02f5c":"##Random Forest","2b99c959":"## Export"}}