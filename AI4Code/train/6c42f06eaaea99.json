{"cell_type":{"a6871f27":"code","18123a1d":"code","5d3b2791":"code","c8b66556":"code","5cb0693d":"code","a3c3e4a9":"code","506bfce0":"code","2eec60e9":"code","e93a3792":"code","fe117b0d":"code","c5222294":"code","63d1a88e":"code","69035eb1":"code","050e1c54":"code","1360cf8b":"code","bd610fce":"code","2873bc06":"code","1bbeec01":"code","330db128":"code","2d8ee2f1":"code","43e05423":"code","420d2a3a":"code","df2c4f33":"code","49f72d82":"code","bf38837e":"code","0495fe55":"code","2d94d9af":"markdown","dba22c4a":"markdown","3666bf02":"markdown","2b0f001f":"markdown","446694f1":"markdown","4631c9fc":"markdown","54d58922":"markdown"},"source":{"a6871f27":"#import packages\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gc\nimport os\nfrom tqdm.notebook import tqdm","18123a1d":"#reduce memory usage by optimizing data format\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5d3b2791":"#open input files\npath = \"..\/input\/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))","c8b66556":"sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","5cb0693d":"#preprocess calendar information\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ncalendar = prep_calendar(calendar)","a3c3e4a9":"#preprocess selling prices\ndef prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) \/ (1 + gr.cummax() - gr.cummin())\n    df = reduce_mem_usage(df)\n    return df\n\nselling_prices = prep_selling_prices(selling_prices)","506bfce0":"def reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales = reshape_sales(sales, 1000)","2eec60e9":"# add statistical parameters to selling data\ndef prep_sales(df):\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    df['rolling_mean_t60'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n    df['rolling_mean_t90'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df['rolling_mean_t180'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t180))]\n    df = reduce_mem_usage(df)\n\n    return df\n\nsales = prep_sales(sales)","e93a3792":"sales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()","fe117b0d":"sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()","c5222294":"del selling_prices","63d1a88e":"cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(cat_id_cols)):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n\nsales = reduce_mem_usage(sales)\ngc.collect()","69035eb1":"num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t30\", \"rolling_mean_t60\", \n            \"rolling_mean_t90\", \"rolling_mean_t180\", \"rolling_std_t7\", \"rolling_std_t30\"]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in tqdm(enumerate(num_cols)):\n    sales[v] = sales[v].fillna(sales[v].median())\n    ","050e1c54":"test = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ngc.collect()","1360cf8b":"# Input dict for training with a dense array and separate inputs for each embedding input\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n# One month of validation data\nflag = (sales.d < 1914) & (sales.d >= 1914 - 28)\nvalid = (make_X(sales[flag]),\n         sales[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales.d < 1914 - 28\nX_train = make_X(sales[flag])\ny_train = sales[\"demand\"][flag]\n                             \ndel sales, flag\ngc.collect()","bd610fce":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam","2873bc06":"def create_model(lr=0.002):\n    tf.random.set_seed(173)\n\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Dense input\n    dense_input = Input(shape=(len(dense_cols), ), name='dense1')\n\n    # Embedding input\n    wday_input = Input(shape=(1,), name='wday')\n    month_input = Input(shape=(1,), name='month')\n    year_input = Input(shape=(1,), name='year')\n    event_name_1_input = Input(shape=(1,), name='event_name_1')\n    event_type_1_input = Input(shape=(1,), name='event_type_1')\n    event_name_2_input = Input(shape=(1,), name='event_name_2')\n    event_type_2_input = Input(shape=(1,), name='event_type_2')\n    item_id_input = Input(shape=(1,), name='item_id')\n    dept_id_input = Input(shape=(1,), name='dept_id')\n    store_id_input = Input(shape=(1,), name='store_id')\n    cat_id_input = Input(shape=(1,), name='cat_id')\n    state_id_input = Input(shape=(1,), name='state_id')\n\n    wday_emb = Flatten()(Embedding(7, 1)(wday_input))\n    month_emb = Flatten()(Embedding(12, 1)(month_input))\n    year_emb = Flatten()(Embedding(6, 1)(year_input))\n    event_name_1_emb = Flatten()(Embedding(31, 1)(event_name_1_input))\n    event_type_1_emb = Flatten()(Embedding(5, 1)(event_type_1_input))\n    event_name_2_emb = Flatten()(Embedding(5, 1)(event_name_2_input))\n    event_type_2_emb = Flatten()(Embedding(5, 1)(event_type_2_input))\n\n    item_id_emb = Flatten()(Embedding(3049, 3)(item_id_input))\n    dept_id_emb = Flatten()(Embedding(7, 1)(dept_id_input))\n    store_id_emb = Flatten()(Embedding(10, 1)(store_id_input))\n    cat_id_emb = Flatten()(Embedding(3, 1)(cat_id_input))\n    state_id_emb = Flatten()(Embedding(3, 1)(state_id_input))\n\n    # Combine dense and embedding parts and add dense layers. Exit on linear scale.\n    x = concatenate([dense_input, wday_emb, month_emb, year_emb, \n                     event_name_1_emb, event_type_1_emb, \n                     event_name_2_emb, event_type_2_emb, \n                     item_id_emb, dept_id_emb, store_id_emb,\n                     cat_id_emb, state_id_emb])\n\n    #reshape to input in convolutional layer\n    x = tf.reshape(x, [tf.shape(x)[0],tf.shape(x)[1],1])\n    \n    #2 convolutional layers\n    x = Conv1D(filters=100, kernel_size=15, activation='tanh', input_shape=(None,None),kernel_regularizer= regularizers.l2(0.001), bias_regularizer= regularizers.l2(0.001))(x)\n    x = Conv1D(filters=100, kernel_size=10, activation='tanh',kernel_regularizer= regularizers.l2(0.001), bias_regularizer= regularizers.l2(0.001))(x)\n    x = (MaxPooling1D(5))(x)\n    x = (Dropout(0.5))(x)\n    \n    #3 feed-forward layers\n    x = Dense(150, activation=\"tanh\")(x)\n    x = Dense(75, activation=\"tanh\")(x)\n    x = Dense(10, activation=\"tanh\")(x)\n    \n    outputs = Dense(1, activation=\"linear\", name='output')(x)\n\n    inputs = {\"dense1\": dense_input, \"wday\": wday_input, \"month\": month_input, \"year\": year_input, \n              \"event_name_1\": event_name_1_input, \"event_type_1\": event_type_1_input,\n              \"event_name_2\": event_name_2_input, \"event_type_2\": event_type_2_input,\n              \"item_id\": item_id_input, \"dept_id\": dept_id_input, \"store_id\": store_id_input, \n              \"cat_id\": cat_id_input, \"state_id\": state_id_input}\n\n    # Connect input and output\n    model = Model(inputs, outputs)\n\n    model.compile(loss=keras.losses.mean_squared_error,\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()],\n                  optimizer=Adam(learning_rate=lr))\n    \n    return model","1bbeec01":"model = create_model(3e-4)\nmodel.summary()\nkeras.utils.plot_model(model, 'model.png', show_shapes=True)","330db128":"history = model.fit(X_train, \n                    y_train,\n                    batch_size=10000,\n                    epochs=30,\n                    shuffle=True,\n                    validation_data=valid)","2d8ee2f1":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","43e05423":"history.history[\"val_loss\"]","420d2a3a":"model.save('DCNN30.h5')","df2c4f33":"pred = model.predict(X_test, batch_size=10000)","49f72d82":"predr = np.reshape(pred,(-1,1))\n","bf38837e":"test[\"demand\"] = predr.clip(0)\nsubmission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\nsubmission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\nsubmission.head()","0495fe55":"submission.to_csv(\"submission.csv\", index=False)","2d94d9af":"#### Plot the evaluation metrics over epochs","dba22c4a":"### Architecture with embeddings","3666bf02":"# M5 Forecast: Deep CNN\n\nThis notebook is based on a notebook the notebook of MichaelMayer https:\/\/www.kaggle.com\/mayer79\/m5-forecast-keras-with-categorical-embeddings-v2\n\nWe are using the same preprocessing technique and inputting that data to our architecture. We are applying deep convolutionals layers to the forecasting task, which to the best of our knowledge was not used before in this competition. The results of our model are around the best public results for the neural network solutions for this challege","2b0f001f":"## Preprocessing data","446694f1":"## The model","4631c9fc":"## Submission","54d58922":"#### Make training data"}}