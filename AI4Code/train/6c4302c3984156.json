{"cell_type":{"d6ff9062":"code","f8731696":"code","7947cee0":"code","ca1eec1f":"code","84e7a484":"code","84055cf8":"code","ef2266d4":"code","c41ea194":"code","af83de57":"code","ec789612":"code","a9a16bff":"code","4957760e":"code","baec2acc":"code","4604286f":"code","ba2cb684":"code","6db4274b":"code","d3cbc336":"code","93a75eb4":"code","e50c6ee4":"code","81928a6f":"code","a9b4bfa6":"code","adb00d85":"code","68455298":"code","ef14c339":"code","69665f1e":"code","e6a92874":"code","af3d16c2":"code","cd99d6f0":"code","f1d6c493":"code","7ec0b134":"code","631c42f2":"code","67892c75":"code","b7394fff":"code","7a65a4df":"code","d312e7d7":"code","34a7eabe":"code","09fb7f39":"code","63af9743":"code","b2982325":"code","46e17d2e":"code","46fed5eb":"code","b53c8e3c":"code","386bb7c6":"code","c49f70f0":"code","c7bc3ecd":"code","f828a454":"code","36baeff7":"code","ddacd20e":"code","d993f73b":"code","6fd8e223":"code","09b9bd79":"code","38e08ce0":"code","f2281fd0":"markdown","72a6ca2f":"markdown","b70ea9bc":"markdown","a13bbe5f":"markdown","c07c0b20":"markdown","6a37980f":"markdown","5783e29f":"markdown","fe7ebf8c":"markdown","2c19858b":"markdown","8de4e3eb":"markdown","7da7dfe7":"markdown","06337893":"markdown","04eefd92":"markdown","d0fc6ef9":"markdown","3b1773e0":"markdown","b267b715":"markdown","f2e3dce7":"markdown","ecc3560b":"markdown","350272b6":"markdown"},"source":{"d6ff9062":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# read the data\npath = '..\/input\/house-prices-advanced-regression-techniques\/'\nX_full = pd.read_csv(path + 'train.csv', index_col=0)\ntest = pd.read_csv(path + 'test.csv', index_col=0)\n\n# separate X and y data\ntarget = 'SalePrice'\ny = X_full[target]\nX = X_full.drop(target, axis=1)\n\ndataset = [X, test]\nX.head()\n\n# random state\nSEED = 2007","f8731696":"X.shape","7947cee0":"X.describe()","ca1eec1f":"def apply(dfs, func, *largs, **kwargs):\n    for df in dfs:\n        df = func(df, *largs, **kwargs)\n    return dfs","84e7a484":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef plot_dist_prob(y, title):\n    fig = plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    sns.distplot(y, fit=stats.norm);\n    (mu, sigma) = stats.norm.fit(y)\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title('SalePrice distribution')\n    plt.subplot(1,2,2)\n    res = stats.probplot(y, plot=plt)\n    plt.suptitle(title)\n    \nplot_dist_prob(y, 'Original')","84055cf8":"s = y.skew()\nk = y.kurtosis()\nprint('Skewness: {0:.3f}, Kurtosis: {1:.3f}'.format(s, k))\nbin = 1000\nmode = ((y \/ bin).mode() * bin)[0]\nprint('Most common price: {:,.0f}'.format(mode))\n","ef2266d4":"import numpy as np\ndef norm(df):\n    return np.log1p(df)\n\ndef un_norm(df):\n    return np.expm1(df)\n\ny = norm(y)\nplot_dist_prob(y, 'Normalized')\n\nX[target] = y","c41ea194":"# remove outliers\ndef drop(df, remove, axis=0):\n    df.drop(remove, axis=axis, inplace=True)\n    return df\n\nmean = y.mean()\nstd = y.std()\n\noutliers = (y > mean + 3 * std) | (y < mean - 3 * std)\noutliers = outliers[outliers].index\nun_norm(y[outliers])\n\nX = drop(X, outliers, axis=0)\ny = drop(y, outliers, axis=0)\n\n","af83de57":"def make_categorical(dfs):\n    col_transform = ['MSSubClass', 'MoSold']\n\n    for col in col_transform:\n        for df in dfs:\n            df[col] = df[col].apply(str)\n    return dfs\n\ndataset = make_categorical(dataset)","ec789612":"def col_with_nan(df):\n    c = df.isna().sum() >0\n    nans = c[c]\n    cols_nans = list(nans.index)\n    message = '{0}({1}) \\nMissing {2}({3:.2f}%)\\n'\n    print('There are {} features with NAN values'.format(len(cols_nans)))\n    return cols_nans\n\ncols_nans = col_with_nan(X)","a9a16bff":"# map type to categorical or numeric\ncat_dict = {'object': 'Categorical'}\nrow = X.shape[0] \/ 100\n\nmiss_df = pd.DataFrame({\n    'type': X[cols_nans].dtypes,\n    'percent': X[cols_nans].isna().sum()\/row,\n})\n\nmiss_df.sort_values('percent').plot(kind='barh')","4957760e":"def investigate_missing(df, col, col_measure):\n    no = df[col_measure]==0\n    print('Values for {0} when no {1}: {2}'.format(col,\n                                                   col_measure,\n                                                   X[no][col].unique())\n         )\n\n    yes = -no\n    print('AND when there are {1}: {2}\\n'.format(col,\n                                                   col_measure,\n                                                   X[yes][col].unique())\n         )\n\ninvestigate_missing(X, 'PoolQC', 'PoolArea')    \ninvestigate_missing(X, 'FireplaceQu', 'Fireplaces')    \ninvestigate_missing(X, 'GarageQual', 'GarageArea')    \ninvestigate_missing(X, 'BsmtExposure', 'TotalBsmtSF')    \ninvestigate_missing(X, 'BsmtQual', 'TotalBsmtSF')    \ninvestigate_missing(X, 'MiscFeature', 'MiscVal')    \ninvestigate_missing(X, 'MasVnrType', 'MasVnrArea')    \n","baec2acc":"def make_drop(df, cols):\n    return df.drop(cols, axis=1, inplace=True)\n\ndef drop_problematic(dfs):\n    problem_cols = ['MasVnrType', \n                    'MasVnrArea', \n                    'MiscFeature']\n    for df in dfs:\n        df = make_drop(df, problem_cols)\n    return dfs\n  \ndataset = drop_problematic(dataset)","4604286f":"def make_impute(df):\n    df['GarageYrBlt'].fillna(df['YearBuilt'], inplace=True)\n\n    df['Exterior1st'].fillna('Other', inplace=True)\n    df['Exterior2nd'].fillna('Other', inplace=True)\n    \n    mode_cols = ['MSZoning', 'Electrical', 'SaleType']\n    for col in mode_cols:\n        mode = X[col].mode()[0]\n        df[col].fillna(mode, inplace=True)        \n        \n    zero_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n                 'GarageCars', 'LotFrontage']\n    for col in zero_cols:\n        df[col].fillna(0, inplace=True)\n    \n    return df\n\ndef impute(dfs):\n    for df in dfs:\n        df = make_impute(df)\n    return dfs\n\ndataset = impute(dataset)","ba2cb684":"# Let's go give numeric meaning to some columns\ndef categorical_to_meaning(dfs):\n    col_order = [\n         ('Alley', ['', 'Grvl', 'Pave']),\n         ('Utilities', ['', 'ELO', 'NoSeWa', 'NoSewr', 'AllPub']),\n         ('LotShape', ['Reg', 'IR1', 'IR2', 'IR3']),\n         ('LandContour', ['Lvl', 'Bnk', 'HLS', 'Low']),\n         ('LandSlope', ['Gtl', 'Mod', 'Sev']),\n         ('Fence', ['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']),\n         ('PoolQC', ['NA', 'Fa', 'Gd', 'Ex']),\n         ('PavedDrive', ['N', 'P', 'Y']),\n         ('GarageCond', ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('GarageQual', ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('GarageFinish', ['NA', 'Unf', 'RFn', 'Fin']),\n         ('GarageType', ['NA', 'Detchd', 'CarPort', 'BuiltIn', \n                         'Basment', 'Attchd', '2Types']),\n         ('FireplaceQu', ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('Functional', ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', \n                         'Min2', 'Min1', 'Typ']),\n         ('KitchenQual', ['', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('HeatingQC',  ['', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('BsmtFinType1', ['', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']),\n         ('BsmtFinType2', ['', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']),\n         ('BsmtExposure', ['', 'No', 'Mn', 'Av', 'Gd']),\n         ('BsmtCond', ['', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('BsmtQual', ['', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('Foundation', ['Wood', 'Stone', 'Slab', 'PConc', 'CBlock', 'BrkTil']),\n         ('ExterCond', ['', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('ExterQual', ['', 'Po', 'Fa', 'TA', 'Gd', 'Ex']),\n         ('CentralAir', ['N', 'Y']), \n         ('Street', ['Grvl', 'Pave']),\n    ]\n\n    for co, order in col_order:\n        imp = order[0]\n        d = {v: i for i, v in enumerate(order)}\n        for df in dfs:\n            df[co] = df[co].fillna(imp).apply(lambda x: d.get(x, x))\n        \n    return dfs\n\ndataset = categorical_to_meaning(dataset)  \n\n","6db4274b":"nans = test['MSZoning'].isna()\ntest[nans]\n\nX['Electrical'].mode()[0]\nX['Electrical'].value_counts()","d3cbc336":"def has_features(dfs):\n    have = ['GarageArea', 'PoolArea',\n            'TotalBsmtSF', '2ndFlrSF', \n           ]\n    for df in dfs:\n        for col in have:\n            df['Has'+col] = df[col] > 0\n        \n        df['has_remod'] = df['YearRemodAdd'] != df['YearBuilt']\n        df['is_new'] = df['YrSold'] == df['YearBuilt']\n    \n    return dfs\n\ndataset = has_features(dataset)","93a75eb4":"corr = X.corr()\nf, ax = plt.subplots(figsize=(20, 18))\nsns.heatmap(corr, cbar=False)","e50c6ee4":"twins_col = ['Fence', 'Fireplaces', 'FireplaceQu', \n             'PoolArea', 'PoolQC', 'GarageQual', \n             'GarageArea','GarageCond', '2ndFlrSF',\n             'BsmtCond', 'TotRmsAbvGrd','GarageFinish',\n            ]\n\nX.drop(twins_col, axis=1, inplace=True)\ntest.drop(twins_col, axis=1, inplace=True)","81928a6f":"C1, C2 = cols = ['Condition1', 'Condition2']\n\ndef condition_weight(val, weights):\n    return weights.get(val, 0) \n\ndef extract_condition(df, new_features):\n    for col, wei in new_features.items():\n        df[col] = df[C1].apply(\n                        lambda x: condition_weight(x, wei)\n                    ) + df[C2].apply(\n                        lambda x: condition_weight(x, wei)\n                    ) \n    df.drop(cols, axis=1, inplace=True)\n    return df\n        \ndef conditions(dfs):\n    \n    artery = {'Feedr': 1, 'Artery': 2}\n    rail = {'RRNe': -2, 'RRNe': -2, 'RRAe': -1, 'RRAn': -1}\n    posi = {'PosN': 1, 'PosA': 2}\n    \n    new_features = {\n        'Artery': artery,\n        'RailRoad': rail,\n        'Positive': posi,\n    } \n    for df in dfs:\n        df = extract_condition(df, new_features)\n\n    return dfs\n\ndataset = conditions(dataset) ","a9b4bfa6":"def age(df):\n    df['Age'] = df['YrSold'] - df['YearBuilt']\n    \n    df.drop(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'], \n            axis=1, inplace=True)\n    df['YrSold'] = df['YrSold'].apply(str)\n    return df\n\ndef calc_age(dfs):\n    for df in dfs:\n        df = age(df)\n    return dfs\n    \ndataset = calc_age(dataset)","adb00d85":"col_with_nan(X)\ncol_with_nan(test)\n","68455298":"def summarize(df):\n    df['total_baths'] = df['BsmtFullBath'] + df['FullBath'] \\\n            + ((df['BsmtHalfBath'] + df['HalfBath']) \/ 2)\n    df['total_habit'] = df[['TotalBsmtSF', 'GrLivArea']].sum(axis=1)\n    porch_cols = ['WoodDeckSF', 'OpenPorchSF',\n                  'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n    df['total_porch'] = df[porch_cols].sum(axis=1)\n    df.drop(porch_cols, axis=1, inplace=True)                                            \n    return df\n\ndef sum_rooms(dfs):\n    for df in dfs:\n        df = summarize(df)\n    return dfs\n\ndataset = sum_rooms(dataset)","ef14c339":"X_full = X\n\nquantils = dict()\n\nfor q in list(range(2, 10, 2)):\n    q \/= 10\n    quantils[q] = X_full.SalePrice.quantile(q)\nprint(quantils)\n\nsteps = [mean + std * i for i in range(-3, 4)]\n\ndef relation_withprice(df, col, quantils=steps):\n    order = df.groupby(col)[target].describe()['50%'].sort_values().index\n    f, ax = plt.subplots(figsize=(20, 18))\n    ax = sns.violinplot(x=col, y=target, data=df, \n                        scale=\"width\", order=order,\n                        inner=\"quartile\")\n\n# X_full = pd.concat([X, y], axis=1)\n\nrelation_withprice(X_full, 'Neighborhood')\n","69665f1e":"def binning(sort):\n    r = dict()\n    for i, vals in enumerate(sort):\n        p = {v: i for v in vals}\n        r = {**r, **p} \n    return r\n\npoor = [\n    'MeadowV', 'IDOTRR', 'BrDale',\n]\nmiddle_low = [\n     'OldTown', 'Edwards','BrkSide', \n    'Sawyer', 'Blueste', 'SWISU',  \n    'NAmes', \n]\nmiddle_med = [\n    'NPkVill', 'Mitchel',\n]\nmiddle_hig = [\n    'SawyerW', 'Gilbert', 'NWAmes',\n    'Blmngtn', 'CollgCr', 'ClearCr',\n    'Crawfor',\n    'Veenker','Somerst','Timber',\n]\nhigh = [\n    'StoneBr', 'NoRidge', 'NridgHt',\n]\nsort = [poor, middle_low, middle_med, middle_hig, high]\n\nneig_bin = binning(sort)  \nrank_dict = {\n    'Neighborhood': neig_bin,\n}","e6a92874":"relation_withprice(X_full, 'MSSubClass')\n","af3d16c2":"sort = [\n    ['180', '30', '45'],\n    ['190', '50', '90', '85', '40'],\n    ['160', '70', '20', '75', '80', '150'],\n    ['120', '60'],\n]\nrank_dict['MSSubClass'] = binning(sort)  ","cd99d6f0":"relation_withprice(X_full, 'MSZoning')\n","f1d6c493":"rank_dict['MSZoning'] = {\n    'C (all)': 0,\n    'RM': 1,\n    'RL': 2, \n    'FV': 3, \n    'RH': 4\n}\ntest['MSZoning'].unique()","7ec0b134":"relation_withprice(X_full, 'BldgType')","631c42f2":"sort = [\n    ['2fmCon'],\n    ['Duplex', 'Twnhs' ],\n    ['1Fam','TwnhsE'],\n]\nrank_dict['BldgType'] = binning(sort)  ","67892c75":"relation_withprice(X_full, 'HouseStyle')\n","b7394fff":"sort = [\n    ['1.5Unf'],\n    ['1.5Fin', '2.5Unf', 'SFoyer'],\n    ['1Story','SLvl'],\n    ['2Story','2.5Fin'],\n]\nrank_dict['HouseStyle'] = binning(sort) ","7a65a4df":"relation_withprice(X_full, 'SaleCondition')\n","d312e7d7":"sort = [\n    ['AdjLand'],\n    ['Abnorml', 'Family'],\n    ['Alloca','Normal'],\n    [],\n    ['Partial']\n]\nrank_dict['SaleCondition'] = binning(sort) \nrank_dict['SaleCondition'] = {'AdjLand': 0,\n    'Abnorml': 1, 'Family': 2,\n    'Alloca': 3, 'Normal': 4,\n    'Partial': 5}","34a7eabe":"def make_rank(df, ranks):\n    for col, d in ranks.items():\n        df[col] = df[col].apply(lambda x: d.get(x, x))\n    return df\n\ndataset = apply(dataset, make_rank, rank_dict)","09fb7f39":"relation_withprice(X_full, 'Exterior1st')","63af9743":"relation_withprice(X_full, 'Exterior2nd')","b2982325":"E1, E2 = ecols = ['Exterior1st', 'Exterior2nd']\n\nvalues = set(X[E1].unique()\n            ).union(set(X[E2].unique()))\nvalues = values.union(set(test[E1].unique()\n            ).union(set(test[E2].unique())))\n\nprint(len(values))\nprint(values)","46e17d2e":"mistakes = {\n    'Brk Cmn': 'BrkComm',\n    'CmentBd': 'CemntBd',\n}\n\ndef correct_mistakes(df, cols, mistake_dict):\n    for col in cols:\n        for mistake, correct in mistake_dict.items():\n            df[col] = df[col].replace(mistake, correct)\n    return df\n\ndataset = apply(dataset, correct_mistakes, ecols, mistakes)","46fed5eb":"def make_exterior(df, values):\n    E1, E2 = ecols = ['Exterior1st', 'Exterior2nd']\n    for v in values:\n        df['Exterior_' + v] = (df[E1] == v) | (df[E2] == v)\n        \n    df = df.drop(ecols, axis=1, inplace=True)    \n    return df\n\n\nvalues = set(X[E1].unique()\n            ).union(set(X[E2].unique()))\n\ndataset = apply(dataset, make_exterior, values)","b53c8e3c":"c = test.dtypes=='object'\nfor col in c[c].index:\n    print(col, X[col].unique())\n    ","386bb7c6":"\nnoise_cols = ['LotConfig', 'RoofStyle', 'RoofMatl', \n              'Heating', 'Electrical', 'SaleType',\n             ]\n\ndataset = apply(dataset, make_drop, noise_cols)\n","c49f70f0":"X = pd.get_dummies(X)\ntest = pd.get_dummies(test)\n\nprint(list(X.columns))","c7bc3ecd":"# Remove overfit for 99.95% of data is zero\nimport math\nrow = X.shape[0]\noverfit_min = row - math.ceil(row * 5 \/ 10000) \n\nc = (X == 0).sum() >= overfit_min\noverfit_cols = set(c[c].index)\n\n\nX.drop(overfit_cols, axis=1, inplace=True)\n\nboth = set(test.columns).intersection(overfit_cols)\ntest.drop(both, axis=1, inplace=True)\n\n# columns in X is in test too\ntest_overfit = list(set(X.columns) - set(test.columns))\nif test_overfit:\n    X.drop(test_overfit, axis=1, inplace=True)\n    \nif target in X.columns:\n    X.drop([target], axis=1, inplace=True)","f828a454":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler(with_centering=True)\nX_scale = pd.DataFrame(scaler.fit_transform(X),\n                       columns=X.columns)\ntest_scale = pd.DataFrame(scaler.transform(test),\n                       columns=test.columns)","36baeff7":"# define metric function\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LassoCV, LassoLarsCV, Lasso, LassoLars\n\ndef RMSE(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n\n# a random seed for deterministic results\nSEED = 2007\n\ncommon_args = {\n    'random_state': SEED,\n}\n","ddacd20e":"# model selection functions\n\ncv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n\ndef cross_val(models, X=X, y=y):\n    r = dict()\n    for name, model in models.items():\n        cv_results = np.sqrt(\n            -cross_val_score(model, X, y,\n                             cv=cv, \n                             scoring='neg_mean_squared_error')\n                            )\n        r[name] = cv_results\n        print('Cross validation ready!')\n    return r\n    \ndef choose_best(results):\n    errors = dict()\n\n    for name, arr in results.items():\n        errors[name] = arr.mean()\n        print(name, 'RMSE {0:.4f}, Std {1:.4f}'.format(\n            errors[name], arr.std()))\n\n    best_model =  [m for m, e in errors.items() \n                   if e == min(errors.values())][0]\n    return best_model","d993f73b":"def output(predicts, filename):\n    output = pd.DataFrame({'Id': test.index,\n                       'SalePrice': predicts})\n    output.to_csv(filename + '.csv', index=False)\n\ndef train_predict_out(model, filename, X=X, y=y, test=test):\n    model.fit(X, y)\n    predicts = un_norm(model.predict(test))\n    output(predicts, filename)\n    \n    return predicts","6fd8e223":"# Linear models\nlin_models = {\n    'Lasso': LassoCV(cv=cv,\n         **common_args\n    ),\n    'LassoLars': LassoLarsCV(cv=cv\n    ),\n}\n\nr = cross_val(lin_models, X_scale)\nbest_linear = choose_best(r)\nlinear_model = lin_models[best_linear]\n","09b9bd79":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\ntree_models = {\n    'Random Forest': RandomForestRegressor(n_estimators=100, \n                                           max_depth=10,\n                                           max_features='sqrt',\n                                           warm_start=False,                                          \n                                          **common_args),\n    'XGBoost': XGBRegressor(n_estimators=2000, \n                            max_depth=3, \n                            learning_rate=0.01,\n                            verbosity=0,\n                            objective='reg:squarederror',\n                            **common_args),\n}\nresults = cross_val(tree_models, X)\nbest_treemodel = choose_best(results)\ntree_model = tree_models[best_treemodel]","38e08ce0":"lin_predicts = train_predict_out(linear_model, 'linear', \n                                 X=X_scale, test=test_scale)\ntre_predicts = train_predict_out(tree_model, 'tree')\n\nblend_predicts = (lin_predicts + tre_predicts) \/ 2\noutput(blend_predicts, 'blended')","f2281fd0":"Let's investigate columns with missing values.\n\nMissing values appear as Nan (Not a number) or null values.\n\nIt's convinient fill all missing values.\n","72a6ca2f":"Conditions\n\nSome important feature is condition. ","b70ea9bc":"Define a general function to apply a function to an itearable of datasets.","a13bbe5f":"> ","c07c0b20":"## Make a model","6a37980f":"## Label Analysis\n\nLet examine the label distribution and some important stats.","5783e29f":"Observations:\n* The current train data has 36 numeric features. However, some of them must be categorical, i.e. MSSubClass\n* There are some columns with missing values, i.e. LotFrontage\n* The data has sales from year 2006 to 2010 and the build of house as old as 1872 to new houses.","fe7ebf8c":"BldgType ['1Fam' '2fmCon' 'Duplex' 'TwnhsE' 'Twnhs']  \nHouseStyle ['2Story' '1Story' '1.5Fin' '1.5Unf' 'SFoyer' 'SLvl' '2.5Unf' '2.5Fin']  \nSaleCondition ['Normal' 'Abnorml' 'Partial' 'AdjLand' 'Alloca' 'Family']****","2c19858b":"Seems some correlated variables that say same history.\nLet's drop some of them.\n\nFence and HasFence, I will drop Fence.\nHasFireplace with Fireplaces and FireplacesQU, I will keep HasFireplace.\nHasPoolArea with PoolArea and PoolQC,  I will keep HasPoolArea.\n\nGarageQual and GarageCond seem twins!!! I left GarageCond\n\nGarageCars and GarageArea twins too. I choose GarageCars","8de4e3eb":"The missing values for PoolQC, FireplaceQu, GarageQual, BsmtExposure, BsmtQual seem missing because there is no Pool, fireplace, garage or basement respectively.\n\nWe can impute these with the value for no feature present.\n\n`MiscFeature`, could be the same assumption, however is missing in 96,3%. We drop this column.\n\n`MasVnrAre` has missing values with type of material, and some area with nan on material. This column seem problematic and with almost nothing weight for our model (noise). Hence, I will drop masonary columns.\n","7da7dfe7":"# Label normalization\nAffortunately, we can make a log transformation on the data, that normalize the data, removing the skewness and kutosis, and improve our probability as well.\n","06337893":"Let's investigate why these columns are missing.","04eefd92":"Some categorical features, seems to be ranking meaning, i.e. GarageCond, `Po` is Poor is less than `Gd` is Good.\nWe could turn these categorical features into numeric ones. ","d0fc6ef9":"It seems the prices have a positive skewness, this was expected because most people cannot afford an expensive house, the most the sales concentrate in the lower values.\n\nAlso, it is observed a large kurtosis, this is a huge peak in the distribution, close to 140k aprox.\n\nThese large deviations in distribution, damage our prediction, as you can see in the probability plot.\n\n","3b1773e0":"## Ranking Features\nSome categorical features don't have a clear order like 'Neighborhood' (What is most costly 'OldTown' or 'Crawfor'? are they equivalent?).\n\nLet's compare these categorical columns with its median sale price in each category.\n\nThen we assign a bin to each category, helping with subject knowledge, intuition and some quantile borderlines that separate them into strategic groups.\n\nThe result will be a numeric ranking heuristic column, that I hope, will improve our model.\n\n","b267b715":"# Investigate correlations","f2e3dce7":"Our train data has 1460 rows (sample) and 79 columns (features)","ecc3560b":"## Remove outliers","350272b6":"## Categorical Cast\nSome features are numeric type, however these need to be categorical, because they have no numeric meaning."}}