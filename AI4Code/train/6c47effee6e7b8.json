{"cell_type":{"0e7c8fa7":"code","deab985f":"code","2dc35d96":"code","ab981a1b":"code","4d47c128":"code","3873647b":"code","a82636ae":"code","5e7d0326":"code","a2d1305c":"code","51b3b23f":"code","0464a76e":"code","dbc703d7":"code","371df06e":"code","76338d26":"code","71f94fce":"code","f320313c":"code","30fc7ac6":"code","fb7f65de":"code","7a4ff113":"code","2906b58f":"code","c259fc7e":"code","bd68b932":"code","52e4ef60":"code","af3b0bcf":"code","3b780ec0":"code","e6ed2703":"code","0517b095":"code","e4503b19":"code","578e6b74":"code","1ed408be":"markdown","e9e4c365":"markdown","f036ddc1":"markdown","60cbd2b2":"markdown","a3306ad8":"markdown","8f507c8c":"markdown","c5097e58":"markdown","25e248a4":"markdown","6e6e5df0":"markdown","12b106e7":"markdown","2554d7c9":"markdown","27bca760":"markdown","e215cbea":"markdown","a1e9829c":"markdown","f08001ae":"markdown","ee65805c":"markdown","651e28a2":"markdown","aa267b4a":"markdown","d34fff28":"markdown","29211ddf":"markdown","d80578b6":"markdown","f337706d":"markdown","53041823":"markdown","0c79a163":"markdown","c8455c97":"markdown","b9d03296":"markdown","c5323ef1":"markdown","3b5bf5b9":"markdown","ec3370fc":"markdown","0fc60c2a":"markdown","80e266e0":"markdown","d3918767":"markdown","d7369337":"markdown","1a89e4b0":"markdown","c499d655":"markdown","f047901f":"markdown","78341d52":"markdown","a8b1ec8e":"markdown","47522511":"markdown","42286482":"markdown","478362b6":"markdown","3edc9d8f":"markdown","e7bc0c61":"markdown","c8da8753":"markdown","c85d0263":"markdown","1e33ba48":"markdown","ddb30aeb":"markdown","79076aa4":"markdown","7ac00946":"markdown","287aeeb3":"markdown","a3202d77":"markdown","667bbf59":"markdown","d1567262":"markdown","a4e3ed63":"markdown","b900e29b":"markdown","b7fe2ded":"markdown","24072b8f":"markdown"},"source":{"0e7c8fa7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n\nfrom matplotlib import ticker\nimport time\nimport warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n\nRANDOM_STATE = 12 \nFOLDS = 5","deab985f":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv\")","2dc35d96":"train.head()","ab981a1b":"print(f'\\033[92mNumber of rows in train data: {train.shape[0]}')\nprint(f'\\033[94mNumber of columns in train data: {train.shape[1]}')\nprint(f'\\033[91mNumber of values in train data: {train.count().sum()}')\nprint(f'\\033[91mNumber missing values in train data: {sum(train.isna().sum())}')","4d47c128":"train.describe()","3873647b":"test.head()","a82636ae":"print(f'\\033[92mNumber of rows in test data: {test.shape[0]}')\nprint(f'\\033[94mNumber of columns in test data: {test.shape[1]}')\nprint(f'\\033[91mNumber of values in train data: {test.count().sum()}')\nprint(f'\\033[91mNo of rows with missing values  in test data: {sum(test.isna().sum())}')","5e7d0326":"test.describe()","a2d1305c":"submission.head()","51b3b23f":"train.drop([\"row_id\"] , axis = 1 , inplace = True)\ntest.drop([\"row_id\"] , axis = 1 , inplace = True)\nTARGET = 'target'\nFEATURES = [col for col in train.columns if col not in ['row_id', TARGET]]\nRANDOM_STATE = 12 ","0464a76e":"train.iloc[:, :-1].describe().T.sort_values(by='std' , ascending = False)\\\n                     .style.background_gradient(cmap='GnBu')\\\n                     .bar(subset=[\"max\"], color='#F8766D')\\\n                     .bar(subset=[\"mean\",], color='#00BFC4')","dbc703d7":"df = pd.concat([train[FEATURES], test[FEATURES]], axis=0)\n\ncat_features = [col for col in FEATURES if df[col].nunique() < 25]\ncont_features = [col for col in FEATURES if df[col].nunique() >= 25]\n\ndel df\nprint(f'Total number of features: {len(FEATURES)}')\nprint(f'\\033[92mNumber of categorical (<25 Unique Values) features: {len(cat_features)}')\nprint(f'\\033[96mNumber of continuos features: {len(cont_features)}')\n\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical(<25 Unique Values)', 'Continuos'],\n        colors=['#F8766D', '#00BFC4'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","371df06e":"ncols = 5\nnrows = 20\nn_features = cont_features[:100]\nfig, axes = plt.subplots(nrows, ncols, figsize=(25, 15*4))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = n_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#F8766D', label='Train data' , fill =True )\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#00BFC4', label='Test data', fill =True)\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(6)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","76338d26":"ncols = 5\nnrows = 20\nn_features = cont_features[100:200]\nfig, axes = plt.subplots(nrows, ncols, figsize=(25, 60))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = n_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#F8766D', label='Train data' , fill =True )\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#00BFC4', label='Test data', fill =True)\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(6)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","71f94fce":"ncols = 5\nnrows = 15\nn_features = cont_features[200:]\nfig, axes = plt.subplots(nrows, ncols, figsize=(25, 45))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = n_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#F8766D', label='Train data' , fill =True )\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#00BFC4', label='Test data', fill =True)\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(6)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","f320313c":"print(f'\\033[92mNo Categorical features.')\nprint(f'\\033[92mAll feature distribution with less than 25 unique values plotted above with continous feature distributions')\nprint(f'\\033[94mContinous Features with their unique value count:')\nfor cat in cat_features:\n    print(str(cat) + \" -   \" + str(train[cat].nunique()))","30fc7ac6":"target_df = pd.DataFrame(train[TARGET].value_counts()).reset_index()\ntarget_df.columns = [TARGET, 'count']\nfig = px.bar(data_frame =target_df, \n             x = TARGET,\n             y = 'count' , \n             color = \"count\",\n             color_continuous_scale=\"Emrld\") \nfig.update_layout(template = \"plotly_white\")\nfor idx,target in enumerate(target_df[\"target\"]):\n    print(\"\\033[94mPercentage of \" + str(target) + \" category  : {:.2f} %\".format(target_df[\"count\"][idx] *100 \/ train.shape[0]))\nfig.show()","fb7f65de":"train[\"mean\"] = train[FEATURES].mean(axis=1)\ntrain[\"std\"] = train[FEATURES].std(axis=1)\ntrain[\"min\"] = train[FEATURES].min(axis=1)\ntrain[\"max\"] = train[FEATURES].max(axis=1)\n\ntest[\"mean\"] = test[FEATURES].mean(axis=1)\ntest[\"std\"] = test[FEATURES].std(axis=1)\ntest[\"min\"] = test[FEATURES].min(axis=1)\ntest[\"max\"] = test[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'std', 'min', 'max'])","7a4ff113":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntrain[TARGET] = encoder.fit_transform(train[TARGET])","2906b58f":"lgb_params = {\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'device' : 'gpu',\n}\n\n\nlgb_predictions = []\nlgb_scores = []\nlgb_fimp = []\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n    \n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    \n    X_train, X_valid = train.iloc[train_idx][FEATURES], train.iloc[valid_idx][FEATURES]\n    y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]\n    \n    model = LGBMClassifier(**lgb_params)\n    model.fit(X_train, y_train,verbose=0)\n    \n    preds_valid = model.predict(X_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    lgb_scores.append(acc)\n    run_time = time.time() - start_time\n    \n    print(f\"Fold={fold+1}, Accuracy: {acc:.2f}, Run Time: {run_time:.2f}s\")\n    fim = pd.DataFrame(index=FEATURES,\n                 data=model.feature_importances_,\n                 columns=[f'{fold}_importance'])\n    lgb_fimp.append(fim)\n    test_preds = model.predict(test[FEATURES])\n    lgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(lgb_scores))","c259fc7e":"lgbm_fis_df = pd.concat(lgb_fimp, axis=1).head(15)\nlgbm_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10),\n                                       title='Feature Importance Across Folds')\nplt.show()","bd68b932":"catb_params = {\n    \"objective\": \"MultiClass\",\n    \"task_type\": \"GPU\",\n}\n\ncatb_predictions = []\ncatb_scores = []\ncatb_fimp = []\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n    \n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    \n    X_train, X_valid = train.iloc[train_idx][FEATURES], train.iloc[valid_idx][FEATURES]\n    y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]\n    \n    model = CatBoostClassifier(**catb_params)\n    model.fit(X_train, y_train,verbose=0)\n    \n    preds_valid = model.predict(X_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    catb_scores.append(acc)\n    run_time = time.time() - start_time\n    \n    print(f\"Fold={fold+1}, Accuracy: {acc:.2f}, Run Time: {run_time:.2f}s\")\n    fim = pd.DataFrame(index=FEATURES,\n                 data=model.feature_importances_,\n                 columns=[f'{fold}_importance'])\n    catb_fimp.append(fim)\n    test_preds = model.predict(test[FEATURES])\n    catb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(catb_scores))","52e4ef60":"catb_fis_df = pd.concat(catb_fimp, axis=1).head(15)\ncatb_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10),\n                                       title='Feature Importance Across Folds')\nplt.show()","af3b0bcf":"xgb_params = {\n    'objective': 'multi:softmax',\n    'eval_metric': 'mlogloss',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    }\n\n\nxgb_predictions = []\nxgb_scores = []\nxgb_fimp = []\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n    \n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    \n    X_train, X_valid = train.iloc[train_idx][FEATURES], train.iloc[valid_idx][FEATURES]\n    y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]\n    \n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train,verbose=0)\n    \n    preds_valid = model.predict(X_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    xgb_scores.append(acc)\n    run_time = time.time() - start_time\n    \n    print(f\"Fold={fold+1}, Accuracy: {acc:.2f}, Run Time: {run_time:.2f}s\")\n    test_preds = model.predict(test[FEATURES])\n    fim = pd.DataFrame(index=FEATURES,\n                 data=model.feature_importances_,\n                 columns=[f'{fold}_importance'])\n    xgb_fimp.append(fim)\n    xgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(xgb_scores))","3b780ec0":"xgb_fis_df = pd.concat(xgb_fimp, axis=1).head(15)\nxgb_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10),\n                                       title='Feature Importance Across Folds')\nplt.show()","e6ed2703":"lgb_submission = submission.copy()\nlgb_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(lgb_predictions),axis = 1)[0]).astype('int'))\nlgb_submission.to_csv(\"lgb-subs.csv\",index=False)\nlgb_submission.head()","0517b095":"catb_submission = submission.copy()\ncatb_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(catb_predictions),axis = 1)[0]).astype('int'))\ncatb_submission.to_csv(\"catb-subs.csv\",index=False)\ncatb_submission.head()","e4503b19":"xgb_submission = submission.copy()\nxgb_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(xgb_predictions),axis = 1)[0]).astype('int'))\nxgb_submission.to_csv(\"xgb-subs.csv\",index=False)\nxgb_submission.head()","578e6b74":"ensemble_submission = submission.copy()\nensemble_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(lgb_predictions+catb_predictions+xgb_predictions),axis = 1)[0]).astype('int'))\nensemble_submission.to_csv(\"ensemble_submission.csv\",index=False)\nensemble_submission.head()","1ed408be":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","e9e4c365":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Data Distribution :<\/u><\/b><br>\n    \n* <i>Out of 286 features <b><u>278<\/u><\/b> features are continous <\/i><br>\n* <i>The reamining <b><u>8<\/u><\/b> features are categorical. <b><u>(can be considered as categorical,since they have less than 25 unique values)<\/u><\/b><\/i><br>\n    \n<\/div>\n","f036ddc1":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","60cbd2b2":"<a id=\"4.2\"><\/a>\n## Continuos and Categorical Data Distribution","a3306ad8":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","8f507c8c":"Below is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","c5097e58":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","25e248a4":"### Feature Distribution of 101-200 Features","6e6e5df0":"### Basic statistics of test data","12b106e7":"<a id=\"4.4\"><\/a>\n## Feature Distribution of Categorical Features","2554d7c9":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Null Value Distribution :<\/u><\/b><br>\n\n* <i> No Null values. <\/i><br>\n<\/div>","27bca760":"<a id=\"4.5\"><\/a>\n## Target Distribution","e215cbea":"<a id=\"6.2\"><\/a>\n## Catboost Classifier","a1e9829c":"<a id=\"5\"><\/a>\n#  Feature Engineering","f08001ae":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","ee65805c":"### LGBM Classifier Submission","651e28a2":"Below is the first 5 rows of train dataset:","aa267b4a":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \n    \n### <center>Thank you for reading\ud83d\ude42<\/center>\n### <center>If you have any feedback or find anything wrong, please let me know!<\/center>\n","d34fff28":"Below is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","29211ddf":"<a id=\"4.1\"><\/a>\n## Overview of Data","d80578b6":"### Feature Importance for XGBoost Classifier (Top 15 Features)","f337706d":"# Table of Contents\n<a id=\"toc\"><\/a>\n- [1. Introduction](#1)\n- [2. Imports](#2)\n- [3. Data Loading and Preperation](#3)\n    - [3.1 Exploring Train Data](#3.1)\n    - [3.2 Exploring Test Data](#3.2)\n    - [3.3 Submission File](#3.3)\n- [4. EDA](#4)\n    - [4.1 Overview of Data](#4.1)\n    - [4.2 Null Value Distribution](#4.7)\n    - [4.3 Continuos and Categorical Data Distribution](#4.2)\n    - [4.4 Feature Distribution of Continous Features](#4.3)\n    - [4.5 Feature Distribution of Categorical Features](#4.4)\n    - [4.6 Target Distribution ](#4.5)\n- [5. Feature Engineering](#5)   \n- [6. Modelling](#6)\n    - [6.1 LGBM Classifier](#6.1)\n    - [6.2 Catboost Classifier](#6.2)\n    - [6.3 XGBoost Classifier](#6.3)\n- [7. Submission](#7)   ","53041823":"### Quick view of Train Data","0c79a163":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","c8455c97":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Target Modelling :<\/u><\/b><br>\n    \n* <i> <u><b>LGBMClassifier<\/u><\/b> , <u><b>CatBoostClassifier<\/u><\/b> and <u><b>XGBClassifier<\/u><\/b> used in modelling on 5-fold validation.<\/i><br>\n* <i> Further Hyperparameter tuning can imporve the results.<\/i><br>\n    \n<\/div>","b9d03296":"**The task of this compeition is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. The dataset used for this compeition is derived from this [paper](https:\/\/www.frontiersin.org\/articles\/10.3389\/fmicb.2020.00257\/full).**\n\n**Submissions are evaluated based on their categorization accuracy..**","c5323ef1":"<a id=\"4.3\"><\/a>\n## Feature Distribution of Continous Features","3b5bf5b9":"<a id=\"3.2\"><\/a>\n## Exploring Test Data","ec3370fc":"# Ensemble","0fc60c2a":"<a id=\"6.3\"><\/a>\n## XGBoost Classifier","80e266e0":"<a id=\"6\"><\/a>\n#  Modelling","d3918767":"### Basic statistics of training data","d7369337":"#### <i><u>(NOTE : THE ABOVE DISCUSSED CATEGORICAL FEATURES ARE INCLUDED IN THE FOLLOWING CONTINUOS FEATURE DISTRIBUTION PLOTS)<\/u><\/i>\n### Feature Distribution of first 100 Features","1a89e4b0":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Target Distribution :<\/u><\/b><br>\n\n* <i>There are <b><u>10<\/u><\/b> different target values<\/i><br>\n* <i>All target values are equally distributed approx - <b><u>10%<\/u><\/b> of total observations for each target.<\/i><br>\n    \n<\/div>","c499d655":"<a id=\"3.3\"><\/a>\n## Submission File","f047901f":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Test Data:<\/u><\/b><br>\n\n* <i> There are total of <b><u>287<\/u><\/b> columns : <b><u>278<\/u><\/b> continous , <b><u>8<\/u><\/b> categorical <b><u>1<\/u><\/b> row_id in <b><u>test<\/u><\/b> dataset.<\/i><br>\n* <i> There are total of <b><u>100000<\/u><\/b> rows in test dataset.<\/i><br>\n* <i> Test dataset contain <b><u>28700000<\/u><\/b> observation with <b><u>0<\/u><\/b>  missing values.<\/i><br>\n* <i> No <b><u>NULL<\/u><\/b> Values again. \ud83d\ude42<\/i><br>\n    \n<\/div>","78341d52":"### Quick view of Submission File","a8b1ec8e":"<a id=\"3\"><\/a>\n# Data Loading and Preperation","47522511":"### Feature Distribution of 201-275 Features","42286482":"<a id=\"3.1\"><\/a>\n## Exploring Train Data","478362b6":"**Created by Sanskar Hasija**\n\n**[TPS-FEB-22] \ud83d\udccaEDA + Modelling\ud83d\udcc8**\n\n**01 February 2022**\n","3edc9d8f":"###  Basic Feature Engineering","e7bc0c61":"### Catboost Classifier Submission","c8da8753":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","c85d0263":"<a id=\"1\"><\/a>\n# Introduction","1e33ba48":"<a id=\"2\"><\/a>\n# Imports","ddb30aeb":"### XGBoost Classifier Submission","79076aa4":"### Feature Importance for LGBM Classifier (Top 15 Features)","7ac00946":"<a id=\"6.1\"><\/a>\n## LGBM Classifier","287aeeb3":"<a id=\"4\"><\/a>\n# EDA","a3202d77":"### Quick view of Test Data","667bbf59":"# <center> [TPS-FEB-22] \ud83d\udccaEDA + Modelling\ud83d\udcc8 <\/center>\n## <center>If you find this notebook useful, support with an upvote\ud83d\udc4d<\/center>","d1567262":"<a id=\"6\"><\/a>\n#  Submission","a4e3ed63":"**Credit**: I start from a great notebook by [SANSKAR HASIJA](https:\/\/www.kaggle.com\/odins0n\/tps-feb-22-eda-modelling) and try to add some improvements by considering an ensemble of the top tree boosting models.","b900e29b":"### Feature Importance for Catboost Classifier (Top 15 Features)","b7fe2ded":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Train Data:<\/u><\/b><br>\n\n* <i>There are total of <b><u>288<\/u><\/b> columns : <b><u>278<\/u><\/b> continous , <b><u>8<\/u><\/b> categorical <b><u>1<\/u><\/b> row_id and <b><u>1<\/u><\/b> target column<\/i><br>\n* <i> There are total of <b><u>200000<\/u><\/b> rows in train dataset.<\/i><br>\n* <i> <b><u>target<\/u><\/b> is the target variable which is only available in the <b><u>train<\/u><\/b> dataset..<\/i><br>\n* <i> Train dataset contain <b><u>57600000<\/u><\/b> observation with <b><u>0<\/u><\/b>  missing \/ null values.<\/i><br>\n* <i> No <b><u>NULL<\/u><\/b> Values \ud83d\ude42 <\/i><br>\n    \n<\/div>","24072b8f":"<a id=\"4.7\"><\/a>\n## Null Value Distribution "}}