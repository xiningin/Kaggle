{"cell_type":{"456b1192":"code","399d07be":"code","f15174c5":"code","db00f66d":"code","394914fc":"code","e3f10bbd":"code","a0c80db1":"code","4b6e5c9a":"code","b4cfe4b7":"code","91ca2e44":"code","b8864761":"code","ed96a35e":"code","f12cc151":"code","b2a65e6e":"code","51c634ca":"code","1fb9a8db":"code","8d1d3e1b":"code","e21a83c6":"code","b517a8e2":"code","71a78d13":"code","064c2560":"code","94087e5f":"code","1d9aa3c6":"code","0188ed3c":"code","2ea733ea":"code","f6bd35e8":"code","b41a7635":"code","4f976678":"code","f3b76963":"code","f24a9144":"code","d6067d99":"code","5635ee61":"code","639c0ad1":"code","343aba28":"code","a7eb140e":"code","a7b59278":"code","4d365fb9":"code","ec513a2a":"code","6480a926":"code","770b3af0":"code","8fd67234":"code","39bec612":"markdown","b2e447a6":"markdown","6411ef3d":"markdown","2187a7d1":"markdown","b727d416":"markdown","ced98edc":"markdown","24fafe3b":"markdown","441ab9c5":"markdown","07ae3a3d":"markdown","ba06c759":"markdown","bb364c3f":"markdown","10f01b62":"markdown","244b17cd":"markdown","30b21edc":"markdown","100d5cbd":"markdown","1450ee8c":"markdown","37246ac1":"markdown","2b61d1ce":"markdown","0047d3b1":"markdown","db4094e7":"markdown","3108c0ea":"markdown"},"source":{"456b1192":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","399d07be":"#read the training data file\ndf = pd.read_csv('..\/input\/train.csv')\nprint(df.head())","f15174c5":"#let's look at the unique values in the ID column\nprint(df['Id'].describe())","db00f66d":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,10))\nsns.countplot(y = df['Id'] == 'new_whale', palette = 'Dark2')","394914fc":"#dimension of our original training dataframe\nprint(df.shape)","e3f10bbd":"#Let's get our x_train and y_train from our dataframe\nx_train = df['Image']\ny_train = df['Id']","a0c80db1":"#import all the necessary libraries from the keras API\nimport keras\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.imagenet_utils import preprocess_input","4b6e5c9a":"#define a function to prepare our trianing images\ndef PrepareTrainImages(dataframe, shape, path):\n    \n    #obtain the numpy array filled with zeros having the format --> (batch_size, height, width, channels)\n    x_train = np.zeros((shape, 100, 100, 3))\n    count = 0\n    \n    for fig in dataframe['Image']:\n        \n        #load images into images of size 100x100x3\n        img = load_img(\"..\/input\/\" + path + \"\/\" + fig, target_size = (100, 100, 3))\n        \n        #convert images to array\n        x = img_to_array(img)\n        x = preprocess_input(x)\n\n        x_train[count] = x\n        count += 1\n    \n    return x_train\n    ","b4cfe4b7":"x_train = PrepareTrainImages(df, df.shape[0], 'train')","91ca2e44":"print(x_train.shape) #we got the data in the format that we need for the CNN model","b8864761":"#let's normalize the data.\nx_train[0] # we can see that the pixel values in the following array have large differene in their values\n#so it's always better the obtain all the values in the same range\nx_train = x_train.astype('float32') \/ 255 #data normalized","ed96a35e":"#Let's visualize some of our taining images\nplt.figure(figsize = (12,8))\nplt.subplot(2, 2, 1)\nplt.imshow(x_train[0][:,:,0], cmap = 'gray') #the first image\nplt.title(df.iloc[0,0])\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(2, 2, 2)\nplt.imshow(x_train[100][:,:,0], cmap = 'gray')\nplt.title(df.iloc[100,0])\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(2, 2, 3)\nplt.imshow(x_train[1000][:,:,0], cmap = 'gray')\nplt.title(df.iloc[1000,0])\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(2, 2, 4)\nplt.imshow(x_train[4000][:,:,0], cmap = 'gray')\nplt.title(df.iloc[4000,0])\nplt.xticks([])\nplt.yticks([])","f12cc151":"from keras.utils import np_utils #to obtain the one hot encodings of the id values\nfrom sklearn.preprocessing import LabelEncoder #to obtain the unique integer values for each id values","b2a65e6e":"le = LabelEncoder()\ny_train = np_utils.to_categorical(le.fit_transform(y_train))","51c634ca":"print(y_train[:10])\nprint(y_train.shape)","1fb9a8db":"#let's start by importing all the necessary libraries for building the CNN model\nimport keras\nfrom keras.layers import Conv2D\nfrom keras.layers import Activation, BatchNormalization\nfrom keras.layers import MaxPooling2D, Dropout\nfrom keras.layers import Flatten, Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam","8d1d3e1b":"#start building the model\nmodel = Sequential()\nmodel.add(Conv2D(32, (3,3), input_shape = (x_train.shape[1:]), padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size =  (2,2)))\n\nmodel.add(Conv2D(64, (3,3), padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size =  (2,2)))\n\nmodel.add(Conv2D(128, (3,3), padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size =  (2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(y_train.shape[1]))\nmodel.add(Activation('softmax'))","e21a83c6":"#looking at the summary for our model\nmodel.summary()","b517a8e2":"#compile the model\noptim = Adam(lr = 0.001) #using the already available learning rate scheduler\nmodel.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = ['accuracy'])","71a78d13":"#fit the model on our dataset\nhistory = model.fit(x_train, y_train, epochs = 30, batch_size = 64)","064c2560":"#let's look how our model performed by plotting the accuracy and loss curves\nsns.set(style = 'darkgrid')\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(30), history.history['acc'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING ACCURACY')\nplt.title('TRAINING ACCURACY vs EPOCHS')\n\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(30), history.history['loss'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING LOSS')\nplt.title('TRAINING LOSS vs EPOCHS')","94087e5f":"#start building the model\nmodel1 = Sequential()\nmodel1.add(Conv2D(32, (5,5), strides = (2,2), input_shape = (x_train.shape[1:]), padding = 'same'))\nmodel1.add(Activation('relu'))\nmodel1.add(BatchNormalization())\n#model1.add(MaxPooling2D(pool_size =  (2,2)))\n\nmodel1.add(Conv2D(32, (3,3), strides = (2,2), padding = 'same'))\nmodel1.add(Activation('relu'))\nmodel1.add(BatchNormalization())\n#model1.add(MaxPooling2D(pool_size =  (2,2)))\n\n# model1.add(Conv2D(128, (3,3), padding = 'same'))\n# model1.add(Activation('relu'))\n# model1.add(BatchNormalization())\n# model1.add(MaxPooling2D(pool_size =  (2,2)))\n\nmodel1.add(Flatten())\n\nmodel1.add(Dense(32))\nmodel1.add(Activation('relu'))\nmodel1.add(BatchNormalization())\n\nmodel1.add(Dense(y_train.shape[1]))\nmodel1.add(Activation('softmax'))\n\nmodel1.summary()","1d9aa3c6":"#compile the model\noptim = Adam(lr = 0.001) #using the already available learning rate scheduler\nmodel1.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = ['accuracy'])","0188ed3c":"#fit the model on our dataset\nhistory1 = model1.fit(x_train, y_train, epochs = 25, batch_size = 64)","2ea733ea":"#let's look how our model performed by plotting the accuracy and loss curves\nsns.set(style = 'darkgrid')\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(25), history1.history['acc'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING ACCURACY')\nplt.title('TRAINING ACCURACY vs EPOCHS')\n\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(25), history1.history['loss'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING LOSS')\nplt.title('TRAINING LOSS vs EPOCHS')","f6bd35e8":"#start building the model\nmodel2 = Sequential()\nmodel2.add(Conv2D(32, (5,5), input_shape = (x_train.shape[1:]), padding = 'same'))\nmodel2.add(Activation('relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(MaxPooling2D(pool_size =  (2,2), strides = (2,2)))\nmodel2.add(Dropout(0.2))\n\nmodel2.add(Conv2D(32, (3,3), padding = 'same'))\nmodel2.add(Activation('relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(MaxPooling2D(pool_size =  (2,2), strides = (2,2)))\nmodel2.add(Dropout(0.2))\n\n# model1.add(Conv2D(128, (3,3), padding = 'same'))\n# model1.add(Activation('relu'))\n# model1.add(BatchNormalization())\n# model1.add(MaxPooling2D(pool_size =  (2,2)))\n\nmodel2.add(Flatten())\n\nmodel2.add(Dense(128))\nmodel2.add(Activation('relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.5))\n\nmodel2.add(Dense(y_train.shape[1]))\nmodel2.add(Activation('softmax'))\n\nmodel2.summary()","b41a7635":"#compile the model\noptim = Adam(lr = 0.001) #using the already available learning rate scheduler\nmodel2.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = ['accuracy'])","4f976678":"#fit the model on our dataset\nhistory2 = model2.fit(x_train, y_train, epochs = 100, batch_size = 64)","f3b76963":"#let's look how our model performed by plotting the accuracy and loss curves\nsns.set(style = 'darkgrid')\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(100), history2.history['acc'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING ACCURACY')\nplt.title('TRAINING ACCURACY vs EPOCHS')\n\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(100), history2.history['loss'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING LOSS')\nplt.title('TRAINING LOSS vs EPOCHS')","f24a9144":"import keras\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.applications.vgg19 import VGG19\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator","d6067d99":"datagen = ImageDataGenerator(rescale = 1 \/ 255.,\n                            horizontal_flip = True,\n                            rotation_range = 10,\n                            width_shift_range = 0.1,\n                            height_shift_range = 0.1)","5635ee61":"mobilenet_model = MobileNet(weights = None, input_shape = (100, 100, 3), classes = 5005)\nmobilenet_model.summary()","639c0ad1":"#compile the model\noptim = Adam(lr = 0.001)\nmobilenet_model.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = ['accuracy'])","343aba28":"#fit the model on our data\nh2 = mobilenet_model.fit_generator(datagen.flow(x_train, y_train, batch_size = 64), epochs = 300, steps_per_epoch = len(x_train) \/\/ 64)","a7eb140e":"#let's look how our model performed by plotting the accuracy and loss curves\nsns.set(style = 'darkgrid')\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(300), h2.history['acc'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING ACCURACY')\nplt.title('TRAINING ACCURACY vs EPOCHS')\n\nplt.figure(figsize = (18, 14))\nplt.subplot(2, 1, 1)\nplt.plot(range(300), h2.history['loss'])\nplt.xlabel('EPOCHS')\nplt.ylabel('TRAINING LOSS')\nplt.title('TRAINING LOSS vs EPOCHS')","a7b59278":"test_data = os.listdir(\"..\/input\/test\/\")\nprint(len(test_data))","4d365fb9":"test_data = pd.DataFrame(test_data, columns = ['Image'])\ntest_data['Id'] = ''","ec513a2a":"x_test = PrepareTrainImages(test_data, test_data.shape[0], \"test\")\nx_test = x_test.astype('float32') \/ 255","6480a926":"predictions = mobilenet_model.predict(np.array(x_test), verbose = 1)","770b3af0":"for i, pred in enumerate(predictions):\n    test_data.loc[i, 'Id'] = ' '.join(le.inverse_transform(pred.argsort()[-5:][::-1]))","8fd67234":"test_data.to_csv('model_submission4.csv', index = False)","39bec612":"From the above plot we can observe that after about 90 epochs the loss became stagnant. ","b2e447a6":"**CNN WITH BATCH NORMALIZATION**","6411ef3d":">**We will start by building the CNN model without implementing any regularization techniques such as dropout or batch normalization just to analyze how important it is to use some of the regularization techniques almost everytime to avoid overfitting of your model.","2187a7d1":"> Here we have two columns. One colum corresponds to the images for the whales while the other column corresponds to the image id for the whale.","b727d416":"**Once we got the training data the next step would be to normalize the data so that all the pixel values lie in the same range.**","ced98edc":"*** 1. READING IN THE FILES**","24fafe3b":"> **Let's prepare our data so that we can train our CNN model onto it. The images here are in the form of string. We know that our CNN model takes images in the form of array as input. So we need to convert our string into array format so that we can feed them to our CNN.**","441ab9c5":"> If we try to analyze the above three different CNN models, we can clearly see some difference between the model using a regularization technique and the one which isn't. The first model in which no regularization technique was implemented resulted in a good accuracy but to reach that accuracy we required very little time. This is not always bad but also not ideal. The model must take it's time to learn all the features from our dataset and as the epochs progresses the model starts to learn more and more about the dataset and the loss starts to gradually decrease. In the first model the loss did decrease but the decrease was very steep. While in the third model where we implemented both dropout as well as batch normalization the decrease in loss was gradual which is what it should be. While for the model one reached good accuracy in less number of epochs it took more number of epochs for model three to match the performance of the model one and two.\n\n>We can train the model three for even more number of epochs to achieve the accuracy close to 99% on the training dataset.","07ae3a3d":"> Here instead of increasing the strides I have used Maxpooling for downsampling. Also dropout and Batch normalization is also been used as regularization techniques.","ba06c759":" **MAKING PREDICTIONS ON TEST DATA**","bb364c3f":"**4. DATA VISUALIZATION**","10f01b62":">**while obtaining our data into an array format our data has to follow a specific format. The usual format for the input data that is to be feed to the CNN model is of the form --> (batch_size, height, width, channels). Here the batch size is nothing but the total number of rows in our train dataframe i.e 25361. The height and width is something that we can arbitrarily choose. It's always a good practice to choose the height and width in such a way as it should be as minimum as possible but not as small that it would become difficult for us to interpret the image itself. So the values should be such that we should be able to tell the content of the image by looking at it. The main idea is to reduce the total number of parameters as far as possible to reduce the computational time.**","244b17cd":"**6. BUILDING THE CNN MODEL**","30b21edc":"**USING MOBILENET ARCHITECTURE WITH IMAGE DATA GENERATOR**","100d5cbd":">From the above plot we can see that the count of ID's corresponding to the pics that were labelled as new_whale is maximum. The images with new whales amount to a total of about 9700 which is more than half of total available images.","1450ee8c":"**3. NORMALIZE THE DATA**","37246ac1":">Here I haven't used maxpooling for downsampling. Instead I have increased the amount of stride each (5,5) filter will take while moving over the input image. Also Batch normalization is implemented as a regularization technique to avoid overfitting.","2b61d1ce":"**2. PREPARING OUR TRAINING IMAGE DATA**","0047d3b1":"**Now that we have our training dataset prepared we are ready to build our CNN model. Let's start by building a simple CNN model\nThis model will have the following layer arrangements**\n\n***(CONV2D -> ACTIVATION -> MAXPOOLING) --- (CONV2D -> ACTIVATION -> MAXPOOLING) --- (FLATTEN)---(DENSE -> ACTIVATION)***","db4094e7":"**5. LABLE ENCODING AND ONE HOT ENCODING THE ID COLUMN  VALUES**","3108c0ea":"**CNN WITH BATCH NORMALIZATION AND DROPOUT**"}}