{"cell_type":{"8ac99922":"code","d3aefb60":"code","d7e419e7":"code","643cbe24":"code","a377cc89":"code","475da744":"code","f92d7f38":"code","d5eaab84":"code","38d7cd83":"code","a308e542":"code","4ab69292":"code","5d7671a8":"code","3f672e89":"code","593f4353":"code","aaeb3acc":"code","596f0d24":"code","8ea91648":"code","f8a0dbe2":"code","92936a18":"code","73d0fab8":"code","e0adc7f9":"code","cb794b19":"code","303a5ff4":"code","2539b23d":"code","7454ea6f":"code","978a51eb":"code","3df8f39c":"code","c235723c":"code","811c861b":"code","79a8ecc3":"code","01b157fd":"code","1432fd55":"code","907a19dd":"code","afe10573":"code","10a65b93":"code","2e42d98e":"code","8e984ec1":"code","aad0b525":"code","f0ea3b17":"code","86451225":"code","13d90437":"code","e475fdf6":"code","44b5ce09":"code","5775d3b7":"code","4b5e1882":"code","e73b4e9a":"code","23d7cbbc":"code","4d780ddd":"code","e878da47":"code","4aea2e42":"code","7d7b05c9":"code","99ebeb06":"code","c5c17ee7":"code","c29ac819":"code","596e4bfe":"code","d067e1df":"code","06b24a04":"code","d08717c3":"code","708fe73e":"code","6f9d96bb":"code","db3bc2ab":"code","2c03edaa":"code","97ce34aa":"code","864c779d":"code","dd3e3467":"markdown","8c171a74":"markdown","544f090d":"markdown","0c2af782":"markdown","81b96d26":"markdown","27aba03e":"markdown","1678218a":"markdown","39fe8860":"markdown"},"source":{"8ac99922":"import os, shutil\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nfrom keras.applications import VGG16\nimport numpy as np\nimport pandas as pd","d3aefb60":"data = os.listdir('\/kaggle\/input\/dogs-vs-cats\/train\/train')\nlen(data)","d7e419e7":"original_dataset_dir = '\/kaggle\/input\/dogs-vs-cats\/train\/train'","643cbe24":"base_dir = 'catndogs_small'\nos.mkdir(base_dir)","a377cc89":"train_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)","475da744":"train_cats_dir = os.path.join(train_dir, 'cats')\nos.mkdir(train_cats_dir)\n\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nos.mkdir(train_dogs_dir)","f92d7f38":"test_cats_dir = os.path.join(test_dir, 'cats')\nos.mkdir(test_cats_dir)\n\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nos.mkdir(test_dogs_dir)","d5eaab84":"validation_cats_dir = os.path.join(validation_dir, 'cats')\nos.mkdir(validation_cats_dir)\n\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\nos.mkdir(validation_dogs_dir)","38d7cd83":"fnames = ['cat.{}.jpg'.format(i) for i in range(10000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)","a308e542":"fnames = ['cat.{}.jpg'.format(i) for i in range(10000, 12000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)","4ab69292":"fnames = ['cat.{}.jpg'.format(i) for i in range(12000, 12500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)","5d7671a8":"fnames = ['dog.{}.jpg'.format(i) for i in range(10000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)","3f672e89":"fnames = ['dog.{}.jpg'.format(i) for i in range(10000, 12000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)","593f4353":"fnames = ['dog.{}.jpg'.format(i) for i in range(12000, 12500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)","aaeb3acc":"print('Total training cat images: ', len(os.listdir(train_cats_dir)))\nprint('Total training dog images: ', len(os.listdir(train_dogs_dir)))\nprint('Total validation cat images: ', len(os.listdir(validation_cats_dir)))\nprint('Total validation dog images: ', len(os.listdir(validation_dogs_dir)))\nprint('Total test cat images: ', len(os.listdir(test_cats_dir)))\nprint('Total test dog images: ', len(os.listdir(test_dogs_dir)))","596f0d24":"model = Sequential()\nmodel.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(64, (3,3), activation='relu'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\nmodel.summary()","8ea91648":"from keras.preprocessing.image import ImageDataGenerator","f8a0dbe2":"train_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","92936a18":"train_generator = train_datagen.flow_from_directory(train_dir, target_size=(150, 150), batch_size=20, class_mode='binary')","73d0fab8":"validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(150, 150), batch_size=20, class_mode='binary')","e0adc7f9":"history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=30, validation_data=validation_generator, validation_steps=50)\nmodel.save('cats_and_dogs_smallnew.h5')","cb794b19":"!pip install keras2onnx","303a5ff4":"from tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.models import load_model\nimport onnx\nimport keras2onnx\n\nonnx_model_name = 'catndogvr.onnx'\n\nmodel = load_model('cats_and_dogs_smallnew.h5')\nonnx_model = keras2onnx.convert_keras(model, model.name)\nonnx.save_model(onnx_model, onnx_model_name)","2539b23d":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)","7454ea6f":"plt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and Validation accuracy')\nplt.legend()\nplt.figure()","978a51eb":"plt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","3df8f39c":"datagen = ImageDataGenerator(rotation_range=40,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            shear_range=0.2,\n                            zoom_range=0.2,\n                            horizontal_flip=True,\n                            fill_mode='nearest')","c235723c":"fnames =[os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]","811c861b":"img_path = fnames[3]","79a8ecc3":"img = image.load_img(img_path, target_size = (150,150))\nx = image.img_to_array(img)\nx = x.reshape((1, ) + x.shape)\n\ni= 0\nfor batch in datagen.flow(x, batch_size=1):\n    plt.figure(i)\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i+= 1\n    if i % 4 ==0:\n        break\nplt.show()","01b157fd":"model = Sequential()\nmodel.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(64, (3,3), activation='relu'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\nmodel.summary()","1432fd55":"train_datagen = ImageDataGenerator(\n                            rescale= 1.\/255,\n                            rotation_range=40,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            shear_range=0.2,\n                            zoom_range=0.2,\n                            horizontal_flip=True)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","907a19dd":"train_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(150,150),\n        batch_size=32,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150,150),\n        batch_size=32,\n        class_mode='binary')","afe10573":"history = model.fit_generator(train_generator,\n                             steps_per_epoch=100,\n                             epochs=30,\n                             validation_data=validation_generator,\n                             validation_steps=50)\nmodel.save('cats_and_dogs_small_2(aug and dropout).h5')","10a65b93":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)","2e42d98e":"plt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and Validation accuracy')\nplt.legend()\nplt.figure()","8e984ec1":"plt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","aad0b525":"conv_base = VGG16(weights = 'imagenet',\n                 include_top = False,\n                 input_shape = (150, 150, 3))","f0ea3b17":"conv_base.summary()","86451225":"datagen = ImageDataGenerator(rescale=1.\/255)\nbatch_size = 20","13d90437":"def extract_features(directory, sample_count):\n    features = np.zeros(shape=(sample_count,4 ,4, 512))\n    labels = np.zeros(shape=(sample_count))\n    generator = datagen.flow_from_directory(\n                directory,\n                target_size=(150, 150),\n                batch_size=batch_size,\n                class_mode='binary')\n    i=0\n    \n    for input_batch , label_batch in generator:\n        features_batch = conv_base.predict(input_batch)\n        features[i* batch_size : (i + 1)* batch_size] = features_batch\n        labels[i*batch_size : (i + 1)* batch_size] = label_batch\n        i +=1\n        if i* batch_size >= sample_count:\n            break\n    return features, labels","e475fdf6":"train_features, train_lablels = extract_features(train_dir, 2000)\nvalidation_features, validation_lablels = extract_features(validation_dir, 1000)\ntest_features, test_lablels = extract_features(test_dir, 1000)","44b5ce09":"train_features = np.reshape(train_features, (2000, 4*4*512))\nvalidation_features = np.reshape(validation_features, (1000, 4*4*512))\ntest_features = np.reshape(test_features, (1000, 4*4*512))","5775d3b7":"model = Sequential()\nmodel.add(Dense(256, activation = 'relu', input_dim = 4 *4* 512))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","4b5e1882":"model.compile(optimizer= optimizers.RMSprop(lr = 2e-5), loss='binary_crossentropy', metrics = ['acc'])","e73b4e9a":"history = model.fit(train_features, train_lablels, epochs = 30, batch_size = 20, validation_data = (validation_features, validation_lablels))","23d7cbbc":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)","4d780ddd":"plt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and Validation accuracy')\nplt.legend()\nplt.figure()","e878da47":"plt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","4aea2e42":"model = Sequential()\nmodel.add(conv_base)\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dense(1, activation = 'relu'))","7d7b05c9":"conv_base.trainable = False\nmodel.summary()","99ebeb06":"train_datagen = ImageDataGenerator(\n                            rescale= 1.\/255,\n                            rotation_range=40,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            shear_range=0.2,\n                            zoom_range=0.2,\n                            horizontal_flip=True)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","c5c17ee7":"train_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(150,150),\n        batch_size=32,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150,150),\n        batch_size=32,\n        class_mode='binary')","c29ac819":"model.compile(optimizer= optimizers.RMSprop(lr = 2e-5), loss='binary_crossentropy', metrics = ['acc'])","596e4bfe":"history = model.fit_generator(train_generator,\n                             steps_per_epoch=100,\n                             epochs=30,\n                             validation_data=validation_generator,\n                             validation_steps=50)\nmodel.save('cats_and_dogs_small_2_VGG16_with_Aug.h5')","d067e1df":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)","06b24a04":"plt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and Validation accuracy')\nplt.legend()\nplt.figure()","d08717c3":"plt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","708fe73e":"conv_base.trainable = True\n\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","6f9d96bb":"model.compile(optimizer= optimizers.RMSprop(lr = 1e-5), loss='binary_crossentropy', metrics = ['acc'])","db3bc2ab":"model.summary()","2c03edaa":"history = model.fit_generator(train_generator,\n                             steps_per_epoch=100,\n                             epochs=30,\n                             validation_data=validation_generator,\n                             validation_steps=50)\nmodel.save('cats_and_dogs_small_2_VGG16_with_Aug_with_unfreeze_some_layers.h5')","97ce34aa":"test_generator = test_datagen.flow_from_directory(test_dir,\n                                                 target_size=(150,150),\n                                                 batch_size=20,\n                                                 class_mode='binary')","864c779d":"test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\nprint('Test Acc :', str(test_acc))","dd3e3467":"### Overfitting can be seen in the model,now we use data augmentation from keras to increase the validation accuracy","8c171a74":"### Freezing all layers upto a specific one layer.","544f090d":"### we achieved the accuracy of almost 87% with this optimisation, now to further increase the accuracy we need to put in more data, for that we are using pretrained convnet","0c2af782":"## Cats Vs Dogs using Convnets, VGG16, and Freeezing","81b96d26":"### Feature extraction without data augmentation","27aba03e":"#### Training a neural network with dropout and image augmentation","1678218a":"### Data Preprocessing","39fe8860":"### Feature Extraction using Data Augmentation"}}