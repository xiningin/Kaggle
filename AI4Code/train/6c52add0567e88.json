{"cell_type":{"07e122c4":"code","27ac0ed1":"code","3f4640d5":"code","9a5c2287":"code","31a39658":"code","f0613ae4":"code","782f90a8":"code","97316b00":"code","4936ba99":"code","fb2d5688":"code","e2ae26eb":"code","e6ad8d80":"code","a655de6e":"code","9bc4788a":"code","1f759755":"code","e9402d72":"code","82c5be2d":"code","629ec816":"code","c96ca6e1":"code","4f6a4b0a":"code","0fe7da9b":"code","a8f6ae1d":"code","0888bfbe":"markdown","181d19f8":"markdown","167d3e7e":"markdown","c2ae3fe1":"markdown","a1ed4190":"markdown","879dd9a3":"markdown","5fe4f3ba":"markdown","68fc8888":"markdown","e2c4ba48":"markdown","5a9d9d59":"markdown","280f9b26":"markdown","fddf292a":"markdown","9367d5c2":"markdown","11da1b04":"markdown","a559b1ea":"markdown"},"source":{"07e122c4":"import tensorflow as tf\nimport cv2\nimport numpy as np\n# from google.colab.patches import cv2_imshow\nfrom tqdm import tqdm\nimport os","27ac0ed1":"train_zip_path = r'\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip'\ntest_zip_path = r'\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip'\nbase_dir = '\/kaggle\/working\/Data\/'\nunzip_dir = '\/kaggle\/working\/data\/'","3f4640d5":"if not os.path.exists(unzip_dir):\n    os.makedirs(unzip_dir)\n    print(f'Directory ceated at {unzip_dir}')\n    \nimport zipfile\nwith zipfile.ZipFile(train_zip_path) as f:\n    f.extractall(unzip_dir)\nf.close","9a5c2287":"\nlabels_path = tf.keras.utils.get_file(\n    'ImageNetLabels.txt',\n    'https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ImageNetLabels.txt')\nlabels = np.array(open(labels_path).read().splitlines())","31a39658":"dog_classes = []\nfor idx, i in enumerate(labels):\n    if 'dog' in i:\n        dog_classes.append(idx)\n        print(i)\ndog_classes.append(207) # After going though some classes i realized to add golden-retriver present at index 207 \ndog_classes.append(245)","f0613ae4":"os.listdir('\/kaggle\/working\/data\/train\/')[1750]","782f90a8":"path =r'\/kaggle\/working\/data\/train\/dog.11215.jpg'\nimg = cv2.imread(path)","97316b00":"import matplotlib.pyplot as plt\nplt.imshow(img)","4936ba99":"print(img.shape) # In the openCv image is of shape (h, w)\norg = img.copy()","fb2d5688":"window_size = [150,100] \nwindow_size","e2ae26eb":"model = tf.keras.applications.ResNet50()","e6ad8d80":"\nbounding_box = []  # We will store coordinate of bounding box (x1, y1, x2, y2)\ncropped_images = [] # We will store cropped images, So that we can predict the class of an image\n\nfor w in range(8): # I have choosen random value as 8, This means our window will increase 20% each time.\n    for y in tqdm(range(0, img.shape[0], 80)): # window will cover 80 pixel in the vertical direction \n        for inc in range(0, img.shape[1], 50): # window will cover 40 pixel in the horizontal direction\n\n            # This condition will prevent window to go outside of image\n            if inc +window_size[0]>img.shape[1] or y+window_size[1]>img.shape[0]:  \n                break\n            \n            # This commented line of code will draw sliding window\n#             img = cv2.rectangle(img, (inc, y), (inc + window_size[0], y+window_size[1]), (255,125,210), 2)\n            \n            # We will crop the image of size of sliding window, Make sure we specify dimention (y, x) in opencv slicing\n            crop_img = img[ y:y+window_size[1], inc:inc + window_size[0]]\n            \n            # Seperating B G R channel, as opencv read image as BGR, But to pass image to model we want RGB.\n            # So by doing this we will convert BGR to RGB\n            \n            # This is fficiant way to do slicing or you can use cv.split\n            b = crop_img[:,:,0]\n            g = crop_img[:,:,1]\n            r = crop_img[:,:,2]\n            merge = cv2.merge([r,g,b])\n\n            cropped_images.append(merge)\n            bounding_box.append([ inc,y, inc + window_size[0], y+window_size[1]])\n            \n            \n            ########################################################################################\n            ## This code below was to to predict cropped image one by one, and to show sliding window, \n            ## If uncomment and run it will give you, good visualization how sliding window is working \n            #########################################################################################\n            # n_img = tf.keras.applications.resnet50.preprocess_input(merge)\n            # prediction = model.predict(tf.expand_dims(tf.image.resize(n_img, (224, 224)), 0))\n            # label_no = np.argmax(prediction) \n\n            # if label_no == 6 and prediction[0][label_no] > 0.80:    # Set prob of class here\n            #     bounding_box.append([ inc,y, inc + window_size[0], y+window_size[1]])\n                \n            # lab = labels[label_no] + '  ' + str(prediction[0][label_no]) \n        \n            # cv2.putText(img, lab, (15,15),cv2.FONT_HERSHEY_COMPLEX,  0.7,\n            #        (0,0,0),2,4 )\n            # cv2.imshow(img)\n            # cv2.imshow( crop_img)\n            img = org.copy()\n            # if cv2.waitKey(1) & 0xFF == ord('q'):\n            #     break\n            ############################################################################################\n            \n    window_size = [int(1.2*i) for i in window_size]  # increasing y and x by 20% \n# cv2.waitKey(0)\n# cv2.destroyAllWindows()","a655de6e":"def resize_image(x):\n    x = tf.convert_to_tensor(x)\n    return tf.image.resize(x, (224,224))\n\ndef convert_images(cropped_images):\n    return [resize_image(i) for i in cropped_images]\nli = convert_images(cropped_images)","9bc4788a":"# Let's preprocess images using resnet50 preprocessing\nall_images_in_array = tf.keras.applications.resnet50.preprocess_input(tf.stack(li))","1f759755":"predict_prob = model.predict(all_images_in_array) # Get probability distrubution of each images\nindex_of_required_box = []                        # We will store idex of image which belongs to dog class having probability >=85%\nfor idx, prob in enumerate(predict_prob):\n    id = np.argmax(prob)                     # Get index with highest probability\n    if id in dog_classes and prob[id]>0.60:  # Condition to check if probability >= 85%\n        index_of_required_box.append(idx)    # Storing indexes of images in list","e9402d72":"np.argmax(predict_prob, axis = 1)","82c5be2d":"index_of_required_box","629ec816":"np.array(bounding_box)[[index_of_required_box]]","c96ca6e1":"all_bnding_box = org.copy()\nfor i in np.array(bounding_box)[[index_of_required_box]]:\n    cv2.rectangle(all_bnding_box, (i[0],  i[1]), (i[2], i[3]), (255,125,210), 3)\n\nplt.imshow(all_bnding_box)","4f6a4b0a":"def NMS(boxes, overlapThresh):\n    #return an empty list, if no boxes given\n    if len(boxes) == 0:\n        return []\n    x1 = boxes[:, 0]  # x coordinate of the top-left corner\n    y1 = boxes[:, 1]  # y coordinate of the top-left corner\n    x2 = boxes[:, 2]  # x coordinate of the bottom-right corner\n    y2 = boxes[:, 3]  # y coordinate of the bottom-right corner\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by the bottom-right y-coordinate of the bounding box\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1) # We have a least a box of one pixel, therefore the +1\n    indices = np.arange(len(x1))\n    for i,box in enumerate(boxes):\n        temp_indices = indices[indices!=i]\n        xx1 = np.maximum(box[0], boxes[temp_indices,0])\n        yy1 = np.maximum(box[1], boxes[temp_indices,1])\n        xx2 = np.minimum(box[2], boxes[temp_indices,2])\n        yy2 = np.minimum(box[3], boxes[temp_indices,3])\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n        # compute the ratio of overlap\n        overlap = (w * h) \/ areas[temp_indices]\n        if np.any(overlap) > overlapThresh:\n            indices = indices[indices != i]\n    return boxes[indices].astype(int)","0fe7da9b":"NMS_box = NMS(np.array(bounding_box)[[index_of_required_box]], overlapThresh = 0.6)\nNMS_box = NMS_box.flatten()","a8f6ae1d":"bnding_box_image = org.copy()\nfor i in bounding_box:\n    cv2.rectangle(bnding_box_image, (NMS_box[0],  NMS_box[1]), (NMS_box[2], NMS_box[3]), (255,125,210), 3)\n\nplt.imshow(bnding_box_image)","0888bfbe":"## 1. Import  Liabraries  \n<a id = \"section_1\"><\/a>\n","181d19f8":"reference for [Non maximum seperation](https:\/\/www.pyimagesearch.com\/2015\/02\/16\/faster-non-maximum-suppression-python\/) function","167d3e7e":"## 3. Load human readable Imagenet labels  \n<a id ='label'><\/a>","c2ae3fe1":"## Index  \n* 1. [Import Liabraries](#section_1)    \n* 2. [Unzip the files](#section_2)  \n* 3. [Human Readable Lable](#label)  \n* 4. [Load Pretrained Model](#model)  \n* 5. [Predict Crop Image](#Bounding_Box)  \n* 6. [NMS](#NMS)\n","a1ed4190":"## 4. Pretrained Model  \n<a id ='model'><\/a>\n","879dd9a3":"Let's visualize windows predicted by our model for dog","5fe4f3ba":"## 5. Predict Cropped Images","68fc8888":"I will be happy, If you let me know the improvenment and upvote if you like.","e2c4ba48":"## 6. NMS","5a9d9d59":"We will need labels of imagenet , So we will download text file of labels of imagenet.","280f9b26":"## Code to detect the Object  \n<a id ='object'><\/a>","fddf292a":"We are using pretrained resnet 50, So this model requires 224,224 image. But our cropped images are different shapes.\nUsing tf.image.resize we will resize all images in an array.","9367d5c2":"![flowchart for object detection.png](attachment:127637fa-9825-42b2-95e5-ed6fb42bcf8c.png)","11da1b04":"## 2. Unzip file  \n<a id = 'section_2'><\/a>","a559b1ea":"Now, We know what kind of classes we trying to classify, for example in our case dog, Now, there are several types of dog in imagenet    \nlabels, So we will store in list so that we can use later in the prediction. (To verify predicted class as dog)  \n"}}