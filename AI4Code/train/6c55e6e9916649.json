{"cell_type":{"b277d348":"code","e6952fc4":"code","3becf438":"code","7260a056":"code","ee2f9e73":"code","6e5ec2da":"code","18b9d80a":"code","cf9a892d":"code","f0063e4d":"code","8eae3438":"code","ff7f7f08":"code","f25d33bb":"code","66a2c7b3":"code","a80b4364":"code","14793a15":"code","ec1d8abb":"code","d29a606d":"code","501fa3ce":"code","44555592":"code","367a5c5a":"code","29462ac5":"code","566b4857":"code","b7d83e35":"code","8bdbf034":"code","c5d520af":"code","78d32d4a":"code","87e763ad":"code","d4e11484":"code","fde97912":"code","0156b946":"code","f517b24c":"code","4a220f05":"code","a816064c":"code","d7579b57":"code","ad3b1c7a":"markdown","abbc2b59":"markdown","c15bd8b9":"markdown","de2080c3":"markdown","1b6258f7":"markdown","930713e0":"markdown","333ba930":"markdown","13a92e9b":"markdown","7558a0f3":"markdown","6cedf5c9":"markdown"},"source":{"b277d348":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport datetime,random\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom time import time\nfrom scipy.stats import skew, boxcox, mstats\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e6952fc4":"SEED = 42\nFOLDS = 2\n\ndef seed_env(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\npd.set_option('display.max_columns', 380)\npd.set_option('display.max_rows', 500)\nseed_env()\n\nsns.set()\n%matplotlib inline","3becf438":"def reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","7260a056":"def download_train_dataframe(csv_sources, full_download = True):\n    if full_download:\n        train = reduce_mem_usage(pd.read_csv(csv_sources[0]))\n    else:\n        train = reduce_mem_usage(pd.read_csv(csv_sources[0],nrows = 10000000))\n        \n    building_metadata_df = reduce_mem_usage(pd.read_csv(csv_sources[1]))\n    weather_df = reduce_mem_usage((pd.read_csv(csv_sources[2])))\n    \n    train = train.merge(building_metadata_df, on=\"building_id\", how=\"left\")\n    train.drop(\"meter_reading\", axis=1, inplace=True)\n    train = train.merge(weather_df, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    train.reset_index()\n    \n    del building_metadata_df,weather_df\n    gc.collect()\n\n    return train\n\ndef download_test_dataframe(csv_sources):\n    test = reduce_mem_usage(pd.read_csv(csv_sources[0],skiprows = 800, nrows = 20000000,names = ['row_id','building_id','meter','timestamp']))\n    building_metadata_df = reduce_mem_usage(pd.read_csv(csv_sources[1]))\n    weather_df = reduce_mem_usage((pd.read_csv(csv_sources[2])))\n    \n    test = test.merge(building_metadata_df, on=\"building_id\", how=\"left\")\n    test.drop(\"row_id\", axis=1, inplace=True)\n    test = test.merge(weather_df, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    test.reset_index()\n    \n    del building_metadata_df,weather_df\n    gc.collect()\n\n    return test\n\ndef dataframe_num_feature_summary(df):\n    summary = pd.DataFrame(df.dtypes,columns = [\"dtypes\"])\n    summary = summary.reset_index()\n    summary[\"Feature\"] = summary[\"index\"]\n    summary = summary[[\"Feature\",\"dtypes\"]]\n    summary[\"Missed\"] = df.isna().sum().values\n    df_num_describe = df.describe()\n    summary[\"Count\"] = df_num_describe.iloc[0,:].values\n    summary[\"% Missed\"] = (summary[\"Missed\"] \/ df.shape[0]) * 100\n    summary[\"% Missed\"] = round(summary[\"% Missed\"], 2)\n    summary[\"Mean\"] = df_num_describe.iloc[1,:].values\n    summary[\"Std\"] = df_num_describe.iloc[2,:].values\n    summary[\"Min\"] = df_num_describe.iloc[3,:].values\n    summary[\"Max\"] = df_num_describe.iloc[7,:].values\n    summary[\"25 %\"] = df_num_describe.iloc[4,:].values\n    summary[\"75 %\"] = df_num_describe.iloc[6,:].values\n    \n    return summary","ee2f9e73":"def visualize_feature_importance(model,columns):\n    feature_imp = pd.DataFrame(sorted(zip(model.feature_importance(),columns), reverse=True), columns=[\"Value\",\"Feature\"])\n    plt.figure(figsize=(10,10))\n    importance_bar = sns.barplot(data=feature_imp, x='Value', y='Feature')\n    plt.show()","6e5ec2da":"csv_sources = ['..\/input\/ashrae-energy-prediction\/train.csv', '..\/input\/ashrae-energy-prediction\/building_metadata.csv', '..\/input\/ashrae-energy-prediction\/weather_train.csv']\ntrain_df = download_train_dataframe(csv_sources)\nprint(train_df.shape)\n\ncsv_sources = ['..\/input\/ashrae-energy-prediction\/test.csv', '..\/input\/ashrae-energy-prediction\/building_metadata.csv', '..\/input\/ashrae-energy-prediction\/weather_test.csv']\ntest_df = download_test_dataframe(csv_sources)\nprint(test_df.shape)","18b9d80a":"train_df[\"SELECTION\"] = 1\ntest_df[\"SELECTION\"] = 0\ntrain_test_df = pd.concat([train_df,test_df])\ntrain_test_df[\"SELECTION\"] = train_test_df[\"SELECTION\"].astype(np.uint8)\ntrain_test_df.reset_index()\ndel train_df,test_df\ngc.collect()","cf9a892d":"train_test_df[\"primary_use\"] = train_test_df[\"primary_use\"].astype(str)\nle = LabelEncoder()\ntrain_test_df[\"primary_use\"] = le.fit_transform(train_test_df[\"primary_use\"])\n    \ntrain_test_df['timestamp'] = pd.to_datetime(train_test_df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\ntrain_test_df['month'] = train_test_df['timestamp'].dt.month.astype(np.int8)\ntrain_test_df['day'] = train_test_df['timestamp'].dt.day.astype(np.int8)\ntrain_test_df['hour'] = train_test_df['timestamp'].dt.hour.astype(np.int8)\ntrain_test_df['weekday'] = train_test_df['timestamp'].dt.weekday.astype(np.int8)\ntrain_test_df.drop('timestamp',axis=1,inplace=True)","f0063e4d":"train_test_df.head()","8eae3438":"train_test_target = train_test_df[\"SELECTION\"]\ntrain_test_df.drop('SELECTION',axis=1,inplace=True)\ncategorical_features = [\"meter\", \"primary_use\", \"cloud_coverage\",\"building_id\", \"month\", \"day\", \"hour\",\"weekday\",\"site_id\"]","ff7f7f08":"%%time\nparams = {\n          \"objective\" : \"binary\",\n          \"metric\" : \"auc\",\n          \"boosting_type\": \"gbdt\",\n          \"num_leaves\" : 11,\n          \"learning_rate\" : 0.05,\n          \"feature_fraction\": 0.85,\n          \"random_state\" : SEED}\n\ntrain_X, val_X, train_y, val_y = train_test_split(train_test_df, train_test_target, test_size=0.3, random_state=SEED, shuffle = True)\n        \nlgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categorical_features)\n\ndel train_X, val_X, train_y, val_y\ngc.collect()\n\nlgbmodel = lgb.train(params, lgb_train, num_boost_round=60, valid_sets=[lgb_train, lgb_eval], early_stopping_rounds=60, verbose_eval=60)","f25d33bb":"visualize_feature_importance(lgbmodel, train_test_df.columns)","66a2c7b3":"def visualize_train_test_feature_distribution(df):\n    train = df.iloc[:20216100]\n    test = df.iloc[20216100:]\n    print(train.shape)\n    print(test.shape)\n    fig,axis = plt.subplots(3, 3, figsize=(12, 16))\n    \n    sns.distplot(train[\"building_id\"], ax=axis[0,0], color = 'blue')\n    sns.distplot(test[\"building_id\"], ax=axis[0,0], color = 'red')\n    \n    sns.distplot(train[\"site_id\"], ax=axis[0,1], color = 'blue')\n    sns.distplot(test[\"site_id\"], ax=axis[0,1], color = 'red')\n    \n    sns.distplot(train[\"day\"], ax=axis[0,2], color = 'blue')\n    sns.distplot(test[\"day\"], ax=axis[0,2], color = 'red')\n    \n    sns.distplot(train[\"month\"], ax=axis[1,0], color = 'blue')\n    sns.distplot(test[\"month\"], ax=axis[1,0], color = 'red')\n    \n    sns.distplot(train[\"air_temperature\"].dropna(), ax=axis[1,1], color = 'blue')\n    sns.distplot(test[\"air_temperature\"].dropna(), ax=axis[1,1], color = 'red')\n    \n    sns.distplot(train[\"sea_level_pressure\"].dropna(), ax=axis[1,2], color = 'blue')\n    sns.distplot(test[\"sea_level_pressure\"].dropna(), ax=axis[1,2], color = 'red')\n    \n    sns.distplot(train[\"year_built\"].dropna(), ax=axis[2,0], color = 'blue')\n    sns.distplot(test[\"year_built\"].dropna(), ax=axis[2,0], color = 'red')\n    \n    sns.distplot(train[\"square_feet\"].dropna(), ax=axis[2,1], color = 'blue')\n    sns.distplot(test[\"square_feet\"].dropna(), ax=axis[2,1], color = 'red')\n    \n    sns.distplot(train[\"primary_use\"].dropna(), ax=axis[2,2], color = 'blue')\n    sns.distplot(test[\"primary_use\"].dropna(), ax=axis[2,2], color = 'red')\n    \n    plt.show()\n    del train, test\n    gc.collect()","a80b4364":"visualize_train_test_feature_distribution(train_test_df)","14793a15":"del train_test_target,lgbmodel,lgb_train,lgb_eval\ngc.collect()","ec1d8abb":"def display_corr_map(df,cols):\n    plt.figure(figsize=(16,16))\n    sns.heatmap(df[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\n    plt.title(cols[0]+' - '+cols[-1],fontsize=14)\n    plt.show()","d29a606d":"cols_to_analyze = [\"square_feet\",\"year_built\", \"floor_count\",\"air_temperature\", \"dew_temperature\",\"precip_depth_1_hr\",\"sea_level_pressure\",\"wind_direction\",\"wind_speed\",\"meter\", \"primary_use\", \"cloud_coverage\",\"building_id\",\"site_id\",\"day\",\"hour\",\"weekday\"]\ndisplay_corr_map(train_test_df,cols_to_analyze)","501fa3ce":"dataframe_num_feature_summary(train_test_df.drop([\"primary_use\"], axis=1, inplace = False))","44555592":"train_test_df[\"primary_use\"] = train_test_df[\"primary_use\"].astype(str)\nprint(train_test_df[\"primary_use\"].describe())\nprint(\"% Missed = {0}\".format((train_test_df[\"primary_use\"].isna().sum() \/ train_test_df[\"primary_use\"].shape[0]) * 100))","367a5c5a":"del train_test_df\ngc.collect()","29462ac5":"csv_sources = ['..\/input\/ashrae-energy-prediction\/train.csv', '..\/input\/ashrae-energy-prediction\/building_metadata.csv', '..\/input\/ashrae-energy-prediction\/weather_train.csv']\ntrain_df = download_train_dataframe(csv_sources, full_download = False)\nprint(train_df.shape)","566b4857":"train_df.drop(\"building_id\",axis=1,inplace=True)\ntrain_df.drop(\"site_id\",axis=1,inplace=True)\ntrain_df[\"primary_use\"] = train_df[\"primary_use\"].astype(str)\nle = LabelEncoder()\ntrain_df[\"primary_use\"] = le.fit_transform(train_df[\"primary_use\"])\ntrain_df[\"primary_use\"] = train_df[\"primary_use\"].astype(np.int8)\n\ntrain_df['timestamp'] = pd.to_datetime(train_df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\ntrain_df['month'] = train_df['timestamp'].dt.month.astype(np.int8)\ntrain_df['day'] = train_df['timestamp'].dt.day.astype(np.int8)\ntrain_df['hour'] = train_df['timestamp'].dt.hour.astype(np.int8)\ntrain_df['weekday'] = train_df['timestamp'].dt.weekday.astype(np.int8)\ntrain_df.drop('timestamp',axis=1,inplace=True)\n\ndel le\ngc.collect()","b7d83e35":"def adversarial_data(df, feature):\n    adv_data = df.dropna(subset=[feature])\n    target = adv_data[feature].copy()\n    adv_data = adv_data.drop(feature, axis=1)\n    return adv_data, target","8bdbf034":"def train_adv_models(data,target):\n    params = {\n    \"objective\": \"regression\",\n    \"boosting\": \"gbdt\",\n    \"num_leaves\": 40,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.85,\n    \"reg_lambda\": 2,\n    \"metric\": \"rmse\"}\n    \n    kf = KFold(n_splits = FOLDS, shuffle=True, random_state=SEED)\n    models = list()\n    \n    for train_idx, valid_idx in kf.split(data, target):\n        train_X = data.iloc[train_idx]\n        val_X = data.iloc[valid_idx]\n        train_y = target.iloc[train_idx]\n        val_y = target.iloc[valid_idx]\n        \n        lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categorical_features)\n        lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categorical_features)\n        \n        del train_X,val_X,train_y,val_y\n        gc.collect()\n        \n        model = lgb.train(params,\n                lgb_train,\n                num_boost_round=200,\n                valid_sets=(lgb_train, lgb_eval),\n                early_stopping_rounds=200,\n                verbose_eval = 200)\n        \n        models.append(model)\n        del lgb_train, lgb_eval\n        gc.collect()\n    \n    return models","c5d520af":"def plot_results(target,prediction,models,columns):\n    plt.subplots(2, 2, figsize=(10, 10))\n    ax1 = plt.subplot(2,2,(1,2))\n    sns.distplot(target, ax=ax1, color = 'blue')\n    sns.distplot(prediction, ax=ax1, color = 'red')\n    ax2 = plt.subplot(223)\n    feature_imp1 = pd.DataFrame(sorted(zip(models[0].feature_importance(),columns), reverse=True), columns=[\"Value\",\"Feature\"])\n    importance_bar1 = sns.barplot(data=feature_imp1, x='Value', y='Feature', ax=ax2)\n    ax3 = plt.subplot(224)\n    feature_imp2 = pd.DataFrame(sorted(zip(models[1].feature_importance(),columns), reverse=True), columns=[\"Value\",\"Feature\"])\n    importance_bar2 = sns.barplot(data=feature_imp2, x='Value', y='Feature', ax=ax3)\n    plt.show()\n    del feature_imp1, feature_imp2\n    gc.collect()","78d32d4a":"%%time\ncategorical_features = [\"meter\", \"primary_use\", \"cloud_coverage\",\"month\", \"day\", \"hour\",\"weekday\"]\nadv_data, target = adversarial_data(train_df, 'floor_count')\nmodels = train_adv_models(adv_data, target)\npredictions = sum([model.predict(adv_data, num_iteration=model.best_iteration) for model in models])\/FOLDS\nplot_results(target,predictions,models,adv_data.columns)\ndel target,predictions,models,adv_data\ngc.collect()","87e763ad":"adv_data, target = adversarial_data(train_df,\"year_built\")\nmodels = train_adv_models(adv_data, target)\npredictions = sum([model.predict(adv_data, num_iteration=model.best_iteration) for model in models])\/FOLDS\nplot_results(target,predictions,models,adv_data.columns)\ndel target,predictions,models,adv_data\ngc.collect()","d4e11484":"adv_data, target = adversarial_data(train_df,\"precip_depth_1_hr\")\nmodels = train_adv_models(adv_data, target)\npredictions = sum([model.predict(adv_data, num_iteration=model.best_iteration) for model in models])\/FOLDS\nplot_results(target,predictions,models,adv_data.columns)\ndel target,predictions,models,adv_data\ngc.collect()","fde97912":"adv_data, target = adversarial_data(train_df, 'cloud_coverage')\ncategorical_features = [\"meter\", \"primary_use\", \"month\", \"day\", \"hour\",\"weekday\"]","0156b946":"%%time\nparams = {\n          \"objective\" : \"multiclass\",\n          \"num_class\" : 10,\n          \"num_leaves\" : 40,\n          \"learning_rate\" : 0.05,\n          \"feature_fraction\": 0.85,\n          \"bagging_seed\" : SEED}\n\ntrain_X, val_X, train_y, val_y = train_test_split(adv_data, target, test_size=0.3, random_state=SEED)\n        \nlgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categorical_features)\n\nlgbmodel = lgb.train(params, lgb_train, num_boost_round=200, valid_sets=[lgb_train, lgb_eval], early_stopping_rounds=200, verbose_eval=200)","f517b24c":"del lgb_train,lgb_eval,train_X,val_X,train_y, val_y,train_df\ngc.collect()","4a220f05":"predictions = lgbmodel.predict(adv_data, num_iteration=lgbmodel.best_iteration)","a816064c":"sns.distplot(target, color = \"blue\")\nsns.distplot(np.argmax(predictions, axis=1), color = \"red\")","d7579b57":"feature_imp1 = pd.DataFrame(sorted(zip(lgbmodel.feature_importance(),adv_data.columns), reverse=True), columns=[\"Value\",\"Feature\"])\nsns.barplot(data=feature_imp1, x='Value', y='Feature')","ad3b1c7a":"## Summary","abbc2b59":"## Adversarial validation","c15bd8b9":"## year_built","de2080c3":"## Correlation Analysis","1b6258f7":"First analyze with the biggest missing values","930713e0":"## floor_count","333ba930":"1st aim:\n\nFind and select training examples most similar to test examples and using them as a validation set. The core of this idea is training a probabilistic classifier to distinguish train\/test examples.\n\nConclusion:\n\nFrom the results of visualize_train_test_feature_distribution we see that there is a good classification based on building_id and site_id. Thus we can select the ids from peaks for validation sets.\n\n2nd aim:\n\nCorrelation analysis to find strong linear dependencies.\nConclusion:\n\nbuilding_id and site_id have a strong linear dependency. I guess we can eliminate site_id from the models. Same for air_temperature dew_temperature (in addition they have a similar % of missing values) and floor_count and squre_feet.\n\n3rd aim:\n\nFind ways to recover missing values.\n\nConclusion:\n\nfloor_count - 76 % of missing values. It can be well recovered by square_feet and year_built\n\nyear_built - 47 % of missing values. Can be well recovered by square_feet and year_built and primary_use\n\nprecip_depth_1_hr - 17 % Can be well recovered by hour air temperature and sea level\n\ncloud coverage - Can be well discovered by day, hour, temperature\n\nWe can buld models for missing values imputation before training instead of using simple imputer","13a92e9b":"## Setup Environment","7558a0f3":"## precip_depth_1_hr","6cedf5c9":"## cloud_coverage"}}