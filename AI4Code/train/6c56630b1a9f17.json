{"cell_type":{"44e3f4c5":"code","eb286c66":"code","f1f66812":"code","783c4ca7":"code","0893e071":"code","7ef5792f":"code","9478fea7":"code","eae93d1c":"code","d07e6a5b":"code","39ff9ac5":"code","ce2a67ae":"markdown","7c32c69d":"markdown","ca530c3a":"markdown","f67954b7":"markdown"},"source":{"44e3f4c5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.probability import FreqDist\n\ndataset = pd.read_csv('..\/input\/kindle_reviews.csv', na_filter=False)\ndf = dataset[:10000]","eb286c66":"df.head()","f1f66812":"df.columns","783c4ca7":"df.dtypes","0893e071":"print (\"Shape of the dataset - \", df.shape)\n#check for the missing values\ndf.apply(lambda x: sum(x.isnull()))","7ef5792f":"df['overall'].value_counts()","9478fea7":"# Remove neutral rated\ndf = df[df['overall'] != 3]\ndf['Positively Rated'] = np.where(df['overall'] > 3, 1, 0)\n\n# 22 rows from reviewText are blank. Lets add sample review for it\n#df['reviewText']=newdf['reviewText'].fillna(\"No Review\", inplace=True)\n#df = newdf.replace(np.nan, '', regex=True)\n#df.apply(lambda x: sum(x.isnull()))\n#print (newdf['reviewText'].head(10))","eae93d1c":"# Number of rating which are positively rated \ndf['Positively Rated'].mean()","d07e6a5b":"from  sklearn.model_selection import train_test_split\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['reviewText'],df['Positively Rated'], random_state=0)\nprint('X_train first entry: ', X_train.iloc[1])\nprint('\\nX_train shape: ', X_train.shape)","39ff9ac5":"from  sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom  sklearn.metrics import roc_auc_score\n\n# Fit the CountVectorizer to the training data\n# transform the documents in the training data to a document-term matrix\nvect = CountVectorizer().fit(X_train)\nX_train_vectorized = vect.transform(X_train)\n# Train the model\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n# Predict the transformed test documents\npredictions = model.predict(vect.transform(X_test))\nprint('AUC: ', roc_auc_score(y_test, predictions))\n# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n","ce2a67ae":" ### Let's split the data in train and test datasets and apply Logistic Regression model.","7c32c69d":"### Lets Review the data ","ca530c3a":"### Lets label all the reviews as \"negative review\" where rating = 1 or 2 and else as \"Postive reviews\"","f67954b7":"###  We have reviews in the range of [1-5]. Lets consider \"3\" as the neutral review, we can summarize the following points:"}}