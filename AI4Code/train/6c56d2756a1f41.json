{"cell_type":{"fd0ad8d7":"code","c1e26447":"code","f705d86e":"code","a1feb1f0":"code","4e3418c5":"code","9f8bccf4":"code","bc957df0":"code","1db6cb10":"code","4b158a6d":"code","162d9376":"code","fba14211":"code","fc273967":"code","61d8bcb0":"code","b0ca4681":"code","5c624e4f":"code","c6b94ccf":"code","2e4db503":"code","350c1378":"code","0ef7a95e":"code","f34b0ef9":"code","4903d8c9":"code","206e29b4":"code","b648666e":"code","e0401dc2":"code","9d6b7fc5":"code","bbe1c83f":"code","6ceea389":"code","6c3cdad6":"code","d2f05795":"code","4b1e8847":"markdown","a94bd798":"markdown","f4db08f0":"markdown","d3c562ea":"markdown","ec19201d":"markdown","2b335348":"markdown","11a6def2":"markdown","62de4e38":"markdown","9002b93c":"markdown","f1f5ea76":"markdown","f779c523":"markdown","cd8d5b3e":"markdown","1f5c223b":"markdown","13313ca3":"markdown"},"source":{"fd0ad8d7":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = '.\/'\nDATA_DIR = \"..\/input\/ventilator-pressure-prediction\/\"\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    # testing","c1e26447":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    competition='ventilator'\n    apex=True\n    print_freq=1000\n    num_workers=4\n    model_name='rnn'\n    scheduler='CosineAnnealingWarmRestarts' # ['linear', 'cosine', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    batch_scheduler=True\n    #num_warmup_steps=100 # ['linear', 'cosine']\n    #num_cycles=0.5 # 'cosine'\n    factor=0.995 # ReduceLROnPlateau\n    patience=7 # ReduceLROnPlateau\n    eps=1e-6 # ReduceLROnPlateau\n    T_max=50 # CosineAnnealingLR\n    T_0=20 # CosineAnnealingWarmRestarts\n    epochs=300 # 200\n    max_grad_norm=1000\n    gradient_accumulation_steps=1\n    hidden_size=512\n    lr=1e-3\n    min_lr=3e-5\n    weight_decay=1e-6\n    batch_size=256\n    n_fold=5\n    trn_fold=[0] # [0,1,2,3,4]\n    cate_seq_cols=[]\n    cont_seq_cols=['R', 'C', 'time_step', 'u_in', 'u_out']\n    train=True\n    inference=True\n    debug=False\n\nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold=[0]","f705d86e":"# ====================================================\n# Library\n# ====================================================\n\nimport os\nimport gc\nimport sys\nimport json\nimport math\nimport random\nfrom time import time\nfrom datetime import datetime\nfrom collections import Counter, defaultdict\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom tqdm.auto import tqdm\nimport category_encoders as ce\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#if CFG.apex:\n#    from apex import amp\n\nDEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(\"Python     : \" + sys.version.split(\"\\n\")[0])\nprint(\"Numpy      : \" + np.__version__)\nprint(\"Pandas     : \" + pd.__version__)\nprint(\"PyTorch    : \" + torch.__version__)\nprint(\"Running on device: {}\".format(DEVICE))","a1feb1f0":"# ====================================================\n# Utils\n# ====================================================\n\ndef get_score(y_trues, y_preds):\n    score = mean_absolute_error(y_trues, y_preds)\n    return score\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()\n\ndef decorate(s: str, decoration=None):\n    if decoration is None:\n        decoration = '\u2605' * 20\n\n    return ' '.join([decoration, str(s), decoration])\n\nclass Timer:\n    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n\n        if prefix: format_str = str(prefix) + sep + format_str\n        if suffix: format_str = format_str + sep + str(suffix)\n        self.format_str = format_str\n        self.logger = logger\n        self.start = None\n        self.end = None\n        self.verbose = verbose\n\n    @property\n    def duration(self):\n        if self.end is None:\n            return 0\n        return self.end - self.start\n\n    def __enter__(self):\n        self.start = time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = time()\n        if self.verbose is None:\n            return\n        out_str = self.format_str.format(self.duration)\n        if self.logger:\n            self.logger.info(out_str)\n        else:\n            print(out_str)","4e3418c5":"# ====================================================\n# Data Loading\n# ====================================================\n\n# only load a sample if in debug mode otherwise load full data...\n\nif CFG.debug:\n    print(decorate(\"DEBUG DATA MODE\"))\n    train = pd.read_csv(DATA_DIR + 'train.csv', nrows=80*5000)\n    test = pd.read_csv(DATA_DIR + 'test.csv', nrows=80*5000)\n    print(decorate(\"DEBUG DATA MODE\"))\nelse:\n    print(decorate(\"FULL DATA MODE\"))\n    train = pd.read_csv(DATA_DIR + 'train.csv')\n    test = pd.read_csv(DATA_DIR + 'test.csv')\n    print(decorate(\"FULL DATA MODE\"))\n\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n\n# this is wrt the scaling o\/p\nall_pressure = sorted(train.pressure.unique())\nPRESSURE_MIN = np.min(all_pressure)\nPRESSURE_MAX = np.max(all_pressure)\nPRESSURE_STEP = all_pressure[1] - all_pressure[0]\n\ndisplay(train.head())\ndisplay(test.head())\ndisplay(sub.head())","9f8bccf4":"class AbstractBaseBlock:\n    def fit(self, input_df: pd.DataFrame, y=None):\n        return self.transform(input_df)\n\n    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n        raise NotImplementedError()\n\n\nclass AddMultiplyingDividing(AbstractBaseBlock):\n    def transform(self, input_df):\n        # log-scale\n        input_df['u_in_log']=np.log1p(input_df['u_in'])\n        \n        # area fe\n        input_df['area'] = input_df['time_step'] * input_df['u_in']\n        input_df['area'] = input_df.groupby('breath_id')['area'].cumsum()\n        \n        input_df['cross'] = input_df['u_in']*input_df['u_out']\n        input_df['cross2'] = input_df['time_step']*input_df['u_out']\n        \n        input_df['u_in_cumsum'] = (input_df['u_in']).groupby(input_df['breath_id']).cumsum()\n        \n        input_df['one'] = 1\n        input_df['count'] = (input_df['one']).groupby(input_df['breath_id']).cumsum()\n        input_df['u_in_cummean'] = input_df['u_in_cumsum'] \/ input_df['count']\n        input_df = input_df.merge(\n            input_df[input_df[\"u_out\"]==0].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out0_\").reset_index(),\n            on=\"breath_id\"\n        )\n        input_df = input_df.merge(\n            input_df[input_df[\"u_out\"]==1].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out1_\").reset_index(),\n            on=\"breath_id\"\n        )\n\n        output_df = pd.DataFrame(\n            {\n                \"area\": input_df['area'],\n                #\"cross\": input_df['cross'],\n                #\"cross2\": input_df['cross2'],\n                \"u_in_cumsum\": input_df['u_in_cumsum'],\n                \"u_in_cummean\": input_df['u_in_cummean'],\n                \"u_out0_mean\": input_df['u_out0_mean'],\n                \"u_out0_max\": input_df['u_out0_max'],\n                \"u_out0_max\": input_df['u_out0_std'],\n                \"u_out1_mean\": input_df['u_out1_mean'],\n                \"u_out1_max\": input_df['u_out1_max'],\n                \"u_out1_max\": input_df['u_out1_std'],\n            }\n        )\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\n\nclass RCDummry(AbstractBaseBlock):\n    def transform(self, input_df):\n        input_df['R_dummy'] = input_df['R'].astype(str)\n        input_df['C_dummy'] = input_df['C'].astype(str)\n        #input_df['RC_dummy'] = input_df['R_dummy'] + input_df['C_dummy']\n        output_df = pd.get_dummies(input_df[[\"R_dummy\", \"C_dummy\"]])\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\n\nclass PublicLGBMFeatures(AbstractBaseBlock):\n    def transform(self, input_df):\n        g = input_df.groupby('breath_id')['u_in']\n        \n        output_df = pd.DataFrame(\n            {\n                \"ewm_u_in_mean\": g.ewm(halflife=10).mean().reset_index(level=0, drop=True),\n                \"ewm_u_in_std\": g.ewm(halflife=10).std().reset_index(level=0, drop=True),\n                \"ewm_u_in_corr\": g.ewm(halflife=10).corr().reset_index(level=0, drop=True),\n                \n                \"rolling_10_mean\": g.rolling(window=10, min_periods=1).mean().reset_index(level=0, drop=True),\n                \"rolling_10_max\": g.rolling(window=10, min_periods=1).max().reset_index(level=0, drop=True),\n                \n                \"expand_mean\": g.expanding(2).mean().reset_index(level=0, drop=True),\n                \"expand_max\": g.expanding(2).max().reset_index(level=0, drop=True),\n                \"expand_std\": g.expanding(2).std().reset_index(level=0, drop=True),\n            }\n        )\n        \n        for col in output_df.columns:\n            output_df[col] = output_df[col].fillna(output_df[col].mean())\n#             output_df[col] = output_df[col].fillna(0)\n        \n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        del g; gc.collect();\n        return output_df\n        \n\nclass AddBreathTimeAndUInTime(AbstractBaseBlock):\n    def transform(self, input_df):\n        output_df = pd.DataFrame(\n            {\n                \"breath_time\": input_df['time_step'] - input_df['time_step'].shift(1),\n                \"u_in_time\": input_df['u_in'] - input_df['u_in'].shift(1)\n            }\n        )\n        output_df.loc[input_df['time_step'] == 0, 'breath_time'] = output_df['breath_time'].mean()\n        output_df.loc[input_df['time_step'] == 0, 'u_in_time'] = output_df['u_in_time'].mean()\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\n\nclass LagFeatures(AbstractBaseBlock):\n    def transform(self, input_df):\n        output_df = pd.DataFrame(\n            {\n                \"u_in_lag1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(1).fillna(0),\n                \"u_in_lag-1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-1).fillna(0),\n                \n                \"u_in_lag2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(2).fillna(0),\n                \"u_in_lag-2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-2).fillna(0),\n                \n                \"u_in_lag3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(3).fillna(0),\n                \"u_in_lag-3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-3).fillna(0),\n                \n                \"u_in_lag4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(4).fillna(0),\n                \"u_in_lag-4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-4).fillna(0),\n                \n                \"u_out_lag1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(1).fillna(0),\n                \"u_out_lag2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(2).fillna(0),\n                \"u_out_lag3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(3).fillna(0),\n                \"u_out_lag4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(4).fillna(0),\n                \n                #\"u_out_lag-1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-1).fillna(0),\n                #\"u_out_lag-2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-2).fillna(0),\n                #\"u_out_lag-3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-3).fillna(0),\n                #\"u_out_lag-4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-4).fillna(0),\n            }\n        )\n        output_df[\"u_in_lag1_diff\"] = output_df[\"u_in_lag1\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag2_diff\"] = output_df[\"u_in_lag2\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag3_diff\"] = output_df[\"u_in_lag3\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag4_diff\"] = output_df[\"u_in_lag4\"] - input_df[\"u_in\"]\n\n        output_df[\"u_in_rolling_mean2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).mean()[\"u_in\"].reset_index(drop=True)\n        output_df[\"u_in_rolling_mean4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).mean()[\"u_in\"].reset_index(drop=True)\n        output_df[\"u_in_rolling_mean10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).mean()[\"u_in\"].reset_index(drop=True)\n        \n        if not CFG.debug:\n            output_df[\"u_in_rolling_max2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_max4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_max10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).std()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).std()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).std()[\"u_in\"].reset_index(drop=True)\n        \n        for col in output_df.columns:\n            output_df[col] = output_df[col].fillna(output_df[col].mean())\n        \n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df","bc957df0":"feature_blocks = [\n    PublicLGBMFeatures(),\n    AddMultiplyingDividing(),\n    AddBreathTimeAndUInTime(),\n    RCDummry(),\n    LagFeatures(),\n]","1db6cb10":"def run_blocks(input_df, blocks, y=None, test=False):\n    out_df = pd.DataFrame()\n\n    if test:\n        print(decorate('start run blocks [test_set]...'))\n    else:\n        print(decorate('start run blocks [training_set]...'))\n\n    with Timer(prefix='run test={}'.format(test)):\n        for block in feature_blocks:\n            with Timer(prefix='out_df shape: {} \\t- {}'.format(out_df.shape, str(block))):\n                if not test:\n                    out_i = block.fit(input_df.copy(), y=y)\n                else:\n                    out_i = block.transform(input_df.copy())\n\n            assert len(input_df) == len(out_i), block\n            name = block.__class__.__name__\n            out_df = pd.concat([out_df, out_i.add_suffix(f'@{name}')], axis=1)\n            gc.collect()\n    print(f\"out_df shape: {out_df.shape}\")\n\n    return pd.concat([input_df, out_df], axis=1)\n\n\ntrain = run_blocks(train, blocks=feature_blocks)\ntest = run_blocks(test, blocks=feature_blocks, test=True)\nCFG.cont_seq_cols = list(set(CFG.cont_seq_cols))\n\ndisplay(train.head())\ndisplay(test.head())","4b158a6d":"# we might want to avoid scaling the dummy cols?\n\nprint(decorate(\"NORMALISATION START\"))\ntrain_col_order = [\"u_out\"] + train.columns.drop(\"u_out\").tolist()\ntest_col_order = [\"u_out\"] + test.columns.drop(\"u_out\").tolist()\n\ntrain = train[train_col_order]\ntest = test[test_col_order]\n\n\nscaler = RobustScaler()\nscaler_targets = [col for col in CFG.cont_seq_cols if col != \"u_out\"]\n\nprint(f\"Apply Standerd Scaler these columns: {scaler_targets}\")\n\nfor scaler_target in tqdm(scaler_targets):\n    print(\"scaling\", scaler_target)\n    scaler.fit(train.loc[:,[scaler_target]])\n    train.loc[:,[scaler_target]] = scaler.transform(train.loc[:,[scaler_target]])\n    test.loc[:,[scaler_target]] = scaler.transform(test.loc[:,[scaler_target]])\n    gc.collect()\n\nprint(decorate(\"NORMALISATION END\"))\n\ndisplay(train.head())\ndisplay(test.head())","162d9376":"print(set(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).columns) - set(CFG.cont_seq_cols))\nprint(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).shape)\nprint(len(CFG.cont_seq_cols))\n\nX = np.float32(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))\ny = np.float32(train[\"pressure\"]).reshape(-1, 80, 1)\n\nX_test = np.float32(test.drop([\"id\", \"breath_id\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))\ngc.collect()","fba14211":"# ====================================================\n# CV split\n# ====================================================\n# Fold = GroupKFold(n_splits=5)\n# groups = train['breath_id'].values\n# for n, (train_index, val_index) in enumerate(Fold.split(train, train['pressure'], groups)):\n#     train.loc[val_index, 'fold'] = int(n)\n# train['fold'] = train['fold'].astype(int)\n# print(train.groupby('fold').size())","fc273967":"class L1Loss_masked(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, preds, y, u_out):\n        \n        # expiratory\n#         mask = u_out\n#         mae_exhale = torch.abs(mask * (y - preds))\n#         mae_exhale = torch.sum(mae_exhale) \/ torch.sum(mask)\n        \n        # inspiratory_phase\n        mask = 1 - u_out\n        mae_inspiratory = torch.abs(mask * (y - preds))\n        mae_inspiratory = torch.sum(mae_inspiratory) \/ torch.sum(mask)\n        \n        # final weighted_loss.\n        final_loss = mae_inspiratory\n        return final_loss","61d8bcb0":"# scaling layer\n\nclass my_round_func(torch.autograd.Function):\n    \n    @staticmethod\n    def forward(ctx, input):\n        return torch.round(input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input\n\nclass ScaleLayer(nn.Module):\n    def __init__(self):\n        super(ScaleLayer, self).__init__()\n        self.min = PRESSURE_MIN\n        self.max = PRESSURE_MAX\n        self.step = PRESSURE_STEP\n        self.my_round_func = my_round_func()\n\n    def forward(self, inputs):\n        steps = inputs.add(-self.min).divide(self.step)\n        int_steps = self.my_round_func.apply(steps)\n        rescaled_steps = int_steps.multiply(self.step).add(self.min)\n        clipped = torch.clamp(rescaled_steps, self.min, self.max)\n        return clipped\n\nPRESSURE_MIN, PRESSURE_MAX, PRESSURE_STEP","b0ca4681":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        \n        self.cfg = cfg\n        self.hidden_size = self.cfg.hidden_size\n        \n        self.seq_emb = nn.Sequential(\n            nn.Linear(len(cfg.cont_seq_cols), self.hidden_size),\n            nn.LayerNorm(self.hidden_size),\n            nn.SELU(),\n        )\n        \n        self.lstm1 = nn.LSTM(self.hidden_size, self.hidden_size, dropout=0.1, batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(self.hidden_size * 2, self.hidden_size\/\/2, dropout=0.1, batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(self.hidden_size\/\/2 * 2, self.hidden_size\/\/4, dropout=0.1, batch_first=True, bidirectional=True)\n        \n        self.head = nn.Sequential(\n            # nn.Linear(self.hidden_size\/\/8 * 2, self.hidden_size\/\/8 * 2),\n            nn.LayerNorm(self.hidden_size\/\/4 * 2),\n            nn.SELU(),\n            #nn.Dropout(0.),\n            nn.Linear(self.hidden_size\/\/4 * 2, 1),\n            ScaleLayer(),\n        )\n        \n#         for n, m in self.named_modules():\n#             if isinstance(m, nn.LSTM):\n#                 print(f'init {m}')\n#                 for param in m.parameters():\n#                     if len(param.shape) >= 2:\n#                         nn.init.orthogonal_(param.data)\n#                     else:\n#                         nn.init.normal_(param.data)\n#             elif isinstance(m, nn.GRU):\n#                 print(f\"init {m}\")\n#                 for param in m.parameters():\n#                     if len(param.shape) >= 2:\n#                         init.orthogonal_(param.data)\n#                     else:\n#                         init.normal_(param.data)\n\n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow\/Keras-like initialization\n        \"\"\"\n        for name, param in self.named_parameters():\n            if 'lstm' in name:\n                print(f'init lstm {name}')\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(param.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(param.data)\n                elif 'bias' in name:\n                    nn.init.xavier_uniform_(param.data)\n            if 'gru' in name:\n                print(f'init gru {name}')\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(param.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(param.data)\n                elif 'bias' in name:\n                    nn.init.xavier_uniform_(param.data)\n            elif 'fc' in name:\n                print(f'init linear {name}')\n                if 'weight' in name:\n                    nn.init.xavier_uniform_(param.data)\n                elif 'bias' in name:\n                    nn.init.xavier_uniform_(param.data)\n\n    def forward(self, cont_seq_x):\n        bs = cont_seq_x.size(0)\n        seq_emb = self.seq_emb(cont_seq_x)\n        seq_emb, _ = self.lstm1(seq_emb)\n        seq_emb, _ = self.lstm2(seq_emb)\n        seq_emb, _ = self.lstm3(seq_emb)\n        output = self.head(seq_emb)#.view(bs, -1)\n        return output\n\n# print(CustomModel(CFG))","5c624e4f":"# ====================================================\n# helper function\n# ====================================================\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\nscaler = GradScaler()\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    losses = AverageMeter()\n    start = end = time()\n    iters = len(train_loader)\n    \n    for step, (inputs, y) in enumerate(train_loader):\n        \n        optimizer.zero_grad()\n        inputs, y = inputs.to(device), y.to(device)\n        batch_size = inputs.size(0)\n        \n        with autocast():\n            pred = model(inputs)\n            loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n        \n        losses.update(loss.item(), batch_size)\n        \n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        \n        if CFG.apex:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n        \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        \n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            if CFG.apex:\n                scaler.step(optimizer)\n            else:\n                optimizer.step()\n            \n            lr = 0\n            \n            if CFG.batch_scheduler:\n                # is the scheduler is cosine-with-warm-restarts... (check docs)\n                scheduler.step(epoch + step \/ iters)\n                lr = scheduler.get_lr()[0]\n        \n        if CFG.apex:\n            scaler.update()\n        \n        end = time()\n    \n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    model.eval()\n    preds = []\n    losses = AverageMeter()\n    start = end = time()\n    \n    for step, (inputs, y) in enumerate(valid_loader):\n        \n        inputs, y = inputs.to(device), y.to(device)\n        batch_size = inputs.size(0)\n        \n        with torch.no_grad():\n            pred = model(inputs)\n        \n        loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n        losses.update(loss.item(), batch_size)\n        \n        preds.append(pred.view(-1).detach().cpu().numpy())\n        \n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        \n        end = time()\n    \n    preds = np.concatenate(preds)\n    return losses.avg, preds\n\n\ndef inference_fn(test_loader, model, device):\n    model.eval()\n    model.to(device)\n    preds = []\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    for step, (cont_seq_x) in tk0:\n        cont_seq_x = cont_seq_x.to(device)\n        with torch.no_grad():\n            pred = model(cont_seq_x)\n        preds.append(pred.view(-1).detach().cpu().numpy())\n    preds = np.concatenate(preds)\n    return preds","c6b94ccf":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold, trn_idx, val_idx):\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n\n    train_folds = X[trn_idx]\n    valid_folds = X[val_idx]\n    \n    y_train = y[trn_idx]\n    y_true = y[val_idx]\n    \n    groups = train[\"breath_id\"].unique()[val_idx]\n    oof_folds = train[train[\"breath_id\"].isin(groups)].reset_index(drop=True)\n\n    train_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(train_folds),\n        torch.from_numpy(y_train)\n    )\n    \n    valid_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(valid_folds),\n        torch.from_numpy(y_true)\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CFG.batch_size,\n        shuffle=True,\n        num_workers=CFG.num_workers, pin_memory=True, drop_last=True\n    )\n    \n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=CFG.batch_size,\n        shuffle=False,\n        num_workers=CFG.num_workers, pin_memory=True, drop_last=False\n    )\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    \n    model = CustomModel(CFG)\n    model.to(device)\n    print(model)\n\n    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    num_train_steps = int(len(train_folds) \/ CFG.batch_size * CFG.epochs)\n    \n    def get_scheduler(optimizer):\n        if CFG.scheduler=='linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif CFG.scheduler=='cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=CFG.num_cycles\n            )\n        elif CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n\n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # apex\n    # ====================================================\n    #if CFG.apex:\n    #    model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    \n    criterion = L1Loss_masked()\n\n    best_score = np.inf\n\n    avg_losses = []\n    avg_val_losses = []\n    \n    for epoch in range(CFG.epochs):\n\n        start_time = time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n        avg_losses.append(avg_loss)\n        \n        # eval\n        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        avg_val_losses.append(avg_val_loss)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        else:\n            pass\n\n        # scoring\n        score = avg_val_loss\n        elapsed = time() - start_time\n\n        best_notice = \"\"\n        if score < best_score:\n            best_notice = \"Best Score\"\n            best_score = score\n            torch.save(\n                {\n                    'model': model.state_dict(),\n                    'preds': preds\n                },\n                OUTPUT_DIR+f\"fold{fold}_best.pth\"\n            )\n    \n        LOGGER.info(\n            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s, lr: {optimizer.param_groups[0][\"lr\"]:.6f}, MAE Score: {score:.4f}, {best_notice}'\n        )\n\n    plt.figure(figsize=(14,6))\n    plt.plot(avg_losses, label=\"Train Loss\")\n    plt.plot(avg_val_losses, label=\"Train Loss\")\n    plt.title(f\"Fold {fold + 1} - Best score {best_score:.4f}\", size=18)\n    plt.show(block=False)\n\n    preds = torch.load(OUTPUT_DIR+f\"fold{fold}_best.pth\", map_location=torch.device('cpu'))['preds']\n    oof_folds['preds'] = preds.flatten()\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return oof_folds","2e4db503":"# ====================================================\n# main\n# ====================================================\ndef main():\n    \n    \"\"\"\n    Prepare: 1.train 2.test\n    \"\"\"\n    \n    def get_result(result_df):\n        result_df.reset_index(drop=True, inplace=True)\n        preds = result_df['preds'].values\n        labels = result_df['pressure'].values\n        non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n        score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n        LOGGER.info(f'Score (without expiratory phase): {score:<.6f}')\n    \n    if CFG.train:\n        # train\n        oof_df = pd.DataFrame()\n        kfold = KFold(n_splits=CFG.n_fold, random_state=42, shuffle=True)\n        \n        for fold, (trn_idx, val_idx) in enumerate(kfold.split(X=X, y=y)):\n            if fold in CFG.trn_fold:\n                _oof_df = train_loop(X, fold, trn_idx, val_idx)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        \n        # CV result\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        \n        # save result\n        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n    \n    for i, breath_id in enumerate(oof_df[\"breath_id\"].unique()):\n        oof_df[oof_df[\"breath_id\"]==breath_id].plot(\n            x=\"time_step\", \n            y=[\"preds\", \"pressure\", \"u_out\"],\n            figsize=(16, 5),\n            title = f\"Preds Plot for breath_id : {breath_id}\"\n        )\n        plt.show(block=False)\n        if i == 15:\n            break\n    \n    if CFG.inference:\n        test_loader = torch.utils.data.DataLoader(X_test, batch_size=512, shuffle=False, pin_memory=True)\n        \n        for fold in CFG.trn_fold:\n            model = CustomModel(CFG)\n            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n            state = torch.load(path, map_location=torch.device('cpu'))\n            model.load_state_dict(state['model'])\n            predictions = inference_fn(test_loader, model, device)\n            test[f'fold{fold}'] = predictions\n            del state, predictions; gc.collect()\n            torch.cuda.empty_cache()\n        \n        # submission\n        test['pressure'] = test[[f'fold{fold}' for fold in CFG.trn_fold]].mean(1)\n        test[['id', 'pressure'] + [f'fold{fold}' for fold in CFG.trn_fold]].to_csv(OUTPUT_DIR+'raw_submission.csv', index=False)\n        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission.csv', index=False)","350c1378":"if __name__ == '__main__':\n    main()","0ef7a95e":"del X, X_test\ngc.collect()","f34b0ef9":"def get_result_oof(result_df):\n    result_df.reset_index(drop=True, inplace=True)\n    preds = result_df['preds'].values\n    labels = result_df['pressure'].values\n    non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n    score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n    return score","4903d8c9":"oof_df = pd.read_csv(\"oof_df.csv\", usecols=[\"breath_id\", \"pressure\", \"preds\", \"u_out\"])\nunique_breath_ids = set(oof_df[\"breath_id\"].unique())\noof_df.head()","206e29b4":"%%time\nall_breath_ids_scores = []\nfor breath_id in tqdm(unique_breath_ids, total=len(unique_breath_ids), desc=\"computing mae's for all OOF\"):\n    _result_df = oof_df[oof_df[\"breath_id\"] == breath_id]\n    all_breath_ids_scores.append((breath_id, get_result_oof(_result_df)))","b648666e":"df = pd.DataFrame(all_breath_ids_scores, columns=[\"breath_id\", \"mae_oof\"])","e0401dc2":"import plotly.express as px\n\nfig = px.scatter(\n    data_frame=df[df[\"mae_oof\"] >=1], x=\"breath_id\", y=\"mae_oof\", color=\"mae_oof\", hover_data={'breath_id':':.6d', 'mae_oof': ':.4f'},\n)\nfig.show()","9d6b7fc5":"print(df[df[\"mae_oof\"]>=2][\"breath_id\"].unique())","bbe1c83f":"fig = px.scatter(\n    data_frame=df, x=\"breath_id\", y=\"mae_oof\", color=\"mae_oof\", hover_data={'breath_id':':.6d', 'mae_oof': ':.4f'},\n)\nfig.show()","6ceea389":"from torch.optim.lr_scheduler import _LRScheduler\n\nclass NoamLR(_LRScheduler):\n    \"\"\"\n    Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate\n    linearly for the first ``warmup_steps`` training steps, and decreasing it thereafter proportionally\n    to the inverse square root of the step number, scaled by the inverse square root of the\n    dimensionality of the model. \n    Time will tell if this is just madness or it's actually important.\n    \n    Parameters\n    ----------\n    warmup_steps: ``int``, required.\n        The number of steps to linearly increase the learning rate.\n    \"\"\"\n\n    def __init__(self, optimizer, warmup_steps):\n        self.warmup_steps = warmup_steps\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        last_epoch = max(1, self.last_epoch)\n        scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n        return [base_lr * scale for base_lr in self.base_lrs]","6c3cdad6":"\"\"\"\nThe general idea of hard example mining is once the loss(and gradients) are computed for every sample in the batch, \nyou sort batch samples in the descending order of losses and pick top-k samples from it and do backward pass only for those k samples. \ni.e samples which contribute to more learning(aka hard example).\nThis ensures that samples which do not contribute much to the network\u2019s learning is ignored.\n\"\"\";","d2f05795":"# !curl -X POST -H 'Content-type: application\/json' --data '{\"text\":\"kaggle commit done!\"}' https:\/\/hooks.slack.com\/services\/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","4b1e8847":"# End","a94bd798":"# normalization","f4db08f0":"# Train Loop","d3c562ea":"# create features","ec19201d":"# Main","2b335348":"# Utils","11a6def2":"# data loading","62de4e38":"# Loss","9002b93c":"# Model","f1f5ea76":"# reshape","f779c523":"# cv split","cd8d5b3e":"# Configuration","1f5c223b":"# helper function","13313ca3":"# import"}}