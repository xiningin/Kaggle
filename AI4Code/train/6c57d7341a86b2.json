{"cell_type":{"a41b004d":"code","8d71085c":"code","bc1d2de4":"code","b51c9c1d":"code","40a33083":"code","0bf4c5fb":"code","4e5cf59f":"code","e93a6a7c":"code","96ec80b3":"code","11b4746c":"code","c2d262f7":"code","1a05a35f":"code","d60b49e2":"code","898631bf":"code","dbc4eada":"code","1cb09988":"code","2f323ae2":"code","796dc108":"code","eb378b25":"code","1c4036e7":"code","832d04b0":"code","048314ee":"code","4ff7fff2":"code","403b15e8":"code","faadff7c":"code","7026cc8c":"code","049f760c":"code","88068fc0":"code","b04ffea9":"code","0b2d8ef9":"markdown","e1d9a1f1":"markdown","7ec81496":"markdown","66b4e33a":"markdown","5fa2e690":"markdown","b8e63fd4":"markdown","a9d5780d":"markdown","7b27e43d":"markdown","d52421d1":"markdown","535c5f68":"markdown","d506cdd2":"markdown","bced783c":"markdown","ce7f7fb1":"markdown","99f8b971":"markdown","f2ac9555":"markdown","37a6567f":"markdown"},"source":{"a41b004d":"import spacy\nimport gzip, pickle\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport json","8d71085c":"vocabularies=[\n    'viruses',\n    'diseases',\n    'bacteria'\n]","bc1d2de4":"def read_df(file_name):\n    with tqdm.wrapattr(gzip.open(file_name, 'rb'), \"read\", desc='read from ' +file_name) as file:\n        return pickle.load(file)","b51c9c1d":"def get_vocabulary_id_map(v):\n    id_map={}\n    with open(f'..\/input\/snmi-disease-vocabularyjson\/{v}.json') as file:\n        v_json = json.load(file)\n        for key, value in v_json.items():\n            id_map[value['ID']]=value\n    return id_map","40a33083":"all_vocabulary_id_map={}\nfor v in vocabularies:\n    all_vocabulary_id_map[v]=get_vocabulary_id_map(v)\n    print('load vocabulary for', v, len(all_vocabulary_id_map[v].keys()))\n","0bf4c5fb":"def wordcloud(df, name, title = None):\n    # Set random seed to have reproducible results\n    np.random.seed(64)\n    \n    wc = WordCloud(\n        background_color=\"white\",\n        max_words=200,\n        max_font_size=40,\n        scale=4,\n        random_state=0\n    ).generate_from_frequencies(df)\n\n    wc.recolor(color_func=wcolors)\n    \n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n\n    if title:\n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wc),\n    wc.to_file(name)\n    plt.show()\n\ndef wcolors(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    colors = [\"#7e57c2\", \"#03a9f4\", \"#011ffd\", \"#ff9800\", \"#ff2079\"]\n    return np.random.choice(colors)","4e5cf59f":"for vocabulary in vocabularies: \n    # load pre-built DataFrame\n    df = read_df(f'..\/input\/cord19precomputeddata\/corona_mentioned_with_{vocabulary}_mentioned.df.pklz')\n    # get id map for combining text matched\n    id_map=all_vocabulary_id_map[f'{vocabulary}']\n    # combine text matched to the label of the term.\n    df[f'{vocabulary}_label_combined']=df[f'{vocabulary}_mentions'].apply(lambda mentions: [id_map[mention['id']]['properties']['label'] for mention in mentions])\n    \n    # concatenate all labels\n    res = np.concatenate([labels for labels in df[f'{vocabulary}_label_combined']])\n\n    # remove common terms for viruses, diseases and bacteria\n    if vocabulary == 'viruses':\n        res = [r for r in res if r != 'Coronavirus' and r != 'Virus' ]\n    if vocabulary == 'diseases':\n        res = [r for r in res if r != 'Disease']\n    if vocabulary == 'bacteria':\n        res = [r for r in res if r != 'Bacterium']\n        \n    freqs = pd.Series(res).value_counts()\n    wordcloud(freqs, f'word-cloud\/corona-{vocabulary}.png', f'Most frequent words for matched {vocabulary}')","e93a6a7c":"# import required libraries\nimport json\nimport spacy\nfrom unidecode import unidecode\nimport glob\nimport json\nimport os\nimport sys\nimport pickle, gzip\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nnlp = spacy.load(\"en_core_web_sm\")","96ec80b3":"# read all data files\nall_data_files=glob.glob(f'data\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/*.json', recursive=True)\nlen(all_data_files)","11b4746c":"# read all vocab files\nall_vocab_files=glob.glob(f'Owncloud\/CORD-19-Hackathon\/V-team\/vocabulary\/*.json', recursive=True)\nlen(all_vocab_files)\nprint(all_vocab_files)","c2d262f7":"# get all labels from all vocab json\nall_labels = []\nfor file in all_vocab_files:\n    #print(file)\n    with open(file) as json_file :\n        virus_data = json.load(json_file)\n        max_count = max(virus_data.keys())\n        for count in range(0, int(max_count)+1):\n            all_labels.append(virus_data[\"{}\".format(count)][\"properties\"][\"label\"])\n        print(all_labels)","1a05a35f":"# remove unwanted labels from all_labels\nnew_all_labels = []\nremoved_labels = []\nunwanted_labels = ['coronavirus','Coronavirus','disease','Disease','bacteria','Bacteria','virus','Virus']\nfor label in all_labels:\n    if label in unwanted_labels:\n        removed_labels.append(label)\nprint(\"labels removed are - \", removed_labels)\nnew_all_labels = list(set(all_labels) - set(removed_labels)) \nprint(new_all_labels)","d60b49e2":"# get all vocab labels with synonyms\nsynonyms = []\nsynonyms_dict = {}\nfor file in all_vocab_files:\n    print(file)\n    with open(file) as json_file :\n        virus_data = json.load(json_file)\n        max_count = max(virus_data.keys())\n        for count in range(0, int(max_count)+1):\n            #labels.append(virus_data[\"{}\".format(count)][\"properties\"][\"label\"])\n            #synonyms.extend(virus_data[\"{}\".format(count)][\"properties\"][\"synonyms\"])\n            synonyms_dict.update({virus_data[\"{}\".format(count)][\"properties\"][\"label\"]: virus_data[\"{}\".format(count)][\"properties\"][\"synonyms\"]})\n#print(labels)\n#print(synonyms)\nprint(synonyms_dict)","898631bf":"# pickles file generated locally\nwith gzip.open('NLP\/mentions\/all_data_corona_mentioned.pklz') as file:\n    df=pickle.load(file)\ndf.head()","dbc4eada":"corona_mentioned_paper_ids = df['paper_id']\n#print(corona_mentioned_paper_ids)\ncorona_mentioned_paper_ids_list = df['paper_id'].tolist()\nprint(corona_mentioned_paper_ids_list)","1cb09988":"# list of all corona_mentioned_files\ncorona_mentioned_file_ids = []\ncorona_mentioned_files = []\nfor file in all_data_files:\n    with open(file) as json_file :\n        file_name = file.split(\"\/\")[-1].split(\".\")[0]\n        #print(file_name)\n        if file_name in corona_mentioned_paper_ids_list:\n            corona_mentioned_file_ids.append(file_name)\n            corona_mentioned_files.append(file)\n#print(corona_mentioned_files)\nprint(\"no. of corona_mentioned_files = \", len(corona_mentioned_files))\nprint(\"no. of all_data_files = \", len(all_data_files))","2f323ae2":"# occurence of all labels in corona_mentioned_paper_ids_list\n\nfinal_dict = {}\nid_twc_dict = {}\n\nfor file in corona_mentioned_files:\n    label_dict = {}\n    with open(file) as json_file :\n        file_name = file.split(\"\/\")[-1].split(\".\")[0]\n        #print(file_name)\n        data = json.load(json_file)\n        str = \"\"\n        try:\n            for k in data['abstract']:\n                str = str + k['text']                     \n            doc = nlp(unidecode(str)) \n            #print (doc,\"\\n\")\n            \n            new_all_labels = set(new_all_labels)\n            #print(new_all_labels)\n            for label in new_all_labels:\n                counter=0\n                for word in doc:             \n                    if(word.text.lower() == label.lower()):\n                        counter = counter + 1\n                        for lbl in synonyms_dict[label]:\n                            if (word.text == lbl):\n                                counter = counter + 1\n                if len(doc) > 0:\n                    freq = (counter\/len(doc))*100\n                else:\n                    freq = 0\n                if counter > 0:\n                    label_dict.update({label:counter})\n                    print(label,\"appears in\",file_name,counter,\"times\",\", i.e.(\",freq,\"%, as this text has\",len(doc),\"\\\"tokens\\\")\")\n                    id_twc_dict.update({file_name:len(doc)})\n                    final_dict[file_name] = label_dict\n        except:\n            print(sys.exc_info[0])\n\nprint(id_twc_dict)   \nprint(final_dict)\n\nwith open('total_word_count_in_corona_mentioned_files.json', 'w') as outfile:\n    json.dump(id_twc_dict, outfile)\n\nwith open('all_labels_occurences_in_corona_mentioned_files.json', 'w') as outfile:\n    json.dump(final_dict, outfile)\n#print(final_dict)","796dc108":"# pandas df for labels and occurences in corona_mentioned_files\nlabels_occurences_corona_df = pd.read_json (r'all_labels_occurences_in_corona_mentioned_files.json')\nprint(labels_occurences_corona_df)\nlabels_occurences_corona_df.to_csv('labels_occurences_corona_df.csv')","eb378b25":"# occurence of all labels in all research papers\n\nfinal_dict = {}\n\n#id_twc_df = pd.DataFrame(columns=['paper_id','total_word_count'])\nid_twc_dict = {}\n\nfor file in all_data_files:\n    label_dict = {}\n    with open(file) as json_file :\n        file_name = file.split(\"\/\")[-1].split(\".\")[0]\n        #print(file_name)\n        data = json.load(json_file)\n        str = \"\"\n        try:\n            for k in data['abstract']:\n                str = str + k['text']                     \n            doc = nlp(unidecode(str)) \n            #print (doc,\"\\n\")\n\n            new_all_labels = set(new_all_labels)\n            #print(new_all_labels)\n            for label in new_all_labels:\n                counter=0\n                for word in doc:             \n                    if(word.text.lower() == label.lower()):\n                        counter = counter + 1\n                        for lbl in synonyms_dict[label]:\n                            if (word.text == lbl):\n                                counter = counter + 1\n                if len(doc) > 0:\n                    freq = (counter\/len(doc))*100\n                else:\n                    freq = 0\n\n                if counter > 0:\n                    label_dict.update({label:counter})\n\n                    print(label,\"appears in\",file_name,counter,\"times\",\", i.e.(\",freq,\"%, as this text has\",len(doc),\"\\\"tokens\\\")\")\n                    id_twc_dict.update({file_name:len(doc)})\n                    final_dict[file_name] = label_dict\n                    #final_dict.update({file_name:label_dict})\n        except:\n            print(sys.exc_info[0])                \n\nprint(id_twc_dict)   \nprint(final_dict)\n\nwith open('total_word_count_in_all_files.json', 'w') as outfile:\n    json.dump(id_twc_dict, outfile)\n\nwith open('all_labels_occurences_in_all_files.json', 'w') as outfile:\n    json.dump(final_dict, outfile)\n#print(final_dict)","1c4036e7":"# pandas df for labels and occurences in all files\nlabels_occurences_all_df = pd.read_json (r'all_labels_occurences_in_all_files.json')\nprint(labels_occurences_all_df)\nlabels_occurences_all_df.to_csv('labels_occurences_all_df.csv')","832d04b0":"import pandas as pd\nimport math\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json","048314ee":"# loading data from precomputed files\nroot=\"postsubmission\/\"\n#reading noncommon-use-subset\nncu_labelocc = pd.read_csv(root + \"ncu-labels_occurences_all_df.csv\")\nncu_labelocc.rename(columns={ ncu_labelocc.columns[0]: \"term\" }, inplace = True)\nncu_labelocc_c=pd.read_csv(root + \"ncu-labels_occurences_corona_df.csv\")\nncu_labelocc_c.rename(columns={ ncu_labelocc_c.columns[0]: \"term\" }, inplace = True)\n#ncu_labelocc_c)\n\n#reading biorxiv\nbio_labelocc = pd.read_csv(root +\"biorxiv-labels_occurences_all_df.csv\")\nbio_labelocc.rename(columns={ bio_labelocc.columns[0]: \"term\" }, inplace = True)\nbio_labelocc_c=pd.read_csv(root +\"biorxiv-labels_occurences_corona_df.csv\")\nbio_labelocc_c.rename(columns={ bio_labelocc_c.columns[0]: \"term\" }, inplace = True)\n#print(bio_labelocc_c)\n#print(ncu_labelocc_c)","4ff7fff2":"# joining ncu and bio dataframes\n\nlabelocc = pd.concat([ncu_labelocc, bio_labelocc], ignore_index=True)\n#labelocc=ncu_labelocc\nlabelocc=labelocc.groupby(\"term\").sum().reset_index()\nlabelocc.dropna(inplace=True)\n\n\nlabelocc_c = pd.concat([ncu_labelocc_c, bio_labelocc_c], ignore_index=True)\n#labelocc_c=ncu_labelocc\nlabelocc_c=labelocc_c.groupby(\"term\").sum().reset_index()\nlabelocc_c.dropna(inplace=True)\n\n\n#optional: save joined dfs to files\n#labelocc.to_csv(\"labels_occurences_corona_df_allds.csv\")\n#labelocc_c.to_csv(\"labels_occurences_all_df_allds.csv\")\n\n#import precomputed abstract word counts\nwith open(root +'ncu-bio-total_word_count_in_all_files.json', 'r') as f:\n    wordcountdict = json.load(f)\n\n#print(len(wordcountdict))","403b15e8":"# collecting terms\nterms = list(labelocc_c[\"term\"])\n\n# collecting IDs of all papers, whole subset\nIDList = list(labelocc)\nIDList.pop(0)\n\n# collecting IDs of all papers, corona subset\nIDList_c = list(labelocc_c)\nIDList_c.pop(0)\n\n","faadff7c":"#print(len(IDList_c))\n#print(len(IDList))\n#print(len(IDrest))\n\nIDrest = []\nnomatch = 0\nfor paper1 in IDList:\n    nomatch = 0\n    for paper2 in IDList_c:\n        if paper1!=paper2:\n            nomatch +=1\n            if nomatch==len(IDList_c):\n                IDrest.append(paper1)\n                \n","7026cc8c":"def getWordCount(term, IDList, sourcedf) :\n    wordcount = 0\n    for paper in IDList:\n        #print(paper)\n        wc_paperID = sourcedf[paper][sourcedf[\"term\"] == term].values[0]\n        #print(wc_paperID)\n        wordcount = wordcount + wc_paperID\n    return int(wordcount)\n\n\ndef getTotWords(term, IDListin, sourcedf, sourcedict=wordcountdict) :\n    total_words = 0;\n    tokens_of_term = len(term.split())\n    #add check if \n    for paper in IDListin :\n        #wctot_paperID = sourcedf[sourcedf[\"term\"] == term][paper][0]\n        wctot_paperID = sourcedict[paper]\n        #print(wctot_paperID)\n        total_words = total_words + wctot_paperID\n        #if tokens_of_term > 1:\n        #    total_words = total_words + int(wctot_paperID)\n        #    #compute the total number of tokens considering the overlapping window\n        #    total_words = wctot_paperID - tokens_of_term +1\n    return total_words\n\n\n#print(getTotWords(\"Influenza\", IDList_c, labelocc_c))","049f760c":"EFDict = {}; pDict = {}; logEFDict ={};\t\t\t\t\t# Other variables\n\n####### Probability of finding the term corona in entire literature #######\n\n# taken from previous calculations (see above)\ncorona_count = 11130 # mentions of \"corona\" or it's synonyms in all biorxiv and non-common-subset articles\ncorona_tot_wc = getTotWords(\"Corona\", IDList, labelocc) # total word count of biorxiv and non-common-subset articles, that mentioned corona\n \npCorona = corona_count \/ corona_tot_wc\n\nprint (\"Prob of finding the term corona in entire literature = \" + str(pCorona))\n\n###############################################################################\n\nfor term in terms: \n    if True :\n        wc1 = getWordCount(term, IDList, labelocc); #print (wc1)\n        wc_tot = getTotWords(term, IDList, labelocc);\n        pL = wc1 \/ wc_tot;  # Prob of finding the term across entire Lit\n        #print (\"Prob of finding the term \"+term+\" in the entire literature =\" + str(pL))\n        #print(wc1)\n        #print(wc_tot)\n        #print(pL)\n\n        wc2 = getWordCount(term, IDList_c, labelocc_c)\n        wc_totC = getTotWords(term, IDList_c, labelocc_c)\n        pC = wc2 \/ wc_totC   # Prob of finding the term across CORONA Lit\n        #print (\"Prob of finding the term \"+term+\" in the CORONA literature =\" + str(pC))\n        #print(wc2)\n        #print(wc_totC)\n        #print(pC)\n\n    ######## Preferential association of vocab terms with CORONA literature #########\n        if pL != 0:\n            EF = pC\/pL\n            #print (\"CORONA Lit has \"+str(EF)+ \" fold higher probability of finding the term \"+term)\n            if EF>0: \n                #print (math.log10(EF))  # as Log10(0) is undefined\n                EFDict[term] = EF\n                logEFDict[term] = math.log10(EF)\n            posteriorT = (pC*pCorona)\/pL\n            #print(posteriorT)\n            #print(pCorona)\n            pDict[term] = posteriorT\n        else :\n            #print (\"The term \"+term+\" does not appear in any literature and hence will be neglected!\\n\")\n            pass\n\n","88068fc0":"#collecting results and saving to a dataframe\nresDf = pd.DataFrame(columns=[\"term\", \"EF\", \"EFlog\", \"posterior\", ])\nfor term in terms:\n    resDf = resDf.append({\"term\": term, \"EF\":EFDict[term], \"EFlog\": logEFDict[term], \"posterior\":pDict[term]}, ignore_index=True)\nresDf = resDf.sort_values(\"EF\", ascending=False).reset_index(drop=True)   \nprint(resDf)\n","b04ffea9":"plt.figure(figsize=(10,20))\n\n\nax=sns.barplot(data=resDf, x=\"EF\", y=\"term\")\nax.set_ylabel(\"\")\nax.set_xlabel(\"Enrichment Factor\")\nax.axvline(1, ls='--')\n","0b2d8ef9":"# COVID-19 Risk Factors: Diseases, viruses and bacteria mentioned in relevant literature","e1d9a1f1":"### Define Function to Display Word Cloud","7ec81496":"# Vocabulary Word Cloud Generation\n\nIt generates word cloud for the vocabularies found in the corpus in different perspectives.","66b4e33a":"#### Statistical analyses identify potential risk factors over-represented in the context of COVID-19 \n    \nWe computed for each vocabulary term a ratio of probability with which it is mentioned in literature records that also mention COVID-19 (the \u201ccorona-subset\u201d described above) to the probability with which it is mentioned in all literature records irrespective of whether they mention COVID-19 or not (expected probability). We call this ratio the \u201cenrichment factor\u201d or the EF. EF > 1 would mean that the term is mentioned in COVID-19 related literature more often than expected (over-represented). EF is 0 for all the terms that are not mentioned even once in COVID-19 related literature (under-represented). Of the total 112 virus terms, 142 bacterial terms and 209 disease terms that are found in COVID-19 related literature,multiple entries had EF>1 respectively. Below is shown a bar-chart showing the top 50 terms from all vocabularies combined together arranged in descending order of the corresponding EFs. Thus its presents an overview of the risk factors most likely associated to COVID-19.   \n\n![barplot](https:\/\/i.imgur.com\/EM4tM2l.png)\n\n\n\n#### Bayes' theorem estimates the risk potential\n\nFor each vocabulary term, we also computed the likelihood of COVID-19 mentions given that the literature record already mentions the term. As expected, the term giving the highest posterior probability of mentioning COVID-19, also had the highest enrichment factor (EF). These posterior probabilities follow Gaussian distribution and may be used to extract the most relevant risk factors as those with posterior probabilities close to 1. For details have a look at the table in section \"Statistical Analysis\".","5fa2e690":"### Outlook\n\nOur approach is basic work that can lead to highly interesting analyses. Next steps cover analysis of the term context, utilizing the semantic linking power and extending the NLP analyses:\n\nWhile counting the co-occurrence of coronavirae and other viruses, bacteria and diseases already gives input into potential risk-factors, analyzing the context of the identified terms will give even more value, e.g. finding prefixes like \"pre-existing\", \"chronic\", \"vaccine\". Also, we could expand our analyses of the literature records from just reading and analyzing the abstracts to the body text or specific sections of the publications like the results. Furthermore, the approach can easily be extended to analyze other risk factors like socioeconomics, health status, and others by simply feeding it with other vocabularies. Also, the semantic links of the ontologies can be further explored to allow conclusions that go even beyond the pure text analysis. More advanced NLP algorithms could be utilized to extract and annotate the context of the risk factors more.\n\n            ","b8e63fd4":"### Results\n\n\n\n#### Ontologies of viruses, bacteria and diseases reveal co-occurrence with coronavirus\n* The results of the analysis of occurrences of the virus\/bacteria\/disease in the corona-subset of the literature can be found in detail in section \"Word counting\". \n* The biorxiv_medrxiv and noncomm_use_subset datasets are used for analysis (referred as all-data) out of the complete literature provided.\n* While the virus vocabulary contains 1379 terms with synonyms, only 112 are found in the corona-subset of the literature. The word cloud shows the found terms (main label, not synonyms) with their size indicating the occurrence count. The bar chart shows the 20 terms found in most literature records (number of papers that mention the term or one of its synonyms is plotted on the x-axis). In this list, HIV is the most prominent, indicating that a HIV co-infection might be a risk factor. Also other highly mentioned viruses like Ebola or Zika are interesting results.  \n\n\n![corona-viruses.png](https:\/\/i.imgur.com\/eKNMf6U.png)\n\n\n\n![bar-chart-viruses.png](https:\/\/i.imgur.com\/Nx31PnQ.png)\n\n\n* The bacteria vocabulary contains 4068 terms of which 142 are found in the corona-subset of the literature. The word cloud shows the found terms (main label, not synonyms) with their size indicating the occurrence count. The bar chart shows the 20 terms found in most literature records (number of papers that mention the term or one of its synonyms is plotted on the x-axis). Among the top ones are E.coli, mycoplasma and salmonella, indicating that these could poste a risk factor. \n\n\n\n![corona-bacteria.png](https:\/\/i.imgur.com\/AE50cEC.png)\n\n![bar-chart-bacteria.png](https:\/\/i.imgur.com\/d5zffTy.png)\n\n\n* The disease vocabulary contains 13310 terms with synonyms of which 209 are found in the corona-subset of the literature. The word cloud shows the found terms (main label, not synonyms) with their size indicating the occurrence count. The bar chart shows the 20 terms found in most literature records (number of papers that mention the term or one of its synonyms is plotted on the x-axis). It lists well known symptoms of COVID-19 like pneumonia but also interestingly others like hepatitis which might indicate a risk-factor. \n\n![](https:\/\/i.imgur.com\/eKNMf6U.png)\n\n![](https:\/\/i.imgur.com\/tdoJvhU.png)\n\n\n\n\n\n\n\n\n\n\n","a9d5780d":"![osthus.png](attachment:osthus.png)","7b27e43d":"### Create Id Map for All Vocabularies\nCreate Id Map to combine synonyms later\n\nVocabulary data structure is like below, label and synonyms are used for matching in the corpus.\n```json\n{\n  \"ID\": \"http:\/\/purl.bioontology.org\/ontology\/SNMI\/L-30605\",\n  \"properties\": {\n    \"label\": \"Human enterovirus 72\",\n    \"synonyms\": [\n      \"Hepatitis A virus\",\n      \"Infectious hepatitis virus\"\n    ],\n    \"associated\": \"http:\/\/purl.bioontology.org\/ontology\/SNMI\/DE-35101\",\n    \"parents\": \"http:\/\/purl.bioontology.org\/ontology\/SNMI\/L-30600\"\n  }\n}\n```","d52421d1":"### Initialize vocabularies","535c5f68":"## OSTHUS Team Members\n\n\n|Name|Email|Kaggle User ID| Kaggle User Name|\n|---|---|---|---|\n|Arne Balzer| arne.balzer(a)osthus.com| 4786361|arnebalzer|\n|Nikhil Damle |nikhil.damle(a)osthus.com | 4786390|nikhildamle|\n|Jing Guo |jing.guo(a)osthus.com| 4896719 | osthusjingguo|\n|Ning Meng | ning.meng(a)osthus.com| 4893125| ningmengosthus|\n|Sujit Kumar  |sujit.kumar(a)osthus.com| 4783844|de00215|\n|Karen Schomburg| karen.schomburg(a)osthus.com| 4786990|drkarenschomburg|\n|Chuan-Lu Yu| chuan-lu.yu(a)osthus.com| 4787352|alexchuanluyu| ","d506cdd2":"### Display Word Cloud for Vocabularies Mentioned\n\n* mention labels will be combined into term's label (pref label)\n* common terms are removed, like Virus, Diseases, and Bacterium\n\n* Result Saved As PNG Image\n    - [Word Cloud For Viruses Mentions](.\/word-cloud\/corona-viruses.png)\n    - [Word Cloud For Diseases Mentions](.\/word-cloud\/corona-diseases.png)\n    - [Word Cloud For Bacteria Mentions](.\/word-cloud\/corona-bacteria.png)\n","bced783c":"# Word Counts Based on Virus, Bacteria and Diseases Vocabularies","ce7f7fb1":"### Define Function to Read Data\nData file is saved in format of pickle and gzipped for better disk usage, pandas DataFrame will be returned\n\n\n#### Pre-Built Data Frame Structure\n![df-example](df-example.png)\n\n\nExample to read data from pre-built data for virus vocabulary\n```python\ndf = read_df('.\/vocabulary-mentions\/corona_with_viruses_mentioned_and_label_only.df.pklz')\nprint(json.dumps(df.viruses_mentions[69], indent=2))\n```\n\nThe virus mention result will be a list of matches, with id, text, and context.\n\n* id is the unique URI from ontology\n* text is the matched world\n* context is a list of tokens before and after the matched text.\n\n```json\n[\n  {\n    \"id\": \"http:\/\/purl.bioontology.org\/ontology\/SNMI\/L-33500\",\n    \"text\": \"Coronavirus\",\n    \"context\": [\n      \"East\",\n      \"Respiratory\",\n      \"Syndrome\",\n      \"Coronavirus\",\n      \"Antibodies\",\n      \"Bactrian\",\n      \"Hybrid\"\n    ]\n  }\n]\n```","99f8b971":"## Problem Statement: What do we know about COVID-19 risk factors?\n\nWe undertook the task of investigating what the literature reports about the potential risks factors. Specifically, we asked whether the literature informs us if co-infections by other pathogens and co-existing health conditions pose any risk of corona-infection. We have also developed statistical approaches to assess the extent of the risk.\n    \n\n## Abstract\n\nCOVID-19 is one of the deadliest pathogens of recent times with ~2 million positive cases and >100,000 reported deaths worldwide. In absence of a vaccine, social distancing is the only way in which its spread can be contained. Vaccine development therefore is the need of the hour and understanding the risk factors leading to the viral infection is a key step in that direction. We focused on two potential risk factors - co-infection by other viruses and bacteria and co-existing health conditions or diseases. Using the ontologies corresponding to viruses, bacteria and diseases, we mined the abstracts of the literature records for simultaneous mentions of the ontology terms and COVID-19, and assessed their risk potential using two statistical metrics. In the co-occurrence analysis, HIV and Escherichia Coli were ranked as the top co-infecting viral and bacterial pathogens respectively whereas the disease \"Feline infectious peritonitis and pleuritis\" was ranked as the co-existing health condition that poses the highest risk of corona-infection. An assessment of statistical probability, reveals that among others, the viruses \u201cmastadenovirus\u201dand \u201ccardiovirus\u201d, the bacteria \u201cbordetella\u201d and \u201cgemella\u201d and the diseases \u201cpancreatitis\u201d and \u201cmyocarditis\u201d are enriched within the COVID-19 related literature.\nThese analyses may be extended\/improved by including the semantic context and\/or by adapting more sophisticated machine-learning and statistical approaches to prioritize these risk factors and also to integrate them in a weighted manner with other potential risk factors.\n\n## Introduction\n\n\n### Utilizing semantic links for risk factor identification\n\nRelatively early after the start of the COVID-19 epidemic it was observed that patients show drastically different disease outcomes. One of the first identified risk factors was the age of the patients, but the specific risk factors remain an active field of study. In our submission, we investigate the impact of co-infections with other viruses and bacteria and also the risk of other diseases. For the analysis we use a semantic approach and utilize the rich power of ontologies: For identification of viruses and bacteria, we use the SNMI ontology (Cote, Roger A., editor. Systematized Nomenclature of Human and Veterinary Medicine: SNOMED International. Northfield (IL): College of American Pathologists; Schaumburg (IL): American Veterinary Medical Association, Version 3.5, 1998.), available here (https:\/\/www.kaggle.com\/arnebalzer\/snmi-disease-vocabularyjson under UMLS License). By utilizing the ontology we can not only look for specific terms but also for the already listed synonyms. Also, the ontology provides the hierarchy which allows us the clustering, e.g. to group all respiratory malfunctions in the class \u201cdisease of respiratory system\u201d. Furthermore, we can in our analysis go beyond counting the occurrence of terms in the literature records: The ontology contains links between viruses, bacteria and diseases, thus if we as an example find the term  \u201cadenovirus (CUI:C000148)\u201d, the vocabulary links us to the diseases \u201cadenoviral meningitis\u201d (CUI: C0276160), \"adenoviral myocarditis\"(CUI:C0276163), \"adenoviral respiratory disease\" (CUI:C0276150)\u2026.and others. These semantic links can be exploited for analysis beyond pure text parsing.\n\n### Our Approach\n\nWe took these steps (for details look into the code sections)\n*\tFiltering of all literature records for occurrence of Coronavirus and its synonyms (from here on called \u201ccorona-subset\u201d)\n*\tCounting viruses, bacteria and disease terms with their synonyms in the literature records abstracts\n*\tAssessing the statistic relevance of the occurrences of the terms by comparing the counts in the \u201ccorona-subset\u201d with the counts of the complete literature and assessing the probability of the occurrence with a Bayesian analysis. \n* Our result shows possible risk factors regarding co-infections with other viruses and bacteria as well as diseases. ","f2ac9555":"### Discussion\n\nOur results and analysis give first ideas and hints where important risk factors might be found. However, the approach is still only scratching the surface of the topic and researchers need to closely evaluate what it means if we find that a term is enriched in the corona-literature. We see our analysis as an entry point into the topic and in the Outlook section poste various ideas how to continue and deliver more value. ","37a6567f":"# Statistical Evaluation"}}