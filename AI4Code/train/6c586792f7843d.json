{"cell_type":{"80ea0d13":"code","80ba092a":"code","3e20df34":"code","284cf50b":"code","32d05a74":"code","9defdc15":"code","c3e3a63b":"code","e253b6bd":"code","98cef209":"code","6833f635":"code","f7380a4c":"code","99f2ef3e":"code","7837c498":"code","2c1dae50":"code","115f3eeb":"code","a8a58524":"code","3ebef747":"code","e2c39701":"code","b5e19791":"code","fac64b25":"code","0e9180ca":"code","cd92412b":"code","8dfacb28":"code","8ab38fba":"code","1e6e535d":"code","25ff7944":"code","0b64ec6f":"code","cbbe4f76":"code","39bd2775":"code","b7bfb08e":"code","c5fa40e6":"code","d946771b":"code","75974304":"code","30ce5e59":"code","a9c492f9":"markdown","bb304e71":"markdown","e28b5be5":"markdown","b081acaa":"markdown","f600e265":"markdown","f4d9b84a":"markdown","aae60742":"markdown","649c8cfb":"markdown","74047eef":"markdown","1f311ea2":"markdown","0f379660":"markdown","88c225fd":"markdown","1c229cfa":"markdown","70a02540":"markdown"},"source":{"80ea0d13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n%matplotlib inline\nsns.set_style('darkgrid')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80ba092a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.ensemble import RandomForestClassifier","3e20df34":"train_df = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('Training data contains {} rows and {} columns '.format(train_df.shape[0],train_df.shape[1]))","284cf50b":"train_df.head()","32d05a74":"test_df = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Test data contains {} rows and {} columns '.format(test_df.shape[0],test_df.shape[1]))","9defdc15":"px.histogram(train_df, x='TARGET', color='TARGET',title='Repaid vs. No Repaid')","c3e3a63b":"def missing_values(df):\n    n_miss_val = df.isnull().sum()\n    n_miss_per = 100 * df.isnull().sum() \/ len(df)\n    miss_tbl = pd.concat([n_miss_val,n_miss_per],axis=1).sort_values(1,ascending=False).round(1)\n    miss_tbl = miss_tbl[miss_tbl[1] !=0]\n    \n    miss_tbl = miss_tbl.rename(columns ={0: 'Missing Values',1:'%(Percentage) Missing Values'})\n    print(\"{} columns that have missing values.\".format(miss_tbl.shape[0]))\n    \n    return miss_tbl\n    \n    ","e253b6bd":"missing_values_table = missing_values(train_df)\nmissing_values_table","98cef209":"def categorical_features(df):\n    categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n    types=df[categorical_cols].dtypes\n    uniques = df[categorical_cols].nunique()\n    categorical_tbl = pd.concat([uniques,types],axis=1).sort_values(0,ascending=False).rename(columns = {0:'Unique Values',1:'Data Types'})\n    print(\"The dataset contains {} categorical values\".format(df[categorical_cols].shape[1]))\n    \n    return categorical_tbl","6833f635":"categorical_features(train_df)","f7380a4c":"categorical_cols = train_df.select_dtypes(include=[\"object\"]).columns.tolist()\nfig , axs = plt.subplots(ncols=2,nrows=8,figsize=(30,50))\nindex=0\naxs = axs.flatten()\nfor cols in categorical_cols:\n        g = sns.countplot(x=cols,hue='TARGET',data=train_df,ax=axs[index],palette=\"spring\")\n        index +=1","99f2ef3e":"def numerical_features(df):\n    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n    numerical_tbl = pd.DataFrame(train_df[numerical_cols].dtypes).rename(columns = {0:'Data Types'})\n    print(\"The dataset contains {} numerical values.(included target value)\".format(df[numerical_cols].shape[1]))\n    \n    return numerical_tbl","7837c498":"numerical_features(train_df)","2c1dae50":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ncategorical_cols = train_df.select_dtypes(include=[\"object\"])\nfor col in categorical_cols:\n    if len(list(categorical_cols[col].unique())) <=2:\n            le.fit(train_df[col])\n            le.fit(test_df[col])\n            train_df[col] = le.transform(train_df[col])\n            test_df[col] = le.transform(test_df[col])","115f3eeb":"train_df= pd.get_dummies(train_df)\ntest_df = pd.get_dummies(test_df)\n\nprint('Train data : ', train_df.shape)\nprint('Test data : ',test_df.shape)","a8a58524":"corrs = train_df.corr()['TARGET'].sort_values(ascending=False)\ncorr_top_df=pd.DataFrame(corrs.head(20))\ncorr_top_df\n#Twenty most positive correlations","3ebef747":"corrs = train_df.corr()['TARGET'].sort_values(ascending=False)\ncorr_top_df=pd.DataFrame(corrs.tail(20))\ncorr_top_df\n#Twenty most negative correlations","e2c39701":"    corr_matrx = pd.DataFrame(train_df).corr()\n    corr_df = corr_matrx.where(np.triu(np.ones(corr_matrx.shape),k=1).astype(np.bool))\n    corr_df =corr_df.unstack().reset_index()\n    corr_df.columns = ['Feature_1','Feature_2','Correlation']\n    corr_df.dropna(subset=['Correlation'],inplace=True)\n    corr_df['Correlation'] = round(corr_df['Correlation'],3)\n    corr_df['Correlation'] = abs(corr_df['Correlation'])\n    mtrx = corr_df.sort_values(by = 'Correlation', ascending = False)\n    mx_corr = mtrx[mtrx['Correlation'] > 0.75]\n    print('{} highly correlated features were found.'.format(mx_corr.shape[0]))\n    mx_corr.sample(10)\n","b5e19791":"corr= train_df.corr()\nkot = corr[corr>=.75]\nplt.figure(figsize=(50,50))\nsns.heatmap(kot,cmap=\"Greens\");","fac64b25":"#Features with high correlation value that we calculated earlier\ncolls=mx_corr['Feature_1'].unique()","0e9180ca":"test_df = test_df.drop(test_df[colls],axis=1)\ntrain_df = train_df.drop(train_df[colls], axis = 1)\nprint('Training Features shape:',train_df.shape)\nprint('Testing Features shape:',test_df.shape)","cd92412b":"f,ax = plt.subplots(ncols=3,figsize=(10,5))\n\nsns.distplot(train_df.EXT_SOURCE_1,kde=True,color=\"b\",ax=ax[0])\nsns.distplot(train_df.EXT_SOURCE_2,kde=True,color=\"g\",ax=ax[1])\nsns.distplot(train_df.EXT_SOURCE_3,kde=True,color=\"r\",ax=ax[2]);","8dfacb28":"\nsns.histplot(train_df,x=abs(train_df['DAYS_BIRTH']\/365),hue=\"TARGET\",color=\"g\",kde=True);\nplt.title('Age of Client')\nplt.xlabel('Ages');","8ab38fba":"fig = px.histogram(train_df,x=abs(train_df['DAYS_BIRTH']\/365),color=\"TARGET\")\nfig.show()","1e6e535d":"target_corr = train_df.corr()['TARGET']\ntop_features = target_corr[(target_corr>-0.05) & (target_corr<0.05)]\ntrain_df.drop(top_features.index,axis=1,inplace=True)\nprint('Training Features shape:',train_df.shape)","25ff7944":"corrs = train_df.corr()['TARGET'].sort_values()\nplt.subplots(figsize=(20,15))\nplt.xticks(rotation=90)\nplt.plot(corrs);","0b64ec6f":"sns.set_theme(style=\"white\")\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(train_df.corr(), dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\n\n# Generate a custom diverging colormap\ncmap =sns.color_palette(\"Spectral\", as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(train_df.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True);","cbbe4f76":"nan_cols=list(train_df.isnull().columns)\nnan_cols","39bd2775":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan,strategy='mean')\nimp.fit(train_df[nan_cols])\ntrain_df[nan_cols] = imp.transform(train_df[nan_cols])","b7bfb08e":"X ,y = (train_df.drop(['TARGET'],axis=1).values,train_df.TARGET.values)\nprint('Training Features shape:',X.shape)","c5fa40e6":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=0)\nprint('X_train Features shape:',X_train.shape)\nprint('X_test Features shape:',X_test.shape)\nprint('y_train Features shape:',y_train.shape)\nprint('y_test Features shape:',y_test.shape)","d946771b":"\ndef predict(chosed_model,name=\"Model\"):\n    mdl = chosed_model\n    mdl = mdl.fit(X_train,y_train)\n    y_prob = mdl.predict_proba(X_test)[:,1]\n    y_pred = mdl.predict(X_test)\n\n    print(\"Performances with {}\".format(name))\n    auc_test = roc_auc_score(y_test,y_prob)\n    print(\"AUC Performance: \", auc_test)\n    recall_test = recall_score(y_test, y_pred, average='weighted')\n    print(\"Recall Performance: \", recall_test)","75974304":"predict((DecisionTreeClassifier(criterion='gini')),'DecisionTreeClassifier')","30ce5e59":"predict((RandomForestClassifier(max_depth=4 , random_state=0)),'RandomForestClassifier')","a9c492f9":"Let's visualize categorical columns","bb304e71":"One-Hot Encoding for other categorical variables with [get_dummies](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html)","e28b5be5":"Ages 25-30 can be seen as the age when payment cannot be made. As the age progresses, the problem of non-payment disappears.\n\nLets model the dataset after examine the correlations which is about target . Choosing features with high correlation also provides better predict in building our model.","b081acaa":"After visulize the features that has high correlations, let's move on to the training model.","f600e265":"Machine learning models can't work with missing numerical data. The process of filling missing values is called imputation so we will have to fill in these missing values (known as imputation).The data contains many missing values.Let's examine our Categorical and Numerical columns","f4d9b84a":"Let's convert the categorical columns to numeric value, will use `Label Encoding` for any categorical variables with only 2 categories and `One-Hot Encoding`for any categorical variables with more than 2 categories. We need to convert our categorical columns to numeric value, will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. We will use pandas `get_dumies`,(convert categorical variable into dummy\/indicator variables) for `One-Hot Encoding`.","aae60742":"We can visuliaze the features with high correlation with each other in a simple way with the heatmap.","649c8cfb":"`DAYS_BIRTH` has the positive correlation with `TARGET` \n\n`EXT_SOURCE_3`,`EXT_SOURCE_2` , `EXT_SOURCE_1` has most negative correlation with `TARGET`\n\nWe can examine these values in more detail.Let's take a look at Collinear Variables before examine the values\n\nA [collinearity](https:\/\/medium.com\/future-vision\/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168) is a special case when two or more variables are exactly correlated.\nThis means the regression coefficients are not uniquely determined. In turn it hurts the interpretability of the model as then the regression coefficients are not unique and have influences from other features.\nSince we don't want this to happen, we will remove one of the highly correlated values.","74047eef":"0 --> Individuals who paid their loan\n\n1 --> Individuals who defaulted on their loan\n \nThere are far more loans that were repaid on time than loans that were not repaid\n\n","1f311ea2":" \n#### Correlations\nOne way to try and understand the data is by looking for `correlations` between the features and the target.\n\nLet's examine the columns that have a high correlation with the target value.","0f379660":"For each of these pairs of highly correlated variables,we will remove one of the variables.We found 127 highly correlated features above. So lets remove features from test and train dataset that are unique.","88c225fd":"Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nLet's load the dataset","1c229cfa":"`DAYS_BIRTH` can analyze the conversion of the number of birthdays to years according to the age of the customers.","70a02540":"Before the train our model, let's extract `NaN,Null` values. We use imputation `SimpleImputer`.\n\nThe SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.[(more detail)](https:\/\/scikit-learn.org\/stable\/modules\/impute.html#impute)"}}