{"cell_type":{"6a4fe553":"code","d366547d":"code","49f3776a":"code","b7876b4f":"code","a8226cdb":"code","f9ef1be1":"code","fd48541c":"code","cc6913c3":"code","0c3fc0de":"code","7ad81c8c":"code","b297c525":"code","bbe9a5a5":"code","e25cc039":"code","7d8fdb20":"code","d8a5c5ca":"code","8a3ddf1b":"code","4a2a994c":"code","adf6bea3":"code","32a0febf":"code","af709d97":"code","1393741a":"code","53468709":"code","8c97f41e":"code","2c6bdc5f":"code","b8ae246e":"code","2403a5a2":"code","90236f68":"code","1e8a9b86":"code","b1e6fb05":"code","0c17ae7a":"markdown","3125c1a1":"markdown","249a6369":"markdown","a2cf0b04":"markdown","0981eec9":"markdown","6ea60fb3":"markdown","31c3319d":"markdown","d3fcd69e":"markdown","fb099daf":"markdown","23f04f93":"markdown","5ef9d623":"markdown","5a2138f3":"markdown"},"source":{"6a4fe553":"!pip install pyconll","d366547d":"import numpy as np\nimport tensorflow as tf\nMAX_SEQ_LEN=128 #89","49f3776a":"import pyconll\n#validate dataset\nval_data_path='..\/input\/ud-englishesl\/data\/corrected\/en_cesl-ud-dev.conllu'\ntrain_data_path='..\/input\/ud-englishesl\/data\/corrected\/en_cesl-ud-train.conllu'\ntest_data_path='..\/input\/ud-englishesl\/data\/corrected\/en_cesl-ud-test.conllu'\npathes=[train_data_path,val_data_path,test_data_path]\ntrain_data,val_data,test_data=map(pyconll.load_from_file,pathes)\nprint(len(train_data),len(val_data),len(test_data))","b7876b4f":"def extract_splited_sents_from_data(data):\n    '''retruns list of list of tokens in data'''\n    sents_splited=[]\n    for sent in data:\n        sents_splited.append([token.form for token in sent])\n    return sents_splited\ndef extract_POS_from_data(data,tag_type='xpos'):\n    '''retruns list of list of POS tags for each sentence in data'''\n    tags=[]\n    if tag_type=='upos':\n        for sent in data:\n            tags.append([token.upos for token in sent])\n    elif tag_type=='xpos':\n        for sent in data:\n            tags.append([token.xpos for token in sent])\n    return tags\ndef extract_from_data(data,tag_type='xpos'):\n    return extract_splited_sents_from_data(data),extract_POS_from_data(data,tag_type)","a8226cdb":"train_sents,train_xpos_tags=extract_from_data(train_data)\nval_sents,val_xpos_tags=extract_from_data(val_data)\ntest_sents,test_xpos_tags=extract_from_data(test_data)","f9ef1be1":"all_sents=train_sents+val_sents+test_sents\nall_xpos_tags=train_xpos_tags+val_xpos_tags+test_xpos_tags\nprint(len(all_sents),len(all_xpos_tags))\nchecked=0\nfor sent,xpos_tag in zip(all_sents,all_xpos_tags):\n    if len(sent)!=len(xpos_tag):\n        print(sent,xops_tag)\n    else:\n        checked+=1\nprint(f\"{checked} sentence-tag pairs checked\")","fd48541c":"# def count_tokens(token_seqs):\n#     token_count={}\n#     for token_seq in token_seqs:\n#         for token in token_seq:\n#             if token not in token_count:\n#                 token_count[token]=1\n#             else:\n#                 token_count[token]+=1\n#     return token_count\n# #check the token freq\n# print(sorted(count_tokens(all_sents).items(),key=lambda x:x[1],reverse=True)[:10])\n# print(sorted(count_tokens(all_xpos_tags).items(),key=lambda x:x[1],reverse=True))","cc6913c3":"def get_tokenizer(text):\n    tokenizer=tf.keras.preprocessing.text.Tokenizer(lower=False,filters=None)\n    tokenizer.fit_on_texts(text)\n    return  tokenizer\nsent_tokenizer,xpos_tag_tokenzier=get_tokenizer(all_sents),get_tokenizer(all_xpos_tags)\nprint(len(sent_tokenizer.word_index),xpos_tag_tokenzier.word_counts)","0c3fc0de":"def tokenize_and_pad_seqs(seqs,tokenizer):\n    tokenized=tokenizer.texts_to_sequences(seqs)\n    padded=tf.keras.preprocessing.sequence.pad_sequences(tokenized,maxlen=MAX_SEQ_LEN,value=0)\n    return padded\nX_train,X_test,X_val=[tokenize_and_pad_seqs(seqs,sent_tokenizer) \\\n                      for seqs in [train_sents,test_sents,val_sents]]\nY_xpos_train,Y_xpos_test,Y_xpos_val=[tokenize_and_pad_seqs(seqs,xpos_tag_tokenzier) \\\n                                     for seqs in [train_xpos_tags,test_xpos_tags,val_xpos_tags]]\nprint(len(X_train),len(X_test),len(X_val))","7ad81c8c":"VOCAB_SIZE=len(sent_tokenizer.word_index)+1\nXPoS_TAG_VOCAB=len(xpos_tag_tokenzier.word_index)+1\ndef PoS_tagger_model():\n    model_input=tf.keras.layers.Input(shape=128)\n#     print(model_input.shape)\n    mask=tf.keras.layers.Masking(mask_value=0)\n    embedding_layer=tf.keras.layers.Embedding(input_dim=VOCAB_SIZE,output_dim=50)\n    embedded=embedding_layer(mask(model_input))\n#     print(embedded.shape)\n    seq,_,_=tf.keras.layers.LSTM(128,return_sequences=True,return_state=True)(embedded)\n#     print(seq.shape)\n    output=tf.keras.layers.Dense(XPoS_TAG_VOCAB)(seq)\n    model=tf.keras.Model(inputs=model_input,outputs=output)\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',metrics=['accuracy'])\n    model.summary()\n    return model\nmodel=PoS_tagger_model()","b297c525":"#simple plot of model illusturation \ntf.keras.utils.plot_model(model)","bbe9a5a5":"early_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)\nhistory=model.fit(X_train,Y_xpos_train,validation_data=(X_val,Y_xpos_val),epochs=128,shuffle=True,batch_size=128,callbacks=[early_stopping],verbose=2)","e25cc039":"import matplotlib.pyplot as plt\n# training history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# training history for token-level accuracy (tensorflow default)\n# zero-padding was included(reimplemented accuracy caculation function later)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","7d8fdb20":"Y_predicted=model.predict(X_test)\n\ndef logits_to_tag_index(logits):\n    return np.argmax(logits,axis=2)\n\n# def get_token_accuracy(predicted_labels,true_labels,from_logits=True):\n#     if from_logits==True:\n#         predicted_labels=logits_to_tag_index(predicted_labels)\n#     compare_mat=predicted_labels==true_labels\n#     token_accuracy=np.sum(np.sum((compare_mat),axis=1)\/compare_mat.shape[1])\/compare_mat.shape[0]\n#     print(f'Accuracy for tagging each token: {token_accuracy}')\n#     return token_accuracy\n\ndef get_token_accuracy(predicted_labels,true_labels,from_logits=True):\n    if from_logits==True:\n        predicted_labels=logits_to_tag_index(predicted_labels)\n    padding_mask=true_labels==0\n    predicted_labels=np.ma.masked_array(predicted_labels,mask=padding_mask)\n    num_of_zero=np.sum((predicted_labels==true_labels).mask)\n    predicted_labels=predicted_labels.filled(fill_value=0)\n    compare_mat=predicted_labels==true_labels\n    token_accuracy=(np.sum(np.sum((compare_mat),axis=1))-num_of_zero)\/(true_labels.size-num_of_zero)\n    print(f'Accuracy for tagging each token: {token_accuracy}')\n    return token_accuracy\n\ndef get_complete_sent_accuracy(predicted_labels,true_labels,from_logits=True):\n    if from_logits==True:\n        predicted_labels=logits_to_tag_index(predicted_labels)\n    compare_mat=predicted_labels==true_labels\n    complete_sent_accuracy=np.sum(np.sum((compare_mat),axis=1)\/compare_mat.shape[1]==1)\/compare_mat.shape[0]\n    print(f'Accuracy for whole sentence tagging: {complete_sent_accuracy}')\n    return(complete_sent_accuracy)","d8a5c5ca":"get_token_accuracy(Y_predicted,Y_xpos_test),get_complete_sent_accuracy(Y_predicted,Y_xpos_test)","8a3ddf1b":"padding_mask=Y_xpos_test==0\nmasked_predict=np.ma.masked_array(np.argmax(Y_predicted,axis=2),mask=padding_mask).filled(fill_value=0)\ntoken_accuracy=np.sum(np.sum((masked_predict==Y_xpos_test),axis=1)\/128)\/500\ncomplete_sent_accuracy=np.sum(np.sum((masked_predict==Y_xpos_test),axis=1)\/128==1)\/500\nprint(f'Accuracy for tagging each token: {token_accuracy}\\nAccuracy for whole sentence tagging: {complete_sent_accuracy}')","4a2a994c":"def generate_subset_index_form_train_set(train,ratio):\n    subset_index=np.random.choice(len(train),int(len(train)*ratio))\n    return subset_index\n#sample one instance from X_val \nsubset_index=generate_subset_index_form_train_set(X_val,0.002) #500*0.002=1 smaple\nprint(X_val[subset_index])","adf6bea3":"from tqdm.notebook import tqdm\nsubsets_index=[ generate_subset_index_form_train_set(X_train,ratio) for ratio in np.arange(0.1,1.1,0.1)]\nprint([len(index) for index in subsets_index])\nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)\nhistorys=[]\ntoken_accuracys=[]\ncomplete_sent_accuracys=[]\nfor index in tqdm(subsets_index):\n    model_subtrain=PoS_tagger_model()\n    historys.append(model_subtrain.fit(X_train[index],Y_xpos_train[index],validation_data=(X_val,Y_xpos_val),\\\n              epochs=128,shuffle=True,batch_size=128,callbacks=[early_stopping],verbose=0)) # train in slient mode\n    predicted=model_subtrain.predict(X_test)\n    token_accuracys.append(get_token_accuracy(predicted,Y_xpos_test))\n    complete_sent_accuracys.append(get_complete_sent_accuracy(predicted,Y_xpos_test))                          ","32a0febf":"#prepare annotation text for data points\ntoken_acc_text=[np.format_float_positional(token_accuracy,precision=3) \\\n       for token_accuracy in token_accuracys]\nprint(complete_sent_accuracys)","af709d97":"plt.figure(figsize=(11,6),dpi=100)\nplt.plot(np.arange(1,len(token_accuracys)+1),token_accuracys,'-^')\nplt.plot(np.arange(1,len(complete_sent_accuracys)+1),complete_sent_accuracys,'->')\nplt.title('Model accuracy on different traning set size')\nplt.ylabel('accuracy')\nplt.xlabel('training set size')\nplt.legend(['Token-level', 'Sentence-level'])\nplt.xticks(np.arange(1,len(token_accuracys)+1), ['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\nfor i in np.arange(len(token_accuracys)):\n    plt.annotate(token_acc_text[i],(np.arange(1,len(token_accuracys)+1)[i]-0.1,token_accuracys[i]-0.05))\n    plt.annotate(complete_sent_accuracys[i],(np.arange(1,len(token_accuracys)+1)[i]-0.1,complete_sent_accuracys[i]+0.05))\nplt.grid()\nplt.show()\n","1393741a":"def get_tag_accuracy(predicted_labels,true_labels):\n    tag_count={}\n    tag_correct_count={}\n    tag_error_count={}\n    for i,tag_seq in enumerate(true_labels):\n        for j,tag in enumerate(tag_seq):\n            if tag not in tag_count:\n                tag_count[tag]=1\n            else:\n                tag_count[tag]+=1\n            if tag==predicted_labels[i][j]:\n                if tag not in tag_correct_count:\n                    tag_correct_count[tag]=1\n                else:\n                    tag_correct_count[tag]+=1\n    tag_count.pop(0)\n    tag_correct_count.pop(0)\n    for tag_index in tag_count:\n        if tag_index not in tag_error_count:\n            if tag_index in tag_correct_count:\n                tag_error_count[tag_index]=tag_count[tag_index]-tag_correct_count[tag_index]\n            else:\n                tag_error_count[tag_index]=tag_count[tag_index]\n    return tag_count,tag_correct_count,tag_error_count\n\ndef get_word_accuracy(predicted_labels,true_labels,sent_seqs):\n    word_count={}\n    word_correct_count={}\n    word_error_count={}\n    for i,tag_seq in enumerate(true_labels):\n        for j,tag in enumerate(tag_seq):\n            if tag!=0:\n                def index_to_string(index_num,tokenizer):\n                    return tokenizer.sequences_to_texts([[index_num]])[0]\n                tag=index_to_string(tag,xpos_tag_tokenzier)\n                word=index_to_string(sent_seqs[i][j],sent_tokenizer)\n                if word not in word_count:\n                    word_count[word]={}\n                    word_correct_count[word]={}\n                    word_error_count[word]={}\n                    if tag not in  word_count[word]:\n                        word_count[word][tag]=1\n                        word_correct_count[word][tag]=0\n                        word_error_count[word][tag]=1\n                        \n                    if tag==index_to_string(predicted_labels[i][j],xpos_tag_tokenzier):\n                        word_correct_count[word][tag]+=1\n                        word_error_count[word][tag]-=1\n                    word_count[word][tag]+=1\n                    word_error_count[word][tag]+=1\n    return word_count,word_correct_count,word_error_count\n","53468709":"tag_count,tag_correct_count,tag_error_count=get_tag_accuracy(masked_predict,Y_xpos_test)\nword_count,word_correct_count,word_error_count=get_word_accuracy(masked_predict,Y_xpos_test,X_test)\nword_accuracy={}\n# Get the accuracy of each word tagged in terms of each type of tags it has\nfor word in word_error_count:\n    word_accuracy[word]={}\n    for tag in word_error_count[word]:\n        word_accuracy[word][tag]=word_error_count[word][tag]\/word_count[word][tag]\n# Check that if there is any word has more than one kind of POS tags  Conclusion: None       \nfor word in word_accuracy:\n    if len(word_count[word])>1:\n        print(word)","8c97f41e":"#find most common error tagged word\nsorted_word_error=sorted(list(word_error_count.items()),key=lambda x:list(x[1].values())[0],reverse=True)\nsorted_word_error[:10]","2c6bdc5f":"#find most common correctly tagged word\nsorted_word_correct=sorted(list(word_correct_count.items()),key=lambda x:list(x[1].values())[0],reverse=True)\nsorted_word_correct[:10]","b8ae246e":"# Find the words has the highest and lowest tagging accuracy\nTOP_K=500\nsorted_word_accuracy=sorted(list(word_accuracy.items()),key=lambda x:list(x[1].values())[0])\nhighest_word_top=list(reversed(sorted_word_accuracy))[:TOP_K]\nlowest_word_top=sorted_word_accuracy[:TOP_K]\n# print(highest_word_top,'\\n',lowest_word_top)\n#Find the words frequencey in training dataset, or even does not exist\ntokenizer=tf.keras.preprocessing.text.Tokenizer(lower=False,filters=None)\ntokenizer.fit_on_texts(train_sents)\nhighest_freq=[]\nhigh_non_exist=[]\nlowest_freq=[]\nlow_non_exist=[]\nfor word in highest_word_top:\n    if word[0] in tokenizer.word_counts:\n        highest_freq.append((word,tokenizer.word_counts[word[0]]))\n    else:\n        high_non_exist.append(word)\nfor word in lowest_word_top:\n    if word[0] in tokenizer.word_counts:\n        lowest_freq.append((word,tokenizer.word_counts[word[0]]))\n    else:\n        low_non_exist.append(word)\n# print(f'highest_freq {highest_freq}\\n')\nprint(f'high_non_exist {high_non_exist}\\n')\n# print(f'lowest_freq {lowest_freq}\\n')\nprint(f'low_non_exist {low_non_exist}\\n')","2403a5a2":"#Find the tags with the highest and lowest tagging accuracy\ndef index_to_string(index_num,tokenizer):\n                    return tokenizer.sequences_to_texts([[index_num]])[0]\ndef index_key_to_string(count_dict,tokenizer=xpos_tag_tokenzier):\n    new_count_dict={}\n    for tag in count_dict:\n        count_num=count_dict[tag]\n        tag=index_to_string(tag,tokenizer)\n        new_count_dict[tag]=count_num\n    return new_count_dict\nerror_tag_count=index_key_to_string(tag_error_count)\nall_tag_count=index_key_to_string(tag_count)\ncorrect_tag_count=index_key_to_string(tag_correct_count)\nprint(sorted(list(error_tag_count.items()),key=lambda x:x[1]))\nprint('\\n')\nprint(sorted(list(all_tag_count.items()),key=lambda x:x[1]))\n#Find tag accuracy\ntag_accuracy={}\nfor tag in all_tag_count:\n    if tag in correct_tag_count:\n        tag_accuracy[tag]=correct_tag_count[tag]\/all_tag_count[tag]\nprint('\\n',sorted(list(tag_accuracy.items()),key=lambda x:x[1]))","90236f68":"# find precent of sentences tagged with less than 5 , 4 ,3 ,2 ,1 words\n[np.sum(np.sum((masked_predict==Y_xpos_test),axis=1)>=x)\/500 for x in [128,127,126,125,124,123]]","1e8a9b86":"predicted_tag=xpos_tag_tokenzier.sequences_to_texts(logits_to_tag_index(Y_predicted))\nground_truth_tag=xpos_tag_tokenzier.sequences_to_texts(Y_xpos_test)\n#Show the worsted tagged sents\nsorted_sent_index=np.argsort(np.sum((masked_predict==Y_xpos_test),axis=1))\nfor i in sorted_sent_index[:5]:\n    print(list(zip(test_sents[i],predicted_tag[i],ground_truth_tag[i])))\n    print(test_sents[i])\n    print(predicted_tag[i])\n    print(ground_truth_tag[i])","b1e6fb05":"#Show some well\/best tagged sents\nfor i in sorted_sent_index[-5:]:\n    print(list(zip(test_sents[i],predicted_tag[i],ground_truth_tag[i])))\n    print(test_sents[i])\n    print(predicted_tag[i])\n    print(ground_truth_tag[i])","0c17ae7a":"**Check:** If padding affects the result?\n**Coclusion:**  Padding with zero have effect on token accuracy (thus reimplemented with mask)","3125c1a1":"https:\/\/cs.nyu.edu\/~grishman\/jet\/guide\/PennPOS.html","249a6369":"## Define the tagger model","a2cf0b04":"Random sampling from train set","0981eec9":"Plot two kinds of accuracys","6ea60fb3":"## Evaluate the accuracy","31c3319d":"Extract annotation of POS tags and sentences from *conllu* files","d3fcd69e":"## Analysis","fb099daf":"[CoNLL-U Format](https:\/\/universaldependencies.org\/format.html#conll-u-format)\n\nWe use a revised version of the CoNLL-X format called CoNLL-U. Annotations are encoded in plain text files (UTF-8, normalized to NFC, using only the LF character as line break, including an LF character at the end of file) with three types of lines:\n\nWord lines containing the annotation of a word\/token in 10 fields separated by single tab characters; see below.\nBlank lines marking sentence boundaries.\nComment lines starting with hash (#).\nSentences consist of one or more word lines, and word lines contain the following fields:\n\n1. ID: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n2. FORM: Word form or punctuation symbol.\n3. LEMMA: Lemma or stem of word form.\n4. UPOS: Universal part-of-speech tag.\n5. XPOS: Language-specific part-of-speech tag; underscore if not available.\n6. FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n7. HEAD: Head of the current word, which is either a value of ID or zero (0).\n8. DEPREL: Universal dependency relation to the 9. 9. HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n9. DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n10. MISC: Any other annotation.","23f04f93":"Merge all data for building an overall vocabulary","5ef9d623":"Caucluating with zero-padding masked but get same result","5a2138f3":"## Training with different sized subsets of the training set"}}