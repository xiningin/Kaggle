{"cell_type":{"c92f671b":"code","307491df":"code","79c7daff":"code","41fb64a1":"code","9ae31d2c":"code","92f9e9ae":"code","2e538e49":"code","6930c9cb":"code","05bb3df5":"code","77a58568":"code","db666ade":"code","e0796d57":"code","d98d5668":"code","d347f165":"code","d08fd383":"code","addd6fe6":"markdown","6fb73b7c":"markdown","d4686597":"markdown","bcc29d5d":"markdown","6e4ce5d8":"markdown","eda71a91":"markdown","0d705b1d":"markdown","4df09a9a":"markdown","56a532d9":"markdown","b03bd36a":"markdown"},"source":{"c92f671b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline","307491df":"train = pd.read_csv('train_data.csv')\nval= pd.read_csv('validation_data.csv')\ntest = pd.read_csv('test_data.csv')\nsub = pd.read_csv('submission.csv')","79c7daff":"train.head()","41fb64a1":"train.describe()","9ae31d2c":"X=train.drop(['target_column',axis=1])\ny=train.target_column","92f9e9ae":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2,stratify=y)","2e538e49":"import lightgbm as lgb\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"auc\",\n        \"num_leaves\" : 64,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.9,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 221,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model\n\n# Training the model #\npred_test, model = run_lgb(X_train, y_train,X_test,y_test, test_X) \n\nbst_iter = model.best_iteration \n\nprint(\"best iteration is:\",bst_iter)","6930c9cb":"import xgboost as xgb\ndef runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n    param = {}\n    param['objective'] = 'binary:logistic'\n    param['eta'] = 0.05\n    param['max_depth'] = 6\n    param['silent'] = 1\n    #param['num_class'] = 3\n    param['eval_metric'] = \"auc\"\n    param['min_child_weight'] = 100 \n    param['subsample'] = 0.96\n    param['colsample_bytree'] = 0.95\n    param['seed'] = seed_val\n    param['scale_pos_weight']=13\n    num_rounds = num_rounds\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest)\n    return pred_test_y, model","05bb3df5":"# Run This instead of cell 14\n\npreds, model = runXGB(dev_X, dev_y, val_X, val_y)\nbst_iter = model.best_iteration        \nprint(\"best iteration is:\",bst_iter) ","77a58568":"from sklearn import model_selection, preprocessing, ensemble\nkf = model_selection.KFold(n_splits=3, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = y[dev_index], y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n        \nbst_iter = model.best_iteration        \nprint(\"best iteration is:\",bst_iter) ","db666ade":"preds, model = runXGB(train_X, y, test_X, num_rounds=int(bst_iter\/0.8))","e0796d57":"sub = pd.read_csv('submission.csv')","d98d5668":"sub['target_feature']=preds","d347f165":"sub.isnull().sum()","d08fd383":"sub.to_csv('xgb2.csv',index=False)","addd6fe6":"<center style=\"font-family:cursive; font-size:35px; color:black;\">Thank You .<\/center>","6fb73b7c":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:red;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;\">\n            In case you dont have validation data then simply split the data into 3 parts train test and validation or just leave validation data its upto you , and results may vary accordingly..\n        <\/p>\n    <\/div>","d4686597":"## ` So the submission file will be genereated and will be labelled with there model name as prefix and will be stored in the location you will spacify in 2 nd cell ` ","bcc29d5d":"## Do the Data Preprocessing on the data and split the independent and target variable and save them in X and y ","6e4ce5d8":"<center style=\"font-family:cursive; font-size:25px; color:#159364;\">Do Upvote if you find this helpful.<\/center>","eda71a91":"****","0d705b1d":"## Just upload your data and replace the train , test , validatation data with your data location and do some basic data preprocessing on it and just simply pass the data into the function.","4df09a9a":"### Data Preprocessing includes-\n- Null value treatment \n- Removing duplicated \n- Encoding the categorical columns\n- Scaliing (upto you as we are using Trees its not necessary)\n- etc.......","56a532d9":"<h1 style=\"font-family:verdana;\"> <center>\ud83d\udcda Base Model for Any Classification Problem Better Using XGB and LGBM \ud83d\udcda<\/center> <\/h1>\n<p><center  style=\"color:#159364;font-size:25px; font-family:cursive;\">DO UPVOTE If THIS KERNEL WAS HELPFUL <\/center><\/p>\n\n***","b03bd36a":"<h1 id=\"final\">\n        Base Model \n        <a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/shubhamksingh\/formatting-notebooks-tutorial-html-markdown\/notebook#final\">\u00b6<\/a>\n    <\/h1>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n    Here I am sharing my notebook with the base model i generally use for any <span style=\"color:crimson;\">Hackathon or Machine Learning Problem.<\/span> \n    This Model gives me a brief idea about the approach and how is data performing , and it sometimes gives some amazing results which helped me a lot.\n    Feel free to use this notebook as a reference when you work on a new Problem. I hope that this notebook helps all my fellow kagglers in some way.\n    <br><br>\n    I will keep uploading new notebooks with new Base Models \u23f3\n    <br><br>\n    <center style=\"font-family:cursive; font-size:18px; color:#159364;\">Keep Sharing and keep Growing.<\/center>\n<\/p>\n\n***"}}