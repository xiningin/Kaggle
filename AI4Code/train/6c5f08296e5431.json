{"cell_type":{"cd9d6f46":"code","b29af8cd":"code","74fbb33c":"code","6fadeb67":"code","a1400376":"code","540d9259":"code","14f41305":"code","21d57de4":"code","786e817c":"code","e0a34980":"code","bb1f2ebe":"code","aa67ac63":"code","ac97c96d":"markdown","f8a9dd65":"markdown","f57e17ab":"markdown","7d3aec90":"markdown"},"source":{"cd9d6f46":"import numpy as np\nimport pandas as pd\n\nfrom keras.layers import Conv3D, MaxPool3D, Flatten, Dense\nfrom keras.layers import Dropout, Input, BatchNormalization, Activation\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import adam, adadelta\nfrom keras.models import Model\nfrom keras.utils.np_utils import to_categorical\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\n%matplotlib inline\n\nimport seaborn as sns\n\nimport h5py\n\nimport os\nprint(os.listdir(\"..\/input\"))","b29af8cd":"with h5py.File('..\/input\/3d-mnist\/full_dataset_vectors.h5', 'r') as dataset:\n    x_train, x_test = dataset[\"X_train\"][:], dataset[\"X_test\"][:]\n    y_train, y_test = dataset[\"y_train\"][:], dataset[\"y_test\"][:]\n\nprint (\"x_train shape: \", x_train.shape)\nprint (\"y_train shape: \", y_train.shape)\n\nprint (\"x_test shape:  \", x_test.shape)\nprint (\"y_test shape:  \", y_test.shape)","74fbb33c":"y_train[1], y_test[1]","6fadeb67":"## Introduce the channel dimention in the input dataset \nxtrain = np.ndarray((x_train.shape[0], 4096, 3))\nxtest = np.ndarray((x_test.shape[0], 4096, 3))\n\n# Translate data to color\ndef add_rgb_dimention(array):\n    scalar_map = cm.ScalarMappable(cmap=\"Oranges\")\n    return scalar_map.to_rgba(array)[:, : -1]\n\n## iterate in train and test, add the rgb dimention \nfor i in range(x_train.shape[0]):\n    xtrain[i] = add_rgb_dimention(x_train[i])\nfor i in range(x_test.shape[0]):\n    xtest[i] = add_rgb_dimention(x_test[i])\n\n## convert to 1 + 4D space (1st argument represents number of rows in the dataset)\nxtrain = xtrain.reshape(x_train.shape[0], 16, 16, 16, 3)\nxtest = xtest.reshape(x_test.shape[0], 16, 16, 16, 3)\n\n## convert target variable into one-hot\ny_train = to_categorical(y_train, 10)","a1400376":"xtrain.shape, y_train.shape","540d9259":"## input layer\ninput_layer = Input((16, 16, 16, 3))\n\n## convolutional layers\nx = Conv3D(filters=8, kernel_size=(3, 3, 3), use_bias=False, padding='Same')(input_layer)\nx = BatchNormalization()(x)\nx = Activation(\"relu\")(x)\nx = Conv3D(filters=16, kernel_size=(3, 3, 3), use_bias=False, padding='Same')(x)\nx = BatchNormalization()(x)\nx = Activation(\"relu\")(x)\n\n## Pooling layer\nx = MaxPool3D(pool_size=(2, 2, 2))(x) # the pool_size (2, 2, 2) halves the size of its input\n\n## convolutional layers\nx = Conv3D(filters=32, kernel_size=(3, 3, 3), use_bias=False, padding='Same')(x)\nx = BatchNormalization()(x)\nx = Activation(\"relu\")(x)\nx = Conv3D(filters=64, kernel_size=(3, 3, 3), use_bias=False, padding='Same')(x)\nx = BatchNormalization()(x)\nx = Activation(\"relu\")(x)\n\n## Pooling layer\nx = MaxPool3D(pool_size=(2, 2, 2))(x)\nx = Dropout(0.25)(x) #No more BatchNorm after this layer because we introduce Dropout\n\nx = Flatten()(x)\n\n## Dense layers\nx = Dense(units=4096, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(units=1024, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput_layer = Dense(units=10, activation='softmax')(x)\n\n## define the model with input layer and output layer\nmodel = Model(inputs=input_layer, outputs=output_layer, name=\"3D-CNN\")\nmodel_name = model.name\n\n#https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/\n#\"Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models which combines the best properties of the AdaGrad and RMSProp algorithms.\n#It provides an optimization algorithm that can handle sparse gradients on noisy problems. The default configuration parameters do well on most problems.\"\"\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=adam(), #default: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0\n              metrics=['acc'])\n\nmodel.summary()","14f41305":"from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n#patience:\npatience_earlystop = 7\npatience_ReduceLROnPlateau = 3\n\nfilepath = 'best_weight.h5'\nmcp = ModelCheckpoint(filepath, monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True, verbose=1)\nearlystop = EarlyStopping(monitor='val_loss',\n                          mode='min',\n                          patience=patience_earlystop,\n                          verbose=1)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=patience_ReduceLROnPlateau, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=1e-5)","21d57de4":"# Hyper Parameter\nbatch_size = 64\nepochs = 50\nhistory = model.fit(x=xtrain,\n                    y=y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.15,\n                    verbose=1,\n                    callbacks=[earlystop, learning_rate_reduction, mcp])","786e817c":"#Define a smooth function to display the training and validation curves\ndef plot_smoothed_learning_curves(history):\n    val_loss = history.history['val_loss']#[-30:-1] #Uncomment if you want to see only the last epochs\n    loss = history.history['loss']#[-30:-1]\n    acc = history.history['acc']#[-30:-1]\n    val_acc = history.history['val_acc']#[-30:-1]\n    \n    epochs = range(1, len(acc)+1 )\n    \n    # Plot the loss and accuracy curves for training and validation \n    fig, ax = plt.subplots(2,1, figsize=(12, 12))\n    ax[0].plot(epochs, smooth_curve(loss), 'bo', label=\"Smoothed training loss\")\n    ax[0].plot(epochs, smooth_curve(val_loss), 'b', label=\"Smoothed validation loss\",axes =ax[0])\n    legend = ax[0].legend(loc='best', shadow=True)\n    ax[0].set_xlabel('Epochs')\n    ax[0].set_ylabel('Loss')\n\n    ax[1].plot(epochs, smooth_curve(acc), 'bo', label=\"Smoothed training accuracy\")\n    ax[1].plot(epochs, smooth_curve(val_acc), 'b',label=\"Smoothed validation accuracy\")\n    legend = ax[1].legend(loc='best', shadow=True)\n    ax[1].set_xlabel('Epochs')\n    ax[1].set_ylabel('Accuracy')\n    return\n\ndef smooth_curve(points, factor=0.8):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous*factor + point*(1-factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points","e0a34980":"# Visualisation:\nplot_smoothed_learning_curves(history)","bb1f2ebe":"#Load the best weights:\nmodel.load_weights('best_weight.h5')","aa67ac63":"def plot_confusion_matrix(model_name):\n    # Predict the values from the validation dataset\n    y_pred = model_name.predict(xtest)\n    # Because Y_pred is an array of probabilities, we have to convert it to one hot vectors \n    y_pred = np.argmax(y_pred,axis = 1)\n    #Compute and print the accuracy scores:\n    print('accuracy score:', accuracy_score(y_test,y_pred))\n    # compute the confusion matrix \n    # By definition a confusion matrix C is such that C_i,j is equal to the number of observations known to be in group i but predicted to be in group j.\n    cm = confusion_matrix(y_test, y_pred)\n    cm = pd.DataFrame(cm, index = range(10), columns = range(10))\n    # plot the confusion matrix\n    plt.figure(figsize=(8,8))\n    sns.heatmap(cm, cmap=\"Reds\", annot=True, fmt='.0f')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    return\n\nplot_confusion_matrix(model)","ac97c96d":"## Confusion matrix","f8a9dd65":"## Visualisation","f57e17ab":"In adition to the original point clouds, the data in \"full_dataset_vectors.h5\" contains randomly rotated copies with noise.","7d3aec90":"This [blogpost](https:\/\/medium.com\/shashwats-blog\/3d-mnist-b922a3d07334) and this [kernel](https:\/\/www.kaggle.com\/shivamb\/3d-convolutions-understanding-use-case) helped me to produce this notebook."}}