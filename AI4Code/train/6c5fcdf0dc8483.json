{"cell_type":{"1efd87cf":"code","b7aa5cfb":"code","20fa87e5":"code","3fa0c5c9":"code","925b2138":"code","0e3cbb24":"code","202abd24":"code","fb8f252b":"code","b76fd822":"code","8c653a9e":"code","07d8514c":"code","2997668a":"code","2777f22a":"code","0b2738f9":"code","85e752dd":"code","0bebc51e":"code","10415477":"code","a0490c0d":"code","b0bc41d7":"code","b2ffec14":"code","f677c423":"code","1c5a8deb":"code","00edb414":"code","720510f4":"code","383f1e96":"code","312fb16b":"code","657c9e02":"code","9e70e510":"code","fe076586":"code","6f490e4c":"code","52699d2c":"code","035a49d9":"code","32f2105d":"code","1f4dcabf":"code","bf2410ee":"code","46807f53":"code","f758ca9a":"code","582b5d90":"code","26fa0c52":"code","931024ad":"code","0c1e07ae":"code","04df882e":"code","c25c3afc":"code","336d642b":"code","5c02d556":"code","d7be94c6":"code","3f890400":"code","4827178b":"code","069d124b":"code","89a31812":"markdown","d22d0eda":"markdown","3be6afa6":"markdown","5b7eacfd":"markdown","a87614e4":"markdown","3ecb1eb8":"markdown","97a6788a":"markdown","0dd133c9":"markdown","4304c9b9":"markdown","8197cf98":"markdown","2b3862a5":"markdown","28c7b0f9":"markdown","41799449":"markdown","21af30a3":"markdown","c3358104":"markdown"},"source":{"1efd87cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import boxcox\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.express as px\n#pd.options.plotting.backend = \"plotly\"\n\n# settings\nplt.style.use('seaborn')\nplt.rcParams[\"figure.figsize\"] = (16, 8)\n\nimport tensorflow as tf\n\npd.options.display.max_columns = 500\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7aa5cfb":"from datetime import datetime\ncustom_date_parser = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\n\ndf = {}\nbodytype = {'Aquifer','Water','Lake','River'}\nAquifer = {}\nWater = {}\nLake = {}\nRiver = {}\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        if filename[-4:]=='.csv':\n            if filename.startswith('Aquifer'):\n                Aquifer[filename[:-4].replace('Aquifer_','')] = pd.read_csv(\n                    os.path.join(dirname, filename)#,parse_dates = ['Date']#, infer_datetime_format = True\n                )\n            elif filename.startswith('Water'):\n                Water[filename[:-4].replace('Water_Spring_','')] = pd.read_csv(\n                    os.path.join(dirname, filename)\n                    #,parse_dates = ['Date']#, infer_datetime_format = True\n                )\n            elif filename.startswith('Lake'):\n                Lake[filename[:-4].replace('Lake_','')] = pd.read_csv(\n                    os.path.join(dirname, filename)\n                   ,parse_dates = ['Date']#, infer_datetime_format = True\n                )\n            elif filename.startswith('River'):\n                River[filename[:-4].replace('River_','')] = pd.read_csv(\n                    os.path.join(dirname, filename)\n                    ,parse_dates = ['Date'], date_parser = custom_date_parser\n                )\n                \n                    \n            #df[filename[:-4]] = pd.read_csv(\n            #    os.path.join(dirname, filename),\n            #    parse_dates = ['Date'], infer_datetime_format = True\n            #)\n            print('\\nDone File: ',filename[:-4])\n            #print(df[filename[:-4]].info())","20fa87e5":"print('Available Aquifers:')\nfor key in Aquifer.keys():\n    print(key)\n    #print(Aquifer[key].info())\nprint('\\n')\nprint('Available Waters:')\nfor key in Water.keys():\n    print(key)\nprint('\\n')\nprint('Available Lakes:')\nfor key in Lake.keys():\n    print(key)\nprint('\\n')\nprint('Available River:')\nfor key in River.keys():\n    print(key)","3fa0c5c9":"River['Arno'].info()","925b2138":"River['Arno'].describe().transpose()","0e3cbb24":"River['Arno']['yyyymm'] = (River['Arno'].Date.dt.year)*100 + River['Arno'].Date.dt.month\nRiver['Arno']['yyyy'] = River['Arno'].Date.dt.year","202abd24":"River['Arno'].groupby('yyyy').agg({'Date':'count',\n                                  'Rainfall_Le_Croci':'count','Rainfall_Cavallina':'count', 'Rainfall_S_Agata':'count',\n                                   'Rainfall_Mangona':'count', 'Rainfall_S_Piero':'count', 'Rainfall_Vernio':'count',\n                                   'Rainfall_Stia':'count', 'Rainfall_Consuma':'count', 'Rainfall_Incisa':'count',\n                                   'Rainfall_Montevarchi':'count', 'Rainfall_S_Savino':'count', 'Rainfall_Laterina':'count',\n                                   'Rainfall_Bibbiena':'count', 'Rainfall_Camaldoli':'count', 'Temperature_Firenze':'count',\n                                   'Hydrometry_Nave_di_Rosano':'count'\n                                  }).transpose()","fb8f252b":"columns = [\n    'Date','Rainfall_Le_Croci','Rainfall_Cavallina',\n    'Rainfall_S_Agata','Rainfall_Mangona','Rainfall_S_Piero',\n    'Temperature_Firenze',\n    'Hydrometry_Nave_di_Rosano'\n]\n\n\ndata = River['Arno'][River['Arno'].yyyy>2003][columns]\n\ndata['Rainfall'] = data[['Rainfall_Le_Croci','Rainfall_Cavallina',\n    'Rainfall_S_Agata','Rainfall_Mangona','Rainfall_S_Piero',]].sum(axis = 1)\n\nprint(data.describe().transpose())\nprint('\\n')\nfor column in columns[1:]:\n    data.plot.line(x='Date',y=column,figsize = (20,9),title = column.replace('_',' '))\n","b76fd822":"data.info()","8c653a9e":"sns.lineplot( \n             x=data.Date.dt.dayofyear, \n             y=data.Temperature_Firenze, \n             hue=data.Date.dt.year, \n             legend='full')\n\n# add title\nplt.title('Seasonal plot')\n\n# move the legend outside of the main figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2);","07d8514c":"sns.lineplot( \n             x=data.index, \n             y=data.Hydrometry_Nave_di_Rosano, \n             hue=data.Date.dt.year, \n             legend='full')\n\n# add title\nplt.title('Seasonal plot')\n\n# move the legend outside of the main figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2);","2997668a":"data.info()","2777f22a":"temperature = pd.read_csv(\"http:\/\/sir.toscana.it\/archivio\/download.php?IDST=termo_csv&IDS=TOS01001095\", \n                          sep =';',skiprows=18,parse_dates=True, infer_datetime_format =True, dayfirst=True).rename(\n    columns = {'gg\/mm\/aaaa':'Date', 'Max [\u00b0C]':'Max', 'Min [\u00b0C]':'Min'}\n)\ntemperature.Date = pd.to_datetime(temperature.Date,format='%d\/%m\/%Y')\ntemperature","0b2738f9":"temperature['data_int'] = temperature.Date.dt.strftime('%Y%m%d').astype(int)\ndata['data_int'] = data.Date.dt.strftime('%Y%m%d').astype(int)\n\ntemperature['Avg'] = temperature.Max*0.65+temperature.Min*0.35\n#x = pd.merge(data,temperature, on ='Date',how = 'left')","85e752dd":"data = data.merge(temperature, on ='data_int', how = 'left')\ndata.Temperature_Firenze.fillna(data.Avg, inplace= True)\ndata.drop('Date_y', inplace=True,axis = 1)\ndata.rename(columns={'Date_x':'Date'}, inplace=True)\nprint(data.groupby(data.Date.dt.year).agg({'Date':'count', 'Temperature_Firenze':'count'\n                                  }).transpose())\ndata.reset_index().plot.line('index','Temperature_Firenze')","0bebc51e":"data['Year sin'] = np.sin((data.Date.dt.dayofyear\/365.2425)*2*np.pi)\ndata['Year cos'] = np.cos((data.Date.dt.dayofyear\/365.2425)*2*np.pi)\ndata['Year'] = data.Date.dt.year\n#data.plot.line(x='Date', y=['Year sin','Year cos'])","10415477":"start = data.Date.min()\nend  = data.Date.max()\ndata.set_index('Date',inplace=True)\n\nindex_col = pd.date_range(start = start, end = end, freq = 'D')\ndata = data.reindex(index_col)","a0490c0d":"data['Hydrometry_Nave_di_Rosano'][data.Hydrometry_Nave_di_Rosano==0]=np.nan\n#data = data.assign(RollingMedian=data.Hydrometry_Nave_di_Rosano.fillna(data.Hydrometry_Nave_di_Rosano.rolling(7,min_periods=1,).median()))\ndata['Hydrometry_Nave_di_Rosano'].interpolate(method='spline',order = 5,inplace=True)\ndata.interpolate(method='linear',inplace=True)\n\n# this method is rather stupid but it's fast and quite simple. I'll try to build something better in a second step. \n#data['Temperature_Firenze'].fillna(data['Temperature_Firenze'].shift(periods=365*4), inplace=True)\n\n\ndata.reset_index().plot.line(x='index',y='Hydrometry_Nave_di_Rosano',figsize = (20,9),title = column.replace('_',' '))","b0bc41d7":"data.reset_index().plot.line(x='index',y='Temperature_Firenze',figsize = (20,9),title = column.replace('_',' '))","b2ffec14":"pd.plotting.autocorrelation_plot(data['Temperature_Firenze'])","f677c423":"data.drop(['data_int','Max','Min','Avg','Year'\n           ,'Rainfall_Le_Croci','Rainfall_Cavallina',\n    'Rainfall_S_Agata','Rainfall_Mangona','Rainfall_S_Piero'\n          ],axis = 1,inplace = True)\npd.plotting.scatter_matrix(data, figsize = (20,16))","1c5a8deb":"df = data[data.index.year<2020]\n#df.set_index('Date', inplace=True)\nprint(df.columns)\ncolumn_indices = {name: i for i, name in enumerate(df.columns)}\nnum_features = len(column_indices)\nn = len(df)\ntrain_df = df[0:int(n*0.7)]\nval_df = df[int(n*0.7):int(n*0.9)]\ntest_df = df[int(n*0.9):]\n\nrainfall_columns = [\n    #'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n    #   'Rainfall_Mangona', 'Rainfall_S_Piero',\n    'Hydrometry_Nave_di_Rosano',\n    'Rainfall'\n]\n\nnum_features = df.shape[1]\n\ntrain_df[rainfall_columns] = train_df[rainfall_columns].apply(lambda x: np.power(x,1\/4))\nval_df[rainfall_columns] = val_df[rainfall_columns].apply(lambda x: np.power(x,1\/4))\ntest_df[rainfall_columns] = test_df[rainfall_columns].apply(lambda x: np.power(x,1\/4))\n\nfor feature in rainfall_columns:\n    train_df[feature] = boxcox(train_df[feature] + 0.5,-1)\n    val_df[feature] = boxcox(val_df[feature] + 0.5,-1)\n    test_df[feature] = boxcox(test_df[feature] + 0.5,-1)\n\ntrain_mean = train_df.mean()\ntrain_std= train_df.std()\n\n\n\ntrain_df = (train_df - train_mean) \/ train_std\nval_df = (val_df - train_mean) \/ train_std\ntest_df = (test_df - train_mean) \/ train_std\n\ndf_std = (df - train_mean) \/ train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(df.keys(), rotation=90)","00edb414":"class WindowGenerator():\n    def __init__(self, input_width, label_width, shift,\n                 train_df=train_df, val_df=val_df, test_df=test_df,\n                 label_columns=None):\n        # Store the raw data.\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n\n        # Work out the label column indices.\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n\n        # Work out the window parameters.\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n\n        self.total_window_size = input_width + shift\n\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input indices: {self.input_indices}',\n            f'Label indices: {self.label_indices}',\n            f'Label column name(s): {self.label_columns}'])","720510f4":"def split_window(self, features):\n    inputs = features[:, self.input_slice, :]\n    labels = features[:, self.labels_slice, :]\n    if self.label_columns is not None:\n        labels = tf.stack(\n            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n            axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n    inputs.set_shape([None, self.input_width, None])\n    labels.set_shape([None, self.label_width, None])\n    \n    return inputs, labels\n\nWindowGenerator.split_window = split_window","383f1e96":"def plot(self, model=None, plot_col='Hydrometry_Nave_di_Rosano', max_subplots=3):\n    inputs, labels = self.example\n    plt.figure(figsize=(12, 8))\n    plot_col_index = self.column_indices[plot_col]\n    max_n = min(max_subplots, len(inputs))\n    for n in range(max_n):\n        plt.subplot(3, 1, n+1)\n        plt.ylabel(f'{plot_col} [normed]')\n        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n                 label='Inputs', marker='.', zorder=-10)\n        \n        if self.label_columns:\n            label_col_index = self.label_columns_indices.get(plot_col, None)\n        else:\n            label_col_index = plot_col_index\n\n        if label_col_index is None:\n            continue\n\n        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n        if model is not None:\n            predictions = model(inputs)\n            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                        marker='X', edgecolors='k', label='Predictions',\n                        c='#ff7f0e', s=64)\n\n        if n == 0:\n            plt.legend()\n\n    plt.xlabel('Time')\n\nWindowGenerator.plot = plot","312fb16b":"def make_dataset(self, data):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n        data=data,\n        targets=None,\n        sequence_length=self.total_window_size,\n        sequence_stride=1,\n        shuffle=True,\n        batch_size=256)\n\n    ds = ds.map(self.split_window)\n\n    return ds\n\nWindowGenerator.make_dataset = make_dataset","657c9e02":"@property\ndef train(self):\n    return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n    return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n    return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n    result = getattr(self, '_example', None)\n    if result is None:\n        # No example batch was found, so get one from the `.train` dataset\n        result = next(iter(self.train))\n        # And cache it for next time\n        self._example = result\n    return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","9e70e510":"MAX_EPOCHS = 50\n\ndef compile_and_fit(model, window, learning_rate = 0.001, epochs = MAX_EPOCHS, patience=2):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n    model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(learning_rate = learning_rate),\n                metrics=[tf.metrics.MeanAbsoluteError(),tf.metrics.MeanSquaredError()])\n\n    history = model.fit(window.train, epochs=epochs,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n    \n    x = pd.DataFrame(history.history)\n    x.plot.line(y=['loss','val_loss'],figsize = (5,3))\n    x.plot.line(y=['mean_absolute_error','val_mean_absolute_error'],figsize = (5,3))\n    \n    del x\n    \n    return history","fe076586":"OUT_STEPS = 3\nw1 = WindowGenerator(input_width=7, label_width=OUT_STEPS, shift=3\n                     #,label_columns=['Hydrometry_Nave_di_Rosano']\n                    )\nw1","6f490e4c":"multi_val_performance = {}\nmulti_performance = {}","52699d2c":"class MultiStepLastBaseline(tf.keras.Model):\n    def call(self, inputs):\n        return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n\nlast_baseline = MultiStepLastBaseline()\nlast_baseline.compile(loss=tf.losses.MeanSquaredError(),\n                      metrics=[tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError()])\n\nmulti_val_performance['Last'] = last_baseline.evaluate(w1.val)\nmulti_performance['Last'] = last_baseline.evaluate(w1.test, verbose=0)\nw1.plot(last_baseline)","035a49d9":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units]\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_lstm_model, w1)\n\nmulti_val_performance['M-LSTM'] = multi_lstm_model.evaluate(w1.val)\nmulti_performance['M-LSTM'] = multi_lstm_model.evaluate(w1.test, verbose=0)\nw1.plot(multi_lstm_model)","32f2105d":"class Baseline(tf.keras.Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        result = inputs[:, :, self.label_index]\n        return result[:, :, tf.newaxis]","1f4dcabf":"single_step_window = WindowGenerator(\n    input_width=7, label_width=1, shift=1,\n    label_columns=['Hydrometry_Nave_di_Rosano'])\n#single_step_window\n\nbaseline = Baseline(label_index=column_indices['Hydrometry_Nave_di_Rosano'])\n\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError(),tf.metrics.MeanSquaredError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(single_step_window.val)\nperformance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)","bf2410ee":"CONV_WIDTH = 2\nconv_window = WindowGenerator(\n    input_width=CONV_WIDTH,\n    label_width=1,\n    shift=1,\n    label_columns=['Hydrometry_Nave_di_Rosano'])\n\nconv_model = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=16,\n                           kernel_size=(CONV_WIDTH,),\n                           activation='relu'),\n    #tf.keras.layers.Dense(units=10, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n])\n\nhistory = compile_and_fit(conv_model, conv_window)\n\nval_performance['Conv'] = conv_model.evaluate(conv_window.val)\nperformance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)","46807f53":"SHIFT = 1\nCONV_WIDTH = 2\nLABEL_WIDTH = SHIFT\nINPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\nwide_conv_window = WindowGenerator(\n    input_width=INPUT_WIDTH,\n    label_width=LABEL_WIDTH,\n    shift=SHIFT,\n    label_columns=['Hydrometry_Nave_di_Rosano'])\n\nwide_conv_window.plot(conv_model)","f758ca9a":"wide_window = WindowGenerator(\n    input_width=21, label_width=21, shift=1,\n    label_columns=['Hydrometry_Nave_di_Rosano'])\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])\n\nhistory = compile_and_fit(lstm_model, wide_window, learning_rate = 0.003)\n\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\n\nwide_window.plot(lstm_model)","582b5d90":"linear = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1)\n])\n\nhistory = compile_and_fit(linear, single_step_window, epochs = 100, learning_rate = 0.01)\n\nval_performance['Linear'] = linear.evaluate(single_step_window.val)\nperformance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)","26fa0c52":"plt.bar(x = range(len(train_df.columns)),\n        height=linear.layers[0].kernel[:,0].numpy())\naxis = plt.gca()\naxis.set_xticks(range(len(train_df.columns)))\n_ = axis.set_xticklabels(train_df.columns, rotation=90)","931024ad":"x = np.arange(len(performance))\nwidth = 0.3\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\n\nplt.ylabel('mean_absolute_error [Hydrometry, normalized]')\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\n_ = plt.legend()","0c1e07ae":"for name, value in performance.items():\n    print(f'{name:12s}: {value[1]:0.4f}')","04df882e":"plt.bar(x = range(0,lstm_model.layers[0].weights[0].numpy().shape[0]),\n        height = np.average(lstm_model.layers[0].weights[0].numpy(),axis = 1), \n        yerr = np.var(lstm_model.layers[0].weights[0].numpy(),axis = 1),\n       align='center', ecolor='black', capsize=10)\naxis = plt.gca()\naxis.set_xticks(range(len(train_df.columns)))\n_ = axis.set_xticklabels(train_df.columns, rotation=90)","c25c3afc":"df = data[data.index.year<2020]\n#df.set_index('Date', inplace=True)\nprint(df.columns)\ncolumn_indices = {name: i for i, name in enumerate(df.columns)}\nnum_features = len(column_indices)\nn = len(df)\n\nrainfall_columns = [\n    #'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n    #   'Rainfall_Mangona', 'Rainfall_S_Piero',\n    'Rainfall',\n    'Hydrometry_Nave_di_Rosano'\n]\n\nnum_features = df.shape[1]\n\ndf[rainfall_columns] = df[rainfall_columns].apply(lambda x: np.power(x,1\/4))\n\nfor feature in rainfall_columns:\n    df[feature] = boxcox(df[feature] + 0.5,-1)\n\ndf_std = (df - train_mean) \/ train_std\n\nx = lstm_model.predict(wide_window.make_dataset(df_std))[:,-1,0]\n\nx = x*train_std['Hydrometry_Nave_di_Rosano'] + train_mean['Hydrometry_Nave_di_Rosano']\n\np = len(x)\n\nlstm_df = df[-p:]\nlstm_df['predict'] = x","336d642b":"lstm_df[:800].reset_index().plot.line(x = 'index',y=['Hydrometry_Nave_di_Rosano','predict'])","5c02d556":"lstm_df.corr()","d7be94c6":"River['Arno'].corr()","3f890400":"River['Arno']['Rainfall_Zona_A'] = River['Arno'][['Rainfall_Le_Croci','Rainfall_Cavallina','Rainfall_S_Agata'\n               ,'Rainfall_Mangona','Rainfall_S_Piero','Rainfall_Vernio']].sum(axis = 1)\nRiver['Arno']['Rainfall_Zona_B'] = River['Arno'][['Rainfall_Stia','Rainfall_Consuma','Rainfall_Incisa','Rainfall_Montevarchi'\n                                                 ,'Rainfall_S_Savino','Rainfall_Laterina','Rainfall_Bibbiena','Rainfall_Camaldoli']].sum(axis = 1)","4827178b":"sns.heatmap(River['Arno'].groupby(River['Arno'].Date.dt.year).count().transpose())","069d124b":"River['Arno'].plot.line(x='Date',y=['Rainfall_Zona_A','Rainfall_Zona_B'])","89a31812":"### Single Step Models","d22d0eda":"1. # An interpretable model\n\nWe want to understand which is the weight for each feature using the lstm model. Similarly to the linear model, we can study the weights (mean and variance) from the first layer of the LSTM model.","3be6afa6":"In the following, you can find the details about the different waterbodies:\n* River Arno: the Arno is the second largest river in peninsular Italy and the main waterway in Tuscany and it  has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays). Output = Hydrometry\n* Lake Bilancino: it is an artificial lake in Mugello, in the province of Florence. It has a maximum depth of thirty-one metres and a surface area of 5 square kilometres. Output(s): Lake level, flow rate\n* Aquifer: Output(s): Depth to Groundwater\n    * Auser: this water body consists of two subsystems, that we call NORH and SOUTH, where the former partly influences the behaviour of the latter. The levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well.\n    * Luco: it is an underground aquifer not fed by rivers or lakes but fed by meteoric infiltration and it is accessed through wells called Pozzo_1, Pozzo_3 and Pozzo_4.\n    * Petrignano: it is fed by three underground aquifers separated by low permeability septa; the water table can be considered groundwater and is also fed by the Chiascio river.\n    * Doganella: the Doganella well field is fed by two underground aquifers: the upper stratum is a water table with a thickness of about 30m while the lower one is a semi-confined artesian aquifer with a thickness of 50m.\n* WaterSprings: Output(s): Flow rate\n    * Amiata : This aquifer is accessed through the Ermicciolo, Arbure, Bugnano and Galleria Alta springs. The levels and volumes of the four springs are influenced by the parameters: pluviometry, sub-gradation, hydrometry, temperatures and drainage volumes.\n    * Madonna di Canneto: The Madonna di Canneto spring is situated at an altitude of 1010m above sea level in the Canneto valley. It does not consist of an aquifer and its source is supplied by the water catchment area of the river Melfa\n    * Lupa: It is located in the Arrone area and is used for drinking use.","5b7eacfd":"# Retrieve source datasets\n\nLet's start retrieving the datasets. To differentatie the different sources by waterbody type, I've decided to organize the datasets in dictionaries.","a87614e4":"## Last Point EDA","3ecb1eb8":"From the previous pivot table, we see that:\n* for year 1998, we have just the Hydrometry. All the metrics are null => I decide to drop this year\n* from 1999 to 2003, we have values on temparature but not Rainfall(s) => for the moment, I want to drop these years\n* several attributes for specific location are available only in a precise time range.\n\nThe last point is interesting. I can hypothesize the following explanations:\n* the weather reports from these locations are outdated, so they are not used anymore for forecasting\n* the weather reports are missing in this export, so we could try to retrieve these reports from open datasets\n\nFor the moment, since there're several years missing and the rainfalls are probably correlated, I prefer to focus on the following:\n* Rainfall_Le_Croci\t\n* Rainfall_Cavallina\n* Rainfall_S_Agata\n* Rainfall_Mangona\n* Rainfall_S_Piero\n* Rainfall_Vernio\n\nWe have missing Temperature values (almost 2 years). However, I expect this attribute to have a strong seasonality. Let's see more deeply...\n ","97a6788a":"## Multi-step models","0dd133c9":"Here we have all the available Acquifers, Waters, Lake (just one) and River (just one).","4304c9b9":"From the previous plots we see that:\n* Temperature and Hydrometry are strongly seasonal (and probably correlated)\n* We could \"forecast\" the temperature for the missing years\n* The hydrometer seems to have some \"wrong\" values equal to 0. ","8197cf98":"### Baseline Models\n\nIn this subsection, we build a baseline model which predict the future values by past duplication.","2b3862a5":"# Introduction\n\nIn this notebook I'll try to approach the Acea Smart Water Analytics competion. \nThe Acea Group is one of the leading Italian multiutility operators. The company manages and develops water and electricity networks and environmental services. I'm a bit familiar with Acea and the names in the datasets since I live in Italy. \n\\\nIn this notebook, I don't plan to address all the waterbodies but I plan to outline an precise pipeline (see their schema below):\n* to analyze the different datasets grouped by waterbody type\n* clean the data\n* build new features\n* train and validate forecasting model(s)\n\\\n\\\n![image](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6195295%2Fcca952eecc1e49c54317daf97ca2cca7%2FAcea-Input.png?generation=1606932492951317&alt=media)\n\\\n\\\nI'll try to keep it as simple as possible. In particular I will start with simple EDAs, simple models and then try to add complexity. ","28c7b0f9":"### River Arno\n\nWe start with the (maybe) simplest one: river Arno. Indeed, we have just one river and we need to forecast a single attribute (hydrometer). ","41799449":"### Enrich datasets with external datasources\n\nAt some point in my analysis, I had the following idea: what if there's an open dataset on the Region [website](http:\/\/sir.toscana.it\/idrometria-pub) about this data?\nAfter a bit of research, I found them (at least I think so)! Be aware this data should be validated and evetually cleaned, however I want just to show you how we can use this external dataset to enrich ours. This method is much more simpler than build an LSTM model to...forecast missing data. ","21af30a3":"### Multistep LSTM Model","c3358104":"# Forecasting Models\n\nIn this section we focus on building a forecasting model for our scenario. I've decided to use Tensorflow (because I'm studying it). In particular, I've largely used and experimented what is written in the [official tutorial](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/time_series). The following functions and classes are retrieved from this tutorial. \n\nI want to build a single step model to predict what happens netxt and a multi-step models, i.e. given data about past events, I want to predict a sequence of future values."}}