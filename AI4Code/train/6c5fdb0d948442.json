{"cell_type":{"1c6d80cf":"code","c545b456":"code","5792cbc9":"code","74eae4ec":"code","8cd160ec":"code","9fa717d3":"code","778e60fd":"code","3df277be":"code","b18231bd":"code","83f813fe":"code","41c0c8b8":"code","5b9f7341":"markdown","59e9395f":"markdown","e0028329":"markdown","2b1abde8":"markdown"},"source":{"1c6d80cf":"from sklearn.datasets import load_wine\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 8),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large',\n         'text.color' : \"green\",\n          \n         }\npylab.rcParams.update(params)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nsns.set_style(\"darkgrid\")","c545b456":"data = load_wine()","5792cbc9":"df = pd.DataFrame(data=data['data'],columns=data['feature_names'])\ndf['target']=data['target']\ndf['class']=df['target'].map(lambda ind: data['target_names'][ind])\ndf.head()","74eae4ec":"df.describe().T","8cd160ec":"sns.distplot(df['alcohol'],kde=1)\nplt.title(\"Alcohol conttent in all wines\")\nplt.show()","9fa717d3":"for i in df.target.unique():\n    sns.distplot(df['alcohol'][df.target==i],\n                 kde=1,label='{}'.format(i))\nplt.legend()\nplt.title(\"alcohol content distribution  by class\")\nplt.show()","778e60fd":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = \\\n    train_test_split(data['data'],data['target'],\n                     test_size=0.2)\nprint(len(X_train),' samples in training data\\n',\n      len(X_test),' samples in test data\\n', )","3df277be":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import accuracy_score\nfrom collections import namedtuple\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport time","b18231bd":"clf_dict = {\"Gradient Boosting Classifier\": \n            {'classifier': GradientBoostingClassifier(),\n                 'params': [\n                            {\n                             'learning_rate': [0.01, 0.1, 1.0],\n                             'n_estimators' :[10, 50, 100],\n                             'max_depth':[5, 3,7],\n                             'max_features' : [3, 5, 7, 11]\n                            }\n                           ]\n            },\n           }\n","83f813fe":"res_df  = pd.DataFrame()\nnum_clf = len(clf_dict.keys())\nres_df = pd.DataFrame(\n    data=np.zeros(shape=(num_clf, 4)),\n    columns = ['classifier',\n                   'train_score', \n                   'test_score',\n                   'training_time']\n)","41c0c8b8":"count = 0\nfor key, clf in clf_dict.items():\n    print(key, clf)\n    start = time.clock()\n    grid = GridSearchCV(clf[\"classifier\"],\n                        clf[\"params\"],\n                        refit=True,\n                        cv=10,\n                        scoring = 'accuracy',\n                        n_jobs = -1,\n                        verbose=0\n                        \n                       )\n    estimator = grid.fit(\n                        X_train,\n                        Y_train)\n    end = time.clock()\n    t_diff = end - start\n    train_score = estimator.score(X_train,\n                                      Y_train)\n    test_score = estimator.score(X_test,\n                                 Y_test)\n    count+=1\n    \n    res_df.loc[count,'classifier'] = key\n    res_df.loc[count,'train_score'] = train_score\n    res_df.loc[count,'test_score'] = test_score\n    res_df.loc[count,'training_time'] = t_diff\n    print(\"trained {c} in {f:.2f} s\".format(c=key,f=t_diff))\n    print(f\"{key} best params: {grid.best_params_}\")\nres_df.iloc[1:, :]","5b9f7341":"<center style=\"background-color:yellow\" > 1. solution <\/center>","59e9395f":"1. Write a Python program that tests a Gradient Boosting Machine on the sklearn\nwine dataset 1 for all combinations of the following parameters:\nNumber of trees (n_estimators): 10, 50, 100\nLearning rate: 0.01, 0.1, 1.0\nNumber of samples used to fit each tree (subsample): 0.5, 0.7, 1.0\nMaximum tree depth (max_depth): 3, 5, 7\nNumber of features (max_features): 3, 5, 7, 11\nUse 10-fold cross-validation. Your program should identify (and output) which\ncombination produces the best accuracy (and what that accuracy is). Include a\nlisting of your source code and the output from running it. \"","e0028329":"# Wine data set analysis\nsource : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_wine.html","2b1abde8":"### Model selection"}}