{"cell_type":{"b8c279f4":"code","aaaea9c9":"code","79be22b3":"code","57f9923f":"code","91cac899":"code","c232ea58":"code","0b5b1615":"code","eeefbfd4":"code","5e527032":"code","dba59956":"code","5e2170c5":"code","77acdbde":"code","c2ace7b7":"code","f18a1fa9":"code","0595a14a":"code","711a3de0":"code","4f389b0b":"code","c6c31c43":"code","3e209367":"code","fb739936":"code","a11df9f0":"code","cb0f41d3":"code","06f62a73":"code","b0577d36":"code","90f1765a":"code","b1de0518":"code","1a91eaaf":"code","86f7259b":"code","f014bdfd":"code","d4281d0f":"markdown","df87ccc8":"markdown","dbe2dbd5":"markdown","e70de792":"markdown","bcb63d51":"markdown","709607d2":"markdown","8c5cd121":"markdown"},"source":{"b8c279f4":"# import python standard library\nimport math\n\n# import data manipulation library\nimport numpy as np\nimport pandas as pd\n\n# import data visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import model function from sklearn\nfrom sklearn.ensemble import RandomForestRegressor\n\n# import sklearn model selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n# import sklearn model evaluation regression metrics\nfrom sklearn.metrics import mean_squared_error","aaaea9c9":"# acquiring training and testing data\ndf_train = pd.read_csv('..\/input\/train.csv', nrows=2000000, parse_dates=['pickup_datetime'])\ndf_test = pd.read_csv('..\/input\/test.csv', parse_dates=['pickup_datetime'])","79be22b3":"# visualize head of the training data\ndf_train.head(n=3)","57f9923f":"# visualize tail of the testing data\ndf_test.tail(n=3)","91cac899":"# combine training and testing dataframe\ndf_train['datatype'], df_test['datatype'] = 'training', 'testing'\ndf_test.insert(1, 'fare_amount', 0)\ndf_data = pd.concat([df_train, df_test], ignore_index=True)","c232ea58":"def scatterplot(numerical_x: list or str, numerical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a scatter plot applied for numerical variable in x-axis vs numerical variable in y-axis.\n    \n    Args:\n        numerical_x (list or str): The numerical variable in x-axis.\n        numerical_y (list or str): The numerical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    numerical_x, numerical_y = [numerical_x] if type(numerical_x) == str else numerical_x, [numerical_y] if type(numerical_y) == str else numerical_y\n    if nrows is None: nrows = (len(numerical_x)*len(numerical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.scatterplot(x=vj, y=vi, data=data, ax=axes[i*len(numerical_x) + j], rasterized=True) for i, vi in enumerate(numerical_y) for j, vj in enumerate(numerical_x)]\n    return fig","0b5b1615":"def distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n    \"\"\" Return the distance between 2 points of latitude and longitude.\n    \n    Args:\n        lat1 (float): The latitude of the first coordinate.\n        lon1 (float): The longitude of the first coordinate.\n        lat2 (float): The latitude of the second coordinate.\n        lon2 (float): The longitude of the second coordinate.\n    \n    Returns:\n        float: The distance between 2 points of latitude and longitude.\n    \"\"\"\n    angle = 0.017453292519943295 #math.pi \/ 180\n    x = 0.5 - np.cos((lat2 - lat1) * angle) \/ 2 + np.cos(lat1 * angle) * np.cos(lat2 * angle) * (1 - np.cos((lon2 - lon1) * angle)) \/ 2\n    return 0.6213712 * 12742 * np.arcsin(np.sqrt(x))","eeefbfd4":"# describe training and testing data\ndf_data.describe(include='all')","5e527032":"# list all features type number\ncol_number = df_data.select_dtypes(include=['number']).columns.tolist()\nprint('features type number:\\n items %s\\n length %d' %(col_number, len(col_number)))\n\n# list all features type object\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\nprint('features type object:\\n items %s\\n length %d' %(col_object, len(col_object)))","dba59956":"# feature exploration: histogram of all numeric features\n_ = df_data.hist(bins=20, figsize=(20, 15))","5e2170c5":"# feature extraction: fare amount\ndf_data['fare_amount'] = np.log1p(df_data['fare_amount'])","77acdbde":"# feature extraction: combination of keyword date\ndf_data['year'] = df_data['pickup_datetime'].dt.year\ndf_data['quarter'] = df_data['pickup_datetime'].dt.quarter\ndf_data['month'] = df_data['pickup_datetime'].dt.month\ndf_data['weekofyear'] = df_data['pickup_datetime'].dt.weekofyear\ndf_data['weekday'] = df_data['pickup_datetime'].dt.weekday\ndf_data['dayofweek'] = df_data['pickup_datetime'].dt.dayofweek\ndf_data['hour'] = df_data['pickup_datetime'].dt.hour","c2ace7b7":"# feature extraction: distance\ndf_data['distance_euclidean'] = distance(df_data['pickup_latitude'], df_data['pickup_longitude'], \\\n                                         df_data['dropoff_latitude'], df_data['dropoff_longitude'])\ndf_data['distance_latitude'] = df_data['dropoff_latitude'] - df_data['pickup_latitude']\ndf_data['distance_longitude'] = df_data['dropoff_longitude'] - df_data['pickup_longitude']","f18a1fa9":"# feature extraction: distance to specific location\nnyc = (40.7128, -74.0060)\njfk = (40.6413, -73.7781)\newr = (40.6895, -74.1745)\ndf_data['distance_pickup_to_nyc'] = distance(df_data['pickup_latitude'], df_data['pickup_longitude'], nyc[0], nyc[1])\ndf_data['distance_pickup_to_jfk'] = distance(df_data['pickup_latitude'], df_data['pickup_longitude'], jfk[0], jfk[1])\ndf_data['distance_pickup_to_ewr'] = distance(df_data['pickup_latitude'], df_data['pickup_longitude'], ewr[0], ewr[1])\ndf_data['distance_dropoff_to_nyc'] = distance(df_data['dropoff_latitude'], df_data['dropoff_longitude'], nyc[0], nyc[1])\ndf_data['distance_dropoff_to_jfk'] = distance(df_data['dropoff_latitude'], df_data['dropoff_longitude'], jfk[0], jfk[1])\ndf_data['distance_dropoff_to_ewr'] = distance(df_data['dropoff_latitude'], df_data['dropoff_longitude'], ewr[0], ewr[1])","0595a14a":"# feature extraction: fare amount per mile\ndf_data['fare_per_mile'] = df_data['fare_amount'] \/ df_data['distance_euclidean']\ndf_data['fare_per_mile'] = df_data['fare_per_mile'].apply(lambda x: 0 if x == float('inf') else x)\ndf_data['fare_per_mile'] = df_data['fare_per_mile'].fillna(0)","711a3de0":"# feature exploration: fare amount\ncol_number = df_data.select_dtypes(include=['number']).columns.tolist()\n_ = scatterplot(col_number, 'fare_amount', df_data[df_data['datatype'] == 'training'])","4f389b0b":"# feature exploration: fare per mile\ncol_number = df_data.select_dtypes(include=['number']).columns.tolist()\n_ = scatterplot(col_number, 'fare_per_mile', df_data[df_data['datatype'] == 'training'])","c6c31c43":"# feature exploration: season dataframe\ndf_season = df_data[df_data['datatype'] == 'training'].groupby(['year', 'month'], as_index=False).agg({\n    'fare_amount': 'mean'\n})\nfig, axes = plt.subplots(figsize=(20, 3))\n_ = sns.pointplot(x='month', y='fare_amount', data=df_season, join=True, hue='year')","3e209367":"# feature exploration: season dataframe\ndf_season = df_data[df_data['datatype'] == 'training'].groupby(['year', 'hour'], as_index=False).agg({\n    'fare_amount': 'mean'\n})\nfig, axes = plt.subplots(figsize=(20, 3))\n_ = sns.pointplot(x='hour', y='fare_amount', data=df_season, join=True, hue='year')","fb739936":"# feature extraction: drop na\ndf_data = df_data.dropna()","a11df9f0":"# convert category codes for data dataframe\ndf_data = pd.get_dummies(df_data, columns=['datatype'], drop_first=True)","cb0f41d3":"# describe data dataframe\ndf_data.describe(include='all')","06f62a73":"# verify dtypes object\ndf_data.info()","b0577d36":"# compute pairwise correlation of columns, excluding NA\/null values and present through heat map\ncorr = df_data[df_data['datatype_training'] == 1].drop(['key'], axis=1).corr()\nfig, axes = plt.subplots(figsize=(20, 15))\nheatmap = sns.heatmap(corr, annot=True, cmap=plt.cm.RdBu, fmt='.1f', square=True, vmin=-0.8, vmax=0.8)","90f1765a":"# select all features\nx = df_data[df_data['datatype_training'] == 1].drop(['key', 'pickup_datetime', 'fare_amount', 'fare_per_mile', 'datatype_training'], axis=1)\ny = df_data[df_data['datatype_training'] == 1]['fare_amount']","b1de0518":"# perform train-test (validate) split\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=0.25, random_state=58)","1a91eaaf":"# random forest regression model setup\nmodel_forestreg = RandomForestRegressor(n_estimators=10, max_depth=20, min_samples_split=1000, random_state=58)\n\n# random forest regression model fit\nmodel_forestreg.fit(x_train, y_train)\n\n# random forest regression model prediction\nmodel_forestreg_ypredict = model_forestreg.predict(x_validate)\n\n# random forest regression model metrics\nmodel_forestreg_rmse = mean_squared_error(y_validate, model_forestreg_ypredict) ** 0.5\nmodel_forestreg_cvscores = np.sqrt(np.abs(cross_val_score(model_forestreg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('random forest regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_forestreg_rmse, model_forestreg_cvscores.mean(), 2 * model_forestreg_cvscores.std()))","86f7259b":"# model selection\nfinal_model = model_forestreg\n\n# prepare testing data and compute the observed value\nx_test = df_data[df_data['datatype_training'] == 0].drop(['key', 'pickup_datetime', 'fare_amount', 'fare_per_mile', 'datatype_training'], axis=1)\ny_test = pd.DataFrame(np.expm1(final_model.predict(x_test)), columns=['fare_amount'], index=df_data.loc[df_data['datatype_training'] == 0, 'key'])","f014bdfd":"# submit the results\nout = pd.DataFrame({'key': y_test.index, 'fare_amount': y_test['fare_amount']})\nout.to_csv('submission.csv', index=False)","d4281d0f":"> **Acquiring training and testing data**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames.","df87ccc8":"> **Supply or submit the results**\n\nOur submission to the competition site Kaggle is ready. Any suggestions to improve our score are welcome.","dbe2dbd5":"> **Problem overview**\n\nIn this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!\n\nTo learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org\/NEXTextended to claim this specialization for free for the first month!","e70de792":"> **Feature exploration, engineering and cleansing**\n\nHere we generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution together with exploring some data.","bcb63d51":"> **Model, predict and solve the problem**\n\nNow, it is time to feed the features to Machine Learning models.","709607d2":"After extracting all features, it is required to convert category features to numerics features, a format suitable to feed into our Machine Learning models.","8c5cd121":"> **Analyze and identify patterns by visualizations**\n\nLet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilize the Seaborn plotting package which allows us to plot very conveniently as follows.\n\nThe Pearson Correlation plot can tell us the correlation between features with one another. If there is no strongly correlated between features, this means that there isn't much redundant or superfluous data in our training data. This plot is also useful to determine which features are correlated to the observed value.\n\nThe pairplots is also useful to observe the distribution of the training data from one feature to the other.\n\nThe pivot table is also another useful method to observe the impact between features."}}