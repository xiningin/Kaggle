{"cell_type":{"d871e7e9":"code","6ecb7a34":"code","3b48e9bf":"code","83c2170c":"code","701f0fcf":"code","16acc791":"code","cf21fadd":"code","df11c7b1":"code","7d139ebe":"code","3b0dfd79":"code","84ca3bd1":"code","f7f751dc":"code","7a7d0aaf":"code","3a22b59e":"code","4e98827e":"code","ede21892":"code","a0581af5":"code","495d17f5":"code","ad7ca324":"code","db6ace53":"code","7cb5a95d":"code","92caf69b":"code","41b8d07b":"markdown","8b52b651":"markdown","446c1177":"markdown","a0d1e570":"markdown","60ef10ec":"markdown","a99fa4c8":"markdown","c7a77848":"markdown","00fff06c":"markdown","34323e64":"markdown","61744c6c":"markdown","b8cff34e":"markdown","53da50e2":"markdown","676baac5":"markdown","e610923d":"markdown","f9756f12":"markdown"},"source":{"d871e7e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ecb7a34":"# Plotting Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\n\n# Sklearn and other libraries for building ML models\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# Feature selection\nfrom sklearn.feature_selection import RFE\n\n# Various notebook related imports\nimport warnings\nwarnings.filterwarnings('ignore')","3b48e9bf":"# Import the data for training into a Pandas DataFrame\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n# Import the data for test into a Pandas DataFrame\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","83c2170c":"print(f\"The train dataset has {train_data.shape[1]} columns and {train_data.shape[0]} records (rows).\")\nprint(f\"The test dataset has {test_data.shape[1]} columns and {test_data.shape[0]} records (rows).\")","701f0fcf":"train_data.head()","16acc791":"train_data.columns","cf21fadd":"features = train_data.columns[1:-1]\ntarget = train_data.columns[-1]","df11c7b1":"def plotly_bar_plot(x, y, x_label=None, y_label=None, title=None, threshold=None):\n    \"\"\"\n    Create plotly bar plots with a consistent style.\n    Inputs\n    ------\n    x: The x-axis values\n    y: The y-axis values\n    x_label: The label of the x axis.\n    y_label: The lable of the y axis.\n    title: The title of the plot.\n    Returns\n    -------\n    None\n    \"\"\"\n    fig = go.Figure([\n        go.Bar(x=x, y=y, marker_color=\"red\", marker_line_color=\"black\",),\n    ])    \n    fig.update_xaxes(showline=True, showgrid=False, linewidth=1, linecolor='#d6d6d6', mirror=True, title=\"Feature\")\n    fig.update_yaxes(showline=True, showgrid=False, linewidth=1, linecolor='#d6d6d6', mirror=True, title=\"Number of null values\")\n    fig.update_layout(template=\"plotly_white\",width=1200, height=500, title=\"Columns with nan values\")\n    fig.show()","7d139ebe":"plotly_bar_plot(x = features, y=train_data[features].isnull().mean().sort_values(ascending=False))","3b0dfd79":"def drop_nan_cols(df, threshold):\n    \"\"\"\n    Drop (remove) columns in a Pandas DataFrame that have a percentage of nan values greater that the threshold\n    Input\n    -----\n    df: (Pandas DataFrame) The DataFrame to check columns for nan values\n    threshold: (int) The percentage of nan values (>=) over which the column is dropped from the DataFrame. \n                     e.g 50 means that columns with nan values over 50% is dropped \n    \"\"\"\n    for col_name, null_ in zip(list(df.columns), df.isnull().sum()):\n        null_perc = (null_ \/ df.shape[0])*100\n        if null_perc >= threshold:\n            print(f\"Dropping column with name {col_name} - Null value percentage: {null_perc} %\")\n            df.drop(col_name, axis='columns', inplace=True)","84ca3bd1":"# Drop the columns with nan values more than 50%\ndrop_nan_cols(train_data, 50)\n# Update the features after droping a few columns\nfeatures = train_data.columns[1:-1]","f7f751dc":"plotly_bar_plot(x = features, y=train_data[features].isnull().mean().sort_values(ascending=False))","7a7d0aaf":"def cat_to_num(df1, df2=None, nan_replacement_value=None):\n    \"\"\" \n    Check the columns of a dataframe for numerical or categorical values and convert categorical data to numerical\n    The conversion involves corresponding an integer for every possible category. \n    \"\"\"\n    # Initially assign the nan_replacement_value to nan the values in the dataframe\n    df1.fillna(0, inplace=True)\n    bool_numeric_data_cols = df1.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())\n\n    for b, col_name in zip(bool_numeric_data_cols, df1.columns):\n        if not b:\n            possible_classes = list(set(df1[col_name]))\n            new_values = []\n            for value in df1[col_name]:\n                new_values.append(possible_classes.index(value))\n            df1.drop(col_name, axis='columns', inplace=True)\n            df1[col_name] = new_values\n            \ncat_to_num(train_data)\ncat_to_num(test_data)","3a22b59e":"train_data.head()","4e98827e":"rfe = RFE(XGBRegressor(n_estimators=50), n_features_to_select=30, step=1)\nselector = rfe.fit(train_data[features],train_data['SalePrice'])\nselector.support_\nreduced_features = []\nfor f, s in zip(features, selector.support_):\n    if s:\n        reduced_features.append(f)\nprint(reduced_features)","ede21892":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier(n_estimators=20)\nmodel.fit(train_data[features],train_data['SalePrice'])\nprint(model.feature_importances_)","a0581af5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_log_error\n\n\nLR = LogisticRegression(solver='liblinear', max_iter=10000)\nRF = RandomForestClassifier(max_depth=2)\n\nprint(\"Logistic Regression - Full Features\")\nscores_LR_full = cross_val_score(LR, train_data[features],train_data[target], scoring='neg_mean_squared_log_error', cv=5)\nprint(scores_LR_full)\n\n# print(\"Logistic Regression - Reduced Features\")\n# scores_LR_reduced = cross_val_score(LR, train_data[reduced_features],train_data[target], scoring='neg_mean_squared_log_error', cv=5)\n# print(scores_LR_reduced)\n\nprint(\"Random Forest - Full Features\")\nscores_RF_full = cross_val_score(RF, train_data[features],train_data[target], scoring='neg_mean_squared_log_error', cv=5)\nprint(scores_LR_full)\n\n# print(\"Random Forest - Reduced Features\")\n# scores_RF_reduced = cross_val_score(RF, train_data[reduced_features],train_data[target], scoring='neg_mean_squared_log_error', cv=5)\n# print(scores_RF_reduced)\n\nprint(\"XGBoost - Full Features\")\nscores_XGBoost_full = cross_val_score(XGBRegressor(n_estimators=100), train_data[features],train_data[target], scoring='neg_mean_squared_log_error', cv=5)\nprint(scores_XGBoost_full)\n\n# print(\"XGBoost - Reduced Features\")\n# scores_XGBoost_reduced = cross_val_score(XGBRegressor(n_estimators=100), train_data[reduced_features],train_data[target], scoring='neg_mean_squared_log_error', cv=5)\n# print(scores_XGBoost_reduced)\n\nprint(\"CatBoost - Full Features\")\nscores_CatBoost_full = cross_val_score(CatBoostRegressor(iterations=2, learning_rate=1, depth=2), train_data[features],train_data[target], scoring='neg_mean_squared_log_error', cv=5)\nprint(scores_CatBoost_full)","495d17f5":"# Make Predictions\nRF_model = RF.fit(train_data[features],train_data[target])\nRF_pred = RF_model.predict(test_data[features])","ad7ca324":"from xgboost import XGBRegressor\nscores = cross_val_score(XGBRegressor(n_estimators=50),\n                         train_data[features],train_data[target],\n                         scoring='neg_mean_squared_log_error', cv=10)\nscores","db6ace53":"XGBReg = XGBRegressor().fit(train_data[reduced_features],train_data[target])\nXGB_pred = XGBReg.predict(test_data[reduced_features])\n","7cb5a95d":"df_submit = pd.DataFrame()\ndf_submit[\"id\"] = test_data[\"Id\"]\ndf_submit[\"SalePrice\"] = XGB_pred","92caf69b":"df_submit.to_csv(\"sub_XGB.csv\", index=False)","41b8d07b":"<p style=\"font-size:16px;text-align:justify;\">\n    Let's check for <span\nstyle=\"font-style:italic; font-weight:bold;\">nan values<\/span> in the <span style=\"font-style:italic; font-weight:bold;\">train<\/span> dataset.\n<\/p","8b52b651":"<p style=\"font-size:16px;text-align:justify;\">\n    Removing from the list of the train_data columns the <span style=\"font-weight:bold;\">Id<\/span> and the <span style=\"font-weight:bold;\">SalePrice<\/span> we get the features list of the train dataset.\n<\/p","446c1177":"<p style=\"font-size:20px; color:#bd431e; border-bottom:1px solid #bd431e;padding-bottom:10px\" id=\"IMPORTING_AND_CLEANING\">2. FEATURE ENGINEERING<\/p>","a0d1e570":"<p style=\"font-size:18px; color:#de5c35;\">3.1 Recursive Feature Elimination (RFE)<\/p>","60ef10ec":"<p style=\"font-size:16px;text-align:justify;\">\n    Analysis of the <span><a href=\"https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\">advanced regression house prices<a><\/span> dataset and build Machine Learning (ML) models for predicting the <span style=\"font-style:italic;font-weight:bold;\">Sale Price<\/span> of a house. This is a supervised regression ML Let's start by importing the necessary Python Libraries.\n<\/p","a99fa4c8":"<p style=\"font-size:16px;text-align:justify;\">\n    In this section, we import and clean the data to prepare it for use with the ML models that we will develop later on. The dataset is already split into <span style=\"font-style:italic; font-weight:bold;\">train<\/span> and <span style=\"font-style:italic; font-weight:bold;\">test<\/span> datasets. The test dataset does not contain the <span style=\"font-style:italic; font-weight:bold;\">Sale Price<\/span> column that is the target that we have to predict.\n<\/p","c7a77848":"<p>\n    <a style=\"font-size:17px;color:black;text-decoration:none;\" href=\"#INTRODUCTION\">1. INTRODUCTION<\/a>\n<\/p>\n<p>\n<a style=\"font-size:17px;color:black;text-decoration:none;\" href=\"#IMPORTING_AND_CLEANING\">2. FEATURE ENGINEERING<\/a>\n<\/p>","00fff06c":"<p style=\"font-size:20px; color:#bd431e; border-bottom:1px solid #bd431e;padding-bottom:10px\">4. MODELING<\/p>","34323e64":"<p style=\"font-size:16px;text-align:justify;\">\n    The train and the test datasets are almost of equal size. Further, the train dataset contains both numerical and categorical data. We need to convert the categorical data to numerical before proceeding with the ML models.  \n<\/p","61744c6c":"<p style=\"font-size:20px; color:#bd431e; border-bottom:1px solid #bd431e;padding-bottom:10px\">Table Of Contents<\/p>","b8cff34e":"<p style=\"font-size:20px; color:#bd431e; border-bottom:1px solid #bd431e;padding-bottom:10px\">3. FEATURE SELECTION<\/p>","53da50e2":"<p style=\"font-size:20px; color:#bd431e; border-bottom:1px solid #bd431e;padding-bottom:10px\" id=\"INTRODUCTION\">1. INTRODUCTION<\/p>","676baac5":"<p style=\"font-size:16px;text-align:justify;\">\n    In this section we convert all categorical data to numerical in both the train and test datasets. \n<\/p","e610923d":"<p style=\"font-size:18px; color:#de5c35;\">2.2 Coverting categorical values to numerical<\/p>","f9756f12":"<p style=\"font-size:18px; color:#de5c35;\">2.1 Checking for nan values<\/p>"}}