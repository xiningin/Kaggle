{"cell_type":{"6abed9be":"code","bb38a31f":"code","d0e556fe":"code","9f16e3e9":"code","dab4b595":"code","aec4c625":"code","fee92051":"code","4645f972":"code","e7ac65b2":"code","c2db9541":"code","1ea79481":"code","cc4a8c27":"code","236e22c1":"code","80032140":"code","8fbfd5e9":"code","e65d9c66":"code","e136aa96":"code","6a3c032f":"code","6facfe9e":"code","e4711596":"code","6565b3e6":"code","c2402bd6":"code","a18342b6":"code","a31e9255":"code","2bdff954":"code","9b7722e9":"code","0447337d":"code","707d9bec":"code","71f8d2c3":"code","e7e38956":"markdown","bf1979f5":"markdown","38166b92":"markdown","db8f77b8":"markdown","41c84c09":"markdown","665f8768":"markdown","7861e350":"markdown","2e606f76":"markdown","3f45d628":"markdown","c87a519f":"markdown","6a4d2995":"markdown","d5807136":"markdown","c4fc5c16":"markdown","326114aa":"markdown","f6bd12b1":"markdown","3debc838":"markdown","9dd57e9d":"markdown","341f8494":"markdown","5a36920f":"markdown","4e0bdb55":"markdown","e14c8d9c":"markdown","adf6fcf1":"markdown","7440e226":"markdown","d59f267f":"markdown","d0b72263":"markdown","7089439b":"markdown","af792c5e":"markdown","f05cdaf8":"markdown","26d71b13":"markdown","a85f4c0f":"markdown","2ad98d10":"markdown","5f498779":"markdown","ade3e303":"markdown","e6c7dc14":"markdown","7fb11185":"markdown","76d43e18":"markdown","32ea003f":"markdown","b9d5abb0":"markdown","6ab397b2":"markdown","5edd56fe":"markdown","65498e08":"markdown","7e49c21c":"markdown","e715c2a1":"markdown","1128483a":"markdown","3623417f":"markdown","f491c213":"markdown"},"source":{"6abed9be":"# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb38a31f":"import re\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndatapath=\"\/kaggle\/input\/amazon-top-50-bestselling-books-2009-2019\/bestsellers with categories.csv\"\n","d0e556fe":"rawdata=pd.read_csv(datapath)\nrawdata","9f16e3e9":"sns.pairplot(rawdata)","dab4b595":"genredata=rawdata.groupby(\"Name\").mean()\ngenredata[\"Genre\"]=genredata.index.map(lambda name: rawdata[rawdata[\"Name\"]==name][\"Genre\"].values[0])\n\nbins_=np.arange(3.15,5.05,0.1)\n\nsns.displot(genredata, x=\"User Rating\",hue=\"Genre\", element=\"step\", bins=bins_)\nsns.displot(rawdata, x=\"User Rating\",hue=\"Year\", element=\"poly\", bins=bins_,fill=False)","aec4c625":"cleandata=rawdata\nyearsBSdata=rawdata.groupby(\"Name\").count().Year\nyearsELdata=rawdata.groupby(\"Name\").min().Year.apply(lambda x: 2020-x)\ncleandata[\"YearsBs\"]=cleandata[\"Name\"].apply(lambda name: yearsBSdata[name])\ncleandata[\"YearsEl\"]=cleandata[\"Name\"].apply(lambda name: yearsELdata[name])\ncleandata=cleandata.rename(columns={\"User Rating\":\"Rating\"})\ncleandata","fee92051":"yearsdata=cleandata.groupby(\"Name\").mean()\nsns.scatterplot(x=yearsdata.YearsBs,y=yearsdata.Rating)\nsns.displot(yearsdata, x=\"Rating\",hue=\"YearsBs\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","4645f972":"sns.scatterplot(x=yearsdata.YearsEl,y=yearsdata.Rating)\nsns.displot(yearsdata, x=\"Rating\",hue=\"YearsEl\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","e7ac65b2":"def is_name(string_searched,string_evaluated):\n    match=re.search(string_searched,string_evaluated)\n    if match!=None:\n       return True\n    else:\n       return False\n    \ndirtydata1=cleandata[cleandata.Name.apply(lambda name: is_name(r\"The 5 Love Languages: The Secret to Love [Tt]hat Lasts\", name))]\ndirtydata1","c2db9541":"dirtydata2=cleandata[(cleandata.Rating.round(2)!=cleandata.Name.apply(lambda name: yearsdata.Rating[name].round(2))) |\n                   (cleandata.Price!=cleandata.Name.apply(lambda name: yearsdata.Price[name]))]\ndirtydata2[\"MeanRating\"]=dirtydata2.Name.apply(lambda name: yearsdata.Rating[name].round(2))\ndirtydata2[\"MeanPrice\"]=dirtydata2.Name.apply(lambda name: yearsdata.Price[name].round(2))\ndirtydata2","1ea79481":"anomalous_ratio=(dirtydata1.shape[0]+dirtydata2.shape[0])\/cleandata.shape[0]\nprint(\"The anomalous data ratio is {:.2f} %\".format(anomalous_ratio*100.))","cc4a8c27":"cleanerdata=cleandata\ncleanerdata=cleanerdata.drop(index=dirtydata1.index)\ncleanerdata=cleanerdata.drop(index=dirtydata2.index)","236e22c1":"def outlier_remover(columnseries):\n    Q1=columnseries.describe()[\"25%\"]\n    Q3=columnseries.describe()[\"75%\"]\n    Q13=Q3-Q1\n    lowerbound=Q1-1.5*Q13\n    upperbound=Q3+1.5*Q13\n    newcolumnseries=columnseries[columnseries.between(lowerbound,upperbound)]\n    return newcolumnseries\n\nfor columnname in cleanerdata.columns:\n    if columnname not in [\"Name\",\"Author\",\"Genre\"]: cleanerdata[columnname]=outlier_remover(cleanerdata[columnname])\ncleanerdata=cleanerdata.dropna()\ncleanerdata.describe()","80032140":"newgenredata=cleanerdata.groupby(\"Name\").mean()\nnewgenredata[\"Genre\"]=newgenredata.index.map(lambda name: cleanerdata[cleanerdata[\"Name\"]==name][\"Genre\"].values[0])\n\nbins_=np.arange(3.15,5.05,0.1)\n\nsns.displot(newgenredata, x=\"Rating\",hue=\"Genre\", element=\"step\", bins=bins_)\nsns.displot(cleanerdata, x=\"Rating\",hue=\"Year\", element=\"poly\", bins=bins_,fill=False)","8fbfd5e9":"groupeddata=cleanerdata.groupby(\"Name\").mean()\nsns.scatterplot(x=groupeddata.YearsBs,y=groupeddata.Rating)\nsns.displot(groupeddata, x=\"Rating\",hue=\"YearsBs\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","e65d9c66":"sns.scatterplot(x=groupeddata.YearsEl,y=groupeddata.Rating)\nsns.displot(groupeddata, x=\"Rating\",hue=\"YearsEl\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","e136aa96":"author_dummies=pd.get_dummies(cleanerdata[\"Author\"], prefix=\"author\")\nyear_dummies=pd.get_dummies(cleanerdata[\"Year\"], prefix=\"year\")\ngenre_dummies=pd.get_dummies(cleanerdata[\"Genre\"], prefix=\"genre\")\ndummydata=pd.concat([cleanerdata[\"Name\"],author_dummies,year_dummies,genre_dummies] ,axis=1)\ndummydata_summed=dummydata.groupby(\"Name\").max()\nfor column in dummydata_summed.columns:\n    cleanerdata[column]=cleanerdata[\"Name\"].apply(lambda name: dummydata_summed.loc[name][column]) ","6a3c032f":"learningdata=cleanerdata.groupby(\"Name\").mean().drop(columns=\"Year\")","6facfe9e":"learningdata_=learningdata\nlearningdata_y=learningdata_.pop(\"Rating\").values\nlearningdata_X=learningdata_.values\nX_train, X_test, y_train, y_test = train_test_split(learningdata_X,learningdata_y,test_size=0.15, random_state=29)","e4711596":"Xscaler=StandardScaler().fit(X_train)\nyscaler=StandardScaler().fit(y_train.reshape(-1,1))\nX_train=Xscaler.transform(X_train)\nX_test=Xscaler.transform(X_test)\ny_train=np.ravel(yscaler.transform(y_train.reshape(-1,1)))\ny_test=np.ravel(yscaler.transform(y_test.reshape(-1,1)))","6565b3e6":"predratings_train=pd.DataFrame()\npredratings_test=pd.DataFrame()\n\npredratings_train[\"Reality\"]=np.ravel(yscaler.inverse_transform(y_train.reshape(-1,1)))\npredratings_test[\"Reality\"]=np.ravel(yscaler.inverse_transform(y_test.reshape(-1,1)))\n\nmodelRMSE_train={}\nmodelRMSE_test={}\n\ndef tuned_model(model, param_grid, results=False):\n    tuner=GridSearchCV(model,param_grid)\n    tuner.fit(X_train,y_train)\n    best_model=tuner.best_estimator_\n    best_params=tuner.best_params_\n    print(f\"Best model: {best_model}\")\n    if results==True:\n\n        print(pd.DataFrame(tuner.cv_results_).sort_values(by=['rank_test_score']).dropna())\n    return best_model\n\ndef execute_model(model,modelname):\n    model.fit(X_train,y_train)\n    y_train_pred=model.predict(X_train)\n    y_test_pred=model.predict(X_test)\n    predratings_train[modelname]=np.ravel(yscaler.inverse_transform(y_train_pred.reshape(-1,1)))\n    predratings_test[modelname]=np.ravel(yscaler.inverse_transform(y_test_pred.reshape(-1,1)))\n\n\n    modelRMSE_train[modelname]=mean_squared_error(y_train,y_train_pred,squared=False)\n    modelRMSE_test[modelname]=mean_squared_error(y_test,y_test_pred,squared=False)","c2402bd6":"SGDR_param_grid={\"alpha\":[1,0.5,0.1,0.05,0.01],\"loss\":[\"squared_loss\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\"penalty\":[\"l2\", \"l1\", \"elasticnet\"]}\nSGDR=tuned_model(SGDRegressor(),SGDR_param_grid,results=True)\nexecute_model(SGDR,\"Stochastic Gradient Descent\")","a18342b6":"SVectoR_param_grid={\"C\":[0.1,0.5,1,5,10],\"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\nSVectoR=tuned_model(SVR(),SVectoR_param_grid,results=True)\nexecute_model(SVectoR,\"Support Vector\")","a31e9255":"DTreeR_param_grid={\"ccp_alpha\":[0.8,0.5,0.01,0.005,0.0001],\"max_depth\":[None, 10, 20, 40],\"criterion\":[\"mse\", \"friedman_mse\", \"mae\", \"poisson\"]}\nDTreeR=tuned_model(DecisionTreeRegressor(),DTreeR_param_grid,results=True)\nexecute_model(DTreeR,\"Decision Tree\")","2bdff954":"MLPR_param_grid={\"alpha\":[0.1,0.01,0.001,0.0001],\n                 \"max_iter\":[10,20,40,80], \n                 \"hidden_layer_sizes\":[10,20,40,60],\n                 \"activation\":[\"identity\",\"tanh\",\"logistic\",\"relu\"]}\nMLPR=tuned_model(MLPRegressor(),MLPR_param_grid,results=True)\nexecute_model(MLPR,\"Multi-Layer Perceptron\")","9b7722e9":"fig,ax=plt.subplots(figsize=(20,10),ncols=4, nrows=2)\n\nfig.suptitle(\"Model predictions vs actual ratings (Top: Training data, Bottom: Testing data)\",fontsize=16)\nfigrows=ax.shape[0]\nfigcols=ax.shape[1]\nfor i in range(0,figrows):\n    for j in range(0,figcols):\n        ax[i][j].set_xlim(4,5)\n        ax[i][j].set_ylim(4,5)\n\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Stochastic Gradient Descent\"],ax=ax[0][0])\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Support Vector\"],ax=ax[0][1])\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Decision Tree\"],ax=ax[0][2])\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Multi-Layer Perceptron\"],ax=ax[0][3])\n\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Stochastic Gradient Descent\"],ax=ax[1][0])\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Support Vector\"],ax=ax[1][1])\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Decision Tree\"],ax=ax[1][2])\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Multi-Layer Perceptron\"],ax=ax[1][3])\n","0447337d":"print(\"Root Mean Square Error (Training data)\")\nfor key in modelRMSE_train:\n    print(\"{}: {}\".format(key,modelRMSE_train[key]))\nprint(\"\\n\")    \nprint(\"Root Mean Square Error (Test data)\")\nfor key in modelRMSE_test:\n    print(\"{}: {}\".format(key,modelRMSE_test[key]))","707d9bec":"predratings_train.round(2)","71f8d2c3":"predratings_test.round(2)","e7e38956":"There is some data that wasn't scrapped as intended. Some books have different versions.\n\nThere are books with slightly altered names for each version:","bf1979f5":"#### 4.1.4 Multi-Layer Perceptron Regressor","38166b92":"The anomalous data is less than 10% of total data. We can remove it.","db8f77b8":"The plots show that fiction books are slightly better rated than the not fictional ones and the books that have been bestsellers in the last few years have better ratings than the ones they have not. In both cases the correlation is not very strong.","41c84c09":"## 3. Preparing the data to be machine learnt","665f8768":"Fortunately our initial assumptions haven't changed so much.","7861e350":"#### 3.4.2 Scaling","2e606f76":"#### 4.1.1 Stochastic Gradient Descent Regressor","3f45d628":"Now that we have the data properly structured, we can prepare it to be machine learned. First we will split it into training and testing data and then we will rescale it so it will be easier to learn. We will use the StandardScaler tool, which will make our data have mean=0 and variance=1.","c87a519f":"We see that the more years as a bestseller a book has been, the more likely is to have a good rating. Also the newer bestsellershave slightly better ratings that the older ones.","6a4d2995":"## 5. Conclusions","d5807136":"## 2. Exploring the data","c4fc5c16":"## 4. Machine learning the data","326114aa":"### 3.1 Data cleaning","f6bd12b1":"#### 4.2.2 RMSE comparation","3debc838":"### 3.3 Dummy variables generation","9dd57e9d":"# Amazon Top 50 Bestselling Books 2009 - 2019 | Rating prediction using several methods\n\nFirst we will take a general look at the data, then we will generate more useful variables based on the original ones and then we will explore several methods to see which one predicts user ratings better.","341f8494":"All 4 algorythms struggle to learn the data as shown in the width of the training plot \"lines\", that limits the quality of the testing data rating predictions.","5a36920f":"### 2.1 Creating new variables from the original ones","4e0bdb55":"Before continuing we may repeat the rating distribution figures to see how much the anomalous data affected the results.","e14c8d9c":"### 2.2 Looking for new correlations","adf6fcf1":"We will compute the number of years as a bestseller (YearsBs) and the number of years elapsed since the first time a book was a bestseller (YearsEl).","7440e226":"#### 4.1.3 Decision Tree Regressor","d59f267f":"## 1. First look at the data","d0b72263":"To evaluate each algorythm we use cross-validation and parameter searching to avoid overfitting issues and to select a decent parameter combination.","7089439b":"Now we generate the dummy variables of \"author\", which indicate the author of the book, and \"year\", which indicate in which years the book was a bestseller. First we generate them and assign them to \"Name\", but in multiple year bestsellers each row will only activate the dummy variable corresponding to that year, so we have to group by \"Name\" and take the maximum value of the dummy variables so each year as a bestseller will be activated. Then we assign the new dummy variables to the cleaned dataset. In the end, multirow books will have each row with the author, genre and corresponding year dummy variables activated and they will have the same other variables except \"Year\".","af792c5e":"### 4.2 Algorythm comparation","f05cdaf8":"### 1.1 Visualizing the original data","26d71b13":"However there are some books with 2 versions with the same name variable.","a85f4c0f":"### 1.2 Looking for direct correlations.","2ad98d10":"The data shows the user rating, the total number of reviews and the price at 2019 and the genre (fic\/non fic) each book, which occupy a row for each year as a bestseller. Now let's see the pairplot of the numeric variables.","5f498779":"#### 4.1.2 Support Vector Regression","ade3e303":"### 4.1 Model evaluation","e6c7dc14":"## 0. Setting up the analysis. Importing the libraries and the datafile.","7fb11185":"#### 3.4.1 Splitting","76d43e18":"At first glance we don't see a direct correlation between the user ratings and the other numeric variables with the pairplot. Let's try to visualize the user rating distribution for fiction\/not fiction books and the distribution of the bestsellers of each year.","32ea003f":"#### 4.2.3 Data comparation","b9d5abb0":"1. The first data analysis shows that:\n   - The newer bestsellers tend to have slightly better ratings than the older ones.\n   - The more years a book has been a bestseller, the more likely is to have a good rating.\n   - Fiction books are better rated than non fiction ones.\n2. About the model selection and the machine learning algorythms used:\n   - The lack of samples after the data cleaning have made the data difficult to learn properly.\n   - The Decision Tree algorythm is notably the less effective algorythm used.\n   - The Stochastic Gradient Descent is the one which gives better results, closely followed by the Support Vector\n     and the Multi-Layer Perceptron.\n   ","6ab397b2":"Finally we group the clean data by \"Name\" with the mean values to drop the non numeric columns (\"Author\", \"Genre\"). Then we drop \"Year\" so now each book occupies a single row with all the correct numeric and dummy variables.","5edd56fe":"#### 4.2.1 Graphical comparation.","65498e08":"Now we will clean the outliers, using the quantile criteria.\n\n\\begin{equation}\nQ_1- 1.5 \\ Q_{13}<{(X,y)} \\leq Q_3+1.5 \\ Q_{13}\n\\end{equation}\n\nwhere\n\n\\begin{equation}\nQ_{13}=Q_3-Q_1\n\\end{equation}","7e49c21c":"TRAINING DATASET","e715c2a1":"### 3.2 Outlier cleaning","1128483a":"Now let's evaluate if we can remove the anomalous data or if it's worth it to keep it and try to correct it.","3623417f":"TESTING DATASET","f491c213":"### 3.4 Splitting and scaling the data"}}