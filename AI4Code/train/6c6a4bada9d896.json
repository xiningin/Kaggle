{"cell_type":{"453f2c70":"code","ce261bdb":"code","98b4edae":"code","44e9148e":"code","a8284918":"code","4aa79bc5":"code","ee9400b9":"code","5555c24c":"code","26fb74d3":"code","8197b348":"code","a03d7a46":"code","54857205":"code","f8c9c023":"code","81639323":"code","4c95844c":"code","2921bcf8":"code","7e537406":"code","3e146bbf":"code","79b523c9":"code","1bd0bdb6":"code","807db4ee":"code","428ce701":"code","1934a0c5":"code","4292b358":"code","b892c5ee":"markdown","71eccd94":"markdown","7de761da":"markdown","d5e54f76":"markdown","9ca9cb44":"markdown","fd319054":"markdown","6d4ac5e4":"markdown","8f040f7e":"markdown","ee75124f":"markdown","387b5214":"markdown","d1c67401":"markdown","1a557814":"markdown","aa9e5b88":"markdown","590b97c9":"markdown","6b1b2ae6":"markdown","1ebd21ff":"markdown"},"source":{"453f2c70":"!pip install pyspark","ce261bdb":"from pyspark.sql import SparkSession\nspark_ex = SparkSession.builder.getOrCreate()\nprint(spark_ex)","98b4edae":"# Don't change this file path\nfile_path = \"..\/input\/titanic\/test.csv\"\n\n# Read in the titanic data\ntitanic = spark_ex.read.csv(file_path,header=True)\n\n# Show the data\ntitanic.show()","44e9148e":"titanic = titanic.filter(titanic.Age.isNotNull()).filter(titanic.SibSp.isNotNull()).filter(titanic.Fare.isNotNull()).filter(titanic.Parch.isNotNull())\ntitanic.show()","a8284918":"titanic.count()","4aa79bc5":"titanic.filter(\"Fare > 200\").show()","ee9400b9":"titanic.filter(titanic.Fare > 200).show()","5555c24c":"# selecting some columns\nselected1 = titanic.select(\"Pclass\", \"Sex\", \"Age\")\nselected1.show()","26fb74d3":"# trying in a different way\ntemp = titanic.select(titanic.Pclass, titanic.Sex, titanic.Age)\ntemp.show()","8197b348":"# first filter\nfilterA = titanic.Pclass == \"2\"\n\n# second filter\nfilterB = titanic.Sex == \"female\"\n\n# Filter the data, first by filterA then by filterB\nselected2 = temp.filter(filterA).filter(filterB)\nselected2.show()","a03d7a46":"titanic=titanic.withColumn(\"Parch\", titanic.Parch.cast(\"Int\")).withColumn(\"SibSp\", titanic.SibSp.cast(\"Int\")).withColumn(\"Fare\", titanic.Fare.cast(\"Int\"))","54857205":"#lets check schema \ntitanic.printSchema()","f8c9c023":"# Lets calculate avg fare per member (total memebers= sibilings+ parents + 1(that person))\navg_fare = (titanic.Fare\/(titanic.SibSp+titanic.Parch+1)).alias(\"avg_fare\")\n\n#lets check the dataframe\navg = titanic.select(\"PassengerId\",\"Fare\",avg_fare)\navg.show()","81639323":"# We can do the same using expression\nfare_avg = titanic.selectExpr(\"PassengerId\", \"Fare\", \"Fare\/(SibSp+Parch+1) as avg_fare\")\nfare_avg.show()","4c95844c":"# Find the min fare from females\ntitanic.filter(titanic.Sex == \"female\").groupBy().min(\"Fare\").show()\n\n# Find the max fare from males\ntitanic.filter(titanic.Sex == \"male\").groupBy().max(\"Fare\").show()","2921bcf8":"# avg fare of male travelling in Pclass\ntitanic.filter(titanic.Pclass==\"3\").filter(titanic.Sex==\"male\").groupBy().avg(\"Fare\").show()","7e537406":"# Group by Sex\nby_sex = titanic.groupBy(\"Sex\")\n\n# Number of flights each plane made\nby_sex.count().show()","3e146bbf":"\n# Group by origin\nby_class = titanic.groupBy(\"Pclass\")\n\n# Average duration of flights from PDX and SEA\nby_class.avg(\"Fare\").show()","79b523c9":"import pyspark.sql.functions as F\n\n# Group by Sex and Pclass\nby_month_dest = titanic.groupBy(\"Sex\",\"Pclass\")\n\n# Average departure delay by month and destination\nby_month_dest.avg(\"Fare\").show()","1bd0bdb6":"# Standard deviation of departure delay\nby_month_dest.agg(F.stddev(\"Fare\")).show()","807db4ee":"#creating table1\nt1 = titanic.select(\"PassengerId\", \"Name\", \"Sex\")\nt1.show()","428ce701":"#creating table2\nt2 = titanic.select(\"PassengerId\", \"Age\", \"Fare\")\nt2.show()","1934a0c5":"t2 = t2.withColumnRenamed(\"Fare\", \"New_Fare\")\nt2.show()","4292b358":"joined=t1.join(t2,on=\"PassengerId\",how=\"leftouter\")\njoined.show()","b892c5ee":"<a id=\"7\"><\/a>\n# 7. Performing Joins\n\nA join will combine two different tables along a column that they share which is called the key. Since we only have one table \"titanic\" we will break it into two tables t1 and t2 and it will have a common key \"PassengerId\"\n\nAfter combining tables t1 and t2 we will have another table which will combine the information of t1 and t2 as a third table\n\nFirst lets split the dataframes so that we can perfrom the join","71eccd94":"<a id=\"6\"><\/a>\n# 6. Grouping and Aggregating\n\n PySpark has a whole class devoted to grouped data frames: pyspark.sql.GroupedData. With Aggregating we can make more groups as per our requirement as shown below.","7de761da":"<a id=\"5\"><\/a>\n# 5. Aggregating\n\nAggregation methods, like .min(), .max(), and .count() are GroupedData methods and are created by calling the .groupBy() DataFrame method.\n\nFrist create a GroupedData object and then apply aggregating method like .min(), .max()","d5e54f76":"<a id=\"4\"><\/a>\n# 4. Selecting\n\nThe Spark variant of SQL's SELECT is the .select() method.\n\nAgrugments to provide: one for each column you want to select. eg (\"col1\",\"col2\")\n\nThese arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax). When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside .withColumn().\n\n**The difference between .select() and .withColumn() methods is that .select() returns only the columns you specify, while .withColumn() returns all the columns of the DataFrame in addition to the one you defined.**","9ca9cb44":"<a id=\"1\"><\/a>\n# 1. What is Spark?\n\nSpark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n\nAs each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n\nHowever, with greater computing power comes greater complexity.\n\nIf you are deciding whether or not Spark is the best solution for your problem you can consider questions like:\n* Is my data too big to work with on a single machine?\n* Can my calculations be easily parallelized?","fd319054":"# KEY NOTE\n\nThis notebook is the second notebook in series of complete guide to pyspark. Here we will perform some data manipulations, we ll use the local cluserter here but in real life for Big Data manipulation real time comuter clustering is used to process huge data in range of Terrabytes and Petabytes.\n\nThese are just my personal notes. I am sharing these so that it helps others too, who are trying to learn the similar concepts. Feel free to comment if you have any doubts or correct me if I am wrong anywhere in the code. Any Feedback is appreciated.\n\nIf you havent checked the first notebook, please feel free to do so [here](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-dataframes-nb1)","6d4ac5e4":"<a id=\"8\"><\/a>\n# 8. Epilogue\n\nOver the course of two notebooks we perfromed some basic data manipulations on local cluster using Spark Dataframes. This is something we use with Big Data as V's of Big data make the computation on the single computer impossible.\n\nIn coming notebooks I will try and implement some ML pipelines using Spark\n\nFeel free to check other related notebooks here:\n\n[1. Scalable Data Science: PySpark Core RDD](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-core-rdd)\n\n[2. Scalable Data Science: Pyspark DataFrames NB1](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-dataframes-nb1)","8f040f7e":"<a id=\"2\"><\/a>\n# 2. Using Spark in Python\n\nThe first step in using Spark is connecting to a cluster.\n\nIn practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n\nWhen just getting started with Spark it's simpler to just run a cluster locally. Thus for now, instead of connecting to another computer, all computations will be run on Kaggle's servers in a simulated cluster.\n\nCreating the connection is as simple as creating an instance of the SparkContext class. The class constructor takes a few optional arguments that allow us to specify the attributes of the cluster we're connecting to.\n\nAn object holding all these attributes can be created with the SparkConf() constructor. ","ee75124f":"We can also .select() method to perform column-wise operations. Before doing that lets convert some columns into interger types as they are marked as string in the schema, to do that we will perform integer casting","387b5214":"First lets remove rows with null values in column Age, SibSp, ParchFare","d1c67401":"Lets perform a left outer join, you can perform any joins as per your requirement. if you know joins from SQL feel free to perform it here","1a557814":"We can also rename a column as follows, if we wish to","aa9e5b88":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Navigation<\/h3>\n\n[1. What is Spark?](#1)     \n[2. Using Spark in Python](#2)    \n[3. Filtering](#3)     \n[4. Selecting](#4)     \n[5. Aggregating](#5)     \n[6. Grouping and Aggregating](#6)     \n[7. Performing Joins](#7)         \n[8. Epilogue](#8)              ","590b97c9":"<a id=\"3\"><\/a>\n# 3. Filtering Data\n\nWe will now use .filter() method. this is the Spark counterpart of SQL's WHERE clause. The .filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True\/False) values.\n\nThese two expressions are the same:\n> \n> titanic.filter(\"Fare > 200\").show()\n> \n> titanic.filter(titanic.Fare > 200).show()\n\nNotice that in the first case, we pass a string to .filter().\n\nIn the second case, we actually pass a column of boolean values to .filter().","6b1b2ae6":"We can also use .agg() method with which we can pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.\n\nThis submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table.","1ebd21ff":"As you can see null values are removed. Lets count the number of recordsw e are left with, in general its a bad idea to do so when dealing with Big Data as the data is gonna be in range of Terrabytes!\n\nBut We are safe for now, Lol"}}