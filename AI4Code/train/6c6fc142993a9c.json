{"cell_type":{"3790cced":"code","c0eadc3b":"code","069f7d4d":"code","75667edd":"code","1c756354":"code","900a1392":"code","20a30fac":"code","ed3054c7":"code","87ec2fea":"code","b15cbd98":"code","cccf83f3":"code","d48a2886":"code","cc65403f":"code","5d34e255":"code","1dc345a1":"code","05835ada":"code","8e1c4e03":"code","de5fec1e":"code","b7e1ff4a":"code","f2e46c78":"code","f9a6366c":"code","bbc318f2":"code","112ee45e":"code","3411f312":"code","187f56c7":"code","d5c05d00":"code","5cfa4fa6":"code","1bb29c18":"code","5f38a332":"code","cc8120c1":"code","d39cdfce":"code","66946993":"code","cc3a896b":"code","3e158eec":"code","a45f7da0":"code","542d861f":"code","c8e16f43":"code","97be9fa4":"code","5f6edcae":"code","b184e353":"code","aa17a213":"code","84e32dfb":"code","d0c8d442":"code","9526c7c3":"code","c933411b":"code","01156e47":"code","c809b870":"code","29ee20a5":"code","005a0da9":"code","c40dc086":"code","737f9314":"markdown","760af51e":"markdown","5327dea8":"markdown","b8cb63bf":"markdown","1ef58c4e":"markdown","441eaf23":"markdown","c1a5ebcc":"markdown","c93df1b2":"markdown","100e4b16":"markdown","084fbc27":"markdown","4e13083d":"markdown","700e1857":"markdown","f087eb47":"markdown","4c32b493":"markdown","acc1a5f3":"markdown","70da906d":"markdown","2d7a49eb":"markdown","18f0a96d":"markdown","22a807f7":"markdown","47ced047":"markdown","ac2c7a6e":"markdown","18d0a86b":"markdown","cf0e28f3":"markdown","84810d72":"markdown","1c3e6430":"markdown","9189c3c0":"markdown","56b3ab50":"markdown","f9cf179b":"markdown","8976eaa1":"markdown","359f9b67":"markdown","43684a30":"markdown","7dbe307b":"markdown","94644594":"markdown","0a74d0e8":"markdown","01a107ef":"markdown","2e0676b7":"markdown","ebffe07a":"markdown","98a0ea3c":"markdown","382f7c7b":"markdown","1f192c1b":"markdown","2856c74a":"markdown","fb43a6fe":"markdown","3b034ffb":"markdown","137f2a0e":"markdown","4862bb89":"markdown","9020be57":"markdown","ce03e32b":"markdown","8be78247":"markdown","dd1b252e":"markdown","4a285dcd":"markdown","addec967":"markdown","426ca4ea":"markdown","f82e6f63":"markdown","ae07f930":"markdown","7c6044ba":"markdown","5c302257":"markdown","4575c69c":"markdown","93c81d2b":"markdown","2bde69cc":"markdown","0defdd0c":"markdown","4fb4f522":"markdown","1baf7b9b":"markdown","ea469ed2":"markdown","c425f08c":"markdown","fc54c8f7":"markdown","6e3d5116":"markdown","96c1b4ef":"markdown","53c9620f":"markdown"},"source":{"3790cced":"!pip install --user -q tensorflow==2.7","c0eadc3b":"!pip install --user -q openpyxl","069f7d4d":"!pip install --user -q tensorflow-data-validation==1.5","75667edd":"import sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('Installing TensorFlow Data Validation')\n!pip install -q tensorflow_data_validation[visualization]\n\nimport tensorflow_data_validation as tfdv\nprint('TFDV version: {}'.format(tfdv.version.__version__))\n# Confirm that we're using Python 3\nassert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'","1c756354":"!pip install --user -q tensorflow-transform==1.5","900a1392":"!pip install --user -q tensorflow-hub==0.12","20a30fac":"!pip install --user -q Fire==0.4","ed3054c7":"import os\nimport pandas as pd\nimport numpy as np\nfrom operator import itemgetter\n# import fire # If you want to execute the train & evalute in a form of Shell command.\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import feature_column as fc\nimport tensorflow_data_validation as tfdv\nimport tensorflow_transform as tft\nfrom tensorflow.keras import layers\nprint(tf.__version__)","87ec2fea":"from typing import Text, Dict, List, Union, Tuple, Optional, NamedTuple","b15cbd98":"pyshical_devices = tf.config.experimental.list_physical_devices('GPU')\nprint(f\"List of GPUs Available: {len(pyshical_devices)}\")\ntry:\n    tf.config.experimental.set_memory_growth(pyshical_devices[0],True)\nexcept IndexError:\n    print('GPU Not Found!')","cccf83f3":"train_data = pd.read_excel(\"..\/input\/predict-book-prices\/train.xlsx\")\ntest_data = pd.read_excel(\"..\/input\/predict-book-prices\/test.xlsx\")","d48a2886":"train_data.head()","cc65403f":"train_data.info()","5d34e255":"train_data.describe()","1dc345a1":"# Generating protobuf data for visualizing the statistics\ntrain_stats = tfdv.generate_statistics_from_dataframe(train_data)\ntest_stats = tfdv.generate_statistics_from_dataframe(test_data)","05835ada":"# Visualize the input statistics using Facets.\ntfdv.visualize_statistics(lhs_statistics=train_stats, lhs_name='Train Dataset',\n                          rhs_statistics=test_stats, rhs_name=\"Test Dataset\")","8e1c4e03":"train_data[['Genre', 'BookCategory']].groupby('BookCategory').count()","de5fec1e":"train_data.pivot_table(index='Genre', columns='BookCategory').count()","b7e1ff4a":"msk = np.random.rand(len(train_data)) < 0.8\ntrain_dataframe = train_data[msk]\ndev_dataframe = train_data[~msk]","f2e46c78":"len(train_dataframe)","f9a6366c":"print(f\"length of Train Dataset: {len(train_dataframe)}\")\nprint(f\"length of Dev Dataset: {len(dev_dataframe)}\")","bbc318f2":"train_dataframe.columns = [feature.lower() for feature in train_dataframe.columns.to_list()]\ndev_dataframe.columns = [feature.lower() for feature in dev_dataframe.columns.to_list()]","112ee45e":"train_dataframe.columns","3411f312":"os.makedirs(os.path.join('Dataset'))\ntrain_dataframe.to_csv('.\/Dataset\/train_data.csv', index=False)\ndev_dataframe.to_csv('.\/Dataset\/dev_data.csv', index=False)\ntest_data.to_csv(\".\/Dataset\/test_data.csv\", index=False)","187f56c7":"dev_dataframe.iloc[:50, :].to_csv('Dataset\/batch_data.csv', index=False)","d5c05d00":"# By running this line, make sure that you're not doing any kind of operations.\n# or you will face a problem due to graph execution\ntf.config.run_functions_eagerly(False)","5cfa4fa6":"class Features(object):\n    \"\"\"\n    This class contains all the main features I'm using in this project.\n    \"\"\"\n    DEFAULTS_COLUMNS = ['title', 'author', 'edition', 'reviews', 'ratings', 'synopsis', 'genre',\n                        'bookcategory', 'price']\n    INFER_COLUMNS = ['title', 'author', 'edition', 'reviews', 'ratings', 'synopsis', 'genre',\n                     'bookcategory']\n    UNWANTED_FEATURES = ['synopsis', 'edition']\n    FEATURES = ['title', 'author', 'genre', 'bookcategory','reviews', 'ratings']\n    LABEL = 'price'\n    DEFAULTS = [['null'], ['null'], ['null'], ['null'],['null'], ['null'], ['null'], ['null'], [0.0]]","1bb29c18":"def create_dataset(pattern: Text,\n                   mode: Optional[Union[Text, None]],\n                   batch_size: int,\n                   num_epochs: int) -> Tuple[tf.data.Dataset,\n                                             tf.data.Dataset]:\n    \"\"\"\n    Create dataset using tf.data API from CSV file.\n        Args:\n            Pattern[Text]: Path of the CSV file.\n            batch_size [int]: numbers of example per batch.\n            mode [Optional[Text, None]]: decide whether we're going to shuffle the dataset \n                                         or not.\n            num_epochs [int]: numbers of times proving the data for training.\n\n    \"\"\"\n    def features_label(row_data: Dict[Text, tf.Tensor]) -> Tuple[\n                                                            Dict[Text,\n                                                                 tf.Tensor],\n                                                            tf.Tensor]:\n        \"\"\"\n        This function responsible for splitting the dataset into features & label\n            Args:\n                raw_data[Dict[Text, tf.Tensor]]: dictionary of CSV column names and tensor values.\n            Returns:\n                Tuple[Dict[Text, tf.Tensor], tf.Tensor]: Tuple of Dictionary of features' Tensors & label Tensor\n        \"\"\"\n        label = row_data.pop(Features.LABEL)\n        features = row_data\n        for unwanted in Features.UNWANTED_FEATURES:\n            features.pop(unwanted)\n\n        return features, label\n\n    dataset = tf.data.experimental.make_csv_dataset(file_pattern=pattern,\n                                                    batch_size=batch_size,\n                                                    column_names=Features.DEFAULTS_COLUMNS,\n                                                    column_defaults=Features.DEFAULTS)\n    dataset = dataset.map(features_label)\n\n    if mode == 'train':\n        num_epochs = None\n        dataset = dataset.shuffle(buffer_size=batch_size * 10)\n    else:\n        num_epochs = num_epochs\n\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset","5f38a332":"dataset = create_dataset(\"Dataset\/batch_data.csv\", \"train\", 10, 9)\ndataset","cc8120c1":"os.makedirs(os.path.join('.', 'Vocabulary'))\nwith open('Vocabulary\/list_of_genre.txt', 'w') as log:\n    for line in train_data['Genre'].unique().tolist():\n        log.write(f\"{line}\\n\")\nwith open('Vocabulary\/list_of_bookCat.txt', 'w') as lobc:\n    for line in train_data['BookCategory'].unique().tolist():\n        lobc.write(f\"{line}\\n\")\nwith open('Vocabulary\/list_of_authors.txt', 'w') as auth:\n    for line in train_data['Author'].tolist():\n        auth.write(f'{line}\\n')","d39cdfce":"train_data['Reviews'].unique()","66946993":"# decorate all the tensors of reviews instead of one tensor\n@tf.function\ndef format_reviews(review:\n                   tf.TensorSpec(shape=[None],\n                                 dtype=tf.string)) -> tf.Tensor:\n    def formating(tensor: str) -> float:\n        return float(tf.strings.split(tensor, maxsplit=1).numpy()[0])\n    return tf.ensure_shape(tf.map_fn(lambda tensor:\n                                        tf.py_function(formating, [tensor], tf.float32),\n                                     review,\n                                     fn_output_signature=tf.float32,\n                                     parallel_iterations=1,\n                                     swap_memory=True),\n                           (None, ))\n\n\n@tf.function\ndef scale_reviews(review:\n                  tf.TensorSpec(shape=[None],\n                                dtype=tf.float32)) -> tf.Tensor:\n    return tft.scale_by_min_max(review,\n                                output_min=1.0,\n                                output_max=5.0)","cc3a896b":"train_data['Ratings'].unique()","3e158eec":"# decorate all the tensors of ratings instead of one tensor\n@tf.function\ndef format_ratings(rate:\n                   tf.TensorSpec(shape=[None],\n                                 dtype=tf.string)) -> tf.Tensor:\n    def formating(tensor: tf.Tensor) -> tf.Tensor:\n        return tf.strings.to_number(\n                    tf.strings.regex_replace(\n                        tf.strings.split(tensor, maxsplit=1).numpy()[0], ',', ''),\n                    out_type=tf.float32)\n    return tf.ensure_shape(\n               tf.map_fn(lambda tensor:\n                             tf.py_function(formating, [tensor], tf.float32),\n                         rate,\n                         fn_output_signature=tf.float32,\n                         parallel_iterations=1,\n                         swap_memory=True),\n               (None,))\n\n\ndef format_ratings_ds(dataset: tf.data.Dataset,\n                      batch_size: int,\n                      steps: int) -> tf.data.Dataset:\n    \"\"\"Prepare the `ratings` feature for normalization\"\"\"\n    dataset = dataset.map(lambda features, label: features['ratings'])\n    dataset = dataset.map(format_ratings)\n    scale = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n    scale.adapt(data=dataset, batch_size=batch_size, steps=steps)\n    return scale\n\n\n# Create our custom z-score\n@tf.keras.utils.register_keras_serializable()\nclass ZScoreCustomPreprocessor(tf.keras.layers.Layer):\n    \"\"\"Custom Class for calculating the z-score\"\"\"\n\n    def __init__(self,\n                 scale_layer,\n                 **kwargs):\n\n        super(ZScoreCustomPreprocessor, self).__init__(**kwargs)\n        self.lambda_layer = tf.keras.layers.Lambda\n        self._scale = scale_layer\n\n    @tf.function\n    def transform(self,\n                  value:\n                      tf.TensorSpec(shape=[None], dtype=tf.float32)):\n        return self._scale(value)\n\n    def call(self, inputs):\n        return self.lambda_layer(self.transform)(inputs)\n\n    def get_config(self):  # For Keras custom Serializing\n        config = super(ZScoreCustomPreprocessor, self).get_config()\n        config.update({'scale_layer': self._scale})\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","a45f7da0":"authors_list = open('Vocabulary\/list_of_authors.txt').read().splitlines()\nlen(authors_list)","542d861f":"def transformer(inputs: Dict[Text, tf.Tensor],\n                adapt_data: tf.data.Dataset,\n                batch_size: int,\n                steps: int,\n                authors_list: List[Text],\n                categorical_columns: List[Text]) -> Tuple[Dict[Text,\n                                                               tf.Tensor],\n                                                          Dict[Text,\n                                                               Union[fc.numeric_column,\n                                                                     fc.bucketized_column,\n                                                                     fc.categorical_column_with_vocabulary_file,\n                                                                     fc.embedding_column]]]:\n    \"\"\"\n    Transformer function responsibles for passes our numerical and sting column features as an input to\n    the model after applying feature engineering to these features.\n\n        Args:\n            inputs[Dict[Text, tf.Tensor]]: A Dictionary of feature columns and tensor values.\n            categorical_columns[List[Text]]: represents our list of features that \n                                             we're going to apply feature engineering on them.\n        Returns:\n            Tuple[Dict[Text, tf.Tensor],\n                  Dict[Text, Union[fc.numeric_column,\n                                  fc.bucketized_column,\n                                  fc.categorical_column_with_vocabulary_file,\n                                  fc.embedding_column]]]\n    \"\"\"\n    # Have a copy from the input features to pass-through columns.\n    transformed = inputs.copy()\n\n    # Now, we need to do our preprocessing and add it to the graph\n    transformed['reviews'] = layers.Lambda(format_reviews,\n                                           name='scrape_reviews')(inputs[\"reviews\"])\n    transformed['ratings'] = layers.Lambda(format_ratings,\n                                           name='scrape_ratings',)(inputs[\"ratings\"])\n\n    feature_columns = {\n        feature: fc.numeric_column(feature)\n        for feature in ['reviews', 'ratings']\n    }\n\n    # We need to discretize our Ratings & Reviews\n    range_of_reviews = np.arange(1.0,5.0,0.1).tolist()\n    range_of_ratings = list(range(1, int(1e+4), 5))\n    bucketize_reviews = fc.bucketized_column(source_column=\n                                                 feature_columns['reviews'],\n                                             boundaries=range_of_reviews)\n    bucketize_ratings = fc.bucketized_column(source_column=\n                                                 feature_columns['ratings'],\n                                             boundaries=range_of_ratings)\n\n#     hash_size_ratings_reviews = int(.5 * np.sqrt(len(range_of_reviews) * len(range_of_ratings))) # appply collisions\n    hash_size_ratings_reviews = int(2e+3)  # appply collisions\n\n    crossed_ratings_reviews = fc.crossed_column(\n                                    keys=[bucketize_reviews, bucketize_ratings],\n                                    hash_bucket_size=hash_size_ratings_reviews)\n\n    # On-hot encodding the our crossed Ratings & Reviews\n    feature_columns['crossed_ratings_reviews'] = fc.indicator_column(crossed_ratings_reviews)\n\n    # Scaling each of Reviews (scaling to Data Distribution using z-score\n    # & ratings (scaling it using min-max as it has a discrete range)\n    transformed[\"reviews\"] = layers.Lambda(scale_reviews,\n                                           name='scaled_reviews')(transformed[\"reviews\"])\n\n    scale = format_ratings_ds(dataset=adapt_data, batch_size=batch_size, steps=steps)\n    transformed[\"ratings\"] = ZScoreCustomPreprocessor(scale,\n                                                      name='scale_ratings')(transformed[\"ratings\"])\n\n    # Embed each of `author` and `title`\n    feature_columns['title'] = hub.text_embedding_column_v2(\n                                    key='title',\n                                    module_path='https:\/\/tfhub.dev\/google\/nnlm-en-dim50\/2',\n                                    trainable=False\n                               )\n    author_categorical_feature = fc.categorical_column_with_hash_bucket(\n                                    key='author',\n                                    hash_bucket_size=len(authors_list) + 10)\n\n    feature_columns['author'] = fc.embedding_column(\n                                    categorical_column=author_categorical_feature,\n                                    dimension=10)\n    # We need to embed our `Genre` and `BookCategory`\n    genre_vocab = fc.categorical_column_with_vocabulary_file(key=\"genre\",\n                                                             vocabulary_file=\n                                                                 'Vocabulary\/list_of_genre.txt',\n                                                             num_oov_buckets=5)\n    bookCat_vocab = fc.categorical_column_with_vocabulary_file(key='bookcategory',\n                                                             vocabulary_file=\n                                                                 'Vocabulary\/list_of_bookCat.txt',\n                                                             num_oov_buckets=2)\n    genre_vocab_size = 345\n    bookCat_vocab_size = 11\n#     bookcategory_genre_coll = int(.5 * np.sqrt(genre_vocab_size * bookCat_vocab_size))\n    feature_columns[\"genre_column\"] = fc.indicator_column(genre_vocab)\n    feature_columns[\"bookcategory_column\"] = fc.indicator_column(bookCat_vocab)\n\n    cross_genre_and_bookCat = fc.crossed_column([genre_vocab, bookCat_vocab],\n                                                hash_bucket_size=genre_vocab_size * bookCat_vocab_size)\n    feature_columns['bookcategory_genre'] = fc.embedding_column(cross_genre_and_bookCat,\n                                                                dimension=2)\n\n    return transformed, feature_columns","c8e16f43":"@tf.keras.utils.register_keras_serializable()\ndef rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n\n@tf.keras.utils.register_keras_serializable()\ndef rlmse(y_true, y_pred):\n    return tf.sqrt(\n            tf.reduce_mean(\n                tf.square(\n                    tf.experimental.numpy.log10(y_true + 1)\\\n                    - tf.experimental.numpy.log10(y_pred + 1))))","97be9fa4":"def model_build(adapt_data: tf.data.Dataset,\n                batch_size: int,\n                steps: int,\n                authors_list: List[Text],\n                linear_dnn_units: List[int],\n                categorical_dnn_units: List[int],\n                hidden_layer_unit: int) -> tf.keras.models.Model:\n    \"\"\"\n    Building the skeleton of the model\n    Args:\n        linear_dnn_units [List[int]]: List contains number of nodes inside\n                                      each of linear hidden layer.\n        categorical_dnn_units [List[int]]: List contains number of nodes inside\n                                          each of categorical hidden layer.\n        hidden_layer_units [int]: Numbers of unit of our pre-last layer.\n    Returns:\n        tf.keras.models.Model\n    \"\"\"\n    inputs = {\n        feature: layers.Input(name=feature, shape=(), dtype='string')\n        for feature in Features.FEATURES\n    }\n\n    transformed, feature_columns = transformer(inputs=inputs,\n                                               adapt_data=adapt_data,\n                                               batch_size=batch_size,\n                                               steps=steps,\n                                               authors_list=authors_list,\n                                               categorical_columns=Features.FEATURES)\n\n    numerical_dense_features = layers.DenseFeatures(\n                                  feature_columns=itemgetter(*[\"reviews\",\n                                                               \"ratings\",\n                                                               \"crossed_ratings_reviews\"])(feature_columns),\n                                  name='numerical_dense_features')(dict(\n                                                                    list(\n                                                                        transformed.items())[4:6]))\n\n    embedded_dense_features = layers.DenseFeatures(\n                                     feature_columns=itemgetter(*[\"title\",\n                                                                  \"author\",\n                                                                  \"genre_column\",\n                                                                  \"bookcategory_column\",\n                                                                  \"bookcategory_genre\"])(feature_columns),\n                                      name='embedded_dense_features')(dict(\n                                                                            list(\n                                                                                transformed.items())[:4]))\n    # Building dnn for Reviews & ratings features\n    linear_hidden_layers = numerical_dense_features\n    for layerNo, numNodes in enumerate(linear_dnn_units):\n        linear_hidden_layers = layers.Dense(units=numNodes,\n                                            activation=\"relu\",\n                                            kernel_initializer='normal',\n                                            name=f'linear_dnn_{layerNo+1}')(linear_hidden_layers)\n\n    # Building our depth layers using Genre & BookCategory for our categorical layers\n    categorical_hidden_layers = embedded_dense_features\n    for layerNo, numNodes in enumerate(categorical_dnn_units):\n        categorical_hidden_layers = layers.Dense(units=numNodes,\n                                            activation=\"relu\",\n                                            kernel_initializer='normal',\n                                            name=f'embedded_dnn_{layerNo+1}')(categorical_hidden_layers)\n\n    concatenation = layers.concatenate([linear_hidden_layers, categorical_hidden_layers],\n                                       name='features_concatenation')\n    hidden_layer_3 = layers.Dense(units=hidden_layer_unit,\n                                  activation=\"relu\", \n                                  kernel_initializer='normal',\n                                  name='hidden_layer',\n                                  kernel_regularizer=\n                                      tf.keras.regularizers.l1(l1=1e-2))(concatenation)\n\n    outputs = layers.Dense(1, activation='linear', name='price')(hidden_layer_3)\n\n    model = tf.keras.models.Model(outputs=outputs, inputs=inputs)\n    model.compile(optimizer=\"adam\",\n                  loss=rlmse,\n                  metrics=[rmse])\n\n    return model","5f6edcae":"model = model_build(adapt_data=dataset,\n                    batch_size=10,\n                    steps=873,\n                    authors_list=authors_list,\n                    linear_dnn_units=[64, 32, 16],\n                    categorical_dnn_units= [32, 16],\n                    hidden_layer_unit=8)\n\nmodel.summary()\n# tf.keras.utils.plot_model(model, 'model_graph.png', show_shapes=False)","b184e353":"from pprint import pprint\npprint(tf.keras.layers.serialize(model))","aa17a213":"del dataset","84e32dfb":"del model","d0c8d442":"def train_and_evaluate(train_path: Text,\n                       dev_path: Text,\n                       linear_dnn_units: List[int],\n                       categorical_dnn_units: List[int],\n                       hidden_layer_unit: int,\n                       train_examples: int,\n                       dev_examples: int,\n                       batch_size: int,\n                       epochs: int,\n                       steps: int,\n                       authors_list: List[Text],\n                       model_dir: Text,\n                       checkpoint_path: Text,\n                       tensorboard_logs_path: Text,\n                       start_from_latest_checkpoint: Optional[bool]) -> None:\n    \"\"\"\n    Train & Evaluate function is responsible for training and evaluating the model\n    in a distributed manner. After finishing training, it will save the model for serving.\n        Args:\n            train_path [Text]: Path where we're going to retrieve our training data.\n            dev_path [Text]: Path where we're going to retrieve our development data.\n            linear_dnn_units [List[int]]: List contains number of nodes inside\n                                          each of linear hidden layer.\n            categorical_dnn_units [List[int]]: List contains number of nodes inside\n                                               each of categorical hidden layer.\n            hidden_layer_units [int]: numbers of unit of our pre-last layer.\n            train_examples [int]: number of examples in our training dataset.\n            dev_examples [int]: number of examples in your development dataset.\n            batch_size [int]: numbers of example per batch.\n            epochs [int]: numbers of times proving the data for training.\n            model_dir [Text]: location for saving our model.\n            checkpoint_path [Text]: location for saving our model's checkpoints.\n            tensorboard_logs_path [Text]: location where we're going to save our model's logs.\n            start_from_latest_checkpoint [Optional[bool]]: it gives you the option that start\n                                                           training from the last point you\n                                                           finished last time.\n    \"\"\"\n    import logging\n    import datetime\n\n    # Wrapping the Callback to trace the nodes values.\n    # It is used in monitoring the weights of the network while examinate training.\n#     class wPrint(tf.keras.callbacks.Callback):\n#         def on_train_begin(self, logs={}):\n#             tf.print('\\nTrain Begin!')\n#             tf.print(logs.keys())\n#             tf.print(model.trainable_variables)\n#             tf.print('=' * 40)\n\n#         def on_epoch_end(self, epoch, logs={}):\n#             tf.print('\\nVariables After Epochs End!')\n#             tf.print(logs.keys())\n#             tf.print(model.trainable_variables)\n\n    # Let's Build our distributed training model\n    strategy = tf.distribute.MirroredStrategy()\n\n    GLOBAL_BATCH_SIZE = batch_size * strategy.num_replicas_in_sync\n    checkpoint_file_prefix = \"model-checkpoints-epochs-{epoch:02d}-rlmse-{val_loss:0.4f}-time-\" +\\\n                             datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    # Load train & Eval dataset and train the model.\n    train_dataset = create_dataset(train_path,\n                                   mode='train',\n                                   batch_size=GLOBAL_BATCH_SIZE,\n                                   num_epochs=None)\n\n    dev_dataset = create_dataset(dev_path,\n                                 mode=None,\n                                 batch_size=GLOBAL_BATCH_SIZE,\n                                 num_epochs=epochs).take(dev_examples \/\/ 1000)\n    with strategy.scope():\n        try:\n            model = model_build(adapt_data=train_dataset,\n                                batch_size=GLOBAL_BATCH_SIZE,\n                                steps=steps,\n                                authors_list=authors_list,\n                                linear_dnn_units=linear_dnn_units,\n                                categorical_dnn_units=categorical_dnn_units,\n                                hidden_layer_unit=hidden_layer_unit)\n\n            if start_from_latest_checkpoint:\n                latest = tf.train.latest_checkpoint('checkpoints')\n                model.load_weights(latest)\n        except ValueError:\n            logging.error(\"You've changed the layers' structure of the model.\\nThis model can't execute the previous checkpoint on this recent model's skeleton.\")\n\n        # Initializing the callbacks\n        checkpoints = tf.keras.callbacks.ModelCheckpoint(\n            filepath=os.path.join(checkpoint_path,\n                                  checkpoint_file_prefix),\n                                  save_best_only=True,\n                                  save_weights_only=True,\n                                  mode= 'min',\n                                  verbose=1)\n        tensorboard = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\n        ES = tf.keras.callbacks.EarlyStopping(patience=2,\n                                              mode='min',\n                                              verbose=1)\n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                         factor=1e-2,\n                                                         mode='min',\n                                                         patience=0,\n                                                         verbose=1)\n        model.fit(train_dataset,\n                  validation_data=dev_dataset,\n                  steps_per_epoch=train_examples \/\/ GLOBAL_BATCH_SIZE,\n                  epochs=epochs,\n#                   validation_steps=1,\n                  callbacks=[checkpoints, tensorboard, ES, reduce_lr])\n\n        # Save the model\n        model_dir = os.path.join(model_dir, f\"model-{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n\n        tf.keras.models.save_model(model,\n                                   model_dir)\n# if __name__ == '__main__':\n#     fire.Fire(train_and_evaluate)","9526c7c3":"# Create Model's Directories\nos.makedirs('Models')\nos.makedirs('checkpoints')\nos.makedirs('Tensorboard')","c933411b":"train_and_evaluate(\n                \"Dataset\/train_data.csv\",\n                \"Dataset\/dev_data.csv\",\n                [64, 32, 16],\n                [64, 32],\n                8,\n                train_data.shape[0],\n                dev_dataframe.shape[0],\n                64,\n                9,\n                873,\n                authors_list,\n                \"Models\",\n                \"checkpoints\",\n                \"Tensorboard\",\n                False)","01156e47":"def streaming(model: tf.keras.models.Model,\n              row_data: Dict[Text, Text]) -> Dict[Text, float]:\n\n    \"\"\"\n    The streaming function is responsible for predicting streaming values \u2013\n    you're sending the features of the book, and the streaming function predicts the price.\n\n    Args:\n        model [tf.keras.models.Model]: model we're using for predictions.\n        row_data [Dict[Text, Text]]: we treats the variable here as a Dict or JSON, so you can\n                                     insert both for prediction.\n     Returns:\n         [Dict[Text, float]]: this will be the predictions in form of JSON (since this is \"\" not '')\n                              but also can treated like a Dict (if you checked the type).\n    \"\"\"\n    import warnings\n    warnings.filterwarnings('ignore')\n\n    pred_dict = {\n        feature: tf.convert_to_tensor([row_data[feature]])\n        for feature in Features.FEATURES\n    }\n    return {\"price\":\n            np.round(float(model.predict(pred_dict)), decimals=4)}\n","c809b870":"def batching(model: tf.keras.models.Model,\n             dataset_dir: Text,\n             save_dir: Text,\n             batch_size: int) -> Dict[Text, float]:\n    \"\"\"\n    The batching function is responsible for predicting batching data \u2013\n    you're sending the features of the books, and the batching function predicts the prices.\n\n    Args:\n        model [tf.keras.models.Model]: model we're using for predictions.\n        dataset_dir [Text]: directory of the features' file.\n        save_dir [Text]: Place where we have to save our prediction in.\n        batch_size [int]: numbers of example per batch.\n     Returns:\n         [Dict[Text, float]]: this will be the predictions in form of JSON (since this is \"\" not '')\n                              but also can treated like a Dict (if you checked the type).\n    \"\"\"\n    import itertools\n    output = []\n    TEST_EXAMPLES_SIZE = pd.read_csv(dataset_dir, index_col=False).shape[0]\n    STEPS_PER_EPOCH = TEST_EXAMPLES_SIZE \/\/ batch_size\n    dataset = tf.data.experimental.make_csv_dataset(dataset_dir,\n                                                    batch_size=batch_size,\n                                                    column_names=Features.INFER_COLUMNS,\n                                                    select_columns=Features.FEATURES,\n                                                    shuffle=False)\n    \n    for index, (batch, _) in enumerate(zip(dataset, range(STEPS_PER_EPOCH))):\n        output.append(model.predict(batch).tolist())\n#         tf.print(f'Batch No. {index} is Done!')\n    # Directory to save the data in\n    try:\n        pd.DataFrame({\"price\": [value[0] for value in itertools.chain(*output)]}).\\\n        to_csv(os.path.join(save_dir, 'test_batch_predictions.csv'), index=False)\n    except Exception as e:\n        return tf.print('Something wrong happened!: ', e)\n    else:\n        return tf.print('Your Predictions has been saved, Sucessfully!')","29ee20a5":"def serving_fn(prediction_type: Optional[Union['stream',\n                                               'batch']],\n               model_dir: Optional[Union[Text, bool]],\n               data: Optional[Union[Dict[Text, Text], Text]],\n               save_dir: Text,\n               batch_size: Optional[Union[int, None]] = 0,\n               ) -> Dict[Text,\n                                                                  Optional[\n                                                                           Union[float,\n                                                                                 List[float]]]]:\n    \"\"\"\n    Responsible for providing a unified serving predictions.\n    Args:\n        model_dir [Optional[Union[Text, book]]]: whether you want to insert the model path,\n                                                 or you want to retrieve the latest updated model\n                                                 for prediction.\n        data [Optional[Union[Dict[Text, Text], Text]]]: whether it is a row of data | batched data.\n        save_dir [Text]: Place where we have to save our prediction in.\n        batch_size [int]: numbers of example per batch.\n     Returns:\n         [Dict[Text,\n               Optional[\n                   Union[float,\n                         List[float]]]]]: whether it returns dictionary of price for single value \u2013\n                                          or returns a dictionary of list of prices for books\n    \"\"\"\n    output = None\n    strategy = tf.distribute.MirroredStrategy()\n    GLOBAL_BATCH_SIZE = batch_size * strategy.num_replicas_in_sync\n    # try:\n    if model_dir == True: # It means that you want to predict using latest trained model.\n        path = os.path.join('Models', os.listdir('Models')[-1])\n    else:\n        path = model_dir\n    # We want to use Synchronous Distributed training for batch predictions since it might\n    # take too long if your dataset is large.\n    options = tf.data.Options()\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n    with strategy.scope():\n        model = tf.keras.models.load_model(path,\n                                           custom_objects={\n                                                \"format_reviews\": format_reviews,\n                                                \"scale_reviews\": scale_reviews,\n                                                \"format_ratings\": format_ratings\n                                            }, compile=False)  # We don't need to retrain the model,\n                                                   # we can do that by invoke the checkpoints\n                                                   # using train & evaluate.\n        if prediction_type == \"stream\":\n            if type(data) is dict:\n                output = streaming(model,\n                                   row_data=data)\n            else:\n                tf.print('You can sent single row of data to stream ONLY!')\n        elif prediction_type == 'batch':\n            if batch_size != 0:\n                output = batching(model=model,\n                                  dataset_dir=data,\n                                  save_dir=save_dir,\n                                  batch_size=GLOBAL_BATCH_SIZE)\n            else:\n                tf.print(\"If you want batch prediction, you MUST insert `batch_size`.\")\n        else:\n            tf.print('Incorrect inference type!')\n    return output","005a0da9":"# Cleared the outupt of this cell due to the AutoShard problem in Kaggle Envoironment!\nserving_fn('batch', True, 'Dataset\/test_data.csv',\"Dataset\", 60)","c40dc086":"pd.read_csv('Dataset\/test_batch_predictions.csv', index_col=False).head()","737f9314":"#### Run Model","760af51e":"## Table of Contents","5327dea8":"<p style='font-size: 18px;font-weight:bold'>For Data Analysis & Visualization<\/p>","b8cb63bf":"Serializes a Layer object into a JSON-compatible representation.\n\n<span style='color:blue;'>\n<a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/serialize\" ><b>tf.keras.layers.serialize<\/b><\/a>\n<\/span>","1ef58c4e":"## An Overview","441eaf23":"### (D) Data Preparation ","c1a5ebcc":"Keras keeps a master list of all built-in layer, model, optimizer, and metric classes, which is used to find the correct class to call from_config. If the class can't be found, then an error is raised (Value Error: Unknown layer).","c93df1b2":"If you notice \u2013 in the previous two cells, we started by executing the functions, eagerly. We calculate the actual values; for testing the return values, Then \u2013 we had to wrap both functions using `tf.function` to convert from the  [eager execution](https:\/\/towardsdatascience.com\/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6#:~:text=TVBEATS%20on%20Unsplash-,eager%20execution,-Eager%20execution%20is) environment to the [graph execution](https:\/\/towardsdatascience.com\/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6#:~:text=the%20Graph%20Execution.-,graph%20execution,-We%20covered%20how) environment to have the speed and the scalability of graph execution besides the parallelism.","100e4b16":"So, what exactly is;\n```python\n@ tf.keras.utils.register_keras_serializable\n```\n>This decorator injects the decorated class or function into the Keras custom object dictionary, so that it can be serialized and deserialized without needing an entry in the user-provided custom object dict. It also injects a function that Keras will call to get the object's serializable string key.\n\n\n<span style='color:green;'>\n\nRead more about [**Keras Serialization & Deserialization**](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/register_keras_serializable)\n<\/p>","084fbc27":"### (A) Data Ingestion","4e13083d":"Experiment the Model","700e1857":"<p style='font-size: 18px;font-weight:bold'>For Shell  Arguments' execution & Commanding<\/p>","f087eb47":"### (F) Evaluate Model","4c32b493":"Since graph execution doesn't save any python code, we can't insert any eager function with the serving model. It leads to incompatibility issues, which means that we will not be able to serve our model without adding our transformed features' functions and the input functions.\n\nSince graph execution doesn't save any python code, we can't insert any eager function with the serving model. It leads to incompatibility issues, which means that we will not be able to serve our model without adding our transformed features' functions and the input functions.\n\n\n<span style='color:CornflowerBlue;'>\n\n>Custom-defined functions (e.g. activation loss or initialization) do not need a get_config method. The function name is sufficient for loading as long as it is registered as a custom object.\n  \n<\/span>\n<span style='background-color:yellow'>\n    Here, you can see the lack of not building an End-to-End without using <b>TFX<\/b>.\n<\/span>\n<span style='color:MediumTurquoise;'>\n\n> **[Important](https:\/\/www.tensorflow.org\/guide\/saved_model#specifying_signatures_during_export\n)**: *Unless you need to export your model to an environment other than TensorFlow 2.x with Python, you probably don't need to export signatures explicitly. If you're looking for a way of enforcing an input signature for a specific function, see the <u>[input_signature](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/function#input_signatures)<\/u> argument to [tf.function](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/function)*.\n\n<\/span>\n\n<span style='color: green;'>\n    Read More about: <a href=\"https:\/\/www.tensorflow.org\/tfx\/guide\/keras#keras_module_file_with_transform\">\n        Keras Module file with Transform<\/a>\n<\/span>\n","acc1a5f3":"### (G) Serving Model","70da906d":"We're going to read the files so we can do some data exploration & Data Analysis","2d7a49eb":"<span style='color:green;'>You can read more about how the `tf.function` is efficient when there's a high-level of operations overheating, so you can use it as an alternative<\/span>: [tf.function](https:\/\/www.tensorflow.org\/guide\/function)","18f0a96d":"The serving model is the function responsible for serving the data to our model in the right manner without applying preprocessing for the real values (that's why we relied on creating our transform preprocessing pipeline inside the model's graph).","22a807f7":"### (A) Install Dependencies","47ced047":"I used to use **Tensorboard** for monitoring the model performance while training & validating the model. Therefore, I will be able to trace and track the model's linkage, function's losses, and the graphical environment of my Tensorflow.","ac2c7a6e":"---","18d0a86b":"Make sure, you've lowercase the names of the columns for the features name of the execution graph (You will understand in the upcoming phases ","cf0e28f3":"We're going to do some analysis to explore the patterns, which features we're going to use, and also the differences between the Train & Test datasets so we can check if there's any kinds of anomalies, drifts, or skews using Tensorflow Data Validation","84810d72":"<p style='font-size: 18px;font-weight:bold'>Transform<\/p>\n\n\n- One of the best methodologies for building a fully managed DAG using [TFX](https:\/\/www.tensorflow.org\/tfx).\n\n\n- This [DAG](https:\/\/en.wikipedia.org\/wiki\/Directed_acyclic_graph)  is built based on Feature Engineering:\n\n    1. Input Layers:\n\n        A. **Ratings**: it takes the rating of the customers about a book \u2013 the `scrape_Ratings` responsible for scraping the float value out of the binary tensors then scales the values using z-score in a Data Distributed manner and memory contributed way using [TF.Transform](https:\/\/www.tensorflow.org\/tfx\/tutorials\/transform\/census).\n\n        B. **Reviews**: it similars to `Ratings` but the difference here is, using min-max normalization since the reviews have discrete values also by using TF.Transform. (TF. Transform is way better than Sklearn for preprocessing. Sklearn eats the memory, crucially.\n\n        C. **BookCategory**: is a vocabulary list Built based on the [TensorFlow vocabulary file](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/categorical_column_with_vocabulary_file).\n\n        D. **Genre**: Similar to BookCategory.\n\n    2. **DenseFeatures**: Responsible for creating `feature_column` layer out of all these layers (i.e embedding feature columns which are used after crossing all these features together.\n\n\n- **How did I choose these layers out of all the other features?**\n\n    * I can't forget to mention my favorite API of all times: [**TensorFlow Data Validation (TFDV)**.](https:\/\/www.tensorflow.org\/tfx\/data_validation\/get_started) This API is one of the open-source APIs based on [**Facets**](https:\/\/pair-code.github.io\/facets\/). Using it helps me to discover the anomalies, Skew, Distribution Skew, Drifts, Data Interpolation & Extrapolation, etc. You will definitely want to use it when you want to visualize and analyze your data.","1c3e6430":"Now, let's discover and dive into the dataset to see what features we can use for this regression problem to help up find the best accuracy for predicting the prices of these books","9189c3c0":"<p style='font-size: 18px;font-weight:bold'>For Reusable Embedding & Transfor Learning<\/p>","56b3ab50":"### (C) Hardware Dependencies","f9cf179b":"tf.distribute.ReplicaContext(\n    strategy, replica_id_in_sync_group\n)\n**For the next couple of cells, we'r going to build two transfomed functions that would help us in our pre-processing with `Reviews` & `Ratings` Features.**","8976eaa1":"### (D) Data Dependences","359f9b67":"**Make sure you serialized the Custom functions & Subclass before loading the model**","43684a30":"<span style='color:red'>A wondering question, why we don't use **MSE** as our loss function?<\/span>\n\n-  <span style='color:green'>Answer; **MSE** is not known as a loss function used in Regression models. It is efficient if you use **RLMSE** as we created in our `build_model` function.\n.<\/span> ","7dbe307b":"We're going to create a function that apply a unified predictions (Batch & Streaming):-","94644594":"I still see that the 4 features we selected not enough to help with the price predictions. \n\nTest it! You will notice that it gives average predictions. There're multiple ways to use the benefits from the other features we neglected, but how!?\n\nFor example, we can use Transfer learning or reusable embedding, reusable embedding!?? Yes! We can use reusable embedding to embed one of our features and convert it from a useless feature to something we can rely on with high accuracy.\n\nWelcome to **TensorFlow Hub**!\n> The TensorFlow Hub lets you search and discover hundreds of trained, ready-to-deploy machine learning models in one place.\n\nWe're going to use [`nnlm-en-dim50`](https:\/\/tfhub.dev\/google\/nnlm-en-dim50\/2):\n> Token based text embedding trained on English Google News 7B corpus.\n\n<span style='color:green;'> You can read more about the [Neural Probalistic Language Model](https:\/\/www.linkedin.com\/posts\/drxavier997_neural-probabilistic-language-model-activity-6861266851578679296-LofX)<\/span> ","0a74d0e8":"<p style='text-align:center;'>Thanks for reaching this level of expermenting\nthe idea of <b>Transform<\/b><\/p>\n<p style='text-align:center;'>Data Scientist & ML Engineer: <a href='https:\/\/www.linkedin.com\/in\/drxavier997\/'>Ahmed<\/a><\/p>\n<p style='text-align:center;'>Created at: 2021-01-06","01a107ef":"### (C) Data Analysis","2e0676b7":"---","ebffe07a":"### (B) Importing Libraries","98a0ea3c":"<table>\n<thead>\n  <tr>\n      <th><a href='#Table-of-Contents'>Table of Contents<\/a><\/th>\n    <th><\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n      <td><a href='#An-Overview'>An Overview<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><a href='#Dependencies'>Dependencies<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(A)-Install-Dependencies'>(A) Install Dependencies<\/a><\/td>\n  <\/tr>\n      <tr>\n    <td><\/td>\n          <td><a href='#(B)-Importing-Libraries'>(B) Importing Libraries<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(C)-Hardware-Dependencies'>(C) Hardware Dependencies<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(D)-Data-Dependences'>(D) Data Dependences<\/a><\/td>\n  <\/tr>\n  <tr>\n      <td><a href='#Workflow-pipeline'>Workflow pipeline<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(A)-Data-Ingestion'>(A) Data Ingestion<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(B)-Data-Exploration'>(B) Data Exploration<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(C)-Data-Analysis'>(C) Data Analysis<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(D)-Data-Preparation'>(D) Data Preparation<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(E)-Train-Model-&-Validate-Model'>(E) Train Model & Validate Model<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(F)-Evaluate-Model'>(F) Evaluate Model<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(G)-Serving-Model'>(G) Serving Model<\/a><\/td>\n  <\/tr>\n<\/tbody>\n<\/table>","382f7c7b":"<p style='font-size: 18px;font-weight:bold'>Training Performance<\/p>\n\n- We're going to compare two different training performance:\n\n    1. **First Failed Model**\n\n        <img src=\"https:\/\/i.imgur.com\/DzbU9D0.png\" style='width:100%'>This image shows the loss function curve across the number of epochs using `rlmse` as my loss function evaluator and how hard the model is barely training.<\/img>\n\n        <img src=\"https:\/\/i.imgur.com\/csMHX7F.png\" style='width:100%'>This image shows the model performance in the training phase along with the number of iterations for the validation data. <\/img>\n\n    2. **Second Model**\n\n        <img src=\"https:\/\/i.imgur.com\/9V4YFhF.png\" style='width:100%'>This image shows the loss function curve across the number of epochs using `rlmse` as my loss function evaluator<\/img>\n\n        <img src=\"https:\/\/i.imgur.com\/m5r8BYg.png\" style='width:100%'>This image shows the model performance in the training phase along with the number of iterations for the validation data.<\/img>\n\n<span style=\"color:green;\">Examinate the monitored training performance in this follow link:<\/span>[Tensorboard.dev](https:\/\/tensorboard.dev\/experiment\/QAIWXZmOQvWtxRnzDA3eGA\/#scalars)","1f192c1b":"<p style='font-size: 18px;font-weight:bold'>For Preprocessing & Transformation<\/p>","2856c74a":"<span style='color:DodgerBlue;'>\n\n> You can use `tf.function` to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use SavedModel.\n<\/span>","fb43a6fe":"**The Batch prediction**","3b034ffb":"### (E) Train Model & Validate Model","137f2a0e":"<p style='font-size: 18px;font-weight:bold'> For Developing and ML Models<\/p>","4862bb89":"Let's visualize the datasets and see the differences","9020be57":"## Workflow pipeline","ce03e32b":"<img src='https:\/\/cloud.google.com\/architecture\/images\/data-preprocessing-for-ml-with-tf-transform-tf-transform-behavior.svg' style='width:100%'><\/img>\n\nThis diagram shows \u2013 how `tf.Transform`  applies the behavior of preprocessing and transforming in **Training (Fitting)** and also **prediction (Serving, or Inferencing)**.\n\n<span style='color:green;'>To Read More about `tf.Transform` preprocessing and transforming, Read This article by [**Google Cloud**](https:\/\/cloud.google.com\/architecture\/data-preprocessing-for-ml-with-tf-transform-pt2)<\/span>","8be78247":"---","dd1b252e":"First, we need to scrape the average of stars that clients have submitted to this book. Also, we know that the max stars are 5 and min number of stars is 1. (Realistically, a person may not submit a review. Therefore, the review value is zero. At the same time, it won't be logical to pass a zero start to books that not invoked in our scope)","4a285dcd":"<p style='font-size: 18px;font-weight:bold'>Train & Test Split<\/p>","addec967":"We're preparing our data for our mode: **Data Preparation** is one of the hardest, brainstorming phases you may struggle with within the ML Pipeline.\n\nIt requires the talent of understanding the data from domain knowledge. It shows how good you understand the data you held.","426ca4ea":"Copyright [2021] [Data Scientist & ML Engineer: [Ahmed](https:\/\/machinehack.com\/user\/profile\/ui\/61c4874bf292faa49acf07a8)]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.","f82e6f63":"<p style='font-size: 18px;font-weight:bold'>From Structure Prospective<\/p>\n\n- We're going to compare two Skeleton which has built for this project:\n\n<img src=\"https:\/\/i.imgur.com\/KlZ0vkR.png\" style='width:100%'>This image shows the problem of connecting float datapoints without scaling them, besides the lack of layers shown in the image<\/img>\n\n<img src=\"https:\/\/i.imgur.com\/pd8izTP.png\" style='width:100%'>On the other hand, we can see that only the scaled values are connected with the `feature_column` layer which is `DenseFeature`. Also, we can see \u2013 we managed to split the categorical embedded vocabularies in a different `DenseFeature` layer; this helps to have deep & managed data distribution across the whole graph.<\/img>\n","ae07f930":"### (B) Data Exploration","7c6044ba":"<p style='font-size: 18px;font-weight:bold'>From this statistics we saw that;<\/p>\n\n- The distribution skew of the `training dataset` is totally difference from the `test dataset` in these features:\n\n    + `Edition`\n    + `Author`\n    + `Synopsis`\n    + `Title`\n\n    which means \u2013 we can't rely on them to predict the price.\n\n- Also, we notice the data distribution in some other training dataset features are intrapolating with some other testing dataset features:\n    \n    + `Reviews`\n    + `Ratings`\n    + `Genre`\n    + `BookCategory`\n\n    which means \u2013 we can use them for predicting the price.","5c302257":"[<p style='font-size: 18px;font-weight:bold'>Custom functions & Custom subclasses<\/p>](https:\/\/www.tensorflow.org\/guide\/keras\/save_and_serialize#:~:text=aware%20of%20it.-,custom%20functions,-Custom-defined%20functions)\n","4575c69c":"<p style='font-size: 18px;font-weight:bold'>Let's do some Feature Engineering<\/p>\n","93c81d2b":"One for Batch Testing, later.","2bde69cc":"# Predict The price of Books\n---","0defdd0c":"**Run the below cells. Restart the kernel (Kernel > Restart kernel > Restart). Re-run the below cell and proceed further.**","4fb4f522":"<span style='color:green;'>You can download the data using this link: <\/span><a href=\"https:\/\/machinehack.com\/hackathon\/predict_the_price_of_books\/data\">Predict The Price Of Books<\/a>","1baf7b9b":"<center>\n________________________________\n<\/center>","ea469ed2":"## Dependencies","c425f08c":"In training and validating the model \u2013 we're responsible for building the model function \u2013 which is responsible for structuring the model's skeleton, compiling the loss function, and the learning rate.\n\n- `model_build`: \n    + It is building the skeleton of the model.\n\n\n- `train_and_evaluate`:\n    + Train & Evaluate function is responsible for training and evaluating the model in a distributed manner. After finishing training, it will save the serving model.\n","fc54c8f7":"First, we need to save all the titles of `Genre` & `BookCategory`","6e3d5116":"Now, let's save our 3 dataframes in CSV format, so we can prepare them ","96c1b4ef":"Similarly, we need to scrape the number of ratings' people who submitted a review \u2013 then we want to scale them down.\n\nWhat interest here is, it will be less efficient to use min-max scaling; why!?\n\nThe number of customers is submitting ratings may be infinite numbers not controlled by any barriers. Here, if we tried to invoke each tensor of ratings to the scale layer, we would end up with `NaN` values since it requires the mean and standard deviation of all the tensors to calculate the standard scaling (i.e. data normal distributed within each feature)[$^1$](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35#:~:text=data%20is%20normally%20distributed%20within%20each%20feature).\n\nIt might be crazy, but I had to come up with an idea of how to scale all the tensors inside the dataset. But, I still have to add this into the model so, I can apply the **Transform** methodology.\n\n<span style='background-color:yellow'>\nAccount this as the first limitation of using Transform out of using it inside the TFX framework.\n<\/span>","53c9620f":"Lastly, you may find the result quite an average due to the lack of data. Obviously, you're not going to rely on the `reviews` and `ratings` alone in this project, or the author's name. These are multiple more features that may help if it was there like; readers' opinions, bins of the `reviews` and `ratings` and so on.\n\nThe idea of this project is to show to you the power of using **Transform** and at the same time how it can be really powerful if we applied that using the production framework \u2013 **TFX**.\n\nI hope this project was interesting for you! Please, if you find any comment regards to the project, contact me!"}}