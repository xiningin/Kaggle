{"cell_type":{"c3372b87":"code","fb5f2b27":"code","b9d52ae3":"code","6dc8009e":"code","5e27b6a1":"code","a640ba6d":"code","3544f84a":"code","4ffd032c":"code","9063c955":"code","ac0f7aa3":"code","5ee0c0d5":"markdown","28c1701c":"markdown","983ec8e4":"markdown","52f048d8":"markdown","ba72324b":"markdown"},"source":{"c3372b87":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom datetime import datetime","fb5f2b27":"interactions = pd.read_csv(\"\/kaggle\/input\/olx-jobs-interactions\/interactions.csv\")","b9d52ae3":"interactions.head()","6dc8009e":"interactions.info()","5e27b6a1":"n_users = interactions[\"user\"].nunique()\nn_items = interactions[\"item\"].nunique()\nn_interactions = interactions.shape[0]\n\ninteractions_per_user = interactions.groupby(\"user\").size()\ninteractions_per_item = interactions.groupby(\"item\").size()\n\nprint(f\"We have {n_users} users, {n_items} items and {n_interactions} interactions.\")\nprint(f\"Data sparsity (% of missing entries) is {100 * (1- n_interactions \/ (n_users * n_items)):.2f}%.\")\nprint(f\"Average number of interactions per user is {interactions_per_user.mean():.2f} (standard deviation {interactions_per_user.std(ddof=0):.2f}).\")\nprint(f\"Average number of interactions per item is {interactions_per_item.mean():.2f} (standard deviation {interactions_per_item.std(ddof=0):.2f}).\")","a640ba6d":"def compute_quantiles(series, quantiles=[0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99]):\n    return pd.DataFrame(\n        [[quantile, series.quantile(quantile)] for quantile in quantiles],\n        columns=[\"quantile\", \"value\"],\n    )\n\n\ndef plot_interactions_distribution(series, aggregation=\"user\", ylabel=\"Users\", bins=30):\n    matplotlib.rcParams.update({\"font.size\": 22})\n    series.plot.hist(bins=bins, rwidth=0.9, logy=True, figsize=(16, 9))\n    plt.title(f\"Number of interactions per {aggregation}\")\n    plt.xlabel(\"Interactions\")\n    plt.ylabel(ylabel)\n    plt.grid(axis=\"y\", alpha=0.5)","3544f84a":"print(\"Interactions distribution per user:\")\ndisplay(compute_quantiles(interactions_per_user))\nplot_interactions_distribution(interactions_per_user, \"user\", \"Users\")","4ffd032c":"print(\"Interactions distribution per item:\")\ndisplay(compute_quantiles(interactions_per_item))\nplot_interactions_distribution(interactions_per_item, \"item\", \"Items\")","9063c955":"event_frequency = pd.DataFrame(\n    interactions[\"event\"].value_counts() \/ len(interactions)\n).rename(columns={\"event\": \"frequency\"})\n\nevent_frequency[\"frequency\"] = event_frequency[\"frequency\"].apply(\n    lambda x: f\"{100*x:.2f}%\"\n)\nevent_frequency","ac0f7aa3":"def unix_to_day(timestamps):\n    min_timestamp = timestamps.min()\n    seconds_in_day = 60*60*24\n    return (timestamps - min_timestamp) \/\/ seconds_in_day + 1\n\ndef plot_interactions_over_time(series):\n    freq = series.value_counts()\n    labels, counts = freq.index, freq.values\/10**6\n    \n    matplotlib.rcParams.update({\"font.size\": 22})\n    plt.figure(figsize=(16,5))\n    plt.bar(labels, counts, align='center')\n    plt.gca().set_xticks(labels)\n    plt.title(f\"Interactions by days\")\n    plt.xlabel(\"Day\")\n    plt.ylabel(\"Interactions [mln]\")\n    plt.grid(axis=\"y\")\n\nplot_interactions_over_time(unix_to_day(interactions[\"timestamp\"]))","5ee0c0d5":" # Interactions distribution per user","28c1701c":"# Interactions distribution per item","983ec8e4":"# Interactions over time","52f048d8":"# Basics statistics of the dataset","ba72324b":"#  Events distribution"}}