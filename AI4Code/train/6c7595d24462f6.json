{"cell_type":{"76fbaf72":"code","39aa6076":"code","880c0c57":"code","f139408e":"code","a708a62d":"code","629fe36e":"code","52496aa6":"code","75a43f1b":"code","6426e7c4":"code","5d828784":"code","2f27b808":"code","52050db5":"code","5d04d60c":"code","e60424c9":"code","d294ae73":"code","0942c780":"code","164ccded":"code","e2f87291":"code","635c1356":"code","c236b56f":"code","176ceab7":"code","20954b11":"code","3eb108dd":"code","6164d10d":"code","7cabc07f":"code","3581bb64":"code","a74c6f16":"code","960d2667":"code","26d4545f":"code","fd772e27":"code","2a4dabde":"code","2c4b08bd":"code","4078afaa":"code","7b85396a":"code","3824d491":"code","e644d726":"code","b09f1ddf":"code","4521682d":"code","67d4cb79":"code","225a6d3a":"code","c187c2a4":"code","a835144b":"code","6f6b0c7a":"code","8651befa":"code","b368dc3d":"code","529a211f":"code","ace40458":"code","d177544d":"code","c3cc2fff":"code","5072e344":"code","e48d3f7f":"code","70e31083":"code","7bb9c0a9":"code","2e68669a":"code","42cc0f21":"code","7581a817":"code","bf78f362":"code","489a4e65":"code","8c5f62a4":"code","b2dfa9af":"code","0c9b34c0":"code","98b9c695":"code","d888ff46":"code","82a5367b":"code","9835afef":"code","24795f9c":"code","c53d1910":"code","f0c2564d":"code","fd4295cd":"code","2775f887":"code","5302e62e":"code","ff1fed37":"code","fc0aca9f":"code","c36bba71":"code","11e4966d":"code","836ab107":"code","3f9c492a":"code","7c8a1ca3":"code","251e8c1b":"code","f7bdff54":"code","e78c0bde":"code","654049c7":"code","9b8e7a96":"code","ece3421a":"code","faca201c":"code","95c4cc5f":"code","e3c0a4c3":"code","0b0cb2b9":"code","8859e382":"code","4ac03d0b":"code","7e6d3e5b":"code","0cb721f6":"code","68023cf7":"code","836b58b8":"code","cc8ce666":"code","2272b0b0":"code","5056ddb7":"code","a5eb6408":"code","8842bfee":"code","d7347ecd":"code","2d86469e":"code","c38739f9":"code","486a6243":"code","000cc84b":"code","6039f763":"code","b9e942bf":"code","c3d590c9":"code","6d9a54dc":"code","57c41a33":"code","f8dbc2a1":"code","240a068f":"code","a486429c":"code","8638fe21":"code","dcaa3728":"code","8e072028":"code","d35ab923":"code","0df503d3":"code","63bf104b":"code","b19fb8d6":"code","2aadb796":"code","45050aab":"code","63746135":"code","f0f8b79b":"code","9e8a65d5":"code","180ccbfc":"code","99a510e2":"code","c7521665":"markdown","d59b9cef":"markdown","201dc7f7":"markdown","2c5d0836":"markdown","48dc4ba7":"markdown"},"source":{"76fbaf72":"# Laoding Basic data analysis libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","39aa6076":"# Basic programming and data visualization libraries\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","880c0c57":"def hist_perc(df_col,bin_size,rng_st,rng_end):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.ticker as mticker\n    #plt.hist(x = train_df.RevolvingUtilizationOfUnsecuredLines,bins=10,range=(,1))\n    plt.hist(x = df_col,bins=bin_size,range=(rng_st,rng_end))\n    formatter = mticker.FuncFormatter(lambda v, pos: str(round((v*100\/train_df.shape[0]),2)))\n    plt.gca().yaxis.set_major_formatter(formatter)\n    plt.show()\n\n","f139408e":"def plotHist(df,nameOfFeature):\n    cls_train = df[nameOfFeature]\n    data_array = cls_train\n    hist_data = np.histogram(data_array)\n    binsize = .5\n\n    trace1 = go.Histogram(\n        x=data_array,\n        histnorm=\"\",\n        name='Histogram of Wind Speed',\n        autobinx=False,\n        xbins=dict(\n            start=df[nameOfFeature].min()-1,\n            end=df[nameOfFeature].max()+1,\n            size=binsize\n        )\n    )\n\n    trace_data = [trace1]\n    layout = go.Layout(\n        bargroupgap=0.3,\n         title='The distribution of ' + nameOfFeature,\n        xaxis=dict(\n            title=nameOfFeature,\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=18,\n                color='#7f7f7f'\n            )\n        ),\n        yaxis=dict(\n            title='Number of labels',\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=19,\n                color='#7f7f7f'\n            )\n        )\n    )\n    fig = go.Figure(data=trace_data, layout=layout)\n    py.iplot(fig)","a708a62d":"from scipy.stats import skew\nfrom scipy.stats import kurtosis\ndef plotBarCat(df,feature,target):\n    x0 = df[df[target]==0][feature]\n    x1 = df[df[target]==1][feature]\n\n    trace1 = go.Histogram(\n        x=x0,\n        opacity=0.75\n    )\n    trace2 = go.Histogram(\n        x=x1,\n        opacity=0.75\n    )\n\n    data = [trace1, trace2]\n    layout = go.Layout(barmode='overlay',\n                      title=feature,\n                       yaxis=dict(title='Count'\n        ))\n    fig = go.Figure(data=data, layout=layout)\n\n    py.iplot(fig, filename='overlaid histogram')\n    \n    def DescribeFloatSkewKurt(df,target):\n        \"\"\"\n            A fundamental task in many statistical analyses is to characterize\n            the location and variability of a data set. A further\n            characterization of the data includes skewness and kurtosis.\n            Skewness is a measure of symmetry, or more precisely, the lack\n            of symmetry. A distribution, or data set, is symmetric if it\n            looks the same to the left and right of the center point.\n            Kurtosis is a measure of whether the data are heavy-tailed\n            or light-tailed relative to a normal distribution. That is,\n            data sets with high kurtosis tend to have heavy tails, or\n            outliers. Data sets with low kurtosis tend to have light\n            tails, or lack of outliers. A uniform distribution would\n            be the extreme case\n        \"\"\"\n        print('-*-'*25)\n        print(\"{0} mean : \".format(target), np.mean(df[feature]))\n        print(\"{0} var  : \".format(target), np.var(df[feature]))\n        print(\"{0} skew : \".format(target), skew(df[feature]))\n        print(\"{0} kurt : \".format(target), kurtosis(df[feature]))\n        print('-*-'*25)\n    \n    DescribeFloatSkewKurt(df,feature)","629fe36e":"def OutLiersBox(df,nameOfFeature):\n    \n    trace0 = go.Box(\n        y = df[nameOfFeature],\n        name = \"All Points\",\n        jitter = 0.3,\n        pointpos = -1.8,\n        boxpoints = 'all',\n        marker = dict(\n            color = 'rgb(7,40,89)'),\n        line = dict(\n            color = 'rgb(7,40,89)')\n    )\n\n    trace1 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Only Whiskers\",\n        boxpoints = False,\n        marker = dict(\n            color = 'rgb(9,56,125)'),\n        line = dict(\n            color = 'rgb(9,56,125)')\n    )\n\n    trace2 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Suspected Outliers\",\n        boxpoints = 'suspectedoutliers',\n        marker = dict(\n            color = 'rgb(8,81,156)',\n            outliercolor = 'rgba(219, 64, 82, 0.6)',\n            line = dict(\n                outliercolor = 'rgba(219, 64, 82, 0.6)',\n                outlierwidth = 2)),\n        line = dict(\n            color = 'rgb(8,81,156)')\n    )\n\n    trace3 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Whiskers and Outliers\",\n        boxpoints = 'outliers',\n        marker = dict(\n            color = 'rgb(107,174,214)'),\n        line = dict(\n            color = 'rgb(107,174,214)')\n    )\n\n    data = [trace0,trace1,trace2,trace3]\n\n    layout = go.Layout(\n        title = \"{} Outliers\".format(nameOfFeature)\n    )\n\n    fig = go.Figure(data=data,layout=layout)\n    py.iplot(fig, filename = \"Outliers\")\n","52496aa6":"from scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\ndef OutLierDetection(df,feature1,feature2,outliers_fraction=.1):\n    \n    new_df = df.copy()\n    rng = np.random.RandomState(42)\n\n    # Example settings\n    n_samples = new_df.shape[0]\n#     outliers_fraction = 0.2 # ************************************** imp\n    clusters_separation = [0]#, 1, 2]\n\n    # define two outlier detection tools to be compared\n    classifiers = {\n        \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n                                         kernel=\"rbf\", gamma=0.1),\n        \"Robust covariance\": EllipticEnvelope(contamination=outliers_fraction),\n        \"Isolation Forest\": IsolationForest(max_samples=n_samples,\n                                            contamination=outliers_fraction,\n                                            random_state=rng),\n        \"Local Outlier Factor\": LocalOutlierFactor(\n            n_neighbors=35,\n            contamination=outliers_fraction)}\n\n    \n    xx, yy = np.meshgrid(np.linspace(new_df[feature1].min()-new_df[feature1].min()*10\/100, \n                                     new_df[feature1].max()+new_df[feature1].max()*10\/100, 50),\n                         np.linspace(new_df[feature2].min()-new_df[feature2].min()*10\/100,\n                                     new_df[feature2].max()+new_df[feature2].max()*10\/100, 50))\n\n\n    n_inliers = int((1. - outliers_fraction) * n_samples)\n    n_outliers = int(outliers_fraction * n_samples)\n    ground_truth = np.ones(n_samples, dtype=int)\n    ground_truth[-n_outliers:] = -1\n\n    # Fit the problem with varying cluster separation\n    for i, offset in enumerate(clusters_separation):\n        np.random.seed(42)\n        # Data generation\n\n        X = new_df[[feature1,feature2]].values.tolist()\n\n        # Fit the model\n        plt.figure(figsize=(9, 7))\n        for i, (clf_name, clf) in enumerate(classifiers.items()):\n            # fit the data and tag outliers\n            if clf_name == \"Local Outlier Factor\":\n                y_pred = clf.fit_predict(X)\n                scores_pred = clf.negative_outlier_factor_\n            else:\n                clf.fit(X)\n                scores_pred = clf.decision_function(X)\n                y_pred = clf.predict(X)\n            threshold = stats.scoreatpercentile(scores_pred,\n                                                100 * outliers_fraction)\n            n_errors = (y_pred != ground_truth).sum()\n            \n            unique, counts = np.unique(y_pred,return_counts=True)\n            print(clf_name,dict(zip(unique, counts)))\n            \n            new_df[feature1+'_'+feature2+clf_name] = y_pred\n#             print(clf_name,y_pred) \n            # plot the levels lines and the points\n            if clf_name == \"Local Outlier Factor\":\n                # decision_function is private for LOF\n                Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            subplot = plt.subplot(2, 2, i + 1)\n            subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n                             cmap=plt.cm.Blues_r)\n            a = subplot.contour(xx, yy, Z, levels=[threshold],\n                                linewidths=2, colors='red')\n            subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n                             colors='orange')\n            b = plt.scatter(new_df[feature1], new_df[feature2], c='white',\n                     s=20, edgecolor='k')\n\n            subplot.axis('tight')\n\n            subplot.set_xlabel(\"%s\" % (feature1))\n \n            plt.ylabel(feature2)#, fontsize=18)\n            plt.title(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n\n        plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n#         plt.suptitle(\"Outlier detection\")\n\n    plt.show()\n    return new_df","75a43f1b":"# Declaration of Varibles\nSEED = 7\n","6426e7c4":"# IMporting the dataset into memory\n\nnp.random.seed(SEED)\ntrain_df = pd.read_csv(\"cs-training.csv\")\ntest_df = pd.read_csv(\"cs-test.csv\")\n","5d828784":"train_df = train_df.sample(frac=0.1)","2f27b808":"#Drop the serial number column\ntrain_df.drop(train_df[['Unnamed: 0']],axis=1,inplace=True)\ntest_df.drop(test_df[['Unnamed: 0']],axis=1,inplace=True)","52050db5":"target = train_df['SeriousDlqin2yrs']","5d04d60c":"#train_df.drop(train_df[['SeriousDlqin2yrs']],axis=1,inplace=True)","e60424c9":"test_df.head(20)","d294ae73":"#Identify NA values\n\ntrain_df.isna().sum()\n","0942c780":"#train_df[(train_df['MonthlyIncome'].isna()) & (train_df['SeriousDlqin2yrs']!=0)]\ntrain_df[(train_df['MonthlyIncome'].isna())].SeriousDlqin2yrs.value_counts().plot.bar()\n\n","164ccded":"train_df[(train_df['MonthlyIncome']==0)].SeriousDlqin2yrs.value_counts().plot.bar()","e2f87291":"train_df[(train_df['MonthlyIncome']==0) | (train_df['MonthlyIncome'].isna()) ].SeriousDlqin2yrs.value_counts().plot.bar()","635c1356":"plt.figure(figsize=(10,5))\nplt.hist(x = train_df.MonthlyIncome,range=(0,500),bins=50)","c236b56f":"plt.figure(figsize=(20,10))\ntrain_df.MonthlyIncome.plot.kde()","176ceab7":"train_df.isna().sum()","20954b11":"train_df.drop(train_df[train_df['MonthlyIncome'].isna()==True].index,inplace=True)","3eb108dd":"train_df.drop(train_df[train_df['MonthlyIncome']==0].index,inplace=True)","6164d10d":"train_df.isna().sum()","7cabc07f":"train_df.head()","3581bb64":"train_df.dtypes","a74c6f16":"train_df.NumberOfDependents = train_df.NumberOfDependents.astype('int64')","960d2667":"sns.pairplot(train_df,hue=\"SeriousDlqin2yrs\", palette=\"husl\")","26d4545f":"sns.heatmap(train_df.corr(),cmap = \"Greens\")\n\n\n#sns.heatmap(df, cmap=\"YlGnBu\")\n#sns.heatmap(df, cmap=\"Blues\")\n#sns.heatmap(df, cmap=\"BuPu\")\n#sns.heatmap(df, cmap=\"Greens\")\n","fd772e27":"OutLiersBox(train_df,\"NumberOfOpenCreditLinesAndLoans\")","2a4dabde":"# this implies that the people who defaulted later than 30 days, most of them ran their accounts into bad debt as \n#they were termed as bad loans after 90+ days of default\n\n# EDA\n# 1. Revolving Utillization of unsecured lines\n#plt.hist(x = train_df.RevolvingUtilizationOfUnsecuredLines,bins=10,range=(0,1))\n#train_df[train_df.RevolvingUtilizationOfUnsecuredLines>0.4 & train_df.SeriousDlqin2yrs=1]\nhist_perc(train_df.RevolvingUtilizationOfUnsecuredLines,10,0,1.5)\n\n","2c4b08bd":"# Anamolies analysis in the data\ntrain_df.describe()","4078afaa":"# The consolidated description confirms the presence of outliers in the data.\n# Investigating further to detect outliers and visuallize them wrt different predictors.","7b85396a":"OutLierDetection(train_df,'RevolvingUtilizationOfUnsecuredLines','MonthlyIncome',outliers_fraction=.1)","3824d491":"plotHist(train_df,'MonthlyIncome')","e644d726":"train_df.head()","b09f1ddf":"plotBarCat(df,df_name[i],'Outcome')","4521682d":"# Model Definition data cleaning","67d4cb79":"X_train = train_df.iloc[:,1:]\nY_train = train_df.iloc[:,0:1]\n\nprint(X_train.shape,Y_train.shape)\nScoreCard = pd.DataFrame()","225a6d3a":"ScoreCard.shape","c187c2a4":"from pandas import set_option\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","a835144b":"def GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR'   , LogisticRegression()))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN'  , KNeighborsClassifier()))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('SVM'  , SVC(probability=True)))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n    basedModels.append(('RF'   , RandomForestClassifier()))\n    basedModels.append(('ET'   , ExtraTreesClassifier()))\n\n    \n    return basedModels","6f6b0c7a":"def BasedLine2(X_train, Y_train,models,scoring_type):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = scoring_type\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits=num_folds, random_state=SEED)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results","8651befa":"class PlotBoxR(object):\n    \n    \n    def __Trace(self,nameOfFeature,value): \n    \n        trace = go.Box(\n            y=value,\n            name = nameOfFeature,\n            marker = dict(\n                color = 'rgb(0, 128, 128)',\n            )\n        )\n        return trace\n\n    def PlotResult(self,names,results):\n        \n        data = []\n\n        for i in range(len(names)):\n            data.append(self.__Trace(names[i],results[i]))\n\n\n        py.iplot(data)","b368dc3d":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)","529a211f":"def ScoreDataFrame(names,results,score_name):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, score_name: scores})\n    return scoreDataFrame","ace40458":"basedLineF1Score = ScoreDataFrame(names,results,'baseline_f1_Score')\nbasedLineF1Score","d177544d":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, Y_train,models,'accuracy')\nPlotBoxR().PlotResult(names,results)\n\nbasedLineAccuracyScore = ScoreDataFrame(names,results,'baseline_accuracy')\nbasedLineAccuracyScore","c3cc2fff":"ScoreCard = pd.concat([basedLineAccuracyScore,\n                       basedLineF1Score], axis=1)\nScoreCard","5072e344":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n    \n\n    return pipelines ","e48d3f7f":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandard = ScoreDataFrame(names,results,'standard_f1_score')\n","70e31083":"ScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandard], axis=1)\nScoreCard","7bb9c0a9":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMax = ScoreDataFrame(names,results,'minmax_f1_score')\n","2e68669a":"ScoreCard = pd.concat([ScoreCard,\n                          scaledScoreMinMax], axis=1)\nScoreCard","42cc0f21":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'+'CW'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression(class_weight='balanced'))])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART'+'CW', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier(class_weight='balanced'))])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM'+'CW' , Pipeline([('Scaler', scaler),('SVM' , SVC(class_weight='balanced'))])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'+'CW'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier(class_weight='balanced'))])  ))\n    pipelines.append((nameOfScaler+'ET'+'CW'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier(class_weight='balanced'))])  ))\n    \n\n    return pipelines ","7581a817":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandardCW = ScoreDataFrame(names,results,'standard_f1_score')\n","bf78f362":"ScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandardCW], axis=1)\nScoreCard","489a4e65":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMaxCW = ScoreDataFrame(names,results,'minmax_f1_score')\n","8c5f62a4":"ScoreCard = pd.concat([ScoreCard,\n                          scaledScoreMinMaxCW], axis=1)\nScoreCard","b2dfa9af":"train_df = train_df.reset_index(drop=True)\ndf_t = train_df.copy()\ndf_t_name = df_t.columns\ndf_name = train_df.columns","0c9b34c0":"def TurkyOutliers(df_out,nameOfFeature,drop=False):\n\n    #feature_number = 1\n    #df_out = df_t\n    #nameOfFeature = df_name[feature_number]\n    #drop = True\n    \n    valueOfFeature = df_out[nameOfFeature]\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(valueOfFeature, 25.)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(valueOfFeature, 75.)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3-Q1)*1.5\n    # print \"Outlier step:\", step\n    outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n    # df[~((df[nameOfFeature] >= Q1 - step) & (df[nameOfFeature] <= Q3 + step))]\n\n\n    # Remove the outliers, if any were specified\n    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n    if drop:\n        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n        print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n        return good_data\n    else: \n        print (\"Nothing happens, df.shape = \",df_out.shape)\n        return df_out","98b9c695":"feature_number  = 1\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","d888ff46":"df_clean = TurkyOutliers(df_t,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","82a5367b":"feature_number  = 2\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","9835afef":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","24795f9c":"feature_number  = 3\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","c53d1910":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","f0c2564d":"train_df.describe()","fd4295cd":"df_clean.drop(df_clean[(df_clean['NumberOfTime30-59DaysPastDueNotWorse']>50) &(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)\ndf_clean.shape\n#df_clean.drop(df_clean[df_clean['NumberOfTime30-59DaysPastDueNotWorse']>50],axis=1,inplace=True)","2775f887":"feature_number  = 4\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","5302e62e":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","ff1fed37":"df_clean.describe()","fc0aca9f":"hist_perc(df_clean.DebtRatio,100,5,3000)","c36bba71":"df_clean.drop(df_clean[(df_clean.DebtRatio>5)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)","11e4966d":"feature_number  = 5\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","836ab107":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","3f9c492a":"hist_perc(df_clean.MonthlyIncome,1000,0,100000)","7c8a1ca3":"df_clean.describe()","251e8c1b":"df_clean.drop(df_clean[(df_clean.MonthlyIncome>30000)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)\n#df_clean[(df_clean.MonthlyIncome>30000)&(df_clean.SeriousDlqin2yrs==0)].SeriousDlqin2yrs.value_counts()","f7bdff54":"df_clean.shape","e78c0bde":"df_clean.describe()","654049c7":"feature_number  = 6\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","9b8e7a96":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","ece3421a":"hist_perc(df_clean.NumberOfOpenCreditLinesAndLoans,10,20,50)","faca201c":"df_clean.drop(df_clean[(df_clean.NumberOfOpenCreditLinesAndLoans>20)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)","95c4cc5f":"feature_number  = 7\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","e3c0a4c3":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","0b0cb2b9":"# df_clean.drop(df_clean[(df_clean.NumberOfTimes90DaysLate>90)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)\ndf_clean[(df_clean.NumberOfTimes90DaysLate>10)].SeriousDlqin2yrs.value_counts()","8859e382":"feature_number  = 8\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","4ac03d0b":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","7e6d3e5b":"df_clean[(df_clean['NumberRealEstateLoansOrLines']>10)].SeriousDlqin2yrs.value_counts()","0cb721f6":"feature_number  = 9\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","68023cf7":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","836b58b8":"df_clean[(df_clean['NumberOfTime60-89DaysPastDueNotWorse']>10)].SeriousDlqin2yrs.value_counts()","cc8ce666":"feature_number  = 10\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","2272b0b0":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","5056ddb7":"df_clean[(df_clean['NumberOfDependents']>6)].SeriousDlqin2yrs.value_counts()","a5eb6408":"df_clean.drop(df_clean[(df_clean.NumberOfDependents>6)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)","8842bfee":"df_clean.SeriousDlqin2yrs.value_counts()","d7347ecd":"# New X_train Y Train with cleaned dataset\n\nX_train = df_clean.iloc[:,1:]\nY_train = df_clean.iloc[:,0:1]\n\nprint(X_train.shape,Y_train.shape)\n","2d86469e":"ScoreCard","c38739f9":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandardCW_Clean = ScoreDataFrame(names,results,'standard_f1_score_clean')\nScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandardCW_Clean], axis=1)\nScoreCard","486a6243":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMaxCW_clean = ScoreDataFrame(names,results,'minmax_f1_score_clean')\nScoreCard = pd.concat([ScoreCard,\n                          scaledScoreMinMaxCW_clean], axis=1)\nScoreCard","000cc84b":"def HeatMap(df,x=True):\n        correlations = train_df.corr()\n        ## Create color map ranging between two colors\n        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n        fig, ax = plt.subplots(figsize=(10, 10))\n        fig = sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',square=True, linewidths=.5, annot=x, cbar_kws={\"shrink\": .75})\n        fig.set_xticklabels(fig.get_xticklabels(), rotation = 90, fontsize = 10)\n        fig.set_yticklabels(fig.get_yticklabels(), rotation = 0, fontsize = 10)\n        plt.tight_layout()\n        plt.show()\n\nHeatMap(train_df,x=True)\n","6039f763":"# Feature Importance\n\nclf = ExtraTreesClassifier(n_estimators=250,\n                              random_state=SEED)\n\nclf.fit(X_train, Y_train)\n\n# #############################################################################\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, train_df.columns[sorted_idx])#boston.feature_names[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","b9e942bf":"X_train = df_clean[['NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','RevolvingUtilizationOfUnsecuredLines','NumberOfOpenCreditLinesAndLoans','age']]","c3d590c9":"print(X_train.shape,Y_train.shape)","6d9a54dc":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandardCW_Clean_VI = ScoreDataFrame(names,results,'standard_f1_score_clean_vi')\n","57c41a33":"ScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandardCW_Clean_VI], axis=1)\nScoreCard","f8dbc2a1":"#ScoreCard = pd.concat([ScoreCard.iloc[0:11,0:15],ScoreCard.iloc[0:11,19:20]],axis=1)\n","240a068f":"ScoreCard","a486429c":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom scipy.stats import uniform","8638fe21":"\nX_test = test_df.sample(frac=0.1)[['SeriousDlqin2yrs','NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','RevolvingUtilizationOfUnsecuredLines','NumberOfOpenCreditLinesAndLoans','age']]\nX_test.drop(test_df[['SeriousDlqin2yrs']],axis=1,inplace=True)\nX_test = X_test.dropna()\n#Y_test = X_test['SeriousDlqin2yrs']\n\n","dcaa3728":"class RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred","8e072028":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred","d35ab923":"# model\nmodel = LogisticRegression()\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter distribution using uniform distribution\nC = uniform(loc=0, scale=4)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)","0df503d3":"LR_RandSearch = RandomSearch(X_train,Y_train,model,hyperparameters,scoring='f1_weighted')\nLR_best_model,LR_best_params = LR_RandSearch.RandomSearch()\nPrediction_LR = LR_RandSearch.BestModelPridict(X_train)\n\n","63bf104b":"cp = classification_report(Y_train,Prediction_LR)\nprint(cp)\n\n\n","b19fb8d6":"hpt = [['hptuningLR',floatingDecimals(0.929252,4),0.91]]\nhpt","2aadb796":"model_KNN = KNeighborsClassifier()\n\nneighbors = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nparam_grid = dict(n_neighbors=neighbors)","45050aab":"KNN_GridSearch = GridSearch(X_train,Y_train,model_KNN,param_grid,scoring='f1_weighted')\nPrediction_KNN = KNN_GridSearch.BestModelPridict(X_train)\n","63746135":"cp = classification_report(Y_train,Prediction_KNN)\nprint(cp)\n","f0f8b79b":"hpt.append(['modehptuningKNN',floatingDecimals(0.926401,4),0.89])","9e8a65d5":"c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel_SVC = SVC()","180ccbfc":"SVC_GridSearch = GridSearch(X_train,Y_train,model_SVC,param_grid,scoring='f1_weighted')\nPrediction_SVC = SVC_GridSearch.BestModelPridict(X_train)","99a510e2":"C","c7521665":"###### LOGISTIC REGRESSION","d59b9cef":"###### Detect and Remove Outliers","201dc7f7":"###### FEATURE SELECTION","2c5d0836":"### HYPER PARAMETER TUNING","48dc4ba7":"###### KNN"}}