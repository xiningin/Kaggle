{"cell_type":{"27662e70":"code","e99b3b6c":"code","86af6754":"code","b95b2aeb":"code","d3fbb984":"code","60ccd42a":"code","b2cafa6d":"markdown","371b86e6":"markdown","fd071a81":"markdown","35e86f13":"markdown"},"source":{"27662e70":"#\u4f5c\u8005\uff1a1621430024\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb","e99b3b6c":"train_data = pd.read_csv('..\/input\/sf-crime\/train.csv.zip', parse_dates=['Dates'])\ntest_data = pd.read_csv('..\/input\/sf-crime\/test.csv.zip', parse_dates=['Dates'])","86af6754":"train_data.info()\ntest_data.info()","b95b2aeb":"all_features = pd.concat((train_data.iloc[:, [0, 3, 4, 6, 7, 8]],\n                          test_data.iloc[:, [1, 2, 3, 4, 5, 6]]),\n                         sort=False)\n\nnum_train = train_data.shape[0]\n\ntrain_labels = pd.get_dummies(train_data['Category']).values\nnum_outputs = train_labels.shape[1]\ntrain_labels = np.argmax(train_labels, axis=1)\n\nall_features['year'] = all_features.Dates.dt.year\nall_features['month'] = all_features.Dates.dt.month\nall_features['new_year'] = all_features['month'].apply(\n    lambda x: 1 if x == 1 or x == 2 else 0)\nall_features['day'] = all_features.Dates.dt.day\nall_features['hour'] = all_features.Dates.dt.hour\nall_features['evening'] = all_features['hour'].apply(lambda x: 1\n                                                     if x >= 18 else 0)\n\nwkm = {\n    'Monday': 0,\n    'Tuesday': 1,\n    'Wednesday': 2,\n    'Thursday': 3,\n    'Friday': 4,\n    'Saturday': 5,\n    'Sunday': 6\n}\nall_features['DayOfWeek'] = all_features['DayOfWeek'].apply(lambda x: wkm[x])\nall_features['weekend'] = all_features['DayOfWeek'].apply(\n    lambda x: 1 if x == 4 or x == 5 else 0)\n\nOneHot_features = pd.get_dummies(all_features['PdDistrict'])\n\nall_features['block'] = all_features['Address'].apply(\n    lambda x: 1 if 'block' in x.lower() else 0)\n\nPCA_features = all_features[['X', 'Y']].values\nStandard_features = all_features[['DayOfWeek', 'year', 'month', 'day',\n                                  'hour']].values\nOneHot_features = pd.concat([\n    OneHot_features, all_features[['new_year', 'evening', 'weekend', 'block']]\n],\n                            axis=1).values\n\nscaler = StandardScaler()\nscaler.fit(Standard_features)\nStandard_features = scaler.transform(Standard_features)\n\npca = PCA(n_components=2)\npca.fit(PCA_features)\nPCA_features = pca.transform(PCA_features)\n\nall_features = np.concatenate(\n    (PCA_features, Standard_features, OneHot_features), axis=1)\n\ntrain_features = all_features[:num_train]\nnum_inputs = train_features.shape[1]\ntest_features = all_features[num_train:]","d3fbb984":"data_train = lgb.Dataset(train_features, label = train_labels)","60ccd42a":"params = {\n    'boosting': 'gbdt', \n    'objective': 'multiclass',\n    'metrics' : 'multi_logloss',\n    'num_class': num_outputs,\n    'verbosity': 1,\n    'device_type':'gpu',\n    'gpu_platform_id':0,\n    'gpu_device_id':0,\n    'max_depth': 6,\n    'num_leaves': 51,\n    'min_data_in_leaf' : 25,\n    'feature_fraction': 0.79,\n    'learning_rate': 0.01,\n    }\ngbm = lgb.train(params, data_train, num_boost_round = 2000)\ngbm.save_model('..\/working\/gbm(v2).txt')\ntestResult = gbm.predict(test_features)\nsampleSubmission = pd.read_csv('..\/input\/sf-crime\/sampleSubmission.csv.zip')\nResult_pd = pd.DataFrame(testResult,\n                         index=sampleSubmission.index,\n                         columns=sampleSubmission.columns[1:])\nResult_pd.to_csv('..\/working\/sampleSubmission(gbmv2).csv', index_label='Id')","b2cafa6d":"Start up!","371b86e6":"Load data","fd071a81":"Data info","35e86f13":"Transform data"}}