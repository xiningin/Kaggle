{"cell_type":{"951a10c9":"code","03a06d2a":"code","b635c26e":"code","48e13412":"code","f35b6683":"code","a4873bbe":"code","76d5303e":"code","5df2fc1e":"code","5e1074f7":"code","8d6636ec":"code","69d154b0":"code","ec46bd8a":"markdown"},"source":{"951a10c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","03a06d2a":"# Merge the two Data set together\ndf = pd.read_csv('..\/input\/pdb_data_no_dups.csv').merge(pd.read_csv('..\/input\/pdb_data_seq.csv'), how='inner', on='structureId')\n# Drop rows with missing labels\ndf = df[[type(c) == type('') for c in df.classification.values]]\ndf = df[[type(c) == type('') for c in df.sequence.values]]\n# select proteins\ndf = df[df.macromoleculeType_x == 'Protein']\ndf.reset_index()\ndf.shape","b635c26e":"from collections import Counter\n\n# count numbers of instances per class\ncnt = Counter(df.classification)\n# select only 10 most common classes!\ntop_classes = 10\n# sort classes\nsorted_classes = cnt.most_common()[:top_classes]\nclasses = [c[0] for c in sorted_classes]\ncounts = [c[1] for c in sorted_classes]\nprint(\"at least \" + str(counts[-1]) + \" instances per class\")\n\n# apply to dataframe\nprint(str(df.shape[0]) + \" instances before\")\ndf = df[[c in classes for c in df.classification]]\nprint(str(df.shape[0]) + \" instances after\")\n\nseqs = df.sequence.values\nlengths = [len(s) for s in seqs]","48e13412":"import matplotlib.pyplot as plt\n\n# visualize\nfig, axarr = plt.subplots(1,2, figsize=(20,5))\naxarr[0].bar(range(len(classes)), counts)\nplt.sca(axarr[0])\nplt.xticks(range(len(classes)), classes, rotation='vertical')\naxarr[0].set_ylabel('frequency')\n\naxarr[1].hist(lengths, bins=100, normed=False)\naxarr[1].set_xlabel('sequence length')\naxarr[1].set_ylabel('# sequences')\nplt.show()","f35b6683":"from sklearn.preprocessing import LabelBinarizer\n\n# Transform labels to one-hot\nlb = LabelBinarizer()\nY = lb.fit_transform(df.classification)","a4873bbe":"from keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\n\n# maximum length of sequence, everything afterwards is discarded!\nmax_length = 256\n\n#create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(seqs)\n#represent input data as word rank number sequences\nX = tokenizer.texts_to_sequences(seqs)\nX = sequence.pad_sequences(X, maxlen=max_length)","76d5303e":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 8\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(top_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#Model summary for our use\nprint(model.summary())","5df2fc1e":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.2)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=128)","5e1074f7":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nimport itertools\n\ntrain_pred = model.predict(X_train)\ntest_pred = model.predict(X_test)\nprint(\"train-acc = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test-acc = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","8d6636ec":"# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(10,10))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","69d154b0":"print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","ec46bd8a":"Further improvements are possible (deeper model, smaller sequences, using n-grams as words for word embedding to accelerate learning) and should be validated via cross-validation. Another major concern is about overfitting small training data, which can be solved by using droupout."}}