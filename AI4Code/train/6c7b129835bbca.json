{"cell_type":{"084708b9":"code","91b8c013":"code","a7d11332":"code","baa54750":"code","908b9e6b":"code","dfcdc99d":"code","aaa0f015":"code","734bccda":"code","2e41ffb7":"code","0064d8a7":"code","ba9d7495":"code","925c9d7f":"code","a18795b4":"code","affa329e":"code","9a50bf60":"code","f480c98e":"code","efb00038":"code","d5f25efd":"code","c8dbc3bf":"code","c83e6156":"code","b2bc2e97":"code","f79b2c2b":"code","f2b10166":"code","97808544":"code","c1730742":"code","c87eff50":"code","d380bd91":"code","bbb55118":"code","c3d8555e":"code","ada2d7c5":"code","a55b332e":"code","1810acee":"code","74f6c1e8":"markdown","45aeb206":"markdown","39fed083":"markdown","21cf93f3":"markdown","e5761dde":"markdown","13d31028":"markdown","9470f976":"markdown","8519f72a":"markdown","dfd75a2f":"markdown","d758f38e":"markdown","3e552bca":"markdown","6592189f":"markdown","7766984b":"markdown","dfcda856":"markdown","8a350cf3":"markdown","839ab104":"markdown","252444f1":"markdown","de8f6b7d":"markdown","bcb0a875":"markdown","c2cd5222":"markdown","a71d69bd":"markdown","62d974d1":"markdown","e228b052":"markdown","fa2b9a53":"markdown","94474526":"markdown","87663c20":"markdown","4bd1186f":"markdown","98f283e0":"markdown","7abe8f1b":"markdown","da74c05a":"markdown","dd01ba82":"markdown","70c0389f":"markdown","d9a62aca":"markdown","78383f11":"markdown","33f4d76c":"markdown","5eb4e81b":"markdown","c961d990":"markdown","5bae45c4":"markdown","4af77e03":"markdown","cb318d9e":"markdown","849dc0fb":"markdown","ce7c1701":"markdown","260c2fb8":"markdown","d63db544":"markdown","fc790156":"markdown"},"source":{"084708b9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nplt.rcParams.update({'font.size': 12})    # It just updates the fontsize to be used in matplotlib plots to 12 which is 10 by default.","91b8c013":"data = pd.read_csv('..\/input\/Iris.csv', index_col=\"Id\")\ndata.head()  # Checking the first 5 samples from the imported data.","a7d11332":"print(\"Number of samples in our dataset: \", data.shape[0])\nprint(\"Number of columns in our dataset: \", data.shape[1])","baa54750":"data.dtypes","908b9e6b":"data.isnull().any()","dfcdc99d":"data.info()","aaa0f015":"classes = data[\"Species\"].unique()\nprint(\"We have {} classes which are {}\".format(len(classes), classes))","734bccda":"print(data[\"Species\"].value_counts())\n\nsns.countplot(data[\"Species\"])\nplt.xlabel(\"Species\", labelpad=10, fontsize=14)\nplt.ylabel(\"Counts\", labelpad=10, fontsize=14)\nplt.show()","2e41ffb7":"data.hist(figsize=(18,8), edgecolor=\"black\")\nplt.show()","0064d8a7":"fig = plt.figure(figsize=(18,6))\n\nax1 = fig.add_subplot(121)\nsns.boxplot(data=data, orient=\"h\", ax=ax1)\nax1.set_title(\"Box and Whisker plot for different features\")\n\nax2 = fig.add_subplot(122)\nsns.kdeplot(data[\"SepalLengthCm\"], shade=True, shade_lowest=True, ax=ax2)\nsns.kdeplot(data[\"SepalWidthCm\"], shade=True, shade_lowest=True, ax=ax2)\nsns.kdeplot(data[\"PetalLengthCm\"], shade=True, shade_lowest=True, ax=ax2)\nsns.kdeplot(data[\"PetalWidthCm\"], shade=True, shade_lowest=True, ax=ax2)\nax2.set_title(\"Kernel Density Estimation plot for different features\")\n\nax2.legend(fontsize=12)\n\nplt.show()","ba9d7495":"fig, ax = plt.subplots(1,4, figsize=(18,6))\n\nsetosa = data[data[\"Species\"]==\"Iris-setosa\"]\nversicolor = data[data[\"Species\"]==\"Iris-versicolor\"]\nvirginica = data[data[\"Species\"]==\"Iris-virginica\"]\n\nsns.kdeplot(setosa[\"PetalWidthCm\"], label=\"setosa\", shade=True, ax=ax[0])\nsns.kdeplot(versicolor[\"PetalWidthCm\"], label=\"versicolor\", shade=True, ax=ax[0])\nsns.kdeplot(virginica[\"PetalWidthCm\"], label=\"virginica\", shade=True, ax=ax[0])\nax[0].set_title(\"Species vs PetalWidthCm\")\nax[0].legend()\n\nsns.kdeplot(setosa[\"PetalLengthCm\"], label=\"setosa\", shade=True, ax=ax[1])\nsns.kdeplot(versicolor[\"PetalLengthCm\"], label=\"versicolor\", shade=True, ax=ax[1])\nsns.kdeplot(virginica[\"PetalLengthCm\"], label=\"virginica\", shade=True, ax=ax[1])\nax[1].set_title(\"Species vs PetalLengthCm\")\nax[1].legend()\n\nsns.kdeplot(setosa[\"SepalWidthCm\"], label=\"setosa\", shade=True, ax=ax[2])\nsns.kdeplot(versicolor[\"SepalWidthCm\"], label=\"versicolor\", shade=True, ax=ax[2])\nsns.kdeplot(virginica[\"SepalWidthCm\"], label=\"virginica\", shade=True, ax=ax[2])\nax[2].set_title(\"Species vs SepalWidthCm\")\nax[2].legend()\n\nsns.kdeplot(setosa[\"SepalLengthCm\"], label=\"setosa\", shade=True, ax=ax[3])\nsns.kdeplot(versicolor[\"SepalLengthCm\"], label=\"versicolor\", shade=True, ax=ax[3])\nsns.kdeplot(virginica[\"SepalLengthCm\"], label=\"virginica\", shade=True, ax=ax[3])\nax[3].set_title(\"Species vs SepalLengthCm\")\nax[3].legend()\n\nplt.show()","925c9d7f":"fig = plt.figure(figsize=(18,6))\nax1 = fig.add_subplot(141)\nsns.swarmplot(x=\"Species\", y=\"PetalWidthCm\", data=data, ax=ax1)\nax2 = fig.add_subplot(142)\nsns.swarmplot(x=\"Species\", y=\"PetalLengthCm\", data=data, ax=ax2)\nax3 = fig.add_subplot(143)\nsns.swarmplot(x=\"Species\", y=\"SepalWidthCm\", data=data, ax=ax3)\nax4 = fig.add_subplot(144)\nsns.swarmplot(x=\"Species\", y=\"SepalLengthCm\", data=data, ax=ax4)\n\nplt.show()","a18795b4":"sns.pairplot(data, x_vars=[\"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], y_vars=\"SepalLengthCm\", kind=\"reg\", height=3, aspect=1.2)\nsns.pairplot(data, x_vars=[\"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], y_vars=\"SepalLengthCm\", hue=\"Species\", kind=\"scatter\", height=3, aspect=1.2)\nplt.show()","affa329e":"sns.pairplot(data, x_vars=[\"SepalLengthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], y_vars=\"SepalWidthCm\", kind=\"reg\", height=3, aspect=1.2)\nsns.pairplot(data, x_vars=[\"SepalLengthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], y_vars=\"SepalWidthCm\", hue=\"Species\", kind=\"scatter\", height=3, aspect=1.2)\nplt.show()","9a50bf60":"sns.pairplot(data, x_vars=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalWidthCm\"], y_vars=\"PetalLengthCm\", kind=\"reg\", height=3, aspect=1.2)\nsns.pairplot(data, x_vars=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalWidthCm\"], y_vars=\"PetalLengthCm\", hue=\"Species\", kind=\"scatter\", height=3, aspect=1.2)\nplt.show()","f480c98e":"sns.pairplot(data, x_vars=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\"], y_vars=\"PetalWidthCm\", kind=\"reg\", height=3, aspect=1.2)\nsns.pairplot(data, x_vars=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\"], y_vars=\"PetalWidthCm\", hue=\"Species\", kind=\"scatter\", height=3, aspect=1.2)\nplt.show()","efb00038":"sns.heatmap(data.corr(), annot=True)\nplt.show()","d5f25efd":"X = data.iloc[:, :-1]    # Here we select all the columns except the last one.\ny = data.iloc[:, -1]    # Here, selecting the last column as the output column.\nprint('Shape of X : ', X.shape)\nprint('Shape of y : ', y.shape)","c8dbc3bf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodels = []\nmodels.append((\"LR\", LogisticRegression()))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"SVC_linear\", SVC(kernel=\"linear\", gamma='auto')))\nmodels.append((\"SVC_rbf\", SVC(kernel=\"rbf\", gamma='auto')))\nmodels.append((\"DT\", DecisionTreeClassifier()))","c83e6156":"from sklearn.model_selection import KFold\n\nseed = 22\nscoring = 'accuracy'\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=seed)","b2bc2e97":"from sklearn.model_selection import cross_validate\n\nresults = []\nmodel_name = []\n\nfor name, model in models:\n    scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n    test_score = scores[\"test_score\"]\n    results.append(test_score.mean())\n    model_name.append(name)\n    msg = \"{:^12s}: {:.3f} \u00b1({:.2f})\".format(name, test_score.mean(), test_score.std())\n    print(msg)","f79b2c2b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=seed)  #Splitting the data into train and test set.\nprint('No. of training examples : ', X_train.shape[0])\nprint('No. of test examples : ', X_test.shape[0])","f2b10166":"from sklearn.model_selection import GridSearchCV\n\nC = np.arange(0.1, 2, 0.1)  # We will try different values of C between 0.1 and 1.9\nkernel = [\"linear\", \"rbf\"]\nparam_grid = {\"C\":C, \"kernel\":kernel}\n\ngs = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=kfold, verbose=True)\ngs.fit(X_train, y_train)\n\nprint(\"Best Score: \", gs.best_score_)\nprint(gs.best_estimator_)","97808544":"from sklearn.metrics import classification_report\n\nmodel = gs.best_estimator_\n\nprint(\" GridSearchCV best estimator \".center(50, \"=\"))\n\nprint(\"Training set classification report\".center(50, \"_\"))\nprint(classification_report(y_train, model.predict(X_train)))\n\nprint(\"Test set classification report\".center(50, \"_\"))\nprint(classification_report(y_test, model.predict(X_test)))","c1730742":"# Storing the value of C and kernel to use in other SVC() models when trying something different.\n\nC = gs.best_estimator_.C\nkernel = gs.best_estimator_.kernel","c87eff50":"data_copy = data.copy()  # Making a copy of the dataframe\ndata_copy[\"pwidth_plength\"] = data_copy[\"PetalWidthCm\"] * data_copy[\"PetalLengthCm\"]  # Adding a new feature to the dataframe as described above\ndata_copy.drop(columns=[\"PetalWidthCm\", \"PetalLengthCm\"], inplace=True)  # Droping both the columns\ndata_copy.head()","d380bd91":"y_copy = data_copy[\"Species\"]\nX_copy = data_copy.drop(columns=\"Species\")\nX_copy_train, X_copy_test, y_copy_train, y_copy_test = train_test_split(X_copy, y_copy, test_size=0.2, random_state=seed)","bbb55118":"svc = SVC(C=C, kernel=kernel)\nsvc.fit(X_copy_train, y_copy_train)\n\nprint(\" Combined PetalWidthCm and PetalLengthCm \".center(50, \"=\"))\n\nprint(\"Training set classification report\".center(50, \"_\"))\nprint(classification_report(y_copy_train, svc.predict(X_copy_train)))\n\nprint(\"Test set classification report\".center(50, \"_\"))\nprint(classification_report(y_copy_test, svc.predict(X_copy_test)))","c3d8555e":"sns.boxplot(data[\"SepalWidthCm\"])\nplt.show()","ada2d7c5":"Q1, Q3 = data[\"SepalWidthCm\"].quantile(q=[0.25, 0.75])  # We use the pandas.Series.quantile function to get the value of Q1 and Q3\nIQR = Q3 - Q1  # Calculating the Inter Quartile range(IQR)\n\ndata_wo_outliers = data_copy[(data_copy[\"SepalWidthCm\"] > (Q1 - IQR*1.5)) & (data_copy[\"SepalWidthCm\"] < (Q3 + IQR*1.5))]\nprint(data_wo_outliers.shape)\nprint(\"{} samples were outliers\".format(150-data_wo_outliers.shape[0]))","a55b332e":"y_wo_outliers = data_wo_outliers[\"Species\"]\nX_wo_outliers = data_wo_outliers.drop(columns=\"Species\")\n\nX_copy_train, X_copy_test, y_copy_train, y_copy_test = train_test_split(X_wo_outliers, y_wo_outliers, test_size=0.2, random_state=seed)","1810acee":"svc = SVC(C=C, kernel=kernel)\nsvc.fit(X_copy_train, y_copy_train)\n\nprint(\" Removed Outliers \".center(50, \"=\"))\n\nprint(\"Training set classification report\".center(50, \"_\"))\nprint(classification_report(y_copy_train, svc.predict(X_copy_train)))\n\nprint(\"Test set classification report\".center(50, \"_\"))\nprint(classification_report(y_copy_test, svc.predict(X_copy_test)))","74f6c1e8":"We got 5 columns in our dataset to work on this supervised learning problem.","45aeb206":"As we see it doesn't helps. No problem, we tried.","39fed083":"### Removing the outliers","21cf93f3":"### Relation between the independent variables","e5761dde":"Finally we arrived at the end of this kernel.\n\nIt is my first public kernel and had been working on it for many days (yeah on this simple iris dataset I've been spending days) worrying that if I'm doing something wrong, missing something or overdoing it. But finally I did it!\n\nHope you liked this kernel and learned something new from it, then **<span style='color:blue'>please motivate me by upvoting this kernel.<\/span>**\n\nIf I did something wrong then please do comment it, I'm always ready to improve myself.\n\n### Thanks alot for going through this kernel. Keep Learning, Keep growing!","13d31028":"Now, let's check the relation between the species and other features.","9470f976":"### Hello and welcome to this kernel where we will explore the iris dataset, use predictive modeling to predict the class of the species given its features and then will experiment with the dataset to improve the results.","8519f72a":"## Experimenting with the dataset","dfd75a2f":"Checking out the **number of samples** and columns in our dataset.","d758f38e":"When it comes to continuous features the samples can take any values over a range so density plots or histograms are useful in such situations.","3e552bca":"Since there was a very high correlation between PetalWidthCm and PetalLengthCm, lets make a new feature by taking the product of both the features, then drop both the features and use it to train our classifier.","6592189f":"Neither the dataset we have is large nor do we have a lot of outliers. Still we can try to remove the outliers and see if it helps. After all in machine learning \"Trial and Error\" is the way forward so we should try out the things before jumping to any conclusion by guessing.","7766984b":"A correlation as high as 0.96 which is almost equal to perfectly correlated (i.e., correlation=1) so both the features produces the same information.\n>It's like you are an officer in crime branch and two of your informers always gives you the same information before executing an operation, so you are like..(*thinks*) REMOVE ONE OF THE INFORMER as it does not helps, feature in our case.\n\nBut for now we train with all the features and will try to experiment later to predict the species, before that let's **prepare our data to train the models**.","dfcda856":"We plot the distribution of every feature\/independent variables using histograms which plots the frequencies\/counts as bars over equal ranges (or, intervals).","8a350cf3":"### How many examples we have per class?","839ab104":"Summary:\n\n1. SepalWidthCm shows weak negative correlation with all the other features\n2. PetalLengthCm and PetalWidthCm shows strong positive correaltion and the samples are separable across the 3 classes, except some overlap between Iris-versicolor and Iris-virginica.\n3. SepalLengthCm is positively correlated with PetalLengthCm and PetalWidthCm. Iris-setosa is isolated but Iris-versicolor and Iris-virginica has overlapped.","252444f1":"We have **no** Null\/NaN values in our dataset.","de8f6b7d":"Let's check the **data type of the columns.**","bcb0a875":"BTW the information we got in above two steps could be done in a single step by calling the method `info()` on the dataframe.","c2cd5222":"### What are the classes in our dataset?","a71d69bd":"**1. PetalWidthCm**\n>We can see that Iris-setosa take totally different values but there is some overlap between the distribution of Iris-versicolor and Iris-virginica.\n\n**2. PetalLengthCm**\n>Here also Iris-setosa take unique values and there is some intersection between the other two species.\n\n**3. SepalWidthCm**\n>All the species' distribution is highly overlapped.\n\n**4. SepalLegthCm**\n>Iris-setosa's and Iris-virginica's distribution are different but Iris-versicolor's distribution has overlapped with the other two.\n\n*PetalLengthCm and PetalWidthCm is going to be crucial for classifying the species of the samples*","62d974d1":"Now let's **import the data.**\n\nIris dataset has a column \"Id\" which is (you can say) a serial number given to the samples, I have used that column as index in our dataframe by passing the column name as an argument to the \"index_col\" parameter in `pandas.read_csv()` method which makes it the index while making a dataframe.","e228b052":"Wow it worked, now both the Training and Test accuracy is 97%.\n\nHere we dropped the PetalWidthCm and PetalLengthCm features after taking their product, you can experiment with other things and see the result.","fa2b9a53":"## Predictive Modeling","94474526":"SepalWidthCm shows weak negative correlation with all the other features and the samples are very much spread out.","87663c20":"We have 4 continuous variables which are the independent variables:\n>1. SepalLengthCm \n>2. SepalWidthCm\n>3. PetalLengthCm\n>4. PetalWidthCm\n\nI don't know much about flowers to explain what is sepal length\/width, petal length\/width :p, but all of them contains the length of the  parts (given by the column name) in flowers and all of them are measured in centimeter.\n\nAnd the dependent or target variable which contains the output class for each sample is:\n>Species","4bd1186f":"**PetalLengthCm vs Other features**","98f283e0":"At first, let's **import the packages and methods**, and then we will see how our data looks like and what to do with it.","7abe8f1b":"The dataset contains equal number of samples from all the 3 classes. A totally balanced dataset!","da74c05a":"## Conclusion\n1. We analysed and visualized the different features, their relation with other features and with the output variable.\n2. All the features we had were continuous in nature so we had to mostly rely on the distribution of data and related plots to visualize it.\n>When you would work with categorical features then can explore other types of plots and get the idea or difference between working with categorical features or continuous features or ordinal features.\n3. We got an accuracy of 97% on both train and test dataset, which is a satisfying result.","dd01ba82":"**SepalLengthCm vs Other features**","70c0389f":"From both the distribution plots we can see that:\n\n>**PetalLengthCm** values are distributed over a long range.\n\n>**SepalWidthCm** values are concentrated in a small range compared to others and it has few outliers too.","d9a62aca":"**SepalWidthCm vs Other features**","78383f11":"We saw earlier that SepalWidthCm had some outliers, let's try to train a model after removing the outliers and see what results we can get.\n\n**How a sample is determined to be an outlier?**\n\nLets understand few things before we can find outliers. We can divide the distribution of our data into \n1. **First quartile(Q1)** which contains the 25% of the total data. The line where the box starts in the box plot below is Q1.\n2. **Second quartile(Q2)** which contains the 50% of the total data. The line you see in the middle is the median line or Q2.\n3. **Third quartile(Q3)** which contains the 75% of the total data. The line where the box ends is Q3.\n\n**Inter Quartile Range(IQR)** is the range between Q3 and Q1. The box in the box plot below shows the IQR. It's given by the formula\n>IQR = Q3 - Q1\n\nThe samples that takes value less than (Q1 - 1.5 &ast; IQR) and values greater than (Q3 + 1.5 &ast; IQR) are treated as outliers. So we will expel those samples from our previously modified dataset and see how it affects our prediction.","33f4d76c":"We use the classification report to evaluate our model. It gives the precision, recall and f1-score for each class separately along with the overall accuracy.\n\nIris-setosa has f1-score of 1 for both train and test set. It is clear from the visualizations we saw earlier that setosa was totally separate from all other classes so was very accurately predictable.\n\nTrain set has an accuracy of 99% and Test set has an accuracy of 93%. Looks like a little overfit, let's see if we can generalize further.","5eb4e81b":"Let's check the distribution of the features using other types of plots:\n\n**Box and Whisker Plot**\n>It plots the distribution of data according to quartiles and also plots the outliers(samples which takes unusual values).\n\n**Kernel Density Estimation Plot**\n>It is a variation of histogram which plots a smoother distribution of data which helps in determining the shape of distribution.","c961d990":"We will be trying Logistic Regression, KNN, SVC with linear kernel, rbf kernel and Decision Tree.","5bae45c4":"### Correlation between the features in numbers","4af77e03":"SVC linear kernel uses a linear decision boundary to classify the samples.<br>SVC rbf kernel uses a non-linear decision boundary to classify the samples.\n\nFor our classification problem both the algorithms performed well as we saw the samples were linearly separable even though Iris-versicolor and Iris-virginica had some overlap.\n\n*We calculated the standard deviation to show the uncertainity associated with the measurement.*\n\nNow, we should try some hypertuning to see if we can improve the accuracy.\n\nBefore doing that lets keep 20% of the data separate so that we can evaluate our model on totally unseen data.","cb318d9e":"These are swarm plots which is like scatter plot to visualize our samples against a categorical feature(\"Species\" in our case).\n\nThese plots shows the same thing we have seen in the distribution plots.","849dc0fb":"PetalLengthCm shows strong positive correlation with PetalWidthCm and the samples are separable(visually) through their relation.","ce7c1701":"There is weak negative (as the slope of the regression line is negative) correlation between SepalLengthCm and SepalWidthCm but a good positive correlation with other two features.\n\nIn the 2nd row we plotted the relation between the features along with visualizing the different Species.","260c2fb8":"## Know your data","d63db544":"## Univariate Analysis","fc790156":"## Bivariate Analysis"}}