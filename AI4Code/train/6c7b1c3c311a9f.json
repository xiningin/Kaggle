{"cell_type":{"aae44b1a":"code","30d1409e":"code","960123d7":"code","b2c344a1":"code","af44d77f":"code","25935291":"code","58b4ee3a":"code","49811aa3":"code","1014bc8a":"code","34dda097":"code","fb6e308e":"code","4ff3fa71":"code","af81a4dd":"code","f421ad97":"code","86df83d1":"code","d9d8afb4":"code","dc2bd38d":"code","e092cc3d":"code","44fbc115":"code","aee05d4f":"code","982d421f":"code","d839051f":"code","8b2cdb67":"code","139dd06c":"code","d0c5a160":"code","cc142991":"markdown","c072117d":"markdown","45ba78c8":"markdown","9804b422":"markdown","a8c6cae1":"markdown","40b9ddf9":"markdown","308d5c9c":"markdown","61dc3656":"markdown","33b5b8b2":"markdown","860f02b5":"markdown"},"source":{"aae44b1a":"COMPETITION_DATASET_IMAGES = [\n    f\"\/kaggle\/input\/fire-198-competition-dataset\/{image_name}\"\n    for image_name in [\"a.jpg\", \"b.jpg\", \"c.jpg\", \"d.jpg\"]\n]\nCLEAN_UP = True\nSUBMISSION_PATH = \"\/kaggle\/working\/submission.csv\"\n\n\nDETR_CHECKPOINTS_ID = \"1osiy2z7SJvK7ZWaU9boMXsoakVZUWj39\"\nDETR_PRETRAINED_URL = \"https:\/\/dl.fbaipublicfiles.com\/detr\/detr-r50-e632da11.pth\"\nDETR_CHECKPOINT = \"\/kaggle\/working\/detr_checkpoint.pth\"\nDETR_DATA = \"\/kaggle\/working\/detr_data\"\nDETR_DIR = \"\/kaggle\/working\/detr\"\nRETRAIN_DETR = False\n\nRETRAIN_EMBEDDING_MODEL = False\n\nEMBEDDING_PAIRS = \"\/kaggle\/working\/pairs.txt\"\nEMBEDDING_DATA_DIR = \"\/kaggle\/working\/images_dir\"\nEMBEDDING_WEIGHTS = \"\/kaggle\/working\/face_model_0.h5\"\nEMBEDDING_BATCH_SIZE = 50\nEMBEDDING_EPOCHS = 100\n\nDOWNLOAD_EMBEDDING_TRANSFER_WEIGHTS = False\nCLOBBER_EMBEDDING_TRANSFER_WEIGHTS = False\nEMBEDDING_TRANSFER_WEIGHTS_ID = \"1XYB0EPGICcGpknobnhxFIguPVUSlKFEq\"\nTRANSFER_EMBEDDING_MODEL = False\nEMBEDDING_TRANSFER_WEIGHTS = \"\/kaggle\/working\/face_model_1.h5\"\n\nDOWNLOAD_LFW_DATA = False\nLFW_DATA_URL = \"http:\/\/vis-www.cs.umass.edu\/lfw\/lfw.tgz\"\nLFW_PAIRS_URL = \"http:\/\/vis-www.cs.umass.edu\/lfw\/pairsDevTrain.txt\"\n\nDOWNLOAD_FIRE_DATA = True\nFIRE_DATA_ID = \"1fF12LzutTTnHJ-JKHEw6yNdD5yHYEViB\"\nFIRE_PAIRS_ID = \"1NmEIhDJtzzmrDarcUHH-wPS4yr0GYsk4\"\n\nCLOBBER_ALL_EMBEDDING_DATA = True\n\nCOMPUTED_CLASS_EMBEDDINGS_ID = \"1VcAShSpQke1UmPRFzzd3yOW4xhlpuFy7\"\nCOMPUTED_CLASS_EMBEDDINGS = \"\/kaggle\/working\/class_embeddings.npy\"\nRECOMPUTE_CLASS_EMBEDDINGS = True\n\nLANDMARKS_ID = \"1jqZUxswYsc4lyJpqaxd5LUedx7XnVLHN\"\nMAPPINGS_ID = \"1wszVOXqpm8hX1you5C_X_dUQfwXJEW3D\"\nCHECKPOINTS_ID = \"1osiy2z7SJvK7ZWaU9boMXsoakVZUWj39\"\n\nLANDMARKS = \"\/kaggle\/working\/landmarks.dat\"\nMAPPINGS = \"\/kaggle\/working\/mapping.csv\"\n\nEMBEDDING_WEIGHTS_ID = \"19XlioJdG9apvhLVMqmIjJq02aBHfjjtg\"\nDOWNLOAD_EMBEDDING_WEIGHTS = True","30d1409e":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nimport shutil\nfrom tqdm.notebook import tqdm\n\nimport cv2\nfrom cv2 import cv2\n\nimport pathlib\nimport os, sys, json\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nimport torch \nimport torchvision.transforms as T \nimport argparse\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa","960123d7":"!pip install gdown pycocotools\n\nif not pathlib.Path('face_recognition').exists():\n    print(\"Downloading the face_recognition library.\")\n    !git clone https:\/\/github.com\/varun-ramani\/face-recognition face_recognition\n\nif not pathlib.Path(DETR_DIR).exists():\n    print(\"Now downloading DETR\")\n    !git clone https:\/\/github.com\/facebookresearch\/detr.git $DETR_DIR\n    os.chdir(DETR_DIR)\n    !git checkout a54b77800eb8e64e3ad0d8237789fcbf2f8350c5\n\nif not pathlib.Path(LANDMARKS).exists():\n    print(\"Downloading face landmarks for Dlib alignment.\")\n    !gdown --id '1jqZUxswYsc4lyJpqaxd5LUedx7XnVLHN' -O $LANDMARKS\n\nif not pathlib.Path(MAPPINGS).exists():\n    print(\"Downloading id mappings\")\n    !gdown --id '1wszVOXqpm8hX1you5C_X_dUQfwXJEW3D' -O $MAPPINGS\n\nif CLOBBER_ALL_EMBEDDING_DATA and (DOWNLOAD_LFW_DATA or DOWNLOAD_FIRE_DATA):\n    !rm -rf $EMBEDDING_DATA_DIR $EMBEDDING_PAIRS\n\nif DOWNLOAD_LFW_DATA:\n    print(\"Downloading LFW data\")\n    if not pathlib.Path(EMBEDDING_DATA_DIR).exists():\n        print(\"Downloading and extracting the LFW image data\")\n        !curl $LFW_DATA_URL | tar xzf -\n        print(f\"Moving the LFW dataset to directory {EMBEDDING_DATA_DIR}\")\n        !mv lfw $EMBEDDING_DATA_DIR\n    if not pathlib.Path(EMBEDDING_PAIRS).exists():\n        print(f\"Downloading the LFW pairs to {EMBEDDING_PAIRS}\")\n        !curl $LFW_PAIRS_URL > $EMBEDDING_PAIRS\n\nif DOWNLOAD_FIRE_DATA:\n    print(\"Downloading FIRE data\")\n    if not pathlib.Path(EMBEDDING_DATA_DIR).exists():\n        print(\"Downloading and extracting the FIRE image data\")\n        !rm -rf convertedfiredataset\n        !gdown --id $FIRE_DATA_ID -O convertedfiredata.zip\n        !unzip convertedfiredata.zip \n        !mv convertedfiredataset $EMBEDDING_DATA_DIR\n    if not pathlib.Path(EMBEDDING_PAIRS).exists():\n        print(f\"Downloading the FIRE pairs to {EMBEDDING_PAIRS}\")\n        !gdown --id $FIRE_PAIRS_ID -O $EMBEDDING_PAIRS\n        \nif DOWNLOAD_EMBEDDING_TRANSFER_WEIGHTS:\n    print(\"Downloading the transfer weights\")\n    if CLOBBER_EMBEDDING_TRANSFER_WEIGHTS:\n        !rm -f $EMBEDDING_TRANSFER_WEIGHTS\n        \n    !gdown --id $EMBEDDING_TRANSFER_WEIGHTS_ID -O $EMBEDDING_TRANSFER_WEIGHTS\n\nif not pathlib.Path(LANDMARKS).exists():\n    !gdown --id $LANDMARKS_ID -O $LANDMARKS\n\nif not pathlib.Path(MAPPINGS).exists():\n    !gdown --id $MAPPINGS_ID -O $MAPPINGS\n    \n\n    \nif RETRAIN_DETR:\n    !gdown --id 1nKot1S6ARpB6wnyx3R1tefduUiUIjO9p\n    !unzip -q WIDER_val.zip -d $DETR_DATA\n    !gdown --id 1rkUK6OG7js5gYt1bhCUanPNqmuxwkVZX\n    !unzip -q WIDER_train.zip -d $DETR_DATA\n    !wget http:\/\/mmlab.ie.cuhk.edu.hk\/projects\/WIDERFace\/support\/bbx_annotation\/wider_face_split.zip\n    !unzip -q wider_face_split.zip -d $DETR_DATA\nelse:\n    !gdown --id $DETR_CHECKPOINTS_ID -O $DETR_CHECKPOINT\n\nif DOWNLOAD_EMBEDDING_WEIGHTS:\n    if pathlib.Path(EMBEDDING_WEIGHTS).exists():\n        print(\"The embedding weights already exist, not going to clobber.\")\n    else:\n        !gdown --id $EMBEDDING_WEIGHTS_ID -O $EMBEDDING_WEIGHTS\n        \nif not RECOMPUTE_CLASS_EMBEDDINGS:\n    if pathlib.Path(COMPUTED_CLASS_EMBEDDINGS).exists():\n        print(\"The computed class embeddings already exist, not going to clobber.\")\n    else:\n        !gdown --id $COMPUTED_CLASS_EMBEDDINGS_ID -O $COMPUTED_CLASS_EMBEDDINGS\n\nfrom models import build_model\nfrom main import get_args_parser","b2c344a1":"parser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nargs.resume = DETR_PRETRAINED_URL\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False \nprint(args)","af44d77f":"face_model, criterion, postprocessors = build_model(args)\n\ndevice = torch.device('cpu')\nface_model.to(device)","25935291":"# COCO classes\nCLASSES = [\n   'N\/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A',\n   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack',\n   'umbrella', 'N\/A', 'N\/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N\/A', 'wine glass',\n   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n   'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table', 'N\/A',\n   'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A',\n   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n   'toothbrush'\n]\n\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\ndef detect(im, model, transform):\n  # mean-std normalize the input image (batch-size: 1)\n  img = transform(im).unsqueeze(0)\n\n  # demo the model only support by default images with aspect ration betweeen 0.5 and 2\n  # if you want to use images with an aspect ration outside this range \n  # rescale your image so that the maximum size at most 1333 for best results \n  assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n \n  # propagate through the model \n  outputs = model(img)\n\n  # keep only predictions with 0.7+ confidence \n  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n  keep = probas.max(-1).values > 0.7\n\n  # convert boxes from [0; 1] to image scales \n  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n  return probas[keep], bboxes_scaled\n\ndef plot_results(pil_img, prob, boxes, classes):\n  plt.figure(figsize=(16, 10))\n  plt.imshow(pil_img)\n  ax = plt.gca()\n  for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n    cl = p.argmax()\n    if CLASSES[cl] not in classes: # only plot these classes \n      continue \n    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n\n    text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n    ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n  plt.axis('off')\n  plt.show()","58b4ee3a":"def widerface2coco(outputpath, image_root,json_name, annopath):\n  dataset = { \"info\": {\n              \"description\": \"WIDER face in COCO format.\",\n              \"url\": \"\",\n              \"version\": \"1.1\",\n              \"contributor\": \"aimhabo\",\n              \"date_created\": \"2020-09-29\"},\n              \"images\": [],\n              \"annotations\": [],\n              \"categories\": [{\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 3, \"name\": \"car\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 4, \"name\": \"motorcycle\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 5, \"name\": \"airplane\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 6, \"name\": \"bus\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 7, \"name\": \"train\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 8, \"name\": \"truck\"}, \n                             {\"supercategory\": \"vehicle\", \"id\": 9, \"name\": \"boat\"}, \n                             {\"supercategory\": \"outdoor\", \"id\": 10, \"name\": \"traffic light\"}, \n                             {\"supercategory\": \"outdoor\", \"id\": 11, \"name\": \"fire hydrant\"}, \n                             {\"supercategory\": \"outdoor\", \"id\": 13, \"name\": \"stop sign\"}, \n                             {\"supercategory\": \"outdoor\", \"id\": 14, \"name\": \"parking meter\"}, \n                             {\"supercategory\": \"outdoor\", \"id\": 15, \"name\": \"bench\"}, \n                             {\"supercategory\": \"animal\", \"id\": 16, \"name\": \"bird\"}, \n                             {\"supercategory\": \"animal\", \"id\": 17, \"name\": \"cat\"}, \n                             {\"supercategory\": \"animal\", \"id\": 18, \"name\": \"dog\"}, \n                             {\"supercategory\": \"animal\", \"id\": 19, \"name\": \"horse\"}, \n                             {\"supercategory\": \"animal\", \"id\": 20, \"name\": \"sheep\"}, \n                             {\"supercategory\": \"animal\", \"id\": 21, \"name\": \"cow\"}, \n                             {\"supercategory\": \"animal\", \"id\": 22, \"name\": \"elephant\"}, \n                             {\"supercategory\": \"animal\", \"id\": 23, \"name\": \"bear\"}, \n                             {\"supercategory\": \"animal\", \"id\": 24, \"name\": \"zebra\"}, \n                             {\"supercategory\": \"animal\", \"id\": 25, \"name\": \"giraffe\"}, \n                             {\"supercategory\": \"accessory\", \"id\": 27, \"name\": \"backpack\"}, \n                             {\"supercategory\": \"accessory\", \"id\": 28, \"name\": \"umbrella\"}, \n                             {\"supercategory\": \"accessory\", \"id\": 31, \"name\": \"handbag\"}, \n                             {\"supercategory\": \"accessory\", \"id\": 32, \"name\": \"tie\"}, \n                             {\"supercategory\": \"accessory\", \"id\": 33, \"name\": \"suitcase\"}, \n                             {\"supercategory\": \"sports\", \"id\": 34, \"name\": \"frisbee\"}, \n                             {\"supercategory\": \"sports\", \"id\": 35, \"name\": \"skis\"}, \n                             {\"supercategory\": \"sports\", \"id\": 36, \"name\": \"snowboard\"}, \n                             {\"supercategory\": \"sports\", \"id\": 37, \"name\": \"sports ball\"}, \n                             {\"supercategory\": \"sports\", \"id\": 38, \"name\": \"kite\"}, \n                             {\"supercategory\": \"sports\", \"id\": 39, \"name\": \"baseball bat\"}, \n                             {\"supercategory\": \"sports\", \"id\": 40, \"name\": \"baseball glove\"}, \n                             {\"supercategory\": \"sports\", \"id\": 41, \"name\": \"skateboard\"}, \n                             {\"supercategory\": \"sports\", \"id\": 42, \"name\": \"surfboard\"}, \n                             {\"supercategory\": \"sports\", \"id\": 43, \"name\": \"tennis racket\"}, \n                             {\"supercategory\": \"kitchen\", \"id\": 44, \"name\": \"bottle\"}, \n                             {\"supercategory\": \"kitchen\", \"id\": 46, \"name\": \"wine glass\"}, \n                             {\"supercategory\": \"kitchen\", \"id\": 47, \"name\": \"cup\"}, \n                             {\"supercategory\": \"kitchen\", \"id\": 48, \"name\": \"fork\"}, \n                             {\"supercategory\": \"kitchen\", \"id\": 49, \"name\": \"knife\"}, \n                             {\"supercategory\": \"kitchen\", \"id\": 50, \"name\": \"spoon\"}, \n                             {\"supercategory\": \"kitchen\", \"id\": 51, \"name\": \"bowl\"}, \n                             {\"supercategory\": \"food\", \"id\": 52, \"name\": \"banana\"}, \n                             {\"supercategory\": \"food\", \"id\": 53, \"name\": \"apple\"}, \n                             {\"supercategory\": \"food\", \"id\": 54, \"name\": \"sandwich\"}, \n                             {\"supercategory\": \"food\", \"id\": 55, \"name\": \"orange\"}, \n                             {\"supercategory\": \"food\", \"id\": 56, \"name\": \"broccoli\"}, \n                             {\"supercategory\": \"food\", \"id\": 57, \"name\": \"carrot\"}, \n                             {\"supercategory\": \"food\", \"id\": 58, \"name\": \"hot dog\"}, \n                             {\"supercategory\": \"food\", \"id\": 59, \"name\": \"pizza\"}, \n                             {\"supercategory\": \"food\", \"id\": 60, \"name\": \"donut\"}, \n                             {\"supercategory\": \"food\", \"id\": 61, \"name\": \"cake\"}, \n                             {\"supercategory\": \"furniture\", \"id\": 62, \"name\": \"chair\"}, \n                             {\"supercategory\": \"furniture\", \"id\": 63, \"name\": \"couch\"}, \n                             {\"supercategory\": \"furniture\", \"id\": 64, \"name\": \"potted plant\"}, \n                             {\"supercategory\": \"furniture\", \"id\": 65, \"name\": \"bed\"}, \n                             {\"supercategory\": \"furniture\", \"id\": 67, \"name\": \"dining table\"}, \n                             {\"supercategory\": \"furniture\", \"id\": 70, \"name\": \"toilet\"}, \n                             {\"supercategory\": \"electronic\", \"id\": 72, \"name\": \"tv\"}, \n                             {\"supercategory\": \"electronic\", \"id\": 73, \"name\": \"laptop\"}, \n                             {\"supercategory\": \"electronic\", \"id\": 74, \"name\": \"mouse\"}, \n                             {\"supercategory\": \"electronic\", \"id\": 75, \"name\": \"remote\"}, \n                             {\"supercategory\": \"electronic\", \"id\": 76, \"name\": \"keyboard\"}, \n                             {\"supercategory\": \"electronic\", \"id\": 77, \"name\": \"cell phone\"}, \n                             {\"supercategory\": \"appliance\", \"id\": 78, \"name\": \"microwave\"}, \n                             {\"supercategory\": \"appliance\", \"id\": 79, \"name\": \"oven\"}, \n                             {\"supercategory\": \"appliance\", \"id\": 80, \"name\": \"toaster\"}, \n                             {\"supercategory\": \"appliance\", \"id\": 81, \"name\": \"sink\"}, \n                             {\"supercategory\": \"appliance\", \"id\": 82, \"name\": \"refrigerator\"}, \n                             {\"supercategory\": \"indoor\", \"id\": 84, \"name\": \"book\"}, \n                             {\"supercategory\": \"indoor\", \"id\": 85, \"name\": \"clock\"}, \n                             {\"supercategory\": \"indoor\", \"id\": 86, \"name\": \"vase\"}, \n                             {\"supercategory\": \"indoor\", \"id\": 87, \"name\": \"scissors\"}, \n                             {\"supercategory\": \"indoor\", \"id\": 88, \"name\": \"teddy bear\"}, \n                             {\"supercategory\": \"indoor\", \"id\": 89, \"name\": \"hair drier\"}, \n                             {\"supercategory\": \"indoor\", \"id\": 90, \"name\": \"toothbrush\"}],\n  }\n\n  phase = json_name\n\n  with open(annopath,'r') as f:\n      lines = f.readlines()\n      num_lines = len(lines)\n      i_l=0\n      img_id=1\n      anno_id=1\n      imagepath=None\n\n      pbar = tqdm(total=num_lines)\n\n      while i_l < num_lines:\n          if len(lines[i_l]) < 1:\n              break\n          if '--' in lines[i_l]:\n              imagepath=lines[i_l].strip()\n              im=image_root+imagepath\n              if (not os.path.exists(im)):\n                print(im)\n                break\n              im = cv2.imread(im)\n              height, width, channels = im.shape\n              dataset[\"images\"].append({\"file_name\": imagepath, \"coco_url\": \"local\", \"height\": height, \"width\": width, \"flickr_url\": \"local\", \"id\": img_id})\n              i_l+=1\n              pbar.update(1)\n              num_gt=int(lines[i_l])\n              while num_gt>0:\n                  i_l+=1\n                  pbar.update(1)\n                  x1,y1,wid,hei=list(map(int, lines[i_l].split()))[:4]\n                  num_gt-=1\n                  dataset[\"annotations\"].append({\n                      \"segmentation\": [],\n                      \"iscrowd\": 0,\n                      \"area\": wid * hei,\n                      \"image_id\": img_id,\n                      \"bbox\": [x1, y1, wid, hei],\n                      \"category_id\": 1,\n                      \"id\": anno_id})\n                  anno_id = anno_id + 1\n                  #if im is not None:\n                  #    cv2.rectangle(im,(x1,y1),(x1+wid,y1+hei), (0,0,0), 3)\n                  #    cv2.rectangle(im,(x1,y1),(x1+wid,y1+hei), (255,255,255), 1)\n              img_id+=1\n              #if im is not None:\n              #    cv2.imshow('img', im)\n              #    cv2.waitKey(0)\n          i_l+=1\n          pbar.update(1)\n          \n      pbar.close()\n\n  json_name = os.path.join(outputpath, \"{}.json\".format(phase))\n\n  with open(json_name, 'w') as f:\n      json.dump(dataset, f)","49811aa3":"if RETRAIN_DETR:\n    outputpath = pathlib.Path(DETR_DATA)\n    # Validation set \n    image_root_val = str(outputpath.joinpath(\"WIDER_val\/images\/\"))\n    json_name_val = \"WIDERFaceValCOCO\"\n    annopath_val = str(outputpath.joinpath(\"wider_face_split\/wider_face_val_bbx_gt.txt\"))\n    # Training set \n    image_root_train = str(outputpath.joinpath(\"WIDER_train\/images\/\"))\n    json_name_train = \"WIDERFaceTrainCOCO\"\n    annopath_train = str(outputpath.joinpath(\"wider_face_split\/wider_face_train_bbx_gt.txt\"))\n\n    widerface2coco(DETR_DATA, image_root_train, json_name_train, annopath_train)\n    widerface2coco(DETR_DATA, image_root_val, json_name_val, annopath_val)\n    \n    json_path_val = outputpath.joinpath(json_name_val)\n    json_path_train = outputpath.joinpath(json_name_train)\n    \n    with open(str(json_path_val), 'r') as fp:\n        train_json = json.load(fp)\n        \n    with open(str(json_path_train), 'r') as fp:\n        val_json = json.load(fp)\n        \n    !python main.py --coco_path $DETR_DATA --epochs 2 --batch_size 3 --resume $DETR_PRETRAINED_URL --output_dir $DETR_DIR\/checkpoints\n    !mv $DETR_DIR\/checkpoints\/checkpoint.pth $DETR_CHECKPOINT","1014bc8a":"parser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nargs.resume = DETR_CHECKPOINT\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False \nprint(args)","34dda097":"output_dir = pathlib.Path(args.output_dir)\nif args.resume:\n  # the model will download the weights and model state from the https link provided \n  if args.resume.startswith('https'):\n    checkpoint = torch.hub.load_state_dict_from_url(\n        args.resume, map_location = 'cpu', check_hash = True\n    ) \n  else:\n    checkpoint = torch.load(args.resume, map_location='cpu')\n  \n  # this load the weights and model state into the model \n  face_model.load_state_dict(checkpoint['model'], strict = True)","fb6e308e":"from face_recognition.model import create_model\nfrom face_recognition.align import AlignDlib","4ff3fa71":"# A wrapper around AlignDlib that provides alignment functionality to a variety of places in the program.\nclass AlignmentModel:\n    def __init__(self, landmarks_file):\n        self.model = AlignDlib(landmarks_file)\n\n    def align_image(self, img, anchor_shape=(96, 96)):\n        bb = self.model.getLargestFaceBoundingBox(img)\n\n        if bb is None:\n            return cv2.resize(img, anchor_shape)\n        else:\n            return self.model.align(anchor_shape[0], img, self.model.getLargestFaceBoundingBox(img), landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n\nalign = AlignmentModel(LANDMARKS)","af81a4dd":"# A wrapper around imgaug that provides augmentation functionality.\nclass ImageAugmentation:\n    def __init__(self):\n        self.seq_aug = iaa.Sequential([\n            iaa.Crop(px=(0, 16)),\n            iaa.Fliplr(0.5),\n            iaa.GaussianBlur(sigma=(0, 1.0)),\n            iaa.LinearContrast((0.75, 1.5))\n        ])\n    \n    def augment_images(img_batch):\n        return self.seq_aug(images=img_batch)\n\naugment = ImageAugmentation()","f421ad97":"# This loads data in batches for the training loop to consume.\nclass DataGenerator(keras.utils.Sequence):\n    def __init__(self, pairs_file_name, data_path, batch_size, anchor_shape=(96, 96), n_channels=3, img_aug=False, shuffle=True):\n        self.pairs_file_name = pairs_file_name\n        self.batch_size = batch_size\n        self.anchor_shape = anchor_shape\n        self.n_channels = n_channels\n        self.img_aug = img_aug\n        self.shuffle = shuffle\n        self.data_path = data_path\n        self.processed_images = {}\n\n        # The first array contains positive pairs, the second contains negative pairs.\n        self.pairs = [[], []]\n\n        with open(pairs_file_name, 'r') as pairs_file:\n            for line in pairs_file:\n                tokens = line.split()\n                if len(tokens) == 3:\n                    self.pairs[0].append([\n                        (tokens[0], int(tokens[1])),\n                        (tokens[0], int(tokens[2]))\n                    ])\n                elif len(tokens) == 4:\n                    self.pairs[1].append([\n                        (tokens[0], int(tokens[1])),\n                        (tokens[2], int(tokens[3]))\n                    ])\n\n        print(len(self.pairs[0]))\n\n        self.on_epoch_end()\n\n    def on_epoch_end(self):\n        # Returns a 2D array containing arrays of positive, then negative indices.\n        self.indices = [np.arange(len(pair_set)) for pair_set in self.pairs]\n\n        if self.shuffle:\n            for pair_set in self.pairs:\n                np.random.shuffle(pair_set)\n\n    def __getitem__(self, index):\n        # Returns a 2D array containing the positive and negative indices of the requested batch\n        indices = [index_set[index*self.batch_size:(index+1)*self.batch_size] for index_set in self.indices]\n        batches = [[self.pairs[i][k] for k in indices[i]] for i in [0, 1]]\n\n        output_arrays = np.empty((3, self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32)\n\n        for i, pos_pair in enumerate(batches[0]):\n            pos_name = pos_pair[0]\n\n            for j in range(0, 3):\n                if j == 2:\n                    name, num = batches[1][i][0]\n                    if name == pos_name:\n                        name, num = batches[1][i][1]\n                else:\n                    name, num = batches[0][i][j]\n\n                image_path = pathlib.Path(self.data_path).joinpath(name).joinpath(\"%s_%04d.jpg\" % (name, num))\n\n                if image_path in self.processed_images:\n                    img = self.processed_images[image_path]\n                else:\n                    img = cv2.imread(str(image_path))\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                    img = align.align_image(img, self.anchor_shape)\n                    img = img.astype('float32')\n                    img = img \/ 255.0\n                    self.processed_images[image_path] = img\n\n                output_arrays[j][i] = img\n\n        if self.img_aug:\n            output_arrays = [augment.augment_images(output_arrays[j]) for j in range(0, 3)]\n\n        return [output_arrays[0], output_arrays[1], output_arrays[2]], None\n\n    def __len__(self):\n        return int(np.floor(len(self.indices[0]) \/ self.batch_size))","86df83d1":"# Start setting up the model so it can be used later\nclass TripletLossLayer(Layer):\n    def __init__(self, alpha, **kwargs):\n        self.alpha = alpha\n        super(TripletLossLayer, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'alpha': self.alpha\n        })\n        return config\n\n    def triplet_loss(self, inputs):\n        a, p, n = inputs\n\n        p_dist, n_dist = [K.sum(K.square(a - elem), axis=-1)\n                          for elem in (p, n)]\n\n        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n\n    def call(self, inputs):\n        loss = self.triplet_loss(inputs)\n        self.add_loss(loss)\n\n        return loss","d9d8afb4":"# A model class that can train itself.\nclass EmbeddingModel:\n    def __init__(self, weights_file=None):\n        self.base_model: Model = create_model()\n\n        need_load_weights = False\n            \n        if weights_file != None:\n            try:\n                self.base_model.load_weights(weights_file)\n            except Exception as e:\n                print(\"Wasn't able to load weights into the base model.\")\n                need_load_weights = True\n\n        self.inputs = [Input(shape=(96, 96, 3), name=name) for name in (\"img_a\", \"img_p\", \"img_n\")]\n        self.embeddings = [self.base_model(inp) for inp in self.inputs]\n\n        self.triplet_loss_layer = TripletLossLayer(alpha=0.1, name=\"triplet_loss_layer\")(self.embeddings)\n\n        self.model = Model(self.inputs, self.triplet_loss_layer)\n        self.model.compile(loss=None, optimizer='adam')\n\n        if need_load_weights:\n            try:\n                self.model.load_weights(weights_file)\n                print(\"Loaded weights into the full model.\")\n            except Exception as e:\n                print(\"Wasn't able to load weights into the full model either, giving up.\")\n            \n\n    def train(self, generator: DataGenerator, epochs=100):\n            model_checkpoint_callback = ModelCheckpoint(\n                filepath=EMBEDDING_WEIGHTS,\n                monitor=\"loss\",\n                save_best_only=True,\n                mode=\"auto\",\n                save_weights_only=True,\n                verbose=1\n            )\n\n            early_stopping_callback = EarlyStopping(\n                monitor=\"loss\",\n                mode=\"auto\",\n                patience=20,\n                verbose=True\n            )\n\n            history = self.model.fit(\n                generator,\n                epochs=epochs,\n                callbacks=[model_checkpoint_callback, early_stopping_callback]\n            )\n\n    def make_prediction(self, aligned_images):\n        aligned_images = aligned_images.astype('float32') \/ 255.0\n        return self.model.layers[3].predict(aligned_images)","dc2bd38d":"embedding_model: EmbeddingModel = None\n\nif RETRAIN_EMBEDDING_MODEL:\n    if TRANSFER_EMBEDDING_MODEL:\n        embedding_model = EmbeddingModel(EMBEDDING_TRANSFER_WEIGHTS)\n\n    generator = DataGenerator(EMBEDDING_PAIRS, EMBEDDING_DATA_DIR, EMBEDDING_BATCH_SIZE)\n    embedding_model.train(generator, EMBEDDING_EPOCHS)\n\nelse:\n    embedding_model = EmbeddingModel(EMBEDDING_WEIGHTS)","e092cc3d":"class ClassEmbeddingMapping:\n    def __init__(self, mappings_file, embedding_model):\n        self.mappings = pd.read_csv(mappings_file)['person_name']\n        self.embedding_model = embedding_model\n        self.corrected_names = [name.replace(\"'\", \"_\").replace(\" \", \"_\") for name in self.mappings]\n\n    def load_embeddings(self, embeddings_file):\n        self.embeddings = np.load(embeddings_file)\n\n    def compute_embeddings(self, data_dir, embeddings_file):\n        self.embeddings = np.empty((len(self.corrected_names), 128))\n        images = np.empty((len(self.corrected_names), 96, 96, 3))\n\n        print(\"Now computing embeddings:\")\n        plt.show()\n        for index, name in enumerate(self.corrected_names):\n            print(f\"Computing {index + 1} \/ 49: {name}\")\n            name_dir = pathlib.Path(data_dir).joinpath(name)\n            image_path = list(name_dir.glob('*'))[0]\n            aligned_image = align.align_image(cv2.imread(str(image_path)))\n            images[index] = aligned_image\n\n        self.embeddings = self.embedding_model.make_prediction(images)\n\n        np.save(embeddings_file, self.embeddings)\n\n    def predict_names(self, aligned_images):\n        return self.embeddings[self.predict_ids(aligned_images)]\n\n    def predict_ids(self, aligned_images):\n        predicted_embeddings = self.embedding_model.make_prediction(aligned_images)\n\n        diff = np.array([\n            self.embeddings - predicted_embedding\n            for predicted_embedding in predicted_embeddings\n        ])\n\n        squared_diff = np.sum(diff*diff, axis=2)\n        distances = np.sqrt(squared_diff)\n        \n        return np.argmin(distances, axis=1)","44fbc115":"class_embeddings = ClassEmbeddingMapping(MAPPINGS, embedding_model)\n\nif RECOMPUTE_CLASS_EMBEDDINGS:\n    class_embeddings.compute_embeddings(EMBEDDING_DATA_DIR, COMPUTED_CLASS_EMBEDDINGS)\nelse:\n    class_embeddings.load_embeddings(COMPUTED_CLASS_EMBEDDINGS)","aee05d4f":"def slice_image(image: np.ndarray, bounding_boxes: np.ndarray) -> np.ndarray:\n    return [image[bb_ymin:bb_ymax, bb_xmin:bb_xmax] for (bb_xmin, bb_ymin, bb_xmax, bb_ymax) in bounding_boxes]","982d421f":"def get_ids(image: np.ndarray, bounding_boxes: np.ndarray) -> np.ndarray:\n    aligned_images = np.array([\n        align.align_image(image_slice) for image_slice in slice_image(image, bounding_boxes)\n    ])\n\n    return class_embeddings.predict_ids(aligned_images)","d839051f":"last_bounding_boxes = None\n\ndef make_dataframe(filename: str):\n    global last_bounding_boxes\n    \n    filename_full_name = pathlib.Path(filename).name\n    print(f\"Now processing {filename_full_name}\")\n    \n    filename_letter_name = filename_full_name.split('.')[0]\n\n    opened_image = Image.open(filename)\n\n    # Compute bounding boxes, then convert them to numpy format.\n    _, tensor_boxes = detect(opened_image, face_model, transform)\n    bounding_boxes = np.absolute(tensor_boxes.detach().numpy().astype(\"int\"))\n    last_bounding_boxes = bounding_boxes\n    \n    # There's probably a way to do this in a single matrix computation with numpy, \n    # but \u00af\\_(\u30c4)_\/\u00af\n    # Basically, DETR sometimes returns double boxes for the same face, so we're trying to get\n    # rid of those by finding boxes that are *really* close but not identical.\n    flagged_indices = []\n    for current_index, current_box in enumerate(bounding_boxes):\n        for checking_index, checking_box in enumerate(bounding_boxes[current_index:]):\n            distance = np.linalg.norm(current_box - checking_box)\n            if distance > 0 and distance < 100:\n                flagged_indices.append(checking_index)\n    print(flagged_indices)\n    \n    print(bounding_boxes.shape)\n    \n    predicted_ids = get_ids(np.array(opened_image), bounding_boxes)\n\n    new_df = pd.concat(\n        [\n            pd.DataFrame([[\n                filename_full_name, predicted_ids[i], f\"{filename_letter_name}_{predicted_ids[i]}\",\n                bounding_boxes[i][0], bounding_boxes[i][2], bounding_boxes[i][1], bounding_boxes[i][3]\n            ]], columns=['filename', 'person_id', 'id', 'xmin', 'xmax', 'ymin', 'ymax'] ) for i in range(len(predicted_ids))\n        ], \n        ignore_index = True\n    )\n\n    return new_df\n\ndef files_to_df(input_files: list):\n    return pd.concat([make_dataframe(filename) for filename in input_files], ignore_index=True)","8b2cdb67":"print(\"Now processing the images\")\n\ndataframes = {filename: make_dataframe(filename) for filename in [\"\/kaggle\/input\/fire-198-competition-dataset\/a.jpg\", \"\/kaggle\/input\/fire-198-competition-dataset\/b.jpg\", \"\/kaggle\/input\/fire-198-competition-dataset\/c.jpg\", \"\/kaggle\/input\/fire-198-competition-dataset\/d.jpg\"]}\npd.concat([dataframes[filename] for filename in dataframes]).to_csv(SUBMISSION_PATH)","139dd06c":"for filename in dataframes:\n    dataframe = dataframes[filename]\n    image = plt.imread(filename)\n    plt.figure(figsize=(12, 12))\n    plt.imshow(image)\n    ax = plt.gca()\n    \n    for index, row in dataframe.iterrows():\n        name = class_embeddings.mappings[row['person_id']]\n        \n        bb_top_left = (row['xmin'], row['ymin'])\n        bb_bottom_right = (row['xmax'], row['ymax'])\n        \n        ax.add_patch(\n            plt.Rectangle(\n                (row['xmin'], row['ymin']), \n                (row['xmax'] - row['xmin']), \n                (row['ymax'] - row['ymin']),\n                fill=False,\n                color=(0, 0, 1),\n                linewidth=3)\n        )\n        ax.text(row['xmin'], row['ymin'], name.replace(\"_\", \"\"), fontsize=15, color='white',\n                bbox=dict(facecolor='blue', alpha=0.5))\n    \n\n    plt.show()","d0c5a160":"if CLEAN_UP:\n    !rm -rf \/kaggle\/working\/face_recognition \/kaggle\/working\/images_dir \/kaggle\/working\/detr \/kaggle\/working\/.txt \/kaggle\/working\/*checkpoint* \/kaggle\/working\/*.dat \/kaggle\/working\/*.h5 \/kaggle\/working\/*.hdf5 \/kaggle\/working\/*.npy \/kaggle\/working\/mapping.csv \/kaggle\/working\/pairs.txt","cc142991":"# Facial Recognition\nNow begins the facial recognition part of this project.","c072117d":"# Downloads\nDownload any necessary modules and data.","45ba78c8":"## Embedding Model Preparation\nWe start out by training a facial recognition model, or perhaps just loading it from precomputed weights.","9804b422":"# Notebook Configuration\nThis is where parameters for this notebook can be tweaked.","a8c6cae1":"# Prediction\n\nWe can now use the model that we built earlier, in addition to DETR, to create predictions. We define functions for slicing the image using DETR bounding boxes, producing IDs, and writing the result to CSV.","40b9ddf9":"### The Model\nThis is where we either load or train the model. Either way, at the culmination of this set of cells, we have a working model that we can simply call \npredict with.","308d5c9c":"# Face Detection\nThis section will identify the faces in a given photo and generate the bounding boxes of each face","61dc3656":"### Class Embeddings\nNow, we obtain embeddings for the face of each person in the class. If we're not recomputing embeddings, then this just loads an existing file.","33b5b8b2":"# Clean Up\n\nWe conclude by getting rid of every file with the exception of the submission.","860f02b5":"### Data Management\/Preprocessing\nWe declare a set of classes that provide image alignment, data augmentation, and batching functionality."}}