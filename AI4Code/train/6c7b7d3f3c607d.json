{"cell_type":{"fc688a0d":"code","39478408":"code","d97d43c3":"code","27f322c1":"code","94b3ef1f":"code","abce975e":"code","2a33d3bf":"code","552a621e":"code","7eedd0d0":"code","7179c677":"markdown"},"source":{"fc688a0d":"import numpy as np\nimport pandas as pd\nimport time","39478408":"np.random.seed(2)","d97d43c3":"N_STATES = 6\nACTIONS = ['left', 'right'] \nEPSILON = 0.9  \nALPHA = 0.1    \nGAMMA = 0.9    \nMAX_EPISODES = 13  \nFRESH_TIME = 0.3   \n","27f322c1":"def build_q_table(n_states, actions):\n    table = pd.DataFrame(\n        np.zeros((n_states, len(actions))),\n        columns=actions,  )\n\n    return table\n","94b3ef1f":"def choose_action(state, q_table):\n    state_actions = q_table.iloc[state, :]\n    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):\n        action_name = np.random.choice(ACTIONS)\n    else: \n        action_name = state_actions.idxmax()\n    return action_name","abce975e":"def get_env_feedback(S, A):\n    if A == 'right':   \n        if S == N_STATES - 2: \n            S_ = 'terminal'\n            R = 1\n        else:\n            S_ = S + 1\n            R = 0\n    else:   # move left\n        R = 0\n        if S == 0:\n            S_ = S \n        else:\n            S_ = S - 1\n    return S_, R","2a33d3bf":"\ndef update_env(S, episode, step_counter):\n  \n    env_list = ['-']*(N_STATES-1) + ['T']  \n    if S == 'terminal':\n        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n        print('\\r{}'.format(interaction), end='')\n        time.sleep(2)\n        print('\\r                                ', end='')\n    else:\n        env_list[S] = 'o'\n        interaction = ''.join(env_list)\n        print('\\r{}'.format(interaction), end='')\n        time.sleep(FRESH_TIME)","552a621e":"def rl():\n    # main part of RL loop\n    q_table = build_q_table(N_STATES, ACTIONS)\n    for episode in range(MAX_EPISODES):\n        step_counter = 0\n        S = 0\n        is_terminated = False\n        update_env(S, episode, step_counter)\n        while not is_terminated:\n\n            A = choose_action(S, q_table)\n            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n            q_predict = q_table.loc[S, A]\n            if S_ != 'terminal':\n                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n            else:\n                q_target = R     # next state is terminal\n                is_terminated = True    # terminate this episode\n\n            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n            S = S_  # move to next state\n\n            update_env(S, episode, step_counter+1)\n            step_counter += 1\n    return q_table","7eedd0d0":"\n\nif __name__ == \"__main__\":\n    q_table = rl()\n    print('\\r\\nQ-table:\\n')\n    print(q_table)","7179c677":"This replicate is made to enhance my research into Reinforcement Learning. I followed this youtube video to understand it better.\nThe code is not mine. Its solely for me to test out\n\nhttps:\/\/www.youtube.com\/watch?v=gWNeMs1Fb8I"}}