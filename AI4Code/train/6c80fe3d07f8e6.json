{"cell_type":{"bc272b95":"code","32ea0b7b":"code","ca0ef5ba":"code","45e47a3d":"code","c09a3b36":"code","e282c049":"code","6c98980b":"code","a4d32e1a":"code","5a69bf6a":"code","b91b4f5e":"code","2bc427ff":"code","ad28d7d5":"code","a9bdea5f":"code","db90828f":"code","170024d0":"code","b56769fc":"code","1efb6806":"code","3106a44a":"code","0de472cd":"code","1692c70e":"code","587eea6d":"code","a18df780":"code","fe2c1924":"code","30e51c2e":"code","14dad9a7":"code","a588033b":"code","14596e9a":"code","c01611c6":"code","6e6b0cbe":"code","ba4469e2":"code","33feacf4":"code","18a24d88":"code","4cb5f911":"code","b797069d":"code","f3aef303":"markdown","a46ae4d2":"markdown","5d10b3d8":"markdown","f15e5812":"markdown","5d14008d":"markdown","7754fc61":"markdown","909e2a42":"markdown","d773a63e":"markdown"},"source":{"bc272b95":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nfrom random import shuffle  \n \n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n\n#preprocess.\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n#dl libraraies\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\n\n# specifically for cnn\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Flatten,Activation\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\n ","32ea0b7b":"class_names=['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']","ca0ef5ba":"X=[]\nZ=[]\nIMG_SIZE=150\nFLOWER_DAISY_DIR='..\/input\/flowers-recognition\/flowers\/daisy'\nFLOWER_SUNFLOWER_DIR='..\/input\/flowers-recognition\/flowers\/sunflower'\nFLOWER_TULIP_DIR='..\/input\/flowers-recognition\/flowers\/tulip'\nFLOWER_DANDI_DIR='..\/input\/flowers-recognition\/flowers\/dandelion'\nFLOWER_ROSE_DIR='..\/input\/flowers-recognition\/flowers\/rose'","45e47a3d":"def assign_label(img,flower_type):\n    return flower_type","c09a3b36":"def make_train_data(flower_type,DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label=assign_label(img,flower_type)\n        path = os.path.join(DIR,img)\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        \n        X.append(np.array(img))\n        Z.append(str(label))\n        ","e282c049":"# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\nimport cv2                  \nfrom tqdm import tqdm\nimport os                     ","6c98980b":"make_train_data('Daisy',FLOWER_DAISY_DIR)\nprint(len(X))","a4d32e1a":"make_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\nprint(len(X))","5a69bf6a":"make_train_data('Tulip',FLOWER_TULIP_DIR)\nprint(len(X))","b91b4f5e":"make_train_data('Dandelion',FLOWER_DANDI_DIR)\nprint(len(X))","2bc427ff":"make_train_data('Rose',FLOWER_ROSE_DIR)\nprint(len(X))","ad28d7d5":"def display_random_image(class_names, images, labels):\n    \"\"\"\n        Display a random image from the images array and its correspond label from the labels array.\n    \"\"\"\n    \n    index = np.random.randint(len(labels))\n    plt.figure()\n    plt.imshow(images[index])\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(True)\n    plt.title('Image #{} : '.format(index) + labels[index])\n    plt.show()","a9bdea5f":"display_random_image(class_names, X, Z)\n","db90828f":"le=LabelEncoder()\nY=le.fit_transform(Z)\nY=to_categorical(Y,5)\nX=np.array(X)\nX=X\/255","170024d0":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)","b56769fc":"x_train.shape","1efb6806":"np.random.seed(42)","3106a44a":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(5, activation = \"softmax\"))","0de472cd":"batch_size=128\nepochs=50\n\nfrom keras.callbacks import ReduceLROnPlateau\nred_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)\n","1692c70e":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","587eea6d":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","a18df780":"model.summary()","fe2c1924":"History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size)","30e51c2e":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","14dad9a7":"plt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","a588033b":"# getting predictions on val set.\npred=model.predict(x_test)\npred_digits=np.argmax(pred,axis=1)","14596e9a":"test_digits = np.argmax(y_test,axis=1)","c01611c6":"print(\"Accuracy : {}\".format(accuracy_score(test_digits, pred_digits)))","6e6b0cbe":"pred_labels = le.inverse_transform(pred_digits)","ba4469e2":"display_random_image(class_names, x_test, pred_labels)\n","33feacf4":"def display_examples(class_names, images, labels):\n    \"\"\"\n        Display 25 images from the images array with its corresponding labels\n    \"\"\"\n    \n    fig = plt.figure(figsize=(10,10))\n    fig.suptitle(\"Some examples of images of the dataset\", fontsize=16)\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(images[i], cmap=plt.cm.binary)\n        plt.xlabel(class_names[labels[i]])\n    plt.show()","18a24d88":"def print_mislabeled_images(class_names, test_images, test_labels, pred_labels):\n    \"\"\"\n        Print 25 examples of mislabeled images by the classifier, e.g when test_labels != pred_labels\n    \"\"\"\n    BOO = (test_labels == pred_labels)\n    mislabeled_indices = np.where(BOO == 0)\n    mislabeled_images = test_images[mislabeled_indices]\n    mislabeled_labels = pred_labels[mislabeled_indices]\n\n    title = \"Some examples of mislabeled images by the classifier:\"\n    display_examples(class_names,  mislabeled_images, mislabeled_labels)","4cb5f911":"print_mislabeled_images(class_names, x_test, test_digits, pred_digits)\n","b797069d":"CM = confusion_matrix(test_digits, pred_digits)\nax = plt.axes()\nsns.heatmap(CM, annot=True, \n           annot_kws={\"size\": 10}, \n           xticklabels=class_names, \n           yticklabels=class_names, ax = ax)\nax.set_title('Confusion matrix')\nplt.show()","f3aef303":"# Error analysis\n","a46ae4d2":"# Introduction\n\nWe will be using the [Flowers Recognition](https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition) database.\n\nThis dataset contains 4242 images of flowers.\nThe data collection is based on the data flicr, google images, yandex images.\n\nThe pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion.\nFor each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!","5d10b3d8":"# EDA","f15e5812":"# Modelling using CNN.","5d14008d":"# Objective\nwe can use this datastet to recognize plants from the photo usning **Convolutional Neural Networks**.","7754fc61":"# Conclusion: The classifier has trouble with 1 kind of images.\nIt has trouble with `rose and tulip`. it can detects dandelion very accurately!","909e2a42":"# Possible flaws and next steps","d773a63e":"We have overfitting problem and we can face it by :\n1. using more data\n2. regularization(L2,dropout,Data_augmentation)\n3. find better neural network Architecture\n\n`or` we can stop at 20 epochs as shown in the model complexity curve"}}