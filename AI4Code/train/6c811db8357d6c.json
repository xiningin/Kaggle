{"cell_type":{"2a6fab72":"code","2ec7884e":"code","136e15aa":"code","a1eb04a6":"code","61102f74":"code","7469bc8c":"code","3823dfb3":"code","4135e961":"code","22c16b5d":"code","3ec86bc5":"code","28258396":"code","b7ab6d22":"code","ce94293e":"code","55851d2f":"code","629118e6":"code","64217a6a":"code","431f3f8c":"code","2866ad42":"code","2474fecc":"code","0d2d1fde":"code","913fd458":"code","7e954034":"code","ac202e25":"code","0604ef9f":"code","b230c78a":"code","845648b0":"code","5f24aae6":"code","3cc15030":"code","8dc16b04":"code","e21ec715":"code","9fdb478d":"code","90e9e4be":"code","b562397f":"code","ffeb8806":"code","6bd758c6":"code","2b681bab":"code","c1128b8f":"code","a32521c2":"code","3e277a9c":"code","a68384fe":"code","cc77cf2d":"code","550d3948":"code","1a212b87":"code","1a44341c":"code","40e09b51":"code","b1637992":"code","bb06d59e":"code","6b1b1171":"markdown","43071da7":"markdown","af8ff4ac":"markdown","f2f02a76":"markdown","d16fef2f":"markdown","6d6df53e":"markdown","0f4b9112":"markdown","42a18fb0":"markdown","cdaaf3af":"markdown","3a5bdb9a":"markdown","6e38528e":"markdown","1d3e1997":"markdown","e7bc8bc1":"markdown","b24498b2":"markdown","97520a61":"markdown","3a910b30":"markdown","f421d973":"markdown","422a8c26":"markdown","4a1aad1d":"markdown","bcff1dde":"markdown","7b544135":"markdown","66ac7f07":"markdown","df5a0893":"markdown","04077fd8":"markdown","31a6c9a1":"markdown","b06cfd3a":"markdown","dabb2ff6":"markdown"},"source":{"2a6fab72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2ec7884e":"print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","136e15aa":"df_train.columns","a1eb04a6":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(df_train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(df_test.shape))\n\n#Save the 'Id' column\ntrain_ID = df_train['Id']\ntest_ID = df_test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(df_train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(df_test.shape))","61102f74":"sns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()\n\nprint(\"Skewness before log transform = \" + str(df_train['SalePrice'].skew()))\nprint(\"Kurtosis before log transform = \" + str(df_train['SalePrice'].kurt()))","7469bc8c":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()\n\nprint(\"Skewness after after log transform = \" + str(df_train['SalePrice'].skew()))\nprint(\"Kurtosis after after log transform = \" + str(df_train['SalePrice'].kurt()))","3823dfb3":"df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","4135e961":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\n\nplt.subplots(0,0,figsize=(15,3))\n\n\nall_data.isnull().mean().sort_values(ascending=False).plot.bar(color='black')\nplt.axhline(y=0.1, color='r', linestyle='-')\nplt.title('(Pre Removal)Missing values average per column --->', fontsize=20, weight='bold' )\nplt.show()\n\n\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nplt.subplots(0,0,figsize=(15,3))\nall_data.isnull().mean().sort_values(ascending=False).plot.bar(color='green')\nplt.axhline(y=0.1, color='r', linestyle='-')\nplt.title('(Post Removal)Missing values average per column --->', fontsize=20, weight='bold' )\nplt.show()\n\n","22c16b5d":"NA=all_data[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','GarageYrBlt','BsmtFinType2','BsmtFinType1','BsmtCond', 'BsmtQual','BsmtExposure', 'MasVnrArea','MasVnrType','Electrical','MSZoning','BsmtFullBath','BsmtHalfBath','Utilities','Functional','Exterior1st','BsmtUnfSF','Exterior2nd','TotalBsmtSF','GarageArea','GarageCars','KitchenQual','BsmtFinSF2','BsmtFinSF1','SaleType']]","3ec86bc5":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","28258396":"NAcatTest=all_data.select_dtypes(include='object')\nNAnumTest=all_data.select_dtypes(exclude='object')\nprint('We have :',NAcatTest.shape[1],'categorical features with missing values')\nprint('We have :',NAnumTest.shape[1],'numerical features with missing values')","b7ab6d22":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","ce94293e":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","55851d2f":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","629118e6":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","64217a6a":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","431f3f8c":"fig = plt.figure(figsize=(20,20))\nax1 = plt.subplot2grid((6,2),(0,0))\nplt.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(0,1))\nplt.scatter(x=df_train['TotalBsmtSF'], y=df_train['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(1,0))\nplt.scatter(x=df_train['1stFlrSF'], y=df_train['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(1,1))\nplt.scatter(x=df_train['MasVnrArea'], y=df_train['SalePrice'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(2,0))\nplt.scatter(x=df_train['GarageArea'], y=df_train['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(2,1))\nplt.scatter(x=df_train['TotRmsAbvGrd'], y=df_train['SalePrice'], color=('tan'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(3,0))\nplt.scatter(x=df_train['EnclosedPorch'], y=df_train['SalePrice'], color=('crimson'),alpha=0.5)\nplt.axvline(x=400, color='r', linestyle='-')\nplt.title('EnclosedPorch - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(3,1))\nplt.scatter(x=df_train['OpenPorchSF'], y=df_train['SalePrice'], color=('gray'),alpha=0.5)\nplt.axvline(x=470, color='r', linestyle='-')\nplt.title('OpenPorchSF - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(4,0))\nplt.scatter(x=df_train['LotArea'], y=df_train['SalePrice'], color=('skyblue'),alpha=0.9)\nplt.axvline(x=90000, color='r', linestyle='-')\nplt.title('LotArea  - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(4,1))\nplt.scatter(x=df_train['BedroomAbvGr'], y=df_train['SalePrice'], color=('mediumorchid'),alpha=0.5)\nplt.axvline(x=7.5, color='r', linestyle='-')\nplt.title('BedroomAbvGr - Price scatter plot', fontsize=15, weight='bold' )\nplt.tight_layout(0.85)\nplt.xticks(weight='bold')\nplt.show()","2866ad42":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n\n#saleprice correlation matrix\nk = 8#number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","2474fecc":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show()","0d2d1fde":"Num=corrmat['SalePrice'].sort_values(ascending=False).head(10).to_frame()\ncm = sns.light_palette(\"cyan\", as_cmap=True)\n\ncorrplt = Num.style.background_gradient(cmap=cm)\ncorrplt","913fd458":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","7e954034":"from sklearn.preprocessing import LabelEncoder\ncols = ('BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","ac202e25":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","0604ef9f":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","b230c78a":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.5\nfor feature in skewed_features:\n    all_data[feature] = boxcox1p(all_data[feature], lam)","845648b0":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","5f24aae6":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","3cc15030":"df_train = all_data[:ntrain]\ndf_test = all_data[ntrain:]","8dc16b04":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n    rmse= np.sqrt(-cross_val_score(model, df_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","e21ec715":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n","9fdb478d":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","90e9e4be":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","b562397f":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ffeb8806":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6bd758c6":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.005,\n                                   max_depth=4, max_features='sqrt',\n                                 min_samples_leaf=15, min_samples_split=5, \n                                   loss='huber', random_state =5)\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2b681bab":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c1128b8f":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","a32521c2":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n    ","3e277a9c":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a68384fe":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","cc77cf2d":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","550d3948":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1a212b87":"stacked_averaged_models.fit(df_train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(df_train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(df_test.values))\nprint(rmsle(y_train, stacked_train_pred))","1a44341c":"model_xgb.fit(df_train, y_train)\nxgb_train_pred = model_xgb.predict(df_train)\nxgb_pred = np.expm1(model_xgb.predict(df_test))\nprint(rmsle(y_train, xgb_train_pred))","40e09b51":"model_lgb.fit(df_train, y_train)\nlgb_train_pred = model_lgb.predict(df_train)\nlgb_pred = np.expm1(model_lgb.predict(df_test.values))\nprint(rmsle(y_train, lgb_train_pred))","b1637992":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","bb06d59e":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)\nprint(\"Submitted Successfully ...!\")","6b1b1171":"Fill the features more than 10 percent of the missing data with None Values.","43071da7":"> When working with statistics, it\u2019s important to recognize the different types of data: numerical (discrete and continuous), categorical, and ordinal. Data are the actual pieces of information that you collect through your study. For example, if you ask five of your friends how many pets they own, they might give you the following data: 0, 2, 1, 4, 18. (The fifth friend might count each of her aquarium fish as a separate pet.) Not all data are numbers; let\u2019s say you also record the gender of each of your friends, getting the following data: male, male, female, male, female.\n\n> Most data fall into one of two groups: numerical or categorical.\n\n* Numerical data. These data have meaning as a measurement, such as a person\u2019s height, weight, IQ, or blood pressure; or they\u2019re a count, such as the number of stock shares a person owns, how many teeth a dog has, or how many pages you can read of your favorite book before you fall asleep. (Statisticians also call numerical data quantitative data.)\n\n* Numerical data can be further broken into two types: discrete and continuous.\n\n    * Discrete data represent items that can be counted; they take on possible values that can be listed out. The list of possible values may be fixed (also called finite); or it may go from 0, 1, 2, on to infinity (making it countably infinite). For example, the number of heads in 100 coin flips takes on values from 0 through 100 (finite case), but the number of flips needed to get 100 heads takes on values from 100 (the fastest scenario) on up to infinity (if you never get to that 100th heads). Its possible values are listed as 100, 101, 102, 103, . . . (representing the countably infinite case).\n\n    * Continuous data represent measurements; their possible values cannot be counted and can only be described using intervals on the real number line. For example, the exact amount of gas purchased at the pump for cars with 20-gallon tanks would be continuous data from 0 gallons to 20 gallons, represented by the interval [0, 20], inclusive. You might pump 8.40 gallons, or 8.41, or 8.414863 gallons, or any possible number from 0 to 20. In this way, continuous data can be thought of as being uncountably infinite. For ease of recordkeeping, statisticians usually pick some point in the number to round off. Another example would be that the lifetime of a C battery can be anywhere from 0 hours to an infinite number of hours (if it lasts forever), technically, with all possible values in between. Granted, you don\u2019t expect a battery to last more than a few hundred hours, but no one can put a cap on how long it can go (remember the Energizer Bunny?).\n\n* Categorical data: Categorical data represent characteristics such as a person\u2019s gender, marital status, hometown, or the types of movies they like. Categorical data can take on numerical values (such as \u201c1\u201d indicating male and \u201c2\u201d indicating female), but those numbers don\u2019t have mathematical meaning. You couldn\u2019t add them together, for example. (Other names for categorical data are qualitative data, or Yes\/No data.)\n\n> **Ordinal data mixes numerical and categorical data. The data fall into categories, but the numbers placed on the categories have meaning. For example, rating a restaurant on a scale from 0 (lowest) to 4 (highest) stars gives ordinal data. Ordinal data are often treated as categorical, where the groups are ordered when graphs and charts are made. However, unlike categorical data, the numbers do have mathematical meaning. For example, if you survey 100 people and ask them to rate a restaurant on a scale from 0 to 4, taking the average of the 100 responses will have meaning. This would not be the case with categorical data.**\n\nWe achieve the following by either using Label Encoder or Zero Hot encoding(thorugh pd.get_dummies). It also reduces the complexity for operations on dataset. Convert all the categorical variables into numerical data.","af8ff4ac":"Now split the data set into train and test, which is now ready for preprocssing.","f2f02a76":"Check for null values in the dataset.If its zero we can move forward in treating categorical and numerical variables in the dataset.","d16fef2f":"Check different values of lamda. Usually follows tukey's transformation(Lamda = 0, 0.5, 1, 2) as all the features are positively skewed we can go for lambda = 0 or 0.5. ). The output generated as 0.5 gave the lowest RMSE score after different regressions was applied on the dataset.","6d6df53e":"3. Optional Step : Feature Engineering. We will add all floor areas to compute total area. Creative Feature engineering can lead to great results, but one has to develop the proper context and the defination of the problem before one can go into creative feature engineering. ","0f4b9112":"Visualise the graphs of the features we found highly correlated and look for outliers if any in the EDA step mentioned after this.","42a18fb0":"**Data Cleaning**. This will remove null values and outliers from the dataset. We will also transform the target variable if it has high skewness and kurtosis. Four Assumptions must be tested for both the target and input variable. We will deal with transformation of input variable at the later stages in the notebook after removal of outliers and filling of missing values. These are the fundamanetal principles of multivariate analysis:\n* Normality - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue.\n* Homoscedacity - dependent variables show equal level of variance across the predictor variables.\n* Linearity- The relationship between two variables(They can be either directly proportional or inversely proportional).\n* Absence of correlated errors - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n\nSteps to clean the data for data transformation :\n* Drop the SalePrice(target variable) as we have to predict that variable in the data cleaning process of the predictor variables.\n* Split the dataset into categorical and numerical features. \n* Drop NAn values from the dataset and hot encode the categorical features for efficient dataset operations in the further stages.\n* Declare the categorical Features below.\n* Avoid mixing of train and test data, Conduct seperate operations on them.","cdaaf3af":"Checking the shape of train and test data to get an idea of how many samples we are dealing with.","3a5bdb9a":"Merge the training data and test data. Remove the id and Saleprice before merging. I also dropped a outlier which i found in the later visualisations as before putting the saleprice in ytrain variable make sure it is transformed and clean from outliers.","6e38528e":"Lets start with initialising the data and playing with different features of it.","1d3e1997":"Fill the columns with None or zero accordingly. I have done it on my own analysis. Like some houses dont have garage in them so i have filled them with zero and there metafeatures like 'GarageQual' to be None. This is a pure choice of how you want to view the dataset and can be done differently. Eliminate Nan values completly from the dataset. ","e7bc8bc1":"Stacking models\nSimplest Stacking approach : Averaging base models\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)\n\n**Averaged base models class**","b24498b2":"**Stacking Averaged models Score**\n\nTo make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.","97520a61":"**Data Preprocssing and Machine Learning**","3a910b30":"I really loved notebooks people published for this problem. This is inspired by a collection of them with some minor tweaks.\nThe notebook will comprise of five steps which will enable us to predict the sale price feature with least RMSE error(As mentioned in the Evaluation).\n1. Data cleaning(Transfomation)\n2. Exploratory Data Analysis\n3. Feature Engineering(Optional)\n4. Data Preprocssing \n5. Pedict the sale price for a given ID by applying different machine learning models.","f421d973":"We take the weighted sum of the predictions of the three models to form a ensmeble model and submit the ensemble predictions in a submission.csv file.","422a8c26":"No missing value found. Data cleaning done successfully.","4a1aad1d":"**Averaged base models score**\n\nWe just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","bcff1dde":"Transform the target variable if skewness is greater than 1 or smaller than -1(which represents highly skewed data).\nHigh kurtosis(greater than 3) represents presence of outliers. \nFor positive skewness, log transformation works really well.\n* Skewness -->\n   The deviation from normal behaviour of the curve. For analysis and manipulation of data, skewness is a great                  measure in defining the quality and quantity of data given for the particular problem to be solved. Even after transformation your data maybe highly skewed which tells you the high noise(This means high non linearity.Also can be due to less quantity of data present). Skewness, in statistics, is the degree of distortion from the symmetrical bell curve in a probability distribution. Distributions can exhibit right (positive) skewness or left (negative) skewness to varying degrees. Investors note skewness when judging a return distribution because it, like kurtosis, considers the extremes of the data set rather than focusing solely on the average. In short, high skewness or kurtosis leads to erroneous predictions.\n   \ufffc![image.png](attachment:image.png)\n* Kurtosis \u2014> \n    It describes the distribution of data around an average.\n    It measures tailedness rather than peakedness.\n\nThree types of kurtosis \u2014>\n1. Mesokurtic\n2. Leptokurtic(presence of outliers)\n3. Platykurtic(Paucity of outliers)\n\n\ufffc![image.png](attachment:image.png)","7b544135":"**Exploratory Data Analysis.**\n> Form a corelation heatmap of the dataset. Play with it and list the highly correlated features. Also check the correlation with the target variable. Multiple high correlated features gives a rise to a problem of multi-collinearity. ","66ac7f07":"We get again a good score by adding a meta learner.","df5a0893":"Machine Learning. Apply different models and the model with the lowest RMSE is submitted. Lasso and Ridge penalise and avoid the problem of multicollinearity. Lasso performed better as it eliminates the some features from the prediction. Elastic Net was also tuned to a Lasso close model and it gave a low RMSE. Others are applied accordingly.","04077fd8":"Inspect some features and see there relationships in the dataset.","31a6c9a1":"Dropping the outliers as suggested from visualisations. Sort and check for index to drop outliers. Be careful in dropping outlier as it directly effects your preditction directly. Tune outliers out of the dataset by directly seeing with the RMSE after you have applied the model.","b06cfd3a":"It is positively skewed. We have to apply log transformation when the feature is positively skewed(Log transformation performs the best. But others can be applied and tested. Lesser the skewness better the prediction). These are derived from the fundamental Tukey's fundamental power transformation which has the scaled version called as Box Cox Transformation. It has a value Lambda, equating that to zero leads to the famous log transformation. For further details read about, Tukeys Power Transformation ladder and Box Cox Transformation but it requires prequisite knowledge of Multivariate Analysis.\nhttp:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html","dabb2ff6":"It seems even the simplest stacking approach really improve the score . This encourages us to go further and explore a less simple stacking approch.\n\nLess simple Stacking : Adding a Meta-model\nIn this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\n* The procedure, for the training part, may be described as follows:\n\n* Split the total training set into two disjoint sets (here train and .holdout )\n\n* Train several base models on the first part (train)\n\n* Test these base models on the second part (holdout)\n\n* Use the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.\n![image.png](attachment:image.png)\n\n\nWe can stack and apply permutaion and combination of almost all the models and pick the combination which gives the lowest RMSE using this technique.\n\n**Stacking averaged Models Class**"}}