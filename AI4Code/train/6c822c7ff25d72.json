{"cell_type":{"c5283055":"code","fd6f05f6":"code","8175b24c":"code","7bed7db1":"code","d3ed9079":"code","800ce79f":"code","f5e2ab92":"code","516498fd":"code","e91b6a2c":"code","26275d48":"code","34daaed7":"code","65820c04":"code","b3b79b5e":"code","c7f70bd5":"code","029cea4d":"code","a7e8c1c8":"code","941ca81c":"code","4119ee2d":"code","f0a6f40c":"code","f636b35f":"code","fa9aaae1":"code","6e3b881a":"code","a4be9420":"code","060de0b3":"code","253b6b29":"code","6197f1e8":"code","b02c85ce":"code","a9623b77":"code","4070a973":"code","f32aaacb":"code","88b9bdaa":"code","2f36f6ea":"code","6ecab682":"markdown","26da857e":"markdown","89c62b66":"markdown","945736bb":"markdown","0edadbdd":"markdown","630152df":"markdown","323f7f7e":"markdown","aea03734":"markdown"},"source":{"c5283055":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.xkcd()\nimport seaborn as sns\ncolor = sns.color_palette()\n\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nDATA_PATH = '..\/input\/abzoobaassessmentdata\/'","fd6f05f6":"from sklearn.model_selection import StratifiedShuffleSplit, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,make_scorer,r2_score\nfrom sklearn.inspection import plot_partial_dependence\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api  as sm\nfrom scipy import stats\nfrom scipy.stats import norm\n\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\nfrom sklearn.linear_model import Lasso, Ridge, SGDRegressor,LinearRegression,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,RobustScaler\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import ExtraTreeRegressor,DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nimport lightgbm as lgb","8175b24c":"preprocessed_train = pd.read_csv(DATA_PATH + 'preprocessed_train.csv')\npreprocessed_test = pd.read_csv(DATA_PATH + 'preprocessed_test.csv')\nsubmission = pd.read_csv(DATA_PATH + 'sample_submission.csv')","7bed7db1":"preprocessed_train.head()","d3ed9079":"print(f\"Train Rows and columns {preprocessed_train.shape[0]}, {preprocessed_train.shape[1]}\")\nprint(f\"Test Rows and columns {preprocessed_test.shape[0]}, {preprocessed_test.shape[1]}\")","800ce79f":"X = preprocessed_train.drop('rent', axis=1)\ny =  preprocessed_train['rent']","f5e2ab92":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","516498fd":"print(\"Shape of X_train :\", X_train.shape)\nprint(\"Shape of X_test :\", X_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","e91b6a2c":"scaler  = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","26275d48":"## Scoring Metrics as RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)","34daaed7":"%%time\nclfs = []\n\nclfs.append((\"LinearRegression\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LinearRegression())])))\n\nclfs.append((\"XGB\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBRegressor())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsRegressor())]))) \n\nclfs.append((\"DTR\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeRegressor())]))) \n\nclfs.append((\"RFRegressor\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestRegressor())]))) \n\nclfs.append((\"GBRegressor\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingRegressor())]))) \n\n\nscoring = 'r2'\nn_folds = 10\nmsgs = []\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=None)\n    cv_results = cross_val_score(model, X, y, \n                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    msgs.append(msg)\n    print(msg)","65820c04":"lr = LinearRegression()\nlr.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\ny_train_pred_lr = lr.predict(X_train)\ny_test_pred_lr = lr.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred_lr,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred_lr,y_test))\n\n\n\n# Plot residuals\nplt.scatter(y_train_pred_lr, y_train_pred_lr - y_train, c = \"red\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_lr, y_test_pred_lr - y_test, c = \"green\", marker = \"o\", label = \"Validation data\")\nplt.title(\"Linear Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred_lr, y_train, c = \"red\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_lr, y_test, c = \"green\", marker = \"o\", label = \"Validation data\")\nplt.title(\"Linear Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","b3b79b5e":"dt_lr = DecisionTreeRegressor(random_state=42)\ndt_lr.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(dt_lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(dt_lr).mean())\ny_train_pred_svr = dt_lr.predict(X_train)\ny_test_pred_svr = dt_lr.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred_svr,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred_svr,y_test))\n\n\n\n# Plot residuals\nplt.scatter(y_train_pred_svr, y_train_pred_svr - y_train, c = \"blue\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_svr, y_test_pred_svr - y_test, c = \"green\", marker = \"o\", label = \"Validation data\")\nplt.title(\"Decision Tree Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred_svr, y_train, c = \"blue\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_svr, y_test, c = \"green\", marker = \"o\", label = \"Validation data\")\nplt.title(\"Decision Tree Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","c7f70bd5":"knn_lr = KNeighborsRegressor()\nknn_lr.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(knn_lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(knn_lr).mean())\ny_train_pred_knn = knn_lr.predict(X_train)\ny_test_pred_knn = knn_lr.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred_knn,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred_knn,y_test))\n\n\n\n# Plot residuals\nplt.scatter(y_train_pred_knn, y_train_pred_knn - y_train, c = \"red\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_knn, y_test_pred_knn - y_test, c = \"blue\", marker = \"o\", label = \"Validation data\")\nplt.title(\"KNN Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred_knn, y_train, c = \"red\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_knn, y_test, c = \"blue\", marker = \"o\", label = \"Validation data\")\nplt.title(\"KNN Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","029cea4d":"rfr = RandomForestRegressor(n_estimators=100, random_state=42)\nrfr.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(rfr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(rfr).mean())\ny_train_pred = rfr.predict(X_train)\ny_test_pred = rfr.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred,y_test))\n\n\n# Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"o\", label = \"Validation data\")\nplt.title(\"RF Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"o\", label = \"Validation data\")\nplt.title(\"RF Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","a7e8c1c8":"params = {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 2,'learning_rate': 0.05, 'loss': 'ls'}\nGBoost = GradientBoostingRegressor(**params)\nGBoost.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(GBoost).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(GBoost).mean())\ny_train_pred_gb = GBoost.predict(X_train)\ny_test_pred_gb = GBoost.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred_gb,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred_gb,y_test))\n\n\n# Plot residuals\nplt.scatter(y_train_pred_gb, y_train_pred_gb - y_train, c = \"red\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred_gb, y_test_pred_gb - y_test, c = \"green\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Gradient Boosting Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred_gb, y_train, c = \"red\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred_gb, y_test, c = \"green\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Gradient Boosting Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","941ca81c":"params =  {'learning_rate':0.05, 'max_depth':6,'n_estimators':100,\"random_state\":42}\nmodel_xgb = xgb.XGBRegressor(**params)\nmodel_xgb.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(model_xgb).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(model_xgb).mean())\ny_train_pred_xgb = model_xgb.predict(X_train)\ny_test_pred_xgb = model_xgb.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred_xgb,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred_xgb,y_test))\n\n\n# Plot residuals\nplt.scatter(y_train_pred_xgb, y_train_pred_xgb - y_train, c = \"blue\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_xgb, y_test_pred_xgb - y_test, c = \"lightgreen\", marker = \"o\", label = \"Validation data\")\nplt.title(\"XGradient Boosting Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred_xgb, y_train, c = \"blue\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_xgb, y_test, c = \"lightgreen\", marker = \"o\", label = \"Validation data\")\nplt.title(\"XGradient Boosting Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","4119ee2d":"model_lgb = lgb.LGBMRegressor(objective='regression',learning_rate=0.05, n_estimators=100)\nmodel_lgb.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(model_lgb).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(model_lgb).mean())\ny_train_pred_lgb = model_lgb.predict(X_train)\ny_test_pred_lgb = model_lgb.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred_lgb,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred_lgb,y_test))\n\n\n# Plot residuals\nplt.scatter(y_train_pred_lgb, y_train_pred_lgb - y_train, c = \"red\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_lgb, y_test_pred_lgb - y_test, c = \"lightgreen\", marker = \"o\", label = \"Validation data\")\nplt.title(\"LG Boosting Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred_lgb, y_train, c = \"red\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_lgb, y_test, c = \"lightgreen\", marker = \"o\", label = \"Validation data\")\nplt.title(\"LG Boosting Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","f0a6f40c":"!pip install optuna -q","f636b35f":"import optuna","fa9aaae1":"def objective(trial):\n    \n    param = {\n        'metric': 'rmse', \n        'random_state': 101,\n        'n_estimators': trial.suggest_categorical('n_estimators', [100,200,300,400,500,600,700,800,900,1000]),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.001,0.008,0.01,0.02,0.03,0.04,0.05]),\n        'max_depth': trial.suggest_categorical('max_depth', [1,2,3,4,5,6,7,8,9,10]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100)\n    }\n    model = lgb.LGBMRegressor(**param)  \n    \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(X_test)\n    \n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    \n    return rmse","6e3b881a":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","a4be9420":"## parameter Importance for study\noptuna.visualization.plot_param_importances(study)","060de0b3":"tuned_params=study.best_params   \ntuned_params","253b6b29":"tuned_params['random_state'] = 101 \ntuned_params['metric'] = 'rmse'","6197f1e8":"model_lgb_tuned = lgb.LGBMRegressor(**tuned_params)\nmodel_lgb_tuned.fit(X_train, y_train)\n\n\nprint(\"RMSE on Training set :\", rmse_cv_train(model_lgb_tuned).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(model_lgb_tuned).mean())\ny_train_pred_lgbt = model_lgb_tuned.predict(X_train)\ny_test_pred_lgbt = model_lgb_tuned.predict(X_test)\nprint(\"R Square score on Train :\", r2_score(y_train_pred_lgbt,y_train))\nprint(\"R Square score on Test :\", r2_score(y_test_pred_lgbt,y_test))\n\n\n# Plot residuals\nplt.scatter(y_train_pred_lgbt, y_train_pred_lgbt - y_train, c = \"violet\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_lgbt, y_test_pred_lgbt - y_test, c = \"red\", marker = \"o\", label = \"Validation data\")\nplt.title(\"LG Boosting Regressor Tuned\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred_lgbt, y_train, c = \"violet\", marker = \"o\", label = \"Training data\")\nplt.scatter(y_test_pred_lgbt, y_test, c = \"red\", marker = \"o\", label = \"Validation data\")\nplt.title(\"LG Boosting Regressor Tuned\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","b02c85ce":"feat_imp = pd.Series(model_lgb_tuned.feature_importances_, index=preprocessed_train.drop(['rent'], axis=1).columns)\nfeat_imp.nlargest(20).plot(kind='bar', figsize=(8,10))","a9623b77":"model_pipeline_final = Pipeline(steps=[('scaling',StandardScaler()),('LGB',lgb.LGBMRegressor(**tuned_params))])\nprint(model_pipeline_final)\nX_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X,y, test_size=0.3, random_state=42)\nmodel_pipeline_final.fit(X_train_p, y_train_p)\nmodel_pipeline_pred = model_pipeline_final.predict(X_test_p)\ny_train_pipe_pred = model_pipeline_final.predict(X_train_p)\ny_test_pipe_pred = model_pipeline_final.predict(X_test_p)\nprint(\"RMSE score:\", np.sqrt(mean_squared_error(y_test_p, y_test_pipe_pred)))","4070a973":"preprocessed_test.head()","f32aaacb":"test_data = pd.read_csv(DATA_PATH + 'test.csv')\ntest_id = test_data['id']\ntest_data_with_id = pd.DataFrame(test_id, columns=['Id'])","88b9bdaa":"preprocessed_test = scaler.transform(preprocessed_test)\ntest_prediction_lgbm=model_lgb_tuned.predict(preprocessed_test)\ntest_prediction_lgbm= test_prediction_lgbm.reshape(-1,1)\ntest_prediction_lgbm =np.exp(test_prediction_lgbm)\ntest_prediction_lgbm = pd.DataFrame(test_prediction_lgbm, columns=['rent'])","2f36f6ea":"result = pd.concat([test_data_with_id,test_prediction_lgbm], axis=1)\nresult.to_csv('submission.csv',index=False)","6ecab682":"## 3.6 Generating Submission file\n### Preprocessed Test data is used to Generate Submission File","26da857e":"### 3.4 Hyper parameter Tuning using Optuna for LG Boosting Regressor\n#### Check out this [link](https:\/\/optuna.org\/) for more info.\n#### Parameters for LGBM taken from - [link](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html)","89c62b66":"#### It is observed that Light Gradient Boosting Regressor gave us minimum RMSE and better R2 Score on train and test set.\n#### Conclusion --> We will go ahead with LGB Regressor as our result of Model Selection. ","945736bb":"### 3.3 Model Selection and Evaluation\n#### Idea here is to Select the best model among available regressors and perform hyper parameter tuning.Finally encapsulating final model to sklearn Pipeline for deployment in real world.\n\n### Model Used-\n* Linear Regression\n* Support Vector Machines\n* KNN Regressor\n* Random Forest\n* Gradient Boosting\n* XGradient Boosting\n* Light GBM\n\n### Scoring Metrics - Root Mean Squared Error","0edadbdd":"## 3.5 Preparing Final Model Pipeline\n### Can be used to serve model on cloud-server using REST API.","630152df":"### 3.2 Scaling Training features for Models","323f7f7e":"## Stage III Predictive Modelling\n### Model Building and Evaluation\n","aea03734":"### 3.1 Train Test Split\n#### Performing Train Test Split with 30% Test Data"}}