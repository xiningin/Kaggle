{"cell_type":{"ea05bac9":"code","f5af2021":"code","556f2a79":"code","446381e6":"code","b05bf246":"code","e49fcbce":"code","15594b8b":"code","2e18b405":"code","9ae113f9":"code","73604db8":"code","2056044e":"code","235d875b":"code","3cd3ae92":"code","7ad91d2b":"code","07ae07e4":"code","02591bbd":"code","03704ffc":"code","0f0e211b":"code","70c14aa4":"code","55125ce2":"code","6a83873d":"code","db16dab9":"code","3be34127":"code","b5da307d":"code","b59c88dd":"code","04796d0a":"code","2988fd2b":"code","7e52499c":"code","79072511":"code","6849e90c":"code","3f3d0f90":"code","a684d498":"code","7b4ca9b3":"code","890ff783":"markdown","a7b45cdf":"markdown","c5db2f78":"markdown","3f9aec43":"markdown","682c0448":"markdown","aa88294a":"markdown"},"source":{"ea05bac9":"import warnings\nwarnings.filterwarnings(\"ignore\")","f5af2021":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.models as models \nimport torchvision.transforms as transforms","556f2a79":"with open(\"..\/input\/imagenett\/data\/imagenet_labels.txt\") as f:\n    classes = eval(f.read())","446381e6":"# print(classes)","b05bf246":"transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])","e49fcbce":"evalset = torchvision.datasets.ImageFolder(root = '..\/input\/imagenett\/data\/imagenet', transform=transform)","15594b8b":"batch_size = 1","2e18b405":"evalloader = torch.utils.data.DataLoader(evalset, batch_size = batch_size, shuffle=True)","9ae113f9":"dataiter = iter(evalloader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(images[0].shape)\nprint(labels[0].item())","73604db8":"model = models.vgg16(pretrained=True)","2056044e":"model.eval()","235d875b":"def imshow(img, title):\n    \n    std_correction = np.asarray([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n    mean_correction = np.asarray([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n    npimg = np.multiply(img.numpy(),std_correction) + mean_correction\n    \n    plt.figure(figsize=(batch_size * 4, 4))\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.title(title)\n    plt.show()","3cd3ae92":"def show_batch_images(dataloader):\n    images, _ = next(iter(dataloader))\n    \n    outputs = model(images)\n    _, pred = torch.max(outputs.data, 1)\n    \n    img = torchvision.utils.make_grid(images)\n    imshow(img, title=[classes[x.item()] for x in pred])\n    \n    return images, pred","7ad91d2b":"images, pred = show_batch_images(evalloader)","07ae07e4":"outputs = model(images)\nprint(outputs.shape)\noutputs = nn.functional.softmax(outputs, dim=1)\nprob_no_occ, pred = torch.max(outputs.data, 1)\nprob_no_occ = prob_no_occ[0].item()\nprint(prob_no_occ)","02591bbd":"def occlusion(model, image, label, occ_size=50, occ_stride=50, occ_pixel=0.5):\n    \n    width, height = image.shape[-2], image.shape[-1]\n    \n    output_height = int(np.ceil((height-occ_size)\/occ_stride))\n    output_width = int(np.ceil((width-occ_size)\/occ_stride))\n    \n    heatmap = torch.zeros((output_height, output_width))\n    \n    for h in range(0, height):\n        for w in range(0, width):\n            \n            h_start = h*occ_stride\n            w_start = w*occ_stride\n            h_end = min(height, h_start + occ_size)\n            w_end = min(width, w_start + occ_size)\n            \n            if (w_end) >= width or (h_end) >= height:\n                continue\n                \n            input_image = image.clone().detach()\n            input_image[:, :, w_start:w_end, h_start:h_end] = occ_pixel\n            \n            output = model(input_image)\n            output = nn.functional.softmax(output, dim=1)\n            prob = output.tolist()[0][label]\n            \n            heatmap[h, w] = prob\n            \n    return heatmap","03704ffc":"heatmap = occlusion(model, images, pred[0].item(), 32, 14)","0f0e211b":"imgplot = sns.heatmap(heatmap, xticklabels=False, yticklabels=False, vmax = prob_no_occ)","70c14aa4":"alexnet = models.alexnet(pretrained = True)","55125ce2":"print(alexnet)","6a83873d":"def plot_filters_single_channel_big(t):\n    \n    nrows = t.shape[0]*t.shape[2]\n    ncols = t.shape[1]*t.shape[3]\n          \n    npimg = np.array(t.numpy(), np.float32)\n    npimg = npimg.transpose((0, 2, 1, 3))\n    npimg = npimg.ravel().reshape(nrows, ncols)\n    \n    npimg = npimg.T\n    \n    fig, ax = plt.subplots(figsize=(ncols\/10, nrows\/200))    \n    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='Greys', ax=ax, cbar=False)","db16dab9":"def plot_filters_single_channel(t):\n    \n    nplots = t.shape[0]*t.shape[1]\n    ncols = 12\n    nrows = 1 + nplots\/\/ncols\n    \n    npimg = np.array(t.numpy(), np.float32)\n    \n    count = 0\n    \n    fig = plt.figure(figsize=(ncols, nrows))\n    for i in range(t.shape[0]):\n        for j in range(t.shape[1]):\n            count += 1\n            ax1 = fig.add_subplot(nrows, ncols, count)\n            npimg = np.array(t[i, j].numpy(), np.float32)\n            npimg = (npimg - np.mean(npimg)) \/ np.std(npimg)\n            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n            ax1.imshow(npimg)\n            ax1.set_title(str(i) + ',' + str(j))\n            ax1.axis('off')\n            ax1.set_xticklabels([])\n            ax1.set_yticklabels([])\n   \n    plt.tight_layout()\n    plt.show()","3be34127":"def plot_filters_multi_channel(t):\n    \n    num_kernels = t.shape[0]    \n    \n    num_cols = 12\n    num_rows = num_kernels\n    \n    fig = plt.figure(figsize=(num_cols,num_rows))\n    for i in range(t.shape[0]):\n        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n        \n        npimg = np.array(t[i].numpy(), np.float32)\n        npimg = (npimg - np.mean(npimg)) \/ np.std(npimg)\n        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n        npimg = npimg.transpose((1, 2, 0))\n        ax1.imshow(npimg)\n        ax1.axis('off')\n        ax1.set_title(str(i))\n        ax1.set_xticklabels([])\n        ax1.set_yticklabels([])\n\n    plt.tight_layout()\n    plt.show()","b5da307d":"def plot_weights(model, layer_num, single_channel = True, collated = False):\n    \n    layer = model.features[layer_num]\n    if isinstance(layer, nn.Conv2d):\n        weight_tensor = model.features[layer_num].weight.data\n        if single_channel:\n            if collated:\n                plot_filters_single_channel_big(weight_tensor)\n            else:\n                plot_filters_single_channel(weight_tensor)\n        else:\n            if weight_tensor.shape[1] == 3:\n                plot_filters_multi_channel(weight_tensor)\n            else:\n                'Can only plot weights with three channels with single_channel = False'\n    else:\n        print('Can only visualise layers which are convolutional')","b59c88dd":"# plot_weights(alexnet, 0, single_channel = False)","04796d0a":"plot_weights(alexnet, 0, single_channel = True)","2988fd2b":"plot_weights(alexnet, 3, single_channel = True)","7e52499c":"plot_weights(alexnet, 0, single_channel = True, collated = True)","79072511":"plot_weights(alexnet, 3, single_channel = True, collated = True)","6849e90c":"plot_weights(alexnet, 6, single_channel = True, collated = True)","3f3d0f90":"plot_weights(model, 0, single_channel = True, collated = True)","a684d498":"plot_weights(model, 2, single_channel = True, collated = True)","7b4ca9b3":"plot_weights(model, 5, single_channel = True, collated = True)","890ff783":"### Occlusion Analysis","a7b45cdf":"### Filter Visalisation","c5db2f78":"### Visualise Image","3f9aec43":"### Load dataset","682c0448":"### Outline\n1. Using torchvision.datasets with a custom folder of images\n2. Occlusion analysis with pretrained model\n3. Filter visualisation with pretrained model","aa88294a":"### Load pretrained model"}}