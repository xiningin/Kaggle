{"cell_type":{"3e4f7ea0":"code","c7934a25":"code","fb08c9aa":"code","86bb47fd":"code","19a1adc4":"code","59bfb6e2":"code","12a867a5":"code","e4f3c48d":"code","ab6ddf0d":"code","dd4e0145":"code","6afed4bc":"code","4934aa02":"code","87116d61":"code","850a0d82":"code","a56a84bb":"code","6cfda71a":"code","851874a4":"code","005137f3":"code","aeeccdf9":"code","0655f95f":"code","fe88707d":"code","b4c113ad":"code","e3c0a27d":"code","1d7f3002":"code","d6c5678b":"code","2030e4b6":"code","41a71802":"markdown","80db37dd":"markdown","70ddd091":"markdown","583580a2":"markdown","979cca13":"markdown","3d4f1be5":"markdown","6b14cdb6":"markdown","ec9845b1":"markdown","ae460881":"markdown","6311afa6":"markdown","11188f3d":"markdown","62ad2a23":"markdown","245d4b82":"markdown"},"source":{"3e4f7ea0":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchtext\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nimport re\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.nn.utils.rnn import pad_sequence\nimport pytorch_lightning as pl\nfrom torch.nn.functional import binary_cross_entropy_with_logits, binary_cross_entropy\nfrom torchmetrics import Accuracy, F1\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nimport string\nimport statistics\nimport nltk","c7934a25":"nltk.download('stopwords')\nnltk.download('wordnet')","fb08c9aa":"class MODEL_EVAL_METRIC:\n    accuracy = \"accuracy\"\n    f1_score = \"f1_score\"\n\nclass Config:\n    VOCAB_SIZE = 0\n    BATCH_SIZE = 512\n    EMB_SIZE = 300\n    OUT_SIZE = 2\n    NUM_FOLDS = 5\n    NUM_EPOCHS = 20\n    NUM_WORKERS = 8\n    # Whether to update the pretrained embedding weights during training process\n    EMB_WT_UPDATE = True\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n    FAST_DEV_RUN = False    \n    PATIENCE = 6    \n    IS_BIDIRECTIONAL = True\n    # model hyperparameters\n    MODEL_PARAMS = {\n        \"hidden_size\": 141, \n        \"num_layers\": 2,         \n        \"drop_out\": 0.4258,\n        \"lr\": 0.000366,\n        \"weight_decay\": 0.00001\n    }\n\nDATA_PATH = \"\/kaggle\/input\/nlp-getting-started\/\"    \n    \n# For results reproducibility \n# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\npl.seed_everything(42, workers=True)","86bb47fd":"df_train = pd.read_csv(DATA_PATH + \"train.csv\")\ndf_test = pd.read_csv(DATA_PATH + \"test.csv\")\nprint(f\"Rows in train.csv = {len(df_train)}\")\nprint(f\"Rows in test.csv = {len(df_test)}\")\npd.set_option('display.max_colwidth', None)\ndf_train.head()","19a1adc4":"df_train_pos = df_train[df_train.target == 1]\ndf_train_neg = df_train[df_train.target == 0]\nprint(f\"No. of positive training examples = {len(df_train_pos)}\")\nprint(f\"No. of negative training examples = {len(df_train_neg)}\")\ntrain_keywords_unique = df_train.keyword.unique()\nprint(f\"No. of unique keywords = {len(train_keywords_unique)}\")\ndf_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\nprint(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")","59bfb6e2":"# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n# on the data. We use stratified kfold if the target distribution is unbalanced\ndef strat_kfold_dataframe(df, target_col_name, num_folds=5):\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    # randomize of shuffle the rows of dataframe before splitting is done\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    # get the target data\n    y = df[\"target\"].values\n    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n        df.loc[val_index, \"kfold\"] = fold\n    return df        \n\ndf_train = strat_kfold_dataframe(df_train, target_col_name=\"target\", num_folds=5)    ","12a867a5":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\ndef clean_special_chars(text, punct):\n    for p in punct:\n        text = text.replace(p, ' ')\n    return text\n\ndef process_tweet(df, text, keyword):\n    lemmatizer = WordNetLemmatizer()    \n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n    processed_text = []\n    stop = stopwords.words(\"english\")\n    for tweet, keyword in zip(df[text], df[keyword]):\n        tweets_clean = []        \n        # remove stock market tickers like $GE\n        #tweet = tweet + \" \" + keyword\n        tweet = re.sub(r'\\$\\w*', '', tweet)\n        # remove old style retweet text \"RT\"\n        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n        # remove hyperlinks\n        tweet = re.sub(r'http\\S+', '', tweet)\n        # remove hashtags\n        # only removing the hash #, @, ... sign from the word\n        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n        tweet = clean_special_chars(tweet, punct)\n        # remove junk characters which don't have an ascii code\n        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n        # tokenize tweets        \n        tweet_tokens = tokenizer.tokenize(tweet)\n        for word in tweet_tokens:\n            # remove stopwords and punctuation\n            #if (word.isalpha() and len(word) > 2 and word not in stop \n            #    and word not in string.punctuation):\n                #stem_word = stemmer.stem(word)  # stemming word            \n                #lem_word = lemmatizer.lemmatize(word)\n                #tweets_clean.append(lem_word) \n                tweets_clean.append(word)\n        processed_text.append(\" \".join(tweets_clean))        \n    df['processed_text'] = np.array(processed_text)","e4f3c48d":"# Fill in missing values\ndf_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\ndf_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\nprocess_tweet(df_train, 'text', \"keyword\")\nprocess_tweet(df_test, 'text', \"keyword\")\n# length of the processed tweet\ndf_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\ndf_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\ndf_train.iloc[50:52, :]","ab6ddf0d":"# Load the fasttext word embedding for tweets\nFASTTEXT_EMB_FILE = \"\/kaggle\/input\/fasttext-2m-word-embeddings\/wiki.en.vec\"\nemb = torchtext.vocab.Vectors(name=FASTTEXT_EMB_FILE, cache=\".\/vector_cache\")","dd4e0145":"# build tweets vocab from training data\ndef yield_tokens(df):\n    for index, row in df.iterrows():\n        yield row[\"processed_text\"].split()\n    \ntweet_vocab = build_vocab_from_iterator(yield_tokens(df_train), specials=[\"<unk>\", \"<pad>\"])   \nConfig.VOCAB_SIZE = len(tweet_vocab)","6afed4bc":"# For the problem specific vocab, get the embedding vectors from the pre-trained embedding\n# for each word in vocab and return a matrix of shape vocab_size, embedding_dim. This matrix\n# will be the pretrained embedding weight matrix which we will use to create the embedding layer\ndef get_vocab_pt_emb_matrix(text_vocab, emb):\n    embedding_matrix = []\n    for token in text_vocab.get_itos():\n        embedding_matrix.append(emb.get_vecs_by_tokens(token))\n    return torch.stack(embedding_matrix)\n\npt_emb_weights = get_vocab_pt_emb_matrix(tweet_vocab, emb)\npt_emb_layer = nn.Embedding.from_pretrained(pt_emb_weights)","4934aa02":"# vectorize the processed tweet, i.e. replace each token in the tweet with its corresponding index\n# in the tweet vocab\ndf_train[\"vectorized_tweet\"] = df_train[\"processed_text\"].apply(\n    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n    )","87116d61":"class VectorizedTweetDataSet(Dataset):\n    def __init__(self, tweet_vecs, labels):\n        self.tweet_vecs = tweet_vecs\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        tweet_vec = self.tweet_vecs[idx]\n        label = self.labels[idx]\n        tweet_len = len(tweet_vec)\n        return (tweet_vec, label)\n","850a0d82":"def get_fold_dls(fold, df):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    X_train = train_df[\"vectorized_tweet\"].to_numpy()\n    y_train = train_df[\"target\"].to_numpy()\n    X_valid = valid_df[\"vectorized_tweet\"].to_numpy()\n    y_valid = valid_df[\"target\"].to_numpy()\n    ds_train = VectorizedTweetDataSet(X_train, y_train)\n    ds_valid = VectorizedTweetDataSet(X_valid, y_valid)\n    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n    return dl_train, dl_valid","a56a84bb":"# If the goal is to train with mini-batches, one needs to pad the sequences in each batch. \n# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n# one needs to pad every sequence with a length of smaller than L with zeros and make their \n# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n# descending order.\ndef pad_collate(batch):\n    # Each element in the batch is a tuple (data, label)\n    # sort the batch (based on tweet word count) in descending order\n    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n    sequences = [x[0] for x in sorted_batch]\n    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n    # Also need to store the length of each sequence.This is later needed in order to unpad \n    # the sequences\n    seq_len = torch.Tensor([len(x) for x in sequences])\n    labels = torch.Tensor([x[1] for x in sorted_batch])\n    return sequences_padded, seq_len, labels\n","6cfda71a":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass DisasterModel(nn.Module):\n    \"\"\"The RNN model.\"\"\"\n    def __init__(self, vocab_size, num_layers, is_bidirect, emb_size, hidden_size, output_size, \n                pt_emb_weights, emb_wt_update=False, drop_prob=0.5):\n        super().__init__()        \n        self.vocab_size = vocab_size\n        self.num_layers = num_layers        \n        # size of the embedding vector\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size   \n        self.output_dim = output_size\n        self.is_bidirect = is_bidirect\n        # Embedding layer\n        self.emb_layer = nn.Embedding(self.vocab_size, emb_size)\n        # copy the vocab specific weights(emb vectors) from pretrained embeddings to model embedding layer\n        self.emb_layer.weight.data.copy_(pt_emb_weights)    \n        # whether to update the pretrained embedding layer weights during model training\n        self.emb_layer.weight.requires_grad = emb_wt_update            \n        # LSTM Layer        \n        self.lstm_layer = nn.LSTM(\n                        input_size=emb_size, \n                        hidden_size=hidden_size, \n                        batch_first=True, \n                        bidirectional=is_bidirect, \n                        num_layers=num_layers, \n                        dropout=drop_prob\n                        )\n        self.dropout = nn.Dropout(p = drop_prob)                        \n        \n        # If the RNN is bidirectional `num_directions` should be 2, else it should be 1.        \n        if not is_bidirect:\n            self.num_directions = 1\n            self.linear = nn.Linear(self.hidden_size, self.output_dim)\n        else:       \n            self.num_directions = 2     \n            self.linear = nn.Linear(self.hidden_size * self.num_directions, self.output_dim)\n        # The activation layer which converts output to 0 or 1            \n        self.act = nn.Sigmoid()            \n\n    def forward(self, inputs, input_lengths, state):        \n        # inputs = [batch_size, batch_max_seq_length]        \n        # embeds is of shape batch_size * num_steps * emb_dim and is the input to lstm layer\n        embeds = self.emb_layer(inputs)        \n        batch_size = inputs.shape[0]        \n        # embeds = [batch_size, max_seq_length, emb_dim]\n        # pack_padded_sequence before feeding into LSTM. This is required so pytorch knows\n        # which elements of the sequence are padded ones and ignore them in computation.\n        # This step is done only after the embedding step\n        embeds_pack = pack_padded_sequence(embeds, input_lengths.to(\"cpu\"), batch_first=True)                \n        lstm_out_pack, (h_n, c_n) = self.lstm_layer(embeds_pack)\n        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n        # unpack the output\n        lstm_out, lstm_out_len = pad_packed_sequence(lstm_out_pack, batch_first=True)        \n        #print(f\"lstm_out.shape = {lstm_out.shape}\")\n        #print(f\"lstm_out_len.shape = {lstm_out_len.shape}\")\n        # lstm_out = [batch_size, max_seq_length, hidden_size * num_directions]\n        if self.is_bidirect:            \n            # each batch item has different seq length, so to select the hidden state at t_end for each batch item\n            # a for comprehension like below is needed, a vectorized operation doesn't seem plausible\n            #lstm_out = [lstm_out[batch_item_index, seq_length_index-1, :] for batch_item_index, seq_length_index in enumerate(lstm_out_len)]            \n            #lstm_out = torch.cat(lstm_out, dim=0).reshape(batch_size, 4 * self.hidden_size)\n            #print(f\"lstm_out.shape = {lstm_out.shape}\")\n            # Another way to extract the last hidden state for the forward and backward lstm layers\n            # in a BiRNN is to use h_n like this\n            h_tend_fwd = h_n[-2, :, :]\n            h_tend_bwd = h_n[-1, :, :]\n            lstm_out = torch.cat((h_tend_fwd, h_tend_bwd), dim=1)\n            #print(f\"lstm_out.shape = {lstm_out.shape}\")\n        else:                        \n            lstm_out = h_n[-1, :, :]                    \n        \n        out = self.dropout(lstm_out)                \n        output = self.linear(out)        \n        # apply sigmoid activation to convert output to probability \n        output = self.act(output)\n        # [batch_size, 2]\n        return output\n\n    def init_state(self, batch_size=1):\n        \"\"\" Initialize the hidden state i.e. initialize all the neurons in all the hidden layers \n        to zero\"\"\"\n        if not isinstance(self.lstm_layer, nn.LSTM):\n            # `nn.GRU` takes a tensor as hidden state\n            return torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n        else:\n            # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n            # hidden state for each element in the batch, c0 = initial cell state\n            # for each element in the batch\n            return (torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n                    torch.zeros((self.num_directions * self.num_layers,batch_size, self.hidden_size)))","851874a4":"class DisasterTweetLitModel(pl.LightningModule):\n    def __init__(self, vocab_size, emb_size, output_size, pt_emb_weights, emb_wt_update, \n                hparams, model_eval_metric=MODEL_EVAL_METRIC.accuracy):\n        super().__init__()\n        #self.save_hyperparameters()\n        self.lr = hparams[\"lr\"]\n        self.weight_decay = hparams[\"weight_decay\"]\n        self.model_eval_metric = model_eval_metric\n        self.network = DisasterModel(\n            vocab_size = vocab_size,\n            num_layers = hparams[\"num_layers\"],\n            is_bidirect = Config.IS_BIDIRECTIONAL,\n            emb_size = emb_size,\n            hidden_size = hparams[\"hidden_size\"],\n            output_size = output_size,\n            pt_emb_weights = pt_emb_weights,\n            emb_wt_update = emb_wt_update,\n            drop_prob = hparams[\"drop_out\"]\n        )\n\n    def forward(self, tweets, tweet_lengths, state):\n        return self.network(tweets, tweet_lengths, state)\n\n    def configure_optimizers(self):\n        model_optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n        return {\n            \"optimizer\": model_optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": lr_scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }\n\n    def training_step(self, batch, batch_idx):\n        tweets, tweet_lengths, targets = batch\n        # initialize the hidden and cell state of the LSTM\n        h0, c0 = self.network.init_state()\n        targets_pred = self(tweets, tweet_lengths, (h0, c0))        \n        #print(f\"targets_pred.shape = {targets_pred.shape}\")\n        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n        loss_targets = loss_targets.float()        \n        train_loss = binary_cross_entropy(targets_pred, loss_targets)\n        train_metric = None\n        train_metric_str = \"\"\n        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:            \n            targets_pred = torch.argmax(targets_pred, dim=1)            \n            train_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n            train_metric_str = \"train_acc\"\n        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n            train_metric = F1(targets_pred, targets)            \n            train_metric_str = \"train_f1\"\n        self.log(\"train_loss\", train_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        self.log(train_metric_str, train_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        return train_loss\n\n    def validation_step(self, batch, batch_idx):\n        tweets, tweet_lengths, targets = batch\n        # initialize the hidden and cell state of the LSTM\n        h0, c0 = self.network.init_state()\n        targets_pred = self(tweets, tweet_lengths, (h0, c0))\n        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n        loss_targets = loss_targets.float()        \n        val_loss = binary_cross_entropy(targets_pred, loss_targets)\n        val_metric = None\n        val_metric_str = \"\"\n        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:\n            targets_pred = torch.argmax(targets_pred, dim=1)\n            val_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n            val_metric_str = \"val_acc\"\n        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n            val_metric = F1(targets_pred, targets)            \n            val_metric_str = \"val_f1\"\n        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        self.log(val_metric_str, val_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        return val_loss","005137f3":"from pytorch_lightning.callbacks import Callback\nfrom pytorch_lightning import LightningModule, Trainer\n# Monitor multiple metric values that are calculated either in training or validation step and return the\n# best metric values for each epoch\nclass MetricsAggCallback(Callback):\n    def __init__(self, train_metrics_to_monitor, val_metrics_to_monitor):\n        # dictionary with metric name as key and monitor mode (min, max) as the value\n        # ( the same names used to log metric values in training and validation step)\n        self.val_metrics_to_monitor = val_metrics_to_monitor\n        self.train_metrics_to_monitor = train_metrics_to_monitor\n        # dictionary with metric_name as key and list of metric value for each epoch\n        self.train_metrics = {metric: [] for metric in train_metrics_to_monitor.keys()}\n        self.val_metrics = {metric: [] for metric in val_metrics_to_monitor.keys()}\n        # dictionary with metric_name as key and the best metric value for all epochs\n        self.train_best_metric = {metric: None for metric in train_metrics_to_monitor.keys()}\n        self.val_best_metric = {metric: None for metric in val_metrics_to_monitor.keys()}\n        # dictionary with metric_name as key and the epoch number with the best metric value\n        self.train_best_metric_epoch = {metric: None for metric in train_metrics_to_monitor.keys()}     \n        self.val_best_metric_epoch = {metric: None for metric in val_metrics_to_monitor.keys()}     \n        self.epoch_counter = 0           \n\n    @staticmethod\n    def process_metrics(metrics_to_monitor, metrics, best_metric, best_metric_epoch, trainer):\n        metric_str = \"\"\n        for metric, mode in metrics_to_monitor.items():\n            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n            metric_str += f\"{metric} = {metric_value}, \"\n            metrics[metric].append(metric_value)\n            if mode == \"max\":\n                best_metric[metric] = max(metrics[metric])            \n            elif mode == \"min\":            \n                best_metric[metric] = min(metrics[metric])            \n            best_metric_epoch[metric] = metrics[metric].index(best_metric[metric]) \n        print(metric_str[:-2])\n\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n        self.epoch_counter += 1        \n        self.process_metrics(self.train_metrics_to_monitor, self.train_metrics, self.train_best_metric, self.train_best_metric_epoch, trainer)\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):        \n        print(f\"For epoch {self.epoch_counter}\")\n        self.process_metrics(self.val_metrics_to_monitor, self.val_metrics, self.val_best_metric, self.val_best_metric_epoch, trainer)\n","aeeccdf9":"def run_training(fold, dl_train, dl_val, pt_emb_weights, find_lr=True):\n    fold_str = f\"fold{fold}\"\n    print(f\"Running training for {fold_str}\")\n    disaster_tweet_model = DisasterTweetLitModel(\n        vocab_size=Config.VOCAB_SIZE,\n        emb_size=Config.EMB_SIZE,\n        output_size=Config.OUT_SIZE,\n        pt_emb_weights=pt_emb_weights,\n        emb_wt_update=Config.EMB_WT_UPDATE,\n        hparams=Config.MODEL_PARAMS,\n        model_eval_metric=Config.MODEL_EVAL_METRIC                \n        )\n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n    chkpt_file_name = fold_str + \"_best_model_{epoch}_{val_loss:.4f}\"\n    train_metrics_to_monitor = {\n        \"train_loss\": \"min\",\n        \"train_acc\": \"max\"\n    }\n    val_metrics_to_monitor = {\n        \"val_loss\": \"min\",\n        \"val_acc\": \"max\",\n        }\n    loss_chkpt_callback = ModelCheckpoint(dirpath=\".\/model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n    metric_chkpt_callback = MetricsAggCallback(train_metrics_to_monitor, val_metrics_to_monitor)\n    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n    trainer = pl.Trainer(\n        gpus = 1,\n        deterministic = True,\n        auto_select_gpus = True,\n        progress_bar_refresh_rate = 20,\n        max_epochs = Config.NUM_EPOCHS,\n        logger = tb_logger,\n        auto_lr_find = True,    \n        #precision = Config.PRECISION,   \n        fast_dev_run = Config.FAST_DEV_RUN, \n        gradient_clip_val = 1.0,        \n        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n    )        \n    if find_lr:\n        trainer.tune(model=disaster_tweet_model, train_dataloaders=dl_train)\n        print(disaster_tweet_model.lr)\n    trainer.fit(disaster_tweet_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n    fold_train_metrics = {\n        metric: (metric_chkpt_callback.train_best_metric[metric], metric_chkpt_callback.train_best_metric_epoch[metric]) \n        for metric in train_metrics_to_monitor.keys()\n    }\n    fold_val_metrics = {\n        metric: (metric_chkpt_callback.val_best_metric[metric], metric_chkpt_callback.val_best_metric_epoch[metric]) \n        for metric in val_metrics_to_monitor.keys()\n    }            \n    best_model = loss_chkpt_callback.best_model_path\n    del trainer, disaster_tweet_model, loss_chkpt_callback, metric_chkpt_callback \n    return fold_train_metrics, fold_val_metrics, best_model","0655f95f":"find_lr = True\nall_fold_val_loss = []\nall_fold_val_acc = []\n\nfor fold in range(Config.NUM_FOLDS):\n    dl_train, dl_val = get_fold_dls(fold, df_train)\n    fold_train_metrics, fold_val_metrics, chkpt_file_name = run_training(fold, dl_train, dl_val, pt_emb_weights, find_lr=False)    \n    all_fold_val_loss.append((fold_val_metrics[\"val_loss\"][0], chkpt_file_name))\n    all_fold_val_acc.append(fold_val_metrics[\"val_acc\"][0])\n    print(f\"Best train metrics values for fold{fold}\")    \n    print(fold_train_metrics)\n    print(f\"Best val metrics values for fold{fold}\")    \n    print(fold_val_metrics)        ","fe88707d":"all_fold_val_loss","b4c113ad":"fold_val_loss_sorted = sorted(all_fold_val_loss, key=lambda x:x[0])\nall_fold_val_loss = [item[0] for item in all_fold_val_loss]\nprint(f\"val loss across folds = {all_fold_val_loss}\")\nprint(f\"val accuracy across folds = {all_fold_val_acc}\")\nmean_loss = statistics.mean(all_fold_val_loss)\nmean_acc = statistics.mean(all_fold_val_acc)\nstd_loss = statistics.stdev(all_fold_val_loss)\nstd_acc = statistics.stdev(all_fold_val_acc)\nprint(f\"mean val loss across folds = {mean_loss}, val loss stdev across fold = {std_loss}\")\nprint(f\"mean val accuracy across folds = {mean_acc}, val accuracy stdev across fold = {std_acc}\")","e3c0a27d":"all_fold_val_loss","1d7f3002":"fold_val_loss_sorted","d6c5678b":"import os\nos.getcwd()","2030e4b6":"best_model_across_folds = fold_val_loss_sorted[0][1]\nprint(f\"Using best model = {best_model_across_folds} for prediction on test set\")\nbest_model = DisasterTweetLitModel.load_from_checkpoint(\n    checkpoint_path=best_model_across_folds,\n    vocab_size=Config.VOCAB_SIZE,\n    emb_size=Config.EMB_SIZE,\n    output_size=Config.OUT_SIZE,\n    pt_emb_weights=pt_emb_weights,\n    emb_wt_update=Config.EMB_WT_UPDATE,\n    hparams=Config.MODEL_PARAMS,\n    model_eval_metric=Config.MODEL_EVAL_METRIC \n    )\ntweet_vocab.set_default_index(0)\ndf_test[\"vectorized_tweet\"] = df_test[\"processed_text\"].apply(\n    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n    )\n\n# Do prediction with best performing model on the test set\ndef predict(df_test):\n    test_output = []\n    for index, row in df_test.iterrows():    \n        vec_tweet = row[\"vectorized_tweet\"]\n        if len(vec_tweet) == 0:\n            test_output.append(0)\n            continue\n        vec_tweet_len = torch.IntTensor([len(vec_tweet)])\n        vec_tweet = vec_tweet.view(1, -1)    \n        #print(vec_tweet, vec_tweet_len)\n        output = best_model(vec_tweet, vec_tweet_len, state=None)\n        #print(output)\n        test_output.append(torch.argmax(output).item())    \n    return test_output        \n\ntest_output = predict(df_test)\nprint(f\"Completed prediction for {len(test_output)} test rows\")\n\ndf_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_submission['target']= test_output\ndf_submission.to_csv('my_submission.csv',index=False)","41a71802":"### Model building starts from here","80db37dd":"### Configuration for training","70ddd091":"### Load the data","583580a2":"### K Fold CV","979cca13":"### Pytorch lightning wrapper for model","3d4f1be5":"### Custom lightning callback \nTo record training and validation metric values at each epoch and the best metric values across all epochs","6b14cdb6":"### Text classification using pytorch lightning\nWe use a bidirectional LSTM and fasttext pretrained word embeddings","ec9845b1":"### Pad the input sequence","ae460881":"### Get train and validation data for a fold","6311afa6":"### Build the model \nWe use multilayer bidirectional LSTM as the encoder, followed by an attention layer\n\n### Bidirectional RNN\noutputs is of size [src len, batch size, hid dim * num directions] where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other\n\nhidden is of size [n layers * num directions, batch size, hid dim], where [-2, :, :] gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and [-1, :, :] gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n","11188f3d":"### Some EDA","62ad2a23":"### Tweet preprocessing","245d4b82":"### Tweet dataset"}}