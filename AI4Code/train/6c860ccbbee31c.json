{"cell_type":{"055accca":"code","82dcbb7b":"code","7eab6361":"code","65ff7ca9":"code","d2adf836":"code","887dadd0":"code","6336a69e":"code","d45fc686":"code","ef08222c":"code","afe7aae9":"code","b08ff9e5":"code","148a85b2":"code","477c95b4":"code","c3ae2555":"code","f12f07f8":"code","7bc4c55a":"code","60bce886":"code","52719c89":"code","61a9456f":"code","0edc86b2":"code","85a05108":"code","315adc95":"code","9cf52936":"code","6d7e0c51":"code","16fd65c0":"code","a4a62831":"code","49e0c5d0":"code","c3d45923":"code","20fe0a72":"code","f2011b7a":"code","ebc45804":"code","5c027233":"code","570fc47a":"code","3ccb518e":"code","bd9018b0":"code","f8ce56c8":"code","c1b94911":"code","99d5339c":"code","d185cb9c":"code","04a1223b":"code","7f2d7d4b":"code","9815ef56":"code","a8f7bf86":"code","7ccb5ef0":"code","252436ea":"code","a0b67763":"code","e8f19b84":"code","bb52699c":"code","775b7b5c":"code","7b612c3b":"code","89bc6781":"code","9c58fb13":"code","459e23fe":"code","5b9ca8e3":"code","aa30ed20":"code","340909d8":"markdown","4a350771":"markdown","d6114a24":"markdown","3151dd33":"markdown","8c052192":"markdown","20624168":"markdown","3314ab98":"markdown","058e7125":"markdown","411db3dd":"markdown","7eb18bd6":"markdown","2b765836":"markdown","7960eceb":"markdown","1acc3709":"markdown","b57353fe":"markdown","38422212":"markdown","f55e47f6":"markdown","e765eaf8":"markdown","356e3d42":"markdown","133c48e7":"markdown","03782e2e":"markdown","80727ad3":"markdown","30ce428e":"markdown","7e7df035":"markdown","ffb95a49":"markdown","a440fbf5":"markdown","5e77e93e":"markdown","bb5138ff":"markdown","3e87d349":"markdown"},"source":{"055accca":"# Importing basic libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","82dcbb7b":"house_data = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\nhouse_data.head()","7eab6361":"house_data.info()","65ff7ca9":"house_data.describe().transpose()","d2adf836":"house_data['date'] = pd.to_datetime(house_data['date'])","887dadd0":"# The feature we want to predict is the price. Lets get a better understanding of that feature in particular.\n\nplt.figure(figsize=(6,8))\nsns.boxplot(y='price',data=house_data)","6336a69e":"# Upper limit of I\n\n6.450000e+05 + 1.5 * (6.450000e+05 - 3.219500e+05)","d45fc686":"house_data[house_data['price']>=1129575.0]['price'].count()","ef08222c":"1146\/len(house_data)*100","afe7aae9":"house_data = house_data[house_data['price']<1129575.0]","b08ff9e5":"# Lets now check the distribution of the price\n\nplt.figure(figsize=(10,6))\nsns.histplot(data=house_data,x='price')","148a85b2":"# Lets now check the correlation of the features with the price\n\nhouse_data.corr()['price'].sort_values()","477c95b4":"# And also lets visualize it\n\nplt.figure(figsize=(8,8))\nsns.heatmap(data=house_data.corr(),cmap='coolwarm')","c3ae2555":"# Livig square foot\n\nplt.figure(figsize=(13,7))\nsns.scatterplot(x='price',y='sqft_living',data=house_data,alpha=0.15)","f12f07f8":"# Grade\n\nsns.countplot(x='grade', data=house_data)","7bc4c55a":"sns.boxplot(x='grade',y='price',data=house_data)","60bce886":"# Waterfront\n# Usually people pay more to live in the waterfront. I'll check if this happens here.\n\nsns.boxplot(x='waterfront',y='price',data=house_data)","52719c89":"plt.figure(figsize=(10,8))\nsns.scatterplot(x='long',y='lat',data=house_data,hue='price',alpha=0.4,palette='coolwarm',edgecolor=None)","61a9456f":"house_data['year'] = house_data['date'].apply(lambda x:x.year)\nhouse_data['month'] = house_data['date'].apply(lambda x:x.month)\n\nhouse_data.head()","0edc86b2":"# Now lets check the relation between price and time\n\nhouse_data.groupby(by='year').mean()['price'].plot()","85a05108":"house_data.groupby(by='month').mean()['price'].plot()","315adc95":"house_data['zipcode'].nunique()","9cf52936":"house_data['zipcode'].unique()","6d7e0c51":"house_data.drop(['zipcode','id','date'],axis=1,inplace=True)","16fd65c0":"house_data.head()","a4a62831":"house_data.columns","49e0c5d0":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error","c3d45923":"X = house_data.drop('price',axis=1)\ny = house_data['price']","20fe0a72":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)","f2011b7a":"# Now I'll create a linear regression model (model_lr).\n\nmodel_lr = LinearRegression()","ebc45804":"# Here I'm fitting the linear regression model with the training set.\n\nmodel_lr.fit(X_train,y_train)","5c027233":"model_lr.coef_","570fc47a":"house_coef = pd.DataFrame(data=model_lr.coef_, index=X.columns, columns=['Coeff'])\nhouse_coef","3ccb518e":"predictions_lr = model_lr.predict(X_test)","bd9018b0":"sns.scatterplot(x=y_test,y=predictions_lr,alpha=0.1)","f8ce56c8":"# MAE (Mean Absolute Error)\nMAE_lr = mean_absolute_error(y_test,predictions_lr)\nMAE_lr","c1b94911":"# MSE (Mean Squared Error)\nMSE_lr = mean_squared_error(y_test,predictions_lr)\nMSE_lr","99d5339c":"# RMSE (Root Mean Squared Error)\nRMSE_lr = np.sqrt(mean_squared_error(y_test,predictions_lr))\nRMSE_lr","d185cb9c":"Xnn = house_data.drop('price',axis=1).values\nynn = house_data['price'].values","04a1223b":"Xnn_train, Xnn_test, ynn_train, ynn_test = train_test_split(Xnn, ynn, test_size=0.3, random_state=None)","7f2d7d4b":"from sklearn.preprocessing import StandardScaler","9815ef56":"scaler = StandardScaler()","a8f7bf86":"Xnn_train = scaler.fit_transform(Xnn_train)\nXnn_test = scaler.transform(Xnn_test)","7ccb5ef0":"# Now lets perform the required imports to create the model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","252436ea":"Xnn_train.shape","a0b67763":"model_nn = Sequential()\n\nmodel_nn.add(Dense(units = 19,activation='relu'))\nmodel_nn.add(Dense(units = 19,activation='relu'))\nmodel_nn.add(Dense(units = 19,activation='relu'))\nmodel_nn.add(Dense(units = 19,activation='relu'))\n\nmodel_nn.add(Dense(units = 1))\n\nmodel_nn.compile(optimizer='adam',loss='mse')","e8f19b84":"model_nn.fit(x=Xnn_train, y=ynn_train,\n          validation_data=(Xnn_test,ynn_test),\n          batch_size=64,\n          epochs = 2000,\n          verbose = 0)","bb52699c":"df_losses = pd.DataFrame(model_nn.history.history)\ndf_losses","775b7b5c":"df_losses.plot()","7b612c3b":"predictions_nn = model_nn.predict(Xnn_test)","89bc6781":"plt.scatter(x=ynn_test,y=predictions_nn,alpha=0.1)","9c58fb13":"# MAE (Mean Absolute Error)\n\nMAE_nn = mean_absolute_error(ynn_test,predictions_nn)\nMAE_nn","459e23fe":"# MSE (Mean Squared Error)\n\nMSE_nn = mean_squared_error(ynn_test,predictions_nn)\nMSE_nn","5b9ca8e3":"# RMSE (Root Mean Squared Error)\n\nRMSE_nn = np.sqrt(mean_squared_error(ynn_test,predictions_nn))\nRMSE_nn","aa30ed20":"data = {'Linear Regression':[\"{:,.2f}\".format(MAE_lr), \"{:,.2f}\".format(MSE_lr), \"{:,.2f}\".format(RMSE_lr)],\n        'Neural Network':[\"{:,.2f}\".format(MAE_nn), \"{:,.2f}\".format(MSE_nn), \"{:,.2f}\".format(RMSE_nn)],\n       'Difference':[\"{:,.2f}\".format(MAE_lr-MAE_nn),\"{:,.2f}\".format(MSE_lr-MSE_nn),\"{:,.2f}\".format(RMSE_lr-RMSE_nn)]}\npd.DataFrame(data,index=['MAE','MSE','RMSE'])","340909d8":"# Comparison\n\nIn the next dataframe you can compare how both models did regarding the prediction of prices.","4a350771":"There are many outliers in the `price` feature. Let's get a look on how many points are labeled as outliers and if we can manage them somehow. I will check the amount of outliers by calculating the upper limit of the interval.\n\nI= (q0.25\u22121.5\u2217IQR ; q0.75+1.5\u2217IQR)\n\nEverything outside that interval is labeled as outlier. The IQR is the interquartile range (the difference between the 3rd and 1st quartile).","d6114a24":"# Linear regression model\n\nLet's first do the required imports.","3151dd33":"We can see how both losses decrease and stabilize. \nI'll now create some predictions using this model, so i can check the metrics and compare them with the ones i got in the Linear Regression model.","8c052192":"The dataset has 21,613 entries and there aren't missing values. I can also notice the `date` column is not in the right data type. I'll fix this right now.","20624168":"Now the dataframe looks like this:","3314ab98":"This goes in line with the increase of house prices with the time, but since I have only 2 years of data, I can't be sure about\na real tendency.\nRegarding the months, i can see a decrease in prices after June, reaching the lowest level in February. But again, since we are dealing with just 2 years of data, I can't trust 100% in this oscilation.\n\nNow, i'll check the zipcode information, which is the feature with almost no correlation with the price in the dataset.","058e7125":"Now that the linear regression model has been created and has been fitted with the training data, I can take a look at the coefficients the model predicted.","411db3dd":"The amount of data points labeled as outlier is 1146, which represents just 5.3% of the dataset. I will procede to drop it in order to build a more accurate prediction model.","7eb18bd6":"When dealing with gradient descent algorithms, it's better to scale the data since it's very sensitive to the range of the data points. There are many types of scalers, but i'll use the Standar Scaler, which will transform the mean of the data to 0 and its variance to 1. ","2b765836":"Once again i will divide the features of the data set between the features that is going to be predicted which is going to be saved in `ynn` and the features that are going to be used to perform the prediction which are going to be saved in `Xnn`. And then I'll apply the train test split as done in the linear regression model.  \n\nNote here i transform the features into NumPy arrays using `.values`. I do this because it appears as a recommendation in Keras documentation for better training of the model.","7960eceb":"Those coefficients indicate the change in the `price` when an increase of 1 unit of the feature occurs, leaving all the other features fixed.  \n\nNow I can use the X_test data in the linear regression model to create predictions.","1acc3709":"# Exploratory data analysis\n\nI'll load the dataset and perform some exploratory data analysis to get a better understanding of the different features.\n","b57353fe":"Now I'll check the date information. Since i fixed the data type, I can perform operations with that feature. For example, it will be helpful to create new columns for the month and year.","38422212":"Using Pandas I'll create a dataframe of the losses of the training and validation sets. Then I'll plot the dataframe for easily visualize the changes.","f55e47f6":"The `grade` and the `sqft_living` features are the ones with the highest correlation with the price. I'll take a look at them.","e765eaf8":"# Introduction and dataset\n\nIn this notebook i'll compare the evaluation on the prediction of house prices in King county, Washington, USA, using Linear Regression and an Artificial Neural Network.\nThe data used is from the houses in King county, USA between 2014 and 2015. \n\nHere is a description of the columns in the dataset.\n\n`id` - Unique ID for each home sold.\n\n`date` - Date of the home sale.\n\n`price` - Price of each home sold.\n\n`bedrooms` - Number of bedrooms.\n\n`bathrooms` - Number of bathrooms, where .5 accounts for a room with a toilet but no shower.\n\n`sqft_living` - Square footage of the apartments interior living space.\n\n`sqft_lot` - Square footage of the land space.\n\n`floors` - Number of floors.\n\n`waterfront` - A dummy variable for whether the apartment was overlooking the waterfront or not.\n\n`view` - An index from 0 to 4 of how good the view of the property was.\n\n`condition` - An index from 1 to 5 on the condition of the apartment.\n\n`grade` - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n\n`sqft_above` - The square footage of the interior housing space that is above ground level.\n\n`sqft_basement` - The square footage of the interior housing space that is below ground level.\n\n`yr_built` - The year the house was initially built.\n\n`yr_renovated` - The year of the house\u2019s last renovation.\n\n`zipcode` - What zipcode area the house is in.\n\n`lat` - Latitude.\n\n`long` - Longitude.\n\n`sqft_living15` - The square footage of interior housing living space for the nearest 15 neighbors.\n\n`sqft_lot15` - The square footage of the land lots of the nearest 15 neighbors.","356e3d42":"Now i will perform the fit and transformation on the `Xnn_train` data.  \nNote i will only perform the transformation in the `Xnn_test` data, in order to prevent data leakage into the model.","133c48e7":"Now that the model has been created, I will proceed to train it. For this i will use 1500 epochs and a batch size of 64. I'll also use validation data in order to check if i am overfitting the model. I'll set verbose = 0 so it will not display a huge output.","03782e2e":"# Neural Network","80727ad3":"I don't know the area enough to infere information with the area location of the zipcodes. Besides, its correlation with the price is almost zero.  \nAnother column i will not use is `id` since it doesn't give any useful information.  \nThe `date` feature is also not important, since I've already extracted the information i required from it.","30ce428e":"Most of houses have are labeled with a grade number 7, which indicates an average on \"construction and design\".\n\nLet's check some other features i consider interesting, like the waterfront, latitud, longitud, and date.","7e7df035":"I can see on average, the houses in the waterfront are more expensive. But the most expensive ones are actually not in the waterfront.\n\nNow, I'll take a look at latitud and longitud. We can plot it in a way to resemble the map of the area.\n","ffb95a49":"Now I'll divide the variables I've just created using `train test split`, in which I'll separate the `X` and `y` variables into a test and a train version. The train version is going to be used to train the linear regression model. Once done this, I'm going to use the `X_test` version to create price predictions that later I'll use to compare with the `y_test` version in order to evaluate how precise was or model.","a440fbf5":"### Linear Regression evaluation\n\nLets check first how the predictions and the true values of the prices (y_test) look together. If I get a straigth line, it would mean the model is excelent.","5e77e93e":"In order to be able to test the prediction level of the linear regression model, I'll separate the dataset into X and y. In \"X\" I'm going to save the features that are going to be used in the model to predict the `price`, and in \"y\" I'm going to save the feature to be predicted.","bb5138ff":"I'll check the shape of `Xnn_train` in order to know the number of features and assign that number as the number of neurons per layer. Except for the last layer which is going to have one neuron since I am looking for a single value: the house price. \n\nThe neural network model is going to be a sequencial dense, which means all the neurons on a layer are connected with the neurons of the next layer.\n\nI will use `relu` (Rectified Linear Unit) as activation function, `adam` (Adaptive Movement) as optimizer, and `mse` (Mean Squared Error) as a way to measure the loss, since it will punish harder the predicted values that are more distant from the real ones.\n\nWhat the optimizer does is to adjust the parameters in order to minimize the cost function.","3e87d349":"That plot has the shape of the King County since i used the longitud and latitud. The red areas show the most expensive\nhouses, so we have an idea now what are the expensive areas in the city."}}