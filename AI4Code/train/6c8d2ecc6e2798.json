{"cell_type":{"7b4a0961":"code","274c3be7":"code","8fb27c38":"code","22bce3c9":"code","8c925b24":"code","2f23652c":"markdown","c4626241":"markdown","df34313a":"markdown"},"source":{"7b4a0961":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","274c3be7":"import pandas as pd\nimport numpy as np\n\nbankrupt = pd.read_csv('\/kaggle\/input\/company-bankruptcy-prediction\/data.csv')\ncolumns = bankrupt.columns\nbankrupt.info()\n\ndef IV_calc(data, var):\n    if data[var].dtypes == 'object':\n        dataf = data.groupby([var])['Bankrupt?'].agg(['count','sum'])\n        dataf.columns = ['Total','bad']\n        dataf['good'] = dataf['Total'] - dataf['bad']\n        dataf['bad_per'] = dataf['bad']\/dataf['bad'].sum()\n        dataf['good_per'] = dataf['good'].dataf['good'].sum()\n        dataf['I_V'] = (dataf['good_per'] - dataf['bad_per'])*np.log(dataf['good_per']\/dataf['bad_per'])\n        return dataf\n    else:\n        data['bin_var'] = pd.qcut(data[var].rank(method='first'),10)\n        dataf = data.groupby(['bin_var'])['Bankrupt?'].agg(['count','sum'])\n        dataf.columns = ['Total','bad']\n        dataf['good'] = dataf['Total'] - dataf['bad']\n        dataf['bad_per'] = dataf['bad']\/dataf['bad'].sum()\n        dataf['good_per'] = dataf['good']\/dataf['good'].sum()\n        dataf['I_V'] = (dataf['good_per'] - dataf['bad_per']) * np.log(dataf['good_per']\/dataf['bad_per'])\n        return dataf","8fb27c38":"discrete_columns = []\ncontinuous_columns = []\n\nfor col in columns:\n    if col != 'Bankrupt?':\n        if bankrupt[col].dtype == 'object':\n            discrete_columns.append(col)\n        else:\n            continuous_columns.append(col)\n\ntotal_columns = discrete_columns + continuous_columns\n\nIv_list = []\nfor col in total_columns:\n    assigned_data = IV_calc(bankrupt,col)\n    iv_val = round(assigned_data['I_V'].sum(),3)\n    dt_type = bankrupt[col].dtypes\n    Iv_list.append((iv_val, col, dt_type))\n\nIv_list = sorted(Iv_list, reverse=True)\n\nfor iv in Iv_list:\n    print(iv[1], iv[2], iv[0])","22bce3c9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\n\nx_train, x_test, y_train, y_test = train_test_split(bankrupt.drop(['Bankrupt?','bin_var'], axis=1), bankrupt['Bankrupt?'], train_size = 0.7, random_state=42)\nrf_fit = RandomForestClassifier(n_estimators = 1000, criterion = 'gini', max_depth = 100, min_samples_split = 3, min_samples_leaf = 2)\nrf_fit.fit(x_train, y_train)\ncrosstab = pd.crosstab(y_train, rf_fit.predict(x_train),rownames = ['Actual'], colnames = ['Predicted'])\n\nprint('Train Accuracy \\n\\n', crosstab)\nprint('\\n\\n Train Accuracy Score', round(accuracy_score(y_train, rf_fit.predict(x_train)),3))\n\nprint('Test Accuracy \\n\\n', crosstab)\nprint('Test Accuracy Score', round(accuracy_score(y_test, rf_fit.predict(x_test)),3))\n\nprecision = crosstab[1][1] \/ (crosstab[1][1] + crosstab[1][0])\nrecall = crosstab[1][1] \/ (crosstab[1][1] + crosstab[0][1])\nf1_score = 2 * (precision * recall) \/ (precision + recall)\n\nprint('\\n\\n Precision: ', round(precision,3))\nprint('Recall: ', round(recall,3))\nprint('f1_score: ', round(f1_score,3))","8c925b24":"import matplotlib.pyplot as plt\n\nrf_fit.fit(x_train, y_train)\nimportances = rf_fit.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf_fit.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\ncolnames = list(x_train.columns)\nprint('Feature Ranking')\nfor f in range(x_train.shape[1]):\n    print('Feature', indices[f], ',',colnames[indices[f]], round(importances[indices[f]], 4))\n    \nplt.figure()\nplt.bar(range(x_train.shape[1]), importances[indices], color='r', yerr = std[indices], align='center')\nplt.ylabel('Feature Importance Value')\nplt.xlabel('Indexed as Importance Value')\nplt.show()","2f23652c":"Random Forest Classifier with Confusion Matrix.\n\nAccuracy, Precision, Recall and F1 Score displayed.","c4626241":"The top 5 most important features are:\n* Net Value Growth Rate - 0.0335\n* Net Income to Stockholder's Equity - 0.0275\n* Persistent EPS in the Last Four Seasons - 0.022\n* Borrowing dependency - 0.0202\n* Non-industry income and expenditure\/revenue - 0.0194\n\nThe feature importance barplot is in order of importance value and not feature ID. ","df34313a":"Preliminary investigation of the data and the definition of an IV Calculation function used in assessing the predictive power of each independent variable against the dependent variable."}}