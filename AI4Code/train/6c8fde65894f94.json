{"cell_type":{"a29dea6b":"code","6c73263f":"code","c1d3ee2e":"code","43c6a61f":"code","bdc322e4":"code","b11df697":"code","d45fe97f":"code","122b3c4e":"code","e71cf6a3":"code","107723d0":"code","82dd7cde":"code","a4d92151":"code","051b8fc7":"code","e227748b":"code","ce979c0e":"code","61a092f5":"code","2f3a14a3":"code","8c0d8ea8":"code","573227ca":"code","0e9f7964":"code","7a2c9a2c":"code","e6d6f6c0":"code","0cfddfab":"code","5ebdcfcf":"code","b71eecc7":"code","b3de453e":"code","89d647a4":"code","86d730b5":"code","a1163bd0":"code","512f0537":"code","b01139af":"code","fd995f92":"code","d77948a6":"code","aee77f3f":"code","315898e9":"code","ac07cfd8":"code","31359443":"code","0b16a090":"code","78bf1f47":"code","29707d17":"code","04f1acb0":"code","191ed452":"code","0cf185fc":"code","2abc3d83":"code","958cc25a":"code","31401533":"code","9e49f114":"code","c724db1e":"code","ba561801":"code","415a1a9d":"code","61c99a4a":"code","270082f2":"code","f70bccb5":"code","15854b55":"code","20b341f4":"code","4a351981":"code","81517e8a":"code","18f42792":"code","467b6856":"code","533838ea":"code","1b9a9496":"code","25a200a1":"code","b023b284":"code","57b2ae73":"code","5527ab6c":"code","a6361d81":"code","172b2bb8":"code","016c9b03":"code","e490b415":"code","3c2c7c6d":"code","e48ff82a":"code","b502e4c7":"code","665e6341":"code","01d14e3f":"code","1b6531de":"code","6d89cf9e":"code","40e04eaa":"code","86f60e74":"code","c0098734":"code","2489ce00":"code","7d965438":"code","0475cd47":"code","23a154cf":"code","113de87b":"code","dd227331":"code","3d6704f0":"markdown","d607dce2":"markdown","66106207":"markdown","f099ec5f":"markdown","c6ad367b":"markdown","3da7bb2d":"markdown","901f871f":"markdown","8beb5483":"markdown","c5becc76":"markdown","16a9878a":"markdown","ef75dced":"markdown","f5e10deb":"markdown","c2c57788":"markdown","2bce5e3c":"markdown","3ff85898":"markdown","7a33c21e":"markdown","ea0ee6f7":"markdown","8b9d4b1c":"markdown","1e6c6573":"markdown","6def2fd2":"markdown","423b4472":"markdown","87892248":"markdown","e32c8073":"markdown","bc86e362":"markdown","be30ce77":"markdown","7a95b5b4":"markdown","b9b30de5":"markdown","222892d7":"markdown","00c2bef3":"markdown","67c844ab":"markdown","c778768a":"markdown","4e7f75f2":"markdown","32aaf4ae":"markdown","31c36c48":"markdown","8111d4dd":"markdown","982b5d8f":"markdown","dfbf730d":"markdown","452209ff":"markdown","6bd7a059":"markdown","5ac2b3a2":"markdown","bf814e73":"markdown","a6a10f52":"markdown","4afdc954":"markdown","0301f4b5":"markdown","1b372092":"markdown","9012b070":"markdown","a3ec9d9b":"markdown","4feb5c13":"markdown","de39ab88":"markdown","e7f1e01f":"markdown","13bae9a4":"markdown","6d40da93":"markdown","cffadcb9":"markdown","8176f84c":"markdown","13161c3c":"markdown","18ca93c0":"markdown","efd0f813":"markdown","51072f1f":"markdown","ab8eb3d2":"markdown","b9af9fed":"markdown","13c532cb":"markdown","04d7975a":"markdown","11851631":"markdown","3789021c":"markdown","5c618c75":"markdown","2c644fd4":"markdown","468d80fb":"markdown","eef588ce":"markdown","10dafe79":"markdown","14304fc4":"markdown","b4408959":"markdown","9cd52eb5":"markdown","dfacbbfd":"markdown","fe75296f":"markdown","789fa495":"markdown","28b32e28":"markdown","f8b9a747":"markdown","1ba8d928":"markdown","305efbba":"markdown","69a57b82":"markdown","774da881":"markdown"},"source":{"a29dea6b":"! pip install langdetect","6c73263f":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport matplotlib.pyplot as plt\n\nfrom langdetect import detect\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\nimport random\n\n\nplt.style.use('ggplot')\n\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\npd.set_option('max_rows', None)","c1d3ee2e":"df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\nval_df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')","43c6a61f":"print(f\"The shape of the Comments to Score dataset is {df.shape} \\n\"\nf\"The shape of the validation dataset is {val_df.shape}\")","bdc322e4":"df.info()","b11df697":"val_df.info()","d45fe97f":"df.head(2)","122b3c4e":"val_df.head(2)","e71cf6a3":"def clean_text(text):\n    text = re.sub(r'<[^<]+?>', '', text)\n    text = text.replace('\\n', ' ')\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'<[^<]+?>', '', text) \n    text = text.replace('(\\xa0)', ' ')\n    text = text.replace('(&lt)', '')\n    text = text.replace('(&gt)', '')\n    text = text.replace(\"\\\\\", \"\")\n    \n    return text","107723d0":"df['text'] = df['text'].apply(clean_text)","82dd7cde":"df.head(2)","a4d92151":"df['language'] = df['text'].apply(detect)","051b8fc7":"count_all_language = df['language'].value_counts()\ncount_language_not_eng = df['language'][df.language != 'en'].value_counts()\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = count_all_language.plot(kind='bar', color = \"#640372\")\nax1.set_title('Frequency of languages in all comments')\nax1.set_xlabel(\"Languages\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = count_language_not_eng.plot(kind='bar', color = \"#640372\")\nax2.set_title('Frequency of languages in non-English comments')\nax2.set_xlabel(\"Languages\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","e227748b":"df['text'][df.language=='de'].head(2)","ce979c0e":"df['text'][df.language=='it'].head(2)","61a092f5":"comment_length = df['text'].apply(len)\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = comment_length.plot(kind='hist', color = \"#640372\", bins=100)\nax1.set_title('Comment Length Distribution')\nax1.set_xlabel(\"Comment Length\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","2f3a14a3":"word_count = df['text'].apply(lambda x: len(str(x).split()))\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = word_count.plot(kind='hist', color = \"#640372\", bins=100)\nax1.set_title('Word Count Distribution')\nax1.set_xlabel(\"Word Count\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","8c0d8ea8":"def get_top_n_words(corpus, n=None, remove_stop_words=False, n_words=1): # if n_words=1 -> unigrams, if n_words=2 -> bigrams..\n    if remove_stop_words:\n        vec = CountVectorizer(stop_words = 'english', ngram_range=(n_words, n_words)).fit(corpus)\n    else:\n        vec = CountVectorizer(ngram_range=(n_words, n_words)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","573227ca":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=False, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","0e9f7964":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","7a2c9a2c":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=True, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","e6d6f6c0":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","0cfddfab":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=False, n_words=2)\nfor word, freq in common_words:\n    print(word, freq)","5ebdcfcf":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","b71eecc7":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=True, n_words=2)\nfor word, freq in common_words:\n    print(word, freq)","b3de453e":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","89d647a4":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=False, n_words=3)\nfor word, freq in common_words:\n    print(word, freq)","86d730b5":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution')\nax1.set_xlabel(\"Trigram\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","a1163bd0":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=True, n_words=3)\nfor word, freq in common_words:\n    print(word, freq)","512f0537":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","b01139af":"sorted(set([\"b\", \"a\"]))","fd995f92":"def get_unique_words(string):\n    string = string.lower()\n    regex = re.compile('[^a-zA-Z]')\n    string = regex.sub(' ', string)\n    words = string.split()\n    new_string = \" \".join(sorted(set(words), key=words.index))\n    return new_string","d77948a6":"df['set_of_words'] = df['text'].apply(get_unique_words)","aee77f3f":"df.head(2)","315898e9":"common_words = get_top_n_words(df['set_of_words'], 20, remove_stop_words=False, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","ac07cfd8":"df_tmp = pd.DataFrame(common_words, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","31359443":"common_words = get_top_n_words(df['set_of_words'], 20, remove_stop_words=True, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","0b16a090":"df_tmp = pd.DataFrame(common_words, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","78bf1f47":"df['text'].iloc[3028]","29707d17":"df['text'].iloc[4949]","04f1acb0":"polarity = df['text'].map(lambda text: TextBlob(text).sentiment.polarity)","191ed452":"fig = plt.figure(figsize=(10,8))\n\nax1 = polarity.plot(kind='hist', color = \"#640372\", bins=100)\nax1.set_title('Polarity Distribution')\nax1.set_xlabel(\"Sentiment\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","0cf185fc":"df['polarity'] = df['text'].map(lambda text: TextBlob(text).sentiment.polarity)\nprint(f\"\"\"A comment with the most neutral polarity: \\n {df['text'][df.polarity == 0].sample(1, random_state=42).values[0]} \\n\nA comment with negative polarity: \\n {df['text'][df.polarity == -1].sample(1, random_state=42).values[0]} \\n\nA comment with positive polarity: \\n {df['text'][df.polarity == 1].sample(1, random_state=42).values[0]}\"\"\")","2abc3d83":"print(\"10 comments with neutral polarity: \\n\")\ncomments = df.loc[df.polarity == 0, ['text']].sample(10, random_state=42).values\nfor comment in comments:\n    print(f\"\"\"- {comment[0]}\\n\"\"\")","958cc25a":"print(\"10 comments with positive polarity: \\n\")\ncomments = df.loc[df.polarity == 1, ['text']].sample(10, random_state=42).values\nfor comment in comments:\n    print(f\"\"\"- {comment[0]}\\n\"\"\")","31401533":"mask = np.array(Image.open(\"..\/input\/wiki-img\/Wikipedia_W.png\"))","9e49f114":"mask = mask[:,:,3]\ntext = df.text.values","c724db1e":"def purple_color_func(word, font_size, position, orientation, random_state=None,\n                    **kwargs):\n    return f\"hsl(312, {random.randint(20, 60)}%, {random.randint(20, 60)}%)\"","ba561801":"wc= WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask,stopwords=set(STOPWORDS))\nwc.generate(\" \".join(text))\nplt.figure(figsize=(15,10))\nplt.axis(\"off\")\nplt.title(\"Word Cloud\", fontsize=20)\nplt.imshow(wc.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nplt.show()","415a1a9d":"mask_joy = np.array(Image.open(\"..\/input\/emoji-imgs\/joy.png\"))\nmask_sad = np.array(Image.open(\"..\/input\/emoji-imgs\/sad.png\"))","61c99a4a":"mask_joy = mask_joy[:,:,1]\nmask_sad = mask_sad[:,:,3]","270082f2":"text_positive_polarity = df[(df.polarity > 0.9)].text.values\ntext_negative_polarity = df[(df.polarity < -0.9)].text.values","f70bccb5":"wc_positive_polarity = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_joy,stopwords=set(STOPWORDS))\nwc_negative_polarity = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_sad,stopwords=set(STOPWORDS))\n\nwc_positive_polarity.generate(\" \".join(text_positive_polarity))\nwc_negative_polarity.generate(\" \".join(text_negative_polarity))\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_positive_polarity.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Positive Polarity Comments\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_negative_polarity.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Negative Polarity Comments\", fontsize=20)\n\nplt.show()","15854b55":"val_df.head(2)","20b341f4":"val_df['less_toxic'] = val_df['less_toxic'].apply(clean_text)\nval_df['more_toxic'] = val_df['more_toxic'].apply(clean_text)","4a351981":"comment_length_less_toxic = val_df['less_toxic'].apply(len)\ncomment_length_more_toxic = val_df['more_toxic'].apply(len)\n\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = comment_length_less_toxic.plot(kind='hist', color = \"#ff7033\", bins=100, alpha=1)\nax1 = comment_length_more_toxic.plot(kind='hist', color = \"#622864\", bins=100, alpha=0.6)\nax1.set_title('Comment Length Distribution More Toxic vs Less Toxic')\nax1.set_xlabel(\"Comment Length\")\nax1.set_ylabel(\"Frequency\")\nax1.legend()\n\nplt.show()","81517e8a":"word_count_less_toxic = val_df['less_toxic'].apply(lambda x: len(str(x).split()))\nword_count_more_toxic = val_df['more_toxic'].apply(lambda x: len(str(x).split()))\n\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = word_count_less_toxic.plot(kind='hist', color = \"#ff7033\", bins=100)\nax1 = word_count_more_toxic.plot(kind='hist', color = \"#622864\", bins=100, alpha=0.7)\nax1.set_title('Word Count Distribution More Toxic vs Less Toxic')\nax1.set_xlabel(\"Word Count\")\nax1.set_ylabel(\"Frequency\")\nax1.legend()\n\nplt.show()","18f42792":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=False, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=False, n_words=1)","467b6856":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","533838ea":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=True, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=True, n_words=1)","1b9a9496":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","25a200a1":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=False, n_words=2)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=False, n_words=2)","b023b284":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Bigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","57b2ae73":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=True, n_words=2)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=True, n_words=2)","5527ab6c":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Bigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","a6361d81":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=False, n_words=3)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=False, n_words=3)","172b2bb8":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Trigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","016c9b03":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=True, n_words=3)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=True, n_words=3)","e490b415":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Trigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","3c2c7c6d":"val_df['set_of_words_less_toxic'] = val_df['less_toxic'].apply(get_unique_words)\nval_df['set_of_words_more_toxic'] = val_df['more_toxic'].apply(get_unique_words)","e48ff82a":"common_words_less_toxic = get_top_n_words(val_df['set_of_words_less_toxic'], 20, remove_stop_words=False, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['set_of_words_more_toxic'], 20, remove_stop_words=False, n_words=1)","b502e4c7":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","665e6341":"common_words_less_toxic = get_top_n_words(val_df['set_of_words_less_toxic'], 20, remove_stop_words=True, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['set_of_words_more_toxic'], 20, remove_stop_words=True, n_words=1)","01d14e3f":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","1b6531de":"polarity_less_toxic = val_df['less_toxic'].map(lambda text: TextBlob(text).sentiment.polarity)\npolarity_more_toxic = val_df['more_toxic'].map(lambda text: TextBlob(text).sentiment.polarity)","6d89cf9e":"fig = plt.figure(figsize=(10,8))\n\nax1 = polarity_less_toxic.plot(kind='hist', color = \"#622864\", bins=100)\nax1 = polarity_more_toxic.plot(kind='hist', color = \"#ff7033\", bins=100, alpha=0.7)\nax1.set_title('Polarity Distribution Less vs More Toxic Comments')\nax1.set_xlabel(\"Sentiment\")\nax1.set_ylabel(\"Frequency\")\nax1.legend()\n\nplt.show()","40e04eaa":"text_less_toxic = val_df['less_toxic'].values\ntext_more_toxic = val_df['more_toxic'].values","86f60e74":"wc_less_toxic = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_joy,stopwords=set(STOPWORDS))\nwc_more_toxic = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_sad,stopwords=set(STOPWORDS))\n\nwc_less_toxic.generate(\" \".join(text_less_toxic))\nwc_more_toxic.generate(\" \".join(text_more_toxic))\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_less_toxic.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Word Cloud Less Toxic Comments\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_more_toxic.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Word Cloud More Toxic Comments\", fontsize=20)\n\nplt.show()","c0098734":"print(f'We have {val_df.worker.nunique()} different workers')","2489ce00":"dict_check = {}\nfor i in range(val_df.shape[0]):\n    comment = [val_df['less_toxic'][i], val_df['more_toxic'][i]]\n    key_comment = comment.copy()\n    key_comment.sort()\n    key_comment = tuple(key_comment)\n    if key_comment not in dict_check.keys():\n        dict_check[key_comment] = {}\n        dict_check[key_comment]['less_more'] = 0\n        dict_check[key_comment]['more_less'] = 0\n        if comment[0]<comment[1]:\n            dict_check[key_comment]['less_more'] += 1\n        else:\n            dict_check[key_comment]['more_less'] += 1\n    else:\n        if comment[0]<comment[1]:\n            dict_check[key_comment]['less_more'] += 1\n        else:\n            dict_check[key_comment]['more_less'] += 1","7d965438":"print(f'Unlike the total pairs of comments in the dataset ({val_df.shape[0]}) the unique pairs of comments are much less:\\n'\n      f'number of unique pairs of comments: {len(dict_check.keys())}')","0475cd47":"key_col = []\nless_more_col = []\nmore_less_col = []\nfor key in dict_check.keys():\n    key_col.append(key)\n    less_more_col.append(dict_check[key]['less_more'])\n    more_less_col.append(dict_check[key]['more_less'])\nfinal_df = pd.concat([pd.Series(key_col), pd.Series(less_more_col), pd.Series(more_less_col)], axis = 1)\nfinal_df.columns = ['comments', 'less_more', 'more_less']","23a154cf":"max_time = np.max(final_df['less_more']+final_df['more_less'])\nprint(f'Maximum number of times a single comment pair was rated: {max_time}')","113de87b":"differently_classified = final_df[(final_df.less_more>0)&(final_df.more_less>0)]","dd227331":"print(f\"The number of comment pairs ranked differently based on who ranked them is: {differently_classified.shape[0]} out of a total of {len(dict_check.keys())}. \\n\"\n\"This is expected, often both comments contain terms that are considered toxic, so it is subjective how these two comments are categorized into 'more toxic' or 'less toxic'.\")","3d6704f0":"In terms of word count, the most toxic comments have a lower word count on average than the least toxic comments.","d607dce2":"## Distribution of Top Bigrams","66106207":"## Distribution of Top Unigrams","f099ec5f":"## Table of Contents\n* [Dataset Information](#data_information)\n    - Data description\n    - Files\n* [Preliminary Data Exploration](#preliminary_eda)\n    - Install and Import Libraries\n    - Load Data\n    - General Dataset Information\n* [Exploratory Data Analysis](#eda)\n    - [Comments to Score Dataset](#cts)\n        - Clean Text\n        - Language Detection\n        - Comment Length Distribution\n        - Word Count Distribution\n        - Distribution of Top Unigrams\n        - Distribution of Top Bigrams\n        - Distribution of Top Trigrams\n        - Unique Words Analysis\n        - Sentiment Polarity\n        - Word Clouds\n    - [Validation Dataset](#val)\n        - Clean Text\n        - Comment Length Distribution\n        - Word Count Distribution\n        - Distribution of Top Unigrams\n        - Distribution of Top Bigrams\n        - Distribution of Top Trigrams\n        - Unique Words Analysis\n        - Sentiment Polarity\n        - Word Clouds\n        - Worker Analysis","c6ad367b":"This notebook ends here, I will try to update it as I go along with new analysis. Thanks for making it to the end :) !","3da7bb2d":"Distribution of top trigrams after removing stop words:","901f871f":"Distribution of top bigrams before removing stop words:","8beb5483":"Let's proceed with the exploration of the validation dataset, in which we have for each row two comments, classified as 'more toxic' or 'less toxic'. This evaluation is done by considering only those two comments.","c5becc76":"We can see a lot of difference between the top unigrams of the least toxic comments, where there are few swear words, compared to those of the most toxic comments.","16a9878a":"## Load Data","ef75dced":"<a id='data_information'><\/a>\n# Dataset Information\n## Data Description","f5e10deb":"## Distribution of Top Trigrams","c2c57788":"Several 'toxic' words can be spotted in the less toxic comments. This is because one comment is defined as less toxic than another, there is no guarantee that it is not toxic in general.","2bce5e3c":"Comments classified as German:","3ff85898":"Let's detect the language of each comment:","7a33c21e":"Distribution of top trigrams before removing stop words:","ea0ee6f7":"Distribution of top unigrams before removing stop words:","8b9d4b1c":"Most comments have zero polarity, so neutral sentiment, let's see if the polarity found in this way is reliable:","1e6c6573":"Distribution of top bigrams after removing stop words:","6def2fd2":"The same thing about bigrams applies here. Toxic trigrams are very frequently present in less toxic comments, this is because one comment is defined as less toxic than another, there is no guarantee that it is not toxic in general.","423b4472":"Distribution of top trigrams after removing stop words:","87892248":"## Word Count Distribution","e32c8073":"Note how the most toxic comments tend to have shorter lengths in general, however, with a peak for length 500.","bc86e362":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'><\/div>","be30ce77":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'><\/div>","7a95b5b4":"<div style='color:white;background-color:#f2dde8; height: 50px; border-radius: 25px;'><h1 style='text-align:center;padding: 1%'>The End<\/h1><\/div>","b9b30de5":"The situation does not change much by keeping or removing the stop words in the case of trigrams.","222892d7":"<a id='preliminary_eda'><\/a>\n# Preliminary Data Exploration","00c2bef3":"<div>\nThe data used for this competition are Wikipedia Talk page comments. The purpose is to rank the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes.<\/br>\n<b>Important<\/b>:\nThere is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models.\n\n<h3>Files<\/h3>\n<span style=\"background-color:#e1e6e3;\">comments_to_score.csv<\/span> - collection of comments <\/br>\n<span style=\"background-color:#e1e6e3;\">validation_data.csv<\/span> - pair rankings that can be used to validate models <\/br>\n<span style=\"background-color:#e1e6e3;\">sample_submission.csv<\/span> - a sample submission file in the correct format <\/br>\n<\/br>\n<b style='margin-top:1.5%;background-color:#fbffb3'><i>Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.<\/i><\/b><\/div>","67c844ab":"<div style='color:white;background-color:#f2dde8; height: 100px; border-radius: 25px;'><h1 style='text-align:center;padding: 3%'>Jigsaw Rate Severity of Toxic Comments Competition<\/h1><\/div>","c778768a":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'><\/div>","4e7f75f2":"Distribution of top unigrams before removing stop words:","32aaf4ae":"Distribution of top unigrams after removing stop words:","31c36c48":"## Distribution of Top Trigrams","8111d4dd":"After removing stop words:","982b5d8f":"## Sentiment Polarity","dfbf730d":"## Distribution of Top Bigrams","452209ff":"## Unique Words Analysis","6bd7a059":"## Word Clouds","5ac2b3a2":"Let's use TextBlob to calculate sentiment polarity. The sentiment polarity value lies in the range of [-1, 1] where 1 means positive sentiment and -1 means a negative sentiment:","bf814e73":"Word clouds based on sentiment:","a6a10f52":"Let's look at how many comments were ranked differently by different workers:","4afdc954":"## Unique Words Analysis","0301f4b5":"## Comment Length Distribution","1b372092":"Let's take 10 random comments with positive polarity:","9012b070":"Distribution of top unigrams before removing stop words:","a3ec9d9b":"Let's look at the most frequent words after removing words present more than once from the texts.","4feb5c13":"## Install and Import Libraries","de39ab88":"Before removing stop words:","e7f1e01f":"## Word Count Distribution","13bae9a4":"## Sentiment Polarity","6d40da93":"Distribution of top unigrams before removing stop words:","cffadcb9":"We can see that the less toxic comments have a more neutral or positive polarity. While comments classified as toxic have a more negative polarity.","8176f84c":"Distribution of top unigrams after removing stop words:","13161c3c":"<a id='cts'><\/a>\n# Comments to Score Dataset\n## Clean Text","18ca93c0":"## Comment Length Distribution","efd0f813":"## Worker Analysis","51072f1f":"## Distribution of Top Unigrams","ab8eb3d2":"Comments classified as in Italian language:","b9af9fed":"As for bigrams, even without removing the stop words we can see how the comments differ between the two categories.","13c532cb":"<a id='val'><\/a>\n# Validation Dataset","04d7975a":"## Word Clouds","11851631":"Distribution of top bigrams after removing stop words:","3789021c":"Let's take a look at the first two clean comments:","5c618c75":"Distribution of top bigrams before removing stop words:","2c644fd4":"Like for the dataset of the previous section, we go to see the unigrams present in the text most frequently after selecting only the unique words of the comments.","468d80fb":"Word cloud of all the comments:","eef588ce":"Let's use TextBlob to calculate sentiment polarity. The polarity value lies in the range of [-1, 1] where 1 means positive sentiment and -1 means a negative sentiment:","10dafe79":"## Language Detection","14304fc4":"So we can see that the polarity is not very accurate.","b4408959":"Distribution of top trigrams before removing stop words:","9cd52eb5":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'><\/div>","dfacbbfd":"As seen before several swear words are found to lose positions relative to the most frequent words, after removing repeated words.This can be traced to the fact that there are many comments within the dataset with swear words repeated many times.","fe75296f":"## Clean Text","789fa495":"Actually the comments are in English, even those classified as other language. Probably the high number of swear words\/terms belonging to specific slangs affects the accuracy of \"langdetect\".","28b32e28":"<a id='eda'><\/a>\n# Exploratory Data Analysis","f8b9a747":"Several swear words are found to lose positions relative to the most frequent words, after removing repeated words. This can be traced to the fact that there are many comments within the dataset with swear words repeated many times. For example:","1ba8d928":"You can see from these analyses that many comments are nothing more than repeated words. Let's try to do some analysis by considering only the unique words contained in a text. We perform this analysis by also removing anything that is not an alphabet character and making all text lowercase.","305efbba":"Let's look at the length of the comments according to the column they belong to:","69a57b82":"## General Dataset Information","774da881":"Let's see if the order of toxicity between two comments belonging to a pair is consistent across workers."}}