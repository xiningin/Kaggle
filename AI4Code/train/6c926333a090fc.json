{"cell_type":{"891d887b":"code","8088b30e":"code","fe49313c":"code","b81b54a3":"code","1d6d365c":"code","6e561da3":"code","c327403a":"code","99f9c84e":"code","4c2e168e":"code","4bb40d2d":"code","6ab75ebb":"code","e923e064":"code","8e49a56f":"code","b703c7c7":"code","4acb041a":"code","1cc7c28a":"code","88c3eeec":"code","1efdfb63":"code","50f130cf":"code","d4a7c7c8":"code","fc3910a7":"code","94d5a456":"code","a658c8d9":"code","dd7d5fe8":"code","3f351c9a":"code","a7d4b146":"code","0d14bbf9":"code","e675dc43":"code","c0f9023c":"code","026ac31c":"code","bc829b97":"code","10586310":"code","2e19c87a":"code","02e2a681":"code","ce7b25d3":"code","cfef17db":"code","459e2062":"code","bfae226a":"code","231b418e":"code","cdbdff3b":"code","4f4bcf7a":"code","6f5d11c2":"code","884693c5":"code","eaddbca5":"code","97644fd7":"code","6ba3ad45":"code","896888bd":"code","f42651fe":"code","7cb884d1":"code","e26b8ac0":"code","c2c17ebc":"code","8ff1e8a3":"markdown","8fdf2fe7":"markdown","52e78caf":"markdown","82e67cb2":"markdown","39f088cb":"markdown","9bc0f99c":"markdown","8816ecd3":"markdown","67d62250":"markdown","aaa1f2c4":"markdown","86d882d0":"markdown","2a375698":"markdown","f540b369":"markdown","c947ef7e":"markdown","ae839bfb":"markdown","9f1fd486":"markdown","1931cba8":"markdown","bd9ba6b5":"markdown","2cf64b88":"markdown","9112f2b9":"markdown","e30340e3":"markdown","06eaf87a":"markdown","ecf514e9":"markdown","146b9f6b":"markdown","ee2008fb":"markdown","b7ddc537":"markdown","9b70c797":"markdown","d88bcb50":"markdown","de73a303":"markdown","94d5d22c":"markdown","e1851e71":"markdown","e1d3b126":"markdown","b16c6ba0":"markdown","fd5af626":"markdown","90d81a67":"markdown","27b79696":"markdown","201d42ab":"markdown","fa91ad0c":"markdown","7a652316":"markdown","2fbf251d":"markdown","e651c6f3":"markdown","289029a9":"markdown","3de1d4c4":"markdown","37cdccec":"markdown","e3e4fc10":"markdown","dfe5b2f3":"markdown","7fc4257a":"markdown","dfc5cf09":"markdown","4bed6037":"markdown","0e0b0ca0":"markdown","97a8294b":"markdown","4c5eea35":"markdown","16bdf797":"markdown","f88947d5":"markdown","6958dea7":"markdown","57262d47":"markdown","c51ced37":"markdown","d9712333":"markdown","fa8afeb4":"markdown","4d648dc8":"markdown","2415b35e":"markdown","82bc563d":"markdown","5d8759a7":"markdown","f6e2e2db":"markdown","64090283":"markdown","acba8ac1":"markdown"},"source":{"891d887b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8088b30e":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency, normaltest","fe49313c":"#Wczytanie bazy danych do lokalnej zmiennej\ndata = pd.read_csv(\"..\/input\/customer-analytics\/Train.csv\")\ndata.head()","b81b54a3":"data.isna().sum()","1d6d365c":"quant_col = ['Customer_care_calls', 'Cost_of_the_Product', 'Prior_purchases','Discount_offered', 'Weight_in_gms']\nquant_stats = data[quant_col].agg([\"count\",\"mean\",\"median\",\"min\", \"max\", \"std\", \"var\",])\nquant_stats = quant_stats.append(data[quant_col].mode().rename(index={0:\"mode\"}))\nquant_stats","6e561da3":"quali_cols = ['Warehouse_block', 'Mode_of_Shipment','Product_importance', 'Gender', 'Customer_rating']\nfig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres rozk\u0142adu liczno\u015bci dla {col}\")\n    sns.histplot(data[col], ax=axes[i])\n    print(f\"\"\"Dla kolumny {col} tabela liczno\u015bci wygl\u0105da nast\u0119puj\u0105co:\\n\\n\\n{pd.DataFrame(data[col].value_counts())}\\n\"\"\")","c327403a":"sns.histplot(x=data[\"Mode_of_Shipment\"],hue=data[\"Warehouse_block\"])\ndata[[\"Mode_of_Shipment\", \"Warehouse_block\", \"ID\"]].groupby([\"Warehouse_block\", \"Mode_of_Shipment\"]).count()","99f9c84e":"for col in quali_cols:\n    print(pd.crosstab(data[col], data[\"Reached.on.Time_Y.N\"], normalize=\"index\"))\n    print(\"\\n\\n\")","4c2e168e":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    #sns.histplot(data = data, x=data[\"Reached.on.Time_Y.N\"], hue=col, ax=axes[i])\n    sns.histplot(data = data, x=data[col], hue=\"Reached.on.Time_Y.N\", ax=axes[i], multiple=\"dodge\")","4bb40d2d":"for col in quali_cols[:-1]:\n    print(pd.crosstab(data[col], data['Customer_rating'], normalize=\"index\"))\n    print(\"\\n\\n\")","6ab75ebb":"fig, axes = plt.subplots(len(quali_cols[:-1]), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols[:-1]):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    #sns.histplot(data = data, x=data[\"Reached.on.Time_Y.N\"], hue=col, ax=axes[i])\n    sns.histplot(data = data, x=data[col], hue=\"Customer_rating\", ax=axes[i], multiple=\"dodge\")","e923e064":"hipo_3 = data.copy()\nhipo_3.Discount_offered = hipo_3.Discount_offered.astype(str)\nhipo_3.Discount_offered.values[data[\"Discount_offered\"].values < 17] = \"<17\"\nhipo_3.Discount_offered.values[(data[\"Discount_offered\"].values < 34) & (data[\"Discount_offered\"].values >= 17)] = \"17<=x<34\"\nhipo_3.Discount_offered.values[(data[\"Discount_offered\"].values < 51) & (data[\"Discount_offered\"].values >= 34)] = \"34<=x<51\"\nhipo_3.Discount_offered.values[(data[\"Discount_offered\"].values <= 65) & (data[\"Discount_offered\"].values >= 51)] = \"51<=x<=65\"\n\nfor col in quali_cols:\n    print(pd.crosstab(hipo_3[col], hipo_3['Discount_offered'], normalize=\"index\"))\n    print(\"\\n\\n\")","8e49a56f":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    #sns.histplot(data = data, x=data[\"Reached.on.Time_Y.N\"], hue=col, ax=axes[i])\n    sns.histplot(data = hipo_3, x=hipo_3[col], hue='Discount_offered', ax=axes[i], multiple=\"dodge\")","b703c7c7":"fig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(data.corr(), annot=True, ax=ax)\ndata.corr()","4acb041a":"cols_to_drop = [\"ID\", \"Reached.on.Time_Y.N\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"Reached.on.Time_Y.N\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} warto\u015b\u0107 p testu niezale\u017cno\u015bci wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Warto\u015b\u0107 p jest mniejsza dla za\u0142o\u017conego poziomu istotno\u015bci co pozwala na odrzucenie hipotezy zerowej - zmienne s\u0105 zale\u017cne\\n\")\n        else:\n            print(f\"Warto\u015b\u0107 p jest wi\u0119ksza dla za\u0142o\u017conego poziomu istotno\u015bci co potwierdza hipotez\u0119 zerow\u0105 - zmienne s\u0105 niezale\u017cne\\n\")","1cc7c28a":"cols_to_drop = [\"ID\", \"Customer_rating\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"Customer_rating\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} warto\u015b\u0107 p testu niezale\u017cno\u015bci wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Warto\u015b\u0107 p jest mniejsza dla za\u0142o\u017conego poziomu istotno\u015bci co pozwala na odrzucenie hipotezy zerowej - zmienne s\u0105 zale\u017cne\\n\")\n        else:\n            print(f\"Warto\u015b\u0107 p jest wi\u0119ksza dla za\u0142o\u017conego poziomu istotno\u015bci co potwierdza hipotez\u0119 zerow\u0105 - zmienne s\u0105 niezale\u017cne\\n\")","88c3eeec":"cols_to_drop = [\"ID\", \"Discount_offered\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"Discount_offered\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} warto\u015b\u0107 p testu niezale\u017cno\u015bci wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Warto\u015b\u0107 p jest mniejsza dla za\u0142o\u017conego poziomu istotno\u015bci co pozwala na odrzucenie hipotezy zerowej - zmienne s\u0105 zale\u017cne\\n\")\n        else:\n            print(f\"Warto\u015b\u0107 p jest wi\u0119ksza dla za\u0142o\u017conego poziomu istotno\u015bci co potwierdza hipotez\u0119 zerow\u0105 - zmienne s\u0105 niezale\u017cne\\n\")\n","1efdfb63":"fig, axes = plt.subplots(len(quant_col),1,  figsize=(10,30))\nfor i, col in enumerate(quant_col):\n    axes[i].set_title(f\"Wykres pude\u0142kowy kolumny {col}\")\n    #sns.histplot(data = data, x=data[\"Reached.on.Time_Y.N\"], hue=col, ax=axes[i])\n    sns.boxplot(data = data, x=data[col], ax=axes[i], orient = \"h\")","50f130cf":"data[data[\"Prior_purchases\"]>=6]","d4a7c7c8":"import random\ndata.loc[data[\"Prior_purchases\"] >= 6,\"Prior_purchases\"] = data[\"Prior_purchases\"].apply(lambda x: random.randrange(2,5))","fc3910a7":"sns.boxplot(data = data, x=data[\"Prior_purchases\"], orient = \"h\")","94d5a456":"data[data[\"Discount_offered\"]>20]","a658c8d9":"sns.boxplot(data = data, x=\"Reached.on.Time_Y.N\", y = \"Cost_of_the_Product\", orient=\"v\")","dd7d5fe8":"sns.boxplot(data = data, x=\"Product_importance\", y = \"Cost_of_the_Product\", orient=\"v\")","3f351c9a":"\nfig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quant_col):\n    axes[i].set_title(f\"Wykres rozk\u0142adu liczno\u015bci dla {col}\")\n    sns.histplot(data[col], ax=axes[i], kde=True)","a7d4b146":"for i, col in enumerate(quant_col):\n    print(f\"Warto\u015b\u0107 p dla testu normalnego dla kolumny {col} wynosi {normaltest(data[col].values).pvalue}\\n\")","0d14bbf9":"# improtowanie potrzebnych bibliotek\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, precision_recall_fscore_support\npd.options.mode.chained_assignment = None\n\n# oddzielenie potrzebnych danych\nhipo_1_data = data[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\nhipo_1_y = data[\"Reached.on.Time_Y.N\"]\n\n# jedna ze zmiennych (\"Product_importance\") to zmienna jako\u015bciowa w postaci, s\u0142\u00f3w. \n#Aby algorytm m\u00f3g\u0142 przetworzy\u0107 te dane, nale\u017cy zamieni\u0107 je na posta\u0107 liczbow\u0105. \n#Z racji tego, \u017ce s\u0105 to zmienne typu uporz\u0105dkowanego mo\u017cemy je zmapowa\u0107 od najni\u017cej do najwy\u017ceszj\n\ndecode = {\n    \"low\" : 1,\n    \"medium\" : 2,\n    \"high\" : 3\n}\n\nhipo_1_data.loc[:, \"Product_importance\"] = hipo_1_data.Product_importance.map(decode).values\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_1_data, hipo_1_y, train_size=0.7, random_state=1)\nX_train\n\n# \u0142adowanie modelu\ntree_model = DecisionTreeClassifier(min_samples_leaf=200)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dok\u0142\u0105dno\u015bci modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nacc = accuracy_score(y_test, preds)\nprint(f\"Dok\u0142adno\u015b\u0107 modelu wynosi {100*acc:.2f}%\")","e675dc43":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Wa\u017cno\u015b\u0107 predyktor\u00f3w\")\nplt.barh(range(hipo_1_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_1_data.shape[1]), hipo_1_data.columns)\nplt.ylim([-1, hipo_1_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()\n","c0f9023c":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"On Time\",\"Not on Time\"])\nplt.show()","026ac31c":"plot_confusion_matrix(tree_model, X_test, y_test, display_labels=[\"Not on time\", \"On Time\"])\nprecission, recall, f_score, support = precision_recall_fscore_support(y_test, preds)\nprint(f\"Dok\u0142adno\u015b\u0107 (precission) modelu podczas okre\u015blania czy dana przesy\u0142ka dotrze do adresata wynosi {precission[1]:.2f}, natomiast rozpoznanie ilo\u015b\u0107 (recall) dla tej klasy wynosi {recall[1]:.2f}\")\nprint(f\"Oznacza to, \u017ce {100 * precission[1]:.2f}% paczek, kt\u00f3re zosta\u0142y sklasyfikowane jako paczki dostaczonych na czas zosta\u0142o poprawnie sklasyfikowanych, a {100 * recall[1]:.2f}% wszystkich paczek dostarczonych na czas zosta\u0142o poprawnie sklasyfikowanych.\")","bc829b97":"# oddzielenie potrzebnych danych\nhipo_2_data = data[[\"Cost_of_the_Product\", \"Customer_care_calls\"]]\nhipo_2_y = data[\"Customer_rating\"]\n\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_2_data, hipo_2_y, train_size=0.7, random_state=1)\n\n# \u0142adowanie modelu\ntree_model = DecisionTreeClassifier(min_samples_leaf=500)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dok\u0142\u0105dno\u015bci modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nacc = accuracy_score(y_test, preds)\nprint(f\"Dok\u0142adno\u015b\u0107 modelu wynosi {100*acc:.2f}%\")","10586310":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Wa\u017cno\u015b\u0107 predyktor\u00f3w\")\nplt.barh(range(hipo_2_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_2_data.shape[1]), hipo_2_data.columns)\nplt.ylim([-1, hipo_2_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","2e19c87a":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"1\",\"2\", \"3\", \"4\", \"5\"])\nplt.show()","02e2a681":"display_labels=[\"1\",\"2\", \"3\", \"4\", \"5\"]\nplot_confusion_matrix(tree_model, X_test, y_test, display_labels=display_labels)\nprecission, recall, f_score, support = precision_recall_fscore_support(y_test, preds)\nfor i, label in enumerate(display_labels):\n    print(f\"Dok\u0142adno\u015b\u0107 (precission) modelu podczas okre\u015blania czy dana przesy\u0142ka zostanie oceniona na {label} wynosi {precission[i]:.2f}, natomiast rozpoznanie ilo\u015b\u0107 (recall) dla tej klasy wynosi {recall[i]:.2f}\")\n    print(f\"Oznacza to, \u017ce {100 * precission[i]:.2f}% paczek, kt\u00f3re zosta\u0142y sklasyfikowane jako ocenione na {label} zosta\u0142o poprawnie sklasyfikowanych, a {100 * recall[i]:.2f}% wszystkich paczek sklasyfikowanych jako ocenione na {label} zosta\u0142o poprawnie sklasyfikowanych.\\n\\n\")","ce7b25d3":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import r2_score\n\n\n# oddzielenie potrzebnych danych\nhipo_3_data = data[[\"Reached.on.Time_Y.N\", \"Weight_in_gms\", \"Customer_care_calls\"]]\nhipo_3_y = data[\"Discount_offered\"]\n\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_3_data, hipo_3_y, train_size=0.7, random_state=1)\n\n# \u0142adowanie modelu\ntree_model = DecisionTreeRegressor(min_samples_leaf=500)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dok\u0142\u0105dno\u015bci modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nr2score = r2_score(y_test, preds)\nprint(f\"Wsp\u00f3\u0142czynnik zbie\u017cno\u015bci wynosi {((1 - r2score) * 100):.2f}% - Dopasowanie modelu jest tym lepsze im bardziej wsp\u00f3\u0142czynnik zbie\u017cno\u015bci jest bli\u017cej 0%\")","cfef17db":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Wa\u017cno\u015b\u0107 predyktor\u00f3w\")\nplt.barh(range(hipo_3_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_3_data.shape[1]), hipo_3_data.columns)\nplt.ylim([-1, hipo_3_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","459e2062":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns)\nplt.show()","bfae226a":"# improtowanie potrzebnych bibliotek\nfrom sklearn.cluster import KMeans\npd.options.mode.chained_assignment = None\nfrom sklearn.decomposition import PCA\n\ndecode = {\n    \"M\" : 0,\n    \"F\" : 1,\n}\n\n# oddzielenie potrzebnych danych\ncluster_data = data[[\"Weight_in_gms\", \"Gender\", \"Cost_of_the_Product\"]]\ncluster_data.loc[:, \"Gender\"] = cluster_data.Gender.map(decode).values\n\nsse = []\n\n# \u0142adowanie modelu\nfor k in range(1,11):\n    cluster_model = KMeans(n_clusters=k)\n    cluster_model.fit(cluster_data)\n    sse.append(cluster_model.inertia_)\n    \n# sprawdzanie jak\u0105 warto\u015b\u0107 k wybra\u0107 - metoda \u0142okcia\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Warto\u015b\u0107 k\")\nplt.ylabel(\"SSE\")\nplt.show()","231b418e":"from sklearn.metrics import silhouette_score\n\nsil_score_max = -1\nbest_n_clusters = 0\n\nfor k in range(2,11):\n  cluster_model = KMeans(n_clusters = k)\n  labels = cluster_model.fit_predict(cluster_data)\n  sil_score = silhouette_score(cluster_data, labels)\n  print(f\"\u015arednia warto\u015b\u0107 silhouette score dla {k} klastr\u00f3w wynosi {sil_score}\")\n  if sil_score > sil_score_max:\n    sil_score_max = sil_score\n    best_n_clusters = k\n    \nprint(f\"Najlepsza ilo\u015b\u0107 klastr\u00f3w to: {best_n_clusters}\")","cdbdff3b":"cluster_model = KMeans(n_clusters = best_n_clusters)\n# trenowanie modelu\nresult = cluster_model.fit_predict(cluster_data)\n\nlabels = cluster_model.labels_\n\nresult_data = cluster_data.copy()\nresult_data[\"labels\"] = labels\n\nresults_0 = cluster_data[result_data.labels == 0]\nresults_1 = cluster_data[result_data.labels == 1]","4f4bcf7a":"results_0.describe()","6f5d11c2":"results_1.describe()","884693c5":"# za\u0142adowanie modu\u0142u do analizy metod\u0105 EM\nfrom sklearn.mixture import GaussianMixture\n\n# wczytanie modelu z dwoma klastrami\nem_model = GaussianMixture(n_components=2, random_state=0)\n\nlabels = em_model.fit_predict(cluster_data)\n\n#labels = em_model.labels_\n\nresult_data = cluster_data.copy()\nresult_data[\"labels\"] = labels\n\nresults_0 = cluster_data[result_data.labels == 0]\nresults_1 = cluster_data[result_data.labels == 1]","eaddbca5":"results_0.describe()","97644fd7":"results_1.describe()","6ba3ad45":"# rysowanie wykresu dla ka\u017cdej zmiennej \nimport scipy.stats as stats\nimport math\n\nfor col in results_1:\n    mu_0 = results_0[col].mean()\n    variance_0 = results_0[col].var()\n    mu_1 = results_1[col].mean()\n    variance_1 = results_1[col].var()\n    sigma_0 = math.sqrt(variance_0)\n    sigma_1 = math.sqrt(variance_1)\n    x_0 = np.linspace(mu_0 - 3*sigma_0, mu_0 + 3*sigma_0, 100)\n    x_1 = np.linspace(mu_1 - 3*sigma_1, mu_1 + 3*sigma_1, 100)\n    plt.figure(figsize=(8,5))\n    plt.title(f\"Wykres dystrybucji zmiennych w klastrach dla zmiennej {col}\")\n    plt.plot(x_0, stats.norm.pdf(x_0, mu_0, sigma_0))\n    plt.plot(x_1, stats.norm.pdf(x_1, mu_1, sigma_1))\n    plt.show()","896888bd":"# zapisywanie danych w postaci tensora\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nhipo_1_data = data[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\nhipo_1_y = data[\"Reached.on.Time_Y.N\"]\n\ndecode = {\n    \"low\" : 1,\n    \"medium\" : 2,\n    \"high\" : 3\n}\n\nhipo_1_data.loc[:, \"Product_importance\"] = hipo_1_data.Product_importance.map(decode).values\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_1_data, hipo_1_y, train_size=0.7, random_state=1)\n\nmodel = keras.models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dense(2, activation=\"softmax\")\n])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nmodel.build()\nmodel.summary()","f42651fe":"#trenowanie modelu\nhistory = model.fit(X_train, y_train, validation_split=0.33, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Dok\u0142adno\u015b\u0107 modelu')\nplt.ylabel('Dok\u0142adno\u015b\u0107')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Funkcja koszt\u00f3w modelu')\nplt.ylabel('Strata')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","7cb884d1":"from sklearn import preprocessing","e26b8ac0":"scaler = preprocessing.StandardScaler()\nX_train_scal = scaler.fit_transform(X_train)\nX_test_scal = scaler.transform(X_test)\n\n#trenowanie modelu\nhistory = model.fit(X_train, y_train, validation_split=0.33, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Dok\u0142adno\u015b\u0107 modelu')\nplt.ylabel('Dok\u0142adno\u015b\u0107')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Funkcja koszt\u00f3w modelu')\nplt.ylabel('Strata')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","c2c17ebc":"model.evaluate(X_test, y_test)","8ff1e8a3":"Por\u00f3wnuj\u0105c modele drzewa decyzyjnego i sieci neuronowej w celu potwierdzenia hipotezy 1 model drzewa sprawdza\u0142 si\u0119 du\u017co lepiej ni\u017c sie\u0107 neuronowa nie (r\u00f3\u017cnica o 10%). Oznacza to, \u017ce nie zawsze skomplikowane rozwi\u0105zania s\u0105 dobre dla ka\u017cdego rozwi\u0105zania. W sytuacji, kiedy jest niewielka korelacja zmiennych, a rozk\u0142ady nie s\u0105 normalne modele nieparametryczne mog\u0105 sprawdza\u0107 si\u0119 lepiej ni\u017c modele parametryczne. Jednak\u017ce hipotez\u0119 mo\u017cna potwierdzi\u0107 przy wykorzystaniu modelu sieci neuronowej.","8fdf2fe7":"Nast\u0119pnym wykresem, gdzie wyst\u0119puj\u0105 warto\u015bci odstaj\u0105ce to \"Discount_offered\". Jednak w tym przypadku zdecydowano zostawi\u0107 te warto\u015bci, poniewa\u017c mimo, \u017ce znacz\u0105co odstaj\u0105 od reszty to zawieraj\u0105 bardzo istotn\u0105 informacj\u0119, kt\u00f3ra nie wynika z b\u0142\u0119du pomiarowego. Mozemy si\u0119 przyjrze\u0107 tym warto\u015bciom.","52e78caf":"Jak wida\u0107 standaryzacja delikatnie poprawi\u0142a rozrzut pomi\u0119dzy warto\u015bciami i zmniejszy\u0142a ilos\u0107 skok\u00f3w warto\u015bci funkcji koszt\u00f3w oraz funkcji dok\u0142adno\u015bci modelu je\u017celi chodzi o dane treningowe. Dodatkowo funckja du\u017co szybciej osi\u0105gne\u0142a optimum.","82e67cb2":"Wybrany zestaw danych nie posiada, \u017cadnych brak\u00f3w.","39f088cb":"Poni\u017cej wizualizacja wynik\u00f3w","9bc0f99c":"Jak wida\u0107 na powy\u017cszym wykresie najrozs\u0105dniejsza ilo\u015b\u0107 klastr\u00f3w jak\u0105 powinno si\u0119 wybra\u0107 jest 2.\nDo automatycznego doboru ilo\u015bci klastr\u00f3w mo\u017cna wykorzysta\u0107 silhouette coefficient score.\n\nSilhouette Coefficient jest obliczany przy u\u017cyciu \u015bredniej odleg\u0142o\u015bci wewn\u0105trz klastra (a) i \u015bredniej odleg\u0142o\u015bci do najbli\u017cszego klastra (b) dla ka\u017cdej pr\u00f3bki. Wsp\u00f3\u0142czynnik sylwetki dla pr\u00f3bki to (b - a) \/ max (a, b). Aby wyja\u015bni\u0107, b to odleg\u0142o\u015b\u0107 mi\u0119dzy pr\u00f3bk\u0105 a najbli\u017csz\u0105 gromad\u0105, kt\u00f3rej pr\u00f3bka nie jest cz\u0119\u015bci\u0105. Zwr\u00f3\u0107 uwag\u0119, \u017ce silhouette coefficient jest definiowany tylko wtedy, gdy liczba etykiet wynosi 2 <= n_labels <= n_samples - 1.","8816ecd3":"Analiza klastra 1:","67d62250":"Z powodu jednolitego rozk\u0142adu zmiennych badanie skupie\u0144 nie przynosi dodatkowych informacji, poniewa\u017c metody typu najni\u017cszych s\u0105siad\u00f3w dziel\u0105 te zbiory na podobne klatry o podobnym wygl\u0105dzie. Jedyna r\u00f3\u017cnica pomi\u0119dzy klastrami jest w wadze produkt\u00f3w, gdzie pierwszy klaster skupia w sobie produkty, kt\u00f3re wa\u017c\u0105 powy\u017cej 3330 gram\u00f3w, a drugi klaster te produkty, kt\u00f3re wa\u017c\u0105 miej ni\u017c 3330 gram\u00f3w. Patrz\u0105c na p\u0142e\u0107 konsument\u00f3w, b\u0105d\u017a koszt produkt\u00f3w, to nie ma ona wp\u0142ywu na grupy produkt\u00f3w.","aaa1f2c4":"Dla hipotezy 2 sporz\u0105dzono tabele wielodzielcze dla zmeinnych jako\u015bciowych wzgl\u0119dem kolumny 'Customer_rating'","86d882d0":"2. Opracuj tabele liczno\u015bci dla ka\u017cdej zmiennej jako\u015bciowej z hipotez\n\nDo zbadania hipotez wykorzystane zostan\u0105 zmienne jako\u015bciowe z kolumn: 'Warehouse_block', 'Mode_of_Shipment','Product_importance' oraz 'Gender'. Poni\u017cej przedstawione zostan\u0105 tabele liczno\u015bci dla warto\u015bci tych zmiennych w formie histogram\u00f3w","2a375698":"Patrz\u0105c na sporz\u0105dzone tabele wielodzielcze dla hipotezy 1, ci\u0119\u017cko zauwa\u017cy\u0107 zale\u017cno\u015b\u0107 zmiennych jako\u015bciowych od powodzenia dostarczenia przesy\u0142ki na czas. W ka\u017cdym przypadku jest to stosunek ok. 40%-60% paczek dostarczonych na czas od niedostarczonych. Wyj\u0105tkiem od tej regu\u0142y s\u0105 paczki o wysokim priorytecie, gdzie stosunek paczek dostarczonych na czas do paczek niedostarczonych na czas wynosi 35%-65%. ","f540b369":"3. Odczytaj i sformalizuj na podstawie drzewa 3-5 regu\u0142 dla najbardziej wyrazistych klas \n    lub dla li\u015bci o najmniejszej wariancji (dla ka\u017cdej hipotezy).","c947ef7e":"Podobnie jak w cze\u015bniejszym przypadku nie mo\u017cna zauwa\u017cy\u0107 zale\u017cno\u015bci pomi\u0119dzy zmiennymi jako\u015bciowymi, a opiniami klient\u00f3w. Jeste\u015bmy w stanie zobaczy\u0107 rozk\u0142ad jednostajny udzia\u0142\u00f3w ka\u017cdej z kategorii dla ocen klient\u00f3w.","ae839bfb":"Jak wida\u0107 z ka\u017cdego bloku paczki wychodz\u0105 w podobnym stosunku co do rodzaju \u015brodka transportu 1:1:5 (Fliht:Road:Ship)","9f1fd486":"4. Oce\u0144 pewno\u015b\u0107 (prawdopodobie\u0144stwo) i wsparcie tych regu\u0142 (w drzewie regresyjnym oce\u0144 wariancj\u0119 w li\u015bciach).","1931cba8":"Wykresy pude\u0142kowe pozwalaj\u0105 lepiej zapozna\u0107 sie z rozk\u0142adem zmiennych w zespole danych oraz dodatkowo pozwalaj\u0105 w \u0142atwy spos\u00f3b zauwa\u017cy\u0107 warto\u015bci odstaj\u0105ce w postaci punkt\u00f3w wystaj\u0105cych poza w\u0105sy (punkty te znajduj\u0105 si\u0119 w zakresach kwantyli 0-25 oraz 75-100). Jednak zawsze przed zakwalifikowaniem jaki\u015b danych do warto\u015bci odstaj\u0105cych nale\u017cy si\u0119 zastanowi\u0107, co powinno si\u0119 z nimi zrobi\u0107 i czy wp\u0142ywaj\u0105 one w du\u017cym stopniu negatywnie na modele predykcyjne.\n    W przypadku Prior_purchases mamy doczynienia z 1003 warto\u015bciami odstaj\u0105cymi. Mo\u017cemy przygl\u0105dn\u0105\u0107 si\u0119 tym przesy\u0142kom.","bd9ba6b5":"Regu\u0142y:\n\nRegu\u0142a 1: przesy\u0142ki, kt\u00f3rym udzielono wi\u0119cej ni\u017c 10,5% rabatu nie dociera\u0142y na czas. Jest to relatywnie szybka konkuzja bo zosta\u0142a ona podj\u0119ta ju\u017c z samego korzenia. Warto\u015b\u0107 gini r\u00f3wna 0 sugeruje, \u017ce wszystkie dane, gdzie zni\u017cka wynosi\u0142a ponad 10,5% by\u0142y zaklasyfikowane jako przesy\u0142ki, kt\u00f3re nie zosta\u0142y dostarczone na czas. Mog\u0142o tak by\u0107, w\u0142a\u015bnie dlatego, \u017ce firma dawa\u0142a zni\u017cki na produkty, kt\u00f3rych dostawa si\u0119 op\u00f3\u017ania\u0142a. Im wi\u0119ksze op\u00f3\u017anienie tym wi\u0119ksza zni\u017cka. \n\nWsparcie tej regu\u0142y wynosi:\n1842\/7699 * 100% = 23,93%\n\nUfno\u015b\u0107 tej regu\u0142y wynosi:\n1842\/7699 * 100% = 23,93%\n\nRegu\u0142a 2: Przesy\u0142ki, kt\u00f3rym udzielono mniej lub r\u00f3wno 10,5% rabatu oraz, kt\u00f3re wa\u017c\u0105 mniej ni\u017c 4005,5g ale wi\u0119cej ni\u017c 2003,5g nie dociera\u0142y na czas. Konkluzja ta zosta\u0142a podj\u0119ta w w\u0119\u017ale 3 i r\u00f3wnie\u017c charakteryzuje si\u0119 bardzo nisk\u0105 warto\u015bci\u0105 gini (0.076) co implikuej, \u017ce wi\u0119kszo\u015b\u0107 pr\u00f3bek (oko\u0142o 92%) spe\u0142niaj\u0105ca t\u0105 regu\u0142\u0119 nie zostaje dostarczona na czas.\n\nWsparcie tej regu\u0142y wynosi:\n201\/7699 * 100% = 2,61%\n\nUfno\u015b\u0107 tej regu\u0142y wynosi:\n201\/462 * 100% = 43,51%\n\n\u017badne inne regu\u0142y nie s\u0105 na tyle wyraziste, \u017ceby je wymieni\u0107\n\n","2cf64b88":"Do sprawdzenia hipotezy drugiej dotycz\u0105cej wp\u0142ywu na oceny klient\u00f3w wybrano nast\u0119puj\u0105ce zmienne niezale\u017cne:\n\n* Cost_of_the_Product\n* Customer_care_calls\n\nZmienne zale\u017cne wybrano dzi\u0119ki obserwacjom z podpunktu drugiego. Zmienna Cost_of_the_product wykazywa\u0142a s\u0142ab\u0105 korelacj\u0119 ze zmienn\u0105 niezale\u017cn\u0105, natomiast Customer_care_calls zosta\u0142o wybrane jako dodatkowa zmienna. T\u0105 hipotez\u0119 bardzo trudno udowodni\u0107 z powodu braku zale\u017cnosci zmeinnej jakosciowej z innymi zmiennymi","9112f2b9":"Do sprawdzenia hipotezy trzeciej dotycz\u0105cej wp\u0142ywu na zastosowan\u0105 zni\u017ck\u0119 wybrano zmienne niezale\u017cne:\n\n* Reached.on.Time_Y.N\n* Weight_in_gms\n* Customer_care_calls\n\nZmienne zale\u017cne wybrano dzi\u0119ki obserwacjom z podpunktu drugiego. W tym wypadku mamy doczynienia z przewidywania warto\u015bci dlatego, nale\u017cy wykorzysta\u0107 drzewo regresyjne.","e30340e3":"Obserwuj\u0105c skategoryzowane wykresy pude\u0142kowe mo\u017cna zauwa\u017cy\u0107, \u017ce nie ma wi\u0119kszych r\u00f3znic pomi\u0119dzy poszzceg\u00f3lnymi kategoriami, a zmiennymi ilo\u015bciowymi","06eaf87a":"W wyniku testu niezale\u017cno\u015bci dla hipotezy pierwszej uzyskali\u015bmy informacj\u0119, \u017ce zmienne Customer_care_calls, Cost_of_the_Product, Prior_purchases, Product_importance, Discount_offered i Weight_in_gms s\u0105 zale\u017cne ze zmienn\u0105 jako\u015bciow\u0105 Reached.on.Time_Y.N\nNast\u0119pnie przeprowadzamy taki sam test dla hipotezy drugiej.","ecf514e9":"Przeprowadzona powy\u017cej analiza pokazuje, \u017ce nie jest mo\u017cliwe wyznaczenie konkrentych grup produkt\u00f3w kupowanych przez kobiety b\u0105d\u017a m\u0119\u017cczyzny.","146b9f6b":"1. Utw\u00f3rz drzewo klasyfikacyjne dla zmiennej jako\u015bciowej. Je\u015bli zmienna zale\u017cna jest ilo\u015bciowa, utw\u00f3rz drzewo regresyjne.\n\nDo sprawdzenia hipotezy pierwszej dotycz\u0105cej dostarczenia przesy\u0142ek na czas wybrano nast\u0119puj\u0105ce zmienne niezale\u017cne:\n* Discount_offered\n* Weight_in_gms\n* Product_importance\n\n\nDzi\u0119ki obserwacjom z podpunktu drugiego. Zrezygnowano ze zmiennej dotycz\u0105cej rodzaju \u015brodku transportu, oraz kosztu produktu poniewa\u017c, zbadana korelacja oraz test niezale\u017cnos\u0107 wskaza\u0142y niestotno\u015b\u0107 tych zmiennych na badan\u0105 zmienn\u0105 zale\u017cn\u0105.","ee2008fb":"Poni\u017cej wizyalizacja wynik\u00f3w","b7ddc537":"Z wcze\u015bniejszych analiz wiemy, \u017ce Prior_purchuases nie maj\u0105 wi\u0119kszej korelacji z innymi zmiennymi, jednak nie chc\u0105c traci\u0107 1003 rekord\u00f3w zdecydowano si\u0119 na zast\u0105pienie warto\u015bci Prior_purchases wi\u0119kszych b\u0105d\u017a r\u00f3wnych 6 przez losow\u0105 warto\u015b\u0107 w przedziale <2,5>","9b70c797":"# **VI. Zako\u0144czenie**\n\n**Projekt zosta\u0142 wykonany przez *Remigiusz Pomorskiego***","d88bcb50":"2. Okre\u015bl wa\u017cno\u015b\u0107 predyktor\u00f3w z u\u017cyciem wykresu.","de73a303":"Do wykonania wykres\u00f3w pude\u0142kowych skategoryzowanych wybrano pary:\nZmienna jako\u015bciowa: Reached.on.Time_Y.N, Zmienna ilo\u015bciowa: Cost_of_the_product\nZmienna jako\u015bciowa: Product_improtance, Zmianna ilo\u015bciowa: Cost_of_the_product","94d5d22c":"Dla hipotezy 3 i przygotowanej tabeli wielodzielczej, r\u00f3wnie\u017c jak w poprzednich przyk\u0142adach nie wida\u0107 wi\u0119kszej zale\u017cno\u015bci pomi\u0119zy zmiennymi jako\u015bciowymi a rodzajem zni\u017cki. Wszystkie s\u0105 na podobnym poziomie","e1851e71":"Z powodu s\u0142abej zale\u017cnosci pomi\u0119dzy zmiennymi wyja\u015bniaj\u0105cymi, a zmienn\u0105 obja\u015bnian\u0105 wygenerowane drzewo osi\u0105gn\u0119\u0142o bardzo ma\u0142\u0105 dok\u0142adno\u015bc jak i reg\u00f3\u0142y, kt\u00f3re zosta\u0142y wygenerowane maj\u0105 bardzo nisk\u0105 czysto\u015b\u0107 (wsp\u00f3\u0142czynnik gini na poziomie 0,8). Jednak\u017ce, na potrzeby zadania wybrano regu\u0142y:\n\n**Regu\u0142a 1:** Produkty, kt\u00f3re kosztuj\u0105 mniej b\u0105d\u017a r\u00f3wno 157,5 zosta\u0142y oceniane przez klient\u00f3w na 4.\n\nWsparcie tej regu\u0142y wynosi:\n854\/7699 * 100% = 11,09%\n\nUfno\u015b\u0107 tej regu\u0142y wynosi:\n854\/1492 * 100% = 57,24%\n\n**Regu\u0142a 2:** Produkty, kt\u00f3re kosztuj\u0105 mniej b\u0105d\u017a r\u00f3wno 245,5 oraz w sprawie kt\u00f3rych wykonano mniej ni\u017c 4 telefony zosta\u0142y oceniane przez klient\u00f3w na 1.\n\nWsparcie tej regu\u0142y wynosi:\n532\/7699 * 100% = 6,91%\n\nUfno\u015b\u0107 tej regu\u0142y wynosi:\n532\/1126 * 100% = 47,25%\n\n\n**Regu\u0142a 3:** Produkty, kt\u00f3re kosztuj\u0105 mniej b\u0105d\u017a r\u00f3wno 245,5 oraz w sprawie kt\u00f3rych wykonano wi\u0119cej ni\u017c 3 telefony zosta\u0142y oceniane przez klient\u00f3w na 5.\n\nWsparcie tej regu\u0142y wynosi:\n594\/7699 * 100% = 7,72%\n\nUfno\u015b\u0107 tej regu\u0142y wynosi:\n532\/1126 * 100% = 52,75%","e1d3b126":"# **V. Wybrany algorytm data mining**\n\n\nJako dodatkowy algorytm wybrano sieci neuronowe implementowane przy pomocy biblioteki tensorflow. Model ten zosta\u0142 wybrany, poniewa\u017c sieci neuronowe s\u0105 szeroko wykorzystywane w data science, a sam bardzo ma\u0142o pracowa\u0142em na tym typie modeli. Z tego powodu podj\u0105\u0142em decyzj\u0119 o zastosowaniu tego modelu w ostatnim podpunkcie tego projektu.","b16c6ba0":"Model jaki zbudowa\u0142em sk\u0142ada si\u0119 z 5 warstw:\n\n1. Warstwa wej\u015bciowa - sk\u0142ada si\u0119 z 3 neuron\u00f3w (po jednym na ka\u017cd\u0105 zmienn\u0105)\n2. Pierwsza warstwa wewn\u0119trzna - sk\u0142ada si\u0119 z 10 neuron\u00f3w, ka\u017cdy po\u0142\u0105czony z neuronami z warstwy wej\u015bciowej. Zastosowano w nim funkcj\u0119 aktywacyjn\u0105 relu. Neuron z funkcj\u0105 aktywacji ReLU przyjmuje dowolne warto\u015bci rzeczywiste jako swoje wej\u015bcie (a), ale aktywuje si\u0119 tylko wtedy, gdy te wej\u015bcie (a) s\u0105 wi\u0119ksze ni\u017c 0.\n3. Druga warstwa wewn\u0119trzna - warstwa przej\u015bciowa, kt\u00f3ra losowo ustawia jednostki wej\u015bciowe na 0 z cz\u0119stotliwo\u015bci\u0105 na ka\u017cdym kroku podczas treningu, co pomaga zapobiega\u0107 nadmiernemu dopasowaniu. Wej\u015bcia nie ustawione na 0 s\u0105 skalowane w g\u00f3r\u0119 o 1 \/ (1 - stawka) tak, \u017ce suma wszystkich wej\u015b\u0107 pozostaje niezmieniona.\n4. Trzecia warstwa wewn\u0119trzna - sk\u0142ada si\u0119 z 10 neuron\u00f3w, kt\u00f3ry ka\u017cdy po\u0142\u0105czony jest z neuronem drugiej warstwy wewn\u0119trznej. W warstwie tej zastosowan\u0105 funkcj\u0119 aktywacyjn\u0105 relu.\n5. Warstwa wyj\u015bciowa - sk\u0142ada si\u0119 z dw\u00f3ch neuron\u00f3w, kt\u00f3ry ka\u017cdy jest po\u0142\u0105czony z trzeci\u0105 warstw\u0105 wewn\u0119trzn\u0105. Zastosowano tutaj funkcj\u0119 aktywacyjn\u0105 softmax, kt\u00f3ra przetwarza warto\u015bci na ko\u0144cu neuron\u00f3w i zamienia je na warto\u015bci mi\u0119dzy 0 a 1\n\nDodatkowo sie\u0107 zosta\u0142a zbudowana z okre\u015bleniem optymalizatora Adam, kt\u00f3ry implementuje wyk\u0142adnicz\u0105 \u015bredni\u0105 ruchom\u0105 gradient\u00f3w, aby skalowa\u0107 tempo uczenia. Utrzymuje wyk\u0142adniczo malej\u0105c\u0105 \u015bredni\u0105 poprzednich gradient\u00f3w. Adam jest wydajny obliczeniowo i ma bardzo ma\u0142e wymagania dotycz\u0105ce pami\u0119ci. Adam Optimizer jest jednym z najpopularniejszych algorytm\u00f3w optymalizacji zst\u0119powania gradientu.\n    Opr\u00f3cz optymalizatora jako funkcj\u0119 koszt\u00f3w wybrano sparse_categorical_crossentropy, a do pomiar\u00f3w jako\u015bci sieci wyprano parametr accuracy.\n    \n![Relu function](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fmedium.com%2F%40toprak.mhmt%2Factivation-functions-for-deep-learning-13d8b9b20e&psig=AOvVaw3EEzpecG9FyBlkDr96vWuF&ust=1622138854520000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCOCRq6P45_ACFQAAAAAdAAAAABAJ)    ","fd5af626":"Analiza klastra 2:","90d81a67":"Analiza klastra 1:","27b79696":"3. Odczytaj i sformalizuj na podstawie drzewa 3-5 regu\u0142 dla najbardziej wyrazistych klas \n    lub dla li\u015bci o najmniejszej wariancji (dla ka\u017cdej hipotezy).","201d42ab":"W wyniku testu niezale\u017cno\u015bci dla hipotezy trzeciej uzyskali\u015bmy informacj\u0119, \u017ce zmienne Customer_care_calls, Prior_purchases, Product_importance, Weight_in_gms i Reached.on.Time_Y.N s\u0105 zale\u017cne ze zmienn\u0105 jako\u015bciow\u0105 Discount_offered\n\n8. Wykonaj wykresy ramka-w\u0105sy dla wszystkich zmiennych ilo\u015bciowych z hipotez.\n    Wybierz dwie pary zmiennych (ilo\u015bciowa-jako\u015bciowa) i wykonaj wykresy","fa91ad0c":"Analiza klastra 2:","7a652316":"Jak wida\u0107 po powy\u017cszym przyk\u0142adzie model osi\u0105gn\u0105\u0142 nieco gorsze wyniki je\u017celi chodzi o dok\u0142adno\u015b\u0107 por\u00f3wnuj\u0105c do modelu drzewa. Wida\u0107 spore skoki je\u017celi chodzi dok\u0142adno\u015b\u0107 modelu oraz o wartos\u0107 funkcji kosztu. Mo\u017ce to oznacza\u0107, \u017ce s\u0105 du\u017ce zmiany je\u017celi chodzi o warto\u015bci niekt\u00f3rych zmiennych, kt\u00f3re rozstrajaj\u0105 model. ZAby temu zaradzi\u0107, mo\u017cemy przeprowadzi\u0107 standaryzacj\u0119 cech.","2fbf251d":"Por\u00f3wnuj\u0105c wyniki pomi\u0119dzy analiz\u0105 skupie\u0144 metod\u0105 K-najbli\u017cszych s\u0105siad\u00f3w a metod\u0105 EM, mo\u017cna zauwa\u017cy\u0107 niewielkie r\u00f3\u017cnice. Analiza metod\u0105 EM podzieli\u0142a zbi\u00f3r danych g\u0142\u00f3wnie ze wzgl\u0119du na mas\u0119 produkt (dla klastra 1 produkty ci\u0119\u017ckie w przedziale wagowym od 2655g do 7846g, a dla klastra 2 od 1001g do 2697g), ale w odr\u00f3\u017cnieniu od K-najbli\u017cszych s\u0105siad\u00f3w wzi\u0119\u0142a r\u00f3wnie\u017c pod uwag\u0119 koszt produktu, gdzie klaster 2 jest nieco bardziej przesuni\u0119ty w praw\u0105 stron\u0119 i \u015brednia warto\u015b\u0107 wynosi dla niego 216, natomiast dla klastra 1 wynosi 206","e651c6f3":"Analizuj\u0105c wyniki predykcji drzewa decyzyjnego jeste\u015bmy w stanie potwierdzi\u0107 hipotez\u0119, \u017ce to czy przesy\u0142ki zosta\u0142y dostarczone na czas zale\u017c\u0105 od za\u0142o\u017conych zmiennych obja\u015bniaj\u0105cych. Dok\u0142adno\u015b\u0107 modelu zosta\u0142a okre\u015blona na poziomie oko\u0142o 70% co jest bardzo dobrym wynikiem patrz\u0105c na bardzo s\u0142abe zale\u017cno\u015bci pomi\u0119dzy zmiennymi obja\u015bniaj\u0105cymi a zmienn\u0105 obja\u015bnian\u0105 oraz patrz\u0105c na rodzaj dostarczonych zmiennych (brak przewidywalnego rozk\u0142adu zmiennych).Dodatkowo nast\u0119pnie sprawdzone precission i recall pokazuj\u0105, \u017ce model spe\u0142nia swoje za\u0142o\u017cenia w formie zadowalaj\u0105cej. Przeprowadzaj\u0105c dodatkow\u0105 optymalizacj\u0119 mo\u017cnaby by\u0142o uzyska\u0107 jeszcze lepsze wyniki i poprawienie warto\u015bci recall, jednak\u017ce bior\u0105c pod uwag\u0119 dostarczone dane wyniki s\u0105 zadowalaj\u0105ce.","289029a9":"2. Okre\u015bl wa\u017cno\u015b\u0107 predyktor\u00f3w z u\u017cyciem wykresu.","3de1d4c4":"1. Opracuj podstawowe statystyki dla ka\u017cdej zmiennej ilo\u015bciowej.\n\nDla wybranego zbioru zmienne ilo\u015bciowe obejmuj\u0105 kolumny:\nCustomer_care_calls, Cost_of_the_Product, Prior_purchuases, Discount_offered, Weight_in_gms. I te kolumny zostan\u0105 poddane analizie statystycznej","37cdccec":"# **II. Czyszczenie i analiza danych**","e3e4fc10":"Sprawdzamy jak zmieni\u0142 si\u0119 rozk\u0142ad zmiennych","dfe5b2f3":"# **III Indukcja drzew decyzyjnych**","7fc4257a":"4. Analiza skupie\u0144 metod\u0105 EM.","dfc5cf09":"Przed podj\u0119ciem pracy nad danym, zestawem danych nale\u017cy przebada\u0107, czy dane nie posiadaj\u0105 brak\u00f3w.","4bed6037":"Dla hipotezy 3 sporz\u0105dzono tabele wielodzielcze dla zmeinnych jako\u015bciowych wzgl\u0119dem kolumny 'Discount_offered'. Z powodu, \u017ce kolumna \"Discount_offered\" zawiera r\u00f3\u017cne warto\u015bci to, aby ja\u015bniej i bardziej zrozumiale przedstawi\u0107 dane, warto\u015bci te zosta\u0142y zamkni\u0119te w 3 r\u00f3wnych przedzia\u0142ach pomi\u0119dzy warto\u015bci\u0105 minimum 0 a maksimum 65 (warto\u015bci z wcze\u015bniejszej analizy zmiennych ilo\u015bciowych).","0e0b0ca0":"Na samym pocz\u0105tku sprawdzimy niezale\u017cno\u015b\u0107 ka\u017cdej z kolumn wzgl\u0119dem kolumny \"Reached.on.Time_Y.N\" (hipoteza 1)","97a8294b":"W wyniku testu niezale\u017cno\u015bci dla hipotezy drugiej uzyskali\u015bmy informacj\u0119, \u017ce wszystkie zmienne s\u0105 niezale\u017cne ze zmienn\u0105 jako\u015bciow\u0105 Customer_rating\nNa samym ko\u0144cu przeprowadzamy ten sam test dla trzeciej ostatniej hipotezy.","4c5eea35":"Na powy\u017cszych histogramach mo\u017cna zauwa\u017cy\u0107, \u017ce dane dostarczone do analizy posiadaj\u0105 zbli\u017cony do siebie rozk\u0142ad prawdopodobie\u0144stwa dla p\u0142ci oraz oceny klient\u00f3w. Dla kolumny Warehouse_block widzimy, \u017ce dane zawieraj\u0105 tak\u0105 sam\u0105 ilo\u015b\u0107 zam\u00f3wie\u0144 pochodz\u0105cych z blok\u00f3w A,B,C oraz D. Z bloku F natomiast pochodzi ok. 2 razy wi\u0119cej zamowie\u0144 ni\u017c z pozosta\u0142ych. Na kolejnym histogramie (Mode_of_shipement) mo\u017cna zauwa\u017cy\u0107 analogiczn\u0105 sytuacj\u0119, gdzie paczki s\u0105 rzadziej dostarczane samolotem i drog\u0105 l\u0105dow\u0105 ni\u017c statkiem. Mo\u017cna sprawdzi\u0107 jak dostarczane s\u0105 pazcki z ka\u017cdego bloku.","16bdf797":"Automatyczny wyb\u00f3r ilo\u015bci klastr\u00f3w metod\u0105 silhouette score r\u00f3wnie\u017c\u00a0wskazuje na 2 klastry","f88947d5":"Zamiast wariancji sklearn wykorzystuje MSE (Mean Square Error) do okre\u015blenia jako\u015bci drzewa. Jak wida\u0107 lewa cz\u0119\u015b\u0107 drzewa wygl\u0105da bardzo dobrze (MSE w li\u015bciach na niskim poziomie ok 300-400) co wskazuje na bardzo dobre dopasowanie \u015bredniej warto\u015bci w li\u015bciach do warto\u015bci rzeczywistych. Prawa strona drzewa natomiast posiada bardzo wysokie warto\u015b\u0107i MSE (na poziomie oko\u0142o 8000) co powoduje, \u017ce drzewo jest bardzo ma\u0142o wiarygodne je\u017celi bedziemy chcieli przewidywa\u0107 warto\u015bci zaoferowanych zni\u017cek dla paczek kt\u00f3re wa\u017c\u0105 wi\u0119cej ni\u017c 4000g. Potwierdza to warto\u015bc dopasowania modelu (51,99%) wskazuj\u0105c, \u017ce model jest dobrze dopasowany tylko w po\u0142owie. Dlatego hipotez\u0119 t\u0105 mo\u017cna przyj\u0105\u0107 tylko dla paczek poni\u017cej wagi 4000g. Dla ca\u0142ej populacji nale\u017cu odrzuci\u0107 hipotez\u0119.","6958dea7":"Poni\u017cej wizualizacja wynik\u00f3w","57262d47":"# **I. Wyb\u00f3r tematu i zbioru danych**\n\nDo wykonania projektu zdecydowa\u0142em si\u0119 wykorzysta\u0107 j\u0119zyk Python. Jako \u015brodowisko notatnik Kaggle, a jako baz\u0119 danych E-commerce shipping Data.\n\nPowy\u017cszy zbi\u00f3r danych posiada nast\u0119puj\u0105ce kolumny:\n* **ID:** numer identyfikacyjny klient\u00f3w.\n* **Warehouse block:** Firma posiada du\u017cy Magazyn, kt\u00f3ry jest podzielony na bloki takie jak A, B, C, D, E.\n* **Mode of shipment:** Firma wysy\u0142a produkty na wiele sposob\u00f3w, np. Drog\u0105 morsk\u0105, lotnicz\u0105 i drogow\u0105.\n* **Customer care calls:** Liczba po\u0142\u0105cze\u0144 wykonanych z zapytania o zapytanie o przesy\u0142k\u0119.\n* **Customer rating:** firma wystawi\u0142a ocen\u0119 od ka\u017cdego klienta. 1 to najni\u017csza (najgorsza), 5 to najwy\u017csza (najlepsza).\n* **Cost of the product:** Koszt produktu w dolarach ameryka\u0144skich.\n* **Prior purchases:** liczba wcze\u015bniejszych zakup\u00f3w.\n* **Product importance:** Firma sklasyfikowa\u0142a produkt pod k\u0105tem r\u00f3\u017cnych parametr\u00f3w, takich jak niski, \u015bredni, wysoki.\n* **Gender:** m\u0119\u017cczyzna i kobieta.\n* **Discount offered:** Rabat oferowany na ten konkretny produkt.\n* **Weight in gms:** jest to waga w gramach.\n* **Reached on time:** Jest to zmienna docelowa, gdzie 1 oznacza, \u017ce produkt NIE zosta\u0142 dostarczony na czas, a 0 oznacza, \u017ce zosta\u0142 dostarczony na czas.\n\n**Cel analizy**\nWybrano trzy hipotezy, kt\u00f3re zostan\u0105 sprawdzone w poni\u017cszej pracy. Ka\u017cda z hipotez zosta\u0142a oparta o inn\u0105 zmienn\u0105 zale\u017cn\u0105.\n\n**Hipoteza 1:** Przedmioty o wy\u017cszym priorytecie, dro\u017csze i transportowane drog\u0105 lotnicz\u0105 zostaj\u0105 dostarczone na czas.\n\n**Hipoteza 2:** Osoby ch\u0119tniej daj\u0105 wy\u017csze wyniki w zale\u017cno\u015bci od obs\u0142ugi klienta i jako\u015bci us\u0142ug.\n\n**Hipoteza 3:** Badanie wysoko\u015bci zni\u017cek zaproponowanym klientom w zale\u017cno\u015bci od ich p\u0142ci, koszt\u00f3w produktu i historii ich zakup\u00f3w.","c51ced37":"Dla hipotezy 1 sporz\u0105dzono tabele wielodzielcze dla zmeinnych jako\u015bciowych wzgl\u0119dem kolumny Reached_on_time","d9712333":"7. Test niezale\u017cno\u015bci przy u\u017cyciu testu chi^2 przy za\u0142o\u017ceniu poziomu istotno\u015bci alfa = 0.05, zak\u0142adaj\u0105c za hipotez\u0119 zerow\u0105, \u017ce zmienne s\u0105 niezale\u017cne.","fa8afeb4":"4. Oce\u0144 pewno\u015b\u0107 (prawdopodobie\u0144stwo) i wsparcie tych regu\u0142 (w drzewie regresyjnym oce\u0144 wariancj\u0119 w li\u015bciach).","4d648dc8":"# **IV. Analiza skupie\u0144:**\n\n1. Wybierz zmienne do analizy - uzasadnij.\n\nDo wykonania analizy skupie\u0144 wybrano zmienne:\n\n* Gender\n* Weight_in_gms\n* Cost_of_the_Product\n\nCelem analizy jest sprawdzenie, czy jeste\u015bmy w stanie wyznaczy\u0107 jakie\u015b konkretne grupy produkt\u00f3w kupowanych przez konsument\u00f3w r\u00f3\u017cnych p\u0142ci w zale\u017cno\u015bci od ich masy i koszt\u00f3w produktu.","2415b35e":"2. Okre\u015bl wa\u017cno\u015b\u0107 predyktor\u00f3w z u\u017cyciem wykresu.","82bc563d":"5. Wykonanie macierzy korelacji","5d8759a7":"9. Wykonaj test normalny dla zmiennych - oznacz warto\u015bci odstaj\u0105ce \n\nNa samym pocz\u0105tku sporz\u0105dzono wykresy rozk\u0142adu warto\u015bci zmiennych ilo\u015bciowych, aby sprawdzi\u0107 czy dana analiza jest potrzebna i przydatna do analizy.","f6e2e2db":"Jak wida\u0107 dla ka\u017cdej kolumny zmiennej ilo\u015bciowej nie posiadamy rozk\u0142adu normalnego. Jest to cecha tego zbioru danych i nie powinno si\u0119 zmienia\u0107 jego warto\u015bci, aby rozk\u0142ad zmiennych by\u0142 bliski do rozk\u0142adu normalnego. Niestety z tego powodu nie mo\u017cemy stosowa\u0107 parametrycznych metod statystycznych w celu przewidywania zmiennych zale\u017cnych.","64090283":"Analizuj\u0105c wyniki drzew decyzyjnych dla hipotezy drugiej nale\u017cy j\u0105 odrzuci\u0107. Model radzi sobie bardzo s\u0142abo (podobne prawdopodobie\u0144stwo mo\u017cna by by\u0142o uzyska\u0107 losuj\u0105c liczby przy pomocy generatora, b\u0105d\u017a kostki). Wyniki potwierdza dodatkowo wykonana macierz klasyfikacji i wylizcone z niej parametry. Dodatkowo regu\u0142y drzewa decyzyjnego s\u0105 obarczone bardzo du\u017c\u0105 niepewno\u015bci\u0105.","acba8ac1":"Wykluczaj\u0105c kolumny ID, mo\u017cemy zauwa\u017cy\u0107, \u017ce s\u0142aba korelacja wyst\u0119puje dla par zmiennych:\n\n* Weight_in_gms i Customer_care_calls (-0.28) - jest to s\u0142aba ujemna korelacja co oznacza, \u017ce wraz ze wzrostem jednej zmiennej druga maleje. Oznacza\u0142oby to, \u017ce wraz ze wzrostem wagi spada zainteresowanie sprzedawcy o satysfakcj\u0119 klienta co wydaje si\u0119 nielogiczne, wi\u0119c mo\u017cemy odrzuci\u0107 t\u0105 par\u0119.\n* Weight_in_gms i Discount_offered (-0.38) - jest to s\u0142aba ujemna korelacja, kt\u00f3ra oznacza\u0142aby, \u017ce sprzedawca obiecuje mniejsze zni\u017cki na ci\u0119\u017csze produkty. Jest to mo\u017cliwy scenariusz, poniewa\u017c l\u017cejsze przedmioty zajmuj\u0105 mniej miejsca, przez co spadaj\u0105 koszty wysy\u0142ki i mo\u017cliwe jest, \u017ce sprzedawca b\u0119dzie oferowa\u0142 za takie przedmioty wi\u0119ksze zni\u017cki.\n* Weight_in_gms i Reached_on_time (-0.27) - jest to s\u0142aba ujemna korelacja, kt\u00f3ra oznacza\u0142aby, \u017ce ci\u0119\u017csze przesy\u0142ki cz\u0119\u015bciej zostaj\u0105 dostarczone na czas. Jest to mo\u017cliwy scenariusz, poniewa\u017c ci\u0119\u017csze przeys\u0142ki mog\u0105 kosztowa\u0107 wi\u0119cej przez co sprzedawca przyk\u0142ada wi\u0119cej uwagi takim przesy\u0142kom. Niestety hipoteza ta nie jest prawdziwa, poniewa\u017c patrz\u0105c nast\u0119pnie na korelacj\u0119 kosztu produktu z dostarczaniem przesy\u0142ek wida\u0107, \u017ce nie s\u0105 one skorelowane.\n* Discount_offered i Reached_on_time (0.4) - jest to s\u0142aba dodatnia korelacja, kt\u00f3ra oznacza\u0142aby, \u017ce im wi\u0119ksza jest zni\u017cka na przeys\u0142k\u0119 tym cz\u0119\u015bciej przesy\u0142ki zostaj\u0105 dostarczone. Mo\u017ce to by\u0107 korelacja pozorna, poniewa\u017c ci\u0119\u017cko znale\u017a\u0107 logiczne wyt\u0142umaczenie takiego stanu rzeczy.\n* Customer_care_call i Cost_of_the_Product (0.32) - jest to s\u0142aba dodatnia korelacja, kt\u00f3ra oznacza\u0142aby, \u017ce im wi\u0119ksza cena produktu tym ch\u0119tniej sprzedawca, dba o klienta. Jest to logiczne wyt\u0142umaczenie tej korelacji.\n\nZ powy\u017cszych wymienionych korelacji jedyn\u0105, kt\u00f3ra wydaje si\u0119 mie\u0107 logiczne wyt\u0142umaczenie jest korelacja pomi\u0119dzy Customer_care_call i Cost_of_the_Product. Reszta wydaje si\u0119 by\u0107 korelacj\u0105 pozorn\u0105."}}