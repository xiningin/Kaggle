{"cell_type":{"37743dd0":"code","b2947852":"code","86a3fe18":"code","87e14d81":"code","cb09e935":"code","7aadf6a7":"code","87462b5f":"code","e2094144":"code","dca9acb5":"code","74749aa0":"code","f968db27":"code","764d0be9":"code","8212b9e5":"code","2c6007e7":"markdown"},"source":{"37743dd0":"import numpy as np \nimport pandas as pd \nimport shutil\nimport os\nimport zipfile\nimport torch\nimport torch.nn as nn\nimport cv2\nimport matplotlib.pyplot as plt\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import transforms\nimport copy\nimport tqdm\nfrom PIL import Image\n\nimport albumentations\nfrom albumentations import pytorch as AT\n\n\n\n%matplotlib inline","b2947852":"train_dir = '\/kaggle\/input\/korpus-ml-2\/train\/train\/train\/'\ntest_dir = '\/kaggle\/input\/korpus-ml-2\/test\/test\/test\/'\ntrain_files = os.listdir(train_dir)\ntest_files = os.listdir(test_dir)","86a3fe18":"print(len(train_files), len(test_files))","87e14d81":"train_files[:10]","cb09e935":"class PrepareDataset(Dataset):\n\n    def __init__(self, file_list, dir, transform=None, mode='train'):\n        self.file_list = file_list\n        self.dir = dir\n        self.mode = mode\n        self.transform = transform\n        self.label = 0\n        self.name_to_label = {'bar_chart' : 1, 'diagram' : 2, 'flow_chart' : 3,\n                              'graph' : 4, 'growth_chart' : 5, 'pie_chart' : 6,\n                              'table' : 7, 'just_image' : 0}\n        self.label_to_name = {1 : 'bar_chart', 2 : 'diagram', 3 : 'flow_chart',\n                              4 : 'graph', 5 : 'growth_chart', 6 : 'pie_chart',\n                              7 : 'table', 0 : 'just_image'}\n            \n    def __len__(self):\n        return len(self.file_list)\n    \n    #\u043c\u0435\u0442\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0430\u043c \u0438\u043d\u0434\u0435\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n    def __getitem__(self, idx):\n        #\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\n        image_name = self.file_list[idx]\n        full_path = os.path.join(self.dir, image_name)\n        if image_name.split(\".\")[1] == \"gif\":\n          gif = cv2.VideoCapture(full_path)\n          _, image = gif.read()\n        else:\n            image = cv2.imread(full_path)\n \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.mode == 'train':\n            for name, label in self.name_to_label.items():\n                if name in self.file_list[idx]:\n                    self.label = label\n                    break\n\n        #\u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        if self.mode == 'train':\n            return image, self.label\n        else:\n            return image, image_name","7aadf6a7":"!lscpu","87462b5f":"import multiprocessing\n\nmultiprocessing.cpu_count()","e2094144":"#\u0437\u0430\u0434\u0430\u0434\u0438\u043c \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n\n\nbatch_size = 32\nnum_workers = multiprocessing.cpu_count()\nimg_size = 256","dca9acb5":"#\u041f\u0440\u0438\u043c\u0435\u0440 \u0442\u043e\u0433\u043e \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u043e\u0431\u044b\u0447\u043d\u043e \u043d\u0430\u0431\u043e\u0440 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439. \n#\u0412 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0437\u0430\u0434\u0430\u0447\u0438 \u043c\u043e\u0436\u0435\u0442 \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c\u0441\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c\u044e.\n\ndata_transforms = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n#     albumentations.Normalize(),\n    AT.ToTensor()\n    ])\n\n#\u043e\u0431\u044b\u0447\u043d\u043e \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432 \u0440\u0430\u0437\u0434\u0435\u043b\u0430\u044e\u0442. \n#\u041d\u0430 \u0442\u0435\u0441\u0442\u0435 \u043e\u0431\u044b\u0447\u043d\u043e \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u0441\u0438\u043b\u044c\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u044f\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n#     albumentations.Normalize(),\n    AT.ToTensor()\n    ])","74749aa0":"#\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u044b\ntrainset = PrepareDataset(train_files, train_dir, transform = data_transforms)\ntestset = PrepareDataset(test_files, test_dir, \n                        transform=data_transforms_test, mode = \"test\")","f968db27":"#\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0442\u0440\u0435\u0439\u043d\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u043d\u0430 \u0442\u0440\u0435\u0439\u043d \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e.\n\nvalid_size = int(len(train_files) * 0.15) #\u0440\u0430\u0437\u043c\u0435\u0440 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 (10-15%)\nindices = torch.randperm(len(trainset)) #\u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043b\u0438\u0441\u0442 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432\n#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0434\u043b\u044f \u0442\u0440\u0435\u0439\u043d \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\ntrain_indices = indices[:len(indices)-valid_size] \nvalid_indices = indices[len(indices)-valid_size:]\n\n#\u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u0434\u0435\u0440\u044b \u0434\u043b\u044f \u0432\u0441\u0435\u0445 3\u0445 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a.\ntrainloader = torch.utils.data.DataLoader(trainset, pin_memory=True, \n                                        batch_size=batch_size,\n                                        sampler=SubsetRandomSampler(train_indices))\nvalidloader = torch.utils.data.DataLoader(trainset, pin_memory=True, \n                                        batch_size=batch_size,\n                                        sampler=SubsetRandomSampler(valid_indices))\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size = batch_size,\n                                         num_workers = num_workers)","764d0be9":"#\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u043e\u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c\nsamples, labels = next(iter(validloader))\nplt.figure(figsize=(32,48))\ngrid_imgs = torchvision.utils.make_grid(samples[:24])\nnp_grid_imgs = grid_imgs.numpy()\n# in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\nplt.imshow(np.transpose(np_grid_imgs, (1,2,0)), interpolation='nearest')\n","8212b9e5":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0443\u0431\u0435\u0434\u0438\u043c\u0441\u044f, \u0447\u0442\u043e \u043c\u0435\u0442\u043a\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u043a\u043b\u0430\u0441\u0441\u0430\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\nfor label in labels[:24]:\n    print(trainset.label_to_name.get(int(label)))","2c6007e7":"# Time to train!"}}