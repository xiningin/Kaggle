{"cell_type":{"9953131e":"code","31791cfd":"code","51f5be42":"code","bffe35f6":"code","9ad8a0d2":"code","a87debb7":"code","b8d31626":"code","0ee48761":"code","91390595":"code","b2a78997":"code","56700cbf":"markdown","fa96a4ce":"markdown","fcc74980":"markdown","912141be":"markdown","1ecf4f3e":"markdown","4030ae3f":"markdown","11a09f3b":"markdown","895dc996":"markdown","67dce115":"markdown","49e307f5":"markdown"},"source":{"9953131e":"import torch\ntorch.set_printoptions(precision=4, sci_mode=False, linewidth=150)\n\ndef focal_binary_cross_entropy(logits, targets, gamma=2):\n    l = logits.reshape(-1)\n    t = targets.reshape(-1)\n    p = torch.sigmoid(l)\n    p = torch.where(t >= 0.5, p, 1-p)\n    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n    loss = logp*((1-p)**gamma)\n    loss = num_label*loss.mean()\n    return loss","31791cfd":"num_label = 8\nlogits = torch.tensor([[-5., -5, 0.1, 0.1, 5, 5, 100, 100]])\ntargets = torch.tensor([[0, 1, 0, 1, 0, 1, 0, 1]])\nlogits.shape, targets.shape","51f5be42":"l = logits.reshape(-1)\nt = targets.reshape(-1)\nl.shape, t.shape","bffe35f6":"p = torch.sigmoid(l)\np","9ad8a0d2":"p = torch.where(t >= 0.5, p, 1-p)\np","a87debb7":"clamp_p = torch.clamp(p, 1e-4, 1-1e-4)\nclamp_p","b8d31626":"logp = - torch.log(clamp_p)\nlogp","0ee48761":"gamma = 2\nprint(f'logp: {logp}')\nprint(f'p: {p}')\nprint(f'1-p: {1-p}')\nx = (1-p)**gamma\nprint(f'(1-p)**gamma: {x}')\nloss = logp*((1-p)**gamma)\nprint(f'logp*((1-p)**gamma):{loss}')","91390595":"loss = num_label*loss.mean()\nloss","b2a78997":"import tensorflow_addons as tfa\nimport tensorflow as tf\nfl = tfa.losses.SigmoidFocalCrossEntropy(from_logits=True, alpha=0.5)\ntlogits = tf.constant([[-5.], [-5], [0.1], [0.1], [5],[5], [100], [100]], dtype=tf.float32)\nttargets = tf.constant([[0], [1], [0], [1], [0], [1], [0], [1]], dtype=tf.float32)\nfl(ttargets, tlogits) * 2","56700cbf":"Logits come directly from the model output - let's use a range of possible values. For each logit value, let's have both 0 an 1 targets. We'll work with batch size = 1 and 8 classes. ","fa96a4ce":"The additional thing that happens here is clamping the input to avoid being to close to zero or one. This is probably for numeric stability. ","fcc74980":"Now finally the focal loss magic: `loss = logp*((1-p)**gamma)`. Let's take it apart to really understand what happens. Our default for gamma is `2`. ","912141be":"So what happened here?\n1. logp is the classic BCE loss\n2. p is close to 1 for good predictions, close to 0 for bad predictions\n3. 1-p is the opposite: close to 0 for good predictions, close to 1 for bad predictions\n4. With gamma=2 we square (1-p): the better the predictions, the closer to zero we will take them\n5. We multiply this with the original BCE loss\n\nThe outcome: \n- for good predictions, the loss is much smaller than it was, getting close to zero\n- for bad predictions, the loss is not changed much\n\nThis will force the model to learn more from bad predictions, while the good predictions will have limited impact on the parameters.\n\nFinally, we need to revert the flattening than we've done at the beginning. We'll simply take the mean of all loss values and multiply it by the number of labels. ","1ecf4f3e":"# Focal Multilabel Loss in Pytorch Explained\n\nTop solutions in the previous HPA competition used focal loss. I'd like to understand this loss function better and this notebook helped me, hoping it can be helpful for others too! Please upvote if it is :) \n\nThe [original paper](https:\/\/arxiv.org\/pdf\/1708.02002.pdf) defines focal loss in multiclass classification scenario which is different from multilabel that we're dealing with in this competition. Here's a visual with high level summary copied from [Papers with Code](https:\/\/paperswithcode.com\/method\/focal-loss). The main promise is that this will help us deal with imbalanced datasets by focusing on hard-to-predict classes.\n\n![focal_loss.png](attachment:focal_loss.png)\n\nThe loss function code is coming from @hengck23 script shared in [this thread](https:\/\/www.kaggle.com\/c\/hpa-single-cell-image-classification\/discussion\/217238). Let's take it apart!","4030ae3f":"Now we apply negative log - this is the BCE loss. This will convert good predictions to a loss that is close to 0, and bad predictions will go to infinity.","11a09f3b":"Next, we apply sigmoid to the logits to squeeze the values between 0 and 1. ","895dc996":"We are following now the standard binary cross entropy with logits loss. For positive examples, we'll take the sigmoid, for negative examples, we'll take 1-sigmoid. Good predictions will be close to 1, bad predictions will be close to 0. ","67dce115":"The first thing that happens is flattening of logits and targets so that we can compare two vectors (rank-1 tensors). ","49e307f5":"Based on @dschettler8845 comment I tried to reproduce the same in tensorflow - I think the snippet below gets \"close enough\", the key difference is that I omitted alpha in the implementation above (in Pytorch, alpha can be handled by weights). "}}