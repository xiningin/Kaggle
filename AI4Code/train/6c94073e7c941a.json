{"cell_type":{"c9f9604e":"code","fb8e93e4":"code","6fd433a4":"code","5d0caba5":"code","7ef98763":"code","87b2a2a9":"code","d0acf9fa":"code","86212d1b":"code","aa2b583d":"code","d644ba11":"code","07802e25":"code","520cb643":"code","f7472b8a":"code","df2a4d50":"code","4d91c197":"code","93e171cb":"code","7a71a7c6":"code","564e9447":"code","85b3f1c7":"code","cc5b82cf":"code","c25bcf49":"code","5d584080":"markdown","9c5b1ff5":"markdown","7aff0aac":"markdown","cda0a7d0":"markdown","8b486e61":"markdown","80208813":"markdown","c8f3aad1":"markdown","c39356e0":"markdown","61987b59":"markdown","77f5f658":"markdown","ddf02679":"markdown","cd82f685":"markdown"},"source":{"c9f9604e":"!pip install sentence_transformers\n!pip install textstat","fb8e93e4":"from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.mixture import GaussianMixture\nimport umap\nimport textstat\nplt.style.use('fivethirtyeight')","6fd433a4":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","5d0caba5":"train_df.head()","7ef98763":"plt.figure(figsize = (8,5))\nsns.histplot(train_df.target)","87b2a2a9":"plt.figure(figsize = (8,5))\nsns.histplot(train_df.standard_error)","d0acf9fa":"ind = np.where(train_df.standard_error == train_df.standard_error.min())[0]\ntrain_df.loc[ind]","86212d1b":"train_df.drop(ind, inplace = True)\ntrain_df.reset_index(inplace = True,drop = True)","aa2b583d":"# bert = SentenceTransformer('bert-base-uncased')\nroberta = SentenceTransformer('roberta-base')\nvects = roberta.encode(train_df.excerpt)","d644ba11":"# Probably isn't neccesary to scale these vectors\nscaler = StandardScaler()\nscaled = scaler.fit_transform(vects)","07802e25":"tsne_embedding = TSNE(2).fit_transform(scaled)","520cb643":"px.scatter(train_df, x = tsne_embedding[:, 0], y = tsne_embedding[:, 1], color = 'target',\n                 labels = {'x' : 'Dimension 1', 'y' : 'Dimension 2'},\n                 title = 'TSNE Projection of Roberta Sentence Representations')","f7472b8a":"reducer = umap.UMAP(random_state = 123)\numap_embedding = reducer.fit_transform(scaled)","df2a4d50":"px.scatter(train_df, x = umap_embedding[:, 0], y = umap_embedding[:, 1], color = 'target',\n                 labels = {'x' : 'Dimension 1', 'y' : 'Dimension 2'},\n                 title = 'UMAP Projection of Roberta Sentence Representations')","4d91c197":"px.scatter(train_df, x = umap_embedding[:, 0], y = umap_embedding[:, 1], color = 'standard_error',\n                 labels = {'x' : 'Dimension 1', 'y' : 'Dimension 2'},\n                 title = 'UMAP Projection of Roberta Sentence Representations')","93e171cb":"gmm = GaussianMixture(n_components = 6, random_state = 123)\nclusters = gmm.fit_predict(vects)","7a71a7c6":"px.scatter(train_df, x = umap_embedding[:, 0], y = umap_embedding[:, 1], color = clusters,\n                 labels = {'x' : 'Dimension 1', 'y' : 'Dimension 2'},\n                 title = 'UMAP Projection of Roberta Sentence Representations')","564e9447":"train_df['is_licensed'] = train_df.license.notna()*1 # might be interesting to look at?\n\ntrain_df['character_count'] = train_df['excerpt'].apply(lambda x: len(str(x)))\ntrain_df['digit_count'] = train_df['excerpt'].apply(lambda x: np.sum(([int(word.isdigit()) for word in str(x).split()])))\ntrain_df['word_count'] = train_df['excerpt'].apply(textstat.lexicon_count)\ntrain_df['unique_word_count'] = train_df['excerpt'].apply(lambda x: len(set(str(x).split())))\ntrain_df['mean_word_length'] = train_df['excerpt'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\ntrain_df['syllable_count'] = train_df['excerpt'].apply(textstat.syllable_count)\ntrain_df['sentence_count'] = train_df['excerpt'].apply(textstat.sentence_count)\ntrain_df['flesch_reading_ease'] = train_df['excerpt'].apply(textstat.flesch_reading_ease)\ntrain_df['flesch_kincaid_grade'] = train_df['excerpt'].apply(textstat.flesch_kincaid_grade)\ntrain_df['smog_index'] = train_df['excerpt'].apply(textstat.smog_index)\ntrain_df['automated_readability_index'] = train_df['excerpt'].apply(textstat.automated_readability_index)\ntrain_df['coleman_liau_index'] = train_df['excerpt'].apply(textstat.coleman_liau_index)\ntrain_df['linsear_write_formula'] = train_df['excerpt'].apply(textstat.linsear_write_formula)","85b3f1c7":"numeric_df = train_df.select_dtypes(include=np.number)\nscaled_df = pd.DataFrame(MinMaxScaler(feature_range=(0, 1)).fit_transform(numeric_df), \n                         index = numeric_df.index, \n                         columns = numeric_df.columns)\n\nscaled_df['cluster'] = clusters","cc5b82cf":"agg = scaled_df.groupby('cluster').mean()\nagg.reset_index(inplace = True)\nagg","c25bcf49":"import plotly.graph_objects as go\n\ncategories = ['target', 'standard_error', 'sentence_count', 'mean_word_length',\n              'automated_readability_index', 'character_count', 'unique_word_count']\n\nfig = go.Figure()\n\nfor row in agg.itertuples():\n    fig.add_trace(go.Scatterpolar(\n    r = [getattr(row, i) for i in categories],\n    theta = categories,\n    fill = 'toself',\n    name = row.cluster\n    ))\nfig.show()","5d584080":"Class `1` appears to have higher sentence counts, lower character counts and has a noticeably lower average score in the automated readability index  \n90% of records in class `1` also have url\/licenses associated with them, which is much higher than most other clusters\n","9c5b1ff5":"Looking at the latent representations, we see some interesting findings:\n* We can see some separation across scores - the finely grouped 'cluster' of good observations in the bottom left is noticeable \n* This plot looks like some kind of flipped Australia as well (to me at least)","7aff0aac":"## Introduction\nI was interested in seeing how separable some of these sentence-level representations are in our data set.  \nIn this notebook, we'll go through a brief EDA and and into some unsupervised learning.  \n**Sentence transformers** are used to obtain the sentence embeddings and **textstat** is used for feature engineering","cda0a7d0":"These clusters seem to be relatively well-defined.  \nWe can perform some feature engineering to compare across clusters","8b486e61":"## TSNE Dimensionality Reduction","80208813":"Readability target appears to be relativel normally distributed.    \nThe standard error is right-skewed... and seems to have some outlier values on the left","c8f3aad1":"##  Feature Engineering\nI used the same textstat augmentations from this excellent EDA notebook https:\/\/www.kaggle.com\/gunesevitan\/commonlit-readability-prize-eda  \nFor reference, the augmentations are defined below:\n* `character_count` - number of characters in the text\n* `digit_count` - number of digits in the text\n* `word_count` - number of words in the text\n* `unique_word_count` - number of unique words in the text\n* `mean_word_length` - average number of character that the words have in the text\n* `syllable_count` - number of syllables in the text\n* `sentence_count` - number of sentences in the text\n* `flesch_reading_ease` - [flesch reading ease score](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease) of the text\n* `flesch_kincaid_grade` - [flesch-kincaid grade level](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level) of the text\n* `smog_index` - [smog index](https:\/\/en.wikipedia.org\/wiki\/SMOG) of the text\n* `automated_readability_index` - [automated readability index](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index) of the text\n* `coleman_liau_index` - [coleman\u2013liau index](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index) of the text\n* `linsear_write_formula` - [linsear write grade](hhttps:\/\/en.wikipedia.org\/wiki\/Linsear_Write) of the text","c39356e0":"### UMAP Dimensionality Reduction","61987b59":"## Clustering\nWe'll fit a GMM to the original sentence embeddings to cluster our datapoints  ","77f5f658":"## EDA and Data Preparation","ddf02679":"## Obtaining Sentence Representations\nHere we'll use a Roberta base model to encode our sequences.  ","cd82f685":"This row has a target which looks like an integer, and a 0 standard error.  \nWe'll remove it for now, as the standard error is largely out of distribution, which could affect dimensionality reduction"}}