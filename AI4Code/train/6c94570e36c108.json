{"cell_type":{"271f45d3":"code","3fb6ef03":"code","b265259d":"code","b9d4c361":"code","dc5e985b":"code","6a1e7262":"code","81e0375f":"code","53c368b7":"code","0c28fdc6":"code","7d8f87fa":"code","22414f7f":"code","e2da0324":"code","7345d783":"code","7adf9921":"code","b1753f9f":"code","65ef9cdd":"code","ef97eef9":"code","039a770b":"code","799ab34d":"code","8ad811ac":"code","f6f51337":"code","dbc090e2":"code","d38582b6":"code","77409ef7":"code","12eb80e1":"code","f1c98c89":"code","44e5baa9":"code","bc5e9d78":"code","17c47f24":"markdown","b64b9ba9":"markdown","0f1c5c2f":"markdown","418533b6":"markdown","74597de4":"markdown","8eca8b31":"markdown","525ccbcf":"markdown","a03485bd":"markdown","34a1f15a":"markdown","40e2f85d":"markdown","e00873b2":"markdown","6f3c65a9":"markdown","4f778b6a":"markdown","ce28117a":"markdown","946b307d":"markdown","451b3238":"markdown","6c06ae01":"markdown","07d49ac3":"markdown","d47f491e":"markdown","b454e725":"markdown","b7975f0d":"markdown","2e833225":"markdown","0a747e62":"markdown","c5e632a3":"markdown","f1a993e5":"markdown","aa58d8d3":"markdown","ea4bbf81":"markdown","ea2a53cb":"markdown","4b972c23":"markdown","d371973c":"markdown","5b859705":"markdown"},"source":{"271f45d3":"# Import needed libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Import statements required for Plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\n# Import Models\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold\nfrom sklearn import svm \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Define constants\/variables to use\nseed_Num = 8 # aka Random State\nnum_folds = 5","3fb6ef03":"# Read in the data into pandas dataframe\ndata = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","b265259d":"pd.DataFrame(data.columns, columns=['Column Names'])","b9d4c361":"data.sample(5, random_state=8)","dc5e985b":"# Display Number of Uniques, If Column has NA, number of NAs, and data types for each column\npd.DataFrame(data = {'Data Type': data.dtypes,\n                     'Number of Unique Values': data.nunique().sort_values(),\n                     'Contains NAs': data.isnull().any(),\n                     'Number of NAs': data.isnull().sum()}).sort_values('Number of Unique Values')","6a1e7262":"cols_to_drop_for_modeling = ['Over18','StandardHours','EmployeeCount','EmployeeNumber']\ndata.drop(cols_to_drop_for_modeling, axis = 1,inplace=True)","81e0375f":"cat_cols = list(data.select_dtypes(exclude=np.number).columns)\n#cat_cols.remove('Attrition') \n\nfig_rows = 2\nfig_cols = 4\nfig = tls.make_subplots(rows=fig_rows, cols=fig_cols, \n                          subplot_titles=tuple(cat_cols));\ncurr_row = 1\ncurr_col = 1\nfor i, col in enumerate(cat_cols):\n    trace = go.Bar(name=col,\n                x= data[col].value_counts().index.values,\n                y= data[col].value_counts().values,\n                marker=dict(line=dict(color='black',width=1)))\n    fig.append_trace(trace, curr_row, curr_col)\n    curr_col+=1\n    if curr_col >= fig_cols+1: # Zero Indexing for '-1'\n        curr_row += 1\n        curr_col = 1\n\nfig['layout'].update(title =  'Count of Categorical Variables in Dataset', width = 900, height = 600,\n                    showlegend=False)\n\npy.iplot(fig)","53c368b7":"attritionBarPlot = go.Bar(\n            x= data[\"Attrition\"].value_counts().index.values,\n            y= data[\"Attrition\"].value_counts().values,\n            marker=dict( color=['Orange', 'steelblue'],line=dict(color='black',width=1)))\nlayout = dict(title =  'Count of Attrition in Dataset', width = 800, height = 400)\nfig = dict(data = [attritionBarPlot], layout=layout)\npy.iplot(fig)","0c28fdc6":"#  Plot areas are called axes\nimport warnings    # We want to suppress warnings\nwarnings.filterwarnings(\"ignore\")    # Ignore warnings\n\nfig_rows = 5\nfig_cols = 5\nfig,ax = plt.subplots(fig_rows,fig_cols, figsize=(16,20)) \nfake_numeric_cols = ['Education','EnvironmentSatisfaction', 'JobInvolvement' , 'JobSatisfaction' , \n                     'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance', 'JobLevel',\n                    'StockOptionLevel']\n#numericCols = [x for x in list(data.select_dtypes(include=np.number).columns) if x not in fake_numeric_cols]\nnumericCols = list(data.select_dtypes(include=np.number).columns)\ncurr_row = 0\ncurr_col = 0\nfor i, col in enumerate(numericCols):\n    sns.distplot(data[col], ax = ax[(curr_row,curr_col)],rug=True, kde =False) \n    curr_col+=1\n    if curr_col >= fig_cols: # Zero Indexing for '-1'\n        curr_row += 1\n        curr_col = 0\n\nplt.show()","7d8f87fa":"data[numericCols].describe().T","22414f7f":"heatmapCols = numericCols + ['Attrition']\ntemp = data.copy()\ntemp['Attrition'] = data['Attrition'].replace('Yes',1).replace('No',0)\nheatmapGo = [go.Heatmap(\n        z= temp[heatmapCols].astype(float).corr().values, # Generating the Pearson correlation\n        x= temp[heatmapCols].columns.values,\n        y= temp[heatmapCols].columns.values,\n        colorscale='Cividis',\n        reversescale = False,\n        opacity = 1.0)]\n\nlayout = go.Layout(\n    title='Pearson Correlation Matrix Numerical Features',\n    xaxis = dict(ticks='', tickfont = dict(size = 10)),\n    yaxis = dict(ticks='', tickfont = dict(size = 7)),\n    width = 900, height = 700)\n\nfig = go.Figure(data=heatmapGo, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","e2da0324":"pseudo_numeric_cols = ['Education','EnvironmentSatisfaction', 'JobInvolvement' , 'JobSatisfaction' , \n                     'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance', 'JobLevel',\n                    'StockOptionLevel'] \nexcludeCols = pseudo_numeric_cols + ['YearsWithCurrManager','YearsSinceLastPromotion','YearsInCurrentRole','YearsAtCompany']\npairplotCols = [x for x in list(data.select_dtypes(include=np.number).columns) if x not in excludeCols]+ ['Attrition']\nsns.pairplot(data[pairplotCols], plot_kws={'scatter_kws': {'alpha': 0.1}},\n             kind=\"reg\", diag_kind = \"kde\"  , hue = 'Attrition' );","7345d783":"numeric_cols = list(data.select_dtypes(include=np.number).columns)\n\nfig_rows = 6\nfig_cols = 4\nfig = tls.make_subplots(rows=fig_rows, cols=fig_cols, \n                          subplot_titles=tuple(numeric_cols));\ncurr_row = 1\ncurr_col = 1\nfor i, col in enumerate(numeric_cols):\n    trace1 = go.Histogram(name = \"No Attrition\", \n                          marker=dict( line=dict(color='black',width=1)), #color=['steelblue']),\n                          x = list(data[data['Attrition'] == 'No'][col]),\n                          opacity=0.5)\n    trace2 = go.Histogram(name = 'Yes Attrition', \n                          marker=dict(line=dict(color='black',width=1)),#,color=['Orange']),\n                          x=data[data['Attrition'] == 'Yes'][col],\n                          opacity=0.5)\n    tmp3 = pd.DataFrame(pd.crosstab(data[col],\n                    data['Attrition'].replace('Yes',1).replace('No',0)), )\n    tmp3['Attr%'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n    extra_yAxis = 'y' + str(fig_rows * fig_cols + i+1)\n    trace3 =  go.Scatter(x=tmp3.index,y=tmp3['Attr%'],\n        yaxis = extra_yAxis,name='% Attrition', opacity = .8, \n        marker=dict(color='black',line=dict(color='black',width=0.5)))\n    fig.append_trace(trace1, curr_row, curr_col)\n    fig.append_trace(trace2, curr_row, curr_col)\n    if col not in ['MonthlyRate','DailyRate', 'MonthlyIncome']:\n        fig.append_trace(trace3, curr_row, curr_col)\n        fig['data'][-1].update(yaxis=extra_yAxis)\n        yaxisStr = ''\n        if curr_col == fig_cols:\n            yaxisStr = '% Attrition'\n        fig['layout']['yaxis' + str(fig_rows * fig_cols + i+1)] = dict(range= [0, max(tmp3['Attr%'])+10], \n                         showgrid=True,  overlaying= 'y'+str(i + 1), anchor= 'x'+str(i+1), side= 'right',\n                         title= yaxisStr)\n    curr_col+=1\n    if curr_col >= fig_cols+1: # Zero Indexing for '-1'\n        curr_row += 1\n        curr_col = 1\nfig['layout'].update(title =  'Numerical Distributions colored by Attrition', width = 900, height = 900,\n                    barmode = 'overlay',showlegend=False, font=dict(size=10))#,\n                    #yaxis2=dict(range= [0, 100], overlaying= 'y', anchor= 'x', \n                    #      side= 'right',zeroline=False,showgrid= False, title= '% Attrition'))\n\npy.iplot(fig)","7adf9921":"cat_cols = list(data.select_dtypes(exclude=np.number).columns)\n\nfig_rows = 2\nfig_cols = 4\nfig = tls.make_subplots(rows=fig_rows, cols=fig_cols, \n                          subplot_titles=tuple(cat_cols));\ncurr_row = 1\ncurr_col = 1\nfor i, col in enumerate(cat_cols):\n    yaxisStr = ''\n    offset_val = -0.3\n    if col not in ['Attrition']:\n        offset_val = -0.2\n    trace1 = go.Bar(name='No Attrition', opacity = .8, width= 0.6,#offset = -0.03,\n                x= data[data['Attrition'] == 'No'][col].value_counts().index.values,\n                y= data[data['Attrition'] == 'No'][col].value_counts().values,\n                marker=dict(color = 'steelblue', line=dict(color='black',width=1)))\n    trace2 = go.Bar(name='Yes Attrition', opacity = .8, width= 0.6,offset = offset_val,\n                x= data[data['Attrition'] == 'Yes'][col].value_counts().index.values,\n                y= data[data['Attrition'] == 'Yes'][col].value_counts().values,\n                marker=dict(color = 'orange',line=dict(color='black',width=1)))\n    tmp3 = pd.DataFrame(pd.crosstab(data[col],\n                    data['Attrition'].replace('Yes',1).replace('No',0)), )\n    tmp3['Attr%'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n    extra_yAxis = 'y' + str(fig_rows * fig_cols + i+1)\n    trace3 =  go.Scatter(x=tmp3.index,y=tmp3['Attr%'],mode = 'markers',\n        yaxis = extra_yAxis,name='% Attrition', opacity = .8, \n        marker=dict(color='black',size= 10))\n    fig.append_trace(trace1, curr_row, curr_col)\n    fig.append_trace(trace2, curr_row, curr_col)\n    if col not in ['Attrition']:\n        fig.append_trace(trace3, curr_row, curr_col)\n        fig['data'][-1].update(yaxis=extra_yAxis)\n        if curr_col == fig_cols:\n            yaxisStr = '% Attrition'\n        fig['layout']['yaxis' + str(fig_rows * fig_cols + i+1)] = dict(range= [0, max(tmp3['Attr%'])+10], \n                         showgrid=True,  overlaying= 'y'+str(i + 1), anchor= 'x'+str(i+1), side= 'right',\n                         title= yaxisStr)\n    \n    curr_col+=1\n    if curr_col >= fig_cols+1: # Zero Indexing for '-1'\n        curr_row += 1\n        curr_col = 1\n\nfig['layout'].update(title =  'Count of Categorical Variables in Dataset Colored by Attrition', \n                     width = 900, height = 600,\n                     showlegend=False, barmode = 'overlay', font=dict(size=10))\n\npy.iplot(fig)","b1753f9f":"numericCols = list(data.select_dtypes(include=np.number).columns)","65ef9cdd":"\ncat_cols = list(data.select_dtypes(exclude=np.number).columns)\nif 'Attrition' in cat_cols:\n    cat_cols.remove('Attrition')","ef97eef9":"X_cat = pd.get_dummies(data[cat_cols])\nX_cat.head()","039a770b":"y = data['Attrition'].replace('Yes',1).replace('No',0)","799ab34d":"X = pd.concat([data[numericCols], X_cat], axis=1, sort=False)\nX.head()","8ad811ac":"def get_cv_results(model, X, y):\n    pipe = make_pipeline(StandardScaler(),model)\n    cv_scores_dict = cross_validate(pipe, X, y, cv=num_folds, \n                                    scoring= ['roc_auc', 'accuracy', 'f1', 'precision','recall'],\n                                   return_train_score = False)\n    cv_scores_df = pd.DataFrame(cv_scores_dict, \n                 index = ['Fold {}'.format(x) for x in range(1,len(cv_scores_dict['test_accuracy'])+1)])\n    return pd.concat([cv_scores_df, pd.DataFrame(cv_scores_df.mean(), columns=['Avg']).T])","f6f51337":"logRegScores = get_cv_results(LogisticRegression(random_state=seed_Num), X, y)\nlogRegScores","dbc090e2":"from sklearn.naive_bayes import GaussianNB\nnaiveBayesScores = get_cv_results(GaussianNB(), X, y)\nnaiveBayesScores","d38582b6":"from sklearn.neighbors import KNeighborsClassifier\n#knnNeighborComparison = pd.DataFrame()\n#for numOfK in range(1,51,2):\n#    knnScores = get_cv_results(KNeighborsClassifier(n_neighbors=numOfK), X, y)\n#    df = pd.DataFrame(knnScores.loc['Avg']).T\n#    df.index = ['K='+str(numOfK) + ' Avg']\n#    knnNeighborComparison = pd.concat([knnNeighborComparison,df])\n#knnNeighborComparison\n## Notes on Iterating through neighbors:\n### Seems to plateau around roc_auc of 0.78 and f1 score generally decrease as k increases\n### K = 7 seems to be last significant bump in roc_auc with a relatively decent f1\nknnScores = get_cv_results(KNeighborsClassifier(n_neighbors=7), X, y)\nknnScores","77409ef7":"import lightgbm as lgb\nlgbmScores = get_cv_results(lgb.LGBMClassifier(random_state=1, n_jobs = -1), X, y)\nlgbmScores","12eb80e1":"from sklearn.svm import SVC\nsvcScores = get_cv_results(SVC(), X, y)\nsvcScores","f1c98c89":"from sklearn.svm import SVC\nsvcLinearScores = get_cv_results(SVC(kernel='linear'), X, y)\nsvcLinearScores","44e5baa9":"from sklearn.neural_network import MLPClassifier\nmlpScores = get_cv_results(MLPClassifier(hidden_layer_sizes = (2,2),\n                                         random_state=seed_Num), X, y)\nmlpScores","bc5e9d78":"cvAvgResultsCombined = pd.DataFrame()\n# Get variables ending with Scores\nvarNames = [s for s in list(locals().keys()) if s.endswith('Scores')]\nlocal_var_dict = locals()\nfor varName in varNames:\n    df = pd.DataFrame(local_var_dict[varName].loc['Avg']).T\n    df.index = [varName+' Avg']\n    cvAvgResultsCombined = pd.concat([cvAvgResultsCombined,df])\ncvAvgResultsCombined.sort_values(by='test_roc_auc', ascending=False)","17c47f24":"## Variable\/Feature Relationships\n\nHere we will take a look at how variables related to each other. There are various methods\/visualizations for this.\n\n### Correlation Matrix\n\nOne method to see how features are related to each other is a correlation matrix given below:\n\nNote: We convert Attrition to 1 for Yes and 0 for no to be able to see if any variables\/features are correlated with the target variable.","b64b9ba9":"We see there is a significant imbalance between the two classes of our target variable. About 83% have no attrition while the remaining do. There are many techniques to help address a dataset with an imbalanced target variable. These mainly fall in either oversampling (the minority) or undersampling (the majority) type of techniques. \n","0f1c5c2f":"The leftmost column of numbers next to Age represent the index of the row in the data (so we have rows 1441, 81, 243, 1142, and 51). Remember, each row represents an employee. As an example to understand what a row is telling us, reading the first few features (columns) of the 1441 row represents an employee who is 56 years old (Age), has not left the company (Attrition), does not travel (BusinessTravel), gets paid 667 a day (DailyRate), and is part of the research & development department (Department).\n\nJust looking at all the columns for these few rows, some variables\/features have the same values, like department, employee count, number of companies worked, over 18, and standard hours. Features with the same value for all employees do not help us with predicting attrition (but can potentially give other information). Also, features with a unique value for each sample\/employee, like employee ID number, will not be used for helping predict attrition.\n\nOf course, the features mentioned above having the same values may be just due to the rows we have shown here. \n\nWith that being said, we can get a better picture of which variables only have 1 unique value by looking at the number of unique values for each feature. Also, we will look at each column's type and the number of NA values in each column to see what cleaning might need to be done..\n","418533b6":"## Numeric Variable Distribuions\n\nWith that data a bit more clean now, we shall look at the distribution of the numeric features in the dataset. It should be noted that some of the variables are read in as integers (numeric), but represent a discrete amount of values. These variables (and the integer meanings if known) are presented below:\n\n* Education - 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\n* EnvironmentSatisfaction - 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n* JobInvolvement - 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n* JobSatisfaction - 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n* PerformanceRating - 1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'\n* RelationshipSatisfaction - 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n* WorkLifeBalance - 1 'Bad' 2 'Good' 3 'Better' 4 'Best'\n* JobLevel\n* StockOptionLevel\n\nFor now, we will treat these as numeric variables. Although these variables have an order to them, their meanings as integers may be flawed.. For example, having a 'Bachelor Degree' (3) does not necessarily mean it is 3 times 'greater' than having 'Below Education' (1) education; however, these features are ordinal (i.e. the categories have an order to them of one being greater than another).\n\n___\n**Distributions of Numeric Features**","74597de4":"## Cateogrical Variable Counts\n\nNow we will look at the cateogorical variables and see their counts and how the employee counts are distributed\n","8eca8b31":"### SVC","525ccbcf":"Here, the black dots help show the percentage of employees that turned over (left\/attrition) for that partuclar value of that category.\n\nOne of the more obvious differences in percentages is Overtime. Of the employees working overtime, about 30% left the company; while only 10% of the employees not working overtime left the company. There are also similar observations for employees with a jobrole of sales representation, employees who travel frequently, and employees who are single.\n\n\n# Modeling\n\nIn this section, we will being modeling, but first we will take some notes on preprocessing.\n\n## Normalize and Center Numeric Variables\n\nSome of the models used here will benefit from centering and scaling down the numeric variables. The scaling will be done during CV (in a pipeline) and not the entire dataset (as not to introduce any leakage or undesired effects from the unseen test\/validation data)","a03485bd":"Here, we see that none of the other categorical variables shown here is as imbalanced as Attrition, although business travel comes close. \n\nAlso, looking at department, education field, and jobrole, we can also see these employees look to represent some type of business in the medial research industry.\n\nOf course, one of the most important ones for this notebook is Attrition, so let us take a look at that one with a larger plot for easier interpretation.","34a1f15a":"### Gradient Boosting Tree based Model (LightGBM)","40e2f85d":"### Logistic Regression","e00873b2":"Here, we see the count, mean, and standard deviation along with the five number summary. \n","6f3c65a9":"## Pair Plot\n\nAnother method to view relationship between variables is a pairplot. It is a grid of plots with the same features on the x and y grid axis. It can show distributions along the diagonal (like the histograms in the earlier section). It can also show scatter plots on the off diagonals to show how 2 variables relate. Here, we also color by Attrition to see if any easy\/obvious relationships pop up and also only show total working years for the 5 year variables to make plot slightly easier to read.\n\nLet us first start with the truly numeric vairables:","4f778b6a":"None of the plots show a huge differnce between employees who have stayed (i.e. no attrition) versus employees. I do not like using pair plots for dissecting slight differences as there may be other interactions with other features that this visualization does not show.","ce28117a":"## Initial Cleaning\n\nThis section will involve some initial cleaning of the data (mainly removing any unecessary features or filling na values if appropriate).\n\n___\n**A Look at Unique Values and NAs**","946b307d":"\nThe data is pretty clean seeing that there are no NA values. Also, we see Over18, StandardHours, and EmployeeCount only have 1 value. Looking back at the first (tabular) look at a few random rows of data, we see Over18 is 'Y', StandardHours is 80, and EmployeeCount is '1'. This means that all employees in this data are over 18, work standard hours of 80, and I assume employee count of 1 corresponds to one row counting as one employee. Employee Count might be used if grouping employees by certain features to count the number of employees in each group.\n\nEmployeeNumber has 1470 values for 1470 employees, so each employee has an unique employee number (which is somewhat expected). Since Over18, StandardHours, EmployeeCount, and EmployeeNumber either have only 1 value or a unique value for each sample\/employee, these variables will not be used in helping predict attrition.","451b3238":"Here we see that the linear models performed the best with regard to receiver operating characteristic area under the curve (linear support vector classifier and logisitic regression). These two also had similar accuracies and f1 scores. A lightly tuned and simple neural net also performed similarily to the linear models. Naive Bayes and K-Nearest Neighbors performed the worst out of the models for this situation.\n\n# Next Steps\n\nFor next steps, addressing the target class imbalance via undersampleing or oversampling techniques can be explored. For example, SMOTE (Synthetic Minority Over-sampling Technique) can be used to synthetically create more of the minority class (Yes Attrition)  for a more balanced target of Yes\/No Attrition.\n\nAnother step to be done is more formal hyperparameter tuning using methods like grid search cv, random search cv, or bayesian hyperparameter optimization. \n","6c06ae01":"### K-Nearest Neighbors","07d49ac3":"# Data\n\nThe data contains information on 1470 employees. There are 35 features to each employee with one being the unique identifier (employee ID number). Another important feature is attrition which contains 'Yes' or 'No' for wether the employee left or not. This will be our target variable (or y). \n\nIt should be noted that this is a fictional data set created by IBM data scientists.","d47f491e":"### SVC (Linear)","b454e725":"## One Hot Encode Cateogrical Variables","b7975f0d":"### Naive Bayes","2e833225":"## Distributions Colored by Attrition\n\nWe can also color the distributions shown earlier by a cateogrical variable. In this case, we will color by attrition since that will be our target variable. Some this was seen in the diagonals in the pair plot shown earlier.\n\nIn addition,  we add a black line showing the percentage of employees that attrition over the numeric variables. This helps give an overall view of how the % of Attrition changes over the range of the numeric features. The y axis on the right side of the plot shows the percentage.","0a747e62":"___\nHere we see histograms of each numeric variable in the dataset along with blue tick marks ontop of the x axis representing where a single observation (employee) lies. This helps visualize the distributions of the numeric variables. For example, we see Age is almost normally distributed with a slight right tail; whereas the rate features (Daily, Monthly, Hourly) look to be close to uniformly distributed.\n\nIt is interesting to note that the Monthly Rate and Monthly Income distributions (and numbers) are very different. I would assume they would beI tried searching for the meaning for these features in other notebooks along with some IBM articles on the dataset, but was unable to get their definitions.\n\nSimilar to some of the information provided in the plots above, we can also take note of some common statistics of the numeric features given below:","c5e632a3":"Here we see most attributes are not well correlated with each other (i.e. values close to zero); however, there are a few 'hot' or 'dark' spots in the plot.\n\nFor one, the top right 'hot' area with the yellow\/brown colors correspond to the 'year' variables being somewhat correlated with one another. This makes sense. For example, if one has more years with the company, one can reaonsably expect more years in the current role.\n\nThe other interesting hot spot is between Job Level and Monthly Income at just above 0.95! The two rows and two columns corresponding to these features have similar colors. This shows that these two features\/variable provide similar information.\n\nAnother interesting thing to note is the lack of correlation between HourlyRate, DailyRate, and MonthlyRate. One would think these variables should correlate well with one another; however, the data seems to indicate otherwise.","f1a993e5":"# Exploratory Data Analysis (and Cleaning Data)\n\nThis section will go into some exploratory data analysis along with cleaning the data for further analysis. \n\nLet us now take a look at a few random rows of data to get a sense of what the data looks like.\n","aa58d8d3":"# Model Comparison and Conclusion\nHere we will compare the average scores of the models.\n","ea4bbf81":"In the above plots, there is quite a bit of information that can be taken away. I just want to highlight the following points that mainly focus on attrition rates\/percentages (black line):\n1. Younger Employees (Age) tend to have higher Attrition rates\n- Similarly to Age, employees with lowest level stock options and lowest jobs also tend to have higher attrition rates\n- Also, many of the year features (Total Working Years, Years with Current Manager, Years at Company, etc.) have a general downward trend with regard to attrition rates. In other words, people with less years have higher attrition percentages\n- In addition, many of the survey like features (Job Involvement, Work Life Balance, Job Satisfaction, etc.) tend to have higher attrition percentages the lower an employee scored.\n- There are a few strange observations as the number of employees for a particular range die down. For example, there is one employee with Attrition with 40 total working years (and no other employees in that range), so that results in a '100%' attrition percentage for that range. This can be misleading.\n\nLet us now take a look at the discrete\/categorical features\n\n**Discrete\/Categorical Features Colored by Attrition**","ea2a53cb":"# Sources and References\n\nFor some general ideas and what to visualize (along with what to not visualize), I look at some other notebooks.\n\n[1] [IBM HR Data Visualization - Devendray](https:\/\/www.kaggle.com\/devendray\/ibm-hr-data-visualization) \n\n[2] [Employee Attrition via Ensemble Tree Based Methods - Arthur Tok](https:\/\/www.kaggle.com\/arthurtok\/employee-attrition-via-ensemble-tree-based-methods)","4b972c23":"# Introduction\n\nThis notebook will pay attention to retention (of employees)! \n\nFor almost any company, employees leaving (aka attrition) can have significant impact and cost. For one, the employee may have a depth of experience and talent that is vital to the company (or a team). In addition, replacing such an employee may be difficult (if not nearly impossible). For another, it takes resources and time to bring on a new employee to 'replace' the talent\/expertise\/knowledge of the former employee. \n\nWith that being said, this notebook will use employee (attrition) data provided by IBM data scientists. \n\nExploratory data analysis will be performed to look at the data and visualize certain aspects of it. During EDA, some preprocessing will be done to the initial data set in order to prepare it for some machine learning techniques. \n\nThen, classification models like Logistic Regression, kNN, LightGBM, SVC, Neural Nets, etc. will be used to help predict which employees are at risk of leaving. The classification models and techniques will be compared with each other to assess performance against each other. ","d371973c":"## Modeling\n\nWe shall use roc_auc as our main scoring metric, but will also keep track of accuracy and f1 score (that combine recall\/precision).","5b859705":"### Neural Net"}}