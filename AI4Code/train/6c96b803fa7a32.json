{"cell_type":{"d67e5eea":"code","9c296752":"code","ca5c4d5e":"code","fb25d528":"code","313828b6":"code","5508e841":"code","246be527":"code","840e34cc":"code","72ab4e58":"code","11707997":"code","6eba5334":"code","baaca854":"code","620b3a23":"code","b2a422f8":"code","8131d7ba":"code","410176b6":"code","7a7b8441":"code","5bbff2db":"code","af1620d9":"code","818081a6":"code","d59d6698":"code","3df50902":"code","9dafe64a":"code","fe87fc5d":"code","24554973":"code","4018733e":"code","e7478090":"code","cc6c7cad":"code","820b1781":"code","eadc9b4c":"code","bd1c01b3":"code","0e73c5ed":"code","9e71e7db":"code","f76b4f37":"code","16c972a0":"code","f636163e":"code","c3703dd1":"code","6d1b07e4":"code","fe15e76d":"code","014cfba2":"code","3b32cc19":"code","8adea3d9":"code","559eb8a9":"code","9fd9e943":"code","52e7eac2":"code","71910670":"markdown","de6a3382":"markdown","010e34fd":"markdown"},"source":{"d67e5eea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport sklearn\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.neighbors import KDTree\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport gensim\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('punkt')\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nnp.random.seed(2018)\nnltk.download('wordnet')\nimport matplotlib.pyplot as plt\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c296752":"data=pd.read_csv('\/kaggle\/input\/india-headlines-news-dataset\/india-news-headlines.csv')\ndata['cat']=data['headline_category'].str.split('.').map(lambda x : x[0])\ndata=data.loc[(data['cat']=='entertainment') | (data['cat']=='business') | (data['cat']=='sports') | (data['cat']=='india') ]","ca5c4d5e":"data1=data.loc[data['cat']=='sports'].head(15000)\ndata2=data.loc[data['cat']=='business'].head(15000)\ndata3=data.loc[data['cat']=='entertainment'].head(15000)\ndata4=data.loc[data['cat']=='india'].head(15000)\ndata=data1.append(data2).append(data3).append(data4)\ndata=data.sample(frac=1)\ndata=data.set_index(pd.Series([i for i in range(0,len(data))]))","fb25d528":"#data=pd.DataFrame({\"headline_text\":np.array(data['headline_text'].sample(frac=0.1))})\n#data['index']=data.index\n#documents=data\n","313828b6":"#def lemmatize_stemming(text):\n#    stemmer = PorterStemmer()\n#    return WordNetLemmatizer().lemmatize(text, pos='v')\n#def preprocess(text):\n#    result = []\n#    for token in gensim.utils.simple_preprocess(text):\n#        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n#            result.append(lemmatize_stemming(token))\n#    return result","5508e841":"#doc_sample = documents[documents['index'] == 4310].values[0][0]\n#print('original document: ')\n#corp=[]\n#for u in range(0,len(documents)):\n#    words = []\n#    for word in documents['headline_text'][u].split(' '):\n#        words.append(word)\n#    corp.append(words)\n","246be527":"#processed_docs = documents['headline_text'].map(preprocess)\n#processed_docs[:10]","840e34cc":"def filter_func(sen):\n    filter_list=['CC','RB','IN','TO','WRB','JJ','PRP','DT',':',')','(','POS','CD','.','MD']\n    text=nltk.word_tokenize(sen)\n    tagged=nltk.pos_tag(text)\n    for g in tagged:\n        if(g[1] in filter_list):\n            text.remove(g[0])\n    text1=[word.lower() for word in text]\n    return(text1)\n    ","72ab4e58":"tags=[]\nfor i in range(0,len(data)):\n    tags.append(filter_func(data['headline_text'][i]))","11707997":"#corp=[]\n#for i in list(data['headline_text']): \n#    corp.append(nltk.word_tokenize(i))","6eba5334":"#import nltk\n#nltk.download('stopwords')\n#from nltk.tokenize import word_tokenize \n#from nltk.corpus import stopwords","baaca854":"#stop_words = set(stopwords.words('english')) \n#filtered=[]\n#for i in range(0,len(corp)):\n#    filtered.append([w for w in corp[i] if not w in stop_words]) ","620b3a23":"model = Word2Vec(tags, min_count=1,size=1000)\nmodel.wv.vectors.shape\nwordvecs=model.wv.vectors   \n","b2a422f8":"len(wordvecs)","8131d7ba":"def clustering_on_wordvecs(word_vectors, num_clusters):\n    # Initalize a k-means object and use it to extract centroids\n    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n    idx = kmeans_clustering.fit_predict(word_vectors);\n    \n    return kmeans_clustering.cluster_centers_, idx;\ncenters, clusters = clustering_on_wordvecs(wordvecs, 7);\ncentroid_map = dict(zip(model.wv.index2word, clusters));","410176b6":"def get_top_words(index2word, k, centers, wordvecs):\n    tree = KDTree(wordvecs);\n#Closest points for each Cluster center is used to query the closest 20 points to it.\n    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n    closest_words_idxs = [x[1] for x in closest_points];\n#Word Index is queried for each position in the above array, and added to a Dictionary.\n    closest_words = {};\n    for i in range(0, len(closest_words_idxs)):\n        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n#A DataFrame is generated from the dictionary.\n    df = pd.DataFrame(closest_words);\n    df.index = df.index+1\n    return(df)\n","7a7b8441":"top_words = get_top_words(model.wv.index2word, 5000, centers, wordvecs);\ntop_words.head(20)","5bbff2db":"#from sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.feature_extraction.text import TfidfVectorizer\n\n#sns.set_style('whitegrid')\n#%matplotlib inline\n## Helper function\n#def plot_20_most_common_words(count_data, count_vectorizer):\n#    import matplotlib.pyplot as plt\n#    words = count_vectorizer.get_feature_names()\n#    total_counts = np.zeros(len(words))\n#    for t in count_data:\n#        total_counts+=t.toarray()[0]\n#    \n#    count_dict = (zip(words, total_counts))\n#    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:30]\n#    words = [w[0] for w in count_dict]\n#    counts = [w[1] for w in count_dict]\n#    x_pos = np.arange(len(words)) \n#    \n#    plt.figure(2, figsize=(12, 12\/1.6180))\n#    plt.subplot(title='30 most common words')\n#    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n#    sns.barplot(x_pos, counts, palette='husl')\n#    plt.xticks(x_pos, words, rotation=90) \n#    plt.xlabel('words')\n#    plt.ylabel('counts')\n#    plt.show()\n## Initialise the count vectorizer with the English stop words\n#count_vectorizer = TfidfVectorizer(stop_words='english')\n## Fit and transform the processed titles\n#count_data = count_vectorizer.fit_transform(data['headline_text'])\n## Visualise the 20 most common words\n#plot_20_most_common_words(count_data, count_vectorizer)","af1620d9":"#import warnings\n#warnings.simplefilter(\"ignore\", DeprecationWarning)\n## Load the LDA model from sk-learn\n#from sklearn.decomposition import LatentDirichletAllocation as LDA\n# \n## Helper function\n#def print_topics(model, count_vectorizer, n_top_words):\n#    words = count_vectorizer.get_feature_names()\n#    for topic_idx, topic in enumerate(model.components_):\n#        print(\"\\nTopic #%d:\" % topic_idx)\n#        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n#        \n## Tweak the two parameters below\n#number_topics = 6\n#number_words = 20\n## Create and fit the LDA model\n#lda = LDA(n_components=number_topics, n_jobs=-1)\n#lda.fit(count_data)\n## Print the topics found by the LDA model\n#print(\"Topics found via LDA:\")\n#print_topics(lda, count_vectorizer, number_words)","818081a6":"from gensim import corpora, models\ndef bow(tags):\n    dictionary = gensim.corpora.Dictionary(tags)\n    bow_corpus = [dictionary.doc2bow(doc) for doc in tags]\n    return(bow_corpus)\n\ndef bow_tfidf(tags):\n    dictionary = gensim.corpora.Dictionary(tags)\n    bow_corpus = [dictionary.doc2bow(doc) for doc in tags]\n    tfidf = models.TfidfModel(bow_corpus)\n    corpus_tfidf = tfidf[bow_corpus]\n    return(corpus_tfidf)","d59d6698":"bow_corpus =bow(tags)\ncorpus_tfidf=bow_tfidf(tags)","3df50902":"#bow_doc_4310 = bow_corpus[4310]\n#for i in range(len(bow_doc_4310)):\n#    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0],dictionary[bow_doc_4310[i][0]],bow_doc_4310[i][1]))","9dafe64a":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=gensim.corpora.Dictionary(tags), passes=2, workers=2)","fe87fc5d":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=6, id2word=gensim.corpora.Dictionary(tags), passes=2, workers=4)","24554973":"#for idx, topic in lda_model.print_topics(-1):\n#    print('Topic: {} \\nWords: {}'.format(idx, topic))","4018733e":"#for idx, topic in lda_model_tfidf.print_topics(-1):\n#    print('Topic: {} Word: {}'.format(idx, topic))","e7478090":"#for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n#    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n","cc6c7cad":"#for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n#    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))","820b1781":"#unseen_document = 'Two suspects killed by police'\n#unseen2='One more lok sabha constituency added'\n#unseen3='Modi ji doubts over ballot papers'\n#bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n#bow_vector3=dictionary.doc2bow(preprocess(unseen2))\n#bow_vector4=dictionary.doc2bow(preprocess(unseen3))\n\n#for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n#    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n","eadc9b4c":"flag=[]\nfor i in range(0,len(bow_corpus)):\n    vec=[0]*6\n    for j in lda_model_tfidf[corpus_tfidf[i]]:\n        vec[j[0]]=j[1]\n    vec=np.array(vec)\n    flag.append(vec)\n    \n    ","bd1c01b3":"flag2=[]\nfor i in range(0,len(bow_corpus)):\n    vec=[0]*6\n    for j in lda_model[bow_corpus[i]]:\n        vec[j[0]]=j[1]\n    vec=np.array(vec)\n    flag2.append(vec)\n    \n    ","0e73c5ed":"def get_topic(c):\n    return(sorted(lda_model_tfidf[bow_corpus[c]], key=lambda tup: -1*tup[1])[0][0])","9e71e7db":"def get_sents(num,a):\n    sents=[]\n    for i in range(0,num):\n        p=get_topic(i)\n        if(p==a):\n            sents.append(data['cat'][i])\n    return(sents)","f76b4f37":"import matplotlib.pyplot as plt\ndef plot_cat(f):\n    cats=['business','sports','entertainment','india']\n    cout=[]\n    for i in range(0,4):\n        cout.append(get_sents(len(bow_corpus),f).count(cats[i]))\n    plt.bar(cats,cout)\n        \n    ","16c972a0":"#plot_cat(2)","f636163e":"from sklearn import mixture\nfrom sklearn.cluster import AffinityPropagation\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\n#g = mixture.GaussianMixture(n_components=6)\ng=KMeans(n_clusters=6)\n#g=AffinityPropagation()\ng.fit(flag)\nlabels=g.labels_","c3703dd1":"def get_label(a):\n    cats=['business','sports','entertainment','india']\n    lb=[]\n    cout=[]\n    for i in range(0,len(labels)):\n        if(labels[i]==a):\n            lb.append(data['cat'][i])\n    for i in range(0,4):\n        cout.append(lb.count(cats[i]))\n    plt.bar(cats,cout)","6d1b07e4":"get_label(0)","fe15e76d":"get_label(1)  #sports","014cfba2":"get_label(2)  #business","3b32cc19":"get_label(3)  #sports","8adea3d9":"get_label(4)    #entertainment","559eb8a9":"get_label(5)   #india","9fd9e943":"def pred_func(sent):\n    sef=filter_func(sent)\n    b=bow_tfidf([sef])[0]\n    lvec=[0]*6\n    for j in lda_model_tfidf[b]:\n        lvec[j[0]]=j[1]\n    lvec=np.array(lvec)\n    print(lvec)\n    pred=g.predict([lvec])[0]\n    print(pred)","52e7eac2":"pred_func(data['headline_text'][9])","71910670":"# Clustering the vectors generated by LDA","de6a3382":"# Topic exploration using Word embeddings","010e34fd":"# Latent dirichlet alloacation"}}