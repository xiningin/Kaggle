{"cell_type":{"769197ef":"code","f1a16d85":"code","4a3fb984":"code","0a5faf87":"code","eb52cf08":"code","6b99203a":"code","a1b58eb2":"code","c7803b7f":"code","258b973b":"code","6eaab3f7":"code","64618bd4":"code","640ee12a":"code","87067084":"code","1025c811":"code","883bc14a":"code","ad359a8f":"code","96577a1d":"markdown","4e02c3f9":"markdown","2b14b073":"markdown","b3b1e609":"markdown","e7b7cbd5":"markdown","92629c84":"markdown","0c155010":"markdown","d451771c":"markdown","0f562813":"markdown","985d2573":"markdown","3f83ec3e":"markdown","868f3973":"markdown"},"source":{"769197ef":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nfrom scipy import stats","f1a16d85":"df = pd.read_csv('\/kaggle\/input\/customer-analytics\/Train.csv')\ndf = df.drop(['ID'],axis=1)\n\ndf.columns = ['ware_block','mode_ship','cust_call','cust_rating','product_cost','prior_purchase','product_impt','gender','discount','weight','not_ontime_delivery']\n\ndf_num = ['cust_call','cust_rating','product_cost','prior_purchase','discount','weight']\ndf_cat = ['ware_block','mode_ship','gender']\n\n#Normalisasi data\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\ndf['cust_call'] = StandardScaler().fit_transform(df['cust_call'].values.reshape(len(df), 1))\ndf['cust_rating'] = StandardScaler().fit_transform(df['cust_rating'].values.reshape(len(df), 1))\ndf['prior_purchase'] = StandardScaler().fit_transform(df['prior_purchase'].values.reshape(len(df), 1))\n\ndf['product_cost_norm'] = MinMaxScaler().fit_transform(df['product_cost'].values.reshape(len(df), 1))\ndf['discount_norm'] = MinMaxScaler().fit_transform(df['discount'].values.reshape(len(df), 1))\ndf['weight_norm'] = MinMaxScaler().fit_transform(df['weight'].values.reshape(len(df), 1))\ndf.drop(['product_cost','discount','weight'], axis=1, inplace=True)\n\n#labelling pada product importance\ndef product_impt(x):\n    if 'low' in x['product_impt']:\n        product_impt = 1\n    elif 'medium' in x['product_impt']:\n        product_impt = 2\n    else:\n        product_impt = 3\n    return product_impt\ndf['product_impt'] = df.apply(lambda x: product_impt(x), axis=1)\n\n# Feature encoding\nfor cat in df_cat:\n    onehots = pd.get_dummies(df[cat], prefix=cat)\n    df = df.join(onehots)\n#Drop feature awal yang sudah masuk proses feature encoding\ndf.drop(['ware_block','mode_ship','gender','gender_F'], axis=1, inplace=True)\n\nX = df[[col for col in df.columns if (str(df[col].dtype) != 'object') and col not in ['not_ontime_delivery']]]\ny = df['not_ontime_delivery'].values\nfrom imblearn import under_sampling, over_sampling\nX_under, y_under = under_sampling.RandomUnderSampler(0.9).fit_resample(X, y)\nX_over, y_over = over_sampling.RandomOverSampler(0.9).fit_resample(X, y)\nX_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE().fit_resample(X, y)\nprint(pd.Series(y_under).value_counts())\nprint(pd.Series(y_over).value_counts())\nprint(pd.Series(y_over_SMOTE).value_counts())","4a3fb984":"df.to_csv('mycsvfile.csv',index=True)","0a5faf87":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\n\nX = df.drop(columns=['not_ontime_delivery'])\ny = df['not_ontime_delivery'] # target \/ label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\ndef eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n    \n    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) # pos_label: label yang kita anggap positive\n    print(\"AUC: %.2f\" % auc(fpr, tpr))\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","eb52cf08":"X = df.drop(columns=['not_ontime_delivery','ware_block_C','mode_ship_Road','gender_M'])\ny = df['not_ontime_delivery'] # target \/ label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","6b99203a":"X = df.drop(columns=['not_ontime_delivery','ware_block_C','mode_ship_Road','gender_M','ware_block_D','ware_block_B','prior_purchase'])\ny = df['not_ontime_delivery'] # target \/ label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","a1b58eb2":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\n# List of hyperparameter\nmax_depth = [int(x) for x in np.linspace(1, 20, num = 20)] # Maximum number of levels in tree\ncriterion = ['gini','entropy']\nsplitter = ['best','random']\nmin_samples_split = [int(x) for x in np.linspace(1, 100, num = 100)] # Minimum number of samples required to split a node\nmin_samples_leaf = [int(x) for x in np.linspace(1, 1100, num = 1100)] # Minimum number of samples required at each leaf node\nmax_features = ['auto','sqrt', 'log2'] # Number of features to consider at every split\n\nhyperparameters = dict(max_depth=max_depth,\n                       criterion=criterion,\n                       splitter=splitter,\n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                      )\n\n# Inisialisasi Model\ndt = DecisionTreeClassifier(random_state=42)\nmodel = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=42, scoring='recall')\nmodel.fit(X_train, y_train)\n\n# Predict & Evaluation\ny_pred = model.predict(X_test)#Check performa dari model\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\n\nprint('\\nBest max_depth:', model.best_estimator_.get_params()['max_depth'])\nprint('Best Criterion:', model.best_estimator_.get_params()['criterion'])\nprint('Best Splitter:', model.best_estimator_.get_params()['splitter'])\nprint('Best min_samples_split:', model.best_estimator_.get_params()['min_samples_split'])\nprint('Best min_samples_leaf:', model.best_estimator_.get_params()['min_samples_leaf'])\nprint('Best max_features:', model.best_estimator_.get_params()['max_features'])\n\nprint('\\nUnderfitting\/overfitting check')\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))\n","c7803b7f":"#Tree Check\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(30, 10))\ntree.plot_tree(model.best_estimator_,\n               feature_names = X.columns.tolist(), \n               class_names=['0','1'],\n               filled = True, max_depth=7, fontsize=12)\n\nplt.show()","258b973b":"#Check Feature Importance\nfeat_importances = pd.Series(model.best_estimator_.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\nax.invert_yaxis()\n\nplt.xlabel('score')\nplt.ylabel('feature')\nplt.title('feature importance score')","6eaab3f7":"## Random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_test)\neval_classification(rf, y_pred, X_train, y_train, X_test, y_test)","64618bd4":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n#List Hyperparameters yang akan diuji\nhyperparameters = dict(\n                       n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)], # Jumlah subtree \n                       bootstrap = [True], # Apakah pakai bootstrapping atau tidak\n                       criterion = ['gini','entropy'],\n                       max_depth = [int(x) for x in np.linspace(10, 110, num = 11)],  # Maximum kedalaman tree\n                       min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)], # Jumlah minimum samples pada node agar boleh di split menjadi leaf baru\n                       min_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)], # Jumlah minimum samples pada leaf agar boleh terbentuk leaf baru\n                       max_features = ['auto', 'sqrt', 'log2'], # Jumlah feature yg dipertimbangkan pada masing-masing split\n                       n_jobs = [-1], # Core untuk parallel computation. -1 untuk menggunakan semua core\n                      )\n\n# Init\nrf = RandomForestClassifier(random_state=42)\nrf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=42, scoring='recall')\nrf_tuned.fit(X_train,y_train)\n\n# Predict & Evaluation\ny_pred = rf_tuned.predict(X_test)#Check performa dari model\neval_classification(rf_tuned, y_pred, X_train, y_train, X_test, y_test)","640ee12a":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Original Logistic Regression:')\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","87067084":"from sklearn.ensemble import AdaBoostClassifier\nab = AdaBoostClassifier(random_state=42)\nab.fit(X_train,y_train)\n\ny_pred = ab.predict(X_test)\neval_classification(ab, y_pred, X_train, y_train, X_test, y_test)","1025c811":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n# List of hyperparameter\nhyperparameters = dict(n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)], # Jumlah iterasi\n                       learning_rate = [float(x) for x in np.linspace(start = 0.001, stop = 0.1, num = 20)],  \n                       algorithm = ['SAMME', 'SAMME.R']\n                      )\n\n# Init model\nab = AdaBoostClassifier(random_state=42)\nab_tuned = RandomizedSearchCV(ab, hyperparameters, random_state=42, cv=5, scoring='recall')\nab_tuned.fit(X_train,y_train)\n\n# Predict & Evaluation\ny_pred = ab_tuned.predict(X_test)#Check performa dari model\neval_classification(ab_tuned, y_pred, X_train, y_train, X_test, y_test)","883bc14a":"from xgboost import XGBClassifier\nxg = XGBClassifier(random_state=42)\nxg.fit(X_train, y_train)\n\ny_pred = xg.predict(X_test)\neval_classification(xg, y_pred, X_train, y_train, X_test, y_test)","ad359a8f":"# List of hyperparameter\nhyperparameters = {\n                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],\n                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],\n                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],\n\n                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],\n\n                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    \n                    'verbosity': [0] # add this line to slient warning message\n                }\n\n# Init\nxg = XGBClassifier(random_state=42)\nxg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=5, random_state=42, scoring='recall')\nxg_tuned.fit(X_train,y_train)\n\n# Predict & Evaluation\ny_pred = xg_tuned.predict(X_test)#Check performa dari model\neval_classification(xg_tuned, y_pred, X_train, y_train, X_test, y_test)","96577a1d":"## AdaBoost","4e02c3f9":"**Conclusion:** Model dari algoritma Decision tree menghasilkan recall sebesar 98%. Artinya barang 98% diprediksi akan datang terlambat berkaitan dengan variabel:\n- Berat barang yang dikirim (weight)\n- Harga barang (Product cost)\n- Tingkat kepentingan barang (Product importance)\n- Barang yang berasal dari gudang block A (warehouse block A)\n- Jenis pengiriman menggunakan kapal dan pesawat","2b14b073":"## Random Forest","b3b1e609":"## Decision Tree 1","e7b7cbd5":"## Decision Tree 3","92629c84":"## Boosting: XGBoost","0c155010":"**Hasil check** : Model termasuk **fit**, karena train score dan test score mempunyai akurasi yang hampir sama!","d451771c":"## Turning Hyperparameter model 3","0f562813":"## Decision Tree 2","985d2573":"## Logistic Regression","3f83ec3e":"# Cek Model Lainya","868f3973":"#### Dari ketiga jenis Decision Tree yang paling bagus parameter adalah model 3, oleh karena itu kami memutuskan untuk membuat hyperparameter turning untuk memaksimalkan modelnya"}}