{"cell_type":{"c0699b5e":"code","33e4e804":"code","a812fa70":"code","48a98e4c":"code","9a483d62":"code","ef8205d4":"code","d1427a7d":"code","33621419":"code","ed998c91":"code","220fb439":"code","b4a35d31":"markdown","cbd391db":"markdown","0e6b099a":"markdown","633830a3":"markdown","710f7c9a":"markdown","e58e0122":"markdown","a2078f1e":"markdown","8e834c02":"markdown","86c669d6":"markdown"},"source":{"c0699b5e":"import pandas as pd\nimport numpy as np\nimport random\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\nfrom tensorflow.keras.layers import LSTM,GRU,Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n# \u4e71\u6570\u30b7\u30fc\u30c9\u56fa\u5b9a\nseed_everything(2020)","33e4e804":"df = pd.read_excel('..\/input\/datarobot-ai-academy-deep-learning-for-dic\/sales_prediction.xlsx')\n\ndf['Sales_lag7'] = df['Sales'].shift(7)\ndf['Num_Customers_lag7'] = df['Num_Customers'].shift(7)\ndf['Num_Employees_lag7'] = df['Num_Employees'].shift(7)\ndf['Pct_On_Sale_lag7'] = df['Pct_On_Sale'].shift(7)\ndf['Pct_Promotional_lag7'] = df['Pct_Promotional'].shift(7)\ndf['Returns_Pct_lag7'] = df['Returns_Pct'].shift(7)\n\ndf = df[df['Sales_lag7'].notnull()]\ndf = df[df['Sales']>0].reset_index(drop=True)\n\ndisplay(df.head())\ndisplay(df.shape)","a812fa70":"ts_cols = ['Sales_lag7', 'Num_Customers_lag7','Num_Employees_lag7', 'Pct_On_Sale_lag7', \n           'Pct_Promotional_lag7','Returns_Pct_lag7'] \nstep = 7\n\n# \u6b20\u640d\u5024\u88dc\u586b\ndf[ts_cols] = df[ts_cols].fillna(0)\n\n# \u6b63\u898f\u5316\nscaler = StandardScaler()\ndf[ts_cols] = scaler.fit_transform(df[ts_cols]) \n\n# \u7279\u5fb4\u91cf\u914d\u5217\nts_x = np.array([])\nfor i in range(df.shape[0]):\n    if i + step == df.shape[0]:\n        break\n    ts_x = np.append(ts_x, df.loc[i: i+step-1, ts_cols])\n    \nprint ('ts_x',ts_x.shape)","48a98e4c":"target = 'Sales'\n\n# train\u306f\u6700\u7d42\u4e00\u9031\u9593\u524d\u306e\u30c7\u30fc\u30bf\u3001test\u306f\u6700\u7d42\u4e00\u9031\u9593\u306e\u30c7\u30fc\u30bf\ntrain = df[(df['Date']<='2014-06-07')].reset_index(drop=True)\ntest = df[(df['Date']>'2014-06-07')].reset_index(drop=True) \n\n# target\u306e\u958b\u59cb\u4f4d\u7f6e\u306f1 step\u305a\u3089\u3057\u3066\u3068\u308b\ntrain_y = train[step:][target].values\ntest_y = test[target].values\n\nlen_train = len(train)\nlen_test = len(test)\nlen_ts_cols = len(ts_cols)\n\n# \u7279\u5fb4\u91cf\u306e\u7d42\u4e86\u4f4d\u7f6e\u30821\u3000step\u305a\u3089\u3057\u3066\u3068\u308b\ntrain_x_ts = ts_x[:(len_train-step)*step*len_ts_cols]\ntest_x_ts = ts_x[(len_train-step)*step*len_ts_cols:]\n\n# \u5165\u529b\u30c7\u30fc\u30bf\u30923\u6b21\u5143\u306b\u5909\u63db(\u30b5\u30f3\u30d7\u30eb\u6570\u3001\u30bf\u30a4\u30e0\u30b9\u30c6\u30c3\u30d7\u3001\u7279\u5fb4\u91cf\u6570)\ntrain_x_ts = train_x_ts.reshape(len_train-step, step, len_ts_cols)\ntest_x_ts = test_x_ts.reshape(len_test, step, len_ts_cols)\n\nprint ('train_x_ts',train_x_ts.shape)\nprint ('test_x_ts',test_x_ts.shape)\nprint ('train_y',train_y.shape)\nprint ('test_y',test_y.shape)","9a483d62":"def rnn(step, ts_cols):    \n    model = Sequential()\n    \"\"\"\n    \u6f14\u7fd2:BiLSTM\u3001GRU\u306b\u5909\u66f4\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\n    from tensorflow.keras.layers import Bidirectional,GRU\n    LSTM -> Bidirectional(LSTM)\n    LSTM -> GRU\n    \"\"\"\n    model.add(GRU(units=512, activation='relu', kernel_initializer='he_normal', \n                   return_sequences=True, input_shape=(step, len(ts_cols))))\n    model.add(Dropout(0.1)) \n    model.add(GRU(units=256, activation='relu', kernel_initializer='he_normal', return_sequences=True))\n    model.add(Dropout(0.1)) \n    model.add(GRU(units=128, activation='relu', kernel_initializer='he_normal', return_sequences=False))\n    model.add(Dropout(0.1)) \n    model.add(Dense(256, activation='relu', kernel_initializer='he_normal',))\n    model.add(Dropout(0.1)) \n    model.add(Dense(128, activation='relu', kernel_initializer='he_normal',))  \n    model.add(Dropout(0.1)) \n    model.add(Dense(32, activation='relu', kernel_initializer='he_normal',))  \n    model.add(Dropout(0.1)) \n    model.add(Dense(1, activation='linear'))\n    model.compile(loss='mape', optimizer='adam', metrics=['mape']) \n    return model","ef8205d4":"# callback parameter\nfilepath = \"rnn_best_model.hdf5\" \nes = EarlyStopping(patience=5, mode='min', verbose=1) \ncheckpoint = ModelCheckpoint(monitor='val_loss',filepath=filepath, save_best_only=True,mode='auto') \nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1,mode='min')\n\n# \u8a13\u7df4\u5b9f\u884c\nmodel = rnn(step,ts_cols)\nhistory = model.fit(train_x_ts, train_y, batch_size=32, epochs=30, validation_data=(test_x_ts, test_y),\n                 callbacks=[es, checkpoint, reduce_lr_loss], verbose=1)\n","d1427a7d":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n# load best model weights\nmodel.load_weights(filepath)\n\n# predict\ntest_pred = model.predict(test_x_ts, batch_size=32).reshape((-1,1))\npred_score = mean_absolute_percentage_error(test_y,  test_pred)\nprint ('test mape:',pred_score)","33621419":"model.summary()","ed998c91":"plot_model(model, to_file='rnn.png')","220fb439":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))\nplt.plot(epochs, loss, 'bo' ,label = 'training loss')\nplt.plot(epochs, val_loss, 'b' , label= 'validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","b4a35d31":"# \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1","cbd391db":"# \u6642\u7cfb\u5217 \u7279\u5fb4\u91cf\u914d\u5217\u4f5c\u6210","0e6b099a":"# \u30e2\u30c7\u30eb\u53ef\u8996\u5316","633830a3":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","710f7c9a":"# \u30e2\u30c7\u30eb\u306e\u8a13\u7df4","e58e0122":"# LSTM \u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b","a2078f1e":"# \u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0","8e834c02":"# \u8a13\u7df4\u5c65\u6b74\u53ef\u8996\u5316","86c669d6":"# \u8a13\u7df4\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4f5c\u6210"}}