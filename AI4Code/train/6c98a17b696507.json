{"cell_type":{"b44d52c9":"code","aecb4c1a":"code","0d944753":"code","611a9749":"code","16c4ceab":"code","0a559ada":"code","959298c7":"code","e87b2b92":"code","5e75a089":"code","5488db99":"code","08f247ff":"code","89fd0be0":"code","6f48bed5":"code","2bd579b9":"code","fd292a78":"code","0aadac13":"code","69247e86":"code","c4a30245":"code","a39cdf38":"code","89931d18":"code","7a037d3b":"markdown","d9dd6e7f":"markdown","8eec2c2f":"markdown","d97880cc":"markdown"},"source":{"b44d52c9":"import seaborn as sns\nimport sys\nimport csv\nimport datetime\nimport operator\nimport joblib\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom statsmodels.formula.api import ols\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy.stats import norm, skew, probplot","aecb4c1a":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","0d944753":"df_train.isna().sum()","611a9749":"df_train=df_train.drop(['Name','Ticket','Cabin'],axis=1)\ndf_test=df_test.drop(['Name','Ticket','Cabin'],axis=1)","16c4ceab":"embarked_mode = df_train['Embarked'].mode()\ndf_train['Embarked'] = df_train['Embarked'].fillna(embarked_mode)\ndf_test['Embarked'] = df_test['Embarked'].fillna(embarked_mode)\n\ndf_train['Age'] = df_train['Age'].fillna(-1)\ndf_test['Age'] = df_test['Age'].fillna(-1)","0a559ada":"df_train = pd.get_dummies(df_train)\ntest = pd.get_dummies(df_test)","959298c7":"df_train.info()","e87b2b92":"# \ud5a5\ud6c4 train Data\uac00 \ud544\uc694\ud560 \uac83\uc774\uae30\uc5d0 df(data file)\uc5d0 \ud560\ub2f9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n# Since we will need train data in the future, we allocated it to df (data file)\ndf = df_train","5e75a089":"random_state_val =42\ntest_size_val =0.01\ndf_trval,df_test = train_test_split(df, test_size = test_size_val, random_state = random_state_val)\n\ntest_size_val = 0.2\ndf_train, df_val = train_test_split(df_trval, test_size = test_size_val, random_state = random_state_val)","5488db99":"drop_col = ['Survived']\ny_nm = 'Survived'\n\ndf_train_x = df_train.drop(drop_col, axis = 1)\ndf_train_y = pd.DataFrame(df_train[y_nm])\n\ndf_val_x = df_val.drop(drop_col, axis = 1)\ndf_val_y = pd.DataFrame(df_val[y_nm])\n\ndf_test_x = df_test.drop(drop_col, axis = 1)\ndf_test_y = pd.DataFrame(df_test[y_nm])","08f247ff":"XGBClassifier = xgb.XGBClassifier(max_depth = 9,\n                                 learning_rate = 0.001,\n                                 n_estimators = 5000,\n                                 objective = 'binary:logistic',\n                                 booster = 'gbtree',\n                                 gamma = 0.04,\n                                 max_delta_step =5,\n                                 min_child_weight = 2.8189,\n                                 subsample = 0.8104,\n                                 colsample_bytree = 0.6332\n                                 )","89fd0be0":"start = datetime.datetime.now()\nxgb = XGBClassifier.fit(df_train_x.values,\n                       df_train_y.values.ravel(),\n                       eval_set = [(df_train_x.values, df_train_y), (df_val_x.values, df_val_y)],\n                       eval_metric ='logloss',\n                       early_stopping_rounds = 10,\n                       verbose = False)\nend = datetime.datetime.now()\nend-start","6f48bed5":"fi_vals = xgb.get_booster().get_score(importance_type = 'weight')\nfi_dict = {df_train_x.columns[i]:float(fi_vals.get('f'+str(i),0.)) for i in range(len(df_train_x.columns))}\nfeature_importance_ = sorted(fi_dict.items(), key=operator.itemgetter(1), reverse=True)\nfeature_importance_result = OrderedDict(feature_importance_)\n\nimportance = pd.DataFrame(feature_importance_)\nimportance.columns = ['feature','weight']\nimportance.head(10)","2bd579b9":"importance_ten = importance[:10]\nimportance_ten.set_index('feature').sort_values(by='weight').plot(kind='barh', figsize=(5, 5))","fd292a78":"result_lst =[]\nmax_accuracy =0.\nopt_threshold =0.\nval_y_prob = xgb.predict_proba(df_val_x.values)[:, 1]\n\nfor n in range(0,50):\n    threshold = round(((n+1)*0.01),2)\n    pred_yn = val_y_prob.copy()\n    pred_yn = np.where(pred_yn > threshold, 1., 0.)\n    \n    result_dict = {}\n    precision, recall, f1_score, support = precision_recall_fscore_support(df_val_y.values.ravel(), pred_yn, average='binary')\n    accuracy = accuracy_score(df_val_y.values.ravel(), pred_yn)\n    kappa = cohen_kappa_score(df_val_y.values.ravel(), pred_yn)\n    \n    result_dict ={'Threshold': threshold, 'Accuracy': round(accuracy,4), 'Precision': round(precision,4), 'Recall': round(recall,4), 'F1_Score': round(f1_score,4), 'Kappa': round(kappa,4)}\n    result_lst.append(result_dict)\n    \n    if max_accuracy <= accuracy:\n        max_accuracy = accuracy\n        opt_threshold = threshold\n        \n    confMat = confusion_matrix(df_val_y.values.ravel(), pred_yn, labels=[1,0])\n    \nmatric_df = pd.DataFrame(result_lst, columns=['Threshold','Accuracy', 'Precision', 'Recall', 'F1_Score', 'Kappa'])\nmatric_df.to_csv('REC_scores.csv',sep=',', header=True, index=False, encoding='UTF-8')\n\nprint('Max Accuracy =%f, optimized_threshold=%f'%(max_accuracy, opt_threshold))\nprint('Complete')","0aadac13":"predict_xgb = xgb.predict_proba(df_train_x.values)[:,1]\npred_train = np.where(predict_xgb > opt_threshold, 1., 0.)\n\ntp, fn, fp, tn = confusion_matrix(df_train_y.values.ravel(), pred_train, labels=[1,0]).ravel()","69247e86":"conf_matrix = pd.DataFrame(\n    confusion_matrix(df_train_y.values.ravel(), pred_train),\n    columns=['Predicted Value 0', 'Predicted Value 1'],\n    index=['True Value 0', 'True Value 1']\n)\n\nprint(\"1. Counfusion Matrix\")\nprint(conf_matrix.T)\nprint(\"\")\n\nprint(\"2. Classification Report\")\nprint(classification_report(df_train_y.values.ravel(), pred_train))","c4a30245":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(df_train_y.values.ravel(), predict_xgb)\n\nimport matplotlib.pyplot as plt\nroc_auc = auc(fpr, tpr)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","a39cdf38":"Accuracy_Rate = (tp + tn) \/ (tp + tn + fp + fn)\nRecall_Rate = tp \/ (tp + fn)\nPrecision_Rate = tp \/ (tp + fp)\nSpecificity_Rate = tn \/ (tn + fp)\nF1_Score = (Precision_Rate * Recall_Rate) \/ (Precision_Rate + Recall_Rate) * 2\n\nprint(\"3. Model Metric Sumamry\")\nprint(\" - Accuracy Rate    : {:2.3f} %\".format(Accuracy_Rate*100))\nprint(\" - Recall Rate      : {:2.3f} %\".format(Recall_Rate*100))\nprint(\" - Precision Rate   : {:2.3f} %\".format(Precision_Rate*100))\nprint(\" - Specificity Rate : {:2.3f} %\".format(Specificity_Rate*100))\nprint(\" - F1 Score         : {:2.3f} \".format(F1_Score*100))\nprint(\" - ROC AUC          : {:2.3f} \".format(roc_auc*100))","89931d18":"predict_xgb = xgb.predict_proba(test.values)[:,1]\npred_test = np.where(predict_xgb > opt_threshold, 1., 0.)\n\ntest_result= pd.DataFrame(pred_test)\ntest_result.columns = ['Survived']\npredict = test_result['Survived']\nId_No = test['PassengerId']\nsubmission = pd.DataFrame({'PassengerId': Id_No, \"Survived\": predict})\nsubmission['Survived'] = submission['Survived'].astype('Int64')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","7a037d3b":"### 3. \ub370\uc774\ud130 \uc804\ucc98\ub9ac(Pretreatment)\n\n#### \uc774\ubbf8 \ub9ce\uc740 \ubd84\ub4e4\uc774 EDA\ub97c \ud558\uc5ec \ubcc0\uc218\uc5d0 \ub300\ud55c \ubd84\uc11d\uc774 \ub418\uc5b4 \uc788\ub294\ubc14, \uc774\ubd80\ubd84\uc740 \uc81c\uc678\ud558\uace0 \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n#### A lot of people have already done EDA to analyze the variables, so I will exclude this part.","d9dd6e7f":"### 1. \ubaa8\ub4c8 \ubd88\ub7ec\uc624\uae30 (Import module)\n \n#### \uc81c\uac00 \uac00\uc7a5 \uc790\uc8fc \uc4f0\ub294 \ubaa8\ub4c8\ub4e4\uc744 \ubd88\ub7ec\uc62c \uc608\uc815\uc785\ub2c8\ub2e4.\n#### It will load all the modules I use the most.","8eec2c2f":"#### \ub2e8\uacc4\uc801\uc73c\ub85c EDA\ub97c \ud1b5\ud574 \uc804\ucc98\ub9ac\uc640 \ubaa8\ub378 \uc815\uaddc\ud654\ub97c \uc9c4\ud589 \uc608\uc815\uc785\ub2c8\ub2e4.\n#### \uc774\ubc88 \ucf54\ub4dc\ub294 \uacfc\uc5f0 \ucd5c\uc18c\ud55c\uc758 \uc791\uc5c5\uc73c\ub85c \uc5bc\ub9c8 \uc815\ub3c4\uc758 \uc131\uacfc\uac00 \ub098\uc62c\uc218 \uc788\ub294\uc9c0 \ud655\uc778\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n#### \ub2e4\uc74c\uc5d0\ub294 \uc0c1\uc704\uad8c \uacb0\uacfc\uac00 \ub098\uc62c\uc218 \uc788\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n#### I plan to proceed with preprocessing and model normalization through EDA in stages.\n#### Let's check how much performance can be achieved with this code with the least amount of work.\n#### In Second, I will make sure that the top results come out.","d97880cc":"### 2. \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30(Read Dataset)"}}