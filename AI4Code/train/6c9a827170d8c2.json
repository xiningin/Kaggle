{"cell_type":{"72d1d990":"code","28ade5fe":"code","9d86af03":"code","88b80363":"code","28388a7f":"code","6e5ffa9f":"code","7cffdd08":"code","3b7c7c68":"code","8989164a":"code","f17a3247":"code","cc91c43c":"code","f9e6b1bd":"code","92af3c47":"code","4e9ad52b":"code","7f1c422a":"code","77a6e79b":"code","8d955bbe":"code","184da658":"code","8b3ea5e8":"code","1cdce132":"code","8944a6db":"code","d729f52d":"code","fc7551f3":"code","8b445387":"code","a22d275d":"code","7efaf8ca":"code","7d0e60aa":"code","25966c56":"code","92228daa":"code","b8edd41c":"code","a72ff308":"code","f7c49f2a":"code","d863b6b9":"code","a0576e1e":"code","7739fd0b":"code","18d850a2":"code","a203e947":"markdown","c499b714":"markdown","06a0bb67":"markdown","efe6ee3b":"markdown","4742fca0":"markdown","c647b045":"markdown","354bcb8d":"markdown","eb3716f0":"markdown","6ffb22c2":"markdown","80a78141":"markdown","2c631847":"markdown","f171be1f":"markdown","5c3b1ded":"markdown","ffe14338":"markdown","c211f102":"markdown","a05e9a4d":"markdown","56c9ed5b":"markdown","352724e8":"markdown","69e96f91":"markdown","02003410":"markdown","8c2978f1":"markdown","f297b408":"markdown","0f1cd78a":"markdown","56e87342":"markdown","b51dd71f":"markdown","ede77059":"markdown","00cb7963":"markdown","440fe620":"markdown","afc4ba31":"markdown","8e73c6b4":"markdown","279fe005":"markdown","c23d2c0e":"markdown","e2182834":"markdown","234699f1":"markdown","af475c7c":"markdown","5782102c":"markdown"},"source":{"72d1d990":"#importing libs\nimport numpy as np \nimport pandas as pd \nimport pandas_profiling\nfrom pandas_profiling import ProfileReport\nimport matplotlib\nimport matplotlib.pyplot as plt \nimport seaborn as sns #data visualization\nimport warnings\n\nfrom sklearn import preprocessing\n\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve\nfrom sklearn import tree\nfrom sklearn.tree import plot_tree \n\n# Cross Validation\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, precision_score, f1_score\n\nwarnings.filterwarnings(\"ignore\")","28ade5fe":"#Reading the csv file car.data in data frame\ndf = pd.read_csv('..\/input\/car-data\/car.data', names=['buying','main','doors','persons','lug_boot','safety','class'])","9d86af03":"#printing first 5 rows\ndf.head(5)","88b80363":"#printing last 5 rows\ndf.tail()","28388a7f":"print('Number of rows are :-',df.shape[0], ',and number of columns are :-',df.shape[1])","6e5ffa9f":"# Performing Exploratory Data Analysis using Panda Profiling\n#profile = ProfileReport(df)\n# putting Exploratory Data Analysis report to the notebook\n#profile.to_file(\"Exploratory Data Analysis Report.html\")\n#profile.to_widgets()","7cffdd08":"sns.countplot('class', data= df)","3b7c7c68":"print(df.info())","8989164a":"# Visualizing the feature: \"buying\" with Target\nsns.countplot(x=\"buying\", data=df, hue=\"class\")\nplt.show()","f17a3247":"# Visualizing the feature: \"main\" with Target\nsns.countplot(x=\"main\", data=df, hue=\"class\")\nplt.show()","cc91c43c":"# Visualizing the feature: \"doors\" with Target\nsns.countplot(x=\"doors\", data=df, hue=\"class\")\nplt.show()","f9e6b1bd":"# Visualizing the feature: \"persons\" with Target\nsns.countplot(x=\"persons\", data=df, hue=\"class\")\nplt.show()","92af3c47":"# Visualizing the feature: \"lug_boot\" with Target\nsns.countplot(x=\"lug_boot\", data=df, hue=\"class\")\nplt.show()","4e9ad52b":"# Visualizing the feature: \"safety\" with Target\nsns.countplot(x=\"safety\", data=df, hue=\"class\")\nplt.show()","7f1c422a":"# Other way to confirm any value is missing in the data set apart from panda profiling\ndf.isna().sum()","77a6e79b":"#defining all the labels\nlabels = {\n    'buying': ['vhigh', 'high', 'med', 'low'],\n    'main': ['vhigh', 'high', 'med', 'low'],\n    'doors': ['2', '3', '4', '5more'],\n    'persons': ['2', '4', 'more'],\n    'lug_boot': ['small', 'med', 'big'],\n    'safety': ['low', 'med', 'high'],\n    'class': ['unacc', 'acc', 'good', 'vgood']\n}","8d955bbe":"#Lable encoding catagorical variables\nlabel_encoders = {}\ndf_encoded = pd.DataFrame()\nfor column in df:\n    if column in labels:\n        label_encoders[column] = preprocessing.LabelEncoder()\n        label_encoders[column].fit(labels[column])\n        df_encoded[column] = label_encoders[column].transform(df[column])\n    else:\n        df_encoded[column] = df[column]\n#printing first 5 rows after encoding\ndf_encoded.head(5)","184da658":"#checking corelation of different features\nfig = plt.figure(figsize=(15,10))\nsns.heatmap(df_encoded.corr(), cmap=\"Spectral\", annot=True)\nplt.title(\"Correlation Heatmap\")\nplt.show()","8b3ea5e8":"# defining the features and target\nX = df_encoded.drop(['class'],axis=1)\ny = df_encoded[['class']]\nX.head(5)","1cdce132":"#Train and test split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint(\"The shape of X_train is      \", X_train.shape)\nprint(\"The shape of X_test is       \",X_test.shape)\nprint(\"The shape of y_train is      \",y_train.shape)\nprint(\"The shape of y_test is       \",y_test.shape)","8944a6db":"#Create Decision Tree Classification Model\ndt_model = DecisionTreeClassifier(criterion = \"entropy\", random_state = 42)\ndt_model.fit(X_train, y_train) \n  \n#Predict the value for new, unseen data\npredict_dt = dt_model.predict(X_test)","d729f52d":"#Getting the complete Classification Report for Decision Tree Classification Model\nprint(classification_report(y_test, predict_dt))","fc7551f3":"#Create logistic Regression Model\nlogmodel = LogisticRegression(random_state=42)\nlogmodel.fit(X_train,y_train)\n\n#Predict the value for new, unseen data\npredict_logmodel = logmodel.predict(X_test)","8b445387":"#Getting the complete Classification Report for logistic Regression Model\nprint(classification_report(y_test, predict_logmodel))","a22d275d":"#Create K-Nearest Neighbor Model\nknn_model = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2) #p=2 represents Euclidean distance\nknn_model.fit(X_train, y_train) \n  \n#Predict the value for new, unseen data\npredict_knn = knn_model.predict(X_test)","7efaf8ca":"#Getting the complete Classification Report for K-Nearest Neighbor Model\nprint(classification_report(y_test, predict_knn))","7d0e60aa":"#Create the Support Vector Machine Model\nsvc_model = SVC(kernel='linear', random_state=42, probability=True)\nsvc_model.fit(X_train,y_train)\n\n#Predict the value for new, unseen data\npredict_svc = svc_model.predict(X_test)","25966c56":"#Getting the complete Classification Report for Support Vector Machine Model\nprint(classification_report(y_test, predict_svc))","92228daa":"#Function to print confusion matrix\ndef plot_confusion_matrix(y_true, y_pred, figsize=(10,7)):\n    names = [\"unacc\", \"acc\", \"good\", \"vgood\"]\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = '0'\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(names), columns=np.unique(names))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(cm, cmap= \"Blues\", annot=annot, fmt='', ax=ax)","b8edd41c":"#confusion matrix for decision tree\nplot_confusion_matrix(y_test,predict_dt)","a72ff308":"#confusion matrix for logistic reg\nplot_confusion_matrix(y_test,predict_logmodel)","f7c49f2a":"#confusion matrix for KNN\nplot_confusion_matrix(y_test,predict_knn)","d863b6b9":"#confusion matrix for support vector\nplot_confusion_matrix(y_test,predict_svc)","a0576e1e":"dt_df = pd.DataFrame(data=[f1_score(y_test,predict_dt, average='macro'),accuracy_score(y_test, predict_dt), recall_score(y_test, predict_dt,average='macro'), precision_score(y_test, predict_dt, average='macro')], \n             columns=['Decision Tree'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\"])\nlr_df = pd.DataFrame(data=[f1_score(y_test,predict_logmodel, average='macro'),accuracy_score(y_test, predict_logmodel), recall_score(y_test, predict_logmodel, average='macro'),precision_score(y_test, predict_logmodel, average='macro')], \n             columns=['logistic Regression'],index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\"])\nknn_df = pd.DataFrame(data=[f1_score(y_test,predict_knn, average='macro'),accuracy_score(y_test, predict_knn), recall_score(y_test, predict_knn, average='macro'), precision_score(y_test, predict_knn, average='macro')], \n             columns=['K-Nearest Neighbor'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\"])\nsvc_df = pd.DataFrame(data=[f1_score(y_test,predict_svc, average='macro'),accuracy_score(y_test, predict_svc), recall_score(y_test, predict_svc, average='macro'), precision_score(y_test, predict_svc, average='macro')], \n             columns=['Support Vector Machine'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\"])\n\ndf_models = round(pd.concat([dt_df,knn_df,svc_df,lr_df], axis=1),4)\n\nbackground_color = \"white\"\n\nfig = plt.figure(figsize=(18,26)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap='Blues',annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\nplt.show()","7739fd0b":"importance = dt_model.feature_importances_\nprint(\"Feature importance table:\\n\")\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print(X.columns[i], ':', round((v*100),2), '%')","18d850a2":"#pridicting class for test data using best and wrost model\n#Predicted DT --> pridiction using decision tree model\n#Predicted LR --> pridiction using logistic regression\n\nclass_labels = [\"unacc\", \"acc\", \"good\", \"vgood\"]\nnew_dp = X_test.copy().reset_index(drop=True)\nactual = y_test.copy()\nactual = actual.reset_index(drop=True)\npredicted_dt = dt_model.predict(X_test)\npredicted_lr = logmodel.predict(X_test)\nnew_dp['Actual']=''\nnew_dp['Predicted DT']=''\nnew_dp['Predicted LR']=''\nfor x in range(len(X_test)):\n  new_dp['Predicted DT'].values[x] = class_labels[predicted_dt[x]]\n  new_dp['Predicted LR'].values[x] = class_labels[predicted_lr[x]]\n  new_dp['Actual'].values[x] = class_labels[actual['class'][x]]\n\n#printing first 20 rows \nnew_dp.head(20)","a203e947":"**Observation:**\n*  Accuracy is 97%","c499b714":"# Prediction for the test data ","06a0bb67":"# Data Pre-processing and cleaning","efe6ee3b":"# Data Preparation","4742fca0":"**Observation**\n*  When mentinance cost is high or very high then car is not good ","c647b045":"**Observation**\n*  Buying price is the most co related with class.\n*  Safety is the least corelated with class.","354bcb8d":"**Observation:**\n\n*  More the size of luggage boot of the car better is the rating\/class\n* Medium size of luggage boot is having better rating than big size of luggage boot","eb3716f0":"# Results and conclusion\n","6ffb22c2":"**Observation**\n*  When buying price is high or very high then car is no not good or very good\n*  only medium and low buying price cars are good and very good","80a78141":"## Logistic Regression Model","2c631847":"**Observation:**\n*  Accuracy is 66%","f171be1f":"# Car acceptability Rating\n## About this Assignment\nToday's market is full of varity of cars from hyndreds of manufacturer. It has server both status and utility requirement for the civilization. In this analysis we will try focus on the acceptability of cars based on features they have. \n\n## Objectives\n\n## Data Sources\nThe model evaluates cars according to the following concept structure:\n\n*  buying -> buying price\n*  main -> price of the maintenance\n*  doors -> number of doors\n*  persons -> capacity in terms of persons to carry\n*  lug_boot -> the size of luggage boot\n*  safety -> estimated safety of the car\n\n## Team\nA team of 3 individual will be working on this project. Two of the team member will be focusing on data analytics where one of the team member will be working as Machine Learning engineer. However there is no hard boundary.","5c3b1ded":"## K-Nearest Neighbor Model","ffe14338":"**Observation:**\n\n*  It is observed that there isn't much of a correlation between cars door and its class","c211f102":"**Observation**\n*  91.6% of acceptable class has been pridicted correctly\n*  90.9% of good class has been pridicted correctly\n*  100% of unacceptable class has been pridicted correctly\n*  94.1% of very class has been pridicted correctly","a05e9a4d":"**Observation:**\n*  Accuracy is 89%","56c9ed5b":"## Final comparison","352724e8":"**Observation**\n*  74.7% of acceptable class has been pridicted correctly\n*  54.5% of good class has been pridicted correctly\n*  100% of unacceptable class has been pridicted correctly\n*  35.3% of very class has been pridicted correctly","69e96f91":"**Observation:**\n\n*  More the capacity of the car better is the rating\/class","02003410":"## Confusion Matrix","8c2978f1":"**Observation**\n*  13.3% of acceptable class has been pridicted correctly\n*  0% of good class has been pridicted correctly\n*  92.3% of unacceptable class has been pridicted correctly\n*  0% of very class has been pridicted correctly","f297b408":"**Observation:**\n\n*  More the safety of the car better is the rating\/class","0f1cd78a":"#  Import Libraries\/Dataset","56e87342":"# Model Building","b51dd71f":"**Observation**\n*  The class distribution is not **balance**\n*  Most of the cars are unacceptable\n*  There is very less number of good and very good cars","ede77059":"**Observation**\n*  10.8% of acceptable class has been pridicted correctly\n*  0% of good class has been pridicted correctly\n*  98.7% of unacceptable class has been pridicted correctly\n*  0% of very class has been pridicted correctly","00cb7963":"## Support Vector Machine Model","440fe620":"# Data Understanding\nWe will be doing statistical analysis of the data. Let us start by looking into first and last 5 records of our data set.","afc4ba31":"# Data Visualization and Exploration","8e73c6b4":"**Observation:**\n*   We can see that there are no missing rows in the entire dataset. so we do not need to fill\/drop any value","279fe005":"# Performance Evaluation","c23d2c0e":"**Observation**\n*  Decision tree is the best model suited for this problem with accuracy of 97.4%","e2182834":"**Observation:**\n*  Accuracy is 70%","234699f1":"## Decision Tree Classification Model","af475c7c":"*  There was no missing rows in the entire dataset. so we do not need to fill\/drop any value.\n*  When buying price is high or very high then car is no not good or very good\n*  When mentinance cost is high or very high then car is not good \n*  It is observed that there isn't much of a correlation between cars door and its class\n*  More the capacity of the car better is the rating\/class\n*  More the size of luggage boot of the car better is the rating\/class\n*  More the safety of the car better is the rating\/class\n*  We tested and validated 4 data model. **Decision Tree achieved a max accuracy of 97.4%.**\n\n*  Top 4 significant factors that contribute towards prediction the heart attack risk of a patient are:\n1.  safety\n2.  maintinance cost\n3.  capacity\n4.  buying price\n","5782102c":"**Observation**\n* Most important features are safety, maintinance cost, capacity and buying price\n* Least important features are no of door and lug boot"}}