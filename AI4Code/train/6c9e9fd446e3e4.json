{"cell_type":{"83cf69f6":"code","8e5e18b5":"code","9a57114b":"code","a74db63b":"code","b6febdfe":"code","6df4c341":"code","c5c3738e":"code","40f24507":"code","a8cd73fd":"code","9818366e":"code","b7235536":"code","84e3e28f":"code","63f2c9a1":"code","694b2163":"code","fec08f27":"code","80469adc":"code","d3326bec":"code","0924ae13":"code","84ba52ee":"code","5d0043c3":"code","664b36a8":"markdown","0d27753d":"markdown","6df91401":"markdown","d123ce81":"markdown","b820cb56":"markdown","bf3dba70":"markdown","75c3489d":"markdown","c6e78b95":"markdown","619d2d6a":"markdown","9a31527d":"markdown","7a1800fa":"markdown","816dd128":"markdown"},"source":{"83cf69f6":"# In this study I have utilized a dataset that exist at: \n# https:\/\/moodle.telt.unsw.edu.au\/mod\/resource\/view.php?id=2535204.\n# AND I UPLOADED THE SAME IN MY KAGGLE FOLDER INPUT\\Credit Card Balanced Data. \n# Please don't get confused by MY folder name, \n# the folder has UN-BALANCED DATA, WHICH WILL BE BALANCED as we PROCEED.\n# Please note, this\n# dataset used in my Research Report is not exactly the same as the one used in the paper\n# by Wickramasinghe, R. I. P. (2017), but very close. \nimport pandas as pd\ncreditcard = pd.read_csv(\"..\/input\/credit-card-balanced-data\/creditcard.csv\")\n\n# ****** THE PAPER by Wickramasinghe, R. I. P. (2017). \n# titled \"Attribute Noise, Classification Technique, and Classification Accuracy\". \n# In Data Analytics and Decision Support for Cybersecurity (pp. 201-220). Springer, Cham. USES THE FOLLOWING\n# DATASET \n# This secondary dataset has been modified from the initial dataset, which contains\n# credit cards\u2019 transactions by European credit cards holders within two days in\n# September 2013. This dataset includes 29 features including time, amount, and the\n# time duration of the transaction in seconds\n# Copyright\u00a9: This dataset is made available under the Open Database License\n# (http:\/\/opendatacommons.org\/licenses\/odbl\/1.0\/). The Open Database License\n# (ODbL) is license agreement intended to allow users to freely share, modify,\n# and use this Dataset while maintaining this freedom for others, provided that the\n# original source and author(s) are credited.\n# *** BUT I AM NOT USING THE SAME for my Research Report *** BUT THE ONE PICKED FROM  \n# https:\/\/moodle.telt.unsw.edu.au\/mod\/resource\/view.php?id=2535204\n# AND UPLOADED IN MY KAGGLE FOLDER INPUT\\Credit Card Balanced Data. Please don't get confused by folder name, \n# the folder has UN-BALANCED DATA, WHICH WILL BE BALANCED as we PROCEED.\n\n","8e5e18b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('..\/input\/credit-card-balanced-data\/creditcard.csv')\ndf.head()","9a57114b":"df.describe()","a74db63b":"# Good No Null Values!\ndf.isnull().sum().max()","b6febdfe":"df.columns","6df4c341":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","c5c3738e":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","40f24507":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","a8cd73fd":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","9818366e":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","b7235536":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","84e3e28f":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows. \nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","63f2c9a1":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\nsns.countplot('Class', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","694b2163":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","fec08f27":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V2 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","80469adc":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","d3326bec":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","0924ae13":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","84ba52ee":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","5d0043c3":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","664b36a8":"<h3> Correlation Matrices <\/h3>\nCorrelation matrices are the essence of understanding our data. I want to know if there are features that influence heavily for a specific transaction that is a fraud. However, it is important that we use the correct data-frame (sub-sample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\n### Summary and Explanation: \n<ul>\n<li><b>Negative Correlations: <\/b>V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  <\/li>\n<li> <b> Positive Correlations: <\/b> V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. <\/li>\n<li> <b>BoxPlots: <\/b>  Boxplots can be used to better understanding of the distribution of these features in fraudulent and non fraudulent transactions. <\/li>\n<\/ul>\n**Note: ** We have to make sure we use the sub-sample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original data-frame.","0d27753d":"##  Equally Distributing and Correlating: \n<a id=\"correlating\"><\/a>\nNow that I have our dataframe correctly balanced, we can go further with our <b>analysis<\/b> and <b>data preprocessing<\/b>.","6df91401":"## Gather Sense of Data-set Provided:\n<a id=\"gather\"><\/a>\nExcept for the <b>transaction<\/b> and <b>amount<\/b> and  <b>time duration <\/b> we don't know what the other columns due to privacy reasons.Though it would be interesting to know all the included attribute names in the data-set, due to the confidentiality issues the data do not disclose all the background information.   \n\n<h3> Data Analysis: <\/h3>\n<ul>\n<li>The transaction amount is relatively <b>small<\/b>. The mean of all the amounts made is approximately 88 USD (precisely 88.349619) <\/li>\n<li>There are no <b>\"Null\"<\/b> values, so I don't have to work on ways to replace values. <\/li>\n<li> Most of the transactions are <b>Non-Fraud<\/b> (99.83%) of the time, while <b>Fraud<\/b> transactions occurs (0.17%) of the time in the data-frame. <\/li>\n<\/ul>\n\n<h3> Feature Technicalities: <\/h3>\n<ul>\n    <li> <b>**Note:**<\/b>  Our original data-set is imbalanced! Most of the transactions are non-fraud.<\/li>\n    <li>If we use this data-frame as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably over-fit since it will \"assume\" that most transactions are not fraud.<\/li>\n<li> But we don't want our model to assume, we want our model to detect patterns that give signs of fraud! <\/li>\n<\/ul>","d123ce81":"## Comparative Results Logistic Regression vs Support Vector Classifier:\n<a id=\"testing_logistic\"><\/a>\n## Confusion Matrix:\n**Positive\/Negative:** Type of Class (label) [\"No\", \"Yes\"]\n**True\/False:** Correctly or Incorrectly classified by the model.<br><br>\n\n**True Negatives (Top-Left Square):** This is the number of **correctly** classifications of the \"No\" (No Fraud Detected) class. <br><br>\n\n**False Negatives (Top-Right Square):** This is the number of **incorrectly** classifications of the \"No\"(No Fraud Detected) class. <br><br>\n\n**False Positives (Bottom-Left Square):** This is the number of **incorrectly** classifications of the \"Yes\" (Fraud Detected) class <br><br>\n\n**True Positives (Bottom-Right Square):** This is the number of **correctly** classifications of the \"Yes\" (Fraud Detected) class.\n\nIn section 3.2 the author explains about the Performance Indicators namely precision, specificity, sensitivity (recall), and F-Measure and same Indicators I have used to prove my results.\nLet\u2019s assume P to be the total number of positive instances and N be the total\nnumber of negative instances. Then the performance indicators ca be defined as\nfollows.\nSensitivity = FP \/P\n\nSpecificity = FP \/ N\n\nPrecision = TP\/ FP + TP\nwhere P represents total number of positive (Class A), instances while N represents\nthe total number of class B instances.\nF-Measure = 2[Precision * Sensitivity\/Precision + Sensitivity]\n\n<b>Looking at the summary of my results, Logistics Regression is Winner over Support Vector Classifier(SVC)<\/b>\n ![image.png](attachment:image.png) \n\n","b820cb56":"### Accuracy Score: Logistic Regression vs KNearest vs Support Vector Classifier vs Decision Tree Classifier\n<a id=\"logistic\"><\/a>\nIn this section I will implement the classifiers and show comparative results\n\n\n### Terms:\n<ul>\n<li><b>True Positives:<\/b> Correctly Classified Fraud Transactions <\/li>\n<li><b>False Positives:<\/b> Incorrectly Classified Fraud Transactions<\/li>\n<li> <b>True Negative:<\/b> Correctly Classified Non-Fraud Transactions<\/li>\n<li> <b>False Negative:<\/b> Incorrectly Classified Non-Fraud Transactions<\/li>\n<li><b>Precision: <\/b>  True Positives\/(True Positives + False Positives)  <\/li>\n<li><b> Recall: <\/b> True Positives\/(True Positives + False Negatives)   <\/li>\n<li> Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.<\/li>\n<li><b>Precision\/Recall Tradeoff: <\/b> The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect. <\/li>\n<li> <b>Precision starts to descend<\/b> between 0.90 and 0.92 nevertheless, our precision score is still pretty high and still we have a descent recall score. <\/li>\n<\/ul>","bf3dba70":"**Distributions:** \n<p>In Introduction, according to author, apart from the noise, the shape of the attributes, DISTRIBUTIONS can make an impact on the quality of data classification. It is a fact that most of the natural continuous random variables adhere some sort of Gaussian distribution. When the data show a departure from the normality, it is considered as skewed. Osborne [31] points out that mistake in data entry, missing data values, presence of outliers, and\nnature of the variable itself are some of the reasons for the skewness of the data.<\/p>\n<p>In section 6 \"Skewness of the Data and Classification Accuracy\", inside \"subsection  6.1 Skewness\", the author explains Skewness provides a measure about the symmetry of the distribution. This measurement can be either positive or negative, in either case, skewness makes the symmetric distribution asymmetric. Transformation of skewed data is done into symmetric.The following are some of the popular transformations. As Howell [17] suggested, if data are positively skewed sqrt(X) is used. If the skewness is moderate, then the log(X) can be used. Brown [5] stated the identification of skewness based on the Joanes and Gill [21] and Cramer [10] formulas.<\/p>\n<ul>\n    <li>I analysed the distributions and can see the skewness of these features. <\/li>\n    <li>I am further analysing the distributions of these other features as below<\/li>  \n <\/ul> ","75c3489d":"<h2> Scaling and Distributing to Make Nearly Balanced Data<\/h2>\n<a id=\"distributing\"><\/a>\nIn this phase, I will first scale the columns comprising of <b>Time<\/b> and <b>Amount <\/b>. Time and amount should be scaled as the other columns.On the other hand, I need to also create a sub-sample of the data-frame in order to have an equal amount of Fraud and Non-Fraud cases, helping my selected algorithms better understand patterns that determines whether a transaction is a fraud or not.\n\n<h3> What do you mean by sub-Sample?<\/h3>\nIn this scenario, my sub-sample will be a data-frame with a 50\/50 ratio of fraud and non-fraud transactions. My sub-sample will have the same amount of fraud and non-fraud transactions.\n\n<h3> Why do we create a sub-Sample?<\/h3>\nIn the beginning of our analysis of data we saw that the original data-frame was heavily imbalanced! Using the original data-frame  will cause the following issues:\n<ul>\n<li><b>Over-fitting: <\/b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. <\/li>\n<li><b>Wrong Correlations:<\/b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of these features can influence the result (Fraud or No Fraud).  If I have imbalanced data-frame, I cannot see the true correlations between the class and features. <\/li>\n<\/ul>\n\n<h3>Result Summary of Balanced Data: <\/h3> \n<ul>\n<li> <b>Scaled Amount <\/b> and <b> Scaled Time <\/b> are the columns with scaled values. <\/li>\n<li> There are <b>492 cases <\/b> of fraud in our data-set so we can randomly get 492 cases of non-fraud to create our new sub data-frame. <\/li>\n<li>We concatenated the 492 cases of fraud and non-fraud, <b>creating a new sub-sample. <\/b><\/li>\n<\/ul>","c6e78b95":"<ul>\n    <li>The classes are heavily skewed we need to solve this issue later.<\/li>    \n<li>The Data is imbalanced is the original data-set given as told by author on page number 209 of paper given. The author had balanced the data to get nearly balanced data-set containing 997 fraud instances and 983 non-fraud instances. Please note,we are not using same dataset, but a dataset which is near to the dataset used by the author <\/li>\n<li>I plan to take ORIGINAL NOT BALANCED data from .csv file and balance it  and compare my results with that mentioned by the author.<\/li>\n<li>Most of the transactions are non-fraud. If I use this data-frame as the base for my predictive models and analysis I might get a lot of errors and my algorithms will probably over-fit since it will \"assume\" that most transactions are not fraud.<\/li> \n<li>I don't want my model to assume most transactions are fraud, but I want my model to detect patterns that give signs of fraud!<\/li>    \n<\/ul>","619d2d6a":"### Splitting the Data (Original DataFrame)\n<a id=\"splitting\"><\/a>\nBefore proceeding with the <b> Random Under-Sampling technique<\/b> I  want to separate the original data-frame for testing.<b>\nAlthough I am splitting the data when implementing Random Under-Sampling or Over-Sampling techniques, I want to test my Models on the Original Testing Set and NOT on the testing set created by either of these techniques.<\/b> The main goal is to fit the Model either with the data-frames that were under-sample and over-sample (in order for my Models to detect the patterns), and test it on the original testing set. ","9a31527d":"### Conclusion: \n<ul>\n<li> <b>Classification Models: <\/b> The Model that performed is <b>logistic regression <\/b> and after that comes <b>support vector classifier (SVC)<\/b>  when compared to KNearest Neighbors and Decision Tree Classifier.<\/li>\n    <li>Based on the accuracy score, precision, recall and F-score the logistic regression classifier (94%) is found to detect more accurate credit card fraud as compared to Support Vector Classifier (93%) and Decision Tree Classifier (91%).<\/li>\n<li>The accuracy of results is verified using XGBoost algorithm using same dataset and it was found XGBoost be a more accurate classifier to detect Credit Card Fraud as compared to Logistic Regression <\/li>   \n    <li>XGBoost Algorithm can be easily used to find fraud and non-fraud transactions and the result count matches as that used by other methods like Logistics Regression and SVM as discussed later.<\/li>\n<li>It is easier to use than other algorithms like SVM, PCA, RPCA and Random forest discussed in paper\n<\/li>   \n<\/ul>\n","7a1800fa":"## Random Under-Sampling:\n\nIn this phase, I will implement *\"Random Under Sampling\"* which consists of removing data in order to have a more <b> balanced dataset <\/b> and thus avoiding my models to overfitting.\n\n#### Steps:\n<ul>\n<li>The first thing I have to do is determine how <b>imbalanced<\/b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  <\/li>\n<li>Once I determine the count of instances are considered <b>fraud transactions <\/b> (Fraud = \"1\") , I will bring the <b>non-fraud transactions<\/b> to the SAME COUNT as fraud transactions (assuming we want a 50\/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  <\/li>\n<li> After implementing this technique, I will have a sub-sample of the dataframe with a 50\/50 ratio with regards to our classes. Then the next step I will implement steps to <b>shuffle the data<\/b> to see if my models can maintain a certain accuracy everytime I run this script.<\/li>\n<\/ul>\n\n**Note:** The main issue with \"Random Under-Sampling\" is that there is a risk that the classification models selected will not perform as accurately as we would like to, as there is a great deal of <b>information loss<\/b> (bringing 492 non-fraud transaction  from 284,315 non-fraud transaction)","816dd128":"<h2 align=\"center\"> Credit Fraud Detector: Which Classification Technique gives accurate results?<\/h2>\n<h3 align=\"center\"> Comparing Logistics Regression vs K-Nearest vs Support Vector Classifier vs Decision Tree Classifier<\/h3>\n<h3 align=\"center\"> Verify the Accuracy Score: XGBoost Vs Logistics Regression<\/h3>\n<h2> Outline: <\/h2>\nI. <b>Understanding the data-set given<\/b><br>\na) [Gather Sense of Data-set Provided](#gather)<br><br>\n\nII. <b>Pre-Processing<\/b><br>\na) [Scaling and Distributing to Make Nearly Balanced Data](#distributing)<br>\nb) [Splitting the Data (Original DataFrame)](#splitting)<br><br>\n\nIII. <b>Random Under-Sampling and Oversampling<\/b><br>\na) [Equally Distributing and Correlating](#correlating)<br>\nb) [Accuracy Score: Logistic Regression vs KNearest vs Support Vector Classifier vs Decision Tree Classifier](#logistic)<br>\n\nIV. <b>Testing My Models<\/b><br>\na) [Comparative Results Logistic Regression Vs Support Vector Classifier](#testing_logistic)<br>\n<h2>Summary<\/h2>\n<p>This Research Report is based on a paper by Wickramasinghe, R. I. P. (2017). titled \"Attribute Noise, Classification Technique, and Classification Accuracy\". In Data Analytics and Decision Support for Cybersecurity (pp. 201-220). Springer, Cham.\nThe objective of this Research Report is to do comparative study of:\n1)\tClassification Techniques to detect credit card fraud.\n2)\tFurther comparing the results achieved using XgBoost with that of achieved by Logistic Regression.\nThis report does a comparative study of classification techniques like Logistics Regression, Support Vector Classifier, K-nearest neighbour(kNN), Decision Tree Classifier and proves Logistics Regression is the most accurate when it comes to detecting fraud in credit card data.\nThe data set taken is unbalanced with most of the transactions being non fraud and the remaining (0.17%) being fraudulent.\nThe data set has been balanced using scaling and distribution (scaled amount and scaled time values) creating 492 cases of fraud and 492 cases of non-fraud giving a 50:50 sub-data-frame ratio.\nNext random under-sampling was used to have a more balanced dataset by equally distributing and corelating sub sample data set.\nBased on the accuracy score, precision, recall and F-score the logistic regression classifier (94%) is found to detect more accurate credit card fraud as compared to Support Vector Classifier (93%) and Decision Tree Classifier (91%). \nThe results were verified using XGBoost algorithm using same data set and it is found XGBoost be a more accurate classifier as compared to Logistic Regression.<\/p>\n\n<h2>Introduction <\/h2>\nAustralia\u2019s love of online shopping and the move towards an increasingly cashless society have seen an explosion in credit card fraud.\nAnalysis by consumer comparison website finder.com.au found \u2018card-not-present\u2019 fraud rose a staggering 76 per cent in the 12 months to June 30, 2018, to 1.8 million dodgy transactions.\n<ul>\n  <li> In this report, I have done comparative study of classification techniques to detect credit card fraud discussed by the author by R. Indika P. Wickramasinghe in the paper titled \"Attribute Noise, Classification Technique,and Classification Accuracy\"\n  <\/li>\n  <li> \n    I have done my own analysis using the above data-set Classification Techniques like Logistics Regression, Support Vector Classifier, K-nearest neighbour(kNN), Decision Tree Classifier and prove Logistics Regression has most accuracy when it comes to detecting fraud in credit card data.\n  <\/li>\n  <li>The author has discussed the Support Vector Machine (SVM) one of the most popular classification techniques. I have used the model Support Vector Classifier that implements SVM.\n  <\/li>\n  <li> In the paper given, on page number 203 in section \"2. Related Work\", the author compares the accuracy of algorithms comparing Logistics Regression,SVM,K-nearest neighbour, Naive-Bayes,Classification and Regression Trees.  <\/li>\n  <li>In this kernel I have used Logistics Regression classifier, K-Nearest  and compare against Support Vector Classifier and Decision Tree Classifier to find out, and conclude with algorithm that can accurately detect if a transaction is a normal payment or a fraud.\n  <\/li>\n    <li> Next I Compared the Accuracy of Results Achieved using XGBoost (See for more details  https:\/\/www.kaggle.com\/maqsaid\/detect-credit-card-fraud-using-xgboost) and found to be 0.9994850368081645 and better than that can be achieved by using Logistic Regression (0.94). More details in [Accuracy Score: Logistic Regression vs KNearest vs Support Vector Classifier vs Decision Tree Classifier](#logistic)  \n<\/ul>\n<h2> My Goals: <\/h2>\n<ul>\n<li> Perform the Analysis of the current Credit Card Data. <\/li>\n<li> Understand the distribution of the data. <\/li>\n<li> Create a 50\/50 sub-data-frame ratio of \"Fraud\" and \"Non-Fraud\" transactions using Near-Miss Algorithm.<\/li>\n<li> Use Logistics Regression Classifier to decide its accuracy.<\/li>\n<li> Compare Logistics Regression Classifier with other algorithms as mention above.<\/li>    \n<\/ul>\n\n"}}