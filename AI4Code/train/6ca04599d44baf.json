{"cell_type":{"7481d65f":"code","aa8f8c29":"code","b59ae8cc":"code","74ac73bc":"code","72e3ebfe":"code","4ebb2ea5":"code","a228294e":"code","9f1cebd8":"code","be9f092b":"code","8b9ee43b":"code","84380a0e":"code","22e889ad":"code","40b79357":"code","994147d0":"code","95d5ff8f":"code","b91ff9a8":"markdown","de551f8c":"markdown","0c6aa41a":"markdown","dff443a3":"markdown","99426680":"markdown","fac16097":"markdown","5dea78f0":"markdown","707d6a40":"markdown","36139bc5":"markdown","c3644b41":"markdown","e2c10a3b":"markdown","218eb62c":"markdown"},"source":{"7481d65f":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.style as style\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\nfrom scipy import stats \nfrom scipy.stats import norm, skew  # norm\u7528\u6765\u751f\u6210\u6b63\u6001\u5206\u5e03\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) \n","aa8f8c29":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"\\nThe train data size is : {} \".format(train.shape)) \nprint(\"The test data size is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))\n\n\ndef customized_num_scatterplot(y, x, title=None):\n    style.use('fivethirtyeight')  # \u597d\u770b\u7684\u753b\u98ce\n    plt.subplots(figsize=(12, 8))  # (\u5bbd, \u9ad8)\n    ax = sns.scatterplot(y=y, x=x)\n    ax.set_title(title)\n\ncustomized_num_scatterplot(train.SalePrice, train.GrLivArea, title='Before Deleting outliers')\n\n# \u5220\u9664\u79bb\u7fa4\u70b9,\u5c45\u4f4f\u9762\u79ef\u592a\u5927\u5e76\u4e14\u623f\u4ef7\u592a\u4fbf\u5b9c\u7684\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n\ncustomized_num_scatterplot(train.SalePrice, train.GrLivArea, title='After Deleting outliers')","b59ae8cc":"style.use('fivethirtyeight')\nfig, axes = plt.subplots(3, 1, figsize=(12, 24))\n# 1.\u627e\u51fa\u6700\u63a5\u8fd1\u7684norm\u5206\u5e03\u66f2\u7ebf\nax0 = sns.distplot(train['SalePrice'] , fit=norm, ax=axes[0]);  \nax0.set_title('SalePrice before normalized')\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\n\n# 2.\u7528QQ\u56fe\u5224\u65ad\u6570\u636e\u662f\u5426\u4e3a\u6b63\u6001\u5206\u5e03,\u84dd\u70b9\u548c\u7ea2\u7ebf\u8d8a\u91cd\u5408\u5c31\u8d8a\u7b26\u5408\u6b63\u6001\u5206\u5e03\nstats.probplot(train['SalePrice'], plot=axes[1])\n\n# \u5bf9\u623f\u4ef7\u53d6log\u8ba9\u5b83\u8d8b\u8fd1\u4e8e\u6b63\u6001\u5206\u5e03\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# 3.\u53d8\u6362\u540e\u7684\u623f\u4ef7\u66f2\u7ebf\nax2 = sns.distplot(train['SalePrice'] , fit=norm, ax=axes[2]);\nax2.set_title('SalePrice after normalized')\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')","74ac73bc":"# \u5148\u5408\u5e76\u627e\u51fa\u7f3a\u5931\u6bd4\u4f8b\u6700\u591a\u7684\u524d20\u540d\nn_train = train.shape[0]\nn_test = test.shape[0]\ny_train = train.SalePrice.values  # pd->np,\u628alog\u540e\u7684y\u62bd\u51fa\u6765\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop('SalePrice', axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nprint('\u603b\u5171\u7684\u7f3a\u5931\u5c5e\u6027\u6570: ', all_data_na.shape[0], '\u4e2a')\n# drop()\u62ec\u53f7\u91cc\u8981\u8ddfindex,\n# print(all_data_na)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data)\n\nfig, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='45')  # \u7279\u5f81\u6587\u5b57\u7684\u503e\u659c\u89d2\u5ea6\nax.set_facecolor('white')  # \u56fe\u4e2d\u7684\u80cc\u666f\u8272\nsns.barplot(x=all_data_na.index, y=all_data_na)\nsns.color_palette('rocket', as_cmap=True)  # cmap:\u6570\u5b57\u5230\u989c\u8272\u7684\u6620\u5c04\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","72e3ebfe":"corrmat = train.corr()  # \u53ea\u6709\u6570\u5b57\u53d8\u91cf\u80fd\u6c42\u5173\u8054\u6027,\u4e00\u517137\u4e2a\n# \u753b\u70ed\u529b\u56fe\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, cmap=\"YlGnBu\", vmax=0.9, square=True)\n","4ebb2ea5":"# \u4e00\u517134\u4e2a\u6709\u7f3a\u5931\u503c\u7684\u5c5e\u6027\n# 1.\u90a3\u79cd\u7f3a\u5931\u5c31\u4ee3\u8868\u6ca1\u6709\u7684\u53d8\u91cf,\u628aNA\u4ee3\u8868\u6ca1\u6709\u7684\u66ff\u6362\u6210None\n# \u7b2c\u4e00\u79cd\u586bNone\u7684(15\u4e2a)\nfill_None = [\"FireplaceQu\", \"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \n             'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', \n             'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"MasVnrType\"]\n\nfor col in (fill_None):\n    all_data[col] = all_data[col].fillna('None')\n\n# 2.\u6309\u6876\u5206\u7c7b\u7528\u4e2d\u4f4d\u6570\u4ee3\u66ff\u7f3a\u5931\u503c(1\u4e2a)\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n\n\n# 3.\u586b0(10\u4e2a)\nfill_zero = ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', \n             'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\"MasVnrArea\"]\n\nfor col in (fill_zero):\n    all_data[col] = all_data[col].fillna(0)\n\n\n# 4.\u586b\u4f17\u6570(6\u4e2a)\nfill_mode = ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', \n            'SaleType']\n\nfor col in (fill_mode):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n\n    \n# 5.\u76f4\u63a5\u4e22\u6389\u7279\u5f81(1\u4e2a)\nall_data = all_data.drop(['Utilities'], axis=1)\n\n# 6.\u586b\u5165\u6307\u5b9a\u503c(1\u4e2a)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n","a228294e":"# \u770b\u4e0b\u8fd8\u6709\u6ca1\u6709\u7f3a\u5931\u503c\u4e86\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data)","9f1cebd8":"# \u6237\u578b\u4ece\u6570\u5b57\u53d8\u6210\u79cd\u7c7b\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n# \u603b\u4f53\u60c5\u51b5\u6709\u4e2a\u6253\u5206\u4e5f\u4ece\u6570\u5b57\u53d8\u6210\u79cd\u7c7b\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n# \u552e\u51fa\u5e74\u6708\u4e5f\u6539\u6210\u7c7b\u522b\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n\n\n# \u628an\u4e2a\u79cd\u7c7b\u6807\u7b7e\u5316\u6210(0~n-1)\u7684\u6570\u5b57\n# \u4e0eone-hot\u4e0d\u540c,one-hot\u4f1a\u628a\u6027\u522b(\u7537or\u5973)\u62c6\u6210\u4e24\u4e2a\u53d8\u91cf,Label\u4f1a\u628a\u7537=0,\u5973=1\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', \n        'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure',\n        'GarageFinish', 'LandSlope','LotShape', 'PavedDrive', 'Street',\n        'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold',\n        'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n\n# \u52a0\u4e00\u4e2a\u65b0\u7279\u5f81,\u603b\u9762\u79ef\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n","be9f092b":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\u6570\u503c\u7279\u5f81\u7684\u504f\u5ea6:\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness","8b9ee43b":"# \u4fee\u6b63\u504f\u659c\u7684\u7279\u5f81\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"\u4e00\u5171\u6709{}\u4e2a\u504f\u659c\u7279\u5f81\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15  # \u4f30\u8ba1\u51fa\u6765\u7684\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    ","84380a0e":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)  # (\u6269\u5c55\u6210220\u4e2a\u7279\u5f81)\n\ntrain = all_data[:n_train]\ntest = all_data[n_train:]\n\n# \u5b58\u4e00\u4e0b\u5904\u7406\u8fc7\u7684\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\nidx_all = pd.DataFrame({'Id': all_data.index+1})\nall_id = pd.concat((idx_all, all_data), axis=1)  # \u5e26Id\u7684\u5168\u6570\u636e\u96c6\n\nprocessed_train = pd.concat((all_id[:n_train], pd.DataFrame({'SalePrice': y_train})), axis=1)\nprocessed_test = all_id[n_train:]\n\nprocessed_train.to_csv('processed_train.csv', index=False)\nprocessed_test.to_csv('processed_test.csv', index=False)","22e889ad":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n# \u4e0a\u9762\u8fd9\u4e00\u6761\u5c31\u662f\u4e3a\u4e86\u6a21\u578b\u878d\u5408\u7684\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):  # \u8ba1\u7b97\u8bad\u7ec3\u96c6\u7684\u8bef\u5dee(rmse)\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n","40b79357":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","994147d0":"# 1.\u5bf9\u591a\u4e2a\u6a21\u578b\u7684\u8f93\u51fa\u53d6\u5e73\u5747\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # clone\u5c31\u662fdeepcopy,fit\u7528\u6765\u62df\u5408\u4e0d\u540c\u7684\u6570\u636e\u96c6\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        for model in self.models_:\n            model.fit(X, y)\n\n        return self  # \u8fd4\u56de\u6a21\u578b\u672c\u8eab,\u65b9\u4fbf\u94fe\u5f0f\u8c03\u7528,Ex:model.fit().predict()\n    \n    # \u5bf9\u591a\u4e2a\u6a21\u578b\u8f93\u51fa\u7684\u7ed3\u679c\u53d6\u5e73\u5747\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n    \naveraged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n# 0.1087 (0.0077)\n","95d5ff8f":"# 2.\u628a\u591a\u4e2a\u6a21\u578b\u7684\u8f93\u51fa\u5582\u5165\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models  # \u591a\u4e2a\u6a21\u578b\n        self.meta_model = meta_model  # \u6700\u540e\u7684\u8f93\u51fa\u5c42\n        self.n_folds = n_folds\n   \n    # \u62df\u5408\u8bad\u7ec3base_models\u548cmeta_model\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]  # [[], [], []]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # \u5b9a\u4e49base_models\u7684\u8f93\u51fa,\u5582\u5165meta_model, n_train\u884c,3\u5217\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):  # \u4eba\u4eba\u90fd\u8f6e\u5f97\u5230\u5f53\u6d4b\u8bd5\u96c6\n                instance = clone(model)\n                self.base_models_[i].append(instance)  \n                # \u6bcf\u4e2amodel\u90fd\u5b58\u6709\u4e0d\u540c\u8bad\u7ec3\u96c6\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # \u6700\u540e\u7528\u7ebf\u6027\u6a21\u578b\u62df\u5408\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    # \u6bcf\u4e2a\u6a21\u578b\u91cc\u90fd\u6709\u53c2\u6570\u4e0d\u540c\u7684\u6a21\u578b,\u4ed6\u4eec\u5171\u540c\u51b3\u7b56\u7136\u540e\u53d6\u5e73\u5747\u6c42\u51fafeatures,\u518d\u628a\u7279\u5f81\u5582\u7ed9meta_model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        # \u53f3\u8fb9\u7684for\u5faa\u73af\u91ccbase_models\u4ee3\u8868\u6a21\u578b\u7684\u79cd\u7c7b(GB, KRR, ENet)\n        # \u5de6\u8fb9\u7684for\u5faa\u73af\u91ccmodel\u4ee3\u8868\u540c\u4e00\u79cd\u7c7b\u7684\u6a21\u578b\u4e0d\u540c\u7684\u53c2\u6570\n        # \u751f\u6210\u7684meta_features\u5c31\u662f\u4e00\u5217\u5411\u91cf\n        return self.meta_model_.predict(meta_features)\n    \nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# 0.1081 (0.0073)\n\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nstacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))\n\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))\n\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))\n\n'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))\n\nensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)\nprint('\u9884\u6d4b\u7ed3\u679c\u8f93\u51fa\u5b8c\u6bd5')","b91ff9a8":"# \u6709\u4e24\u79cd\u5806\u53e0\u6a21\u578b\u7684\u65b9\u6cd5\n## 1.\u5bf9\u591a\u4e2a\u6a21\u578b\u53d6\u5e73\u5747\n## 2.\u5c06\u591a\u4e2a\u6a21\u578b\u7684\u8f93\u51fa\u63a5\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52","de551f8c":"# \u586b\u5145\u7f3a\u5931\u503c","0c6aa41a":"\u56de\u5f52\u9884\u6d4b\u4efb\u52a1, \u5148\u62ff\u4e09\u4e2a\u6a21\u578b\u8f93\u51fa,\u518d\u63a5\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52,\u518d\u5c06\u878d\u5408\u6a21\u578b\u4e0e\u53e6\u5916\u4e24\u4e2a\u6a21\u578b\u52a0\u6743\u5e73\u5747.","dff443a3":"# \u52a0\u8f7d\u6570\u636e&\u6570\u636e\u9884\u5904\u7406","99426680":"# \u524d\u9762\u79cd\u7c7b\u5c11\u7684Label\u4e86,\u73b0\u5728\u89e3\u51b3\u79cd\u7c7b\u591a\u7684One-hot","fac16097":"# \u5bfc\u5165\u5305","5dea78f0":"# \u7279\u5f81\u5de5\u7a0b\n### \u6bd4\u8f83\u5c11\u7684\u7c7b\u578b\u53d8\u91cf\u53d8\u6210label(\u4e00\u5217\u641e\u5b9a)\n### \u6bd4\u8f83\u591a\u7684\u7c7b\u578b\u53d8\u6210\u5c55\u5f00\u6210one-hot(\u53d8\u6210\u5f88\u591a\u5217)","707d6a40":"# \u591a\u79cd\u6a21\u578b\u7684\u7ec4\u5408\n### \u5305\u62ecLASSO Regression, Elastic Net Regression, Kernel Ridge Regression, Gradient Boosting Regression, XGBoost, and LightGBM.","36139bc5":"\u53ef\u4ee5\u770b\u5230\u53f3\u4e0b\u89d2\u4e24\u4e2a\u79bb\u8c31\u7684\u70b9\u5df2\u7ecf\u88ab\u5220\u6389\u4e86","c3644b41":"# \u6570\u636e\u7684\u5173\u8054\u6027\u5206\u6790","e2c10a3b":"# \u7279\u5f81\u5de5\u7a0b","218eb62c":"# \u770b\u4e00\u4e0b\u6570\u503c\u7279\u5f81\u7684\u504f\u5ea6"}}