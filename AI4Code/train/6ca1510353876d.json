{"cell_type":{"b4776837":"code","2e789699":"code","afa084d1":"code","d7e3bf52":"code","621dc4cb":"code","cd6a020c":"code","860c945a":"code","70e69a3e":"code","84b6e0f2":"code","02335c40":"code","bf796a4e":"code","367caa9f":"code","be878a6f":"code","78980aa3":"code","620cba10":"code","054e53cb":"code","e537a949":"code","0e33b76a":"code","44b4bfa9":"code","e286318d":"code","c7ee5fef":"code","4d2bf150":"code","8be5316a":"code","d4da060a":"code","49079678":"code","40184398":"code","458ab3a6":"code","b5ebdf57":"code","5098a8fe":"code","29b82381":"code","7fb08264":"code","ca984970":"code","49ba41b4":"code","d98b4051":"code","2ab05be5":"code","17c9d427":"code","d0f7bed3":"code","cbb4a7d0":"code","69cb8082":"code","7b3b828c":"code","e0f9cf16":"code","8ed23ed4":"code","daeb3b53":"code","5706ce81":"code","0c753aea":"code","047593e6":"code","d2d67ae3":"code","e3a25afc":"code","41d9c853":"code","4ffebb42":"code","976e3445":"code","fa7adc13":"code","0810c20b":"code","6a5e63b2":"code","b6f2a8db":"code","c22c5c80":"code","579b5aeb":"code","a8288cd7":"code","51e13aa1":"code","4ea667af":"code","1259abb5":"code","8d9195ab":"code","8f333782":"code","3d8be7f6":"code","b341f1f3":"code","8999bd12":"code","c0e8dc6d":"code","d8321e91":"code","6552c3ab":"code","ec81b2cc":"code","0a603e32":"code","d15d2096":"code","f8a35c8c":"code","12b7edff":"code","97179782":"code","3edfc7c8":"code","6fd760bf":"code","6c23aacc":"code","512c28c6":"code","82f0eca6":"code","327d0bd0":"code","0c8a2ce4":"code","2688939f":"code","2db943fb":"code","5ac94b6e":"code","dc0839fa":"code","cd5fac51":"code","c5477be0":"code","1844ef27":"code","51fa21fe":"code","3e2a12e6":"code","3bd6056c":"code","a4bb22bf":"code","8090614e":"code","520d1751":"code","b8a03d89":"code","1d4612ab":"code","45b51b9f":"code","5e5dc707":"code","15fdf51d":"code","6943875c":"code","f4677598":"code","ddafc7d9":"code","2cc8a6b1":"code","e0c7d546":"code","4c19702d":"code","f27d4eb2":"markdown","213fa9c6":"markdown","57e7c60c":"markdown","50411a9d":"markdown","8a1bb8b1":"markdown","a38668fd":"markdown","46fa2929":"markdown","47770de5":"markdown","27ba2baf":"markdown","2fa0d780":"markdown","dc4ecd7d":"markdown","dfab74f5":"markdown","f43bea2a":"markdown","de35d081":"markdown","5ed1d3a6":"markdown","17e6d5f6":"markdown","5cca279f":"markdown","42fcd428":"markdown","eb7bef84":"markdown","cded5393":"markdown","2364b0a1":"markdown","075d818b":"markdown","36e72db1":"markdown","8e7e092d":"markdown","344fad28":"markdown","c0ca83a9":"markdown","8a95cc18":"markdown","ae92d915":"markdown","126138a4":"markdown","187213b3":"markdown","761bd3b3":"markdown","c4731ee1":"markdown","d2c7e059":"markdown","00b0ccbe":"markdown","aec2abbc":"markdown","19c9f159":"markdown","069a10fe":"markdown","1b0e4996":"markdown","52c13d40":"markdown","1d5f5d95":"markdown","bfbb93ad":"markdown","e0207d4d":"markdown","542b7c48":"markdown","a66bc0ea":"markdown","908bd9c2":"markdown","2dca016a":"markdown","de6578c5":"markdown","322717d6":"markdown","c887a0a3":"markdown","b9758b71":"markdown","b0b62782":"markdown","6c6a7254":"markdown","d08bb03d":"markdown","13e894e9":"markdown","f6cb90bd":"markdown","a547b2d8":"markdown","e6a3b8cb":"markdown","79e141fb":"markdown","26b1f2be":"markdown","d966f201":"markdown","19d5ece6":"markdown","7c5d33c0":"markdown","b262de1a":"markdown","df9579c1":"markdown","53e544f5":"markdown","b8c25e00":"markdown","01e54266":"markdown","b0a8ffbd":"markdown","830884c1":"markdown","279628d8":"markdown","14060483":"markdown","9984bc85":"markdown","c1ec0ab0":"markdown","e14ae1e9":"markdown","b4f798b2":"markdown","5880d965":"markdown","8e48792f":"markdown","542bebb7":"markdown","f07c06dc":"markdown","f685260e":"markdown","ae35ddb9":"markdown","cfb9d480":"markdown","0f21a9e4":"markdown","cba87432":"markdown","1c41da78":"markdown","8c882a0c":"markdown","02340586":"markdown","abcf56e8":"markdown","f555caa3":"markdown","41cb9b0d":"markdown","89819d3a":"markdown","6bc1c573":"markdown","87067bbb":"markdown","5c2e4974":"markdown","ba1ebeda":"markdown","edecd558":"markdown","c68049a7":"markdown","52044622":"markdown","566e099f":"markdown","7985296b":"markdown","00745b4e":"markdown","35a81699":"markdown","cdf7035b":"markdown","ec0f973d":"markdown","3610098d":"markdown","49a4eedd":"markdown","bd032af4":"markdown","636d62fc":"markdown","9e77b1fb":"markdown","e70707af":"markdown","7733a39d":"markdown","d216568a":"markdown"},"source":{"b4776837":"# importing libraries\n\n# preprocessing and validation libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn \nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler,LabelEncoder as le\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV, cross_val_score, KFold, learning_curve\n\n# model related libraries\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor\nfrom xgboost import XGBRegressor \nfrom sklearn.tree import DecisionTreeRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_squared_log_error, r2_score, mean_squared_error\nimport statsmodels.api as sm  # for p values analysis\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","2e789699":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntrain_data.head(5) # printing some of them","afa084d1":"# test data info\ntest_data.head()","d7e3bf52":"# data exploration\nprint('no.of examples in train data : ', len(train_data))\nprint('no.of examples in test data : ', len(test_data))\nprint('no.of features in data : ', train_data.shape[1]-1)\n\n","621dc4cb":"# Firstly remove the SalePrice Column in train data in order to merge train and test\n\n# take it as y_train\ny_train = train_data['SalePrice']\n\n# X_train\nX_train = train_data.drop('SalePrice',axis=1)\n\n# concatenate the test data an train data to apply same operations\ndataset = pd.concat(objs = [X_train,test_data],axis=0,sort=False).reset_index(drop=True)\nprint(len(dataset))\ndataset.head(5)","cd6a020c":"# finding the no. of null values in both train and test\ndataset_null_val = dataset.isnull().sum()\n\n# finding those columns which contains null values\nprint('For Dataset \\n')\nprint('{} no of features in  dataset contain missing values \\n'.format(len(dataset_null_val.values[dataset_null_val.values !=0])))\nprint('column names with null values {}\\n'.format(dataset.columns[dataset_null_val.values !=0]))      \n","860c945a":"#Percentage of NAN Values \nNAN = [(c, dataset[c].isna().mean()*100) for c in dataset]\nNAN = pd.DataFrame(NAN, columns=[\"column_name\", \"percentage\"])\nNAN","70e69a3e":"NAN = NAN[NAN.percentage > 50]\nNAN.sort_values(\"percentage\", ascending=False)","84b6e0f2":"#Drop PoolQC, MiscFeature, Alley and Fence features\ndataset = dataset.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1)\ndataset.shape[1]","02335c40":"object_columns_df = dataset.select_dtypes(include=['object'])\nnumerical_columns_df =dataset.select_dtypes(exclude=['object'])\n","bf796a4e":"object_columns_df.dtypes","367caa9f":"numerical_columns_df.dtypes","be878a6f":"#Number of null values in each feature\nnull_counts = object_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))","78980aa3":"columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\nobject_columns_df[columns_None]= object_columns_df[columns_None].fillna('None')","620cba10":"columns_with_lowNA = ['MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType','Electrical','KitchenQual','Functional','SaleType']\n#fill missing values for each column (using its own most frequent value)\nobject_columns_df[columns_with_lowNA] = object_columns_df[columns_with_lowNA].fillna(object_columns_df.mode().iloc[0])","054e53cb":"#Number of null values in each feature\nnull_counts = numerical_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))","e537a949":"print((numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt']).median())  # median age of a house\nprint(numerical_columns_df[\"LotFrontage\"].median())","0e33b76a":"numerical_columns_df['GarageYrBlt'] = numerical_columns_df['GarageYrBlt'].fillna(numerical_columns_df['YrSold']-35)\nnumerical_columns_df['LotFrontage'] = numerical_columns_df['LotFrontage'].fillna(68)\n","44b4bfa9":"numerical_columns_df= numerical_columns_df.fillna(0)","e286318d":"object_columns_df['Utilities'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Utilities'].value_counts() ","c7ee5fef":"object_columns_df['Street'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Street'].value_counts() ","4d2bf150":"object_columns_df['Condition2'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Condition2'].value_counts() ","8be5316a":"object_columns_df['RoofMatl'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['RoofMatl'].value_counts() ","d4da060a":"object_columns_df['Heating'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Heating'].value_counts() #======> Drop feature one Type","49079678":"object_columns_df = object_columns_df.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)","40184398":"numerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()","458ab3a6":"Negatif = numerical_columns_df[numerical_columns_df['Age_House'] < 0]\nNegatif","b5ebdf57":"numerical_columns_df.loc[numerical_columns_df['YrSold'] < numerical_columns_df['YearBuilt'],'YrSold' ] = 2009\nnumerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()","5098a8fe":"numerical_columns_df.head()","29b82381":"bin_map  = {'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,'IR3':0,\"None\" : 0,\n            \"No\" : 2, \"Mn\" : 2, \"Av\": 3,\"Gd\" : 4,\"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6\n            }\nobject_columns_df['ExterQual'] = object_columns_df['ExterQual'].map(bin_map)\nobject_columns_df['ExterCond'] = object_columns_df['ExterCond'].map(bin_map)\nobject_columns_df['BsmtCond'] = object_columns_df['BsmtCond'].map(bin_map)\nobject_columns_df['BsmtQual'] = object_columns_df['BsmtQual'].map(bin_map)\nobject_columns_df['HeatingQC'] = object_columns_df['HeatingQC'].map(bin_map)\nobject_columns_df['KitchenQual'] = object_columns_df['KitchenQual'].map(bin_map)\nobject_columns_df['FireplaceQu'] = object_columns_df['FireplaceQu'].map(bin_map)\nobject_columns_df['GarageQual'] = object_columns_df['GarageQual'].map(bin_map)\nobject_columns_df['GarageCond'] = object_columns_df['GarageCond'].map(bin_map)\nobject_columns_df['CentralAir'] = object_columns_df['CentralAir'].map(bin_map)\nobject_columns_df['LotShape'] = object_columns_df['LotShape'].map(bin_map)\nobject_columns_df['BsmtExposure'] = object_columns_df['BsmtExposure'].map(bin_map)\nobject_columns_df['BsmtFinType1'] = object_columns_df['BsmtFinType1'].map(bin_map)\nobject_columns_df['BsmtFinType2'] = object_columns_df['BsmtFinType2'].map(bin_map)\n\nPavedDrive =   {\"N\" : 0, \"P\" : 1, \"Y\" : 2}\nobject_columns_df['PavedDrive'] = object_columns_df['PavedDrive'].map(PavedDrive)","7fb08264":"#Select categorical features\nrest_object_columns = object_columns_df.select_dtypes(include=['object'])\n#Using One hot encoder\nobject_columns_df = pd.get_dummies(object_columns_df, columns=rest_object_columns.columns) ","ca984970":"object_columns_df.head()","49ba41b4":"numerical_columns_df.describe()","d98b4051":"# finding min values\nnumerical_columns_df.describe().iloc[3,:]","2ab05be5":"# finding max values\nnumerical_columns_df.describe().iloc[7,:]","17c9d427":"# we will use robust scaler of sklearn as it is less prone to outliers in data\n\nrs = RobustScaler()\nscaled_features = rs.fit_transform(numerical_columns_df.iloc[:,1:])\n\nnumerical_columns_df.iloc[:,1:] = scaled_features","d0f7bed3":"numerical_columns_df.describe()","cbb4a7d0":"# final conactenate of both features\ndf_final = pd.concat([object_columns_df, numerical_columns_df], axis=1,sort=False)\ndf_final.head()","69cb8082":"# seprating train and test data features\n# firstly drop id\ndf_final = df_final.drop('Id',axis=1)\n\n# train features\nx_train = df_final.iloc[0:len(train_data),:]\n# test feature\nx_test = df_final.iloc[len(train_data):,:]\nlen(x_train)","7b3b828c":"## firstly let's plot the correlation of each individual on SalePrice \n\nplt.figure(figsize = (16,16))\n\n# making heatmap of correlation\nsns.heatmap(pd.concat([x_train.iloc[:,:30],y_train]).corr(),annot = True, fmt = '.2f')","e0f9cf16":"# checking importance using f-static\nimport statsmodels.api as sm\nX2 = sm.add_constant(x_train)\nregressor_OLS = sm.OLS(endog = y_train, exog = X2).fit()\nregressor_OLS.summary()","8ed23ed4":"#  applying PCA for all features\n# identifying which features adds how much variance to data\n\ncovar_matrix = PCA(n_components = df_final.shape[1]) #we have 8 numerical features\ncovar_matrix.fit(StandardScaler().fit_transform(df_final))\nvariance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n\nvar=np.cumsum(np.round(variance, decimals=3)*100)\n#components = covar_matrix.fit_transform(dataset[['Answers','Reputation','Tag','Username','Views']])\nplt.plot(var)#cumulative sum of variance explained with [n] features\nplt.title('Variance percenatage vs no. of features')\nvar","daeb3b53":"#plotting the distribution of sale price\ng = sns.boxplot(train_data['SalePrice'])\n","5706ce81":"# looks like the distribution of price is skewed \nf = sns.violinplot(train_data['SalePrice'], color = 'r')","0c753aea":"# using k folds cross validation with 5 splits\nkfold = KFold( n_splits = 5)\n","047593e6":"# import from sklearn\nlinear_reg = LinearRegression()\nMSLE = cross_val_score(linear_reg,x_train, y = y_train, scoring = sklearn.metrics.make_scorer(mean_squared_log_error), cv = kfold , n_jobs =-1)\nmean_squared_log_err = np.mean(np.sqrt(MSLE))\nprint('mean squared log error is : ',mean_squared_log_err)","d2d67ae3":"import math\n# now fitting the model on entire data for final predictions\nlinear_reg.fit(x_train,y_train)\n\n# let's find out the error on whole train data to see is there any overfitting of model or not\nmath.sqrt(mean_squared_log_error(y_train,linear_reg.predict(x_train)))","e3a25afc":"print('R-squared value for simple regression on train data is ', r2_score(y_train,linear_reg.predict(x_train)))","41d9c853":"plt.figure(figsize = (15,50))\nft_importances_lr  = pd.Series(linear_reg.coef_ , index = x_train.columns)\nft_importances_lr.plot(kind = 'barh')\n","4ffebb42":"linear_reg_pred = linear_reg.predict(x_test)","976e3445":"ridge = Ridge()\n\n# ridge has a hyperparameter lambda \nparameters = {'alpha' : [1,2]}  # possible values to search\n\n# applying GridSearch cv\n\nrid = GridSearchCV(ridge,param_grid = parameters, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_log_error), n_jobs= -1, verbose = 1)\n\nrid.fit(x_train,y_train) # fitting model\n\n\nprint(math.sqrt(rid.best_score_))\nprint(rid.best_params_)","fa7adc13":"rid_best = rid.best_estimator_  # Getting best estimator based on cv","0810c20b":"print('R-squared value for ridge regression on train data is ', r2_score(y_train,rid_best.predict(x_train)))","6a5e63b2":"plt.figure(figsize = (15,50))\nft_importances_rid  = pd.Series(rid_best.coef_ , index = x_train.columns)\nft_importances_rid.plot(kind = 'barh')\n","b6f2a8db":"ridge_pred = rid_best.predict(x_test)","c22c5c80":"lasso = Lasso()\n\n# lasso also has a hyperparameter lambda \nparameters = {'alpha' : [0.1,1,2]}  # possible values to search\n\n# applying GridSearch cv\n\nlas = GridSearchCV(lasso,param_grid = parameters, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_log_error), n_jobs= -1, verbose = 1)\n\nlas.fit(x_train,y_train) # fitting model\n\n\nprint(math.sqrt(las.best_score_))\nprint(las.best_params_)","579b5aeb":"las_best = las.best_estimator_  # Getting best estimator based on cv","a8288cd7":"print('R-squared value for lasso regression on train data is ', r2_score(y_train,las_best.predict(x_train)))","51e13aa1":"plt.figure(figsize = (15,50))\nft_importances_las  = pd.Series(las_best.coef_ , index = x_train.columns)\nft_importances_las.plot(kind = 'barh')\n","4ea667af":"# predicting on test data\nlasso_pred = las_best.predict(x_test)","1259abb5":"elastic = ElasticNet()\n\n# lasso also has a hyperparameter lambda \nparameters = {'alpha' : [ .5],\n               'l1_ratio' : [1e-5,0.1,1]}  # possible values to search\n\n# applying GridSearch cv\n\nelas = GridSearchCV(elastic,param_grid = parameters, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_error), n_jobs= -1, verbose = 1)\n\nelas.fit(x_train,y_train) # fitting model\n\n\nprint(math.sqrt(elas.best_score_))\nprint(elas.best_params_)","8d9195ab":"elas_best = elas.best_estimator_  # Getting best estimator based on cv","8f333782":"print('R-squared value for elasticnet regression on train data is ', r2_score(y_train,elas_best.predict(x_train)))","3d8be7f6":"# plotting feature representation for elasticnet\nplt.figure(figsize = (15,50))\nft_importances_elas  = pd.Series(elas_best.coef_ , index = x_train.columns)\nft_importances_elas.plot(kind = 'barh')\n","b341f1f3":"# predicting on test data\nelastic_pred = elas_best.predict(x_test)","8999bd12":"### SVR classifier\nSVMC = SVR(kernel = 'rbf')\nsvc_param_grid = { \n                  'gamma': [500],\n                  'C': [500,1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_log_error), n_jobs= -1, verbose = 1)\n\ngsSVMC.fit(x_train,y_train)\n\n\n\n# Best score\nprint(math.sqrt(gsSVMC.best_score_))\nprint(gsSVMC.best_params_)","c0e8dc6d":"svm_best = gsSVMC.best_estimator_  # Getting best estimator based on cv","d8321e91":"print('R-squared value for Supprt vector regression on train data is ', r2_score(y_train,svm_best.predict(x_train)))","6552c3ab":"# predicting on test data\nsvm_pred = svm_best.predict(x_test)","ec81b2cc":"### SVR classifier\nSVML = SVR(kernel = 'linear')\nsvc_param_grid = { \n                  'gamma': [0.001],\n                  'C': [10]}\n\ngsSVML = GridSearchCV(SVML,param_grid = svc_param_grid, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_log_error), n_jobs= -1, verbose = 1)\n\ngsSVML.fit(x_train,y_train)\n\n\n\n# Best score\nprint(math.sqrt(gsSVML.best_score_))\nprint(gsSVML.best_params_)","0a603e32":"svml_best = gsSVML.best_estimator_  # Getting best estimator based on cv","d15d2096":"print('R-squared value for Supprt vector regression on train data is ', r2_score(y_train,svml_best.predict(x_train)))","f8a35c8c":"# predicting on test data\nsvml_pred = svml_best.predict(x_test)","12b7edff":"# first drfining random state it is needed for all tree based models if you want to produce reproducible results\nrandom_state = 2\n\n# now call model from sklearn library\nDT = DecisionTreeRegressor(random_state=random_state)\n\n# Applying cross validation to see the validation score\nMSLE = cross_val_score(DT,x_train, y = y_train, scoring = sklearn.metrics.make_scorer(mean_squared_log_error), cv = kfold , n_jobs =-1)\nmean_squared_log_err = np.mean(np.sqrt(MSLE))\nprint('mean squared log error is : ',mean_squared_log_err)\n\n","97179782":"\n# now fitting the model on entire data for final predictions\nDT.fit(x_train,y_train)\n\n# let's find out the error on whole train data to see is there any overfitting of model or not\nmath.sqrt(mean_squared_log_error(y_train,DT.predict(x_train)))","3edfc7c8":"# predicting on test data\nDT_pred = DT.predict(x_test)","6fd760bf":"\n# RFC\nRFC = RandomForestRegressor(random_state = random_state)\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [2],\n              \"min_samples_split\": [5],\n              \"min_samples_leaf\": [2],\n              \"bootstrap\": [True],\n              \"n_estimators\" :[200],\n              }\n\n# Applying grid search \ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_log_error), n_jobs= -1, verbose = 1)\n\ngsRFC.fit(x_train,y_train)\n\n\n# Best score\n\nprint(math.sqrt(gsRFC.best_score_))\nprint(gsRFC.best_params_)","6c23aacc":"RFC_best = gsRFC.best_estimator_  # Getting best estimator based on cv","512c28c6":"# predicting on test data\nrfc_pred = RFC_best.predict(x_test)","82f0eca6":"# Gradient boosting\n\nGBC = GradientBoostingRegressor(random_state = random_state)\ngb_param_grid = {}\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_log_error), n_jobs= -1, verbose = 1)\n\ngsGBC.fit(x_train,y_train)\n\n\n# Best score\n\nprint(math.sqrt(gsGBC.best_score_))","327d0bd0":"GBC_best = gsGBC.best_estimator_","0c8a2ce4":"# predicting on test data\ngbc_pred = GBC_best.predict(x_test)","2688939f":"\n# using desicion tree as base regressor\nDT1 = DecisionTreeRegressor(random_state=random_state)\n\nadaDTC = AdaBoostRegressor(DT1, random_state=random_state)\n\nada_param_grid = {\"n_estimators\":[100,500],\n              \"learning_rate\":  [ 0.01]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring= sklearn.metrics.make_scorer(mean_squared_log_error) , n_jobs= -1, verbose = 1)\n\ngsadaDTC.fit(x_train,y_train)\n\n","2db943fb":"Ada_best = gsadaDTC.best_estimator_","5ac94b6e":"## checking the cross val score of best estimator\n\n# finding best params\nprint(Ada_best.get_params())\n\n# finding ccv score\nmath.sqrt(cross_val_score(Ada_best, x_train, y = y_train, scoring = sklearn.metrics.make_scorer(mean_squared_log_error) , cv = kfold , n_jobs =-1).mean())","dc0839fa":"# predicting on test data\nada_pred = Ada_best.predict(x_test)","cd5fac51":"xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2500,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\n\n\n","c5477be0":"MSLE = cross_val_score(xgb,x_train, y = y_train, scoring = sklearn.metrics.make_scorer(mean_squared_log_error), cv = kfold , n_jobs =-1)\nmean_squared_log_err = np.mean(np.sqrt(MSLE))\nprint('mean squared log error is : ',mean_squared_log_err)","1844ef27":"# now fitting the model on entire data for final predictions\nxgb.fit(x_train,y_train)\n\n# predicting on test data\nxgb_pred = xgb.predict(x_test)","51fa21fe":"lgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=11000, \n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, \n                                       )\n","3e2a12e6":"MSLE = cross_val_score(lgbm,x_train, y = y_train, scoring = sklearn.metrics.make_scorer(mean_squared_log_error), cv = kfold , n_jobs =-1)\nmean_squared_log_err = np.mean(np.sqrt(MSLE))\nprint('mean squared log error is : ',mean_squared_log_err)","3bd6056c":"# now fitting the model on entire data for final predictions\nlgbm.fit(x_train,y_train,eval_metric= sklearn.metrics.make_scorer(mean_squared_log_error))\n\n# predicting on test data\nlgbm_pred = lgbm.predict(x_test)","a4bb22bf":"# plotting learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(lgbm,\"LGBM learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(xgb,\"xgboost learning curves\",x_train,y_train,cv=kfold)","8090614e":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,50))\n\nnames_classifiers = [(\"AdaBoosting\", Ada_best),(\"XGB\",xgb),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=x_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","520d1751":"# making panda series of predictions on test data\ntest_LReg = pd.Series(linear_reg_pred, name=\"linear_reg\")\ntest_ridge = pd.Series(ridge_pred, name=\"ridge\")\ntest_lasso = pd.Series(lasso_pred, name=\"lasso\")\ntest_elastic = pd.Series(elastic_pred, name=\"elasticnet\")\ntest_SVMR = pd.Series(svm_pred, name=\"SVC-R\")\ntest_SVML = pd.Series(svml_pred, name=\"SVC-L\")\ntest_DT = pd.Series(DT_pred, name=\"DT\")\ntest_RFC = pd.Series(rfc_pred, name=\"RFC\")\ntest_AdaC = pd.Series(ada_pred, name=\"Ada\")\ntest_GBC = pd.Series(gbc_pred, name=\"GBC\")\ntest_XGB = pd.Series(xgb_pred, name=\"XGB\")\ntest_LGBM = pd.Series(lgbm_pred, name=\"LGBM\")\n\n\n\n# Concatenate all regressors results\nensemble_results = pd.concat([test_LReg, test_ridge, test_lasso, test_elastic, \n                              test_SVMR, test_SVML, test_DT, test_RFC,test_GBC,\n                              test_AdaC, test_XGB, test_LGBM ],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True, fmt = '.2f')","b8a03d89":"## stacking will be automated by using vecstack library\n\nfrom vecstack import stacking\n\n# firstly defining base models with optimal parameters find using grid search cv\n\nmodels = [ linear_reg,\n          \n           Ridge(alpha = 1),\n          \n           Lasso(alpha = 0.1),\n          \n           ElasticNet(alpha = 0.5, l1_ratio = 1e-5),\n          \n           SVR(kernel = 'rbf', C= 500, gamma = 500 ),\n          \n           SVR(kernel = 'linear',C = 10, gamma = 0.001),\n           \n           DT,  # Decision tree\n\n           \n           RandomForestRegressor(random_state=2, n_jobs=-1,\n                                 min_samples_split= 5,\n                                 min_samples_leaf = 2,\n                                  bootstrap = True,\n                                n_estimators=120,\n                                 max_features = 200,\n                               max_depth= 2),\n         \n         \n          GradientBoostingRegressor(random_state = random_state),\n          \n          AdaBoostRegressor(DT1, random_state=random_state, learning_rate = 0.01), \n          \n          xgb,\n          \n          lgbm\n        \n         ]","1d4612ab":"\n# Here Mean squared error will be used as the metric\n\n# S_train, S_test will be the out of fold predictions which will be used for meta model\n\n\nS_train, S_test = stacking(models,                   \n                           x_train, y_train, x_test,   \n                           regression= True, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric= mean_squared_error, \n    \n                           n_folds=5, \n                 \n                           stratified=False,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","45b51b9f":"# defining metal model as linear reg, it is sometimes called as blending\n\nmeta_model1 = LinearRegression()\n    \nmeta_model1.fit(S_train, y_train)\ny_pred1 = meta_model1.predict(S_train)\nstack_pred1 = meta_model1.predict(S_test)\n\nprint('Final train prediction score: [%.8f]' % math.sqrt(mean_squared_log_error(y_train, y_pred1)))","5e5dc707":"plt.figure(figsize = (10,10))\nmodel_importances  = pd.Series(meta_model1.coef_ )\nmodel_importances.plot(kind = 'barh')","15fdf51d":"# making weighted predictions by combine both\nweighted_pred = (0.45*xgb_pred + 0.55*lgbm_pred)","6943875c":"# taking best 4 models\n\nmodels1 = [ \n         \n         GradientBoostingRegressor(random_state = random_state),\n     \n          xgb,\n          \n          lgbm\n        \n         ]","f4677598":"# doing same tasks for these models also\n\nS1_train, S1_test = stacking(models1,                   \n                           x_train, y_train, x_test,   \n                           regression= True, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric= mean_squared_error, \n    \n                           n_folds=5, \n                 \n                           stratified=False,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","ddafc7d9":"# Again using metal model as linear reg\n\nmeta_model2 = LinearRegression()\n    \nmeta_model2.fit(S1_train, y_train)\ny_pred2 = meta_model2.predict(S1_train)\nstack_pred2 = meta_model2.predict(S1_test)\n\nprint('Final train prediction score: [%.8f]' % math.sqrt(mean_squared_log_error(y_train, y_pred2)))","2cc8a6b1":"# checking weights assigned to each system\nplt.figure(figsize = (10,10))\nmodel_importances  = pd.Series(meta_model2.coef_ )\nmodel_importances.plot(kind = 'barh')","e0c7d546":"# checking cross validation score for this model\nMSLE = cross_val_score(meta_model2,S1_train, y = y_train, scoring = sklearn.metrics.make_scorer(mean_squared_log_error), cv = kfold , n_jobs =-1)\nmean_squared_log_err = np.mean(np.sqrt(MSLE))\nprint('mean squared log error is : ',mean_squared_log_err)","4c19702d":"# submitting predictions finally using weighted\nsample_submission['SalePrice'] = (0.55*stack_pred2 + 0.45*weighted_pred)\nsample_submission.to_csv('stacking.csv',index = False)","f27d4eb2":"* Now we will treat categorical and numerical features seprately","213fa9c6":"**Nice!** It also performs similar to xgboost","57e7c60c":"If you find my work helpful, feel free to **fork** and **Upvote** this kernel as it keeps me motivated to make more such kernels \n\nAlso you can fork, optimize and apply new things if you want and if you have any suggestions please Comment","50411a9d":"XGboost is one of the **state of the art** model which is a faster implementation of gradient boosting with slight modification ","8a1bb8b1":"\n## F-statistic\nNow let's try to find the **F-statistic** of data\n\nif you don't know about F-statistic you can read this [blog](https:\/\/towardsdatascience.com\/linear-regression-understanding-the-theory-7e53ac2831b5), **in brief it is an extend version of p value for higher dimension feature selection**","a38668fd":"Let's check which model **contribute** how much in ensembling","46fa2929":"**Dealing with categorical features**","47770de5":" ### SVR with 'rbf' kernel","27ba2baf":"let's now move to **Ridge** and **lasso Regression** one by one","2fa0d780":"* With only first 30 features it looks like there is a lot of correlation between the independent(explanatory variables), ridge or lasso models may come into play or some techniques like PCA etc. ","dc4ecd7d":"Now one more thing we may experiment as we know **Xgboost, LGBM, Adaboost and Gradient boosting fits well** so let's try to combine only these four using stacking","dfab74f5":"*let's check the **R-squared value** which is a percentage measure of variance explained by model*","f43bea2a":"**RMSLE**\nThe metrics used is root mean squared log error for the data which is just RMS of logarithmic predictions and log of actual values","de35d081":"# Feature selection and scaling","5ed1d3a6":"* **Categorical Features** :","17e6d5f6":"## Now merging the train and test data\nWe merge both, in order to apply the same transformation to both data","5cca279f":"* After making some plots we found that we have some colums with low variance so we decide to delete them","42fcd428":"let's move onto elasticnet\n\n# 4. Elasticnet\n\n\nIt is a combination of both Ridge and lasso\n* Elasticnet has two main parameters\n   1. Alpha (it decides the amount of participation of both)\n   2. l1_ratio(it decides the amount of multiplication of overall contribution)","eb7bef84":"**let's predict test values**","cded5393":"**From above we saw that even in linear reg some of the cofficients are 0 means they have no relative contribution**","2364b0a1":"making predictions on test data","075d818b":"# 1. Decision Tree","36e72db1":"**Feature importance**","8e7e092d":"checking **R-squared**","344fad28":"If you want to learn more about these tree based models check [here](https:\/\/bookdown.org\/castillo_sam_d\/Exam-PA-Study-Manual\/tree-based-models.html)","c0ca83a9":"# Data Cleaning","8a95cc18":"We here used Gradientboosting without any params because it fit's well without any tuning\n\nTill now it's the best we get, hopefully later models perform more better","ae92d915":"Let's plot the dependence\/contribution of features on sale price","126138a4":"**Now xgb and lgm performs well, we can take weighted average predictions of LGBM and XGboost to see what happens**\n","187213b3":"If you want to learn more about **Regularization, ridge, lasso and ElaticNet** check [here](https:\/\/towardsdatascience.com\/intro-to-linear-model-selection-and-regularization-d47bd2c5d54)","761bd3b3":"Random forest is an ensembling model based on bagging method.\n\n**what random forest done is that it combines the predictions of several independent decision trees which helps in reduce overfitting problems**","c4731ee1":"**Learning Curves**","d2c7e059":"It seems that adaboost and gradient boosting are performing comparatively similar, let's see what xgboost can bring","00b0ccbe":"**let's see the distribution of SalePrice**","aec2abbc":"* **Numerical Features** :","19c9f159":"we can remove *PoolQC, MiscFeature, Alley, Fence* as these features contain more than 80% of null values","069a10fe":"*looks like most of the features add some amount of variance let's see during model building is it beneficial or not* ","1b0e4996":"* So we will fill the year of bulit of garage corresponding to the median built age of home and the Lot frontage with 68","52c13d40":"* Now the next step is to encode categorical features","1d5f5d95":"firstly finding columns in dataset which contain null values","bfbb93ad":"*Finally make predictions on test data for linear reg*","e0207d4d":"Nice with a simple algo we get a pretty decent loss","542b7c48":"Now we will check how **correlated** are the predictions on **test data of all the models** applied in this kernel\n\n","a66bc0ea":"* Ridge regression is a regualrized version of linear regression means it shrinks those features which are unnecessary for predictions\n* It has a hyperparameter alpha which needed to be set before if alpha is high all features shrinks to zero, if low behaves as simple linear regression\n* Using Grid search techniques to choose alpha","908bd9c2":"# 2.Ridge Regression","2dca016a":"**Now plotting the correlation maps of some features say first 30 features on SalePrice**\n* it will help to build understanding of interaction between features","de6578c5":"*It's strange to see no variance (negative) explained by it*","322717d6":"Looks like random forest is also not doing well","c887a0a3":"* Will we use One hot encoder to encode the rest of categorical features","b9758b71":"**Let's starting with the series of tress based algorithms or cart algorithms**\n\nIn this series there are different types of models:\n     * Decision Tree\n     * Random Forest\n     * Adaboost\n     * GBT (Gradient Boosting Trees)\n     * XGBoost\n     * LGBM\n\nWe will apply all these one by one","b0b62782":"For learning purpose, let's start with the simplest model like **linear regression**  and later to complex models like **LGBM, Xgboost** ","6c6a7254":"Now time for linar kernel\n\n### SVR with 'linear' kernel","d08bb03d":"Firstly we will be using different types of linear models and analyze their performances\n\nLinear models consists:\n * Linear Regression\n * Ridge\n * Lasso\n * ElasticNet\n * SVM\n\nlet's applying them one by one","13e894e9":"* Fill GarageYrBlt and LotFrontage\n* Fill the rest of columns with 0","f6cb90bd":"* We will fill -- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond** -- with \"None\" (Take a look in the data description).\n* We will fill the rest of features with the most frequent value (using its own most frequent value).","a547b2d8":"Looks like that Elastic net may perform better on test data because it combines the features of both ridge and lasso","e6a3b8cb":"# 2. Random Forest","79e141fb":"Now comes the last tree model for this kernel\n\n# 6. LGBM\n\nThis model is one of the recent developed model,It is also one of the start of the art models, sorry to say even I don't know much about it, I am also learning about it","26b1f2be":"Looks like linear kernel performs comparatively similar to linear regression let's see the variance it captures from data","d966f201":"**Since the dataset is comparatively small and f-static is significantly greater than 1 so initially we can keep all featutes**","19d5ece6":"* On observing we get that the scaling is not uniform so we can perform feature scaling","7c5d33c0":"lasso also acts as a regularized model but unlike Ridge, lasso not shrinks unimportant features but makes their cofficients zero so it also acts as a feature selection model","b262de1a":"*Firstly call kfold for cross validation*","df9579c1":"# 3. Gradient Boosting ","53e544f5":"from the curve it seems that **XGB, GBT, LGBM** fits well but **RFC,Adaboost** don't fit well to the data","b8c25e00":"Great to see such automotaic linear combination of models","01e54266":"An intresting thing which is common here is that all the models uses only **few features** into consideration for predictions, nice to see such adaptibility ","b0a8ffbd":"let's check the feature importance for ridge regression","830884c1":"# 4. SVR (Support vector Regression)","279628d8":"* **Ordinal categories features** - Mapping from 0 to N\n","14060483":"* fill all others with 0","9984bc85":"* Features with more than 50% of none values","c1ec0ab0":" calculating **R-squared** value for ridge","e14ae1e9":"On applying grid search cv we get best value of alpha = **1**","b4f798b2":" **WOW!** XGBoost performs very well, let's make predictions on test data","5880d965":"On comparison with linear regression, Ridge shrinks the size of some variables to reduce model variance","8e48792f":"*Now the numerical features is scaled and categorical features encoded*","542bebb7":"## Loading Data","f07c06dc":"## 1. Linear Regression","f685260e":"# 5. XGBoost ","ae35ddb9":"**Looks like the ridge regression does partial regualrization as cv score slightly improved**","cfb9d480":"* We finally end up with a clean dataset","0f21a9e4":"looks like most of them contains null values,we need to clean the data\n\n**Calculating how much percent of null values in each columns**","cba87432":"* Due to some negative errors here we not directly applies **RMLSE** instead we use **RMSE**\n* After searching we get the best parameter as alpha = **0.5** and l1_ratio = **1e-5**\n","1c41da78":"On applying grid search cv we get best value of alpha = **1**\n\n\n\n\n\n\n","8c882a0c":"It clearly implies that desicion tree will have a tendency to **overfit** the data quickly\n\nDue to this reason we need to look for different tree based models","02340586":"Gradient boosting is an ensembling model based on boosting method\n\nIt betters the predictions by adding a desicion tree at each successive steps based on the methods of residual errors","abcf56e8":"It seems that rbf kernel doesn't perform well on data\n\nbefore moving to next kernel, let's get **R-squared** value for it","f555caa3":"\n* Linear regression tries to find the interaction between features and dependent variable by fitting a straight line to the data","41cb9b0d":"**In the end**\n\n## A detailed analysis of my submissions using different models on the test data\n*  Linear Regression - **0.20062**\n*  Ridge Regression - **0.18594**\n*  Lasso Regression - **0.19666**\n*  ElasticNet - **0.15887**\n*  SVR(kernel - rbf) - **0.41655**\n*  SVR(kernel - linear) - **0.19974**\n*  Desicion tree - **0.20370**\n*  RandomForest - **0.26055**\n*  Gradient Boosting - **0.13851**\n*  Adaboost - **0.14603**\n*  XGBoost - **0.12702**\n*  LGBM - **0.12386**\n*  Stacking with meta model (LR) - **0.12501** \n*  Weighted Average Of LGBM and XGB - **0.12330** \n*  Stacking of Xgboost, LGBM ang Gradient boosting - **0.12298**\n*  Weighted Average of last two - **0.12245**\n\nBest According to my submissions is **Last One**(Under 15%)\n","89819d3a":"Looks like that lasso performs less better than ridge as it will not able to regularize well\n\nNow plotting Features importance in lasso","6bc1c573":"let's check the **R-Squared** for lasso","87067bbb":"Looks like that the predictions have enough variation to encourage the **ensembling** of all these models","5c2e4974":"* Means approx 88% variance is explained ","ba1ebeda":"## Checking Null values","edecd558":" Like linear regression, svm is also believed to be a linear model but under conditions of kernel\n \n **Kernel** - Kernel is just a transformation applied between data type to make the data seprable by a linear plane\n \nThere are many different types of kernel that we will use here: \n  * Linear kernel\n  * rbf (Gaussian)\n\n\nFor more about **SVR**. see [this](https:\/\/towardsdatascience.com\/support-vector-machines-svm-c9ef22815589)  ","c68049a7":"* Now we have a clean categorical features\n* In the next step we will deal with the **numerical** features","52044622":"* Like we see here tha the minimun is -1 ???\n* It is strange to find that the house was sold in 2007 before the YearRemodAdd 2009. So we decide to change the year of sold to 2009","566e099f":"1. we have 81 columns \n2. Taget is SalePrice\n3. Id is not useful only needed for submission\n\n***We have 79 featues in our dataset, let's see all are useful or not***","7985296b":"* There is significant difference between error on cv and whole data means that the model is slightly overfit, hence we need to look for some regularized models","00745b4e":"**We finally merge those features**","35a81699":"# Stacking Ensembling\n\nit is a technique in which the contribution of all base models is combined by a one metal model, meta model helps in extracting the best from all base models ","cdf7035b":"Numbers of features are huge let's explore data","ec0f973d":"# MODEL BUILDING","3610098d":"Now finally we will see **feature importance** of all tree models and their **learning curves**\n","49a4eedd":"Adaboost is also a boosting ensemble model but it based on the feedback from previous model, it uses the same model at each step giving more priority to the error made on the last step by this model","bd032af4":"Desicion tree is a simple tree based models which divides the predictions data into several desicion boundaries based on some set of conditions and approximate the predictions on new data","636d62fc":"*Now let's check the scales of all numerical features, to verify is there any scaling required or not*","9e77b1fb":"* Now we will create one new feature that is age of house for simplicity","e70707af":"Let's explore a bit more about the variance each feature add to data","7733a39d":"# 4. AdaBoost","d216568a":"now let's move on to 3 type\n# 3. Lasso Regression"}}