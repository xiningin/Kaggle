{"cell_type":{"cf71f8d3":"code","6ac25dc6":"code","ee0c4a7c":"code","2b29979d":"code","a918a81b":"code","e3ed6c20":"code","8f880eaa":"code","669ed267":"code","e10d791b":"code","81b535a3":"code","0317b832":"code","c02bef6e":"code","aba81352":"code","9b42b86a":"code","dcdd587c":"code","0ef9051e":"code","69c56add":"code","aef1e7f9":"code","47ecd03a":"code","3fd4cd01":"code","482dd927":"code","fcf02355":"code","e26f8f8b":"code","ceceaf70":"code","0da969be":"code","7111984e":"code","289a46d6":"code","6d937ac1":"code","ac2f3bd7":"code","d81a0084":"code","22681334":"code","965966f3":"code","f68fa05e":"code","584cca7c":"code","73fa8192":"code","81b6be18":"code","d1d6ce6c":"code","2e1b1ab0":"code","f940b78c":"code","3a061c17":"code","4f33077a":"code","e92f8436":"code","ed8c405b":"code","debd67f0":"code","67938c70":"code","e65128d7":"markdown","da8db8af":"markdown","2b38504e":"markdown","69ec903e":"markdown","b1863344":"markdown","d005e602":"markdown","45809845":"markdown","bf79a1e2":"markdown","616acb93":"markdown","8492160f":"markdown"},"source":{"cf71f8d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ac25dc6":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","ee0c4a7c":"df.head(2)","2b29979d":"import seaborn as sns\nsns.heatmap(df.corr(),annot=True)","a918a81b":"#we can see 'Glucose' , 'BMI' and 'Age' are highly co-related with our data","e3ed6c20":"x = df.iloc[:,:8]\ny =df.iloc[:,8]","8f880eaa":"#rescaling our data for better calculatons\nfrom sklearn import preprocessing\nx = preprocessing.scale(x)","669ed267":"x #preprocessed data","e10d791b":"sns.countplot(x = 'Outcome',data=df,palette='hls')","81b535a3":"#now import LogisticRegression library from sklearn\nfrom sklearn.linear_model import LogisticRegression\n#making a logistic regression objext\nlogit_reg = LogisticRegression()","0317b832":"\n\n\n\n#splitting data into train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)","c02bef6e":"x_train","aba81352":"y_train","9b42b86a":"logit_reg.fit(x_train,y_train)\n#fitting our model","dcdd587c":"pred_y = logit_reg.predict(x_test)\n#predicting on our test data","0ef9051e":"\n#checking accuracy of logistic regreesion model\nprint('score of model on test data is ',logit_reg.score(x_test,y_test))\nprint('score of model on train data is',logit_reg.score(x_train,y_train))","69c56add":"from sklearn.metrics import confusion_matrix , classification_report\nconfusion_matrix = confusion_matrix(y_test, pred_y)\n\nprint('classification report is ',classification_report(y_test,pred_y))","aef1e7f9":"#visualising confusion matrix\nsns.heatmap(confusion_matrix,annot= True)","47ecd03a":"#checking statistical method\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y,x)\nresult=logit_model.fit()\nprint(result.summary())","3fd4cd01":"\n# we should also check how many k neaighbours we should have for our model for best results\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nk_range = range(1,26)\nscores = {}\nscore_list = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    y_pred = knn.predict(x_test)\n    scores[k] = metrics.accuracy_score(y_test,y_pred)\n    score_list.append(metrics.accuracy_score(y_test,y_pred))","482dd927":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(k_range,score_list)\nplt.xlabel('value of k for knn')\nplt.ylabel('test set accuracy')","fcf02355":"knn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(x_train,y_train)","e26f8f8b":"print('accuracy on test data is',knn.score(x_test,y_test))\nprint('accuracy on train data is',knn.score(x_train,y_train))\nprint('model is generalising well')\n","ceceaf70":"import keras","0da969be":"from keras import Sequential\nfrom keras.layers import Dense","7111984e":"classifier = Sequential()\nclassifier.add(Dense(5,activation = 'relu',input_dim=8))\nclassifier.add(Dense(5,activation = 'relu'))\n#output layer\nclassifier.add(Dense(1,activation='sigmoid'))","289a46d6":"classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])","6d937ac1":"history =  classifier.fit(x_train,y_train,batch_size=10,epochs=20,validation_split=0.2)","ac2f3bd7":"eval_model_train=classifier.evaluate(x_train, y_train)\nprint('training set accuracy',eval_model_train)\neval_model_test = classifier.evaluate(x_test,y_test)\nprint('testing set accuracy',eval_model_test)","d81a0084":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","22681334":"from sklearn.model_selection import GridSearchCV,KFold\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam","965966f3":"def create_model():\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = 'normal',activation='relu'))\n    model.add(Dense(4,kernel_initializer = 'normal',activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    \n    adam = Adam(lr=0.01)\n    model.compile(loss = 'binary_crossentropy',optimizer=adam,metrics = ['accuracy'])\n    \n    return model","f68fa05e":"#create model\nmodel = KerasClassifier(build_fn=create_model,verbose = 0)\n\n#define grid search parameters\nbatch_size = [10,20,40]\nepochs = [10,50,100]\n\nparam_grid = dict(batch_size=batch_size,epochs=epochs)\ngrid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(x_train,y_train)","584cca7c":"print(grid_result.best_score_,grid_result.best_params_)","73fa8192":"#turning learning rate and drop out rate\nfrom keras.layers import Dropout\n\n# Defining the model\n\ndef create_model(learning_rate,dropout_rate):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = learning_rate)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nlearning_rate = [0.001,0.01,0.1]\ndropout_rate = [0.0,0.1,0.2]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(x_train,y_train)\n","81b6be18":"print(grid_result.best_score_,grid_result.best_params_)","d1d6ce6c":"#best results are at drop out rate of 0.2 and learning rate of 0.01","2e1b1ab0":"def create_model(activation_function,init):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nactivation_function = ['softmax','relu','tanh','linear']\ninit = ['uniform','normal','zero']\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(activation_function = activation_function,init = init)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 2)\ngrid_result = grid.fit(x_train,y_train)","f940b78c":"print(grid_result.best_score_,grid_result.best_params_)","3a061c17":"def create_model(neuron1,neuron2):\n    model = Sequential()\n    model.add(Dense(neuron1,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nneuron1 = [4,8,16]\nneuron2 = [2,4,8]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(x_train,y_train)","4f33077a":"print(grid_result.best_score_,grid_result.best_params_)","e92f8436":"from sklearn.metrics import classification_report, accuracy_score\n\n# Defining the model\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(16,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.2))\n    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Fitting the model\n\nmodel.fit(x_train,y_train)","ed8c405b":"y_pred_tuned =model.predict(x_test)","debd67f0":"print('classification report is',classification_report(y_pred_tuned,y_test))","67938c70":"print(accuracy_score(y_pred_tuned,y_test))","e65128d7":"LOGISTIC REGRESSION FOR CLASSIFICATION","da8db8af":"from this graph it is clear that optimum result lies near 21\nwe would try 21 ","2b38504e":"#further we can improve model by removing those columns whose f-values(p>|z|) are greater than 0.05 to increase the efficency but i would first try other algorithm before doing this","69ec903e":"# now we would use keras to make ANN for our predictions","b1863344":"The optimum values of Hyperparameters are as follows :-\nBatch size = 40\nEpochs = 10\nDropout rate = 0.2\nLearning rate = 0.001\nActivation function = tanh\nKernel Initializer = uniform\nNo. of neurons in layer 1 = 16\nNo. of neurons in layer 2 = 4","d005e602":"Most Test Subjects dont have Diabetes :)","45809845":"now we would tune our keras model using gridcv","bf79a1e2":"now that our logistic regression model is ready , we can check its accuracy","616acb93":"# 2. Trying KNN model","8492160f":"best result is at 40 batch size and 10 epochs"}}