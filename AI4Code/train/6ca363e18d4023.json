{"cell_type":{"e8b08d36":"code","57223bc4":"code","17bcbf4c":"code","68d039c6":"code","10073b74":"code","5b6a02fb":"code","363e8d62":"code","853546ff":"code","92f9f35d":"code","dbcf1f8d":"code","e1d3f107":"code","6f947434":"code","c0fa669d":"code","8f397181":"code","a540550b":"code","97474539":"code","4939c817":"code","0eb6156f":"code","1455ebfb":"code","809ca73b":"code","335f0ac3":"code","7de97fad":"code","67951a5b":"code","dbe2b0c0":"code","8200aef0":"code","d2f803b9":"code","227d78d2":"code","7584d621":"code","36bf8a88":"code","a85e4f41":"code","f8f2ec59":"code","e6bdfad6":"code","fc77db71":"code","77b0a4d1":"code","3e6081ce":"markdown","703b0c18":"markdown","09beb1e5":"markdown","59bd03b8":"markdown","44c31e28":"markdown","ea85ab7b":"markdown","e09d32d4":"markdown","b77db055":"markdown","951c7ce2":"markdown","39b3ae6d":"markdown","21cd40f1":"markdown","78118b59":"markdown","8d383d63":"markdown","a36314c1":"markdown","eaaff2b0":"markdown","fde3de51":"markdown","8917d1ac":"markdown","63fc7858":"markdown","7c17284a":"markdown","2656e963":"markdown","120ee1c1":"markdown","fcaeea09":"markdown","27a80ec6":"markdown","c4b241a1":"markdown","5c0039d2":"markdown","55e8b779":"markdown","52823008":"markdown","9dbb2916":"markdown","62c43d7f":"markdown","0a13d76a":"markdown","4129f8c7":"markdown","21b3cc9b":"markdown","3a6fbf82":"markdown","a70fbb73":"markdown","00d54e24":"markdown"},"source":{"e8b08d36":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","57223bc4":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom collections import Counter, defaultdict\nimport tokenization\nimport string\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n\nimport operator\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\nimport gc","17bcbf4c":"df_path = \"..\/input\/bible\/t_kjv.csv\"\ndf = pd.read_csv(df_path)\nprint(\"The number of rows: \" + format(df.shape[0]) + \" The number of factors: \" + format(df.shape[1]))","68d039c6":"df_aux_path = \"..\/input\/biblical-books-with-information\/BibleBooks.csv\"\ndf_aux = pd.read_csv(df_aux_path)\nprint(\"The number of rows: \" + format(df_aux.shape[0]) + \" The number of factors: \" + format(df_aux.shape[1]))","10073b74":"sw = stopwords.words('english')\nsw.extend(['from', 'upon','away','even','unto'])\nsw[0:5]","5b6a02fb":"df['t'] = df['t'].astype('str')\ndf.loc[df['b'] <= 39, 'Testament'] = 'Old'\ndf.loc[df['b'] > 39, 'Testament'] = 'New'\ndf","363e8d62":"df_enriched = df_aux.drop(['Tanakh','New Jerusalem Version'], axis=1)\ndf_enriched['King James Version']=df_enriched['King James Version'].replace(np.nan, 0)\ndf_enriched['King James Version'] = df_enriched['King James Version'].astype('int')\ndf_enriched","853546ff":"df = df.merge(df_enriched, left_on='b', right_on='King James Version')\ndf","92f9f35d":"ps = PorterStemmer()\n\ndef lower_column_t(data):\n    values = data['t']\n    values = values.lower()\n    data['t'] = values\n    return data\n\ndef clean_interpunction(data):\n    values = data['t']\n    values = values.replace('.','')\n    values = values.replace(';','')\n    values = values.replace(':','')\n    values = values.replace(',','')\n    values = values.replace(\"'\",\"\")\n    values = values.replace('\"','')\n    data['t'] = values\n    return data\n\ndef stem(a):\n    p = nltk.PorterStemmer()\n    b = []\n    for line in a:\n\n        split_line = line.split(' ')\n        length=len(split_line)\n        new_line = []\n\n        for word in range(length):\n            if word == 0:\n                new_line.append(str(p.stem(split_line[word])))\n            else:\n                new_line[0] = new_line[0] + ' ' + (str(p.stem(split_line[word])))\n\n        b.append(new_line[0])\n\n    return b\n\ndef lem(a):\n    p = nltk.WordNetLemmatizer()\n    b = []\n    for line in a:\n\n        split_line = line.split(' ')\n        length=len(split_line)\n        new_line = []\n\n        for word in range(length):\n            if word == 0:\n                new_line.append(str(p.lemmatize(split_line[word], pos=\"v\")))\n            else:\n                new_line[0] = new_line[0] + ' ' + (str(p.lemmatize(split_line[word], pos=\"v\")))\n\n        b.append(new_line[0])\n\n    return b\n\ndef tokenize(a):  \n    b = []\n    for line in a:\n        b.append(word_tokenize(line))\n                 \n    return b\n\ndef flatten(a):\n    b = []\n    for line in a:\n        b = ' '.join(line)\n    \n    return b\n\ndef count_words(a):\n    b=0\n    for line in a:\n        b = b + sum([i.strip(string.punctuation).isalpha() for i in line.split()])\n        \n    return b\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in sw]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","dbcf1f8d":"df = df.apply(lower_column_t, axis=1)\ndf = df.apply(clean_interpunction, axis=1)\ndf['t_lem']=lem(df.t)\ndf['t_stem']=stem(df.t)\ndf['t_stem_lem']=lem(df.t_stem)\ndf['NoWords'] = df['t'].str.split().str.len()\n#df=df.set_index('id')\ndf","e1d3f107":"color_1 = plt.cm.Blues(np.linspace(0.6, 1, 66))\ncolor_2 = plt.cm.Purples(np.linspace(0.6, 1, 66))\n\nwords_versets = df.groupby('Book').agg({'b':'count','NoWords':'sum'}).sort_values(by='b',ascending=False)\ndata1 = words_versets['b']\ndata2 = words_versets['NoWords']\n\nplt.figure(figsize=(16,8))\nx = np.arange(66)\nax1 = plt.subplot(1,1,1)\nw = 0.3\n\ncolor = color_1\nplt.title('Number of words vs number of versets')\nplt.xticks(x + w \/2, data1.index, rotation=-90)\nax1.set_xlabel('Books of Bible')\nax1.set_ylabel('Number of versets')\nax1.bar(x,data1.values,color=color_1,width=w,align='center')\n\nax2 = ax1.twinx()\n\ncolor = color_2\nax2.set_ylabel('Number of words')\nax2.bar(x + w,data2, color=color_2,width=w,align='center')\n\nplt.show()","6f947434":"color_1 = plt.cm.Blues(np.linspace(0.6, 1, 66))\ncolor_2 = plt.cm.Purples(np.linspace(0.6, 1, 66))\n\nwords_versets_ = df.groupby('Time').agg({'b':'mean','NoWords':'mean','Time':'mean'})\ndata1 = words_versets_['b']\ndata2 = words_versets_['NoWords']\n\nplt.figure(figsize=(16,8))\nx = np.arange(31)\nax1 = plt.subplot(1,1,1)\nw = 0.3\n\ncolor = color_1\nplt.title('Number of words vs number of versets')\nplt.xticks(x + w \/2, data1.index, rotation=-90)\nax1.set_xlabel('Books of Bible')\nax1.set_ylabel('Number of versets')\nax1.bar(x,data1.values,color=color_1,width=w,align='center')\n\nax2 = ax1.twinx()\n\ncolor = color_2\nax2.set_ylabel('Number of words')\nax2.bar(x + w,data2, color=color_2,width=w,align='center')\n\nplt.show()","c0fa669d":"words_versets['Length versets'] = round(words_versets['NoWords']\/words_versets['b'],0)\nwords_versets = words_versets.sort_values(by='Length versets',ascending=False)\ndf_enriched_to_merge = df_enriched.set_index('Book')\nwords_versets = words_versets.merge(df_enriched_to_merge, left_on='Book', right_on='Book')\n\nSpearmanCorr = words_versets[['NoWords','Time','Length versets']].corr(method=\"spearman\")\nplt.figure(figsize=(7,7))\nsns.heatmap(SpearmanCorr, vmax=.9, square=True, annot=True, linewidths=.3, cmap=\"YlGnBu\", fmt='.1f')","8f397181":"mpl.rcParams['figure.figsize']=(10,10)    \nmpl.rcParams['font.size']=12                        \nmpl.rcParams['figure.subplot.bottom']=.1 \n\nwordcloud_t = WordCloud(background_color='white',stopwords=sw).generate(str(df['t'].values))\nwordcloud_t_stem = WordCloud(background_color='white',stopwords=sw).generate(str(df['t_stem'].values))\nwordcloud_t_lem = WordCloud(background_color='white',stopwords=sw).generate(str(df['t_lem'].values))\nwordcloud_t_stem_lem = WordCloud(background_color='white',stopwords=sw).generate(str(df['t_stem_lem'].values))\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\nax0, ax1, ax2, ax3= ax.flatten()\n\nax0.imshow(wordcloud_t, aspect=\"auto\")\nax0.title.set_text('Untouched')\nax0.axis('off')\n\nax1.imshow(wordcloud_t_stem, aspect=\"auto\")\nax1.title.set_text('Stemmed')\nax1.axis('off')\n\nax2.imshow(wordcloud_t_lem, aspect=\"auto\")\nax2.title.set_text('Lemmatised')\nax2.axis('off')\n\nax3.imshow(wordcloud_t_stem_lem, aspect=\"auto\")\nax3.title.set_text('Stemmed and lemmatised')\nax3.axis('off')\n\nplt.show()","a540550b":"mpl.rcParams['figure.figsize']=(10,10)    \nmpl.rcParams['font.size']=12                        \nmpl.rcParams['figure.subplot.bottom']=.1 \n\nOldTestament = df[df.b < 40]\nNewTestmanet = df[df.b >= 40]\n\nwordcloud_Old = WordCloud(background_color='white',stopwords=sw).generate(str(OldTestament.t_lem.values))\nwordcloud_New = WordCloud(background_color='white',stopwords=sw).generate(str(NewTestmanet.t_lem.values))\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\nax0, ax1 = ax.flatten()\n\nax0.imshow(wordcloud_Old, aspect=\"auto\")\nax0.title.set_text('Old Testament')\nax0.axis('off')\n\nax1.imshow(wordcloud_New, aspect=\"auto\")\nax1.title.set_text('New Testament')\nax1.axis('off')\n\nplt.show()","97474539":"# Unigrams\nNT_unigrams = defaultdict(int) #New Testament\nOT_unigrams = defaultdict(int) #Old Testament\n\nfor line in NewTestmanet['t_lem']:\n    for word in generate_ngrams(line):\n        NT_unigrams[word] += 1\n        \nfor line in OldTestament['t_lem']:\n    for word in generate_ngrams(line):\n        OT_unigrams[word] += 1\n        \ndf_NT_unigrams = pd.DataFrame(sorted(NT_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_OT_unigrams = pd.DataFrame(sorted(OT_unigrams.items(), key=lambda x: x[1])[::-1])\n\nfig, axes = plt.subplots(ncols=2, figsize=(12, 10), dpi=80)\nplt.tight_layout()\n\nN = 25\n\nsns.barplot(y=df_NT_unigrams[0].values[:N], x=df_NT_unigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_OT_unigrams[0].values[:N], x=df_OT_unigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\n    \naxes[0].set_title(f'Top {N} most common unigrams in New Testament', fontsize=10)\naxes[1].set_title(f'Top {N} most common unigrams in Old Testament', fontsize=10)\n\nplt.show()","4939c817":"The frequency of bigrams, so phrases containing two words:","0eb6156f":"# Bigrams\nNT_bigrams = defaultdict(int)\nOT_bigrams = defaultdict(int)\n\nfor line in NewTestmanet['t_lem']:\n    for word in generate_ngrams(line, n_gram=2):\n        NT_bigrams[word] += 1\n        \nfor line in OldTestament['t_lem']:\n    for word in generate_ngrams(line, n_gram=2):\n        OT_bigrams[word] += 1\n        \ndf_NT_bigrams = pd.DataFrame(sorted(NT_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_OT_bigrams = pd.DataFrame(sorted(OT_bigrams.items(), key=lambda x: x[1])[::-1])\n\nfig, axes = plt.subplots(ncols=2, figsize=(12, 10), dpi=80)\nplt.tight_layout()\n\nN = 25\n\nsns.barplot(y=df_NT_bigrams[0].values[:N], x=df_NT_bigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_OT_bigrams[0].values[:N], x=df_OT_bigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\n    \naxes[0].set_title(f'Top {N} most common bigrams  in New Testament', fontsize=10)\naxes[1].set_title(f'Top {N} most common bigrams  in Old Testament', fontsize=10)\n\nplt.show()","1455ebfb":"# Trigrams\nNT_trigrams = defaultdict(int)\nOT_trigrams = defaultdict(int)\n\nfor line in NewTestmanet['t_lem']:\n    for word in generate_ngrams(line, n_gram=3):\n        NT_trigrams[word] += 1\n        \nfor line in OldTestament['t_lem']:\n    for word in generate_ngrams(line, n_gram=3):\n        OT_trigrams[word] += 1\n        \ndf_NT_trigrams = pd.DataFrame(sorted(NT_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_OT_trigrams = pd.DataFrame(sorted(OT_trigrams.items(), key=lambda x: x[1])[::-1])\n\nfig, axes = plt.subplots(ncols=2, figsize=(12, 10), dpi=80)\nplt.tight_layout()\n\nN = 25\n\nsns.barplot(y=df_NT_trigrams[0].values[:N], x=df_NT_bigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_OT_trigrams[0].values[:N], x=df_OT_bigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\n    \naxes[0].set_title(f'Top {N} most common trigrams  in New Testament', fontsize=10)\naxes[1].set_title(f'Top {N} most common trigrams  in Old Testament', fontsize=10)\n\nplt.show()","809ca73b":"vectorizer = TfidfVectorizer(max_df = 0.5, max_features = 1000)\nX = vectorizer.fit_transform(df.t_lem)\n\nsvd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\nX = svd.fit_transform(X)\n\ncluster_data = pd.DataFrame({'Comp1': X[:,0], 'Comp2': X[:,1], 'Testament': df.Testament, 'Book': df.Book, 'Period': df.Period, 'Location': df.Location, 'Time': df.Time})\ncluster_data.head()","335f0ac3":"f, axes = plt.subplots(1, 2, figsize=(18, 6))\n\nsns.scatterplot('Comp1', 'Comp2', data=cluster_data, hue='Testament', ax=axes[0], style=\"Testament\").set_title('By Testament')\nsns.scatterplot('Comp1', 'Comp2', data=cluster_data, hue='Period', ax=axes[1], style=\"Period\").set_title('By Period')","7de97fad":"sns.set(rc={'figure.figsize':(19, 19)})\nsns.scatterplot('Comp1', 'Comp2', data=cluster_data, hue='Book').set_title('By Book')\nplt.show()","67951a5b":"Target = df['Testament']\ndf['Testament relabeled'] = df['Testament']\ndf['Testament relabeled'] = df['Testament relabeled'].replace(\"New\",1)\ndf['Testament relabeled'] = df['Testament relabeled'].replace(\"Old\",0)\ndf_train,df_test,y_train,y_test = train_test_split(df,Target,test_size=0.2,random_state=0)","dbe2b0c0":"def build_vocab(X):    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\ndef check_embeddings_coverage(X, embeddings):   \n    vocab = build_vocab(X)       \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) \/ len(vocab)\n    text_coverage = (n_covered \/ (n_covered + n_oov))   \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage","8200aef0":"glove_embeddings = np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle=True)\nfasttext_embeddings = np.load('..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl', allow_pickle=True)","d2f803b9":"train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['t_lem'], glove_embeddings)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['t_lem'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n\ntrain_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(df_train['t_lem'], fasttext_embeddings)\ntest_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(df_test['t_lem'], fasttext_embeddings)\nprint('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))\nprint('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))","227d78d2":"del glove_embeddings, fasttext_embeddings, train_glove_oov, test_glove_oov, train_fasttext_oov, test_fasttext_oov\ngc.collect()","7584d621":"K = 2\nskf = StratifiedKFold(n_splits=K, random_state=123, shuffle=True)\n\nclass ClassificationReport(Callback):\n    \n    def __init__(self, train_data=(), validation_data=()):\n        super(Callback, self).__init__()\n        \n        self.X_train, self.y_train = train_data\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        \n        self.X_val, self.y_val = validation_data\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = [] \n               \n    def on_epoch_end(self, epoch, logs={}):\n        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n        self.train_precision_scores.append(train_precision)        \n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        \n        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n        self.val_precision_scores.append(val_precision)        \n        self.val_recall_scores.append(val_recall)        \n        self.val_f1_scores.append(val_f1)\n        \n        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1)) ","36bf8a88":"bert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1', trainable=True)","a85e4f41":"class TestamentDetector:\n    \n    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n        \n        # BERT and Tokenization params\n        self.bert_layer = bert_layer\n        \n        self.max_seq_length = max_seq_length        \n        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        \n        # Learning control params\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        \n        self.models = []\n        self.scores = {}\n\n    def encode(self, texts):\n                \n        all_tokens = []\n        all_masks = []\n        all_segments = []\n\n        for text in texts:\n            text = self.tokenizer.tokenize(text)\n            text = text[:self.max_seq_length - 2]\n            input_sequence = ['[CLS]'] + text + ['[SEP]']\n            pad_len = self.max_seq_length - len(input_sequence)\n\n            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n            tokens += [0] * pad_len\n            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n            segment_ids = [0] * self.max_seq_length\n\n            all_tokens.append(tokens)\n            all_masks.append(pad_masks)\n            all_segments.append(segment_ids)\n\n        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n    def build_model(self):\n        \n        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n        \n        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n        clf_output = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(clf_output)\n        \n        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        \n        return model\n    \n    def train(self, X):\n        \n        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['t_lem'], X['Book'])):\n            \n            print('\\nFold {}\\n'.format(fold))\n        \n            X_trn_encoded = self.encode(X.loc[trn_idx, 't_lem'].str.lower())\n            y_trn = X.loc[trn_idx, 'Testament relabeled']\n            X_val_encoded = self.encode(X.loc[val_idx, 't_lem'].str.lower())\n            y_val = X.loc[val_idx, 'Testament relabeled']\n        \n            # Callbacks\n            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n            \n            # Model\n            model = self.build_model()        \n            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train': {\n                    'precision': metrics.train_precision_scores,\n                    'recall': metrics.train_recall_scores,\n                    'f1': metrics.train_f1_scores                    \n                },\n                'validation': {\n                    'precision': metrics.val_precision_scores,\n                    'recall': metrics.val_recall_scores,\n                    'f1': metrics.val_f1_scores                    \n                }\n            }\n            \n    def plot_learning_curve(self):\n        \n        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n    \n        for i in range(K):\n            \n            # Classification Report curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n\n            axes[i][0].legend() \n            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n\n            # Loss curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n\n            axes[i][1].legend() \n            axes[i][1].set_title('Fold {} Train \/ Validation Loss'.format(i), fontsize=14)\n\n            for j in range(2):\n                axes[i][j].set_xlabel('Epoch', size=12)\n                axes[i][j].tick_params(axis='x', labelsize=12)\n                axes[i][j].tick_params(axis='y', labelsize=12)\n\n        plt.show()\n        \n    def predict(self, X):\n        \n        X_test_encoded = self.encode(X['t_lem'].str.lower())\n        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n\n        for model in self.models:\n            y_pred += model.predict(X_test_encoded) \/ len(self.models)\n\n        return y_pred","f8f2ec59":"clf = TestamentDetector(bert_layer, max_seq_length=128, lr=0.0001, epochs=4, batch_size=32)\n\nclf.train(df)","e6bdfad6":"clf.plot_learning_curve()","fc77db71":"y_pred = clf.predict(df_test)\ndf_test['Probabilities'] = y_pred\ndf_test['Testament_predicted'] = 0\ndf_test.loc[df_test['Probabilities'] > 0.5, 'Testament_predicted'] = 1\ndf_test.head(50)","77b0a4d1":"fpr, tpr, _ = roc_curve(df_test['Testament relabeled'], df_test.Testament_predicted)\n\nplt.title('Logistic regression ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ' ,format(round(auc(fpr,tpr),5)))","3e6081ce":"# 1.Data preparation\n\nI will load in the libraries which I need to use. I also need to download one package which is not in standard Kaggle suite.","703b0c18":"Some interesting patterns between Testaments:\n* 'god' got much smaller\n* 'jesus' is big in New Testament, together with 'Chirst'. First word is used mostly in Gospels, second one in 'Letters of Apostoles'\n* word 'face' and 'earth' are often used in Old Testament, this comes from Persian circle of culture\n* New Testament is full of names, it is more descriptive, it gives infromation about certain stories with actual characters. For example: 'jacob' (disciple) or thamar (woman close to Jesus)\n* 'void' was the big word in Old Testment as contrary to 'earth'. It depicts ancient view of Israeli people on the world\n* pay attention to one peculiarity: 'abraham' appears more in New Testament than in old. Why? People are reffered as 'children of abraham', so all people are the center on this Testament attention. In Old Testament this role is taken by 'moses', so only Israeli people are relevant for the history\n* next 'david' appears a lot in New Testament, because Jesus origins are of interest, Jesus is often reffered as 'son of David'\n* characteristic for Bible word 'amen' appears almost only exclusively in New Testament. This id due to Koine Greek language\n* the center of Old Testament topics are 'judgments','servant','law','statutes','curse'. This highly evolves to 'man','life','generation','grace'. These tags cover how the message of the books evolves along the centuries\n\nIf you observe another interesting trends, please share them at the end of the notebook.\n\n**2.3.Frequency plotting**\n\nNext, I analyse the frequency of occurence for different words in the given list:","09beb1e5":"Let's define the variable which will split the data set in New and Old Testament. To give some background: Bible is divided into two parts:\n* after Jesus: it's New Testament with stories about Jesus (gospels) and some other about disciplies \n* before Jesus: Old Testament with the hisotry of Israel and its people","59bd03b8":"Let's plot the learning curves and our metrics along the epochs:","44c31e28":"I observe how these clusters look by 'Testament' and by historical 'Period':","ea85ab7b":"# Introduction\n\nThe goal fo this notebook is to analyze the text of Bible by using couple of machine & deep learning methods. I am passionate about two topics especially: **how the nature of the text evolved along the years & whether we can reasonably differentiate between language of Old and New Testmaent**. Why the topic is fascinating? The Bible is the text which contains passages from very old times (between XVIII - XV centuries BC), these texts are the product of common culture of Middle East. The perfect example is a history about Great Flood, which is present in different cultures of region: in Sumerian Epic about Gilgamesh, Aramaic creation mith and Egyptian manuscripts. Second, **the Bible is the text which was created by common effort almost 1100 years**, first books which survived to our times were inscribed around VIII century BC, the last ones around the beginning of II century.\n\nPlease follow me to my investigation.","e09d32d4":"I define the words which I don't find interesting for the analysis. They are called 'stop words', this list contains mainly words like 'up', 'to', 'with' etc, for example:","b77db055":"Below the functions are applied. Following algorithms are applied:\n* lowering the case for the text\n* interpunction cleaning\n* lemmatization\n* stemming\n* stemming of lemmatised results","951c7ce2":"Now, I enrich above data set with infromation from auxiliary data set:","39b3ae6d":"# 2.Data analysis\n\nWith this given data set, I have the comfort to apply complete analysis. First, I am starting with interesting topic: how the verse length evolves along the Bible's books. Why this information can be useful? We can more less estimate when the given books were written but it's harder to understand what was the given background and the lcoation of the book. The length of the verse gives us a lot of knowledge about the underlying culture and background behind it. ","21cd40f1":"At the end the classic ROC curve:","78118b59":"**2.1.One way-graphs**","8d383d63":"I predict the reuslts, please look random 50 results. Look at 'relabeled' versus 'predictied':","a36314c1":"I load two algorithms. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.","eaaff2b0":"Further, three blocks can be seen. In the first cross valdiation is defined. In second BERT himself and third one contains class defined set of function for this text analysis. The most important question is: what is BERT? There are couple of answers:\n* [towardsdatascience](https:\/\/towardsdatascience.com\/bert-for-dummies-step-by-step-tutorial-fb90890ffe03)\n* [medium](https:\/\/medium.com\/huggingface\/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)\n* and of course: [wiki](https:\/\/en.wikipedia.org\/wiki\/BERT_(language_model))","fde3de51":"Some of the functions useful for the analysis.","8917d1ac":"For Testaments, New one has much less extreme values and is mainly centered around the middle. Namely, a lot of text structures, which were present in Old one, are not present in New one.\n\nFurtmermore, I can look at this cluster also by book, but the results are confusing:","63fc7858":"First, please acknowledge that BERT application was taken from: [this great NLP guide](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert). If you like my notebook, please upvote also the effort of this Kaggler.\n\n**Second, if you are interested only in the results but not technical application then just don't unhide the code and go with me through graphs and tables. If second one holds, I also discuss technical topics.** Regarding technical topics, please find short list of methods I will use:\n* data enrichment - I introduce auxiliary data set with information about Biblical books\n* cleaning - regular text cleaning with removal of interpuction\n* stemming - I universalize the words minimizing them to just a stem\n* lemmatization - grouping together the inflected forms of a word\n* word clouds - plotting different parts of Bible and analysis of most frequent words\n* unigrams, digrams, trigrams - the most frequent words, two-words, three-words phrases are plotted\n* embedding - collective name for a set of language modeling and feature learning techniques\n* BERT - I apply powerful and well-known model for this piece of text","7c17284a":"# 4.Embedding\n\nI will analyse the vocabulary present in the Bible and check how much can be covered by our external dictionaries. First, I label 'New Testament' as '1' and 'Old' one as '0'.","2656e963":"And the frequencies of trigrams:","120ee1c1":"Let's compare what words were the most frequent in Old & New Testaments:","fcaeea09":"Fascinating to observe tthe difference between BC Bible and AD Bible. It's important to know that New Testament was written in so-called 'Koine Greek', the number of words in each verse is much higher than in Hebrew or Aramaic. And to fullfil our correlation huner, Spearman presents his best:","27a80ec6":"Ok, this enriched data is ready to go!","c4b241a1":"**2.2.Word clouds**\n\nNext, I look at 4 word clouds. Word cloud is the set of the most frequently appearing words in the form of cloud. I look how my text has changed by application of different methods mentined before:\n* plain text\n* lemmatization\n* stemming\n* stemming of lemmatised results","5c0039d2":"# 6.What's next\n\nWe see that using NLP we can understand much more than we thought about the Bible. Without using archeology, deciphering old manusricpts and traveling to excavation in Asia Minor, we can see how Bible evolved in its 1100 years history gathering the linguistic treasures of more than one milennium. This simple notebook is just a sample what we can do with the existing algortihms.\n\n**If you like my work, please upvote this notebook.**","55e8b779":"I apply these algorithms to Bible. Performance of these algorithms:","52823008":"And then, the function from the class is applied. What is happening? BERT is applied with full cross validation on the basis of the given epoch input.","9dbb2916":"![image.png](attachment:image.png)","62c43d7f":"Not bad, they cover approximately 98% of the text already. Next, I delete some files to make space.","0a13d76a":"**2.4.Clustering and SVD**\n\nI use vectorizer and singular value decomposition to create text clusters and merge them together with the data:","4129f8c7":"# 5.Estimation\n\nThis is the most interesting part of this notebook. I will apply the algorithm which will tell you whther given verse belongs to Old or New Testament. For example, I will give him the 20 words verse from Gospel and the algorithm will tell me what is the probability that it is the part of the given Testament. I use the methods from the notebook mentioned at the beginning of this file. ","21b3cc9b":"The result:","3a6fbf82":"Second, the auxiliary data set with the information about the books of the Bible:","a70fbb73":"Next, two data sets are loaded. First, the Bible text, where each record corresponds to one verse.","00d54e24":"Next, I define some functions:"}}