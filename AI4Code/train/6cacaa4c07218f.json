{"cell_type":{"cd5c7210":"code","27713677":"code","46e36ddf":"code","b26c08ef":"code","b829cb8c":"code","4732c0c8":"code","c60454a7":"code","a67f419d":"code","1ce10c03":"code","dca04f22":"code","a94cfd0e":"code","9c346c51":"code","d463d547":"code","aacd8a88":"code","b4276c9c":"code","08a4ee13":"markdown","1ab79a71":"markdown","73179a8a":"markdown","12113615":"markdown","dc771b44":"markdown","4f8d1d86":"markdown","3d51921c":"markdown","e7dc0160":"markdown","c9e957d9":"markdown","6b4d4881":"markdown"},"source":{"cd5c7210":"import gensim","27713677":"from gensim.models import KeyedVectors","46e36ddf":"!ls","b26c08ef":"!wget https:\/\/dl.fbaipublicfiles.com\/fasttext\/vectors-english\/wiki-news-300d-1M.vec.zip","b829cb8c":"!unzip wiki-news-300d-1M.vec.zip","4732c0c8":"!ls","c60454a7":"# loading the model\nmodel = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')","a67f419d":"model['hello'].shape","1ce10c03":"model['hello'][:50]","dca04f22":"print(f'Similarity between night and nights: {model.similarity(\"night\", \"nights\")}')\nprint(f'Similarity between reb and blue: {model.similarity(\"red\", \"blue\")}')\nprint(f'Similarity between hello and heyy: {model.similarity(\"hello\", \"heyy\")}')\nprint(f'Similarity between king and queen: {model.similarity(\"king\", \"queen\")}')\nprint(f'Similarity between london and moscow: {model.similarity(\"london\", \"moscow\")}')\nprint(f'Similarity between car and bike: {model.similarity(\"car\", \"bike\")}')","a94cfd0e":"similar = model.most_similar(\"january\")\nfor i in similar:\n    print(i)","9c346c51":"print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))","d463d547":"model.most_similar(positive=[\"women\", \"king\"], negative=[\"queen\"])","aacd8a88":"def analogy(x1, x2, y1):\n    result = model.most_similar(positive=[y1, x2], negative=[x1])\n    return result[0][0]","b4276c9c":"analogy('japan', 'japanese', 'china')","08a4ee13":"### Analogy difference\n\nWhich word is to women as king is to queen?","1ab79a71":"# Gensim\n\nGensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n\nGensim includes streamed parallelized implementations of fastText,word2vec and doc2vec algorithms, as well as latent semantic analysis (LSA, LSI, SVD), non-negative matrix factorization (NMF), latent Dirichlet allocation (LDA), tf-idf and random projections. [source](https:\/\/en.wikipedia.org\/wiki\/Gensim)\n\nReferences:\n\n- [How to download Pretrained word embeddings in gensim](https:\/\/radimrehurek.com\/gensim\/auto_examples\/howtos\/run_downloader_api.html)\n\n- [Using Fasttext in gensim](https:\/\/radimrehurek.com\/gensim\/auto_examples\/tutorials\/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py)","73179a8a":"### Odd-One-Out\n\nHere, we ask our model to give us the word that does not belong to the list!","12113615":"## Resources\n\n- [Word Embeddings - Sebastian Ruder](https:\/\/ruder.io\/word-embeddings-1\/)\n- [Skip Gram Model - Chris McCormick](http:\/\/mccormickml.com\/2016\/04\/19\/word2vec-tutorial-the-skip-gram-model\/)\n- [Learning Word Embeddings Andrew NG](https:\/\/www.youtube.com\/watch?v=xtPXjvwCt64)\n- [Word2Vec Andrew NG](https:\/\/www.youtube.com\/watch?v=jak0sKPoKu8)\n- [Stanford NLP Lecture 1](https:\/\/www.youtube.com\/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=1)\n- [Word2Vec Paper](https:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n- [Google Word2Vec](https:\/\/code.google.com\/archive\/p\/word2vec\/)\n","dc771b44":"### Word Similarity\n\nHere, we will see how similar are two words to each other ","4f8d1d86":"### Loading the Pretrained Word2Vec model\n\nI am using the model downloaded from fasttext. You can download any of the word vectors provided by fasttext [here](https:\/\/fasttext.cc\/docs\/en\/english-vectors.html).\n\nFasttext provides pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.","3d51921c":"### Initial Setup","e7dc0160":"# Word2Vec\n\nWord2Vec is one of the most popular pretrained word embeddings developed by Google. Word2Vec is trained on the Google News dataset (about 100 billion words).\n\nThe architecture of Word2Vec is really simple. It\u2019s a feed-forward neural network with just one hidden layer. Hence, it is sometimes referred to as a Shallow Neural Network architecture.\n\nDepending on the way the embeddings are learned, Word2Vec is classified into two approaches:\n\n- Continuous Bag-of-Words (CBOW)\n- Skip-gram model\n\nContinuous Bag-of-Words (CBOW) model learns the focus word given the neighboring words whereas the Skip-gram model learns the neighboring words given the focus word. \n\n![word2vec](https:\/\/drive.google.com\/uc?id=1oqfxoxUK5HT6QEE6nUlr7tUICbptfWeC)\n\nThere are a lot of online material available to explain the concept about Word Embeddings. I can't explain any better than that. So my focus here will be on, how to use pre-trained word embeddings. I will provide relevant resources to look into more details.","c9e957d9":"There are many libraries available which support Word2Vec based models natively. Pretrained models are also available. Covering each and everything will be overwhelming. So I will provide the usage with the prominent library:\n- [Gensim](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html)\n","6b4d4881":"### Most Similar Words\n\nHere, we will ask our model to find the words which are most similar"}}