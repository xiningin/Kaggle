{"cell_type":{"23d2db4e":"code","c3dc5ece":"code","63db1456":"code","3f702c28":"code","b38f7cf0":"code","d6ee42aa":"code","7c0a5f51":"code","bff9588a":"code","4f962132":"code","5a4cab30":"code","5106799a":"code","6631cb65":"code","caf3c9af":"code","05fd64c4":"code","4f06a1bb":"code","78fddbb5":"code","5fc540fb":"code","3faca3a1":"code","eb808bf5":"code","252599a6":"code","c27e8fc8":"code","5c39cc8c":"code","2213dca2":"code","3615d9a4":"code","93cc9179":"code","79c8d32f":"code","d7922107":"code","fee01101":"code","c2d12a89":"code","cc9fabde":"code","577e8c2c":"code","901f4db9":"code","e8e6e5d6":"code","d3d8ab1c":"code","e89966cf":"code","0906b5b1":"code","be004f33":"code","a9e0c354":"code","5884c028":"code","2bd9e005":"code","1bc5b634":"code","663e98bc":"code","341edcae":"code","0e2b3c99":"code","cfef9e64":"code","5634b68e":"code","85631042":"code","47163eaa":"code","f72e8602":"code","f1581a19":"code","3ae8d1a5":"code","ca7974f2":"code","763dff32":"code","93b911b3":"code","8d5bd105":"code","34168944":"code","48303527":"code","18fdc9e3":"code","c40407f9":"code","85f91d90":"code","66396282":"code","01a0054b":"code","f0f129c7":"code","8c96bd13":"code","3c969763":"code","9735f3ff":"code","d7f3bad2":"code","1f306fa6":"code","4da22af2":"code","4d54d99d":"code","9164160b":"code","ad494580":"code","73de0af5":"code","7dfd7ae5":"code","e2f8e3d3":"code","d15e96ea":"code","3489ee00":"code","cc319129":"markdown","be091f4c":"markdown","034e3c38":"markdown","03792959":"markdown","dbfa7dd7":"markdown","c07a3cde":"markdown","77d7c229":"markdown","c7fa5ca0":"markdown","0a9d26c3":"markdown","ba8e0919":"markdown","913965cc":"markdown","624abc71":"markdown","5f576e70":"markdown","2e3f784d":"markdown","3d2f416f":"markdown","a370adf9":"markdown"},"source":{"23d2db4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3dc5ece":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","63db1456":"df=pd.read_csv('\/kaggle\/input\/airplane-accidents-severity-dataset\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/airplane-accidents-severity-dataset\/test.csv')","3f702c28":"df.head()","b38f7cf0":"df_test.head()","d6ee42aa":"# check the null values\n# we don't have any null values\ndf.isnull().sum()","7c0a5f51":"df.dtypes","bff9588a":"df.info()","4f962132":"df.describe().transpose()","5a4cab30":"df['Severity'].value_counts()","5106799a":"plt.figure(figsize=(12,8))\ndf['Severity'].value_counts().plot(kind='bar',color='red')","6631cb65":"plt.figure(figsize=(12,8))\norder=sorted(df['Severity'].unique())\nchart=sns.countplot(x='Severity',data=df,order=order)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)","caf3c9af":"# So Highly_Fatal_And_Damaging are in highest percentage amomgst all the damage\n(df['Severity'].value_counts())\/len(df['Severity'])*100","05fd64c4":"df['class']=df['Severity'].map({'Highly_Fatal_And_Damaging':0,'Significant_Damage_And_Serious_Injuries':1,'Minor_Damage_And_Injuries':2,'Significant_Damage_And_Fatalities':3})","4f06a1bb":"df.head()['class']","78fddbb5":"#Finding the correlation to target variable\ndf.corr()['class'].sort_values(ascending=False)","5fc540fb":"plt.figure(figsize=(12,8))\nsns.heatmap(df.corr(),cmap='coolwarm')","3faca3a1":"plt.figure(figsize=(12,6))\ndf.corr()['class'].sort_values(ascending=False).plot(kind='bar',color='red')\n# safety_score is highly correlated with the Severity","eb808bf5":"plt.figure(figsize=(12,6))\nsns.distplot(df['Safety_Score'],kde=False,bins=100)","252599a6":"# from here we can see that Highly_Fatal_And_Damaging severity having lower Safety_Score as expected.\ndf.groupby('Severity').mean()['Safety_Score'].sort_values(ascending=False)","c27e8fc8":"plt.figure(figsize=(12,6))\ndf.groupby('Severity').mean()['Safety_Score'].sort_values(ascending=False).plot(kind='bar',color='pink')","5c39cc8c":"df[df['Turbulence_In_gforces']==df['Turbulence_In_gforces'].max()]['Severity'].value_counts()","2213dca2":"df[df['Turbulence_In_gforces']==df['Turbulence_In_gforces'].min()]['Severity'].value_counts()","3615d9a4":"plt.figure(figsize=(12,8))\nchart=sns.boxplot(x='Severity',y='Total_Safety_Complaints',data=df)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)\n# Highly_Fatal_And_Damaging have highest no of complaints","93cc9179":"df['Accident_Type_Code'].value_counts().sort_values(ascending=False)","79c8d32f":"plt.figure(figsize=(12,12))\nchart=sns.boxplot(x='Severity',y='Total_Safety_Complaints',data=df,hue='Accident_Type_Code')\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)","d7922107":"df['Violations'].value_counts().sort_values(ascending=False)\n# so maximum violations are done of type 2","fee01101":"# to see which type of violation results in which type of severity\nplt.figure(figsize=(12,12))\nchart=sns.barplot(x='Severity',y='Turbulence_In_gforces',data=df,hue='Violations')\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)","c2d12a89":"# here we can see that as the turbulance increases the control of the airplane decreases.\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='Turbulence_In_gforces',y='Control_Metric',data=df)\n","cc9fabde":"# Highly_Fatal_And_Damaging have less control metric as compared with others \nplt.figure(figsize=(12,8))\nchart=sns.barplot(x='Severity',y='Control_Metric',data=df)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)","577e8c2c":"#Highly_Fatal_And_Damaging has maximum elevation\nplt.figure(figsize=(12,8))\nchart=sns.barplot(x='Severity',y='Max_Elevation',data=df)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)","901f4db9":"# here we can see that in adverse weather metric there is highly fatal and damaging\nplt.figure(figsize=(12,8))\nchart=sns.boxplot(x='Severity',y='Adverse_Weather_Metric',data=df)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)","e8e6e5d6":"df.groupby('Severity').mean()['Cabin_Temperature']","d3d8ab1c":"#Cabin_Temperature doe snot have any effect on severity as all have about same temperature \ndf.groupby('Severity').mean()['Cabin_Temperature'].plot(kind='bar')","e89966cf":"# 2nd type of violations are mostly done \nsns.countplot(df['Violations'])","0906b5b1":"df.drop('Severity',axis=1,inplace=True)\ndf.head()","be004f33":"X=df.drop('class',axis=1)\ny=df['class']","a9e0c354":"X.head()","5884c028":"y.head()","2bd9e005":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","1bc5b634":"\nX=StandardScaler().fit(X).transform(X.astype(float))","663e98bc":"X","341edcae":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\nprint('size of train set',X_train.shape,y_train.shape)\nprint('size of test set',X_test.shape,y_test.shape)","0e2b3c99":"from sklearn.neighbors import KNeighborsClassifier","cfef9e64":"\nk=4\nmodel=KNeighborsClassifier(n_neighbors=k)","5634b68e":"model.fit(X_train,y_train)","85631042":"pred=model.predict(X_test)","47163eaa":"pred","f72e8602":"\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report,log_loss","f1581a19":"print(classification_report(pred,y_test))","3ae8d1a5":"\nk_val=20\nmean_acc=np.zeros((k_val-1))\nfor n in range(1,k_val):\n    model1=KNeighborsClassifier(n_neighbors=n)\n    model1.fit(X_train,y_train)\n    pred=model1.predict(X_test)\n    mean_acc[n-1]=accuracy_score(pred,y_test)\n\n\nmean_acc","ca7974f2":"p=np.arange(1,k_val)\nplt.style.use('ggplot')\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(12,8))\n    plt.plot(p,mean_acc,marker='o', markerfacecolor='red', linestyle='dashed', color='green', markersize=10)\n    plt.legend(('Accuracy ', '+\/- 3xstd'))\n    plt.ylabel('Accuracy ')\n    plt.xlabel('Number of Nabors (K)')\n    plt.tight_layout()\nplt.show()","763dff32":"print(\"best accuracy is\",mean_acc.max(),'for k value=',mean_acc.argmax())","93b911b3":"from sklearn import svm","8d5bd105":"clf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train)","34168944":"yhat = clf.predict(X_test)","48303527":"# SVM has perform better than KNN\nprint(classification_report(yhat,y_test))","18fdc9e3":"confusion_matrix(yhat,y_test)","c40407f9":"# import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# instantiate the classifier \nrfc = RandomForestClassifier(random_state=0)\n\n# fit the model\nrfc.fit(X_train, y_train)\n\n# Predict the Test set results\ny_pred = rfc.predict(X_test)\n\nprint('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","85f91d90":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint","66396282":"model=RandomForestClassifier(n_jobs=-1)","01a0054b":"parameters={'max_depth':[3,5,7,10,None],\n           'n_estimators':[100,200,300,400,500],\n           'max_features':randint(1,13),\n           'criterion':['gini','entropy'],\n           'bootstrap':[True,False],\n           'min_samples_leaf':randint(1,5)}","f0f129c7":"def hyperparameter_tuning(model,parameters,n_of_itern,X_train,y_train):\n    random_search=RandomizedSearchCV(estimator=model,\n                                    param_distributions=parameters,\n                                    n_jobs=-1,\n                                     n_iter=n_of_itern,\n                                     cv=9)\n    random_search.fit(X_train,y_train)\n    params=random_search.best_params_\n    score=random_search.best_score_\n    return params,score","8c96bd13":"final_params,final_score=hyperparameter_tuning(model,parameters,40,X_train,y_train)","3c969763":"#this is our final best parameters for random forest classifier\nfinal_params","9735f3ff":"# final accuracy with tuned parameters\nfinal_score","d7f3bad2":"# import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# instantiate the classifier \nrfc = RandomForestClassifier(n_estimators=300,\n                             criterion='entropy',\n                             max_depth=None,\n                             max_features=7,\n                             min_samples_leaf=2,\n                             bootstrap=False\n                             )\n                            \n\n# fit the model\nrfc.fit(X_train, y_train)\n\n# Predict the Test set results\ny_pred = rfc.predict(X_test)\n\nprint('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","1f306fa6":"print(classification_report(y_pred,y_test))","4da22af2":"print(confusion_matrix(y_pred,y_test))","4d54d99d":"from sklearn.linear_model import LogisticRegression","9164160b":"log_reg=LogisticRegression()\nlog_reg.fit(X_train,y_train)\nlog_predict=log_reg.predict(X_test)","ad494580":"print(classification_report(log_predict,y_test))","73de0af5":"print(confusion_matrix(log_predict,y_test))","7dfd7ae5":"data_dict={'Algorithms':['KNN', 'SVM', 'RandomForest', 'logistic regression'],'accuracy':[0.676,0.86,0.91,0.61],'accuracy_after_hyperparameter tuning':['-',0.89,0.95,'-']}\naccuracy_df=pd.DataFrame(data_dict)","e2f8e3d3":"accuracy_df.set_index('Algorithms',inplace=True)","d15e96ea":"accuracy_df","3489ee00":"prediction_test=rfc.predict(X_test)\nprediction_test","cc319129":"## **2.SVM**","be091f4c":"#  As we have made a new(integer) column for Severity so we will drop severity","034e3c38":"# Data Preprocessing","03792959":"**1.KNN Classifier**","dbfa7dd7":"# Data Wrangling","c07a3cde":"# Let us map the values of Severity to numerical","77d7c229":"**Choosing the best value of k for KNN**","c7fa5ca0":"# **Model Building**","0a9d26c3":"## Standardisation ","ba8e0919":"# **DataFrame for all the algorithms and their accuracy**","913965cc":"# **3.Random Forest Classifier**","624abc71":"# **Building the model again**","5f576e70":"# **Tuned parameters are:**","2e3f784d":"# splitting the data","3d2f416f":"# **HyperParameter Tuning using RandomizedSearchCV**","a370adf9":"# **4.Logistic Regresssion**"}}