{"cell_type":{"08afbd3b":"code","a6149a69":"code","1a47c086":"code","c2cca4cd":"code","c8b6fb48":"code","f0ffb781":"code","a08ec92e":"code","d0dc180a":"code","5ffda040":"code","43f18907":"code","ac462ed1":"code","1814c856":"code","b714f11a":"code","1abe1d17":"code","c8e4cb76":"code","0ba6f4dc":"code","1f58fe31":"code","621b15ea":"code","58d0b23d":"code","95b36796":"code","e62e81f8":"code","d787413d":"code","2408d253":"code","fc12f3e7":"code","e499fabf":"code","f27d56c6":"code","5e01a223":"code","3bb5ef14":"code","c47c7201":"code","b310236e":"code","cfc8fb65":"code","06985fc8":"code","108e1f0f":"code","8aedb91e":"code","c29feae1":"code","ed624a04":"code","696bc2e6":"code","4cb30ab5":"code","15b27af5":"code","04008f34":"code","f36b1fca":"code","006cc06e":"code","94e9bbca":"code","20ec0bba":"code","3c5a5eb9":"code","ab2ed941":"code","9e7bafda":"code","d55670d3":"code","3784dc9e":"code","b7529551":"code","2bd56b58":"code","230c5ac8":"code","8d0c5647":"code","bb8359f3":"code","9dcf5eb5":"code","f1021bb2":"code","6bf5e968":"code","12ee54e1":"code","6028000b":"code","56745832":"code","92373593":"code","053fd92d":"code","27147df2":"code","38a31a05":"code","d806f976":"code","ecf31327":"code","ad61bb0d":"code","4d173d61":"code","8ec40513":"code","011cb0e6":"code","6313fb2f":"code","f746c481":"code","80d03666":"code","050f78ee":"code","e2f5aa1c":"code","04078d6b":"code","857e2ee3":"code","a7d3b239":"code","479a6a70":"code","be703526":"code","05d784b7":"code","ca7802ae":"code","5518b441":"code","86fb9657":"code","c40b4ee5":"code","b3843dd3":"code","491613c7":"code","acc665da":"code","5fa15bbf":"code","6726773d":"code","a4a9e05a":"code","8e805627":"code","a6a59b3f":"code","c80090c8":"code","c579da8a":"code","472c1c2b":"code","79e92394":"code","c9b038c8":"code","7864ba90":"code","65fcfa30":"code","6f539353":"code","dca90332":"code","128e4629":"code","00f7c2d3":"code","4f3a5488":"code","004f7086":"code","faf7f6d1":"code","e5e16656":"code","719c1ccb":"code","cc325f48":"code","f6c25bed":"code","7c5869ea":"code","f9d22b40":"code","7a5621a8":"code","2b9e6b85":"code","c79a73f2":"code","f39c8fd3":"code","be5306ea":"code","8eedf14e":"code","6924d937":"code","0bb138a1":"code","cfed3a1b":"code","4d1c482d":"code","297a00d5":"code","6929aa03":"code","fda3e4a6":"code","778a6dc8":"code","930bbefb":"code","a55e02ad":"code","03cdecae":"code","e0bcd88b":"code","ac9beedf":"code","47fa9a4a":"code","739abc4a":"code","f1440ee4":"code","65d0ccff":"code","b56535b4":"code","0a424a25":"code","58ed3552":"code","ae8099ea":"code","fde1105f":"code","1af9681e":"code","a46d5b51":"code","6dd3d157":"code","db54e71f":"code","722ba203":"code","5d07392d":"code","409db746":"code","aa403883":"code","2373f0d4":"code","a9a44e9e":"code","df8549b3":"markdown","5c7ce6fe":"markdown","c09a3149":"markdown","44a25e84":"markdown","472b380c":"markdown","0b7c9de2":"markdown","b2ca878a":"markdown","f81f63ea":"markdown","fd18831f":"markdown","a6471f7e":"markdown","c9e326a0":"markdown","2339ec8a":"markdown","e780bf62":"markdown","b480869d":"markdown","9403fd0f":"markdown","f381e7b9":"markdown","e66b4e45":"markdown","c4bd9f63":"markdown","286396cf":"markdown","26322beb":"markdown","dd146ba1":"markdown","3637276b":"markdown","ebf078ec":"markdown","2b723671":"markdown","40e8581f":"markdown","2785c1a8":"markdown","d6f5bad9":"markdown","2bb62f76":"markdown","88de80dd":"markdown","632c8beb":"markdown","a4b152f3":"markdown","6f6dba89":"markdown","34381e5b":"markdown","fe0a8129":"markdown","d97b6cf7":"markdown","0721a2a0":"markdown","9a5e82cd":"markdown","fa051fd8":"markdown","7927fc60":"markdown","109c310b":"markdown","458e806f":"markdown","0e6231ed":"markdown","8c3632e8":"markdown"},"source":{"08afbd3b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","a6149a69":"df=pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")","1a47c086":"df.head()","c2cca4cd":"df.info()","c8b6fb48":"df.drop(['Unnamed: 0'], axis = 1, inplace = True) #remove unnecessary columns from data\n","f0ffb781":"#checking for duplicated entries\nduplicates = df.duplicated().sum() \nif  duplicates == 0:\n    print(\"There are no duplicted rows in this data\")\nelse:\n    print('There are: ', str(duplicates), \" duplicates\")","a08ec92e":"df=df.drop_duplicates()","d0dc180a":"df[\"Review Text\"].isnull().sum()","5ffda040":"df.dropna(subset=['Review Text'], inplace= True)\n","43f18907":"df[\"Review Text\"].isnull().sum()","ac462ed1":"df[\"Recommended IND\"].isnull().sum()","1814c856":"df.info()","b714f11a":"df[\"Recommended IND\"].value_counts()","1abe1d17":"plt.figure(figsize = (15, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x = df['Recommended IND'], data = df)\n\nplt.subplot(1, 2, 2)\nplt.pie(x= df['Recommended IND'].value_counts(), labels = (\"1\",\"0\"), explode = (0, 0.08), shadow = True, autopct = '%1.2f%%')\n\nplt.show()","c8e4cb76":"from sklearn.utils import resample\n\n#create two different dataframe of majority and minority class \ndf_majority = df[(df['Recommended IND']==1)] \ndf_minority = df[(df['Recommended IND']==0)] \n\n# upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,    # sample with replacement\n                                 n_samples=15539 , # to match majority class\n                                 random_state=42)  # reproducible results\n                                 \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_minority_upsampled, df_majority])","0ba6f4dc":"df_upsampled['Recommended IND'].value_counts","1f58fe31":"plt.figure(figsize = (15, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x = df_upsampled['Recommended IND'], data = df)\n\nplt.subplot(1, 2, 2)\nplt.pie(x= df_upsampled['Recommended IND'].value_counts(), labels = (\"1\",\"0\"), explode = (0, 0.08), shadow = True, autopct = '%1.2f%%')\n\nplt.show()","621b15ea":"df[\"Rating\"].value_counts()","58d0b23d":"round(df['Rating'].value_counts()\/df.shape[0]*100).plot.pie(autopct='%1.1f') ","95b36796":"df[\"Division Name\"].value_counts()","e62e81f8":"plt.figure(figsize = (15, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x = df['Division Name'], data = df)\n\nplt.subplot(1, 2, 2)\n\nplt.pie(x= df['Division Name'].value_counts(),labels=('General','General Petite',\"Intimates\") ,explode = (0, 0.05,0.08), shadow = True, autopct = '%1.2f%%')\n\nplt.show()","d787413d":"df[\"Department Name\"].value_counts()","2408d253":"plt.figure(figsize = (15, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x = df['Department Name'], data = df)\n\nplt.subplot(1, 2, 2)\nplt.pie(x= df['Department Name'].value_counts(),labels=('Tops','Dresses',\"Bottoms\",\"Intimate\",\"Jackets\",\"Trend\") ,explode = (0, 0.08, 0.01,0.02,0.2,0.2), shadow = True, autopct = '%1.2f%%')\n\nplt.show()","fc12f3e7":"plt.figure(figsize = (15, 6))\nplt.subplot(1, 2, 1)\nplt.title = ('Age_distribution')\nsns.distplot(df['Age']).set_title(\"Age Distribution\")","e499fabf":"plt.figure(figsize = (15, 6))\nplt.subplot(1, 2, 1)\nplt.title = ('Positive Feedback Count')\nsns.distplot(df['Positive Feedback Count']).set_title(\"Positive Feedback Count dist\")","f27d56c6":"sns.catplot(x=\"Department Name\", y=\"Age\", hue=\"Recommended IND\",data= df)","5e01a223":"sns.catplot(x=\"Rating\", y=\"Age\", hue=\"Recommended IND\",data= df)","3bb5ef14":"df.info()","c47c7201":"df.drop([ \"Clothing ID\",\"Age\",\"Title\",\"Rating\",\"Positive Feedback Count\",\"Division Name\",\"Department Name\",\"Class Name\"], axis = 1, inplace = True) #remove unnecessary columns from data\n","b310236e":"df.info()","cfc8fb65":"df.head()","06985fc8":"df[\"Review Text\"].isnull().value_counts()","108e1f0f":"df[\"Recommended IND\"].isnull().value_counts()","8aedb91e":"import nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nnltk.download(\"stopwords\")\nstop_words=stopwords.words(\"english\")\nnew_stopping_words = stop_words[:len(stop_words)-36]\nnew_stopping_words.remove(\"not\")\nnltk.download('punkt')","c29feae1":"def remove_punc(df):\n    new_text= re.sub(\"n't\",'not', df)\n    new_text= re.sub('[^\\w\\s]','', df)\n    return new_text","ed624a04":"processed_text=remove_punc(str(df[\"Review Text\"]))\nprocessed_text","696bc2e6":"def tokenizze(df):\n    newdata= word_tokenize(df)\n    return newdata","4cb30ab5":"tokenized_data=tokenizze(processed_text.lower())\ntokenized_data","15b27af5":"def remove_num(df):\n    text_without_num=[w for w in df if w.isalpha()]\n    return text_without_num","04008f34":"textwithoutnum= remove_num(tokenized_data)\ntextwithoutnum","f36b1fca":"def remove_stops(df):\n    \n    newdata=[t for t in df if t not in new_stopping_words]\n    return newdata","006cc06e":"data=remove_stops(textwithoutnum)\ndata","94e9bbca":"def lemmatizze(df):\n\n    \n    newdata= [WordNetLemmatizer().lemmatize(t) for t in df]\n    return newdata","20ec0bba":"final_data=lemmatizze(data)\nfinal_data","3c5a5eb9":"joined_data=\" \".join(final_data)","ab2ed941":"joined_data","9e7bafda":"def Cleaning_process(df):\n    processed_text=remove_punc(str(df))\n    tokenized_data=tokenizze(processed_text.lower())\n    textwithoutnum= remove_num(tokenized_data)\n    data=remove_stops(textwithoutnum)\n    final_data=lemmatizze(data)\n    return \" \".join(final_data)\n\n    ","d55670d3":"text = Cleaning_process(str(df[\"Review Text\"]))\ntext","3784dc9e":"df[\"Review Text\"]= df[\"Review Text\"].apply(Cleaning_process)","b7529551":"\ndf[\"Review Text\"].head()","2bd56b58":"df.head()","230c5ac8":"#importing \nfrom sklearn.model_selection import train_test_split\n#from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF are word frequency scores that try to highlight words that are more interesting,","8d0c5647":"X = df_upsampled[\"Review Text\"]\ny= df_upsampled[\"Recommended IND\"]","bb8359f3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=101)","9dcf5eb5":"#this is count vectorization methode but it wont work good with our situation \n\n#vectorizer = CountVectorizer()\n\n#X_train_count = vectorizer.fit_transform(X_train)\n#X_test_count = vectorizer.transform(X_test)\n\n#type(X_train_count)\n#X_train_count.toarray()\n#vectorizer.get_feature_names()\n#pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names())\n","f1021bb2":"X_train.shape , X_test.shape","6bf5e968":"tf_idf_vectorizer = TfidfVectorizer()\n\nX_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\nX_test_tf_idf = tf_idf_vectorizer.transform(X_test)","12ee54e1":"X_test_tf_idf.toarray().shape","6028000b":"X_train_tf_idf.toarray().shape","56745832":"X_train_tf_idf.toarray()","92373593":"pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())","053fd92d":"#import models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n","27147df2":"#importing validation models\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import  f1_score\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix,ConfusionMatrixDisplay","38a31a05":"def classification_task( model,X_train_scaled, y_train ,X_test_scaled ,y_test, predic,model_name):\n \n    perf_df=pd.DataFrame({'Train_Score':model.score(X_train_scaled,y_train),\"Test_Score\":model.score(X_test_scaled,y_test),\n                       \"Precision_Score\":precision_score(y_test,predic),\"Recall_Score\":recall_score(y_test,predic),\n                       \"F1_Score\":f1_score(y_test,predic) , \"accuracy\":accuracy_score(y_test,predic)}, index=[model_name])\n    return perf_df","d806f976":"# Train the Logistic Regression model on the scaled data and print the model score\nlr = LogisticRegression()\n# Fit the model\nlr.fit(X_train_tf_idf, y_train)\n#predict x_test\npred_2 = lr.predict(X_test_tf_idf)","ecf31327":"#calling the score function\nEval_lr= classification_task(lr,X_train_tf_idf, y_train ,X_test_tf_idf ,y_test,pred_2,\"logisitc regression\" )\nEval_lr","ad61bb0d":"#creating confusion matrix to know the errors\nconf = confusion_matrix(y_test, pred_2 ,normalize=\"all\")\ndisp = ConfusionMatrixDisplay(conf).plot(cmap=plt.cm.PuBuGn)","4d173d61":"# Train the svc model on the scaled data and print the model score\nsvc = SVC(random_state=0,C=0.2,kernel='rbf')\n#fitting model\nsvc.fit(X_train_tf_idf, y_train)\n#predict\npred_svc = svc.predict(X_test_tf_idf)","8ec40513":"#calling the score function\nEval_svc= classification_task(svc,X_train_tf_idf, y_train ,X_test_tf_idf ,y_test,pred_svc,\"SVC\" )\nEval_svc  ","011cb0e6":"#creating confusion matrix to know the errors\nconf = confusion_matrix(y_test, pred_svc ,normalize=\"all\")\ndisp = ConfusionMatrixDisplay(conf).plot(cmap=plt.cm.PuBuGn)","6313fb2f":"# Train Random forest model on the scaled data\nRandom_Forest= RandomForestClassifier()\n# Fit the model\nRandom_Forest.fit(X_train_tf_idf,y_train)\n#predict x_test_scaled\npred_rand = Random_Forest.predict(X_test_tf_idf)","f746c481":"#calling the score function\nEval_fores= classification_task(Random_Forest,X_train_tf_idf, y_train ,X_test_tf_idf ,y_test,pred_rand,\"Random Forest\" )\nEval_fores","80d03666":"#creating confusion matrix to know the errors\nconf = confusion_matrix(y_test, pred_rand ,normalize=\"all\")\ndisp = ConfusionMatrixDisplay(conf).plot(cmap=plt.cm.PuBuGn)","050f78ee":"# Train decision tree model on the scaled data\nDecision_Tree = DecisionTreeClassifier() \n# Fit the model\nDecision_Tree.fit(X_train_tf_idf,y_train)\n#predict x_test_scaled\npred_ = Decision_Tree.predict(X_test_tf_idf)","e2f5aa1c":"#calling the score function\nEval_dec = classification_task(Decision_Tree,X_train_tf_idf, y_train ,X_test_tf_idf,y_test,pred_,\"Decision Tree\" )\nEval_dec","04078d6b":"#creating confusion matrix to know the errors\nconf = confusion_matrix(y_test, pred_ ,normalize=\"all\")\ndisp = ConfusionMatrixDisplay(conf).plot(cmap=plt.cm.PuBuGn)","857e2ee3":"# Train adaboost model on the scaled data\nada = AdaBoostClassifier(n_estimators= 500, random_state = 42)\n# Fit the model\nada.fit(X_train_tf_idf, y_train)\n#predict x_test\npred_ada = ada.predict(X_test_tf_idf)\n","a7d3b239":"#calling the score function\nEval_ada = classification_task(ada,X_train_tf_idf, y_train ,X_test_tf_idf,y_test,pred_,\"AdaBoosting\" )\nEval_ada","479a6a70":"#creating confusion matrix to know the errors\nconf = confusion_matrix(y_test, pred_ada ,normalize=\"all\")\ndisp = ConfusionMatrixDisplay(conf).plot(cmap=plt.cm.PuBuGn)","be703526":"#model comparison \npd.concat([Eval_dec , Eval_fores , Eval_ada , Eval_lr, Eval_svc]) ","05d784b7":"x = df_upsampled['Review Text'].values\ny = df_upsampled['Recommended IND'].values","ca7802ae":"#tokenization\n\nfrom keras.preprocessing.text import Tokenizer\nnum_words = 10000\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(x)\nxtokens= tokenizer.texts_to_sequences(x)\n","5518b441":"#padding \n\nfrom keras.preprocessing.sequence import pad_sequences\n\nmaxlen=50\nxpad=pad_sequences(xtokens,padding='post', maxlen=maxlen)\n","86fb9657":"print(x[3])\n","c40b4ee5":"print(xpad[3])\n","b3843dd3":"#train_test splot\nx_train, x_test, y_train, y_test = train_test_split(xpad, y, test_size=0.2, stratify=y,random_state=42)","491613c7":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout,GRU,Bidirectional\nfrom keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.regularizers import l2\n\n\nmodel=Sequential()\n\nmodel.add(Embedding(input_dim=num_words,output_dim=32,input_length=maxlen))\n\nmodel.add(LSTM(units=64,dropout=0.4,return_sequences=True))\nmodel.add(LSTM(units=64,dropout=0.4,return_sequences=True))\n\nDense(16, activation='relu')\n\nmodel.add(LSTM(units=20))\nmodel.add(Dropout(0.5))\n\n\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n\n\nmodel.summary()","acc665da":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor=\"val_loss\", mode=\"auto\", \n                           verbose=1, patience = 10, restore_best_weights=True)","5fa15bbf":"model.fit(x_train,y_train, epochs=20, batch_size=32, validation_data=(x_test , y_test), callbacks= [early_stop])","6726773d":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","a4a9e05a":"model_loss.plot()","8e805627":"model.evaluate(x_train, y_train)\n","a6a59b3f":"model.evaluate(x_test, y_test)\n","c80090c8":"review1 = \"it fits me so well\"\nreview = [review1 ]","c579da8a":"tokens = tokenizer.texts_to_sequences(review) \n","472c1c2b":"tokens_pad = pad_sequences(tokens, maxlen=maxlen)\ntokens_pad.shape","79e92394":"mod_pred = model.predict(tokens_pad)\nmod_pred\n","c9b038c8":"df_pred = pd.DataFrame(mod_pred, index=review)\ndf_pred.rename(columns={0: 'Pred_Proba'}, inplace=True)","7864ba90":"df_pred[\"Predicted_Feedbaack\"] = df_pred[\"Pred_Proba\"].apply(lambda x: 1 if x>=0.5 else 0)","65fcfa30":"df_pred","6f539353":"model2 = Sequential()\n\n\nmodel2.add(Embedding(input_dim=num_words,output_dim=32,input_length=maxlen)) \n\nmodel2.add(GRU(units=48, return_sequences=True))  \nmodel2.add(GRU(units=24, return_sequences=True))\nDense(16, activation='relu')\n\nmodel2.add(GRU(units=12)) \nmodel2.add(Dropout(0.4))\n\nmodel2.add(Dense(1, activation='sigmoid')) \noptimizer = Adam(learning_rate=0.006)\n \nmodel2.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"Accuracy\"])","dca90332":"model2.summary() \n","128e4629":"model2.fit(x_train, y_train, epochs=20, batch_size=32, \n         validation_data=(x_test, y_test), callbacks=[early_stop])","00f7c2d3":"model_loss = pd.DataFrame(model2.history.history)\nmodel_loss.head()","4f3a5488":"model_loss.plot()","004f7086":"model2.evaluate(x_train, y_train)\n","faf7f6d1":"model2.evaluate(x_test, y_test)\n","e5e16656":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, roc_auc_score\n\ny_train_pred = (model2.predict(x_train) >= 0.5).astype(\"int32\")  \n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"-------------------------------------------------------\")\nprint(classification_report(y_train, y_train_pred))","719c1ccb":"y_pred = (model2.predict(x_test) >= 0.5).astype(\"int32\")\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"-------------------------------------------------------\")\nprint(classification_report(y_test, y_pred))","cc325f48":"review1 = \"Love this dress\"\nreview2 = \"Absolutely wonderful. silky and sexy and comfortable\"\nreview3 = \"i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up!\"\nreview4 = \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\"\nreview5 = 'This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!'\nreview6 = 'I love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress.'\nreview7 = 'I love this dress. i usually get an xs but it runs a little snug in bust so i ordered up a size. very flattering and feminine with the usual retailer flair for style.'\nreview8 = 'Dress runs small esp where the zipper area runs. i ordered the sp which typically fits me and it was very tight! the material on the top looks and feels very cheap that even just pulling on it will cause it to rip the fabric. pretty disappointed as it was going to be my christmas dress this year! needless to say it will be going back.'\nreview9 =  \"if you are at least average height or taller, this may look good on you.\"\nreview10 = \"sadly will be returning, but i'm sure i will find something to exchange it for!\"\nreview11 = \"Cute little dress fits tts. it is a little high waisted. good length for my 5'9 height. i like the dress, i'm just not in love with it. i dont think it looks or feels cheap. it appears just as pictured.\"\nreview12 = 'Loved the material, but i didnt really look at how long the dress was before i purchased both a large and a medium. im 5\\'5\" and there was atleast 5\" of material at my feet. the gaps in the front are much wider than they look. felt like the dress just fell flat. both were returned. im usually a large and the med fit better. 36d 30 in jeans'\nreview13 = \"I have been waiting for this sweater coat to ship for weeks and i was so excited for it to arrive. this coat is not true to size and made me look short and squat.\"\nreview14 = 'not bad '\nreviews = [review1, review2, review3, review4, review5, review6, review7, review8, review9, review10, review11, review12, review13, review14]","f6c25bed":"tokens = tokenizer.texts_to_sequences(reviews) \ntokens_pad = pad_sequences(tokens, maxlen=maxlen)\ntokens_pad.shape","7c5869ea":"mod_pred = model2.predict(tokens_pad)\nmod_pred","f9d22b40":"df_pred = pd.DataFrame(mod_pred, index=reviews)\ndf_pred.rename(columns={0: 'Pred_Proba'}, inplace=True)","7a5621a8":"df_pred[\"Predicted_Feedbaack\"] = df_pred[\"Pred_Proba\"].apply(lambda x: 1 if x>=0.5 else 0)","2b9e6b85":"df_pred","c79a73f2":"model3 = Sequential()\n\nmodel3.add(Embedding(input_dim = num_words, output_dim = 32,input_length = maxlen))\n\nmodel3.add(Bidirectional(LSTM(units = 48,return_sequences = True )))\nmodel3.add(Bidirectional(LSTM(units = 24, return_sequences = True)))\nDense(16, activation='relu')\n\n\nmodel3.add(Bidirectional(LSTM(units = 20)))\nmodel3.add(Dropout(0.5))\n\nmodel3.add(Dense(1, activation = 'sigmoid'))\n\n\n","f39c8fd3":"optimizer = Adam(lr = 0.004)\nmodel3.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [\"Accuracy\"])","be5306ea":"model3.summary()","8eedf14e":"model3.fit(x_train, y_train, epochs = 20, batch_size = 32,\n         validation_data = (x_test, y_test), callbacks = [early_stop])","6924d937":"model_loss = pd.DataFrame(model3.history.history)\nmodel_loss.head()","0bb138a1":"model_loss.plot()","cfed3a1b":"model3.evaluate(x_train, y_train)\n","4d1c482d":"model3.evaluate(x_test, y_test)\n","297a00d5":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, roc_auc_score\n\ny_train_pred = (model3.predict(x_train) >= 0.5).astype(\"int32\")  \n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"-------------------------------------------------------\")\nprint(classification_report(y_train, y_train_pred))","6929aa03":"y_pred = (model3.predict(x_test) >= 0.5).astype(\"int32\")\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"-------------------------------------------------------\")\nprint(classification_report(y_test, y_pred))","fda3e4a6":"review1 = \"Love this dress\"\nreview2 = \"Absolutely wonderful. silky and sexy and comfortable\"\nreview3 = \"i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up!\"\nreview4 = \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\"\nreview5 = 'This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!'\nreview6 = 'I love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress.'\nreview7 = 'I love this dress. i usually get an xs but it runs a little snug in bust so i ordered up a size. very flattering and feminine with the usual retailer flair for style.'\nreview8 = 'Dress runs small esp where the zipper area runs. i ordered the sp which typically fits me and it was very tight! the material on the top looks and feels very cheap that even just pulling on it will cause it to rip the fabric. pretty disappointed as it was going to be my christmas dress this year! needless to say it will be going back.'\nreview9 =  \"if you are at least average height or taller, this may look good on you.\"\nreview10 = \"sadly will be returning, but i'm sure i will find something to exchange it for!\"\nreview11 = \"Cute little dress fits tts. it is a little high waisted. good length for my 5'9 height. i like the dress, i'm just not in love with it. i dont think it looks or feels cheap. it appears just as pictured.\"\nreview12 = 'Loved the material, but i didnt really look at how long the dress was before i purchased both a large and a medium. im 5\\'5\" and there was atleast 5\" of material at my feet. the gaps in the front are much wider than they look. felt like the dress just fell flat. both were returned. im usually a large and the med fit better. 36d 30 in jeans'\nreview13 = \"I have been waiting for this sweater coat to ship for weeks and i was so excited for it to arrive. this coat is not true to size and made me look short and squat.\"\nreview14 = 'not bad '\nreviews = [review1, review2, review3, review4, review5, review6, review7, review8, review9, review10, review11, review12, review13, review14]","778a6dc8":"tokens = tokenizer.texts_to_sequences(reviews) \ntokens_pad = pad_sequences(tokens, maxlen=maxlen)\ntokens_pad.shape","930bbefb":"tokens","a55e02ad":"tokens_pad","03cdecae":"mod_pred = model3.predict(tokens_pad)\nmod_pred","e0bcd88b":"df_pred = pd.DataFrame(mod_pred, index=reviews)\ndf_pred.rename(columns={0: 'Pred_Proba'}, inplace=True)","ac9beedf":"df_pred[\"Predicted_Feedbaack\"] = df_pred[\"Pred_Proba\"].apply(lambda x: 1 if x>=0.5 else 0)","47fa9a4a":"df_pred","739abc4a":"!wget --no-check-certificate \\\n    http:\/\/nlp.stanford.edu\/data\/glove.6B.zip \\\n    -O \/tmp\/glove.6B.zip","f1440ee4":"import zipfile\n\nwith zipfile.ZipFile('\/tmp\/glove.6B.zip', 'r') as zip_ref:\n    zip_ref.extractall('\/tmp\/glove')","65d0ccff":"vocab_size = 5000\nembedding_dim = 16\nmax_len = 100\ntrunction_type='post'\npadding_type='post'\noov_token = \"<OOV>\"","b56535b4":"tokenize = Tokenizer(num_words = vocab_size, oov_token=oov_token)\ntokenize.fit_on_texts(x)","0a424a25":"word_index = tokenize.word_index","58ed3552":"X_train = tokenize.texts_to_sequences(x)\n","ae8099ea":"X_train_pad = pad_sequences(X_train,maxlen=max_len, padding=padding_type, \n                            truncating=trunction_type)","fde1105f":"x_train, x_test, y_train, y_test = train_test_split(X_train_pad, y, test_size=0.2, stratify=y,random_state=42)","1af9681e":"embeddings_index = {}\nf = open('\/tmp\/glove\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    try:\n       coefs = np.asarray(values[1:], dtype='float32')\n       embeddings_index[word] = coefs\n    except ValueError:\n       pass\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","a46d5b51":"num_words = len(word_index) + 1\n\nembedding_matrix = np.zeros((num_words, max_len))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word) \n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","6dd3d157":"embedding_layer = Embedding(num_words,\n                            max_len,\n                            weights=[embedding_matrix],\n                            input_length=max_len,\n                            trainable=False)","db54e71f":"import tensorflow as tf","722ba203":"model4 = Sequential([\n    embedding_layer,\n    Bidirectional(LSTM(64, return_sequences=True)), \n    Bidirectional(LSTM(32)),\n    Dense(16, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\noptimizer = tf.keras.optimizers.Adam(lr=1e-4)\nmodel4.compile(loss='binary_crossentropy', \n               optimizer=optimizer,\n               metrics=['accuracy'])\n\nmodel4.summary()","5d07392d":"callbacks = EarlyStopping(monitor = 'val_accuracy', \n                          mode = 'max', \n                          patience = 10,\n                          verbose = 1)","409db746":"history2 = model4.fit(x_train, y_train,\n                    batch_size=32,\n                    epochs=20,\n                    validation_data=(x_test, y_test),\n                    verbose=1,)","aa403883":"loss, accuracy = model4.evaluate(x_test,y_test)\nprint('Test accuracy :', accuracy)","2373f0d4":"model_loss = pd.DataFrame(history2.history)\nmodel_loss.head()","a9a44e9e":"model_loss.plot()","df8549b3":"- It can be concluded that clothes in Tops section are the most available Products, while Trendy clothes are the least available ones.","5c7ce6fe":"# **DL models**\n\n","c09a3149":"## **Bi - LSTM**","44a25e84":"-Noise Removal (Punctuation)","472b380c":"**Resampling**","0b7c9de2":"- According to the desired output of generating a model that can classify Positive and Negative reviews, all the unnecessary features have been dropped. From now on, the DataFrame contains only two features: \"Review Text\" and \"Recommended index\". ","b2ca878a":"# - Special Preprocessing","f81f63ea":"# About the Data\n","fd18831f":"- It is clear that for every department, the majority of the products are recommended.","a6471f7e":"Random Forest With TF-IDF Vectorizer","c9e326a0":"## **GRU Model**","2339ec8a":"-Tokenization","e780bf62":"Splitting data","b480869d":"-Lemmatization","9403fd0f":"- Products rated 1 and 2 are almost non-recommended by all the customers, while products rated 4 and 5 are nearly recommended by all the customers. ","f381e7b9":"Support Vector Machine (SVM) With TF-IDF Vectorizer\n","e66b4e45":"decision tree With TF-IDF Vectorizer","c4bd9f63":"## **Bi-LSTM2**","286396cf":"Checking for Null values","26322beb":"## **LSTM MODEL**","dd146ba1":"vectorization ","3637276b":"**Women's E-Commerce Clothing Reviews** ","ebf078ec":"function to validate score ","2b723671":"Cleaning Data","40e8581f":"-Apparently, the Count of Null values is small compared to the dataset size, So they are to be dropped.","2785c1a8":"1) Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n\n2) Age: Positive Integer variable of the reviewers age.\n\n3) Title: String variable for the title of the review.\n\n4) Review Text: String variable for the review body.\n\n5) Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n\n6) Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n\n7) Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\n\n8) Division Name: Categorical name of the product high level division.\n\n9) Department Name: Categorical name of the product department name.\n\n10) Class Name: Categorical name of the product class name.","d6f5bad9":"Logistic Regression With TF-IDF Vectorizer","2bb62f76":"AdaBoost With TF-IDF Vectorizer","88de80dd":"-Removing Numbers","632c8beb":"# Text Manipulation","a4b152f3":"# EDA ","6f6dba89":"- As shown in the above graph, Although more than 50% of customers rated \"5\" for the store's products, about more than 11% of customers were not satisfies by the products and voted for \"1\" and \"2\".","34381e5b":"Feature Selection","fe0a8129":"Removing Duplicates","d97b6cf7":"## **preparing**","0721a2a0":"# ML modeling ","9a5e82cd":"Modeling ","fa051fd8":"Models i use:\n\n1-Logistic regression\n\n2-Linear Support Vector Machine\n\n3-Decision Tree\n\n4-Random Forest\n\n6-AdaBoosting","7927fc60":"***After resampling***","109c310b":"-Removing stopping words\n","458e806f":"# Data PreProcessing","0e6231ed":"- The majority of customers age between 30 and 50 yeare old.","8c3632e8":"- 0 means \"not recommended product\", while 1 means \"Recommeded product\". \n- The above graph means that the majority of the store's products are recmmended for purchasing."}}