{"cell_type":{"05a8c7f9":"code","466f6aba":"code","58822c66":"code","7723a716":"code","eaade8f1":"code","8b28354c":"code","bced4654":"code","89d5a90e":"code","90924c0f":"code","1dfe18e3":"code","0ff3f854":"code","9d0509ef":"code","50dc0b9f":"code","826fde1b":"code","26552458":"code","59acebaa":"code","d0e9af9d":"code","0e2ecdd8":"code","c3952fdc":"code","6b316356":"code","f35b5311":"code","a1983a8d":"code","baa15398":"code","c8d10136":"code","92357bec":"code","10086c29":"code","d6b5fdc8":"code","c6dce36b":"code","5c8519a8":"code","9bc6ed32":"code","760d3a22":"code","39083e17":"code","f6ab0f06":"code","b95612c7":"code","7b21fb14":"code","348c9e0a":"code","0f15cb72":"code","509e7545":"code","8bb79901":"code","82659451":"code","9364c9b4":"code","ec55c5f8":"code","d44d3659":"code","27536384":"code","176a1458":"code","628b0a6f":"code","04b610d3":"code","fdcbce13":"code","d3fb9a98":"code","7a85cbfc":"code","3124fe99":"code","37716150":"code","a0a56e11":"code","1b1ad5ff":"code","33a91c80":"code","d84656f2":"code","faf628de":"code","71c15c93":"code","0b7ee475":"code","eb1d4526":"code","0012f117":"code","222bc9e9":"code","77a19aec":"code","4c7b5644":"code","13c1be8e":"code","4af0c457":"code","b2640b92":"code","8b90d611":"code","054e3314":"code","c95db8e2":"code","9e118536":"code","9552ae71":"code","28647131":"code","17b7d290":"code","a3f65691":"code","67672b0d":"code","4735ca51":"code","1bae3de8":"code","5666b135":"code","0e98a1b5":"markdown","20ef6dff":"markdown","c5ab757e":"markdown","91beb97d":"markdown","f9ee7acf":"markdown","fd18e8f2":"markdown","1ba0c91d":"markdown","61a9a945":"markdown","c5c3d3dc":"markdown","89aa233f":"markdown","eabd9167":"markdown","a48848ec":"markdown","c69954e4":"markdown","e3279aab":"markdown","57bf1be2":"markdown","5bdd824c":"markdown","13e9c52c":"markdown","29ff410c":"markdown","016cdab7":"markdown","9eb2a85d":"markdown","f87191fc":"markdown","2009c87f":"markdown","58c550a6":"markdown","ad6b9790":"markdown","ef95216b":"markdown","1c4f5a60":"markdown","6e2f7b31":"markdown","be88c789":"markdown","2a4e3013":"markdown","f3049c2f":"markdown","0981ee42":"markdown","71f3b893":"markdown","f55b8b39":"markdown","fcacec9d":"markdown","db20ecb1":"markdown"},"source":{"05a8c7f9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","466f6aba":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\nsns.set(rc={'figure.figsize': (15, 10)})","58822c66":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","7723a716":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler","eaade8f1":"from sklearn.linear_model import LogisticRegression","8b28354c":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bced4654":"raw_data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\nraw_data.head(10)","89d5a90e":"raw_data_row_count, raw_data_column_count = raw_data.shape\nprint('Row Count:', raw_data_row_count)\nprint('Column Count:', raw_data_column_count)","90924c0f":"raw_data.id.isnull().sum()","1dfe18e3":"raw_data.age.describe()","0ff3f854":"len(raw_data.id.unique())","9d0509ef":"raw_data = raw_data.drop(columns='id')","50dc0b9f":"raw_data.shape","826fde1b":"raw_data.gender.isnull().sum()","26552458":"raw_data.gender.value_counts()","59acebaa":"raw_data['gender'] = raw_data['gender'].replace('Other', list(raw_data.gender.mode().values)[0])","d0e9af9d":"raw_data.gender.value_counts()","0e2ecdd8":"raw_data.gender = raw_data.gender.map({'Male': 1, 'Female': 0})","c3952fdc":"raw_data.age.isnull().sum()","6b316356":"raw_data.age.dtypes","f35b5311":"raw_data.age.describe()","a1983a8d":"pd.cut(raw_data['age'], bins=np.arange(0, 100, 10)).value_counts(sort=False)","baa15398":"sns.displot(raw_data.age)\nplt.title('Age Distribution Plot', fontdict={'fontsize': 20})\nplt.xlabel('Age', fontdict={'fontsize': 12})\nplt.ylabel('Patient Count', fontdict={'fontsize': 12})\nplt.show()","c8d10136":"plt.figure(figsize=(15,6))\nsns.boxplot(raw_data.age)\nplt.title('Age Distribution Box Plot', fontdict={'fontsize': 20})\nplt.xlabel('Age', fontdict={'fontsize': 12})\nplt.show()","92357bec":"raw_data.hypertension.isnull().sum()","10086c29":"raw_data.hypertension.value_counts()","d6b5fdc8":"raw_data.heart_disease.isnull().sum()","c6dce36b":"raw_data.heart_disease.value_counts()","5c8519a8":"raw_data.ever_married.isnull().sum()","9bc6ed32":"raw_data.ever_married.value_counts()","760d3a22":"raw_data.ever_married = raw_data.ever_married.map({'Yes': 1, 'No': 0})","39083e17":"raw_data.work_type.isnull().sum()","f6ab0f06":"raw_data.work_type.value_counts()","b95612c7":"dummy_train_df = pd.get_dummies(raw_data['work_type'], drop_first=True)\nraw_data = pd.concat([raw_data, dummy_train_df], axis=1)\nraw_data = raw_data.drop(columns=['work_type'])","7b21fb14":"raw_data.Residence_type.isnull().sum()","348c9e0a":"raw_data.Residence_type.value_counts()","0f15cb72":"raw_data.Residence_type = raw_data.Residence_type.map({'Rural': 0, 'Urban': 1})","509e7545":"raw_data.avg_glucose_level.isnull().sum()","8bb79901":"raw_data.avg_glucose_level.dtypes","82659451":"raw_data.avg_glucose_level.describe()","9364c9b4":"pd.cut(raw_data['avg_glucose_level'], bins=np.arange(50, 300, 25)).value_counts(sort=False)","ec55c5f8":"sns.displot(raw_data.avg_glucose_level)\nplt.title('Average Glucose Distribution Plot', fontdict={'fontsize': 20})\nplt.xlabel('Average Glucose', fontdict={'fontsize': 12})\nplt.ylabel('Patient Count', fontdict={'fontsize': 12})\nplt.show()","d44d3659":"plt.figure(figsize=(15,6))\nsns.boxplot(raw_data.avg_glucose_level)\nplt.title('Average Glucose Box Plot', fontdict={'fontsize': 20})\nplt.xlabel('Average Glucose', fontdict={'fontsize': 12})\nplt.show()","27536384":"raw_data.bmi.dtypes","176a1458":"raw_data.bmi.describe()","628b0a6f":"raw_data.bmi.isnull().sum()","04b610d3":"raw_data[\"bmi\"].fillna(raw_data.bmi.mean(), inplace=True)","fdcbce13":"raw_data.bmi.isnull().sum()","d3fb9a98":"pd.cut(raw_data['bmi'], bins=np.arange(10, 110, 10)).value_counts(sort=False)","7a85cbfc":"sns.displot(raw_data.bmi)\nplt.title('BMI Distribution Plot', fontdict={'fontsize': 20})\nplt.xlabel('BMI', fontdict={'fontsize': 12})\nplt.ylabel('Patient Count', fontdict={'fontsize': 12})\nplt.show()","3124fe99":"plt.figure(figsize=(15,6))\nsns.boxplot(raw_data.bmi)\nplt.title('BMI Box Plot', fontdict={'fontsize': 20})\nplt.xlabel('BMI', fontdict={'fontsize': 12})\nplt.show()","37716150":"raw_data.smoking_status.isnull().sum()","a0a56e11":"raw_data.smoking_status.value_counts()","1b1ad5ff":"dummy_train_df = pd.get_dummies(raw_data['smoking_status'], drop_first=True)\nraw_data = pd.concat([raw_data, dummy_train_df], axis=1)\nraw_data = raw_data.drop(columns=['smoking_status'])\nraw_data","33a91c80":"raw_data.stroke.isnull().sum()","d84656f2":"raw_data.stroke.value_counts()","faf628de":"raw_data.head()","71c15c93":"X = raw_data.drop(columns=['stroke'])","0b7ee475":"y = raw_data.stroke","eb1d4526":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=14)","0012f117":"# std_scaler = StandardScaler()\n# X_train = pd.DataFrame(std_scaler.fit_transform(X_train), columns = X_train.columns)\n# X_test = pd.DataFrame(std_scaler.transform(X_test),columns = X_test.columns)","222bc9e9":"col = list(X_train.columns)","77a19aec":"model = LogisticRegression()\nmodel = model.fit(X_train, y_train)\npred_probs_train = model.predict_proba(X_train[col])","4c7b5644":"y_train_pred_final = pd.DataFrame(y_train)\ny_train_pred_final['stroke_probability'] = pred_probs_train[:,1]\nnumbers = np.arange(0.0, 1.0, 0.001)\nfor i in numbers:\n    y_train_pred_final[i] = y_train_pred_final.stroke_probability.map(lambda x: 1 if x > i else 0)","13c1be8e":"cutoff_df = pd.DataFrame(columns=['prob', 'accuracy', 'sensi', 'speci'])\nfor i in numbers:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.stroke, y_train_pred_final[i])\n    accuracy = (cm1[0, 0] + cm1[1, 1]) \/ sum(sum(cm1))\n    speci = cm1[0, 0] \/ (cm1[0, 0] + cm1[0, 1])\n    sensi = cm1[1, 1] \/ (cm1[1, 0] + cm1[1, 1])\n    cutoff_df.loc[i] = [i, accuracy, sensi, speci]\ncutoff_df[(cutoff_df['sensi'] < 0.8) & (cutoff_df['sensi'] > 0.7)]","4af0c457":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.xlabel('Probability', fontdict={'fontsize': 15})\nplt.title('Cut-Off for Logisitic Regression Model', fontdict={'fontsize': 20})\nplt.show()","b2640b92":"cut_off = 0.064","8b90d611":"conf_matrix = metrics.confusion_matrix(y_train_pred_final.stroke, y_train_pred_final[cut_off])\n\nTN = conf_matrix[0, 0]\nFP = conf_matrix[0, 1]\nFN = conf_matrix[1, 0]\nTP = conf_matrix[1, 1]\n\naccuracy_score = metrics.accuracy_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\naccuracy_score = round(accuracy_score*100, 2)\n\nprecision_score = metrics.precision_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\nprecision_score = round(precision_score*100, 2)\n\nrecall_score = metrics.recall_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\nrecall_score = round(recall_score*100, 2)\n\nsensitivity = TP \/ float(FN + TP)\nsensitivity = round(sensitivity*100, 2)\n\nspecificity = TN \/ float(TN + FP)\nspecificity = round(specificity*100, 2)\n\nf1_score = metrics.f1_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\nf1_score = round(f1_score*100, 2)\n\nauc_score = metrics.roc_auc_score(y_train_pred_final.stroke, y_train_pred_final.stroke_probability)\nauc_score = round(auc_score*100, 2)","054e3314":"fpr, tpr, thresholds = metrics.roc_curve(y_train_pred_final.stroke, y_train_pred_final.stroke_probability, drop_intermediate=False )\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]', fontdict={'fontsize': 15})\nplt.ylabel('True Positive Rate', fontdict={'fontsize': 15})\nplt.title('ROC (Receiver Operating Characteristic) Curve - Train Data\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.legend(loc=\"lower right\")\nplt.show()","c95db8e2":"data = pd.DataFrame({'Parameter': ['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'],\n                     'Value': [accuracy_score, sensitivity, specificity, precision_score, recall_score, f1_score, auc_score]}, index=['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'])   \ndata = data.groupby(by='Parameter').Value.sum().sort_index()\ngraph = sns.barplot(x=data.index, y=data.values)\nplt.title('Model Metrices (With Train Data)\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.xlabel('Parameters', fontdict={'fontsize': 15})\nplt.ylabel('Score Value (In Percent)', fontdict={'fontsize': 15})\nlabel_deviation_above_y_axis = data.max() * 0.015\nfor index, value in enumerate(data.iteritems()):\n    graph.text(index, value[1] + label_deviation_above_y_axis, str(round(value[1], 1))+'%', color='black', ha=\"center\")\nplt.show()","9e118536":"pred_probs_test = model.predict_proba(X_test[col])","9552ae71":"y_test_pred_final = pd.DataFrame(y_test)\ny_test_pred_final['stroke_probability'] = pred_probs_test[:,1]\ny_test_pred_final['stroke_predicted'] = y_test_pred_final.stroke_probability.map(lambda x: 1 if x > cut_off else 0)","28647131":"conf_matrix_test = metrics.confusion_matrix(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\n\nTN_test = conf_matrix[0, 0]\nFP_test = conf_matrix[0, 1]\nFN_test = conf_matrix[1, 0]\nTP_test = conf_matrix[1, 1]\n\naccuracy_score_test = metrics.accuracy_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\naccuracy_score_test = round(accuracy_score_test*100, 2)\n\nprecision_score_test = metrics.precision_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\nprecision_score_test = round(precision_score_test*100, 2)\n\nrecall_score_test = metrics.recall_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\nrecall_score_test = round(recall_score_test*100, 2)\n\nsensitivity_test = TP_test \/ float(FN_test + TP_test)\nsensitivity_test = round(sensitivity_test*100, 2)\n\nspecificity_test = TN_test \/ float(TN_test + FP_test)\nspecificity_test = round(specificity_test*100, 2)\n\nf1_score_test = metrics.f1_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\nf1_score_test = round(f1_score_test*100, 2)\n\nauc_score_test = metrics.roc_auc_score(y_test_pred_final.stroke, y_test_pred_final.stroke_probability)\nauc_score_test = round(auc_score_test*100, 2)","17b7d290":"fpr, tpr, thresholds = metrics.roc_curve(y_test_pred_final.stroke, y_test_pred_final.stroke_probability, drop_intermediate=False )\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score_test)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]', fontdict={'fontsize': 15})\nplt.ylabel('True Positive Rate', fontdict={'fontsize': 15})\nplt.title('ROC (Receiver Operating Characteristic) Curve - Test Data\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.legend(loc=\"lower right\")\nplt.show()","a3f65691":"data = pd.DataFrame({'Parameter': ['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'],\n                     'Value': [accuracy_score_test, sensitivity_test, specificity_test, precision_score_test, recall_score_test, f1_score_test, auc_score_test]}, index=['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'])   \ndata = data.groupby(by='Parameter').Value.sum().sort_index()\ngraph = sns.barplot(x=data.index, y=data.values)\nplt.title('Model Metrices (With Test Data)\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.xlabel('Parameters', fontdict={'fontsize': 15})\nplt.ylabel('Score Value (In Percent)', fontdict={'fontsize': 15})\nlabel_deviation_above_y_axis = data.max() * 0.015\nfor index, value in enumerate(data.iteritems()):\n    graph.text(index, value[1] + label_deviation_above_y_axis, str(round(value[1], 1))+'%', color='black', ha=\"center\")\nplt.show()","67672b0d":"train_test_score_df = pd.DataFrame({'train': [accuracy_score, sensitivity, specificity, precision_score, recall_score, f1_score, auc_score], \n                                    'test': [accuracy_score_test, sensitivity_test, specificity_test, precision_score_test, recall_score_test, f1_score_test, auc_score_test]},\n                                   index=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'Recall', 'F1 Score', 'AUC Score'])   \n\nfig, ax = plt.subplots()\nx = np.arange(len(train_test_score_df.index))\nwidth = 0.35\n\n\nax.set_ylabel('Score (In %)', fontdict={'fontsize': 15})\nax.set_xlabel('Parameters', fontdict={'fontsize': 15})\nax.set_title('Logisitic Regression Model', fontdict={'fontsize': 20})\nax.set_xticks(x)\nax.set_xticklabels(train_test_score_df.index)\n\nrects1 = ax.bar(x - width\/2, train_test_score_df['train'], width, label='Train Score')\nfor rect in rects1:\n    height = rect.get_height()\n    ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() \/ 2, height), xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n\nrects2 = ax.bar(x + width\/2, train_test_score_df['test'], width, label='Test Score')\nfor rect in rects2:\n    height = rect.get_height()\n    ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() \/ 2, height), xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n\nax.legend()\nplt.show()","4735ca51":"model.intercept_","1bae3de8":"model.coef_","5666b135":"X_train.columns","0e98a1b5":"##### Verifying if the value was imputed appropriately","20ef6dff":"# 2. Data Sourcing","c5ab757e":"## 5.2. Data Modelling on Test Data","91beb97d":"## 4.2. Splitting Train & Test Data","f9ee7acf":"# 4. Data Preparation (For Modelling)","fd18e8f2":"## 3.4. HyperTension","1ba0c91d":"## 3.8. Residence Type","61a9a945":"##### This data represent a bit like categorical nominal variable. Hence we will keep them as it is.","c5c3d3dc":"##### Replacing Other value with mode","89aa233f":"## 3.2. Gender","eabd9167":"# 5. Data Modelling","a48848ec":"##### From the above graph, probability cut off of 0.064 seems to be respectable enough to behave as threshold value above which patient is likely to have stroke","c69954e4":"# 1. Library Management","e3279aab":"##### From the box plot, we can see that there are many outliers prsent in the higher region. Technically we can neglect the top oultier to predict output.\n##### But from application perspective, we can have pateints with that BMI level and if we neglect these values, our model wont exterpolate higher values. Therefore, we will continue with this missing values.","57bf1be2":"# 3. Data Cleaning","5bdd824c":"## 3.6. Ever Married","13e9c52c":"## 3.9. Average Glucose Level","29ff410c":"## 3.1. ID Column","016cdab7":"## 3.11. Smoking Status","9eb2a85d":"##### Nothing needs to be done with this data column","f87191fc":"##### Gender needs to be categorized as Categorical Nominal Variable. For this, we would be using Dummy Variable Method.\n##### Also, from the analysis perspective, it will be tedious to create another dummy variable just for one row vlue (of Others). Therefore, we will impute this other value with mode in this column.\n#### Therefore, conversion will be as follows:\n- 1. Male: 1\n- 2. Female: 0\n- 3. Others: Mode Value of column","2009c87f":"## 4.1. Defining Input \/ Output Data","58c550a6":"# INDEX\n- ## 1. Library Management\n- ## 2. Data Sourcing\n- ## 3. Data Cleaning\n- ## 4. Data Preparation (For Modelling)\n- ## 5. Data Modelling","ad6b9790":"## 5.3. Model Parameters","ef95216b":"## 3.12. Stroke  Status","1c4f5a60":"## 3.3. Age","6e2f7b31":"## 5.1. Data Modelling on Trained Data","be88c789":"##### Count of missing values is bit high enough to drop respective value. for the same, we will use he mean value to impute these null values.","2a4e3013":"## 3.10. BMI","f3049c2f":"##### Nothing needs to be done with this data column","0981ee42":"##### We see that number number of unique values equals total items in the row.\n##### This can also indicate that, it might be Patient \/ Customer ID. We will drop this, as we already have a unique identifier for the dataframe\n##### Thus, we will drop this column","71f3b893":"## 3.5 Heart Disease","f55b8b39":"## 4.3. Data Scaling","fcacec9d":"## 3.7. Work Type","db20ecb1":"##### These values dont have any definate order. Hence we will asume them to be Categorical Nominal Variable"}}