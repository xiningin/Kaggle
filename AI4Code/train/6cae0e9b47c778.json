{"cell_type":{"4dc14cfa":"code","16f115e3":"code","61d91b97":"code","bed7f312":"code","74b73d1c":"code","9b552b3e":"code","d798aa7d":"code","701002fa":"code","bea6eddf":"code","37b7b95f":"code","5f0775f5":"code","adb1e79c":"markdown","c6b5fd82":"markdown","d44d06c0":"markdown","60321b14":"markdown","54dc0926":"markdown","be5f82fc":"markdown","500e0e2c":"markdown","0be54ccb":"markdown"},"source":{"4dc14cfa":"# Importing the libraries.\n\nimport tensorflow as tf\nimport cv2\nfrom glob import glob\nfrom matplotlib import pyplot as plt\nimport random\nimport math\nimport os\nimport numpy as np\nfrom numpy.random import seed\nseed(100)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n","16f115e3":"# A utility function to display sample images from the dataset.\n\n\ndef plotSample(character):\n    print(\"Samples images for letter \" + character)\n    basePath = '..\/input\/asl_alphabet_train\/asl_alphabet_train\/'\n    imagePath = basePath + character + '\/**'\n    pathData = glob(imagePath)\n    \n    plt.figure(figsize=(16,16))\n    images = random.sample(pathData, 3)\n    plt.subplot(1,3,1)\n    plt.imshow(cv2.imread(images[0]))\n    plt.subplot(1,3,2)\n    plt.imshow(cv2.imread(images[1]))\n    plt.subplot(1,3,3)\n    plt.imshow(cv2.imread(images[2]))\n    plt.colorbar()\n    plt.show()\n    return","61d91b97":"plotSample('H')","bed7f312":"dataPath = \"..\/input\/asl_alphabet_train\/asl_alphabet_train\"\nresizeTuple = (64, 64)\nresizeDim = (64, 64, 3)\nnumLabels = 29\nbatchSize = 64\n\ndata_generator = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center=True, \n                                    samplewise_std_normalization=True, \n                                    validation_split=0.1)\n\ntrain_generator = data_generator.flow_from_directory(dataPath, target_size=resizeTuple, batch_size=batchSize, shuffle=True, subset=\"training\")\nval_generator = data_generator.flow_from_directory(dataPath, target_size=resizeTuple, batch_size=batchSize, subset=\"validation\")","74b73d1c":"# A utility function to plot the loss and accuracy learning curves.\n\ndef plotCurves(history):\n    # Plotting history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['training accuracy', 'validation accuracy'], loc='upper right')\n    plt.show()\n\n\n    # Plotting history for losses\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['training loss', 'validation loss'], loc='upper right')\n    plt.show()","9b552b3e":"model_1 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=resizeDim),\n    tf.keras.layers.Dense(15, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.002)),\n    tf.keras.layers.Dense(15, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.002)),\n    tf.keras.layers.Dense(29, activation = \"softmax\")\n])\n\nmodel_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\nhistory = model_1.fit_generator(train_generator, epochs=10, steps_per_epoch = 1224,validation_data=val_generator, validation_steps = 136, verbose = 1)\n\nplotCurves(history)","d798aa7d":"model_2 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=resizeDim),\n    tf.keras.layers.Dense(15, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.005)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(15, activation = \"relu\"),\n    tf.keras.layers.Dense(29, activation = \"softmax\")\n])\n\nmodel_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\nhistory = model_2.fit_generator(train_generator, epochs=10, steps_per_epoch = 1224,validation_data=val_generator, validation_steps = 136, verbose = 1)\n\nplotCurves(history)","701002fa":"# A utility function to define a learning_rate decay based on epoch schedule.\n\ndef scheduler(epoch):\n    if epoch < 5:\n        return 0.00001\n    else:\n        return 0.00001 * math.exp(0.1 * (5 - epoch))","bea6eddf":"model_3 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=resizeDim),\n    tf.keras.layers.Dense(200, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.005)),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(200, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.005)),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(200, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.005)),\n    tf.keras.layers.Dense(100, activation = \"relu\"),\n    tf.keras.layers.Dense(100, activation = \"relu\"),\n    tf.keras.layers.Dense(50, activation = \"relu\"),\n    tf.keras.layers.Dense(29, activation = \"softmax\")\n])\n\nmodel_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\nhistory = model_3.fit_generator(train_generator, epochs=10, steps_per_epoch = 1224,validation_data=val_generator, validation_steps = 136, callbacks = [callback], verbose = 1)\n\nplotCurves(history)","37b7b95f":"# A utility function to define a learning_rate decay based on epoch schedule.\n\ndef scheduler(epoch):\n    if epoch < 25:\n        return 0.00001\n    else:\n        return 0.00001 * math.exp(0.1 * (25 - epoch))","5f0775f5":"model_4 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=resizeDim),\n    tf.keras.layers.Dense(200, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.005)),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(200, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.005)),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(200, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.005)),\n    tf.keras.layers.Dense(100, activation = \"relu\"),\n    tf.keras.layers.Dense(100, activation = \"relu\"),\n    tf.keras.layers.Dense(50, activation = \"relu\"),\n    tf.keras.layers.Dense(29, activation = \"softmax\")\n])\n\nmodel_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\nhistory = model_4.fit_generator(train_generator, epochs=30, steps_per_epoch = 1224,validation_data=val_generator, validation_steps = 136, callbacks = [callback], verbose = 1)\n\n\nplotCurves(history)","adb1e79c":"### Note: One can observe in the above learning curves that the loss minimization for validation dataset is not consistently decreasing which means that there is less learning happening over validation dataset (no generalization) even though the network is learning comparitively well on the training dataset. This is a clear sign of overfitting. We can avoid the huge gap between the loss curves over training and validation datasets by increasing regularization in our network.","c6b5fd82":"### Importing the data using ImageDataGenerator module from Tensorflow.","d44d06c0":"### The below network includes a dropout layer with keep probability = 0.8 which means that each neuron in the layer has 20% chance of being dropped out. This will introduce some noise to our network thus increasing the regularization effect.\n\n#### Note: We have removed the regularization from our second hidden layer and increased the lambda for L2 regularization in our first layer to 0.005 from 0.002 initially.","60321b14":"# American sign language alphabets recognition using multi-layer perceptron network.","54dc0926":"### Note: The above trained model shows comparitively better performance over validation dataset than training dataset. The loss learning curve for the validation dataset shows that the loss decreases overall throughout the training process but is still inconsistent. We can also notice that the accuracy over the training and validation dataset is extremely low. Therefore, let us now try a compartively more complex model with more layers and neurons per layer and little more regularization to compensate for the added complexity.\n\n#### We also start with a lower learning rate (earlier: default learning_rate was used which is 0.01) in our next model and include a callback to learning_rate scheduler so that the learning_rate is reduced over time to avoid divergence when close to the loss minimum.","be5f82fc":"### Note: The above learning curves show that the network is learning in a better way now. The training accuracy goes well upto 70% and validation accuracy goes upto 56.5%. The training and validation loss curves are still decreasing at the end of the training phase. This indicates that there is more room for learning. Therefore, we can run the model for more epochs. We will also have to tweak our learning_rate scheduler callback according to the number of epochs set.","500e0e2c":"### The above learning curves shows that we have a decent fit over our data.\n\n#### Note: We did not monitor accuracies in our training process since a multi layer perceptron network is not considered to be very efficient over image data. Convolutional neural networks are considered to be more efficient which perform better than MLP networks. The reason behind creating this notebook was for learning purposes and to discover how the dataset performs over the MLP network.","0be54ccb":"### Defining a basic neural network model. This will be a shallow network. The numpy array representing a image will be unpacked and flattened to form a vector. This will be done for all images. The first two hidden layers contains 15 neurons with ReLU activation and some level of L2 regularization with lambda = 0.002. The output layer will contain 29 neurons with softmax activation for multi-class classification."}}