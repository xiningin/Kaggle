{"cell_type":{"3ab92cac":"code","815c1f1f":"code","3498c0b0":"code","c57b8606":"code","2b14244e":"code","685b7ee6":"code","3931e186":"code","cbd5c29e":"code","5ae55833":"code","b4e6bab5":"code","c0a4e8eb":"code","97a66b2b":"code","07883609":"code","bb709292":"code","e8ab751a":"code","b884ab77":"code","3d9bebcb":"code","45aa2c69":"code","c4821961":"code","40acd9d4":"code","e66398f8":"code","7a3a6419":"code","c1e73676":"code","03b65208":"code","017e559a":"code","4a1cd0c1":"code","1ba14d19":"code","114ec5ee":"code","01006e5d":"code","93261280":"code","2aca0733":"code","46577764":"code","ec5f0e3b":"code","5eb97a56":"code","20a98ecc":"code","523ed84b":"code","576f5bac":"markdown","f018e7ae":"markdown","fa081e90":"markdown","ec662c58":"markdown","5aec4195":"markdown","2d5da27f":"markdown","9c18dd95":"markdown","a81c9aa8":"markdown","42bba67f":"markdown","aecf3fdc":"markdown","07578416":"markdown","fb5fed64":"markdown","4f12f012":"markdown","7010cf94":"markdown","ab1ff5a9":"markdown","3806c462":"markdown"},"source":{"3ab92cac":"#define file input \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#import dependencies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import VarianceThreshold, mutual_info_classif, chi2\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","815c1f1f":"#import dataset & display first 5 rows\ndf = pd.read_csv('..\/input\/churn-modeling-dataset\/Churn_Modelling.csv')\ndf.head()","3498c0b0":"#print rows & columns count\nrows = df.shape[0]\ncolumns = df.shape[1]\nprint(f'Number of Rows : {rows}')\nprint(f'Number of Columns : {columns}')","c57b8606":"#check few samples from the dataset\ndf.sample(5)","2b14244e":"#checking for null values\ndf.isnull().sum()","685b7ee6":"#check data types\ndf.dtypes","3931e186":"#drop unwanted columns\ndf.drop(['RowNumber','CustomerId','Surname'],axis=1,inplace=True)","cbd5c29e":"#central tendency of numeric variables\ndf[['CreditScore','Age','Balance','EstimatedSalary']].describe()","5ae55833":"#visualize histogram for the\ndf[['CreditScore','Age','Balance','EstimatedSalary']].hist(figsize=(12,12))\nplt.show()","b4e6bab5":"#visualize how each categorical varible distribute through out the dataset\nfeatures=['Geography', 'Gender','Tenure','NumOfProducts','HasCrCard', 'IsActiveMember', 'Exited']\n\nplt.figure(figsize=(22,12))\nfor i in enumerate(features):\n    plt.subplot(3,3,i[0]+1)\n    df[i[1]].value_counts().plot.pie(autopct='%1.1f%%')\n    centre=plt.Circle((0,0),0.6,fc='white')\n\n    fig=plt.gcf()\n    fig.gca().add_artist(centre)\n","c0a4e8eb":"#releationship between each continues v\nsns.pairplot(df[['CreditScore','Age','Balance','EstimatedSalary']])","97a66b2b":"plt.subplots(figsize = (12,9))\nsns.heatmap(df[['CreditScore','Age','Balance','EstimatedSalary']].corr(),\n            annot=True,fmt='.3g', vmin=-1, vmax=1, center= 0).set_title(\"Correlation Between Continues Variables\")\nplt.show()","07883609":"# label encode for categorical variables\nlabel = LabelEncoder()\n          \ndf['Geography'] = label.fit_transform(df['Geography'])\ndf['Gender'] = label.fit_transform(df['Gender'])","bb709292":"# pre-processing\ny = df['Exited']\nx = df.drop(['Exited'],axis=1)\n'''\n#when we applied oversampling balance techniques we get lower acccuarcy (82%)so we used same dataset without oversampling\nfrom imblearn.over_sampling import RandomOverSampler\n\n#define the undersampling method\n\n\n#balance the dataset using over sampling method\nx,y= RandomOverSampler(random_state=42).fit_resample(x,y)\n\nplt.figure(figsize=(7,7.5))\nsns.countplot(y)\nplt.title(\"Distribution of Traget Variable After Over Sampling\",size=15)\nplt.show()'''","e8ab751a":"#identify varibles with 0 variance\n\nvarr_threshold = VarianceThreshold(threshold=0)\nvarr_threshold.fit(x)\n\nvarr_threshold.get_support()","b884ab77":"#preprocess continus varible for check mutual information gain\nxt_cont = x[['CreditScore','Age','Balance','EstimatedSalary']]\n\n#check information gain with continious variables & target variable (classes)\nmutual_info = mutual_info_classif(np.array(xt_cont),np.array(y))\n\n#print output\nfor col_name, muinfo in zip(xt_cont.columns,mutual_info):\n    print(f'{col_name}    : {muinfo}')","3d9bebcb":"#visualize the mutual gain\n# Define plot space\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Define x and y axes\nax.plot(xt_cont.columns, \n        mutual_info,\n        marker = 'o',\n        color = 'red')\n\n# Set plot title and axes labels\nax.set(title = \"Continues Variables vs Target Variable Mutual Infomation Gain\",\n       #xlabel = \"Models\",\n       ylabel = \"Mutual Infomation Gain\")\nplt.xticks(rotation='vertical',size=12)\nplt.show()","45aa2c69":"#preprocess continus varible for chisqure test\nxt_cate = x[['Geography','Gender','Tenure','NumOfProducts','HasCrCard','IsActiveMember']]\n\n#check most effecting features between categoricla & targer variable(classes)\nchi_test = chi2(np.array(xt_cate),np.array(y))\nchi_test","c4821961":"#visualize the mutual gain\n# Define plot space\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Define x and y axes\nax.plot(xt_cate.columns, \n        chi_test[1],\n        marker = 'o',\n        color = 'blue')\n\n# Set plot title and axes labels\nax.set(title = \"Categorical Variables vs Target Variable with P-Values\",\n       #xlabel = \"Models\",\n       ylabel = \"P-Values\")\nplt.xticks(rotation='vertical',size=12)\nplt.show()","40acd9d4":"#visualize the mutual gain\n# Define plot space\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Define x and y axes\nax.plot(xt_cate.columns, \n        chi_test[0],\n        marker = 'o',\n        color = 'green')\n\n# Set plot title and axes labels\nax.set(title = \"Categorical Variables vs Target with Variable F-Scores\",\n       #xlabel = \"Models\",\n       ylabel = \"P-Values\")\nplt.xticks(rotation='vertical',size=12)\nplt.show()","e66398f8":"pd.crosstab(xt_cate['Gender'], y,normalize='columns', margins=True)*100","7a3a6419":"pd.crosstab(xt_cate['Geography'], y,normalize='columns', margins=True)*100","c1e73676":"pd.crosstab(xt_cate['Tenure'], y,normalize='columns', margins=True)*100","03b65208":"pd.crosstab(xt_cate['NumOfProducts'], y,normalize='columns', margins=True)*100","017e559a":"pd.crosstab(xt_cate['HasCrCard'], y,normalize='columns', margins=True)*100","4a1cd0c1":"pd.crosstab(xt_cate['IsActiveMember'], y,normalize='columns', margins=True)*100","1ba14d19":"# divide into training set & test sets\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)\n\n#select most important varibles after the feature selection process\nx_train_FS = x_train[['Age','Balance','EstimatedSalary','Gender','Geography','NumOfProducts','HasCrCard','IsActiveMember']] \nx_test_FS = x_test[['Age','Balance','EstimatedSalary','Gender','Geography','NumOfProducts','HasCrCard','IsActiveMember']]","114ec5ee":"# Feature Scaling\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train_FS)\nx_test = sc.transform(x_test_FS)","01006e5d":"#import required dependencies\nimport tensorflow as tf \nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense , Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adamax , Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","93261280":"#Initialising the ANN\nclassifier = Sequential()\n#add input layer\nclassifier.add(Dense(units=100,kernel_initializer='he_uniform', activation='relu',input_dim=x_train.shape[1]))\n\n#add hiiden layer 1\nclassifier.add(Dense(units=150,kernel_initializer='he_uniform',activation='relu'))\nclassifier.add(Dropout(0.2))\n\n#add hiiden layer 1\nclassifier.add(Dense(units=100,kernel_initializer='he_uniform',activation='relu'))\nclassifier.add(Dropout(0.2))\n\n#add hiiden layer 1\nclassifier.add(Dense(units=100,kernel_initializer='he_uniform',activation='relu'))\nclassifier.add(Dropout(0.2))\n\n#add output layer\nclassifier.add(Dense(units=1,kernel_initializer='glorot_uniform',activation='sigmoid'))\n\n#define compiles\nclassifier.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'],)\n\n#model summary\nclassifier.summary()","2aca0733":"# Fitting the ANN to the Training set\nmodel_history=classifier.fit(x_train, y_train, \n                             validation_split=0.20, \n                             batch_size= 10, \n                             epochs= 100,\n                             callbacks=EarlyStopping(monitor='val_loss',\n                                                     patience=5))","46577764":"Acc = round((model_history.history['accuracy'][-1])*100,4)\nval_Acc = round((model_history.history['val_accuracy'][-1])*100,4)\nloss= round((model_history.history['loss'][-1])*100,4)\nval_loss = round((model_history.history['val_loss'][-1])*100,4)\n\nprint(f' Model Accuracy : {Acc}')\nprint('')\nprint(f' Model Validation Accuracy : {val_Acc}')\nprint('')\nprint(f' Model Loss : {loss}')\nprint('')\nprint(f' Model Validation Loss : {val_loss}')","ec5f0e3b":"#summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","5eb97a56":"# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","20a98ecc":"#predict values\npredicted = classifier.predict(x_test)\nprint('Propability of Predicted values')\nprint('---------------------------------------------------------')\nprint(predicted )\nprint()\nprint('Predicted values as a classes')\nprint('---------------------------------------------------------')\nx = np.array([1 if i> 0.5 else 0 for i in predicted])\nx","523ed84b":"#confusion metrics\nfrom sklearn.metrics import confusion_matrix , accuracy_score\nconfusion_matrix(y_test,pd.Series(x))","576f5bac":"\n#### *Seems like each continues variables doesnt shows no any significient releationship with each other*","f018e7ae":"# **Bank Exits Prediction - Artificial Neural Network with EDA**","fa081e90":"## **Predict Values & Evaluation**","ec662c58":"## **Data Preprocessing** ","5aec4195":"## **Model Evaluation**","2d5da27f":"#### *Lower P values Means Each Features Higherly Associated*","9c18dd95":"#### *F score Higher Means each features highly associated*","a81c9aa8":"![Image](https:\/\/editor.analyticsvidhya.com\/uploads\/50662High-employee-turnover.jpg)","42bba67f":"#### *There are no columns with 0 variance*","aecf3fdc":"## **Feature Selection Process**","07578416":"## **Exploratory Data Analysis & Visualization**","fb5fed64":"## **Import Dataset**","4f12f012":"## **Import Dependencies**","7010cf94":"### Cross Tabulation (Percentage) For Categorical Variable","ab1ff5a9":"## **Model Implementation**","3806c462":"#### *Higher Value Means Gian Higher Information\/important for predicting the target variable*"}}