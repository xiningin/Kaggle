{"cell_type":{"fdeeae5d":"code","e26b17c6":"code","31efcb90":"code","c4e9469c":"code","6e8d7fa6":"code","908bf268":"code","0f4a9fa8":"code","60e00320":"code","c78fa396":"code","da89a0d1":"code","d2cea766":"code","332fcc65":"code","69f9d1e2":"code","cf83bd9a":"code","a6afa65b":"code","5f28a453":"code","d7c8975a":"code","344288de":"code","4fc48928":"code","cc42414c":"code","47b055f9":"code","dadfe4aa":"code","ea205bec":"code","4fbaeafd":"code","7231241b":"code","e8401fa9":"code","e9a291d3":"code","9ab22ffb":"code","c3c6941a":"code","b9380502":"code","f00be238":"code","83989c2d":"code","c29d2366":"code","1da4f1f2":"code","1c99a174":"code","4c82824d":"code","42be94ce":"code","e270bd02":"markdown","82fe09d7":"markdown","21d7788d":"markdown","805afbca":"markdown","1b03489e":"markdown","5bb5ba67":"markdown","b67a5867":"markdown","208b3ef6":"markdown","9fb9adb5":"markdown","bdc0cf3d":"markdown","02832259":"markdown","88501292":"markdown","43e463e5":"markdown","f08ec505":"markdown","7f149d5a":"markdown","1adaec34":"markdown","98593d15":"markdown","90681b92":"markdown","5e15d9fe":"markdown","aec0c0bc":"markdown","d598453f":"markdown","8b2a35bd":"markdown","70de2a24":"markdown","b01a3153":"markdown","9220d5df":"markdown","f64b84fb":"markdown","d62e8b92":"markdown","f02c36df":"markdown","e2499943":"markdown","5e95aa0a":"markdown","a8865fb7":"markdown","3ab5ee05":"markdown"},"source":{"fdeeae5d":"# so let's load our dataset\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\n\n# selected target to drop this column\ny = data.Price","e26b17c6":"print(data.columns)\nprint(\"column size:\", data.columns.size)","31efcb90":"melb_predictors = data.drop(['Price'], axis = 1)","c4e9469c":"print(melb_predictors.columns)\nprint(\"column size:\", melb_predictors.columns.size)","6e8d7fa6":"# let's print out the data types of each column\nfor col in melb_predictors.columns:\n    print(col, data[col].dtype)","908bf268":"# let's ignore the columns having strings (object dtype)\nX = melb_predictors.select_dtypes(exclude = ['object'])","0f4a9fa8":"# let's see the coulmns\nprint(X.columns)\nprint(\"column size:\", X.columns.size)","60e00320":"X","c78fa396":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = .8, test_size = .2, random_state = 0)","da89a0d1":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function returen absolute mean error of two columns\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n    \n    ","d2cea766":"# just have a look to the datasetn to see NaN or missing values\nX_train","332fcc65":"# Fist we'll see how it works for a single column \n# actually, we're going to see if any missing value(NULL or NaN) present or not\n# this line of code below represents missing values as False\nprint(X_train['BuildingArea'].isnull())\n# we can also print the numner of missing values\nprint(\"Total missing values\", sum(X_train['BuildingArea'].isnull()))","69f9d1e2":"X_train[\"BuildingArea\"].isnull().any()","cf83bd9a":"# you're gonna get the names of the columns containing any single missing value\ncols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\ncols_with_missing","a6afa65b":"reduced_X_train = X_train.drop(cols_with_missing, axis = 1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis = 1)\n# let's print how many columns are ramaning now!\nreduced_X_train.columns.size","5f28a453":"# so let's ca;culate the mean error\nprint(\"MAE from Approach 1: Drop Columns with Missing values\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","d7c8975a":"# let's import it from sklearn libranry\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# declare a numpy array\ntrain_array = np.array([[1, 2], [np.nan, 3], [2, 3]])\ntrain_array","344288de":"imp = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n# and fit the imputer on train_array we have just declared\nimp.fit(train_array)","4fc48928":"# so now we're gonna transfornm our train_array with the inmputer ans replace 'nan' with mean of the entire column\nimp.transform(train_array)","cc42414c":"test_array = np.array([[np.nan, 2], [6, np.nan], [7, 6]])\ntest_array","47b055f9":"imp.transform(test_array)","dadfe4aa":"imp = SimpleImputer(missing_values = np.nan, strategy = 'median')\nimp.fit(test_array)","ea205bec":"imp.transform(test_array)","4fbaeafd":"# we're taking the same 'test_array' which we did previously\nimp = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nimp.fit_transform(test_array)","7231241b":"# we're not passing anything, so the default is strategy = 'mean'\nmy_imputer = SimpleImputer()","e8401fa9":"imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))","e9a291d3":"imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))","9ab22ffb":"imputed_X_train","c3c6941a":"imputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","b9380502":"imputed_X_train","f00be238":"print(\"MAE from Approach 2: Imputation\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","83989c2d":"# making a copy of the original data\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()","c29d2366":"# .isnull will return boolian value for each entry of a column\nfor col in cols_with_missing:\n    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()","1da4f1f2":"new_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(new_imputer.fit_transform(X_train_plus))\nimputed_X_valid_plus = pd.DataFrame(new_imputer.transform(X_valid_plus))","1c99a174":"# putting the column name back again\nimputed_X_train_plus.columns = X_train_plus.columns\nimputed_X_valid_plus.columns = X_valid_plus.columns","4c82824d":"print(\"MAE from Approach 3: Extension to Imputation\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))","42be94ce":"print(X_train.shape)\n\n# number of missing values in each column of X_train\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","e270bd02":"see the column names are back! <br>\nSo, it's time to calculate the *mean_absolute_error* of current dataset","82fe09d7":"**Yes**! Let's do it on our dataset, right!","21d7788d":"See the new dataset has everything except *Price* column. We can have a look to the size of the columns now, it's 20!","805afbca":"Ouw!! The score is not improved here than the previous score. :( <br>","1b03489e":"See! Mssing values showed as *True* and others have *False*<br>\nand there are 5156 missing values in this column. Now, just use *.any()* which shows **True** if *missing values > 0*","5bb5ba67":"# Congratualtions!<br>\nWe now know how to deal with missing values in our dataset with atleast three approaches.","b67a5867":"So, we see the imputer has been created as **SimpleImputer()**<br>\nAs the imputer creation is done, we can  use it for our further use on others.","208b3ef6":"As most often we need to calculate our mean_absolute_error function, we're gonna define a function for it here!<br>\n**Note:** *n_estimators* is the number of trees you want to build before taking the maximum voting or averages of predictions of random forest.<br>\nOr, you may think it as the number of trees consisting the random forest.","9fb9adb5":"In this particular notebook, we're gonna learn about the control over missing values in our datasets.<br>\nWe are going to use the dataset from [here](https:\/\/www.kaggle.com\/dansbecker\/melbourne-housing-snapshot) ","bdc0cf3d":"**Wow!** see how this works!<br>\nLet's do it on our dataset on which we're working off.","02832259":"Another thing, if we want both the *.fit()* and *.transform()* work together,  then we just use *fit_transform()*<br>\nrather than  using them individully!","88501292":"But, we wont do *.fit()* on validation data as this one will be for the test purpose.","43e463e5":"Have a check the columns of our dataset","f08ec505":"Let's drop the *Price* column which was our target.","7f149d5a":"Let's make new column(additional) indicating what will be imputed","1adaec34":"So, again do imputation on our dataset","98593d15":"See,  these three columns are containing one or more missing values.<br>\nSo, just drop those columns and calculate *mean_absolute_error* again.","90681b92":"But, here are so many missing values in our dataset and which is a problem! :(<br>\nForget this, and more forward. Let's split the current dataset into **train** and **validation** set.","5e15d9fe":"This is better! comparing our previous error, right! :)<br>\nSo, the **Approach 2** performs better than the **Approach 1** in this example.","aec0c0bc":"see imputer has removed the column name, let's put them again","d598453f":"Now we're gonna deal with the missing values<br>\n# **Approach 1:** Drop Columns with Missing values","8b2a35bd":"Last thing is to calculate the *mean_absolute_error*. Let's do it!","70de2a24":"**So, why did imputation perform better than dropping the columns?**<br>\nThe training data has 10864 rows and 12 columns, where three columns contain missing data. For each column, less than half of the entries are missing.<br>\nThus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better.","b01a3153":"To learn about how imputation actually works, just go through this link https:\/\/scikit-learn.org\/stable\/modules\/impute.html<br>\nHowever, we're going to practice it before implement as it's kinda new to us, right!","9220d5df":"As we already have the **SimpleImputer**() let's use it on another data","f64b84fb":"Now, we're gonna see the third approach.<r>\n# Approach 3: An Extension to Imputation","d62e8b92":"Now, let's make our dataset more interesting by ignoring strings<br>\nas string have less often efects on our model (for such value based predictions particularly)<br>\n**Note:** String data type here is *object*\n\n\n\n","f02c36df":"Simliarly, we can use this with *median* strategy","e2499943":"Now,  we're going to see the second approach.\n# **Approach 2**: Imputation","5e95aa0a":"Well, now the dataset has only 12 columns. Have a look at the dataframe...","a8865fb7":"Finally, to wrap up. We conclude by drafting the missing values<br>\nof that 3 columns containing some missing values","3ab5ee05":"See, in our numpy array, we have the 'nan' as missing value.<br>\nLet's define an imputer on mssing data with the strategy *mean*"}}