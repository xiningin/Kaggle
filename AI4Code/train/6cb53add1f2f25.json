{"cell_type":{"cc418b9a":"code","203d594f":"code","6adcbc3d":"code","cf88f01c":"code","46790b2b":"code","c2c88a81":"code","91e8fc33":"code","eaa14d8b":"code","1b2edee8":"code","e575ff86":"code","2a524aea":"code","a344fa2c":"markdown","6219e9cd":"markdown"},"source":{"cc418b9a":"import copy\nimport gc\nimport glob\nimport os\nimport time\n\nimport cv2\nimport IPython\nimport IPython.display\nimport joblib\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom tqdm import tqdm\n\n%matplotlib inline","203d594f":"pd.options.display.max_columns = 128\npd.options.display.max_rows = 128\nplt.rcParams['figure.figsize'] = (15, 8)","6adcbc3d":"class EasyDict(dict):\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        delattr(self, k)\n        return super(EasyDict, self).pop(k, d)","cf88f01c":"train_df = pd.read_csv('..\/input\/train_curated.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nprint('train: {}'.format(train_df.shape))\nprint('test: {}'.format(sample_submission.shape))\n\nROOT = '..\/input\/'\ntest_root = os.path.join(ROOT, 'test\/')\ntrain_root = os.path.join(ROOT, 'train_curated\/')\n\n\nCONFIG = EasyDict()\nCONFIG.hop_length = 347 # to make time steps 128\nCONFIG.fmin = 20\nCONFIG.fmax = 44100 \/ 2\nCONFIG.n_fft = 480\n\nN_SAMPLES = 48\nSAMPLE_DIM = 256\n\nTRAINING_CONFIG = {\n    'sample_dim': (N_SAMPLES, SAMPLE_DIM),\n    'padding_mode': cv2.BORDER_REFLECT,\n}\n\nprint(CONFIG)\nprint(TRAINING_CONFIG)\n\ntrain_df.head()","46790b2b":"# Preprocessing functions inspired by:\n# https:\/\/github.com\/xiaozhouwang\/tensorflow_speech_recognition_solution\/blob\/master\/data.py\nclass DataProcessor(object):\n    \n    def __init__(self, debug=False):\n        self.debug = debug\n        \n        # Placeholders for global statistics\n        self.mel_mean = None\n        self.mel_std = None\n        self.mel_max = None\n        self.mfcc_max = None\n        \n    def createMel(self, filename, params, normalize=False):\n        \"\"\"\n        Create Mel Spectrogram sample out of raw wavfile\n        \"\"\"\n        y, sr = librosa.load(filename, sr=None)\n        mel = librosa.feature.melspectrogram(y, sr, n_mels=N_SAMPLES, **params)\n        mel = librosa.power_to_db(mel)\n        if normalize:\n            if self.mel_mean is not None and self.mel_std is not None:\n                mel = (mel - self.mel_mean) \/ self.mel_std\n            else:\n                sample_mean = np.mean(mel)\n                sample_std = np.std(mel)\n                mel = (mel - sample_mean) \/ sample_std\n            if self.mel_max is not None:\n                mel = mel \/ self.mel_max\n            else:\n                mel = mel \/ np.max(np.abs(mel))\n        return mel\n    \n    def createMfcc(self, filename, params, normalize=False):\n        \"\"\"\n        Create MFCC sample out of raw wavfile\n        \"\"\"\n        y, sr = librosa.load(filename, sr=None)\n        nonzero_idx = [y > 0]\n        y[nonzero_idx] = np.log(y[nonzero_idx])\n        mfcc = librosa.feature.mfcc(y, sr, n_mfcc=N_SAMPLES, **params)\n        if normalize:\n            if self.mfcc_max is not None:\n                mfcc = mfcc \/ self.mfcc_max\n            else:\n                mfcc = mfcc \/ np.max(np.abs(mfcc))\n        return mfcc\n    \n    def createLogspec(self, filename, params,\n                      normalize=False,\n                      window_size=20,\n                      step_size=10, eps=1e-10):\n        \"\"\"\n        Create log spectrogram,\n        based on \n        https:\/\/www.kaggle.com\/voglinio\/keras-2d-model-5-fold-log-specgram-curated-only\n        \"\"\"\n        \n        y, sr = librosa.load(filename, sr=None)\n        nperseg = int(round(window_size * sr \/ 1e3))\n        noverlap = int(round(step_size * sr \/ 1e3))\n        freqs, times, spec = scipy.signal.spectrogram(\n            y,\n            fs=sr,\n            window='hann',\n            nperseg=nperseg,\n            noverlap=noverlap,\n            detrend=False)\n        spec = np.log(spec.astype(np.float32) + eps)\n        return spec\n        \n    \n    def prepareSample(self, root, row, \n                      preprocFunc, \n                      preprocParams, trainingParams, \n                      test_mode=False, normalize=False, \n                      proc_mode='split'):\n        \"\"\"\n        Prepare sample for model training.\n        Function takes row of DataFrame, extracts filename and labels and processes them.\n        \n        If proc_mode is 'split':\n        Outputs sets of arrays of constant shape padded to TRAINING_CONFIG shape\n        with selected padding mode, also specified in TRAINING_CONFIG.\n        This approach prevents loss of information caused by trimming the audio sample,\n        instead it splits it into equally-sized parts and pads them.\n        To account for creation of multiple samples, number of labels are multiplied to a number\n        equal to number of created samples.\n        \n        If proc_mode is 'resize':\n        Resizes the original processed sample to (SAMPLE_DIM, N_SAMPLES) shape.\n        \"\"\"\n        \n        assert proc_mode in ['split', 'resize', 'raw'], 'proc_must be one of: split, resize, raw'\n        \n        filename = os.path.join(root, row['fname'])\n        if not test_mode:\n            labels = row['labels']\n            \n        sample = preprocFunc(filename, preprocParams, normalize=normalize)\n        # print(sample.min(), sample.max())\n        \n        if proc_mode == 'split':\n            sample_split = np.array_split(\n                sample, np.ceil(sample.shape[1] \/ SAMPLE_DIM), axis=1)\n            samples_pad = []\n            for i in sample_split:\n                padding_dim = SAMPLE_DIM - i.shape[1]\n                sample_pad = cv2.copyMakeBorder(i, 0, 0, 0, padding_dim, trainingParams['padding_mode'])\n                samples_pad.append(sample_pad)\n            samples_pad = np.asarray(samples_pad)\n            if not test_mode:\n                labels = [labels] * len(samples_pad)\n                labels = np.asarray(labels)\n                return samples_pad, labels\n            return samples_pad\n        elif proc_mode == 'resize':\n            sample_pad = cv2.resize(sample, (SAMPLE_DIM, N_SAMPLES), interpolation=cv2.INTER_NEAREST)\n            sample_pad = np.expand_dims(sample_pad, axis=0)\n            if not test_mode:\n                labels = np.asarray(labels)\n                return sample_pad, labels\n            return sample_pad\n        elif proc_mode == 'raw':\n            if not test_mode:\n                return sample, labels\n            return sample\n\n\nprocessor = DataProcessor()","c2c88a81":"DATA_PREFIX = 'logSpectrRaw'\ntrain_filename = 'train_curated_{}.joblib'.format(DATA_PREFIX)\ntest_filename = 'test_{}.joblib'.format(DATA_PREFIX)\n\n\n# Train processing\/loading:\nif os.path.isfile(train_filename):\n    print('load processed train:')\n    train_dict = joblib.load(train_filename)\n    X_train = train_dict['X']\n    y_train = train_dict['y']\n    print(y_train.shape)\nelse:\n    print('process train...')\n    output = Parallel(n_jobs=-3, verbose=1)(\n        delayed(processor.prepareSample)(\n            train_root, \n            train_df.iloc[f, :],\n            processor.createLogspec,\n            CONFIG,\n            TRAINING_CONFIG,\n            test_mode=False,\n            proc_mode='raw',\n        ) for f in range(100))  # change to number of sample in train data for full processing\n    X_train = [x[0] for x in output]\n    X_train = list(map(lambda x: np.expand_dims(x, axis=-1), X_train))\n    y_train = np.array([x[1] for x in output])\n    y_train = pd.Series(y_train).str.get_dummies(sep=',')\n    print(y_train.shape)\n    # Save output for quicker experiments\n    train_dict = {\n        'X': X_train,\n        'y': y_train,\n    }\n    joblib.dump(train_dict, train_filename)\n    \n\n# Test processing\/loading:\nif os.path.isfile(test_filename):\n    print('load processed test:')\n    test_dict = joblib.load(test_filename)\n    X_test = test_dict['X']\n    print(len(X_test))\nelse:\n    print('process test...')\n    X_test = Parallel(n_jobs=-3, verbose=1)(\n        delayed(processor.prepareSample)(\n            test_root, \n            sample_submission.iloc[f, :],\n            processor.createLogspec,\n            CONFIG,\n            TRAINING_CONFIG,\n            test_mode=True,\n            proc_mode='raw',\n        ) for f in range(100))  # change to number of sample in test data for full processing\n    X_test = list(map(lambda x: np.expand_dims(x, axis=-1), X_test))\n    test_dict = {\n        'X': X_test,\n    }\n    joblib.dump(test_dict, test_filename)","91e8fc33":"# Distribution of multilabel labels\nprint('Multilabel class distribution:')\nprint(y_train.sum(axis=1).value_counts())\n\n# Most of the samples belong to only one class.\n# There are some (around 15%), which belong to two classes.\n# Occurrence of samples belonging to more than two classes at once\n# is quite rare, around 1.5%t_","eaa14d8b":"from collections import Counter\nfrom pprint import pprint\n\ndf_lab = (y_train.loc[y_train.sum(axis=1) > 1] > 0)\nmultilabel_combs = []\nfor i in range(df_lab.shape[0]):\n    row_label = df_lab.iloc[i, :][df_lab.iloc[i, :] > 0].index.tolist()\n    multilabel_combs.append(row_label)\n\nmultilabel_comb_counter = Counter(list(map(lambda x: ' + '.join(x), multilabel_combs)))\npprint(multilabel_comb_counter.most_common(20))\n# 20 most common combinations of labels","1b2edee8":"import numpy as np\nimport sklearn.metrics\n\n\n# Based on https:\/\/www.kaggle.com\/voglinio\/keras-2d-model-5-fold-log-specgram-curated-only\n# Core calculation of label precisions for one test sample.\ndef _one_sample_positive_class_precisions(scores, truth):\n    \"\"\"Calculate precisions for each true class for a single sample.\n\n    Args:\n      scores: np.array of (num_classes,) giving the individual classifier scores.\n      truth: np.array of (num_classes,) bools indicating which classes are true.\n\n    Returns:\n      pos_class_indices: np.array of indices of the true classes for this sample.\n      pos_class_precisions: np.array of precisions corresponding to each of those\n        classes.\n    \"\"\"\n    num_classes = scores.shape[0]\n    pos_class_indices = np.flatnonzero(truth > 0)\n    # Only calculate precisions if there are some true classes.\n    if not len(pos_class_indices):\n        return pos_class_indices, np.zeros(0)\n    # Retrieval list of classes for this sample.\n    retrieved_classes = np.argsort(scores)[::-1]\n    # class_rankings[top_scoring_class_index] == 0 etc.\n    class_rankings = np.zeros(num_classes, dtype=np.int)\n    class_rankings[retrieved_classes] = range(num_classes)\n    # Which of these is a true label?\n    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n    retrieved_class_true[class_rankings[pos_class_indices]] = True\n    # Num hits for every truncated retrieval list.\n    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n    precision_at_hits = (\n        retrieved_cumulative_hits[class_rankings[pos_class_indices]] \/\n        (1 + class_rankings[pos_class_indices].astype(np.float)))\n    return pos_class_indices, precision_at_hits\n\n# All-in-one calculation of per-class lwlrap.\n\n\ndef calculate_per_class_lwlrap(truth, scores):\n    \"\"\"Calculate label-weighted label-ranking average precision.\n\n    Arguments:\n      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n        of presence of that class in that sample.\n      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n        test's real-valued score for each class for each sample.\n\n    Returns:\n      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n        class.\n      weight_per_class: np.array of (num_classes,) giving the prior of each\n        class within the truth labels.  Then the overall unbalanced lwlrap is\n        simply np.sum(per_class_lwlrap * weight_per_class)\n    \"\"\"\n    assert truth.shape == scores.shape\n    num_samples, num_classes = scores.shape\n    # Space to store a distinct precision value for each class on each sample.\n    # Only the classes that are true for each sample will be filled in.\n    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n    for sample_num in range(num_samples):\n        pos_class_indices, precision_at_hits = (\n            _one_sample_positive_class_precisions(scores[sample_num, :],\n                                                  truth[sample_num, :]))\n        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n            precision_at_hits)\n    labels_per_class = np.sum(truth > 0, axis=0)\n    weight_per_class = labels_per_class \/ float(np.sum(labels_per_class))\n    # Form average of each column, i.e. all the precisions assigned to labels in\n    # a particular class.\n    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) \/\n                        np.maximum(1, labels_per_class))\n    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n    #                = np.sum(precisions_for_samples_by_classes) \/ np.sum(precisions_for_samples_by_classes > 0)\n    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n    #                = np.sum(per_class_lwlrap * weight_per_class)\n    return per_class_lwlrap, weight_per_class\n\n# Calculate the overall lwlrap using sklearn.metrics function.\n\n\ndef calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0,\n        scores[nonzero_weight_sample_indices, :],\n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\n\n# Accumulator object version.\n\nclass lwlrap_accumulator(object):\n    \"\"\"Accumulate batches of test samples into per-class and overall lwlrap.\"\"\"\n\n    def __init__(self):\n        self.num_classes = 0\n        self.total_num_samples = 0\n\n    def accumulate_samples(self, batch_truth, batch_scores):\n        \"\"\"Cumulate a new batch of samples into the metric.\n\n        Args:\n          truth: np.array of (num_samples, num_classes) giving boolean\n            ground-truth of presence of that class in that sample for this batch.\n          scores: np.array of (num_samples, num_classes) giving the\n            classifier-under-test's real-valued score for each class for each\n            sample.\n        \"\"\"\n        assert batch_scores.shape == batch_truth.shape\n        num_samples, num_classes = batch_truth.shape\n        if not self.num_classes:\n            self.num_classes = num_classes\n            self._per_class_cumulative_precision = np.zeros(self.num_classes)\n            self._per_class_cumulative_count = np.zeros(self.num_classes,\n                                                        dtype=np.int)\n        assert num_classes == self.num_classes\n        for truth, scores in zip(batch_truth, batch_scores):\n            pos_class_indices, precision_at_hits = (\n                _one_sample_positive_class_precisions(scores, truth))\n            self._per_class_cumulative_precision[pos_class_indices] += (\n                precision_at_hits)\n            self._per_class_cumulative_count[pos_class_indices] += 1\n        self.total_num_samples += num_samples\n\n    def per_class_lwlrap(self):\n        \"\"\"Return a vector of the per-class lwlraps for the accumulated samples.\"\"\"\n        return (self._per_class_cumulative_precision \/\n                np.maximum(1, self._per_class_cumulative_count))\n\n    def per_class_weight(self):\n        \"\"\"Return a normalized weight vector for the contributions of each class.\"\"\"\n        return (self._per_class_cumulative_count \/\n                float(np.sum(self._per_class_cumulative_count)))\n\n    def overall_lwlrap(self):\n        \"\"\"Return the scalar overall lwlrap for cumulated samples.\"\"\"\n        return np.sum(self.per_class_lwlrap() * self.per_class_weight())","e575ff86":"# Class for defining transformations\nclass AudioAugment(object):\n    \n    def __init__(self, debug=False):\n        self.debug = debug\n    \n    def doTransform(self, prob=0.5):\n        if np.random.random() < prob:\n            return True\n        return False\n        \n    def horizontalFlip(self, x, prob=0.5):\n        if self.doTransform(prob):\n            x = np.flip(x, axis=1)\n        return x\n    \n    def verticalFlip(self, x, prob=0.5):\n        if self.doTransform(prob):\n            x = np.flip(x, axis=0)\n        return x\n    \n    def padSample(self, x, shape=(128, 128), prob=1, padding_mode=cv2.BORDER_CONSTANT):\n        if self.doTransform(prob):\n            padding_dim_x = shape[0] - x.shape[0]\n            padding_dim_y = shape[1] - x.shape[1]\n            if padding_dim_x > 0:\n                x = cv2.copyMakeBorder(x, 0, 0, padding_dim_x, 0, padding_mode)\n            if padding_dim_y > 0:\n                x = cv2.copyMakeBorder(x, 0, 0, 0, padding_dim_y, padding_mode)\n        return x\n        \n    def randomCrop(self, x, shape=(128, 128), prob=1):\n        if self.doTransform(prob):\n            xmin = np.random.randint(0, x.shape[0] - shape[0])\n            ymin = np.random.randint(0, x.shape[1] - shape[1])\n            xmax = xmin + shape[0]\n            ymax = ymin + shape[1]\n            x = x[xmin:xmax, ymin:ymax]\n        return x\n    \n    def centerCrop(self, x, shape=(128, 128), prob=1):\n        if self.doTransform(prob):\n            xcent = x.shape[0] \/\/ 2\n            ycent = x.shape[1] \/\/ 2\n            xmin = xcent - shape[0] \/\/ 2\n            ymin = ycent - shape[1] \/\/ 2\n            xmax = xcent + shape[0] \/\/ 2\n            ymax = ycent + shape[1] \/\/ 2\n            x = x[xmin:xmax, ymin:ymax]\n        return x\n    \n    def resizeSample(self, x, shape=(128, 128), prob=1, interpolation=cv2.INTER_NEAREST):\n        if self.doTransform(prob):\n            padding_dim_x = shape[0] - x.shape[0]\n            padding_dim_y = shape[1] - x.shape[1]\n            if padding_dim_x < 0 or padding_dim_y < 0 :\n                x = cv2.resize(x, (shape[1], shape[0]), interpolation=interpolation)\n        return x\n    \n    \n# Class for applying augmentations\nclass AudioTransformer(AudioAugment):\n    \n    def __init__(self, transformations=False, transformations_params=False):\n        self.transformations = transformations\n        self.transformations_params = transformations_params\n        \n    def transform(self, x, args_set=None):\n        for i in range(len(self.transformations)):\n            kwargs = self.transformations_params[i]\n            x = self.transformations[i](x, **kwargs)\n        return x","2a524aea":"# Define augmentations for training & validation:\n# Crop padding to be added before random\/center crop \nTRAINING_CONFIG['sample_dim'] = (48, 256)\nsample_crop_pad = 16\n\n\n# Initialize augmentor\naugmentor = AudioAugment()\n\n# Example set of train transformations\ntrain_transformer = AudioTransformer(\n    transformations=[\n        augmentor.padSample,\n        augmentor.randomCrop,\n    ],\n    transformations_params=[\n        {\n            'shape': (TRAINING_CONFIG['sample_dim'][0] + sample_crop_pad,\n                     TRAINING_CONFIG['sample_dim'][1] + sample_crop_pad),\n            'padding_mode': cv2.BORDER_REFLECT,\n        },\n        {\n            'shape': (TRAINING_CONFIG['sample_dim'][0],\n                     TRAINING_CONFIG['sample_dim'][1]),\n            'prob': 1.0,\n        },\n    ]\n)\n\n# Example set of validation transformations\nvalid_transformer = AudioTransformer(\n    transformations=[\n        augmentor.resizeSample,\n        augmentor.centerCrop,\n    ],\n    transformations_params=[\n        {\n            'shape': (TRAINING_CONFIG['sample_dim'][0],\n                      TRAINING_CONFIG['sample_dim'][1]),\n        },\n        {\n            'shape': (TRAINING_CONFIG['sample_dim'][0],\n                      TRAINING_CONFIG['sample_dim'][1]),\n            'prob': 1.0,\n        },\n    ]\n)\n\n\n# Visualize augmented samples:\nx = X_train[np.random.randint(0, len(X_train))][:, :, 0]\nxv = X_train[np.random.randint(0, len(X_train))][:, :, 0]\nprint('orig sample shape: {}'.format(x.shape))\nprint('orig valid sample shape: {}'.format(xv.shape))\nxc = train_transformer.transform(x)\nxcv = valid_transformer.transform(xv)\nprint('processed shapes:')\nprint(xc.shape, xcv.shape)\n\nfig, ax = plt.subplots(1, 2, figsize=(24, 8))\nax[0].imshow(xc)\nax[0].set_title('training augmentation:')\nax[1].imshow(xcv)\nax[1].set_title('valid augmentation:')","a344fa2c":"## Data Processing class:","6219e9cd":"## Configuration and global parameters:"}}