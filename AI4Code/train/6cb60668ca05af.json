{"cell_type":{"2da9fa72":"code","146896d7":"code","ad89ab71":"code","e8626857":"code","5a05ef0c":"code","1c9f9535":"code","41b09d9a":"code","ca7858b7":"code","80cf4bd5":"code","078e454b":"code","b94e1fa2":"code","818729d5":"code","44500029":"code","d7ee462a":"code","a1a3321a":"code","6c62fc32":"code","3a70103b":"code","b127b4e3":"code","ea1e042d":"code","eb8ff80d":"code","56e5cdb2":"code","1060ea08":"code","724ca381":"code","45908d3f":"code","595944e5":"code","8b46c6ca":"code","f47428eb":"code","2471f89d":"code","f10b2bcf":"code","6428f47a":"code","1300ba4a":"code","d7c0651d":"code","1a1397c2":"code","152efe0b":"code","38a12a21":"code","975ebee7":"code","935eca3b":"code","4cb19aee":"code","a8533583":"code","2ef6fcc5":"code","0609931b":"code","49a840b7":"code","a14d1c11":"code","baa92b2d":"code","6785508c":"code","e35bc41f":"code","3babd8f5":"code","bc44fddb":"code","d192f896":"code","0edb1b11":"code","588a5ea1":"code","94c0fc7b":"code","9b43e5c6":"code","bbed041b":"code","ca50e19a":"code","e0d84987":"code","89906b6a":"code","250689d3":"code","05664dc4":"code","823f4529":"markdown","60219c5c":"markdown","4c811602":"markdown","4547cae1":"markdown","99321906":"markdown","a75b8f1b":"markdown","7ed43634":"markdown","a9b6ab01":"markdown","d7552638":"markdown","3ef70d5b":"markdown","986b18f9":"markdown","dcf20048":"markdown","ec751599":"markdown","5d26f8b7":"markdown","64976388":"markdown","906a6cc1":"markdown","55c4cdb8":"markdown","8cf019af":"markdown","6eb7af32":"markdown","37d15b2e":"markdown","2cb67d57":"markdown","15840734":"markdown","15118bfa":"markdown","da941c60":"markdown","732283d5":"markdown","bdedc16c":"markdown","e4e3cee0":"markdown","39a2935c":"markdown","faa5f9f2":"markdown","b71bb8c3":"markdown","d061d0a8":"markdown","d9f1755f":"markdown","656a4d1f":"markdown","c59e2096":"markdown","527f6821":"markdown","299cd7cf":"markdown","2a8b304a":"markdown"},"source":{"2da9fa72":"import numpy as np\nimport pandas as pd\nimport random\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt","146896d7":"pd.options.display.max_columns = None","ad89ab71":"train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n\ntrain['dataset'] = 'train'\ntest['dataset'] = 'test'\n\ndf = pd.concat([train, test])","e8626857":"train.head()","5a05ef0c":"test.head()","1c9f9535":"print('Number of rows in training set:', train.shape[0])\nprint('Number of columns in training set:', train.shape[1] - 1)\n\nprint('Number of rows in test set:', test.shape[0])\nprint('Number of columns in test set:', test.shape[1] - 1)","41b09d9a":"df.info()","ca7858b7":"sample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsample_submission","80cf4bd5":"cp_width = 500\ncp_height = 400\nscatter_size = 600\nWIDTH=800","078e454b":"ds = df.groupby(['cp_type', 'dataset'])['sig_id'].count().reset_index()\n\nds.columns = [\n    'cp_type', \n    'dataset', \n    'count'\n]\n\nfig = px.bar(\n    ds, \n    x='cp_type', \n    y=\"count\", \n    color='dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_type train\/test counts', \n    width=cp_width,\n    height=cp_height\n)\n\nfig.show()","b94e1fa2":"ds = df.groupby(['cp_time', 'dataset'])['sig_id'].count().reset_index()\n\nds.columns = [\n    'cp_time', \n    'dataset', \n    'count'\n]\n\nfig = px.bar(\n    ds, \n    x='cp_time', \n    y=\"count\", \n    color='dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_time train\/test counts', \n    width=cp_width,\n    height=cp_height\n)\n\nfig.show()","818729d5":"ds = df.groupby(['cp_dose', 'dataset'])['sig_id'].count().reset_index()\n\nds.columns = [\n    'cp_dose', \n    'dataset', \n    'count'\n]\n\nfig = px.bar(\n    ds, \n    x='cp_dose', \n    y=\"count\", \n    color='dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_dose train\/test counts', \n    width=cp_width,\n    height=cp_height\n)\n\nfig.show()","44500029":"ds = df[df['dataset']=='train']\nds = ds.groupby(['cp_type', 'cp_time', 'cp_dose'])['sig_id'].count().reset_index()\n\nds.columns = [\n    'cp_type', \n    'cp_time', \n    'cp_dose', \n    'count'\n]\n\nfig = px.sunburst(\n    ds, \n    path=[\n        'cp_type',\n        'cp_time',\n        'cp_dose' \n    ], \n    values='count', \n    title='Sunburst chart for all cp_type\/cp_time\/cp_dose',\n    width=500,\n    height=500\n)\n\nfig.show()","d7ee462a":"train_columns = train.columns.to_list()\n\ng_list = [i for i in train_columns if i.startswith('g-')]\n\nc_list = [i for i in train_columns if i.startswith('c-')]","a1a3321a":"def plot_set_histograms(plot_list, title):\n    fig = make_subplots(\n        rows=4, \n        cols=3\n    )\n    \n    traces = [\n        go.Histogram(\n            x=train[col], \n            nbinsx=100, \n            name=col\n        ) for col in plot_list\n    ]\n\n    for i in range(len(traces)):\n        fig.append_trace(\n            traces[i], \n            (i \/\/ 3) + 1, \n            (i % 3) + 1\n        )\n\n    fig.update_layout(\n        title_text=title,\n        height=1000,\n        width=WIDTH\n    )\n    fig.show()","6c62fc32":"plot_list = [\n    g_list[\n        np.random.randint(0, len(g_list)-1)\n    ] for i in range(50)\n]\n\nplot_list = list(set(plot_list))[:12]\nplot_set_histograms(plot_list, 'Randomly selected gene expression features distributions')","3a70103b":"plot_list = [\n    c_list[\n        np.random.randint(0, len(c_list)-1)\n    ] for i in range(50)\n]\n\nplot_list = list(set(plot_list))[:12]\nplot_set_histograms(plot_list, 'Randomly selected cell expression features distributions')","b127b4e3":"columns = g_list + c_list\nfor_correlation = random.sample(columns, 50)\ndata = df[for_correlation]\n\nf = plt.figure(\n    figsize=(18, 18)\n)\n\nplt.matshow(\n    data.corr(), \n    fignum=f.number\n)\n\nplt.xticks(\n    range(data.shape[1]), \n    data.columns, \n    fontsize=14, \n    rotation=50\n)\n\nplt.yticks(\n    range(data.shape[1]), \n    data.columns, \n    fontsize=14\n)\n\ncb = plt.colorbar()\ncb.ax.tick_params(\n    labelsize=13\n)","ea1e042d":"%%time\n\ncols = ['cp_time'] + columns\nall_columns = list()\nfor i in range(0, len(cols)):\n    for j in range(i+1, len(cols)):\n        if abs(train[cols[i]].corr(train[cols[j]])) > 0.9:\n            all_columns = all_columns + [cols[i], cols[j]]","eb8ff80d":"all_columns = list(set(all_columns))\nprint('Number of columns:', len(all_columns))","56e5cdb2":"data = df[all_columns]\n\nf = plt.figure(\n    figsize=(18, 18)\n)\n\nplt.matshow(\n    data.corr(), \n    fignum=f.number\n)\n\nplt.xticks(\n    range(data.shape[1]), \n    data.columns, \n    fontsize=14, \n    rotation=50\n)\n\nplt.yticks(\n    range(data.shape[1]), \n    data.columns, \n    fontsize=14\n)\n\ncb = plt.colorbar()\n\ncb.ax.tick_params(\n    labelsize=14\n)","1060ea08":"fig = make_subplots(\n    rows=12, \n    cols=3\n)\n\ntraces = [\n    go.Histogram(\n        x=train[col], \n        nbinsx=100, \n        name=col\n    ) for col in all_columns\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 3) + 1, \n        (i % 3) + 1\n    )\n\nfig.update_layout(\n    title_text='Highly correlated features',\n    height=1200\n)\n\nfig.show()","724ca381":"train_target = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\n\nprint('Number of rows: ', train_target.shape[0])\nprint('Number of cols: ', train_target.shape[1])\n\ntrain_target.head()","45908d3f":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values().reset_index()\n\nx.columns = [\n    'column', \n    'nonzero_records'\n]\n\nx = x.tail(50)\n\nfig = px.bar(\n    x, \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the higher number of positive samples (top 50)', \n    width=WIDTH,\n    height=1000\n)\n\nfig.show()","595944e5":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\n\nx.columns = [\n    'column', \n    'nonzero_records'\n]\n\nx = x.tail(50)\n\nfig = px.bar(\n    x, \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the lowest number of positive samples (top 50)', \n    width=WIDTH,\n    height=1000 \n)\n\nfig.show()","8b46c6ca":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\n\nx.columns = [\n    'column', \n    'count'\n]\n\nx['count'] = x['count'] * 100 \/ len(train_target)\n\nfig = px.bar(\n    x, \n    x='column', \n    y='count', \n    orientation='v', \n    title='Percent of positive records for every column in target', \n    width=1200,\n    height=800 \n)\n\nfig.show()","f47428eb":"data = train_target.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\n\ndata.columns = [\n    'row', \n    'count'\n]\n\ndata = data.groupby(['count'])['row'].count().reset_index()\n\nfig = px.bar(\n    data, \n    y=data['row'], \n    x=\"count\", \n    title='Number of activations in targets for every sample', \n    width=WIDTH, \n    height=500\n)\n\nfig.show()","2471f89d":"data = train_target.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\n\ndata.columns = [\n    'row', \n    'count'\n]\n\ndata = data.groupby(['count'])['row'].count().reset_index()\n\nfig = px.pie(\n    data, \n    values=100 * data['row'] \/ len(train_target), \n    names=\"count\", \n    title='Number of activations in targets for every sample (Percent)', \n    width=WIDTH, \n    height=500\n)\n\nfig.show()","f10b2bcf":"train_target.describe()","6428f47a":"%%time\n\ncorrelation_matrix = pd.DataFrame()\n\nfor t_col in train_target.columns:\n    corr_list = list()\n    if t_col == 'sig_id':\n        continue\n    for col in columns:\n        res = train[col].corr(train_target[t_col])\n        corr_list.append(res)\n    correlation_matrix[t_col] = corr_list","1300ba4a":"correlation_matrix['train_features'] = columns\ncorrelation_matrix = correlation_matrix.set_index('train_features')\n\ncorrelation_matrix","d7c0651d":"maxCol=lambda x: max(x.min(), x.max(), key=abs)\n\nhigh_scores = correlation_matrix.apply(maxCol, axis=0).reset_index()\n\nhigh_scores.columns = [\n    'column', \n    'best_correlation'\n]\n\nfig = px.bar(\n    high_scores, \n    x='column', \n    y=\"best_correlation\", \n    orientation='v', \n    title='Best correlation with train columns for every target column', \n    width=1200,\n    height=800\n)\n\nfig.show()","1a1397c2":"col_df = pd.DataFrame()\ntr_cols = list()\ntar_cols = list()\n\nfor col in correlation_matrix.columns:\n    tar_cols.append(col)\n    tr_cols.append(\n        correlation_matrix[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(1).values[0]\n    )\n\ncol_df['column'] = tar_cols\ncol_df['train_best_column'] = tr_cols\n\ntotal_scores = pd.merge(high_scores, col_df)\n\ntotal_scores","152efe0b":"count_features = total_scores['train_best_column'].value_counts().reset_index().sort_values('train_best_column')\n\ncount_features.columns = [\n    'column', \n    'count'\n]\n\ncount_features = count_features.tail(33)\n\nfig = px.bar(\n    count_features, \n    x='count', \n    y=\"column\", \n    orientation='h', \n    title='Columns from training set with number of high correlations with target columns', \n    width=WIDTH,\n    height=700\n)\n\nfig.show()","38a12a21":"target_columns = train_target.columns.tolist()\ntarget_columns.remove('sig_id')\nfor_analysis = [\n    target_columns[\n        np.random.randint(0, len(target_columns)-1)\n    ] for i in range(5)\n]\n\ncurrent_corr = correlation_matrix[for_analysis]","975ebee7":"col_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntar_cols = list()\n\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[0]\n    )\n    tr_second_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[1]\n    )\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\n\ncol_df","935eca3b":"def plot_scatter(col_df, index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_target[col_df.iloc[index]['column']]\n    analysis['x'] = train[col_df.iloc[index]['train_1_column']]\n    analysis['y'] = train[col_df.iloc[index]['train_2_column']]\n    analysis.columns = [\n        'color', \n        col_df.iloc[index]['train_1_column'], \n        col_df.iloc[index]['train_2_column']\n    ]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 12\n\n    fig = px.scatter(\n        analysis, \n        x=col_df.iloc[index]['train_1_column'], \n        y=col_df.iloc[index]['train_2_column'], \n        color=\"color\", \n        size='size', \n        width=scatter_size,\n        height=scatter_size,\n        title='Scatter plot for ' + col_df.iloc[index]['column']\n    )\n    fig.show()","4cb19aee":"plot_scatter(col_df, 0)","a8533583":"plot_scatter(col_df, 1)","2ef6fcc5":"plot_scatter(col_df, 2)","0609931b":"for_analysis = [\n    target_columns[np.random.randint(0, len(target_columns)-1)] for i in range(5)\n]\n\ncurrent_corr = correlation_matrix[for_analysis]\n\ncol_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntr_third_cols = list()\ntar_cols = list()\n\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[0]\n    )\n    tr_second_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[1]\n    )\n    tr_third_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[2]\n    )\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\ncol_df['train_3_column'] = tr_third_cols\n\ncol_df","49a840b7":"def plot_3dscatter(col_df, index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_target[col_df.iloc[index]['column']]\n    analysis['x'] = train[col_df.iloc[index]['train_1_column']]\n    analysis['y'] = train[col_df.iloc[index]['train_2_column']]\n    analysis['z'] = train[col_df.iloc[index]['train_3_column']]\n    analysis.columns = [\n        'color', \n        col_df.iloc[index]['train_1_column'], \n        col_df.iloc[index]['train_2_column'], \n        col_df.iloc[index]['train_3_column']\n    ]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 20\n\n    fig = px.scatter_3d(\n        analysis, \n        x=col_df.iloc[index]['train_1_column'], \n        y=col_df.iloc[index]['train_2_column'],\n        z=col_df.iloc[index]['train_3_column'], \n        color=\"color\", \n        size='size', \n        height=scatter_size,\n        width=scatter_size,\n        title='Scatter plot for ' + col_df.iloc[index]['column']\n    )\n    fig.show()","a14d1c11":"plot_3dscatter(col_df, 0)","baa92b2d":"plot_3dscatter(col_df, 1)","6785508c":"plot_3dscatter(col_df, 2)","e35bc41f":"last_term = dict()\n\nfor item in target_columns:\n    try:\n        last_term[item.split('_')[-1]] += 1\n    except:\n        last_term[item.split('_')[-1]] = 1\n\nlast_term = pd.DataFrame(last_term.items(), columns=['group', 'count'])\nlast_term = last_term.sort_values('count')\nlast_term = last_term[last_term['count']>1]\nlast_term['count'] = last_term['count'] * 100 \/ 206\n\nfig = px.bar(\n    last_term, \n    x='count', \n    y=\"group\", \n    orientation='h', \n    title='Groups in target columns (Percent from all target columns)', \n    width=WIDTH,\n    height=500\n)\n\nfig.show()","3babd8f5":"answer = list()\n\nfor group in last_term.group.tolist():\n    agent_list = list()\n    for item in target_columns:\n        if item.split('_')[-1] == group:\n            agent_list.append(item)\n    agent_df = train_target[agent_list]\n    data = agent_df.astype(bool).sum(axis=1).reset_index()\n    answer.append(data[0].max())","bc44fddb":"ds = pd.DataFrame()\nds['group'] = last_term.group.tolist()\nds['max_value'] = answer\n\nfig = px.bar(\n    ds, \n    x='max_value', \n    y=\"group\", \n    orientation='h', \n    title='Maximum number of active columns in 1 sample for every group', \n    width=WIDTH,\n    height=500\n)\n\nfig.show()","d192f896":"categories = train[['cp_type', 'cp_time', 'cp_dose']]\ntar = train_target.copy()\ntar = tar.drop(['sig_id'], axis=1)\nanalysis = pd.concat([categories, tar], axis=1)","0edb1b11":"for category in analysis['cp_dose'].unique().tolist():\n    \n    number = 0\n    cols = list()\n    \n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_dose'] == category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","588a5ea1":"analysis[analysis['cp_dose'] == 'D2']['atp-sensitive_potassium_channel_antagonist'].value_counts()","94c0fc7b":"analysis[analysis['cp_dose']=='D2']['erbb2_inhibitor'].value_counts()","9b43e5c6":"for category in analysis['cp_time'].unique().tolist():\n    \n    number = 0\n    cols = list()\n    \n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_time']==category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","bbed041b":"analysis[analysis['cp_time'] == 24]['erbb2_inhibitor'].value_counts()","ca50e19a":"analysis[analysis['cp_time'] == 72]['erbb2_inhibitor'].value_counts()","e0d84987":"analysis[analysis['cp_time'] == 24]['atp-sensitive_potassium_channel_antagonist'].value_counts()","89906b6a":"analysis[analysis['cp_time'] == 72]['atp-sensitive_potassium_channel_antagonist'].value_counts()","250689d3":"for category in analysis['cp_type'].unique().tolist():\n    \n    number = 0\n    cols = list()\n    \n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_type'] == category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","05664dc4":"analysis[analysis['cp_type']=='ctl_vehicle']['igf-1_inhibitor'].value_counts()","823f4529":"We can see that we have 872 float features 1 integer (cp_time) and 3 categorical (sig_id, cp_type and cp_dose).","60219c5c":"<a id=\"2\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>2. Categories Visualization<\/center><h2>","4c811602":"Some distribution of randomly selected columns.","4547cae1":"<a id=\"6\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>6. Train & Targets correlations<\/center><h2>","99321906":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n* [1. Basic Data Overview](#1)\n* [2. Categories visualization](#2)\n* [3. Gene and cell features distribution](#3)\n* [4. Training features correlation](#4)\n* [5. Targets analysis](#5)\n* [6. Train & Targets correlations](#6)\n* [7. Targets & Train features dependecies](#7)\n    \n#### I also started second <a href=\"https:\/\/www.kaggle.com\/isaienkov\/moa-prediction-interesting-findings\/\">kernel<\/a> where I will incrementally add interesting finding extracted from this dataset.","a75b8f1b":"Let's check targets.","7ed43634":"Let's do the same but for 3d plots.","a9b6ab01":"We can see that at least 50 target columns have number pf positive samples less than 20 (about 0.1%) !!!","d7552638":"Target columns and pairs of highly correlated features.","3ef70d5b":"Let's select some random columns and see how they deal with pairs of the highly correlated features.","986b18f9":"Let's check problematic columns for dp_dose = 2.","dcf20048":"Now let's see what columns from training set have the higher number of \"high\" correlations with target columns. Every row from chart means that column `A` `N` times has the best value of correlation with different target columns. ","ec751599":"Let's see some correlation between randomly selected variables.","5d26f8b7":"Let's check target columns with categorical columns from training set.","64976388":"We can see that for groups activator, agent, blocker maximum number of active columns in sample is 1.","906a6cc1":"Time to find the most correlated features for every target column.","55c4cdb8":"Is it possible to have more than 1 activation for 1 sample in every group?","8cf019af":"We can see here that about 40% of sample have zeros in all columns and more than 50% have only one active target column.","6eb7af32":"Let's check problematic columns for cp_time = 24 and 72.","37d15b2e":"<a id=\"1\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>1. Basic Data Overview<\/center><h2>","2cb67d57":"<a id=\"5\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>5. Targets analysis<\/center><h2>","15840734":"<a id=\"7\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>7. Targets & Train features dependecies<\/center><h2>","15118bfa":"Here we are going to check categorical features: cp_type, cp_time, cp_dose.","da941c60":"And we have large correlation matrix.","732283d5":"<a id=\"3\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>3. Gene and cell features distribution<\/center><h2>","bdedc16c":"<h1><center>Mechanisms of Action (MoA) Prediction. Data analysis and visualization<\/center><\/h1>\n\n<center><img src=\"https:\/\/pharmacyinnovations.net\/wp-content\/uploads\/pillsdrugs.png\"><\/center>\n","e4e3cee0":"We can extract several group names from target column names. Looks like that last term in column name is definition of a group. Let's extact them and visualize groups with number of columns > 1.","39a2935c":"The biggest number of positive samples for 1 target column is 3.5%. So we deal here with highly imbalanced data.","faa5f9f2":"**test_features.csv** - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.","b71bb8c3":"We can see that for column ```cp_type``` all records where value is ```ctl_vehicle``` for all targets are 0. The same picture for ```cp_time``` == 72 ana == 24, but only for 2 target columns and for ```cp_dose``` == D2 also for 2 target columns.\n","d061d0a8":"In total we have 35 columns that have correlation with at least another 1 higher than 0.9. Let's visualize them.","d9f1755f":"<a id=\"4\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>4. Training features correlation<\/center><h2>","656a4d1f":"Take a look into training and test sets.","c59e2096":"**train_features.csv** - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).","527f6821":"Time to find pairs of features with high correlation.","299cd7cf":"Let's visualize them.","2a8b304a":"Let's see what is the higher value (absolute) of correlation for target columns with every column from train set. Every column on chart is max correlation of current target column with all of columns from training set."}}