{"cell_type":{"254e314f":"code","fc68c0c4":"code","9c03ba1f":"code","f2e51291":"code","49f0927b":"code","364d6514":"code","f4c7b084":"code","efde116a":"code","500de201":"code","2fd25d92":"code","1db83aa4":"code","ebaba171":"code","2195e74d":"code","5d6d5cbb":"code","b1c7a0b0":"code","253dc78a":"code","76e22ad3":"code","dd323f8f":"code","4e69c08f":"code","ca414501":"code","61f94aa1":"code","650392ae":"code","a6da8f59":"code","44f0db88":"code","59d9812b":"code","ff537b9f":"code","eea1c3e4":"code","5f99cdb6":"code","67896232":"code","c5067183":"code","1894986b":"code","e1cabb40":"code","3beeefd6":"code","344ff87e":"code","2b45738e":"code","a8b5a73f":"code","979ac3bc":"code","15e94db5":"code","e51a5a56":"code","6d487efd":"code","87159004":"code","a9d03e1d":"code","f347aa36":"code","7925cc33":"code","ca941f29":"code","1b9bdbe0":"code","e0b13904":"code","86404baf":"code","9020234e":"code","02a92f75":"code","fcb65f71":"code","a1ca507c":"code","e63ba255":"code","e6f8efaa":"code","6f09fbab":"code","50ad35af":"code","7e624f19":"code","16078648":"markdown","04507cf5":"markdown","831603c1":"markdown","71d7942f":"markdown","dba0b879":"markdown","e8d3f442":"markdown","819b99d5":"markdown","8bc9edaf":"markdown","9d8b1c47":"markdown","28d83a5a":"markdown","ba7823ed":"markdown","bb1cb2a7":"markdown","010ce888":"markdown","31105c8a":"markdown","d21e45f2":"markdown","44e9c56c":"markdown","c9fe8265":"markdown","8637de4f":"markdown","e8455946":"markdown","47c63a47":"markdown","6a2a1154":"markdown","76e0711b":"markdown","048aa9ee":"markdown"},"source":{"254e314f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina'","fc68c0c4":"data = pd.read_csv(\"..\/input\/spam.csv\",encoding='latin-1')","9c03ba1f":"data.head()","f2e51291":"#Drop column and name change\ndata = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})","49f0927b":"data.tail()","364d6514":"#Count observations in each label\ndata.label.value_counts()","f4c7b084":"# convert label to a numerical variable\ndata['label_num'] = data.label.map({'ham':0, 'spam':1})","efde116a":"data.head()","500de201":"from sklearn.model_selection import train_test_split","2fd25d92":"X_train,X_test,y_train,y_test = train_test_split(data[\"text\"],data[\"label\"], test_size = 0.2, random_state = 10)","1db83aa4":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","ebaba171":"from sklearn.feature_extraction.text import CountVectorizer","2195e74d":"vect = CountVectorizer()","5d6d5cbb":"vect.fit(X_train)","b1c7a0b0":"print(vect.get_feature_names()[0:20])\nprint(vect.get_feature_names()[-20:])","253dc78a":"X_train_df = vect.transform(X_train)","76e22ad3":"X_test_df = vect.transform(X_test)","dd323f8f":"type(X_test_df)","4e69c08f":"ham_words = ''\nspam_words = ''\nspam = data[data.label_num == 1]\nham = data[data.label_num ==0]","ca414501":"import nltk\nfrom nltk.corpus import stopwords","61f94aa1":"for val in spam.text:\n    text = val.lower()\n    tokens = nltk.word_tokenize(text)\n    #tokens = [word for word in tokens if word not in stopwords.words('english')]\n    for words in tokens:\n        spam_words = spam_words + words + ' '\n        \nfor val in ham.text:\n    text = val.lower()\n    tokens = nltk.word_tokenize(text)\n    for words in tokens:\n        ham_words = ham_words + words + ' '","650392ae":"from wordcloud import WordCloud","a6da8f59":"# Generate a word cloud image\nspam_wordcloud = WordCloud(width=600, height=400).generate(spam_words)\nham_wordcloud = WordCloud(width=600, height=400).generate(ham_words)","44f0db88":"#Spam Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","59d9812b":"#Ham word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","ff537b9f":"prediction = dict()\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train_df,y_train)","eea1c3e4":"prediction[\"Multinomial\"] = model.predict(X_test_df)","5f99cdb6":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report","67896232":"accuracy_score(y_test,prediction[\"Multinomial\"])","c5067183":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train_df,y_train)","1894986b":"prediction[\"Logistic\"] = model.predict(X_test_df)","e1cabb40":"accuracy_score(y_test,prediction[\"Logistic\"])","3beeefd6":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train_df,y_train)","344ff87e":"prediction[\"knn\"] = model.predict(X_test_df)","2b45738e":"accuracy_score(y_test,prediction[\"knn\"])","a8b5a73f":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_df,y_train)","979ac3bc":"prediction[\"random_forest\"] = model.predict(X_test_df)","15e94db5":"accuracy_score(y_test,prediction[\"random_forest\"])","e51a5a56":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier()\nmodel.fit(X_train_df,y_train)","6d487efd":"prediction[\"adaboost\"] = model.predict(X_test_df)","87159004":"accuracy_score(y_test,prediction[\"adaboost\"])","a9d03e1d":"from sklearn.model_selection import GridSearchCV","f347aa36":"k_range = np.arange(1,30)","7925cc33":"k_range","ca941f29":"param_grid = dict(n_neighbors=k_range)\nprint(param_grid)","1b9bdbe0":"model = KNeighborsClassifier()\ngrid = GridSearchCV(model,param_grid)\ngrid.fit(X_train_df,y_train)","e0b13904":"grid.best_estimator_","86404baf":"grid.best_params_","9020234e":"grid.best_score_","02a92f75":"grid.grid_scores_","fcb65f71":"print(classification_report(y_test, prediction['Multinomial'], target_names = [\"Ham\", \"Spam\"]))","a1ca507c":"conf_mat = confusion_matrix(y_test, prediction['Multinomial'])\nconf_mat_normalized = conf_mat.astype('float') \/ conf_mat.sum(axis=1)[:, np.newaxis]","e63ba255":"sns.heatmap(conf_mat_normalized)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","e6f8efaa":"print(conf_mat)","6f09fbab":"pd.set_option('display.max_colwidth', -1)","50ad35af":"X_test[y_test < prediction[\"Multinomial\"] ]","7e624f19":"X_test[y_test > prediction[\"Multinomial\"] ]","16078648":"# 3. Train Test Split\nBefore performing text transformation, let us do train test split. Infact, we can perform k-Fold cross validation. However, due to simplicity, I am doing train test split.","04507cf5":"Now, let's transform the Test data.","831603c1":"# 8. Model Evaluation","71d7942f":"By seeing the above confusion matrix, it is clear that 5 Ham are mis classified as Spam, and 8 Spam are misclassified as Ham. Let'see what are those misclassified text messages. Looking those messages may help us to come up with more advanced feature engineering.","dba0b879":"# 9. Future works","e8d3f442":"### 9.1 Misclassified as Spam","819b99d5":"### 9.2 Misclassfied as Ham","8bc9edaf":"I increased the pandas dataframe width to display the misclassified texts in full width. ","9d8b1c47":"# 2. Import data","28d83a5a":"# 4.Text Transformation\nVarious text transformation techniques such as stop word removal, lowering the texts, tfidf transformations, prunning, stemming can be performed using sklearn.feature_extraction libraries. Then, the data can be convereted into bag-of-words. <br> <br>\nFor this problem, Let us see how our model performs without removing stop words.","ba7823ed":"### 6.1 Multinomial Naive Bayes\nGenerally, Naive Bayes works well on text data. Multinomail Naive bayes is best suited for classification with discrete features. ","bb1cb2a7":"Note : We can also perform tfidf transformation.","010ce888":"# 1. Import libraries","31105c8a":"Let's drop the unwanted columns, and rename the column name appropriately.","d21e45f2":"### 6.4 Ensemble classifier","44e9c56c":"Based, on the above four ML models, Naive Bayes has given the best accuracy. However, Let's try to tune the parameters of $k$-NN using GridSearchCV","c9fe8265":"# 7. Parameter Tuning using GridSearchCV","8637de4f":"### 6.2 Logistic Regression","e8455946":"# 6. Machine Learning models:","47c63a47":"vect.fit function learns the vocabulary. We can get all the feature names from vect.get_feature_names( ). <br> <br> Let us print first and last twenty features","6a2a1154":"# 5. Visualisations ","76e0711b":"It seems length of the spam text is much higher than the ham. Maybe we can include length as a feature.  In addition to unigram, we can also try bigram features. ","048aa9ee":"### 6.3 $k$-NN classifier"}}