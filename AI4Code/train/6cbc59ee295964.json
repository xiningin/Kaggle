{"cell_type":{"c7c6aa98":"code","3fa8c4fb":"code","6b81311a":"code","017a05bf":"code","fd0cee4f":"code","0462ba6c":"code","8611ac6c":"code","4dab4f88":"code","20020d9a":"code","ef20c065":"code","1a14f0db":"code","403995c6":"code","4789f122":"code","f7519002":"code","91a48dd8":"code","10a172d1":"code","28c7f474":"code","06a8c35d":"code","d371fad0":"code","fb0ff919":"code","f398cb26":"code","a659df20":"code","ee32a8f8":"code","386b8b4c":"code","98549ea9":"code","c00129cd":"code","edaf4268":"code","e77c9846":"code","3df21db9":"code","fc5fbc56":"code","36df1642":"code","caf96993":"code","ffbe98a9":"code","b0c865a8":"code","7b52a90f":"code","9210b1eb":"code","6a8cdddb":"code","081d1332":"code","252cdbb0":"code","5a221562":"code","f02e6d6c":"code","2b20b68b":"code","18a9f4ab":"code","8e466124":"code","66a26a3b":"markdown","612cee75":"markdown","731bfa51":"markdown","2d619ebd":"markdown","df5a196c":"markdown","e4590b55":"markdown","1ad8394c":"markdown","24d71512":"markdown","2e7e4958":"markdown","58fab949":"markdown","18ca9c20":"markdown","92186058":"markdown","1e183229":"markdown"},"source":{"c7c6aa98":"import numpy as np","3fa8c4fb":"corpus = [\n     'This is the first document.',\n     'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]","6b81311a":"from sklearn.feature_extraction.text import CountVectorizer","017a05bf":"# If binary is false, it uses word frequency, otherwise just 0 or 1\ncount_vectorizer = CountVectorizer(strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,\n                                   stop_words=None, ngram_range=(1, 2), analyzer='word',\n                                   max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False)","fd0cee4f":"data = count_vectorizer.fit_transform(corpus)","0462ba6c":"data.toarray()","8611ac6c":"count_vectorizer.vocabulary_","4dab4f88":"# Extracting words index form corpus\nvect = CountVectorizer()\nvect.fit_transform(corpus)\nword_index = vect.vocabulary_\nword_index","20020d9a":"from sklearn.feature_extraction.text import TfidfVectorizer","ef20c065":"tfidf = TfidfVectorizer(strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,\n                        analyzer='word', stop_words=None, ngram_range=(1, 2),\n                        max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False,\n                        norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)","1a14f0db":"data = tfidf.fit_transform(corpus)","403995c6":"data.toarray()","4789f122":"tfidf.vocabulary_","f7519002":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(corpus)\n\ntokenizer.word_index","91a48dd8":"import gensim.downloader as api","10a172d1":"wv = api.load('word2vec-google-news-300')","28c7f474":"word_index","06a8c35d":"embedding_matrix = np.zeros((len(tokenizer.word_index), 300))\nfor word, i in tokenizer.word_index.items():\n    try:\n        embedding_matrix[i] = wv.get_vector(word)\n    except KeyError:\n        print(f\"{word} not found\")","d371fad0":"embedding_matrix.shape","fb0ff919":"from gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom pathlib import Path","f398cb26":"output_file = Path(\".\/test_word2vec.txt\").touch()","a659df20":"# Coverting and loading glove through gensim word2vec interface\n_ = glove2word2vec(\"..\/input\/glove6b\/glove.6B.100d.txt\", \".\/test_word2vec.txt\")\nglove = KeyedVectors.load_word2vec_format(\".\/test_word2vec.txt\")","ee32a8f8":"embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\nfor word, i in tokenizer.word_index.items():\n    try:\n        embedding_matrix[i] = glove.get_vector(word)\n    except KeyError:\n        print(f\"{word} not found\")\n    \nembedding_matrix.shape","386b8b4c":"import fasttext\nimport fasttext.util","98549ea9":"fasttext.util.download_model('en', if_exists='ignore')","c00129cd":"ft = fasttext.load_model('cc.en.300.bin')","edaf4268":"embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, ft.get_dimension()))\nfor word, i in tokenizer.word_index.items():\n    embedding_matrix[i] = ft.get_word_vector(word)    # fasttext can handle out of memory words because it uses sub-word embeddings\n\nembedding_matrix.shape","e77c9846":"import spacy","3df21db9":"nlp = spacy.load(\"en_core_web_lg\")","fc5fbc56":"tokens = nlp(\" \".join(tokenizer.word_index.keys()))","36df1642":"embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\nfor index, tok in enumerate(tokens):\n    embedding_matrix[tokenizer.word_index[tok.text]] = tok.vector\n\nembedding_matrix.shape","caf96993":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument","ffbe98a9":"tagged_data = [TaggedDocument(doc.lower().split(), [i]) for i, doc in enumerate(corpus)]   # Using whitespace tokenize method\ntagged_data","b0c865a8":"model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 50, dm=1)","7b52a90f":"model.wv.vocab","9210b1eb":"model.infer_vector(\"Where is third document?\".lower().split())","6a8cdddb":"import tensorflow_hub as hub","081d1332":"embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")","252cdbb0":"embeddings = embed(corpus)\nembeddings","5a221562":"print(embeddings.shape)","f02e6d6c":"# !pip install sentence-transformers","2b20b68b":"from sentence_transformers import SentenceTransformer","18a9f4ab":"model = SentenceTransformer('paraphrase-distilroberta-base-v1')","8e466124":"sentence_embeddings = model.encode(\"Where is third document?\")\n\nsentence_embeddings.shape","66a26a3b":"# Sentence Embeddings","612cee75":"# TF-IDF","731bfa51":"Spacy also provides way to convert other embeddings in its own format  \nhttps:\/\/spacy.io\/usage\/vectors-similarity#converting","2d619ebd":"## [Doc2vec](https:\/\/radimrehurek.com\/gensim\/models\/doc2vec.html)","df5a196c":"## [Sentence Bert](https:\/\/github.com\/UKPLab\/sentence-transformers)","e4590b55":"## Spacy","1ad8394c":"## [Glove](https:\/\/nlp.stanford.edu\/projects\/glove\/)","24d71512":"## [Universal Sentence Encoder](https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4)","2e7e4958":"## [Word2vec](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html)","58fab949":"# Word Embeddings","18ca9c20":"# One Hot Encoding","92186058":"## [FastText](https:\/\/fasttext.cc\/)","1e183229":"Glove file format \n\nWord its vector\n\nbird 0.78675 0.079368 -0.76597 0.1931 0.55014 0.26493 -0.75841 -0.8818 1.6468 -0.54381 0.33519 0.44399 1.089 0.27044 0.74471 0.2487 0.2491 -0.28966 -1.4556 0.35605 -1.1725 -0.49858 0.35345 -0.1418 0.71734 -1.1416 -0.038701 0.27515 -0.017704 -0.44013 1.9597 -0.064666 0.47177 -0.03 -0.31617 0.26984 0.56195 -0.27882 -0.36358 -0.21923 -0.75046 0.31817 0.29354 0.25109 1.6111 0.7134 -0.15243 -0.25362 0.26419 0.15875  \nunlike 0.40351 0.25297 0.058938 0.016263 -0.16723 0.49866 -0.46665 -0.39667 -0.42482 0.22456 0.17972 0.31613 0.17297 0.25262 0.21552 0.4215 0.092249 0.26599 -0.079251 -0.50896 -0.30306 -0.42921 0.1109 0.4603 -0.072813 -1.1166 -0.6532 -0.16932 0.0043999 -0.035547 2.3802 -0.17461 0.0065762 -0.57749 0.17811 0.080492 -0.087587 0.072499 -0.75795 -0.15779 -0.15288 0.24441 0.076898 0.52795 0.16373 0.37204 -0.21652 -0.3913 -0.56267 0.10644\n"}}