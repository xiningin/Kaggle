{"cell_type":{"2b90b509":"code","762928b8":"code","a67fe561":"code","38099d81":"code","89c6e7e6":"code","3d8770c6":"code","6f34bfeb":"code","6a0169a0":"code","84539d24":"code","d8579539":"code","4570b49b":"code","d40ad1d3":"code","8390fbe9":"code","9c5d8651":"code","f52a1c92":"code","cd823564":"code","adc53804":"code","c55c0a6b":"code","3a783785":"code","8e0d57ea":"code","7bccb3ca":"code","d2ba211b":"code","6428adc7":"code","e3b9c92c":"code","8be0d0c9":"code","0d6d2816":"code","00cf9ddd":"code","ad778f23":"code","50486577":"code","e71dd394":"code","be616951":"code","2c15e7da":"code","bdf15ad9":"code","b04be349":"code","a6bb1784":"code","0cba7bb1":"code","573b2f4b":"code","5f90bfab":"code","99287d05":"code","0e02f7ec":"code","8dd8509a":"code","d476748c":"markdown","f2d443c2":"markdown","cacf5f0d":"markdown","aa8d20ea":"markdown","91ea2b2f":"markdown","2af75c28":"markdown","40f3d6c8":"markdown","467b658b":"markdown","25bc4764":"markdown","986f2d0d":"markdown","6b68b4b5":"markdown","6fd701b4":"markdown","f92680db":"markdown","ebc43ade":"markdown","0f9f2b93":"markdown","18b0edb4":"markdown","92a15e87":"markdown","868b8aad":"markdown","6261cec4":"markdown","e832d73e":"markdown","ce5b6f7f":"markdown","0408b339":"markdown","c4e32f8a":"markdown","8cd8afc7":"markdown","786db895":"markdown","32571404":"markdown","3ff4fb83":"markdown","2b2973a2":"markdown","ecade451":"markdown","868c50fa":"markdown","bee9a143":"markdown","94ad2b45":"markdown","2a4047a3":"markdown","ed7388a3":"markdown","f22781b9":"markdown","cd75d855":"markdown","b3e22363":"markdown"},"source":{"2b90b509":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","762928b8":"df_main = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","a67fe561":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","38099d81":"df = df_main.copy()\ndf.head()","89c6e7e6":"#Dropping duplicates\n\ndf.drop_duplicates(inplace = True)\ndf.reset_index(drop = True, inplace = True)\ndf.shape","3d8770c6":"#Ensuring all data are in appropriate format\n\ndf.info()","6f34bfeb":"#Checking for missing values\nmissing_data = df.isnull()\nmissing_data.sum()","6a0169a0":"df.describe()","84539d24":"fig, ax = plt.subplots(ncols = 3, nrows = 4, figsize = (30, 40), squeeze=False)\nindex = 0\nax = ax.flatten()\n\nfor col in list(df.columns):\n    sns.distplot(df[col], ax=ax[index], hist=True)\n    index += 1\n\nplt.draw()","d8579539":"#Counting the number of samples per quality class\n\ndf['quality'].value_counts().sort_index()","4570b49b":"sns.pairplot(df, diag_kind='hist', corner=True)\nplt.show()","d40ad1d3":"df_log = df.copy()\ndf_log.loc[df_log['citric acid'] == 0, 'citric acid'] += 0.001\n\ndf_log.loc[:, ~df.columns.isin(['pH', 'quality'])] = np.log(df_log)\ndf_log.head()","8390fbe9":"#Correlation heatmap\n\nhmap_mask = np.triu(df_log.corr(), k=1)\nplt.rc('font', size=10)\nplt.figure(figsize=(10,6))\n\nsns.heatmap(df.corr(), mask=hmap_mask, annot = True, fmt='.2f', cmap='coolwarm', annot_kws={\"fontsize\":10})\nplt.show()","9c5d8651":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import classification_report","f52a1c92":"#Splitting data into features (X) and labels (y):\n\nX = df_log.drop('quality', axis = 1)\ny = df_log.iloc[:,-1:]","cd823564":"#Normalising the values with standard scaler:\n\nSt_sc = StandardScaler()\nX_norm=X.copy()\nX_norm[X_norm.columns]=pd.DataFrame(St_sc.fit_transform(X_norm))\nX_norm.head()","adc53804":"#Defining the stratified k-Fold cross validation with 10 splits\n\ncv = StratifiedKFold(n_splits=10, random_state=None, shuffle=False)","c55c0a6b":"#Defining dataframe to save result\n\ndf_reg_result = pd.DataFrame(index=['Linear Regression', 'XGBoost Regression', 'Random Forest Regression', 'Support Vector Regression'], columns=['Mean absolute error', 'R^2 score'])","3a783785":"from sklearn.linear_model import LinearRegression\n\nLModel = LinearRegression()","8e0d57ea":"y_LM = cross_val_predict(LModel, X_norm, y, cv=cv, n_jobs=2)\nMAE_LM = round(mean_absolute_error(y , y_LM),3)\nr2_LM = round(r2_score(y, y_LM),3)\ndf_reg_result['Mean absolute error']['Linear Regression']=MAE_LM\ndf_reg_result['R^2 score']['Linear Regression']=r2_LM\nprint(('MAE, Linear Regression').ljust(30), ': %.3f' % MAE_LM)\nprint(('R2 score, Linear Regression').ljust(30), ': %.3f' % r2_LM)","7bccb3ca":"from xgboost import XGBRegressor","d2ba211b":"XGBModel_best = XGBRegressor(learning_rate = 0.01, max_depth=3, n_estimators=425)","6428adc7":"y_XGB = cross_val_predict(XGBModel_best, X_norm, y, cv=cv, n_jobs=2)\nMAE_XGB = round(mean_absolute_error(y , y_XGB),3)\nr2_XGB = round(r2_score(y, y_XGB),3)\ndf_reg_result['Mean absolute error']['XGBoost Regression']=MAE_XGB\ndf_reg_result['R^2 score']['XGBoost Regression']=r2_XGB\nprint(('MAE, XGBoost Regression').ljust(30), ': %.3f' % MAE_XGB)\nprint(('R2 score, XGBoost Regression').ljust(30), ': %.3f' % r2_XGB)","e3b9c92c":"from sklearn.ensemble import RandomForestRegressor","8be0d0c9":"np.random.seed(0)\nRFModel_best = RandomForestRegressor(max_depth=7, n_estimators=100)\ny_RF = cross_val_predict(RFModel_best, X_norm, y, cv=cv, n_jobs=2)\nMAE_RF = round(mean_absolute_error(y , y_RF),3)\nr2_RF = round(r2_score(y, y_RF),3)\ndf_reg_result['Mean absolute error']['Random Forest Regression']=MAE_RF\ndf_reg_result['R^2 score']['Random Forest Regression']=r2_RF\nprint(('MAE, Random Forest Regression').ljust(30), ': %.3f' % MAE_RF)\nprint(('R2 score, Random Forest Regression').ljust(30), ': %.3f' % r2_RF)","0d6d2816":"from sklearn.svm import SVR","00cf9ddd":"SVRModel_best = SVR(kernel='rbf', C=0.5, gamma= 0.05, epsilon=9e-10)","ad778f23":"y_SVR = cross_val_predict(SVRModel_best, X_norm, y, cv=cv, n_jobs=2)\nMAE_SVR = round(mean_absolute_error(y , y_SVR),3)\nr2_SVR = round(r2_score(y, y_SVR),3)\ndf_reg_result['Mean absolute error']['Support Vector Regression']=MAE_SVR\ndf_reg_result['R^2 score']['Support Vector Regression']=r2_SVR\nprint(('MAE, Support Vector Regression').ljust(30), ': %.3f' % MAE_SVR)\nprint(('R2 score, Support Vector Regression').ljust(30), ': %.3f' % r2_SVR)","50486577":"arr_y_reg=np.column_stack([y_LM, y_XGB, y_RF, y_SVR])\n\ndf_y_reg = pd.DataFrame(arr_y_reg, columns=['Linear Regression', 'XGBoost Regression', 'Random Forest Regression', 'Support Vector Regression'])","e71dd394":"fig, ax = plt.subplots(ncols = 2, nrows = 2, squeeze=False, figsize = (20, 15))\nindex = 0\nax = ax.flatten()\nplt.subplots_adjust(wspace=0.15, \n                    hspace=0.2)\n\nparams = {'axes.titlesize': 30, 'xtick.labelsize':18, 'ytick.labelsize':18}\nplt.rcParams.update(params)\n\ntext_box = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n\nfor col in df_y_reg:\n    sns.regplot(y['quality'], df_y_reg[col], ax=ax[index])\n    ax[index].set_title(col, fontsize=20)\n    \n    if (index % 2) == 0:\n        ax[index].set_ylabel('Predicted value\\n', fontsize=20)\n    else:\n        ax[index].set(ylabel=None)\n    \n    if index < 2:\n        ax[index].set(xlabel=None)\n        ax[index].tick_params(labelbottom=False)\n    else:\n        ax[index].set_xlabel('\\nActual Value', fontsize=20)\n\n    textstr = '\\n'.join(('$MAE = %.3f$' % (df_reg_result['Mean absolute error'][col], ),\n    '$R^2 score = %.3f$' % (df_reg_result['R^2 score'][col], )))\n    \n    ax[index].text(0.05, 0.95, textstr, transform=ax[index].transAxes, fontsize=18, verticalalignment='top', bbox=text_box)\n    index += 1\n\nplt.setp(ax, xlim=(2.5,8.5), ylim=(2.5,8.5))\nplt.draw()","be616951":"y_SVR_mat = pd.DataFrame(columns=['Actual', 'Predicted', 'Round_0.5', 'Round_1.0'])\ny_SVR_mat['Actual'] = y.copy()\ny_SVR_mat['Predicted'] = np.round(y_SVR,3)\ny_SVR_mat['Round_0.5']= np.round(y_SVR)\ny_SVR_mat['Round_1.0'] = np.where(abs(y_SVR_mat['Actual']-y_SVR_mat['Predicted'])<1, y_SVR_mat['Actual'], y_SVR_mat['Round_0.5'])\ny_SVR_mat.head()","2c15e7da":"cm_SVR_05 = confusion_matrix(y_SVR_mat['Actual'], y_SVR_mat['Round_0.5'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_SVR_05, display_labels=np.unique(y_SVR_mat['Actual']))\nplt.rcParams.update(plt.rcParamsDefault)\ndisp.plot()\nplt.title('Support Vector Regressor \\nTolerance (+\/- 0.5)')\nplt.xlabel('\\nPredicted quality', fontsize=12)\nplt.ylabel('True quality\\n', fontsize=12)\nplt.show()","bdf15ad9":"print(classification_report(y_SVR_mat['Actual'], y_SVR_mat['Round_0.5'], digits=3))","b04be349":"cm_SVR_10 = confusion_matrix(y_SVR_mat['Actual'], y_SVR_mat['Round_1.0'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_SVR_10, display_labels=np.unique(y_SVR_mat['Actual']))\nplt.rcParams.update(plt.rcParamsDefault)\ndisp.plot()\nplt.title('Support Vector Regressor \\nTolerance (+\/- 1.0)')\nplt.xlabel('\\nPredicted quality', fontsize=12)\nplt.ylabel('True quality\\n', fontsize=12)\nplt.show()","a6bb1784":"print(classification_report(y_SVR_mat['Actual'], y_SVR_mat['Round_1.0'], digits=3))","0cba7bb1":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","573b2f4b":"#Setting the weights of every class to compensate for the imbalance in the dataset\n\nweights_RFC = {3:5.0, 4:7.0, 5:1.0, 6:1.25, 7:3.0, 8:5.0}","5f90bfab":"np.random.seed(0)\n\nmodel_RFC_best = RandomForestClassifier(max_depth=10, n_estimators=500, class_weight=weights_RFC)\ny_RFC_6 = cross_val_predict(model_RFC_best, X_norm, y, cv=cv, n_jobs=2)\n\nRFC_6_acc = accuracy_score(y['quality'], y_RFC_6)\nprint(\"Accuracy for binary classification with RFC is: \" + str(round(RFC_6_acc*100,2)) + \" %\")","99287d05":"cm_RF = confusion_matrix(y['quality'], y_RFC_6)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_RF, display_labels=np.unique(y['quality']))\ndisp.plot()\nplt.title('Random Forest Classifier \\nConfusion Matrix\\n')\nplt.xlabel('\\nPredicted quality')\nplt.ylabel('True quality\\n')\nplt.show()","0e02f7ec":"print(classification_report(y['quality'], y_RFC_6, digits=3))","8dd8509a":"np.random.seed(0)\nmodel_RFC_best.fit(X_norm, y)\n# get importance\nimportance = model_RFC_best.feature_importances_\n\n# summarize feature importance\nfig, ax = plt.subplots()\nax.barh(X_norm.columns, importance*100)\nax.set_title('Feature importance\\n')\nax.set_xlabel('\\nImportance [%]')\nplt.show()","d476748c":"This is a very similar result to the support vector regressor with slightly better precision values for quality class 4.","f2d443c2":"The correlation coefficients help us to:\n1. estimate the effect size of every feature (Thresholds for +\/- relationships 0.0: No relationship, 0.3: Weak relationship, 0.5: Moderate relationship, >0.7: Strong relationship, 1.0: Perfect relationship.) &\n2. to decide if we need to drop any one of two features which are strongly correlated.\n\nBased on the correlation coefficients we can estimate following order of size effect on quality:\nalcohol(+) < volatile acidity (-) < sulphates (+) < citric acid (+) < density (-) = total sulphur dioxide (-) < chlorides (-) < fixed acidity (+) < pH (-) < free sulphur dioxide (-) < residual sugar\n\nSince no independent variables have a correlation > 0.7 we don't drop any of them. Let's start modelling!","cacf5f0d":"# Conclusion\n\n* Both models exhibit very high levels of utility as a decision support system for an oenologist.\n* Mean absolute error achieved is in line with the initial Bayesian error estimate.\n* High precision of predicted values within a tolerance of +\/- 1.0 were achieved.\n* The precision of the models can be greatly improved for the scale of 0 - 10 if a balanced dataset is provided.","aa8d20ea":"Let's try one more - support vector regressor.","91ea2b2f":"# Classification","2af75c28":"# Exploratory data analyis","40f3d6c8":"By adding to the diagonal elements the one neighbouring element above and below we arrive at the following precision values within a tolerance level of +\/- 1.0 for predicted values in the continuous scale of 0 - 10:\n\n**Rating-1:** 0%;    **Rating-2:** 0%;    **Rating-3:** 0%; **Rating-4:** 83.3%; **Rating-5:** 97.2%; **Rating-6:** 95.5%; **Rating-7:** 94.2%; **Rating-8:** 33.3%; **Rating-9:** 0%; **Rating-10:** 0%;","467b658b":"Again, I arrived at the following hyperparameters after carrying out a grid search to minimise the loss function, mean absolute error.","25bc4764":"# Bayesian Error - Human sensors\n\nSensory perception of taste is a purely neurophysiological phenomenon in our consciousness generated in the brain which is highly susceptible to noise and bias owing to the prior state of the brain, i.e. evolutionary genetics, personal genetics and personal life experiences (eg. wine tasting, mood etc) and only partly relies on the sensory input. This fact of life makes humans poor sensors which will most certainly reflect in the Bayesian error for this dataset. Some ways to minimise this Bayesian error would be:\n\n1. Have the same judges in the same number for every wine\n2. Increase the number of judges per wine\n3. Limit the number of wines rated per day\n4. Ensure similar class distribution of wines rated per day (for eg. 1x bad, 2x average, 1x good)\n5. Ensure each of the judges are in a similar state of mind on different days\n\nGiven these uncertainties, a **Bayesian error of +\/-0.5** seems perfectly plausible on a continous scale of 0 - 10. Nonetheless, the purpose of the model is to provide decision support to an oenologist's wine tasting evaluations and improve wine production. Hence, the rating method is adequate.","986f2d0d":"## Linear Regression\n\nFirst, let's start with a simple multiple linear regression model.","6b68b4b5":"## XGBoost Regressor","6fd701b4":"Sadly, not much of an improvement really.","f92680db":"## Summary Regression","ebc43ade":"## Support Vector Regressor","0f9f2b93":"Next we represent correlations between all variables as a heatmap.","18b0edb4":"# Weber-Fechner Law in psychophysics\n\nPsychophysics is the study of quantitative relations between psychological events and physical events or, more specifically, between sensations and the stimuli that produce them. According to the Weber-Fechner Law, the relationship between stimulus and perception is logarithmic. This is indeed well known for the perception of the intensities of sounds and lights etc. This is attributed to the response of neurons that is in proportion to the logarithm of stimulus intensity.\n\nSee ref: https:\/\/en.wikipedia.org\/wiki\/Weber%E2%80%93Fechner_law","92a15e87":"# Regression","868b8aad":"## Feature importancee\n\nFinally, let's have a look at the feature importances.","6261cec4":"We see that the mean absolute error with all the models are pretty close to our Bayesian error estimate of +\/- 0.5. Clearly, the Support Vector Regressor gives the best results.","e832d73e":"## Evaluation Metric\nThe metric used to evaluate and compare the models will be mean absolute error (MAE). For the purpose of providing decision support to an oenologist a model with an error of **+\/- 0.5** can be very useful. Although this may account for a 5% error margin on a continuous scale of 0 - 10, given the Bayesian error estimate and the inadequacy of the dataset in number of samples for quality ratings lesser or greater than 5 & 6, it is perfectly acceptable.","ce5b6f7f":"Ideally, we'd like to see fairly equal number of wines in every class or rating. We see that the dataset is highly skewed with good number of samples for ratings 5 & 6, and hopelessly few samples for ratings 3, 4 & 8. This is expected if the sampling is done randomnly as the wines will indeed be normally distributed around the mean. Sadly, this means we cannot expect good accuracies and will just have to make the best out of this dataset.","0408b339":"## Random Forest Regressor","c4e32f8a":"We see that most features have a very weak relationship with the quality of the wine.","8cd8afc7":"Let's view the confusion matrix for a tolerance of +\/- 0.5.","786db895":"## Random Forest Classifier\nNow let us build a classification model for comparison. I choose the Random Forest Classifier as it allows one to easily tune the weights in each class and as well as view the feature importances easily.","32571404":"Evidently, the three most important % alcohol, sulphates and volatile acidity.","3ff4fb83":"Let's see how we fare with the XGBoost Regressor.","2b2973a2":"Let's have a look at the features and labels.","ecade451":"There is improvement over the linear regression model. Let's try a random forest regressor next.","868c50fa":"## Support Vector Regressor -> Classifier\n\nLet us view the above regression results in a confusion matrix by rounding the predictions into whole number class bins. To do this we round the predictions to the nearest integer value. This in effect gives us the precision of the model within a tolerance of +\/- 0.5. Moreover, in order to evaluate the precision of the model within a tolerance of +\/- 1.0 we round the predicted values to the actual values when the absolute value of the error < 1.","bee9a143":"I arrived at the best hyperparameters used below after carrying out a grid search for learning rate, maximum depth and number of estimators to minimise mean absolute error.","94ad2b45":"Defining support vector regressor with hyperparameters picked by means of a grid search.","2a4047a3":"As you can see, with a tolerance level of +\/- 1.0 we arrive at the following precision values for predicted values in the continuous scale of 0 - 10:\n\n**Rating-1:** 0%;\n**Rating-2:** 0%;\n**Rating-3:** 0%;\n**Rating-4:** 100%;\n**Rating-5:** 87.8%;\n**Rating-6:** 85.6%;\n**Rating-7:** 94.3%;\n**Rating-8:** 100%;\n**Rating-9:** 0%;\n**Rating-10:** 0%;","ed7388a3":"Let's see how the features and labels are correlated with a pairplot.","f22781b9":"# Introduction\n\nIn the paper \"Modeling wine preferences by data mining from physicochemical properties\" - Cortez et al. 2009, the authors note that \"each sample was evaluated by a ***minimum of three*** sensory assessors (using blind tastes) who graded the wine in a scale that ranges from 0 (very bad) to 10 (excellent). The final sensory score is given by the median of these evaluations.\"\n\nIt is clear from the statement that multiple human judges who were not the same for every sample, and indeed in different numbers with a minimum of three per wine, were involved in tasting and rating the wines.","cd75d855":"## Logarithmic transformation\n\nAs noted in an earlier section, a logarithmic transformation of the features is in line with the Weber-Fechner Law in psychophysics. Care is taken to NOT apply this to the feature **pH**. This is due to the fact that pH is already calculated as logarithm of the concentration of H+ radicals. The base-10 logarithm of pH is not a concern as I will be normalising the dataset prior to modelling.\n\nAs seen under df.describe() in the feature column 'citric acid' there are values = 0. In order to avoid -inf values with the logarithmic transformation the zeros are replaced with a negligible value of 0.001 mg\/dm^3","b3e22363":"# Regression or Classification?\n\nAs noted by the authors of the paper, the wines were rated on a continuous scale from 0 to 10. The model is intended to provide decision support to the oenologist. Hence, a regression model is more useful than a classification model. For instance, if an oenologist rates a wine as a 4.0, the model prediction is more helpful as a floating point number like 4.7 than a rounded class number like 5.0. Hence, this may very well be treated as a regression problem. For the sake of comparison I shall also carry out a multi-class classification."}}