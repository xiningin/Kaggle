{"cell_type":{"778d24a9":"code","051150d3":"code","00a6a275":"code","1bce5588":"code","c1bbdc00":"code","81aad333":"code","c03a5059":"code","0a1426aa":"code","876f592e":"code","a15999ef":"code","c507245f":"markdown","10f73c06":"markdown","73b75f47":"markdown","36aae439":"markdown","17a525ff":"markdown","f9c8bd97":"markdown","024f8eb2":"markdown","9f779fbb":"markdown","dd552e3c":"markdown"},"source":{"778d24a9":"from IPython.display import clear_output\nimport warnings\nwarnings.filterwarnings('ignore')","051150d3":"import pandas as pd\nimport numpy as np\n\nfile = \"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\"\ndf = pd.read_csv(file, encoding='ISO-8859-1', usecols=[0,5], header=None)\\\n        .sample(frac=0.01, random_state=42)\n\ndf.columns = ['label','sentence']\ndf.label = df.label.apply(lambda x: np.float64(1) if x==4 else np.float64(x))\n\nprint(\"df.shape =\",df.shape)\nprint(f\"label distribution :\\n{df.label.value_counts()}\")\nprint(df.head())","00a6a275":"from transformers import AutoTokenizer, TFAutoModel\n\ncheckpoint = \"bert-base-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModel.from_pretrained(checkpoint, output_hidden_states=True)\nclear_output()","1bce5588":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nsequences, test_val_sequences = train_test_split(df, test_size=0.3,\n                                             stratify=df.label, random_state=44)\nval_sequences, test_sequences = train_test_split(test_val_sequences, test_size=0.7,\n                                             stratify=test_val_sequences.label, random_state=44)\ndataset = {\n    \"TRAIN\": sequences['sentence'].values.tolist(),\n    \"TEST\": test_sequences['sentence'].values.tolist(),\n    \"VAL\": val_sequences['sentence'].values.tolist()\n}\ntargets = {\n    \"TRAIN\": sequences['label'].values.tolist(),\n    \"TEST\": test_sequences['label'].values.tolist(),\n    \"VAL\": val_sequences['label'].values.tolist()\n}","c1bbdc00":"def tokenization(data, **kwargs):\n    return tokenizer(data, \n                   padding=kwargs.get('padding','longest'), \n                   max_length=kwargs.get('max_length',55),\n                   truncation=True, \n                   return_tensors=\"tf\")","81aad333":"def get_model(**kwargs):\n    max_seq_length = kwargs.get('max_seq_length',55)\n\n    input_ids = tf.keras.Input(shape=(max_seq_length,),dtype='int32')\n    attention_mask = tf.keras.Input(shape=(max_seq_length,),dtype='int32')\n\n    transformer = model({'input_ids':input_ids, 'attention_mask':attention_mask}, \n                        training=False)    \n    pooler_output  = transformer[\"pooler_output\"] \n\n    # Model Head\n    h1 = tf.keras.layers.Dense(128, activation='relu')(pooler_output)\n    dropout = tf.keras.layers.Dropout(0.2)(h1)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n\n    new_model = tf.keras.models.Model(inputs = [input_ids, attention_mask], \n                                      outputs = output)\n    new_model.compile(tf.keras.optimizers.Adam(lr=1e-4), \n                      loss='binary_crossentropy', \n                      metrics=['accuracy'])\n\n    return new_model","c03a5059":"from sklearn.metrics import classification_report\n\ndef test_result(model):    \n    test_inputs = tokenization(dataset[\"TEST\"])\n    result_proba = model.predict([test_inputs.input_ids, test_inputs.attention_mask])\n    result = [1 if x>0.5 else 0 for x in result_proba.ravel()]\n    print(classification_report(targets['TEST'],result))\n    return result_proba, result","0a1426aa":"new_model = get_model()\nresult_proba_before, result_before = test_result(new_model)","876f592e":"inputs = tokenization(dataset['TRAIN'])\ntrain_targets = tf.convert_to_tensor(targets['TRAIN'])\n\nval_inputs = tokenization(dataset['VAL'])\nval_targets = tf.convert_to_tensor(targets['VAL'])\n\nnew_model.fit([inputs.input_ids, inputs.attention_mask], train_targets, \n              validation_data = ([val_inputs.input_ids, val_inputs.attention_mask], val_targets),\n              epochs=10, batch_size=64)","a15999ef":"result_proba_after, result_after = test_result(new_model)","c507245f":"First, let's see the performance before training on TEST data.","10f73c06":"It's obviously overfit since the validation loss is not decreased, the result is acceptable though.","73b75f47":"## 5. Train model","36aae439":"## 3. Train-test split","17a525ff":"## 2. Load the pre-trained model","f9c8bd97":"## 4. Creating a model\nWe'll use the `bert-base-uncased` model and implement a simple *model head* (just a Dense layer). In fact, we don't have to implement the *model head* ourselves, we can simply use both *base* and *head* model available in Model Hub and fine-tune them with new dataset.","024f8eb2":"Let's train, then see the test result.","9f779fbb":"## 1. Prepare the data","dd552e3c":"# Fine-tuning the pretrained model\n---\nWe will gonna use `bert-base-uncased`  as a base Transformer. Then we fine-tune it to perform sentimental analysis with [Sentiment140 dataset](https:\/\/www.kaggle.com\/kazanova\/sentiment140)"}}