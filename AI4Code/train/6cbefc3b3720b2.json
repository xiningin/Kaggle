{"cell_type":{"4cd217dd":"code","e72d32ea":"code","e7b67619":"code","1e4b221c":"code","b263645b":"code","c8b79875":"code","1bb55310":"code","67eb2b7c":"code","1be17c28":"code","1ecc7ec7":"code","543be553":"code","ed25facd":"code","865deda7":"code","9624d85c":"code","aaba5f29":"code","c74b8f40":"code","ce02302a":"code","58404ce1":"code","0bf80d55":"code","43b28115":"code","9771a025":"code","474d4a02":"code","4fe343e0":"code","dfe083f9":"code","c8f82668":"code","e06a5ca7":"code","0bfd0c48":"code","ae431490":"markdown","8a66c987":"markdown","085b75c6":"markdown","d1f80c88":"markdown","f4259f93":"markdown","84008479":"markdown","20dee7b8":"markdown","81e3e98f":"markdown","3679f310":"markdown","b05cd64f":"markdown","68e155cc":"markdown","ea571f88":"markdown","d57c060e":"markdown","acb2b8f1":"markdown","10ae7af1":"markdown","35d12b2e":"markdown"},"source":{"4cd217dd":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e72d32ea":"# import & display data\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\n#test_df    = pd.read_csv(\"..\/input\/test.csv\")\n\n# preview the data\ntrain_df.head()","e7b67619":"# check missing data\ntrain_df.info()","1e4b221c":"# statistical information (numerical columns)\ntrain_df.describe()","b263645b":"# statistical information (categorical columns)\ntrain_df.describe(include = ['O'])","c8b79875":"# missing values in data-set\ntrain_df.isnull().sum()","1bb55310":"# response variable analysis\nf,ax=plt.subplots(1,2,figsize=(18,8))\nsns.countplot('Survived',data=train_df, ax=ax[0])\nax[0].set_title('Survived count')\nsns.barplot(y=\"Survived\", data=train_df, ax=ax[1])\nax[1].set_title('Survived ratio')\nplt.show()","67eb2b7c":"# Pclass variable analysis\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntrain_df[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Pclass')\nsns.countplot('Pclass',hue='Survived',data=train_df,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","1be17c28":"# Pclass variable analysis\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntrain_df[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=train_df,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","1ecc7ec7":"# SibSp variable analysis\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntrain_df[['SibSp','Survived']].groupby(['SibSp']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs SibSp')\nsns.countplot('SibSp',hue='Survived',data=train_df,ax=ax[1])\nax[1].set_title('SibSp:Survived vs Dead')\nplt.show()","543be553":"# Parch variable analysis\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntrain_df[['Parch','Survived']].groupby(['Parch']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Parch')\nsns.countplot('Parch',hue='Survived',data=train_df,ax=ax[1])\nax[1].set_title('Parch:Survived vs Dead')\nplt.show()","ed25facd":"# Embarked variable analysis\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntrain_df[['Embarked','Survived']].groupby(['Embarked']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Embarked')\nsns.countplot('Embarked',hue='Survived',data=train_df,ax=ax[1])\nax[1].set_title('Embarked:Survived vs Dead')\nplt.show()","865deda7":"# Age variable analysis\nf,ax=plt.subplots(1,2,figsize=(20,10))\ntrain_df.Age.plot.hist(ax=ax[0],bins=20,edgecolor='black')\nax[0].set_title('Age')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\nsns.violinplot(\"Survived\",\"Age\",  data=train_df,split=True,ax=ax[1])\nax[1].set_title('Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","9624d85c":"# Fare variable analysis\nf,ax=plt.subplots(1,2,figsize=(20,10))\ntrain_df.Fare.plot.hist(ax=ax[0],bins=20,edgecolor='black')\nax[0].set_title('Fare')\nx1=list(range(0,600,50))\nax[0].set_xticks(x1)\nsns.violinplot(\"Survived\",\"Fare\",  data=train_df,split=True,ax=ax[1])\nax[1].set_title('Fare vs Survived')\nax[1].set_yticks(range(0,600,50))\nplt.show()","aaba5f29":"# Fare Vs Embarked analysis\nf,ax=plt.subplots(1,2,figsize=(20,10))\ntrain_df.Fare.plot.hist(ax=ax[0],bins=20,edgecolor='black')\nax[0].set_title('Fare')\nx1=list(range(0,600,50))\nax[0].set_xticks(x1)\nsns.violinplot(\"Embarked\",\"Fare\",  data=train_df,split=True,ax=ax[1])\nax[1].set_title('Fare vs Embarked')\nax[1].set_yticks(range(0,600,50))\nplt.show()","c74b8f40":"# Fare Vs Age analysis\ng = sns.FacetGrid(train_df, hue=\"Survived\", margin_titles=True,\n                  palette={1:\"seagreen\", 0:\"gray\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();\n\n# Fare Vs Age on Pclass analysis\ng = sns.FacetGrid(train_df, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n                  palette={1:\"seagreen\", 0:\"gray\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();","ce02302a":"data = train_df.drop(['Name', 'Ticket', 'Cabin','PassengerId', 'Age', 'Pclass'],axis=1)\ndata['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata[\"Embarked\"] = data[\"Embarked\"].fillna('C') # impute missing data\ndata['Embarked'].replace(['C','S', 'Q'],[1,2,3],inplace=True)\n\ndata.head()","58404ce1":"from sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn import svm #support vector Machine\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']\n\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\npredictionRF=model.predict(test_X)\nprint('Accuracy for Random Forests is',metrics.accuracy_score(predictionRF,test_Y))\n\nmodel=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\npredictionSVC=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(predictionSVC,test_Y))\n\nmodel = LogisticRegression()\nmodel.fit(train_X,train_Y)\npredictionLR=model.predict(test_X)\nprint('Accuracy for Logistic Regression is',metrics.accuracy_score(predictionLR,test_Y))","0bf80d55":"# cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Logistic Regression','Random Forest']\nmodels=[svm.SVC(kernel='linear'),LogisticRegression(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe","43b28115":"# drop unnecessary features\ndata = train_df.drop([ 'Ticket', 'PassengerId', 'Cabin'],axis=1)","9771a025":"# Split train & valid data\ntrain,valid=train_test_split(data, test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\nvalid_X=valid[valid.columns[1:]]\nvalid_Y=valid[valid.columns[:1]]","474d4a02":"# handle missing values\n#complete missing age with median\ntrain_X['Age'].fillna(train_X['Age'].median(), inplace = True)\nvalid_X['Age'].fillna(valid_X['Age'].median(), inplace = True)\n\n#complete embarked with mode\ntrain_X['Embarked'].fillna(train_X['Embarked'].mode()[0], inplace = True)\nvalid_X['Embarked'].fillna(valid_X['Embarked'].mode()[0], inplace = True)","4fe343e0":"# feature engineering\n\n# FamilySize feature\ntrain_X['FamilySize'] = train_X ['SibSp'] + train_X['Parch'] + 1\nvalid_X['FamilySize'] = valid_X ['SibSp'] + valid_X['Parch'] + 1\n\n# IsAlone feature\ntrain_X['IsAlone'] = 1 #initialize to yes\/1 is alone\ntrain_X['IsAlone'].loc[train_X['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\nvalid_X['IsAlone'] = 1 #initialize to yes\/1 is alone\nvalid_X['IsAlone'].loc[valid_X['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n# split title from name\ntrain_X['Title'] = train_X['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nvalid_X['Title'] = valid_X['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n# fare bins\ntrain_X['FareBin'] = pd.qcut(train_X['Fare'], 4)\nvalid_X['FareBin'] = pd.qcut(valid_X['Fare'], 4)\n\n# age bins\ntrain_X['AgeBin'] = pd.cut(train_X['Age'].astype(int), 5)\nvalid_X['AgeBin'] = pd.cut(valid_X['Age'].astype(int), 5)\n\n# clean rare titles\nstat_min = 10\ntitle_names = (train_X['Title'].value_counts() < stat_min)\ntrain_X['Title'] = train_X['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\ntitle_names = (valid_X['Title'].value_counts() < stat_min)\nvalid_X['Title'] = valid_X['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)","dfe083f9":"# convert formats\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nlabel = LabelEncoder()\n   \ntrain_X['Sex_Code'] = label.fit_transform(train_X['Sex'])\ntrain_X['Embarked_Code'] = label.fit_transform(train_X['Embarked'])\ntrain_X['Title_Code'] = label.fit_transform(train_X['Title'])\ntrain_X['AgeBin_Code'] = label.fit_transform(train_X['AgeBin'])\ntrain_X['FareBin_Code'] = label.fit_transform(train_X['FareBin'])\n\nvalid_X['Sex_Code'] = label.fit_transform(valid_X['Sex'])\nvalid_X['Embarked_Code'] = label.fit_transform(valid_X['Embarked'])\nvalid_X['Title_Code'] = label.fit_transform(valid_X['Title'])\nvalid_X['AgeBin_Code'] = label.fit_transform(valid_X['AgeBin'])\nvalid_X['FareBin_Code'] = label.fit_transform(valid_X['FareBin'])","c8f82668":"# drop redundant columns\ntrain_X = train_X.drop([ 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'FareBin', 'AgeBin'],axis=1)\nvalid_X = valid_X.drop([ 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'FareBin', 'AgeBin'],axis=1)","e06a5ca7":"# modeling\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\npredictionRF=model.predict(valid_X)\nprint('Accuracy for Random Forests is',metrics.accuracy_score(predictionRF,valid_Y))\n\nmodel=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\npredictionSVC=model.predict(valid_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(predictionSVC,valid_Y))\n\nmodel = LogisticRegression()\nmodel.fit(train_X,train_Y)\npredictionLR=model.predict(valid_X)\nprint('Accuracy for Logistic Regression is',metrics.accuracy_score(predictionLR,valid_Y))","0bfd0c48":"# feature importance\nmodel=RandomForestClassifier(n_estimators=100,random_state=0)\nmodel.fit(train_X,train_Y)\npd.Series(model.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\nplt.title('Feature Importance in Random Forests')","ae431490":"## Drop unnecessary features","8a66c987":"## Split train & valid data","085b75c6":"## Handle missing values","d1f80c88":"# Table of Content\n\n## 1. Problem Understanding\n* [Problem statement](#Problem-statement)\n* [Import libraries & data](#Import-libraries-&-data)\n\n## 2. Data Understanding\n* [Check statistics, outliers & missing data](#Check-statistics,-outliers-&-missing-data)\n* [Exploratory data analysis](#Exploratory-data-analysis)\n* [Quick and dirty model with cross-validation](#Quick-and-dirty-model-with-cross-validation)\n\n## 3. Data Preparation\n* [Drop unnecessary features](#Drop-unnecessary-features)\n* [Split train & valid data](#Split-train-&-valid-data)\n* [Handle missing values](#Handle-missing-values)\n* [Feature engineering](#Feature-engineering)\n* [Convert formats](#Convert-formats)\n* [Drop redundant columns](#Drop-redundant-columns)\n* Treat outliers\n* Clean data\n\n\n## 4. Model Building & Evaluation\n* [Build models](#Build-models)\n* Cross-validate\n* [Feature importance](#Feature-importance)\n\n## 5. Model Tuning & Ensembling\n* Hyper-parameter tuning\n* Ensembling\/stacking","f4259f93":"## Check statistics, outliers & missing data","84008479":"## 1. Problem Understanding","20dee7b8":"## Convert formats","81e3e98f":"## Quick and dirty model with cross-validation","3679f310":"### Problem statement\n\nTo know the problem statement, [click here...](https:\/\/www.kaggle.com\/c\/titanic)\n\n","b05cd64f":"## Build models","68e155cc":"## Feature engineering","ea571f88":"## Drop redundant columns","d57c060e":"## Exploratory data analysis","acb2b8f1":"### Import libraries & data","10ae7af1":"## Feature importance","35d12b2e":"## To be continued...\n* Hyper-parameter tuning\n* Ensembling\/stacking"}}