{"cell_type":{"c7cca98c":"code","53cf81d5":"code","6413a150":"code","80b05e53":"code","74e5dc17":"code","ed97e8b0":"code","3ebd11d4":"code","d2a05f63":"code","e8882088":"code","c4b1bc93":"code","22e2b970":"code","059fefe5":"code","4de7d090":"code","42106716":"code","361f6d18":"code","45532473":"code","170a37b7":"code","9b6bb61e":"code","f1e879d2":"code","3c1ca135":"code","638b5723":"code","c420d9cf":"code","c6622386":"code","90cdee17":"code","219d5712":"code","b2723696":"markdown","502bc0ae":"markdown","731cc676":"markdown","5e36949e":"markdown","30e89575":"markdown","b409443b":"markdown","139d9a80":"markdown","e9c4eb3e":"markdown","2784ff17":"markdown","9d62aab7":"markdown","9ebb82c0":"markdown","de4d1491":"markdown","2fde39fa":"markdown","e93ef295":"markdown","4c4395ec":"markdown","21bf3209":"markdown","4f459852":"markdown","c6157136":"markdown","23cd1e0d":"markdown","663ebd47":"markdown","94a3526a":"markdown","c7e8be08":"markdown","4a505639":"markdown","3c55488e":"markdown","54d2cfdc":"markdown","470c0708":"markdown","91544676":"markdown","e8f108de":"markdown","25ea8579":"markdown","54cb9f8f":"markdown","9b9e711b":"markdown","b4ae760e":"markdown","a6438f2b":"markdown","36b7c1d5":"markdown","600829f3":"markdown","ab342a44":"markdown","0c71e1ba":"markdown","d6636930":"markdown","e109569f":"markdown"},"source":{"c7cca98c":"import tensorflow as tf                       # deep learning library\nimport numpy as np                            # for matrix operations\nimport matplotlib.pyplot as plt               # for visualization\n%matplotlib inline","53cf81d5":"from tensorflow.keras.datasets.mnist import load_data    # To load the MNIST digit dataset\n\n(X_train, y_train) , (X_test, y_test) = load_data()      # Loading data","6413a150":"print(\"There are \", len(X_train), \"images in the training dataset\")     # checking total number of records \/ data points available in the X_train dataset\nprint(\"There are \", len(X_test), \"images in the test dataset\")     # checking total number of records \/ data points available in the X_test dataset","80b05e53":"# Checking the shape of one image\nX_train[0].shape","74e5dc17":"# Take a look how one image looks like\nX_train[0]","ed97e8b0":"plt.matshow(X_train[0])","3ebd11d4":"# we can use y_train to cross check\ny_train[0]","d2a05f63":"# code to view the images\nnum_rows, num_cols = 2, 5\nf, ax = plt.subplots(num_rows, num_cols, figsize=(12,5),\n                     gridspec_kw={'wspace':0.03, 'hspace':0.01}, \n                     squeeze=True)\n\nfor r in range(num_rows):\n    for c in range(num_cols):\n      \n        image_index = r * 5 + c\n        ax[r,c].axis(\"off\")\n        ax[r,c].imshow( X_train[image_index], cmap='gray')\n        ax[r,c].set_title('No. %d' % y_train[image_index])\nplt.show()\nplt.close()","e8882088":"\nX_train = X_train \/ 255\nX_test = X_test \/ 255\n\n\"\"\"\nWhy divided by 255?\nThe pixel value lie in the range 0 - 255 representing the RGB (Red Green Blue) value. \"\"\"","c4b1bc93":"X_train[0]","22e2b970":"X_train.shape","059fefe5":"X_train_flattened = X_train.reshape(len(X_train), 28*28)    # converting our 2D array representin an image to one dimensional\nX_test_flattened = X_test.reshape(len(X_test), 28*28)","4de7d090":"X_train_flattened.shape","42106716":"# Defining the Model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')     # The input shape is 784. \n])","361f6d18":"model.summary()","45532473":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","170a37b7":"model.fit(X_train_flattened, y_train, epochs=5)","9b6bb61e":"model.evaluate(X_test_flattened, y_test)","f1e879d2":"y_predicted = model.predict(X_test_flattened)\ny_predicted[0]","3c1ca135":"np.argmax(y_predicted[0])","638b5723":"plt.matshow(X_test[0])","c420d9cf":"# Defining the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, input_shape=(784,), activation='relu'),\n    tf.keras.layers.Dense(10, activation='sigmoid')\n])\nmodel.summary()\n","c6622386":"# Compiling the model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train_flattened, y_train, batch_size= 128,epochs=20)","90cdee17":"# Evaluate the model\nmodel.evaluate(X_test_flattened,y_test)","219d5712":"model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(100, activation='relu'),\n    tf.keras.layers.Dense(10, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=10)","b2723696":"# Introduction\nIn this notebook we will build a Neural Network multi-class classification model using a dataset popularly known as **'MNIST'**\n\n","502bc0ae":"Let's see the original digit at first index in X_test. Can see this using matshow() function.","731cc676":"**Define the model**","5e36949e":"### Building Neural Network Model Using hidden layer","30e89575":"## Objective\nIn this notebook we will classify handwritten digits using a simple neural network which has only input and output layers. We will then add a hidden layer and see how the performance of the model improves","b409443b":"The activation function used here is 'sigmoid'. Do you recall why was it so from the [Binary Classification Notebook](https:\/\/github.com\/dphi-official\/Deep_Learning_Bootcamp\/blob\/master\/DL%20For%20Classification\/DL_Day6_Binary_Classification.ipynb)?","139d9a80":"## Loading Data\nThe MNIST dataset is available in the TensorFlow only. Let's load the data:","e9c4eb3e":"**np.argmax finds a maximum element from an array and returns the index of it**","2784ff17":"## Building Models\n### Very simple neural network with no hidden layers","9d62aab7":"# Agenda\n*  About the Data\n*  Loading Libraries\n*  Loading Data\n*  Basic EDA\n*  Data Preprocessing\n*  Model Building\n  *  Simple Neural Network With No Hidden Layer\n  *  Building Model Using Hidden Layer\n*  Summary","9ebb82c0":"Now if you look at the data, each pixel value should be in range 0 to 1.","de4d1491":"**Flatten the Data**\n\nWe simply convert a 2 dimensional data (i.e. one image data) to 1 dimensional.\n\nWhy to flatten data?\n\nBefore understanding why let's check the shape of the data","2fde39fa":"## Data Preprocessing","e93ef295":"**predict for the X_test**","4c4395ec":"The above numbers are the probabilities values for different digits. The maximum probability will confirm what is the predicted digit for first image in X_test.\n\nThe value at the 0th index in above array of numbers is saying the probability of the digit being 0. \n\n**Generalize:** The value at the nth index in above array of numbers is saying the probability of the digit being n","21bf3209":"## Basic EDA","4f459852":"## About the Data\n**MNIST (Modified National Institute of Standards and Technology database)** is a large database of 70,000 handwritten digits. \n\nIt has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST (National Institute of Standards and Technology).\n\nThe objective here is to build a model that would recognize the correct digit that the given image is representing.\n\n","c6157136":"Generally for multi-class classification problem, it is suggested to use softmax. We tried both softmax activation and sigmoid activation, but sigmoid found to give better performance. You can also try using both and keep the one which gives better performance.\n\nAlso, if the classes of your data are independent of each other, use sigmoid activation function. A detailed explanation can be found [here](https:\/\/datascience.stackexchange.com\/questions\/39264\/how-does-sigmoid-activation-work-in-multi-class-classification-problems).","23cd1e0d":"You can play with different number of epochs.","663ebd47":"Let's normalize our data (i.e. both X_train and X_test). Normalization is a process that changes the range of pixel intensity values to the range 0 to 1.\n\nBut why to normalize?\n\nThe motivation to normalize is to achieve consistency in dynamic range for a set of data, signals, or images to avoid mental distraction and reduce the data redundancy. Also, normalizing the data can help you improve the model performance.","94a3526a":"### **Reference**\n[Neural Network For Handwritten Digits Classification](https:\/\/www.youtube.com\/watch?v=iqQgED9vV7k&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=7)","c7e8be08":"**A sample example showing the conversion of 3D data to 2D**\n![3Dto2D](https:\/\/dphi-courses.s3.ap-south-1.amazonaws.com\/Deep+Learning+Bootcamp\/3D+to++2D.png)","4a505639":"The performance of the model on very simple model with no hidden layer is 92.6 %. Not Bad!","3c55488e":"**Fit the model**","54d2cfdc":"Now if you check the shape of our data, it should be 2 dimensional","470c0708":"**Try yourself**: \nChange the values of epochs and try adding more hidden layers. Are you able to increase the accuracy above 97.5%?","91544676":"The data is 3 dimensional. The first value i.e. 60000 is nothing but the number of records or images in this case. The second and third dimension represent each individual image i.e. each image is of shape 28X28. \n\nMost of the the supervised learning algorithms that execute classification and regression tasks, as well as some deep learning models built for this purposes, are fed with two-dimensional data. Since we have our data as three-dimensional, we will need to flatten our data to make it two-dimensional.","e8f108de":"**Compile the model**","25ea8579":"# Summary\n*  We learned why we need to normalize and flatten the data.\n*  We observed the performance of very simple neural network with no hidden layer and that of with one hidden layer with 100 hidden neurons. The performance of later model was better than earlier\n*  Then came to know that we can flatten the data in the model building only.","54cb9f8f":"Only numbers! Can't understand what digit does it represent. \n\nThere is a function in matplotlib called as 'matshow()', it helps you to display the image of the array of numbers","9b9e711b":"**Evaluate the model on unseen data (i.e. X_test_flattened)**","b4ae760e":"## Loading Libraries\nAll Python capabilities are not loaded to our working environment by default (even if they are already installed in your system). So, we import each and every library that we want to use. Sometimes we chose alias names for our libraries for the sake of our convenience for example we **import tensorflow as tf** and similarly the other libraries\n","a6438f2b":"*  **adam** is an optimization algorithm which is faster than Stochastic Gradient Descent. If you remember from the learning material of Day 4 (i.e. working of neural networks), we know that Stochastic Gradient Descent (SGD in short) is just a type of Gradient Descent algorithm.\n\n*  **sparse_categorical_crossentropy** is a loss function similar to **binary_crossentropy** (discussed in Binary Classification Notebook), the only difference is that if the target variable is binary we use binary_crossentropy but if your target values are normal integers more then two, use sparse categorical crossentropy. Why not use **categorical_crossentropy**? You may ask. Well, [this article](https:\/\/jovianlin.io\/cat-crossentropy-vs-sparse-cat-crossentropy\/) will help you understand it.\n\n*  The metrics used to evaluate the model is **accuracy**. Accuracy calculates how often the predictions calculated by the model are correct.","36b7c1d5":"Now one can easily say the above number is 5. Well we want to build a model that will tell you what digit does that 28X28 array represent.","600829f3":"**Using Flatten layer so that we don't have to call .reshape on input dataset**","ab342a44":"The predicted digit is 7.","0c71e1ba":"Each image in the dataset is of shape 28X28 numbers (i.e. pixels)","d6636930":"Hence the prediction is correct","e109569f":"![simple neural network](https:\/\/dphi-courses.s3.ap-south-1.amazonaws.com\/Deep+Learning+Bootcamp\/mnist1.png)"}}