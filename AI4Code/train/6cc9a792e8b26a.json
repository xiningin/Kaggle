{"cell_type":{"fca88ebb":"code","8759ff1e":"code","773dd2cf":"code","dc53b729":"code","afbaa78c":"code","e21a3471":"code","a9cb2d2f":"code","077e8149":"code","75f13f8d":"code","24ff3a7c":"code","258a7d3a":"code","4742fdc7":"code","4e12cfaf":"code","94f807de":"code","7b88e508":"code","a8f67753":"code","50cae23c":"code","d08c03d8":"code","68eec720":"code","908df5b0":"code","3d6189b5":"code","6943257c":"code","5d4a4140":"code","824d34be":"code","81310132":"code","cffcd41a":"code","2e1e718f":"code","921be93c":"code","6cc75615":"code","dbe9e606":"code","e7f47bdd":"code","6717eb89":"code","5add17ed":"code","33ed54bf":"code","60cbf075":"code","331e8315":"code","b888a7d3":"markdown","07a1a1c6":"markdown","370202aa":"markdown","532adf53":"markdown","7593530c":"markdown","cbd26799":"markdown","b5bf2616":"markdown","4e694f29":"markdown","3f2acd35":"markdown","bed1097b":"markdown","aa2e5f3f":"markdown","d5c0bb1b":"markdown","62d8d919":"markdown","ff499022":"markdown","88d2e097":"markdown","a4b5c09c":"markdown","ec59d5da":"markdown","97f40a52":"markdown","149da071":"markdown","ef479dd9":"markdown"},"source":{"fca88ebb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8759ff1e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","773dd2cf":"df=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head(10)","dc53b729":"df.isnull().sum(0)","afbaa78c":"df.duplicated().sum(0)","e21a3471":"df.drop_duplicates(keep=False, inplace=True)","a9cb2d2f":"df.duplicated().sum(0)","077e8149":"df.info()","75f13f8d":"df.describe()","24ff3a7c":"from sklearn.preprocessing import StandardScaler\nstdsc=StandardScaler()\ndf['Time']=stdsc.fit_transform(df['Time'].values.reshape(-1,1))\ndf.head()","258a7d3a":"#plt.figure(figsize=(10,6))\n#sns.heatmap(data=df, annot=True,cbar=True)\n#plt.xlabel('various contributing factors')\n","4742fdc7":"df['Class'].value_counts().plot(kind='bar')","4e12cfaf":"X=df.drop([\"Class\"], axis=1)\nY=df[\"Class\"]\nprint(X.shape)\nprint(Y.shape)","94f807de":"# sklearn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12)","7b88e508":"X_train.shape","a8f67753":"X_test.shape","50cae23c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nsel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\nsel.fit(X_train, Y_train)\n","d08c03d8":"selected_feat= X_train.columns[(sel.get_support())]\nlen(selected_feat)","68eec720":"print(selected_feat)","908df5b0":"df=df[['V4', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17','Class']]\ndf.head()","3d6189b5":"from imblearn.combine import SMOTETomek","6943257c":"X=df.drop(\"Class\",axis=1)\ny=df['Class']","5d4a4140":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","824d34be":"smk = SMOTETomek(random_state=42)\nX_train,y_train=smk.fit_resample(X_train,y_train)","81310132":"X_train.shape","cffcd41a":"plt.figure(figsize=(10,6))\nax=y_train.value_counts().plot(kind=\"bar\")\nplt.show()","2e1e718f":"import keras\n","921be93c":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","6cc75615":"model=Sequential()\n# first layer \n# for initializing the weight we use he_normal\nmodel.add(Dense(units=20,kernel_initializer='he_normal',activation='relu',input_dim=7))\n#2nd layer\nmodel.add(Dense(units=15,kernel_initializer='he_normal',activation='relu'))\n#3rd layer\nmodel.add(Dense(units=1,kernel_initializer='he_normal',activation='sigmoid'))\n# compiling!\nmodel.compile(optimizer='Adamax',loss='binary_crossentropy',metrics=['accuracy'])","dbe9e606":"modelhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 20,validation_split=0.2)","e7f47bdd":"output=pd.DataFrame(modelhistory.history)\noutput","6717eb89":"# Predicting the test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)","5add17ed":"from sklearn.metrics import classification_report\nfrom sklearn import metrics\nimport seaborn as sns","33ed54bf":"X_test.shape","60cbf075":"sns.heatmap(metrics.confusion_matrix(y_test,y_pred),annot=True)","331e8315":"print(classification_report(y_test, y_pred))\n","b888a7d3":"the reason to do train test split before SMOTE is because the accuracy will be increased as the rows are duplicated and so test data and train data wont't have much difference.\n\nthat is why in an imabalanced data set to get the \"CORRECT UNDERSTANDING OF ACCURACY\" we need to split first and then SMOTE only the train data set!","07a1a1c6":"the data is HIGHLY IMBALANCED!\n- to make the data set balanced we will use SMOTETomek techinque which is an example of **OverSampling**.\n\n- but before that we wish to find the list of important features and then use ANN to classify","370202aa":"so now we, try to see are there any duplicate values present in this data set?\n","532adf53":"#### We can of course tune the parameters of the Decision Tree.Where we put the cut-off to select features is a bit arbitrary. One way is to select the top 10, 20 features. Alternatively, the top 10th percentile. For this, we can use mutual info in combination with SelectKBest or SelectPercentile from sklearn.","7593530c":"- here we have a normalized time scale having mean zero and std deviation of 1\n- our aim is to get first the MOST IMPORTANCE FEATURES using RANDOM FOREST CLASSIFIER and then work with ANN by using that as our new data set!\n- we may not use some of the features by seeing the heat map and determining which features have higher positive correlation","cbd26799":"here there are 1081 duplicated values, we now drop these values and treat the new \"improved\"data set as the starting step","b5bf2616":"Now we create a new data set with the following and number of features have been significantly reduced","4e694f29":"- you should not use the SMOTE oversampling on the test set \n-  otherwise the model performs very  well since actually lots of your rows in the training are duplicated and present in the testing set \n-  One should first train-test split, then oversample only the training set","3f2acd35":"here all the duplicated values have been deleted and we are good to go. Note that the time varibale has very large values and we will scale it in order","bed1097b":"![Screenshot 2021-12-27 at 18.47.31.png](attachment:9c94503b-0324-4c38-a35d-12f14ba6f131.png)","aa2e5f3f":"#### The model gives a very good accuracy, but we can still try to increase the accuracy (simultaneously avoiding overfitting) by using HYPERPARAMETER TUNING TECHNIQUE","d5c0bb1b":"Selecting features by using tree derived feature importance is a very straightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if we are going to build tree methods.","62d8d919":"### As we can see that the train data set is now Balanced wrt the feature 'Class'\n- now performing the ANN part!\n","ff499022":"we now wish to check now on how the target CLASS is distributed, to check that we use bar plot\n- 0 denotes that there has been no fraudlent activity\n- 1 denotes that there is a fraudlent activity","88d2e097":"he_normal : It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 \/ fan_in) where fan_in is the number of input units in the weight tensor.","a4b5c09c":"# Making use of RandomForest Classifier to find the important features using SelectfromModel","ec59d5da":"cleary there are no null values present in this data set!\n","97f40a52":"### TO BE CONTINUED....","149da071":"## Inference:\nSince the accuracy is good (~ 98.8) and also the validation loss also kept on decreasing we can safely assume that overfitting might not have happened.\n1. The time after the first transaction was not an important quantity.\n2. the amount was also not an important quantituy in the prediction whether the activity is FRAUDLENT or NOT","ef479dd9":"now using SMOTETomek to get balance the  remaining data set having Important features only!"}}