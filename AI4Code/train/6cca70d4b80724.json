{"cell_type":{"1210e63c":"code","82df9841":"code","0e1c0051":"code","97e4d8be":"code","a138c403":"code","8d439c3c":"code","0b72555f":"code","5408d2b0":"code","c929ae1d":"code","2bf7cc2c":"code","11209366":"code","898db7e4":"code","3a9bb705":"code","47729bef":"code","27392945":"code","1ef62da5":"code","ff9fa560":"code","73d5cb24":"code","9f87810e":"code","33717509":"code","478628df":"code","c6161c31":"code","f91f07a0":"code","739ea4e3":"code","7e4fc7a7":"code","32248afd":"code","ab280c4c":"code","7397a545":"code","60e14257":"code","d639127c":"markdown","bfc99cf6":"markdown","8985974f":"markdown","9eeeeb61":"markdown","700121fa":"markdown","d2a6f7d5":"markdown","c9422f72":"markdown","2e8447b0":"markdown","dac1d016":"markdown","2543d804":"markdown","f7730d54":"markdown","fa6bfb34":"markdown","0f19590f":"markdown","e26b0bfe":"markdown","07b8407b":"markdown","636b0449":"markdown","8d24b63c":"markdown","bc44d8cc":"markdown","14c02358":"markdown","19796296":"markdown","62d06a32":"markdown","483647ff":"markdown","38f9302a":"markdown","225cfe71":"markdown","beb76579":"markdown","48c8a7e7":"markdown"},"source":{"1210e63c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82df9841":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","0e1c0051":"#reading data from kaggle input\ndf=pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\nprint('Toatal No oF rows',df.shape[0])\nprint('Toatal No oF columns',df.shape[1])\ndf.info()","97e4d8be":"df.drop([df.columns[21],df.columns[22]],axis=1,inplace=True)\ndf.head(2)","a138c403":"# lets describe the data.... the script written below will give info about many thing..lets check\ndf.describe()","8d439c3c":"df.isnull().sum()","0b72555f":"sns.countplot(df['Attrition_Flag'])","5408d2b0":"df_obj=df.select_dtypes(include=[object])\ndf_numr=df.select_dtypes(exclude=[object])","c929ae1d":"df_obj","2bf7cc2c":"for i in df_obj.columns:\n    print(df_obj[i].value_counts())\n    \n\n\n","11209366":"# lets analyse it with the help of graph\nfig, axes = plt.subplots(3, 2, figsize=(18, 10))\n\nfig.suptitle('countplot')\n\nsns.countplot(ax=axes[0, 0],x=df_obj['Gender'],hue=df_obj['Attrition_Flag'])\nsns.countplot(ax=axes[0, 1], x=df_obj['Marital_Status'],hue=df_obj['Attrition_Flag'])\nsns.countplot(ax=axes[1, 0], x=df_obj['Education_Level'],hue=df_obj['Attrition_Flag'])\nsns.countplot(ax=axes[1, 1], x=df_obj['Income_Category'],hue=df_obj['Attrition_Flag'])\nsns.countplot(ax=axes[2, 0], x=df_obj['Card_Category'],hue=df_obj['Attrition_Flag'])\nsns.countplot(ax=axes[2, 1], x=df_obj['Attrition_Flag'],hue=df_obj['Attrition_Flag'])\n\n\n","898db7e4":"\n# dropping CLIENTNUM which is unique id so its not useful at all\ndf_numr.drop('CLIENTNUM',axis=1,inplace=True)\n# now we gonna analyse numerical feature\ndf_numr.head()","3a9bb705":"# now we will chevck if features are correlated\ndf_numr.corr()\n\n# pretty difficult to understand..not a problem we create visualization for it\nfig, ax = plt.subplots()\n# the size of A4 paper\nfig.set_size_inches(18,10)\n\nsns.heatmap(df_numr.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=3, linecolor='black',ax=ax)","47729bef":"#lets analyse some important numerical feature\nfig, axes = plt.subplots(2, 2, figsize=(18, 10))\n\nfig.suptitle('countplot')\n\nsns.boxplot(ax=axes[0][0],y=df_numr['Customer_Age'])\nsns.boxplot(ax=axes[0][1],y=df_numr['Dependent_count'])\nsns.boxplot(ax=axes[1][0],y=df_numr['Credit_Limit'])\nsns.boxplot(ax=axes[1][1],y=df_numr['Avg_Utilization_Ratio'])","27392945":"# as we already know that some of the features in our dataset are highly correleated...so it is better practice\n# for us to drop those feature\ncorrelated_features = set()\ncorrelation_matrix = df_numr.corr()\n# we gonna drop feature which have pearson coffecirnts greater than 0.6\nfor i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.6:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n            \ndf_numr.drop(labels=correlated_features, axis=1, inplace=True)","1ef62da5":"# one hot encoding of object category data\nobj_data=pd.get_dummies(df_obj.drop(columns=['Attrition_Flag']),drop_first=True)\nobj_data","ff9fa560":"# joining our numr dataframe and object dataframe\nmain_df=pd.concat([obj_data,df_numr],axis=1)\nmain_df.head()","73d5cb24":"# splitting dependent and independent feature \nY=pd.get_dummies(df_obj['Attrition_Flag'],drop_first=True)\nX=main_df","9f87810e":"print(X.shape)\nprint(Y.shape)","33717509":"# before moving any further we have to deal with imbalanced dataset\n# there are two tecnique to do handle imbalanced dataset..oversampling and undersampling\n# if the data is too large we can perform down sampling... butr as our data is not so big we perform oversampling\nsmk=SMOTETomek(random_state=42)\nxres,yres=smk.fit_sample(X,Y)","478628df":"print(xres.shape,yres.shape)","c6161c31":"# dividing dataset into training and testing set \nX_train, X_test, y_train, y_test = train_test_split(xres, yres, test_size=0.25, random_state=42)\nprint(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","f91f07a0":"#we have trained our model with some random parameter and lets see the accuracy\nclassifier=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=50)\nclassifier.fit(X_train,y_train)","739ea4e3":"# calculating accuracy score and confusion matrix\nprediction=classifier.predict(X_test)\nprint(accuracy_score(y_test,prediction))\nprint(confusion_matrix(y_test,prediction))","7e4fc7a7":"from sklearn.model_selection import GridSearchCV","32248afd":"#param_grid = { \n#  'n_estimators': [10,50,200, 500],\n#    'max_features': ['auto', 'sqrt', 'log2'],\n#    'max_depth' : [4,5,6,7,8],\n#   'criterion' :['gini', 'entropy']\n\n#}\n#rfc=RandomForestClassifier(random_state=42)\n#CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n#CV_rfc.fit(X_train, y_train)","ab280c4c":"#CV_rfc.best_params_","7397a545":"# these parameters are obtained by hyperparameter tuning\nclassifierfinal=RandomForestClassifier(n_estimators=500,criterion='gini',random_state=50,max_depth=8,max_features='auto')\nclassifierfinal.fit(X_train,y_train)","60e14257":"\nprediction1=classifierfinal.predict(X_test)\nprint(accuracy_score(y_test,prediction1))\nprint(confusion_matrix(y_test,prediction1))","d639127c":"we will solve our problem in following steps(or how a data science project look like)\n\n* collecting data\n* data wrangling\n* a deep dive analysis on data\n* feature engineering part\n* Data preprocessing\n* building models\n* hyperparameter tuning\n* analyis on model\n","bfc99cf6":"from the above graph we can conclude the following points:\n* females own more credit card than male\n* Married people takes the more credit card\n* mostly people are graduate \n* most people have salary less than 40k \n* almost 90% people have Blue Card\n\nAnd last point is what we have to predict who will gonna leave services in Future.","8985974f":"<h2>So, How we gonna help the bank manager?<\/h2>","9eeeeb61":"<h1> so finally we are done with how a datascience project look like in an industry.. if You find this notebook useful..please upvote it and leave a comment.<\/h1>","700121fa":"wait a min**after tuning the model even we get less accuracy...sometimes it happens because initially our model have been overfitted the data...now this model is good to used **\n<h3> also it depands on the domain expertise what kind of metrics they require<\/h3>","d2a6f7d5":"# WHY correlation matrix\nas we can see from above graph that many feature are highly correletaed to each other which means that increasing a also increase b... they have almost several feature. so it is important for us to drop highly correlated feature\nwhich we gonna perform in feature selection process.","c9422f72":"### Importing some useful libraries","2e8447b0":"model building is the easiest process in data science...there are lots of classification algorithm to go with...there is no certainity that this mdel or that model will perform model..\nwe have to try different model and see which perfortms better.\nwer will go with RANDOM FOREST CLASSIFIER","dac1d016":"**Note:** the above code only works on numerical data\n<br>\nAs of now you can see that it gives the idea about mean ,max,count of the object.\nbefore moving further lets check about the missing values in data.","2543d804":"<h1 style=\"text-align:center;\">Deep dive into data<\/h1>","f7730d54":"<h1 style=\"text-align:center;\">Exploratort data analysis<\/h1>","fa6bfb34":"<h3> Before we begin, let me intoduce you to business problem.<\/h3>\nA bank manager was disappointed as time to time many people have shut down their credit card services from that bank. and the bank manager wants to find out who is gonna leave the bank in future so he can give better offers to those customer to stick with them. so the bank manager collected the data for credit card services and wants to find answers.\n\n","0f19590f":"**AS we can see from above that the data contains outliers in some feature... lets see hoe to solve this problem**","e26b0bfe":"here we are splitting categorical data and numerical data..so that it will be easy for us to do anlysis.","07b8407b":"IT is highly imbalanced dataset...as our target column is having 85-15 ration which needs a fine technuique to train our model.","636b0449":"As already mentioned in the problem statement that the last two columns are not useful at all. so we gonna drop it before proceeding to next step","8d24b63c":"### So we are done with DAta Analysis phase.now we are ready to move further.","bc44d8cc":"Allright pretty good we have reduced our feature.... to avoid the complex model...still it contains lots of feature.","14c02358":"AS we can see there are no missing values in data.. which is good for us.\n","19796296":"**as you can see we got a pretty decent score of 96% accuracy**\nLets tune hyperparameter to check if we increase our accuracy","62d06a32":"<h1 style=\"text-align:center;\">MODEL BUILDING<\/h1>","483647ff":"1. lets do UNIVARIATE analysis on categorical variable","38f9302a":"<h1 style=\"text-align:center;\">Feature Engineering And Data Preprocessing<\/h1>","225cfe71":"Allright so we have deleted the last two columns and now we are done with data collecting process. now we will gain insight about data.","beb76579":"Allright, as you can see now that your dataset is balanced..we are ready to move further. **HAPPY!**","48c8a7e7":"<h1 style=\"text-align:center;\">Churn Prediction<\/h1>"}}