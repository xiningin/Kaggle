{"cell_type":{"cc91762c":"code","02d9453a":"code","791286d2":"code","cb0de57d":"code","5d93c93e":"code","780d289e":"code","ebc1fe7e":"code","63a989b5":"code","5a748875":"code","6a3e2252":"code","e26b1c20":"code","0ef1b9de":"code","2f5d0245":"code","b3846e93":"code","27bfa4f8":"code","2de88d07":"code","bd054cbc":"code","0bdba892":"code","e5a27f84":"code","2d4300c5":"code","089fb0d9":"code","ea4faf79":"code","200c1c6b":"code","d5c17f3a":"code","b8962dc7":"code","a948ba8f":"code","89099035":"code","c51182e5":"code","3882ffd2":"code","0164d702":"code","f2b2ec72":"code","addcd237":"code","9cfd4bc3":"code","6057c23f":"code","ae67bdaa":"code","94c7a4a8":"code","3dfee0dc":"code","e18dd6ac":"code","2f383aaa":"code","89827b90":"code","d0d4715d":"code","ded632b8":"code","b0042cf8":"code","3121197e":"code","9d136961":"code","11f2344b":"markdown","9740438c":"markdown","612db989":"markdown","e930f244":"markdown","bed848ae":"markdown","4a8a0412":"markdown","730200c1":"markdown","bb04035b":"markdown","53e89035":"markdown","72d750d1":"markdown","ae5cb1f7":"markdown","a14bd786":"markdown","e0007619":"markdown","49d1680a":"markdown","e095ec32":"markdown","c990b97a":"markdown"},"source":{"cc91762c":"#import the required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d","02d9453a":"#read the csv file into dataframe\ndf=pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","791286d2":"#getting to know about the datatypes\ndf.info()","cb0de57d":"#view dataframe\ndf\n","5d93c93e":"#finding the shape of dataframe\ndf.shape","780d289e":"#find the mean, count, min, max, percentile, standard deviation of given data\ndf.describe()","ebc1fe7e":"#checking for null values\ndf.isnull().sum()","63a989b5":"#delete unnecessary column\ndel df['lat']","5a748875":"#view first 5 records\ndf.head()","6a3e2252":"#delete unnecessary column\ndel df['long']","e26b1c20":"#view first five records\ndf.head()","0ef1b9de":"#delete unnecessary column\ndel df['view']","2f5d0245":"#view first 5 records\ndf.head()","b3846e93":"#finding value counts of no of bedrooms \ndf.bedrooms.value_counts()","27bfa4f8":"#removing outlier with '33' bedrooms\ndf=df[df['bedrooms']!=33]","2de88d07":"#check if the outlier is removed\ndf.bedrooms.value_counts()","bd054cbc":"#conevrting data column into float type\npd.options.display.float_format = '{:,.0f}'.format\ndf","0bdba892":"#formatting the unncessary values in data column\ndf['date']=[x[:8] for x in df['date']]\ndf","e5a27f84":"#changing date column into date type using pandas\ndf['date']=pd.to_datetime(df[\"date\"])\ndf","2d4300c5":"#checking the data types\ndf.info()","089fb0d9":"#replot for bedrooms and price\nsns.set(style='white')\nsns.relplot(x='bedrooms',y='price',hue='bedrooms',alpha=0.7,height=10,data=df)","ea4faf79":"#plot boxplot graph for bedrooms and price\nsns.set(style='ticks',palette='pastel')\nfig, ax = plt.subplots(figsize=(14,10)) \nsns.boxplot(x='bedrooms',y='price',palette=[\"m\", \"g\"], data=df,ax=ax)","200c1c6b":"#boxplot between bathrooms and price\nsns.set(style='ticks',palette='muted')\nfig,ax=plt.subplots(figsize=(14,10))\nsns.boxplot(x='bathrooms',y='price',palette=[\"b\", \"g\"],data=df,ax=ax)","d5c17f3a":"#find correlation\ndf.corr()","b8962dc7":"#heatmap for all the required features for correlation\ndf1=df[['price', 'bedrooms', 'bathrooms', 'sqft_living',\n    'sqft_lot', 'floors', 'waterfront', 'condition', 'grade',\n    'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n     'sqft_living15', 'sqft_lot15']]\nfig, ax = plt.subplots(figsize=(20,20)) \nsns.heatmap(df.corr(),cmap = 'YlGnBu',annot=True,ax=ax)","a948ba8f":"#graph for dates,price and bedrooms\nfig,ax=plt.subplots(figsize=(14,10))\ndf.set_index('date').groupby('bedrooms')['price'].plot(legend=True)","89099035":"#lineplot for sqft living and price\nfig,ax=plt.subplots(figsize=(12,8))\npalette=sns.color_palette('mako_r',6)\nax=sns.lineplot(x='sqft_living',y='price',data=df,hue='bedrooms',palette='mako_r')","c51182e5":"#lineplot for sqft lot and price\nfig,ax=plt.subplots(figsize=(12,8))\npalette=sns.color_palette('mako_r',6)\nax=sns.lineplot(x='sqft_lot',y='price',data=df,hue='bedrooms',palette='mako_r')","3882ffd2":"#value counts of condition of the house\ndf['condition'].value_counts()","0164d702":"#catplot for condition and price\nsns.catplot(x='condition',y='price',data=df)","f2b2ec72":"#3d plot\nfig=plt.figure(figsize=(20,13))\nax=fig.add_subplot(2,2,1,projection='3d')\nax.scatter(df['floors'],df['bedrooms'],df['bathrooms'],c='darkgreen',alpha=0.5)\nax.set(xlabel='\\nfloors',ylabel='\\nbedrooms',zlabel='\\nbathrooms')\nax.set(ylim=[0,12])\n\nax=fig.add_subplot(2,2,2,projection='3d')\nax.scatter(df['floors'],df['bedrooms'],df['sqft_living'],c='darkgreen',alpha=0.5)\nax.set(xlabel='\\nfloors',ylabel='\\nbedrooms',zlabel='\\nsqft living')\nax.set(ylim=[0,12])\n\nax=fig.add_subplot(2,2,3,projection='3d')\nax.scatter(df['sqft_living'],df['sqft_lot'],df['bathrooms'],c='darkgreen',alpha=0.5)\nax.set(xlabel='\\nsqft living',ylabel='\\nsqft lot',zlabel='\\nbathrooms')\nax.set(ylim=[0,250000])\n\nax=fig.add_subplot(2,2,4,projection='3d')\nax.scatter(df['sqft_living'],df['sqft_lot'],df['bedrooms'],c='darkgreen',alpha=0.5)\nax.set(xlabel='\\nsqft living',ylabel='\\nsqft lot',zlabel='\\nbedrooms')\nax.set(ylim=[0,250000])","addcd237":"from abc import ABC, abstractmethod\n\n# Super class for machine learning models \n\nclass BaseModel(ABC):\n    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n    \n    @abstractmethod\n    def train(self, X, T):\n        pass\n\n    @abstractmethod\n    def use(self, X):\n        pass\n\n    \nclass LinearModel(BaseModel):\n    \"\"\"\n        Abstract class for a linear model \n        \n        Attributes\n        ==========\n        w       ndarray\n                weight vector\/matrix\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n            weight vector w is initialized as None\n        \"\"\"\n        self.w = None\n\n    # check if the matrix is 2-dimensional. if not, raise an exception    \n    def _check_matrix(self, mat, name):\n        if len(mat.shape) != 2:\n            raise ValueError(''.join([\"Wrong matrix \", name]))\n        \n    # add a basis\n    def add_ones(self, X):\n        \"\"\"\n            add a column basis to X input matrix\n        \"\"\"\n        self._check_matrix(X, 'X')\n        return np.hstack((np.ones((X.shape[0], 1)), X))\n\n    ####################################################\n    #### abstract funcitons ############################\n    @abstractmethod\n    def train(self, X, T):\n        \"\"\"\n            train linear model\n            \n            parameters\n            -----------\n            X     2d array\n                  input data\n            T     2d array\n                  target labels\n        \"\"\"        \n        pass\n    \n    @abstractmethod\n    def use(self, X):\n        \"\"\"\n            apply the learned model to input X\n            \n            parameters\n            ----------\n            X     2d array\n                  input data\n            \n        \"\"\"        \n        pass","9cfd4bc3":"# Linear Regression Class for least squares\nclass LinearRegress(LinearModel): \n    \"\"\" \n        LinearRegress class \n        \n        attributes\n        ===========\n        w    nd.array  (column vector\/matrix)\n             weights\n    \"\"\"\n    def __init__(self):\n        LinearModel.__init__(self)\n        \n    # train lease-squares model\n    def train(self, X, T):\n        X = self.add_ones(X)\n        xtrans = X.T.dot(X)\n        self.w = np.linalg.pinv(xtrans).dot(X.T).dot(T)\n        self.w = self.w.T\n        print(self.w)\n        return self.w.T\n    \n    # apply the learned model to data X\n    def use(self, X):\n        X = self.add_ones(X)\n        a=self.w.dot(X.T)\n        return a.T\n       ","6057c23f":"import collections # for checking iterable instance\n\n# LMS class \nclass LMS(LinearModel):\n    \"\"\"\n        Lease Mean Squares. online learning algorithm\n    \n        attributes\n        ==========\n        w        nd.array\n                 weight matrix\n        alpha    float\n                 learning rate\n    \"\"\"\n    def __init__(self, alpha):\n        LinearModel.__init__(self)\n        self.alpha = alpha\n    \n    # batch training by using train_step function\n    def train(self, X, T):\n        for x,t in zip(X,T):\n            self.train_step(x,t)\n            \n    # train LMS model one step \n    # here the x is 1d vector\n    def train_step(self, x, t):\n        x = x.reshape(1,x.size)\n        xr = self.add_ones(x)\n        t = t.reshape(t.size,1)\n        if self.w is None:\n            self.w = np.zeros((xr.shape[1],1))\n        self.w = self.w - self.alpha*(xr@self.w - t)*xr.T     ## TODO: replace this with your codes\n        #else\n            #self.w= self.train_step(x,t)\n    \n    # apply the current model to data X\n    def use(self, X):\n        N = X.shape[0]\n        X1 = np.hstack((np.ones((N, 1)), X.reshape((X.shape[0], -1))))\n        return X1 @ self.w  \n        ","ae67bdaa":"import matplotlib.pyplot as plt\n%matplotlib inline","94c7a4a8":"X = np.linspace(0,10, 11).reshape((-1, 1))\nT = -2 * X + 3.2\nls = LinearRegress()\nls.train(X, T)\nplt.plot(ls.use(X))","3dfee0dc":"fig,ax=plt.subplots(figsize=(8,8))\nX = np.linspace(0,10, 11).reshape((-1, 1))\nT = -2 * X + 3.2\nls = LinearRegress()\nls.train(X, T)\nplt.plot(ls.use(X))","e18dd6ac":"fig,ax=plt.subplots(figsize=(8,8))\nlms = LMS(0.01)\nfor x, t in zip(X, T):\n    lms.train_step(x, t)\n    plt.plot(lms.use(X))","2f383aaa":"lms.train(X, T)\nplt.plot(lms.use(X))","89827b90":"#We create X and y and find their values. Now y is an array and it is transposed.\nX=df[['bedrooms','bathrooms','sqft_living','sqft_lot','condition','grade','floors']].values\ny=df['price'].values\ny=np.array([y])\ny=y.transpose()","d0d4715d":"#We are plotting graph for train and prediction of the data for least squares\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=15)\nls=LinearRegress()\nls.train(X_train,y_train)\ny_pred=ls.use(X_test)\nfig,ax=plt.subplots(figsize=(10,7))\nplt.scatter(y_test,y_pred)\nplt.plot(X_test,ls.use(X_test))  ","ded632b8":"#plot the graph for X training set \nfig,ax=plt.subplots(figsize=(10,7))\nplt.plot(ls.use(X_train))","b0042cf8":"#we plot graph for difference between y test and y prediction\nfig,ax=plt.subplots(figsize=(10,7))\nsns.distplot((y_test-y_pred),bins=20);","3121197e":"#plot graph for y test and y prediction to see the variation of the data\nfig,ax=plt.subplots(figsize=(10,7))\nplt.plot(y_test, 'g-*')\nplt.plot(y_pred, 'r-+')","9d136961":"##################### WHAT I WILL RELEASE ############\n\n# Self-Test code for accuracy of your model - DO NOT MODIFY THIS\n# Primilnary test data\nX = np.array([[2,5],\n              [6,2],\n              [1,9],\n              [4,5],\n              [6,3],\n              [7,4],\n              [8,3]])\nT = X[:,0, None] * 3 - 2 * X[:, 1, None] + 3\nN = X.shape[0]\n\ndef rmse(T, Y):\n    return np.sqrt(np.sum((T-Y)**2))\n\nmodel_names = ['LS', 'LMS_All', 'LMS_1STEP']\nmodels = [LinearRegress(), LMS(0.02), LMS(0.02)]\n#train\nfor i, model in enumerate(models):\n    print(\"training \", model_names[i], \"...\") \n    if i == len(models) -1: \n        # train only one step for LMS2\n        model.train_step(X[0], T[0])\n    else:\n        model.train(X, T)\n\ndef check(a, b, eps=np.finfo(float).eps):\n    if abs(a-b) > eps:\n        print(\"failed.\", a, b)\n    else:\n        print(\"passed.\")\n\nerrors = [1.19e-13, 2.8753214702, 38.0584918251]\nfor i, model in enumerate(models):\n    print(\"---- Testing \", model_names[i], \"...\", end=\" \") \n    \n    # rmse test\n    err = rmse(T, model.use(X))\n    if check(err, errors[i], eps=1e-10):\n        print (\"check your weights: \", model.w)\n        print (\"oracle: \", )","11f2344b":"# Usage Examples","9740438c":"# References\n\n1) https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction\n\n2) https:\/\/towardsdatascience.com\/least-squares-linear-regression-in-python-54b87fc49e77\n\n3) https:\/\/towardsdatascience.com\/linear-regression-using-least-squares-a4c3456e8570\n\n4) Youtube Tutorials,Stackoverflow\n\n5) https:\/\/nbviewer.jupyter.org\/url\/webpages.uncc.edu\/mlee173\/teach\/itcs6156\/notebooks\/assign\/Assign1.ipynb","612db989":"# Analysis \/ Comparision of Algorithms\n\n**Least Squares**\n\nAs the name implies, the method of Least Squares minimizes the sum of the squares of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. We calculate the distance from the line to a given data point by subtracting one from the other. We take the square of the difference because we don\u2019t want the predicted values below the actual values to cancel out with those above the actual values.\n\nThe least squares regression analysis is easy to implement, It is faster, easy to compute and less overhead to the system. It has less senstivity to outliers. But the problem is, it is not normally distributed. It takes a lot of time to compute for larger datasets.\n\n\n**Least mean squares**\n\nAs the name implies, the method of Least Mean Squares minimizes the mean of the squares of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. We calculate the distance from the line to a given data point by subtracting one from the other. We take the square of the difference and find mean of it because we don\u2019t want the predicted values below the actual values to cancel out with those above the actual values.\n\nThe least mean squares regression analysis is faster, the data is normally distributed, it is good for large datsets. It is not much effected by the outliers. Not suggested for smaller datsets as it produces unexpected datasets.\n","e930f244":"### Least Mean Squares","bed848ae":"# Preliminary Observation\n\nAfter observing the data and graphs carefully, we can observe that the price is mostly dependent on the location, no of bedrooms, bathrooms, sqft living, sqft lot, floors, condition. We can note a few points:\n\n(1) The frequency of no of bedrooms=3,4 is higher than any other bedrooms. The price of these are mostly similar, but some have giher price than usual because of the other features like bathrooms, location, etc.\n\n(2) The heatmap identifies the correlations between the features which help us in identifying how the features are dependent on each other which cannot be known by seeing the data. (Example: sqft living is dependent on grade of the house)\n\n(3) The highest priced houses are sold in months: 9th to 11th. This shows people tend to spend more money on houses which are having more comforts in winter.\n\n(4) Most of the houses have sqft living in between 500 to 6000 irrespective of no of bedrooms. The higer the living space, the higher is the cost.\n\n(5) Price of the house is also dependent on sqft of lot (parking) as most people own their own car.\n\n(6) People are tending to pay less if the condition of the house is bad. They are spending more if the house is in good condition.\n\n(7) The 3d plot gives relationship between multiple features.\n","4a8a0412":"# Preprocessing of Data","730200c1":"### Super Classs Definition","bb04035b":"# Data\n\n## Description\n\nIn this dataset we have to predict the **sales price of houses in King County, Seattle**. It includes homes sold between May 2014 and May 2015. Before doing anything we should first know about the dataset what it contains what are its features and what is the structure of data.\n\nThe dataset cantains 20 house features plus the price, along with 21613 observations.\n\nThe description for the 20 features is given below:\n\n1. id :- It is the unique numeric number assigned to each house being sold.\n2. date :- It is the date on which the house was sold out.\n3. price:- It is the price of house which we have to predict so this is our target variable and aprat from it are our features.\n4. bedrooms :- It determines number of bedrooms in a house.\n5. bathrooms :- It determines number of bathrooms in a bedroom of a house.\n6. sqft_living :- It is the measurement variable which determines the measurement of house in square foot.\n7. sqft_lot : It is also the measurement variable which determines square foot of the lot.\n8. floors: It determines total floors means levels of house.\n9. waterfront : This feature determines whether a house has a view to waterfront 0 means no 1 means yes.\n10. view : This feature determines whether a house has been viewed or not 0 means no 1 means yes.\n11. condition : It determines the overall condition of a house on a scale of 1 to 5.\n12. grade : It determines the overall grade given to the housing unit, based on King County grading system on a scale of 1 to 11\n13. sqft_above : It determines square footage of house apart from basement.\n14. sqft_basement : It determines square footage of the basement of the house.\n15. yr_built : It detrmines the date of building of the house.\n16. yr_renovated : It detrmines year of renovation of house.\n17. zipcode : It determines the zipcode of the location of the house.\n18. lat : It determines the latitude of the location of the house.\n19. long : It determines the longitude of the location of the house.\n20. sqft_living15 : Living room area in 2015(implies-- some renovations)\n21. sqft_lot15 : lotSize area in 2015(implies-- some renovations)\n\nBy observing the data, we can know that the **price is dependent on various features** like bedrooms(which is most dependent feature), bathrooms, sqft_living(second most important feature), sqft_lot, floors etc. The price is also dependent on the location of the house where it is present. The other features like waterfront, view are less dependent on the price. Of all the records, there are **no missing values, which helps us creating better model.** \n\nFirst, we **import** the required libraries like pandas, numpy, seaborn, matplotlib. Now import the **csv file.** Now we should get to know how the data is, what datatype using info function. We observe that date is in 'object' format. To know the no of rows and columns we use shape function. Describe the dataframe to know the mean, minumum, ,maximum, standard deviation, percentiles. \n\nNow, find if there are any null values. Luckily, there are no null values which helps us in getting the accurate model. We drop the unnecessary columns ike 'lat', 'long', 'view'. **There is an outlier in the given data.** Although it is not exactly a outlier, i would like to remove the value so that we can obtain better graph. The **no of bedrooms is '33'** for one observation. I removed that value. \n\nThe **date is in object format, so it is converted into date format** using pandas. Now, the graphs are plotted for better understanding or better analysis. The graphs like relplot, boxplot, heatplot, lineplot, catplot are used for analysis.\n","53e89035":"# Plots for understanding or Analysis\n\nA plot is a graphical technique for representing a data set, usually as a graph showing the relationship between two or more variables. Graphs are a visual representation of the relationship between variables, which are very useful for humans who can then quickly derive an understanding which may not have come from lists of values.\n\nWe have plotted the graphs like **replot, boxplot, heatmap, lineplot, catplot, 3d graph**. We plotted graphs between price, bedrooms, bathrooms, sqft_living, sqft_lot, date of purchase, condition for analyzing the data. Relplot is ised to find the relation between bedrooms and price. We plotted boxplot for further understanding. A graph is plotted between bathrooms and price. Heatmap is plotted between all the columns for finding correlation. \n\n> A plot is plotted for finding dates vs price. Graphs are plotted for finding the relationship between sqft_living vs price, sqft_lot vs price, condition vs price.","72d750d1":"# Conclusion\n\nThe objective of this kernel is to create a **linear regression model**. We performed data preprocessing and Exploratory data analysis on the dataset 'House Sales in King County, USA' to obtain the correlations between the features and find which features are useful for predicting linear regression model. We are performing least squares and least mean sqaures for the given dataset. The least squares is perfect for smaller datasets, it is not normalized where as least mean squares is perfect for large datasets, we normalize the data for reducing the skewness of the data. We plot graphs for the line and see if it fits correctly for the data or not. The preliminary tests shows that we obtained both least squares and least mean squares correctly.","ae5cb1f7":"### Least Squares","a14bd786":"# Method\n\n\n## Review\n\n**Least Squares**\n\nAs the name implies, the method of Least Squares minimizes the sum of the squares of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. We calculate the distance from the line to a given data point by subtracting one from the other. We take the square of the difference because we don\u2019t want the predicted values below the actual values to cancel out with those above the actual values.\n\n**Least mean squares**\n\nAs the name implies, the method of Least Mean Squares minimizes the mean of the squares of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. We calculate the distance from the line to a given data point by subtracting one from the other. We take the square of the difference and find mean of it because we don\u2019t want the predicted values below the actual values to cancel out with those above the actual values.\n\n## Explination of Codes\n\n### Least Squares\n\nFirst we initialize the model with __init__. We defined two classes train(self,X,T) and use(self,X). This is subclass of Basemodel ABC.\n\n**Train(self,X,T):**\n\nThe function trains the weight w for the whole training dataset. We add new data to the vector xtx. It accepts both dependent and independent variables and uses both in the formula of calculating the best weight for the model. The weight is calculated using self.w. \n\n**Use(self,X):**\n\nuse(self,X) function accepts the test values of X. Based on the values of X the function estimates the value of Y and returns it to the calling statement. It creates hypothesis for the data and returns hypothesis.\n\n### Least Mean Squares\n\nFirst we initialize the model with __init__. We define three classes train(self,X,T), train_step(self,x,t) and use(self,X).\n\n**Train(self,X,Y)**\n\nThe function trains the weight w for the whole training dataset. For every x,t we initialize train_step(self,x,t) function. The function accepts the dependent and the independent values as parameters and based on the recieved data it sends the data one by one to the train_step(self, x, t) function to update the step function w sequentialy. Thus it recursively calls the train_step(self, x, t) function until all the values of X and T is exhausted.\n\n**train_step(self,x,t)**\n\nThis function will first check if the x has 2 dimensional matrix or not. If it is not 2 dimensional, it will insert the values. Now it will check if the self.w is empty or not. If it is empty then it will add zeroes to self.w array. If it is not empty, then it will check for the alpha value for the line.\n\n**use(self,X)**\n\nuse(self,X) function accepts the test values of X as parameters. It initializes the hstack. Based on the values of X the function estimates the value of the dependent variable and returns it to the calling statement. ","e0007619":"# Preliminary Test","49d1680a":"# Linear Regression","e095ec32":"# Introduction\n\n\nThe objective of this kernel is to create a **linear regression model** for a given dataset( House Sales in King County, USA). The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable?  (2) Which variables in particular are significant predictors of the outcome variable, and in what way do they\u2013indicated by the magnitude and sign of the beta estimates\u2013impact the outcome variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables.\n\n**Linear Regression Analysis** consists of more than just fitting a linear line through a cloud of data points.  It consists of 3 stages \u2013 (1) analyzing the correlation and directionality of the data, (2) estimating the model, i.e., fitting the line, and (3) evaluating the validity and usefulness of the model.\n\n(1) First import the file, perform data preprocessing(removing null values, changing data type, etc) and then perform EDA(Exploratory Data Analysis) for the data. Find if there are any correlations between the data.\n\n(2) Now we create a base model ABC, create required abstract classes. Create linearmodel class which is sub class of ABC class. Define the required methods. Now we create two classes **least squares** and **least mean squares** which are linear regression classes. We intialize the models, create train method to train the data. We intialize the train step so that the line can take the no of steps required.\n\n(3) In order to know that the created model is vaild, we create plots to see the predicted line and the points where the required value should be vaild. We know that the model is valid and is useful when it passes the preliminary test, where the least mean square and least squares are verified with various data.\n","c990b97a":"# Experiments\n\n### Description, Codes, Visualization\n\nFirst we create X,y by definind their features. Now consider y as an array. Transpose the array. We plot different graphs for the training set, test for viewing the linear regression."}}