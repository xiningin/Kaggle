{"cell_type":{"7f398d89":"code","77b11d17":"code","481feaf6":"code","66028df6":"code","f236d0cf":"code","6ce03b80":"code","5f5bc77d":"code","83cc14d3":"code","1362f200":"code","cd38c6da":"code","2132d05a":"code","52e9a41b":"code","ca81ed61":"code","fadce841":"code","14125960":"code","c3ef9f9c":"code","e44dd074":"code","4ca615da":"code","6871b7bb":"code","fb46e0c4":"code","fa645f43":"code","b338b757":"code","a5f8ec6d":"code","872f9b7f":"code","d5c658de":"code","29955e5b":"code","5fd4da23":"code","f501f3e9":"code","0e039542":"code","4ae964d2":"code","d73e7ca5":"code","6800e6e3":"code","98b3a0d6":"code","639b1556":"code","54ef4330":"code","b24739a8":"code","d9484254":"markdown","5dbba4e9":"markdown","e04aaf10":"markdown","be03fb95":"markdown","da4504fa":"markdown","f98f3468":"markdown","c1c272cf":"markdown","3d80752e":"markdown","3eb968dc":"markdown","af53c5d1":"markdown","690fa9e3":"markdown","2038a22e":"markdown","c07f6e3c":"markdown","ebd24d2f":"markdown","1e300c4e":"markdown","8a4dacf1":"markdown","df28373a":"markdown","eb0624be":"markdown","15a4bf2a":"markdown","34b2c396":"markdown","519ccd87":"markdown","28573a08":"markdown"},"source":{"7f398d89":"# Loading packages\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc","77b11d17":"import os\nprint(os.listdir(\"..\/input\"))","481feaf6":"train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of train : \",train.shape)","66028df6":"train.head()","f236d0cf":"test = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of test : \",test.shape)","6ce03b80":"# checking missing data\ntotal = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","5f5bc77d":"# checking missing data\ntotal = test.isnull().sum().sort_values(ascending = False)\npercent = (test.isnull().sum()\/test.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","83cc14d3":"import datetime\n\nfor df in [train,test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n\ntarget = train['target']\ndel train['target']","1362f200":"train.head()","cd38c6da":"ht = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nprint(\"shape of historical_transactions : \",ht.shape)","2132d05a":"ht.head()","52e9a41b":"ht['authorized_flag'] = ht['authorized_flag'].map({'Y':1, 'N':0})","ca81ed61":"def aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(ht)\ndel ht\ngc.collect()","fadce841":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')","14125960":"merchant = pd.read_csv(\"..\/input\/merchants.csv\")\nprint(\"shape of merchant : \",merchant.shape)","c3ef9f9c":"merchant.head()","e44dd074":"# checking missing data\ntotal = merchant.isnull().sum().sort_values(ascending = False)\npercent = (merchant.isnull().sum()\/merchant.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","4ca615da":"new_merchant = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nprint(\"shape of new_merchant_transactions : \",new_merchant.shape)","6871b7bb":"new_merchant.head()","fb46e0c4":"# checking missing data\ntotal = new_merchant.isnull().sum().sort_values(ascending = False)\npercent = (new_merchant.isnull().sum()\/new_merchant.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","fa645f43":"new_merchant['authorized_flag'] = new_merchant['authorized_flag'].map({'Y':1, 'N':0})","b338b757":"def aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_merchant)","a5f8ec6d":"train = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')","872f9b7f":"train.info(verbose=False)","d5c658de":"use_cols = [col for col in train.columns if col not in ['card_id', 'first_active_month']]\n\ntrain = train[use_cols]\ntest = test[use_cols]\n\nfeatures = list(train[use_cols].columns)\ncategorical_feats = [col for col in features if 'feature_' in col]","29955e5b":"for col in categorical_feats:\n    print(col, 'have', train[col].value_counts().shape[0], 'categories.')","5fd4da23":"from sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","f501f3e9":"df_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","0e039542":"train.info(verbose=False)","4ae964d2":"def mem_usage(pandas_obj):\n    usage_b = pandas_obj.memory_usage(deep=True).sum()\n    usage_mb = usage_b \/ 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\ntrain_mem_int = train.select_dtypes(include=['int'])\nconverted_int = train_mem_int.apply(pd.to_numeric,downcast='unsigned')\n\nprint(\"Size of integer types before {}\".format(mem_usage(train_mem_int)))\nprint(\"Size of integer types after {}\".format(mem_usage(converted_int)))\n\ncompare_ints = pd.concat([train_mem_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['No. of types before','No. of types after']\nprint(compare_ints.apply(pd.Series.value_counts))\n\ntrain_col_int = list(train_mem_int.columns)\nprint(train_col_int)","d73e7ca5":"train_mem_float = train.select_dtypes(include=['float'])\nconverted_float = train_mem_float.apply(pd.to_numeric,downcast='float')\n\nprint(\"Size of float types before: {}\".format(mem_usage(train_mem_float)))\nprint(\"Size of float types after: {}\".format(mem_usage(converted_float)))\n\ncompare_floats = pd.concat([train_mem_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['No. of types before','No. of types after']\nprint(compare_floats.apply(pd.Series.value_counts))\n\nprint(\" \")\n\ntrain_col_float = list(train_mem_float.columns)\nprint(train_col_float)","6800e6e3":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport time\n\n\nlgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 11, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 128, \"learning_rate\" : 0.005, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=8, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nstart = time.time()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 1000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval = 400)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \nprint(np.sqrt(mean_squared_error(oof_lgb, target)))\nend = time.time()\nprint('Time taken in predictions: {0: .2f}'.format(end - start))\n#time_first = end - start","98b3a0d6":"#replace the floats with smaller datatypes\ncolumns_to_overwrite_float = train_col_float \ntrain.drop(labels=columns_to_overwrite_float, axis=\"columns\", inplace=True)\ntrain[columns_to_overwrite_float] = converted_float[columns_to_overwrite_float]","639b1556":"#replace the floats with smaller datatypes\ncolumns_to_overwrite_int = train_col_int \ntrain.drop(labels=columns_to_overwrite_int, axis=\"columns\", inplace=True)\ntrain[columns_to_overwrite_int] = converted_int[columns_to_overwrite_int]","54ef4330":"train.info(verbose=False)","b24739a8":"FOLDs = KFold(n_splits=8, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nstart = time.time()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 1000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval = 400)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \nprint(np.sqrt(mean_squared_error(oof_lgb, target)))\nend = time.time()\nprint('Time taken in predictions: {0: .2f}'.format(end - start))\n#time_first = end - start","d9484254":"### Missing value","5dbba4e9":"First_active_month of Train and Test looks similiar","e04aaf10":"**There are several datatypes, below we convert to smaller types**","be03fb95":"**Have reduced size of dataframe from 62MB to 33MB**\n\n**Now rerun with optimised datatypes**","da4504fa":"## DataFrame Optimisation\n**Lets look at the dataframe**","f98f3468":"## How to Reduce Training Times\n\nThis notebook takes the code EDA and LightGBM training from the following notebook:\n- https:\/\/www.kaggle.com\/yhn112\/data-exploration-lightgbm-catboost-lb-3-760\n\nAfter using the above code we optimise the pandas dataframe and then rerun the prediction and see the change in training time.  \nThis work will show you that by using just a couple of simple functions, you can reduce your training time.   When dataframes become very large this may save you many hours \n\nFor further info check out https:\/\/www.kaggle.com\/richarde\/random-forest-with-50-reduction-in-training-time\n\nGo down to **DataFrame Optimisation** header to see the added work\n","c1c272cf":"### Simple Exploration","3d80752e":"- first_active_month : ''YYYY-MM', month of first purchase\n- feature_1,2,3 : Anonymized card categorical feature\n- target : Loyalty numerical score calculated 2 months after historical and evaluation period","3eb968dc":"### Simple Exploration : new_merchants.csv","af53c5d1":"### Make a Baseline model\nkernel : https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you","690fa9e3":"**In this case the time saving has been quite small but as the size of the dataframe increases, applying these few simple tricks could save considerable time**","2038a22e":"### Simple Exploration : merchants.csv","c07f6e3c":"- card_id\t: Card identifier\n- month_lag\t: month lag to reference date\n- purchase_date\t: Purchase date\n- authorized_flag\t: Y' if approved, 'N' if denied\n- category_3\t: anonymized category\n- installments\t: number of installments of purchase\n- category_1 : \tanonymized category\n- merchant_category_id\t: Merchant category identifier (anonymized )\n- subsector_id\t: Merchant category group identifier (anonymized )\n- merchant_id\t: Merchant identifier (anonymized)\n- purchase_amount\t: Normalized purchase amount\n- city_id\t: City identifier (anonymized )\n- state_id\t: State identifier (anonymized )\n- category_2 : anonymized category","ebd24d2f":"feature_2 has not 3 when feature_1 == 5\nbut what is target low than -30???","1e300c4e":"`feature_1 - feature_3` has 0.58 but `target - feature` low correaltion value","8a4dacf1":"- merchant_id : Unique merchant identifier\n- merchant_group_id\t: Merchant group (anonymized )\n- merchant_category_id\t: Unique identifier for merchant category (anonymized )\n- subsector_id\t: Merchant category group (anonymized )\n- numerical_1\t: anonymized measure\n- numerical_2\t: anonymized measure\n- category_1\t: anonymized category\n- most_recent_sales_range\t: Range of revenue (monetary units) in last active month --> A > B > C > D > E\n- most_recent_purchases_range\t: Range of quantity of transactions in last active month --> A > B > C > D > E\n- avg_sales_lag3\t: Monthly average of revenue in last 3 months divided by revenue in last active month\n- avg_purchases_lag3\t: Monthly average of transactions in last 3 months divided by transactions in last active month\n- active_months_lag3\t: Quantity of active months within last 3 months\n- avg_sales_lag6\t: Monthly average of revenue in last 6 months divided by revenue in last active month\n- avg_purchases_lag6\t: Monthly average of transactions in last 6 months divided by transactions in last active month\n- active_months_lag6\t: Quantity of active months within last 6 months\n- avg_sales_lag12\t: Monthly average of revenue in last 12 months divided by revenue in last active month\n- avg_purchases_lag12\t: Monthly average of transactions in last 12 months divided by transactions in last active month\n- active_months_lag12\t: Quantity of active months within last 12 months\n- category_4\t: anonymized category\n- city_id\t: City identifier (anonymized )\n- state_id\t: State identifier (anonymized )\n- category_2\t: anonymized category","df28373a":"- card_id\t: Card identifier\n- month_lag\t: month lag to reference date\n- purchase_date\t: Purchase date\n- authorized_flag\t: Y' if approved, 'N' if denied\n- category_3\t: anonymized category\n- installments\t: number of installments of purchase\n- category_1\t: anonymized category\n- merchant_category_id\t: Merchant category identifier (anonymized )\n- subsector_id\t: Merchant category group identifier (anonymized )\n- merchant_id\t: Merchant identifier (anonymized)\n- purchase_amount\t: Normalized purchase amount\n- city_id\t: City identifier (anonymized )\n- state_id\t: State identifier (anonymized )\n- category_2\t: anonymized category\n","eb0624be":"feature_3 has 1 when feautre_1 high than 3","15a4bf2a":"**From above we have reduced the sizes of our int and float datatypes and reduced the size considerably.  We will run lightgbm with the original types then rerun with these downcast datatypes.**","34b2c396":"- train.csv - the training set\n- test.csv - the test set\n- sample_submission.csv - a sample submission file in the correct format - contains all card_ids you are expected to predict for.\n- historical_transactions.csv - up to 3 months' worth of historical transactions for each card_id\n- merchants.csv - additional information about all merchants \/ merchant_ids in the dataset.\n- new_merchant_transactions.csv - two months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.","519ccd87":"**Note the time taken above and final rmse score**\n\n**We have run lightgbm with the original datatypes.  Below we change to the optimised types**","28573a08":"### Simple Exploration : historical_transactions"}}