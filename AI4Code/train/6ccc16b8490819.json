{"cell_type":{"32a9714b":"code","80382971":"code","665a5dff":"code","034e5cf6":"code","6a84cabb":"code","6777b293":"code","663cbc71":"code","20b2e36c":"code","3602bc75":"code","ba15e62e":"code","b952bb06":"code","e2157c38":"code","ceb73394":"code","3f181e76":"code","8dd6d21e":"code","b4cf7d6f":"code","1c058e96":"code","2d82de74":"code","710b605b":"code","89ad9b3b":"code","93805e6f":"code","4205b344":"code","0d5c6081":"code","5caef3a6":"code","8c35b12a":"markdown","d3f18338":"markdown","7f3d70bf":"markdown","756cb0d2":"markdown","72fc38d7":"markdown","891126d1":"markdown","5a9ca5a2":"markdown","69954e3e":"markdown","74f7622e":"markdown","72fd1c3f":"markdown","a85179db":"markdown","2c18b4f9":"markdown","36ffceb6":"markdown","a9f97914":"markdown","69189ca6":"markdown","2620702e":"markdown","d7b98c95":"markdown","2c92dfcf":"markdown","5ec79432":"markdown","e94a7a1d":"markdown","1c73df81":"markdown","8aab1ff9":"markdown","9b02c840":"markdown"},"source":{"32a9714b":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline\n\n# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets","80382971":"import tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential, load_model\n\nfrom tensorflow.keras.layers import InputLayer, Dense, Dropout, Flatten\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, MaxPool2D\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator","665a5dff":"from tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomFlip, RandomRotation, RandomZoom, RandomContrast, RandomTranslation","034e5cf6":"def plot_scores(train) :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.title('Scores')\n    plt.legend()\n    plt.show()","6a84cabb":"img = load_img('..\/input\/alien-vs-predator-images\/data\/train\/alien\/0.jpg')","6777b293":"plt.imshow(img)","663cbc71":"data_augmentation = Sequential([\n    RandomFlip(\"horizontal\"),\n    RandomRotation(1.\/16),\n    RandomZoom((-0.1,0.1)),\n    RandomContrast(0.2),  \n    RandomTranslation(0.1,0.1)\n])","20b2e36c":"batch = np.expand_dims(img,0)","3602bc75":"for i in range(20):\n  augmented_image = data_augmentation(batch)\n  plt.imshow(augmented_image[0])\n  plt.axis(\"off\")\n  plt.show()","ba15e62e":"train_data_dir = \"..\/input\/alien-vs-predator-images\/data\/train\"\nimage_size = (299, 299)\n\ndataset = image_dataset_from_directory(\n    train_data_dir,\n    image_size=image_size,\n)","b952bb06":"plt.figure(figsize=(15, 25))\nclass_names = dataset.class_names\nfor images, labels in dataset.take(1):\n    for i in range(32):\n        plt.subplot(7, 5, i + 1)\n        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","e2157c38":"train_data_dir = \"..\/input\/alien-vs-predator-images\/data\/train\"\nimage_size = (299, 299)\n\ntrain_dataset = image_dataset_from_directory(\n    train_data_dir,\n    validation_split=0.2,\n    seed=1,\n    subset=\"training\",\n    label_mode=\"categorical\",\n    image_size=image_size\n)\n\nvalidation_dataset = image_dataset_from_directory(\n    train_data_dir,\n    validation_split=0.2,\n    seed=1,\n    subset=\"validation\",\n    label_mode=\"categorical\",\n    image_size=image_size\n)","ceb73394":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)","3f181e76":"data_augmentation = Sequential([\n    RandomFlip(\"horizontal\"),\n    RandomRotation(0.1),\n    RandomZoom((-0.1,0.1)),\n    RandomContrast(0.05),  \n    RandomTranslation(0.1,0.1)\n])","8dd6d21e":"# Mod\u00e8le CNN \nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(299, 299, 3)))\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(20, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n#model.add(Dense(2, activation='softmax', kernel_initializer=tf.keras.initializers.Constant(0.01)))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])","b4cf7d6f":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=50,\n    verbose=1)","1c058e96":"plot_scores(history)","2d82de74":"model = Sequential()\nmodel.add(InputLayer(input_shape=(299, 299, 3)))\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax', kernel_initializer=tf.keras.initializers.Constant(0.01)))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])","710b605b":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=50,\n    verbose=1)","89ad9b3b":"plot_scores(history)","93805e6f":"from tensorflow.keras.applications import Xception\nxception = Xception(weights='imagenet', include_top=False, input_shape=(299,299,3))\nxception.trainable = False","4205b344":"model = Sequential()\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(xception)\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])","0d5c6081":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=10,\n    verbose=1)","5caef3a6":"plot_scores(history)","8c35b12a":"On transforme l'image en un *batch* d'une image en ajoutant une dimension :","d3f18338":"On peut cr\u00e9er des datasets d'apprentissage et de validation \u00e0 partir du m\u00eame r\u00e9pertoire avec l'option **validation_split=0.2** (20% des donn\u00e9es pour la validation)  \nDans ce cas, il faut donner la m\u00eame valeur **seed** (initialisation du g\u00e9n\u00e9rateur al\u00e9atoire) pour les deux datasets (sinon, il ne seront pas disjoints)  \n\n**label_mode** d\u00e9termine la forme des labels :\n- **categorical** pour le multiclasses (utiliser une *loss* de type *categorical_crossentropy*)\n- **int** pour des \u00e9tiquettes au format entier (utiliser une *loss* de type *sparse_categorical_crossentropy*)\n- **binary** pour une classification binaire (utiliser une *loss* de type *binary_crossentropy*)\n","7f3d70bf":"On int\u00e8gre la couche d'augmentation dans le mod\u00e8le   \nOn ajoute \u00e9galement une couche **Rescaling**, puisque le dataset est g\u00e9n\u00e9r\u00e9 \"\u00e0 la vol\u00e9e\" (pour rappel, les algorithmes de descente du gradient convergent mieux pour des valeurs entre 0 et 1) :","756cb0d2":"**dataset.take(1)** g\u00e9n\u00e8re un premier batch (32 images par d\u00e9faut) :","72fc38d7":"## Transformations d'images","891126d1":"## Mod\u00e8le et entra\u00eenement","5a9ca5a2":"**image_dataset_from_directory** cr\u00e9e un *dataset Tensorflow* \u00e0 partir des images d'un r\u00e9pertoire donn\u00e9.  \n\nOn suppose que les images sont rang\u00e9es dans des sous-r\u00e9pertoires, dont les noms donnent les noms de classes (attribut **class_names**)  \n\nLes images sont lues en m\u00e9moire par *batch* (32 iamges par d\u00e9faut)","69954e3e":"# Alien vs predator avec augmentation","74f7622e":"On va utiliser un mod\u00e8le Xception   \nhttps:\/\/towardsdatascience.com\/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568","72fd1c3f":"Pour des datasets de grande taille, il est difficile de les charger en m\u00e9moire sans risquer de saturation de la m\u00e9moire  \nC'est pourquoi on charge g\u00e9n\u00e9ralement les datasets par *batch* depuis le disque pour les traiter (avec des g\u00e9n\u00e9rateurs comme **ImageDataGenerator**). N\u00e9anmoins, la performance peut en souffrir, notamment avec l'utilisation de GPU, l'apprentissage \u00e9tant principalement ralenti par les temps d'acc\u00e8s au disque.  \nAvec **image_dataset_from_directory**, on peut r\u00e9gler la *pr\u00e9lecture* (*prefetch*) de mani\u00e8re \u00e0 optimiser le s\u00e9quen\u00e7age des op\u00e9rations entre lecture des donn\u00e9es et apprentissage  \nLe param\u00e8tre **AUTOTUNE** permet de r\u00e9gler dynamiquement la taille du buffer  \n<img src=\"https:\/\/www.tensorflow.org\/guide\/images\/data_performance\/naive.svg\">  \n\nhttps:\/\/www.tensorflow.org\/guide\/data_performance   \nhttp:\/\/restanalytics.com\/2020-08-17-Transfer_Learning_and_Fine_tuning_withTensorFlow__Pneumonia_Classification_on_X_rays4\/","a85179db":"## Initialisations","2c18b4f9":"On cr\u00e9e une couche d'augmentation (attention : il n'est pas forc\u00e9ment souhaitable de r\u00e9aliser trop de transformations, au risque de compromettre la convergence de l'entra\u00eenement) :","36ffceb6":"### Divers","a9f97914":"### Fonctions utiles","69189ca6":"### Keras","2620702e":"Les *couches d'augmentation* (*augmentation layers*) de Keras sont des couches particuli\u00e8res d'un mod\u00e8le Keras qui permettents de transformer les images d'un *batch* d'images (ou d'un dataset) \n- **RandomFlip(*mode*)** : retourne une image ; *mode* peut \u00eatre \"horizontal\", \"vertical\", ou \"horizontal_and_vertical\"\n- **RandomRotation(*factor*)** : effectue une rotation de l'image de *factor**2*Pi\n- **RandomZoom(*height_factor*)** : effectue un zoom ; par exemple RandomZoom((-0.1,0.1)) effectue un zoom al\u00e9atoire entre -10% et +10%\n- **RandomContrast(*factor*)** : ajuste le contraste\n- **RandomTranslation(*height_factor*,*width_factor*)** : d\u00e9cale l'image de *width_factor* horizontalement et *height_factor* verticalement \n \n https:\/\/keras.io\/guides\/preprocessing_layers\/","d7b98c95":"Avec un mod\u00e8le de type VGG16 :","2c92dfcf":"On lance l'entra\u00eenement   \n**Remarque :** l'augmentation **ne g\u00e9n\u00e8re pas un dataset plus important en taille** ; \u00e0 chaque *epoch*, on utilise un \"nouveau\" dataset, avec des images (l\u00e9g\u00e8rement) transform\u00e9es","5ec79432":"## Lecture des images depuis un r\u00e9pertoire","e94a7a1d":"## Cr\u00e9ation des datasets ","1c73df81":"## Transfer learning","8aab1ff9":"## Am\u00e9lioration des performances (optionnel)","9b02c840":"On peut g\u00e9n\u00e9rer autant d'images transform\u00e9es qu'on le souhaite :"}}