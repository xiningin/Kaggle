{"cell_type":{"c6bc6341":"code","c3086dd5":"code","e7831530":"code","da983fce":"code","a8a7e8e8":"code","48efbd20":"code","18a3712a":"code","84ab77b5":"code","f1a42e5f":"code","4ce3d5d3":"code","c8cf1cf6":"code","dd596762":"code","d36dcc8e":"code","31147659":"code","f47bc731":"code","ba5402d2":"code","94a98b36":"code","41cc40dd":"code","2a793a95":"code","abb7c2d0":"code","252ae3f8":"code","3094e253":"code","1542b2bd":"code","09d8e5ca":"code","84ba081d":"code","64da87b1":"code","385a6b21":"code","ee774802":"code","35586437":"code","d9e79faa":"code","7ea132de":"code","b91658cc":"code","06978314":"code","7551e4bf":"code","3688a0b5":"code","8a90a16c":"code","6cc14174":"code","f7206788":"code","5ff37a5c":"code","7fd1d853":"code","d261f433":"code","d8e36451":"code","cf234296":"code","78f7a188":"code","238be4a1":"code","d76f3947":"code","41125119":"markdown","6da740c7":"markdown","8eb87ebf":"markdown","5316aad2":"markdown","7a102315":"markdown","e502f0d5":"markdown","3e92e64d":"markdown","6890f667":"markdown","870dc9b9":"markdown","f9eb67af":"markdown","0384c9d6":"markdown","39a14719":"markdown","0c2f7677":"markdown","7d38522c":"markdown","8a3ad82e":"markdown","cb8df52a":"markdown","d672e9bb":"markdown","f8ef36ad":"markdown","21a3c507":"markdown","7d41f95b":"markdown","cba393bf":"markdown","a7afd478":"markdown","33fb8fc9":"markdown","2bbb251f":"markdown","ac226768":"markdown","cdb864bc":"markdown","77b5ddd7":"markdown","464cb584":"markdown"},"source":{"c6bc6341":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport re\n\nprint(\"Tensorflow Version\",tf.__version__)","c3086dd5":"!pip install scikit-learn>=0.24\n!pip install ktrain # for BERT model","e7831530":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv',\n                 encoding = 'latin')\ndf.head()","da983fce":"df = df.drop(['id', 'keyword', 'location'], axis=1)\ndf.columns = ['text', 'sentiment']\ndf.head()","a8a7e8e8":"lab_to_sentiment = {0:\"Negative\", 1:\"Positive\"}\ndef label_decoder(label):\n  return lab_to_sentiment[label]\ndf.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\ndf.head()","48efbd20":"val_count = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","18a3712a":"import random\nrandom_idx_list = [random.randint(1,len(df.text)) for i in range(10)] # creates random indexes to choose from dataframe\ndf.loc[random_idx_list,:].head(10) # Returns the rows with the index and display it","84ab77b5":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","f1a42e5f":"def preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","4ce3d5d3":"df.text = df.text.apply(lambda x: preprocess(x))","c8cf1cf6":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\nplt.imshow(wc , interpolation = 'bilinear')","dd596762":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\nplt.imshow(wc , interpolation = 'bilinear')","d36dcc8e":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 30","31147659":"train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n                                         random_state=7) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size\", len(test_data))","f47bc731":"train_data.head(10)","ba5402d2":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)","94a98b36":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","41cc40dd":"labels = train_data.sentiment.unique().tolist()","2a793a95":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","abb7c2d0":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","252ae3f8":"GLOVE_EMB = '\/kaggle\/working\/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 15\nMODEL_PATH = '...\/output\/kaggle\/working\/best_model.hdf5'","3094e253":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","1542b2bd":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","09d8e5ca":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","84ba081d":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","64da87b1":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","385a6b21":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","ee774802":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","35586437":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","d9e79faa":"s, (at, al) = plt.subplots(2,1)\nat.plot(history.history['accuracy'], c= 'b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')","7ea132de":"def decode_sentiment(score):\n    return \"Positive\" if score>0.5 else \"Negative\"\n\n\nscores = model.predict(x_test, verbose=1, batch_size=10000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","b91658cc":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)","06978314":"cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()","7551e4bf":"print(classification_report(list(test_data.sentiment), y_pred_1d))","3688a0b5":"# import ktrain\n# from ktrain import text","8a90a16c":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","6cc14174":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","f7206788":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","5ff37a5c":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","7fd1d853":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","d261f433":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","d8e36451":"train_input = bert_encode(train_data.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(train_data.text.values, tokenizer, max_len=160)\ntrain_labels = y_train","cf234296":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","78f7a188":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, y_train,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","238be4a1":"model.load_weights('.\/model.h5')\ntest_pred = model.predict(test_input)","d76f3947":"test_pred","41125119":"### Model Evaluation","6da740c7":"### Classification Scores","8eb87ebf":"### Word Emdedding","5316aad2":"![![image.png](attachment:125d6032-ba1c-489f-8fda-033c7e51a57f.png)![image.png](attachment:4bd86b03-dee4-42f2-b45a-c361b1faeb48.png)](http:\/\/)","7a102315":"Reccurent Neural Networks can handle a seqence of data and learn a pattern of input seqence to give either sequence or scalar value as output. In our case, the Neural Network outputs a scalar value prediction.\n\nFor model architecture, we use\n\n1) **Embedding Layer** - Generates Embedding Vector for each input sequence.\n\n2) **Conv1D Layer** - Its using to convolve data into smaller feature vectors.\n\n3) **LSTM - Long Short Term Memory**, its a variant of RNN which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN.\n\n4) **Dense** - Fully Connected Layers for classification","e502f0d5":"### Positive Words","3e92e64d":"Now that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch.","6890f667":"\n\nLet's start training... It takes a heck of a time if training in CPU, be sure your GPU turned on... May the CUDA Cores be with you....\n","870dc9b9":"\n\nThe model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as POSITIVE sentiment.\n","f9eb67af":"We are building the model to predict class in enocoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings.","0384c9d6":"### Model Training - LSTM","39a14719":"**Sentiment Classification** is a perfect problem in NLP for getting started in it. You can really learn a lot of concepts and techniques to master through doing project. Kaggle is a great place to learn and contribute your own ideas and creations. I learnt lot of things from other, now it's my turn to make document my project.","0c2f7677":" If you find this notebook helpful, please leave a UPVOTE to encourage me","7d38522c":"In Language Model, words are represented in a way to intend more meaning and for learning the patterns and contextual meaning behind it.\n\nWord Embedding is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\nBasically, it's a feature vector representation of words which are used for other natural language processing applications.\n\nWe could train the embedding ourselves but that would take a while to train and it wouldn't be effective. So going in the path of Computer Vision, here we use Transfer Learning. We download the pre-trained embedding and use it in our model.\n\nThe pretrained Word Embedding like GloVe & Word2Vec gives more insights for a word which can be used for classification. If you want to learn more about the Word Embedding, please refer some links that I left at the end of this notebook.\n\nIn this notebook, I use GloVe Embedding from Stanford AI which can be found here","8a3ad82e":"\n### Confusion Matrix\n\nConfusion Matrix provide a nice overlook at the model's performance in classification task\n","cb8df52a":"# BERT","d672e9bb":"\n\nIt's a pretty good model we trained here in terms of NLP. Around 80% accuracy is good enough considering the baseline human accuracy also pretty low in these tasks. Also, you may go on and explore the dataset, some tweets might have other languages than English. So our Embedding and Tokenizing wont have effect on them. But on practical scenario, this model is good for handling most tasks for Sentiment Analysis.\n","f8ef36ad":"We are clear to build our Deep Learning model. While developing a DL model, we should keep in mind of key things like Model Architecture, Hyperparmeter Tuning and Performance of the model.\n\nAs you can see in the word cloud, the some words are predominantly feature in both Positive and Negative tweets. This could be a problem if we are using a Machine Learning model like Naive Bayes, SVD, etc.. That's why we use Sequence Models.","21a3c507":"![https:\/\/miro.medium.com\/max\/1458\/1*SICYykT7ybua1gVJDNlajw.png](http:\/\/)","7d41f95b":"### Label Encoding","cba393bf":"### Negative Words","a7afd478":"### Sequence Model","33fb8fc9":"EDA: \n[NLP & Exploratory Data Analysis](https:\/\/www.kaggle.com\/mnavaidd\/nlp-exploratory-data-analysis)","2bbb251f":"### Train and Test Split","ac226768":"# If you find this notebook usefull kindly UPVOTE this notebook.","cdb864bc":"### Optimization Algorithm","77b5ddd7":"# Natural Language Processing\n\n**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject. ","464cb584":"This notebook uses Adam, optimization algorithm for Gradient Descent. You can learn more about Adam here\n\n### Callbacks\n\nCallbacks are special functions which are called at the end of an epoch. We can use any functions to perform specific operation after each epoch. I used two callbacks here,\n\n    LRScheduler - It changes a Learning Rate at specfic epoch to achieve more improved result. In this notebook, the learning rate exponentionally decreases after remaining same for first 10 Epoch.\n\n    ModelCheckPoint - It saves best model while training based on some metrics. Here, it saves the model with minimum Validity Loss.\n"}}