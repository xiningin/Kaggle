{"cell_type":{"3804a0af":"code","82d713ec":"code","d9e49e0b":"code","16b1fff9":"code","104da1d5":"code","0a1054a9":"code","67f65ad1":"code","98208f44":"code","1a354546":"code","de8d13b4":"code","f17fca27":"code","021ad8fe":"code","760de27b":"code","33e8c14e":"code","c69caddd":"code","41504336":"code","ed950d4c":"code","f067f462":"code","fbf48ff9":"code","0bb054e1":"code","f4b7a91b":"code","80b72159":"code","b7a14833":"code","ad487786":"code","f2f68ce5":"code","3a3fd37a":"code","2deea3db":"code","2c366786":"code","3d3d7c1f":"code","6ffcc86f":"code","58749923":"code","24010351":"code","c6bc375f":"code","5c7362c5":"code","b7c0f61b":"code","ccbde3fd":"code","86f101b0":"code","bfdd8a45":"code","5d46039c":"code","c9b994c3":"code","288cb45f":"code","7c8c3d9c":"code","9eb7412b":"code","1bd69ff9":"markdown","bfdae4c8":"markdown","106e8702":"markdown","b0dee9c7":"markdown","7d96e207":"markdown"},"source":{"3804a0af":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, confusion_matrix\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","82d713ec":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","d9e49e0b":"data.info()","16b1fff9":"data.describe()","104da1d5":"plt.figure(figsize=(20,12), dpi= 60)\nplt.title('Distribution of Outcome variable')\nplt.pie(data['Outcome'].value_counts(), labels = ['healthy','diabetic'], colors = ['gold', 'lightcoral'], autopct='%1.1f%%', shadow=True, startangle=140)\nplt.show()","0a1054a9":"for i in data.columns:\n    print(i, data[i][data[i] == 0].count())","67f65ad1":"plt.figure(figsize = (12, 8))\nax = sns.boxplot(data = data, orient = 'h', palette = 'Set2')\nplt.title('Boxplot overview dataset')\nplt.xlabel('values')\nplt.xlim(-3, 300)\nplt.show()","98208f44":"plt.figure(figsize = (12, 8))\nsns.heatmap(data.corr(), annot = True)\nplt.title('Correlation matrix')\nplt.show()","1a354546":"def median_feature(feature):\n    temp = data[data[feature] > 0]\n    med_cat = temp.groupby('Outcome')[feature].median().reset_index()\n    return med_cat","de8d13b4":"def preparing_feature(feature, median_data):\n    data.loc[(data['Outcome'] == 0) & (data[feature] == 0), feature] = median_data[median_data['Outcome'] == 0][feature].median()\n    data.loc[(data['Outcome'] == 1) & (data[feature] == 0), feature] = median_data[median_data['Outcome'] == 1][feature].median()","f17fca27":"def kdeplot(feature, xlabel, title):\n    plt.figure(figsize = (12, 8))\n    ax = sns.kdeplot(data[feature][(data['Outcome'] == 0) & \n                             (data[feature].notnull())], color = 'darkturquoise', shade = True)\n    ax = sns.kdeplot(data[feature][(data['Outcome'] == 1) & \n                             (data[feature].notnull())], color = 'lightcoral', shade= True)\n    plt.xlabel(xlabel)\n    plt.ylabel('frequency')\n    plt.title(title)\n    ax.grid()\n    ax.legend(['healthy','diabetic'])\nkdeplot('Glucose', 'concentration', 'Glucose')","021ad8fe":"median_feature_glucose = median_feature('Glucose')\nmedian_feature_glucose","760de27b":"preparing_feature('Glucose', median_feature_glucose)","33e8c14e":"kdeplot('Insulin', 'mu U\/ml', 'Insulin')","c69caddd":"median_feature_insulin = median_feature('Insulin')\nmedian_feature_insulin","41504336":"data['Insulin'] = data['Insulin'].astype('float')\npreparing_feature('Insulin', median_feature_insulin)","ed950d4c":"kdeplot('BloodPressure', 'mm Hg', 'BloodPressure')","f067f462":"median_feature_bpressure = median_feature('BloodPressure')\nmedian_feature_bpressure","fbf48ff9":"data['BloodPressure'] = data['BloodPressure'].astype('float')\npreparing_feature('BloodPressure', median_feature_bpressure)","0bb054e1":"kdeplot('SkinThickness', 'mm', 'SkinThickness')","f4b7a91b":"median_feature_skinthickness = median_feature('SkinThickness')\nmedian_feature_skinthickness","80b72159":"preparing_feature('SkinThickness', median_feature_skinthickness)","b7a14833":"kdeplot('BMI', 'weight in kg\/(height in m)^2', 'BMI')","ad487786":"median_feature_bmi = median_feature('BMI')\nmedian_feature_bmi","f2f68ce5":"preparing_feature('BMI', median_feature_bmi)","3a3fd37a":"for i in data.columns:\n    print(i, data[i][data[i] == 0].count())","2deea3db":"kdeplot('Age', 'years', 'Age')","2c366786":"kdeplot('DiabetesPedigreeFunction', 'diabetes pedigree function', 'DiabetesPedigreeFunction')","3d3d7c1f":"kdeplot('Pregnancies', 'number of times pregnant', 'Pregnancies')","6ffcc86f":"X = data.drop(['Outcome'], axis = 1)\ny = data['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 12345)","58749923":"numeric = []\nfor i in X_train.columns:\n        numeric += [i]\nscaler = StandardScaler()\nscaler.fit(X_train[numeric])\nX_train[numeric] = scaler.transform(X_train[numeric])\nX_test[numeric] = scaler.transform(X_test[numeric])","24010351":"def confusion_m(model, title):\n    cm = confusion_matrix(y_test, model.predict(X_test))\n    f, ax = plt.subplots(figsize = (8, 6))\n    sns.heatmap(cm, annot = True, linewidths = 0.5, cmap = 'Greens', fmt = '.0f', ax = ax)\n    plt.xlabel('y_predicted')\n    plt.ylabel('y_true')\n    plt.title(title)\n    plt.show()\n\ndef feature_importance(model, title):\n    dataframe = pd.DataFrame(model, X_train.columns).reset_index()\n    dataframe = dataframe.rename(columns = {'index':'features', 0:'coefficients'})\n    dataframe = dataframe.sort_values(by = 'coefficients', ascending = False)\n    plt.figure(figsize=(13,10), dpi= 60)\n    ax = sns.barplot(x = 'coefficients', y = 'features', data = dataframe ,palette = 'viridis')\n    plt.title(title, fontsize = 20)\n    plt.grid()","c6bc375f":"lr = LogisticRegression(random_state = 12345)\nparameters_lr = {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 3, 5, 7, 10, 15, 20, 25, 30, 50], \n                 'penalty':['l1', 'l2', 'elasticnet', 'none'],\n                 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                 'class_weight': [1, 3, 10],\n                 'max_iter': [200, 500, 800, 1000, 2000]}\nsearch_lr = RandomizedSearchCV(lr, parameters_lr, cv=5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_lr.fit(X_train, y_train)\nbest_lr = search_lr.best_estimator_\npredict_lr = best_lr.predict(X_test)\nauc_lr = cross_val_score(best_lr, X_test, y_test, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_lr = cross_val_score(best_lr, X_test, y_test, scoring = 'accuracy', cv = 10, n_jobs = -1)\nprint('AUC-ROC for Logistic Regression on test dataset:', sum(auc_lr)\/len(auc_lr))\nprint('Accuracy for Logistic Regression on test dataset:', sum(acc_lr)\/len(acc_lr))","5c7362c5":"feature_importance(best_lr.coef_[0], 'Feature importance for Logistic Regression')\nconfusion_m(best_lr, 'Confusion matrix for Logistic Regression')","b7c0f61b":"rf = RandomForestClassifier(random_state = 12345)\nparameters_rf = {'n_estimators': range(1, 1800, 25), \n                 'criterion': ['gini', 'entropy'], \n                 'max_depth':range(1, 100), \n                 'min_samples_split': range(1, 12), \n                 'min_samples_leaf': range(1, 12), \n                 'max_features':['auto', 'log2', 'sqrt', 'None']}\nsearch_rf = RandomizedSearchCV(rf, parameters_rf, cv=5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\n\nsearch_rf.fit(X_train, y_train)\nbest_rf = search_rf.best_estimator_\npredict_rf = best_rf.predict(X_test)\nauc_rf = cross_val_score(best_rf, X_test, y_test, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_rf = cross_val_score(best_rf, X_test, y_test, scoring = 'accuracy', cv = 10, n_jobs = -1) \nprint('AUC-ROC for Random Forest on test dataset:', sum(auc_rf)\/len(auc_rf))\nprint('Accuracy for Random Forest on test dataset:', sum(acc_rf)\/len(acc_rf))","ccbde3fd":"feature_importance(best_rf.feature_importances_, 'Feature importance for Random Forest')\nconfusion_m(best_rf, 'Confusion matrix for Random Forest')","86f101b0":"xgb = XGBClassifier(random_state = 12345, eval_metric='auc')\nparameters_xgb = {'eta': [0.01, 0.05, 0.1, 0.001, 0.005, 0.04, 0.2, 0.0001],  \n                  'min_child_weight':range(1, 5), \n                  'max_depth':range(1, 6), \n                  'learning_rate': [0.01, 0.05, 0.1, 0.001, 0.005, 0.04, 0.2], \n                  'n_estimators':range(0, 2001, 50)}\nsearch_xgb = RandomizedSearchCV(xgb, parameters_xgb, cv = 5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_xgb.fit(X_train, y_train)\nbest_xgb = search_xgb.best_estimator_\npredict_xgb = best_xgb.predict(X_test)\nauc_xgb = cross_val_score(best_xgb, X_test, y_test, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_xgb = cross_val_score(best_xgb, X_test, y_test, scoring = 'accuracy', cv = 10, n_jobs = -1)\nprint('AUC-ROC for XGBoost on test dataset:', sum(auc_xgb)\/len(auc_xgb))\nprint('Accuracy for XGBoost on test dataset:', sum(acc_xgb)\/len(acc_xgb))","bfdd8a45":"feature_importance(best_xgb.feature_importances_, 'Feature importance for XGBoost')\nconfusion_m(best_xgb, 'Confusion matrix for XGBoost')","5d46039c":"cb = CatBoostClassifier(random_state = 12345, iterations = 300, eval_metric='Accuracy', verbose = 100)\nparameters_cb = {'depth': range(6, 11),\n                 'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.0001]}\nsearch_cb = RandomizedSearchCV(cb, parameters_cb, cv = 5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_cb.fit(X_train, y_train, verbose = 100)\nbest_cb = search_cb.best_estimator_\npredict_cb = best_cb.predict(X_test)\nauc_cb = cross_val_score(best_cb, X_test, y_test, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_cb = cross_val_score(best_cb, X_test, y_test, scoring = 'accuracy', cv = 10, n_jobs = -1)\nprint('AUC-ROC for CatBoost on test dataset:', sum(auc_cb)\/len(auc_cb))\nprint('Accuracy for CatBoost on test dataset:', sum(acc_cb)\/len(acc_cb))","c9b994c3":"feature_importance(best_cb.feature_importances_, 'Feature importance for CatBoost')\nconfusion_m(best_cb, 'Confusion matrix for CatBoost')","288cb45f":"vc = VotingClassifier(estimators=[('lr', best_lr), ('xgb', best_xgb), ('rf', best_rf), ('cb', best_cb)], voting='soft')\nvc.fit(X_train, y_train)\npredict_vc = vc.predict(X_test)\nauc_vc = cross_val_score(vc, X_test, y_test, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_vc = cross_val_score(vc, X_test, y_test, scoring = 'accuracy', cv = 10, n_jobs = -1)\nprint('AUC-ROC for ensemble models on test dataset:', sum(auc_vc)\/len(auc_vc))\nprint('Accuracy for ensemble models on test dataset:', sum(acc_vc)\/len(acc_vc))","7c8c3d9c":"confusion_m(vc, 'Confusion matrix for VotingClassifier')","9eb7412b":"models = ['logistic_regression', 'random_forest',\n          'xgboost', 'catboost', 'voting']\ndict_values = {'auc_roc': [auc_lr.mean(), auc_rf.mean(),\n                           auc_xgb.mean(), auc_cb.mean(), auc_vc.mean()],\n              'accuracy': [acc_lr.mean(), acc_rf.mean(),\n                            acc_xgb.mean(), acc_cb.mean(), acc_vc.mean()]}\ndf_score = pd.DataFrame(dict_values, index = models, columns = ['auc_roc', 'accuracy'])\ndf_score","1bd69ff9":"## 3. Preprocessing data","bfdae4c8":"## 1. Data analysis","106e8702":"## 5. Result","b0dee9c7":"## 4. Create and fit models","7d96e207":"## 2. Data cleaning and feature engineering"}}