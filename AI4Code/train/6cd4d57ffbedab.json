{"cell_type":{"3f0d7c9b":"code","255f1e57":"code","65b42b44":"code","84a6d0eb":"code","b9604be4":"code","34e95036":"code","c37389a8":"code","b82682a3":"code","606ca651":"code","2b02b207":"code","85e6c7d8":"code","4e1cc1af":"code","643f081a":"code","20a99453":"code","d8455a5c":"code","845e6331":"code","7bd9bc83":"code","e8bc6c29":"code","a07cd0b7":"code","f4f50b09":"code","00951b6e":"code","f1064ed4":"code","9c56e882":"code","be652737":"code","cc454234":"code","4b31a6c8":"code","f906c8dc":"code","ed127d27":"code","c41d11ed":"code","6d3ee1d7":"code","479523ae":"code","d92a78f6":"code","2410f71b":"code","b8d10d54":"code","bb04e46b":"code","39f8c5f0":"code","ead448f3":"code","9523b6bd":"code","53f97552":"code","b35b0cde":"code","4d9c7a30":"code","88307277":"code","dbba0e0e":"code","45157dfe":"code","9573fbf3":"code","b28325f8":"code","c732a071":"code","8d4849e6":"code","8c59dbfc":"code","d4baef8e":"code","d543912d":"code","281b88fb":"code","d13ad118":"code","21fe32b6":"code","efb6db45":"code","50681c96":"code","b53f4221":"code","568ba8f2":"code","023b9e35":"code","f9748652":"code","d124b324":"code","4829493b":"code","9844e8b2":"code","1590edee":"code","d6092a76":"code","ceb552a3":"code","3ee4b7d5":"code","c83afa4e":"markdown","06b56dee":"markdown","6a1faa2a":"markdown","4733d703":"markdown","f7277862":"markdown","cd08a32d":"markdown","8ecb3bf4":"markdown","ac653e7f":"markdown","6a4134fe":"markdown","e86d6175":"markdown","25a0ee14":"markdown","705a8f17":"markdown"},"source":{"3f0d7c9b":"# importing necessary libraries \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xg\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom lightgbm import LGBMRegressor\nimport seaborn as sns","255f1e57":"# importing dataset\ntrain = pd.read_csv('..\/input\/jobathon-sales-sept2021\/TRAIN.csv')\ntest = pd.read_csv('..\/input\/jobathon-sales-sept2021\/TEST_FINAL.csv')\ntrain.head()","65b42b44":"# printing dtypes and null\/not null information\nprint(train.info())\nprint(test.info())","84a6d0eb":"# Converting date column from object to datetime type\ntrain['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])","b9604be4":"# Extracting year, date and day to make new columns as a feature\ntrain['Year'] = train['Date'].dt.year\ntest['Year'] = test['Date'].dt.year\n\ntrain['Month'] = train['Date'].dt.month\ntest['Month'] = test['Date'].dt.month\n\ntrain['Day'] = train['Date'].dt.day\ntest['Day'] = test['Date'].dt.day\n","34e95036":"# Looking at new changes in train data\ntrain.head()","c37389a8":"# Looking at new changes in test data\ntest.head()","b82682a3":"# Confirming dtype changes and looking at new column dtypes in train\ntrain.info()","606ca651":"# In sales, day of the week, starting of the month and quarter of the year matters a lot\n# So I'm creating these new features as it helped in improving my score in the model\n\ntrain['dayofweek'] = train['Date'].dt.dayofweek\ntest['dayofweek'] = test['Date'].dt.dayofweek\n\ntrain['is_month_start'] = train['Date'].dt.is_month_start\ntest['is_month_start'] = test['Date'].dt.is_month_start\n\ntrain['is_quarter_start'] = train['Date'].dt.is_quarter_start\ntest['is_quarter_start'] = test['Date'].dt.is_quarter_start","2b02b207":"# Creating one more feature if it's a weekend or not\ntrain['is_weekend'] = train['dayofweek'].apply(lambda x: 1 if x == 5 or x == 6 else 0)\ntest['is_weekend'] = test['dayofweek'].apply(lambda x: 1 if x == 5 or x == 6 else 0)","85e6c7d8":"# Looking at train data again\ntrain.head()","4e1cc1af":"# Dropping unnecessary columns(especially #order which is not in test data)\ntrain = train.drop(['ID','#Order','Date'], axis=1)\ntest = test.drop(['Date', 'ID'], axis=1)","643f081a":"# Plotting heatmap using correlation\nplt.figure(figsize=(10,10))\nsns.heatmap(train.corr(), annot=True)","20a99453":"# Checking some figures how each store type has sales \ntrain.groupby(['Store_Type'])['Sales'].sum().sort_values(ascending=False)","d8455a5c":"# Now plotting Store type against their sales value\n\nax = sns.barplot(x='Store_Type', y='Sales', data=train)\nax.set(xticklabels=['S3', 'S1', 'S4', 'S2']) ","845e6331":"# Now plotting Store type against their sales value using boxplot\nplt.figure(figsize=(6,6))\nax = sns.boxplot(x='Store_Type', y='Sales', data=train)\nax.set(xticklabels=['S3', 'S1', 'S4', 'S2'])","7bd9bc83":"# Taking insights of sales based on location type\ntrain.groupby(['Location_Type'])['Sales'].sum().sort_values(ascending=False)","e8bc6c29":"# Sales according to the location type\nplt.figure(figsize=(6,6))\nax = sns.boxplot(x='Location_Type', y='Sales', data=train)\nax.set(xticklabels=['L3', 'L1', 'L2', 'L5', 'L4'])","a07cd0b7":"# Taking insights of sales based on store, location and region\ntrain.groupby(['Store_Type', 'Location_Type', 'Region_Code'])['Sales'].count()","f4f50b09":"# Plotting it after previous insights by locationwise\nsns.barplot(x='Store_Type', y='Sales', hue='Location_Type', data=train)","00951b6e":"# Plotting it after previous insights by regionwise\nsns.barplot(x='Store_Type', y='Sales', hue='Region_Code', data=train)","f1064ed4":"# sales according to holiday and discount\ntrain.groupby(['Holiday', 'Discount'])['Sales'].sum().sort_values(ascending=False)","9c56e882":"# barplot for previous insights\nsns.barplot(x='Discount', y='Sales', hue='Holiday', data=train)","be652737":"# Mapping month and quarter start from boolean to 1 and 0\ntrain['is_month_start'] = train['is_month_start'].map({True: 1, False: 0})\ntest['is_month_start'] = test['is_month_start'].map({True: 1, False: 0})\n\ntrain['is_quarter_start'] = train['is_quarter_start'].map({True: 1, False: 0})\ntest['is_quarter_start'] = test['is_quarter_start'].map({True: 1, False: 0})","cc454234":"# filtering all object dtype column to convert them to 1 and 0 using get_dummies\nobj_col = train.loc[:, train.dtypes == object]\ndummy = pd.get_dummies(obj_col, drop_first=True)\ndummy","4b31a6c8":"# doing same for the test data\ntest_obj_col = test.loc[:, test.dtypes == object]\ntest_dummy = pd.get_dummies(test_obj_col, drop_first=True)\ntest_dummy","f906c8dc":"# adding back the dummy data to train\ntrain = pd.concat([train, dummy], axis=1)\ntrain.head()","ed127d27":"# dropping the original columns after dummy creation and addition to original train data\n\ntrain = train.drop(list(obj_col.columns), axis=1)\ntrain.head()","c41d11ed":"# adding back the dummy data to test\n\ntest = pd.concat([test, test_dummy], axis=1)\ntest.head()","6d3ee1d7":"# dropping the original columns after dummy creation and addition to original test data\n\ntest = test.drop(list(test_obj_col.columns), axis=1)\ntest.head()","479523ae":"# Checking the shape for train and test data\ntrain.shape, test.shape","d92a78f6":"# Checking info for both dataset again\nprint(train.info())\nprint(test.info())","2410f71b":"# Splitting the data into X and y\nX = train.drop('Sales', axis=1)\ny = train.Sales","b8d10d54":"# Splitting the data into X_train, X_test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)","bb04e46b":"rfr = RandomForestRegressor(n_estimators=300, max_depth=50, random_state=200, max_leaf_nodes=60,min_samples_leaf=1,max_samples=0.08)","39f8c5f0":"# Fitting the model before scaling to see the prediction\nrfr.fit(X_train, y_train)","ead448f3":"# Checking root mean square error on X_train\nrmse = np.sqrt(mean_squared_error(y_train, rfr.predict(X_train)))\nrmse","9523b6bd":"# Checking root mean square on X_test\nrmse = np.sqrt(mean_squared_error(y_test, rfr.predict(X_test)))\nrmse","53f97552":"# adding constant before passing Xtrain\nX_train_sm = sm.add_constant(X_train)\nLR = sm.OLS(y_train, X_train)\nLR_model = LR.fit()","b35b0cde":"# Printing the summary\nLR_model.summary()\n\n# Rsquare is pretty good","4d9c7a30":"# Checking VIF score so that if any feature score is > 5, we can drop it to improve our model(except constant)\nvif = pd.DataFrame()\nvif['Features'] = X_train_sm.columns\nvif['VIF'] = [variance_inflation_factor(X_train_sm.values, i) for i in range(X_train_sm.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif.sort_values(by='VIF',ascending=False)","88307277":"# Choosing high range columns for scaling\ncol_scale=X_train.loc[:, X_train.dtypes != 'uint8'].drop(['Holiday', 'is_month_start', 'is_quarter_start', 'is_weekend'], axis=1)","dbba0e0e":"# checking the columns after defining it\ncol_scale.columns","45157dfe":"# Fit and transform on col_scale dataframe\nscaler = StandardScaler()\nscaled = scaler.fit_transform(col_scale)\nscaled","9573fbf3":"# replacing with original column and their values\nX_train.loc[:, list(col_scale.columns)] = scaled","b28325f8":"# Checking X_train after scaling\nX_train.head()","c732a071":"# preparing high scale column for test dataset\ncol_scale_test =X_test.loc[:, X_train.dtypes != 'uint8'].drop(['Holiday', 'is_month_start', 'is_quarter_start', 'is_weekend'], axis=1)","8d4849e6":"# replacing with scaled value in original test dataset \nscaled_X_test = scaler.transform(col_scale_test)\nX_test.loc[:,list(col_scale_test.columns)] = scaled_X_test","8c59dbfc":"# Checking test data\nX_test.head()","d4baef8e":"# Defining RandomForest again to see any difference\nrfr = RandomForestRegressor(n_estimators=300, max_depth=50, random_state=200, max_leaf_nodes=60,min_samples_leaf=1,max_samples=0.08)","d543912d":"# fit\nrfr.fit(X_train, y_train)","281b88fb":"rmse = np.sqrt(mean_squared_error(y_train, rfr.predict(X_train)))\nrmse","d13ad118":"rmse = np.sqrt(mean_squared_error(y_test, rfr.predict(X_test)))\nrmse","21fe32b6":"lgbm = LGBMRegressor(colsample_bytree= 0.66, learning_rate= 0.045, max_depth= 10, max_features= 93, \n                         min_impurity_decrease= 0, min_impurity_split= 1e-07, min_samples_leaf= 1, min_data_in_leaf=2,verbose=-1,\n                         min_samples_split= 95, n_estimators= 150, num_leaves= 15, reg_alpha= 1, reg_lambda= 1, \n                         subsample= 0.4,random_state=1,metric='mae',max_iter=4)\nlgbm.fit(X_train, y_train)","efb6db45":"rmse = np.sqrt(mean_squared_error(y_train, lgbm.predict(X_train)))\nrmse","50681c96":"rmse = np.sqrt(mean_squared_error(y_test, lgbm.predict(X_test)))\nrmse","b53f4221":"boost = xg.XGBRegressor(booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.1, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=4,\n             min_child_weight=5,\n             n_estimators=110, n_jobs=8, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\nboost.fit(X_train, y_train)","568ba8f2":"rmse = np.sqrt(mean_squared_error(y_train, boost.predict(X_train)))\nrmse","023b9e35":"rmse = np.sqrt(mean_squared_error(y_test, boost.predict(X_test)))\nrmse","f9748652":"# Scaling final test data\ntest.head()","d124b324":"# Choosing same column of train to scale\ntest_col_scale=test.loc[:, test.dtypes != 'uint8'].drop(['Holiday', 'is_month_start', 'is_quarter_start', 'is_weekend'], axis=1)","4829493b":"# only transform it\nfinal_test = scaler.transform(test_col_scale)\ntest[list(test_col_scale.columns)] = final_test","9844e8b2":"# looking at it after transform\ntest.head()","1590edee":"# final prediction\nfinal_test_pred = lgbm.predict(test)","d6092a76":"# importing test_final data to extract id\ntest_file = pd.read_csv('..\/input\/jobathon-sales-sept2021\/TEST_FINAL.csv')\nid = test_file.ID\nid","ceb552a3":"# creation of submission dataframe\nmy_sub = pd.DataFrame({'ID': id, 'Sales': final_test_pred})\nmy_sub","3ee4b7d5":"# Converting to csv\nmy_sub.to_csv('finalboost8.csv', index=False)","c83afa4e":"# EDA","06b56dee":"# Using xgboost","6a1faa2a":"# Handling categorical data","4733d703":"<h3>Output is still same after scaling. I did this just in case you \nwould like to see what scaling does. It is actually to process the data easily \nwithout getting stuck in local minima<\/h3>","f7277862":"<h3>It's funny. Here xgboost looks good but at the time of submission score, it's score was always worst.<\/h3>","cd08a32d":"# Model creation","8ecb3bf4":"# Using Statsmodel to check variance distribution","ac653e7f":"# Using RandomForestRegressor","6a4134fe":"# Using LightGBM","e86d6175":"# Final submission","25a0ee14":"<h3>That's here how I did not use lightgdm for the jobathon and my score did not improve.\n    It did not struck to me at that time.<\/h3>","705a8f17":"# If You enjoyed this simple explanation of this regression sales problem of jobathon, please upvote it and comment your views. Thank you guys...........!!!!!!!!"}}