{"cell_type":{"6bd8aec2":"code","b95555df":"code","70119a50":"code","cfe87abd":"code","f85cc7f2":"code","0bfb1323":"code","5cf51593":"code","a6fde3dc":"code","052c4bbd":"code","b81d2a7f":"code","5567d44e":"code","473dfc28":"code","806f27c8":"code","b71a72a3":"code","1e4e8646":"code","417af66c":"code","7eb34428":"code","01c3c17b":"code","4b45b521":"code","e240fe9b":"code","d73e354f":"code","3700429b":"code","8f51efeb":"code","d9b70b71":"code","60e0329a":"code","aca49c7b":"code","38ec571c":"code","a913be47":"code","a06e24b5":"code","c9f74b13":"code","14567b87":"code","d1b84b92":"code","52dfd560":"code","b9fbe88b":"code","14caade1":"code","781fc8cf":"code","fa57f710":"code","70638143":"code","3d512d65":"code","8a85b456":"code","4524f450":"code","ea167fb7":"code","011bcf15":"code","6c6c73a0":"code","da18d6f6":"code","56b675ef":"code","1b3dfc46":"code","7909488d":"code","182969d7":"code","d2ebf7bb":"code","5c91fd41":"code","7bd47d0a":"code","937cb7e8":"code","fbb1f739":"code","f3097624":"code","83044ec6":"code","8d665f60":"code","3ee84702":"code","81efa548":"code","8b3613dc":"code","3ea9186e":"code","ba616445":"code","1f403fa5":"code","635b448b":"code","f4cd1acc":"code","606a1d21":"code","eb3d67c8":"markdown","11b9fc93":"markdown","ec4b2776":"markdown","e3f141d6":"markdown","ccb7f10e":"markdown","82eb5128":"markdown","fe1ea981":"markdown","cbd3a2ee":"markdown","fc6941a8":"markdown","dcfb718e":"markdown","76117f58":"markdown","ce17e6ee":"markdown","30233aae":"markdown","1ec23ece":"markdown","b3acc2a7":"markdown","e27ecb31":"markdown","e3352938":"markdown","0ea7bfdc":"markdown","10fba0ff":"markdown","bbcc78db":"markdown","1003c176":"markdown","3b581a59":"markdown","b682655d":"markdown","d8d9f789":"markdown","b6dfa5ba":"markdown","1b56843f":"markdown","bcb5a6f3":"markdown","5e89133e":"markdown","f670372d":"markdown","6c08e9d7":"markdown","7b530f5f":"markdown","48ef1f90":"markdown","f49e934f":"markdown","7e95aad7":"markdown","fc1ef599":"markdown","1c7c8d37":"markdown"},"source":{"6bd8aec2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly as py\n#import chart_stdio.plotly as py\nimport plotly.express as px\nimport cufflinks as cf\n\n%matplotlib inline\n\nfrom plotly.offline import iplot,plot, download_plotlyjs, init_notebook_mode\n\ninit_notebook_mode()\ncf.go_offline()\nimport plotly.graph_objects as go\n","b95555df":"df = pd.read_csv('..\/input\/used-cars-price-prediction\/train-data.csv')","70119a50":"df.head()","cfe87abd":"df.shape","f85cc7f2":"df.describe()","0bfb1323":"df.info()","5cf51593":"df.dtypes","a6fde3dc":"df.isnull().sum()","052c4bbd":"df = df.drop(['New_Price','Unnamed: 0'],axis = 1)","b81d2a7f":"df.dropna(axis = 0, inplace=True)","5567d44e":"df['Engine'] = df['Engine'].str.split().str[0]\ndf['Power'] = df['Power'].str.split().str[0]","473dfc28":"df['Name'] = df['Name'].str.split().str[0]","806f27c8":"#Let's convert km\/kg tito kmpl because most of the data point are kmpl\nCorrect_Mileage = []\n\nfor i in df.Mileage:\n    if str(i).endswith('km\/kg'):\n        i = i[:-6]\n        i = float(i)*1.40\n        Correct_Mileage.append(float(i))\n        \n    elif str(i).endswith('kmpl'):\n        i = i[:-6]\n        Correct_Mileage.append(float(i))","b71a72a3":"df['Mileage'] = Correct_Mileage","1e4e8646":"#Let's clear 'null' values from our dataset if any present\ndf[pd.to_numeric(df.Power, errors = 'coerce').isnull()]","417af66c":"\ndf =df[df.Power != 'null']","7eb34428":"df.head()","01c3c17b":"#First have the Price distribution\nsns.distplot(df['Price'])","4b45b521":"#Let's have the Company frequency\nplt.figure(figsize = [15,10])\nsns.countplot(y =  'Name',data = df)\nplt.title(\"Most frequent Company\")","e240fe9b":"#First let's see how our target variable is dependent on  individual Categorical features\n\nfeatures = df[['Fuel_Type','Transmission','Owner_Type']].columns\nfor i in features:\n    sns.catplot(x = i , y = 'Price',kind = 'bar',data = df)\n    plt.title(\"Price based on \" + i)\n    plt.show()","d73e354f":"sns.countplot(x = 'Transmission', hue ='Owner_Type', data = df )\nplt.title('Counting transamission based on owner type')","3700429b":"sns.countplot(x = 'Transmission', hue ='Fuel_Type', data = df )\nplt.title('Counting transamission based on Fuel type')","8f51efeb":"#Let's have the relationship between kilometer driven and Owner type\nsns.catplot(x = 'Owner_Type', y = 'Kilometers_Driven',hue = 'Fuel_Type',kind = 'bar', data = df)\nplt.title('kilometer driven')","d9b70b71":"plt.figure(figsize = [15,15])\nsns.catplot(y = 'Price', x = 'Year',hue = 'Location',kind = 'bar', data = df)\nplt.title('Price by year and Location')","60e0329a":"#Let's have a new featuresa from Year columns.\ndf['Current_Year'] = 2021\ndf['No_Year'] = df['Current_Year'] - df['Year']","aca49c7b":"df.head()","38ec571c":"df = df.drop(['Year','Current_Year'],axis = 1)","a913be47":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","a06e24b5":"df['Name'] = le.fit_transform(df['Name'])\ndf['Location'] = le.fit_transform(df['Location'])\ndf['Fuel_Type'] = le.fit_transform(df['Fuel_Type'])\ndf['Transmission'] = le.fit_transform(df['Transmission'])\ndf['Owner_Type'] = le.fit_transform(df['Owner_Type'])","c9f74b13":"df.head()","14567b87":"X = df.drop(['Price'], axis = 1)\ny = df['Price']","d1b84b92":"#Let's have the feature importence.\nfrom sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","52dfd560":"print(model.feature_importances_)","b9fbe88b":"feat_importance = pd.Series(model.feature_importances_,index = X.columns)\nfeat_importance.nlargest(11).plot(kind = 'barh')\nplt.show()","14caade1":"from sklearn.model_selection import train_test_split","781fc8cf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","fa57f710":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n\n","70638143":"y_pred = lr.predict(X_test)","3d512d65":"lr.score(X_test,y_test)","8a85b456":"y_pred = lr.predict(X_test)\nmse = mean_squared_error(y_pred, y_test)\nprint('Mean Squared Error on test data: ', + mse)","4524f450":"#Putting together the coefficients and their corresponding variable name\nlr_coef = pd.DataFrame()\nlr_coef['Columns'] = X_train.columns\nlr_coef['Coefficient'] = pd.Series(lr.coef_)\nprint(lr_coef)","ea167fb7":"fig, ax = plt.subplots(figsize = (20,10))\nax.bar(lr_coef['Columns'], lr_coef['Coefficient'])\n\nax.spines['bottom'].set_position('zero')\n#plt.style.use('ggplot')\nplt.show()","011bcf15":"#Import ridge regression from sklearn.\nfrom sklearn.linear_model import Ridge\n\nridgeR = Ridge(alpha = 1)\nridgeR.fit(X_train,y_train)\ny_pred = ridgeR.predict(X_test)\nmse = np.mean((y_pred - y_test)**2)\nprint('Mean Squared Error on test data: ', + mse)","6c6c73a0":"#Putting together the coefficients and their corresponding variable name\nridgeR_coef = pd.DataFrame()\nridgeR_coef['Columns'] = X_train.columns\nridgeR_coef['Coefficient'] = pd.Series(ridgeR.coef_)\nprint(ridgeR_coef)","da18d6f6":"fig, ax = plt.subplots(figsize = (20,10))\nax.bar(ridgeR_coef['Columns'], ridgeR_coef['Coefficient'])\n\nax.spines['bottom'].set_position('zero')\n#plt.style.use('ggplot')\nplt.show()","56b675ef":"#Import ridge regression from sklearn.\nfrom sklearn.linear_model import Lasso\n\nLassoR = Lasso(alpha = 1)\nLassoR.fit(X_train,y_train)\ny_pred = LassoR.predict(X_test)\nmse = np.mean((y_pred - y_test)**2)\nprint('Mean Squared Error on test data: ', + mse)","1b3dfc46":"#Putting together the coefficients and their corresponding variable name\nLassoR_coef = pd.DataFrame()\nLassoR_coef['Columns'] = X_train.columns\nLassoR_coef['Coefficient'] = pd.Series(LassoR.coef_)\nprint(LassoR_coef)","7909488d":"fig, ax = plt.subplots(figsize = (20,10))\nax.bar(LassoR_coef['Columns'], LassoR_coef['Coefficient'])\n\nax.spines['bottom'].set_position('zero')\n#plt.style.use('ggplot')\nplt.show()","182969d7":"from sklearn.neighbors import KNeighborsRegressor","d2ebf7bb":"knn = KNeighborsRegressor(n_neighbors=1)\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)","5c91fd41":"from sklearn.metrics import mean_squared_error\nprint('Mean Squared Error is', mean_squared_error(y_test, y_pred))","7bd47d0a":"Error = []\n\nfor i in range(1, 51):\n    knn = KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    Error.append(mean_squared_error(y_test,y_pred))\n","937cb7e8":"plt.figure(figsize=(10,5))\nplt.plot(range(1,51), Error, color = 'blue', marker = 'o', markerfacecolor = 'red', markersize = 10)\n\nplt.title('Error rate vs K. value')\nplt.xlabel('k')\nplt.ylabel('Error Rate')","fbb1f739":"knn = KNeighborsRegressor(n_neighbors=10)\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)","f3097624":"print('Mean Squared Error is', mean_squared_error(y_test, y_pred))","83044ec6":"from sklearn.ensemble import AdaBoostRegressor\nAdaR = AdaBoostRegressor(base_estimator = None, n_estimators= 50, learning_rate= 1, random_state=2)\nAdaR.fit(X_train, y_train)\ny_pred = AdaR.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(mse)","8d665f60":"Error = []\n\nfor i in range(1, 101):\n    AdaR = AdaBoostRegressor(base_estimator = None, n_estimators= i, learning_rate= 1, random_state=2)\n    AdaR.fit(X_train, y_train)\n    y_pred = AdaR.predict(X_test)\n    Error.append(mean_squared_error(y_test,y_pred))","3ee84702":"plt.figure(figsize=(10,5))\nplt.plot(range(1,101), Error, color = 'blue', marker = 'o', markerfacecolor = 'red', markersize = 10)\n\nplt.title('Error rate vs No. of estimators')\nplt.xlabel('n_estimators')\nplt.ylabel('Error Rate')","81efa548":"AdaR = AdaBoostRegressor(base_estimator = None, n_estimators= 15, learning_rate= 1, random_state=2)\nAdaR.fit(X_train, y_train)\ny_pred = AdaR.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(mse)","8b3613dc":"from sklearn.ensemble import GradientBoostingRegressor\ngredR = GradientBoostingRegressor(max_depth = 2, n_estimators = 4, learning_rate = 1.0)","3ea9186e":"gredR.fit(X_train, y_train)\ny_pred = gredR.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(mse)","ba616445":"from sklearn.model_selection import GridSearchCV\nLR = {'learning_rate': [0.15, .1, .12, 0.05], 'n_estimators' : [50, 100, 150, 200, 250]}\n\ntuning = GridSearchCV(estimator=GradientBoostingRegressor(),\n                           param_grid=LR, scoring ='r2')\ntuning.fit(X_train, y_train)\n\n","1f403fa5":"tuning.best_params_, tuning.best_score_","635b448b":"y_pred = tuning.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(mse)","f4cd1acc":"import time\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\n      ","606a1d21":"start_time=time.time()\nmodel = xgb.XGBRegressor()\n\nparam_grid = {\n        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        'min_child_weight': [0.0001, 0.5, 0.001],\n        'gamma': [0.0,40.0,0.005],\n        'learning_rate': [0.0005,0.3,0.0005],\n        'subsample': [0.01,1.0,0.01],\n        'colsample_bylevel': [0.1,1.0,0.01],\n        'n_estimators' : [100, 200, 500],\n}\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=10)\nran_search = RandomizedSearchCV(model, param_grid, scoring=\"accuracy\",cv=kfold)\nran_result = ran_search.fit(X_train,y_train)\n\n","eb3d67c8":"Here we are seeing that near 15 error rate is small. So let's try with this value.","11b9fc93":"Xgboost:","ec4b2776":"\nFirstly, we will choose the number of neighbors, so we will choose the k=5.\n\nNext, we will calculate the Euclidean distance between the data points. The Euclidean distance is the distance between two points, which we have already studied in geometry. It can be calculated as:\n\n![image.png](attachment:image.png)\n\n","e3f141d6":"# How does K-NN work?\n\nThe K-NN working can be explained on the basis of the below algorithm:\n\nStep-1: Select the number K of the neighbors\n\nStep-2: Calculate the Euclidean distance of K number of neighbors\n\nStep-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n\nStep-4: Among these k neighbors, count the number of the data points in each category.\n\nStep-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n\nStep-6:In the time of Regression it takes the average of the output of the k nearest neighbors as per the calculated Euclidean distance and assign it to the output variable of new input data points.\n\nStep-6: Our model is ready.\n\nSuppose we have a new data point and we need to put it in the required category. Consider the below image:\n\n![image.png](attachment:image.png)","ccb7f10e":"![image.png](attachment:image.png)","82eb5128":"# Boasting","fe1ea981":"Let's have the mean squared error.","cbd3a2ee":"What is Boosting in Machine Learning?\n\nTraditionally, building a Machine Learning application consisted on taking a single learner, like a Logistic Regressor, a Decision Tree, Support Vector Machine, or an Artificial Neural Network, feeding it data, and teaching it to perform a certain task through this data.\nThen ensemble methods were born, which involve using many learners to enhance the performance of any single one of them individually. These methods can be described as techniques that use a group of weak learners (those who on average achieve only slightly better results than a random model) together, in order to create a stronger, aggregated one.\n\nGenerally, ensemble methods are built by grouping variants of individual Decision Trees.\n\nBoosting models fall inside this family of ensemble methods.\n![image.png](attachment:image.png)\n","fc6941a8":"Work in progress.","dcfb718e":"Let's have the only Company name from the name column.","76117f58":"# Data analysing","ce17e6ee":"Short for Adaptive Boosting, AdaBoost works by the exact process described before of training sequentially, predicting, and updating the weights of the miss-classified samples and of the corresponding weak models.\n![image.png](attachment:image.png)\nIt is mostly used with Decision Tree Stumps: decision trees with just a root node and two leave nodes, where only 1 feature of the data is evaluated. As we can see, by taking into account only 1 feature of our data to make predictions, each stump is a very very weak model. However, by combining many of them, a very robust and accurate ensemble model can be built.\n\n\n![image.png](attachment:image.png)","30233aae":"# Data understanding:","1ec23ece":"# Data Cleaning","b3acc2a7":"# Solution\n \nThe solution is divided into the following sections:\n\nData understanding\n\nData cleaning\n\nData preparation\n\nModel building and evaluation\n\nOne thing to mentioned is there is no hard and fast rule that we have to blindly follow those, may have overlap because of the purpose.\n","e27ecb31":"# LinearRegression","e3352938":"here we are seeing that a major part of New_price features is null,that's why we can safely delete this features.","0ea7bfdc":"\n\n\n\n\n\n\n\n\n\nNow data is looks like much cleaner.","10fba0ff":"# Lasso Regression","bbcc78db":"# Let's find perfect k value.","1003c176":"# Problem Statement and Objective:\nwe are given with a dataset of car price prediction.our objective is to find insights from this dataset and buuild a model which can predict car price accurately.\n\nWhich variables are significant in predicting the price of a car\n\nHow well those variables describe the price of a car","3b581a59":"How does it works?\n\nFor choosing the right distribution, here are the following steps:\n\nStep 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n\nStep 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n\nStep 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n\nFinally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classi\ufb01ed or have higher errors by preceding weak rules.","b682655d":"Here we are seeing that near point 10 error is less and stable that's why we are going to chose 10 as a k value.","d8d9f789":"#  Let's tune the hyperparameters","b6dfa5ba":"Here we are seeing that Mileage,Engine and Power are object type and specific units are also presented in the dataset,that's why before fitting into model we have to perform some cleaning. ","1b56843f":"Let's have the relationship pairwise.","bcb5a6f3":"Here we are seeing that 103 rows in Power column are containing null values.As we have almost 7000 datapoints, let's remove those.","5e89133e":"1.K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n\n2.K-NN algorithm assumes the similarity between the new case\/data and available cases and put the new case into the category that is most similar to the available categories.\n\n3.K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n\n4.K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n\n5.K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n\n6.It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n\n7.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.\n\nExample: Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category.\n\n![image.png](attachment:image.png)\n\n\n","f670372d":"#  Feature Engineering","6c08e9d7":"By calculating the Euclidean distance we got the nearest neighbors, as three nearest neighbors in category A and two nearest neighbors in category B. Consider the below image:\n\n\n![image.png](attachment:image.png)\n","7b530f5f":"# Ridge Regression","48ef1f90":"# K-Nearest Neighbor(KNN)","f49e934f":"# Why do we need a K-NN Algorithm?\n\nSuppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset. Consider the below diagram:\n\n![image.png](attachment:image.png)\n","7e95aad7":"# Ada Boasting:","fc1ef599":"# Gradient Boasting Regressor","1c7c8d37":"The column\u201cPrice\u201d is the target variable and rest of the columns are independent variables.\n\nThe independent variables are again divided into Categorical and Numerical variables.\n\nNumerical variables: [ Year,Kilometers_Driven]\n\nCategorical variables: [Fuel_Type,Transmission,Owner_Type]\n    \nThere are also some others features but before doing the analysisa we have clean those."}}