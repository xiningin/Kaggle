{"cell_type":{"48611836":"code","709e883f":"code","d4dda743":"code","bae2ea78":"code","6aa9865a":"code","47defe2e":"code","8bfb7908":"code","eb44ead7":"code","805e00e5":"code","c528581e":"code","88742c27":"code","f1b20167":"code","13287f77":"code","3d7713f5":"code","823d2d84":"code","2eacf4dc":"code","c6c670a0":"code","49e152db":"code","475d0861":"code","cc0c1071":"code","813bfbdf":"code","871ce54d":"code","885601cc":"markdown"},"source":{"48611836":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)","709e883f":"!pip install xlrd","d4dda743":"!pip install openpyxl","bae2ea78":"df0 = pd.read_excel('..\/input\/commonlit-dataset\/0_replace.xlsx', engine='openpyxl')","6aa9865a":"def text2(data):\n    #data['external_text'] = data['external_text'].apply(lambda x: x.replace('\\n',''))\n    out = '\\n'.join(data.tolist())\n    return out","47defe2e":"import numpy as np","8bfb7908":"df0 = df0.external_text.replace(np.nan, '', regex=True)","eb44ead7":"df4 = pd.read_csv('..\/input\/commonlit-dataset\/4_children_stories.csv', encoding='latin1')","805e00e5":"#train_data = pd.read_csv('..\/input\/commonlit-dataset\/train.csv')\n#test_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ndf1 = pd.read_csv('..\/input\/commonlit-dataset\/1_train.csv')\ndf2 = pd.read_csv('..\/input\/commonlit-dataset\/2_test.csv')\ndf3 = pd.read_csv('..\/input\/commonlit-dataset\/3_children_books.csv')\ndf5 = pd.read_csv('..\/input\/commonlit-dataset\/5_enwiki.csv')\ndf6 = pd.read_csv('..\/input\/commonlit-dataset\/6_simwiki.csv')\ndf7 = pd.read_csv('..\/input\/commonlit-dataset\/7_weebit.csv')\n\n#data = pd.concat([train_data,test_data])","c528581e":"def text(data):\n    data['excerpt'] = data['excerpt'].apply(lambda x: x.replace('\\n',''))\n    out = '\\n'.join(data.excerpt.tolist())\n    return out","88742c27":"txt0 = text2(df0)\ntxt1 = text(df1)\ntxt2 = text(df2)\ntxt3 = text(df3)\ntxt4 = text(df4)\ntxt5 = text(df5)\ntxt6 = text(df6)\ntxt7 = text(df7)\n# txt8 = text(df8)","f1b20167":"txt = txt0 + txt1 + txt2 + txt3 + txt4 + txt5 + txt6 + txt7","13287f77":"#data.head()","3d7713f5":"#data['url_legal'] = data['url_legal'].fillna('')\n#data['license'] = data['license'].fillna('')","823d2d84":"#data['excerpt'] = data['excerpt'].apply(lambda x: x.replace('\\n','')) # replace new line with nothing","2eacf4dc":"#data['all'] = data['url_legal'] + ', ' + data['license'] + ', '+ data['excerpt']","c6c670a0":"#data['all2'] = data['all'].values","49e152db":"#text  = '\\n'.join(data.all2.tolist()) # add \\n in between each ","475d0861":"with open('text.txt','w') as f:\n    f.write(txt)","cc0c1071":"model_name = 'roberta-base'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('.\/clrp_roberta_base');","813bfbdf":"train_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention train text file here\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention valid text file here\n    block_size=256)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\".\/clrp_roberta_base_chk\", #select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=200,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","871ce54d":"trainer.train()\ntrainer.save_model(f'.\/clrp_roberta_base')","885601cc":"This notebooks shows how to pretrain any language model easily\n\n\n1. Pretrain Roberta Model: this notebook\n2. Finetune Roberta Model: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune<br\/>\n   Finetune Roberta Model [TPU]: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-inference\n4. Roberta + SVM: https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm\n"}}