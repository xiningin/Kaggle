{"cell_type":{"31708cd2":"code","b0fb935a":"code","855e1ba7":"code","98386854":"code","f148e181":"code","29b525d2":"code","93aa057e":"code","9fbf27a5":"code","fd29fe28":"code","cac50202":"code","6f506c7e":"code","145a38c3":"code","1865e6b9":"code","fcf5463e":"code","c80bf091":"code","d0f07486":"code","d7a080d8":"code","adfab5a4":"code","6c452777":"code","18d13769":"code","5d375e96":"code","43161ade":"code","c10cca70":"code","ca3b275c":"code","7edb35be":"code","12bfac6e":"code","6a499ca1":"code","add39afd":"code","1d6fab0c":"code","71ae2704":"code","b3cc7ebd":"code","00b4ad41":"markdown","ba762699":"markdown","2db0148a":"markdown","0a079041":"markdown","98f48eb7":"markdown","97749cb8":"markdown","f6092ed9":"markdown","027155c2":"markdown","bd85ad94":"markdown","131e6519":"markdown","dd5f18cb":"markdown","1530012f":"markdown","f7e50956":"markdown","406ff866":"markdown","736106a8":"markdown","53b0ed5f":"markdown","f50fd118":"markdown"},"source":{"31708cd2":"import warnings \nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd","b0fb935a":"# reading data\ndata=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata['diagnosis'] = data['diagnosis'].astype('category').cat.codes# encoding labels\ndata.drop(['id','Unnamed: 32'],axis=1,inplace=True)","855e1ba7":"def load_classification_data(df,target_col,test_size):\n    torch_data = np.array(df.drop(target_col,axis=1))\n    torch_labels =np.array(df[target_col])\n    data = np.hstack([torch_data, torch_labels.reshape(-1,1)])\n    gen_names = [f\"feature_{i}\" for i in range(data.shape[-1])]\n    col_names = gen_names\n    col_names[-1] = \"target\"\n    data = pd.DataFrame(data, columns=col_names)\n    cat_col_names = [x for x in gen_names[:-1] if len(data[x].unique())<10]\n    num_col_names = [x for x in gen_names[:-1] if x not in [target_col]+cat_col_names]\n    test_idx = data.sample(int(test_size * len(data)), random_state=42).index\n    test = data[data.index.isin(test_idx)]\n    train = data[~data.index.isin(test_idx)]\n    return (train, test, [\"target\"],cat_col_names,num_col_names)","98386854":"train, test, target_col, cat_col_names, num_col_names= load_classification_data(data,'diagnosis',0.2)","f148e181":"! pip install pytorch_tabular[all]","29b525d2":"from pytorch_tabular import TabularModel\nfrom pytorch_tabular.models import CategoryEmbeddingModelConfig, NodeConfig, TabNetModelConfig\nfrom pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig\nfrom pytorch_tabular.categorical_encoders import CategoricalEmbeddingTransformer","93aa057e":"data_config = DataConfig(\n    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n    continuous_cols=num_col_names,\n    categorical_cols=cat_col_names,\n    continuous_feature_transform=\"quantile_normal\",\n    normalize_continuous_features=True\n)\ntrainer_config = TrainerConfig(\n    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n    batch_size=32,\n    max_epochs=100,\n    gpus=1, #index of the GPU to use. 0, means CPU\n)\noptimizer_config = OptimizerConfig()\nmodel_config = CategoryEmbeddingModelConfig(\n    task=\"classification\",\n    layers=\"4096-4096-512\",  # Number of nodes in each layer\n    activation=\"LeakyReLU\", # Activation between each layers\n    learning_rate = 1e-3,\n    metrics=[\"accuracy\"]\n)\ntabular_model = TabularModel(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config,\n)","9fbf27a5":"tabular_model.fit(train=train, test=test)","fd29fe28":"tabular_model.evaluate(test)","cac50202":"tabular_model.save_model('.\/saved_models\/tab_model')","6f506c7e":"loaded_model = TabularModel.load_from_checkpoint('.\/saved_models\/tab_model\/')","145a38c3":"# reading data\ndata2=pd.read_csv('..\/input\/iris\/Iris.csv')\ndata2.drop(['Id'],axis=1,inplace=True)\ndata2['Species'] = data2['Species'].astype('category').cat.codes# encoding labels","1865e6b9":"train, test, target_col, cat_col_names, num_col_names  = load_classification_data(data2,'Species',0.2)","fcf5463e":"data_config = DataConfig(\n    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n    continuous_cols=num_col_names,\n    categorical_cols=cat_col_names,\n    continuous_feature_transform=\"quantile_normal\",\n    normalize_continuous_features=True\n)\ntrainer_config = TrainerConfig(\n    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n    batch_size=16,\n    max_epochs=100,\n    auto_select_gpus = True,\n    gpus=1, #index of the GPU to use. 0, means CPU\n)\noptimizer_config = OptimizerConfig()\nmodel_config = CategoryEmbeddingModelConfig(\n    task=\"classification\",\n    layers=\"4096-4096-512\",  # Number of nodes in each layer\n    activation=\"LeakyReLU\", # Activation between each layers\n    learning_rate = 1e-3,\n    metrics=[\"accuracy\"]\n)\ntabular_model2 = TabularModel(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config,\n)","c80bf091":"tabular_model2.fit(train=train, test=test)","d0f07486":"tabular_model2.evaluate(test)","d7a080d8":"# reading data\ndata3=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n# data3.drop('id',axis=1,inplace=True)\ndata3['quality'] = data3['Outcome'].astype('category').cat.codes# encoding labels","adfab5a4":"train, test, target_col, cat_col_names, num_col_names= load_classification_data(data3,'quality',0.2)","6c452777":"data_config = DataConfig(\n    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n    continuous_cols=num_col_names,\n    categorical_cols=cat_col_names,\n    continuous_feature_transform=\"quantile_normal\",\n    normalize_continuous_features=True\n)\ntrainer_config = TrainerConfig(\n    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n    batch_size=32,\n    max_epochs=100,\n    gpus=1, #index of the GPU to use. 0, means CPU\n)\noptimizer_config = OptimizerConfig()\nmodel_config = CategoryEmbeddingModelConfig(\n    task=\"classification\",\n    layers=\"1024-1024-32\",  # Number of nodes in each layer\n    activation=\"LeakyReLU\", # Activation between each layers\n    learning_rate = 1e-3,\n    metrics=[\"accuracy\"]\n)\ntabular_model3 = TabularModel(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config,\n)","18d13769":"tabular_model3.fit(train=train, test=test)","5d375e96":"tabular_model3.evaluate(test)","43161ade":"# reading data\ndata4=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\n# data3.drop('id',axis=1,inplace=True)\ndata4['target'] = data4['target'].astype('category').cat.codes# encoding labels","c10cca70":"train, test, target_col, cat_col_names, num_col_names= load_classification_data(data4,'target',0.2)","ca3b275c":"data_config = DataConfig(\n    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n    continuous_cols=num_col_names,\n    categorical_cols=cat_col_names,\n    continuous_feature_transform=\"quantile_normal\",\n    normalize_continuous_features=True\n)\ntrainer_config = TrainerConfig(\n    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n    batch_size=32,\n    max_epochs=100,\n    gpus=1, #index of the GPU to use. 0, means CPU\n)\noptimizer_config = OptimizerConfig()\nmodel_config = CategoryEmbeddingModelConfig(\n    task=\"classification\",\n    layers=\"4096-4096-512\",  # Number of nodes in each layer\n    activation=\"LeakyReLU\", # Activation between each layers\n    learning_rate = 1e-3,\n    metrics=[\"accuracy\"]\n)\ntabular_model4 = TabularModel(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config,\n)","7edb35be":"tabular_model4.fit(train=train, test=test)","12bfac6e":"tabular_model4.evaluate(test)","6a499ca1":"# reading data\ndata5=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndata5['quality'] = data5['quality'].astype('category').cat.codes# encoding labels","add39afd":"train, test, target_col, cat_col_names, num_col_names= load_classification_data(data5,'quality',0.2)","1d6fab0c":"data_config = DataConfig(\n    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n    continuous_cols=num_col_names,\n    categorical_cols=cat_col_names,\n    continuous_feature_transform=\"quantile_normal\",\n    normalize_continuous_features=True\n)\ntrainer_config = TrainerConfig(\n    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n    batch_size=32,\n    max_epochs=100,\n    gpus=1, #index of the GPU to use. 0, means CPU\n)\noptimizer_config = OptimizerConfig()\nmodel_config = CategoryEmbeddingModelConfig(\n    task=\"classification\",\n    layers=\"4096-4096-512\",  # Number of nodes in each layer\n    activation=\"LeakyReLU\", # Activation between each layers\n    learning_rate = 1e-3,\n    metrics=[\"accuracy\"]\n)\ntabular_model5 = TabularModel(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config,\n)","71ae2704":"tabular_model5.fit(train=train, test=test)","b3cc7ebd":"tabular_model5.evaluate(test)","00b4ad41":"# \ud83c\udf77 winequality-red dataset","ba762699":"# Step 5: Saving the Model","2db0148a":"<strong><span style=\"color:crimson\"><span style=\"font-size:150%\">If you like my work, please don't forget leave an upvote!<\/span><\/span><\/strong>\n\n<strong><span style=\"color: seagreen\"><span style=\"font-size:150%\"> If you don't, atleast leave a comment on what should I do to improve it!<\/span><\/span><\/strong>","0a079041":"# Step2: Setting up the Configs:","98f48eb7":"# Step3 & Step 4: Training and Evaluating ","97749cb8":"<p style=\"font-size:150%\">\n        As the name suggest <strong>PyTorch Tabular<\/strong> is a framework based on <a href=\"https:\/\/pytorch.org\/\">pytorch<\/a> and <a href=\"https:\/\/www.pytorchlightning.ai\/\">pytorch lightning<\/a> which works on Tabular Data.<br>\n        Rather than doing everything from scratch, it make out job easy and we can do everything in few simple steps.\n    <\/p>","f6092ed9":"# \ud83c\udf39 iris-flower dataset","027155c2":"# Testing on other datasets","bd85ad94":"# <span style= \"color:crimson\"><u>Going Deep!!!<\/u><\/span>\n<img src = \"https:\/\/i.pinimg.com\/originals\/1a\/10\/6e\/1a106e5fa6cb78e4a89bff36fccc3a02.png\">\n\n\n<p style = \"font-size:150%\"><span style=\"color:blue\">\n    We all know, when we talk about <strong>Machine Learning<\/strong> boosting methods, they kicks the asses of every other ML-Algos when it comes to tabular data\n    <br>\n    Although, deeplearning has proved to be unreasonalby more effective in fields like Vision, Speech or Language, it haven't shown much effectiveness when it comes to     tabular data. <\/span>\n    \n<br>\n<span style= \"color:crimson\"><span style=\"font-size:150%\"><u>Why Go Deeper?<\/u><\/span><\/span>\n    \n<p style=\"font-size:150%\"><span style=\"color:blue\">Today, we can train machines to detect objects in images, extract meaning from text, stop spam emails, drive cars, discover new drug candidates, and beat top players in Chess, Go, and countless other games and most of the credit goes to deep-learning.\n<br>\nMachine doesn't understands the science\/theory, all it does is generalises the ideas by visiting a lot of data, so lacking a perfect theory we have to rely on intutions. <strong> Deeper Neural Network because it generalises more and overfits less<\/strong>. <br>\nSo, even if part of the face is hidden, the network will still pick up a signal from the remaining input, and therefore generalize better. It's a good intuition, and it appears to be what is actually happening. Experiments confirm that deep neural networks outperform shallow ones on common image as well as text tasks.\n<\/span>\n<\/p>","131e6519":"<table><tr><td><img src =\"https:\/\/pytorch-tabular.readthedocs.io\/en\/latest\/imgs\/pytorch_tabular_logo.png\"><\/td>\n\n<td><span style= \"color:black\"><span style=\"font-size:350%\">The <br>Deep Learning Framework<br>for Tabular Data<\/span><\/span><\/td><\/tr><\/table>\n\n\n ","dd5f18cb":"# \u2764 heart-disease-uci dataset","1530012f":"# Conclusion:\nThe module worked, almost ok. <br>\nThere is still a lot of research going on in this field, and I don't want to make a prejudice that it won't get any better.<br>\nPS: I tried it on [tabular-playground-series-jun-2021](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jun-2021\/overview) dataset <br>and it worked so really well and yes it worked better than Classical Algorithms without any preprocessing.","f7e50956":"**Sorry in advance for a lot of printing","406ff866":"# <span style=\"color:crimson\"><u>In this tutorial, we'll learn to implement pytorch-tabular in <strong>5-Simple-Steps<\/strong>:<\/u><\/span>\n### \n<span style=\"font-size:150%\">\nStep 1: Installation<br>\nStep 2: Setting up the Configs<br>\nStep 3: Initializing the Model and Training<br>\nStep 4: Evaluating the Model on unseen data<br>\nStep 5: Saving the Model\n<\/span>","736106a8":"# \ud83d\udc68\u200d\ud83d\udd2c pima-indian-diabetes dataset","53b0ed5f":"# Importing Libraries\nand simple data pre-processing formality for pytorch","f50fd118":"# Step 1: Installation\n***it might depricate some libraries but that will do fine*"}}