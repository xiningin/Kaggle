{"cell_type":{"d09617cb":"code","422167b0":"code","f61bc71f":"code","f35c17bb":"code","b215e8d8":"code","31de0fcb":"code","5b8be9d2":"code","4eaffbce":"code","6b83d183":"code","4503ae02":"code","fad8656d":"code","97ca5e21":"code","56cfc42c":"code","2eb03eb5":"code","09bd6a93":"code","fdf8a8f4":"code","04f7dcf6":"code","0a751d94":"code","09952cb5":"code","660af11b":"code","af787432":"code","04efd4df":"code","91c4999d":"code","4e9e04ec":"code","94ee92fd":"code","ea1a0c1a":"code","dd21aae9":"code","e71b80bc":"code","6fb255b8":"code","93761843":"code","66352e5f":"code","43c97610":"code","5b1992ed":"code","77e5d084":"code","93447983":"code","bffb0815":"code","e1539494":"code","1dcb5ff3":"code","f161030d":"code","8ddeaecd":"code","6ac721c8":"code","bffe6d7a":"markdown","c90ee943":"markdown","8df78e64":"markdown","e37ee9ea":"markdown","b64fa968":"markdown","5e9436b6":"markdown","8396a075":"markdown","7c7597bb":"markdown","9b00e636":"markdown","0d73d89b":"markdown","1760fd64":"markdown","b94b6f69":"markdown","bace85c1":"markdown","2e3f9fef":"markdown","4670f36c":"markdown","19aa501a":"markdown","373d4802":"markdown","a2273963":"markdown","221131a0":"markdown","9495d5c3":"markdown","34260b53":"markdown","21f51b56":"markdown","6c659ff8":"markdown"},"source":{"d09617cb":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport string as s\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n%matplotlib inline\nfrom sklearn.feature_extraction.text  import TfidfVectorizer\nfrom sklearn.metrics  import f1_score, accuracy_score, multilabel_confusion_matrix, confusion_matrix, recall_score, precision_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport sklearn\nfrom keras.utils import to_categorical\nimport itertools","422167b0":"train_df = pd.read_json(\"..\/input\/github-bugs-prediction-challenge-machine-hack\/embold_train.json\").reset_index(drop=True)\ntrain_df.head()","f61bc71f":"test_df = pd.read_json(\"..\/input\/github-bugs-prediction-challenge-machine-hack\/embold_test.json\").reset_index(drop=True)\ntest_df.head()","f35c17bb":"train_ex_df = pd.read_json(\"..\/input\/github-bugs-prediction-challenge-machine-hack\/embold_train_extra.json\")\ntrain_ex_df.head()","b215e8d8":"# Mapping news_id with category_id\nfinal_df = pd.concat([train_df, train_ex_df], ignore_index=True)\nfinal_df = train_df\nfinal_df.head()","31de0fcb":"# Printing Sample Title\nfinal_df.iloc[7][0]","5b8be9d2":"# Printing Sample Body\nfinal_df.iloc[7][1]","4eaffbce":"# Checking for missing snippets\/titles\/descriptions\nfinal_df.info()","6b83d183":"# Check for duplicates\nfinal_df.drop_duplicates(keep='first').count()","4503ae02":"categories = ['Bug','Feature','Question']","fad8656d":"# Converting each of title and body into lower case.\nfinal_df['title'] = final_df['title'].apply(lambda title: str(title).lower())\nfinal_df['body'] = final_df['body'].apply(lambda body: str(body).lower())\ntest_df['title'] = test_df['title'].apply(lambda title: str(title).lower())\ntest_df['body'] = test_df['body'].apply(lambda body: str(body).lower())","97ca5e21":"#calculating the length of title and body\nfinal_df['title_len'] = final_df['title'].apply(lambda x: len(str(x).split()))\nfinal_df['body_len'] = final_df['body'].apply(lambda x: len(str(x).split()))","56cfc42c":"final_df.describe()","2eb03eb5":"def fx(x):\n    return x['title'] + \" \" + x['body']   \nfinal_df['text']=final_df.apply(lambda x : fx(x),axis=1)\ntest_df['text']=test_df.apply(lambda x : fx(x),axis=1)","09bd6a93":"final_df.head()","fdf8a8f4":"def tokenization(text):\n    lst=text.split()\n    return lst","04f7dcf6":"def remove_new_lines(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.replace(r'\\n', ' ').replace(r'\\r', ' ').replace(r'\\u', ' ')\n        new_lst.append(i.strip())\n    return new_lst","0a751d94":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for  j in s.punctuation:\n            i=i.replace(j,' ')\n        new_lst.append(i.strip())\n    return new_lst","09952cb5":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n    for i in  lst:\n        for j in  s.digits:\n            i=i.replace(j,' ')\n        nodig_lst.append(i.strip())\n    for i in  nodig_lst:\n        if  i!='':\n            new_lst.append(i.strip())\n    return new_lst","660af11b":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i.strip())\n    return new_lst","af787432":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatization(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i.strip())\n    return new_lst","04efd4df":"def remove_urls(text):\n    return re.sub(r'http\\S+', ' ', text)","91c4999d":"def split_words(text):\n    return ' '.join(text).split()","4e9e04ec":"def remove_single_chars(lst):\n    new_lst=[]\n    for i in lst:\n        if len(i)>1:\n            new_lst.append(i.strip())\n    return new_lst","94ee92fd":"# Cleaning Text\ndef denoise_text(text):\n    text = remove_urls(text)\n    text = tokenization(text)\n    text = remove_new_lines(text)\n    text = remove_punctuations(text)\n    text = remove_numbers(text)\n    text = remove_stopwords(text)\n    text = split_words(text)\n    text = remove_single_chars(text)\n    text = lemmatization(text)\n    return text\n\nfinal_df['text'] = final_df['text'].apply(lambda x: denoise_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x: denoise_text(x))","ea1a0c1a":"# Word Corpus\ndef get_corpus(text):\n    words = []\n    for i in text:\n        for j in i:\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(final_df.text)\ncorpus[:5]","dd21aae9":"corpus += get_corpus(test_df.text)","e71b80bc":"# Most common words\nfrom collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","6fb255b8":"from sklearn.feature_extraction.text import CountVectorizer\ndef get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","93761843":"#label encoding the categories. After this each category would be mapped to an integer.\nencoder = LabelEncoder()\nfinal_df['categoryEncoded'] = encoder.fit_transform(final_df['label'])","66352e5f":"X_train, X_test, y_train, y_test = train_test_split(final_df['text'], final_df['categoryEncoded'], random_state = 43, test_size = 0.2)","43c97610":"train_x=X_train.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=X_test.apply(lambda x: ''.join(i+' '  for i in x))\ntest_df_final = test_df['text'].apply(lambda x: ''.join(i+' '  for i in x))","5b1992ed":"tfidf=TfidfVectorizer(max_features=10000,min_df=6)\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)\ntest_2=tfidf.transform(test_df_final)\nprint(\"No. of features extracted:\", len(tfidf.get_feature_names()))\nprint(tfidf.get_feature_names()[:20])\n\ntrain_arr=train_1.toarray()\ntest_arr=test_1.toarray()","77e5d084":"test_arr1=test_2.toarray()","93447983":"def eval_model(y,y_pred):\n    print(\"Recall score of the model:\", round(recall_score(y_test, pred, average='weighted'), 3))\n    print(\"Precision score of the model:\", round(precision_score(y_test, pred, average='weighted'), 3))\n    print(\"F1 score of the model:\", round(f1_score(y,y_pred,average='micro'), 3))\n    print(\"Accuracy of the model:\", round(accuracy_score(y,y_pred),3))\n    print(\"Accuracy of the model in percentage:\", round(accuracy_score(y,y_pred)*100,3),\"%\")","bffb0815":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","e1539494":"def confusion_mat(color):\n    cm=confusion_matrix(y_test, pred)\n    plot_confusion_matrix(cm,\n                          categories,\n                          title='Confusion matrix')\n    ","1dcb5ff3":"lgbm=LGBMClassifier()\nlgbm.fit(train_arr,y_train)\npred=lgbm.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(y_test.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","f161030d":"eval_model(y_test,pred)\nb=round(accuracy_score(y_test,pred)*100,3)","8ddeaecd":"confusion_mat('Blues')","6ac721c8":"pred=lgbm.predict(test_arr1)\n#create a submission dataframe\nsubmission_df = pd.DataFrame(pred, columns=['label'])\n#write a .csv file for submission\nsubmission_df.to_csv('lgbm_submission.csv', index=False)","bffe6d7a":"We can observer here, there are no duplicates in the train data","c90ee943":"#### Removal of Numbers(digits)","8df78e64":"### Model Building","e37ee9ea":"**Function for Displaying the Confusion Matrix**\n\nThis function displays the confusion matrix of the model","b64fa968":"As we can see, all articles have a title and body. Going with the intuition that the title is often more descriptive of the category, as well as to provide more text data to the model, we will add title to the body","5e9436b6":"## Feature Extraction\n \n Features are extracted from the dataset and TF-IDF(Term Frequency - Inverse Document Frequency) is used for this purpose.","8396a075":"#### Removal of Stopwords","7c7597bb":"#### Creating Corpus of Words in Text","9b00e636":"#### Split words","0d73d89b":"#### Remove single letter characters","1760fd64":"#### Tokenization of Data","b94b6f69":"#### Removing URL's","bace85c1":"### Model - Decision Tree Classifier","2e3f9fef":"#### Removal of Punctuation Symbols","4670f36c":"The data is preprocessed, in NLP it is also known as text normalization. Some of the most common methods of text normalization are:\n- Tokenization\n- Lemmatization\n- Stemming","19aa501a":"### Data Pre-Processing","373d4802":"### Evaluation of Results","a2273963":"**Function for evaluation of model**\n\nThis function finds the F1-score and Accuracy of the trained model","221131a0":"# Training of Model","9495d5c3":"#### Train-Test Split","34260b53":"#### Replace new lines","21f51b56":"#### Lemmatization of Data","6c659ff8":"### Importing Necessary Libraries"}}