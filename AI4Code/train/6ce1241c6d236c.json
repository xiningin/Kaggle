{"cell_type":{"121649ef":"code","d7fda934":"code","98097e7d":"code","df37ba92":"code","6454933f":"code","7cd8fc1c":"code","da717fa3":"code","4c0f974b":"code","97870fe8":"code","f65cf106":"code","43ee8b63":"code","2dd5490b":"code","b329687c":"code","b36ecc2e":"code","d0ee52d6":"code","76420403":"code","e734e65d":"code","648f1876":"code","1016e2bd":"code","5617bbae":"code","193e8dcc":"code","24f06a87":"code","95526537":"code","162b34e9":"code","faefb57e":"markdown","8dd0a964":"markdown","0e871a9d":"markdown","7637f62b":"markdown","8ff32f6c":"markdown"},"source":{"121649ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7fda934":"d0 = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nprint(d0.head(5)) # print first five rows of d0.\n\n# save the labels into a variable l.\nl = d0['label']\n\n# Drop the label feature and store the pixel data in d.\nd = d0.drop(\"label\",axis=1)","98097e7d":"print(d.shape)\nprint(l.shape)","df37ba92":"# display or plot a number.\nplt.figure(figsize=(7,7))\nidx = 1\n\ngrid_data = d.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(l[idx])","6454933f":"# Pick first 15K data-points to work on for time-effeciency.\nlabels = l.head(15000)\ndata = d.head(15000)\n\nprint(\"the shape of sample data = \", data.shape)","7cd8fc1c":"# Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)","da717fa3":"#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)","4c0f974b":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","97870fe8":"# projecting the original data sample on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nimport matplotlib.pyplot as plt\nprint(sample_data.shape)\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","f65cf106":"import pandas as pd\n\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","43ee8b63":"import pandas as pd\ndf=pd.DataFrame()\ndf['1st']=[-5.558661,-5.043558,6.193635 ,19.305278]\ndf['2nd']=[-1.558661,-2.043558,2.193635 ,9.305278]\ndf['label']=[1,2,3,4]","2dd5490b":"import seaborn as sn\nimport matplotlib.pyplot as plt\nsn.FacetGrid(df, hue=\"label\", height=6).map(plt.scatter, '1st', '2nd').add_legend()\nplt.show()","b329687c":"sn.scatterplot(x=\"1st\",y=\"2nd\",hue=\"label\",data=df)","b36ecc2e":"# ploting the 2d data points with seaborn\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","d0ee52d6":"sn.scatterplot(x=\"1st_principal\",y=\"2nd_principal\",legend=\"full\",hue=\"label\",data=dataframe)","76420403":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()","e734e65d":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","648f1876":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","1016e2bd":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.","5617bbae":"# TSNE\n\nfrom sklearn.manifold import TSNE\n\n# Picking the top 1000 points as TSNE takes a lot of time for 15K points\ndata_1000 = standardized_data[0:1000,:]\nlabels_1000 = labels[0:1000]\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","193e8dcc":"model = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50')\nplt.show()","24f06a87":"model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50, n_iter=5000')\nplt.show()","95526537":"model = TSNE(n_components=2, random_state=0, perplexity=2)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 2')\nplt.show()","162b34e9":"\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(standardized_data)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","faefb57e":"**WORKING With Total Data**","8dd0a964":"**PCA for dimensionality redcution (not for visualization)**","0e871a9d":"**Visualization With PCA Own Implementation**","7637f62b":"**PCA With SKLEARN**","8ff32f6c":"**T-SNE Using SKLEARN**"}}