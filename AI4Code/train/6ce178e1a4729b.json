{"cell_type":{"0040c51a":"code","a24551c8":"code","9be36d1a":"code","b1bd6427":"code","c28abc30":"code","561cf542":"code","9d345db7":"code","b5138667":"code","ff131a73":"code","4f5775f1":"code","5c757f31":"code","2c6f4787":"code","af4e6356":"code","21d70958":"code","a2af1e03":"code","527154c2":"code","9e5c7007":"code","dcfb4653":"code","5e195148":"code","bfa0d787":"code","189b2864":"code","4303bdb8":"code","1dd40ad7":"code","ab8b3581":"code","c305c75e":"code","7aaae91d":"code","e60b2094":"code","b87969a0":"code","641d81ca":"code","c6105bdc":"code","8ab11b59":"code","9af324aa":"code","1fdbef2a":"code","59d2c1ac":"code","812f8ec6":"code","f03e1088":"code","28835bcc":"code","04325fdf":"code","87203cc4":"code","fd249f3f":"code","8b904c13":"code","b31bc5c6":"code","f72de8bb":"code","ac3c4cb5":"markdown","11dd28fa":"markdown","0b62eea5":"markdown","0dd1fe6e":"markdown","278b013c":"markdown","ea85b282":"markdown","d4383e99":"markdown","0068ac57":"markdown","8e7776cd":"markdown","5fcd5405":"markdown","87bf3f6a":"markdown","3df6864e":"markdown","2dfd9806":"markdown","4e1b4755":"markdown","6383503d":"markdown","b78d6638":"markdown","bb6b4a2d":"markdown","dbe79add":"markdown","ac650839":"markdown","aa887877":"markdown","16f24d55":"markdown","e6584793":"markdown","6d519184":"markdown","f11d99e0":"markdown","64c3483a":"markdown","d6acd18e":"markdown","2d67016b":"markdown","064bb9ed":"markdown","fbc50cfc":"markdown","d8e0c1e9":"markdown","320810f9":"markdown","f6e46340":"markdown","53e40958":"markdown","b33dc3c6":"markdown","141e9944":"markdown","eaaed186":"markdown","918a6955":"markdown"},"source":{"0040c51a":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport math\nimport xgboost as xgb\nimport time\n\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom statsmodels.graphics.gofplots import qqplot","a24551c8":"data = pd.read_csv('\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndata.info()","9be36d1a":"data.describe()","b1bd6427":"data[:3]","c28abc30":"class DataHelper:\n    '''\n    Helper class for plots and data handling functions.\n    '''\n    \n    def __init__(self, data):\n        self.data = data\n        \n    def update_data(self, data):\n        self.data = data\n        \n    def drop_columns(self, column_names):\n        '''\n        Drop some columns. Must update data with the returned new data set\n        '''\n        self.data = self.data.drop(column_names, axis = 1)\n        return self.data\n        \n    def get_count_unique_vals(self, column_names):\n        '''\n        For each of the column names as parameter, count the unique different values\n        '''\n        counts = {}\n        for column_name in column_names:\n            counts[column_name] = self.data.groupby(column_name)[column_name].nunique().count()\n        return counts\n    \n    def show_price_distribution(self):\n        '''\n        Show the price distribution in 3 graphs:\n        - price histogram distribution\n        - log of price histogram distribution\n        - how close the distribution matches a normal distribution (the red line)\n        '''\n        fig, ax = plt.subplots(1, 3, figsize=(23, 5))\n        sns.distplot(self.data['price'], ax = ax[0])\n        sns.distplot(self.data['log_price'], ax = ax[1])\n        qqplot(self.data['log_price'], line ='s', ax = ax[2])\n        ax[2].set_title('Comparison with theoritical quantils of normal distribution')\n        \n    def show_count_and_distrib_categorical_feature(self, column_name):\n        '''\n        For a categorical feature (with not too many categories to be rendered), display the count\n        and the price distribution for each category.\n        '''\n        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n        sns.countplot(self.data[column_name], ax=ax[0])\n        ax[0].set_title(\"Offers count per \" + column_name)\n        ax[0].set_xlabel(column_name)\n        sns.boxplot(x=column_name, y='log_price', data = self.data, ax=ax[1])\n        ax[1].set_title(\"Price per \" + column_name)\n        ax[1].set_xlabel(column_name)\n        ax[1].set_ylabel(\"Log of price\")\n        fig.show()\n        \n    def show_numerical_feature_distribution(self, column_name, bins=30):\n        '''\n        Show the count distribution for a numerical feature in two plots, one with a log xscale and another\n        with normal xscale.\n        '''\n        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n        ax[0].hist(self.data[column_name], bins)\n        ax[0].set_title('count of ' + column_name)\n        ax[0].set_xlabel(column_name)\n        ax[0].set_ylabel(\"count\")\n        ax[0].set_yscale('log')\n        ax[1].hist(np.log(1 + self.data[column_name]), bins)\n        ax[1].set_title('count of ' + column_name)\n        ax[1].set_yscale('log')\n        ax[1].set_xlabel('Log of ' + column_name)\n        ax[1].set_ylabel(\"count\")\n        ax[1].set_yscale('log')\n        \n    def get_pearson_features_correlation(self):\n        '''\n        Show matrix of pearson features correlation\n        '''\n        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n        ax.grid(True)\n        ax.set_title(\"Pearson's correlation\")\n        corr = self.data.corr()\n        sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, ax=ax)\n        \n    def get_dummies(self):\n        '''\n        Get dummies for every categorical feature in the data set. Remove the categorical columns.\n        Must update data with the returned new data set\n        '''\n        categorical_columns = list(self.data.select_dtypes(include=['object']).columns)\n        for column in categorical_columns:\n            self.data = self.data.drop(column, axis = 1).join(pd.get_dummies(self.data[column]))\n        return self.data\n            \n    def standardize_columns(self, column_names):\n        '''\n        Standardize every columns passed in arg.\n        '''\n        self.data[column_names] = StandardScaler().fit_transform(self.data[column_names])\n        \n# Log price will be need in many places later\ndata['log_price'] = np.log(1 + data['price'])\ndata_helper = DataHelper(data)","561cf542":"data_helper.get_count_unique_vals(['name', 'host_name', 'host_id', 'neighbourhood'])","9d345db7":"data = data_helper.drop_columns(['host_id', 'id', 'name', 'host_name'])","b5138667":"# Count missing values\ndata[pd.isnull(data['last_review'])]['price'].count()","ff131a73":"data[pd.isnull(data['last_review'])]['number_of_reviews'].mean()","4f5775f1":"data['never_reviewed'] = pd.isnull(data['last_review'])\ndata['reviews_per_month'] = data['reviews_per_month'].fillna(0)\n\n# Remove na and convert to int for usage\ndata['last_review'] = pd.to_datetime(data['last_review'])\nmin_date_review = data['last_review'].min()\ndata['last_review'] = data['last_review'].fillna(min_date_review)\ndata['last_review'] = data[['last_review']].apply(lambda x: x[0].timestamp(), axis=1)","5c757f31":"data.info()","2c6f4787":"data_helper.show_price_distribution()","af4e6356":"oldCount = data['price'].count()\ndata = data[data['log_price'] > 2.6][data['log_price'] < 8.5]\ndata_helper.update_data(data)\nprint(\"Data points lost: \", oldCount - data['price'].count())\n\n#qqplot(np.log(1 + data['price']), line ='s').show()\ndata_helper.show_price_distribution()","21d70958":"data_helper.show_count_and_distrib_categorical_feature('neighbourhood_group')","a2af1e03":"# Density distribution of the offers geographical position\nsns.jointplot(x=\"longitude\", y=\"latitude\", data=data, kind=\"kde\")","527154c2":"data_helper.show_count_and_distrib_categorical_feature('room_type')","9e5c7007":"data_helper.show_numerical_feature_distribution('minimum_nights')","dcfb4653":"data_helper.show_numerical_feature_distribution('number_of_reviews')","5e195148":"fig, ax = plt.subplots(1, 1, figsize=(15, 5))\nax.hist(data['last_review'], 100)\nax.set_title('count of last_review')\nax.set_xlabel('last_review (timestamps)')\nax.set_ylabel(\"count\")\nax.set_yscale('log')","bfa0d787":"data_helper.show_numerical_feature_distribution('reviews_per_month')","189b2864":"data_helper.show_numerical_feature_distribution('calculated_host_listings_count')","4303bdb8":"data_helper.show_numerical_feature_distribution('availability_365')","1dd40ad7":"data[data['availability_365'] == 0]['availability_365'].count()","ab8b3581":"data['never_available'] = data['availability_365'] == 0","c305c75e":"data_helper.get_pearson_features_correlation()","7aaae91d":"# Standardize the data (necessary for Ridge and does not afect xgBoost)\ndata_helper.standardize_columns(['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365'])\n\n# Get dummies\ndata = data_helper.get_dummies()","e60b2094":"# Important to use a seed to have the same base of comparison and data reproducability\nseed = 7805\n\n# Train set and test set that will only be used at the end to compare the results at the very end.\nX_train, X_test, y_train, y_test = train_test_split(data.drop(['price', 'log_price'], axis=1), data['log_price'], test_size=0.20, random_state=seed)","b87969a0":"class RidgeRegressionHelper:\n    def __init__(self, X_train, y_train, X_test, y_test):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        \n    def find_best_params(self, seed=None, kfolds=5, alphas=[1]):\n        '''\n        Perform grid search over the list of alphas\n        '''\n        kfolds = KFold(n_splits=kfolds, random_state=seed)\n        results = []\n        for alpha in alphas:\n            model = Ridge(alpha = alpha)\n            scores = cross_val_score(model, self.X_train, self.y_train, cv=kfolds, scoring='neg_mean_absolute_error')\n            results.append([alpha, -scores.mean(), scores.std()])\n            \n        return pd.DataFrame(results, columns=['alpha', 'mae-mean', 'mae-std'])\n    \n    def get_performance_score(self, seed=None, kfolds=5, alpha=0.3):\n        '''\n        Display performance metrics on the training and testing data sets after training the model using the training data and the parameters passed as arguments.\n        '''\n        # Cross validation\n        kfolds = KFold(n_splits=kfolds, random_state=seed)\n        model = Ridge(alpha = alpha)\n        scores = cross_val_score(model, self.X_train, self.y_train, cv=kfolds, scoring='neg_mean_absolute_error')\n        \n        # Train and fit to testing set\n        model.fit(self.X_train, self.y_train)\n        \n        y_pred = model.predict(self.X_test)\n        \n        results = []\n        results.append(['Ridge', -scores.mean(), scores.std(), mean_absolute_error(self.y_test, y_pred), r2_score(self.y_test, y_pred)])\n        return pd.DataFrame(results, columns=['Model', 'CV MAE', 'CV MAE std', 'test MAE', 'r2 test score'])\n    \n    def show_results(self, results):\n        '''\n        Plot the results MAE relative to the alpha values (use log x scale).\n        '''\n        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n        ax.plot(results['alpha'], results['mae-mean'])\n        ax.set_xlabel('alpha')\n        ax.set_ylabel('MAE')\n        ax.set_xscale('log')","641d81ca":"class XGBoostRegressionHelper:\n    def __init__(self, X_train, y_train, X_test, y_test):\n        self.dtrain = xgb.DMatrix(X_train, label=y_train)\n        self.dX_test = xgb.DMatrix(X_test)\n        self.y_test = y_test\n        \n    def find_best_params(self, seed=None, kfolds=5, learning_rates=[0.3], max_depths=[6], min_child_weights=[1], num_boost_round=1000):\n        '''\n        Performs a grid search given the list of parameters.\n        \n        :param int seed: number used for replicable results\n        :param int kfolds: number of folds to use during the cross validation\n        :param array of floats learning_rates: different learning rates values to test\n        :param array of int max_depths: different max depth values to test\n        :param array of int min_child_weights: different mi child weight values to test\n        :param int num_boost_round: (same a n_estimators) max number of boosting rounds to do, can stop before\n        :returns Dataframe: result set containing mae results and time taken for each combinaton of parameters\n        '''\n        results = []\n        for learning_rate in learning_rates:\n            for max_depth in max_depths:\n                for min_child_weight in min_child_weights:\n                    params = {\n                        'max_depth':max_depth,\n                        'min_child_weight': min_child_weight,\n                        'eta':learning_rate,\n                        'objective':'reg:squarederror',\n                    }\n                    timestamp = time.time()\n                    cv_results = xgb.cv(\n                        params,\n                        self.dtrain,\n                        num_boost_round=num_boost_round,\n                        seed=seed,\n                        nfold=kfolds,\n                        metrics={'mae'},\n                        early_stopping_rounds=10\n                    )\n                    totalTime = time.time() - timestamp\n                    boostRounds = cv_results['test-mae-mean'].idxmin()\n                    results.append([learning_rate, max_depth, min_child_weight, cv_results['test-mae-mean'][boostRounds], cv_results['test-mae-std'][boostRounds], boostRounds, totalTime])\n        \n        return pd.DataFrame(results, columns=['learning_rate', 'max_depth', 'min_child_weight', 'mae-mean', 'mae-std', 'boostRounds', 'totalTime_seconds'])\n    \n    def show_results_and_times_relative_to_parameter(self, results, parameter_name):\n        '''\n        Given a result set from find_best_params, show the mae-mean score and time taken given the best score of each value of one of the parameters.\n        \n        :param DataFrame results: result set obtained from find_best_params\n        :param str parameter_name: name of one of the parameter to optimize in find_best_params\n        '''\n        best_scores = results.loc[results.groupby(parameter_name)['mae-mean'].idxmin()]\n        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n        ax[0].plot(best_scores[parameter_name], best_scores['mae-mean'])\n        ax[0].set_xlabel(parameter_name)\n        ax[0].set_ylabel('MAE')\n        ax[1].plot(best_scores[parameter_name], best_scores['totalTime_seconds'])\n        ax[1].set_xlabel(parameter_name)\n        ax[1].set_ylabel('Total time for CV in seconds')\n        \n    def get_performance_score(self, seed=None, kfolds=5, learning_rate=0.3, max_depth=6, min_child_weight=1, num_boost_round=1000):\n        '''\n        Display performance metrics on the training and testing data sets after training the model using the training data and the parameters passed as arguments.\n        '''\n        params = {\n            'max_depth':max_depth,\n            'min_child_weight': min_child_weight,\n            'eta':learning_rate,\n            'objective':'reg:squarederror',\n        }\n        \n        # Cross validation part\n        cv_results = xgb.cv(\n            params,\n            self.dtrain,\n            num_boost_round=num_boost_round,\n            seed=seed,\n            nfold=kfolds,\n            metrics={'mae'},\n            early_stopping_rounds=10\n        )\n        boostRounds = cv_results['test-mae-mean'].idxmin()\n        \n        # Train and fit to testing set\n        xgb_reg = xgb.train(\n            params,\n            self.dtrain,\n            num_boost_round=num_boost_round,\n        )\n        \n        y_pred = xgb_reg.predict(self.dX_test)\n        \n        results = []\n        results.append(['XGBoost', cv_results['test-mae-mean'][boostRounds], cv_results['test-mae-std'][boostRounds], mean_absolute_error(self.y_test, y_pred), r2_score(self.y_test, y_pred)])\n        return pd.DataFrame(results, columns=['Model', 'CV MAE', 'CV MAE std', 'test MAE', 'r2 test score'])\n    \n    def get_features_importance(self, learning_rate=0.3, max_depth=6, min_child_weight=1, num_boost_round=50, max_num_features=10):\n        '''\n        Get the ranking of feature importance as a barh graph.\n        '''\n        params = {\n            'max_depth':max_depth,\n            'min_child_weight': min_child_weight,\n            'eta':learning_rate,\n            'objective':'reg:squarederror',\n        }\n        \n        xgb_reg = xgb.train(\n            params,\n            self.dtrain,\n            num_boost_round=num_boost_round,\n        )\n        \n        xgb.plot_importance(xgb_reg, max_num_features=max_num_features)","c6105bdc":"# Ridge baseline\nridgeHelper = RidgeRegressionHelper(X_train, y_train, X_test, y_test)\nridgeHelper.get_performance_score(seed=seed)","8ab11b59":"# XGBoost baseline\nxgbHelper = XGBoostRegressionHelper(X_train, y_train, X_test, y_test)\nxgbHelper.get_performance_score(seed=seed)","9af324aa":"# Finding best alpha\nresults = ridgeHelper.find_best_params(seed = seed, alphas=[0.5, 1, 2, 5, 10, 20])\nridgeHelper.show_results(results)","1fdbef2a":"# Finer tuning around 5\nfiner_results = ridgeHelper.find_best_params(seed = seed, alphas=[1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7])\nridgeHelper.show_results(finer_results)","59d2c1ac":"best_result = finer_results.loc[finer_results['mae-mean'].idxmin()]\nbest_alpha = best_result['alpha']\nbest_result","812f8ec6":"ridgePerformance = ridgeHelper.get_performance_score(seed=seed, alpha=best_alpha)\nridgePerformance","f03e1088":"# Try to find the best parameters for XGBoost\nresults = xgbHelper.find_best_params(seed = seed, learning_rates=[0.05, 0.1, 0.3], max_depths=[5, 7, 9], min_child_weights=[1, 3, 5])","28835bcc":"# Let's have a look at how the learning rate influence the results and the time it takes\nxgbHelper.show_results_and_times_relative_to_parameter(results, 'learning_rate')","04325fdf":"# For a better understanding, we can directly look at the results\nresults","87203cc4":"finer_results = xgbHelper.find_best_params(seed = seed, learning_rates=[0.01, 0.025, 0.05], max_depths=[9, 11, 13], min_child_weights=[1])","fd249f3f":"xgbHelper.show_results_and_times_relative_to_parameter(finer_results, 'learning_rate')\nfiner_results","8b904c13":"best_result = finer_results.loc[finer_results['mae-mean'].idxmin()]\nbest_result","b31bc5c6":"xgbPerformance = xgbHelper.get_performance_score(seed=seed, learning_rate=0.01, max_depth=11, min_child_weight=1, num_boost_round=500)\nxgbPerformance","f72de8bb":"xgbHelper.get_features_importance(learning_rate=0.01, max_depth=11, min_child_weight=1, num_boost_round=500)","ac3c4cb5":"### 1.5.2 Geographic position","11dd28fa":"Data seems here very skewed around 0. We will try to look a it further.","0b62eea5":"## 2.5 Discussion of the results","0dd1fe6e":"'name' has any useful information with complex processing (using NLP for instance). There are too many distinct values in 'host_name' and 'host_id' to be usable. We can use the 'neighbourhood' value however.","278b013c":"Open data set, cleaning","ea85b282":"## 2.1 Models classes\n### 2.1.1 RidgeRegression Model class","d4383e99":"The data seems to follow a normal distribution for the most part but there are some outliers at extreme prices (around <2.6 and >8.5). They might not real announces for Airbnb (we can imagine there is no price at 0 except if they specify the price in another manner in the rental. And some of the very expensive at 10000 might be the maximum price, put for trolling or also because maximum price is reached). Depending of the model used (if the model assumes a normal distribution, such as Naives Bayes), it would be better to remove those data points to get better results. For other models such as Random Forests, we don't assume any distribution so we could keep it. As we might not even want to predict such outliers\/trolls and it simplifies to always remove them, we will simply remove them all the time.","0068ac57":"## 1.4 Price distribution","8e7776cd":"## 1.2 Clean useless columns","5fcd5405":"We focus here on the neighbourhood groups and not neighbourhood as there are 221 different neighbourhoods!","87bf3f6a":"Format data for predictions. Note that we will always use the log_price as target.","3df6864e":"## 1.5 Features exploration","2dfd9806":"### 2.1.2 XGBoostRegression Model class","4e1b4755":"We can already see that optimizing alpha will change almost nothing in the MAE results. We can still try to tune alpha slightly more around 1-7.","6383503d":"Firstly we can see that we get much better results with xgBoost regression with r2 score 0.623 vs 0.557 for Rigde regression.\nIn general xgb performs rather well on every problem including regression. Besides it runs fast, its computation time scales about linearly with the data set size and can be run using the gpu (which I did not do here as it required more configurations to have it running and for this small project scale was not worth it).\n\nLooking at the features importance we see that the localization (latitude and longitude) are used the most by some margin along with the various neighbourhood groups, as they are 221 of them and one of them is seen in the top ten ranking, it makes it likely that they sum up to a very large importance together. This is not surprising as one could expect the location of any house or appartment is very determinant of its price in general.\n\nWhat is surprising is how the 'availability_365' (number of days when listing is available for booking) is so used, I would not have thought that it could really be used much to predict the price of a renting. One hypothesis to explain that could be that some people are renting they own appartment\/one of their room, only when it suits them and then are only ready to do so at a higher price. While people that rent them all year along are using their property for the sole purpose of renting and can accept lower price as the place would not be used otherwise. We can also note that that the 'never_available' column was not so important after all, perhaps because rentings never available are just not up to date and could have a very broad price distribution without much general information.\n\nWe can apply a similar logic for 'minimum_nights', 'last_review', 'number_of_reviews', 'review_per_month' and 'calculated_host_listing_count', for each of them having no minimum nights\/many or recent review\/many listing on their account would tend to show they are doing more \"professional\" full time renting with probably more predicable and perhaps lower prices.\n\n\n### Future improvements\nTo go further we could try to apply some basic NLP to the listings' names or even see if the names contain some value (for instance duplex\/studio\/X rooms\/penthouse) as it seems a priori a good (but naive) way of telling if a listing will be expensive or not, that would basically be like expending the room-type feature to be much more specific and hold more information. A good sign in that direction is the importance of 'Private room' as it was the smallest group with the most information to get from.\n\nWe could also try keep some information about the host_id for the biggest host listers and put a default value for the ones with few listing to avoid too much of an explosion of features, following my reasoning on why the availability feature is so important.\n\nThere might also be some work that could be done on the host names to get their social background (hence standards for renting) from their name. But I do not know how much information can be acquired this way and it would need some data outside of this data set about it to get some good data. It probably would not help much but might still be useful as a lead.","b78d6638":"### 1.5.10 Features correlation (Pearson's correlation)","bb6b4a2d":"About a third of the offers are never available, they might not be up to date anymore. Hence there is potentially a big difference between 0 and any other value here. We could try to turn this feature into multiple ones (such as boolean features if available) or not, but we could loose some information this way or simply add some redondant information for when it is exactly 0. It seems the safest option to improve the results without much risk degrading them.","dbe79add":"## 2.4 XGBoostRegression hyperparameters tunning","ac650839":"### 1.5.4 minimum of nights","aa887877":"We can also look at the feature importance, which is often very important to know what part of the analysis to give greater focus and what kind of data is most valuable to collect in the future. However since we separated the categorical features into dummy variables, we would need to do some aggregate over them to get the real total value of this feature.","16f24d55":"## 1.1 Helper classes","e6584793":"### 1.5.8 calculated_host_listings_count","6d519184":"### 1.5.5 number_of_reviews","f11d99e0":"## 2.2 Baselines","64c3483a":"### 1.5.9 availability_365","d6acd18e":"# 2 Predictions","2d67016b":"From these results, we can see that we might be able to get slightly better results by reducing the learning rate further and\/or increasing the max_depth. min_child_wieght cannot be further reduced however so we will keep it at 1. However we can also see the execution time growing a lot, so it might not be a worth using a smaller learning rate if the execution time increase too much compared to the improvement.","064bb9ed":"### 1.5.7 reviews_per_month","fbc50cfc":"The results are indeed slightly better but as expected the execution time went up 5 folds! Knowing that it would only cost a fifth of that to train the model (we were using 5-fold cross-validation), it would be around 2m30s. This is a very acceptable value in this context but if we were to often update this data set with more values or simply have a bigger dataset, we should pick some higher learning rate with only slightly worse results (like 0.05 with a max_depth of 11).","d8e0c1e9":"### 1.5.1 Neighbourhood group","320810f9":"The pike at the beginning is due to the handling of nan, so beside that, nothing particular to observe here.","f6e46340":"## 1.3 Deal with missing values\n\nFrom the data info, last_review and reviews_per_month are missing the same number of values, let's see what we should fill them by.","53e40958":"## 2.3 RidgeRegression hyperparameter tunning","b33dc3c6":"### 1.5.6 last_review","141e9944":"### 1.5.3 Room type","eaaed186":"# 1. Data preprocessing and exploration","918a6955":"Number of reviews are always 0, reviews_per_month missing values should be 0. We will use the earliest date of the dataset to fill last_review as it never was reviewed. This is definitely not ideal as maybe it is just caused by the offer being too recent to have been rated yet, so we could also argue to use the mean of every date instead (or even try to predict it from the other features but this would be very difficult). We will add a new column \"never_reviewed\" to make the distinction better as it might be an important feature."}}