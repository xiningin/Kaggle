{"cell_type":{"319113e1":"code","15da6a14":"code","6c2bf223":"code","ec7693e3":"code","12dbed92":"code","0a5b553f":"code","eb3b6836":"code","e3f98241":"code","764d06de":"code","63f81063":"code","176c51d9":"code","6d0d38b8":"code","9ebc105c":"code","a393dcb5":"code","adb2c15a":"code","37096039":"code","fcec8fba":"code","c662fb44":"code","55b0335c":"code","2cbde367":"code","b372b7af":"code","8b36a751":"code","1f17dbf0":"code","7f408808":"code","3bcab844":"code","28f260d4":"code","a798d0b4":"code","376a7f18":"code","41b3874f":"code","c297cdcc":"code","803ff546":"code","e3e30b98":"code","497b7b74":"code","cd58ff27":"code","fbb4c020":"code","1b5e252d":"code","d9f95d90":"code","95de1368":"code","499417f1":"code","7aa9a628":"code","a1be665a":"code","4c3788ac":"code","2edc3b46":"code","09b89c95":"code","836a6b81":"code","5ddd8395":"code","dbab9e23":"code","c41a2ddb":"code","b726d3f5":"code","adb7b8c2":"code","de76c626":"code","e0f2cce2":"code","5abfc56e":"code","997c3c9b":"code","c88d441d":"code","4b0d7e70":"code","391ea16e":"code","fb28385c":"code","b10bffa6":"code","b5027627":"code","1d36fd7f":"code","61c77ef9":"code","e595b8a9":"code","ec5dd61a":"code","8a8efdd1":"code","f703dd46":"code","66335545":"code","c7ca02c2":"code","354f6bbb":"code","7e5071de":"code","05dd562b":"code","40574c1f":"code","fbc3020a":"code","9b813593":"code","90c564a2":"code","d7a26bcb":"code","cdb4dae9":"code","d8b78bef":"code","2ffe786b":"code","9826d5ce":"code","dbdf7cd7":"code","80d38866":"code","b8c9a7ac":"code","62c6048f":"code","d57b5fb1":"code","558790ab":"code","9b08df39":"code","5cc49259":"code","580cffcc":"code","dfcb9768":"code","eef62b4d":"code","32b63837":"code","9e583898":"code","7919e831":"code","adfa12a8":"code","d24b0271":"markdown","0b3eb7ca":"markdown","5be89248":"markdown","dc2486a5":"markdown","d96dfa2f":"markdown","469db640":"markdown","81120368":"markdown","c5cc5115":"markdown","f506eb95":"markdown","0e37d606":"markdown","bbd39004":"markdown","29458842":"markdown","ec25d97b":"markdown","ac87fdfd":"markdown","f97c48ce":"markdown","78c3b0d3":"markdown","113aa9fd":"markdown","4c1d96c5":"markdown","7156cf8b":"markdown","00798469":"markdown","57f533c5":"markdown"},"source":{"319113e1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\n","15da6a14":"gender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\n\n#gender_submission.head()\n#test.head()\n#train.head()","6c2bf223":"sns.pairplot(train)","ec7693e3":"train.head()\n","12dbed92":"print(gender_submission.shape)\nprint(test.shape)\nprint(train.shape)","0a5b553f":"train.info(), test.info()","eb3b6836":"train.isnull().sum()\n","e3f98241":"test.isnull().sum()\n","764d06de":"def pie_chart(df, column ,explode , labels,title,no):\n    \n    plt.pie(df[column].value_counts(),\n            explode=explode,    #explode=[0.04,0]\n            startangle=90, \n            autopct='%1.1f%%',\n            labels=labels, #labels=['Males','Females']\n            colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99'],\n            pctdistance=.6,\n            textprops={'fontsize': 20})\n    plt.title(title)\n    plt.figure(no)\n\npie_chart(train, \"Pclass\" ,[0.05,0.05,0.05], ['3','1',\"2\"],\"Pclass for train data\",0)\npie_chart(test ,\"Pclass\" ,[0.05,0.05,0.05], ['3','1',\"2\"],\"Pclass for test data\",1)\n","63f81063":"survived = train[train[\"Survived\"] == 1]","176c51d9":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[20, 5])\n\nsns.countplot(train['Sex'], ax =axes[0]).set_title(\"number of males and females on the ship\", fontsize=18)\n\nsns.countplot(survived['Sex'], ax =axes[1]).set_title(\"number of males and females who survived\", fontsize=18)","6d0d38b8":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[20, 7])\n\nsns.countplot(train[\"Embarked\"], ax =axes[0]).set_title(\"Train data Embarked distribution\", fontsize=18)\n\nsns.countplot(test[\"Embarked\"], ax =axes[1]).set_title(\"Test data Embarked distribution\", fontsize=18)\n","9ebc105c":"train[\"Embarked\"].fillna(\"S\", inplace = True)","a393dcb5":"train[train[\"Age\"] <2]","adb2c15a":"train_notnull_age = train[pd.notnull(train[\"Age\"])]\ntest_notnull_age = test[pd.notnull(test[\"Age\"])]\ntrain_notnull_age","37096039":"train_notnull_age[\"Age\"] = train_notnull_age[\"Age\"].astype(int)\ntest_notnull_age[\"Age\"] = test_notnull_age[\"Age\"].astype(int)\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=[30, 9])\n\nsns.countplot(train_notnull_age['Age'],ax =axes[0]).set_title(\"Age distribution for train data\", fontsize=20)\nsns.countplot(test_notnull_age['Age'],ax =axes[1]).set_title(\"Age distribution for test data\", fontsize=20)","fcec8fba":"train[\"Age\"].fillna(train[\"Age\"].median(), inplace = True)\ntest[\"Age\"].fillna(test[\"Age\"].median(), inplace = True)","c662fb44":"train[\"Age\"] = train[\"Age\"].astype(int)\ntest[\"Age\"] = test[\"Age\"].astype(int)\n","55b0335c":"test[\"Fare\"].fillna(test[\"Fare\"].median(), inplace = True)","2cbde367":"train[\"Fare\"].describe()","b372b7af":"train[train[\"Fare\"] < 4]","8b36a751":"fare = train[train[\"Fare\"] > 4]\nfare[\"Fare\"]","1f17dbf0":"train.loc[ train.Fare == 0, \"Fare\" ] = fare[\"Fare\"].median()\n","7f408808":"train[train[\"Fare\"] < 4]","3bcab844":"train.isnull().sum(), test.isnull().sum()\n","28f260d4":"train[\"Cabin\"].describe()","a798d0b4":"train.head()","376a7f18":"train_without_cabin= train[train[\"Cabin\"].isnull()]\ntrain_with_cabin= train[train[\"Cabin\"].notnull()]\n\ntest_without_cabin= test[test[\"Cabin\"].isnull()]\ntest_with_cabin= test[test[\"Cabin\"].notnull()]","41b3874f":"train_without_cabin[\"Pclass\"].value_counts(), train_with_cabin[\"Pclass\"].value_counts()","c297cdcc":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[24, 5])\n\nsns.countplot(train_without_cabin['Pclass'],ax =axes[0]).set_title(\"Pclass without cabin for train data\", fontsize=18)\nsns.countplot(test_without_cabin['Pclass'],ax =axes[1]).set_title(\"Pclass without cabin for test data\", fontsize=18)\n","803ff546":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[24, 5])\n\nsns.countplot(train_with_cabin['Pclass'],ax =axes[0]).set_title(\"Pclass with cabin for train data\", fontsize=18)\nsns.countplot(test_with_cabin['Pclass'],ax =axes[1]).set_title(\"Pclass with cabin for test data\", fontsize=18)\n","e3e30b98":"train_with_cabin[train_with_cabin[\"Pclass\"] == 3]","497b7b74":"train_with_cabin[\"Cabin\"].unique()","cd58ff27":"train[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"C\", na=False) ] = \"C\"\ntrain[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"E\", na=False) ] = \"E\"\ntrain[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"A\", na=False) ] = \"A\"\ntrain[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"B\", na=False) ] = \"B\"\ntrain[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"G\", na=False) ] = \"G\"\ntrain[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"F\", na=False) ] = \"F\"\ntrain[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"D\", na=False) ] = \"D\"\ntrain[\"Cabin\"].loc[ train[\"Cabin\"].str.contains(\"B\", na=False) ] = \"B\"\n\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"C\", na=False) ] = \"C\"\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"E\", na=False) ] = \"E\"\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"A\", na=False) ] = \"A\"\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"B\", na=False) ] = \"B\"\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"G\", na=False) ] = \"G\"\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"F\", na=False) ] = \"F\"\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"D\", na=False) ] = \"D\"\ntest[\"Cabin\"].loc[ test[\"Cabin\"].str.contains(\"B\", na=False) ] = \"B\"\n","fbb4c020":"train.head(20)","1b5e252d":"train[\"Cabin\"].unique()","d9f95d90":"train_with_cabin= train[train[\"Cabin\"].notnull()]\ntrain_with_cabin= train_with_cabin[train_with_cabin[\"Pclass\"] == 3]\n\ntest_with_cabin= train[train[\"Cabin\"].notnull()]\ntest_with_cabin= test_with_cabin[test_with_cabin[\"Pclass\"] == 3]\ntest_with_cabin[\"Cabin\"].unique(), train_with_cabin[\"Cabin\"].unique()\n","95de1368":"train[\"Cabin\"].fillna(\"G\", inplace = True)\ntest[\"Cabin\"].fillna(\"G\", inplace = True)\n","499417f1":"train[\"Cabin\"].value_counts(),test[\"Cabin\"].value_counts()","7aa9a628":"train[\"Cabin\"].unique(), test[\"Cabin\"].unique()","a1be665a":"train.isnull().sum(), test.isnull().sum()","4c3788ac":"sns.pairplot(train, palette='deep')","2edc3b46":"ax = sns.barplot(data=train.drop([\"PassengerId\",\"Age\",\"Fare\"],axis=1), capsize=.2)\nplt.tick_params(axis='x', rotation=30)","09b89c95":"ax = sns.barplot(data=train[[\"Age\",\"Fare\"]], capsize=.2)\nplt.tick_params(axis='x', rotation=30)","836a6b81":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,9))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","5ddd8395":"train.columns , test.columns","dbab9e23":"train[\"family\"] = train[\"SibSp\"] + train[\"Parch\"]+1\ntrain.drop([\"Parch\",\"SibSp\"],axis=1,inplace=True)\n\n\ntest[\"family\"] = test[\"SibSp\"] + test[\"Parch\"]+1\ntest.drop([\"Parch\",\"SibSp\"],axis=1,inplace=True)\n","c41a2ddb":"train['Title'] = train['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ntrain['Is_Married'] = 0\ntrain['Is_Married'].loc[train['Title'] == 'Mrs'] = 1\n\ntest['Title'] = test['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ntest['Is_Married'] = 0\ntest['Is_Married'].loc[test['Title'] == 'Mrs'] = 1","b726d3f5":"train['Title'].unique()","adb7b8c2":"plt.rcParams['figure.figsize'] = (11, 6)\n\nsns.countplot(train[\"Title\"]).set_title(\"Is_Married feature distribution\", fontsize=18)","de76c626":"train.head()","e0f2cce2":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,9))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","5abfc56e":"train.Cabin.unique()","997c3c9b":"train[['S','C','Q']] = pd.get_dummies(train['Embarked'])\ntrain[['male','female']] = pd.get_dummies(train['Sex'])\n\ntest[['S','C','Q']] = pd.get_dummies(test['Embarked'])\ntest[['male','female']] = pd.get_dummies(test['Sex'])","c88d441d":"train.head()","4b0d7e70":"#train = pd.get_dummies(train, columns=[\"Embarked\",\"Sex\"])\n#test = pd.get_dummies(test, columns=[\"Embarked\",\"Sex\"])\n#train.head()","391ea16e":"from sklearn import preprocessing\n#creating labelEncoder\nle = preprocessing.LabelEncoder()\n# Converting string labels into numbers.\ntrain[\"Ticket\"]=le.fit_transform(train[\"Ticket\"])\ntrain[\"Embarked\"]=le.fit_transform(train[\"Embarked\"])\ntrain[\"Name\"]=le.fit_transform(train[\"Name\"])\ntrain[\"Sex\"]=le.fit_transform(train[\"Sex\"])\ntrain[\"Cabin\"]=le.fit_transform(train[\"Cabin\"])\ntrain[\"Title\"]=le.fit_transform(train[\"Title\"])\n\n\ntest[\"Ticket\"]=le.fit_transform(test[\"Ticket\"])\ntest[\"Embarked\"]=le.fit_transform(test[\"Embarked\"])\ntest[\"Name\"]=le.fit_transform(test[\"Name\"])\ntest[\"Sex\"]=le.fit_transform(test[\"Sex\"])\ntest[\"Cabin\"]=le.fit_transform(test[\"Cabin\"])\ntest[\"Title\"]=le.fit_transform(test[\"Title\"])\n\n#list(Name)\n#le.inverse_transform([2])","fb28385c":"#test['Sex'] = test['Sex'].apply(lambda x: 0 if x == \"male\" else 1)\n\n#train['Sex'] = train['Sex'].apply(lambda x: 0 if x == \"male\" else 1)","b10bffa6":"train.columns","b5027627":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n\nfeatures = train.drop([\"Survived\",'PassengerId','Pclass','Sex','Age','Embarked','S','male'], axis=1)\nlabel = train[\"Survived\"]\n\nx_train,x_test,y_train,y_test = train_test_split(features, label, test_size=0.2, random_state=42)","1d36fd7f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)","61c77ef9":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(x_train,y_train)","e595b8a9":"LR_Predict = LR.predict(x_test)\nprint(classification_report(LR_Predict,y_test))","ec5dd61a":"svc = SVC()\nsvc.fit(x_train,y_train)\nSVC_predict = svc.predict(x_test)\nprint(classification_report(SVC_predict,y_test))","8a8efdd1":"X_train = train.drop([\"Survived\",'PassengerId','Pclass','Sex','Age','Embarked','S','male'], axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(['PassengerId','Pclass','Sex','Age','Embarked','S','male'], axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape\n#Y_test = gender_submission[\"Survived\"]","f703dd46":"X_train.head()","66335545":"X_test.head()","c7ca02c2":"#from sklearn.feature_selection import f_regression\n#f_regression(X_train, Y_train, center=True)","354f6bbb":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n#y_train = scaler.fit_transform(y_train)\n#y_test = scaler.fit_transform(y_test)\n#scaler.transform(X_train)","7e5071de":"#from sklearn.preprocessing import MaxAbsScaler\n#MAscaler = MaxAbsScaler()\n#X_train = MAscaler.fit_transform(X_train)\n#X_test = MAscaler.fit_transform(X_test)","05dd562b":"#from sklearn.preprocessing import MinMaxScaler\n#MMscaler = MinMaxScaler(feature_range=(0,1))\n#X_train = MMscaler.fit_transform(X_train)\n#X_test = MMscaler.fit_transform(X_test)","40574c1f":"#from sklearn.preprocessing import Normalizer\n#normal = Normalizer(norm=\"l2\")\n#X_train = normal.fit_transform(X_train)\n#X_test = normal.fit_transform(X_test)","fbc3020a":"scoring=\"accuracy\"\nk_fold = KFold(n_splits= 10 ,shuffle = True,random_state=0)","9b813593":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(solver='liblinear',max_iter=2000)\nLR.fit(X_train,Y_train)\n\n\nLR_acc = round(LR.score(X_train, Y_train) * 100, 2)\nprint(LR_acc)\n\nLR_pred = LR.predict(X_test)\n\nLR_CV = cross_val_score(LR, X_train, Y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(LR_CV)\nprint(LR_CV.mean())\n","90c564a2":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline([('classifier' , LogisticRegression())])\n\nparam_grid = [\n    {'classifier' : [LogisticRegression()],\n     'classifier__penalty' : ['l1', 'l2'],\n    'classifier__C' : np.logspace(-4, 4, 20),\n    'classifier__solver' : ['liblinear']}]\n\nLR2 = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n\n\nbest_LR = LR2.fit(X_train, Y_train)\n\nLR_acc = round(best_LR.score(X_train, Y_train) * 100, 2)\nLR_acc","d7a26bcb":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X_train,Y_train)\n\ngnb_acc = round(gnb.score(X_train, Y_train) * 100, 2)\nprint(gnb_acc)\n\ncv = cross_val_score(gnb,X_train,Y_train,cv=k_fold)\nprint(cv)\nprint(cv.mean())","cdb4dae9":"gnb_pred = gnb.predict(X_test)\ngnb_pred","d8b78bef":"svc = SVC()\nsvc.fit(X_train, Y_train)\nSVC_pred = svc.predict(X_test)\n\nacc_SVC = round(svc.score(X_train, Y_train) * 100, 2)\nprint(acc_SVC)\n\n\nSVC_CV = cross_val_score(svc, X_train, Y_train, cv=k_fold, n_jobs=1, scoring=scoring).mean()\nprint(SVC_CV)\n","2ffe786b":"param_grid = [\n    {'kernel' : ['linear', 'rbf', 'poly']}]\n    #'gamma' : [0.1, 1, 10, 100]\n    #'C' : [0.1, 1, 10, 100, 1000],\n    #'degree' : [0, 1, 2, 3, 4, 5, 6]}]\n\nsvc2 = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n\n\nbest_svc2 = svc2.fit(X_train, Y_train)\n\nsvc2_acc = round(best_svc2.score(X_train, Y_train) * 100, 2)\nsvc2_acc","9826d5ce":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nKNN_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n\nprint(acc_knn)\n\nKNN_CV = cross_val_score(knn, X_train, Y_train, cv=k_fold, n_jobs=1, scoring=scoring).mean()\nprint(KNN_CV)\n","dbdf7cd7":"param_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\n\nKnn2 = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n\n\nbest_Knn2 = Knn2.fit(X_train, Y_train)\n\nKnn2_acc = round(best_Knn2.score(X_train, Y_train) * 100, 2)\nKnn2_acc","80d38866":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrand_forrest_pred = random_forest.predict(X_test)\n\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(acc_random_forest)\n\nrandom_forest_CV = cross_val_score(random_forest, X_train, Y_train, cv=k_fold, n_jobs=1, scoring=scoring).mean()\nprint(random_forest_CV)\n","b8c9a7ac":"param_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n\nrandom_forest2 = GridSearchCV(random_forest, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n\n\nbest_random_forest2 = random_forest2.fit(X_train, Y_train)\n\nrandom_forest2_acc = round(best_random_forest2.score(X_train, Y_train) * 100, 2)\nrandom_forest2_acc","62c6048f":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train,Y_train)","d57b5fb1":"xgb_acc = round(best_clf_xgb.score(X_train, Y_train) * 100, 2)\nxgb_acc","558790ab":"from keras.callbacks import ModelCheckpoint, EarlyStopping\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","9b08df39":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\n\n#Layers\n#Input Layer\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))\n#Hidden Layers\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n\n\n#Ouput Layer\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n#Binary cross_entropy is for classification loss function.\nmodel.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#Fit the model\nmodel.fit(X_train, Y_train, batch_size = 32, epochs = 4000, verbose=2, callbacks=[es])","5cc49259":"model.summary()\n","580cffcc":"y_pred = model.predict(X_test)\ny_fin = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])","dfcb9768":"_, train_acc = model.evaluate(X_train, Y_train, verbose=0)\nprint('Train: %.3f'%(train_acc))","eef62b4d":"#from sklearn.model_selection import cross_val_predict\n#from sklearn.metrics import confusion_matrix\n#predictions = cross_val_predict(L, X_train, Y_train, cv=3)\n#confusion_matrix(Y_train, predictions)","32b63837":"final_report = pd.DataFrame({'classifier': [\"Log_reg_score\",\"SVC_score\",\"KNN_score\",\"random_forest_score\",\"XGBoost_score\",\"NN\", 'GaussianNB']\n                            ,'train_acc(Tuned)':  [LR_acc ,svc2_acc ,Knn2_acc ,random_forest2_acc ,xgb_acc ,train_acc*100, gnb_acc]\n                   })\n","9e583898":"final_report","7919e831":"LR_pred = best_LR.predict(X_test)\nknn_pred = best_Knn2.predict(X_test)\nrf_pred = best_random_forest2.predict(X_test)\nxgb = best_clf_xgb.predict(X_test)\nsvc = best_svc2.predict(X_test)\n","adfa12a8":"LR_submission = pd.DataFrame({'PassengerId': test['PassengerId'], \n              'survived': LR_pred})\nknn_submission = pd.DataFrame({'PassengerId': test['PassengerId'], \n              'survived': knn_pred})\nrf_submission = pd.DataFrame({'PassengerId': test['PassengerId'], \n              'survived': rf_pred})\nxgb_submission = pd.DataFrame({'PassengerId': test['PassengerId'], \n              'survived': xgb})\nsvc_submission = pd.DataFrame({'PassengerId': test['PassengerId'], \n              'survived': svc})\ngnb_submission = pd.DataFrame({'PassengerId': test['PassengerId'], \n              'survived': gnb_pred})\nNN_submission = pd.DataFrame({'PassengerId': test['PassengerId'], \n              'survived': y_fin})\n#submission\n\nLR_submission.to_csv(\"LR_submission.csv\",index= False)\nknn_submission.to_csv(\"knn_submission.csv\",index= False)\nrf_submission.to_csv(\"rf_submission.csv\",index= False)\nxgb_submission.to_csv(\"xgb_submission.csv\",index= False)\nsvc_submission.to_csv(\"svc_submission.csv\",index= False)\ngnb_submission.to_csv(\"gnb_submission.csv\",index= False)\nNN_submission.to_csv(\"NN_submission.csv\",index= False)\n","d24b0271":"## 2. Feature Engineering","0b3eb7ca":"there is no reasonable 0 fares for some passengers, will try to fill these 0s","5be89248":"now let's go and see the correlation between features","dc2486a5":"so after searching in **encyclopedia** i found that **name** feature has this pattern:\n\n> surname ,martial status ,name (name for Mr and husbend name for Mrs) ,(name of Mrs) \n\nfor example : \n\n> Futrelle, Mrs. Jacques Heath (Lily May Peel)\t","d96dfa2f":"## Introduction\n\nin this kernel i will try to explore, clean and explain some features with statistical approaches and visualizations and also i'll see if i can do some feature engineering then i will try different algorithims for predicting who survived for this disaster \n\nif you have any suggest,advice or correction please don't hesitate to write it, i think it will be very helpful for me and if you like this kernel an upvote would be great.","469db640":"### Sex","81120368":"passengers whom age is 0 are actully children which didn't complete their year one, shown in the next table","c5cc5115":"### Table of content\n                1. data Exploration and cleaning\n                2. Feature Engineering\n                3. Applying different ML approaches\n\n","f506eb95":"![main-qimg-5ab46f31803d2242e89996144a228ab1-c.jpeg](attachment:main-qimg-5ab46f31803d2242e89996144a228ab1-c.jpeg)","0e37d606":"### Embarked","bbd39004":"### Age","29458842":"### Cabin","ec25d97b":"## let's check distribution of features and the correlation between them","ac87fdfd":"### Pclass","f97c48ce":"so, as you can see KNN and Random forest is totaly overfitted, but SVC and Logistic Regression was acceptable when i submit them as i got approximately 78% and 77% respectively","78c3b0d3":"![image.jpg](attachment:image.jpg)","113aa9fd":"## 1. data Exploration and cleaning","4c1d96c5":"## 3. Applying different ML approaches","7156cf8b":"### Fare","00798469":"so, the most nan values in Cabin are related to Pclass 3","57f533c5":"these are the tickets for the 3 class, let's go and see the passengers distribution according to Pclass"}}