{"cell_type":{"a7134b26":"code","b61253ef":"code","ae52be40":"code","929415fd":"code","9d7e3b8b":"code","8427e630":"code","24153a8b":"code","f119ad83":"code","d82fb82b":"code","ed6b536f":"code","99e176aa":"code","e5d49f58":"code","dc22e1c3":"code","5c68e262":"code","c3e866db":"code","b4bf88b1":"code","98c7edb5":"code","874ebd84":"code","6334653f":"code","31bf661b":"code","5576cc3a":"code","a59645a1":"code","9cdb5536":"code","66601e81":"code","a7c701e5":"code","d957bdb3":"code","d2f3fb28":"code","478f59a0":"code","b9cd3257":"markdown","001e9aea":"markdown","98e726c5":"markdown","3581b3c9":"markdown","4113c800":"markdown","009ad2d7":"markdown","e40e9d8e":"markdown","80004884":"markdown","0aa3f107":"markdown"},"source":{"a7134b26":"import mlxtend\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport implicit\nimport surprise","b61253ef":"from mlxtend.preprocessing import TransactionEncoder\n\ndata = np.array([\n    ['\uc6b0\uc720', '\uae30\uc800\uadc0', '\uc96c\uc2a4'],\n    ['\uc591\uc0c1\ucd94', '\uae30\uc800\uadc0', '\ub9e5\uc8fc'],\n    ['\uc6b0\uc720', '\uc591\uc0c1\ucd94', '\uae30\uc800\uadc0', '\ub9e5\uc8fc'],\n    ['\uc591\uc0c1\ucd94', '\ub9e5\uc8fc']\n])","ae52be40":"te = TransactionEncoder()\nte_ary = te.fit(data).transform(data)\ndf = pd.DataFrame(te_ary, columns=te.columns_)\ndf","929415fd":"%%time\nfrom mlxtend.frequent_patterns import apriori\n\napriori(df, min_support=0.5, use_colnames=True)","9d7e3b8b":"data = np.array([\n    ['\uc6b0\uc720', '\uae30\uc800\uadc0', '\uc96c\uc2a4'],\n    ['\uc591\uc0c1\ucd94', '\uae30\uc800\uadc0', '\ub9e5\uc8fc'],\n    ['\uc6b0\uc720', '\uc591\uc0c1\ucd94', '\uae30\uc800\uadc0', '\ub9e5\uc8fc'],\n    ['\uc591\uc0c1\ucd94', '\ub9e5\uc8fc']\n])","8427e630":"te = TransactionEncoder()\nte_ary = te.fit(data).transform(data)\ndf = pd.DataFrame(te_ary, columns=te.columns_)\ndf","24153a8b":"%%time\nfrom mlxtend.frequent_patterns import fpgrowth\n\nfpgrowth(df, min_support=0.5, use_colnames=True)","f119ad83":"docs = [\n  '\uba39\uace0 \uc2f6\uc740 \uc0ac\uacfc',\n  '\uba39\uace0 \uc2f6\uc740 \ubc14\ub098\ub098',\n  '\uae38\uace0 \ub178\ub780 \ubc14\ub098\ub098 \ubc14\ub098\ub098',\n  '\uc800\ub294 \uacfc\uc77c\uc774 \uc88b\uc544\uc694'\n] ","d82fb82b":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\ncountvect = vect.fit_transform(docs)\ncountvect_df = pd.DataFrame(countvect.toarray(), columns = sorted(vect.vocabulary_))\ncountvect_df.index = ['\ubb38\uc11c1', '\ubb38\uc11c2', '\ubb38\uc11c3', '\ubb38\uc11c4']\ncountvect_df","ed6b536f":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidv = TfidfVectorizer(use_idf=True, smooth_idf=False, norm=None).fit(docs)\ntfidv_df = pd.DataFrame(tfidv.transform(docs).toarray(), columns = sorted(tfidv.vocabulary_))\ntfidv_df","99e176aa":"from sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity(tfidv_df, tfidv_df)","e5d49f58":"from gensim.models import Word2Vec\n\ndocs = [\n  'you say goodbye and I say hello .'\n]\n\nsentences = [list(sentence.split(' ')) for sentence in docs]\nsentences","dc22e1c3":"model = Word2Vec(size=3, window=1, min_count=1, sg=1)\nmodel.build_vocab(sentences)\nmodel.wv.most_similar(\"say\")","5c68e262":"import surprise\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import cross_validate\nfrom surprise import Reader, Dataset, SVD, SVDpp, NMF, KNNBaseline\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import cross_validate\n\ndata = Dataset.load_builtin('ml-100k')\ndf = pd.DataFrame(data.raw_ratings, columns=[\"user\", \"item\", \"rate\", \"id\"])\ndf = df.astype(np.float32)\n\ndel df[\"id\"]\ndf.head(10)","c3e866db":"%%time \nreader = Reader(rating_scale=(1, 5))\nknndata = Dataset.load_from_df(df[['user', 'item', 'rate']], reader)\n\nsim_options = {'name': 'cosine'}\nknn = surprise.KNNBasic(sim_options=sim_options, k=20)\nscore = cross_validate(knn, knndata, measures=['RMSE'], cv=5, verbose=True)","b4bf88b1":"%%time \nuser = 196\n\nscore_dict = {}\nfor sim in knn.get_neighbors(user, k=20):\n    df_ = df[df['user'] == sim]\n    for item, rate in zip(df_['item'].values, df_['rate'].values):\n        if item not in df[df['user'] == user]['item'].values:\n            try:\n                score_dict[item] += rate\n            except:\n                score_dict[item] = rate","98c7edb5":"# \uc0c1\uc704 10\uac1c\uc758 \uc601\ud654\ub9cc \ucd94\ucc9c \ndict(sorted(score_dict.items(), key = lambda x: -x[1])[0:10]).keys()","874ebd84":"import tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\n\nimport numpy as np\n\n# Base code : https:\/\/yamalab.tistory.com\/92\nclass MatrixFactorization():\n    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n        \"\"\"\n        :param R: rating matrix\n        :param k: latent parameter\n        :param learning_rate: alpha on weight update\n        :param reg_param: beta on weight update\n        :param epochs: training epochs\n        :param verbose: print status\n        \"\"\"\n\n        self._R = R\n        self._num_users, self._num_items = R.shape\n        self._k = k\n        self._learning_rate = learning_rate\n        self._reg_param = reg_param\n        self._epochs = epochs\n        self._verbose = verbose\n\n\n    def fit(self):\n        \"\"\"\n        training Matrix Factorization : Update matrix latent weight and bias\n\n        \ucc38\uace0: self._b\uc5d0 \ub300\ud55c \uc124\uba85\n        - global bias: input R\uc5d0\uc11c \ud3c9\uac00\uac00 \ub9e4\uaca8\uc9c4 rating\uc758 \ud3c9\uade0\uac12\uc744 global bias\ub85c \uc0ac\uc6a9\n        - \uc815\uaddc\ud654 \uae30\ub2a5. \ucd5c\uc885 rating\uc5d0 \uc74c\uc218\uac00 \ub4e4\uc5b4\uac00\ub294 \uac83 \ub300\uc2e0 latent feature\uc5d0 \uc74c\uc218\uac00 \ud3ec\ud568\ub418\ub3c4\ub85d \ud574\uc90c.\n\n        :return: training_process\n        \"\"\"\n\n        # init latent features\n        self._P = np.random.normal(size=(self._num_users, self._k))\n        self._Q = np.random.normal(size=(self._num_items, self._k))\n\n        # init biases\n        self._b_P = np.zeros(self._num_users)\n        self._b_Q = np.zeros(self._num_items)\n        self._b = np.mean(self._R[np.where(self._R != 0)])\n\n        # train while epochs\n        self._training_process = []\n        for epoch in range(self._epochs):\n            # rating\uc774 \uc874\uc7ac\ud558\ub294 index\ub97c \uae30\uc900\uc73c\ub85c training\n            xi, yi = self._R.nonzero()\n            for i, j in zip(xi, yi):\n                self.gradient_descent(i, j, self._R[i, j])\n            cost = self.cost()\n            self._training_process.append((epoch, cost))\n\n            # print status\n            if self._verbose == True and ((epoch + 1) % 10 == 0):\n                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n\n\n    def cost(self):\n        \"\"\"\n        compute root mean square error\n        :return: rmse cost\n        \"\"\"\n\n        # xi, yi: R[xi, yi]\ub294 nonzero\uc778 value\ub97c \uc758\ubbf8\ud55c\ub2e4.\n        # \ucc38\uace0: http:\/\/codepractice.tistory.com\/90\n        xi, yi = self._R.nonzero()\n        # predicted = self.get_complete_matrix()\n        cost = 0\n        for x, y in zip(xi, yi):\n            cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n        return np.sqrt(cost\/len(xi))\n\n\n    def gradient(self, error, i, j):\n        \"\"\"\n        gradient of latent feature for GD\n\n        :param error: rating - prediction error\n        :param i: user index\n        :param j: item index\n        :return: gradient of latent feature tuple\n        \"\"\"\n\n        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n        return dp, dq\n\n\n    def gradient_descent(self, i, j, rating):\n        \"\"\"\n        graident descent function\n\n        :param i: user index of matrix\n        :param j: item index of matrix\n        :param rating: rating of (i,j)\n        \"\"\"\n\n        # get error\n        prediction = self.get_prediction(i, j)\n        error = rating - prediction\n\n        # update biases\n        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n        # update latent feature\n        dp, dq = self.gradient(error, i, j)\n        self._P[i, :] += self._learning_rate * dp\n        self._Q[j, :] += self._learning_rate * dq\n\n\n    def get_prediction(self, i, j):\n        \"\"\"\n        get predicted rating: user_i, item_j\n        :return: prediction of r_ij\n        \"\"\"\n        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n\n\n    def get_complete_matrix(self):\n        \"\"\"\n        computer complete matrix PXQ + P.bias + Q.bias + global bias\n\n        - PXQ \ud589\ub82c\uc5d0 b_P[:, np.newaxis]\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 \uc5f4\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n        - b_Q[np.newaxis:, ]\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 \ud589\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n        - b\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 element\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n\n        - newaxis: \ucc28\uc6d0\uc744 \ucd94\uac00\ud574\uc90c. 1\ucc28\uc6d0\uc778 Latent\ub4e4\ub85c 2\ucc28\uc6d0\uc758 R\uc5d0 \ud589\/\uc5f4 \ub2e8\uc704 \uc5f0\uc0b0\uc744 \ud574\uc8fc\uae30\uc704\ud574 \ucc28\uc6d0\uc744 \ucd94\uac00\ud558\ub294 \uac83.\n\n        :return: complete matrix R^\n        \"\"\"\n        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n\n\n\n# run example\nif __name__ == \"__main__\":\n    # rating matrix - User X Item : (7 X 5)\n    R = np.array([\n        [1, 0, 0, 1, 3],\n        [2, 0, 3, 1, 1],\n        [1, 2, 0, 5, 0],\n        [1, 0, 0, 4, 4],\n        [2, 1, 5, 4, 0],\n        [5, 1, 5, 4, 0],\n        [0, 0, 0, 1, 0],\n    ])\n\n    # P, Q is (7 X k), (k X 5) matrix\n    ","6334653f":"%%time\nfactorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\nfactorizer.fit()","31bf661b":"factorizer.get_complete_matrix()","5576cc3a":"from implicit.evaluation import *\nfrom implicit.als import AlternatingLeastSquares as ALS","a59645a1":"# Implicit data\n# \uc608\uc2dc\ub97c \uc704\ud574\uc11c rate\uc758 \uac12\uc744 1\ub85c \ubcc0\uacbd\ud574\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \ndf['rate'] = 1","9cdb5536":"user2idx = {}\nfor i, l in enumerate(df['user'].unique()):\n    user2idx[l] = i\n    \nmovie2idx = {}\nfor i, l in enumerate(df['item'].unique()):\n    movie2idx[l] = i","66601e81":"idx2user = {i: user for user, i in user2idx.items()}\nidx2movie = {i: item for item, i in movie2idx.items()}","a7c701e5":"useridx = df['useridx'] = df['user'].apply(lambda x: user2idx[x]).values\nmovieidx = df['movieidx'] = df['item'].apply(lambda x: movie2idx[x]).values\nrating = df['rate'].values","d957bdb3":"import scipy\n\npurchase_sparse = scipy.sparse.csr_matrix((rating, (useridx, movieidx)), shape=(len(set(useridx)), len(set(movieidx))))","d2f3fb28":"als_model = ALS(factors=20, regularization=0.08, iterations = 20)\nals_model.fit(purchase_sparse.T)","478f59a0":"als_model.recommend(0, purchase_sparse, N=150)[0:10]","b9cd3257":"## 2. FP-Growth \uc54c\uace0\ub9ac\uc998","001e9aea":"## 1. Apriori \uc54c\uace0\ub9ac\uc998","98e726c5":"\ud574\ub2f9 \ucf54\ub4dc\ub294 https:\/\/yamalab.tistory.com\/92 \uc5d0 \uc788\ub294 Y.LAB\uc758 \ube14\ub85c\uadf8 \uae00\uc744 \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4. (\ub300\ubd80\ubd84\uc758 \ucf54\ub4dc\uac00 \uac19\uace0 \uc911\uac04\uc5d0 \ud55c \ubd80\ubd84\ub9cc \uc218\uc815\ud588\uc2b5\ub2c8\ub2e4.)","3581b3c9":"## 4. Word2Vec \uc54c\uace0\ub9ac\uc998","4113c800":"## Introduction\n\ud574\ub2f9 \uc96c\ud53c\ud130\ud30c\uc77c\uc5d0\uc11c\ub294 \uae30\uc874\uc758 \uc774\ub860\uc5d0\uc11c \uacf5\ubd80\ud588\ub358 \ubc29\ubc95\ub860\ub4e4\uc5d0 \ub300\ud574 \uc0b4\ud3b4\ubcf4\ub294 \uc2dc\uac04\uc744 \uac00\uc838\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \n- Apriori Algorithm \n- FP Growth \n- TF-IDF\n- Word2Vec\n- KNN Neareast Algorithm \n- SGD\n- ALS ","009ad2d7":"## 3. TF-IDF \uc54c\uace0\ub9ac\uc998","e40e9d8e":"## 7. ALS \uc54c\uace0\ub9ac\uc998","80004884":"## 6. SGD \uc54c\uace0\ub9ac\uc998","0aa3f107":"## 5. KNN \uc54c\uace0\ub9ac\uc998"}}