{"cell_type":{"a20ea096":"code","956f27b4":"code","24046cd3":"code","f8f09ba7":"code","f5acc02e":"code","3b733728":"code","a4f7f27d":"code","ab4ce6ee":"code","9820644d":"code","23367998":"code","583c4139":"code","567ea29b":"code","9f26631c":"code","a008b9a0":"code","72236074":"code","e0a6d272":"code","019eb92a":"code","bb510387":"code","52a65f76":"code","d3f16547":"code","f35e198b":"code","dc87c293":"code","7aa965d1":"code","e832276c":"code","aed33118":"code","8260d926":"code","e3f8276f":"code","442cff5b":"code","ada28e92":"code","1ce7570a":"code","24b0197f":"code","b0e72aa6":"code","73cecfd2":"code","13e624fa":"code","e6df0651":"code","b7c6e13a":"code","f554933c":"code","48dad6a2":"code","eb67770f":"code","83e3976e":"code","0e95785e":"code","a7084c11":"code","f7757744":"code","c8a030ff":"code","a25438b9":"code","0bac65ea":"code","ce7cbd1f":"code","28ac5200":"code","bb734025":"code","f7cc78d8":"code","7de4afd7":"code","0d1dad78":"code","e6da6467":"code","c8c35c5f":"code","41a00b64":"code","7a3dff91":"code","2c1a9552":"code","d2f50439":"code","0cf7860f":"code","aa6b8771":"code","9bf580bd":"code","f6ea57a8":"code","c729466d":"code","ff01ebaf":"markdown","7a6a5cbd":"markdown","ac942c54":"markdown","63525066":"markdown","0317742c":"markdown","82403af1":"markdown","e8ff4c59":"markdown","7f3ea23c":"markdown","7212c119":"markdown","8376dd4f":"markdown","19cbfe4d":"markdown","ef7c2365":"markdown","df7c3c17":"markdown","9b66f5ff":"markdown","aff2af86":"markdown","4730705b":"markdown","5f390b97":"markdown","5717b2ef":"markdown","2ba9ce3f":"markdown","b1cf2b61":"markdown","7d5e3df1":"markdown","f08132ca":"markdown","55d56f8e":"markdown","849be9c2":"markdown","95d39959":"markdown","a319574e":"markdown","58bd022d":"markdown","18e8c41d":"markdown","42e94524":"markdown","eabf2a03":"markdown","c1efd72d":"markdown","19e04c45":"markdown","d70bbcfb":"markdown","34bc66f8":"markdown","2f64547e":"markdown","51196ed0":"markdown","1bf022c0":"markdown"},"source":{"a20ea096":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    ","956f27b4":"#Let's load our data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","24046cd3":"train.shape, test.shape","f8f09ba7":"train.head()","f5acc02e":"test.head()","3b733728":"train.count()","a4f7f27d":"train.dtypes","ab4ce6ee":"train.describe(include = \"all\")","9820644d":"train.info","23367998":"train.isnull()","583c4139":"#To count how many missing values are there in the dataset in row as well as column\ntrain.isnull().sum().sum()","567ea29b":"test.isnull().sum().sum()","9f26631c":"train.isnull().sum()","a008b9a0":"#Split the train into x_train and y_train so that SalePrice can be kept separate for training later\ny = train.SalePrice\nX = train.drop(columns=[\"SalePrice\"], axis=1)","72236074":"y.shape, X.shape, test.shape","e0a6d272":"X['Type'] = 'train'\ntest['Type'] = 'test'\n#test['SalePrice'] = -1\ndata = X.append(test)","019eb92a":"data.isnull().sum().sum()","bb510387":"columns_having_null_values = data[data.columns[data.isnull().sum()>0]]\ncolumns_having_null_values","52a65f76":"# We have to check what values are there in the table so that we can fill values according to real world scenario.\ndata['Electrical'].value_counts()","d3f16547":"data['Electrical'].fillna(\"Sbrkr\", inplace=True)","f35e198b":"data['MSZoning'].value_counts()\n#Filling null values with 'RL'\ndata['MSZoning'].fillna(\"RL\",inplace=True)\n\n#Filling nul values with mean\ndata['LotFrontage'].fillna(data['LotFrontage'].mean(), inplace=True)\n\ndata['Alley'].fillna(\"Nothing\", inplace=True)\ndata['Utilities'].fillna(\"AllPub\", inplace=True)\ndata['Exterior1st'].fillna(\"VinylSd\", inplace=True)\ndata['Exterior2nd'].fillna(\"VinylSd\", inplace=True)\ndata['MasVnrArea'].fillna(0, inplace=True)\ndata['MasVnrType'].fillna(\"None\", inplace=True)\ndata['BsmtCond'].fillna(\"No\", inplace=True)\ndata['BsmtExposure'].fillna(\"NB\", inplace=True)\ndata['BsmtFinType1'].fillna(\"NB\", inplace=True)\ndata['BsmtFinSF1'].fillna(0.0, inplace=True)\ndata['BsmtFinSF2'].fillna(0.0, inplace=True)\ndata['BsmtUnfSF'].fillna(0.0, inplace=True)\ndata['TotalBsmtSF'].fillna(0.0, inplace=True)\ndata['BsmtFullBath'].fillna(0.0, inplace=True)\ndata['BsmtHalfBath'].fillna(0.0, inplace=True)\ndata['KitchenQual'].fillna(\"TA\", inplace=True)\ndata['Functional'].fillna(\"Typ\", inplace=True)\ndata['FireplaceQu'].fillna(\"None\", inplace=True)\ndata['GarageType'].fillna(\"No\", inplace=True)\ndata['GarageYrBlt'].fillna(0, inplace=True)\ndata['GarageFinish'].fillna(\"No\", inplace=True)\ndata['GarageCars'].fillna(0, inplace=True)\ndata['GarageArea'].fillna(0, inplace=True)\ndata['GarageQual'].fillna(\"No\", inplace=True)\ndata['GarageCond'].fillna(\"No\", inplace=True)\ndata['PoolQC'].fillna(\"No\", inplace=True)\ndata['Fence'].fillna(\"No\", inplace=True)\ndata['MiscFeature'].fillna(\"No\", inplace=True)\ndata['SaleType'].fillna(\"Con\", inplace=True)\ndata['SaleCondition'].fillna(\"None\", inplace=True)\ndata['BsmtQual'].fillna(\"TA\", inplace=True)\ndata['BsmtFinType2'].fillna(\"Unf\", inplace=True)","dc87c293":"data.isnull().sum().sum()","7aa965d1":"int_columns = data[data.columns[data.dtypes=='int']]\nint_columns.columns","e832276c":"data['MSZoning'].unique()","aed33118":"object_columnns = data[data.columns[data.dtypes=='object']]\nobject_columnns.columns","8260d926":"float_columns = data[data.columns[data.dtypes=='float']]\nfloat_columns.columns","e3f8276f":"data.var()","442cff5b":"corr_matrix = data.corr()\ncorr_matrix","ada28e92":"upper_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\nupper_matrix","1ce7570a":"#Dropping columns with high correlation\ndrop_columns = [col for col in upper_matrix.columns if any(upper_matrix[col] > 0.85)]\ndrop_columns","24b0197f":"data.drop(data[drop_columns], axis=1, inplace=True)\ndata.head()","b0e72aa6":"from sklearn.preprocessing import LabelEncoder\nfor i in object_columnns:\n    label = LabelEncoder()\n    label.fit(data[i].values)\n    data[i] = label.transform(data[i].values)","73cecfd2":"object_columnns = data[data.columns[data.dtypes=='object']]\nobject_columnns.columns","13e624fa":"int_columns = data[data.columns[data.dtypes=='int']]\nint_columns.columns","e6df0651":"data.head()","b7c6e13a":"X_ = data[data.Type==1]\nX_ = X_.drop([\"Type\"], axis=1)\n\ntest_ = data[data.Type==0]\ntest_ = test_.drop([\"Type\"], axis=1)","f554933c":"X_.shape, y.shape, test_.shape ","48dad6a2":"from sklearn import preprocessing\nnames = X_.columns\nprepro = preprocessing.normalize(X_)\nX_scaled = pd.DataFrame(prepro, columns=names)","eb67770f":"X_scaled.head()","83e3976e":"#from sklearn.preprocessing import MinMaxScaler\n#minmaxscaler = MinMaxScaler()\n#x_scaled = minmaxscaler.fit_transform(X_)","0e95785e":"#We can do Scaling directly with formula shown below but we have pre-defined libraries so we will use them.. \n#x_scaled_formula = X_.copy()\n#for cols in x_scaled_formula.columns:\n#    x_scaled_formula[cols] = x_scaled_formula[cols] \/ x_scaled_formula[cols].abs().max()","a7084c11":"#x_scaled_formula.head()","f7757744":"#Scatterplot\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set()\ncols = ['OverallQual', 'TotalBsmtSF', 'YearBuilt']\nsns.pairplot(X_[cols], size = 2.5)\nplt.show();\n","c8a030ff":"#Correlation matrix\n\ncorrmatrix = X_.corr()\nf, ax = plt.subplots(figsize=(15, 9))\nsns.heatmap(corrmatrix, vmax=.8, square=True);","a25438b9":"#Creating lists to collect all the model names and their scores together\n\nscore_test = []\n#score_train = []\nmodel = []\n","0bac65ea":"from sklearn.model_selection import train_test_split\nx_train_90, x_test_10, y_train_90, y_test_10 = train_test_split(X_, y, test_size=0.10, random_state=1)\n\nx_train_75, x_test_25, y_train_75, y_test_25 = train_test_split(X_, y, test_size=0.25, random_state=1)","ce7cbd1f":"from sklearn.model_selection import train_test_split\nx_train_scaled_90, x_test_scaled_10, y_train_scaled_90, y_test_scaled_10 = train_test_split(X_scaled, y, test_size=0.10, random_state=1)\n\nx_train_scaled_75, x_test_scaled_25, y_train_scaled_75, y_test_scaled_25 = train_test_split(X_scaled, y, test_size=0.25, random_state=1)","28ac5200":"from sklearn.ensemble import RandomForestRegressor\nmodel_randomforest_train90 = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=13)\nmodel_randomforest_train90.fit(x_train_90, y_train_90)\n\nmodel_randomforest_train75 = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=13)\nmodel_randomforest_train75.fit(x_train_75, y_train_75)\n\nmodel_randomforest_scaled_train90 = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=13)\nmodel_randomforest_scaled_train90.fit(x_train_scaled_90, y_train_scaled_90)\n\nmodel_randomforest_scaled_train75 = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=13)\nmodel_randomforest_scaled_train75.fit(x_train_scaled_75, y_train_scaled_75)","bb734025":"score_test.append(model_randomforest_train90.score(x_test_10, y_test_10))\nmodel.append(\"model_randomforest_train90\")\n\nscore_test.append(model_randomforest_train75.score(x_test_25, y_test_25))\nmodel.append(\"model_randomforest_train75\")\n\nscore_test.append(model_randomforest_scaled_train90.score(x_test_scaled_10, y_test_scaled_10))\nmodel.append(\"model_randomforest_scaled_train90\")\n\nscore_test.append(model_randomforest_scaled_train75.score(x_test_scaled_25, y_test_scaled_25))\nmodel.append(\"model_randomforest_scaled_train75\")","f7cc78d8":"import xgboost as xgb\nmodel_xgboost_train90 = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n \nmodel_xgboost_train90.fit(x_train_90, y_train_90)\n\nmodel_xgboost_train75 = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n \nmodel_xgboost_train75.fit(x_train_75, y_train_75)\n\nmodel_xgboost_scaled_train90 = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n \nmodel_xgboost_scaled_train90.fit(x_train_scaled_90, y_train_scaled_90)\n\nmodel_xgboost_scaled_train75 = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n \nmodel_xgboost_scaled_train75.fit(x_train_scaled_75, y_train_scaled_75)","7de4afd7":"#score_train.append(model_xgboost.score(x_train, y_train))\n#model.append(\"model_xgboost\")","0d1dad78":"score_test.append(model_xgboost_train90.score(x_test_10, y_test_10))\nmodel.append(\"model_xgboost_train90\")\n\nscore_test.append(model_xgboost_train75.score(x_test_25, y_test_25))\nmodel.append(\"model_xgboost_train75\")\n\nscore_test.append(model_xgboost_scaled_train90.score(x_test_scaled_10, y_test_scaled_10))\nmodel.append(\"model_xgboost_scaled_train90\")\n\nscore_test.append(model_xgboost_scaled_train75.score(x_test_scaled_25, y_test_scaled_25))\nmodel.append(\"model_xgboost_scaled_train75\")","e6da6467":"from sklearn.tree import DecisionTreeRegressor\nmodel_decisiontree_train90 = DecisionTreeRegressor(random_state=0)\nmodel_decisiontree_train90.fit(x_train_90, y_train_90)\n\nmodel_decisiontree_train75 = DecisionTreeRegressor(random_state=0)\nmodel_decisiontree_train75.fit(x_train_75, y_train_75)\n\nmodel_decisiontree_scaled_train90 = DecisionTreeRegressor(random_state=0)\nmodel_decisiontree_scaled_train90.fit(x_train_scaled_90, y_train_scaled_90)\n\nmodel_decisiontree_scaled_train75 = DecisionTreeRegressor(random_state=0)\nmodel_decisiontree_scaled_train75.fit(x_train_scaled_75, y_train_scaled_75)","c8c35c5f":"score_test.append(model_decisiontree_train90.score(x_test_10, y_test_10))\nmodel.append(\"model_decisiontree_train90\")\n\nscore_test.append(model_decisiontree_train75.score(x_test_25, y_test_25))\nmodel.append(\"model_decisiontree_train75\")\n\nscore_test.append(model_decisiontree_scaled_train90.score(x_test_10, y_test_10))\nmodel.append(\"model_decisiontree_scaled_train90\")\n\nscore_test.append(model_decisiontree_scaled_train75.score(x_test_25, y_test_25))\nmodel.append(\"model_decisiontree_scaled_train75\")","41a00b64":"from sklearn.linear_model import Lasso\nmodel_lasso_train90 = Lasso(alpha=0.0005)\nmodel_lasso_train90.fit(x_train_90, y_train_90)\n\nmodel_lasso_train75 = Lasso(alpha=0.0005)\nmodel_lasso_train75.fit(x_train_75, y_train_75)","7a3dff91":"score_test.append(model_lasso_train90.score(x_test_10, y_test_10))\nmodel.append(\"model_lasso_train90\")\n\nscore_test.append(model_lasso_train90.score(x_test_10, y_test_10))\nmodel.append(\"model_lasso_train90\")","2c1a9552":"final_scores = pd.DataFrame()\nfinal_scores['model_name'] = model\nfinal_scores['score_test'] = score_test\nfinal_scores","d2f50439":"best_index = score_test.index(max(score_test))\nbest_model = final_scores['model_name'][best_index]\nbest_model","0cf7860f":"y_predict_best = model_xgboost_train90.predict(test_)","aa6b8771":"#Working on this statement\n#y_predict_bestmodel = best_model.name.predict(test_)","9bf580bd":"result = pd.DataFrame()\nresult['Id'] = test['Id']\nresult['SalePrice'] = y_predict_best","f6ea57a8":"result.head()","c729466d":"result.to_csv('submission.csv', index=False)","ff01ebaf":"##### Now let's see what is the number of null values.","7a6a5cbd":"### *  Random Forest Regressor","ac942c54":"### * XGBoost","63525066":"> ## Label Encoding the categorical variables","0317742c":"##### I can see there are too many True values, hence missing values.","82403af1":"##### As we know all the diagonal elements will be 1 so let's take the upper triamgular matrix","e8ff4c59":"##### There are int, float and object data types.","7f3ea23c":"> ### Submission","7212c119":"**##### Oh damn so there are 6965 and 7000 null values**\n##### Let's see null values in each column","8376dd4f":"**##### So now, 6965 + 7000 = 13965 null values are there in total**\n##### Now as said above, let's find out specific columns which are having null values.","19cbfe4d":"##### Let's deal with different types of data types in the dataset","ef7c2365":"### Let's find out which model scored the best","df7c3c17":"**##### Now we have a figure that 34 columns have null values out of 81 columns. This made our task much easier than before.**","9b66f5ff":"##### So now we can see that all the object columns are turned to int\n#### Let's split back the train and test data ","aff2af86":"##### Ok so there are total of 81 columns, that's hell lot of variables.","4730705b":"##### As there are 81 columns, it's difficult to display all the columns, let's try to display only those columns which have null values.","5f390b97":"**Hi all, this is my first Kaggle notebook**","5717b2ef":"## Now comes the most tidius part to deal with missing values","2ba9ce3f":"> ## Identifying and dealing with Missing values","b1cf2b61":"> ## Hola, we have treated all the null values.","7d5e3df1":"## Scaling\n##### It is required because dataset has columns which varies highly in magnitudes. If scaling is not performed then high magnitude values will have more impact on modelling.","f08132ca":"> ## Starting preliminary analysis of data","55d56f8e":"> ## Data preprocessing","849be9c2":"### Some nomenclatures used in the notebook:\n\n#### train - train data from Kaggle\n#### test - test data from Kaggle\n#### X - independent variables (columns) from train data\n#### y - dependent variable (column) from train data\n#### data - combination of train and test data\n#### X_ - train data after treating missing values\n#### test_ - test data after treating missing values\n#### X_scaled - scaled data from X_\n#### x_train_90, x_test_10, y_train_90, y_test_10 - train test split data with test_size=0.10 from X_\n#### x_train_75, x_test_25, y_train_75, y_test_25 - train test split data with test_size=0.25 from X_\n#### x_train_scaled_90, x_test_scaled_10, y_train_scaled_90, y_test_scaled_10 - train test split where test_size=0.10 data from X_scaled\n#### x_train_scaled_75, x_test_scaled_25, y_train_scaled_75, y_test_scaled_25 - train test split where test_size=0.25 data from X_scaled\n#### score_test - holding scores of all algorithms used on x_test, y_test data\n#### model - name of models\/algorithms\n#### best_model - name of the best model\n#### y_predict_test - submission data","95d39959":"##### All columns except LotArea have equal amount of rows which means there are missing values.","a319574e":"##### In the above values, we can see that \"Sbrkr\" is the mostly used 'Electrical' part. Here we can't put \"None\" in the null values because a house must have \"Electrical\" items\/fuses. So we will fill null values with \"Sbrkr\" in this column.","58bd022d":"### Now before proceeding forward, let's append train and test data so that we can deal with them together.\n##### Before appending them, adding a column named 'type' to distinguish between train and  test data.","18e8c41d":"#### Let's do the train_test_split first","42e94524":"> ## Modelling aka ML\n##### There are so many regression algorithms which we can use, so we need to use most of them and then find out the best out of them.","eabf2a03":"### * Decision Tree","c1efd72d":"### * LASSO ","19e04c45":"##### Now we have to do this task for each columns with null\/nan values (that's why I mentioned it as a tidius part).","d70bbcfb":"### Collect all the models and scores together","34bc66f8":"> ## Data visualization","2f64547e":"### In this version 28, I am enhancing the train_test_split to test the results.","51196ed0":"#### This gave us small gist about the dataset","1bf022c0":"### And now predict the test data with best model"}}