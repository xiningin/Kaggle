{"cell_type":{"b9e1c869":"code","a5c27fa6":"code","c2988efe":"code","ad1f1c65":"code","34e1948b":"code","4b49353a":"code","558579b6":"code","a90125a1":"code","dccf479c":"code","d468f810":"code","50d47848":"code","d8768bd8":"code","2d1bd1b7":"code","c3346eb0":"code","d377297f":"code","86ad617f":"code","48351d29":"code","bf4e1ace":"code","567f931f":"code","107966f2":"code","c2d01ed0":"markdown","03139032":"markdown","591ff025":"markdown","c01f9c9a":"markdown","15b1d500":"markdown","093caddf":"markdown"},"source":{"b9e1c869":"import pandas as pd \nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder #encoding variables\nfrom sklearn.metrics import accuracy_score #accuracy scores\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#TPOT and GBC\nfrom tpot import TPOTClassifier","a5c27fa6":"asteroid = pd.read_csv(\"..\/input\/asteroid-impacts\/orbits - orbits.csv\")\nasteroid.head(5)","c2988efe":"asteroid.columns","ad1f1c65":"a = asteroid.dropna()\na.columns","34e1948b":"features =a.columns.to_list()\nfeatures","4b49353a":"features.remove(\"Hazardous\")\nfeatures","558579b6":"ord_encoder=OrdinalEncoder()\ndata_encoded=ord_encoder.fit_transform(a[features])\ndata_encoded","a90125a1":"a_new = pd.DataFrame(data_encoded, columns=features)\na_new","dccf479c":"a_new['Classification'].nunique()","d468f810":"encoder= LabelEncoder()\ntarget_encoded=encoder.fit_transform(a['Hazardous'])\na_new['Hazardous']=target_encoded","50d47848":"a_new.head()","d8768bd8":"x=a_new.drop('Hazardous',axis=1)\ny=a_new['Hazardous']","2d1bd1b7":"x_train,x_test, y_train,y_test = train_test_split(x,y,train_size=0.75,test_size=0.25,random_state=40)","c3346eb0":"tpot = TPOTClassifier(generations=4, population_size=50,  max_time_mins=5, verbosity=2,random_state=40 ) #max eval time can also be added as a paramet for the same \ntpot.fit(x_train, y_train)","d377297f":"print(tpot.score(x_test,y_test))\ntpot.export('tpot_asteroid_pipeline.py')","86ad617f":"# %load tpot_asteroid_pipeline.py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH\/TO\/DATA\/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=40)\n\n# Average CV score on the training set was: 0.9999147121535181\nexported_pipeline = DecisionTreeClassifier(criterion=\"entropy\", max_depth=6, min_samples_leaf=19, min_samples_split=3)\n# Fix random state in exported estimator\nif hasattr(exported_pipeline, 'random_state'):\n    setattr(exported_pipeline, 'random_state', 40)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n","48351d29":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom tpot.export_utils import set_param_recursive","bf4e1ace":"exported_pipeline = make_pipeline(\n    RFE(estimator=ExtraTreesClassifier(criterion=\"gini\", max_features=0.25, n_estimators=100), step=0.9500000000000001),\n    GradientBoostingClassifier(learning_rate=1.0, max_depth=4, max_features=0.7000000000000001, min_samples_leaf=11, min_samples_split=11, n_estimators=100, subsample=0.5)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 40)\n\nexported_pipeline.fit(x_train, y_train)\nresults = exported_pipeline.predict(x_test)\n","567f931f":"results","107966f2":"acc = accuracy_score(y_test,results)\nacc\nprint(\"Accuracy of TPOT Devised Pipeline is:\",acc*100)","c2d01ed0":"TPOT, Tree Based Pipeline Optimization Tool is an automated approach to hyperparameter training for optimizing model performance. It uses efficient Pareto Front scores for selecting the best possible solution given search space. Here it suggest using Ensemble modelling, boosting in particular, to come to an optimized conclusion. It provides a great solution for automating fine-tuning of hyper parameters and takes into considration nature of data","03139032":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","591ff025":"**** TPOT Documentation: http:\/\/epistasislab.github.io\/tpot\/****","c01f9c9a":"Thus this model is good because, the training accuracy is less than the testing accuracy. Since data is parameterized on training data, training accuracy should be higher. Testing accuracy than should be lower than train accuracy.","15b1d500":"An important TPOT parameter to set is the number of generations. Since our aim is to just illustrate the use of TPOT, we have set maximum optimization time to 2 minutes (max_time_mins=2). On a standard laptop with 4GB RAM, it roughly takes 5 minutes per generation to run. For each added generation, it should take 5 mins more. Thus, for the default value of 100, total run time could be roughly around 8 hours.","093caddf":"# # Genetic Programming using TPOT"}}