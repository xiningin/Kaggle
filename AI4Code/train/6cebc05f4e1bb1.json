{"cell_type":{"af3c3798":"code","ed6390fc":"code","b3d901b0":"code","3ecb12b1":"code","a519c148":"code","96bbd072":"code","303f7d29":"code","e5187687":"code","72dc7eb0":"code","a36996b7":"code","5f437ee0":"code","f2aec49b":"code","4ff3ae75":"code","0cfb4e63":"code","be74b77a":"code","94b7294d":"code","16550093":"code","45cafa7d":"code","87ba32a4":"code","17b890e5":"code","39dabd0d":"code","e64e4e22":"code","9377942a":"code","4590ede2":"code","5746d95a":"code","79472d68":"code","a18a0699":"code","5e9b2289":"code","93b2b153":"code","f2d96ce7":"code","7cc04e18":"code","e39165cd":"code","de9922c3":"code","aa8ea6f1":"code","eeb70a29":"code","09d24f74":"code","72528418":"code","b21f5124":"code","d81fc437":"code","ccd971de":"code","a4027942":"code","3c5154ad":"code","b1010b0f":"code","000cd189":"code","340a038f":"code","62c6131b":"code","8bf55864":"markdown","05dd51b6":"markdown","60216d91":"markdown","72b99e33":"markdown","1d5cd7fb":"markdown","9b3147b2":"markdown","baf69c8e":"markdown","46f037db":"markdown","ed22a74c":"markdown","801073d9":"markdown","c0b84d5c":"markdown","1fc83cbb":"markdown","97b48c72":"markdown","e0b319cf":"markdown","0c1ca2b5":"markdown","4900769a":"markdown","f957a562":"markdown","646fd5ca":"markdown","38e1b3ba":"markdown","349308d6":"markdown","5ad7c93e":"markdown","71d1dd6e":"markdown","5d074ce9":"markdown","b66dafc9":"markdown","d1762716":"markdown","21ae91ab":"markdown","95f325cc":"markdown","a8935c3a":"markdown","07b6b389":"markdown","c241d344":"markdown","6b7ac748":"markdown","12d89e0d":"markdown","77913740":"markdown","3ef9eee6":"markdown","dd98b791":"markdown","9ef81adc":"markdown","e28df265":"markdown"},"source":{"af3c3798":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed6390fc":"telco_base_data = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","b3d901b0":"telco_base_data.head()","3ecb12b1":"telco_base_data.shape #(rows, columns)","a519c148":"telco_base_data.columns.values","96bbd072":"# Checking the data types of all the columns\ntelco_base_data.dtypes","303f7d29":"# Check the descriptive statistics of numeric variables\ntelco_base_data.describe()","e5187687":"telco_base_data['Churn'].value_counts().plot(kind='barh', figsize=(8,6))\nplt.xlabel(\"Count\", labelpad=14)\nplt.ylabel(\"Target Variable\", labelpad=14)\nplt.title(\"Count of Target Variable per category\", y=1.02);","72dc7eb0":"telco_base_data['Churn'].value_counts()","a36996b7":"100*telco_base_data['Churn'].value_counts()\/len(telco_base_data['Churn'])\n# No: Non-churners\n# Yes: Churners","5f437ee0":"# Concise Summary of the dataframe, as we have too many columns, we are using the verbose = True mode\n# give information of non-null count\ntelco_base_data.info(verbose = True)","f2aec49b":"missing = pd.DataFrame((telco_base_data.isnull().sum())*100\/telco_base_data.shape[0]).reset_index()\nplt.figure(figsize=(16,5))\nax = sns.pointplot('index',0,data=missing)\nplt.xticks(rotation = 90, fontsize=7)\nplt.title(\"Percentage of Missing values\")\nplt.ylabel(\"PERCENTAGE\")\nplt.show()","4ff3ae75":"telco_data = telco_base_data.copy()","0cfb4e63":"telco_data.TotalCharges = pd.to_numeric(telco_data.TotalCharges, errors = 'coerce')\ntelco_data.isnull().sum() \n# Isnull: How many values are null? and Return Boolean objects","be74b77a":"telco_data.loc[telco_data['TotalCharges'].isnull() == True]","94b7294d":"#Removing missing values\ntelco_data.dropna(how = 'any', inplace = True)\n\n#telco_data.fillna(0)","16550093":"# Get the max tenure\nprint(telco_data['tenure'].max()) #72","45cafa7d":"# Group the tenure in bins of 12 months\nlabels = [\"{0}-{1}\".format(i, i+11) for i in range(1,72,12)]\n\ntelco_data['tenure_group'] = pd.cut(telco_data.tenure, range(1, 80, 12), right=False, labels=labels)\ntelco_data['tenure_group'].value_counts()","87ba32a4":"#drop column customerID and tenure\ntelco_data.drop(columns= ['customerID', 'tenure'],axis=1, inplace=True)\ntelco_data.head()","17b890e5":"for i, predictor in enumerate(telco_data.drop(columns=['Churn', 'TotalCharges', 'MonthlyCharges'])):\n    plt.figure(i)\n    sns.countplot(data=telco_data, x=predictor, hue='Churn')","39dabd0d":"telco_data['Churn'] = np.where(telco_data.Churn == 'Yes', 1, 0)\ntelco_data.head()","e64e4e22":"telco_data_dummies = pd.get_dummies(telco_data)\ntelco_data_dummies.head()","9377942a":"sns.lmplot(data=telco_data_dummies, x='MonthlyCharges', y='TotalCharges', fit_reg=False)","4590ede2":"Mth = sns.kdeplot(telco_data_dummies.MonthlyCharges[(telco_data_dummies[\"Churn\"]==0)], \n                                                   color=\"Red\", shade=True)\nMth = sns.kdeplot(telco_data_dummies.MonthlyCharges[(telco_data_dummies[\"Churn\"]==1)],\n                                                   ax=Mth, color=\"Blue\", shade=True)\nMth.legend([\"No Churn\", \"Churn\"], loc='upper right')\nMth.set_ylabel('Density')\nMth.set_xlabel('Monthly Charges')\nMth.set_title('Monthly charges by churn')","5746d95a":"Tot = sns.kdeplot(telco_data_dummies.TotalCharges[(telco_data_dummies[\"Churn\"] == 0) ],\n                color=\"Red\", shade = True)\nTot = sns.kdeplot(telco_data_dummies.TotalCharges[(telco_data_dummies[\"Churn\"] == 1) ],\n                ax =Tot, color=\"Blue\", shade= True)\nTot.legend([\"No Churn\",\"Churn\"],loc='upper right')\nTot.set_ylabel('Density')\nTot.set_xlabel('Total Charges')\nTot.set_title('Total charges by churn')","79472d68":"plt.figure(figsize=(20,8))\ntelco_data_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')","a18a0699":"plt.figure(figsize=(12,12))\nsns.heatmap(telco_data_dummies.corr(), cmap=\"Paired\")","5e9b2289":"new_df1_target0=telco_data.loc[telco_data[\"Churn\"]==0]\nnew_df1_target1=telco_data.loc[telco_data[\"Churn\"]==1]","93b2b153":"def uniplot(df,col,title,hue =None):\n    \n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n    plt.rcParams[\"axes.labelsize\"] = 20\n    plt.rcParams['axes.titlesize'] = 22\n    plt.rcParams['axes.titlepad'] = 30\n    \n    \n    temp = pd.Series(data = hue)\n    fig, ax = plt.subplots()\n    width = len(df[col].unique()) + 7 + 4*len(temp.unique())\n    fig.set_size_inches(width , 8)\n    plt.xticks(rotation=45)\n    plt.yscale('log')\n    plt.title(title)\n    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,palette='bright') \n        \n    plt.show()","f2d96ce7":"uniplot(new_df1_target1,col='Partner',title='Distribution of Gender for Churned Customers',hue='gender')","7cc04e18":"uniplot(new_df1_target0,col='Partner',title='Distribution of Gender for Non Churned Customers',hue='gender')","e39165cd":"uniplot(new_df1_target1,col='PaymentMethod',title='Distribution of PaymentMethod for Churned Customers',hue='gender')","de9922c3":"uniplot(new_df1_target1,col='Contract',title='Distribution of Contract for Churned Customers',hue='gender')","aa8ea6f1":"uniplot(new_df1_target1,col='TechSupport',title='Distribution of TechSupport for Churned Customers',hue='gender')","eeb70a29":"uniplot(new_df1_target1,col='SeniorCitizen',title='Distribution of SeniorCitizen for Churned Customers',hue='gender')","09d24f74":"# We will use the data frame where we had created dummy variables\ny = telco_data_dummies['Churn'].values\nX = telco_data_dummies.drop(columns = ['Churn'])\n\n# Scaling all the variables to a range of 0 to 1\nfrom sklearn.preprocessing import MinMaxScaler\nfeatures = X.columns.values\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features","72528418":"# Create Train & Test Data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","b21f5124":"# Running logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nresult = model.fit(X_train, y_train)","d81fc437":"from sklearn import metrics\nprediction_test = model.predict(X_test)\n# Print the prediction accuracy\nprint (metrics.accuracy_score(y_test, prediction_test))","ccd971de":"# To get the weights of all the variables\nweights = pd.Series(model.coef_[0],\n                 index=X.columns.values)\nprint (weights.sort_values(ascending = False)[:10].plot(kind='bar'))","a4027942":"print(weights.sort_values(ascending = False)[-10:].plot(kind='bar'))","3c5154ad":"from sklearn.ensemble import RandomForestClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\nmodel_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,\n                                  random_state =50, max_features = \"auto\",\n                                  max_leaf_nodes = 30)\nmodel_rf.fit(X_train, y_train)\n\n# Make predictions\nprediction_test = model_rf.predict(X_test)\nprint (metrics.accuracy_score(y_test, prediction_test))","b1010b0f":"importances = model_rf.feature_importances_\nweights = pd.Series(importances,\n                 index=X.columns.values)\nweights.sort_values()[-10:].plot(kind = 'barh')","000cd189":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)","340a038f":"from sklearn.svm import SVC\n\nmodel.svm = SVC(kernel='linear') \nmodel.svm.fit(X_train,y_train)\npreds = model.svm.predict(X_test)\nmetrics.accuracy_score(y_test, preds)","62c6131b":"# Create the Confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix  \nprint(confusion_matrix(y_test,preds))  ","8bf55864":"We will develop Logistic Regression, Random Forest, SVM.","05dd51b6":"Total Charges increase as Monthly Charges increase - as expected.","60216d91":"**Insight**: Churn is high when Monthly Charges are high","72b99e33":"Wth SVM I was able to increase the accuracy to upto 82%. However, we need to take a deeper look at the true positive and true negative rates, including the Area Under the Curve (AUC) for a better prediction.","1d5cd7fb":"6. Build a corelation of all predictors with 'Churn'","9b3147b2":"\n5. Churn by Monthly Charges and Total Charges","baf69c8e":"> # *CHURN ANALYSIS - MODEL BUILDING*","46f037db":"**Observations**\n\nWe can see that some variables have a negative relation to our predicted variable (Churn), while some have positive relation. Negative relation means that likeliness of churn decreases with that variable. Let us summarize some of the interesting features below:\n\nAs we saw in our EDA, having a 2 month contract reduces chances of churn. 2 month contract along with tenure have the most negative relation with Churn as predicted by logistic regressions\nHaving DSL internet service also reduces the proability of Churn\nLastly, total charges, monthly contracts, fibre optic internet services and seniority can lead to higher churn rates. This is interesting because although fibre optic services are faster, customers are likely to churn because of it. I think we need to explore more to better understad why this is happening.\nAny hypothesis on the above would be really helpful!","ed22a74c":"5. Divide customers into bins based on tenure e.g. for tenure < 12 months: assign a tenure group if 1-12, for tenure between 1 to 2 Yrs, tenure group of 13-24; so on...","801073d9":"Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).","c0b84d5c":"**1. Logistic Regression**","1fc83cbb":"\n3. Convert all the categorical variables into dummy variables","97b48c72":"**2. Random Forest**","e0b319cf":"> # *CHURN ANALYSIS - Exploratory Data Analysis (EDA)*","0c1ca2b5":"**Observations:**\n\n* From random forest algorithm, monthly contract, tenure and total charges are the most important predictor variables to predict churn.\n* The results from random forest are very similar to that of the logistic regression and in line to what we had expected from our EDA","4900769a":"4. Missing Value Treatement\n\nSince the % of these records compared to total dataset is very low ie 0.15%, it is safe to ignore them from further processing.","f957a562":"**Derived Insight:**\n\n**HIGH** Churn seen in case of **Month to month contracts, No online security, No Tech support, First year of subscription and Fibre Optics Internet**\n\n**LOW** Churn is seens in case of **Long term contracts, Subscriptions without internet service and The customers engaged for 5+ years**\n\nFactors like **Gender, Availability of PhoneService** and **# of multiple lines have alomost NO impact on Churn**\n\nThis is also evident from the **Heatmap** below","646fd5ca":"# CONCLUSION\nThese are some of the quick insights from this data:\n\n* Electronic check medium are the highest churners\n* Contract Type - Monthly customers are more likely to churn because of no contract terms, as they are free to go customers.\n* No Online security, No Tech Support category are high churners\n* Non senior Citizens are high churners","38e1b3ba":"It is important to scale the variables in logistic regression so that all of them are within a range of 0 to 1. This helped to improve the accuracy from 79.7% to 80.7%. Further, you will notice below that the importance of variables is also aligned with what we are seeing in Random Forest algorithm and the EDA we conducted above","349308d6":"Random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement.","5ad7c93e":"\n# Missing Data - Initial Intuition\n* Here, we don't have any missing data.\n\nGeneral Thumb Rules:\n\n* For features with less missing values- can use regression to predict the missing values or fill with the mean of the values present, depending on the feature.\n* For features with very high number of missing values- it is better to drop those columns as they give very less insight on analysis.\n* As there's no thumb rule on what criteria do we delete the columns with high number of missing values, but generally you can delete the columns, if you have more than 30-40% of missing values. But again there's a catch here, for example, Is_Car & Car_Type, People having no cars, will obviously have Car_Type as NaN (null), but that doesn't make this column useless, so decisions has to be taken wisely.","71d1dd6e":"**Surprising insight** as higher Churn at lower Total Charges\n\nHowever if we combine the insights of 3 parameters i.e. Tenure, Monthly Charges & Total Charges then the picture is bit clear :- Higher Monthly Charge at lower tenure results into lower Total Charge. Hence, all these 3 factors viz **Higher Monthly Charge, Lower tenure and Lower Total Charge are linkd to High Churn.**","5d074ce9":"6. Remove columns not required for processing","b66dafc9":"# Data Exploration\n1. Plot distibution of individual predictors by churn","d1762716":"**3. Support Vecor Machine (SVM)**","21ae91ab":"2. Convert the target variable 'Churn' in a binary numeric variable i.e. Yes=1 ; No = 0","95f325cc":"2. Total Charges should be numeric amount. Let's convert it to numerical data type","a8935c3a":"SeniorCitizen is actually a categorical hence the 25%-50%-75% distribution is not proper\n\n75% customers have tenure less than 55 months\n\nAverage Monthly charges are USD 64.76 whereas 25% customers pay more than USD 89.85 per month.","07b6b389":"**Dataset Info**: Sample Data Set containing Telco customer data and showing customers left last month.","c241d344":"# Data Cleaning","6b7ac748":"* Data is highly imbalanced, ratio = 73:27\n* So we analyse the data with other features while taking the target values separately to get some insights.","12d89e0d":"# Bivariate Analysis","77913740":"1. Create a copy of base data for manupulation & processing","3ef9eee6":"\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space .where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes.","dd98b791":"3. As we can see there are 11 missing values in TotalCharges column. Let's check these records","9ef81adc":"# Univariate Analysis","e28df265":"4. Relationship between Monthly Charges and Total Charges"}}