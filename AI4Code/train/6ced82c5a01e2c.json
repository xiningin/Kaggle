{"cell_type":{"7085a334":"code","01226a27":"code","bef780f0":"code","bb3b096d":"code","efaf751e":"code","b045b1c7":"code","d19102e7":"code","0e378300":"code","c67629bb":"code","e49564db":"code","4b845aac":"code","dfb58697":"code","9b71a754":"code","75540f9b":"code","d5272e9c":"code","61be258f":"code","027a49ad":"code","fc4789e6":"code","04e5a4ca":"code","d7d270fe":"code","018cb2c6":"code","5157733a":"code","ab60827d":"code","eadcc2be":"code","796b3fdf":"code","10ff15a9":"code","462c6b09":"markdown","922cffc7":"markdown","cfb8c9b0":"markdown","87078db7":"markdown","69f23cec":"markdown","b8e475ad":"markdown","2356c7a1":"markdown","655fe59e":"markdown","fa2b72dc":"markdown","425bef2d":"markdown","fd7d948c":"markdown","3c1a50f8":"markdown","c3a7a980":"markdown","e302ebe3":"markdown","68c849b8":"markdown","6f2f5af9":"markdown","062cf5c0":"markdown","dc1d3eb8":"markdown","a59e370d":"markdown","e7d77ec2":"markdown","0ac748ec":"markdown","e69d2b31":"markdown"},"source":{"7085a334":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01226a27":"df_train = pd.read_csv('..\/input\/titanic\/train.csv',index_col='PassengerId')\ndf_train","bef780f0":"df_train.isnull().sum()","bb3b096d":"def data_preprocessing(df):\n    df = df.drop(['Name','Cabin'],axis=1)\n    df['Sex'] = pd.get_dummies(df['Sex'],drop_first=True)\n    df['Age'].fillna(df['Age'].mean(),inplace=True)\n    df.dropna(inplace=True)\n    embarked_encoded = pd.get_dummies(df['Embarked'])\n    df = pd.concat([df,embarked_encoded],axis=1)\n    df = df.drop(['Embarked','S','Ticket'],axis=1)\n    return df","efaf751e":"df_train_processed = data_preprocessing(df_train)\ndf_train_processed.info()","b045b1c7":"import matplotlib.pyplot as plt\nimport seaborn as sns","d19102e7":"sns.countplot(x=df_train_processed['Survived'])","0e378300":"X = df_train_processed.drop(['Survived'],axis=1)\ny = df_train_processed['Survived']","c67629bb":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy='minority',random_state=17)\nX_resampled , y_resampled = smote.fit_resample(X,y)","e49564db":"sns.countplot(x=y_resampled)","4b845aac":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X_resampled)","dfb58697":"X_scaled","9b71a754":"from sklearn.model_selection import train_test_split\nX_train , X_test , y_train ,y_test = train_test_split(X_scaled,y_resampled,test_size=0.2)","75540f9b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report,confusion_matrix","d5272e9c":"models = {'Logistic Regression':LogisticRegression,'Random Forest':RandomForestClassifier,'KNN':KNeighborsClassifier,'Support Vector':SVC,'Naive bayes bernoulli':BernoulliNB}\nfor i in models:\n    clf = models[i]()\n    clf.fit(X_train,y_train)\n    print(i)\n    print(classification_report(y_test,clf.predict(X_test)))","61be258f":"from sklearn.model_selection import GridSearchCV\nparams = [{'n_neighbors':[1,2,3,4,5],'algorithm':['ball_tree', 'kd_tree', 'brute']}]\nknn_clf = KNeighborsClassifier()\nfinal_clf = GridSearchCV(knn_clf,params)\nfinal_clf.fit(X_train,y_train)\nprint(classification_report(y_test,final_clf.predict(X_test)))","027a49ad":"params=[{'n_estimators':[10,50,100,150,200,250],'criterion':['gini', 'entropy'],'max_features':['auto','sqrt','log2']}]\nrf_clf = RandomForestClassifier()\nfinal_clf = GridSearchCV(rf_clf,params)\nfinal_clf.fit(X_train,y_train)\nprint(classification_report(y_test,final_clf.predict(X_test)))","fc4789e6":"X_test = pd.read_csv('..\/input\/titanic\/test.csv',index_col='PassengerId')\nsubmission=pd.DataFrame(X_test.index)\nX_test","04e5a4ca":"X_test.info()","d7d270fe":"X_test = X_test.drop(['Name','Cabin'],axis=1)\nX_test['Sex'] = pd.get_dummies(X_test['Sex'],drop_first=True)\nX_test['Age'].fillna(X_test['Age'].mean(),inplace=True)\nX_test['Fare'].fillna(X_test['Fare'].mean(),inplace=True)\nembarked_encoded = pd.get_dummies(X_test['Embarked'])\nX_test = pd.concat([X_test,embarked_encoded],axis=1)\nX_test_processed = X_test.drop(['Embarked','S','Ticket'],axis=1)","018cb2c6":"X_test_processed.info()","5157733a":"X_test_scaled = scaler.transform(X_test_processed)","ab60827d":"X_test_scaled","eadcc2be":"submission['Survived'] = final_clf.predict(X_test_scaled)","796b3fdf":"submission","10ff15a9":"submission.to_csv('predictions.csv',index=False)","462c6b09":"## Data Preprocessing for test set","922cffc7":"# Model Selection","cfb8c9b0":"## Scaling our values in the dataframe using minmax scaler","87078db7":"# Visualization","69f23cec":"# The End\n`If you liked the notebook then don't forget to upvote and suggestions are always welcomed.`\n`Follow me on Linkedin :` __[Atharva_Dumbre](https:\/\/www.linkedin.com\/in\/atharva-dumbre-208b5716b)__","b8e475ad":"## Plotting our target variable","2356c7a1":"# Reading the train file","655fe59e":"### We will use GridSearchCV for hyperparameter tuning","fa2b72dc":"## Checking for null values in the dataframe","425bef2d":"## Balancing our imbalanced dataset with SMOTE","fd7d948c":"## Submission is our dataframe with predictions on test set","3c1a50f8":"## Importing the libraries","c3a7a980":"## Importing the models from sklearn","e302ebe3":"## Random forest gives us the highest f1 score so we will finalize it","68c849b8":"## Importing the test set","6f2f5af9":"## Calling the function on our dataframe","062cf5c0":"## KNN and Random Forest gives us the best results so we will explore them further","dc1d3eb8":"## Plotting our target variable after Smote sampling","a59e370d":"# Splitting our dataset","e7d77ec2":"## Scaling the test set","0ac748ec":"## Defining a function for the data preprocessing step","e69d2b31":"## Splitting our dataframe into X and y"}}