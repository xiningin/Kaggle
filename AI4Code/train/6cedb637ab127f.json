{"cell_type":{"149b080b":"code","f8457682":"code","235a9888":"code","b0fd6123":"code","e30b2d61":"code","89fb294c":"code","b67a3dd1":"code","47148305":"code","afd1aa02":"code","90083896":"code","34d2c72e":"code","f98ed468":"code","ac78aac9":"code","2bb3b5cd":"code","34384596":"code","d570885a":"code","555d888c":"code","b50a43e5":"code","89886730":"code","a37f0816":"code","572b8357":"code","0b9c1c64":"code","45a33dcf":"code","a80c9299":"code","aef9c1d4":"code","efd6f332":"code","56498dad":"code","704750e6":"markdown","8964869e":"markdown","5c989327":"markdown","fa0ce520":"markdown","de713f6f":"markdown","5cba9b3e":"markdown","2ab80886":"markdown","4dc24a87":"markdown","aad8856b":"markdown","90fd0d75":"markdown","65c55dfe":"markdown","35eb4b88":"markdown","214df41b":"markdown","006ee042":"markdown","8fae4bde":"markdown","2b710b2c":"markdown"},"source":{"149b080b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f8457682":"pd.set_option('display.max_columns',100)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder,LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\nfrom sklearn.model_selection import RepeatedKFold,cross_val_score,train_test_split\nfrom sklearn.metrics import mean_squared_error","235a9888":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","b0fd6123":"train.head()","e30b2d61":"print('Number of features: {}'.format(train.shape[1]))\nprint('Number of entries: {}'.format(train.shape[0]))","89fb294c":"plt.figure(figsize=(17, 5))\nsns.heatmap(train.isnull(), cbar=True, cmap='Set3')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=14)\nplt.show()","b67a3dd1":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","47148305":"#dealing with missing data\ntrain = train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)\ntrain.isnull().sum().max() #just checking that there's no missing data missing...","afd1aa02":"#missing data\ntotal_ = test.isnull().sum().sort_values(ascending=False)\npercent_ = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data_ = pd.concat([total_, percent_], axis=1, keys=['Total', 'Percent'])\nmissing_data_.head(20)","90083896":"#dealing with missing data\ntest = test.drop((missing_data_[missing_data_['Total'] > 1]).index,1)\ntest = test.drop(test.loc[test['Electrical'].isnull()].index)\ntest.isnull().sum().max() #just checking that there's no missing data missing...","34d2c72e":"train.describe(include='all')","f98ed468":"### Lets plot histogram for prices less than 500000\nhist_price1=train[\"SalePrice\"][train[\"SalePrice\"]<500000].hist()","ac78aac9":"### Lets plot histogram for prices more than 500000\nhist_price2=train[\"SalePrice\"][train[\"SalePrice\"]>500000].hist()","2bb3b5cd":"train = train[train[\"SalePrice\"]<500000]\ntrain","34384596":"##Data cleaning\n#remove duplicates if any\ntrain.duplicated().sum()\ntrain.drop_duplicates(inplace=True)","d570885a":"#For train data\ncategorical_train=[cat for cat in train.columns if train[cat].dtype=='object']\nnumerical_train=[cat for cat in train.columns if train[cat].dtype=='int64' or train[cat].dtype=='float64']\n\n#For test data\ncategorical_test=[cat for cat in test.columns if test[cat].dtype=='object']\nnumerical_test=[cat for cat in test.columns if test[cat].dtype=='int64' or test[cat].dtype=='float64']","555d888c":"print(categorical_train, '\\n\\n',categorical_test)","b50a43e5":"ss= StandardScaler()\ntrain[numerical_train]= ss.fit_transform(train[numerical_train])\ntest[numerical_test]= ss.fit_transform(test[numerical_test])\n#train.head()\ntest.head()","89886730":"train1= pd.get_dummies(train, columns=categorical_train, drop_first= True)\ntest1= pd.get_dummies(test, columns=categorical_test, drop_first= True)\ntrain1\n","a37f0816":"train2=pd.concat([train,train1],axis=1)\ntest2=pd.concat([test,test1],axis=1)","572b8357":"train=train2.drop(categorical_train,axis=1)\ntest=test2.drop(categorical_test,axis=1)\n# test123=test.copy()\n# train\nid=test['Id'].iloc[:,1]\nid","0b9c1c64":"test","45a33dcf":"y=train['SalePrice'].iloc[:,1]\nX=train.drop(['Id','SalePrice'],axis=1)","a80c9299":"#splitting the dataset into test and training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","aef9c1d4":"print('Dimensions of the training feature matrix: {}'.format(X_train.shape))\nprint('Dimensions of the training target vector: {}'.format(y_train.shape))\nprint('Dimensions of the test feature matrix: {}'.format(X_test.shape))\nprint('Dimensions of the test target vector: {}'.format(y_test.shape))","efd6f332":"# Gradient Boosting Regressor\ngbreg=GradientBoostingRegressor()\ngbreg.fit(X_train,y_train)\n\ny_pred_gb=gbreg.predict(X_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 score: \",r2_score(y_test,y_pred_gb)*100)\nprint(\"RMSE: \",np.sqrt(mean_squared_error(y_test,y_pred_gb)))\n\n\n#Error\nerror_diff = pd.DataFrame({'Actual Values': np.array(y_test), 'Predicted Values': y_pred_gb})\nprint(error_diff.head(5))\n\n#Visualize the error\ndf1 = error_diff.head(25)\ndf1.plot(kind='bar',figsize=(10,7))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","56498dad":"#Random forest regression \n\nregrRM2 = RandomForestRegressor(n_estimators=200, max_depth = 50, min_samples_split = 5,min_samples_leaf =4)\nregrRM2.fit(X_train, y_train)\n\ny_pred_rf=regrRM2.predict(X_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 score: \",r2_score(y_test,y_pred_rf)*100)\nprint(\"RMSE: \",np.sqrt(mean_squared_error(y_test,y_pred_rf)))\n\n#Error\nerror_diff = pd.DataFrame({'Actual Values': np.array(y_test), 'Predicted Values': y_pred_rf})\nprint(error_diff.head(5))\n\n#Visualize the error\ndf1 = error_diff.head(25)\ndf1.plot(kind='bar',figsize=(10,7))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","704750e6":"### Concatenating the Original Dataset & the One after creating Dummies(get_dummies() creates a new DF containing JUST the dummies","8964869e":"### Handling Categorical Data using Get_Dummies()","5c989327":"### Splitting the dataset into test and training data","fa0ce520":"Handling outliers by removing entries having price > 500000\n","de713f6f":"# Predict sales prices","5cba9b3e":"### Building a regression model","2ab80886":"### Percentage of missing values in each column of train dataset\n","4dc24a87":"### Using the StandardScaler library to Standardize the numeric values","aad8856b":"### Check statics of the train dataset\n","90fd0d75":"### Dropping the columns already concatenated after Get_Dummies()","65c55dfe":"### Remove duplicates","35eb4b88":"Wow, we dont have any missing values. Now let us do the same for test data.","214df41b":"The sale price ranges in between 34900 and 755000","006ee042":"### Percentage of missing values in each column of test dataset\n","8fae4bde":"### Plotting heatmap of missing values","2b710b2c":"### Finding all columns with Categorical & Numerical values"}}