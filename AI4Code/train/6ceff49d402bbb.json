{"cell_type":{"36d58b92":"code","f0346215":"code","a3325173":"code","cb12b406":"code","29fdd00f":"code","99d6e2f4":"code","44c5e97a":"code","73dd5c03":"code","2e0402b1":"code","30e215b0":"code","8f1c755c":"code","98e6dc99":"code","c7554a40":"code","6102ad42":"code","c3727f9f":"code","32b772b3":"code","9a33bf68":"code","2d8710aa":"code","f9fedb1d":"code","45c5da68":"code","c89da2b9":"code","782a4fa9":"code","1b714d38":"code","dd219681":"code","1e652b08":"code","f334525f":"code","1ea43fb2":"code","0a65d044":"code","a9d2dcea":"code","62a6f052":"code","34bd9582":"code","303f23c9":"code","527d0eba":"code","4b010e80":"code","ce4eb9b0":"code","0eb81fd3":"code","0a14efc4":"code","7df898e9":"code","d4462c91":"code","302d4bda":"code","bbba8c8e":"code","59200cf8":"code","00d83746":"code","a695a33a":"code","d65b5c3c":"code","1740f751":"code","d2c77fd1":"code","3bce2c5b":"code","8b005873":"code","8b236beb":"code","cede4fbf":"code","08a94ad4":"code","2e61aa7d":"code","71216182":"code","36f9afbe":"code","7d015349":"code","2ce8c2dc":"code","623222f1":"code","5ca71595":"code","06509e59":"code","094c3ab7":"code","b172e329":"code","735d2df2":"code","67560ef1":"code","7aec67db":"markdown"},"source":{"36d58b92":"import os, keras\nimport cv2\nimport glob\nimport PIL\nfrom PIL import Image\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator, img_to_array, array_to_img","f0346215":"#Set some directories\ntrainHQ_zip_path = '\/kaggle\/input\/carvana-image-masking-challenge\/train_hq.zip'\nmasks_zip_path = '\/kaggle\/input\/carvana-image-masking-challenge\/train_masks.zip'","a3325173":"import zipfile\n#Extract train images.\nwith zipfile.ZipFile(trainHQ_zip_path,'r') as zip_ref:\n    zip_ref.extractall('\/kaggle\/working')\n#Extract train masks\/labels.\nwith zipfile.ZipFile(masks_zip_path,'r') as zip_ref:\n    zip_ref.extractall('\/kaggle\/working')\ndata_size = len(os.listdir('\/kaggle\/working\/train_hq'))\nprint('Number of train images: ', len(os.listdir('\/kaggle\/working\/train_hq')))\nprint('Number of train masks: ', len(os.listdir('\/kaggle\/working\/train_masks')))","cb12b406":"#Display ids for images and masks.\ncar_ids = sorted(os.listdir('\/kaggle\/working\/train_hq'))\nmask_ids = sorted(os.listdir('\/kaggle\/working\/train_masks'))\n#Generate some random index.\nrnd_ind = list(np.random.choice(data_size,8))\nfor i in rnd_ind:\n    print(\"Car image id: '{}' -- Corressponding Mask id '{}'\".format(car_ids[i], mask_ids[i]))","29fdd00f":"#Pick the 1553th car&mask ids from ids lists.\nn = 1553\ncar_id = car_ids[n]\nmask_id = mask_ids[n]\n#Load car&mask images using thier ids.\ncar = load_img('\/kaggle\/working\/train_hq\/' + car_id)\nmask = load_img('\/kaggle\/working\/train_masks\/' + mask_id)\nprint(\"Image Size: \", car.size)\nprint(\"Mask Size: \", mask.size)\n#Plot them.\nfig, ax = plt.subplots(1, 2, figsize=(20,20))\nfig.subplots_adjust(hspace=.1, wspace=.01)\nax[0].imshow(car)\nax[0].axis('off')\nax[0].title.set_text('Car Image')\nax[1].imshow(mask)\nax[1].axis('off')\nax[1].title.set_text('Car Mask')","99d6e2f4":"main_image = cv2.imread('\/kaggle\/working\/train_hq\/' + car_id)\nmain_image = load_img('\/kaggle\/working\/train_hq\/' + car_id)\nmain_image = np.asarray(main_image)","44c5e97a":"main_image.shape","73dd5c03":"plt.imshow(main_image)","2e0402b1":"mask_image = Image.open('\/kaggle\/working\/train_masks\/' + mask_id)\n#mask_image = load_img('\/kaggle\/working\/train_masks\/' + mask_id)","30e215b0":"mask_image = np.asarray(mask_image)","8f1c755c":"plt.imshow(mask_image)","98e6dc99":"img_masked = cv2.bitwise_and(main_image, main_image, mask=mask_image)","c7554a40":"plt.imshow(img_masked)","6102ad42":"#Randomly split car&mask ids list to training and validation lists.\n#X is car image ids list, y is mask image ids list.\nX_train_ids, X_val_ids, y_train_ids, y_val_ids= train_test_split(car_ids, mask_ids,\n                                                                 test_size=.2, train_size=.8,\n                                                                 random_state=42)\nX_train_size = len(X_train_ids)\nX_val_size = len(X_val_ids)\nprint('Training images size: ', X_train_size)\nprint('Validation images size: ', X_val_size)","c3727f9f":"#Input size could be 128 or 256 or 512 or 1024.\ninput_size = [128, 128, 3]\ndef data_generator(images_path, masks_path, image_ids, mask_ids, batch_size, img_size=input_size):\n    '''\n    images_path\/masks_path: Images\/Masks folder directory.\n    images_ids\/mask_ids: Ids for '.jpg' images\/masks.\n    img_size: Generated imgs\/masks size.\n    \n    returns: batch of randomly-selected car&mask images value-scaled (0 -> 1). \n    '''\n    data_size = len(image_ids)\n    while True:\n        #Choose random indice for later picking.\n        rnd_ind = np.random.choice(np.arange(data_size),batch_size)\n        imgs = []\n        masks = []\n        for i in rnd_ind:\n            #Pick a random id for car&mask images.\n            img_id, mask_id = image_ids[i], mask_ids[i]\n            #Load\/resize images.\n            img = load_img(images_path + img_id, target_size=img_size) \n            mask = load_img(masks_path + mask_id, target_size=img_size[:-1], color_mode = 'grayscale')\n            #Add to the batch data.\n            imgs.append(img_to_array(img))\n            masks.append(img_to_array(mask).reshape(img_size[:-1] + [1]))\n        yield np.array(imgs, dtype=np.float16) \/ 255., np.array(masks, dtype=np.float16) \/ 255.","32b772b3":"#Try out the generator, generate data samples from the validation set.\ngen = data_generator('\/kaggle\/working\/train_hq\/', '\/kaggle\/working\/train_masks\/',\n                    X_val_ids, y_val_ids, batch_size=32)\n\nimgs, masks = next(gen)\nprint('Images batch shape: ', imgs.shape)\nprint('Masks batch shape: ', masks.shape)","9a33bf68":"#Plot output samples of the generator.\nfig, ax = plt.subplots(2, 4, figsize=(15,7))\nfig.subplots_adjust(hspace=.1, wspace=.05)\ncar_samples, mask_samples = imgs[:4].astype(np.float32), masks[:4][:,:,:,0].astype(np.float32)\nfor i, (car, mask) in enumerate(zip(car_samples, mask_samples)):\n    ax[0, i].imshow(car)\n    ax[0, i].axis('off')\n    ax[0, i].title.set_text('Car Image')\n    \n    ax[1, i].imshow(mask, cmap='gray')\n    ax[1, i].axis('off')\n    ax[1, i].title.set_text('Car Mask')\nplt.show()","2d8710aa":"def dice_coef(y_true, y_pred):\n    '''\n    Metric\n    '''\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\n\ndef dice_loss(y_true, y_pred):\n    '''\n    Loss function\n    '''\n    loss = 1 - dice_coef(y_true, y_pred)\n    return loss\n\n\ndef bce_dice_loss(y_true, y_pred):\n    '''\n    Mixed crossentropy and dice loss.\n    '''\n    loss = keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","f9fedb1d":"def get_unet_128(input_shape=(128, 128, 3),\n                 num_classes=1):\n    inputs = Input(shape=input_shape)\n    # 128\n\n    down1 = Conv2D(64, (3, 3), padding='same')(inputs)\n    down1 = BatchNormalization()(down1)\n    down1 = Activation('relu')(down1)\n    down1 = Conv2D(64, (3, 3), padding='same')(down1)\n    down1 = BatchNormalization()(down1)\n    down1 = Activation('relu')(down1)\n    down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n    # 64\n\n    down2 = Conv2D(128, (3, 3), padding='same')(down1_pool)\n    down2 = BatchNormalization()(down2)\n    down2 = Activation('relu')(down2)\n    down2 = Conv2D(128, (3, 3), padding='same')(down2)\n    down2 = BatchNormalization()(down2)\n    down2 = Activation('relu')(down2)\n    down2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n    # 32\n\n    down3 = Conv2D(256, (3, 3), padding='same')(down2_pool)\n    down3 = BatchNormalization()(down3)\n    down3 = Activation('relu')(down3)\n    down3 = Conv2D(256, (3, 3), padding='same')(down3)\n    down3 = BatchNormalization()(down3)\n    down3 = Activation('relu')(down3)\n    down3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n    # 16\n\n    down4 = Conv2D(512, (3, 3), padding='same')(down3_pool)\n    down4 = BatchNormalization()(down4)\n    down4 = Activation('relu')(down4)\n    down4 = Conv2D(512, (3, 3), padding='same')(down4)\n    down4 = BatchNormalization()(down4)\n    down4 = Activation('relu')(down4)\n    down4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n    # 8\n\n    center = Conv2D(1024, (3, 3), padding='same')(down4_pool)\n    center = BatchNormalization()(center)\n    center = Activation('relu')(center)\n    center = Conv2D(1024, (3, 3), padding='same')(center)\n    center = BatchNormalization()(center)\n    center = Activation('relu')(center)\n    # center\n\n    up4 = UpSampling2D((2, 2))(center)\n    up4 = concatenate([down4, up4], axis=3)\n    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n    up4 = BatchNormalization()(up4)\n    up4 = Activation('relu')(up4)\n    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n    up4 = BatchNormalization()(up4)\n    up4 = Activation('relu')(up4)\n    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n    up4 = BatchNormalization()(up4)\n    up4 = Activation('relu')(up4)\n    # 16\n\n    up3 = UpSampling2D((2, 2))(up4)\n    up3 = concatenate([down3, up3], axis=3)\n    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n    up3 = BatchNormalization()(up3)\n    up3 = Activation('relu')(up3)\n    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n    up3 = BatchNormalization()(up3)\n    up3 = Activation('relu')(up3)\n    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n    up3 = BatchNormalization()(up3)\n    up3 = Activation('relu')(up3)\n    # 32\n\n    up2 = UpSampling2D((2, 2))(up3)\n    up2 = concatenate([down2, up2], axis=3)\n    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n    up2 = BatchNormalization()(up2)\n    up2 = Activation('relu')(up2)\n    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n    up2 = BatchNormalization()(up2)\n    up2 = Activation('relu')(up2)\n    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n    up2 = BatchNormalization()(up2)\n    up2 = Activation('relu')(up2)\n    # 64\n\n    up1 = UpSampling2D((2, 2))(up2)\n    up1 = concatenate([down1, up1], axis=3)\n    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n    up1 = BatchNormalization()(up1)\n    up1 = Activation('relu')(up1)\n    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n    up1 = BatchNormalization()(up1)\n    up1 = Activation('relu')(up1)\n    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n    up1 = BatchNormalization()(up1)\n    up1 = Activation('relu')(up1)\n    # 128\n\n    classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up1)\n\n    model = Model(inputs=inputs, outputs=classify)\n\n    model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coef])\n\n    return model\n\nuNet = get_unet_128()","45c5da68":"#Prepare callbacks\nLR_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=10, factor=.2, min_lr=.00001)\nEarlyStop_callback = keras.callbacks.EarlyStopping(monitor='val_loss',patience=10, restore_best_weights=True)","c89da2b9":"#Perpare data generators.\nbatch_size = 32\ntrain_gen = data_generator('\/kaggle\/working\/train_hq\/', '\/kaggle\/working\/train_masks\/',\n                           X_train_ids, y_train_ids, batch_size=batch_size)\nval_gen = data_generator('\/kaggle\/working\/train_hq\/', '\/kaggle\/working\/train_masks\/',\n                           X_val_ids, y_val_ids, batch_size=batch_size)","782a4fa9":"history = uNet.fit_generator(train_gen, steps_per_epoch=int(X_train_size\/batch_size),\n                             epochs=21, validation_data=val_gen,\n                             validation_steps=int(X_val_size\/batch_size),\n                             callbacks=[LR_callback, EarlyStop_callback])","1b714d38":"uNet.save('unet_main1.h5')","dd219681":"uNet = tf.keras.models.load_model('..\/input\/sir-unet\/sir_unet.h5', custom_objects={'bce_dice_loss': bce_dice_loss, 'dice_coef':dice_coef})\n#load_model(modelPath, custom_objects={'mean_squared_abs_error': mean_squared_abs_error})","1e652b08":"# Plot the loss and accuracy curves for training and validation\nfig, ax = plt.subplots(2,1, figsize=(15,7))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['dice_coef'], color='b', label=\"Training dice loss\")\nax[1].plot(history.history['val_dice_coef'], color='r',label=\"Validation dice loss\")\nlegend = ax[1].legend(loc='best', shadow=True)","f334525f":"#Perdict some imgs.\npred_masks = uNet.predict(imgs)","1ea43fb2":"fig, ax = plt.subplots(32, 3, figsize=(20,150))\nfig.subplots_adjust(hspace=.1, wspace=.05)\nfor i in range(32):\n    ax[i, 0].imshow(imgs[i].astype(np.float32))\n    ax[i, 0].axis('off')\n    ax[i, 0].title.set_text('Car')\n    \n    ax[i, 1].imshow(masks[i,:,:,0].astype(np.float32), cmap='gray')\n    ax[i, 1].axis('off')\n    ax[i, 1].title.set_text('Real Mask')\n    \n    ax[i, 2].imshow(pred_masks[i,:,:,0], cmap='gray')\n    ax[i, 2].axis('off')\n    ax[i, 2].title.set_text('Predicted Mask')\nplt.show()","0a65d044":"import shutil # for removing the directory\nshutil.rmtree('\/kaggle\/working\/train_hq')\nshutil.rmtree('\/kaggle\/working\/train_masks')","a9d2dcea":"# Prediction","62a6f052":"input_image = imgs[0].astype('float32')\ninput_image = np.asarray(input_image)","34bd9582":"plt.imshow(input_image)","303f23c9":"plt.imshow(masks[0, :, :, 0].astype('float32'))","527d0eba":"pred_mask_image = np.reshape(pred_masks[0], (128, 128))\npred_mask_image = pred_mask_image > 0.5\npred_mask_image = np.asarray(Image.fromarray(pred_mask_image, 'L'))","4b010e80":"plt.imshow(pred_mask_image)","ce4eb9b0":"pred_masked_image = cv2.bitwise_and(input_image, input_image, mask=pred_mask_image)","0eb81fd3":"plt.imshow(pred_masked_image)","0a14efc4":"# Prediction","7df898e9":"images = [cv2.imread(image) for image in glob.glob('..\/input\/inputcar\/input car\/*.*')]\n#images = [cv2.imread(file) for file in glob.glob('path\/to\/files\/*.jpg')]","d4462c91":"images2 = images.copy()","302d4bda":"images2[0].shape","bbba8c8e":"#size_images = [cv2.imread(image) for image in glob.glob('..\/input\/inputcar\/input car\/*.*')]","59200cf8":"#image = PIL.Image.open(\"..\/input\/inputcar\/input car\/0010-000222-before.jpg\")\n#image to open\n\n#width, height = image.size","00d83746":"names = glob.glob('..\/input\/inputcar\/input car\/*.*')","a695a33a":"names[0]","d65b5c3c":"names[0].split('\/')[-1].split('.')[0]","1740f751":"len(images)","d2c77fd1":"images = []\nimg = load_img('..\/input\/dataset\/dataset\/AgktOpMQ.jpeg', target_size=(128, 128))\nimages.append(img_to_array(img))\nimg = load_img('..\/input\/dataset\/dataset\/aQzTDcFQ.jpeg', target_size=(128, 128))\nimages.append(img_to_array(img))\nimg = load_img('..\/input\/dataset\/dataset\/eBbAEfLA.jpeg', target_size=(128, 128))\nimages.append(img_to_array(img))\nimg = load_img('..\/input\/dataset\/dataset\/o-Oh9z6Q.jpeg', target_size=(128, 128))\nimages.append(img_to_array(img))\nimg = load_img('..\/input\/dataset\/dataset\/p3S_zKbA.jpeg', target_size=(128, 128))\nimages.append(img_to_array(img))\nimages = np.array(images, dtype=np.float32) \/ 255.","3bce2c5b":"for i in range(len(images)):\n    images[i] = cv2.resize(images[i], (128, 128))\n    #images[i] = images[i]\/255.\n    #images[i] = np.asarray(images[i])\n    images[i] = np.array(images[i]) \/ 255.","8b005873":"images = np.array(images)","8b236beb":"images.shape","cede4fbf":"pred_masks = uNet.predict(images)","08a94ad4":"plt.imshow(pred_masks[0, :, :, 0])","2e61aa7d":"plt.imshow(images2[0])","71216182":"#pred_mask_image = np.reshape(pred_masks[0], (3200, 2400))\npred_mask_image = cv2.resize(pred_masks[0], (3200, 2400))\n\nmax_output_value = 100\nneighborhood_size = 50\nsubtract_from_mean = 2\n'''\nbinarized_images = [cv2.adaptiveThreshold(image, max_output_value,\n                                        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n                                        cv2.THRESH_BINARY,\n                                        neighborhood_size,\n                                        subtract_from_mean) for image in gray_images]\n'''\n# pred_mask_image = cv2.adaptiveThreshold(pred_mask_image, max_output_value, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, neighborhood_size, subtract_from_mean)\npred_mask_image = pred_mask_image > 0.5\npred_mask_image = np.asarray(Image.fromarray(pred_mask_image, 'L'))","36f9afbe":"plt.imshow(pred_mask_image)","7d015349":"pred_mask_image = cv2.adaptiveThreshold(pred_mask_image, max_output_value, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, neighborhood_size, subtract_from_mean)","2ce8c2dc":"main_image = images2[0]","623222f1":"plt.imshow(main_image)","5ca71595":"pred_masked_image = cv2.bitwise_and(main_image, main_image, mask=pred_mask_image)","06509e59":"plt.imshow(pred_masked_image)","094c3ab7":"pred_masked_image = cv2.resize(pred_masked_image, (3200, 2400))","b172e329":"matplotlib.pyplot.imsave('abc.jpeg', pred_masked_image)","735d2df2":"for i in range(5, len(pred_masks)):\n    #pred_mask_image = np.reshape(pred_masks[i], (128, 128))\n    pred_mask_image = cv2.resize(pred_masks[i], (3200, 2400))\n    pred_mask_image = pred_mask_image > 0.5\n    pred_mask_image = np.asarray(Image.fromarray(pred_mask_image, 'L'))\n    \n    main_image = images2[i]\n    \n    pred_masked_image = cv2.bitwise_and(main_image, main_image, mask=pred_mask_image)\n    \n    pred_masked_image = cv2.resize(pred_masked_image, (3200, 2400))\n    \n    matplotlib.pyplot.imsave(names[i].split('\/')[-1].split('.')[0] +'.jpeg', pred_masked_image)\n    print(i)","67560ef1":"                                                '''\n                                                Thank You\n                                            \n                                                Regards,\n                                                Chirag Verma\n                                                '''","7aec67db":"1. * Now let's try  to pridect masks for a batch of 32 images from the validation set."}}