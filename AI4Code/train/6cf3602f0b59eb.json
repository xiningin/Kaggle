{"cell_type":{"946fe68f":"code","01792e9d":"code","da7d8d9d":"code","bdfab1ba":"code","52b9529b":"code","81ebe0d4":"code","346931e1":"code","bd6d2f54":"code","fb2f472c":"code","4944f823":"code","f43ad04e":"code","e9528e05":"code","4367c279":"code","8ed79550":"code","c4aeefe9":"code","7120be90":"code","3f819d08":"code","f00b9b60":"code","3a1c8d6f":"code","4d2a3eab":"code","638c48db":"code","7ab7069f":"code","873c2d23":"code","b6adc054":"code","6465e4b1":"code","04c8861f":"code","2f141f4d":"code","f0ec0aa5":"markdown","1c6321d8":"markdown","e633803a":"markdown","5094fb60":"markdown","a46805e3":"markdown","5b8eeae8":"markdown","68af272e":"markdown","e7940748":"markdown","d8d5299c":"markdown","e196c700":"markdown","fb1557a4":"markdown","0a67b543":"markdown"},"source":{"946fe68f":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve","01792e9d":"# Read the dataset\ndata = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndata.head()","da7d8d9d":"# It's clear that there are some categorical features and continuous ones\ncategorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']\ncontinuous_features = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']","bdfab1ba":"# Info about the dataset attributes\ndata.info()","52b9529b":"# Check nan values \ndata.isnull().sum()","81ebe0d4":"# Describe continuous features\ndata[continuous_features].describe()","346931e1":"# Check the correlation between features of the dataset\nplt.figure(figsize=(18, 9))\nheatmap = sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Features correlation Heatmap', fontdict={'fontsize':12}, pad=12);","bd6d2f54":"# Another visualization of pairwise relationships in our dataset\nsns.pairplot(data,hue='output')\nplt.show()","fb2f472c":"data_f = data\n# Binning continuous features\n# age\ndata_f['age'] = pd.cut(data_f['age'], bins=5, labels=range(5))\n# trtbps\ndata_f['trtbps'] = pd.cut(data_f['trtbps'], bins=5, labels=range(5))\n# chol\ndata_f['chol'] = pd.cut(data_f['chol'], bins=5, labels=range(5))\n# thalachh\ndata_f['thalachh'] = pd.cut(data_f['thalachh'], bins=5, labels=range(5))\n\n# Encoding categorical features\ndata_f = pd.get_dummies(data_f, columns = categorical_features, drop_first = True)","4944f823":"# Define the features and target\nX = data.drop(['output'],axis=1)\ny = data[['output']]","f43ad04e":"# Spliting the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)","e9528e05":"# Logistic regression classifier (simplest one)\n# Without feature engineering\nclf = LogisticRegression()\n# train the classifier\nclf.fit(X_train, y_train)\n# calculating the probabilities\ny_pred_proba = clf.predict_proba(X_test)\n# finding the predicted valued\ny_pred = np.argmax(y_pred_proba,axis=1)\n# printing the test accuracy\nprint(\"The test accuracy score of Logistric Regression Classifier is \", accuracy_score(y_test, y_pred))","4367c279":"y_true = y_test\ny_pred = clf.predict(X_test)\nprint(classification_report(y_true, y_pred))","8ed79550":"# Define the features and target\nX = data_f.drop(['output'],axis=1)\ny = data_f[['output']]\n\n# Scaling continuous features\nscaler = RobustScaler()\nX[continuous_features] = scaler.fit_transform(X[continuous_features])","c4aeefe9":"# Spliting the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)","7120be90":"# Logistic regression classifier (simplest one)\n# With feature engineering\nclf = LogisticRegression()\n# train the classifier\nclf.fit(X_train, y_train)\n# calculating the probabilities\ny_pred_proba = clf.predict_proba(X_test)\n# finding the predicted valued\ny_pred = np.argmax(y_pred_proba,axis=1)\n# printing the test accuracy\nprint(\"The test accuracy score of Logistric Regression Classifier is \", accuracy_score(y_test, y_pred))","3f819d08":"y_true = y_test\ny_pred = clf.predict(X_test)\nprint(classification_report(y_true, y_pred))","f00b9b60":"clf_model = RandomForestClassifier()\n\nparam_grid = {\n    'n_estimators': [400, 700, 1000],\n    'max_depth': [15,20,25],\n    'max_leaf_nodes': [50, 100, 200]\n}\n\ngs = GridSearchCV(\n        estimator=clf_model,\n        param_grid=param_grid, \n        cv=10, \n        n_jobs=-1, \n        scoring='roc_auc',\n        verbose=2\n    )\n\nfitted_clf_model = gs.fit(X_train, y_train)\n\nprint(fitted_clf_model.best_score_)\nprint(fitted_clf_model.best_params_)","3a1c8d6f":"# Show the classification report\ny_true = y_test\ny_pred = fitted_clf_model.predict(X_test)\nprint(classification_report(y_true, y_pred))","4d2a3eab":"# Define the features and target\nX = data_f.drop(['output'],axis=1)\ny = data_f[['output']]","638c48db":"# Spliting the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)","7ab7069f":"lgb_model = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', metric='auc', learning_rate=0.001, num_boost_round=1000)\nlgb_model.fit(X=X_train, y=y_train)\n# calculating the probabilities\ny_pred_proba = lgb_model.predict_proba(X_test)\n# finding the predicted valued\ny_pred = np.argmax(y_pred_proba,axis=1)\n# printing the test accuracy\nprint(\"The test accuracy score of LightGBM Classifier is \", accuracy_score(y_test, y_pred))","873c2d23":"# Show the classification report\ny_true = y_test\ny_pred = lgb_model.predict(X_test)\nprint(classification_report(y_true, y_pred))","b6adc054":"# Plot the ROC curve \ny_pred_prob = lgb_model.predict_proba(X_test)[:,1]\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\nplt.plot([0,1],[0,1],\"k--\",'r+')\nplt.plot(fpr,tpr,label='LighGBM classifier')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"LightGBM classifier ROC Curve\")\nplt.show()","6465e4b1":"# Lets see which features are important to the classification\nlgb.plot_importance(lgb_model)\nplt.rcParams['figure.figsize'] = [20, 9]\nplt.show()","04c8861f":"gkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nlgb_model = lgb.LGBMClassifier(\n    boosting_type=\"gbdt\",\n    objective='binary',\n    metric='auc'\n)\nparam_grid = {\n    'n_estimators': [200, 300, 400],\n    'colsample_bytree': [0.5, 0.6, 0.7],\n    'max_depth': [5, 10, 15],\n    'num_leaves': [20, 30, 40, 50, 60],\n    'reg_alpha': [1, 1.1, 1.2, 1.3],\n    'reg_lambda': [1, 1.1, 1.2, 1.3],\n    'min_split_gain': [0.3, 0.4],\n    'subsample': [0.8, 0.9, ],\n    'subsample_freq': [15, 20, 25],\n    'learning_rate': [0.01, 0.001, 0.0001]\n}\n\ngs = GridSearchCV(\n        estimator=lgb_model,\n        param_grid=param_grid, \n        cv=gkf, \n        n_jobs=-1, \n        scoring='roc_auc',\n        verbose=2\n    )\n\nfitted_lgb_model = gs.fit(X_train, y_train)\n\nprint(fitted_lgb_model.best_score_)\nprint(fitted_lgb_model.best_params_)","2f141f4d":"# Show the classification report\ny_true = y_test\ny_pred = fitted_lgb_model.predict(X_test)\nprint(classification_report(y_true, y_pred))","f0ec0aa5":"# Feature engineering","1c6321d8":"# RandomForest model","e633803a":"**With some basic feature engineering we observe how the model score rises from 0.8 to 0.87**","5094fb60":"From those basic exploration we can conclude that:\n* This dataset doesn't contain nan values\n* The continuous features (age, chol, ...) aren't in the same range\n* The target variables is more correlated with 'cp', 'thalachh' and 'slp'\n* There is no clear linear relationship between categorical features\n* There are some outliers when visualizing the pairwise relationship with the categorical features\n* ...","a46805e3":"# Let's use other models","5b8eeae8":"## With some basic feature engineering","68af272e":"# LightGBM","e7940748":"# Modeling","d8d5299c":"The visualization of features importance allows us to understand more the effect of some features that the model consider more important in its classification. Thus, more process can be done to help the model reach high performance level.\nWe can also continue to finetune the hyper-parameters of the model to gain some % in the accuracy measure.","e196c700":"# Introduction to the dataset, with some exploration","fb1557a4":"## Without feature engineering ","0a67b543":"The starting step of building a machine learning model for a specific task given a dataset is to udnerstand the data provided and try to properly process it in order to help the model reach the best possible performance measure.\nFor our dataset here's the columns (features) defining it: \n\n* age: Age of the patient\n\n* sex: Sex of the patient\n\n* cp: Chest pain type, 0 = Typical Angina, 1 = Atypical Angina, 2 = Non-anginal Pain, 3 = Asymptomatic\n\n* trtbps: Resting blood pressure (in mm Hg)\n\n* chol: Cholestoral in mg\/dl fetched via BMI sensor\n\n* fbs: (fasting blood sugar > 120 mg\/dl), 1 = True, 0 = False\n\n* restecg: Resting electrocardiographic results, 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy\n\n* thalachh: Maximum heart rate achieved\n\n* oldpeak: Previous peak\n\n* slp: Slope\n\n* caa: Number of major vessels\n\n* thall: Thalium Stress Test result ~ (0,3)\n\n* exng: Exercise induced angina ~ 1 = Yes, 0 = No\n\n* output: **Target variable**"}}