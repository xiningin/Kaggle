{"cell_type":{"4d878894":"code","9587664d":"code","448741f3":"code","56e08259":"code","f306ca68":"code","4d629100":"code","c8d46352":"code","89b7051b":"markdown","29031a7c":"markdown"},"source":{"4d878894":"import pandas as pd\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport scipy.stats as stat\nimport time\n","9587664d":"data = pd.read_csv(\"..\/input\/SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\nprint(data.head())\nprint(data.tail())","448741f3":"l = data['l'].values\nt = data['t'].values\ntsq = t * t","56e08259":"def train(x, y, m, c, eta):\n    const = - 2.0\/len(y)\n    ycalc = m * x + c\n    delta_m = const * sum(x * (y - ycalc))\n    delta_c = const * sum(y - ycalc)\n    m = m - delta_m * eta\n    c = c - delta_c * eta\n    error = sum((y - ycalc)**2)\/len(y)\n    return m, c, error\n\ndef train_on_all(x, y, m, c, eta, iterations=1000):\n    for steps in range(iterations):\n        m, c, err = train(x, y, m, c, eta)\n    return m, c, err","f306ca68":"# Init m, c\nm, c = 0, 0\n\n# Learning rate\nlr = 0.01\n\n# Training for 1000 iterations, plotting after every 100 iterations:\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111)\nplt.ion()\nfig.show()\nfig.canvas.draw()\n\nfor num in range(10):\n    m, c, error = train_on_all(l, tsq, m, c, lr, iterations=100)\n    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n    y = m * l + c\n    ax.clear()\n    ax.plot(l, tsq, '.k')\n    ax.plot(l, y)\n    fig.canvas.draw()\n    time.sleep(1)\n","4d629100":"print(\"From our Gradient Descent   m = {0:.06} c = {1:.06}\".format(m, c))\n\nmsp, csp, _, _, _ = stat.linregress(l,tsq)\nprint(\"From scipy.stats.linregress m = {0:.06} c = {1:.06}\".format(msp, csp))","c8d46352":"ms, cs,errs = [], [], []\nm, c = 0, 0\neta = 0.001\nfor times in range(200):\n    m, c, error = train_on_all(l, tsq, m, c, eta, iterations=100) # We will plot the value of for every 100 iterations\n    ms.append(m)\n    cs.append(c)\n    errs.append(error)\nepochs = range(0, 20000,100)\nplt.figure(figsize=(8,5))\nplt.plot(epochs, errs)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Error\")\nplt.title(\"Vanilla Gradient Descent - Batch\")\nplt.show()","89b7051b":"We quickly summarise the essenial parts of the Gradient Descent method:\n\n $y = mx + c$\n \n $E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y - y_i)^2$\n \n $\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y - (mx_i + c))$\n \n $\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y - (mx_i + c))$","29031a7c":"Close enough!\n\n## Plotting error vs iterations\n\nSo far we have seen how the Gradient Descent works by looking at the fit of the regression line. Let us change perspectives and plot the error at various stages. This just shows that the process is converging and gives us a feel for the rate at which it is converging.\n\n$E = \\frac{1}{n} \u2211_{i=1}^n(y_i\u2212y)^2$\n\n$ = \\frac{1}{n} \u2211_{i=1}^n(y_i - mx_i - c)^2$"}}