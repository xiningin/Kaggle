{"cell_type":{"65a5a33c":"code","0cc6ca27":"code","4d643183":"code","bb317a9c":"code","f68dc766":"code","03be8b9e":"code","7279064a":"code","40c10657":"code","de2d23d5":"code","97b70047":"code","70ebc9b3":"code","f7caa977":"code","97911e5f":"code","92b79d42":"code","5a40f670":"code","51df9f05":"code","0c2881a0":"code","b7363057":"code","3f1f7824":"code","cff81d92":"code","cd6c2d72":"code","abac35a8":"code","c7a71273":"code","e4aa8d27":"code","38559771":"code","7a90c4b2":"code","f0b36dc2":"code","6ab3651b":"code","72aeae30":"code","166936f1":"code","20a8e36a":"code","d68301a2":"code","e714490b":"code","be4013a6":"code","5e345ad5":"code","8148fe87":"code","d31a2292":"code","f535f615":"code","11baf353":"code","3250254c":"code","b4a9bc38":"code","cb59bc5e":"code","6b25a5d5":"code","186626bf":"code","e550a455":"code","4b7061f0":"code","c32d4c7a":"markdown","e4579312":"markdown","b08d58dd":"markdown"},"source":{"65a5a33c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cc6ca27":"# MatPlotLib : librairie de visualisation et graphiques\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import HTML # permet d'afficher du code html dans jup","4d643183":"#Fonction pour standardiser les donn\u00e9es quantitatives\n\ndef scale_feat(df,cont_feat) :\n    df1=df\n    scaler = preprocessing.RobustScaler()\n    df1[cont_feat] = scaler.fit_transform(df1[cont_feat])\n    return df1","bb317a9c":"#Fonction pour tracer les courbes d'apprentissage sur l'ensemble d'apprentissage et l'ensemble de validation \n\nfrom sklearn.model_selection import learning_curve\ndef plot_learning_curve(est, X_train, y_train) :\n    train_sizes, train_scores, test_scores = learning_curve(estimator=est, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=5,\n                                                        n_jobs=-1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.figure(figsize=(8,10))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.6, 1.0])\n    plt.show()","f68dc766":"#Fonction pour tracer la courbe ROC\ndef plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","03be8b9e":"#Traitement du dataset\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","7279064a":"df.head().T","40c10657":"df.count()","de2d23d5":"df.info()","97b70047":"df.describe()","70ebc9b3":"# On mappe les valeurs de la colonne cible en 0\/1\ndf['Class'] = df['Class'].map({ False: 0, True: 1 })","f7caa977":"df.columns","97911e5f":"discr_feat = []\ncont_feat = list(set(df.columns) - set(discr_feat)-{'Class'})","92b79d42":"# On convertit les cat\u00e9gories en \u00e9tiquettes num\u00e9riques\nfor col in discr_feat :\n    df[col]=df[col].astype('category')\n    df[col] = df[col].cat.codes\n    df[col]=df[col].astype('int8')","5a40f670":"df.info()","51df9f05":"df.head()","0c2881a0":"#On v\u00e9rifie s'il y a des valeurs ind\u00e9termin\u00e9es dans le dataset\ndf.isnull().values.sum()","b7363057":"df[cont_feat].describe()","3f1f7824":"# On normalise les valeurs des caract\u00e9ristiques\ndf=scale_feat(df,cont_feat)","cff81d92":"df[cont_feat].describe()","cd6c2d72":"#On affiche les distributions des valeurs continues\nfor col in cont_feat :\n    plt.figure(figsize=[10,5])\n    sns.kdeplot(df[col])","abac35a8":"#On construit les ensembles d'apprentissage et de test\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['Class'], axis=1)\ny = df.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","c7a71273":"#On teste les for\u00eats al\u00e9atoires\nfrom sklearn import ensemble\n\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","e4aa8d27":"print(classification_report(y_test, y_rf))","38559771":"# Matrice de confusion\ncm = confusion_matrix(y_test, y_rf)\nprint(cm)","7a90c4b2":"# Sous-\u00e9chantillonage\n# On a plus de clients qui ne fraudent pas que de clients qui fraudent\ndf.Class.value_counts()","f0b36dc2":"#On va garder autant de clients qui ne fraudent pas que de clients qui fraudent dans l'ensemble d'apprentissage (X_train), en tirant al\u00e9atoirement ceux qu'on va garder On dit qu'on \"sous-\u00e9chantillonne la classe majoritaire\"\nfrom imblearn.under_sampling import RandomUnderSampler \n\nrus = RandomUnderSampler()\nX_train, y_train = rus.fit_sample(X_train, y_train)","6ab3651b":"#On v\u00e9rifie qu'on a bien \u00e9quilibr\u00e9 l'ensemble d'apprentissage\ny_train.value_counts()","72aeae30":"#On applique les for\u00eats al\u00e9atoires sur le nouvel ensemble d'apprentissage\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\n\nprint(classification_report(y_test, y_rf))\n\ncm = confusion_matrix(y_test, y_rf)\nprint(cm)","166936f1":"#On cr\u00e9e donc de \"fausses donn\u00e9es\" (mais \"vraisemblables\") pour l'apprentissage\nX = df.drop(['Class'], axis=1)\ny = df.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","20a8e36a":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_sample(X_train, y_train)","d68301a2":"#On a bien \u00e9quilibr\u00e9 l'ensemble d'apprentissage (en \"ajoutant\" des donn\u00e9es)\ny_train.value_counts()","e714490b":"#On teste les for\u00eats al\u00e9atoires avec les donn\u00e9es sur\u00e9chantillonn\u00e9es\nfrom sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","be4013a6":"print(classification_report(y_test, y_rf))","5e345ad5":"# Matrice de confusion\ncm = confusion_matrix(y_test, y_rf)\nprint(cm)","8148fe87":"plot_learning_curve(rf, X_train, y_train)","d31a2292":"plot_roc_curve(rf,X_test,y_test)","f535f615":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\nprint(xgb.score(X_test,y_test))","11baf353":"y_xgb = xgb.predict(X_test)\n\nprint(classification_report(y_test, y_xgb))\n\ncm = metrics.confusion_matrix(y_test, y_xgb)\nprint(cm)","3250254c":"plot_learning_curve(xgb, X_train, y_train)\nplot_roc_curve(xgb,X_test,y_test)","b4a9bc38":"print(classification_report(y_test, y_xgb))","cb59bc5e":"#On reconstitue les jeux de donn\u00e9es sans \u00e9chantillonnage\nX = df.drop(['Class'], axis=1)\ny = df.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","6b25a5d5":"df.Class.value_counts()","186626bf":"#On utilise le param\u00e8tre scale_pos_weight pour donner plus d'impact aux erreurs commises sur la classe minoritaire\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(scale_pos_weight=284315\/492)\n# xgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_xgb = xgb.predict(X_test)","e550a455":"print(classification_report(y_test, y_xgb))\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)","4b7061f0":"plot_learning_curve(xgb, X_train, y_train)\nplot_roc_curve(xgb,X_test,y_test)","c32d4c7a":"Extreme Gradient Boosting : XGBoost avec sur\u00e9chantillonage SMOTE","e4579312":"Sur\u00e9chantillonnage:\n\nOn va r\u00e9\u00e9quilibrer le dataset en sur-\u00e9chantillonnant la classe minoritaire","b08d58dd":"XGBOOST pond\u00e9r\u00e9"}}