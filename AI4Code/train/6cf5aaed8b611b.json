{"cell_type":{"19855334":"code","8db562ec":"code","a3298eba":"code","8c2eb4a0":"code","bfc5ab75":"code","f1239ffa":"code","ee99eb87":"code","eaae2b39":"code","f8606840":"code","2c8f14d1":"code","71708d44":"code","81a1f38f":"code","edef6c1d":"code","d3def1d1":"code","c9b61efe":"code","989791ec":"code","af5cc45a":"code","015047b7":"code","0695aef3":"code","6b1732ed":"code","dbccc3f7":"code","f6351aa2":"code","5389f58a":"code","d9bda730":"code","288aa663":"code","dc326304":"code","e7fc9b19":"code","8f747598":"code","3c6d184f":"code","5967f2f5":"code","666d6e91":"code","e32577f7":"code","977f242f":"code","c0b996a9":"code","842abd78":"code","58a076d9":"code","4dc469a4":"code","d05eedf1":"code","4378ef94":"code","5702d6bd":"code","bd45389b":"code","52ab8a3d":"code","f15e63bb":"code","739fbd46":"code","d3d3ee4f":"code","7ebadc0f":"code","85ce6dfa":"code","6fe15997":"code","554a74ce":"code","9fc7e74d":"markdown","8268bf63":"markdown","3cd499a3":"markdown","cb5fb782":"markdown","4cd0d4e5":"markdown","2e03fa2d":"markdown","e1aa351d":"markdown","0c8c9107":"markdown","68894538":"markdown","67210783":"markdown","c9ed345c":"markdown","9ed70f77":"markdown","f22520f2":"markdown","2842fe20":"markdown","41cb30d0":"markdown","ac1b69bb":"markdown","767b25b4":"markdown","5636ac0c":"markdown","b78b751c":"markdown","d13978ae":"markdown"},"source":{"19855334":"#importing basic libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb","8db562ec":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a3298eba":"df = pd.read_csv('\/kaggle\/input\/source-based-news-classification\/news_articles.csv')","8c2eb4a0":"#get a peek into the data\ndf.tail(15)","bfc5ab75":"df.describe()","f1239ffa":"df.info()","ee99eb87":"df.shape\n#initial size of the dataset","eaae2b39":"#check for null values\ndf.isnull().sum()","f8606840":"#drop the null values\ndf.dropna(inplace=True)","2c8f14d1":"#Date Time split to see if any trend\ntimesplit = df['published'].str.split('T', 1, expand=True)\ndf['Time']=timesplit[1]\ndf['Date']=timesplit[0]","71708d44":"#Split the Month and Day Data\n#Since all our data in from 2016, we might be able to see a trend if any, through the months\ndatesplit = df['Date'].str.split('-',n=-1, expand=True)\ndf['Year']=datesplit[0]\ndf['Month']=datesplit[1]\ndf['Day']=datesplit[2]","81a1f38f":"#drop some error producing rows\ndf.loc[df.Month == 'content\/uploads\/2014\/04\/jucundus']\ndf.drop(848, inplace=True)\ndf.loc[df.Month == 'content\/uploads\/2015\/07\/Earth']\ndf.drop(1838, inplace=True)","edef6c1d":"#Change the type for ease of plotting\ndf = df.astype({\"Month\": float, \"Day\":float})","d3def1d1":"df.tail(5)\n#cleaner and transformed data","c9b61efe":"#Labelize the label column\n#Create Dummies to represent Real and Fake by 1 and 0\ndum_type=pd.get_dummies(df.label,drop_first=True,prefix=\"\")\ndf=df.join(dum_type)\ndf.drop('label', axis=1, inplace=True)\ndf.rename(columns={'_Real':'Real'}, inplace=True)","989791ec":"#plot to see Counts of Real and Fake data\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(x=\"Real\", data=df, palette='Set3')\nax.set_title(\"Count of Real and Fake Data\")","af5cc45a":"#plot to see Counts of Language data\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(x=\"language\", data=df, palette='Set3')\nax.set_title(\"Count of Languages of the news\")","015047b7":"#plot to see Counts of type of news data\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(x=\"type\", data=df, palette='Set3')\nax.set_title(\"Count of Type of the news\")","0695aef3":"#plot to see Counts news data that have images or no\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(x=\"hasImage\", data=df, palette='Set3')\nax.set_title(\"Count of news with Images\")","6b1732ed":"#plot to see top 10 Authors with most news articles\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(y=\"author\", data=df, palette='Set3', order =df.author.value_counts().iloc[:10].index)\nax.set_title(\"Top 10 Authors with count of news\")","dbccc3f7":"#plot to see top 10 Authors with most news urls\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(y=\"site_url\", data=df, palette='Set3', order =df.site_url.value_counts().iloc[:10].index)\nax.set_title(\"Top 10 Site URLS with count of news\")","f6351aa2":"#plot to see Months of pblished news\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(x=\"Month\", data=df, palette='Set3')\nax.set_title(\"Months of published news\")","5389f58a":"#plot to see Days of pblished news\nsb.set_theme(style=\"whitegrid\")\nax = sb.countplot(x=\"Day\", data=df, palette='Set3')\nsb.set(rc = {'figure.figsize':(10,10)})\nax.set_title(\"Days of published news\")","d9bda730":"#Fake Authors\ndf.loc[df.Real == 0].author.value_counts().head(10)","288aa663":"#Fake URL sites\ndf.loc[df.Real == 0].site_url.value_counts().head(10)","dc326304":"#Real Authors\ndf.loc[df.Real == 1].author.value_counts().head(10)","e7fc9b19":"#Real URL sites\ndf.loc[df.Real == 1].site_url.value_counts().head(10)","8f747598":"real = set(df[df['Real'] == 1]['site_url'].unique())\nfake = set(df[df['Real'] == 0]['site_url'].unique())\nprint(f\"{real & fake}\")","3c6d184f":"#target is our column to be predicted and features are the columns that we will use for modelling\ntarget = df.Real\nfeatures = df[['author','site_url','text_without_stopwords']]","5967f2f5":"features","666d6e91":"#since all the feature columns are text data, we want to use text mining techniques, we put all of them in the same column for ease of processing\nfeatures['combined_url_text'] = features[\"author\"]+\" \"+features[\"site_url\"] + \" \" + features[\"text_without_stopwords\"]\nfeatures.drop(['author','site_url', 'text_without_stopwords'], axis = 1, inplace = True)\nfeatures=features.combined_url_text","e32577f7":"features","977f242f":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_words = TfidfVectorizer(stop_words='english')\nX = vectorizer_words.fit_transform(features)","c0b996a9":"#trying to cluster the 2 data centres ie fake and real news\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\nkm.fit(X)","842abd78":"import numpy as np\nnp.unique(km.labels_, return_counts=True)\nfeatures.reset_index(drop=True, inplace=True)","58a076d9":"#Here we are collecting all the fake data into one cluster and all the real data into another.\n#We are doing this as a pre step to the coming logic.\ntext={}\nfor i,cluster in enumerate(km.labels_):\n    oneDocument = features[i]\n    if cluster not in text.keys():\n        text[cluster] = oneDocument\n    else:\n        text[cluster] += oneDocument","4dc469a4":"#importing NLTK libraries for NLP techiniques\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom string import punctuation\nfrom heapq import nlargest\nimport nltk ","d05eedf1":"#here we are finding the keywords that appear in both the clusters and we do not want to include the stop words as those will be the most frequent\n_stopwords = set(stopwords.words('english') + list(punctuation))\n\nkeywords = {}\ncounts={}\nfor cluster in range(2):\n    word_sent = word_tokenize(text[cluster].lower())\n    word_sent=[word for word in word_sent if word not in _stopwords]\n    freq = FreqDist(word_sent)\n    keywords[cluster] = nlargest(100, freq, key=freq.get)\n    counts[cluster]=freq","4378ef94":"keywords","5702d6bd":"#here we are finding keyword that are uniques to each cluster\nset0 = set(keywords[0])\nset1 = set(keywords[1])\nunique0=set0-set1\nunique1 =set1-set0","bd45389b":"unique_keys={}\nunique_keys[0]=nlargest(10, unique0, key=counts[0].get)\nunique_keys[1]=nlargest(10, unique1, key=counts[1].get)","52ab8a3d":"unique_keys","f15e63bb":"#using the Vectorizer before doing the classification\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(features,target,test_size=0.20)\n\nvectorizer = TfidfVectorizer(stop_words = 'english')\ntfidf_train = vectorizer.fit_transform(X_train)\ntfidf_test = vectorizer.transform(X_test)\ntfidf_df = pd.DataFrame(tfidf_train.A, columns=vectorizer.get_feature_names())","739fbd46":"print(tfidf_train.shape)\nprint(y_train.shape)","d3d3ee4f":"#Kneighbors Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nKNb = KNeighborsClassifier(n_neighbors=5)\nKNb.fit(tfidf_train, y_train)\ny_pred = KNb.predict(tfidf_test)\nKNscore=metrics.accuracy_score(y_test,y_pred)\nprint(\"Kneighbors Model accuracy: %0.4f\" %KNscore)","7ebadc0f":"#Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(n_estimators=1000)\nRF.fit(tfidf_train,y_train)\ny_pred = RF.predict(tfidf_test)\nRFscore = metrics.accuracy_score(y_test,y_pred)\nprint(\"Random Forest Model accuracy:  %0.4f\" %RFscore)","85ce6dfa":"#Adaboost Classification\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nAdab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10),n_estimators=5)\nAdab.fit(tfidf_train, y_train)\ny_pred = Adab.predict(tfidf_test)\nABscore = metrics.accuracy_score(y_test,y_pred)\nprint(\"Ada boost Classifier accuracy: %0.4f\" %ABscore)","6fe15997":"#creating a confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(conf_matrix)","554a74ce":"#creating a matrix using seaborn\nimport seaborn as sns\n\ncm_df = pd.DataFrame(conf_matrix, columns=np.unique(y_test), index = np.unique(y_test))\ncm_df.columns.name = 'Predicted'\ncm_df.index.name = 'Actual'\ncm_df\nplt.figure(figsize=(5,5))\nsns.heatmap(cm_df, annot=True, vmax=20, square=True, cmap=\"Reds\",annot_kws={\"size\":12} ,cbar=False, fmt='g')","9fc7e74d":"We have successfully build a model to classify news as Fake or Real.\nAdaboost Classfier gave us an accuracy upto 98% which is really great.\nThis Model can be used best for political news as most of our training records contain political news.\nThe same model can be further extended for all news types with a larger and more diverse dataset.","8268bf63":"### Steps for Project Implementation\n- Data Preparation and Mining\n- Exploratory Data Analysis\n- Feature Engineering\n- Modelling and Prediction \n- Result\n- Out of Sample Prediction","3cd499a3":"### Data Preperation and Mining","cb5fb782":"As we can see in the Confusion matrix, we have 263 records correctly predicted Fake and 139 records correctly predicted as Real. 3 Fake news were predicted as Real and 4 Real news were predicted as Fake.","4cd0d4e5":"### Conclusion","2e03fa2d":"Here we can see that the Authors and Sites for real and fake news are clearly different. Meaning the same source is not producing real and fake news. There are some sites that produce both news but mostly they produce 1 type and some news articles can be of the other type. So with the main text, site urls and author names might also help in the prediction. ","e1aa351d":"###### TFIDF Vectorizer\nTerm Frequency: This summarizes how often a given word appears within a document.\nInverse Document Frequency: This downscales words that appear a lot across documents.\nTF-IDF are word frequency scores that try to highlight words that are more interesting.","0c8c9107":"### Modelling and Prediction","68894538":"## Fake News Detection","67210783":"The features that we can use for our model are author, title, text_without_stopwords and site urls. We have to explore these features in detail.","c9ed345c":"### Result","9ed70f77":"Among all the classifications, Adaboost is performing the best. We will choose AdaBoost Model","f22520f2":"We want to explore the test data.\nWe want to see which words and topics are commom to the fake and real data.\nWe want to see words that are completely unique to the fake and real data.","2842fe20":"Findings from the EDA\n1. There is no bias in the dataset because we have real and fake data\n2. Most of the news articles are in English so we will be using NLP and other techniques with English language in mind\n3. The type of news data is varied and that is good for classification. Also bs stands for Bullshit, all the articles with no type are classified into bs.\n4. Most of the news articles have an Image.\n5. Some of the most recurring site address and author names have been retrieved. We can use the occurance of the same sites to our advantage.\n6. We have a lot of articles towards the end of the month.","41cb30d0":"### Introduction\n\nFake news has become one of the most serious problems of our generation. It has a huge impact on our online and offline conversations. Fake news still poses a clear and timely threat to western democracy and social stability. Social media is a huge pool of content and is the most watched part of all the content available to users. Politicians, television stations, newspaper websites, and even the public can post this information. The credibility of these posts is a serious problem in today's world, as many companies are taking steps to educate the public about the dangers of spreading false information. Need to be verified. Manual classification of messages is tedious, time consuming, and biased, so it is not possible to clearly determine the level of reliability of messages posted online.","ac1b69bb":"### EDA","767b25b4":"### References\n\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\n- https:\/\/realpython.com\/nltk-nlp-python\/\n- Notebooks from Ruchi Bhatia\n\nUsed TF-IDF Vectorizer to process text data and find weighted important words.\nUsed the NLTK toolkit for some functions like tokenize, stopwords removal etc.\n","5636ac0c":"We can see a lot of Nan and null values here.","b78b751c":"These are some of the words that are unique to each class. This gives us an idea about our news data.","d13978ae":"### Feature Engineering"}}