{"cell_type":{"846d290d":"code","e5dbd611":"code","1f946811":"code","fd299401":"code","6109cb1d":"code","2a35dec0":"code","3ad52355":"code","2263667c":"code","a8516f8f":"code","f1b476fa":"code","9c38fa78":"code","0216421b":"code","75e3f25b":"code","83b986a6":"code","c101b788":"code","fa92f544":"code","8ec76d7e":"code","870eeb7c":"code","03a15bef":"code","0c3e1b24":"code","08f7ab91":"markdown","6c48bdeb":"markdown"},"source":{"846d290d":"%%bash\n\necho \"TPU_DEPS_INSTALLED=${TPU_DEPS_INSTALLED:-}\"\nif [[ -z \"${TPU_DEPS_INSTALL:-}\" ]]; then\n    echo \"Installing TPU dependencies.\"\n    pip install --upgrade pip\n    curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n    python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n    export TPU_DEPS_INSTALLED=true\n    echo \"TPU dependencies installed.\"\nelse\n   echo \"TPU dependencies already exist. Skipping step.\"\nfi","e5dbd611":"%%bash\nexport XLA_USE_BF16=1\nexport XRT_TPU_CONFIG=\"tpu_worker;0;10.240.1.2:8470\"","1f946811":"import torch\nimport transformers\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup","fd299401":"import torch_xla.core.xla_model as xm # using TPUs\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl","6109cb1d":"from scipy import stats\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2a35dec0":"accelerator_device = \"tpu\"  # options: cpu, gpu, tpu\nCORES = 1\nif accelerator_device == \"tpu\":\n    CORES = 8 # * xm.xrt_world_size()\n    print(f\"xm.xrt_world_size()={xm.xrt_world_size()}\")\nelif accelerator_device == \"gpu\":\n    accelerator_device = \"cuda\"\n\nmultiple_workers = CORES > 1\n    \nprint(f\"accelerator_device={accelerator_device}\")\nprint(f\"CORES={CORES}\")","3ad52355":"def print_to_console(string_to_print):\n    if accelerator_device == \"tpu\":\n        xm.master_print(string_to_print) \n    else:\n        print(string_to_print)","2263667c":"class BERTBaseUncased(nn.Module):\n    def __init__(self, bert_path):\n        super(BERTBaseUncased, self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 30)\n        \n    def forward(self, ids, mask, token_type_ids):\n        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        bo = self.bert_drop(o2)\n        return self.out(bo)","a8516f8f":"class BERTDatasetTraining:\n    def __init__(self, qtitle, qbody, answer, targets, tokenizer, max_len):\n        self.qtitle = qtitle\n        self.qbody = qbody\n        self.answer = answer\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.answer)\n    \n    def __getitem__(self, item):\n        question_title = str(self.qtitle[item])\n        question_body = str(self.qbody[item])\n        answer = str(self.answer[item])\n                            \n        # [CLS] [Q-TITLE] [Q-BODY] [SEP] [ANSWER] [SEP]\n        inputs = self.tokenizer.encode_plus(\n            f\"{question_title} {question_body}\",\n            answer,\n            add_special_tokens=True,\n            max_len=self.max_len\n        )\n        \n        ids = inputs['input_ids'][0:511]\n        token_type_ids = inputs['token_type_ids'][0:511]\n        mask = inputs['attention_mask'][0:511]\n        \n        padding_len = self.max_len - len(ids)\n        ZERO_PADDING = [0] * padding_len\n        ids = ids + ZERO_PADDING\n        token_type_ids = token_type_ids + ZERO_PADDING\n        mask = mask + ZERO_PADDING\n\n        each_item = { \n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(self.targets[item, :][0:511], dtype=torch.float),\n        }\n        \n        return each_item","f1b476fa":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets)","9c38fa78":"def set_to_device(data_, field_name, device, data_type=torch.long):\n    field = data_[field_name]\n    return field.to(device, dtype=data_type)","0216421b":"def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    for bi, d in enumerate(data_loader):\n        if bi % 10 == 0:\n            print_to_console(f'Started Training: bi={bi}')\n        \n        ids = set_to_device(d, \"ids\", device)\n        mask = set_to_device(d, \"mask\", device)\n        token_type_ids = set_to_device(d, 'token_type_ids', device)\n        targets = set_to_device(d, 'targets', device, data_type=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        perform_optimizer_step(optimizer)\n\n        if scheduler is not None:\n            scheduler.step()\n            \n        if bi % 10 == 0:\n            print_to_console(f'Finished Training: bi={bi}, loss={loss}')\n\n            \ndef perform_optimizer_step(optimizer):\n    if accelerator_device == \"tpu\":\n        if multiple_workers:\n            xm.optimizer_step(optimizer)               # multiple TPUs\n        else:\n            xm.optimizer_step(optimizer, barrier=True) # single TPU\n    else:\n        optimizer.step()\n            \n            \ndef eval_loop_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    for bi, d in enumerate(data_loader):\n        if bi % 10 == 0:\n            print_to_console(f'Started Validation: bi={bi}')\n\n        ids = set_to_device(d, \"ids\", device)\n        mask = set_to_device(d, \"mask\", device)\n        token_type_ids = set_to_device(d, 'token_type_ids', device)\n        targets = set_to_device(d, 'targets', device,  data_type=torch.float)\n                \n        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        loss = loss_fn(outputs, targets)\n        \n        fin_targets.append(targets.cpu().detach().numpy())\n        fin_outputs.append(outputs.cpu().detach().numpy())\n\n        if bi % 10 == 0:\n            print_to_console(f'Finished Validation: bi={bi}, loss={loss}')\n\n    return np.vstack(fin_outputs), np.vstack(fin_targets)","75e3f25b":"def get_train_dataloader(dataset, batch_size):\n    if accelerator_device == \"tpu\":\n        train_sampler = torch.utils.data.DistributedSampler(\n            dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n        )\n        train_data_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            sampler=train_sampler\n        )\n    else:\n        train_data_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=True\n        )\n\n    return train_data_loader","83b986a6":"def get_valid_dataloader(dataset, batch_size):\n    if accelerator_device == \"tpu\":\n        valid_sampler = torch.utils.data.DistributedSampler(\n            dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n        )\n\n        valid_data_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            sampler=valid_sampler\n        )\n    else:\n        valid_data_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False\n        )\n\n    return valid_data_loader","c101b788":"def train_and_evaluate(train_data_loader, valid_data_loader, model, optimizer, device, scheduler):    \n    if accelerator_device == \"tpu\":\n        train_para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_loop_fn(train_para_loader.per_device_loader(device), model, optimizer, device, scheduler)\n        valid_para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        \n        outputs, targets = eval_loop_fn(valid_para_loader.per_device_loader(device), model, device)\n    else:\n        train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n        outputs, targets = eval_loop_fn(valid_data_loader, model, device)\n\n    return outputs, targets","fa92f544":"def run(rank=None):\n    MAX_LEN = 512\n    TRAIN_BATCH_SIZE = int(4 * CORES * (1\/2))\n    VALID_BATCH_SIZE = TRAIN_BATCH_SIZE\n    EPOCHS = 20\n    \n    dfx = pd.read_csv('..\/input\/google-quest-challenge\/train.csv').fillna(\"none\")\n    df_train, df_valid = train_test_split(dfx, random_state=42, test_size=0.1)\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n    \n    sample = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv')\n    target_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n    print_to_console(f\"{len(target_cols)} target_cols: {target_cols}\")\n\n    train_targets = df_train[target_cols].values\n    valid_targets = df_valid[target_cols].values    \n    \n    tokenizer = transformers.BertTokenizer.from_pretrained('..\/input\/bert-base-uncased\/')\n    train_dataset = BERTDatasetTraining(\n        qtitle=df_train.question_title.values, \n        qbody=df_train.question_body.values, \n        answer=df_train.answer.values, \n        targets=train_targets,\n        tokenizer=tokenizer, \n        max_len=MAX_LEN\n    )\n    \n    train_data_loader = get_train_dataloader(train_dataset, TRAIN_BATCH_SIZE)\n\n    valid_dataset = BERTDatasetTraining(\n        qtitle=df_valid.question_title.values, \n        qbody=df_valid.question_body.values, \n        answer=df_valid.answer.values, \n        targets=valid_targets,\n        tokenizer=tokenizer, \n        max_len=MAX_LEN\n    )\n    \n    valid_data_loader = get_valid_dataloader(valid_dataset, VALID_BATCH_SIZE)\n    \n    device, lr = get_device_lr()\n\n    model = BERTBaseUncased(\"..\/input\/bert-base-uncased\").to(device)\n    num_training_steps = int((len(train_dataset) \/ TRAIN_BATCH_SIZE \/ CORES) * EPOCHS) # CORES = xm.xrt_world_size()\n    \n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n    \n    for epoch in range(EPOCHS):\n        print_to_console(\"\")\n        print_to_console(f'{rank}: Started epoch = {epoch}')        \n        \n        outputs, targets = train_and_evaluate(\n            train_data_loader, valid_data_loader, model, optimizer, device, scheduler\n        )\n\n        spear = []\n        for jj in range(targets.shape[1]):\n            p1 = list(targets[:, jj])\n            p2 = list(outputs[:, jj])\n            coef, _ = np.nan_to_num(stats.spearmanr(p1, p2))\n            spear.append(coef)\n            \n        spear = np.mean(spear)    \n        \n        print_to_console(f'{rank}: Finished epoch = {epoch}, spearman = {spear}')\n        print_to_console(\"\")\n        \n        save_model(model)","8ec76d7e":"def get_device_lr():\n    lr = 3e-5\n    device = accelerator_device\n    if accelerator_device == \"tpu\":\n        device = xm.xla_device()\n        lr = lr * CORES # xm.xrt_world_size()\n\n    return device, lr","870eeb7c":"def save_model(model):\n    model_filename='model.pth'\n    print_to_console(f\"Saving model {model_filename}...\")\n    if accelerator_device == \"tpu\":\n        xm.save(model.state_dict(), model_filename)\n    else:\n        torch.save(model, model_filename)\n    print_to_console(f\"Model {model_filename} saved.\")","03a15bef":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run(rank)","0c3e1b24":"%%time\nif accelerator_device == \"tpu\":\n    FLAGS={}\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=CORES, start_method='fork')\nelse:\n    run()","08f7ab91":"## Credits to [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek) for his great work, I created this notebook based on the [original video: BERT on Steroids: Fine-tuning BERT for a dataset using PyTorch and Google Cloud TPUs](https:\/\/www.youtube.com\/watch?v=B_P0ZIXspOU) and also [this notebook](https:\/\/www.kaggle.com\/abhishek\/i-like-clean-tpu-training-kernels-i-can-not-lie).\n\n#### Original notebook: none provided for good reasons ;)\n#### Inference notebook: https:\/\/www.kaggle.com\/abhishek\/bert-inference-of-tpu-model\/\n\nThe notebook in theory should support CPUs and TPUs. The GPU version would need some more modifications but most of the code is in place for it to work on the GPU as well.","6c48bdeb":"### Train"}}