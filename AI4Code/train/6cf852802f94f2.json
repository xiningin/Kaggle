{"cell_type":{"1e4e437f":"code","688ed1a0":"code","5ab857c0":"code","ba38c677":"code","a8bdf67c":"code","ee7d3196":"code","564433e7":"code","05ce8253":"code","ac3c58bb":"code","83faf027":"code","f32eb477":"code","f674a0da":"code","14ca4d97":"code","cf0c7277":"code","14208eb3":"code","ec744289":"code","6594d07e":"code","f9d2d31c":"code","303cdcf7":"code","97e2d51b":"code","31dc9fbe":"code","1fd4d64d":"code","7975ec32":"code","67bec2ef":"code","0c3983b0":"code","2e3feb07":"code","912b6cda":"code","2471e5b1":"code","6363cda8":"code","3e6036b8":"markdown","1f58b21f":"markdown","141c520e":"markdown","b11e9e43":"markdown","7daa9e1b":"markdown","57cef99a":"markdown","56350c4c":"markdown","f056f846":"markdown","5e892705":"markdown","e5385216":"markdown","9e4dd8fc":"markdown","8a9a5df8":"markdown","01332ac6":"markdown","457b7f7a":"markdown","59c94cf4":"markdown","edcab1fa":"markdown","330421a8":"markdown","eae98ee3":"markdown","5b9bee52":"markdown","23a5303d":"markdown","424bead3":"markdown","1ea2cbd1":"markdown","60513125":"markdown","5a4643c3":"markdown","3f506df1":"markdown","8123cfd5":"markdown","33534059":"markdown","27c3879b":"markdown"},"source":{"1e4e437f":"# Basic Operation\nimport pandas as pd\nimport numpy as np\n\n# Text Preprocessing & Cleaning\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport re\nimport emoji\nimport string\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split # Split Data \nfrom imblearn.over_sampling import SMOTE # Handling Imbalanced\n\n# Model Building\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score # Performance Metrics  \n\n# Data Visualization \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom termcolor import cprint\nimport seaborn as sns\nimport warnings  ","688ed1a0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5ab857c0":"dftrain = pd.read_csv('\/kaggle\/input\/twitter-airline-sentiment-analysis\/Train.csv',index_col='Id')\ndftest = pd.read_csv('\/kaggle\/input\/twitter-airline-sentiment-analysis\/TestX.csv',index_col='Id')\ndisplay(dftrain.head(5))\ndftest.sample(5)","ba38c677":"ytrain = dftrain.y.values","a8bdf67c":"dftrain.shape","ee7d3196":"dftest.shape","564433e7":"ytrain.shape","05ce8253":"cprint(\"Anzahl Sentiments Tweets :\",'green')\nprint(dftrain.y.value_counts())\nplt.figure(figsize = (10, 8))\nax = sns.countplot(x = 'y', data = dftrain, palette = 'pastel')\nax.set_title(label = 'Anzahl Sentiments Tweets', fontsize = 20)\nplt.show()","ac3c58bb":"def preprocessing1(text):\n    return re.sub('@([\\w]+)',r'<\\1>',text)\ndef preprocessing2(text):\n    return re.sub('\\d+',r'<digit>',text)","83faf027":"# Remove stop words\n# def remove_stopwords(text):\n    # text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])\n    # return text\n\n# Remove url  \n# def remove_url(text):\n    # url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    # return url.sub(r'',text)\n\n# Remove html \n# def remove_html(text):\n    # html=re.compile(r'<.*?>')\n    # return html.sub(r'',text)\n\n# Remove @username\n# def remove_username(text):\n    # return re.sub('@[^\\s]+','',text)\n\n# Decontraction text\n# def decontraction(text):\n    # text = re.sub(r\"won\\'t\", \" will not\", text)\n    # text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    # text = re.sub(r\"can\\'t\", \" can not\", text)\n    # text = re.sub(r\"don\\'t\", \" do not\", text)\n    # text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    # text = re.sub(r\"ma\\'am\", \" madam\", text)\n    # text = re.sub(r\"let\\'s\", \" let us\", text)\n    # text = re.sub(r\"ain\\'t\", \" am not\", text)\n    # text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    # text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    # text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    # text = re.sub(r\"y\\'all\", \" you all\", text)\n    # text = re.sub(r\"n\\'t\", \" not\", text)\n    # text = re.sub(r\"n\\'t've\", \" not have\", text)\n    # text = re.sub(r\"\\'re\", \" are\", text)\n    # text = re.sub(r\"\\'s\", \" is\", text)\n    # text = re.sub(r\"\\'d\", \" would\", text)\n    # text = re.sub(r\"\\'d've\", \" would have\", text)\n    # text = re.sub(r\"\\'ll\", \" will\", text)\n    # text = re.sub(r\"\\'ll've\", \" will have\", text)\n    # text = re.sub(r\"\\'t\", \" not\", text)\n    # text = re.sub(r\"\\'ve\", \" have\", text)\n    # text = re.sub(r\"\\'m\", \" am\", text)\n    # text = re.sub(r\"\\'re\", \" are\", text)\n    # return text  \n\n# Seperate alphanumeric\n# def seperate_alphanumeric(text):\n    # words = text\n    # words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    # return \" \".join(words)","f32eb477":"text_train = dftrain.text.apply(preprocessing1).apply(preprocessing2) #.apply(preprocessing3) etc.\ntext_test = dftest.text.apply(preprocessing1).apply(preprocessing2)","f674a0da":"# text_train = dftrain.text.apply(remove_stopwords).apply(remove_url).apply(remove_html).apply(remove_username).apply(decontraction).apply(seperate_alphanumeric)\n# text_test = dftest.text.apply(remove_stopwords).apply(remove_url).apply(remove_html).apply(remove_username).apply(decontraction).apply(seperate_alphanumeric)","14ca4d97":"# from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(strip_accents='unicode',lowercase=True)\n\n# Apply TFIDF on cleaned tweets\n# tfid = TfidfVectorizer(tokenizer=spacy_tokenizer)","cf0c7277":"Xtrain = cv.fit_transform(text_train)\nXtest = cv.transform(text_test)\n\n# Xtrain = tfid.fit_transform(text_train)\n# Xtest = tfid.transform(text_test)","14208eb3":"list(cv.vocabulary_)[:10]\n\n# list(tfid.vocabulary_)[:10]","ec744289":"wordcounts = np.sum(Xtrain,axis=0)\nwordcounts = np.array(wordcounts).ravel()","6594d07e":"wordcounts = pd.Series(wordcounts,index=list(cv.vocabulary_))","f9d2d31c":"wordcounts.sort_values(ascending=False).iloc[:80].plot.bar(figsize=(20,5),ylabel='H\u00e4ufigkeit \u00fcber alle Dokumente');","303cdcf7":"from sklearn.model_selection import train_test_split,cross_val_score\nXtrain2,Xvalid,ytrain2,yvalid = train_test_split(Xtrain,ytrain) #nicht strikt n\u00f6tig- aber sinnvoll, um Overfitting zu verhindern.\nXtrain2.shape,Xvalid.shape,ytrain2.shape,yvalid.shape","97e2d51b":"# from sklearn.naive_bayes import MultinomialNB\n# clf = MultinomialNB()\n# cross_val_score(clf,Xtrain2,ytrain2,cv=10)\n\n# from sklearn.ensemble import RandomForestClassifier\n# clf = RandomForestClassifier()\n# cross_val_score(clf,Xtrain2,ytrain2,cv=10)\n\n# from sklearn.svm import SVC\n# clf = SVC()\n# cross_val_score(clf,Xtrain2,ytrain2,cv=10)\n\nfrom xgboost import XGBClassifier\nclf = XGBClassifier()\ncross_val_score(clf,Xtrain2,ytrain2,cv=10)","31dc9fbe":"clf.fit(Xtrain2,ytrain2)","1fd4d64d":"clf_prediction = clf.predict(Xvalid)","7975ec32":"accuracy_score(clf_prediction, yvalid)","67bec2ef":"cr = classification_report(yvalid, clf_prediction)","0c3983b0":"print(\"Classification Report:\\n----------------------\\n\", cr)\n\ncm = confusion_matrix(yvalid,clf_prediction)\n\n\n# plot confusion matrix \nplt.figure(figsize=(8,6))\nsentiment_classes = ['Negative', 'Neutral', 'Positive']\nsns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n            xticklabels=sentiment_classes,\n            yticklabels=sentiment_classes)\nplt.title('Confusion matrix', fontsize=16)\nplt.xlabel('Actual label', fontsize=12)\nplt.ylabel('Predicted label', fontsize=12)\nplt.show()","2e3feb07":"# yhat = ['positive', 'negative', 'neutral', 'positive',...]","912b6cda":"# yhat_ser = pd.Series(yhat,name='y')\n# yhat_ser.index.name='Id'\n# yhat_ser.to_csv('submission.csv')","2471e5b1":"ytest = pd.Series(clf.predict(Xtest),name='y')\nytest.index.name='Id'\nytest.to_csv('Beispiell\u00f6sung.csv')","6363cda8":"Xtest.shape,ytest.shape","3e6036b8":"# 5 Diskussion\n## 5.1 Erfahrungen mit diesem Datensatz\n**Worin lag\/liegt die gr\u00f6sste Herausforderung?**   \nAnders als bei den anderen Competitions, handelt es sich beim vorliegenden Datensatz nicht um nummerische Werte, sondern ausschliesslich um Texte. Zwar hatten wir in der \u00dcbung 1 - Titanic auch Alphabete (z.B. Geschlecht), welche wir ebenfalls in Zahlen umkonvertiert haben. Aber hier gibt es eine Vielzahl von W\u00f6rter. Die gr\u00f6sste Herausforderung lag hierbei festzustellen, welche W\u00f6rter als Features genommen werden sollen.\n\n**Welche Techniken helfen dabei?**   \nEbenfalls wieder systematisches Ausprobieren! Wir haben die Annahme getroffen, dass die Stoppw\u00f6rter, aber auch HTML-Links, Benutzernamen, usw. st\u00f6ren w\u00fcrden. Es hat sich jedoch anhand der Versuchen gezeigt, dass auch nicht ein \u00fcberm\u00e4ssiges Preprocessing stattfinden darf, da dies zu schlechteren Ergebnisse f\u00fchrt. Evtl. sind es immer dieselben Users, welche negative R\u00fcckmeldungen verfassen, weshalb die \"Usernamen\" der Tweets auch wichtige Features sind? Oder die Erw\u00e4hnung von bestimmten Strecken oder Airline-Unternehmen.","1f58b21f":"## 4.5 F\u00fcnfter Versuch\nNeu wurde die Annahme getroffen, ob sich der CountVectorizer im vorliegenden Datensatz besser eignet, als der Tfidf-Vectorizer, da mit diesem sich bessere Ergebnisse erzielen lassen. Es wurde ein weiterer Versuch gestartet mit den gleichen Einstellungen wie vorher, nur diesmal mit dem CountVectorizer. Der Score hat sich ein wenig verbessert.","141c520e":"## 4.4 Vierter Versuch\nDie Preprocessing Schritte wurden leicht angepasst. Aus Sicht der Projektgruppe wurden Funktionen wie \"Remove Emojis\" entfernt, um zu schauen, ob diese evtl. im Datensatz drin sein sollten? Zudem wurde das Experiment mit dem TfidfVectorizer wieder durchgef\u00fcgt. Der Klassifikator wurde weiterhin nicht ver\u00e4ndert. Bei diesem Versucht wurde ein Score von 0.71 erreicht und ist somit schlechter, als mit dem CountVecotrizer. ","b11e9e43":"![\u00dcbung 5 - 4ter Versuch.PNG](attachment:ffcbcc6f-57d7-4666-b775-c0c0c74ffe6f.PNG)","7daa9e1b":"![\u00dcbung 5 - 7ter Versuch.PNG](attachment:b811397f-93ff-4752-b1db-9d87848760ca.PNG)","57cef99a":"## 3.6 Modell Performance visualisieren","56350c4c":"## 5.3 Weiterf\u00fchrende Ideen\n**Wenn Sie ein ganzes Team von smarten Data Scientists zur Verf\u00fcgung h\u00e4tten: Welche fortgeschrittenen Ideen w\u00fcrden Sie als aussichtsreich einsch\u00e4tzen und gerne ausprobieren, um den Wettbewerb zu gewinnen?**   \nAuch hier w\u00fcrden sich evtl. weitere Modelle zum Testen ergeben, wie Logistische Regression, GradientBoostingClassifier, usw. Es gibt eine Vielzahl von Classifier. Mit dem XGBClassifier haben wir das beste Ergebnis erhalten. Evtl. erhalten wir noch bessere mit anderen Classifier?","f056f846":"## 3.7 Abgabe und Upload der Vorhersage\nWenn Sie eine Vorhersage der Form\n\n    yhat = ['positive', 'negative', 'neutral', 'positive',...]\n\nhaben, l\u00e4sst sie sich leicht in die gew\u00fcnschte Textform bringen:\n\n    yhat_ser = pd.Series(yhat,name='y')\n    yhat_ser.index.name='Id'\n    yhat_ser.to_csv('submission.csv')\n\nVersuchen Sie's! Klicken Sie anschliessend oben recht im Kaggle-Notebook auf \"Save Version\" (dann auf Save). Lassen Sie sich das evaluierte Notebook anzeigen und scrollen Sie runter zum Abschnitt `Output`. Dr\u00fccken Sie dort den Knopf **Submit**, um Ihre Vorhersage ins Wettrennen um den Kaggle-Competition-Gewinn zu schicken!","5e892705":"# 1. Vorstellung Gruppe\n\nDas vorliegende Notebook wurde von der Gruppe 2 im Rahmen der praktischen \u00dcbungen des Moduls Data Science (DSCI) im Studiengang MSc Wirtschaftsinformatik erstellt. Die Arbeiten finden im Herbstsemester 2021 statt. Die Gruppe besteht aus folgenden Mitgliedern:\n\n- Jannis Kokkinis\n- Milan Markovic\n- Zafer \u00dcnal\n- Nicoleta Zanotta\n\nS\u00e4mtliche commits zum Leaderboard wurden vom User \"MilanMarkovic9019\" im Namen der gesamten Gruppe 2 durchgef\u00fchrt.","e5385216":"## 3.5 Training Klassifikator","9e4dd8fc":"# 3 Das Notebook\n## 3.1 Pakete importieren","8a9a5df8":"![\u00dcbung 5 - 5ter Versuch.PNG](attachment:d05b9c04-75fd-4de4-9fb4-86fe1b9f334f.PNG)","01332ac6":"![\u00dcbung 5 - 6ter Versuch.PNG](attachment:890c1acf-4fa3-4fde-b035-eba3631ba596.PNG)","457b7f7a":"## 4.3 Dritter Versuch\nNeu wurden weitere Preprocessing Schritte durchgef\u00fchrt. Dazu wurden in der Zeile 80 weitere Funktionen implementiert wie z.B. weitere Stopw\u00f6rter zu entfernen. Eine neue Submission mit dem CountVectorizer hat gezeigt, dass sich der Score nicht verbessert hat.","59c94cf4":"## 3.3 Daten visualizieren","edcab1fa":"![\u00dcbung 5 - 3ter Versuch.PNG](attachment:e8df3be7-7bff-44e2-af60-42a9d6825614.PNG)","330421a8":"## 4.7 Siebter Versuch\nNun wurde ein weiterer Versuch mit einem SupportVektorMachine Klassifikator durchgef\u00fchrt. Es wurde ein neuer Bestscore von 0.80333 erreicht. Es wurde der CountVectorizer genommen und die Preprocessing-Schritte zu den Voreinstellungen \u00fcbernommen.\nEine weitere Submission mit weiteren Preprocessing-Schritten f\u00fchrte dazu, dass sich das Ergebnis nicht verbessert hat.","eae98ee3":"## 5.2 Eigenheiten des Datensatzes\n**Welche Eigenheiten hat dieser Datensatz, die ihn besonders herausfordernd oder einfach erscheinen lassen?**   \nWie bereits erw\u00e4hnt, handelt es sich hier um eine Vielzahl von W\u00f6rtern. Die gr\u00f6sste Herausforderung war festzustellen, welche W\u00f6rter wichtige Features darstellen und welche nicht. Zudem ist die Wahl des Optimizers, aber auch des richtigen Klassifikatores ebenfalls entscheidend. Bei der vorliegenden Competition mussten somit an 3 Stellen experimentiert werden.   \n1. Beim Preprocessing\n2. Beim Optimizer\n3. Beim Modell   \n\nMithilfe der \u00c4nderung an allen 3 Stellen konnten bessere Ergebnisse erzielt werden.","5b9bee52":"# 4. Versuche\n## 4.1 Erster Versuch\nDie Vorlage wurde unver\u00e4ndert gelassen und eine Submission erstellt. Durch den ersten Versuch konnte bereits ein Score von 0.78333 erreicht werden.","23a5303d":"## 3.4 Text Preprocessing und Aufbereitung","424bead3":"## 3.2 Datensatz laden","1ea2cbd1":"## 4.2 Zweiter Versuch\nNun wurde die Annahme getroffen, ob sich bessere Ergebnisse nur anhand des Vectorizers - ohne weiteres Preprocessing der Textdaten - erzielen l\u00e4sst. Statt des CountVectorizers wurde der TfidfVectorizer verwendet. Der Score hat sich nicht verbessert. Stattdessen wurde ein schlechterer Score ausgegeben.","60513125":"# 2. Anwendungskontext\u00b6\nDie Gruppe nimmt an der Kaggle-Competition \"Twitter Airline Sentiment Analysis\" teil: https:\/\/www.kaggle.com\/t\/2968658cd5ce4c0db575b115251fede7   \nDie Aufgabe besteht darin, das vorgegebene Notebook zu verbessern, indem u.a. folgende Schritte durchgef\u00fchrt werden k\u00f6nnen:\n- zus\u00e4tzliches Preprocessing des Texts durchf\u00fchren, wie z.B. die Entfernung von Stoppw\u00f6rtern, Lemmatisierung (Grund- oder Stammformreduktion), Tokenisation, etc.\n- eine andere Termmatrix benutzen, wie z.B. mit Hilfe des tf-idf-Vectorizers\n- einen anderen Klassifikator ausprobieren, \n- dessen Hyperparameter optimieren, oder \n- eigene Features auf den Tweet-Texten, z.B. an Hand von regul\u00e4ren Ausdr\u00fccken einf\u00fchren (re.sub(pattern, repl, string)).\n\nLink zum Notebook, welches von Dr. Beat T\u00f6dtli erstellt wurde: https:\/\/www.kaggle.com\/toedtlifhsg\/vorlage-sentiment-analysis?scriptVersionId=72287212","5a4643c3":"![\u00dcbung 5 - 8ter Versuch.PNG](attachment:362eeb08-ac94-4a3c-8aae-e3e52495ca93.PNG)","3f506df1":"![\u00dcbung 5 - 2ter Versuch.PNG](attachment:88b6d72f-384d-4585-9d6f-37e0982e2a50.PNG)","8123cfd5":"## 4.8 Achter Versuch\nNun wurde ein weiteres Modell getestet. Den XGBClassifier. Der Score hat sich minimalst verbessert. Durch dieses Modell konnte ein neuer Bestwert ausgegeben werden.","33534059":"![\u00dcbung 5 - 1ter Versuch.PNG](attachment:ed19b60a-fcae-4b13-ac72-b8a7752673fe.PNG)","27c3879b":"## 4.6 Sechster Versuch\nDie Projektgruppe hat sich entschieden, den CountVectorizer vor dem Tfidf-Vectorizer zu bevorzugen. Jetzt sollte am Klassifikator experimentiert werden. Statt des MultinomialNB Klassifikators wurde ein RandomForest Modell genommen. Das Ergebnis hat sich minimalst verbessert.\nAusserdem wurden die Preprocessing-Schritte wieder auf den Voreinstellungen der Vorlage zur\u00fcck angepasst. Es wurde die Annahme getroffen, ob ein \u00fcberm\u00e4ssigen Preprocessing doch zu schlechteren Ergebnissen f\u00fchrt?"}}