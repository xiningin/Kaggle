{"cell_type":{"9b9696bb":"code","18bdba9e":"code","706f5e1d":"code","fd992fff":"code","d5890fe8":"code","39111350":"code","b38bd91a":"code","43b9b71c":"code","43e28b0f":"code","581e203d":"code","c3c0b862":"code","291e6f17":"code","d2e7e400":"code","8bc3a76d":"code","6aba903e":"code","7912086e":"code","729072ca":"code","0eab8344":"code","b19e8933":"code","2c1dc7f4":"code","7233da87":"code","f5671d83":"code","55f6ec79":"code","dce4834e":"code","22364c13":"code","fd75fb86":"code","9460afc8":"code","2cba0502":"code","d38ca004":"code","1e0f319b":"code","df286f8a":"code","51eb218a":"code","b18fe1a6":"code","e5da32ed":"code","f751b006":"code","92199aef":"code","bf3cda1d":"code","0dd024e1":"code","497f5219":"code","09c90482":"code","b9d4a5f6":"code","f358c811":"code","1ab43496":"code","930285ad":"code","1ff3d569":"markdown","095ecb4d":"markdown","456db6d9":"markdown","7dfde71e":"markdown","fb542071":"markdown","26d53fa7":"markdown","a9f4d63c":"markdown","ef94e21d":"markdown"},"source":{"9b9696bb":"import numpy as np\nimport pandas as pd \n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Input, Embedding, Dense, Dropout, BatchNormalization, Activation, Bidirectional, LSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nimport re\nfrom operator import itemgetter\nimport collections\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk import pos_tag\nfrom nltk import RegexpParser\nnltk.download('stopwords')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nfrom collections import defaultdict\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","18bdba9e":"df = pd.read_csv(\"\/kaggle\/input\/democratic-debate-transcripts-2020\/debate_transcripts_v3_2020-02-26.csv\", encoding=\"cp1252\")","706f5e1d":"df.head()","fd992fff":"df.shape","d5890fe8":"# Check for null values\ndf.isna().sum()","39111350":"# Drop rows that do not contain a speaking time\ndf.dropna(inplace=True)","b38bd91a":"# Review all sections\ndf.debate_section.unique()","43b9b71c":"# Review all speakers\ndf.speaker.unique()","43e28b0f":"df.speaker.value_counts()","581e203d":"df_total_speaking_time = df.groupby(df.speaker)[\"speaking_time_seconds\"].sum().sort_values()\n# Review mean and median total speaking time\ndf_total_speaking_time.mean(), df_total_speaking_time.median()","c3c0b862":"# Let's drop speakers who had limited speaking time and view the remaining speakers\ndf = df[df.groupby(df.speaker)[\"speaking_time_seconds\"].transform(\"sum\") > 1100]\ndf.speaker.unique()","291e6f17":"plt.figure(figsize=(20,7))\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=15)\nplt.ylabel('Total Speaking Time', fontsize=20)\nplt.xlabel('Speaker', fontsize=20)\ndf.groupby(df.speaker)[\"speaking_time_seconds\"].sum().sort_values().plot.bar()","d2e7e400":"# add a column for the speech with stop words and punctuation removed\nstop_words = set(nltk.corpus.stopwords.words('english'))\nfor word in [\"its\", \"would\", \"us\", \"then\", \"so\", \"it\", \"thats\", \"going\", \"also\"]:\n    stop_words.add(word)\ndf[\"speech_cleaned\"] = df[\"speech\"].apply(lambda x: \" \".join([re.sub(r'[^\\w\\d]','', item.lower()) for item in x.split() if re.sub(r'[^\\w\\d]','', item.lower()) not in stop_words]))","8bc3a76d":"# Let's look the most words used. 'People' has been by far the most used word.\nt = Tokenizer()\nt.fit_on_texts(df.speech_cleaned)\ntop_20_words = sorted(t.word_counts.items(), key=itemgetter(1), reverse=True)[:20]\ntop_20_words","6aba903e":"# Create a word cloud with the most used words\nwordcloud = WordCloud()\nwordcloud.generate_from_frequencies(dict(top_20_words))\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","7912086e":"# Create a tokenizer for each candidate in order to create a bag of words for each.\ntokenizers = defaultdict(Tokenizer)\nfor name in df.speaker.unique():\n    tokenizers[name].fit_on_texts(df.speech_cleaned[df.speaker == name])","729072ca":"for candidate in df.speaker.unique():\n    print(candidate , \"\\n\", sorted(tokenizers[candidate].word_counts.items(), key=itemgetter(1), reverse = True)[:10], \"\\n\")","0eab8344":"fact_dict = dict()\nfor candidate in df.speaker.unique():\n    if \"fact\" in tokenizers[candidate].word_index:\n        fact_dict[candidate] = tokenizers[candidate].word_counts[\"fact\"]\n    else:\n        fact_dict[candidate] = 0\nsorted(fact_dict.items(), key=itemgetter(1), reverse=True)","b19e8933":"# Healthcare appears to be a common word as well, let's see which candidates speak use the word 'healthcare' most often\nhealthcare_dict = dict()\nfor candidate in df.speaker.unique():\n    if \"healthcare\" in tokenizers[candidate].word_index:\n        healthcare_dict[candidate] = tokenizers[candidate].word_counts[\"healthcare\"]\n    else:\n        healthcare_dict[candidate] = 0\nsorted(healthcare_dict.items(), key=itemgetter(1), reverse=True)","2c1dc7f4":"# Tokenize all text in order\ntext = \"\"\ntokenized = list()\nfor speech in df.speech_cleaned:\n    text += \" \" + speech\ntokenized = text.split()","7233da87":"# Most common bi-grams\nn_grams = collections.Counter(ngrams(tokenized, 2))\nn_grams.most_common(10)","f5671d83":"# Most common tri-grams\nn_grams = collections.Counter(ngrams(tokenized, 3))\nn_grams.most_common(10)","55f6ec79":"# Most common quad-grams\nn_grams = collections.Counter(ngrams(tokenized, 4))\nn_grams.most_common(10)","dce4834e":"# Most common quint-grams\nn_grams = collections.Counter(ngrams(tokenized, 5))\nn_grams.most_common(10)","22364c13":"# Tokenize all Joe Biden text in order\ntext = \"\"\ntokenized = list()\nfor speech in df.speech_cleaned[df.speaker==\"Joe Biden\"]:\n    text += \" \" + speech\ntokenized = text.split()","fd75fb86":"# Most common Biden bi-grams\nn_grams = collections.Counter(ngrams(tokenized, 2))\nn_grams.most_common(10)","9460afc8":"# Most common Biden tri-grams\nn_grams = collections.Counter(ngrams(tokenized, 3))\nn_grams.most_common(10)","2cba0502":"# Tokenize all Bernie Sanders text in order\ntext = \"\"\ntokenized = list()\nfor speech in df.speech_cleaned[df.speaker==\"Bernie Sanders\"]:\n    text += \" \" + speech\ntokenized = text.split()","d38ca004":"# Most common Bernie Sanders bi-grams\nn_grams = collections.Counter(ngrams(tokenized, 2))\nn_grams.most_common(10)","1e0f319b":"# Most common Bernie Sanders tri-grams\nn_grams = collections.Counter(ngrams(tokenized, 3))\nn_grams.most_common(10)","df286f8a":"# review the speech lengths\ndf_speech_length = df[\"speech\"].apply(lambda x: len(x.split()))\ndf_speech_length.hist(bins=30)\ndf_speech_length.mean(), df_speech_length.median(), np.percentile(df_speech_length, 80)","51eb218a":"# set max sequence length\nmax_len = 150","b18fe1a6":"# Load reviews for sentiment analysis\ndf_reviews = pd.read_csv(\"..\/input\/imdb-reviews\/dataset.csv\", encoding=\"cp1252\")","e5da32ed":"df_reviews.head()","f751b006":"# Create stemmer\nstemmer = PorterStemmer()","92199aef":"# Clean up review text\ndf_reviews[\"SentimentTextCleaned\"] = df_reviews[\"SentimentText\"].apply(lambda x: \" \".join([stemmer.stem(re.sub(r'[^\\w\\d]','', item.lower())) for item in x.split() if re.sub(r'[^\\w\\d]','', item.lower()) not in stop_words]))                                                                                                      ","bf3cda1d":"# Take a look at cleaned up reviews\ndf_reviews[\"SentimentTextCleaned\"][:10]","0dd024e1":"review_tokenize = Tokenizer()\nreview_tokenize.fit_on_texts(df_reviews[\"SentimentTextCleaned\"])\nX_sentiment = pad_sequences(review_tokenize.texts_to_sequences(df_reviews[\"SentimentTextCleaned\"]), maxlen=max_len, padding=\"post\")\nY_sentiment = df_reviews[\"Sentiment\"]","497f5219":"# build a model for sentiment analysis\nsentiment_model = Sequential([\n    Embedding(len(review_tokenize.word_index) + 1, 64),\n    Bidirectional(LSTM(32, return_sequences=True)),\n    Bidirectional(LSTM(16)),\n    Dense(64),\n    BatchNormalization(),\n    Activation(\"relu\"),\n    Dropout(.25),\n    Dense(16),\n    BatchNormalization(),\n    Activation(\"relu\"),\n    Dropout(.25),\n    Dense(2, activation=\"softmax\")\n])","09c90482":"sentiment_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])","b9d4a5f6":"sentiment_model.fit(X_sentiment, Y_sentiment, validation_split=.1, epochs=4)","f358c811":"# Stem cleaned up speech in the debate data\ndf[\"speech_cleaned\"] = df[\"speech_cleaned\"].apply(lambda x: \" \".join([stemmer.stem(item) for item in x.split()]))\n# Review stemmed speech\ndf[\"speech_cleaned\"][:10]","1ab43496":"# Add a column to show the sentiment of each speech\npredictions = []\nfor speech in df[\"speech_cleaned\"]:\n  prediction = sentiment_model.predict(pad_sequences(review_tokenize.texts_to_sequences([speech]), maxlen=max_len, padding=\"post\"))\n  predictions.append(prediction[0][1])\ndf[\"sentiment\"] = predictions","930285ad":"# Review sentiment by speaker. The higher the number the more positive their speech is.\nplt.figure(figsize=(20,7))\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=15)\nplt.ylabel(\"Average Sentiment\", fontsize=20)\nplt.xlabel(\"Speaker\", fontsize=20)\ndf.groupby(df.speaker)[\"sentiment\"].mean().sort_values().plot.bar()","1ff3d569":"**Word Usage**","095ecb4d":"**Sentiment Analysis**\n\nLet's train a model to detect sentiment, then examine how positive or negative each speaker is. For this, we use IMDB movie reviews as training data. A smaller dataset was selected to keep training time down, but it should be good enough to provide an overview.","456db6d9":"Top Joe Biden N-grams","7dfde71e":"Unsurprisingly, Bernie Sanders and Elizabeth Warren use the word 'healthcare' the most. It is worth noting, that Michael Bloomberg did not use the word once.","fb542071":"Top Bernie Sanders N-grams","26d53fa7":"**N-grams**","a9f4d63c":"**Let's conduct some exploratory data analysis and sentiment analysis on the debate transcripts and see what we find.**\n\n**Credit**\n\nThanks to Branden Ciranni for posting the debate transcripts\nhttps:\/\/www.kaggle.com\/brandenciranni\/democratic-debate-transcripts-2020\n\nThanks to Oumainma Hourrance for posting the sentiment analysis training data\nhttps:\/\/www.kaggle.com\/oumaimahourrane\/imdb-reviews\n\n**Workflow**\n\n1. Exploratory data analysis\n2. Basic NLP on speech text\n3. Review word usage\n4. Train a model to detect sentiment\n5. Conduct sentiment analysis on speech text","ef94e21d":"One thing that jumped out to me was how often Biden uses the word \"fact\", so I compared it across the other cadidates below. Out of curiousity, I also compared use of the word 'Trump'."}}