{"cell_type":{"a443b542":"code","66222338":"code","da7b3966":"code","5c3d3b56":"code","94150e45":"code","be34cb3b":"code","5e1303b6":"code","0d5ce349":"code","2820e83a":"code","604fc1b9":"code","1fbd008c":"code","10ef389c":"code","e8c748c2":"code","51ff467b":"code","9a0f0909":"code","ee9d6c76":"code","37940056":"code","e89411fa":"code","15ade213":"code","afb21a7f":"code","8f29d001":"code","02e173b0":"code","fa01d7fe":"code","b8b64e3e":"code","2599707d":"code","ff96e521":"code","ed663988":"code","6d1189b1":"code","edb0e295":"code","e78dca4a":"code","b9d2e25a":"code","dd819111":"code","cd123688":"code","6b43227c":"code","75cbbd19":"code","9930a6f0":"code","3336046f":"code","2ebca441":"code","834983e2":"code","d22756b3":"code","8f93020a":"code","85b0bb50":"code","ba946605":"code","e8de2a4d":"code","a4625ec7":"code","fd9ae652":"code","eca0eff5":"code","467351a3":"code","de72076e":"code","ff5adeb2":"code","e628a54e":"code","080f51bc":"markdown","9a3a48d9":"markdown","86e0f264":"markdown","e6449d29":"markdown","be7d32e2":"markdown","58f3dcde":"markdown","b6da46a0":"markdown","38f0e1a7":"markdown","29bb7ecf":"markdown","c00ea05c":"markdown","bb0f7a2d":"markdown","98a5af7e":"markdown","c7e4ce0a":"markdown","32dc0f51":"markdown"},"source":{"a443b542":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66222338":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nfrom colorama import Fore\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\nimport math\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","da7b3966":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]\n\nplt.rcParams['axes.facecolor'] = primary_bgcolor\n\ncolors = [primary_blue, primary_blue2, primary_blue3, primary_grey, primary_black, primary_bgcolor, primary_green]\nsns.palplot(sns.color_palette(colors))","5c3d3b56":"plt.rcParams['figure.dpi'] = 120\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['font.family'] = 'serif'","94150e45":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain_df.columns = [column.lower() for column in train_df.columns]\n# train_df = train_df.drop(columns=['passengerid'])\n\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest_df.columns = [column.lower() for column in test_df.columns]\n\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\nsubmission.head()\n\ntrain_df.head()","be34cb3b":"feature_cols = train_df.drop(['survived', 'passengerid'], axis=1).columns\ntarget_column = 'survived'\n\n## Getting all the data that are not of \"object\" type. \nnumerical_columns = ['age', 'fare']\ncategorical_columns = train_df[feature_cols].drop(columns=numerical_columns).columns\n\npure_num_cols = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\npure_cat_cols = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\nprint(len(numerical_columns), len(categorical_columns))","5e1303b6":"train_df.info()","0d5ce349":"fig = px.histogram(\n    train_df, \n    x=target_column, \n    color=target_column,\n    color_discrete_sequence=[primary_blue, primary_grey],\n)\nfig.update_layout(\n    title_text='Target distribution', # title of plot\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n)\nfig.show()","2820e83a":"nan_data = (train_df.isna().sum().sort_values(ascending=False) \/ len(train_df) * 100)[:6]\nfig, ax = plt.subplots(1,1,figsize=(7, 5))\n\nax.bar(nan_data.index, 100, color=primary_grey, width=0.6)\n\nbar = ax.bar(\n    nan_data.index, \n    nan_data, \n    color=primary_blue, \n    width=0.6\n)\nax.bar_label(bar, fmt='%.01f %%')\nax.spines.left.set_visible(False)\nax.set_yticks([])\nax.set_title('Null Data Ratio', fontweight='bold')\n\nplt.show()","604fc1b9":"num_rows, num_cols = 2,1\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(16, 16), facecolor=primary_bgcolor)\nf.suptitle('Distribution of Features', fontsize=20, fontweight='bold', fontfamily='serif', x=0.13)\n\n\nfor index, column in enumerate(train_df[numerical_columns].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    sns.kdeplot(train_df.loc[train_df[target_column] == 0, column], color=primary_grey, shade=True, ax=axes[i])\n    sns.kdeplot(train_df.loc[train_df[target_column] == 1, column], color=primary_blue, shade=True, ax=axes[i])\n\n# f.delaxes(axes[-1, -1])\nplt.tight_layout()\nplt.show()","1fbd008c":"fig = plt.figure(figsize=(12, 8), facecolor=primary_bgcolor)\ngs = fig.add_gridspec(1, 1)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.set_facecolor(primary_bgcolor)\nax0.text(-1.1, 0.26, 'Correlation of Continuous Features with Target', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.1, 0.24, 'There is no features that pass 0.3 correlation with target', fontsize=13, fontweight='light', fontfamily='serif')\n\nchart_df = pd.DataFrame(train_df[pure_num_cols].corrwith(train_df[target_column]))\nchart_df.columns = ['corr']\nsns.barplot(x=chart_df.index, y=chart_df['corr'], ax=ax0, color=primary_blue, zorder=3, edgecolor='black', linewidth=1.5)\nax0.grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax0.set_ylabel('')\n\nfor s in [\"top\",\"right\", 'left']:\n    ax0.spines[s].set_visible(False)\n\nplt.show()","10ef389c":"train_0_df = train_df.loc[train_df[target_column] == 0]\ntrain_1_df = train_df.loc[train_df[target_column] == 1]\n\nnum_rows, num_cols = 4,2\nfig = make_subplots(rows=num_rows, cols=num_cols)\n\nfor index, column in enumerate(train_df[categorical_columns].columns):\n    i,j = ((index \/\/ num_cols)+1, (index % num_cols)+1)\n    data = train_0_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 0',\n        marker_color=primary_grey,\n        marker_line_color=primary_black,\n        marker_line_width=1.5, \n        opacity=0.8,\n    ), row=i, col=j)\n\n    data = train_1_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 1',\n        marker_color=primary_blue,\n        marker_line_color=primary_black,\n        marker_line_width=1.5, \n        opacity=0.8,\n    ), row=i, col=j)\n    \n    fig.update_xaxes(\n        title=column, \n        type='category', \n        row=i, \n        col=j\n    )\n    fig.update_layout(barmode='stack')\n    \nfig.update_layout(\n    autosize=False,\n    width=800,\n    height=1300,\n    showlegend=False,\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Categorical features<\/span>',\n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n)\nfig.show()","e8c748c2":"# label encoding\nle = LabelEncoder()\nle_data = train_df.copy().drop(columns=['passengerid'])\n\nfor col in pure_cat_cols:\n    le_data[col] = le.fit_transform(le_data[col])  \n\ncorrdata = le_data\n\n## correlation \ncorr = corrdata.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\ncorr1 = corr.mask(mask)\n\nfig = ff.create_annotated_heatmap(\n    z=corr1.to_numpy().round(2),\n    x=list(corr1.index.values),\n    y=list(corr1.columns.values),       \n    xgap=3, ygap=3,\n    zmin=0, zmax=1,\n    colorscale='blugrn',\n    colorbar_thickness=30,\n    colorbar_ticklen=3,\n)\n\nfig.update_layout(\n    title_text='<span style=\"font-size:32px; font-family:Times New Roman\">Features Correlation Matrix<\/span>', \n    font_family=\"Serif\",\n    titlefont={'size': 24},\n    width=800, height=700,\n    xaxis_showgrid=False,\n    yaxis_showgrid=False,\n    yaxis_autorange='reversed', \n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n    margin=dict(l=70, r=70, t=70, b=70, pad=1),\n)\n\nfig.show()","51ff467b":"def fix_features(all_df):\n    # Fillna on cabin feature\n    all_df[\"cabin\"] = all_df[\"cabin\"].fillna(\"Nan\")\n    all_df[\"cabin\"] = all_df[\"cabin\"].str[0]\n\n    # Fillna Age based on pclass\n    map_age_pclass = all_df[['age', 'pclass']].dropna().groupby('pclass').mean().to_dict()\n    all_df['age'] = all_df['age'].mask(all_df['age'].isna(), all_df['pclass'].map(map_age_pclass['age']))\n\n    print(all_df['age'].isna().sum())\n\n    # Fillna Age based on pclass\n    map_age_pclass = train_df[['fare', 'pclass']].dropna().groupby('pclass').mean().to_dict()\n    all_df['fare'] = all_df['fare'].mask(all_df['fare'].isna(), all_df['pclass'].map(map_age_pclass['fare']))\n\n    print(all_df['fare'].isna().sum())\n\n    # Ticket, fillna with 'X', split string and take first split \n    all_df['ticket'] = all_df['ticket'].map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n    \n    # Name, take only surnames\n    all_df['name'] = all_df['name'].apply(lambda x: str(x).split(',')[0])\n    \n    # Embarked\n    all_df['embarked'] = all_df['embarked'].fillna('X')\n    \n    return all_df","9a0f0909":"all_df = pd.concat([train_df, test_df]).reset_index(drop = True)\n\nall_df = fix_features(all_df)\n\ntrain_df, test_df = all_df[:len(train_df)], all_df[len(train_df):]\nprint(train_df.shape, test_df.shape)","ee9d6c76":"train_df.head()","37940056":"!pip install -U lightautoml","e89411fa":"# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\n\nimport torch","15ade213":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 300 # Time in seconds for automl run\n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","afb21a7f":"def acc_score(y_true, y_pred, **kwargs):\n    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ndef f1_metric(y_true, y_pred, **kwargs):\n    return f1_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ntask = Task('binary', metric = f1_metric)\n\nroles = {\n    'target': 'survived',\n    'drop': ['passengerid', 'name','ticket'],\n}","8f29d001":"%%time \nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': N_THREADS})\noof_pred = automl.fit_predict(train_df, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","02e173b0":"%%time\ntest_pred = automl.predict(test_df)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(acc_score(train_df['survived'].values, oof_pred.data[:, 0])))","fa01d7fe":"submission['Survived'] = (test_pred.data[:, 0] > 0.5).astype(int)\nsubmission.to_csv('lightautoml_utilized_300s_f1_metric.csv', index = False)\nsubmission.head()","b8b64e3e":"import h2o\nfrom h2o.automl import H2OAutoML","2599707d":"h2o.init()","ff96e521":"train_hf = h2o.H2OFrame(train_df.copy())\ntest_hf = h2o.H2OFrame(test_df.copy())","ed663988":"train_hf['survived'] = train_hf['survived'].asfactor()","6d1189b1":"%%time\naml = H2OAutoML(\n    seed=2021, \n    max_runtime_secs=300,\n    nfolds = 3,\n    exclude_algos = [\"DeepLearning\"]\n)\n\naml.train(\n    x=feature_cols.to_list(), \n    y=target_column, \n    training_frame=train_hf\n)","edb0e295":"lb = aml.leaderboard \nlb.head(rows = lb.nrows)","e78dca4a":"%%time\npreds = aml.predict(h2o.H2OFrame(test_df.drop(columns=[target_column]).copy()))\npreds_df = h2o.as_list(preds)\n\nsubmission['Survived'] = preds_df['predict']\nsubmission.to_csv('h2o_automl_300s_f1_metric.csv', index=False)\nsubmission.head()","b9d2e25a":"%%script false --no-raise-error\n!apt-get install -y build-essential python3-dev","dd819111":"%%script false --no-raise-error\n!pip -q install pip --upgrade\n!pip install graphviz --upgrade\n!pip install dtreeviz\n!pip install mljar-supervised","cd123688":"%%script false --no-raise-error\nfrom supervised.automl import AutoML # mljar-supervised","6b43227c":"%%script false --no-raise-error\n%%time\nautoml = AutoML(\n    mode=\"Compete\", \n    eval_metric=\"f1\",\n    total_time_limit=300,\n    features_selection=False # switch off feature selection\n)\nautoml.fit(\n    train[feature_cols], \n    train[target_column]\n)","75cbbd19":"%%script false --no-raise-error\n%%time\npreds = automl.predict(test[feature_cols])\n\nsubmission['Survived'] = preds\nsubmission.to_csv('mljar_automl_300s_f1_metric.csv', index=False)\nsubmission.head()","9930a6f0":"%%script false --no-raise-error\n!pip install pycaret","3336046f":"%%script false --no-raise-error\nfrom pycaret.classification import *","2ebca441":"%%script false --no-raise-error\nfrom category_encoders.cat_boost import CatBoostEncoder\n\ncat_train_df = train_df.copy()\ncat_test_df = test_df.copy()\n\nce = CatBoostEncoder()\n\ncols_to_encode = ['name', 'sex', 'ticket', 'cabin', 'embarked']\ncat_train_df[pure_cat_cols] = ce.fit_transform(cat_train_df[pure_cat_cols], cat_train_df[target_column])\ncat_test_df[pure_cat_cols] = ce.transform(cat_test_df[pure_cat_cols])","834983e2":"train_df.head()","d22756b3":"%%script false --no-raise-error\nsetup(\n    data = cat_train_df[feature_cols.to_list() + [target_column]], \n    target = target_column,\n    silent = True,\n)","8f93020a":"%%script false --no-raise-error\n%%time\nbest_models = compare_models(\n    sort='F1', \n    n_select=5, \n    budget_time=300,\n) # we will use it later","85b0bb50":"%%script false --no-raise-error\n# select best model \nbest = automl(optimize = 'F1')","ba946605":"%%script false --no-raise-error\nplot_model(best, plot = 'confusion_matrix')\n\nplot_model(best, plot = 'feature_all')","e8de2a4d":"!pip install evalml","a4625ec7":"from evalml.automl import AutoMLSearch","fd9ae652":"X = train_df.drop(columns=[target_column, 'passengerid'])\ny = train_df[target_column]\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2)","eca0eff5":"%%time\nautoml = AutoMLSearch(\n    X_train=X_train, \n    y_train=y_train, \n    problem_type='binary',\n    random_seed=2021,\n    max_time=300,\n)","467351a3":"automl.search()","de72076e":"automl.rankings","ff5adeb2":"%%time\npipeline = automl.best_pipeline\npipeline.fit(X, y)","e628a54e":"preds = pipeline.predict(test_df.drop([target_column, 'passengerid'], axis=1))\n\nsubmission['Survived'] = preds.to_series().astype(int)\nsubmission.to_csv('evalml_automl_300s_f1_metric.csv', index=False)\nsubmission.head()","080f51bc":"<a id='7'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">7. EvalML: AutoML<\/p>\n\n![evalml-logo.png](attachment:d0680f7c-05a5-4681-b9e5-c2aebf4b06f0.png)\n\nEvalML is an AutoML library which builds, optimizes, and evaluates machine learning pipelines using domain-specific objective functions.\n\n**Key Functionality**\n* **Automation** - Makes machine learning easier. Avoid training and tuning models by hand. Includes data quality checks, cross-validation and more.\n* **Data Checks** - Catches and warns of problems with your data and problem setup before modeling.\n* **End-to-end** - Constructs and optimizes pipelines that include state-of-the-art preprocessing, feature engineering, feature selection, and a variety of modeling techniques.\n* **Model Understanding** - Provides tools to understand and introspect on models, to learn how they'll behave in your problem domain.\n* **Domain-specific** - Includes repository of domain-specific objective functions and an interface to define your own.\n\nRef: https:\/\/evalml.alteryx.com\/en\/latest\/_modules\/evalml\/automl\/automl_search.html","9a3a48d9":"<a id='1.4'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.4 Categorical features<\/p>\n\nIn this case we are going to check in general terms, the distribution of categories values based on survival chances:\n- In the case of `name` and `cabin` there seems to be no possible information to stract, but will take a dive dive into those ones.\n- In the case of `sex` and `embarked` we can see that `females` and those in the high class had more chance to survive\n\n### Individual survival rate\n\n* **Sex**\n    * `Female` has higher chance to survived compared to `male`, this may also be the result of lifeboat priority for  `female` than `male`.\n    \n* **SibSp**\n    * Most of the passengers in Synthanic are travel alone, this make the survival rate for passengers without `siblings \/ spouses` higher than passengers with siblings \/ spouses.\n    \n* **Parch**\n    * As stated earlier, that most of the passengers in Synthanic are travel alone, this also make the survival rate for passenger that travel without `parents \/ children` are higher.\n    * Survival rate for passengers that travel without `parents \/ children` is almost the same with the survival rate for passenger that travel without `siblings \/ spouses`.\n    \n* **Embarked**\n    * Passengers that embarked from `Southampton` have the highest chance to survived.\n    * The second highest survival rate are passengers that embarked from `Cherbourg`.","86e0f264":"<a id='3'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. LightAutoML<\/p>\n\n![automl-generic-logo.png](attachment:5532bf4a-417b-4f19-8871-e05b3e1d17e7.png)\n\nLightAutoML is open-source Python library aimed at automated machine learning. It is designed to be lightweight and efficient for various tasks with tabular, text data. LightAutoML provides easy-to-use pipeline creation, that enables:\n\n* Automatic hyperparameter tuning, data processing.\n* Automatic typing, feature selection.\n* Automatic time utilization.\n* Automatic report creation.\n* Graphical profiling system.\n* Easy-to-use modular scheme to create your own pipelines.\n\nRef: https:\/\/github.com\/sberbank-ai-lab\/LightAutoML","e6449d29":"<a id='1.1'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.1 Target Variable<\/p>\n\nThe first thing we are going to check is the distribution of the target feature. It's important to know if the class is balanced or not. If so, we would probably have to handle it.","be7d32e2":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Tabular Playground Series \ud83d\udcda - April 2021 \ud83d\udcc8<\/p>\n\n![kaggle-python.png](attachment:73eea8b7-cb73-4d3c-8bf8-81ee6d793519.png)\n\nThe main idea of this kernel is to compare the most popular AutoMl algorithems in terms of setup and competition performance (f1-score).\n\nIn all the cases I will just limit the time to 5 minutes (it could be better to use more time, but you can always fork and try yourselfs). The idea is to give a baseline so you can use the one is easier for you or the one the better performs.\n\nAs the data is synthetic, I will do just some simple feature engineering. With this method, we can also compare how all the libraries handle categorical data etc.\n\nIn recent years, the demand for machine learning experts has outpaced the supply, despite the surge of people entering the field. To address this gap, there have been big strides in the development of user-friendly machine learning software that can be used by non-experts. The first steps toward simplifying machine learning involved developing simple, unified interfaces to a variety of machine learning algorithms","58f3dcde":"<a id='1.5'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.5 Global correlation matrix<\/p>\n\nWe can see that name and ticket have very low correlation to survival. We can easily drop them or leave them and engineer some features from them later. Sex, Embarked and Pclass have the highest correlation with survival. The highest feature-to-feature correlation is Cabin-Pclass at 0.625.","b6da46a0":"<a id='4'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. H2O AutoML<\/p>\n\n![h2oia-logo.png](attachment:5edfcd80-f2fc-4e2b-8bfb-638813e8b4dd.png)\n\nThe H2O AutoML interface is designed to have as few parameters as possible so that all the user needs to do is point to their dataset, identify the response column and optionally specify a time constraint or limit on the number of total models trained.\n\nIn both the R and Python API, AutoML uses the same data-related arguments, x, y, training_frame, validation_frame, as the other H2O algorithms. Most of the time, all you\u2019ll need to do is specify the data arguments. You can then configure values for max_runtime_secs and\/or max_models to set explicit time or number-of-model limits on your run.\n\nRef: https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html","38f0e1a7":"<a id='6'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">6. PyCaret<\/p>\n\n![pycaret.png](attachment:1f521119-fee2-4755-99ac-e2e72d0be7e9.png)\n\n*These execution cells are not going to be executed as Pycaret spent so much resources and time and cant be executed in Kaggle kernels environment. You can always fork\/clone and execute locally*","29bb7ecf":"<a id='1.3'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.3 Numerical features<\/p>\n\nThe first view over numerical variables is about show the diference distributions based on the class. As we can see, the `distplot` are very similar but there are some diferences:\n- In `age` feature it seems that people between $18-42$ tends to die with higher probability than those between $40-70$ years\n- In `fare` feature, those with lower values tend to die more than those with higher values","c00ea05c":"<a id='1'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Data visualization: EDA \ud83d\udcca<\/p>\n\nThe first thing that we can see is that we have some missing that we will have to handle later and that, as we knew, there are categorical and numerical features. \n\nAlso, we know that the class is binary.","bb0f7a2d":"<a id='5'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. MLJAR AutoML<\/p>\n\n![mljar-logo.png](attachment:53c7fa2c-719d-4001-b380-592cb5df008d.png)\n\nMLJAR is an Automated Machine Learning framework. It is available as Python package with code at GitHub: https:\/\/github.com\/mljar\/mljar-supervised\n\nThe MLJAR AutoML can work in several modes:\n\n* **Explain** - ideal for initial data exploration\n* **Perform** - perfect for production-level ML systems\n* **Compete** - mode for ML competitions under restricted time budget. By the default, it performs advanced feature engineering like golden features search, kmeans features, feature selection. It does model stacking.\n* **Optuna** - uses Optuna to highly tune algorithms: `Random Forest`, `Extra Trees`, `Xgboost`, `LightGBM`, `CatBoost`, `Neural Network`. Each algorithm is tuned with Optuna hperparameters framework with selected time budget (controlled with optuna_time_budget). By the default feature engineering is not enabled (you need to manually swtich it on, in AutoML() parameter).\n\nRef: https:\/\/www.kaggle.com\/mt77pp\/mljar-automl-tps-apr-21\n\n*These execution cells are not going to be executed as MLJAR is getting troubles to be installed in Kaggle kernels environment*","98a5af7e":"<a id='table-of-contents'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Table of Content<\/p>\n\n* [1. Data visualization: Survival Analysis \ud83d\udcca](#1)\n    * [1.1 Target](#1.1)\n    * [1.2 Data missings](#1.2)\n    * [1.3 Numerical Columns](#1.3)\n    * [1.4 Categorical Columns](#1.4)\n    * [1.5 Global correlation matrix](#1.5)\n* [2 Feature Engineering](#2)\n* [3. LightAutoML](#3)\n* [4. H2O: AutoML](#4)\n* [5. MLJAR AutoML](#5)\n* [6. PyCaretL](#6)\n* [7. EvalML alteryx](#7)","c7e4ce0a":"<a id='1.2'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.2 Data missings<\/p>\n\nIn this section we are going to take a fast look about the null values and their distribution.\nWe will handle those missing later,m but is important to have a first reference.\n\nRef: https:\/\/www.kaggle.com\/subinium\/tps-apr-highlighting-the-data","32dc0f51":"<a id='2'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Feature Engineering<\/p>"}}