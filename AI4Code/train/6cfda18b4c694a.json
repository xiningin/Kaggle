{"cell_type":{"aadb5180":"code","2a9fa859":"code","f97edcb8":"code","a8697caf":"code","be5dde07":"code","c051bb23":"code","abf06561":"code","c1c1a8a2":"code","be998517":"code","84d2d755":"code","fa638665":"code","0f40acda":"code","b06f2e1f":"code","eae1d68b":"code","7c06687d":"code","5ffc121d":"code","3f2c94bd":"code","4755cb77":"markdown","09c15ec2":"markdown"},"source":{"aadb5180":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib\nfrom sklearn import svm\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a9fa859":"# get and unpack the berlin Emodb emotional database\n!wget -c https:\/\/tubcloud.tu-berlin.de\/s\/23rPJe5rPAjmrxX\/download\n!mv download emodb_audformat.zip\n!unzip emodb_audformat.zip\n!rm emodb_audformat.zip","f97edcb8":"# install audformat and opensmile\n!pip install audformat\n!pip install opensmile","a8697caf":"import audformat\nimport opensmile","be5dde07":"# open and inspect the database\ndb = audformat.Database.load('.\/emodb')\ndb","c051bb23":"!pwd","abf06561":"# We need to set absolute path on the wav files\nroot = '\/kaggle\/working\/emodb\/'\ndb.map_files(lambda x: os.path.join(root, x))","c1c1a8a2":"# get a pandas dataframe and inspect it \ndf = db.tables['emotion'].df\ndf.head(n=1)","be998517":"# check on the class distribution\ndf.emotion.value_counts().plot(kind='pie')","84d2d755":"# listen to a sample\nimport IPython\nIPython.display.Audio(db.tables['emotion'].df.index[0])","fa638665":"# extract acoustic features with opensmile\nsmile = opensmile.Smile(\n    feature_set=opensmile.FeatureSet.GeMAPSv01b,\n    feature_level=opensmile.FeatureLevel.Functionals,\n)\nfeats_df = smile.process_files(df.index)","0f40acda":"# take a look at the result:\nfeats_df.head(n=1)","b06f2e1f":"# Check the shapes\nprint(df.shape, feats_df.shape)","eae1d68b":"# As a VERY naive approach, split train and dev set by using the first 100 samples as test\ntest_labels = df.iloc[:100,].emotion\ntrain_labels = df.iloc[100:,].emotion\ntest_feats = feats_df.iloc[:100,]\ntrain_feats = feats_df.iloc[100:,]","7c06687d":"# Now use a classsifier with out-of-the-box paremeterization\nclf = svm.SVC()\nclf.fit(train_feats, train_labels)\npred_labels  = clf.predict(test_feats)","5ffc121d":"# Look at the output\ncm = confusion_matrix(test_labels, pred_labels,  normalize = 'true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='gray')","3f2c94bd":"# This was a very lazy classfier that kinf of always decided on the majority class, we try again with some parameters \nclf = svm.SVC(kernel='linear', C=.001)\nclf.fit(train_feats, train_labels)\npred_labels  = clf.predict(test_feats)\ncm = confusion_matrix(test_labels, pred_labels,  normalize = 'true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_).plot(cmap='gray')","4755cb77":"This concludes the tutorial so far, what to do next?\n\nHere are some suggestions:\n\n* What is really problematic with the above approach is that the training and the test set are not speaker independent, i.e. the same 10 speakers appear in both sets. \n  * Which means you can not know if the classifier learned anything about emotions or (more probable) some idiosyncratic peculiarities of the speakers.\n  * With so few speakers it doesn't make a lot of sense to further devide them, so what people often do is perform a LOSO (leave-one-speaker-out) or do x-cross validation by testing x times a part of the speakers against the others (in the case of EmoDB this would be the same if x=10).\n* What's also problematic is that you only looked at one (very small, highly artificial) database and this usually does not result in a usable model for human emotional behaviour. \n  * Try to import a different database or record your own, map the emotions to the EmoDB set and see how this performs.\n* SVMs are great, but you might want to try other classifiers.\n  * Perform a grid search on the best meta-parameters for the SVM.\n  * Try other sklearn classifiers.\n  * Try other famous classifiers like e.g. XGBoost.\n  * Try ANNs (artificial neural nets) with keras or torch.\n* Try other features \n  * There are other opensmile featuresset configurations available.\n  * Do feature selection and to identify the best ones to see if they make sense.\n  * Try other features, e.g. from praat or other packages.\n  * Try embeddings from pretrained ANNs like e.g. Trill or PANN features.\n* The opensmile features are all given as absolute values.\n  * Try to normalize them with respect to the training set or each speaker individually.\n* Generalization is often improved by adding acoustic conditions to the training:\n  * Try augmenting the data by adding samples mixed  with noise or bandbass filters.\n* Last not least: code an interface that lets you test the classifier on the spot.\n","09c15ec2":"# open emodb\nThis tutorial should show how to download, open and have a first sanity check on EmoDB, the Berlin database of emotional speech [1].\n\n>[1] F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, B. Weiss: A Database of German Emotional Speech, Interspeech 2005"}}