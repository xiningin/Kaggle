{"cell_type":{"795e5037":"code","feb355c9":"code","9d843b05":"code","f99427ab":"code","a68d02c9":"code","647d0597":"code","3bf14fe3":"code","9cd26c8c":"code","82fca19b":"code","564932b0":"code","14ac7c72":"code","40d86e8e":"code","f827cd5d":"code","229fdfaa":"code","a6fe723e":"code","f5f96f86":"code","1abe00d1":"code","8af6bc61":"code","73d8a7f1":"code","83a0135c":"code","a96d19df":"code","fe31be85":"code","39c36ca9":"code","5b176c5c":"code","0c7f7111":"code","ed743cf0":"code","60e32b09":"code","b9b5c8c1":"code","ae521174":"code","b980eb32":"code","7ea285c9":"code","55a947fc":"code","6169621b":"code","ea3fc08d":"code","05be679e":"code","b0a4d7bb":"code","b0bfe816":"markdown","5beaecd7":"markdown","adcb3c04":"markdown","56724068":"markdown","cea5358d":"markdown","96bc36c7":"markdown","b97a8edf":"markdown","590b7d87":"markdown","58e7dfbd":"markdown","034307f0":"markdown","9a8d9856":"markdown","8504bba1":"markdown","64618fb6":"markdown","cd149c7b":"markdown","3821c857":"markdown","cadb84f9":"markdown","b743ca08":"markdown","884a86a2":"markdown","521bd390":"markdown","49b78c0f":"markdown","4da095d2":"markdown","cfcfa9c5":"markdown","98e5a035":"markdown"},"source":{"795e5037":"from IPython.display import display_html\nimport matplotlib.gridspec as gridspec\nfrom scipy.stats import gaussian_kde\nfrom nltk.corpus  import stopwords\nfrom itertools import chain,cycle\nfrom collections  import Counter\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport squarify\nimport warnings\nimport nltk\n  \n\nimport folium \nimport string\nimport re\n\npd.set_option('max_colwidth', 200)\nwarnings.filterwarnings(\"ignore\")","feb355c9":"# ---------- > Show two or more Dataset together\n\ndef display_side_by_side(*args): \n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n    \ndef last_replace (word):\n    return ''.join(letter for letter in word if letter != \"'\")\n    \n# ---------- > First preprocessing to remove all thos weird Symbols and more \ndef Prepro_1(text): \n    Ssymbols = [\"\\x89\",\"\\x9d\"]    \n    ''' Lower Replace #@mentions with Mention to save that pattern'''\n    x = re.sub(r'\\S*(\\w+)(_)(\\w+)\\S*',\"MENTION\",text) #Mention\n    x = re.sub(r'(https?:\/\/[^\\s]+)',\" LINK \",x) #Link\n    x = re.sub(r'(?<![@\\w])@(\\w{1,25})',\" MENTION \",x) #Mention\n    x = re.sub(r\"#(\\w+)\\S\",\" MENTION \",x) #MENTION\n    x = re.sub(r\"[\u00e2\u00e3\u00e5\u00e7\u00e8\u00ea\u00ec\u00ef\u00f1\u00f2\u00f3\u00f7\u00fb\u00fc\u00aa\u00a9\u00a8\u00a4\u00ab\u00a3\u00a2\u00ac`\\...\u00a1!\u00bc#\\$%&\\()\\*\\+,-.\/:;<=>?@\\[\\]^_`{|}~,]+\",\" \", x)\n    x = re.sub(r\"'(?!s)\",\"\", x)\n    x = re.sub(r\"\\\\n|\\\\n\\\\\",\" \" ,x) # Valdiate those spaces \/n that python was not able to decode\n    x = x.lower() # MAKE IT INTO LOWER CASE \n    x = re.sub(str(Ssymbols),\" \",x)\n    return x.lower()\n# ----------- > Used to expand contractions such has I'm to I am\ndef cont_to_exp(x, list_): #Used to expand contractions such has I'm to I am \n    x = str(x).lower()\n    if type(x) is str:\n        for key in list_:\n            value = list_[key]\n            x = x.replace(key,value)\n        return x\n    else :\n        return x\n# ---------- > used to Replace \ndef replacing_(text, args_dict):\n    for key in args_dict.keys():\n        text = text.replace(key, str(args_dict[key]))\n    return text\n\n# > --------------------- Scrapping ------------------------- <\n\ndef Scrpping (url , city, google_exe):\n    '''import selenium \n    from selenium.webdriver import Chrome,ChromeOptions, \n    from selenium.webdriver.support.ui import WebDriverWait'''\n    \n    Google_exe = google_exe\n    url = url \n    \n    options = webdriver.ChromeOptions()\n    brw = webdriver.Chrome(options=options, executable_path=Google_exe)\n    \n    #Xpath_selector \n    search_box = \"\/\/input[@class='address']\"\n    find_button = \"\/\/input[@class='big_button']\"\n    checking = \"\/\/div[@id='display_panel']\/\/div[@id='sel_loc']\/\/select\"\n    Latitute_ = \"\/\/div[@id='display_panel']\/\/div[@id='map_coords']\/\/span[@id='lat_dec']\/\/span[@class='value']\"\n    Longitude_ = \"\/\/div[@id='display_panel']\/\/div[@id='map_coords']\/\/span[@id='lon_dec']\/\/span[@class='value']\"\n    \n    coordinate = {}\n\n    for city in cities:\n        if city not in ['everywhere','earth','worldwide']:\n            try:\n                search_box = \"\/\/input[@class='address']\"\n                find_button = \"\/\/input[@class='big_button']\"\n                checking = \"\/\/div[@id='display_panel']\/\/div[@id='sel_loc']\/\/select\"\n                Latitute_ = \"\/\/div[@id='display_panel']\/\/div[@id='map_coords']\/\/span[@id='lat_dec']\/\/span[@class='value']\"\n                Longitude_ = \"\/\/div[@id='display_panel']\/\/div[@id='map_coords']\/\/span[@id='lon_dec']\/\/span[@class='value']\"\n\n                # -----> Clean and Search City\n                brw.find_element_by_xpath(search_box).clear()\n                time.sleep(1)\n\n                brw.find_element_by_xpath(search_box).send_keys(city.capitalize())\n                brw.find_element_by_xpath(find_button).click()\n\n                # -----> Waiting \n                WebDriverWait(brw, 60).until(EC.presence_of_element_located((By.XPATH,checking))).click()\n\n                #Get Info \n                Lat = brw.find_element_by_xpath(Latitute_).text\n                Lon = brw.find_element_by_xpath(Longitude_).text\n\n                #Inserting \n                coordinate[city] = [Lat,Lon]\n            except:\n                print('Error in', city)\n                coordinate[city] = [0,0]\n        else:\n            print('City name', city)\n            coordinate[city] = [0,0]\n\n# -------------------- Dictionaries ----------------------------------\n\n# Dictinaries \ncontractions = {\"'re\":'are',\"u're\":\"you are\",\"they've\":\"they have\",\"i'm\":\"i am\",\"you've\":\"you have\",\"we've\":\"we have\",\"wouldn't\":'would not',\"couldn't\":'could not',\"isn't\":'is not',\"don't\":'do not',\"haven't\":'have not',\"hasn't\":'has not',\"doesn't\":'does not',\"I've\":'i have',\"aren't\":'are not',\"shouldn't\":'should not',\"can't\":'can not',\"hadn't\":'had not',\"wasn't\":'was not',\"won't\":'will not',\"there's\":'there is',\"didn't\":'did not',\"weren't\":'were not',\"that's\":'that is',\"he's\":'he is',\"she's\":'she is',\"they're\": 'they are',\"cya\":'see you',\"we're\":'we are',\"it's\":'it is',\"you're\":\"you are\",\"i've\":'i have',\"wtf\":'what the fuck',\"must've\":'must have',\"should've\":\"should have\",\"would've\":\"would have\",\"could've\":\"could have\",\"how're\":'how are',\"it's\":'it is',\"what's\":'what is',\"\\\\\":\"\",\"\\\\n\":\"\",\"cuz\":\"because\"}\nspecial_words = {\"ayhhhdjjfjrjjrdjjeks\":'','weallheartonedirection':'we all heart one direction','awesomeeeeeeee':'awesome','prettyboyshyflizzy':'pretty boy shy flizzy','championsblackfoot':'champions black foot','mothernaturenetwork':'mother nature network','referencereference':'reference','jaylenejoybeligan':'jay lene joy beligan',\"measuresmention',\":'measure mention','postapocalypticflimflam':'post apocalyptic flim flam',\"antiochhickoryhollowmention',\":'antioch hickory hollow mention','indianperpetrated':'indian perpetrated', \"'tiffanyfrizzell\":'tiffany frizzell',\"'robot_rainstorm\": 'robot rainstorm','hawaiianpaddlesports':'hawaiian paddle sports',\"ayhhhhhdjjfjrjjrdjjeks',\":'','siteinvestigating':'site investigating',\"'hamptonroadsfor\":'hampton roads for','undergroundbestsellers':'underground best sellers',\"zimpapersviews',\":'zim papers views',\"'mentionmentionmentionmentionmention\":'mention',\"kwaaaaamention',\":'mention',\"abbruchsimulator',\":'abbruch simulator','romanatwoodvlogs':'roman at wood blogs','sidjsjdjekdjskdjd':'','electrocutedboiling':'electrocuted boiling','lavenderpoetrycafe':'lavender poetry cafe','deejayempiresound':'dee jay empire sound','lulgzimbestpicts':'lulzim best picts','conditionsprivation':'conditions privation',\"'neil_eastwood77\":'neil eastwood 77','derailed_benchmark':'derailed benchmark'}\nnumer_words = {'sherfield72':'mention','rs40000cr':'mention','air1bullet':'air bullet','960kzim':'mention',\"'revel8ion\":'revelation',\"'news786\":'news 786',\"'mumbai24x7\":'mention','17months':'17 months','ny1burst':'new york burst','tkyonly1fmk':'mention','nikoniko12022':'mention','hamosh84':'mention','ibliz140':'mention','bkb066gp':'mention','tksgs0810':'mention','mhtw4fnet':'mention',\"'psalm34\":'mention',\"76mins\":'76 minutes',\"oliviamiles01\":'mention','friend50':'friend 50', \"'master0fsloths\":'master of sloths',\"'s3xleak\":'sex leak','push2left':'push to left',\"'411naija\":'mention','wsvr1686b':'mention','911bombing':'911 bombing',\"fuckjf9jjzs',\":'mention','b4federal':'mention','7amdollela':'7 am dollela','3million': '3 million','vaping101':'mention',\"'matako_3\":'mention','150bilno':'mention',\"'neil_eastwood77\":'mention','hirochii0':'mention',\"20mins',\":'20 mins',\"'chickmt123\":'mention','drayesha4':'mention','death2usa':'death to usa',\"2gether',\":'together','time2015':'time 2015',\"'nflweek1picks\":'mention','martinmj22':'mention',\"'lizzie363\":'mention','russaky89':'mention','friend59':'mention',\"you5jbwgge',\":'mention'}\nword_numbers = {'06jst':'', '0npzp':'', '10x':'', '12jst':'', '12news':'', '12v':'', '18jst':'', '1x1':'', '24v':'', '325ci':'', '3x':'', '3x5':'', '429cj':'', '43c':'', '4x4':'', '53inch':'', '6c':'', '712c':'', '_':'', '__':'', '_ah':'', '_deo':'', '_keits':'', '_that':'', '_turns':'',\"',\":\"\",\"sherfield72\":\"mention\",\"pemantaujkt48\":\"mention\",\"bracelet10mm\":\"mention\", \"'chickmt123\":\"mention\",\"'tarmineta3\":\"mention\",\"you5jbwgge',\":'mention',\"'mumbai24x7\":'mention','5sosglobalsquad':'mention','dylanmcclure55':'mention','tanstaafl23':'','footballfreestyle24':'football free style 24',\"40hourfamine\":\"40 hour famine\",\"15000270364',\":'number','dylanmcclure55':'mention ',\"'master0fsloths\":'master of sloths','insurers163':'insurers 163',\"15000270653',\":'number', 'zonewolf123':'mention',  \"'tkyonly1fmk\":'mention', \"sherfield72'\":'mention ', \"8437150124',\":'number', '40hourfamine':'40 hour famine','watchmanis216':'mention','tarmineta3':'mention',\"'neil_eastwood77\":'mention',\"oliviamiles01',\":'mention','hyider_ghost2':'mention ','rabidmonkeys1':'mention ','since1970the':'since 1970 the', \"'nflweek1picks\":'mention','then0mads0ul':'mention',\"fuckjf9jjzs',\":'','janenelson097':'mention','087809233445':'0','onlinemh370':'mention', '2slow2report':'to slow to report',\"'nikoniko12022\":'mention', 'shawie17shawie':'mention'}\nrepetead_vowels = {\"soooo\":\"so\",\"baaaack\":\"back\",\"aaaaaaallll\":\"all\",\"sparxxx\":\"mention\",\"oooh\":\"oh\",\"lmfaooo\":\"laughing my fat ass off\",\"fuckkkkkk\":\"fuck\",\"vuuuuu\":\"vu\",\"mxaaaa\":\"mention\",\"caaaaaall\":\"call\",\"goooooooaaaaaal\":\"goal\",'shoook':'shook','maaaaan':'man','sooooooo':'soon','blaaaaaaa':'bla','youuu':'you','yeeessss':'yes','noooo':'no','totoo':'totoo',\"ooohshit\":'oh shit','nooo':'no','wompppp':'womp','maddddd':'mad',\"'thankkk\":\"thank\",\"plsss\":\"please\",'ohhhh':'oh',\"'xoxoxxxooo\":'hugs and kisses',\"'damnnnn\":\"damn\",\"'damnnnn\":\"damn\",\"ooo\":\"oo\",\"oooo\":\"oo\",\"ooooo\":\"oo\",\"ssssss\":\"ss\",\"ssss\":\"ss\",\"yyyy\":'y',\"yyyy\":'y',\"lll\":\" ll \",\"eee\":\"ee\",\"wwwww\":\"w\",\"aaa\":\"a\",\"nnn\":\"n\",\"rrr\":\"r\",\"aaaaaa\":\"a\",\"gggg\":\"g\",\"wwww\":\"w\",\"www\":\"w\",\"mmmm\":\"m\",\"iiii\":\"i\",\"iii\":\"i\",\"sss\":'s',\"yyy\":\"y\",\"uuu\":\"u\",\"eee\":'e','hhh':'h',\"ddd\":\"d\",\"zzzz',\":\"sleep\",\"xxx',\":\"\",\"mhmmm\":\"hmm\",\"hmmm\":'hmm'}\n\n\n# Tokenazation \ndef Pre_toke (corpus):\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    \n    #Removing Stop Words\n    NEC = [word for word in corpus.split() if word.lower() not in stopwords.words('english')]\n    \n    # Converting into Indefinitive tense\n    NEC = ' '.join([lemmatizer.lemmatize(word, 'v') for word in NEC])\n    NEC = NEC.split()\n    return NEC\n\ndef word_replace (sentences,dictionary):\n    new_sentence = []\n    for words in sentences.lower().split():\n        if words in dictionary.keys():\n            new_word= dictionary[words]\n            new_sentence.append(str(new_word))\n        else:\n            new_sentence.append(words)\n    return ' '.join(str(w) for w in new_sentence)","9d843b05":"# ----> Define Path\ntrain_path = '..\/input\/nlp-getting-started\/train.csv'\ntest_path  = '..\/input\/nlp-getting-started\/test.csv'\n\n# ----> Loading\ntrain_, test_ = pd.read_csv(train_path),pd.read_csv(test_path)\n\n# ----> customizing for Visualizations\ncolumns = ['text']\ntrain_['Length'] = train_['text'].apply(lambda x: len(x))\ntest_['Length'] = test_['text'].apply(lambda x: len(x))\n\n\n# Samples \nxprows = train_[(train_['text'].str.contains(\"n't\"))&(train_['Length'] <=60)].index[:10]\nmrows = train_[(train_['text'].str.contains(\"@|#\"))&(train_['Length'] <=60)].index[:10]\nanrow = train_[(train_['text'].str.contains(r'(\\w{1}\\d{1}|\\d{1}\\w{1})', regex=True))&(train_['Length']<=60)].tail(10).index\nlyrow = train_[(train_['text'].str.contains(r'ly>|lly$|<un|ry$|ble$|ing$|ed$|tion$|tional$', regex=True))&(train_['Length']<=60)].head(5).index\n\ntsrows = test_[test_['Length']==60].head(10).index[:5]\ntrows = train_[train_['Length']==60].head(10).index[:5]\n\n#Pandas Styles Format \ntitles = {'selector': 'caption','props': [('color', '#1DA1F2'),('font-size', '13px')]}\ntrain_set = train_.loc[trows , columns].head(10).style.set_caption('Training Set').set_table_styles([titles]).hide_index()\ntest_set =test_.loc[tsrows,columns].head(10).style.set_caption('Testing Set').set_table_styles([titles]).hide_index()\n\ndisplay_side_by_side(train_set,test_set)","f99427ab":"#Remove it to show the sentence \"on the outside you're ablaze and alive\\nbut you're dead inside\"\ncorpus_ = [row for row in train_.text][41] \n\n# ----------- > Removing Spaces \ntrain_['text_'] = train_['text'].apply(lambda x: re.sub(r\"\\\\|\\n|\\\\\\n|\\r|\\t\",\" \" ,x)) ##Removing \ntest_['text_'] = test_['text'].apply(lambda x: re.sub(r\"\\\\|\\n|\\\\\\n\\\\|\\r|\\t\",\" \" ,x)) ##Removing \ncorpus = [row for row in train_.text_]\nprint(f\"Here the example where we can see 'tab-space' in the sentence\\n-----------------> ***{corpus_}***\")\nprint(f\"Here the example after removing 'tab-space'\\n-----------------> ***{corpus[41]}***\")","a68d02c9":"# ------- > Expanda words to avoid losing \"'\"  in words such as Don't or didn't\nex_contractions= []\nfor word in (set(str(corpus).lower().split())):\n    if bool(re.search(r\"'t$|n't$|'re$|'m$|'s>'|'ve\",word)):\n        ex_contractions.append(word)\n        \n# ------- >  Expanding\ntrain_['text_'] = train_.text_.apply(lambda x: cont_to_exp(x,contractions))\ntest_['text_'] = test_.text_.apply(lambda x: cont_to_exp(x,contractions))\n\n# --------> Samples Xprows = Expanding Rows\nbefore = train_.loc[xprows,['text']].style.set_caption('Before Expanding').set_table_styles([titles]).hide_index()\nafter =train_.loc[xprows,['text_']].style.set_caption('After Expanding').set_table_styles([titles]).hide_index()\n\ndisplay_side_by_side(before,after)","647d0597":"# Remove @\/# Mentions, special characters, Links, Symbols\ntrain_['text_'] = train_.text_.apply(lambda x: Prepro_1(x))\ntest_['text_'] = test_.text_.apply(lambda x: Prepro_1(x))\n\nbefore = train_.loc[mrows,['text']].style.set_caption('Before Removing #').set_table_styles([titles]).hide_index()\nafter =train_.loc[mrows,['text_']].style.set_caption('After Removing #').set_table_styles([titles]).hide_index()\n\ndisplay_side_by_side(before,after)","3bf14fe3":"ex_numer_words_ = []\nfor word in (set(str(corpus).lower().split())):\n    if len(word) > 7 and bool (re.search(r'(\\w{1}\\d{1}|\\d{1}\\w{1})', word)) ==True:\n        ex_numer_words_.append(word)\n\ntrain_['text_'] = train_.text_.apply(lambda x: cont_to_exp(x,numer_words))\ntest_['text_'] = test_.text_.apply(lambda x: cont_to_exp(x,numer_words))\n\nbefore = train_.loc[anrow,['text']].style.set_caption('Before Preprocessing').set_table_styles([titles]).hide_index()\nafter =train_.loc[anrow,['text_']].style.set_caption('After Preprocessing #').set_table_styles([titles]).hide_index()\n\ndisplay_side_by_side(before,after)","9cd26c8c":"# Words Excessively Large that dont content \"lly, Un, Dis, ing, tion, tional\"\nex_word_numbers = []\ncorpus = [row for row in train_.text_]\nfor word in (set(str(corpus).lower().split())):\n    if len(word)>10 and bool(re.search(r'ly>|lly$|<un|ry$|ble$|ing$|ed$|tion$|tional$',word))==False:\n        if bool(re.search(r'\\d',word)):\n            ex_word_numbers.append(word)\n            \n# --------> Word Excessively Large\ntrain_['text_'] = train_.text_.apply(lambda x: cont_to_exp(x,word_numbers))\ntest_['text_'] = test_.text_.apply(lambda x: cont_to_exp(x,word_numbers))\n\n\n# --------> Repeated Words \nrepeated_ = []\ncorpus = [row for row in train_.text_]\nfor word in (set(str(corpus).lower().split())):\n#     r'\\S*(\\w)(?=\\1{2,})\\S*\n#     \\S*(a|e|i|o|u)(?=\\1{2,})\\S*\n    if bool(re.search(r'\\S*(a|e|i|o|u)(?=\\1{2,})\\S*',word)):\n            repeated_.append(word)\n            \n\ntrain_['text_'] = train_.text_.apply(lambda x: cont_to_exp(x,repetead_vowels))\ntest_['text_'] = test_.text_.apply(lambda x: cont_to_exp(x,repetead_vowels))\n\n# --------> Repalcing Numbers \ntrain_['text_'] = train_.text_.apply(lambda x: re.sub(r\"\\b\\d+\\b\", \" number \",x))\ntrain_['text_'] = train_.text_.apply(lambda x: re.sub(r\"\\b([0-9+]+[rd|th|nd]+)\\b\", \" number \",x))\n\ntest_['text_'] = test_.text_.apply(lambda x: re.sub(r\"\\b\\d+\\b\", \" number \",x))\ntest_['text_'] = test_.text_.apply(lambda x: re.sub(r\"\\b([0-9+]+[rd|th|nd]+)\\b\", \" number \",x))\n'year', 'years'\n#---------> Replacing time \ntrain_['text_'] = train_.text_.apply(lambda x: re.sub(r\"([0-9]+[am|pm|hr|s|yr|yrs|hours|day|days|years|yesterday|year])\", \" time \",x))\ntest_['text_'] = test_.text_.apply(lambda x: re.sub(r\"([0-9]+[am|pm|hr|s|yr|yrs|hours|day|days|years|yesterday|year]+)\", \" time \",x))\n\n#---------> Measurement\ntrain_['text_'] = train_.text_.apply(lambda x: re.sub(r\"([0-9]+[fps|mm|km|w|ft|oz|lbs|whts|kg]+)\",\" measure \",x))\ntest_['text_'] = test_.text_.apply(lambda x: re.sub(r\"([0-9]+[fps|mm|km|w|ft|oz|lbs|whts|kg]+)\",\" measure \",x))\n\nprint(f\"These words are excessively large\\n{ex_word_numbers[:10]}\")\nprint(f\"There are Numbers \\n{repeated_[:13]}\")\nprint(\"Final Data Set\")\n\ndisplay_side_by_side(train_.loc[xprows,['text']],train_.loc[xprows,['text_']])","82fca19b":"# ----------------> Visualization\nVis = train_.copy()\n\n#DropNa\nVis.dropna(inplace=True)\n\n# Lowering and Splitting and Reseting Index\nVis['location'] = Vis['location'].str.lower()\nVis[['city','country']]= Vis.location.str.split(\",\", expand=True,n=1).rename(columns= {0:'Locations'})#This Will generate 2 columns \nVis.reset_index(drop=True, inplace=True) \n\n#top 50 cities mentions in Twitter\ncity = Vis.city.value_counts().head(50).index\nurl = 'https:\/\/www.findlatitudeandlongitude.com\/'\n\n#--------------> def Scrpping (url , city, google_exe)\n\n# Uploading Coordinates \nCoor = pd.read_csv('..\/input\/coordinates\/Coordinate.csv').rename(columns = {'Unnamed: 0':'cities'})\nCoordinate_ = Coor.values\n\n# Matching and Adding \nfor row in range(len(Vis.city)):\n    location =  Vis.loc[row,'city']\n    for city_ in Coordinate_:\n        if city_[0] == location:\n            Vis.loc[row, 'Lat'],Vis.loc[row, 'Long'] = int(city_[1]) , int(city_[2])\n\n# Grouping and Creating new data\nMap = Vis.groupby(['city','Long','Lat','keyword'])['text'].count().reset_index()\n\n# -----------------> Creating Map     \nMM = folium.Map(tiles='openstreetmap', min_zoom= 3)\n\n# Adding Mentions\nfor row_ in range(0,len(Map.keyword)):\n    CP = Map.loc[row_,'city']  #Current Position will show the current row \n    folium.Circle(radius =  20000 * Map[Map['city']== CP]['text'].max(),\n                  location = [Map.iloc[row_]['Lat'],Map.iloc[row_]['Long']],\n                  popup = '<h4><b>'+str(Map.iloc[row_]['city']).upper()+'<\/b><h4>',\n                  tooltip = '<li><b>Location:<\/b>&nbsp'+ str(CP)+'<\/li>'+\n                  '<li><b>Twitters:<\/b>&nbsp'+ str(' '.join(str('#'+tw.capitalize()) for tw in Map[Map['city']==CP]['keyword'].value_counts()[:5].index.to_list()))+':&nbsp<\/li>'+\n                  '<li>These were the top 5 Mentions out of <b>'+str(Map[Map['city']==CP]['keyword'].nunique())+ '<\/b> in Twitter Disasters in this City<\/li>',\n                  color='#01FE72',fill=True, fill_color='#6ACAFE').add_to(MM)\n# Plotting \nMM","564932b0":"fig,ax = plt.subplots(1, figsize=(14,8))\n# ------------------> Lolipop Graph \n#--> Visualize Keywords \nkey_ = train_[train_.keyword.notna()][['keyword']].value_counts().reset_index().rename(columns = {0:'mentions'})\ntkey_ = test_[test_.keyword.notna()][['keyword']].value_counts().reset_index().rename(columns = {0:'mentions'})\nVkey = pd.merge(key_, tkey_, on='keyword')\nVkey['Dif'] = Vkey.mentions_x - Vkey.mentions_y\n\n#--> Re-Order and Following key_\nordered_ = Vkey.sort_values(by='mentions_x')[:50]\nrange_ = Vkey.sort_values(by='mentions_x')['keyword'].values[:50]\ndif = Vkey.sort_values(by='mentions_x')['Dif'].values[:50]\n\nax.hlines(y=range_ ,xmin=ordered_['mentions_x'], xmax= ordered_['mentions_y'], color = '#6ACAFE', alpha = 0.4)\nax.scatter(ordered_['mentions_x'], range_, color='#AAB8C2', alpha=1, label='Mentions in training set')\nax.scatter(ordered_['mentions_y'], range_, color='#1DA1F2', alpha=0.4 , label='Mentions in testing set')\n\n# Add title and axis names\nax.set_title(\"Comparison of Mentions in both Data sets\", loc='left')\nax.set_xlabel('Number of Mentions')\nax.set_ylabel('Keywords')\n\nax.annotate('Twiters for bombing in Training', xy=(13,15), xycoords='data',xytext=(-100,60), textcoords='offset points',\n            arrowprops=dict(arrowstyle='fancy',fc='0.6',connectionstyle=\"angle3,angleA=0,angleB=-90\",color=\"#14171A\"))\nax.annotate('Twiters for bombing in Testing', xy=(29,15), xycoords='data',xytext=(-100,60), textcoords='offset points',\n            arrowprops=dict(arrowstyle='fancy',fc='0.6',connectionstyle=\"angle3,angleA=0,angleB=-90\",color='#1DA1F2'));","14ac7c72":"fig,ax = plt.subplots(1, figsize=(14,8))\n# -----------> Heat map\ncate = train_['keyword'].value_counts().head(50).index\nvalues = np.round(train_['keyword'].value_counts(normalize=True).head(50).values,4)\n\ncolor = ['#1DA1F2','#657786','#AAB8C2','#E1E8ED','#F5F8FA']\nlabels = [f\"{cate_}\\n{values_}\" for cate_,values_ in zip(cate,values)]\nsquarify.plot(sizes=values, label=labels, alpha=.9 ,ax=ax, color=plt.cm.viridis.colors[1:250:5])\nax.set_title('Keywords Heat map: Keyword Distribution')\nax.axis('off');","40d86e8e":"target_1 = train_[train_['target'] == 1][['Length']]\ntarget_0 = train_[train_['target'] == 0][['Length']]\n\nfig,ax = plt.subplots(1,3,figsize=(20,5))\n\n# # ---------------------> Pie chart\nax[2].set_title('Twitter Disaster Fake\/Reals')\nnames_= list(train_['target'].value_counts().index)\nsize_= list(train_['target'].value_counts().values)\nlabels = [f'{name}: {values}' for name,values in zip(['Fake','Real'],size_)]\n\n#--> Creating circle \nmy_circle = plt.Circle( (0,0), 0.7 , color='white')\n\n#--> customizing Wedges\nplt.pie(size_, labels= labels , wedgeprops = { 'linewidth' : 7,'edgecolor':'white'} ,\n        colors = ['#AAB8C2','#1DA1F2'])\nax[2]= plt.gcf()\nax[2].gca().add_artist(my_circle);\n\nax[0].set_title('Length Distribution segmented by Ham\/spam')\nsns.histplot(data=target_1, x='Length',  fill=True, alpha=1,color= \"#657786\",ax=ax[0], label = 'Fake' )\nsns.histplot(data=target_0, x='Length',  fill=True, alpha=0.8,color= \"#1DA1F2\",ax=ax[0], label = 'Real')\nax[0].legend()\n\n\nax[1].set_title('Length Distribution segmented by Train\/Test')\nsns.histplot(data=train_, x='Length',  fill=True, alpha=1,color= \"#1DA1F2\",ax=ax[1], label = 'Train')\nsns.histplot(data=test_, x='Length',  fill=True, alpha=.8,color= \"#AAB8C2\",ax=ax[1], label = 'Test')\nax[1].legend();","f827cd5d":"# ----> Labels\nMLabel = train_.groupby(['text']).nunique().sort_values(by='target',ascending=False)\nMLabel.head(3)","229fdfaa":"MLabel_ =  MLabel[MLabel['target'] > 1]['target'].index\n\n# ----> Finding \nindexes = []\nfor tweet in MLabel_:\n    position= train_[train_['text']==str(tweet)].index\n    for index_ in position:\n        indexes.append(index_)\n        \ntrain_.loc[indexes,['text','target']][0::2].head(5)","a6fe723e":"new_label_= {'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit':0,\n 'Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!':0,\n\"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\":0,\n \"In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!\":0,\n \"To fight bioterrorism sir.\":0,\n \"Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE\":0,\n \"#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\":1,\n \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\":0,\n \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\":0,\n \"RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG\":1,\n \"Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife\":1,\n \"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring\":1,\n \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\":0,\n \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\":0,\n \".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4'\":1,\n \"Caution: breathing may be hazardous to your health.\":1,\n \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\":0,\n \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\":0,\n\"Businesses are deluged with invoices. Make yours stand out with colour or shape and it's likely to rise to the top of t e pay' pileq\":0}\n\n# ------> Relabeling \ntrain_['new_label'] = train_['target']\nfor key, value in new_label_.items():\n    train_.loc[train_['text'] == str(key) ,['new_label']] = value\n    \ntrain_.loc[indexes,['text','new_label']][0::2].head(5)","f5f96f86":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn import svm\n\n#All models\nmodels = []\n\n#Final Corpus \ny = train_['new_label'].values\ntrain_corpus = train_.text_.values\ntest_corpus = test_.text_.values\n\nX_train, X_test, y_train, y_test = train_test_split(train_corpus, y, test_size=0.3, random_state=0, stratify=y)\n\n#Final Corpus\ndisplay_side_by_side(train_.loc[xprows,['text']],train_.loc[xprows,['text_']])","1abe00d1":"results_ = []\nscoring = ['accuracy','recall','precision','f1']\nModel = [MultinomialNB(),svm.SVC(kernel ='rbf'),PassiveAggressiveClassifier(), RandomForestClassifier(max_depth= 30)]\n\nfor algo in Model:\n    Pipe_ = Pipeline([('CounterVF', CountVectorizer(analyzer = Pre_toke)),\n                      ('TfidF', TfidfTransformer()),('Model', algo)])\n    \n    scores = cross_validate(Pipe_,X_train,y_train,cv=6,scoring = scoring)\n    results_.append(scores)\n    \ndf_result = pd.DataFrame()\nfor position in range(0,len(results_)):\n    df = pd.DataFrame(data = results_[position])\n    df['model'] = str(Model[position].__class__.__name__)\n    df_result = df_result.append(df, ignore_index=True)\n    \ndf_result.sort_values('test_precision',ascending = False).head(4)","8af6bc61":"prediction = []\npre_transform = Pipeline([('CounterVF', CountVectorizer(analyzer = Pre_toke)),\n                      ('TfidF', TfidfTransformer()),]).fit(train_corpus)\n\nModel = [svm.SVC(kernel ='rbf'), RandomForestClassifier(),MultinomialNB()]\n\nfor algo in Model:\n    CLS = algo.fit(pre_transform.transform(train_corpus),y)\n    predict_ = CLS.predict(pre_transform.transform(test_corpus))\n    frame = pd.DataFrame()\n    frame['id'] = test_['id']\n    frame['target'] = predict_\n    frame.to_csv(\"{modelname}.csv\".format(modelname = algo.__class__.__name__))","73d8a7f1":"BParameters_ = []\n\n# Hyper Parameters\nparams = [{'SVC__C': [1,10,100,1000],'SVC__gamma':[1,0.1,0.01,0.001],'SVC__kernel':['rbf','linear']},\n          {'RandomForestClassifier__max_depth':[10,15,30,40,50],'RandomForestClassifier__min_samples_split':[2,4,6,8],'RandomForestClassifier__min_samples_leaf':[3,4,5,6]}]\n\nModel = [svm.SVC(), RandomForestClassifier()]\nfor algo_pos in range(0,len(Model)):\n    Pipe_ = Pipeline([('CounterVF', CountVectorizer(analyzer = Pre_toke)),\n                      ('TfidF', TfidfTransformer()),(str(Model[algo_pos].__class__.__name__), Model[algo_pos])])\n    \n# #     RDSearch  = RandomizedSearchCV(Pipe_,param_distributions=params[algo_pos],cv=4)\n# #     RDSearch.fit(train_corpus,y)\n# #     BParameters_.append(RDSearch.best_params_)\n","83a0135c":"CL1 = RandomForestClassifier(max_depth= 30)\nCL2 = svm.SVC(kernel ='rbf', gamma = 1 , C=1,probability=True)\nCL3 = MultinomialNB()\n\nVC = VotingClassifier(estimators=[('RDF',CL1),('SVM',CL2),('NBM',CL3)],\n                      voting = 'soft')\nVC.fit(pre_transform.transform(train_corpus),y)\nVCpred = VC.predict(pre_transform.transform(test_corpus))\nframe = pd.DataFrame()\nframe['id'] = test_['id']\nframe['target'] = VCpred\n\nframe.to_csv(\"{modelname}_.csv\".format(modelname = VC.__class__.__name__))","a96d19df":"import tensorflow.keras as tf\nfrom tensorflow.keras.layers import Activation, MaxPooling1D,BatchNormalization,Conv1D,Dropout,GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding, LSTM, GRU ,GlobalAveragePooling1D,MaxPooling1D,Bidirectional\nfrom tensorflow.keras.preprocessing.text import one_hot,Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import SGD, Adam , RMSprop\nfrom tensorflow.keras.models import Sequential ","fe31be85":"# Preprocessing \nToken = Tokenizer()\nToken.fit_on_texts(train_corpus)\n\nToken_t = Tokenizer()\nToken_t.fit_on_texts(test_corpus)\n\n# set up \nVoc_size = len(Token.word_index)+1 #13059\nMax_length = 30\noutput_dim = 7\n\n# Enconding twitters\nEnc_Tw = [one_hot(twit,Voc_size) for twit in train_corpus]\nEnc_Tw_test = [one_hot(twit,Voc_size) for twit in test_corpus]\n\n\nPad_Tw = pad_sequences(Enc_Tw, maxlen = Max_length , padding= 'post')\nPad_Tw_test = pad_sequences(Enc_Tw_test, maxlen = Max_length , padding= 'post')\n\n# Splitting Data \nX_train, X_test, y_train, y_test = train_test_split(Pad_Tw, y, test_size=0.2, random_state=104, stratify=y)\n\n# Settting \noptimizer= Adam()\nloss = tf.losses.BinaryCrossentropy(from_logits=True,)","39c36ca9":"model = Sequential()\nmodel.add(Embedding(input_dim = Voc_size,output_dim =output_dim,\n                     input_length = Max_length,embeddings_regularizer = tf.regularizers.L2(1e-4)))\nmodel.add(Dropout(0.3))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(5,kernel_initializer='glorot_uniform',kernel_regularizer=tf.regularizers.L2(0.1),activation = 'relu'))\nmodel.add(Dropout(0.2)) \nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer = optimizer, loss=loss, metrics = ['accuracy'])\nmodel.fit(X_train , y_train , batch_size=128, epochs = 200, validation_data = (X_test,y_test))","5b176c5c":"pd.DataFrame(model.history.history).plot(figsize=(15,7))\nplt.show()","0c7f7111":"#submit\nprediction = model.predict(Pad_Tw_test) \nresults = [1 if pred_ >= 0.5 else 0 for pred_ in prediction]\ndf = pd.DataFrame()\ndf['id'] = test_['id']\ndf['target'] = results\ndf.to_csv('FS_deep_1st.csv')","ed743cf0":"# -----> Using word2vector weights \nEmbeddings= {}\nEmbedding_dimension = 100\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as file:\n    for line in file:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        Embeddings[word] = vectors","60e32b09":"EMB_DIM = 100\nEM_TR_NO,EM_TS_NO = [],[]  # EM = Embedding, TR\/TS = Train \/ Test , NO = No in \nEMB_WEIGHTS = np.zeros((Voc_size,EMB_DIM)) # Matrix of numebrofwordsX100\n\n\nfor word, postition in Token.word_index.items():\n    EMB_VECTOR = Embeddings.get(word) #take the dict key and find the word in the dict\n    if EMB_VECTOR is not None:\n        EMB_WEIGHTS[postition] = EMB_VECTOR # this because the dict position start in 1 not 0 \n    else: # it is because the word from the traning set in not in the embedding\n        EM_TR_NO.append(word)\n\nfor word, i in Token_t.word_index.items():\n    EMB_VECTOR = Embeddings.get(word)\n    if EMB_VECTOR is None: # ithe word from the test set it is not in the embedding\n        EM_TS_NO.append(word)\n        \nTrain_Embedding = len(EM_TR_NO)*100\/len(Token.word_index)\nTest_Embedding = len(EM_TS_NO)*100\/len(Token_t.word_index)\nprint(f\"The Embedding Contains {np.round(100-Train_Embedding,2)}% of the words in the Training set\")\nprint(f\"The Embedding Contains {np.round(100-Test_Embedding,2)}% of the words in the Test set\")\nprint(\"This are some of the missing word in the embedding:\/n\")\nprint(EM_TR_NO[1:5],EM_TS_NO[1:5])","b9b5c8c1":"optimizer= Adam(1e-4)\nmodel = Sequential()\nmodel.add(Embedding(input_dim = Voc_size,output_dim =100,input_length = Max_length, weights= [EMB_WEIGHTS], trainable=False))\nmodel.add(Dropout(0.3))\nmodel.add(GRU(15,dropout=0.3,return_sequences=True))\nmodel.add(Bidirectional(GRU(15)))\nmodel.add(Dense(5,kernel_initializer='glorot_uniform',activation ='relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer = optimizer, loss=loss,metrics = ['accuracy'])\nmodel.fit(X_train , y_train , batch_size = 64, epochs = 200, validation_data = (X_test,y_test))","ae521174":"pd.DataFrame(model.history.history).plot(figsize=(15,7))\nplt.show()","b980eb32":"train_spelling = {'demostration':'demonstration','youare':'you are','mh370':'mention','theyare':'they are','subreddits':'sub reddit', 'disea':'disease','idk':'i dont know','mediterran':'mediterranean','icemoon':'ice min','lgl':'Lee Green Lives',\n                  'org':'organization','selfies':'selfie','lmao':'laughing my ass off','abbswinston':'abbs winston','saddlebrooke':'saddle brook','hwo':'how would one','reaad':'read','theyll':'they will','prepper':'Survivalists','hitchbot':'mention',\n                  'uare':'you are','unkn':'unknown','cpanel':'mention','hahaha':'laughing','retweet':'re tweet','godslove':'god love','weallheartonedirection':'we all heart one direction','selfie':'selfi','specif':'specific','omfg':'mention','dannic':'mention',\n                  'idgaf':'song','boonew':'mention','theyd':'they would','cuase':'because','firebeatz':'mention','hihow':'hi how','stationright':'station right', 'subsecretariat':'sub secretariat','sincerety':'sincerity','prkng':'parking','kaiserjaegers':'army',\n                  'satoshis':'mention','atgames':'at games','bestie':'best friend','timedoes':'time does', 'triplemania':'triple mania','liquidslap':'liquid splap','anq':'and','likkly':'likely','standwout':'stand out','invzices':'invoices','invoicesx':'invoices',\n                  'smth':'something', 'invoicew':'invoices', 'colouj':'colour','ikely':'likely','ivoices':'invoices','smtx':'sometimes','aboortioonnns':'abortions','consis':'consist', 'currong':'currying','mothernaturenetwork':'mother nature network','sideness':'sidedness',\n                  'interestud':'interested','leftside':'left side','demonstratio':'demostration','wroug':'wrong','epilept':'epileptic','fcking':'fucking','befoooore':'before','photoset':'photo set','hookier':'hooker','viayoutube':'via youtube','epicen':'epicenter',\n                  'itll':'it will','anythin':'anything','bedhair':'bed hair','cornwalls':'corn walls','leaveevacuateexitbe':'leave evacuate exit be','evaucation':'evacuation','newsbrokenemergency':'news broken emergency','servicesgold':'service gold',\n                  'coastpowerlinetramtr':'coast power line','ktchn':'kitchen','swayback':'sway back','whattt':'what','fangirl':'fan girl','newsthousands':'news thousand','recks':'reck','kostumes':'costumes','plez':'please','silentmind':'silent mind','liveleakfun':'live leak fun',\n                  'zimpapersviews':'zim paperss views','submitt':'submit', 'contd':'continued','hwrf':'hurricane weather research and forecasting model','turdnado':'tornado','forgeting':'forgetting','alsowhat':'also what','speculatio':'speculation', 'sequalae':'sequelae',\n                  'leonardville':'leonard ville','urg':'urgent','recordand':'record land','barkevious':'mention','follownflnews':'mention', 'ddnt': 'did not','commentes':'comments','pornhub':'mention','megalpolis':'mention','nagaski':'nagasaki','owenrbroadhurst':'owner broad hurst',\n                  'juanmthompson':'mention','blowjobs':'blow jobs', 'hillarymass':'mention', 'bestie':'best friend', 'followback':'follow back', 'dubloadz':'mention', 'droppd':'dropped', 'watchout':'watch out', 'assertative':'assortative', 'worseits':'worst it is','dorett':'dorette',\n                  'dorette':'dorette','hatman':'hat man','unsensibly':'insensible','defensenews':'defense news','subsd':'scalar double precision floating point subtract','sandunes':'mention','hamptonroadsfor':'place','damnn':'damn','renewsit':'re new sit','whensoever':'when soever', \n                  'kontrolled':'controlled','disclos':'disclose', 'notwill':'not will', 'arequiem':'requiem', 'deadgirltalking':'dead girl talking', 'unfortunemelody':'ufortune melody', 'goodlook':'good look', 'headdesk':'head desk', 'savs':'says','laughtraders':'laugh traders', \n                  'astroturfers':'astroturfing', 'tindering':'use tinder', 'fundwhen':'fund when', 'pipeliners':'pipe lines', 'ireporter':' i reporter', 'thnk':'think', 'victimiser':'victimize','reaad':'read', 'marrie':'mention', 'mazing':'amazing', 'transwomen':'trans womn', \n                  'championsblackfoot':'champions black foot', 'notpanic':'not panic', 'firefightr':'fire figther', 'workd':'worked', 'lookg':'looked','postapocalypticflimflam':'post apocalyptic flimflam','lookss':'looks','wordk':'worked','socialwots':'social bots','airhorns':'air horns','cryibg':'crying',\n                  'mommys':'mommy','femnism':'feminism','laighign':'laughing','fangirling':'fan girling', 'swiming':'swimming','whitewalkers':'white walkers','greatt':'great','youare':'you are','mh370':'mention', 'theyare':'they are', 'subreddits':'sub reddit', 'disea':'disease',\n                  'idk':'i dont know','mediterran':'mediterranean','icemoon':'ice min','lgl':'Lee Green Lives','org':'organization', 'selfies':'selfie', 'lmao':'laughing my ass off', 'abbswinston':'abbs winston', 'saddlebrooke':'saddle brook','hwo':'how would one','reaad':'read','theyll':'they will',\n                  'prepper':'Survivalists','hitchbot':'mention','uare':'you are','unkn':'unknown','cpanel':'mention','hahaha':'laughing','retweet':'re tweet','godslove':'god love','weallheartonedirection':'we all heart one direction','selfie':'selfi','specif':'specific','omfg':'mention','dannic':'mention',\n                  'idgaf':'song','boonew':'mention','theyd':'they would','cuase':'because','firebeatz':'mention','hihow':'hi how','stationright':'station right','subsecretariat':'sub secretariat','sincerety':'sincerity', 'prkng':'parking','kaiserjaegers':'army', 'satoshis':'mention','atgames':'at games',\n                  'bestie':'best friend','timedoes':'time does','triplemania':'triple mania','skarlet':'video game character','ny2':'new york two','dilapitated':'dilapidated','darkspawn':'game','monumentalized':'monumentalize','optiks':'opticks','probaly':'probably','xoxoxxxoo':'Hugs and kisses','rutinaofficial':'mention',\n                  'nbchannibal':'mention','nor0603':'mention','criess':'cries','issit':'is sit','adultress':'adulteress','coldwood':'cold wood','loccitane':'company','idiota':'idiot','hanbags':'hand bang','bestfriends':'best friend','somaliaeritrea':'mention','urgentthere':'urgent there','oworoshoki':'mention','grea':'great',\n                  'forestservice':'mention','vandalzing':'vandalizing','wgton':'mention','aldwark':'city','nyorks':'new york','brandace':'mention','hermancranston':'mention','enemity':'enemy','privlege':'privilege','maccocktail':'mention','retweeting':'re tweet','apocaly':'apocalip','nyclass':'new york class','expecte':'expected','unfortun':'unfortunate','smantibatam':'mention','bbecause':'because','niley':'mention','innit':'is it not','hairity':'hairy','spitty':'mention','whoevers':'who ever','damnn':'damn', 'jojowizphilipp':'mention','lmk':'song','appadvice':'app advice','freakingg':'freaking','sportwatch':'sport watch','gigatech':'giga tech','waige':'waige','niggah':'nigga','tradery':'traderie','gunsmissilesbombs':'mention','lotm':'lord of the mics','fleshgod':'mention','emmeryn':'game','memez':'funny picture','politifiact':'politifact','xshanemichaelsx':'mention','instaxbooty':'mention','tumbleon':'mention', 'knw':'know','waianapanapa':'place','news5':'news 5 ','kindlng':'kindling','glowng':'glowing','frnch':'french','tbh':'to be honest','hvy':'company','jplocalfirst':'mention','eliotschool':'mention','ofclans':'of clans','multidimensi':'multidimensions','multidimen':'multidimensions','multidimensio':'multidimensions','structur':'structure','multidimensiona':'multidimensional','multidime':'multidimensions','klinenberg':'mention','investigatin':'investigation','malyasiaairlines':'mention','liveleakfun':'mention','momentsathill':'mention','nonononono':'no','crunchysensible':'mention','snctions':'sanctions','hostagesthrosw':'mention','hlps':'helps','angelriveralib':'mention','times4':'time for','hegot':'he got','cmon':'come on','uiseful':'useful','alredy':'already','joketheyare':'mention','twitterpated':'mention','tommygshow':'mention','lides':'lid','icaseit':'mention','ramify':'ramify','multitudinal':'multitudinal','colorlines':'color lines','yougslavia':'yugoslavia','timesnewsdesk':'time news desk','terraria':'mention','pakistanex':'pakistan','bdays':'brithdays','featcha':'mention','madsummer':'mad summer','yknow':'you know','truthfrequencyradio':'truth frequency radio','fishtails':'fish tails',\n                  'schematization':'schematize','mindkiller':'mind killer','gallups':'company','hangarback':'mention','disclos':'disclose','coinflip':'coin flip','shtfplan':'shift plan','changelessly':'changeless','dirtylying':'dirty lying','ixed':'fixed','firstaid':'first aid','revel8ion':'revelation','skynews':'skyn ews',\n                  'nigeriantribune':'nigerian tribune','hangi':'hanging','carryi':'carrying','theonion':'the onion','allahuakbar':'allahu akbar','elsewere':'else where','cindynoonan':'mention','eurpoe':'europe','lyricaleyes':'mention','forreal':'for real','sayingdarude':'saying darude','yess':'yes','omlette':'omelette','beachboyirh':'company','oorr':'or','worldoil':'world oil','detectado':'detected','escuchando':'heard','collab':'collaboratory','fckng':'fucking','misocapnist':'misocapnist','zouis':'mention','rightways':'rigth ways','thing2':'thing two','godjesus':'god jesus','killedsomebody':'killed somebody','couldve':'could have','idek':'i dont even know','idkidk':'i do not know','fotoset':'fotp set','progessive':'progressive','timesofindia':'time of india','bstrd':'bastard','takig':'taking','anoder':'another','wrld':'world','technews':'tech news','thediyhacks':'mention','illusionimagess':'illusion images','walktrough':'walkthrough','recentlu':'recently','sumthng':'something','approachng':'approaching','suddendly':'suddenly','epicente':'epicenter','seasonfrom':'season from','onshit':'oh shit','terrellt':'mention','splattling':'video game weapon','fucboi':'fuck boy','nowlike':'now like','netnewsledger':'mention','timelapse':'time lapse','competetion':'competition','reddits':'redit','beforeitsnews':'before its news','selfi':'self picture photograph','selfie':'self picture photograph','demostration':'demostration','connecto':'connector','nowplaying':'now playing','worldnews':'world news',\n                  'throwingknifes':'throwing knifes','hahahah':'haha','foxnew' :'fox news','retweeted':'re tweeted','iclown':'i clown','kowing':'knowing','hahah':'haha','abcnews':'abc news','incase':'in case','hahahaha':'haha',\n                  'linerless':'liner less','multidimensions':'multi dimensional','notexplained':'not explained','forsure':'for sure','dickheads':'dick head','douchebag':'douche bag','areddit':'a reddit','bbcnews':'bbc news',\n                  'marketforce':'market force','rigth':'right','coool':'cool','evildead':'evil dead','starmade':'star made','bookslast':'books last','latimes':'last time','hotboy':'hot boy','jokin':'joking','vigilent':'vigilant',\n                  'fantabulous':'awesome','handside':'hand side','benothing':'be nothing','morebut':'more but','infowars':'info wars','loosers':'losers','clergyforced':'clergy forced','bioterrorismi':'bioterrorism','kidnapg':'kidnap',\n                  'alrighty':'all right','skywars':'sky wars','sothwest':'south west','instagrams':'instagram','drumstep':'drum step','everyones':'every one','dialyses':'dialysis','everwhe':'every where','looool':'laughing out loud',\n                  'aggressif':'aggressive','cleav':'clean','bigstar':'big star','mycareer':'my career','bowknot':'bow knot','womengirls':'womengirls','shadowflame':'shadow flame','floatin':'floating','whatevs':'what ever',\n                  'awesomeeee':'awesome','runkeeper':'run keeper','tounge':'tongue','intead':'instead','awesomesauce':'awesome','fatburning':'fat burning','techniqu':'technique','socialtimes':'social times','recip':'recipes','produc':'product','misfortunebut':'misfortune but',\n                  'tweetstorm':'tweet storm','reportly':'report','lindenow':'linde now','saudies':'people from south arabia','eurocrisis':'euro crisis','timestack':'time stack','hadnt':'had not','carhot':'car hot','crashsterling':'crash sterling',\n                  'joeysterling':'joey sterling','havnt':'haven not','womem':'womrn','oclock':\"o'clock\",'fuckboy':'fuck boy','fucktard':'fuck hard','snotgreen':'snot gree','individl':'individual','hhahaha':'haha','lightrail':'ligh trail',\n                  'nbcnightlynews':'nbc night news','cerography':'xerography','susinesses':'businesses','stlnd':'stand','togthe':'together','invokces':'invoices','fuckface':'fuck face','wineisdumb':'wine is dumb','isllikely':'is likely',\n                  'clothesless':'clothes less','precisionistic':'precisionist','todayhave':'today have','urself':'yourself','abbruchsimulator':'abbruch simulator','foodstand':'food stand','worldpay':'word play','selfavowed':' self avowed',\n                  'everythign':'everything','fcked':'fucked','newidea':'new idea','top25':'top 25','psalm34':'psalm 34','copperfields':'copper fields','littlebitofbass':'little bit of bass','fastings':'fasting','dilutional':'dilution',\n                  'foxsportscom':'fox sports com','boylesports':'boy less sports','daughtery':'daugherty','destroyd':'destroyed','specialguest':'special guest','crapgamer':'crap gamer','headlinelike':'head line like','undergroundbestsellers':'under ground best sellers',\n                  'googlemaps':'google map','obtaing':'obtain','blackburns':'black burns','indianperpetrated':'indian perpetrated','massgrave':'mass grave','briliantly':'brilliantly','conditionsprivation':'conditions derivation',\n                  'mydrought':'my drought','reprocussions':'repercussions','protoshoggoth':'proto shoggoth','howare':'how are','riveer':'river','sometimesi':'some times','dravet':'illness','marvins':\"marvin's\",'wednes':'wednesday',\n                  'erally':'literally','myelf':'myself','electrocutedboiling':'electrocuted boiling','speedtech':'speed tech','childfund':'company','treatmen':'treatment','painthey':'pain they','lifeits':'life it is ','yeaahh':'yeah',\n                  'firefigthers':'firefighters','peice':'piece','bfore':'before','chaning':'changing','siteinvestigating':'site investigating','aredistribute':'redistribute','recal':'recall','windwaker':'video game character',\n                  'policylab':'policy lab','gangstermail':'gangster mail','musicvideo':'music video','referencereference':'references','wrked':'worked','transporta':'transportation','sigalert':'sign alert','ohgod':'oh god',\n                  'antiochhickoryhollow':'antioch hickory hollow','firefighte':'firefighter','intertissue':'inter issue','ndetention':'detention','thankkk':'thank','notificationsu':'notification','awesomelove':'awedome love',\n                  'landsli':'land slide','heared':'heard','floorburnt':'floor burnt','alska':'alaska','seeweed':'seaweed','caribean':'caribbean','knowlddge':'knowledge','stormcoming':'storm coming','postering':'postponing',\n                  'cityporn':'city porn','dumbass':'dumb ass','sterotypical':'stereotypical','eversafe':'ever safe','apocalpytic':'apocalyptic','paperss':'papers','nycfc':'new york city footballcClub','greyjoys':'grey joys',\n                  'freebesieged':'free besieged','mwednesday':'wednesday','soever':'howsoever','ufortune':'u fortune','aannd':'and','antipozi':'antiposi','rezaphotography':'mention','romanatwoodvlogs':'mention','arefuse':'a refuse',\n                  'figther':'fighter','ahahahga':'haha','sidjsjdjekdjskdjd':'haha','oamsgajagahahah':'haha','ohshit':'oh shit','win10':'windows 10','roomr':'room','10news':'10 news','intersectio':'intersection','abbandoned':'abandoned',\n                  'horrormovies':'horror movies','queenmy':'queen my','songfor':'song for','heavydirtysoul':'heavy dirty soul','goooo':'go','radneck':'red neck','crqck':'crack','becyme':'young','youngins':'young','matchwood':'match wood',\n                  'thundersnow':'thunder snow','comeee':'come','hawaiianpaddlesports':'hawaiian paddle sports','mypillowstudio':'my pillow studio','favorited':'favorite','niqqa':'nigga','rembr':'remember','alwsl':'windows subsystem for linux',\n                  'hashd':'has hd','canadasuicide':'canada suicide','thursd':'thursday','backty':'mention','lonelyness':'loneliness','ayhhhdjjfjrjjrdjjeks':'nonsense','hotbox':'mention','daubt':'doubt','tsutomi':'tsunami','humanityi':'humanity',\n                  'news786':'news 786','convivted':'convicted','shedid':'she did','islamaphobe':'islamophobia','heheh':'haha','fforecast':'forecast','ofcourse':'of course','prettyboyshyflizzy':'pretty boys shy flizzy','fricken':'stupid peice of crap',\n                  'intragenerational':'intergenerational','learni':'learning','afrikaan':'afrikaans','happing':'happy','crptotech':'crypto tech','dvbbs':'mention','borgeous':'mention','arceen':'arce','todayi':'today','homefolks':'home folks',\n                  'hardside':'hard side','tittie':'boobs','noemotion':'no emotion','liguistic':'linguistic','nbanews':'nba news','postexistence':'post existence','areporting':'reporting','nside':'inside','jasmines':'jazmines',\n                  'blackcats':'black cats','ktfounder':'kt founder','amazingness':'amazing','finall':'final','articals':'articals','farmr':'farm','doessnt':'does not','listenlive':'listen live','lastingness':'durability',\n                  'claimin':'claiming','morethan':'more than','celebrety':'celebrity','sorrower':'sorrow','kindof':'kind of','restrospect':'retrospect','newave':'new wave','amazondeals':'amazon deals','skylanders':'sky landers',\n                  'ombudsmanship':'goverment investigator','gameofkitten':'game of kitten'}\n\ntest_spelling = {'youare':'you are','mh370':'mention', 'theyare':'they are', 'subreddits':'sub reddit', 'disea':'disease', 'idk':'i dont know', 'mediterran':'mediterran', 'icemoon':'ice min', 'lgl':'Lee Green Lives',\n                 'org':'organization', 'selfies':'selfie', 'lmao':'laughing my ass off', 'abbswinston':'abbs winston','saddlebrooke':'saddle brook', 'hwo':'how would one', 'reaad':'read', 'theyll':'they will',\n                 'prepper':'Survivalists', 'hitchbot':'mention', 'uare':'you are', 'unkn':'unknown', 'cpanel':'mention', 'hahaha':'laughing','retweet':'re tweet', 'godslove':'god love','weallheartonedirection':'we all heart one direction',\n                 'selfie':'selfi','specif':'specific', 'omfg':'mention', 'dannic':'mention', 'idgaf':'song', 'boonew':'mention', 'theyd': 'they would',\n                 'cuase':'because','firebeatz':'mention', 'hihow':'hi how', 'stationright':'station right','subsecretariat':'sub secretariat', 'sincerety':'sincerity', 'prkng':'parking', 'kaiserjaegers':'army', \n                 'satoshis':'mention', 'atgames':'at games', 'bestie':'best friend', 'timedoes':'time does', 'triplemania':'triple mania','skarlet':'video game character','ny2':'new york two',\n                 'dilapitated':'dilapidated','darkspawn':'game','monumentalized':'monumentalize','optiks':'opticks','probaly':'probably','xoxoxxxoo':'Hugs and kisses ','rutinaofficial':'mention',\n                 'nbchannibal':'mention','nor0603':'mention','criess':'cries','issit':'is sit','adultress':'adulteress','coldwood':'cold wood','loccitane':'company','idiota':'idiot','hanbags':'hand bang',\n                 'bestfriends':'best friend','somaliaeritrea':'mention','urgentthere':'urgent there','oworoshoki':'mention','grea':'great','forestservice':'mention','vandalzing':'vandalizing','wgton':'mention',\n                 'aldwark':'city', 'nyorks':'new york','brandace':'mention','hermancranston':'mention','enemity':'enemy','privlege':'privilege','maccocktail':'mention','retweeting':'re tweet','apocaly':'apocalip',\n                 'nyclass':'new york class','expecte':'expected','unfortun':'unfortunate','smantibatam':'mention','bbecause':'because','niley':'mention','innit':'is it not','hairity':'hairy','spitty':'mention',\n                 'whoevers':'who ever','damnn':'damn', 'jojowizphilipp':'mention','lmk':'song','appadvice':'app advice','freakingg':'freaking','sportwatch':'sport watch','gigatech':'giga tech','waige':'waige',\n                 'niggah':'african american','tradery':'traderie','gunsmissilesbombs':'mention','lotm':'lord of the mics','fleshgod':'mention','emmeryn':'game','memez':'funny picture','politifiact':'politifact',\n                 'xshanemichaelsx':'mention','instaxbooty':'mention','tumbleon':'mention','knw':'know','waianapanapa':'place','news5':'news 5 ','kindlng':'kindling','glowng':'glowing','frnch':'french','tbh':'to be honest',\n                 'hvy':'company','jplocalfirst':'mention','eliotschool':'mention','ofclans':'of clans','multidimensi':'multidimensions','multidimen':'multidimensions','multidimensio':'multidimensions','structur':'structure',\n                 'multidimensiona':'multidimensions','multidime':'multidimensions','klinenberg':'mention','investigatin':'investigation','malyasiaairlines':'mention','liveleakfun':'mention','momentsathill':'mention','nonononono':'no',\n                 'crunchysensible':'mention','snctions':'sanctions','hostagesthrosw':'mention','hlps':'helps','angelriveralib':'mention','times4':'time for','hegot':'he got','cmon':'come on','uiseful':'useful','alredy':'already',\n                 'joketheyare':'mention','twitterpated':'mention','tommygshow':'mention','lides':'lid','icaseit':'mention','ramify':'ramify','multitudinal':'multitudinal','colorlines':'color lines','yougslavia':'yugoslavia',\n                 'timesnewsdesk':'time news desk','terraria':'mention','pakistanex':'pakistan','bdays':'brithdays','featcha':'mention','madsummer':'mad summer','yknow':'you know','truthfrequencyradio':'truth frequency radio',\n                 'fishtails':'fish tails','schematization':'schematize','mindkiller':'mind killer','gallups':'company','hangarback':'mention','disclos':'disclose','coinflip':'coin flip','shtfplan':'shift plan','changelessly':'changeless',\n                 'dirtylying':'dirty lying','ixed':'fixed','firstaid':'first aid','revel8ion':'revelation','skynews':'skyn ews','nigeriantribune':'nigerian tribune','hangi':'hanging','carryi':'carrying','theonion':'the onion',\n                 'allahuakbar':'allahu akbar','elsewere':'else where','cindynoonan':'mention','eurpoe':'europe','lyricaleyes':'mention','forreal':'for real','sayingdarude':'saying darude','yess':'yes','omlette':'omelette','beachboyirh':'company',\n                 'oorr':'or','worldoil':'world oil','detectado':'detected','escuchando':'heard','collab':'collaboratory','fckng':'fucking','misocapnist':'misocapnist','zouis':'mention','rightways':'rigth ways','thing2':'thing two','godjesus':'god jesus',\n                 'killedsomebody':'killed somebody','couldve':'could have','idek':'i dont even know','idkidk':'i do not know','fotoset':'fotp set','progessive':'progressive','timesofindia':'time of india','bstrd':'bastard','takig':'taking',\n                 'anoder':'another','wrld':'world','technews':'tech news','thediyhacks':'mention','illusionimagess':'illusion images','walktrough':'walkthrough','recentlu':'recently','sumthng':'something','approachng':'approaching','suddendly':'suddenly',\n                 'epicente':'epicenter','seasonfrom':'season from','onshit':'oh shit','terrellt':'mention','splattling':'video game weapong','fucboi':'fuck boy','nowlike':'now like',\n                 'netnewsledger':'mention','timelapse':'time lapse','competetion':'competition'}\n\nprint(\"----------------------Check these dictionaries--------------------\")","7ea285c9":"#  -----> Replacing\ntrain_['text_rep'] = train_.text_.apply(lambda x: word_replace(x,train_spelling))\ntest_['text_rep'] = test_.text_.apply(lambda x: word_replace(x,test_spelling))\n\n# -----> sets\nntrain_corpus = train_.text_rep.values\nntest_corpus = test_.text_rep.values\n\n# Preprocessing \nNToken = Tokenizer()\nNToken.fit_on_texts(ntrain_corpus)\n\nNToken_t = Tokenizer()\nNToken_t.fit_on_texts(ntest_corpus)\n\n# set up \nVoc_size = len(NToken.word_index)+1 #13059\nMax_length = 30\noutput_dim = 7\n\n# Enconding twitters\nNEnc_Tw = [one_hot(twit,Voc_size) for twit in ntrain_corpus]\nNEnc_Tw_test = [one_hot(twit,Voc_size) for twit in ntest_corpus]\n\nNPad_Tw = pad_sequences(NEnc_Tw, maxlen = Max_length , padding= 'post')\nNPad_Tw_test = pad_sequences(NEnc_Tw_test, maxlen = Max_length , padding= 'post')","55a947fc":"NEMB_DIM = 100\nNEM_TR_NO,NEM_TS_NO = [],[]  # EM = Embedding, TR\/TS = Train \/ Test , NO = No in \nNEMB_WEIGHTS = np.zeros((Voc_size,NEMB_DIM)) # Matrix of numebrofwordsX100\n\n# -----> Using word2vector with new weights \n\nfor word, postition in NToken.word_index.items():\n    NEMB_VECTOR = Embeddings.get(word) #take the dict key and find the word in the dict\n    if NEMB_VECTOR is not None:\n        NEMB_WEIGHTS[postition] = NEMB_VECTOR # this because the dict position start in 1 not 0 \n    else: # it is because the word from the traning set in not in the embedding\n        NEM_TR_NO.append(word)\n\nfor word, i in NToken_t.word_index.items():\n    NEMB_VECTOR = Embeddings.get(word)\n    if NEMB_VECTOR is None: # ithe word from the test set it is not in the embedding\n        NEM_TS_NO.append(word)\n        \nNTrain_Embedding = len(NEM_TR_NO)*100\/len(NToken.word_index)\nNTest_Embedding = len(NEM_TS_NO)*100\/len(NToken_t.word_index)\nprint(f\"The Embedding Contains {np.round(100-NTrain_Embedding,2)}% of the words in the Training set\")\nprint(f\"The Embedding Contains {np.round(100-NTest_Embedding,2)}% of the words in the Test set\")\nprint(\"This are some of the missing word in the embedding:\/n\")\nprint(NEM_TR_NO[1:5],NEM_TS_NO[1:5])","6169621b":"# ---------------- >  Machine Learning\nX_train, X_test, y_train, y_test = train_test_split(ntrain_corpus, y, test_size=0.3, random_state=104, stratify=y)\n\nresults_ = []\nscoring = ['accuracy','recall','precision','f1']\nModel = [MultinomialNB(),svm.SVC(kernel ='rbf'),PassiveAggressiveClassifier(), RandomForestClassifier(max_depth= 30)]\n\nfor algo in Model:\n    Pipe_ = Pipeline([('CounterVF', CountVectorizer(analyzer = Pre_toke)),\n                      ('TfidF', TfidfTransformer()),('Model', algo)])\n    \n    scores = cross_validate(Pipe_,X_train,y_train,cv=6,scoring = scoring)\n    results_.append(scores)\n    \ndf_result = pd.DataFrame()\nfor position in range(0,len(results_)):\n    df = pd.DataFrame(data = results_[position])\n    df['model'] = str(Model[position].__class__.__name__)\n    df_result = df_result.append(df, ignore_index=True)\n    \ndf_result.sort_values('test_precision',ascending = False).head(4)","ea3fc08d":"# ---------------- > Deep Learning\n# Settting \noptimizer= Adam(0.002)\nloss = tf.losses.BinaryCrossentropy(from_logits=True,)\n\n# Splitting Data \nDX_train, DX_test, Dy_train, Dy_test = train_test_split(NPad_Tw, y, test_size=0.3, random_state=104, stratify=y)\n\noptimizer= Adam(1e-4)\nmodel = Sequential()\nmodel.add(Embedding(input_dim = Voc_size,output_dim =100,input_length = Max_length, weights= [NEMB_WEIGHTS], trainable=False))\nmodel.add(Dropout(0.3))\nmodel.add(GRU(15,dropout=0.3,return_sequences=True))\nmodel.add(Bidirectional(GRU(15)))\nmodel.add(Dense(5,kernel_initializer='glorot_uniform',activation ='relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer = optimizer, loss=loss,metrics = ['accuracy'])\nmodel.fit(DX_train , Dy_train , batch_size = 64, epochs = 500, validation_data = (DX_test,Dy_test))","05be679e":"# sns.set_style('whitegrid')\npd.DataFrame(model.history.history).plot(figsize=(15,7))\nplt.show()","b0a4d7bb":"#submit\nprediction = model.predict(NPad_Tw_test) \nresults = [1 if pred_ >= 0.5 else 0 for pred_ in prediction]\ndf = pd.DataFrame()\ndf['id'] = test_['id']\ndf['target'] = results\ndf.to_csv('FS_deep.csv')","b0bfe816":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"3\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Data | Data Pulling , Cleaning,Pre-Processing and visualization\n    <\/p>    \n<\/div>\n\n<p><b>Showing both data set -> <\/b> notice that these rows are selected according to Len = 90, for visual representation. In here I am just trying to show you the real data set and all the noise <b>Links Mentions Comments etc<\/b><\/p>\n<p>From this point and foward I will be showing <b>the Before and After<\/b> Processing at each stage. <b>Note<\/b> This is just a small sample out of the totals row chosen on their len, in the next stage you will find details about how I removed all those comments by using <b>Regex<\/b> a very useful library. So let's get started<\/p>\n<p> The challenge in NLP <b>in my humble opinion<\/b> it is the cleaning part, look <b> a model will be always as good as the inputs are <\/b>, So what are we gonna do with so many mentions, links, etc, other users will remove them and ignore them but I will keep them and give them a special word so if I use <b>Naive Bayes<\/b> to count the words it will have more chances to classify a spam because the numbers of <b>LINK<\/b> in a row.<\/p>","5beaecd7":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"6.2\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Word2Vector\n    <\/p>    \n<\/div>\n<p> The hight Accuracy is 79.9 but now using <b> Word2vectore<\/b> I am aming to have a better approach,, because each word will be a vector, it is pretty much the same like embedding layer however this one has been pre-trained with only god knows how many words<\/p>\n<li> Load the corpus from here <b>kaggle<\/b><\/li>\n<li> Trasnform the corpus into arrays<\/li>\n<li> Then compare those word that are in my data and in the Word embedding <\/li>\n<li> Find the <b> percentage of words <\/b> presents in my the embedding from my corpus<\/li>\n<li> Copy the weights and put them into a Embedding layers using keras <b> notice <\/b>that this must not be <b>re-trained <\/b> or we gonna have a mess <\/li>","adcb3c04":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"4.1\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        World Map\n    <\/p>    \n<\/div>","56724068":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"5\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Machine Learning\n    <\/p>    \n<\/div>\n\n<p><h3> One of the most excited part <b> Machine Learning <\/b> Algorithms and more.<\/h3> <\/p>\n<li> In this section I will use <b>Multinominal Naive Bayes<\/b> which is basically coun the number of words and calculate the prior probabilities <\/li>\n<li><b>Support Vector Machine<\/b> another powerfull algorithm which has been used a lot in text classification <\/li>\n<li><b> RandomForest <\/b><\/li>\n<p> <b>Note:<\/b> I am splitting first into train and test and then I will use <b>Pipeline<\/b> for CounterVectorizering and TfidTransforming, next I compare Countervectorize vs Tfidtransform <\/p>\n<li> <b>Pipelines help avoid leaking statistics<\/b> from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.<\/li>","cea5358d":"<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n<p>Second Graph <b>The Tree map<\/b> is just showing the word distribution normalized:<\/p>\n<li>It means the greater the number is or closer to 1, its importance is greater the dataset (for training set onlu) <\/li>","96bc36c7":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"3.2\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Cleaning | Expanding Words  \n    <\/p>    \n<\/div>\n\n#### ***The second step*** is to find all those words that ends in ***'t*** or ***'ve*** such as **can't**, **shouldn't** , ***should've***\n<p> After removing spaces,tab and tab-spaces , I had to expand all contracted words -> <b>I haven't done it - I have not done it<\/b> and many mores. The reason why it is because if I used (Naive Bayes as the title show) this algorithm will count the numbers of words in the corpus to calculate the probabilities Spam\/No spam , another reason is the Word embedding, I can reduce the vector space because I will have only 2 world -> <b>1)have  2)not<\/b> instead of three vectors <b>1)have  2)not  3)haven't.<\/b> <\/p>\n<p>The code below shows how we can do it, if you are familiar with Regex you will undersstand that I might have some error in the searching word. <b>Note:<\/b> The dictionary showing all those words are in the section <b>Fuctions<\/b><\/p>\n\n- Use regex ***r\"t$|n't$|'re$|'m$|'s>'|'ve\"*** to find those words.\n- Keep them in a list and then check each of them as showed below\n- Uncomment the code above to see the results or just see the dictionary ***'contraction'***","b97a8edf":"<p><h3> Which model should I choose? <\/h3><\/p>\n<p>This is the most important part,<b> Before<\/b> I used to think that accuracy was the <b>best way<\/b> to measure a model, get 100% is the best I thought , however now I know that there are many thing to consider<\/p>\n<li>Sensitivity , Specificity , Threshold and many more, if you are not familiar with this think of it like , the rate or ability that my model have to be truthful because models are not 100 accurate and if it is \"probably is overfitting\" <\/li>\n<li> Imagine you have a ML that predict all samples as Disaster, including those which are not <b> think about all the money spent to move all equipment to that place and realize that was a mistake.  <\/b>  this is high Sensitivity<\/li>\n<li> Now think of a model that is only predict 'no disaster' , can you imagine yourself running late to the disaster palce because your system still saying \"there are not diaster\" that is high precision so here is when you have to play with this two factor and decide <b> Do I model that tell me everything is fine but it is not or one that tell me this is a disaster and maybe it is not ? <\/b> <\/li>\n    \n<p><h4>Answer : That depends on your project \/ customer needs. <\/h4><\/p>\n<p> In my case I rather to have a model that tell me \"hey this is a disaster when it is not , however i will train and re-train until this error is minimized:<\/p>\n<li> I will use <b>RandomForestClassifier ans SVC <\/b> because it offers me more hyperparameters, Multinomial is great but depends a lot on the input and PassiveAggressiveClassifier is effective for data processed in real time which is not the case here <\/li>\n<li> Later I will use Deep learning <\/li>","590b7d87":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n     <a id = \"3.1\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Cleaning | Spaces  \n    <\/p>    \n<\/div>\n\n\n#### ***As you can see (in Training Set, Testing Set)*** \n<p>we have so many #Metions and @Mentions, I dont show them in here but the data set have so many <b>***\\\\n***<\/b> or <b>***\\\\\\n***<\/b> spaces or tab space, you can check it by removing the '#' in corpus code, and you will see them.The Code below take all those spaces and replace them with \"SPACE\", I am using <b>Regex<\/b> to find those spaces, then we use lambda and pandas to repalce them<\/p> \n<p>The following are two samples extracted form the train set : \"check the spaces denoted with <b>\"\\n\"<\/b><\/p> \n\n- **SAMPLES 1** : \"on the outside you're ablaze and alive***\\n***but you're dead inside.\"\n- **SAMPLES 2** : \"Progressive greetings!***\\\\n\\n***\\In about a month students would have set their pens ablaze in The Torch Publications'..","58e7dfbd":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"2\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Fuctions | \n    <\/p>    \n<\/div>\n\n\n<p> As mentioned above in here you will find all <b>fuctions<\/b> that I used in this noteook, but before I would like to let you know what I was doing and how I tackled this dataset<\/p>\n<p> After searching for ideas in Kaggle and many other websites, I realized that most of those notebook were focused on <b>splitting the data, train , test and no more <\/b>then i dediced I was going to do it differently , I read thousand of those twitters and find so many <b>grammar mistakes, #mentions @people<\/b> and so many other things, then I decided to use regex to find those inconsistencies, and replace them , this took around 5 hours to make sure that most of them were fixed, however if you find <b>a better way<\/b> please let us know.<\/p>","034307f0":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:white ;\n       font-size:200%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"5\"><\/a>\n    <p style=\"padding: 15px;\n          color:red;\">\n        Warning\n    <\/p>    \n<\/div>\n\n<p style= \"font-size:150%;color:red;\" > ----> Some twitters have been misslabeled before going forward we must fix those <\/p>","9a8d9856":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"3.5\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Cleaning | Preffix and Suffix  \n    <\/p>    \n<\/div>\n\n#### ***Removing Word excessively Large, Repeated words , which length is greater that 10*** \n\n<p> At this point I am realy tired of repeating the process , however I am only one step away to finish the whole-preprocessing <\/p>\n<p> First I wil find word such as interniona<b>lly<\/b> , <b>aaaaaaallll<\/b> ,<b>lmfaaaaoooooooo<\/b> and transforme them, Secondly I will find all number and replace them by the words <b>Number<\/b> , then I will double check that i am not missing any words. <b> the word ain't <\/b> will be the same<\/b> I will note change it into am not, have not or any other. \n\n#### Things to do:\n\n- Replace those words that are not ending in lly, ly, ing, sion, tion, able\n- Names of people or mentions that were not tag with '#' or '@'\n- Extract Numbers and repalce them by Number","8504bba1":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"3.4\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Cleaning | Removing Alpha-Numerics words  \n    <\/p>    \n<\/div>\n\n### Removing Alpha-Numeric ###\n\n<p> After cleaning spaces, expanding words, remove links, mentions, symbols and only god know what else<\/p>\n<p>I need to remove words such as this is <b> 4you <\/b> -> to you ,<b> I am going 2you..<\/b>--> I am going to you, <b> I had a revel8ion<\/b> - > I had a revelation and many more<\/p>\n\n##### How Could I do that?\n\n- Find words which has a number at any given position by using ***Regex*** w{1}\n- Create dictionary and replace them \n- ***Note :*** go to ***Fuctions*** and find **ex_numer_words.**","64618fb6":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"5.1\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        HyperParameter\n    <\/p>    \n<\/div>\n\n<li> <b>SVM and RDF<\/b> Hyperparameters with <b>RandomizedSearchCV <\/b> <\/li>\n<li> <b> Uncomment <\/b> the following code to find the hyper parameters<\\li>","cd149c7b":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"4\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Visulization | - What is going on ?\n    <\/p>    \n<\/div>\n<img src=\"https:\/\/www.clasesdeperiodismo.com\/wp-content\/uploads\/2016\/10\/tuits.gif\" \n     style=\"float:right;\" \n     alt=\"Customer Segmentation\" width=\"500\" height=\"100\"\/>\n<p> <h2 style=\"float:left;\"> What is people talking about ?<\/h2><\/p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n<p> In this part you will find two section<\/p>\n<li>In the frist one I am using Folium to load and pin the countries location in a World map showing <b> The top 5 Twitter<\/b> by city <b>Locations<\/b> where taken using Selenium (webscrapping)<\/li>\n<li>The second part is a Technical analysis about <b> Spam \/ No spam <\/b><\/li>","3821c857":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"6\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Deep Learning\n    <\/p>    \n<\/div>\n<p> And Now another excited part <b> Deep learning<\/b> Using Tensorflow to build Embedding Layers to create FNN LSTM and GRU<\/p>","cadb84f9":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"3.3\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Cleaning | Mentions - Links  \n    <\/p>    \n<\/div>\n\n#### ***Removing -> #Mentionds -> @Mentions -> https:\/\/.com -> Symbols and more.*** \n\n<p> <b>What is next ?<\/b>, The next step is to remove all those <b>'#Metions', '@Comments' , 'https:www.whatever.com', etc<\/b>, I did not drop\/eliminate those string instead I replaced them by \"MENTION\" and \"LINK\" because as you might guess spam message or news have lot of links and few mentions or just think of each possible cobination,<p\/>\n<p> One more time the code below show how I did it. <b> Note: <\/b> go to <b>Fuction<\/b> and find ***<b>Prepro_1<\/b>***. <\/p>\n\n- Replace those #mentions @mentions or www.whateverlink.com with the word MENTION or LINK \n- If we just strip the those #mentions we will lose lot of information and patterns for example the ML could learn that if the sentences have many #mentions it could be Fake\n- In here I used the Fuction PrePro_1 it can remove ***emtoics***, ***links***, ***mentions***","b743ca08":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"6.3\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Checking Spelling\n    <\/p>    \n<\/div>\n<p>After trying several architectures <b> LSTM GRU BiLSTM and many more<\/b> I decided to check those word that were note found in the embedding, once I found them, I correct the spelling of those which I consider could be in the embedding<\/p>\n<li> In the following <b>dicctionaries<\/b> you will find many words that has been corrected<\/li>\n<li> After that I create again another Embedding with new weight according to the corpus<\/li>\n<li> I run again the First machine learning models and deep learning <\/li>\n<li> at this point <b> there is nothing else i can do  <\/b> only submmit my results and wait for more feedbacks<\/li>\n<li> but the way I read that the anserw of this competition is available on the web , but i didnt want to cheat and submit those answer but you can always look for them <\/li>","884a86a2":"<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n<p>Third Graph <b>Histogram & Pie chart <\/b>:<\/p>\n<li>The first histogram is showing the length distribution: <b>Most of the Fake and Real Twitter are between125 - 140s letter by twitter<\/b> therefore try to classify those twitter based on their length it is not doable.<\/li>\n<li>The second histogram shows pretty much the same , however we can notice the peak at 1600 which mens most of the twitter in train have <b>between 130-140 letters<\/b><\/li>\n<li>The Third Graph is the <b>amount of<\/b> Fake twitter vs Real ones <\/li>","521bd390":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n       Twitter Analysis | - Introduction \n    <\/p>    \n<\/div>\n<img src=\"https:\/\/www.downloadsource.es\/uploaded\/News%20January%202015\/Twitter%20GIF\/Twitter%20GIF.gif\" \n     style=\"float:right;\" \n     alt=\"Customer Segmentation\" width=\"1200\" height=\"50\"\/>\n     <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <h3 style=\"float:left;\">Natural Language Process<\/h3>   \n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n   <p>Natural Language Process or  <a href = 'https:\/\/www.ibm.com\/cloud\/learn\/natural-language-processing'> NLP,<\/a> strives to build machines that understand and respond to text or voice data - and respond with text or speech of their own - in much the same way humans do, and \"yes\" it means computers are able to read and understand. NLP is a branch of computer science, more specifically from artificial Intelligence or AI, that combines computational linguistics - rule based on modeling human language - <b>\"in easy words\" language grammar<\/b>, statistical tecniques and Machine Learning Algorithms, all this techniques combined make posible task such as word translation, Sentimental Analysis, span detection, fake new detection, text summarization , and many more , but I know you all know that , therefore <b>let's get our hand dirty and get into the data<\/b>. &nbsp; <\/p>\n    <p> <b>Disclosure:<\/b> First , it is important to mention that the data showed and analyzed is public here in <b>Kaggle<\/b>, however you will find other files that are private and that info was pulled by me, using <b>Selenium<\/b> , all this information is for educational purpose and should not be used for any other purpose.\n    \n    \n    \n<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n       What are you going to find in this notebook? | - Index \n    <\/p>    \n<\/div>\n\n* [1. Importing Libraries and Loading data](#1) import all libraries needed it in this notebook <b> (Pandas ,numpy , Tensorflow, etc) <\/b>\n\n* [2. Cutomize Fuctions - Classes - Code](#2) All fuctions and sub fuctions used in this notebook Dictionaries, Scrappying and more <b> (Pre_1 , Pre_2) <\/b>\n   \n* [3. Data Pulling , Cleaning,Pre-Processing](#3) Get familiar with the Data, Find inconsistencies, Text Processin , Regex and More.\n    - [3.1 Cleaning | Spaces](#3.1) Removing Tag Spaces and Spaces\n    - [3.2 Cleaning | Expanding Words](#3.2) Expading words such as <b>can't<\/b> --> can not \n    - [3.3 Cleaning | Mentions - Links](#3.3) Removing <b>#MarvinGarcia #DataScientist<\/b>\n    - [3.4 Cleaning | Removing Alpha-Numerics words](#3.4) Replacing <b>#Waiting 4 a reval8on<\/b>\n    - [3.5 Cleaning | Preffix and Suffix](#3.5) Replacing <b>ly, able , lly , un<\/b>\n    \n\n* [4. Visualization](#4)\n\n    - [4.1 World Map](#4.1) Top 5 Twitters by City Arround the World\n    - [4.2 Technical Analysis](#4.2) Spam \/ No Spam  \n    \n    \n* [5. Machine Learning ](#5) Using <b>Naive Bayes , Support Vector Machine , RandomForest , Passive Agressive <\/b> for Classification\n\n    - [5.1 First Selection](#5.1) Define the best metrics\n    - [5.2 Hyperparameters and Stacking](#5.2) Votting Classifier\n    \n    \n* [6. Deep Learning ](#6) Using Deep Learning architicture such as Feedfoward NN, LSTM, GRU\n\n    - [6.1 Feed Forward + Embedding layer](#6.1) Define the best metrics\n    - [6.2 WordVecotr Embedding](#6.2) Votting Classifier\n    - [6.3 Checking Spelling](#6.3) Replacing more than 500 words\n    - [6.4 Last models and submission](#6.4) Last model and conclusion\n    \n<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"1\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Python | - Libraries\n    <\/p>    \n<\/div>\n","49b78c0f":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"5.1\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        First Selection\n    <\/p>    \n<\/div>\n<p> In here I will use <b>Pipeline and Cross Vaidation<\/b> to find my best option in term of specificity and precision<\/p>\n\n<li> <b>Pipeline<\/b> Pass all the algorithms and return the Score using <b> Cross validate <\/b> : <b>'accuracy','recall','precision','f1'<\/b> <\/li>","4da095d2":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"4.1\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Technical Analysis\n    <\/p>    \n<\/div>\n\n<p>The First Graph <b>Lolipop Graph<\/b> counts the number of mentions in each data set:<\/p>\n<li> From here we can know that the data sets are not balanced in general and unbalanced in regard the keywords <b>\"we cannot asummed that a keyword is enough to classify the sample as Ham or Spam\"<\/b><\/li>\n\n    ","cfcfa9c5":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"6.3\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Last model and submission\n    <\/p>    \n<\/div>\n<p>Train again the Classifier voter using the <b> new and clean data <\/b> I decided to do again because there are new words that might impact SVM and Multinomal Naive bayes<\/p>\n<li> thank you <b>for reading until this point<\/b> if you have any feedback in the Deep learning archictecture please let me know ... ohh <b>please upvote<\/b><\/li>\n","98e5a035":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#1DA1F2 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n      <a id = \"5.2\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Normal Parameters\n    <\/p>    \n<\/div>\n<li> <b>Algorithms:<\/b> using both algorithms to evaluate the first predictions<\/li> \n<h4>At this poin I got <b> 79.9 Accuracy<\/b> in this Kaggle Competition<\/h4>"}}