{"cell_type":{"6b73790c":"code","285dc2cd":"code","af42b1c9":"code","db143e59":"code","f8763a25":"code","9c061d16":"code","925b70bf":"code","1ce0cc15":"code","cf16deca":"code","c13b7da9":"code","ac80579f":"code","3cd5d7ad":"markdown","310bcfef":"markdown","ffdc9dd2":"markdown","36b239fa":"markdown","207c7248":"markdown"},"source":{"6b73790c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","285dc2cd":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder","af42b1c9":"#Dropping unnecessary features.\ndata_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(data_train)\n\ndata_val = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ndrop_col = ['PassengerId', 'Cabin', 'Ticket']\nval_id = data_val['PassengerId']\ndata_train.drop(drop_col, axis=1, inplace=True)\ndata_val.drop(drop_col, axis=1, inplace=True)\nprint(data_train)","db143e59":"#Identifying and Filling up missing values.\nprint(data_train.isnull().sum())\nprint(data_val.isnull().sum())\n\ndata_train['Age'].fillna(data_train['Age'].median(), inplace=True)\ndata_train['Embarked'].fillna(data_train['Embarked'].mode()[0], inplace=True)\ndata_val['Age'].fillna(data_train['Age'].median(), inplace=True)\ndata_val['Embarked'].fillna(data_train['Embarked'].mode()[0], inplace=True)\ndata_val['Fare'].fillna(data_val['Fare'].mean(), inplace=True)\n\nprint(data_train.isnull().sum())\ndata_val.isnull().sum()","f8763a25":"#Generating some features using existing ones.\ndata_train['family_size'] = data_train['SibSp'] + data_train['Parch'] + 1\ndata_train['is_alone'] = 1\ndata_train['is_alone'].loc[data_train['family_size'] > 1] = 0\n\ndata_val['family_size'] = data_val['SibSp'] + data_val['Parch'] + 1\ndata_val['is_alone'] = 1\ndata_val['is_alone'].loc[data_val['family_size'] > 1] = 0\n\n#Encoding non-numerical distict features as categorical variables.\nlabel_encoder = LabelEncoder()\ndata_train['Embarked'] = label_encoder.fit_transform(data_train['Embarked'])\ndata_train['Sex'] = label_encoder.fit_transform(data_train['Sex'])\ndata_val['Embarked'] = label_encoder.fit_transform(data_val['Embarked'])\ndata_val['Sex'] = label_encoder.fit_transform(data_val['Sex'])\nprint(data_train)","9c061d16":"#Splitting given training dataset into train, test sets to evaluate various classifiers before using actual validation data.\nX_col = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'family_size', 'is_alone']\nX_train, X_test, y_train, y_test = train_test_split(data_train[X_col], data_train['Survived'], random_state=0)\n\n#1. K-Nearest neighbors Classifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nprint(knn.score(X_test, y_test))\n\ncv_scores = cross_validate(knn, data_train[X_col], data_train['Survived'], cv=50, return_train_score=True)\nprint('KNN training score:',cv_scores['train_score'].mean())\nprint('KNN testing score:',cv_scores['test_score'].mean())\nprint('KNN fit time:',cv_scores['fit_time'].mean())","925b70bf":"#2. Logistic Regression\nclf = LogisticRegression(penalty='l2', max_iter=500).fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\n\ncv_scores = cross_validate(clf, data_train[X_col], data_train['Survived'], cv=40, return_train_score=True)\nprint('Logistic Regression training score:',cv_scores['train_score'].mean())\nprint('Logistic Regression testing score:',cv_scores['test_score'].mean())\nprint('Logistic Regression fit time:',cv_scores['fit_time'].mean())","1ce0cc15":"#3. Linear SVM\nclf = SVC(kernel='linear').fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\n\ncv_scores = cross_validate(clf, data_train[X_col], data_train['Survived'], cv=50, return_train_score=True)\nprint('Linear SVM training score:',cv_scores['train_score'].mean())\nprint('Linear SVM testing score:',cv_scores['test_score'].mean())\nprint('Linear SVM fit time:',cv_scores['fit_time'].mean())","cf16deca":"#4. Decision Tree\nclf = DecisionTreeClassifier(max_depth=7).fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\n\ncv_scores = cross_validate(clf, data_train[X_col], data_train['Survived'], cv=40, return_train_score=True)\nprint('Decision Tree training score:',cv_scores['train_score'].mean())\nprint('Decision Tree testing score:',cv_scores['test_score'].mean())\nprint('Decision Tree fit time:',cv_scores['fit_time'].mean())","c13b7da9":"print(data_val[X_col])\nprint(data_val.isnull().sum())\n\n#Using Decision Tree Classifier based on comparison of training results.\ny_val = clf.predict(data_val[X_col])\n\nprint('-'*50)\nprint('Output labels:')\nprint(y_val)","ac80579f":"#Saving predictions for submission.\nlabels = pd.DataFrame()\nlabels['PassengerId'] = val_id\nlabels['Survived'] = y_val\nprint(labels)\n\nlabels.to_csv('Submission.csv', index = False)\nprint('Submission has been saved.')","3cd5d7ad":"# III. Feature Engineering","310bcfef":"# IV. Model Training","ffdc9dd2":"# II. Data Cleaning\n         \n         ","36b239fa":"# V. Generating Predictions","207c7248":"# I. Importing required libraries"}}