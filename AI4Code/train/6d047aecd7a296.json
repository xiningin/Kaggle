{"cell_type":{"e8500648":"code","e5a6e0da":"code","2db8ef5a":"code","8c3d7cfd":"code","b852f1a4":"code","9465465a":"code","349e1b2d":"code","ff4be138":"code","2bdd385a":"code","fadf7c32":"code","76c36f77":"code","82c2b43d":"code","4261c5a8":"code","ec074470":"code","23220fda":"code","977dbecd":"code","30245e11":"code","51d8d655":"code","5db9dc07":"code","1aeb5d23":"code","cdea18e9":"code","16966b64":"code","965bd2a5":"code","9c6e5129":"code","d8ed9e78":"code","d8c71f7e":"code","f4ce93b1":"code","85419c9f":"code","e54bf6e1":"code","f8c9e4d7":"code","a82b2df9":"code","18e24011":"code","82669d22":"code","71e170f0":"code","dcd0408c":"code","1637d944":"code","e9b9d349":"code","5b133983":"code","dbc0a094":"code","5c502874":"code","2a463aef":"code","157e12f5":"code","2a326bee":"code","c0025a9b":"code","d812fa1d":"code","9a168204":"code","0a5075d1":"code","3726b83e":"code","ea038efe":"code","b92e1c97":"code","c7a61b91":"code","b53be7e5":"code","2b61256d":"code","823c2fde":"code","c522e388":"code","a4c5f8b3":"code","e414a358":"code","6411b262":"code","813cf959":"code","ee725ef4":"code","01c8cd48":"code","95d9d379":"markdown","dbc1a581":"markdown","254fbb1b":"markdown","95fec1c1":"markdown","8a990b76":"markdown","91d817a8":"markdown","c644dcea":"markdown","3af19287":"markdown","4e8b369b":"markdown","e91ac03e":"markdown","ffa7ec6a":"markdown","78160bb6":"markdown","8f56a7b7":"markdown","7931ac73":"markdown","9665175a":"markdown","03d59910":"markdown","c093e0e8":"markdown","8e281f95":"markdown","3daf0c47":"markdown","dc7bc0cb":"markdown","0dbee13c":"markdown","32d69315":"markdown"},"source":{"e8500648":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nplt.style.use('seaborn')\n%matplotlib inline","e5a6e0da":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","2db8ef5a":"train.head()","8c3d7cfd":"test.head()","b852f1a4":"print('The train data size before dropping Id column: {}'.format(train.shape))\nprint('The test data size before dropping Id column: {}'.format(test.shape))\ntrain_id = train.Id\ntest_id = test.Id\ntrain.drop('Id',axis=1,inplace=True)\ntest.drop('Id',axis = 1,inplace=True)\nprint('The train data size after dropping Id column : {}'.format(train.shape))\nprint('The test data size after dropping Id column : {}'.format(test.shape))","9465465a":"train.get_dtype_counts().sort_values(ascending=False)","349e1b2d":"def missing_per(df):\n    ms=pd.DataFrame(columns=['col','missing'])\n    idx = 0\n    for i in range(df.shape[1]):\n        if df.isnull().sum()[i]>0:\n            ms.loc[idx,'col'] = df.isnull().sum().index[i]\n            ms.loc[idx,'missing'] = df.isnull().sum()[i]\/df.shape[0] * 100\n            idx+=1\n        else:\n            continue\n    ms=ms.sort_values(by='missing',ascending=False)\n    return ms","ff4be138":"ms_train = missing_per(train)\nms_train","2bdd385a":"plt.figure(figsize=(15,12))\nsns.barplot(data=ms_train,x='col',y='missing')\nplt.xlabel('feature')\nplt.xticks(rotation='60',ha='right')\nplt.title('train missing rate')","fadf7c32":"ms_test = missing_per(test)\nms_test","76c36f77":"plt.figure(figsize=(15,12))\nsns.barplot(data=ms_test,x='col',y='missing')\nplt.xlabel('feature')\nplt.xticks(rotation='60',ha='right')\nplt.title('train missing rate')","82c2b43d":"plt.style.use('fivethirtyeight')\nsns.scatterplot(data=train,x='GrLivArea',y='SalePrice')\nplt.title('SalePrice_GrLivArea correlation')","4261c5a8":"train= train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\nsns.scatterplot(data=train,x='GrLivArea',y='SalePrice')\nplt.title('SalePrice_GrLivArea correlation after removing outliers')","ec074470":"fig,axes = plt.subplots(2,2,figsize=(20,12)) \nsns.scatterplot(data=train,x='1stFlrSF',y='SalePrice',ax=axes[0,0])\naxes[0,0].set(title='SalePrice_1stFlrSF correlation')\nsns.scatterplot(data=train,x='GarageArea',y='SalePrice',ax=axes[0,1])\naxes[0,1].set(title='SalePrice_GarageArea correlation')\nsns.scatterplot(data=train,x='TotalBsmtSF',y='SalePrice',ax=axes[1,0])\naxes[1,0].set(title='SalePrice_ToatalBsmtSF correlation')\nsns.scatterplot(data=train,x='MasVnrArea',y='SalePrice',ax=axes[1,1])\naxes[1,1].set(title='SalePrice_MasVnrArea correlation')\nplt.tight_layout()","23220fda":"fig,ax1 = plt.subplots(figsize=(12,8))\nsns.scatterplot(data=train,x='GrLivArea',y='SalePrice',ax=ax1)\nsns.regplot(train['GrLivArea'],y=train['SalePrice'],line_kws={'color':'red'},ax=ax1)\nax1.set(title = 'SalePrice_GrLivArea regrssion plot')","977dbecd":"fig,axes = plt.subplots(2,2,figsize=(20,12)) \nsns.scatterplot(data=train,x='1stFlrSF',y='SalePrice',ax=axes[0,0])\nsns.regplot(x=train['1stFlrSF'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[0,0])\naxes[0,0].set(title='SalePrice_1stFlrSF regression plot')\nsns.scatterplot(data=train,x='GarageArea',y='SalePrice',ax=axes[0,1])\nsns.regplot(x=train['GarageArea'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[0,1])\naxes[0,1].set(title='SalePrice_GarageArea regression plot')\nsns.scatterplot(data=train,x='TotalBsmtSF',y='SalePrice',ax=axes[1,0])\nsns.regplot(x=train['TotalBsmtSF'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[1,0])\naxes[1,0].set(title='SalePrice_ToatalBsmtSF regression plot')\nsns.scatterplot(data=train,x='MasVnrArea',y='SalePrice',ax=axes[1,1])\nsns.regplot(x=train['MasVnrArea'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[1,1])\naxes[1,1].set(title='SalePrice_MasVnrArea regression plot')\nplt.tight_layout()","30245e11":"plt.figure(figsize=(30,20))\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, center = 0, mask=mask); \nplt.title(\"Heatmap of all the Features\", fontsize = 30)","51d8d655":"from scipy.stats import norm, skew\nfrom scipy import stats\nplt.style.use('seaborn')\nsns.distplot(train['SalePrice'] , fit=norm)\nmu, sigma = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nstats.probplot(train['SalePrice'],plot=plt)\nplt.show()","5db9dc07":"train['SalePrice'] = np.log1p(train['SalePrice'])\nsns.distplot(train['SalePrice'],fit=norm)\nmu,sigma = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nfig=plt.figure()\nstats.probplot(train['SalePrice'],plot=plt)\nplt.show()","1aeb5d23":"ntrain = train.shape[0]\nntest = test.shape[0]\ntrain_y= train['SalePrice'].values\ndata = pd.concat([train,test],join='inner')\nprint('Merging data size is {}'.format(data.shape))","cdea18e9":"ms_all = missing_per(data)\nms_all","16966b64":"plt.figure(figsize=(15,12))\nsns.barplot(data=ms_all,x='col',y='missing')\nplt.xlabel('feature')\nplt.xticks(rotation=60,ha='right')\nplt.title('All data missing values')","965bd2a5":"missing_val_col = [\"Alley\", \n                   \"PoolQC\", \n                   \"MiscFeature\",\n                   \"Fence\",\n                   \"FireplaceQu\",\n                   \"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2',\n                   'MasVnrType']\n\nfor i in missing_val_col:\n    data[i] = data[i].fillna('None')","9c6e5129":"data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","d8ed9e78":"missing_val_col2 = ['BsmtFinSF1',\n                    'BsmtFinSF2',\n                    'BsmtUnfSF',\n                    'TotalBsmtSF',\n                    'BsmtFullBath', \n                    'BsmtHalfBath', \n                    'GarageYrBlt',\n                    'GarageArea',\n                    'GarageCars',\n                    'MasVnrArea']\n\nfor i in missing_val_col2:\n    data[i] = data[i].fillna(0)","d8c71f7e":"missing_col3 = ['Utilities',\n               'Functional',\n               'Exterior1st',\n               'Exterior2nd',\n               'Electrical',\n               'KitchenQual',\n               'SaleType']\nfor i in missing_col3:\n    data[i] = data[i].fillna(data[i].mode()[0])","f4ce93b1":"missing_per(data) # Can't find missing values","85419c9f":"numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna()))\nskewed = pd.DataFrame({'skew':skewed_feats}).sort_values(by='skew',ascending=False)\nskewed.head()","e54bf6e1":"from scipy.special import boxcox1p\nskewed = skewed[abs(skewed)>0.75]\nlam = 0.15\nfor col in skewed.index:\n    data[col] = boxcox1p(data[col],lam)","f8c9e4d7":"data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n\ndata['YrBltAndRemod']=data['YearBuilt']+data['YearRemodAdd']\n\ndata['Total_sqr_footage'] = data['BsmtFinSF1'] +data['BsmtFinSF2'] + data['1stFlrSF'] + data['2ndFlrSF']\n\ndata['Total_Bathrooms'] =data['FullBath'] + (0.5 * data['HalfBath']) +data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\n\ndata['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +data['EnclosedPorch'] + data['ScreenPorch'] +data['WoodDeckSF'])","a82b2df9":"data['hasPool'] = data['PoolArea'].apply(lambda x: 1 if x>0 else 0)\ndata['hasFireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndata.drop('Fireplaces',axis=1,inplace=True)\ndata['hasGarage'] = data['GarageArea'].apply(lambda x: 1 if x>0 else 0)\ndata.drop('GarageArea',axis=1,inplace=True)\ndata['hasBsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x>0 else 0)\ndata['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)","18e24011":"data.drop(['PoolQC','Utilities','Street'],axis=1,inplace=True)","82669d22":"col = ['MSSubClass','OverallCond','YrSold','MoSold']\nfor i in col:\n    data[i] = data[i].astype(str)","71e170f0":"from sklearn.preprocessing import LabelEncoder\ncols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold']\n# LabelEncoder to categorical features\nle = LabelEncoder()\nfor i in cols:\n    data[i] = le.fit_transform(data[i].values)","dcd0408c":"final_data = pd.get_dummies(data).reset_index(drop=True)\nprint('Final data size is {}'.format(final_data.shape))","1637d944":"train_x = final_data[:ntrain]\ntest_x = final_data[ntrain:]","e9b9d349":"def overfit_reducer(df):\n    overfit=[]\n    for i in df.columns:\n        counts = df[i].value_counts().iloc[0]\n        if counts\/len(df) * 100 >99.9:\n            overfit.append(i)\n        else:\n            continue\n    return list(overfit)","5b133983":"overfit_features = overfit_reducer(train_x)\ntrain_x.drop(overfit_features,axis=1,inplace=True)\ntest_x.drop(overfit_features,axis=1,inplace=True)","dbc0a094":"from sklearn.model_selection import KFold,cross_val_score\ndef rmsle_cv(model):\n    n_fold = 5\n    kf = KFold(n_fold, shuffle=True,random_state=42).get_n_splits(train_x)\n    rmse = np.sqrt(-cross_val_score(model,train_x.values,train_y,scoring='neg_mean_squared_error',cv=kf))\n    return rmse","5c502874":"from sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom lightgbm.sklearn import LGBMRegressor","2a463aef":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline","157e12f5":"alpha_las=[0.0005,0.0001,0.00005,0.00001]\ne_ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","2a326bee":"lasso = make_pipeline(RobustScaler(),LassoCV(alphas=alpha_las,random_state=42,max_iter=1e7))\nridge = make_pipeline(RobustScaler(),RidgeCV(alphas = alpha_las))\nelastic = make_pipeline(RobustScaler(),ElasticNetCV(max_iter=1e7,alphas=alpha_las,l1_ratio = e_ratio))","c0025a9b":"krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nrf = RandomForestRegressor(bootstrap=True,max_depth=70,max_features='auto',min_samples_leaf=4,min_samples_split=10,n_estimators=2200)\ngra = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nlgbm = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","d812fa1d":"model = [lasso,ridge,elastic,krr,rf,gra,xgb,lgbm]\nmodel_name = ['Lasso','Ridge','ElasticNet','KernelRidge','RandomForest','GradientBoost','XGBoost','LGBM']\nfor i,j in zip(model,model_name):\n    print('{} rmse socre is {}'.format(j,rmsle_cv(i).mean()))","9a168204":"from mlxtend.regressor import StackingCVRegressor\nstack = StackingCVRegressor(regressors=(ridge,lasso,krr,gra,xgb,lgbm),\n                           meta_regressor=elastic,use_features_in_secondary=True)","0a5075d1":"rmsle_cv(stack).mean()","3726b83e":"rid_fit = ridge.fit(train_x,train_y)\nlasso_fit = lasso.fit(train_x,train_y)\nelastic_fit = elastic.fit(train_x,train_y)\nkrr_fit = krr.fit(train_x,train_y)\ngra_fit = gra.fit(train_x,train_y)\nxgb_fit = xgb.fit(train_x,train_y)\nlgbm_fit = lgbm.fit(train_x,train_y)","ea038efe":"stack_fit = stack.fit(np.array(train_x),np.array(train_y))","b92e1c97":"from sklearn.metrics import mean_squared_error\ndef rmsle(y,pred_y):\n    return np.sqrt(mean_squared_error(y,pred_y))","c7a61b91":"test_y=submission['SalePrice']","b53be7e5":"def blended_model_predict(X):\n    return ((0.2 * lasso_fit.predict(X))+(0.05*rid_fit.predict(X))+(0.2*elastic_fit.predict(X))+(0.05*krr_fit.predict(X))+\n           (0.15*gra_fit.predict(X))+(0.1*xgb_fit.predict(X))+ (0.15*lgbm_fit.predict(X))+\n            (0.1*stack_fit.predict(np.array(X))))\n    ","2b61256d":"print('Blended model score is {:.5f}'.format(rmsle(train_y,blended_model_predict(train_x))))\nblend=[]\nblend.append(rmsle(train_y,blended_model_predict(train_x)))","823c2fde":"test_id = submission['Id']\ntest_price_blend = np.expm1(blended_model_predict(test_x))\ntmp_blend = pd.DataFrame(columns=['Id','SalePrice'])\ntmp_blend['Id'] = test_id\ntmp_blend['SalePrice'] = test_price_blend","c522e388":"tmp_blend.to_csv('blend_predict.csv',index=False)\n# 0.11722 score","a4c5f8b3":"fit_model = [ridge,lasso,elastic,krr,gra,xgb,lgbm]\nfit_model_name = ['Ridge','Lasso','ElasticNet','KernelRidge','GradientBoost','XGBoost','LGBM']\ntmp = pd.DataFrame(columns=['model','score'])\nidx=0\nfor i,j in zip(fit_model,fit_model_name):\n    pred_y = i.predict(train_x)\n    print('{} model score is {:.5f}'.format(j,rmsle(train_y,pred_y)))\n    tmp.loc[idx,'model'] = j\n    tmp.loc[idx,'score'] = rmsle(train_y,pred_y)\n    idx+=1\npred_stack = stack.predict(np.array(train_x))\nprint('Stacking model score is {:.5f}'.format(rmsle(train_y,pred_stack)))\ntmp.loc[idx,'model'] = 'Stack'\ntmp.loc[idx,'score'] = rmsle(train_y,pred_stack)\ntmp.sort_values(by='score',ascending=True)","e414a358":"mix_model = 0.7*np.expm1(gra.predict(test_x))+0.15*np.expm1(lgbm.predict(test_x))+0.15*np.expm1(krr.predict(test_x))","6411b262":"tmp_mix = pd.DataFrame()\ntmp_mix['Id'] = test_id\ntmp_mix['SalePrice'] = mix_model","813cf959":"tmp_mix.to_csv('mix_predict.csv',index=False)\n# 0.11998 score","ee725ef4":"final_model = 0.6*test_price_blend + 0.2*np.expm1(gra.predict(test_x)) +\\\n0.1*np.expm1(lgbm.predict(test_x))+0.1*np.expm1(krr.predict(test_x))","01c8cd48":"tmp_final = pd.DataFrame()\ntmp_final['Id'] = test_id\ntmp_final['SalePrice'] = final_model\ntmp_final.to_csv('final_predict.csv',index=False)","95d9d379":"If you want to know StackingCVRegressor, you will refer to [http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/](http:\/\/)","dbc1a581":"Calculate the RMSE score of the model for weight adjustment.","254fbb1b":"Lasso rmse socre is 0.11285137449325174 - 2 0.2\n\nRidge rmse socre is 0.12369331428588932 - 8 0.05\n\nElasticNet rmse socre is 0.1127704857393996 - 1 0.2\n\nKernelRidge rmse socre is 0.11781442034338312 - 7 0.05\n\nGradientBoost rmse socre is 0.11461781335632337 - 3 0.15\n\nXGBoost rmse socre is 0.11627725614277351 - 5 0.1\n\nLGBM rmse socre is 0.1146187308453809 - 4 0.15\n\nStacking 0.11651581352006143 - 6 0.1","95fec1c1":"#### Method 2\n- Each model prediction","8a990b76":"Lasso and Ridge may be very sensitive outliers. So we need to use RobustScaler.\n\nRobustScaler makes the median to be 0 and the IQR to be 1.","91d817a8":"- Normalized 'SalePrice' feature","c644dcea":"we can see outliers. we will remove outliers","3af19287":"- Method 3\n\nAll mixing model","4e8b369b":"- Checking Missing value","e91ac03e":"### box-cox Transformation\n- How to make non-normalized normalized\n- The reason for using boxcox1p is because Saleprice feature also used log1p","ffa7ec6a":"- feature processing","78160bb6":"# Modeling","8f56a7b7":"- There is 83% correlation between **GarageYrBlt** and **YearBuilt**.\n- 83% correlation between **TotRmsAbvGrd** and **GrLivArea**.\n- 89% correlation between **GarageCars** and **GarageArea**.\n- Similarly many other features such as **BsmtUnfSF**, **FullBath** have good correlation with other independent feature but not so much with the dependent feature.","7931ac73":"# Feature Engineering","9665175a":"ElasticNet model has best score followed by Lasso,Gradient Boost.\n\nAnd RandomForest model has worst score therefore I don't select this model.","03d59910":"- Creating Derived Variable","c093e0e8":"- \uadf8 \uc804\uc5d0 \ubaa8\ub378\uc758 rmsle \uc810\uc218 \ubf51\uc544\uc11c predict(train_x) rmsle \ubaa8\ub378 \uacb0\ud569 \uac00\uc911\uce58 \uc870\uc815(blend model)\n- train\uc5d0\uc11c rmsle \uc810\uc218 \ubf51\uc544\ub0b4\uace0 \uadf8 \ub2f4\uc5d0 predict(test_x)\ud574\uc11c \uac00\uc911\uce58 \ube44\uc728 \uc870\uc815 \ud6c4 submit","8e281f95":"#### Method 1\n- Blended_model","3daf0c47":"Looking at the graph, I thought it was mostly linear.\n\nSo, I will draw regplot.","dc7bc0cb":"# EDA","0dbee13c":"\ubaa8\ub378 \ubaa8\ub450 fit\ud558\uace0 \uac00\uc911\uce58 \uc900 blended_model_predict \ud568\uc218 \uc0dd\uc131 \ud6c4 predict(trian_x) \ud558\uace0 \uc810\uc218 \ud655\uc778\n\nif \ubcc4\ub85c\uba74 blended \uc548\uc4f0\uace0 predict(train_x)\ud55c\ub2e4. \uc5ec\uae30\uc11c \uc810\uc218 \ud655\uc778 \ud6c4 \uac00\uc911\uce58 \ub2e4\uc2dc \uc8fc\uace0 predict(test_x)\n\n\uc544\ub2d8 \uc3f5 \ub2e4 \uc81c\ucd9c blended_model_predict(test_x)\ub3c4 \uc81c\ucd9c","32d69315":"- Missing value processing"}}