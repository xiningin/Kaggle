{"cell_type":{"1e6db699":"code","17b9d732":"code","f471b867":"code","a1e08579":"code","e7604547":"code","98207a2f":"code","faf88a48":"code","64f90871":"code","b91841b2":"code","8e03b3e2":"code","59bbf35f":"code","0a794313":"code","ceeebd6b":"code","16f540c9":"code","3dfa8d17":"code","0603f31a":"code","d36c82c9":"code","c7d1f692":"code","25c96210":"code","1c7a1c3c":"code","0b6b10e7":"code","73b35405":"code","85ed3cdd":"code","5812e3cc":"code","0955c525":"code","58bf0484":"code","a01451dd":"code","c438b702":"code","a220a6c7":"code","9555b3db":"code","77433d3f":"code","93dd42ae":"code","2d3de575":"code","390fac22":"code","d086f6df":"code","52348334":"code","bda3292f":"code","5b495e8c":"code","c98ee87a":"code","004ec266":"markdown","b987018d":"markdown","8591d2b4":"markdown","64a33d33":"markdown","cf09a33a":"markdown","8b03bc38":"markdown","3dff20c0":"markdown","4cf3309a":"markdown","fd2d703a":"markdown","945bba3d":"markdown","b0a2038e":"markdown","f52fae28":"markdown","f0b89518":"markdown","453983a2":"markdown","155a0a53":"markdown","1850fe76":"markdown","8d391b24":"markdown","11265778":"markdown","1ee14012":"markdown","406e5a6a":"markdown","95463772":"markdown","53e1c6f4":"markdown","126241b9":"markdown","689df831":"markdown","45b35fe1":"markdown","ab0b5448":"markdown","9d7477f6":"markdown","00804419":"markdown","758cb77d":"markdown","7113c0ff":"markdown","491d2e31":"markdown","becd4ce9":"markdown","7b693267":"markdown","2fac2439":"markdown","45ea09ff":"markdown","a59832e3":"markdown"},"source":{"1e6db699":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scikitplot as skplt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom IPython.core.display import display, HTML\n#sns.set_style('darkgrid')","17b9d732":"from sklearn.datasets import load_boston\nboston_dataset = load_boston()\nhousing = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)","f471b867":"housing.head()","a1e08579":"housing['MEDV'] = boston_dataset.target","e7604547":"housing.head()","98207a2f":"housing.tail()","faf88a48":"housing.info()","64f90871":"housing.isnull().any()","b91841b2":"housing.isnull().sum()","8e03b3e2":"housing.describe()","59bbf35f":"housing.hist(bins=40,figsize=(20,15))","0a794313":"X = housing.iloc[:, 0:13].values\ny = housing.iloc[:, 13].values.reshape(-1,1)","ceeebd6b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","16f540c9":"print(f\"Shape of X_train: {X_train.shape}\\nShape of X_test: {X_test.shape}\\nShape of y_train: {y_train.shape}\\nShape of y_test {y_test.shape}\\n\")","3dfa8d17":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nsplit.get_n_splits(X, y)\nfor train_index,test_index in split.split(housing,housing['CHAS']):\n    print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n   \n    strat_X_train = housing.loc[train_index]\n    strat_X_test = housing.loc[test_index]","0603f31a":"# A Category in Train Data\nstrat_X_train['CHAS'].value_counts()","d36c82c9":"# A Category in Train Data\nstrat_X_test['CHAS'].value_counts()","c7d1f692":"# Ratio of different values assigned in a category in train data \n(376\/28)","25c96210":"# Ratio of different values assigned in a category in test data \n95\/7","1c7a1c3c":"strat_X_train.describe()","0b6b10e7":"strat_X_train.info()","73b35405":"#Looking for correlation\n\ncorr_matrix = housing.corr()\n#Plot figsize\ncorr_matrix['MEDV'].sort_values(ascending=False)","85ed3cdd":"corr = housing.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(18, 18))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap='RdBu', annot=True, fmt=\".3f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","5812e3cc":"sns.pairplot(housing)\nplt.show()","0955c525":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nmy_pipeline= Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('std_scalar',StandardScaler())])","58bf0484":"housing_tr= my_pipeline.fit_transform(housing)\nhousing_tr","a01451dd":"from sklearn.linear_model import LinearRegression\nregression_linear = LinearRegression()\nregression_linear.fit(X_train, y_train)","c438b702":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score the Test set results\ncv_l = cross_val_score(estimator = regression_linear, X = X_train, y = y_train, cv = 10)\n\n# Predicting R2 Score the Train set results\ny_train_predict_l = regression_linear.predict(X_train)\nr2_score_train_l = r2_score(y_train, y_train_predict_l)\n\n# Predicting R2 Score the Test set results\ny_test_predict_l = regression_linear.predict(X_test)\nr2_score_test_l = r2_score(y_test, y_test_predict_l)\n\n# Predicting RMSE the Test set results\nrmse_l = (np.sqrt(mean_squared_error(y_test, y_test_predict_l)))\nprint(\"CV_mean: \", cv_l.mean())\nprint(\"CV_Std: \", cv_l.std())\nprint('R2_score (train): ', r2_score_train_l)\nprint('R2_score (test): ', r2_score_test_l)\nprint(\"RMSE: \", rmse_l)","a220a6c7":"skplt.estimators.plot_learning_curve(regression_linear, X, y,cv=10)\nplt.show()","9555b3db":"# Fitting the Decision Tree Regression Model to the dataset\nfrom sklearn.tree import DecisionTreeRegressor\nregression_dt = DecisionTreeRegressor(random_state = 0)\nregression_dt.fit(X_train, y_train)","77433d3f":"from sklearn.metrics import r2_score\n\n# For Cross Validation Score\ncv_dt = cross_val_score(estimator = regression_dt, X = X_train, y = y_train, cv = 10)\n\n# For R2 Score the Train set results\ny_train_predict_dt = regression_dt.predict(X_train)\nr2_score_train_dt = r2_score(y_train, y_train_predict_dt)\n\n# For R2 Score the Test set results\ny_test_predict_dt = regression_dt.predict(X_test)\nr2_score_test_dt = r2_score(y_test, y_test_predict_dt)\n\n# For RMSE the Test set results\nrmse_dt = (np.sqrt(mean_squared_error(y_test, y_test_predict_dt)))\nprint('CV_mean: ', cv_dt.mean())\nprint('CV_Std: ', cv_dt.std())\nprint('R2_score (train): ', r2_score_train_dt)\nprint('R2_score (test): ', r2_score_test_dt)\nprint(\"RMSE: \", rmse_dt)","93dd42ae":"skplt.estimators.plot_learning_curve(regression_dt, X, y ,cv=10)\nplt.show()","2d3de575":"# Fitting the Random Forest Regression to the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nregression_rf = RandomForestRegressor(n_estimators = 500, random_state = 0)\nregression_rf.fit(X_train, y_train.ravel())","390fac22":"from sklearn.metrics import r2_score\n\n# For Cross Validation Score\ncv_rf = cross_val_score(estimator = regression_rf, X = X_train, y = y_train.ravel(), cv = 10)\n\n# For R2 Score the Train set results\ny_train_predict_rf = regression_rf.predict(X_train)\nr2_score_train_rf = r2_score(y_train, y_train_predict_rf)\n\n# For R2 Score the Test set results\ny_test_predict_rf = regression_rf.predict(X_test)\nr2_score_test_rf = r2_score(y_test, y_test_predict_rf)\n\n# For RMSE the Test set results\nrmse_rf = (np.sqrt(mean_squared_error(y_test, y_test_predict_rf)))\nprint('CV_mean: ', cv_rf.mean())\nprint('CV_Std: ', cv_rf.std())\nprint('R2_score (train): ', r2_score_train_rf)\nprint('R2_score (test): ', r2_score_test_rf)\nprint(\"RMSE: \", rmse_rf)","d086f6df":"skplt.estimators.plot_learning_curve(regression_rf, X, y.ravel(), cv=10)\nplt.show()","52348334":"models = [('Linear Regression', rmse_l, r2_score_train_l, r2_score_test_l, cv_l.mean(),cv_l.std()),\n          ('Decision Tree Regression', rmse_dt, r2_score_train_dt, r2_score_test_dt, cv_dt.mean(),cv_dt.std()),\n          ('Random Forest Regression', rmse_rf, r2_score_train_rf, r2_score_test_rf, cv_rf.mean(),cv_rf.std())   \n         ]","bda3292f":"predict = pd.DataFrame(data = models, columns=['Model', 'RMSE', 'R2_Score(train)', 'R2_Score(test)', ' Cross-Validation_mean',' Cross-Validation_Std'],)\npredict","5b495e8c":"fig, axes = plt.subplots(2,1, figsize=(18,12))\n\npredict.sort_values(by=['R2_Score(train)'], ascending=False, inplace=True)\n\nsns.barplot(x='R2_Score(train)', y='Model', data = predict, palette='muted', ax = axes[0])\naxes[0].set_xlabel('R2 Score (Train)', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\n\npredict.sort_values(by=['R2_Score(test)'], ascending=False, inplace=True)\n\nsns.barplot(x='R2_Score(test)', y='Model', data = predict, palette='Blues_d', ax = axes[1])\naxes[1].set_xlabel('R2 Score (Test)', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\n\nplt.show()","c98ee87a":"predict.sort_values(by=['RMSE'], ascending=False, inplace=True)\nfig, axe = plt.subplots(1,1, figsize=(10,6))\nsns.barplot(x='Model', y='RMSE', data=predict, ax = axe)\naxe.set_xlabel('Model', size=16)\naxe.set_ylabel('RMSE', size=16)\n\nplt.show()\n","004ec266":"### <span id=\"13\"><\/span> ** Random Forest Regression **\n\nA random regression forest is an ensemble of randomized regression trees. Denote the predicted value at point by the -th tree, where are independent random variables, distributed as a generic random variable , independent of the sample .","b987018d":"# .info()\nThen, the data has what informations. We are learning the information for all data\n\nThe info function shows the data types and numerical values of the features in our data set. In short, this information about our data set. :)","8591d2b4":"So Again i will be checking the Features and their statistical interpretation. ","64a33d33":".. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric\/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n.. topic:: References\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n","cf09a33a":"# Dataframe.iloc[]\n**Here we are seprating X and Y columns for Test and prediction** \n\nPandas provide a unique method to retrieve rows from a Data frame. Dataframe.iloc[] method is used when the index label of a data frame is something other than numeric series of 0, 1, 2, 3\u2026.n or in case the user doesn\u2019t know the index label. Rows can be extracted using an imaginary index position which isn\u2019t visible in the data frame.","8b03bc38":" ## <span id=\"6\"><\/span> ** 6. Creating a pipeline for feature scaling  **","3dff20c0":"Now I want to check whether the Stratified Sampling is working or not. So i have chosen a Feature in which Data is inputted in 0 and 1. So i will be checking that the distribution of zeroes and ones are same in Train data and Test data or not.","4cf3309a":"\n# .isnull () Detects missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).","fd2d703a":"# Splitting the dataset \n\n \nThe train_test_split function is for splitting a single dataset for two different purposes: training and testing. The testing subset is for building your model. The testing subset is for using the model on unknown data to evaluate the performance of the model.","945bba3d":"## <span id=\"10\"><\/span> ** 10.Final Conclusion **","b0a2038e":"# One Visualization to Rule Them All\nWe will perform analysis on the training data. The relationship between the features found in the training data is observed. In this way, inference about the properties can be made.\n\n# sns.pairplot\n\nSeaborn Pairplot uses to get the relation between each and every variable present in Pandas DataFrame. It works like a seaborn scatter plot but it plot only two variables plot and sns paiplot plot the pairwise plot of multiple features\/variable in a grid format.","f52fae28":" ## <span id=\"7\"><\/span> ** 7. Selecting desired models  **","f0b89518":"Now, our data is loaded. We're writing the following snippet to see the loaded data. The purpose here is to see the top five of the loaded data.","453983a2":" ## <span id=\"8\"><\/span> ** 9. Visualizing Model Performance**\n \n#  sns.barplot\n\nA bar plot represents an estimate of central tendency for a numeric variable with the height of each rectangle and provides some indication of the uncertainty around that estimate using error bars. ","155a0a53":"## <span id=\"8\"><\/span> ** 8.Error Measure in Models **\n\nHere we will be measuring the Error in the Respective Models","1850fe76":"## <span id=\"1\"><\/span> ** 1. Overview **","8d391b24":"# SimpleImputer\nis a scikit-learn class which is helpful in handling the missing data in the predictive model dataset. It replaces the NaN values with a specified placeholder.\nIt is implemented by the use of the SimpleImputer() method which takes the following arguments :\n\nmissing_values : The missing_values placeholder which has to be imputed. By default is NaN\nstategy : The data which will replace the NaN values from the dataset. The strategy argument can take the values \u2013 \u2018mean'(default), \u2018median\u2019, \u2018most_frequent\u2019 and \u2018constant\u2019.\nfill_value : The constant value to be given to the NaN data using the constant strategy.\n\n\n# Standardization \nscales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.\n\n![image.png](attachment:image.png)\n\nsklearn provides a class called StandardScaler which will standerdise the data.\n# Pipeline\nA machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.","11265778":"# Stratified Sampling\n\nStratified Sampling: This is a sampling technique that is best used when a statistical population can easily be broken down into distinctive sub-groups. Then samples are taken from each sub-groups based on the ratio of the sub groups size to the total population.","1ee14012":"Please make a valuable comment and let me know if any ,how to improve the performance of the model, visualization, preprocessing, Analysis or something in this kernel. This will definitely help me in future .\n\n<b><font color=\"blue\">Please don't forget to <\/font><\/b> <b><font color=\"orange\">UPVOTE <\/font><\/b> if you think this deserves, thank you. \ud83d\ude42\ud83d\udc4d\ud83d\udc4d\ud83d\udc4d","406e5a6a":"## <span id=\"3\"><\/span> ** 3. Data Analysis **","95463772":"  ## <span id=\"3\"><\/span> ** 4. Data Preprocessing  **","53e1c6f4":"All data contained in our data set have been checked to check for any null values. So there is no problem left for a general analysis.\n\nChecking for missing value. There is not any missing values as shown below.\nNow,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.\n","126241b9":"### <span id=\"12\"><\/span> ** Decision Tree Regression **\n\nDecision Tree - Regression. Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.","689df831":"1. Overview\n2. Importing libraries and Reaading Dataset\n3. Data Analysis\n4. Data Preprocessing\n5. Visualizing Data\n6. Creating a pipeline for feature Scaling\n7. Selecting desired Models\n     * Linear Regression\n     * Decision Tree Regression\n     * Random Forest Regression\n8. Error Measure in Models\n9. Visualizing Model Performance\n10. Final Conclusion \n\n","45b35fe1":"As you seen, there isn't \"MEDV\" column that we will try to predict. Let's add the column to our dataset.\n* Adding target variable to dataFrame","ab0b5448":"# .corr()\n\nPandas dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe. Any na values are automatically excluded. For any non-numeric data type columns in the dataframe it is ignored.\n\n\nSo here i will be looking for correlation in Features.","9d7477f6":"Now we are good to go, let's see the variety of datatypes in our dataset","00804419":" ### <span id=\"7\"><\/span> ** Linear Regression **\n\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.","758cb77d":"## <span id=\"2\"><\/span> ** 2. Importing Libraries and Reading the Dataset **","7113c0ff":"# .hist()\n\nIt plots histogram with customizable features.\n\nA histogram is a graphical display of data using bars of different heights. In a histogram, each bar groups numbers into ranges. Taller bars show that more data falls in that range. A histogram displays the shape and spread of continuous sample data.\n\nEverything fine up to now, Yet i will create histogram for better insights.  ","491d2e31":"# .head() and .tail()\nHead and tail functions are capable of 5 rows per time. But you can change this situation. So you can enter the desired value in the parameter section. The first function, ie head (), returns the initial values. The second function returns the last values.","becd4ce9":"  ## <span id=\"6\"><\/span> ** 5. Visualizing Data  **","7b693267":"# Report","2fac2439":" # **.describe()**\n\nAfter I get the main intuition, I am investigating further to see some analytical attributes.\n\nDescribe function includes analysis of all our numerical data. For this, count, mean, std, min,% 25,% 50,% 75, max values are given. The reason this section is important is that you can estimate the probability that the values found here are deviant data.","45ea09ff":"So In this notebook kernel, I have deployed three regression models using the Boston Housing Dataset. These are linear regression,decision tree regression and random forest regression. Afterward I have visualized and calculated the performance measure of the models. Out of which Random forest regression is the best suit for this dataset. ","a59832e3":"# **.heatmap()**\n\nFor better insight i will plot heatmap. \n\nThe Big colorful picture below which is called Heatmap helps us to understand how features are correlated to each other.\nPostive sign implies postive correlation between two features whereas Negative sign implies negative correlation between two features."}}