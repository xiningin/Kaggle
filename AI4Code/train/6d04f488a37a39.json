{"cell_type":{"029171e8":"code","31e21890":"code","23ed3b16":"code","6cb87f0c":"code","a3f56e39":"code","380e61ea":"code","cb73a54e":"code","c82b00c9":"code","45de8d2e":"code","9d2af8f7":"code","4249738b":"code","a6c133a3":"code","948fde74":"code","6df1ed89":"code","752f86fc":"code","5b1a52c7":"code","e0d50072":"code","c93ba94e":"code","1109de89":"code","9c180241":"code","66a90778":"code","02bb8736":"code","59d2665a":"code","695de6c3":"code","837aa9f1":"code","1d8c8384":"code","0c3c408d":"code","a5784199":"code","5ab89f03":"code","f2cff480":"code","194c33ee":"code","ae45ded8":"code","7c7ec250":"code","9c54e05e":"code","9917f0e1":"code","a3291ea0":"code","e6ebbc6a":"code","b10a19e9":"code","ceb3dd39":"markdown","ec4c3eca":"markdown","ea03593a":"markdown","879c7809":"markdown","6666d904":"markdown","1acf1760":"markdown","773acf4f":"markdown","ee1b7dbb":"markdown","e7ed986a":"markdown","765ccc01":"markdown","d382337c":"markdown","55770fd6":"markdown","7012a3cf":"markdown","1dc841bb":"markdown","322eb547":"markdown","39509519":"markdown","531bf9a8":"markdown","e59b3092":"markdown","d8f38b5a":"markdown","9835f1c0":"markdown","d8199f4e":"markdown"},"source":{"029171e8":"!pip install simple-colors","31e21890":"import numpy as np\nimport pandas as pd\nfrom simple_colors import *\nfrom termcolor import colored\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\nfrom scipy.stats import normaltest\nfrom scipy import stats\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","23ed3b16":"#Setting up options\n\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = \"{:,.3f}\".format","6cb87f0c":"# Load the data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/test.csv')","a3f56e39":"def data_desc(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('*******************')\n    print(cyan('General information of this dataset', 'bold'))\n    print('*******************\\n')\n    print(df.info())\n    \n    print('\\n*******************')\n    print(cyan('Number of rows and columns', 'bold'))\n    print('*******************\\n')\n    print(\"Number of rows:\", colored(df.shape[0], 'green', attrs=['bold']))\n    print(\"Number of columns:\", colored(df.shape[1], 'green', attrs=['bold']))\n    \n    # missing values\n    print('\\n*******************')\n    print(cyan('Missing value checking', 'bold'))\n    print('*******************\\n')\n    if df.isna().sum().sum() == 0:\n        print(colored('There are no missing values', 'green'))\n        print('*******************')\n    else:\n        print(colored('Missing value detected!', 'green', attrs=['bold']))\n        print(\"\\nTotal number of missing values:\", colored(sum(df.isna().sum()), 'green', attrs=['bold']))\n        \n        print('\\n*******************')\n        print(cyan('Missing values of features', 'bold'))\n        print('*******************\\n')\n        display(df.isna().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('Purples', axis = None))\n        print('\\n*******************')\n        print(cyan('Percentage of missing values of features', 'bold'))\n        print('*******************\\n')\n        display(round((df.isnull().sum() \/ (len(df.index)) * 100) , 3).sort_values(ascending = False).to_frame().rename({0:'%'}, axis = 1).T.style.background_gradient('PuBuGn', axis = None))\n\n        \n    # applying describe() method for categorical features\n    cat_feats = [col for col in df.columns if df[col].nunique() < 10 and col not in ('row_id', 'target')]\n    print('\\n*******************')\n    print(cyan('Categorical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total categorical (binary) features:\", colored(len(cat_feats), 'green', attrs=['bold']))\n    if len(cat_feats) == 0:\n        print('\\n*******************')\n        print(colored(\"There is no categorical feature in this data set!\", 'green', attrs = ['bold']))\n    else:\n        df = df[df.columns.difference(['row_id', 'target'], sort = False)]\n        display(df[cat_feats].describe())\n        \n        \n    # describe() for numerical features\n    cont_feats = [col for col in df.columns if df[col].nunique() >= 10 and col not in ('row_id', 'target')]\n    print('\\n*******************')\n    print(cyan('Numerical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total numerical features:\", colored(len(cont_feats), 'green', attrs=['bold']))\n    df = df[df.columns.difference(['row_id', 'target'], sort = False)]\n    display(df[cont_feats].describe())\n    \n    # Checking for duplicated rows -if any-\n    if df.duplicated().sum() == 0:\n        print('\\n*******************')\n        print(colored('There are no duplicates!', 'green', attrs=['bold']))\n        print('*******************')\n    else:\n        print('\\n*******************')\n        print(colored('Duplicates found!', 'green', attrs=['bold']))\n        print('*******************')\n        print()\n        print(colored('These are the fraction of the duplicated rows!', 'green', attrs=['bold']))\n        display(df[df.duplicated()].sample(frac = 0.001).reset_index(drop = True))\n\n    print('\\n*******************')\n    print(cyan('Preview of the data - Top 10 rows', 'bold'))\n    print('*******************\\n')\n    display(df.head(10))\n    print('*******************\\n')\n    \n    print('\\n*******************')\n    print(cyan('End of the report', 'bold'))","380e61ea":"data_desc(train)","cb73a54e":"data_desc(test)","c82b00c9":"plt.figure(figsize=(15, 12))\nax = sns.countplot(y=train[\"target\"], palette='muted', zorder=3, linewidth=5, orient='h', saturation=1, alpha=1)\nax.set_title('Distribution of Target', fontname = 'Times New Roman', fontsize = 30, color = '#8c49e7', x = 0.5, y = 1.05)\nbackground_color = \"#8c49e7\"\nsns.set_palette(['#ffd514']*120)\n\nfor a in ax.patches:\n    value = f'Amount and percentage of values: {a.get_width():,.0f} | {(a.get_width()\/train.shape[0]):,.3%}'\n    x = a.get_x() + a.get_width() \/ 2 - 5500\n    y = a.get_y() + a.get_height() \/ 2 \n    ax.text(x, y, value, ha='left', va='center', fontsize=18, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round4', linewidth=0.7))\n\n\n# ax.margins(-0.12, -0.12)\nax.grid(axis=\"x\")\n\nsns.despine(right=True)\nsns.despine(offset=15, trim=True)","45de8d2e":"categorical_features =[]\nnumerical_features =[]\n\nfor col in train.columns:\n    if train[col].nunique() < 10 and col not in ('row_id', 'target'):\n        categorical_features.append(col)\n    elif train[col].nunique() >= 10 and col not in ('row_id', 'target'):\n        numerical_features.append(col)\nprint('Catagoric features: ', categorical_features)\nprint()\nprint('Numerical features: ', numerical_features)","9d2af8f7":"def box_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last = remove_last - 1\n\n    fig.subplots_adjust(top = 0.975)\n    plt.subplots_adjust(left=0.1,\n                    #bottom=0.01, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        v0 = sns.color_palette(palette = \"pastel\").as_hex()[2]\n        ax = sns.boxplot(x = data[feature], color=v0, saturation=.75)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Values', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    plt.show()","4249738b":"box_plot(train, numerical_features, 'Box Plot of Numerical Features of Train Dataset')","a6c133a3":"box_plot(test, numerical_features, 'Box Plot of Numerical Features of Test Dataset')","948fde74":"def kde_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(40, 180))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.975)\n    plt.subplots_adjust(left=0.1,\n                    #bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(data[feature], color=\"m\", shade=True, label=\"%.3f\"%(data[feature].skew()))  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Density', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    \n    plt.show()","6df1ed89":"train_frac = train.sample(frac = 0.01).reset_index(drop = True)\n\nkde_plot(train_frac, numerical_features, titleText = 'KDE Plot of Numerical Features of Train Dataset', hue = None)","752f86fc":"test_frac = test.sample(frac = 0.01).reset_index(drop = True)\n\nkde_plot(test_frac, numerical_features, titleText = 'KDE Plot of Numerical Features of Test Dataset', hue = None)","5b1a52c7":"def correlation_matrix(data, features):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data[features].corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths = .5, square = True, cbar_kws = {\"shrink\": .70})\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily = 'sans', rotation = 90, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily = 'sans', rotation = 0, fontsize = 12)\n    plt.tight_layout()\n    plt.show()","e0d50072":"correlation_matrix(train, numerical_features)","c93ba94e":"# High-correlated feature pairs w\/o auto-correlations for train data set\nthreshold = 0.7\ncorrelation = train.corr()\n\npairs = (correlation[abs(correlation) > threshold][correlation != 1.0]).unstack().dropna().to_dict()\n\ncorr_pairs_train = pd.DataFrame(\n    list(\n        set([(tuple(sorted(key)), pairs[key]) for key in pairs])\n    ), columns=['Feature Pair', 'Correlation Between Features']\n)","1109de89":"corr_pairs_train","9c180241":"correlation_matrix(test, numerical_features)","66a90778":"# High-correlated feature pairs w\/o auto-correlations for test data set\nthreshold = 0.7\ncorrelation = test.corr()\n\npairs = (correlation[abs(correlation) > threshold][correlation != 1.0]).unstack().dropna().to_dict()\n\ncorr_pairs_test = pd.DataFrame(\n    list(\n        set([(tuple(sorted(key)), pairs[key]) for key in pairs])\n    ), columns=['Feature Pair', 'Correlation Between Features']\n)","02bb8736":"corr_pairs_test","59d2665a":"def hist_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.975)\n    plt.subplots_adjust(left=0.1,\n                    #bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.histplot(data[feature], edgecolor=\"black\", color=\"darkseagreen\", alpha=0.7)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Frequency', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","695de6c3":"hist_plot(train_frac, numerical_features, titleText = 'Histogram of Numerical Features of Train Dataset', hue = None)","837aa9f1":"hist_plot(test_frac, numerical_features, titleText = 'Histogram of Numerical Features of Test Dataset', hue = None)","1d8c8384":"def qqplot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(40, 180))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.975)\n    plt.subplots_adjust(left=0.1,\n                    #bottom=0.20,\n                    right=0.9,  \n                    wspace=0.5, \n                    hspace=0.5)\n        \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)   \n        stats.probplot(data[feature],plot=plt)\n        plt.title('\\nQ-Q Plot')\n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Sample Quantile', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","0c3c408d":"qqplot(train_frac, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","a5784199":"qqplot(test_frac, numerical_features, 'Q-Q Plot of Numerical Features of Test Dataset', hue=None)","5ab89f03":"# D'Agostino and Pearson's Test\n\ndef normality_check(data):\n  for i in numerical_features:\n    # normality test\n    stat, p = normaltest(data[[i]])\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpret results\n    alpha = 1e-2\n    if p > alpha:\n        print(f'{i} looks Gaussian (fail to reject H0)\\n')\n    else:\n        print(f'{i} does not look Gaussian (reject H0)\\n')","f2cff480":"normality_check(train)","194c33ee":"normality_check(test)","ae45ded8":"def detect_outliers(x, c = 1.5):\n    \"\"\"\n    Function to detect outliers.\n    \"\"\"\n    q1, q3 = np.percentile(x, [25,75])\n    iqr = (q3 - q1)\n    lob = q1 - (iqr * c)\n    uob = q3 + (iqr * c)\n\n    # Generate outliers\n\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies","7c7ec250":"# Detect all Outliers \n\n#outliers = detect_outliers(train['target'])\n#print(\"Total Outliers count for claim : \", len(outliers[0]))\n\n#print(\"\\nShape before removing outliers : \",train.shape)\n\n# Remove outliers\n#train.drop(outliers[0],inplace=True, errors = 'ignore')\n#print(\"Shape after removing outliers : \",train.shape)","9c54e05e":"train_iqr = pd.DataFrame()\ntrain_iqr.reindex(columns=[*train_iqr.columns.tolist(), \"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"], fill_value = 0)","9917f0e1":"from scipy.stats import iqr\n\ndata = []\n\nk = 0\ncolumns = [\"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"]\n\nfor i in numerical_features:\n\n    q1 = train[i].quantile(0.25)\n    q3 = train[i].quantile(0.75)\n    \n    iqr = (q3 - q1)\n    lob_1 = q1 - (iqr * 1.5)\n    uob_1 = q3 + (iqr * 1.5)\n    lob_3 = q1 - (iqr * 3)\n    uob_3 = q3 + (iqr * 3)\n    \n    number_uob_1 = f'{round(sum(train[numerical_features[k]] > uob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_1 = f'{round(sum(train[numerical_features[k]] < lob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_uob_3 = f'{round(sum(train[numerical_features[k]] > uob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_3 = f'{round(sum(train[numerical_features[k]] < lob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n\n    values = [number_lob_3, number_lob_1, number_uob_1, number_uob_3]\n    zipped = zip(columns, values)\n    a_dictionary = dict(zipped)\n    print(a_dictionary)\n    data.append(a_dictionary)\n    \n    k = k + 1","a3291ea0":"train_iqr = train_iqr.append(data, True)\ntrain_iqr.set_axis([numerical_features], axis='index')","e6ebbc6a":"def colour(value):\n\n    if float(value.strip('%')) > 10:\n      color = 'red'\n    elif float(value.strip('%')) > 5:\n        color = 'darkorange'   \n    else:\n      color = 'green'\n\n    return 'color: %s' % color\n\n# train_iqr = train_iqr.set_axis([numerical_features], axis='index')\ntrain_iqr = train_iqr.style.applymap(colour)","b10a19e9":"train_iqr","ceb3dd39":"* **It is very obvious that some features contain significant amount of outlier value in both data sets. This has to be handled.**","ec4c3eca":"**Here, it is determined which features are highly correlated. At the model step, pairs of features with high correlation will be handled distinctly.**","ea03593a":"<a id=\"categorical_variables\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.1. Numerical Variables<\/p>","879c7809":"<a id=\"norm_check_outlier_detect\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.2. Normality Check and Outlier Detection<\/p>","6666d904":"<a id=\"box_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.1. Box Plot of Numerical Variables<\/p>","1acf1760":"<a id=\"qq_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.5. Q-Q Plot of Numerical Variables<\/p>","773acf4f":"<a id=\"corr_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.3. Correlation Matrix of Numerical Variables<\/p>","ee1b7dbb":"* **This dataframe about how to manage outlier values during the feature engineering section while developing the model will be very helpful.** ","e7ed986a":"* **Since KDE plots are processed in a long time, plots were created on 1% of the data sets. Supporting the box chart, it can be seen from this chart that there are various outliers.**","765ccc01":"<a id=\"kde_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.2. KDE Plot of Numerical Variables<\/p>","d382337c":"* **The Q-Q plot with clues to the normal distribution also shows tremendously that the data is not normally distributed.**","55770fd6":"* **Since there is a large amount of data, it may make sense to randomly take some of the data and create the plots.**","7012a3cf":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">2. Exploratory Data Analysis (EDA)<\/p>\n\n* **All numerical and categorical variables will be explored in this section.**","1dc841bb":"* **The logic in the KDE plots is also executed in the histogram plots.**","322eb547":"<a id=\"hist_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.4. Histogram Plot of Numerical Variables<\/p>","39509519":"<a id=\"mild_extreme_outlier\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.1. Mild and Extreme Outlier Detection<\/p>","531bf9a8":"## <p style=\"background-color:#3a2c57; font-family:newtimeroman; margin-bottom:2px; font-size:32px; color: white; text-align:center\">Table of Content<\/p>  \n\n<a id=\"table-of-contents\"><\/a>\n1. [Preperation](#preperation)\n    * 1.1. [Loading Packages and Importing Libraries](#load_packages_import_libraries)\n    * 1.2. [Data Description](#data_description)\n2. [Exploratory Data Analysis (EDA)](#eda)\n    * 2.1. [Numerical Variables](#numerical_variables)\n        * 2.1.1. [Box Plot of Numerical Variables](#box_numerical_variables)\n        * 2.1.2. [KDE Plot of Numerical Variables](#kde_numerical_variables)\n        * 2.1.3. [Correlation Matrix of Numerical Variables](#corr_numerical_variables)\n        * 2.1.4. [Histogram Plot of Numerical Variables](#hist_numerical_variables)\n        * 2.1.5. [Q-Q Plot of Numerical Variables](#qq_numerical_variables)\n    * 2.2. [Normality Check and Outlier Detection](#norm_check_outlier_detect)\n       * 2.2.1. [Mild and Extreme Outlier Detection](#mild_extreme_outlier)","e59b3092":"[back to top](#table-of-contents)\n<a id=\"preperation\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">1. Preperation<\/p>\n\n\n<a id=\"load_packages_import_libraries\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.1. Loading Packages and Importing Libraries<\/p>\n\n* **Loading packages and importing some helpful libraries.**","d8f38b5a":"<a id=\"data_description\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.2. Data Description<\/p>\n\n* **First of all, some setting up options were made. It is aimed to show all rows and columns in order to improve the general view of data sets. Next, I will load the train and test data sets and display train and test data sets as well.**","9835f1c0":"* **Obviously, since the target variable is categorical in this model, it is not possible to directly determine the outlier of the target variable.** ","d8199f4e":"* **In fact, the correlation between many variables in both train and test data sets is very weak. But the correlated ones will be managed as said earlier.**"}}